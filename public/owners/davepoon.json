{
  "owner": {
    "id": "davepoon",
    "display_name": "Dave Poon",
    "type": "User",
    "avatar_url": "https://avatars.githubusercontent.com/u/138992?v=4",
    "url": "https://github.com/davepoon",
    "bio": null,
    "stats": {
      "total_repos": 1,
      "total_plugins": 51,
      "total_commands": 357,
      "total_skills": 37,
      "total_stars": 2229,
      "total_forks": 239
    }
  },
  "repos": [
    {
      "full_name": "davepoon/buildwithclaude",
      "url": "https://github.com/davepoon/buildwithclaude",
      "description": "A single hub to find Claude Skills, Agents, Commands, Hooks, Plugins, and Marketplace collections to extend Claude Code",
      "homepage": "https://www.buildwithclaude.com",
      "signals": {
        "stars": 2229,
        "forks": 239,
        "pushed_at": "2026-01-12T10:19:01Z",
        "created_at": "2025-07-25T02:26:45Z",
        "license": "MIT"
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 40237
        },
        {
          "path": ".github",
          "type": "tree",
          "size": null
        },
        {
          "path": ".github/workflows",
          "type": "tree",
          "size": null
        },
        {
          "path": ".github/workflows/discover-plugins.yml",
          "type": "blob",
          "size": 1824
        },
        {
          "path": ".github/workflows/test-plugin.yml",
          "type": "blob",
          "size": 3721
        },
        {
          "path": ".github/workflows/validate-subagents.yml",
          "type": "blob",
          "size": 1817
        },
        {
          "path": ".gitignore",
          "type": "blob",
          "size": 165
        },
        {
          "path": "CONTRIBUTING.md",
          "type": "blob",
          "size": 10776
        },
        {
          "path": "LICENSE",
          "type": "blob",
          "size": 1065
        },
        {
          "path": "README.md",
          "type": "blob",
          "size": 6655
        },
        {
          "path": "UPDATES.md",
          "type": "blob",
          "size": 2374
        },
        {
          "path": "buildwithclaude-commands.png",
          "type": "blob",
          "size": 436517
        },
        {
          "path": "buildwithclaude-homepage.png",
          "type": "blob",
          "size": 359753
        },
        {
          "path": "buildwithclaude-hooks.png",
          "type": "blob",
          "size": 388462
        },
        {
          "path": "buildwithclaude-mcp.png",
          "type": "blob",
          "size": 492144
        },
        {
          "path": "buildwithclaude-plugin-marketplaces.png",
          "type": "blob",
          "size": 490896
        },
        {
          "path": "buildwithclaude-plugins.png",
          "type": "blob",
          "size": 620996
        },
        {
          "path": "buildwithclaude-skills.png",
          "type": "blob",
          "size": 495383
        },
        {
          "path": "buildwithclaude-subagents.png",
          "type": "blob",
          "size": 472924
        },
        {
          "path": "hook-validation-report.json",
          "type": "blob",
          "size": 154
        },
        {
          "path": "mcp-servers.json",
          "type": "blob",
          "size": 100111
        },
        {
          "path": "package-lock.json",
          "type": "blob",
          "size": 412358
        },
        {
          "path": "package.json",
          "type": "blob",
          "size": 1296
        },
        {
          "path": "plugins",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/agents-blockchain-web3",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/agents-blockchain-web3/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/agents-blockchain-web3/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 504
        },
        {
          "path": "plugins/agents-blockchain-web3/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/agents-blockchain-web3/agents/blockchain-developer.md",
          "type": "blob",
          "size": 1989
        },
        {
          "path": "plugins/agents-blockchain-web3/agents/hyperledger-fabric-developer.md",
          "type": "blob",
          "size": 9215
        },
        {
          "path": "plugins/agents-business-finance",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/agents-business-finance/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/agents-business-finance/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 516
        },
        {
          "path": "plugins/agents-business-finance/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/agents-business-finance/agents/business-analyst.md",
          "type": "blob",
          "size": 1832
        },
        {
          "path": "plugins/agents-business-finance/agents/legal-advisor.md",
          "type": "blob",
          "size": 1890
        },
        {
          "path": "plugins/agents-business-finance/agents/payment-integration.md",
          "type": "blob",
          "size": 1997
        },
        {
          "path": "plugins/agents-business-finance/agents/quant-analyst.md",
          "type": "blob",
          "size": 2209
        },
        {
          "path": "plugins/agents-crypto-trading",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/agents-crypto-trading/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/agents-crypto-trading/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 545
        },
        {
          "path": "plugins/agents-crypto-trading/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/agents-crypto-trading/agents/arbitrage-bot.md",
          "type": "blob",
          "size": 2015
        },
        {
          "path": "plugins/agents-crypto-trading/agents/crypto-analyst.md",
          "type": "blob",
          "size": 2004
        },
        {
          "path": "plugins/agents-crypto-trading/agents/crypto-risk-manager.md",
          "type": "blob",
          "size": 2094
        },
        {
          "path": "plugins/agents-crypto-trading/agents/crypto-trader.md",
          "type": "blob",
          "size": 2025
        },
        {
          "path": "plugins/agents-crypto-trading/agents/defi-strategist.md",
          "type": "blob",
          "size": 1910
        },
        {
          "path": "plugins/agents-data-ai",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/agents-data-ai/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/agents-data-ai/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 665
        },
        {
          "path": "plugins/agents-data-ai/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/agents-data-ai/agents/ai-engineer.md",
          "type": "blob",
          "size": 1706
        },
        {
          "path": "plugins/agents-data-ai/agents/context-manager.md",
          "type": "blob",
          "size": 1612
        },
        {
          "path": "plugins/agents-data-ai/agents/data-engineer.md",
          "type": "blob",
          "size": 1676
        },
        {
          "path": "plugins/agents-data-ai/agents/data-scientist.md",
          "type": "blob",
          "size": 1792
        },
        {
          "path": "plugins/agents-data-ai/agents/hackathon-ai-strategist.md",
          "type": "blob",
          "size": 1905
        },
        {
          "path": "plugins/agents-data-ai/agents/llms-maintainer.md",
          "type": "blob",
          "size": 1772
        },
        {
          "path": "plugins/agents-data-ai/agents/ml-engineer.md",
          "type": "blob",
          "size": 1765
        },
        {
          "path": "plugins/agents-data-ai/agents/mlops-engineer.md",
          "type": "blob",
          "size": 1814
        },
        {
          "path": "plugins/agents-data-ai/agents/prompt-engineer.md",
          "type": "blob",
          "size": 1696
        },
        {
          "path": "plugins/agents-data-ai/agents/search-specialist.md",
          "type": "blob",
          "size": 2084
        },
        {
          "path": "plugins/agents-data-ai/agents/task-decomposition-expert.md",
          "type": "blob",
          "size": 1590
        },
        {
          "path": "plugins/agents-design-experience",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/agents-design-experience/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/agents-design-experience/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 485
        },
        {
          "path": "plugins/agents-design-experience/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/agents-design-experience/agents/accessibility-specialist.md",
          "type": "blob",
          "size": 2108
        },
        {
          "path": "plugins/agents-design-experience/agents/ui-ux-designer.md",
          "type": "blob",
          "size": 2496
        },
        {
          "path": "plugins/agents-development-architecture",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/agents-development-architecture/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/agents-development-architecture/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 752
        },
        {
          "path": "plugins/agents-development-architecture/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/agents-development-architecture/agents/backend-architect.md",
          "type": "blob",
          "size": 1506
        },
        {
          "path": "plugins/agents-development-architecture/agents/directus-developer.md",
          "type": "blob",
          "size": 3955
        },
        {
          "path": "plugins/agents-development-architecture/agents/drupal-developer.md",
          "type": "blob",
          "size": 4426
        },
        {
          "path": "plugins/agents-development-architecture/agents/frontend-developer.md",
          "type": "blob",
          "size": 2117
        },
        {
          "path": "plugins/agents-development-architecture/agents/graphql-architect.md",
          "type": "blob",
          "size": 1838
        },
        {
          "path": "plugins/agents-development-architecture/agents/ios-developer.md",
          "type": "blob",
          "size": 1506
        },
        {
          "path": "plugins/agents-development-architecture/agents/laravel-vue-developer.md",
          "type": "blob",
          "size": 3064
        },
        {
          "path": "plugins/agents-development-architecture/agents/mobile-developer.md",
          "type": "blob",
          "size": 1557
        },
        {
          "path": "plugins/agents-development-architecture/agents/nextjs-app-router-developer.md",
          "type": "blob",
          "size": 2545
        },
        {
          "path": "plugins/agents-development-architecture/agents/react-performance-optimization.md",
          "type": "blob",
          "size": 1703
        },
        {
          "path": "plugins/agents-development-architecture/agents/wordpress-developer.md",
          "type": "blob",
          "size": 8311
        },
        {
          "path": "plugins/agents-infrastructure-operations",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/agents-infrastructure-operations/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/agents-infrastructure-operations/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 647
        },
        {
          "path": "plugins/agents-infrastructure-operations/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/agents-infrastructure-operations/agents/cloud-architect.md",
          "type": "blob",
          "size": 1690
        },
        {
          "path": "plugins/agents-infrastructure-operations/agents/database-admin.md",
          "type": "blob",
          "size": 1617
        },
        {
          "path": "plugins/agents-infrastructure-operations/agents/database-optimization.md",
          "type": "blob",
          "size": 1656
        },
        {
          "path": "plugins/agents-infrastructure-operations/agents/database-optimizer.md",
          "type": "blob",
          "size": 1968
        },
        {
          "path": "plugins/agents-infrastructure-operations/agents/deployment-engineer.md",
          "type": "blob",
          "size": 1773
        },
        {
          "path": "plugins/agents-infrastructure-operations/agents/devops-troubleshooter.md",
          "type": "blob",
          "size": 1548
        },
        {
          "path": "plugins/agents-infrastructure-operations/agents/network-engineer.md",
          "type": "blob",
          "size": 1289
        },
        {
          "path": "plugins/agents-infrastructure-operations/agents/terraform-specialist.md",
          "type": "blob",
          "size": 1060
        },
        {
          "path": "plugins/agents-language-specialists",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/agents-language-specialists/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/agents-language-specialists/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 691
        },
        {
          "path": "plugins/agents-language-specialists/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/agents-language-specialists/agents/c-developer.md",
          "type": "blob",
          "size": 1109
        },
        {
          "path": "plugins/agents-language-specialists/agents/cpp-engineer.md",
          "type": "blob",
          "size": 1755
        },
        {
          "path": "plugins/agents-language-specialists/agents/golang-expert.md",
          "type": "blob",
          "size": 1819
        },
        {
          "path": "plugins/agents-language-specialists/agents/java-developer.md",
          "type": "blob",
          "size": 1736
        },
        {
          "path": "plugins/agents-language-specialists/agents/javascript-developer.md",
          "type": "blob",
          "size": 1053
        },
        {
          "path": "plugins/agents-language-specialists/agents/php-developer.md",
          "type": "blob",
          "size": 1994
        },
        {
          "path": "plugins/agents-language-specialists/agents/python-expert.md",
          "type": "blob",
          "size": 2022
        },
        {
          "path": "plugins/agents-language-specialists/agents/rails-expert.md",
          "type": "blob",
          "size": 2283
        },
        {
          "path": "plugins/agents-language-specialists/agents/ruby-expert.md",
          "type": "blob",
          "size": 1930
        },
        {
          "path": "plugins/agents-language-specialists/agents/rust-expert.md",
          "type": "blob",
          "size": 1945
        },
        {
          "path": "plugins/agents-language-specialists/agents/sql-expert.md",
          "type": "blob",
          "size": 1031
        },
        {
          "path": "plugins/agents-language-specialists/agents/typescript-expert.md",
          "type": "blob",
          "size": 1987
        },
        {
          "path": "plugins/agents-quality-security",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/agents-quality-security/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/agents-quality-security/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 786
        },
        {
          "path": "plugins/agents-quality-security/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/agents-quality-security/agents/api-security-audit.md",
          "type": "blob",
          "size": 1195
        },
        {
          "path": "plugins/agents-quality-security/agents/architect-review.md",
          "type": "blob",
          "size": 1236
        },
        {
          "path": "plugins/agents-quality-security/agents/code-reviewer.md",
          "type": "blob",
          "size": 849
        },
        {
          "path": "plugins/agents-quality-security/agents/command-expert.md",
          "type": "blob",
          "size": 1130
        },
        {
          "path": "plugins/agents-quality-security/agents/debugger.md",
          "type": "blob",
          "size": 1965
        },
        {
          "path": "plugins/agents-quality-security/agents/dx-optimizer.md",
          "type": "blob",
          "size": 1543
        },
        {
          "path": "plugins/agents-quality-security/agents/error-detective.md",
          "type": "blob",
          "size": 1061
        },
        {
          "path": "plugins/agents-quality-security/agents/incident-responder.md",
          "type": "blob",
          "size": 1345
        },
        {
          "path": "plugins/agents-quality-security/agents/mcp-security-auditor.md",
          "type": "blob",
          "size": 1754
        },
        {
          "path": "plugins/agents-quality-security/agents/mcp-server-architect.md",
          "type": "blob",
          "size": 2557
        },
        {
          "path": "plugins/agents-quality-security/agents/mcp-testing-engineer.md",
          "type": "blob",
          "size": 2793
        },
        {
          "path": "plugins/agents-quality-security/agents/performance-engineer.md",
          "type": "blob",
          "size": 1856
        },
        {
          "path": "plugins/agents-quality-security/agents/review-agent.md",
          "type": "blob",
          "size": 1915
        },
        {
          "path": "plugins/agents-quality-security/agents/security-auditor.md",
          "type": "blob",
          "size": 1654
        },
        {
          "path": "plugins/agents-quality-security/agents/test-automator.md",
          "type": "blob",
          "size": 1622
        },
        {
          "path": "plugins/agents-sales-marketing",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/agents-sales-marketing/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/agents-sales-marketing/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 578
        },
        {
          "path": "plugins/agents-sales-marketing/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/agents-sales-marketing/agents/content-marketer.md",
          "type": "blob",
          "size": 1694
        },
        {
          "path": "plugins/agents-sales-marketing/agents/customer-support.md",
          "type": "blob",
          "size": 1395
        },
        {
          "path": "plugins/agents-sales-marketing/agents/risk-manager.md",
          "type": "blob",
          "size": 2013
        },
        {
          "path": "plugins/agents-sales-marketing/agents/sales-automator.md",
          "type": "blob",
          "size": 1617
        },
        {
          "path": "plugins/agents-sales-marketing/agents/social-media-clip-creator.md",
          "type": "blob",
          "size": 2691
        },
        {
          "path": "plugins/agents-sales-marketing/agents/social-media-copywriter.md",
          "type": "blob",
          "size": 2172
        },
        {
          "path": "plugins/agents-specialized-domains",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/agents-specialized-domains/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/agents-specialized-domains/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 1585
        },
        {
          "path": "plugins/agents-specialized-domains/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/agents-specialized-domains/agents/academic-research-synthesizer.md",
          "type": "blob",
          "size": 1518
        },
        {
          "path": "plugins/agents-specialized-domains/agents/academic-researcher.md",
          "type": "blob",
          "size": 1429
        },
        {
          "path": "plugins/agents-specialized-domains/agents/agent-expert.md",
          "type": "blob",
          "size": 1356
        },
        {
          "path": "plugins/agents-specialized-domains/agents/api-documenter.md",
          "type": "blob",
          "size": 1818
        },
        {
          "path": "plugins/agents-specialized-domains/agents/audio-quality-controller.md",
          "type": "blob",
          "size": 2372
        },
        {
          "path": "plugins/agents-specialized-domains/agents/comprehensive-researcher.md",
          "type": "blob",
          "size": 1599
        },
        {
          "path": "plugins/agents-specialized-domains/agents/connection-agent.md",
          "type": "blob",
          "size": 1482
        },
        {
          "path": "plugins/agents-specialized-domains/agents/data-analyst.md",
          "type": "blob",
          "size": 1287
        },
        {
          "path": "plugins/agents-specialized-domains/agents/docusaurus-expert.md",
          "type": "blob",
          "size": 1434
        },
        {
          "path": "plugins/agents-specialized-domains/agents/episode-orchestrator.md",
          "type": "blob",
          "size": 1688
        },
        {
          "path": "plugins/agents-specialized-domains/agents/game-developer.md",
          "type": "blob",
          "size": 2007
        },
        {
          "path": "plugins/agents-specialized-domains/agents/legacy-modernizer.md",
          "type": "blob",
          "size": 2152
        },
        {
          "path": "plugins/agents-specialized-domains/agents/markdown-syntax-formatter.md",
          "type": "blob",
          "size": 1944
        },
        {
          "path": "plugins/agents-specialized-domains/agents/market-research-analyst.md",
          "type": "blob",
          "size": 2504
        },
        {
          "path": "plugins/agents-specialized-domains/agents/mcp-deployment-orchestrator.md",
          "type": "blob",
          "size": 2489
        },
        {
          "path": "plugins/agents-specialized-domains/agents/mcp-expert.md",
          "type": "blob",
          "size": 1228
        },
        {
          "path": "plugins/agents-specialized-domains/agents/mcp-registry-navigator.md",
          "type": "blob",
          "size": 1616
        },
        {
          "path": "plugins/agents-specialized-domains/agents/metadata-agent.md",
          "type": "blob",
          "size": 1759
        },
        {
          "path": "plugins/agents-specialized-domains/agents/moc-agent.md",
          "type": "blob",
          "size": 1853
        },
        {
          "path": "plugins/agents-specialized-domains/agents/ocr-grammar-fixer.md",
          "type": "blob",
          "size": 1699
        },
        {
          "path": "plugins/agents-specialized-domains/agents/ocr-quality-assurance.md",
          "type": "blob",
          "size": 1666
        },
        {
          "path": "plugins/agents-specialized-domains/agents/podcast-content-analyzer.md",
          "type": "blob",
          "size": 1168
        },
        {
          "path": "plugins/agents-specialized-domains/agents/podcast-metadata-specialist.md",
          "type": "blob",
          "size": 1847
        },
        {
          "path": "plugins/agents-specialized-domains/agents/podcast-transcriber.md",
          "type": "blob",
          "size": 1707
        },
        {
          "path": "plugins/agents-specialized-domains/agents/podcast-trend-scout.md",
          "type": "blob",
          "size": 1780
        },
        {
          "path": "plugins/agents-specialized-domains/agents/project-supervisor-orchestrator.md",
          "type": "blob",
          "size": 1779
        },
        {
          "path": "plugins/agents-specialized-domains/agents/query-clarifier.md",
          "type": "blob",
          "size": 1281
        },
        {
          "path": "plugins/agents-specialized-domains/agents/report-generator.md",
          "type": "blob",
          "size": 1896
        },
        {
          "path": "plugins/agents-specialized-domains/agents/research-brief-generator.md",
          "type": "blob",
          "size": 3046
        },
        {
          "path": "plugins/agents-specialized-domains/agents/research-coordinator.md",
          "type": "blob",
          "size": 1200
        },
        {
          "path": "plugins/agents-specialized-domains/agents/research-orchestrator.md",
          "type": "blob",
          "size": 2189
        },
        {
          "path": "plugins/agents-specialized-domains/agents/research-synthesizer.md",
          "type": "blob",
          "size": 1682
        },
        {
          "path": "plugins/agents-specialized-domains/agents/seo-podcast-optimizer.md",
          "type": "blob",
          "size": 2156
        },
        {
          "path": "plugins/agents-specialized-domains/agents/tag-agent.md",
          "type": "blob",
          "size": 1267
        },
        {
          "path": "plugins/agents-specialized-domains/agents/technical-researcher.md",
          "type": "blob",
          "size": 1309
        },
        {
          "path": "plugins/agents-specialized-domains/agents/text-comparison-validator.md",
          "type": "blob",
          "size": 1614
        },
        {
          "path": "plugins/agents-specialized-domains/agents/timestamp-precision-specialist.md",
          "type": "blob",
          "size": 1662
        },
        {
          "path": "plugins/agents-specialized-domains/agents/twitter-ai-influencer-manager.md",
          "type": "blob",
          "size": 1564
        },
        {
          "path": "plugins/agents-specialized-domains/agents/url-context-validator.md",
          "type": "blob",
          "size": 1724
        },
        {
          "path": "plugins/agents-specialized-domains/agents/url-link-extractor.md",
          "type": "blob",
          "size": 1606
        },
        {
          "path": "plugins/agents-specialized-domains/agents/visual-analysis-ocr.md",
          "type": "blob",
          "size": 1686
        },
        {
          "path": "plugins/all-agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-agents/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-agents/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 414
        },
        {
          "path": "plugins/all-agents/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-agents/agents/academic-research-synthesizer.md",
          "type": "blob",
          "size": 1518
        },
        {
          "path": "plugins/all-agents/agents/academic-researcher.md",
          "type": "blob",
          "size": 1429
        },
        {
          "path": "plugins/all-agents/agents/accessibility-specialist.md",
          "type": "blob",
          "size": 2108
        },
        {
          "path": "plugins/all-agents/agents/agent-expert.md",
          "type": "blob",
          "size": 1356
        },
        {
          "path": "plugins/all-agents/agents/ai-engineer.md",
          "type": "blob",
          "size": 1706
        },
        {
          "path": "plugins/all-agents/agents/api-documenter.md",
          "type": "blob",
          "size": 1818
        },
        {
          "path": "plugins/all-agents/agents/api-security-audit.md",
          "type": "blob",
          "size": 1195
        },
        {
          "path": "plugins/all-agents/agents/arbitrage-bot.md",
          "type": "blob",
          "size": 2015
        },
        {
          "path": "plugins/all-agents/agents/architect-review.md",
          "type": "blob",
          "size": 1236
        },
        {
          "path": "plugins/all-agents/agents/audio-quality-controller.md",
          "type": "blob",
          "size": 2372
        },
        {
          "path": "plugins/all-agents/agents/backend-architect.md",
          "type": "blob",
          "size": 1506
        },
        {
          "path": "plugins/all-agents/agents/blockchain-developer.md",
          "type": "blob",
          "size": 1989
        },
        {
          "path": "plugins/all-agents/agents/business-analyst.md",
          "type": "blob",
          "size": 1832
        },
        {
          "path": "plugins/all-agents/agents/c-developer.md",
          "type": "blob",
          "size": 1109
        },
        {
          "path": "plugins/all-agents/agents/cloud-architect.md",
          "type": "blob",
          "size": 1690
        },
        {
          "path": "plugins/all-agents/agents/code-reviewer.md",
          "type": "blob",
          "size": 849
        },
        {
          "path": "plugins/all-agents/agents/command-expert.md",
          "type": "blob",
          "size": 1130
        },
        {
          "path": "plugins/all-agents/agents/comprehensive-researcher.md",
          "type": "blob",
          "size": 1599
        },
        {
          "path": "plugins/all-agents/agents/connection-agent.md",
          "type": "blob",
          "size": 1482
        },
        {
          "path": "plugins/all-agents/agents/content-marketer.md",
          "type": "blob",
          "size": 1694
        },
        {
          "path": "plugins/all-agents/agents/context-manager.md",
          "type": "blob",
          "size": 1612
        },
        {
          "path": "plugins/all-agents/agents/cpp-engineer.md",
          "type": "blob",
          "size": 1755
        },
        {
          "path": "plugins/all-agents/agents/crypto-analyst.md",
          "type": "blob",
          "size": 2004
        },
        {
          "path": "plugins/all-agents/agents/crypto-risk-manager.md",
          "type": "blob",
          "size": 2094
        },
        {
          "path": "plugins/all-agents/agents/crypto-trader.md",
          "type": "blob",
          "size": 2025
        },
        {
          "path": "plugins/all-agents/agents/customer-support.md",
          "type": "blob",
          "size": 1395
        },
        {
          "path": "plugins/all-agents/agents/data-analyst.md",
          "type": "blob",
          "size": 1287
        },
        {
          "path": "plugins/all-agents/agents/data-engineer.md",
          "type": "blob",
          "size": 1676
        },
        {
          "path": "plugins/all-agents/agents/data-scientist.md",
          "type": "blob",
          "size": 1792
        },
        {
          "path": "plugins/all-agents/agents/database-admin.md",
          "type": "blob",
          "size": 1617
        },
        {
          "path": "plugins/all-agents/agents/database-optimization.md",
          "type": "blob",
          "size": 1656
        },
        {
          "path": "plugins/all-agents/agents/database-optimizer.md",
          "type": "blob",
          "size": 1968
        },
        {
          "path": "plugins/all-agents/agents/debugger.md",
          "type": "blob",
          "size": 1965
        },
        {
          "path": "plugins/all-agents/agents/defi-strategist.md",
          "type": "blob",
          "size": 1910
        },
        {
          "path": "plugins/all-agents/agents/deployment-engineer.md",
          "type": "blob",
          "size": 1773
        },
        {
          "path": "plugins/all-agents/agents/devops-troubleshooter.md",
          "type": "blob",
          "size": 1548
        },
        {
          "path": "plugins/all-agents/agents/directus-developer.md",
          "type": "blob",
          "size": 3955
        },
        {
          "path": "plugins/all-agents/agents/docusaurus-expert.md",
          "type": "blob",
          "size": 1434
        },
        {
          "path": "plugins/all-agents/agents/drupal-developer.md",
          "type": "blob",
          "size": 4426
        },
        {
          "path": "plugins/all-agents/agents/dx-optimizer.md",
          "type": "blob",
          "size": 1543
        },
        {
          "path": "plugins/all-agents/agents/episode-orchestrator.md",
          "type": "blob",
          "size": 1688
        },
        {
          "path": "plugins/all-agents/agents/error-detective.md",
          "type": "blob",
          "size": 1061
        },
        {
          "path": "plugins/all-agents/agents/frontend-developer.md",
          "type": "blob",
          "size": 2117
        },
        {
          "path": "plugins/all-agents/agents/game-developer.md",
          "type": "blob",
          "size": 2007
        },
        {
          "path": "plugins/all-agents/agents/golang-expert.md",
          "type": "blob",
          "size": 1819
        },
        {
          "path": "plugins/all-agents/agents/graphql-architect.md",
          "type": "blob",
          "size": 1838
        },
        {
          "path": "plugins/all-agents/agents/hackathon-ai-strategist.md",
          "type": "blob",
          "size": 1905
        },
        {
          "path": "plugins/all-agents/agents/hyperledger-fabric-developer.md",
          "type": "blob",
          "size": 9215
        },
        {
          "path": "plugins/all-agents/agents/incident-responder.md",
          "type": "blob",
          "size": 1345
        },
        {
          "path": "plugins/all-agents/agents/ios-developer.md",
          "type": "blob",
          "size": 1506
        },
        {
          "path": "plugins/all-agents/agents/java-developer.md",
          "type": "blob",
          "size": 1736
        },
        {
          "path": "plugins/all-agents/agents/javascript-developer.md",
          "type": "blob",
          "size": 1053
        },
        {
          "path": "plugins/all-agents/agents/laravel-vue-developer.md",
          "type": "blob",
          "size": 3064
        },
        {
          "path": "plugins/all-agents/agents/legacy-modernizer.md",
          "type": "blob",
          "size": 2152
        },
        {
          "path": "plugins/all-agents/agents/legal-advisor.md",
          "type": "blob",
          "size": 1890
        },
        {
          "path": "plugins/all-agents/agents/llms-maintainer.md",
          "type": "blob",
          "size": 1772
        },
        {
          "path": "plugins/all-agents/agents/markdown-syntax-formatter.md",
          "type": "blob",
          "size": 1944
        },
        {
          "path": "plugins/all-agents/agents/market-research-analyst.md",
          "type": "blob",
          "size": 2504
        },
        {
          "path": "plugins/all-agents/agents/mcp-deployment-orchestrator.md",
          "type": "blob",
          "size": 2489
        },
        {
          "path": "plugins/all-agents/agents/mcp-expert.md",
          "type": "blob",
          "size": 1228
        },
        {
          "path": "plugins/all-agents/agents/mcp-registry-navigator.md",
          "type": "blob",
          "size": 1616
        },
        {
          "path": "plugins/all-agents/agents/mcp-security-auditor.md",
          "type": "blob",
          "size": 1754
        },
        {
          "path": "plugins/all-agents/agents/mcp-server-architect.md",
          "type": "blob",
          "size": 2557
        },
        {
          "path": "plugins/all-agents/agents/mcp-testing-engineer.md",
          "type": "blob",
          "size": 2793
        },
        {
          "path": "plugins/all-agents/agents/metadata-agent.md",
          "type": "blob",
          "size": 1759
        },
        {
          "path": "plugins/all-agents/agents/ml-engineer.md",
          "type": "blob",
          "size": 1765
        },
        {
          "path": "plugins/all-agents/agents/mlops-engineer.md",
          "type": "blob",
          "size": 1814
        },
        {
          "path": "plugins/all-agents/agents/mobile-developer.md",
          "type": "blob",
          "size": 1557
        },
        {
          "path": "plugins/all-agents/agents/moc-agent.md",
          "type": "blob",
          "size": 1853
        },
        {
          "path": "plugins/all-agents/agents/network-engineer.md",
          "type": "blob",
          "size": 1289
        },
        {
          "path": "plugins/all-agents/agents/nextjs-app-router-developer.md",
          "type": "blob",
          "size": 2545
        },
        {
          "path": "plugins/all-agents/agents/ocr-grammar-fixer.md",
          "type": "blob",
          "size": 1699
        },
        {
          "path": "plugins/all-agents/agents/ocr-quality-assurance.md",
          "type": "blob",
          "size": 1666
        },
        {
          "path": "plugins/all-agents/agents/payment-integration.md",
          "type": "blob",
          "size": 1997
        },
        {
          "path": "plugins/all-agents/agents/performance-engineer.md",
          "type": "blob",
          "size": 1856
        },
        {
          "path": "plugins/all-agents/agents/php-developer.md",
          "type": "blob",
          "size": 1994
        },
        {
          "path": "plugins/all-agents/agents/podcast-content-analyzer.md",
          "type": "blob",
          "size": 1168
        },
        {
          "path": "plugins/all-agents/agents/podcast-metadata-specialist.md",
          "type": "blob",
          "size": 1847
        },
        {
          "path": "plugins/all-agents/agents/podcast-transcriber.md",
          "type": "blob",
          "size": 1707
        },
        {
          "path": "plugins/all-agents/agents/podcast-trend-scout.md",
          "type": "blob",
          "size": 1780
        },
        {
          "path": "plugins/all-agents/agents/project-supervisor-orchestrator.md",
          "type": "blob",
          "size": 1779
        },
        {
          "path": "plugins/all-agents/agents/prompt-engineer.md",
          "type": "blob",
          "size": 1696
        },
        {
          "path": "plugins/all-agents/agents/python-expert.md",
          "type": "blob",
          "size": 2022
        },
        {
          "path": "plugins/all-agents/agents/quant-analyst.md",
          "type": "blob",
          "size": 2209
        },
        {
          "path": "plugins/all-agents/agents/query-clarifier.md",
          "type": "blob",
          "size": 1281
        },
        {
          "path": "plugins/all-agents/agents/rails-expert.md",
          "type": "blob",
          "size": 2283
        },
        {
          "path": "plugins/all-agents/agents/react-performance-optimization.md",
          "type": "blob",
          "size": 1703
        },
        {
          "path": "plugins/all-agents/agents/report-generator.md",
          "type": "blob",
          "size": 1896
        },
        {
          "path": "plugins/all-agents/agents/research-brief-generator.md",
          "type": "blob",
          "size": 3046
        },
        {
          "path": "plugins/all-agents/agents/research-coordinator.md",
          "type": "blob",
          "size": 1200
        },
        {
          "path": "plugins/all-agents/agents/research-orchestrator.md",
          "type": "blob",
          "size": 2189
        },
        {
          "path": "plugins/all-agents/agents/research-synthesizer.md",
          "type": "blob",
          "size": 1682
        },
        {
          "path": "plugins/all-agents/agents/review-agent.md",
          "type": "blob",
          "size": 1915
        },
        {
          "path": "plugins/all-agents/agents/risk-manager.md",
          "type": "blob",
          "size": 2013
        },
        {
          "path": "plugins/all-agents/agents/ruby-expert.md",
          "type": "blob",
          "size": 1930
        },
        {
          "path": "plugins/all-agents/agents/rust-expert.md",
          "type": "blob",
          "size": 1945
        },
        {
          "path": "plugins/all-agents/agents/sales-automator.md",
          "type": "blob",
          "size": 1617
        },
        {
          "path": "plugins/all-agents/agents/search-specialist.md",
          "type": "blob",
          "size": 2084
        },
        {
          "path": "plugins/all-agents/agents/security-auditor.md",
          "type": "blob",
          "size": 1654
        },
        {
          "path": "plugins/all-agents/agents/seo-podcast-optimizer.md",
          "type": "blob",
          "size": 2156
        },
        {
          "path": "plugins/all-agents/agents/social-media-clip-creator.md",
          "type": "blob",
          "size": 2691
        },
        {
          "path": "plugins/all-agents/agents/social-media-copywriter.md",
          "type": "blob",
          "size": 2172
        },
        {
          "path": "plugins/all-agents/agents/sql-expert.md",
          "type": "blob",
          "size": 1031
        },
        {
          "path": "plugins/all-agents/agents/tag-agent.md",
          "type": "blob",
          "size": 1267
        },
        {
          "path": "plugins/all-agents/agents/task-decomposition-expert.md",
          "type": "blob",
          "size": 1590
        },
        {
          "path": "plugins/all-agents/agents/technical-researcher.md",
          "type": "blob",
          "size": 1309
        },
        {
          "path": "plugins/all-agents/agents/terraform-specialist.md",
          "type": "blob",
          "size": 1060
        },
        {
          "path": "plugins/all-agents/agents/test-automator.md",
          "type": "blob",
          "size": 1622
        },
        {
          "path": "plugins/all-agents/agents/text-comparison-validator.md",
          "type": "blob",
          "size": 1614
        },
        {
          "path": "plugins/all-agents/agents/timestamp-precision-specialist.md",
          "type": "blob",
          "size": 1662
        },
        {
          "path": "plugins/all-agents/agents/twitter-ai-influencer-manager.md",
          "type": "blob",
          "size": 1564
        },
        {
          "path": "plugins/all-agents/agents/typescript-expert.md",
          "type": "blob",
          "size": 1987
        },
        {
          "path": "plugins/all-agents/agents/ui-ux-designer.md",
          "type": "blob",
          "size": 2496
        },
        {
          "path": "plugins/all-agents/agents/url-context-validator.md",
          "type": "blob",
          "size": 1724
        },
        {
          "path": "plugins/all-agents/agents/url-link-extractor.md",
          "type": "blob",
          "size": 1606
        },
        {
          "path": "plugins/all-agents/agents/visual-analysis-ocr.md",
          "type": "blob",
          "size": 1686
        },
        {
          "path": "plugins/all-agents/agents/wordpress-developer.md",
          "type": "blob",
          "size": 8311
        },
        {
          "path": "plugins/all-commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-commands/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-commands/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 416
        },
        {
          "path": "plugins/all-commands/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-commands/commands/act.md",
          "type": "blob",
          "size": 410
        },
        {
          "path": "plugins/all-commands/commands/add-authentication-system.md",
          "type": "blob",
          "size": 3598
        },
        {
          "path": "plugins/all-commands/commands/add-changelog.md",
          "type": "blob",
          "size": 2064
        },
        {
          "path": "plugins/all-commands/commands/add-mutation-testing.md",
          "type": "blob",
          "size": 3707
        },
        {
          "path": "plugins/all-commands/commands/add-package.md",
          "type": "blob",
          "size": 3802
        },
        {
          "path": "plugins/all-commands/commands/add-performance-monitoring.md",
          "type": "blob",
          "size": 35273
        },
        {
          "path": "plugins/all-commands/commands/add-property-based-testing.md",
          "type": "blob",
          "size": 3851
        },
        {
          "path": "plugins/all-commands/commands/add-to-changelog.md",
          "type": "blob",
          "size": 1747
        },
        {
          "path": "plugins/all-commands/commands/all-tools.md",
          "type": "blob",
          "size": 1156
        },
        {
          "path": "plugins/all-commands/commands/architecture-review.md",
          "type": "blob",
          "size": 4013
        },
        {
          "path": "plugins/all-commands/commands/architecture-scenario-explorer.md",
          "type": "blob",
          "size": 15906
        },
        {
          "path": "plugins/all-commands/commands/bidirectional-sync.md",
          "type": "blob",
          "size": 7300
        },
        {
          "path": "plugins/all-commands/commands/big-features-interview.md",
          "type": "blob",
          "size": 515
        },
        {
          "path": "plugins/all-commands/commands/bug-fix.md",
          "type": "blob",
          "size": 534
        },
        {
          "path": "plugins/all-commands/commands/bulk-import-issues.md",
          "type": "blob",
          "size": 11379
        },
        {
          "path": "plugins/all-commands/commands/business-scenario-explorer.md",
          "type": "blob",
          "size": 10614
        },
        {
          "path": "plugins/all-commands/commands/changelog-demo-command.md",
          "type": "blob",
          "size": 299
        },
        {
          "path": "plugins/all-commands/commands/check-file.md",
          "type": "blob",
          "size": 1979
        },
        {
          "path": "plugins/all-commands/commands/check.md",
          "type": "blob",
          "size": 1378
        },
        {
          "path": "plugins/all-commands/commands/ci-setup.md",
          "type": "blob",
          "size": 8741
        },
        {
          "path": "plugins/all-commands/commands/clean-branches.md",
          "type": "blob",
          "size": 7532
        },
        {
          "path": "plugins/all-commands/commands/clean.md",
          "type": "blob",
          "size": 1779
        },
        {
          "path": "plugins/all-commands/commands/code-permutation-tester.md",
          "type": "blob",
          "size": 13617
        },
        {
          "path": "plugins/all-commands/commands/code-review.md",
          "type": "blob",
          "size": 2228
        },
        {
          "path": "plugins/all-commands/commands/code-to-task.md",
          "type": "blob",
          "size": 16419
        },
        {
          "path": "plugins/all-commands/commands/code_analysis.md",
          "type": "blob",
          "size": 1829
        },
        {
          "path": "plugins/all-commands/commands/commit-fast.md",
          "type": "blob",
          "size": 728
        },
        {
          "path": "plugins/all-commands/commands/commit.md",
          "type": "blob",
          "size": 2258
        },
        {
          "path": "plugins/all-commands/commands/constraint-modeler.md",
          "type": "blob",
          "size": 15087
        },
        {
          "path": "plugins/all-commands/commands/containerize-application.md",
          "type": "blob",
          "size": 3347
        },
        {
          "path": "plugins/all-commands/commands/context-prime.md",
          "type": "blob",
          "size": 305
        },
        {
          "path": "plugins/all-commands/commands/create-architecture-documentation.md",
          "type": "blob",
          "size": 3729
        },
        {
          "path": "plugins/all-commands/commands/create-command.md",
          "type": "blob",
          "size": 2465
        },
        {
          "path": "plugins/all-commands/commands/create-database-migrations.md",
          "type": "blob",
          "size": 45780
        },
        {
          "path": "plugins/all-commands/commands/create-docs.md",
          "type": "blob",
          "size": 1652
        },
        {
          "path": "plugins/all-commands/commands/create-feature.md",
          "type": "blob",
          "size": 4403
        },
        {
          "path": "plugins/all-commands/commands/create-jtbd.md",
          "type": "blob",
          "size": 1906
        },
        {
          "path": "plugins/all-commands/commands/create-onboarding-guide.md",
          "type": "blob",
          "size": 3465
        },
        {
          "path": "plugins/all-commands/commands/create-pr.md",
          "type": "blob",
          "size": 996
        },
        {
          "path": "plugins/all-commands/commands/create-prd.md",
          "type": "blob",
          "size": 1509
        },
        {
          "path": "plugins/all-commands/commands/create-prp.md",
          "type": "blob",
          "size": 6615
        },
        {
          "path": "plugins/all-commands/commands/create-pull-request.md",
          "type": "blob",
          "size": 2092
        },
        {
          "path": "plugins/all-commands/commands/create-worktrees.md",
          "type": "blob",
          "size": 5347
        },
        {
          "path": "plugins/all-commands/commands/cross-reference-manager.md",
          "type": "blob",
          "size": 5027
        },
        {
          "path": "plugins/all-commands/commands/debug-error.md",
          "type": "blob",
          "size": 4850
        },
        {
          "path": "plugins/all-commands/commands/decision-quality-analyzer.md",
          "type": "blob",
          "size": 13487
        },
        {
          "path": "plugins/all-commands/commands/decision-tree-explorer.md",
          "type": "blob",
          "size": 14676
        },
        {
          "path": "plugins/all-commands/commands/dependency-audit.md",
          "type": "blob",
          "size": 3714
        },
        {
          "path": "plugins/all-commands/commands/dependency-mapper.md",
          "type": "blob",
          "size": 9681
        },
        {
          "path": "plugins/all-commands/commands/design-database-schema.md",
          "type": "blob",
          "size": 26732
        },
        {
          "path": "plugins/all-commands/commands/design-rest-api.md",
          "type": "blob",
          "size": 41230
        },
        {
          "path": "plugins/all-commands/commands/digital-twin-creator.md",
          "type": "blob",
          "size": 14047
        },
        {
          "path": "plugins/all-commands/commands/directory-deep-dive.md",
          "type": "blob",
          "size": 1342
        },
        {
          "path": "plugins/all-commands/commands/doc-api.md",
          "type": "blob",
          "size": 7141
        },
        {
          "path": "plugins/all-commands/commands/docs.md",
          "type": "blob",
          "size": 2475
        },
        {
          "path": "plugins/all-commands/commands/e2e-setup.md",
          "type": "blob",
          "size": 7323
        },
        {
          "path": "plugins/all-commands/commands/estimate-assistant.md",
          "type": "blob",
          "size": 12896
        },
        {
          "path": "plugins/all-commands/commands/explain-code.md",
          "type": "blob",
          "size": 6806
        },
        {
          "path": "plugins/all-commands/commands/explain-issue-fix.md",
          "type": "blob",
          "size": 955
        },
        {
          "path": "plugins/all-commands/commands/find.md",
          "type": "blob",
          "size": 5283
        },
        {
          "path": "plugins/all-commands/commands/five.md",
          "type": "blob",
          "size": 1811
        },
        {
          "path": "plugins/all-commands/commands/fix-github-issue.md",
          "type": "blob",
          "size": 682
        },
        {
          "path": "plugins/all-commands/commands/fix-issue.md",
          "type": "blob",
          "size": 179
        },
        {
          "path": "plugins/all-commands/commands/fix-pr.md",
          "type": "blob",
          "size": 214
        },
        {
          "path": "plugins/all-commands/commands/future-scenario-generator.md",
          "type": "blob",
          "size": 13468
        },
        {
          "path": "plugins/all-commands/commands/generate-api-documentation.md",
          "type": "blob",
          "size": 3723
        },
        {
          "path": "plugins/all-commands/commands/generate-linear-worklog.md",
          "type": "blob",
          "size": 4438
        },
        {
          "path": "plugins/all-commands/commands/generate-test-cases.md",
          "type": "blob",
          "size": 3611
        },
        {
          "path": "plugins/all-commands/commands/generate-tests.md",
          "type": "blob",
          "size": 2436
        },
        {
          "path": "plugins/all-commands/commands/git-status.md",
          "type": "blob",
          "size": 1515
        },
        {
          "path": "plugins/all-commands/commands/hotfix-deploy.md",
          "type": "blob",
          "size": 8739
        },
        {
          "path": "plugins/all-commands/commands/husky.md",
          "type": "blob",
          "size": 2425
        },
        {
          "path": "plugins/all-commands/commands/implement-caching-strategy.md",
          "type": "blob",
          "size": 15645
        },
        {
          "path": "plugins/all-commands/commands/implement-graphql-api.md",
          "type": "blob",
          "size": 45943
        },
        {
          "path": "plugins/all-commands/commands/init-project.md",
          "type": "blob",
          "size": 3630
        },
        {
          "path": "plugins/all-commands/commands/initref.md",
          "type": "blob",
          "size": 456
        },
        {
          "path": "plugins/all-commands/commands/issue-to-linear-task.md",
          "type": "blob",
          "size": 7215
        },
        {
          "path": "plugins/all-commands/commands/issue-triage.md",
          "type": "blob",
          "size": 12051
        },
        {
          "path": "plugins/all-commands/commands/linear-task-to-issue.md",
          "type": "blob",
          "size": 7849
        },
        {
          "path": "plugins/all-commands/commands/load-llms-txt.md",
          "type": "blob",
          "size": 423
        },
        {
          "path": "plugins/all-commands/commands/log.md",
          "type": "blob",
          "size": 7266
        },
        {
          "path": "plugins/all-commands/commands/market-response-modeler.md",
          "type": "blob",
          "size": 15926
        },
        {
          "path": "plugins/all-commands/commands/memory-spring-cleaning.md",
          "type": "blob",
          "size": 1392
        },
        {
          "path": "plugins/all-commands/commands/mermaid.md",
          "type": "blob",
          "size": 1820
        },
        {
          "path": "plugins/all-commands/commands/migrate-to-typescript.md",
          "type": "blob",
          "size": 3563
        },
        {
          "path": "plugins/all-commands/commands/migration-assistant.md",
          "type": "blob",
          "size": 6975
        },
        {
          "path": "plugins/all-commands/commands/migration-guide.md",
          "type": "blob",
          "size": 7558
        },
        {
          "path": "plugins/all-commands/commands/milestone-tracker.md",
          "type": "blob",
          "size": 10201
        },
        {
          "path": "plugins/all-commands/commands/modernize-deps.md",
          "type": "blob",
          "size": 1388
        },
        {
          "path": "plugins/all-commands/commands/move.md",
          "type": "blob",
          "size": 4891
        },
        {
          "path": "plugins/all-commands/commands/optimize-build.md",
          "type": "blob",
          "size": 5177
        },
        {
          "path": "plugins/all-commands/commands/optimize-bundle-size.md",
          "type": "blob",
          "size": 10333
        },
        {
          "path": "plugins/all-commands/commands/optimize-database-performance.md",
          "type": "blob",
          "size": 18671
        },
        {
          "path": "plugins/all-commands/commands/optimize.md",
          "type": "blob",
          "size": 236
        },
        {
          "path": "plugins/all-commands/commands/pac-configure.md",
          "type": "blob",
          "size": 5018
        },
        {
          "path": "plugins/all-commands/commands/pac-create-epic.md",
          "type": "blob",
          "size": 4518
        },
        {
          "path": "plugins/all-commands/commands/pac-create-ticket.md",
          "type": "blob",
          "size": 5326
        },
        {
          "path": "plugins/all-commands/commands/pac-update-status.md",
          "type": "blob",
          "size": 5263
        },
        {
          "path": "plugins/all-commands/commands/pac-validate.md",
          "type": "blob",
          "size": 5401
        },
        {
          "path": "plugins/all-commands/commands/performance-audit.md",
          "type": "blob",
          "size": 3050
        },
        {
          "path": "plugins/all-commands/commands/pr-review.md",
          "type": "blob",
          "size": 2369
        },
        {
          "path": "plugins/all-commands/commands/prepare-release.md",
          "type": "blob",
          "size": 9151
        },
        {
          "path": "plugins/all-commands/commands/prime.md",
          "type": "blob",
          "size": 461
        },
        {
          "path": "plugins/all-commands/commands/project-health-check.md",
          "type": "blob",
          "size": 7526
        },
        {
          "path": "plugins/all-commands/commands/project-timeline-simulator.md",
          "type": "blob",
          "size": 17192
        },
        {
          "path": "plugins/all-commands/commands/project-to-linear.md",
          "type": "blob",
          "size": 5318
        },
        {
          "path": "plugins/all-commands/commands/refactor-code.md",
          "type": "blob",
          "size": 4558
        },
        {
          "path": "plugins/all-commands/commands/release.md",
          "type": "blob",
          "size": 367
        },
        {
          "path": "plugins/all-commands/commands/remove.md",
          "type": "blob",
          "size": 6714
        },
        {
          "path": "plugins/all-commands/commands/report.md",
          "type": "blob",
          "size": 6548
        },
        {
          "path": "plugins/all-commands/commands/repro-issue.md",
          "type": "blob",
          "size": 221
        },
        {
          "path": "plugins/all-commands/commands/resume.md",
          "type": "blob",
          "size": 7075
        },
        {
          "path": "plugins/all-commands/commands/retrospective-analyzer.md",
          "type": "blob",
          "size": 8485
        },
        {
          "path": "plugins/all-commands/commands/rollback-deploy.md",
          "type": "blob",
          "size": 10804
        },
        {
          "path": "plugins/all-commands/commands/rsi.md",
          "type": "blob",
          "size": 441
        },
        {
          "path": "plugins/all-commands/commands/run-ci.md",
          "type": "blob",
          "size": 1682
        },
        {
          "path": "plugins/all-commands/commands/security-audit.md",
          "type": "blob",
          "size": 2617
        },
        {
          "path": "plugins/all-commands/commands/security-hardening.md",
          "type": "blob",
          "size": 3557
        },
        {
          "path": "plugins/all-commands/commands/session-learning-capture.md",
          "type": "blob",
          "size": 2360
        },
        {
          "path": "plugins/all-commands/commands/setup-automated-releases.md",
          "type": "blob",
          "size": 4138
        },
        {
          "path": "plugins/all-commands/commands/setup-cdn-optimization.md",
          "type": "blob",
          "size": 19869
        },
        {
          "path": "plugins/all-commands/commands/setup-comprehensive-testing.md",
          "type": "blob",
          "size": 3323
        },
        {
          "path": "plugins/all-commands/commands/setup-development-environment.md",
          "type": "blob",
          "size": 3738
        },
        {
          "path": "plugins/all-commands/commands/setup-formatting.md",
          "type": "blob",
          "size": 1429
        },
        {
          "path": "plugins/all-commands/commands/setup-kubernetes-deployment.md",
          "type": "blob",
          "size": 3367
        },
        {
          "path": "plugins/all-commands/commands/setup-linting.md",
          "type": "blob",
          "size": 1868
        },
        {
          "path": "plugins/all-commands/commands/setup-load-testing.md",
          "type": "blob",
          "size": 4017
        },
        {
          "path": "plugins/all-commands/commands/setup-monitoring-observability.md",
          "type": "blob",
          "size": 3652
        },
        {
          "path": "plugins/all-commands/commands/setup-monorepo.md",
          "type": "blob",
          "size": 3746
        },
        {
          "path": "plugins/all-commands/commands/setup-rate-limiting.md",
          "type": "blob",
          "size": 59934
        },
        {
          "path": "plugins/all-commands/commands/setup-visual-testing.md",
          "type": "blob",
          "size": 3743
        },
        {
          "path": "plugins/all-commands/commands/simulation-calibrator.md",
          "type": "blob",
          "size": 13920
        },
        {
          "path": "plugins/all-commands/commands/sprint-planning.md",
          "type": "blob",
          "size": 4473
        },
        {
          "path": "plugins/all-commands/commands/standup-report.md",
          "type": "blob",
          "size": 5568
        },
        {
          "path": "plugins/all-commands/commands/start.md",
          "type": "blob",
          "size": 4656
        },
        {
          "path": "plugins/all-commands/commands/status.md",
          "type": "blob",
          "size": 4938
        },
        {
          "path": "plugins/all-commands/commands/svelte-a11y.md",
          "type": "blob",
          "size": 2724
        },
        {
          "path": "plugins/all-commands/commands/svelte-component.md",
          "type": "blob",
          "size": 1979
        },
        {
          "path": "plugins/all-commands/commands/svelte-debug.md",
          "type": "blob",
          "size": 1833
        },
        {
          "path": "plugins/all-commands/commands/svelte-migrate.md",
          "type": "blob",
          "size": 2146
        },
        {
          "path": "plugins/all-commands/commands/svelte-optimize.md",
          "type": "blob",
          "size": 2750
        },
        {
          "path": "plugins/all-commands/commands/svelte-scaffold.md",
          "type": "blob",
          "size": 2491
        },
        {
          "path": "plugins/all-commands/commands/svelte-storybook-migrate.md",
          "type": "blob",
          "size": 4579
        },
        {
          "path": "plugins/all-commands/commands/svelte-storybook-mock.md",
          "type": "blob",
          "size": 5538
        },
        {
          "path": "plugins/all-commands/commands/svelte-storybook-setup.md",
          "type": "blob",
          "size": 3016
        },
        {
          "path": "plugins/all-commands/commands/svelte-storybook-story.md",
          "type": "blob",
          "size": 3704
        },
        {
          "path": "plugins/all-commands/commands/svelte-storybook-troubleshoot.md",
          "type": "blob",
          "size": 4625
        },
        {
          "path": "plugins/all-commands/commands/svelte-storybook.md",
          "type": "blob",
          "size": 1699
        },
        {
          "path": "plugins/all-commands/commands/svelte-test-coverage.md",
          "type": "blob",
          "size": 2173
        },
        {
          "path": "plugins/all-commands/commands/svelte-test-fix.md",
          "type": "blob",
          "size": 2487
        },
        {
          "path": "plugins/all-commands/commands/svelte-test-setup.md",
          "type": "blob",
          "size": 2469
        },
        {
          "path": "plugins/all-commands/commands/svelte-test.md",
          "type": "blob",
          "size": 1956
        },
        {
          "path": "plugins/all-commands/commands/sync-automation-setup.md",
          "type": "blob",
          "size": 15994
        },
        {
          "path": "plugins/all-commands/commands/sync-conflict-resolver.md",
          "type": "blob",
          "size": 5556
        },
        {
          "path": "plugins/all-commands/commands/sync-issues-to-linear.md",
          "type": "blob",
          "size": 4557
        },
        {
          "path": "plugins/all-commands/commands/sync-linear-to-issues.md",
          "type": "blob",
          "size": 5493
        },
        {
          "path": "plugins/all-commands/commands/sync-pr-to-task.md",
          "type": "blob",
          "size": 8441
        },
        {
          "path": "plugins/all-commands/commands/sync-status.md",
          "type": "blob",
          "size": 10687
        },
        {
          "path": "plugins/all-commands/commands/sync.md",
          "type": "blob",
          "size": 6503
        },
        {
          "path": "plugins/all-commands/commands/system-behavior-simulator.md",
          "type": "blob",
          "size": 17734
        },
        {
          "path": "plugins/all-commands/commands/task-from-pr.md",
          "type": "blob",
          "size": 5465
        },
        {
          "path": "plugins/all-commands/commands/tdd.md",
          "type": "blob",
          "size": 2230
        },
        {
          "path": "plugins/all-commands/commands/team-workload-balancer.md",
          "type": "blob",
          "size": 24657
        },
        {
          "path": "plugins/all-commands/commands/test-changelog-automation.md",
          "type": "blob",
          "size": 318
        },
        {
          "path": "plugins/all-commands/commands/test-coverage.md",
          "type": "blob",
          "size": 6327
        },
        {
          "path": "plugins/all-commands/commands/testing_plan_integration.md",
          "type": "blob",
          "size": 607
        },
        {
          "path": "plugins/all-commands/commands/timeline-compressor.md",
          "type": "blob",
          "size": 15776
        },
        {
          "path": "plugins/all-commands/commands/todo.md",
          "type": "blob",
          "size": 2452
        },
        {
          "path": "plugins/all-commands/commands/troubleshooting-guide.md",
          "type": "blob",
          "size": 9041
        },
        {
          "path": "plugins/all-commands/commands/ultra-think.md",
          "type": "blob",
          "size": 4867
        },
        {
          "path": "plugins/all-commands/commands/unity-project-setup.md",
          "type": "blob",
          "size": 4738
        },
        {
          "path": "plugins/all-commands/commands/update-branch-name.md",
          "type": "blob",
          "size": 584
        },
        {
          "path": "plugins/all-commands/commands/update-docs.md",
          "type": "blob",
          "size": 2575
        },
        {
          "path": "plugins/all-commands/commands/use-stepper.md",
          "type": "blob",
          "size": 247
        },
        {
          "path": "plugins/all-commands/commands/write-tests.md",
          "type": "blob",
          "size": 5542
        },
        {
          "path": "plugins/all-hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-hooks/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-hooks/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 413
        },
        {
          "path": "plugins/all-hooks/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-hooks/hooks/auto-git-add.md",
          "type": "blob",
          "size": 683
        },
        {
          "path": "plugins/all-hooks/hooks/build-on-change.md",
          "type": "blob",
          "size": 781
        },
        {
          "path": "plugins/all-hooks/hooks/change-tracker.md",
          "type": "blob",
          "size": 628
        },
        {
          "path": "plugins/all-hooks/hooks/dependency-checker.md",
          "type": "blob",
          "size": 960
        },
        {
          "path": "plugins/all-hooks/hooks/discord-detailed-notifications.md",
          "type": "blob",
          "size": 1205
        },
        {
          "path": "plugins/all-hooks/hooks/discord-error-notifications.md",
          "type": "blob",
          "size": 1067
        },
        {
          "path": "plugins/all-hooks/hooks/discord-notifications.md",
          "type": "blob",
          "size": 902
        },
        {
          "path": "plugins/all-hooks/hooks/file-backup.md",
          "type": "blob",
          "size": 744
        },
        {
          "path": "plugins/all-hooks/hooks/file-protection-hook.md",
          "type": "blob",
          "size": 806
        },
        {
          "path": "plugins/all-hooks/hooks/file-protection.md",
          "type": "blob",
          "size": 798
        },
        {
          "path": "plugins/all-hooks/hooks/format-javascript-files.md",
          "type": "blob",
          "size": 758
        },
        {
          "path": "plugins/all-hooks/hooks/format-python-files.md",
          "type": "blob",
          "size": 684
        },
        {
          "path": "plugins/all-hooks/hooks/git-add-changes.md",
          "type": "blob",
          "size": 679
        },
        {
          "path": "plugins/all-hooks/hooks/lint-on-save.md",
          "type": "blob",
          "size": 823
        },
        {
          "path": "plugins/all-hooks/hooks/notify-before-bash.md",
          "type": "blob",
          "size": 882
        },
        {
          "path": "plugins/all-hooks/hooks/performance-monitor.md",
          "type": "blob",
          "size": 863
        },
        {
          "path": "plugins/all-hooks/hooks/run-tests-after-changes.md",
          "type": "blob",
          "size": 806
        },
        {
          "path": "plugins/all-hooks/hooks/security-scanner.md",
          "type": "blob",
          "size": 898
        },
        {
          "path": "plugins/all-hooks/hooks/simple-notifications.md",
          "type": "blob",
          "size": 875
        },
        {
          "path": "plugins/all-hooks/hooks/slack-detailed-notifications.md",
          "type": "blob",
          "size": 1222
        },
        {
          "path": "plugins/all-hooks/hooks/slack-error-notifications.md",
          "type": "blob",
          "size": 968
        },
        {
          "path": "plugins/all-hooks/hooks/slack-notifications.md",
          "type": "blob",
          "size": 789
        },
        {
          "path": "plugins/all-hooks/hooks/smart-commit.md",
          "type": "blob",
          "size": 834
        },
        {
          "path": "plugins/all-hooks/hooks/smart-formatting.md",
          "type": "blob",
          "size": 819
        },
        {
          "path": "plugins/all-hooks/hooks/telegram-detailed-notifications.md",
          "type": "blob",
          "size": 1076
        },
        {
          "path": "plugins/all-hooks/hooks/telegram-error-notifications.md",
          "type": "blob",
          "size": 1174
        },
        {
          "path": "plugins/all-hooks/hooks/telegram-notifications.md",
          "type": "blob",
          "size": 889
        },
        {
          "path": "plugins/all-hooks/hooks/test-runner.md",
          "type": "blob",
          "size": 852
        },
        {
          "path": "plugins/all-skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-skills/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-skills/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 1102
        },
        {
          "path": "plugins/all-skills/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-skills/skills/artifacts-builder",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-skills/skills/artifacts-builder/LICENSE.txt",
          "type": "blob",
          "size": 11357
        },
        {
          "path": "plugins/all-skills/skills/artifacts-builder/SKILL.md",
          "type": "blob",
          "size": 3109
        },
        {
          "path": "plugins/all-skills/skills/artifacts-builder/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-skills/skills/artifacts-builder/scripts/bundle-artifact.sh",
          "type": "blob",
          "size": 1517
        },
        {
          "path": "plugins/all-skills/skills/artifacts-builder/scripts/init-artifact.sh",
          "type": "blob",
          "size": 9924
        },
        {
          "path": "plugins/all-skills/skills/artifacts-builder/scripts/shadcn-components.tar.gz",
          "type": "blob",
          "size": 19967
        },
        {
          "path": "plugins/all-skills/skills/brand-guidelines",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-skills/skills/brand-guidelines/LICENSE.txt",
          "type": "blob",
          "size": 11357
        },
        {
          "path": "plugins/all-skills/skills/brand-guidelines/SKILL.md",
          "type": "blob",
          "size": 2265
        },
        {
          "path": "plugins/all-skills/skills/canvas-design",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/LICENSE.txt",
          "type": "blob",
          "size": 11357
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/SKILL.md",
          "type": "blob",
          "size": 11969
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/ArsenalSC-OFL.txt",
          "type": "blob",
          "size": 4373
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/ArsenalSC-Regular.ttf",
          "type": "blob",
          "size": 165848
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/BigShoulders-Bold.ttf",
          "type": "blob",
          "size": 94528
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/BigShoulders-OFL.txt",
          "type": "blob",
          "size": 4397
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/BigShoulders-Regular.ttf",
          "type": "blob",
          "size": 94396
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/Boldonse-OFL.txt",
          "type": "blob",
          "size": 4390
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/Boldonse-Regular.ttf",
          "type": "blob",
          "size": 77168
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/BricolageGrotesque-Bold.ttf",
          "type": "blob",
          "size": 90952
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/BricolageGrotesque-OFL.txt",
          "type": "blob",
          "size": 4403
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/BricolageGrotesque-Regular.ttf",
          "type": "blob",
          "size": 90920
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/CrimsonPro-Bold.ttf",
          "type": "blob",
          "size": 107352
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/CrimsonPro-Italic.ttf",
          "type": "blob",
          "size": 108828
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/CrimsonPro-OFL.txt",
          "type": "blob",
          "size": 4394
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/CrimsonPro-Regular.ttf",
          "type": "blob",
          "size": 106696
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/DMMono-OFL.txt",
          "type": "blob",
          "size": 4392
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/DMMono-Regular.ttf",
          "type": "blob",
          "size": 48852
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/EricaOne-OFL.txt",
          "type": "blob",
          "size": 4410
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/EricaOne-Regular.ttf",
          "type": "blob",
          "size": 24872
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/GeistMono-Bold.ttf",
          "type": "blob",
          "size": 78304
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/GeistMono-OFL.txt",
          "type": "blob",
          "size": 4388
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/GeistMono-Regular.ttf",
          "type": "blob",
          "size": 78232
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/Gloock-OFL.txt",
          "type": "blob",
          "size": 4381
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/Gloock-Regular.ttf",
          "type": "blob",
          "size": 95156
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/IBMPlexMono-Bold.ttf",
          "type": "blob",
          "size": 136008
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/IBMPlexMono-OFL.txt",
          "type": "blob",
          "size": 4363
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/IBMPlexMono-Regular.ttf",
          "type": "blob",
          "size": 133796
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/IBMPlexSerif-Bold.ttf",
          "type": "blob",
          "size": 161000
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/IBMPlexSerif-BoldItalic.ttf",
          "type": "blob",
          "size": 169840
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/IBMPlexSerif-Italic.ttf",
          "type": "blob",
          "size": 170004
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/IBMPlexSerif-Regular.ttf",
          "type": "blob",
          "size": 160380
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/InstrumentSans-Bold.ttf",
          "type": "blob",
          "size": 68084
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/InstrumentSans-BoldItalic.ttf",
          "type": "blob",
          "size": 70004
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/InstrumentSans-Italic.ttf",
          "type": "blob",
          "size": 69900
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/InstrumentSans-OFL.txt",
          "type": "blob",
          "size": 4403
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/InstrumentSans-Regular.ttf",
          "type": "blob",
          "size": 68028
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/InstrumentSerif-Italic.ttf",
          "type": "blob",
          "size": 70868
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/InstrumentSerif-Regular.ttf",
          "type": "blob",
          "size": 69312
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/Italiana-OFL.txt",
          "type": "blob",
          "size": 4394
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/Italiana-Regular.ttf",
          "type": "blob",
          "size": 27184
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/JetBrainsMono-Bold.ttf",
          "type": "blob",
          "size": 114828
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/JetBrainsMono-OFL.txt",
          "type": "blob",
          "size": 4399
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/JetBrainsMono-Regular.ttf",
          "type": "blob",
          "size": 114904
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/Jura-Light.ttf",
          "type": "blob",
          "size": 154308
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/Jura-Medium.ttf",
          "type": "blob",
          "size": 154488
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/Jura-OFL.txt",
          "type": "blob",
          "size": 4380
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/LibreBaskerville-OFL.txt",
          "type": "blob",
          "size": 4449
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/LibreBaskerville-Regular.ttf",
          "type": "blob",
          "size": 147584
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/Lora-Bold.ttf",
          "type": "blob",
          "size": 133828
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/Lora-BoldItalic.ttf",
          "type": "blob",
          "size": 140332
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/Lora-Italic.ttf",
          "type": "blob",
          "size": 139328
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/Lora-OFL.txt",
          "type": "blob",
          "size": 4423
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/Lora-Regular.ttf",
          "type": "blob",
          "size": 133888
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/NationalPark-Bold.ttf",
          "type": "blob",
          "size": 79208
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/NationalPark-OFL.txt",
          "type": "blob",
          "size": 4399
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/NationalPark-Regular.ttf",
          "type": "blob",
          "size": 76424
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/NothingYouCouldDo-OFL.txt",
          "type": "blob",
          "size": 4363
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/NothingYouCouldDo-Regular.ttf",
          "type": "blob",
          "size": 32020
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/Outfit-Bold.ttf",
          "type": "blob",
          "size": 55392
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/Outfit-OFL.txt",
          "type": "blob",
          "size": 4389
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/Outfit-Regular.ttf",
          "type": "blob",
          "size": 54912
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/PixelifySans-Medium.ttf",
          "type": "blob",
          "size": 51072
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/PixelifySans-OFL.txt",
          "type": "blob",
          "size": 4395
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/PoiretOne-OFL.txt",
          "type": "blob",
          "size": 4366
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/PoiretOne-Regular.ttf",
          "type": "blob",
          "size": 45244
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/RedHatMono-Bold.ttf",
          "type": "blob",
          "size": 34420
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/RedHatMono-OFL.txt",
          "type": "blob",
          "size": 4394
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/RedHatMono-Regular.ttf",
          "type": "blob",
          "size": 34488
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/Silkscreen-OFL.txt",
          "type": "blob",
          "size": 4394
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/Silkscreen-Regular.ttf",
          "type": "blob",
          "size": 31960
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/SmoochSans-Medium.ttf",
          "type": "blob",
          "size": 59704
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/SmoochSans-OFL.txt",
          "type": "blob",
          "size": 4396
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/Tektur-Medium.ttf",
          "type": "blob",
          "size": 76248
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/Tektur-OFL.txt",
          "type": "blob",
          "size": 4385
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/Tektur-Regular.ttf",
          "type": "blob",
          "size": 75604
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/WorkSans-Bold.ttf",
          "type": "blob",
          "size": 191304
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/WorkSans-BoldItalic.ttf",
          "type": "blob",
          "size": 175772
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/WorkSans-Italic.ttf",
          "type": "blob",
          "size": 174280
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/WorkSans-OFL.txt",
          "type": "blob",
          "size": 4397
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/WorkSans-Regular.ttf",
          "type": "blob",
          "size": 188916
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/YoungSerif-OFL.txt",
          "type": "blob",
          "size": 4398
        },
        {
          "path": "plugins/all-skills/skills/canvas-design/canvas-fonts/YoungSerif-Regular.ttf",
          "type": "blob",
          "size": 105136
        },
        {
          "path": "plugins/all-skills/skills/changelog-generator",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-skills/skills/changelog-generator/SKILL.md",
          "type": "blob",
          "size": 3126
        },
        {
          "path": "plugins/all-skills/skills/competitive-ads-extractor",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-skills/skills/competitive-ads-extractor/SKILL.md",
          "type": "blob",
          "size": 7944
        },
        {
          "path": "plugins/all-skills/skills/content-research-writer",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-skills/skills/content-research-writer/SKILL.md",
          "type": "blob",
          "size": 14276
        },
        {
          "path": "plugins/all-skills/skills/developer-growth-analysis",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-skills/skills/developer-growth-analysis/SKILL.md",
          "type": "blob",
          "size": 15752
        },
        {
          "path": "plugins/all-skills/skills/docx",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-skills/skills/docx/LICENSE.txt",
          "type": "blob",
          "size": 1467
        },
        {
          "path": "plugins/all-skills/skills/docx/SKILL.md",
          "type": "blob",
          "size": 10180
        },
        {
          "path": "plugins/all-skills/skills/docx/docx-js.md",
          "type": "blob",
          "size": 16509
        },
        {
          "path": "plugins/all-skills/skills/docx/ooxml.md",
          "type": "blob",
          "size": 23572
        },
        {
          "path": "plugins/all-skills/skills/docx/ooxml",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-skills/skills/docx/ooxml/schemas",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-skills/skills/docx/ooxml/schemas/ISO-IEC29500-4_2016",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-skills/skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/dml-chart.xsd",
          "type": "blob",
          "size": 74984
        },
        {
          "path": "plugins/all-skills/skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/dml-chartDrawing.xsd",
          "type": "blob",
          "size": 6956
        },
        {
          "path": "plugins/all-skills/skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/dml-diagram.xsd",
          "type": "blob",
          "size": 51302
        },
        {
          "path": "plugins/all-skills/skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/dml-lockedCanvas.xsd",
          "type": "blob",
          "size": 624
        },
        {
          "path": "plugins/all-skills/skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/dml-main.xsd",
          "type": "blob",
          "size": 152039
        },
        {
          "path": "plugins/all-skills/skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/dml-picture.xsd",
          "type": "blob",
          "size": 1231
        },
        {
          "path": "plugins/all-skills/skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/dml-spreadsheetDrawing.xsd",
          "type": "blob",
          "size": 8862
        },
        {
          "path": "plugins/all-skills/skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/dml-wordprocessingDrawing.xsd",
          "type": "blob",
          "size": 14795
        },
        {
          "path": "plugins/all-skills/skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/pml.xsd",
          "type": "blob",
          "size": 83612
        },
        {
          "path": "plugins/all-skills/skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/shared-additionalCharacteristics.xsd",
          "type": "blob",
          "size": 1269
        },
        {
          "path": "plugins/all-skills/skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/shared-bibliography.xsd",
          "type": "blob",
          "size": 7328
        },
        {
          "path": "plugins/all-skills/skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/shared-commonSimpleTypes.xsd",
          "type": "blob",
          "size": 6382
        },
        {
          "path": "plugins/all-skills/skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/shared-customXmlDataProperties.xsd",
          "type": "blob",
          "size": 1248
        },
        {
          "path": "plugins/all-skills/skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/shared-customXmlSchemaProperties.xsd",
          "type": "blob",
          "size": 880
        },
        {
          "path": "plugins/all-skills/skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/shared-documentPropertiesCustom.xsd",
          "type": "blob",
          "size": 2608
        },
        {
          "path": "plugins/all-skills/skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/shared-documentPropertiesExtended.xsd",
          "type": "blob",
          "size": 3507
        },
        {
          "path": "plugins/all-skills/skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/shared-documentPropertiesVariantTypes.xsd",
          "type": "blob",
          "size": 7507
        },
        {
          "path": "plugins/all-skills/skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/shared-math.xsd",
          "type": "blob",
          "size": 23313
        },
        {
          "path": "plugins/all-skills/skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/shared-relationshipReference.xsd",
          "type": "blob",
          "size": 1367
        },
        {
          "path": "plugins/all-skills/skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/sml.xsd",
          "type": "blob",
          "size": 242277
        },
        {
          "path": "plugins/all-skills/skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/vml-main.xsd",
          "type": "blob",
          "size": 26148
        },
        {
          "path": "plugins/all-skills/skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/vml-officeDrawing.xsd",
          "type": "blob",
          "size": 25279
        },
        {
          "path": "plugins/all-skills/skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/vml-presentationDrawing.xsd",
          "type": "blob",
          "size": 535
        },
        {
          "path": "plugins/all-skills/skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/vml-spreadsheetDrawing.xsd",
          "type": "blob",
          "size": 5712
        },
        {
          "path": "plugins/all-skills/skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/vml-wordprocessingDrawing.xsd",
          "type": "blob",
          "size": 4010
        },
        {
          "path": "plugins/all-skills/skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/wml.xsd",
          "type": "blob",
          "size": 171367
        },
        {
          "path": "plugins/all-skills/skills/docx/ooxml/schemas/ISO-IEC29500-4_2016/xml.xsd",
          "type": "blob",
          "size": 4646
        },
        {
          "path": "plugins/all-skills/skills/docx/ooxml/schemas/ecma",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-skills/skills/docx/ooxml/schemas/ecma/fouth-edition",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-skills/skills/docx/ooxml/schemas/ecma/fouth-edition/opc-contentTypes.xsd",
          "type": "blob",
          "size": 1963
        },
        {
          "path": "plugins/all-skills/skills/docx/ooxml/schemas/ecma/fouth-edition/opc-coreProperties.xsd",
          "type": "blob",
          "size": 2515
        },
        {
          "path": "plugins/all-skills/skills/docx/ooxml/schemas/ecma/fouth-edition/opc-digSig.xsd",
          "type": "blob",
          "size": 2856
        },
        {
          "path": "plugins/all-skills/skills/docx/ooxml/schemas/ecma/fouth-edition/opc-relationships.xsd",
          "type": "blob",
          "size": 1344
        },
        {
          "path": "plugins/all-skills/skills/docx/ooxml/schemas/mce",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-skills/skills/docx/ooxml/schemas/mce/mc.xsd",
          "type": "blob",
          "size": 3127
        },
        {
          "path": "plugins/all-skills/skills/docx/ooxml/schemas/microsoft",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-skills/skills/docx/ooxml/schemas/microsoft/wml-2010.xsd",
          "type": "blob",
          "size": 26549
        },
        {
          "path": "plugins/all-skills/skills/docx/ooxml/schemas/microsoft/wml-2012.xsd",
          "type": "blob",
          "size": 3745
        },
        {
          "path": "plugins/all-skills/skills/docx/ooxml/schemas/microsoft/wml-2018.xsd",
          "type": "blob",
          "size": 901
        },
        {
          "path": "plugins/all-skills/skills/docx/ooxml/schemas/microsoft/wml-cex-2018.xsd",
          "type": "blob",
          "size": 1778
        },
        {
          "path": "plugins/all-skills/skills/docx/ooxml/schemas/microsoft/wml-cid-2016.xsd",
          "type": "blob",
          "size": 1002
        },
        {
          "path": "plugins/all-skills/skills/docx/ooxml/schemas/microsoft/wml-sdtdatahash-2020.xsd",
          "type": "blob",
          "size": 600
        },
        {
          "path": "plugins/all-skills/skills/docx/ooxml/schemas/microsoft/wml-symex-2015.xsd",
          "type": "blob",
          "size": 745
        },
        {
          "path": "plugins/all-skills/skills/docx/ooxml/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-skills/skills/docx/ooxml/scripts/pack.py",
          "type": "blob",
          "size": 5596
        },
        {
          "path": "plugins/all-skills/skills/docx/ooxml/scripts/unpack.py",
          "type": "blob",
          "size": 1037
        },
        {
          "path": "plugins/all-skills/skills/docx/ooxml/scripts/validate.py",
          "type": "blob",
          "size": 1959
        },
        {
          "path": "plugins/all-skills/skills/docx/ooxml/scripts/validation",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-skills/skills/docx/ooxml/scripts/validation/__init__.py",
          "type": "blob",
          "size": 336
        },
        {
          "path": "plugins/all-skills/skills/docx/ooxml/scripts/validation/base.py",
          "type": "blob",
          "size": 39892
        },
        {
          "path": "plugins/all-skills/skills/docx/ooxml/scripts/validation/docx.py",
          "type": "blob",
          "size": 9996
        },
        {
          "path": "plugins/all-skills/skills/docx/ooxml/scripts/validation/pptx.py",
          "type": "blob",
          "size": 12327
        },
        {
          "path": "plugins/all-skills/skills/docx/ooxml/scripts/validation/redlining.py",
          "type": "blob",
          "size": 11179
        },
        {
          "path": "plugins/all-skills/skills/docx/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-skills/skills/docx/scripts/__init__.py",
          "type": "blob",
          "size": 65
        },
        {
          "path": "plugins/all-skills/skills/docx/scripts/document.py",
          "type": "blob",
          "size": 50409
        },
        {
          "path": "plugins/all-skills/skills/docx/scripts/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-skills/skills/docx/scripts/templates/comments.xml",
          "type": "blob",
          "size": 2635
        },
        {
          "path": "plugins/all-skills/skills/docx/scripts/templates/commentsExtended.xml",
          "type": "blob",
          "size": 2643
        },
        {
          "path": "plugins/all-skills/skills/docx/scripts/templates/commentsExtensible.xml",
          "type": "blob",
          "size": 2739
        },
        {
          "path": "plugins/all-skills/skills/docx/scripts/templates/commentsIds.xml",
          "type": "blob",
          "size": 2651
        },
        {
          "path": "plugins/all-skills/skills/docx/scripts/templates/people.xml",
          "type": "blob",
          "size": 147
        },
        {
          "path": "plugins/all-skills/skills/docx/scripts/utilities.py",
          "type": "blob",
          "size": 13694
        },
        {
          "path": "plugins/all-skills/skills/domain-name-brainstormer",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-skills/skills/domain-name-brainstormer/SKILL.md",
          "type": "blob",
          "size": 5728
        },
        {
          "path": "plugins/all-skills/skills/file-organizer",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-skills/skills/file-organizer/SKILL.md",
          "type": "blob",
          "size": 11344
        },
        {
          "path": "plugins/all-skills/skills/image-enhancer",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-skills/skills/image-enhancer/SKILL.md",
          "type": "blob",
          "size": 2570
        },
        {
          "path": "plugins/all-skills/skills/internal-comms",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-skills/skills/internal-comms/LICENSE.txt",
          "type": "blob",
          "size": 11357
        },
        {
          "path": "plugins/all-skills/skills/internal-comms/SKILL.md",
          "type": "blob",
          "size": 1543
        },
        {
          "path": "plugins/all-skills/skills/internal-comms/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-skills/skills/internal-comms/examples/3p-updates.md",
          "type": "blob",
          "size": 3274
        },
        {
          "path": "plugins/all-skills/skills/internal-comms/examples/company-newsletter.md",
          "type": "blob",
          "size": 3295
        },
        {
          "path": "plugins/all-skills/skills/internal-comms/examples/faq-answers.md",
          "type": "blob",
          "size": 2366
        },
        {
          "path": "plugins/all-skills/skills/internal-comms/examples/general-comms.md",
          "type": "blob",
          "size": 602
        },
        {
          "path": "plugins/all-skills/skills/invoice-organizer",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-skills/skills/invoice-organizer/SKILL.md",
          "type": "blob",
          "size": 12042
        },
        {
          "path": "plugins/all-skills/skills/json-canvas",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-skills/skills/json-canvas/SKILL.md",
          "type": "blob",
          "size": 12026
        },
        {
          "path": "plugins/all-skills/skills/lead-research-assistant",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-skills/skills/lead-research-assistant/SKILL.md",
          "type": "blob",
          "size": 6619
        },
        {
          "path": "plugins/all-skills/skills/mcp-builder",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-skills/skills/mcp-builder/LICENSE.txt",
          "type": "blob",
          "size": 11357
        },
        {
          "path": "plugins/all-skills/skills/mcp-builder/SKILL.md",
          "type": "blob",
          "size": 13579
        },
        {
          "path": "plugins/all-skills/skills/mcp-builder/reference",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-skills/skills/mcp-builder/reference/evaluation.md",
          "type": "blob",
          "size": 21663
        },
        {
          "path": "plugins/all-skills/skills/mcp-builder/reference/mcp_best_practices.md",
          "type": "blob",
          "size": 28910
        },
        {
          "path": "plugins/all-skills/skills/mcp-builder/reference/node_mcp_server.md",
          "type": "blob",
          "size": 26709
        },
        {
          "path": "plugins/all-skills/skills/mcp-builder/reference/python_mcp_server.md",
          "type": "blob",
          "size": 26182
        },
        {
          "path": "plugins/all-skills/skills/mcp-builder/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-skills/skills/mcp-builder/scripts/connections.py",
          "type": "blob",
          "size": 4875
        },
        {
          "path": "plugins/all-skills/skills/mcp-builder/scripts/evaluation.py",
          "type": "blob",
          "size": 12579
        },
        {
          "path": "plugins/all-skills/skills/mcp-builder/scripts/example_evaluation.xml",
          "type": "blob",
          "size": 1194
        },
        {
          "path": "plugins/all-skills/skills/mcp-builder/scripts/requirements.txt",
          "type": "blob",
          "size": 29
        },
        {
          "path": "plugins/all-skills/skills/meeting-insights-analyzer",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-skills/skills/meeting-insights-analyzer/SKILL.md",
          "type": "blob",
          "size": 10209
        },
        {
          "path": "plugins/all-skills/skills/obsidian-bases",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-skills/skills/obsidian-bases/SKILL.md",
          "type": "blob",
          "size": 14519
        },
        {
          "path": "plugins/all-skills/skills/obsidian-markdown",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-skills/skills/obsidian-markdown/SKILL.md",
          "type": "blob",
          "size": 7879
        },
        {
          "path": "plugins/all-skills/skills/pdf",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-skills/skills/pdf/LICENSE.txt",
          "type": "blob",
          "size": 1467
        },
        {
          "path": "plugins/all-skills/skills/pdf/SKILL.md",
          "type": "blob",
          "size": 7098
        },
        {
          "path": "plugins/all-skills/skills/pdf/forms.md",
          "type": "blob",
          "size": 9438
        },
        {
          "path": "plugins/all-skills/skills/pdf/reference.md",
          "type": "blob",
          "size": 16692
        },
        {
          "path": "plugins/all-skills/skills/pdf/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-skills/skills/pdf/scripts/check_bounding_boxes.py",
          "type": "blob",
          "size": 3139
        },
        {
          "path": "plugins/all-skills/skills/pdf/scripts/check_bounding_boxes_test.py",
          "type": "blob",
          "size": 8818
        },
        {
          "path": "plugins/all-skills/skills/pdf/scripts/check_fillable_fields.py",
          "type": "blob",
          "size": 362
        },
        {
          "path": "plugins/all-skills/skills/pdf/scripts/convert_pdf_to_images.py",
          "type": "blob",
          "size": 1123
        },
        {
          "path": "plugins/all-skills/skills/pdf/scripts/create_validation_image.py",
          "type": "blob",
          "size": 1603
        },
        {
          "path": "plugins/all-skills/skills/pdf/scripts/extract_form_field_info.py",
          "type": "blob",
          "size": 6127
        },
        {
          "path": "plugins/all-skills/skills/pdf/scripts/fill_fillable_fields.py",
          "type": "blob",
          "size": 4863
        },
        {
          "path": "plugins/all-skills/skills/pdf/scripts/fill_pdf_form_with_annotations.py",
          "type": "blob",
          "size": 3596
        },
        {
          "path": "plugins/all-skills/skills/pptx",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-skills/skills/pptx/LICENSE.txt",
          "type": "blob",
          "size": 1467
        },
        {
          "path": "plugins/all-skills/skills/pptx/SKILL.md",
          "type": "blob",
          "size": 25581
        },
        {
          "path": "plugins/all-skills/skills/pptx/html2pptx.md",
          "type": "blob",
          "size": 19859
        },
        {
          "path": "plugins/all-skills/skills/pptx/ooxml.md",
          "type": "blob",
          "size": 10388
        },
        {
          "path": "plugins/all-skills/skills/pptx/ooxml",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-skills/skills/pptx/ooxml/schemas",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-skills/skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-skills/skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/dml-chart.xsd",
          "type": "blob",
          "size": 74984
        },
        {
          "path": "plugins/all-skills/skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/dml-chartDrawing.xsd",
          "type": "blob",
          "size": 6956
        },
        {
          "path": "plugins/all-skills/skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/dml-diagram.xsd",
          "type": "blob",
          "size": 51302
        },
        {
          "path": "plugins/all-skills/skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/dml-lockedCanvas.xsd",
          "type": "blob",
          "size": 624
        },
        {
          "path": "plugins/all-skills/skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/dml-main.xsd",
          "type": "blob",
          "size": 152039
        },
        {
          "path": "plugins/all-skills/skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/dml-picture.xsd",
          "type": "blob",
          "size": 1231
        },
        {
          "path": "plugins/all-skills/skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/dml-spreadsheetDrawing.xsd",
          "type": "blob",
          "size": 8862
        },
        {
          "path": "plugins/all-skills/skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/dml-wordprocessingDrawing.xsd",
          "type": "blob",
          "size": 14795
        },
        {
          "path": "plugins/all-skills/skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/pml.xsd",
          "type": "blob",
          "size": 83612
        },
        {
          "path": "plugins/all-skills/skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/shared-additionalCharacteristics.xsd",
          "type": "blob",
          "size": 1269
        },
        {
          "path": "plugins/all-skills/skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/shared-bibliography.xsd",
          "type": "blob",
          "size": 7328
        },
        {
          "path": "plugins/all-skills/skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/shared-commonSimpleTypes.xsd",
          "type": "blob",
          "size": 6382
        },
        {
          "path": "plugins/all-skills/skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/shared-customXmlDataProperties.xsd",
          "type": "blob",
          "size": 1248
        },
        {
          "path": "plugins/all-skills/skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/shared-customXmlSchemaProperties.xsd",
          "type": "blob",
          "size": 880
        },
        {
          "path": "plugins/all-skills/skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/shared-documentPropertiesCustom.xsd",
          "type": "blob",
          "size": 2608
        },
        {
          "path": "plugins/all-skills/skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/shared-documentPropertiesExtended.xsd",
          "type": "blob",
          "size": 3507
        },
        {
          "path": "plugins/all-skills/skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/shared-documentPropertiesVariantTypes.xsd",
          "type": "blob",
          "size": 7507
        },
        {
          "path": "plugins/all-skills/skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/shared-math.xsd",
          "type": "blob",
          "size": 23313
        },
        {
          "path": "plugins/all-skills/skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/shared-relationshipReference.xsd",
          "type": "blob",
          "size": 1367
        },
        {
          "path": "plugins/all-skills/skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/sml.xsd",
          "type": "blob",
          "size": 242277
        },
        {
          "path": "plugins/all-skills/skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/vml-main.xsd",
          "type": "blob",
          "size": 26148
        },
        {
          "path": "plugins/all-skills/skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/vml-officeDrawing.xsd",
          "type": "blob",
          "size": 25279
        },
        {
          "path": "plugins/all-skills/skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/vml-presentationDrawing.xsd",
          "type": "blob",
          "size": 535
        },
        {
          "path": "plugins/all-skills/skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/vml-spreadsheetDrawing.xsd",
          "type": "blob",
          "size": 5712
        },
        {
          "path": "plugins/all-skills/skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/vml-wordprocessingDrawing.xsd",
          "type": "blob",
          "size": 4010
        },
        {
          "path": "plugins/all-skills/skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/wml.xsd",
          "type": "blob",
          "size": 171367
        },
        {
          "path": "plugins/all-skills/skills/pptx/ooxml/schemas/ISO-IEC29500-4_2016/xml.xsd",
          "type": "blob",
          "size": 4646
        },
        {
          "path": "plugins/all-skills/skills/pptx/ooxml/schemas/ecma",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-skills/skills/pptx/ooxml/schemas/ecma/fouth-edition",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-skills/skills/pptx/ooxml/schemas/ecma/fouth-edition/opc-contentTypes.xsd",
          "type": "blob",
          "size": 1963
        },
        {
          "path": "plugins/all-skills/skills/pptx/ooxml/schemas/ecma/fouth-edition/opc-coreProperties.xsd",
          "type": "blob",
          "size": 2515
        },
        {
          "path": "plugins/all-skills/skills/pptx/ooxml/schemas/ecma/fouth-edition/opc-digSig.xsd",
          "type": "blob",
          "size": 2856
        },
        {
          "path": "plugins/all-skills/skills/pptx/ooxml/schemas/ecma/fouth-edition/opc-relationships.xsd",
          "type": "blob",
          "size": 1344
        },
        {
          "path": "plugins/all-skills/skills/pptx/ooxml/schemas/mce",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-skills/skills/pptx/ooxml/schemas/mce/mc.xsd",
          "type": "blob",
          "size": 3127
        },
        {
          "path": "plugins/all-skills/skills/pptx/ooxml/schemas/microsoft",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-skills/skills/pptx/ooxml/schemas/microsoft/wml-2010.xsd",
          "type": "blob",
          "size": 26549
        },
        {
          "path": "plugins/all-skills/skills/pptx/ooxml/schemas/microsoft/wml-2012.xsd",
          "type": "blob",
          "size": 3745
        },
        {
          "path": "plugins/all-skills/skills/pptx/ooxml/schemas/microsoft/wml-2018.xsd",
          "type": "blob",
          "size": 901
        },
        {
          "path": "plugins/all-skills/skills/pptx/ooxml/schemas/microsoft/wml-cex-2018.xsd",
          "type": "blob",
          "size": 1778
        },
        {
          "path": "plugins/all-skills/skills/pptx/ooxml/schemas/microsoft/wml-cid-2016.xsd",
          "type": "blob",
          "size": 1002
        },
        {
          "path": "plugins/all-skills/skills/pptx/ooxml/schemas/microsoft/wml-sdtdatahash-2020.xsd",
          "type": "blob",
          "size": 600
        },
        {
          "path": "plugins/all-skills/skills/pptx/ooxml/schemas/microsoft/wml-symex-2015.xsd",
          "type": "blob",
          "size": 745
        },
        {
          "path": "plugins/all-skills/skills/pptx/ooxml/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-skills/skills/pptx/ooxml/scripts/pack.py",
          "type": "blob",
          "size": 5596
        },
        {
          "path": "plugins/all-skills/skills/pptx/ooxml/scripts/unpack.py",
          "type": "blob",
          "size": 1037
        },
        {
          "path": "plugins/all-skills/skills/pptx/ooxml/scripts/validate.py",
          "type": "blob",
          "size": 1959
        },
        {
          "path": "plugins/all-skills/skills/pptx/ooxml/scripts/validation",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-skills/skills/pptx/ooxml/scripts/validation/__init__.py",
          "type": "blob",
          "size": 336
        },
        {
          "path": "plugins/all-skills/skills/pptx/ooxml/scripts/validation/base.py",
          "type": "blob",
          "size": 39892
        },
        {
          "path": "plugins/all-skills/skills/pptx/ooxml/scripts/validation/docx.py",
          "type": "blob",
          "size": 9996
        },
        {
          "path": "plugins/all-skills/skills/pptx/ooxml/scripts/validation/pptx.py",
          "type": "blob",
          "size": 12327
        },
        {
          "path": "plugins/all-skills/skills/pptx/ooxml/scripts/validation/redlining.py",
          "type": "blob",
          "size": 11179
        },
        {
          "path": "plugins/all-skills/skills/pptx/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-skills/skills/pptx/scripts/html2pptx.js",
          "type": "blob",
          "size": 37795
        },
        {
          "path": "plugins/all-skills/skills/pptx/scripts/inventory.py",
          "type": "blob",
          "size": 38126
        },
        {
          "path": "plugins/all-skills/skills/pptx/scripts/rearrange.py",
          "type": "blob",
          "size": 8514
        },
        {
          "path": "plugins/all-skills/skills/pptx/scripts/replace.py",
          "type": "blob",
          "size": 13594
        },
        {
          "path": "plugins/all-skills/skills/pptx/scripts/thumbnail.py",
          "type": "blob",
          "size": 15484
        },
        {
          "path": "plugins/all-skills/skills/raffle-winner-picker",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-skills/skills/raffle-winner-picker/SKILL.md",
          "type": "blob",
          "size": 3828
        },
        {
          "path": "plugins/all-skills/skills/skill-creator",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-skills/skills/skill-creator/LICENSE.txt",
          "type": "blob",
          "size": 11357
        },
        {
          "path": "plugins/all-skills/skills/skill-creator/SKILL.md",
          "type": "blob",
          "size": 11574
        },
        {
          "path": "plugins/all-skills/skills/skill-creator/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-skills/skills/skill-creator/scripts/init_skill.py",
          "type": "blob",
          "size": 10863
        },
        {
          "path": "plugins/all-skills/skills/skill-creator/scripts/package_skill.py",
          "type": "blob",
          "size": 3247
        },
        {
          "path": "plugins/all-skills/skills/skill-creator/scripts/quick_validate.py",
          "type": "blob",
          "size": 2165
        },
        {
          "path": "plugins/all-skills/skills/skill-share",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-skills/skills/skill-share/SKILL.md",
          "type": "blob",
          "size": 2946
        },
        {
          "path": "plugins/all-skills/skills/slack-gif-creator",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-skills/skills/slack-gif-creator/LICENSE.txt",
          "type": "blob",
          "size": 11357
        },
        {
          "path": "plugins/all-skills/skills/slack-gif-creator/SKILL.md",
          "type": "blob",
          "size": 17175
        },
        {
          "path": "plugins/all-skills/skills/slack-gif-creator/core",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-skills/skills/slack-gif-creator/core/color_palettes.py",
          "type": "blob",
          "size": 8724
        },
        {
          "path": "plugins/all-skills/skills/slack-gif-creator/core/easing.py",
          "type": "blob",
          "size": 6289
        },
        {
          "path": "plugins/all-skills/skills/slack-gif-creator/core/frame_composer.py",
          "type": "blob",
          "size": 14468
        },
        {
          "path": "plugins/all-skills/skills/slack-gif-creator/core/gif_builder.py",
          "type": "blob",
          "size": 9565
        },
        {
          "path": "plugins/all-skills/skills/slack-gif-creator/core/typography.py",
          "type": "blob",
          "size": 10761
        },
        {
          "path": "plugins/all-skills/skills/slack-gif-creator/core/validators.py",
          "type": "blob",
          "size": 8225
        },
        {
          "path": "plugins/all-skills/skills/slack-gif-creator/core/visual_effects.py",
          "type": "blob",
          "size": 14862
        },
        {
          "path": "plugins/all-skills/skills/slack-gif-creator/requirements.txt",
          "type": "blob",
          "size": 66
        },
        {
          "path": "plugins/all-skills/skills/slack-gif-creator/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-skills/skills/slack-gif-creator/templates/bounce.py",
          "type": "blob",
          "size": 3055
        },
        {
          "path": "plugins/all-skills/skills/slack-gif-creator/templates/explode.py",
          "type": "blob",
          "size": 11259
        },
        {
          "path": "plugins/all-skills/skills/slack-gif-creator/templates/fade.py",
          "type": "blob",
          "size": 10143
        },
        {
          "path": "plugins/all-skills/skills/slack-gif-creator/templates/flip.py",
          "type": "blob",
          "size": 9458
        },
        {
          "path": "plugins/all-skills/skills/slack-gif-creator/templates/kaleidoscope.py",
          "type": "blob",
          "size": 6349
        },
        {
          "path": "plugins/all-skills/skills/slack-gif-creator/templates/morph.py",
          "type": "blob",
          "size": 11241
        },
        {
          "path": "plugins/all-skills/skills/slack-gif-creator/templates/move.py",
          "type": "blob",
          "size": 9395
        },
        {
          "path": "plugins/all-skills/skills/slack-gif-creator/templates/pulse.py",
          "type": "blob",
          "size": 8638
        },
        {
          "path": "plugins/all-skills/skills/slack-gif-creator/templates/shake.py",
          "type": "blob",
          "size": 3737
        },
        {
          "path": "plugins/all-skills/skills/slack-gif-creator/templates/slide.py",
          "type": "blob",
          "size": 9367
        },
        {
          "path": "plugins/all-skills/skills/slack-gif-creator/templates/spin.py",
          "type": "blob",
          "size": 9096
        },
        {
          "path": "plugins/all-skills/skills/slack-gif-creator/templates/wiggle.py",
          "type": "blob",
          "size": 10060
        },
        {
          "path": "plugins/all-skills/skills/slack-gif-creator/templates/zoom.py",
          "type": "blob",
          "size": 10111
        },
        {
          "path": "plugins/all-skills/skills/theme-factory",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-skills/skills/theme-factory/LICENSE.txt",
          "type": "blob",
          "size": 11357
        },
        {
          "path": "plugins/all-skills/skills/theme-factory/SKILL.md",
          "type": "blob",
          "size": 3154
        },
        {
          "path": "plugins/all-skills/skills/theme-factory/theme-showcase.pdf",
          "type": "blob",
          "size": 124310
        },
        {
          "path": "plugins/all-skills/skills/theme-factory/themes",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-skills/skills/theme-factory/themes/arctic-frost.md",
          "type": "blob",
          "size": 544
        },
        {
          "path": "plugins/all-skills/skills/theme-factory/themes/botanical-garden.md",
          "type": "blob",
          "size": 519
        },
        {
          "path": "plugins/all-skills/skills/theme-factory/themes/desert-rose.md",
          "type": "blob",
          "size": 496
        },
        {
          "path": "plugins/all-skills/skills/theme-factory/themes/forest-canopy.md",
          "type": "blob",
          "size": 506
        },
        {
          "path": "plugins/all-skills/skills/theme-factory/themes/golden-hour.md",
          "type": "blob",
          "size": 528
        },
        {
          "path": "plugins/all-skills/skills/theme-factory/themes/midnight-galaxy.md",
          "type": "blob",
          "size": 513
        },
        {
          "path": "plugins/all-skills/skills/theme-factory/themes/modern-minimalist.md",
          "type": "blob",
          "size": 549
        },
        {
          "path": "plugins/all-skills/skills/theme-factory/themes/ocean-depths.md",
          "type": "blob",
          "size": 555
        },
        {
          "path": "plugins/all-skills/skills/theme-factory/themes/sunset-boulevard.md",
          "type": "blob",
          "size": 558
        },
        {
          "path": "plugins/all-skills/skills/theme-factory/themes/tech-innovation.md",
          "type": "blob",
          "size": 547
        },
        {
          "path": "plugins/all-skills/skills/video-downloader",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-skills/skills/video-downloader/SKILL.md",
          "type": "blob",
          "size": 2701
        },
        {
          "path": "plugins/all-skills/skills/video-downloader/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-skills/skills/video-downloader/scripts/download_video.py",
          "type": "blob",
          "size": 4278
        },
        {
          "path": "plugins/all-skills/skills/webapp-testing",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-skills/skills/webapp-testing/LICENSE.txt",
          "type": "blob",
          "size": 11357
        },
        {
          "path": "plugins/all-skills/skills/webapp-testing/SKILL.md",
          "type": "blob",
          "size": 3940
        },
        {
          "path": "plugins/all-skills/skills/webapp-testing/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-skills/skills/webapp-testing/examples/console_logging.py",
          "type": "blob",
          "size": 1027
        },
        {
          "path": "plugins/all-skills/skills/webapp-testing/examples/element_discovery.py",
          "type": "blob",
          "size": 1463
        },
        {
          "path": "plugins/all-skills/skills/webapp-testing/examples/static_html_automation.py",
          "type": "blob",
          "size": 953
        },
        {
          "path": "plugins/all-skills/skills/webapp-testing/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-skills/skills/webapp-testing/scripts/with_server.py",
          "type": "blob",
          "size": 3693
        },
        {
          "path": "plugins/all-skills/skills/xlsx",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/all-skills/skills/xlsx/LICENSE.txt",
          "type": "blob",
          "size": 1467
        },
        {
          "path": "plugins/all-skills/skills/xlsx/SKILL.md",
          "type": "blob",
          "size": 10662
        },
        {
          "path": "plugins/all-skills/skills/xlsx/recalc.py",
          "type": "blob",
          "size": 6408
        },
        {
          "path": "plugins/claude-hud",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/claude-hud/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/claude-hud/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 575
        },
        {
          "path": "plugins/claude-hud/.gitignore",
          "type": "blob",
          "size": 14
        },
        {
          "path": "plugins/claude-hud/README.md",
          "type": "blob",
          "size": 1106
        },
        {
          "path": "plugins/claude-hud/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/claude-hud/commands/setup.md",
          "type": "blob",
          "size": 1346
        },
        {
          "path": "plugins/claude-hud/dist",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/claude-hud/dist/config-reader.d.ts",
          "type": "blob",
          "size": 249
        },
        {
          "path": "plugins/claude-hud/dist/config-reader.d.ts.map",
          "type": "blob",
          "size": 324
        },
        {
          "path": "plugins/claude-hud/dist/config-reader.js",
          "type": "blob",
          "size": 3611
        },
        {
          "path": "plugins/claude-hud/dist/config-reader.js.map",
          "type": "blob",
          "size": 3970
        },
        {
          "path": "plugins/claude-hud/dist/index.d.ts",
          "type": "blob",
          "size": 627
        },
        {
          "path": "plugins/claude-hud/dist/index.d.ts.map",
          "type": "blob",
          "size": 695
        },
        {
          "path": "plugins/claude-hud/dist/index.js",
          "type": "blob",
          "size": 1876
        },
        {
          "path": "plugins/claude-hud/dist/index.js.map",
          "type": "blob",
          "size": 1971
        },
        {
          "path": "plugins/claude-hud/dist/render",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/claude-hud/dist/render/agents-line.d.ts",
          "type": "blob",
          "size": 168
        },
        {
          "path": "plugins/claude-hud/dist/render/agents-line.d.ts.map",
          "type": "blob",
          "size": 237
        },
        {
          "path": "plugins/claude-hud/dist/render/agents-line.js",
          "type": "blob",
          "size": 1671
        },
        {
          "path": "plugins/claude-hud/dist/render/agents-line.js.map",
          "type": "blob",
          "size": 2355
        },
        {
          "path": "plugins/claude-hud/dist/render/colors.d.ts",
          "type": "blob",
          "size": 537
        },
        {
          "path": "plugins/claude-hud/dist/render/colors.d.ts.map",
          "type": "blob",
          "size": 590
        },
        {
          "path": "plugins/claude-hud/dist/render/colors.js",
          "type": "blob",
          "size": 1049
        },
        {
          "path": "plugins/claude-hud/dist/render/colors.js.map",
          "type": "blob",
          "size": 1386
        },
        {
          "path": "plugins/claude-hud/dist/render/index.d.ts",
          "type": "blob",
          "size": 143
        },
        {
          "path": "plugins/claude-hud/dist/render/index.d.ts.map",
          "type": "blob",
          "size": 213
        },
        {
          "path": "plugins/claude-hud/dist/render/index.js",
          "type": "blob",
          "size": 965
        },
        {
          "path": "plugins/claude-hud/dist/render/index.js.map",
          "type": "blob",
          "size": 1085
        },
        {
          "path": "plugins/claude-hud/dist/render/session-line.d.ts",
          "type": "blob",
          "size": 163
        },
        {
          "path": "plugins/claude-hud/dist/render/session-line.d.ts.map",
          "type": "blob",
          "size": 229
        },
        {
          "path": "plugins/claude-hud/dist/render/session-line.js",
          "type": "blob",
          "size": 1894
        },
        {
          "path": "plugins/claude-hud/dist/render/session-line.js.map",
          "type": "blob",
          "size": 2151
        },
        {
          "path": "plugins/claude-hud/dist/render/todos-line.d.ts",
          "type": "blob",
          "size": 166
        },
        {
          "path": "plugins/claude-hud/dist/render/todos-line.d.ts.map",
          "type": "blob",
          "size": 233
        },
        {
          "path": "plugins/claude-hud/dist/render/todos-line.js",
          "type": "blob",
          "size": 933
        },
        {
          "path": "plugins/claude-hud/dist/render/todos-line.js.map",
          "type": "blob",
          "size": 1211
        },
        {
          "path": "plugins/claude-hud/dist/render/tools-line.d.ts",
          "type": "blob",
          "size": 246
        },
        {
          "path": "plugins/claude-hud/dist/render/tools-line.d.ts.map",
          "type": "blob",
          "size": 306
        },
        {
          "path": "plugins/claude-hud/dist/render/tools-line.js",
          "type": "blob",
          "size": 2017
        },
        {
          "path": "plugins/claude-hud/dist/render/tools-line.js.map",
          "type": "blob",
          "size": 2759
        },
        {
          "path": "plugins/claude-hud/dist/stdin.d.ts",
          "type": "blob",
          "size": 277
        },
        {
          "path": "plugins/claude-hud/dist/stdin.d.ts.map",
          "type": "blob",
          "size": 316
        },
        {
          "path": "plugins/claude-hud/dist/stdin.js",
          "type": "blob",
          "size": 920
        },
        {
          "path": "plugins/claude-hud/dist/stdin.js.map",
          "type": "blob",
          "size": 1122
        },
        {
          "path": "plugins/claude-hud/dist/transcript.d.ts",
          "type": "blob",
          "size": 180
        },
        {
          "path": "plugins/claude-hud/dist/transcript.d.ts.map",
          "type": "blob",
          "size": 230
        },
        {
          "path": "plugins/claude-hud/dist/transcript.js",
          "type": "blob",
          "size": 3558
        },
        {
          "path": "plugins/claude-hud/dist/transcript.js.map",
          "type": "blob",
          "size": 3610
        },
        {
          "path": "plugins/claude-hud/dist/types.d.ts",
          "type": "blob",
          "size": 1150
        },
        {
          "path": "plugins/claude-hud/dist/types.d.ts.map",
          "type": "blob",
          "size": 1315
        },
        {
          "path": "plugins/claude-hud/dist/types.js",
          "type": "blob",
          "size": 44
        },
        {
          "path": "plugins/claude-hud/dist/types.js.map",
          "type": "blob",
          "size": 102
        },
        {
          "path": "plugins/claude-hud/package-lock.json",
          "type": "blob",
          "size": 1510
        },
        {
          "path": "plugins/claude-hud/package.json",
          "type": "blob",
          "size": 471
        },
        {
          "path": "plugins/claude-hud/src",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/claude-hud/src/config-reader.ts",
          "type": "blob",
          "size": 3481
        },
        {
          "path": "plugins/claude-hud/src/index.ts",
          "type": "blob",
          "size": 2036
        },
        {
          "path": "plugins/claude-hud/src/render",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/claude-hud/src/render/agents-line.ts",
          "type": "blob",
          "size": 1720
        },
        {
          "path": "plugins/claude-hud/src/render/colors.ts",
          "type": "blob",
          "size": 1117
        },
        {
          "path": "plugins/claude-hud/src/render/index.ts",
          "type": "blob",
          "size": 961
        },
        {
          "path": "plugins/claude-hud/src/render/session-line.ts",
          "type": "blob",
          "size": 1847
        },
        {
          "path": "plugins/claude-hud/src/render/todos-line.ts",
          "type": "blob",
          "size": 950
        },
        {
          "path": "plugins/claude-hud/src/render/tools-line.ts",
          "type": "blob",
          "size": 1976
        },
        {
          "path": "plugins/claude-hud/src/stdin.ts",
          "type": "blob",
          "size": 966
        },
        {
          "path": "plugins/claude-hud/src/transcript.ts",
          "type": "blob",
          "size": 3794
        },
        {
          "path": "plugins/claude-hud/src/types.ts",
          "type": "blob",
          "size": 1044
        },
        {
          "path": "plugins/claude-hud/tsconfig.json",
          "type": "blob",
          "size": 427
        },
        {
          "path": "plugins/commands-api-development",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commands-api-development/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commands-api-development/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 525
        },
        {
          "path": "plugins/commands-api-development/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commands-api-development/commands/design-rest-api.md",
          "type": "blob",
          "size": 41230
        },
        {
          "path": "plugins/commands-api-development/commands/doc-api.md",
          "type": "blob",
          "size": 7141
        },
        {
          "path": "plugins/commands-api-development/commands/generate-api-documentation.md",
          "type": "blob",
          "size": 3723
        },
        {
          "path": "plugins/commands-api-development/commands/implement-graphql-api.md",
          "type": "blob",
          "size": 45943
        },
        {
          "path": "plugins/commands-automation-workflow",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commands-automation-workflow/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commands-automation-workflow/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 437
        },
        {
          "path": "plugins/commands-automation-workflow/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commands-automation-workflow/commands/act.md",
          "type": "blob",
          "size": 410
        },
        {
          "path": "plugins/commands-ci-deployment",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commands-ci-deployment/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commands-ci-deployment/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 691
        },
        {
          "path": "plugins/commands-ci-deployment/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commands-ci-deployment/commands/add-changelog.md",
          "type": "blob",
          "size": 2064
        },
        {
          "path": "plugins/commands-ci-deployment/commands/changelog-demo-command.md",
          "type": "blob",
          "size": 299
        },
        {
          "path": "plugins/commands-ci-deployment/commands/ci-setup.md",
          "type": "blob",
          "size": 8741
        },
        {
          "path": "plugins/commands-ci-deployment/commands/containerize-application.md",
          "type": "blob",
          "size": 3347
        },
        {
          "path": "plugins/commands-ci-deployment/commands/hotfix-deploy.md",
          "type": "blob",
          "size": 8739
        },
        {
          "path": "plugins/commands-ci-deployment/commands/prepare-release.md",
          "type": "blob",
          "size": 9151
        },
        {
          "path": "plugins/commands-ci-deployment/commands/release.md",
          "type": "blob",
          "size": 367
        },
        {
          "path": "plugins/commands-ci-deployment/commands/rollback-deploy.md",
          "type": "blob",
          "size": 10804
        },
        {
          "path": "plugins/commands-ci-deployment/commands/run-ci.md",
          "type": "blob",
          "size": 1682
        },
        {
          "path": "plugins/commands-ci-deployment/commands/setup-automated-releases.md",
          "type": "blob",
          "size": 4138
        },
        {
          "path": "plugins/commands-ci-deployment/commands/setup-kubernetes-deployment.md",
          "type": "blob",
          "size": 3367
        },
        {
          "path": "plugins/commands-code-analysis-testing",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commands-code-analysis-testing/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commands-code-analysis-testing/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 838
        },
        {
          "path": "plugins/commands-code-analysis-testing/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commands-code-analysis-testing/commands/add-mutation-testing.md",
          "type": "blob",
          "size": 3707
        },
        {
          "path": "plugins/commands-code-analysis-testing/commands/add-property-based-testing.md",
          "type": "blob",
          "size": 3851
        },
        {
          "path": "plugins/commands-code-analysis-testing/commands/check.md",
          "type": "blob",
          "size": 1378
        },
        {
          "path": "plugins/commands-code-analysis-testing/commands/clean.md",
          "type": "blob",
          "size": 1779
        },
        {
          "path": "plugins/commands-code-analysis-testing/commands/code_analysis.md",
          "type": "blob",
          "size": 1829
        },
        {
          "path": "plugins/commands-code-analysis-testing/commands/e2e-setup.md",
          "type": "blob",
          "size": 7323
        },
        {
          "path": "plugins/commands-code-analysis-testing/commands/generate-test-cases.md",
          "type": "blob",
          "size": 3611
        },
        {
          "path": "plugins/commands-code-analysis-testing/commands/generate-tests.md",
          "type": "blob",
          "size": 2436
        },
        {
          "path": "plugins/commands-code-analysis-testing/commands/optimize.md",
          "type": "blob",
          "size": 236
        },
        {
          "path": "plugins/commands-code-analysis-testing/commands/repro-issue.md",
          "type": "blob",
          "size": 221
        },
        {
          "path": "plugins/commands-code-analysis-testing/commands/setup-comprehensive-testing.md",
          "type": "blob",
          "size": 3323
        },
        {
          "path": "plugins/commands-code-analysis-testing/commands/setup-load-testing.md",
          "type": "blob",
          "size": 4017
        },
        {
          "path": "plugins/commands-code-analysis-testing/commands/setup-visual-testing.md",
          "type": "blob",
          "size": 3743
        },
        {
          "path": "plugins/commands-code-analysis-testing/commands/tdd.md",
          "type": "blob",
          "size": 2230
        },
        {
          "path": "plugins/commands-code-analysis-testing/commands/test-changelog-automation.md",
          "type": "blob",
          "size": 318
        },
        {
          "path": "plugins/commands-code-analysis-testing/commands/test-coverage.md",
          "type": "blob",
          "size": 6327
        },
        {
          "path": "plugins/commands-code-analysis-testing/commands/testing_plan_integration.md",
          "type": "blob",
          "size": 607
        },
        {
          "path": "plugins/commands-code-analysis-testing/commands/write-tests.md",
          "type": "blob",
          "size": 5542
        },
        {
          "path": "plugins/commands-context-loading-priming",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commands-context-loading-priming/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commands-context-loading-priming/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 506
        },
        {
          "path": "plugins/commands-context-loading-priming/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commands-context-loading-priming/commands/context-prime.md",
          "type": "blob",
          "size": 305
        },
        {
          "path": "plugins/commands-context-loading-priming/commands/initref.md",
          "type": "blob",
          "size": 456
        },
        {
          "path": "plugins/commands-context-loading-priming/commands/prime.md",
          "type": "blob",
          "size": 461
        },
        {
          "path": "plugins/commands-context-loading-priming/commands/rsi.md",
          "type": "blob",
          "size": 441
        },
        {
          "path": "plugins/commands-database-operations",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commands-database-operations/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commands-database-operations/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 538
        },
        {
          "path": "plugins/commands-database-operations/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commands-database-operations/commands/create-database-migrations.md",
          "type": "blob",
          "size": 45780
        },
        {
          "path": "plugins/commands-database-operations/commands/design-database-schema.md",
          "type": "blob",
          "size": 26732
        },
        {
          "path": "plugins/commands-database-operations/commands/optimize-database-performance.md",
          "type": "blob",
          "size": 18671
        },
        {
          "path": "plugins/commands-documentation-changelogs",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commands-documentation-changelogs/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commands-documentation-changelogs/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 687
        },
        {
          "path": "plugins/commands-documentation-changelogs/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commands-documentation-changelogs/commands/add-to-changelog.md",
          "type": "blob",
          "size": 1747
        },
        {
          "path": "plugins/commands-documentation-changelogs/commands/create-architecture-documentation.md",
          "type": "blob",
          "size": 3729
        },
        {
          "path": "plugins/commands-documentation-changelogs/commands/create-docs.md",
          "type": "blob",
          "size": 1652
        },
        {
          "path": "plugins/commands-documentation-changelogs/commands/create-onboarding-guide.md",
          "type": "blob",
          "size": 3465
        },
        {
          "path": "plugins/commands-documentation-changelogs/commands/docs.md",
          "type": "blob",
          "size": 2475
        },
        {
          "path": "plugins/commands-documentation-changelogs/commands/explain-issue-fix.md",
          "type": "blob",
          "size": 955
        },
        {
          "path": "plugins/commands-documentation-changelogs/commands/load-llms-txt.md",
          "type": "blob",
          "size": 423
        },
        {
          "path": "plugins/commands-documentation-changelogs/commands/migration-guide.md",
          "type": "blob",
          "size": 7558
        },
        {
          "path": "plugins/commands-documentation-changelogs/commands/troubleshooting-guide.md",
          "type": "blob",
          "size": 9041
        },
        {
          "path": "plugins/commands-documentation-changelogs/commands/update-docs.md",
          "type": "blob",
          "size": 2575
        },
        {
          "path": "plugins/commands-framework-svelte",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commands-framework-svelte/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commands-framework-svelte/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 831
        },
        {
          "path": "plugins/commands-framework-svelte/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commands-framework-svelte/commands/svelte-a11y.md",
          "type": "blob",
          "size": 2724
        },
        {
          "path": "plugins/commands-framework-svelte/commands/svelte-component.md",
          "type": "blob",
          "size": 1979
        },
        {
          "path": "plugins/commands-framework-svelte/commands/svelte-debug.md",
          "type": "blob",
          "size": 1833
        },
        {
          "path": "plugins/commands-framework-svelte/commands/svelte-migrate.md",
          "type": "blob",
          "size": 2146
        },
        {
          "path": "plugins/commands-framework-svelte/commands/svelte-optimize.md",
          "type": "blob",
          "size": 2750
        },
        {
          "path": "plugins/commands-framework-svelte/commands/svelte-scaffold.md",
          "type": "blob",
          "size": 2491
        },
        {
          "path": "plugins/commands-framework-svelte/commands/svelte-storybook-migrate.md",
          "type": "blob",
          "size": 4579
        },
        {
          "path": "plugins/commands-framework-svelte/commands/svelte-storybook-mock.md",
          "type": "blob",
          "size": 5538
        },
        {
          "path": "plugins/commands-framework-svelte/commands/svelte-storybook-setup.md",
          "type": "blob",
          "size": 3016
        },
        {
          "path": "plugins/commands-framework-svelte/commands/svelte-storybook-story.md",
          "type": "blob",
          "size": 3704
        },
        {
          "path": "plugins/commands-framework-svelte/commands/svelte-storybook-troubleshoot.md",
          "type": "blob",
          "size": 4625
        },
        {
          "path": "plugins/commands-framework-svelte/commands/svelte-storybook.md",
          "type": "blob",
          "size": 1699
        },
        {
          "path": "plugins/commands-framework-svelte/commands/svelte-test-coverage.md",
          "type": "blob",
          "size": 2173
        },
        {
          "path": "plugins/commands-framework-svelte/commands/svelte-test-fix.md",
          "type": "blob",
          "size": 2487
        },
        {
          "path": "plugins/commands-framework-svelte/commands/svelte-test-setup.md",
          "type": "blob",
          "size": 2469
        },
        {
          "path": "plugins/commands-framework-svelte/commands/svelte-test.md",
          "type": "blob",
          "size": 1956
        },
        {
          "path": "plugins/commands-game-development",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commands-game-development/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commands-game-development/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 432
        },
        {
          "path": "plugins/commands-game-development/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commands-game-development/commands/unity-project-setup.md",
          "type": "blob",
          "size": 4738
        },
        {
          "path": "plugins/commands-integration-sync",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commands-integration-sync/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commands-integration-sync/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 748
        },
        {
          "path": "plugins/commands-integration-sync/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commands-integration-sync/commands/bidirectional-sync.md",
          "type": "blob",
          "size": 7300
        },
        {
          "path": "plugins/commands-integration-sync/commands/bulk-import-issues.md",
          "type": "blob",
          "size": 11379
        },
        {
          "path": "plugins/commands-integration-sync/commands/cross-reference-manager.md",
          "type": "blob",
          "size": 5027
        },
        {
          "path": "plugins/commands-integration-sync/commands/issue-to-linear-task.md",
          "type": "blob",
          "size": 7215
        },
        {
          "path": "plugins/commands-integration-sync/commands/linear-task-to-issue.md",
          "type": "blob",
          "size": 7849
        },
        {
          "path": "plugins/commands-integration-sync/commands/sync-automation-setup.md",
          "type": "blob",
          "size": 15994
        },
        {
          "path": "plugins/commands-integration-sync/commands/sync-conflict-resolver.md",
          "type": "blob",
          "size": 5556
        },
        {
          "path": "plugins/commands-integration-sync/commands/sync-issues-to-linear.md",
          "type": "blob",
          "size": 4557
        },
        {
          "path": "plugins/commands-integration-sync/commands/sync-linear-to-issues.md",
          "type": "blob",
          "size": 5493
        },
        {
          "path": "plugins/commands-integration-sync/commands/sync-pr-to-task.md",
          "type": "blob",
          "size": 8441
        },
        {
          "path": "plugins/commands-integration-sync/commands/sync-status.md",
          "type": "blob",
          "size": 10687
        },
        {
          "path": "plugins/commands-integration-sync/commands/task-from-pr.md",
          "type": "blob",
          "size": 5465
        },
        {
          "path": "plugins/commands-miscellaneous",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commands-miscellaneous/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commands-miscellaneous/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 438
        },
        {
          "path": "plugins/commands-miscellaneous/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commands-miscellaneous/commands/five.md",
          "type": "blob",
          "size": 1811
        },
        {
          "path": "plugins/commands-miscellaneous/commands/mermaid.md",
          "type": "blob",
          "size": 1820
        },
        {
          "path": "plugins/commands-miscellaneous/commands/use-stepper.md",
          "type": "blob",
          "size": 247
        },
        {
          "path": "plugins/commands-monitoring-observability",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commands-monitoring-observability/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commands-monitoring-observability/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 506
        },
        {
          "path": "plugins/commands-monitoring-observability/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commands-monitoring-observability/commands/add-performance-monitoring.md",
          "type": "blob",
          "size": 35273
        },
        {
          "path": "plugins/commands-monitoring-observability/commands/setup-monitoring-observability.md",
          "type": "blob",
          "size": 3652
        },
        {
          "path": "plugins/commands-performance-optimization",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commands-performance-optimization/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commands-performance-optimization/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 613
        },
        {
          "path": "plugins/commands-performance-optimization/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commands-performance-optimization/commands/implement-caching-strategy.md",
          "type": "blob",
          "size": 15645
        },
        {
          "path": "plugins/commands-performance-optimization/commands/optimize-build.md",
          "type": "blob",
          "size": 5177
        },
        {
          "path": "plugins/commands-performance-optimization/commands/optimize-bundle-size.md",
          "type": "blob",
          "size": 10333
        },
        {
          "path": "plugins/commands-performance-optimization/commands/performance-audit.md",
          "type": "blob",
          "size": 3050
        },
        {
          "path": "plugins/commands-performance-optimization/commands/setup-cdn-optimization.md",
          "type": "blob",
          "size": 19869
        },
        {
          "path": "plugins/commands-performance-optimization/commands/system-behavior-simulator.md",
          "type": "blob",
          "size": 17734
        },
        {
          "path": "plugins/commands-project-setup",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commands-project-setup/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commands-project-setup/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 566
        },
        {
          "path": "plugins/commands-project-setup/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commands-project-setup/commands/modernize-deps.md",
          "type": "blob",
          "size": 1388
        },
        {
          "path": "plugins/commands-project-setup/commands/setup-development-environment.md",
          "type": "blob",
          "size": 3738
        },
        {
          "path": "plugins/commands-project-setup/commands/setup-formatting.md",
          "type": "blob",
          "size": 1429
        },
        {
          "path": "plugins/commands-project-setup/commands/setup-linting.md",
          "type": "blob",
          "size": 1868
        },
        {
          "path": "plugins/commands-project-setup/commands/setup-monorepo.md",
          "type": "blob",
          "size": 3746
        },
        {
          "path": "plugins/commands-project-setup/commands/setup-rate-limiting.md",
          "type": "blob",
          "size": 59934
        },
        {
          "path": "plugins/commands-project-task-management",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commands-project-task-management/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commands-project-task-management/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 805
        },
        {
          "path": "plugins/commands-project-task-management/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commands-project-task-management/commands/add-package.md",
          "type": "blob",
          "size": 3802
        },
        {
          "path": "plugins/commands-project-task-management/commands/create-command.md",
          "type": "blob",
          "size": 2465
        },
        {
          "path": "plugins/commands-project-task-management/commands/create-feature.md",
          "type": "blob",
          "size": 4403
        },
        {
          "path": "plugins/commands-project-task-management/commands/create-jtbd.md",
          "type": "blob",
          "size": 1906
        },
        {
          "path": "plugins/commands-project-task-management/commands/create-prd.md",
          "type": "blob",
          "size": 1509
        },
        {
          "path": "plugins/commands-project-task-management/commands/create-prp.md",
          "type": "blob",
          "size": 6615
        },
        {
          "path": "plugins/commands-project-task-management/commands/init-project.md",
          "type": "blob",
          "size": 3630
        },
        {
          "path": "plugins/commands-project-task-management/commands/milestone-tracker.md",
          "type": "blob",
          "size": 10201
        },
        {
          "path": "plugins/commands-project-task-management/commands/pac-configure.md",
          "type": "blob",
          "size": 5018
        },
        {
          "path": "plugins/commands-project-task-management/commands/pac-create-epic.md",
          "type": "blob",
          "size": 4518
        },
        {
          "path": "plugins/commands-project-task-management/commands/pac-create-ticket.md",
          "type": "blob",
          "size": 5326
        },
        {
          "path": "plugins/commands-project-task-management/commands/pac-update-status.md",
          "type": "blob",
          "size": 5263
        },
        {
          "path": "plugins/commands-project-task-management/commands/pac-validate.md",
          "type": "blob",
          "size": 5401
        },
        {
          "path": "plugins/commands-project-task-management/commands/project-health-check.md",
          "type": "blob",
          "size": 7526
        },
        {
          "path": "plugins/commands-project-task-management/commands/project-timeline-simulator.md",
          "type": "blob",
          "size": 17192
        },
        {
          "path": "plugins/commands-project-task-management/commands/project-to-linear.md",
          "type": "blob",
          "size": 5318
        },
        {
          "path": "plugins/commands-project-task-management/commands/todo.md",
          "type": "blob",
          "size": 2452
        },
        {
          "path": "plugins/commands-security-audit",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commands-security-audit/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commands-security-audit/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 524
        },
        {
          "path": "plugins/commands-security-audit/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commands-security-audit/commands/add-authentication-system.md",
          "type": "blob",
          "size": 3598
        },
        {
          "path": "plugins/commands-security-audit/commands/dependency-audit.md",
          "type": "blob",
          "size": 3714
        },
        {
          "path": "plugins/commands-security-audit/commands/security-audit.md",
          "type": "blob",
          "size": 2617
        },
        {
          "path": "plugins/commands-security-audit/commands/security-hardening.md",
          "type": "blob",
          "size": 3557
        },
        {
          "path": "plugins/commands-simulation-modeling",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commands-simulation-modeling/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commands-simulation-modeling/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 664
        },
        {
          "path": "plugins/commands-simulation-modeling/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commands-simulation-modeling/commands/business-scenario-explorer.md",
          "type": "blob",
          "size": 10614
        },
        {
          "path": "plugins/commands-simulation-modeling/commands/constraint-modeler.md",
          "type": "blob",
          "size": 15087
        },
        {
          "path": "plugins/commands-simulation-modeling/commands/decision-tree-explorer.md",
          "type": "blob",
          "size": 14676
        },
        {
          "path": "plugins/commands-simulation-modeling/commands/digital-twin-creator.md",
          "type": "blob",
          "size": 14047
        },
        {
          "path": "plugins/commands-simulation-modeling/commands/future-scenario-generator.md",
          "type": "blob",
          "size": 13468
        },
        {
          "path": "plugins/commands-simulation-modeling/commands/market-response-modeler.md",
          "type": "blob",
          "size": 15926
        },
        {
          "path": "plugins/commands-simulation-modeling/commands/simulation-calibrator.md",
          "type": "blob",
          "size": 13920
        },
        {
          "path": "plugins/commands-simulation-modeling/commands/timeline-compressor.md",
          "type": "blob",
          "size": 15776
        },
        {
          "path": "plugins/commands-team-collaboration",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commands-team-collaboration/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commands-team-collaboration/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 753
        },
        {
          "path": "plugins/commands-team-collaboration/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commands-team-collaboration/commands/architecture-review.md",
          "type": "blob",
          "size": 4013
        },
        {
          "path": "plugins/commands-team-collaboration/commands/decision-quality-analyzer.md",
          "type": "blob",
          "size": 13487
        },
        {
          "path": "plugins/commands-team-collaboration/commands/dependency-mapper.md",
          "type": "blob",
          "size": 9681
        },
        {
          "path": "plugins/commands-team-collaboration/commands/estimate-assistant.md",
          "type": "blob",
          "size": 12896
        },
        {
          "path": "plugins/commands-team-collaboration/commands/issue-triage.md",
          "type": "blob",
          "size": 12051
        },
        {
          "path": "plugins/commands-team-collaboration/commands/memory-spring-cleaning.md",
          "type": "blob",
          "size": 1392
        },
        {
          "path": "plugins/commands-team-collaboration/commands/migration-assistant.md",
          "type": "blob",
          "size": 6975
        },
        {
          "path": "plugins/commands-team-collaboration/commands/retrospective-analyzer.md",
          "type": "blob",
          "size": 8485
        },
        {
          "path": "plugins/commands-team-collaboration/commands/session-learning-capture.md",
          "type": "blob",
          "size": 2360
        },
        {
          "path": "plugins/commands-team-collaboration/commands/sprint-planning.md",
          "type": "blob",
          "size": 4473
        },
        {
          "path": "plugins/commands-team-collaboration/commands/standup-report.md",
          "type": "blob",
          "size": 5568
        },
        {
          "path": "plugins/commands-team-collaboration/commands/team-workload-balancer.md",
          "type": "blob",
          "size": 24657
        },
        {
          "path": "plugins/commands-typescript-migration",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commands-typescript-migration/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commands-typescript-migration/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 459
        },
        {
          "path": "plugins/commands-typescript-migration/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commands-typescript-migration/commands/migrate-to-typescript.md",
          "type": "blob",
          "size": 3563
        },
        {
          "path": "plugins/commands-utilities-debugging",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commands-utilities-debugging/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commands-utilities-debugging/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 730
        },
        {
          "path": "plugins/commands-utilities-debugging/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commands-utilities-debugging/commands/all-tools.md",
          "type": "blob",
          "size": 1156
        },
        {
          "path": "plugins/commands-utilities-debugging/commands/architecture-scenario-explorer.md",
          "type": "blob",
          "size": 15906
        },
        {
          "path": "plugins/commands-utilities-debugging/commands/check-file.md",
          "type": "blob",
          "size": 1979
        },
        {
          "path": "plugins/commands-utilities-debugging/commands/clean-branches.md",
          "type": "blob",
          "size": 7532
        },
        {
          "path": "plugins/commands-utilities-debugging/commands/code-permutation-tester.md",
          "type": "blob",
          "size": 13617
        },
        {
          "path": "plugins/commands-utilities-debugging/commands/code-review.md",
          "type": "blob",
          "size": 2228
        },
        {
          "path": "plugins/commands-utilities-debugging/commands/code-to-task.md",
          "type": "blob",
          "size": 16419
        },
        {
          "path": "plugins/commands-utilities-debugging/commands/debug-error.md",
          "type": "blob",
          "size": 4850
        },
        {
          "path": "plugins/commands-utilities-debugging/commands/directory-deep-dive.md",
          "type": "blob",
          "size": 1342
        },
        {
          "path": "plugins/commands-utilities-debugging/commands/explain-code.md",
          "type": "blob",
          "size": 6806
        },
        {
          "path": "plugins/commands-utilities-debugging/commands/generate-linear-worklog.md",
          "type": "blob",
          "size": 4438
        },
        {
          "path": "plugins/commands-utilities-debugging/commands/git-status.md",
          "type": "blob",
          "size": 1515
        },
        {
          "path": "plugins/commands-utilities-debugging/commands/refactor-code.md",
          "type": "blob",
          "size": 4558
        },
        {
          "path": "plugins/commands-utilities-debugging/commands/ultra-think.md",
          "type": "blob",
          "size": 4867
        },
        {
          "path": "plugins/commands-version-control-git",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commands-version-control-git/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commands-version-control-git/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 644
        },
        {
          "path": "plugins/commands-version-control-git/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commands-version-control-git/commands/bug-fix.md",
          "type": "blob",
          "size": 534
        },
        {
          "path": "plugins/commands-version-control-git/commands/commit-fast.md",
          "type": "blob",
          "size": 728
        },
        {
          "path": "plugins/commands-version-control-git/commands/commit.md",
          "type": "blob",
          "size": 2258
        },
        {
          "path": "plugins/commands-version-control-git/commands/create-pr.md",
          "type": "blob",
          "size": 996
        },
        {
          "path": "plugins/commands-version-control-git/commands/create-pull-request.md",
          "type": "blob",
          "size": 2092
        },
        {
          "path": "plugins/commands-version-control-git/commands/create-worktrees.md",
          "type": "blob",
          "size": 5347
        },
        {
          "path": "plugins/commands-version-control-git/commands/fix-github-issue.md",
          "type": "blob",
          "size": 682
        },
        {
          "path": "plugins/commands-version-control-git/commands/fix-issue.md",
          "type": "blob",
          "size": 179
        },
        {
          "path": "plugins/commands-version-control-git/commands/fix-pr.md",
          "type": "blob",
          "size": 214
        },
        {
          "path": "plugins/commands-version-control-git/commands/husky.md",
          "type": "blob",
          "size": 2425
        },
        {
          "path": "plugins/commands-version-control-git/commands/pr-review.md",
          "type": "blob",
          "size": 2369
        },
        {
          "path": "plugins/commands-version-control-git/commands/update-branch-name.md",
          "type": "blob",
          "size": 584
        },
        {
          "path": "plugins/commands-workflow-orchestration",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commands-workflow-orchestration/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commands-workflow-orchestration/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 538
        },
        {
          "path": "plugins/commands-workflow-orchestration/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/commands-workflow-orchestration/commands/find.md",
          "type": "blob",
          "size": 5283
        },
        {
          "path": "plugins/commands-workflow-orchestration/commands/log.md",
          "type": "blob",
          "size": 7266
        },
        {
          "path": "plugins/commands-workflow-orchestration/commands/move.md",
          "type": "blob",
          "size": 4891
        },
        {
          "path": "plugins/commands-workflow-orchestration/commands/remove.md",
          "type": "blob",
          "size": 6714
        },
        {
          "path": "plugins/commands-workflow-orchestration/commands/report.md",
          "type": "blob",
          "size": 6548
        },
        {
          "path": "plugins/commands-workflow-orchestration/commands/resume.md",
          "type": "blob",
          "size": 7075
        },
        {
          "path": "plugins/commands-workflow-orchestration/commands/start.md",
          "type": "blob",
          "size": 4656
        },
        {
          "path": "plugins/commands-workflow-orchestration/commands/status.md",
          "type": "blob",
          "size": 4938
        },
        {
          "path": "plugins/commands-workflow-orchestration/commands/sync.md",
          "type": "blob",
          "size": 6503
        },
        {
          "path": "plugins/frontend-design-pro",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/frontend-design-pro/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/frontend-design-pro/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 561
        },
        {
          "path": "plugins/frontend-design-pro/README.md",
          "type": "blob",
          "size": 6882
        },
        {
          "path": "plugins/frontend-design-pro/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/frontend-design-pro/commands/analyze-site.md",
          "type": "blob",
          "size": 3868
        },
        {
          "path": "plugins/frontend-design-pro/commands/design.md",
          "type": "blob",
          "size": 5592
        },
        {
          "path": "plugins/frontend-design-pro/commands/review.md",
          "type": "blob",
          "size": 4845
        },
        {
          "path": "plugins/frontend-design-pro/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/frontend-design-pro/skills/color-curator",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/frontend-design-pro/skills/color-curator/SKILL.md",
          "type": "blob",
          "size": 5312
        },
        {
          "path": "plugins/frontend-design-pro/skills/color-curator/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/frontend-design-pro/skills/color-curator/references/color-theory.md",
          "type": "blob",
          "size": 6475
        },
        {
          "path": "plugins/frontend-design-pro/skills/design-wizard",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/frontend-design-pro/skills/design-wizard/SKILL.md",
          "type": "blob",
          "size": 6224
        },
        {
          "path": "plugins/frontend-design-pro/skills/design-wizard/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/frontend-design-pro/skills/design-wizard/references/accessibility-guidelines.md",
          "type": "blob",
          "size": 11935
        },
        {
          "path": "plugins/frontend-design-pro/skills/design-wizard/references/aesthetics-catalog.md",
          "type": "blob",
          "size": 12071
        },
        {
          "path": "plugins/frontend-design-pro/skills/design-wizard/references/anti-patterns.md",
          "type": "blob",
          "size": 10124
        },
        {
          "path": "plugins/frontend-design-pro/skills/design-wizard/references/design-principles.md",
          "type": "blob",
          "size": 9143
        },
        {
          "path": "plugins/frontend-design-pro/skills/inspiration-analyzer",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/frontend-design-pro/skills/inspiration-analyzer/SKILL.md",
          "type": "blob",
          "size": 5236
        },
        {
          "path": "plugins/frontend-design-pro/skills/inspiration-analyzer/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/frontend-design-pro/skills/inspiration-analyzer/references/extraction-techniques.md",
          "type": "blob",
          "size": 6134
        },
        {
          "path": "plugins/frontend-design-pro/skills/moodboard-creator",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/frontend-design-pro/skills/moodboard-creator/SKILL.md",
          "type": "blob",
          "size": 4759
        },
        {
          "path": "plugins/frontend-design-pro/skills/trend-researcher",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/frontend-design-pro/skills/trend-researcher/SKILL.md",
          "type": "blob",
          "size": 3876
        },
        {
          "path": "plugins/frontend-design-pro/skills/typography-selector",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/frontend-design-pro/skills/typography-selector/SKILL.md",
          "type": "blob",
          "size": 6167
        },
        {
          "path": "plugins/frontend-design-pro/skills/typography-selector/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/frontend-design-pro/skills/typography-selector/references/font-pairing.md",
          "type": "blob",
          "size": 7837
        },
        {
          "path": "plugins/hooks-automation",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/hooks-automation/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/hooks-automation/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 468
        },
        {
          "path": "plugins/hooks-automation/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/hooks-automation/hooks/build-on-change.md",
          "type": "blob",
          "size": 484
        },
        {
          "path": "plugins/hooks-automation/hooks/dependency-checker.md",
          "type": "blob",
          "size": 553
        },
        {
          "path": "plugins/hooks-automation/hooks/slack-notifications.md",
          "type": "blob",
          "size": 472
        },
        {
          "path": "plugins/hooks-development",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/hooks-development/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/hooks-development/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 480
        },
        {
          "path": "plugins/hooks-development/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/hooks-development/hooks/change-tracker.md",
          "type": "blob",
          "size": 413
        },
        {
          "path": "plugins/hooks-development/hooks/file-backup.md",
          "type": "blob",
          "size": 419
        },
        {
          "path": "plugins/hooks-development/hooks/lint-on-save.md",
          "type": "blob",
          "size": 477
        },
        {
          "path": "plugins/hooks-development/hooks/smart-formatting.md",
          "type": "blob",
          "size": 429
        },
        {
          "path": "plugins/hooks-formatting",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/hooks-formatting/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/hooks-formatting/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 450
        },
        {
          "path": "plugins/hooks-formatting/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/hooks-formatting/hooks/format-javascript-files.md",
          "type": "blob",
          "size": 517
        },
        {
          "path": "plugins/hooks-formatting/hooks/format-python-files.md",
          "type": "blob",
          "size": 493
        },
        {
          "path": "plugins/hooks-git",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/hooks-git/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/hooks-git/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 434
        },
        {
          "path": "plugins/hooks-git/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/hooks-git/hooks/auto-git-add.md",
          "type": "blob",
          "size": 459
        },
        {
          "path": "plugins/hooks-git/hooks/git-add-changes.md",
          "type": "blob",
          "size": 483
        },
        {
          "path": "plugins/hooks-git/hooks/smart-commit.md",
          "type": "blob",
          "size": 465
        },
        {
          "path": "plugins/hooks-notifications",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/hooks-notifications/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/hooks-notifications/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 730
        },
        {
          "path": "plugins/hooks-notifications/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/hooks-notifications/hooks/discord-detailed-notifications.md",
          "type": "blob",
          "size": 505
        },
        {
          "path": "plugins/hooks-notifications/hooks/discord-error-notifications.md",
          "type": "blob",
          "size": 515
        },
        {
          "path": "plugins/hooks-notifications/hooks/discord-notifications.md",
          "type": "blob",
          "size": 455
        },
        {
          "path": "plugins/hooks-notifications/hooks/notify-before-bash.md",
          "type": "blob",
          "size": 483
        },
        {
          "path": "plugins/hooks-notifications/hooks/simple-notifications.md",
          "type": "blob",
          "size": 484
        },
        {
          "path": "plugins/hooks-notifications/hooks/slack-detailed-notifications.md",
          "type": "blob",
          "size": 495
        },
        {
          "path": "plugins/hooks-notifications/hooks/slack-error-notifications.md",
          "type": "blob",
          "size": 573
        },
        {
          "path": "plugins/hooks-notifications/hooks/telegram-detailed-notifications.md",
          "type": "blob",
          "size": 521
        },
        {
          "path": "plugins/hooks-notifications/hooks/telegram-error-notifications.md",
          "type": "blob",
          "size": 539
        },
        {
          "path": "plugins/hooks-notifications/hooks/telegram-notifications.md",
          "type": "blob",
          "size": 479
        },
        {
          "path": "plugins/hooks-performance",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/hooks-performance/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/hooks-performance/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 422
        },
        {
          "path": "plugins/hooks-performance/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/hooks-performance/hooks/performance-monitor.md",
          "type": "blob",
          "size": 443
        },
        {
          "path": "plugins/hooks-security",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/hooks-security/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/hooks-security/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 461
        },
        {
          "path": "plugins/hooks-security/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/hooks-security/hooks/file-protection-hook.md",
          "type": "blob",
          "size": 463
        },
        {
          "path": "plugins/hooks-security/hooks/file-protection.md",
          "type": "blob",
          "size": 465
        },
        {
          "path": "plugins/hooks-security/hooks/security-scanner.md",
          "type": "blob",
          "size": 475
        },
        {
          "path": "plugins/hooks-testing",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/hooks-testing/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/hooks-testing/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 433
        },
        {
          "path": "plugins/hooks-testing/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/hooks-testing/hooks/run-tests-after-changes.md",
          "type": "blob",
          "size": 493
        },
        {
          "path": "plugins/hooks-testing/hooks/test-runner.md",
          "type": "blob",
          "size": 413
        },
        {
          "path": "plugins/interview",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/interview/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/interview/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 452
        },
        {
          "path": "plugins/interview/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/interview/commands/big-features-interview.md",
          "type": "blob",
          "size": 515
        },
        {
          "path": "plugins/mcp-servers-docker",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mcp-servers-docker/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mcp-servers-docker/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 731
        },
        {
          "path": "plugins/nextjs-expert",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/nextjs-expert/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/nextjs-expert/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 499
        },
        {
          "path": "plugins/nextjs-expert/README.md",
          "type": "blob",
          "size": 5086
        },
        {
          "path": "plugins/nextjs-expert/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/nextjs-expert/commands/add-auth.md",
          "type": "blob",
          "size": 3224
        },
        {
          "path": "plugins/nextjs-expert/commands/optimize.md",
          "type": "blob",
          "size": 4336
        },
        {
          "path": "plugins/nextjs-expert/commands/scaffold.md",
          "type": "blob",
          "size": 2512
        },
        {
          "path": "plugins/nextjs-expert/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/nextjs-expert/skills/app-router",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/nextjs-expert/skills/app-router/SKILL.md",
          "type": "blob",
          "size": 5331
        },
        {
          "path": "plugins/nextjs-expert/skills/app-router/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/nextjs-expert/skills/app-router/examples/dynamic-routes.md",
          "type": "blob",
          "size": 7227
        },
        {
          "path": "plugins/nextjs-expert/skills/app-router/examples/parallel-routes.md",
          "type": "blob",
          "size": 7556
        },
        {
          "path": "plugins/nextjs-expert/skills/app-router/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/nextjs-expert/skills/app-router/references/layouts-templates.md",
          "type": "blob",
          "size": 6028
        },
        {
          "path": "plugins/nextjs-expert/skills/app-router/references/loading-error-states.md",
          "type": "blob",
          "size": 6524
        },
        {
          "path": "plugins/nextjs-expert/skills/app-router/references/routing-conventions.md",
          "type": "blob",
          "size": 5348
        },
        {
          "path": "plugins/nextjs-expert/skills/auth-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/nextjs-expert/skills/auth-patterns/SKILL.md",
          "type": "blob",
          "size": 8453
        },
        {
          "path": "plugins/nextjs-expert/skills/auth-patterns/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/nextjs-expert/skills/auth-patterns/examples/nextauth-setup.md",
          "type": "blob",
          "size": 13008
        },
        {
          "path": "plugins/nextjs-expert/skills/auth-patterns/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/nextjs-expert/skills/auth-patterns/references/middleware-auth.md",
          "type": "blob",
          "size": 10824
        },
        {
          "path": "plugins/nextjs-expert/skills/auth-patterns/references/session-management.md",
          "type": "blob",
          "size": 10710
        },
        {
          "path": "plugins/nextjs-expert/skills/route-handlers",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/nextjs-expert/skills/route-handlers/SKILL.md",
          "type": "blob",
          "size": 7019
        },
        {
          "path": "plugins/nextjs-expert/skills/route-handlers/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/nextjs-expert/skills/route-handlers/examples/crud-api.md",
          "type": "blob",
          "size": 12373
        },
        {
          "path": "plugins/nextjs-expert/skills/route-handlers/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/nextjs-expert/skills/route-handlers/references/http-methods.md",
          "type": "blob",
          "size": 8123
        },
        {
          "path": "plugins/nextjs-expert/skills/route-handlers/references/streaming-responses.md",
          "type": "blob",
          "size": 10692
        },
        {
          "path": "plugins/nextjs-expert/skills/server-actions",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/nextjs-expert/skills/server-actions/SKILL.md",
          "type": "blob",
          "size": 7645
        },
        {
          "path": "plugins/nextjs-expert/skills/server-actions/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/nextjs-expert/skills/server-actions/examples/mutation-patterns.md",
          "type": "blob",
          "size": 12853
        },
        {
          "path": "plugins/nextjs-expert/skills/server-actions/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/nextjs-expert/skills/server-actions/references/form-handling.md",
          "type": "blob",
          "size": 12022
        },
        {
          "path": "plugins/nextjs-expert/skills/server-actions/references/revalidation.md",
          "type": "blob",
          "size": 9242
        },
        {
          "path": "plugins/nextjs-expert/skills/server-components",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/nextjs-expert/skills/server-components/SKILL.md",
          "type": "blob",
          "size": 6009
        },
        {
          "path": "plugins/nextjs-expert/skills/server-components/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/nextjs-expert/skills/server-components/examples/data-fetching-patterns.md",
          "type": "blob",
          "size": 8787
        },
        {
          "path": "plugins/nextjs-expert/skills/server-components/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/nextjs-expert/skills/server-components/references/composition-patterns.md",
          "type": "blob",
          "size": 8483
        },
        {
          "path": "plugins/nextjs-expert/skills/server-components/references/server-vs-client.md",
          "type": "blob",
          "size": 7431
        },
        {
          "path": "plugins/obsidian-skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/obsidian-skills/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/obsidian-skills/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 518
        },
        {
          "path": "plugins/obsidian-skills/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/obsidian-skills/skills/json-canvas",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/obsidian-skills/skills/json-canvas/SKILL.md",
          "type": "blob",
          "size": 12026
        },
        {
          "path": "plugins/obsidian-skills/skills/obsidian-bases",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/obsidian-skills/skills/obsidian-bases/SKILL.md",
          "type": "blob",
          "size": 14519
        },
        {
          "path": "plugins/obsidian-skills/skills/obsidian-markdown",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/obsidian-skills/skills/obsidian-markdown/SKILL.md",
          "type": "blob",
          "size": 7879
        },
        {
          "path": "scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "scripts/clean-marketplace-components.js",
          "type": "blob",
          "size": 1607
        },
        {
          "path": "scripts/command-schema.json",
          "type": "blob",
          "size": 1567
        },
        {
          "path": "scripts/docker-hub-api.js",
          "type": "blob",
          "size": 6923
        },
        {
          "path": "scripts/enhance-docker-stats.js",
          "type": "blob",
          "size": 3509
        },
        {
          "path": "scripts/fetch-docker-mcp.js",
          "type": "blob",
          "size": 25143
        },
        {
          "path": "scripts/fetch-official-mcp.js",
          "type": "blob",
          "size": 10753
        },
        {
          "path": "scripts/fetch-plugins.js",
          "type": "blob",
          "size": 13458
        },
        {
          "path": "scripts/fix-argument-hints.js",
          "type": "blob",
          "size": 5504
        },
        {
          "path": "scripts/fix-hooks-marketplace.js",
          "type": "blob",
          "size": 1872
        },
        {
          "path": "scripts/generate-hooks-config.js",
          "type": "blob",
          "size": 4648
        },
        {
          "path": "scripts/generate-marketplace.js",
          "type": "blob",
          "size": 12658
        },
        {
          "path": "scripts/generate-mcp-config.js",
          "type": "blob",
          "size": 2666
        },
        {
          "path": "scripts/generate-registry.js",
          "type": "blob",
          "size": 10172
        },
        {
          "path": "scripts/hook-schema.json",
          "type": "blob",
          "size": 1647
        },
        {
          "path": "scripts/mcp-importers",
          "type": "tree",
          "size": null
        },
        {
          "path": "scripts/mcp-importers/.env.example",
          "type": "blob",
          "size": 415
        },
        {
          "path": "scripts/mcp-importers/README.md",
          "type": "blob",
          "size": 2465
        },
        {
          "path": "scripts/mcp-importers/fetch-from-registries.js",
          "type": "blob",
          "size": 3836
        },
        {
          "path": "scripts/mcp-importers/package-lock.json",
          "type": "blob",
          "size": 1025
        },
        {
          "path": "scripts/mcp-importers/package.json",
          "type": "blob",
          "size": 629
        },
        {
          "path": "scripts/mcp-importers/registry-fetchers",
          "type": "tree",
          "size": null
        },
        {
          "path": "scripts/mcp-importers/registry-fetchers/docker-hub-api.js",
          "type": "blob",
          "size": 6825
        },
        {
          "path": "scripts/mcp-importers/registry-fetchers/docker-mcp.js",
          "type": "blob",
          "size": 6431
        },
        {
          "path": "scripts/mcp-importers/utils",
          "type": "tree",
          "size": null
        },
        {
          "path": "scripts/mcp-importers/utils/markdown-generator.js",
          "type": "blob",
          "size": 6922
        },
        {
          "path": "scripts/mcp-server-schema.json",
          "type": "blob",
          "size": 5826
        },
        {
          "path": "scripts/migrate-to-plugin-structure.js",
          "type": "blob",
          "size": 5063
        },
        {
          "path": "scripts/process-command-file.js",
          "type": "blob",
          "size": 6300
        },
        {
          "path": "scripts/process-new-commands.sh",
          "type": "blob",
          "size": 2949
        },
        {
          "path": "scripts/remove-hooks-from-marketplace.js",
          "type": "blob",
          "size": 716
        },
        {
          "path": "scripts/subagent-schema.json",
          "type": "blob",
          "size": 1241
        },
        {
          "path": "scripts/validate-all.js",
          "type": "blob",
          "size": 2913
        },
        {
          "path": "scripts/validate-commands.js",
          "type": "blob",
          "size": 3218
        },
        {
          "path": "scripts/validate-hooks.js",
          "type": "blob",
          "size": 6450
        },
        {
          "path": "scripts/validate-mcp-servers.js",
          "type": "blob",
          "size": 6175
        },
        {
          "path": "scripts/validate-subagents.js",
          "type": "blob",
          "size": 7595
        },
        {
          "path": "tests",
          "type": "tree",
          "size": null
        },
        {
          "path": "tests/plugin-test-harness.sh",
          "type": "blob",
          "size": 10343
        },
        {
          "path": "tests/test-results.json",
          "type": "blob",
          "size": 1613
        },
        {
          "path": "web-ui",
          "type": "tree",
          "size": null
        },
        {
          "path": "web-ui/.cursor",
          "type": "tree",
          "size": null
        },
        {
          "path": "web-ui/.cursor/mcp.json",
          "type": "blob",
          "size": 139
        },
        {
          "path": "web-ui/.gitignore",
          "type": "blob",
          "size": 506
        },
        {
          "path": "web-ui/README.md",
          "type": "blob",
          "size": 3617
        },
        {
          "path": "web-ui/app",
          "type": "tree",
          "size": null
        },
        {
          "path": "web-ui/app/api",
          "type": "tree",
          "size": null
        },
        {
          "path": "web-ui/app/api/admin",
          "type": "tree",
          "size": null
        },
        {
          "path": "web-ui/app/api/admin/migrate-plugins",
          "type": "tree",
          "size": null
        },
        {
          "path": "web-ui/app/api/admin/migrate-plugins/route.ts",
          "type": "blob",
          "size": 12668
        },
        {
          "path": "web-ui/app/api/admin/reindex",
          "type": "tree",
          "size": null
        },
        {
          "path": "web-ui/app/api/admin/reindex/route.ts",
          "type": "blob",
          "size": 1128
        },
        {
          "path": "web-ui/app/api/cron",
          "type": "tree",
          "size": null
        },
        {
          "path": "web-ui/app/api/cron/index-all",
          "type": "tree",
          "size": null
        },
        {
          "path": "web-ui/app/api/cron/index-all/route.ts",
          "type": "blob",
          "size": 4796
        },
        {
          "path": "web-ui/app/api/cron/index-marketplaces",
          "type": "tree",
          "size": null
        },
        {
          "path": "web-ui/app/api/cron/index-marketplaces/route.ts",
          "type": "blob",
          "size": 1275
        },
        {
          "path": "web-ui/app/api/cron/index-mcp-servers",
          "type": "tree",
          "size": null
        },
        {
          "path": "web-ui/app/api/cron/index-mcp-servers/route.ts",
          "type": "blob",
          "size": 1412
        },
        {
          "path": "web-ui/app/api/cron/index-plugins",
          "type": "tree",
          "size": null
        },
        {
          "path": "web-ui/app/api/cron/index-plugins/route.ts",
          "type": "blob",
          "size": 1331
        },
        {
          "path": "web-ui/app/api/cron/sync-mcp-stats",
          "type": "tree",
          "size": null
        },
        {
          "path": "web-ui/app/api/cron/sync-mcp-stats/route.ts",
          "type": "blob",
          "size": 1356
        },
        {
          "path": "web-ui/app/api/marketplaces",
          "type": "tree",
          "size": null
        },
        {
          "path": "web-ui/app/api/marketplaces/[id]",
          "type": "tree",
          "size": null
        },
        {
          "path": "web-ui/app/api/marketplaces/[id]/install",
          "type": "tree",
          "size": null
        },
        {
          "path": "web-ui/app/api/marketplaces/[id]/install/route.ts",
          "type": "blob",
          "size": 3550
        },
        {
          "path": "web-ui/app/api/marketplaces/[id]/route.ts",
          "type": "blob",
          "size": 2523
        },
        {
          "path": "web-ui/app/api/marketplaces/list",
          "type": "tree",
          "size": null
        },
        {
          "path": "web-ui/app/api/marketplaces/list/route.ts",
          "type": "blob",
          "size": 1155
        },
        {
          "path": "web-ui/app/api/marketplaces/route.ts",
          "type": "blob",
          "size": 3119
        },
        {
          "path": "web-ui/app/api/mcp-servers",
          "type": "tree",
          "size": null
        },
        {
          "path": "web-ui/app/api/mcp-servers/list",
          "type": "tree",
          "size": null
        },
        {
          "path": "web-ui/app/api/mcp-servers/list/route.ts",
          "type": "blob",
          "size": 2428
        },
        {
          "path": "web-ui/app/api/plugins",
          "type": "tree",
          "size": null
        },
        {
          "path": "web-ui/app/api/plugins/list",
          "type": "tree",
          "size": null
        },
        {
          "path": "web-ui/app/api/plugins/list/route.ts",
          "type": "blob",
          "size": 1613
        },
        {
          "path": "web-ui/app/api/plugins/marketplaces",
          "type": "tree",
          "size": null
        },
        {
          "path": "web-ui/app/api/plugins/marketplaces/route.ts",
          "type": "blob",
          "size": 574
        },
        {
          "path": "web-ui/app/apple-touch-icon.png",
          "type": "blob",
          "size": 26414
        },
        {
          "path": "web-ui/app/command",
          "type": "tree",
          "size": null
        },
        {
          "path": "web-ui/app/command/[slug]",
          "type": "tree",
          "size": null
        },
        {
          "path": "web-ui/app/command/[slug]/page-client.tsx",
          "type": "blob",
          "size": 7977
        },
        {
          "path": "web-ui/app/command/[slug]/page.tsx",
          "type": "blob",
          "size": 621
        },
        {
          "path": "web-ui/app/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "web-ui/app/commands/commands-client.tsx",
          "type": "blob",
          "size": 5946
        },
        {
          "path": "web-ui/app/commands/page.tsx",
          "type": "blob",
          "size": 437
        },
        {
          "path": "web-ui/app/contribute",
          "type": "tree",
          "size": null
        },
        {
          "path": "web-ui/app/contribute/page.tsx",
          "type": "blob",
          "size": 26549
        },
        {
          "path": "web-ui/app/docs",
          "type": "tree",
          "size": null
        },
        {
          "path": "web-ui/app/docs/installation",
          "type": "tree",
          "size": null
        },
        {
          "path": "web-ui/app/docs/installation/page.tsx",
          "type": "blob",
          "size": 41601
        },
        {
          "path": "web-ui/app/docs/page.tsx",
          "type": "blob",
          "size": 8990
        },
        {
          "path": "web-ui/app/favicon-16x16.png",
          "type": "blob",
          "size": 810
        },
        {
          "path": "web-ui/app/favicon-32x32.png",
          "type": "blob",
          "size": 1166
        },
        {
          "path": "web-ui/app/favicon.ico",
          "type": "blob",
          "size": 5039
        },
        {
          "path": "web-ui/app/globals.css",
          "type": "blob",
          "size": 5284
        },
        {
          "path": "web-ui/app/hook",
          "type": "tree",
          "size": null
        },
        {
          "path": "web-ui/app/hook/[slug]",
          "type": "tree",
          "size": null
        },
        {
          "path": "web-ui/app/hook/[slug]/page-client.tsx",
          "type": "blob",
          "size": 9082
        },
        {
          "path": "web-ui/app/hook/[slug]/page.tsx",
          "type": "blob",
          "size": 570
        },
        {
          "path": "web-ui/app/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "web-ui/app/hooks/hooks-client.tsx",
          "type": "blob",
          "size": 7742
        },
        {
          "path": "web-ui/app/hooks/page.tsx",
          "type": "blob",
          "size": 482
        },
        {
          "path": "web-ui/app/layout.tsx",
          "type": "blob",
          "size": 2412
        },
        {
          "path": "web-ui/app/marketplaces",
          "type": "tree",
          "size": null
        },
        {
          "path": "web-ui/app/marketplaces/marketplaces-client.tsx",
          "type": "blob",
          "size": 7375
        },
        {
          "path": "web-ui/app/marketplaces/page.tsx",
          "type": "blob",
          "size": 998
        },
        {
          "path": "web-ui/app/mcp-server",
          "type": "tree",
          "size": null
        },
        {
          "path": "web-ui/app/mcp-server/[slug]",
          "type": "tree",
          "size": null
        },
        {
          "path": "web-ui/app/mcp-server/[slug]/page-client.tsx",
          "type": "blob",
          "size": 21391
        },
        {
          "path": "web-ui/app/mcp-server/[slug]/page.tsx",
          "type": "blob",
          "size": 608
        },
        {
          "path": "web-ui/app/mcp-servers",
          "type": "tree",
          "size": null
        },
        {
          "path": "web-ui/app/mcp-servers/mcp-client.tsx",
          "type": "blob",
          "size": 12958
        },
        {
          "path": "web-ui/app/mcp-servers/page.tsx",
          "type": "blob",
          "size": 958
        },
        {
          "path": "web-ui/app/page-client.tsx",
          "type": "blob",
          "size": 15544
        },
        {
          "path": "web-ui/app/page.tsx",
          "type": "blob",
          "size": 952
        },
        {
          "path": "web-ui/app/plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "web-ui/app/plugin/[slug]",
          "type": "tree",
          "size": null
        },
        {
          "path": "web-ui/app/plugin/[slug]/page-client.tsx",
          "type": "blob",
          "size": 9298
        },
        {
          "path": "web-ui/app/plugin/[slug]/page.tsx",
          "type": "blob",
          "size": 983
        },
        {
          "path": "web-ui/app/plugins",
          "type": "tree",
          "size": null
        },
        {
          "path": "web-ui/app/plugins/page.tsx",
          "type": "blob",
          "size": 1310
        },
        {
          "path": "web-ui/app/plugins/plugins-client.tsx",
          "type": "blob",
          "size": 17178
        },
        {
          "path": "web-ui/app/skill",
          "type": "tree",
          "size": null
        },
        {
          "path": "web-ui/app/skill/[slug]",
          "type": "tree",
          "size": null
        },
        {
          "path": "web-ui/app/skill/[slug]/page-client.tsx",
          "type": "blob",
          "size": 6283
        },
        {
          "path": "web-ui/app/skill/[slug]/page.tsx",
          "type": "blob",
          "size": 961
        },
        {
          "path": "web-ui/app/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "web-ui/app/skills/page.tsx",
          "type": "blob",
          "size": 593
        },
        {
          "path": "web-ui/app/skills/skills-client.tsx",
          "type": "blob",
          "size": 5825
        },
        {
          "path": "web-ui/app/subagent",
          "type": "tree",
          "size": null
        },
        {
          "path": "web-ui/app/subagent/[slug]",
          "type": "tree",
          "size": null
        },
        {
          "path": "web-ui/app/subagent/[slug]/page-client.tsx",
          "type": "blob",
          "size": 7618
        },
        {
          "path": "web-ui/app/subagent/[slug]/page.tsx",
          "type": "blob",
          "size": 1046
        },
        {
          "path": "web-ui/app/subagents",
          "type": "tree",
          "size": null
        },
        {
          "path": "web-ui/app/subagents/page.tsx",
          "type": "blob",
          "size": 431
        },
        {
          "path": "web-ui/app/subagents/subagents-client.tsx",
          "type": "blob",
          "size": 5977
        },
        {
          "path": "web-ui/components.json",
          "type": "blob",
          "size": 344
        },
        {
          "path": "web-ui/components",
          "type": "tree",
          "size": null
        },
        {
          "path": "web-ui/components/category-filter.tsx",
          "type": "blob",
          "size": 966
        },
        {
          "path": "web-ui/components/command-card.tsx",
          "type": "blob",
          "size": 3476
        },
        {
          "path": "web-ui/components/create-marketplace-banner.tsx",
          "type": "blob",
          "size": 3281
        },
        {
          "path": "web-ui/components/hook-card.tsx",
          "type": "blob",
          "size": 4829
        },
        {
          "path": "web-ui/components/installation-modal-enhanced.tsx",
          "type": "blob",
          "size": 8150
        },
        {
          "path": "web-ui/components/installation-modal.tsx",
          "type": "blob",
          "size": 12748
        },
        {
          "path": "web-ui/components/marketplace-registry-item.tsx",
          "type": "blob",
          "size": 4753
        },
        {
          "path": "web-ui/components/mcp-card.tsx",
          "type": "blob",
          "size": 5472
        },
        {
          "path": "web-ui/components/mcp-installation-modal.tsx",
          "type": "blob",
          "size": 8926
        },
        {
          "path": "web-ui/components/navigation.tsx",
          "type": "blob",
          "size": 7302
        },
        {
          "path": "web-ui/components/plugin-card.tsx",
          "type": "blob",
          "size": 2210
        },
        {
          "path": "web-ui/components/search-bar.tsx",
          "type": "blob",
          "size": 641
        },
        {
          "path": "web-ui/components/skill-card.tsx",
          "type": "blob",
          "size": 4155
        },
        {
          "path": "web-ui/components/subagent-card.tsx",
          "type": "blob",
          "size": 3415
        },
        {
          "path": "web-ui/components/ui",
          "type": "tree",
          "size": null
        },
        {
          "path": "web-ui/components/ui/badge.tsx",
          "type": "blob",
          "size": 1140
        },
        {
          "path": "web-ui/components/ui/button.tsx",
          "type": "blob",
          "size": 1917
        },
        {
          "path": "web-ui/components/ui/card.tsx",
          "type": "blob",
          "size": 1828
        },
        {
          "path": "web-ui/components/ui/command.tsx",
          "type": "blob",
          "size": 4887
        },
        {
          "path": "web-ui/components/ui/dialog.tsx",
          "type": "blob",
          "size": 3875
        },
        {
          "path": "web-ui/components/ui/dropdown-menu.tsx",
          "type": "blob",
          "size": 7308
        },
        {
          "path": "web-ui/components/ui/input.tsx",
          "type": "blob",
          "size": 768
        },
        {
          "path": "web-ui/components/ui/pagination.tsx",
          "type": "blob",
          "size": 2843
        },
        {
          "path": "web-ui/components/ui/popover.tsx",
          "type": "blob",
          "size": 1356
        },
        {
          "path": "web-ui/components/ui/select.tsx",
          "type": "blob",
          "size": 5745
        },
        {
          "path": "web-ui/components/ui/tabs.tsx",
          "type": "blob",
          "size": 1891
        },
        {
          "path": "web-ui/components/ui/tooltip.tsx",
          "type": "blob",
          "size": 1258
        },
        {
          "path": "web-ui/components/unified-plugin-card.tsx",
          "type": "blob",
          "size": 6505
        },
        {
          "path": "web-ui/drizzle.config.ts",
          "type": "blob",
          "size": 224
        },
        {
          "path": "web-ui/eslint.config.mjs",
          "type": "blob",
          "size": 393
        },
        {
          "path": "web-ui/lib",
          "type": "tree",
          "size": null
        },
        {
          "path": "web-ui/lib/category-utils.ts",
          "type": "blob",
          "size": 3585
        },
        {
          "path": "web-ui/lib/commands-server.ts",
          "type": "blob",
          "size": 3045
        },
        {
          "path": "web-ui/lib/commands-types.ts",
          "type": "blob",
          "size": 303
        },
        {
          "path": "web-ui/lib/db",
          "type": "tree",
          "size": null
        },
        {
          "path": "web-ui/lib/db/client.ts",
          "type": "blob",
          "size": 369
        },
        {
          "path": "web-ui/lib/db/schema.ts",
          "type": "blob",
          "size": 11335
        },
        {
          "path": "web-ui/lib/github",
          "type": "tree",
          "size": null
        },
        {
          "path": "web-ui/lib/github/client.ts",
          "type": "blob",
          "size": 7186
        },
        {
          "path": "web-ui/lib/hook-utils.ts",
          "type": "blob",
          "size": 8081
        },
        {
          "path": "web-ui/lib/hooks-server.ts",
          "type": "blob",
          "size": 3497
        },
        {
          "path": "web-ui/lib/hooks-types.ts",
          "type": "blob",
          "size": 534
        },
        {
          "path": "web-ui/lib/indexer",
          "type": "tree",
          "size": null
        },
        {
          "path": "web-ui/lib/indexer/marketplace-indexer.ts",
          "type": "blob",
          "size": 10156
        },
        {
          "path": "web-ui/lib/indexer/mcp-server-indexer.ts",
          "type": "blob",
          "size": 21379
        },
        {
          "path": "web-ui/lib/indexer/parser.ts",
          "type": "blob",
          "size": 2705
        },
        {
          "path": "web-ui/lib/indexer/plugin-indexer.ts",
          "type": "blob",
          "size": 16140
        },
        {
          "path": "web-ui/lib/marketplace-server.ts",
          "type": "blob",
          "size": 5008
        },
        {
          "path": "web-ui/lib/marketplace-types.ts",
          "type": "blob",
          "size": 492
        },
        {
          "path": "web-ui/lib/mcp-server-db.ts",
          "type": "blob",
          "size": 9137
        },
        {
          "path": "web-ui/lib/mcp-server.ts",
          "type": "blob",
          "size": 4335
        },
        {
          "path": "web-ui/lib/mcp-types.ts",
          "type": "blob",
          "size": 8385
        },
        {
          "path": "web-ui/lib/plugin-db-server.ts",
          "type": "blob",
          "size": 23939
        },
        {
          "path": "web-ui/lib/plugin-server.ts",
          "type": "blob",
          "size": 4028
        },
        {
          "path": "web-ui/lib/plugin-types.ts",
          "type": "blob",
          "size": 1544
        },
        {
          "path": "web-ui/lib/plugin-utils.ts",
          "type": "blob",
          "size": 2802
        },
        {
          "path": "web-ui/lib/plugins-server.ts",
          "type": "blob",
          "size": 2591
        },
        {
          "path": "web-ui/lib/plugins-types.ts",
          "type": "blob",
          "size": 389
        },
        {
          "path": "web-ui/lib/skills-server.ts",
          "type": "blob",
          "size": 3023
        },
        {
          "path": "web-ui/lib/skills-types.ts",
          "type": "blob",
          "size": 609
        },
        {
          "path": "web-ui/lib/subagents-server.ts",
          "type": "blob",
          "size": 2819
        },
        {
          "path": "web-ui/lib/subagents-types.ts",
          "type": "blob",
          "size": 298
        },
        {
          "path": "web-ui/lib/utils.ts",
          "type": "blob",
          "size": 2253
        },
        {
          "path": "web-ui/next.config.ts",
          "type": "blob",
          "size": 4594
        },
        {
          "path": "web-ui/package-lock.json",
          "type": "blob",
          "size": 264155
        },
        {
          "path": "web-ui/package.json",
          "type": "blob",
          "size": 1467
        },
        {
          "path": "web-ui/postcss.config.mjs",
          "type": "blob",
          "size": 94
        },
        {
          "path": "web-ui/public",
          "type": "tree",
          "size": null
        },
        {
          "path": "web-ui/public/file.svg",
          "type": "blob",
          "size": 391
        },
        {
          "path": "web-ui/public/globe.svg",
          "type": "blob",
          "size": 1035
        },
        {
          "path": "web-ui/public/next.svg",
          "type": "blob",
          "size": 1375
        },
        {
          "path": "web-ui/public/schema",
          "type": "tree",
          "size": null
        },
        {
          "path": "web-ui/public/schema/registry.json",
          "type": "blob",
          "size": 4936
        },
        {
          "path": "web-ui/public/vercel.svg",
          "type": "blob",
          "size": 128
        },
        {
          "path": "web-ui/public/window.svg",
          "type": "blob",
          "size": 385
        },
        {
          "path": "web-ui/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "web-ui/scripts/generate-registry.js",
          "type": "blob",
          "size": 6450
        },
        {
          "path": "web-ui/scripts/migrate-mcp-to-db.ts",
          "type": "blob",
          "size": 5692
        },
        {
          "path": "web-ui/trigger.config.ts",
          "type": "blob",
          "size": 480
        },
        {
          "path": "web-ui/trigger",
          "type": "tree",
          "size": null
        },
        {
          "path": "web-ui/trigger/index-plugins.ts",
          "type": "blob",
          "size": 15865
        },
        {
          "path": "web-ui/trigger/scheduled-indexing.ts",
          "type": "blob",
          "size": 1939
        },
        {
          "path": "web-ui/tsconfig.json",
          "type": "blob",
          "size": 698
        },
        {
          "path": "web-ui/vercel.json",
          "type": "blob",
          "size": 820
        }
      ],
      "marketplace": {
        "name": "buildwithclaude",
        "version": "1.0.0",
        "description": null,
        "owner_info": {
          "name": "BuildWithClaude Community",
          "email": "community@buildwithclaude.com",
          "url": "https://github.com/davepoon/buildwithclaude"
        },
        "keywords": [],
        "plugins": [
          {
            "name": "claude-hud",
            "description": "Real-time statusline HUD for Claude Code - displays context usage, tool activity, agent tracking, and todo progress",
            "source": "./plugins/claude-hud",
            "category": "utilities",
            "version": "1.0.0",
            "author": {
              "name": "Build With Claude",
              "url": "https://github.com/davepoon/buildwithclaude"
            },
            "install_commands": [
              "/plugin marketplace add davepoon/buildwithclaude",
              "/plugin install claude-hud@buildwithclaude"
            ],
            "signals": {
              "stars": 2229,
              "forks": 239,
              "pushed_at": "2026-01-12T10:19:01Z",
              "created_at": "2025-07-25T02:26:45Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/setup",
                "description": "Configure claude-hud as your Claude Code statusline",
                "path": "plugins/claude-hud/commands/setup.md",
                "frontmatter": {
                  "description": "Configure claude-hud as your Claude Code statusline"
                },
                "content": "# Claude HUD Setup\n\nConfigure claude-hud as your Claude Code statusline by adding the following configuration to your settings.\n\n## Configuration\n\nAdd this to your `~/.claude/settings.json`:\n\n```json\n{\n  \"statusLine\": {\n    \"type\": \"command\",\n    \"command\": \"node ${CLAUDE_PLUGIN_ROOT}/dist/index.js\"\n  }\n}\n```\n\n## What This Does\n\nThe HUD displays real-time session information:\n\n- **Session Info**: Model name, context usage bar (color-coded), config counts, session duration\n- **Tool Activity**: Running tools with spinner, completed tools with counts\n- **Agent Tracking**: Active subagents with descriptions and runtime\n- **Todo Progress**: Current task and completion metrics\n\n## Manual Setup\n\nIf automatic configuration doesn't work, you can manually:\n\n1. Open `~/.claude/settings.json`\n2. Add or update the `statusLine` section\n3. Restart Claude Code\n\n## Troubleshooting\n\nIf the HUD doesn't appear:\n\n1. Ensure Node.js 18+ is installed\n2. Check that the plugin is properly installed\n3. Verify the statusLine configuration in settings.json\n4. Try running `node ${CLAUDE_PLUGIN_ROOT}/dist/index.js` manually to check for errors\n\n## First-Time Build\n\nIf this is a fresh installation, build the TypeScript:\n\n```bash\ncd ${CLAUDE_PLUGIN_ROOT}\nnpm install\nnpm run build\n```"
              }
            ],
            "skills": []
          },
          {
            "name": "agents-blockchain-web3",
            "description": "Specialized agents for blockchain development, smart contracts, and Web3 applications",
            "source": "./plugins/agents-blockchain-web3",
            "category": "agents",
            "version": "1.0.0",
            "author": {
              "name": "BuildWithClaude Community",
              "url": "https://github.com/davepoon/buildwithclaude"
            },
            "install_commands": [
              "/plugin marketplace add davepoon/buildwithclaude",
              "/plugin install agents-blockchain-web3@buildwithclaude"
            ],
            "signals": {
              "stars": 2229,
              "forks": 239,
              "pushed_at": "2026-01-12T10:19:01Z",
              "created_at": "2025-07-25T02:26:45Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": []
          },
          {
            "name": "agents-business-finance",
            "description": "Agents for business analysis, financial modeling, and KPI tracking",
            "source": "./plugins/agents-business-finance",
            "category": "agents",
            "version": "1.0.0",
            "author": {
              "name": "BuildWithClaude Community",
              "url": "https://github.com/davepoon/buildwithclaude"
            },
            "install_commands": [
              "/plugin marketplace add davepoon/buildwithclaude",
              "/plugin install agents-business-finance@buildwithclaude"
            ],
            "signals": {
              "stars": 2229,
              "forks": 239,
              "pushed_at": "2026-01-12T10:19:01Z",
              "created_at": "2025-07-25T02:26:45Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": []
          },
          {
            "name": "agents-crypto-trading",
            "description": "Expert agents for cryptocurrency trading, DeFi strategies, and market analysis",
            "source": "./plugins/agents-crypto-trading",
            "category": "agents",
            "version": "1.0.0",
            "author": {
              "name": "BuildWithClaude Community",
              "url": "https://github.com/davepoon/buildwithclaude"
            },
            "install_commands": [
              "/plugin marketplace add davepoon/buildwithclaude",
              "/plugin install agents-crypto-trading@buildwithclaude"
            ],
            "signals": {
              "stars": 2229,
              "forks": 239,
              "pushed_at": "2026-01-12T10:19:01Z",
              "created_at": "2025-07-25T02:26:45Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": []
          },
          {
            "name": "agents-data-ai",
            "description": "Agents for data engineering, machine learning, and AI development",
            "source": "./plugins/agents-data-ai",
            "category": "agents",
            "version": "1.0.0",
            "author": {
              "name": "BuildWithClaude Community",
              "url": "https://github.com/davepoon/buildwithclaude"
            },
            "install_commands": [
              "/plugin marketplace add davepoon/buildwithclaude",
              "/plugin install agents-data-ai@buildwithclaude"
            ],
            "signals": {
              "stars": 2229,
              "forks": 239,
              "pushed_at": "2026-01-12T10:19:01Z",
              "created_at": "2025-07-25T02:26:45Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": []
          },
          {
            "name": "agents-design-experience",
            "description": "Agents for UI/UX design, accessibility, and user experience optimization",
            "source": "./plugins/agents-design-experience",
            "category": "agents",
            "version": "1.0.0",
            "author": {
              "name": "BuildWithClaude Community",
              "url": "https://github.com/davepoon/buildwithclaude"
            },
            "install_commands": [
              "/plugin marketplace add davepoon/buildwithclaude",
              "/plugin install agents-design-experience@buildwithclaude"
            ],
            "signals": {
              "stars": 2229,
              "forks": 239,
              "pushed_at": "2026-01-12T10:19:01Z",
              "created_at": "2025-07-25T02:26:45Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": []
          },
          {
            "name": "agents-development-architecture",
            "description": "Expert agents for software architecture, backend development, and system design",
            "source": "./plugins/agents-development-architecture",
            "category": "agents",
            "version": "1.0.0",
            "author": {
              "name": "BuildWithClaude Community",
              "url": "https://github.com/davepoon/buildwithclaude"
            },
            "install_commands": [
              "/plugin marketplace add davepoon/buildwithclaude",
              "/plugin install agents-development-architecture@buildwithclaude"
            ],
            "signals": {
              "stars": 2229,
              "forks": 239,
              "pushed_at": "2026-01-12T10:19:01Z",
              "created_at": "2025-07-25T02:26:45Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": []
          },
          {
            "name": "agents-infrastructure-operations",
            "description": "Agents for cloud infrastructure, DevOps, and database operations",
            "source": "./plugins/agents-infrastructure-operations",
            "category": "agents",
            "version": "1.0.0",
            "author": {
              "name": "BuildWithClaude Community",
              "url": "https://github.com/davepoon/buildwithclaude"
            },
            "install_commands": [
              "/plugin marketplace add davepoon/buildwithclaude",
              "/plugin install agents-infrastructure-operations@buildwithclaude"
            ],
            "signals": {
              "stars": 2229,
              "forks": 239,
              "pushed_at": "2026-01-12T10:19:01Z",
              "created_at": "2025-07-25T02:26:45Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": []
          },
          {
            "name": "agents-language-specialists",
            "description": "Expert agents for specific programming languages (Python, Go, Rust, etc.)",
            "source": "./plugins/agents-language-specialists",
            "category": "agents",
            "version": "1.0.0",
            "author": {
              "name": "BuildWithClaude Community",
              "url": "https://github.com/davepoon/buildwithclaude"
            },
            "install_commands": [
              "/plugin marketplace add davepoon/buildwithclaude",
              "/plugin install agents-language-specialists@buildwithclaude"
            ],
            "signals": {
              "stars": 2229,
              "forks": 239,
              "pushed_at": "2026-01-12T10:19:01Z",
              "created_at": "2025-07-25T02:26:45Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": []
          },
          {
            "name": "agents-quality-security",
            "description": "Agents for code review, security audits, debugging, and quality assurance",
            "source": "./plugins/agents-quality-security",
            "category": "agents",
            "version": "1.0.0",
            "author": {
              "name": "BuildWithClaude Community",
              "url": "https://github.com/davepoon/buildwithclaude"
            },
            "install_commands": [
              "/plugin marketplace add davepoon/buildwithclaude",
              "/plugin install agents-quality-security@buildwithclaude"
            ],
            "signals": {
              "stars": 2229,
              "forks": 239,
              "pushed_at": "2026-01-12T10:19:01Z",
              "created_at": "2025-07-25T02:26:45Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": []
          },
          {
            "name": "agents-sales-marketing",
            "description": "Agents for content marketing, customer support, and sales automation",
            "source": "./plugins/agents-sales-marketing",
            "category": "agents",
            "version": "1.0.0",
            "author": {
              "name": "BuildWithClaude Community",
              "url": "https://github.com/davepoon/buildwithclaude"
            },
            "install_commands": [
              "/plugin marketplace add davepoon/buildwithclaude",
              "/plugin install agents-sales-marketing@buildwithclaude"
            ],
            "signals": {
              "stars": 2229,
              "forks": 239,
              "pushed_at": "2026-01-12T10:19:01Z",
              "created_at": "2025-07-25T02:26:45Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": []
          },
          {
            "name": "agents-specialized-domains",
            "description": "Domain-specific expert agents for research, documentation, and specialized tasks",
            "source": "./plugins/agents-specialized-domains",
            "category": "agents",
            "version": "1.0.0",
            "author": {
              "name": "BuildWithClaude Community",
              "url": "https://github.com/davepoon/buildwithclaude"
            },
            "install_commands": [
              "/plugin marketplace add davepoon/buildwithclaude",
              "/plugin install agents-specialized-domains@buildwithclaude"
            ],
            "signals": {
              "stars": 2229,
              "forks": 239,
              "pushed_at": "2026-01-12T10:19:01Z",
              "created_at": "2025-07-25T02:26:45Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": []
          },
          {
            "name": "commands-api-development",
            "description": "Commands for designing and documenting REST and GraphQL APIs",
            "source": "./plugins/commands-api-development",
            "category": "commands",
            "version": "1.0.0",
            "author": {
              "name": "BuildWithClaude Community",
              "url": "https://github.com/davepoon/buildwithclaude"
            },
            "install_commands": [
              "/plugin marketplace add davepoon/buildwithclaude",
              "/plugin install commands-api-development@buildwithclaude"
            ],
            "signals": {
              "stars": 2229,
              "forks": 239,
              "pushed_at": "2026-01-12T10:19:01Z",
              "created_at": "2025-07-25T02:26:45Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/design-rest-api",
                "description": "Design RESTful API architecture",
                "path": "plugins/commands-api-development/commands/design-rest-api.md",
                "frontmatter": {
                  "description": "Design RESTful API architecture",
                  "category": "api-development"
                },
                "content": "# Design REST API\n\nDesign RESTful API architecture\n\n## Instructions\n\n1. **API Design Strategy and Planning**\n   - Analyze business requirements and define API scope\n   - Identify resources, entities, and their relationships\n   - Plan API versioning strategy and backward compatibility\n   - Define authentication and authorization requirements\n   - Plan for scalability, rate limiting, and performance\n\n2. **RESTful Resource Design**\n   - Design RESTful endpoints following REST principles:\n\n   **Express.js API Structure:**\n   ```javascript\n   // routes/api/v1/index.js\n   const express = require('express');\n   const router = express.Router();\n\n   // Resource-based routing structure\n   const userRoutes = require('./users');\n   const productRoutes = require('./products');\n   const orderRoutes = require('./orders');\n   const authRoutes = require('./auth');\n\n   // API versioning and middleware\n   router.use('/auth', authRoutes);\n   router.use('/users', userRoutes);\n   router.use('/products', productRoutes);\n   router.use('/orders', orderRoutes);\n\n   module.exports = router;\n\n   // routes/api/v1/users.js\n   const express = require('express');\n   const router = express.Router();\n   const { validateRequest, authenticate, authorize } = require('../../../middleware');\n   const userController = require('../../../controllers/userController');\n   const userValidation = require('../../../validations/userValidation');\n\n   // User resource endpoints\n   router.get('/', \n     authenticate,\n     authorize(['admin', 'manager']),\n     validateRequest(userValidation.listUsers),\n     userController.listUsers\n   );\n\n   router.get('/:id', \n     authenticate,\n     validateRequest(userValidation.getUser),\n     userController.getUser\n   );\n\n   router.post('/',\n     authenticate,\n     authorize(['admin']),\n     validateRequest(userValidation.createUser),\n     userController.createUser\n   );\n\n   router.put('/:id',\n     authenticate,\n     validateRequest(userValidation.updateUser),\n     userController.updateUser\n   );\n\n   router.patch('/:id',\n     authenticate,\n     validateRequest(userValidation.patchUser),\n     userController.patchUser\n   );\n\n   router.delete('/:id',\n     authenticate,\n     authorize(['admin']),\n     validateRequest(userValidation.deleteUser),\n     userController.deleteUser\n   );\n\n   // Nested resource endpoints\n   router.get('/:id/orders',\n     authenticate,\n     validateRequest(userValidation.getUserOrders),\n     userController.getUserOrders\n   );\n\n   router.get('/:id/profile',\n     authenticate,\n     validateRequest(userValidation.getUserProfile),\n     userController.getUserProfile\n   );\n\n   module.exports = router;\n   ```\n\n3. **Request/Response Data Models**\n   - Define comprehensive data models and validation:\n\n   **Data Validation with Joi:**\n   ```javascript\n   // validations/userValidation.js\n   const Joi = require('joi');\n\n   const userSchema = {\n     create: Joi.object({\n       email: Joi.string().email().required(),\n       password: Joi.string().min(8).pattern(/^(?=.*[a-z])(?=.*[A-Z])(?=.*\\d)/).required(),\n       firstName: Joi.string().trim().min(1).max(100).required(),\n       lastName: Joi.string().trim().min(1).max(100).required(),\n       phone: Joi.string().pattern(/^\\+?[\\d\\s\\-\\(\\)]{10,20}$/).optional(),\n       dateOfBirth: Joi.date().max('now').optional(),\n       role: Joi.string().valid('user', 'admin', 'manager').default('user')\n     }),\n\n     update: Joi.object({\n       email: Joi.string().email().optional(),\n       firstName: Joi.string().trim().min(1).max(100).optional(),\n       lastName: Joi.string().trim().min(1).max(100).optional(),\n       phone: Joi.string().pattern(/^\\+?[\\d\\s\\-\\(\\)]{10,20}$/).optional(),\n       dateOfBirth: Joi.date().max('now').optional(),\n       status: Joi.string().valid('active', 'inactive', 'suspended').optional()\n     }),\n\n     list: Joi.object({\n       page: Joi.number().integer().min(1).default(1),\n       limit: Joi.number().integer().min(1).max(100).default(20),\n       sort: Joi.string().valid('id', 'email', 'firstName', 'lastName', 'createdAt').default('id'),\n       order: Joi.string().valid('asc', 'desc').default('asc'),\n       search: Joi.string().trim().min(1).optional(),\n       status: Joi.string().valid('active', 'inactive', 'suspended').optional(),\n       role: Joi.string().valid('user', 'admin', 'manager').optional()\n     }),\n\n     params: Joi.object({\n       id: Joi.number().integer().positive().required()\n     })\n   };\n\n   const validateRequest = (schema) => {\n     return (req, res, next) => {\n       const validationTargets = {\n         body: req.body,\n         query: req.query,\n         params: req.params\n       };\n\n       const errors = {};\n\n       // Validate each part of the request\n       Object.keys(schema).forEach(target => {\n         const { error, value } = schema[target].validate(validationTargets[target], {\n           abortEarly: false,\n           allowUnknown: false,\n           stripUnknown: true\n         });\n\n         if (error) {\n           errors[target] = error.details.map(detail => ({\n             field: detail.path.join('.'),\n             message: detail.message,\n             value: detail.context.value\n           }));\n         } else {\n           req[target] = value;\n         }\n       });\n\n       if (Object.keys(errors).length > 0) {\n         return res.status(400).json({\n           error: 'Validation failed',\n           details: errors,\n           timestamp: new Date().toISOString()\n         });\n       }\n\n       next();\n     };\n   };\n\n   module.exports = {\n     listUsers: validateRequest({ query: userSchema.list }),\n     getUser: validateRequest({ params: userSchema.params }),\n     createUser: validateRequest({ body: userSchema.create }),\n     updateUser: validateRequest({ \n       params: userSchema.params, \n       body: userSchema.update \n     }),\n     patchUser: validateRequest({ \n       params: userSchema.params, \n       body: userSchema.update \n     }),\n     deleteUser: validateRequest({ params: userSchema.params }),\n     getUserOrders: validateRequest({ \n       params: userSchema.params,\n       query: Joi.object({\n         page: Joi.number().integer().min(1).default(1),\n         limit: Joi.number().integer().min(1).max(50).default(10),\n         status: Joi.string().valid('pending', 'processing', 'shipped', 'delivered', 'cancelled').optional()\n       })\n     })\n   };\n   ```\n\n4. **Controller Implementation**\n   - Implement robust controller logic:\n\n   **User Controller Example:**\n   ```javascript\n   // controllers/userController.js\n   const userService = require('../services/userService');\n   const { ApiError, ApiResponse } = require('../utils/apiResponse');\n\n   class UserController {\n     async listUsers(req, res, next) {\n       try {\n         const { page, limit, sort, order, search, status, role } = req.query;\n         \n         const filters = {};\n         if (search) filters.search = search;\n         if (status) filters.status = status;\n         if (role) filters.role = role;\n\n         const result = await userService.findUsers({\n           page,\n           limit,\n           sort,\n           order,\n           filters\n         });\n\n         res.json(new ApiResponse('success', 'Users retrieved successfully', {\n           users: result.users,\n           pagination: {\n             page: result.page,\n             limit: result.limit,\n             total: result.total,\n             totalPages: result.totalPages,\n             hasNext: result.hasNext,\n             hasPrev: result.hasPrev\n           }\n         }));\n       } catch (error) {\n         next(error);\n       }\n     }\n\n     async getUser(req, res, next) {\n       try {\n         const { id } = req.params;\n         const requestingUserId = req.user.id;\n         const requestingUserRole = req.user.role;\n\n         // Authorization check\n         if (id !== requestingUserId && !['admin', 'manager'].includes(requestingUserRole)) {\n           throw new ApiError(403, 'Insufficient permissions to access this user');\n         }\n\n         const user = await userService.findById(id);\n         if (!user) {\n           throw new ApiError(404, 'User not found');\n         }\n\n         // Filter sensitive data based on permissions\n         const filteredUser = userService.filterUserData(user, requestingUserRole, requestingUserId);\n\n         res.json(new ApiResponse('success', 'User retrieved successfully', { user: filteredUser }));\n       } catch (error) {\n         next(error);\n       }\n     }\n\n     async createUser(req, res, next) {\n       try {\n         const userData = req.body;\n         \n         // Check for existing user\n         const existingUser = await userService.findByEmail(userData.email);\n         if (existingUser) {\n           throw new ApiError(409, 'User with this email already exists');\n         }\n\n         const newUser = await userService.createUser(userData);\n         \n         // Remove sensitive data from response\n         const responseUser = userService.filterUserData(newUser, 'admin');\n\n         res.status(201).json(new ApiResponse(\n           'success', \n           'User created successfully', \n           { user: responseUser }\n         ));\n       } catch (error) {\n         next(error);\n       }\n     }\n\n     async updateUser(req, res, next) {\n       try {\n         const { id } = req.params;\n         const updateData = req.body;\n         const requestingUserId = req.user.id;\n         const requestingUserRole = req.user.role;\n\n         // Authorization check\n         if (id !== requestingUserId && !['admin', 'manager'].includes(requestingUserRole)) {\n           throw new ApiError(403, 'Insufficient permissions to update this user');\n         }\n\n         // Restrict certain fields based on role\n         if (updateData.role && !['admin'].includes(requestingUserRole)) {\n           throw new ApiError(403, 'Insufficient permissions to update user role');\n         }\n\n         const existingUser = await userService.findById(id);\n         if (!existingUser) {\n           throw new ApiError(404, 'User not found');\n         }\n\n         const updatedUser = await userService.updateUser(id, updateData);\n         const filteredUser = userService.filterUserData(updatedUser, requestingUserRole, requestingUserId);\n\n         res.json(new ApiResponse('success', 'User updated successfully', { user: filteredUser }));\n       } catch (error) {\n         next(error);\n       }\n     }\n\n     async deleteUser(req, res, next) {\n       try {\n         const { id } = req.params;\n         const requestingUserId = req.user.id;\n\n         // Prevent self-deletion\n         if (id === requestingUserId) {\n           throw new ApiError(400, 'Cannot delete your own account');\n         }\n\n         const existingUser = await userService.findById(id);\n         if (!existingUser) {\n           throw new ApiError(404, 'User not found');\n         }\n\n         await userService.deleteUser(id);\n\n         res.status(204).send();\n       } catch (error) {\n         next(error);\n       }\n     }\n\n     async getUserOrders(req, res, next) {\n       try {\n         const { id } = req.params;\n         const { page, limit, status } = req.query;\n         const requestingUserId = req.user.id;\n         const requestingUserRole = req.user.role;\n\n         // Authorization check\n         if (id !== requestingUserId && !['admin', 'manager'].includes(requestingUserRole)) {\n           throw new ApiError(403, 'Insufficient permissions to access user orders');\n         }\n\n         const orders = await userService.getUserOrders(id, {\n           page,\n           limit,\n           status\n         });\n\n         res.json(new ApiResponse('success', 'User orders retrieved successfully', orders));\n       } catch (error) {\n         next(error);\n       }\n     }\n   }\n\n   module.exports = new UserController();\n   ```\n\n5. **API Response Standardization**\n   - Implement consistent response formats:\n\n   **API Response Utilities:**\n   ```javascript\n   // utils/apiResponse.js\n   class ApiResponse {\n     constructor(status, message, data = null, meta = null) {\n       this.status = status;\n       this.message = message;\n       this.timestamp = new Date().toISOString();\n       \n       if (data !== null) {\n         this.data = data;\n       }\n       \n       if (meta !== null) {\n         this.meta = meta;\n       }\n     }\n\n     static success(message, data = null, meta = null) {\n       return new ApiResponse('success', message, data, meta);\n     }\n\n     static error(message, errors = null) {\n       const response = new ApiResponse('error', message);\n       if (errors) {\n         response.errors = errors;\n       }\n       return response;\n     }\n\n     static paginated(message, data, pagination) {\n       return new ApiResponse('success', message, data, { pagination });\n     }\n   }\n\n   class ApiError extends Error {\n     constructor(statusCode, message, errors = null, isOperational = true, stack = '') {\n       super(message);\n       this.statusCode = statusCode;\n       this.isOperational = isOperational;\n       this.errors = errors;\n       \n       if (stack) {\n         this.stack = stack;\n       } else {\n         Error.captureStackTrace(this, this.constructor);\n       }\n     }\n\n     static badRequest(message, errors = null) {\n       return new ApiError(400, message, errors);\n     }\n\n     static unauthorized(message = 'Unauthorized access') {\n       return new ApiError(401, message);\n     }\n\n     static forbidden(message = 'Forbidden access') {\n       return new ApiError(403, message);\n     }\n\n     static notFound(message = 'Resource not found') {\n       return new ApiError(404, message);\n     }\n\n     static conflict(message, errors = null) {\n       return new ApiError(409, message, errors);\n     }\n\n     static validationError(message, errors) {\n       return new ApiError(422, message, errors);\n     }\n\n     static internalError(message = 'Internal server error') {\n       return new ApiError(500, message);\n     }\n   }\n\n   // Error handling middleware\n   const errorHandler = (error, req, res, next) => {\n     let { statusCode, message, errors } = error;\n\n     if (!error.isOperational) {\n       statusCode = 500;\n       message = 'Internal server error';\n       \n       // Log unexpected errors\n       console.error('Unexpected error:', error);\n     }\n\n     const response = ApiResponse.error(message, errors);\n     \n     // Add request ID for tracking\n     if (req.requestId) {\n       response.requestId = req.requestId;\n     }\n\n     // Add stack trace in development\n     if (process.env.NODE_ENV === 'development') {\n       response.stack = error.stack;\n     }\n\n     res.status(statusCode).json(response);\n   };\n\n   // 404 handler\n   const notFoundHandler = (req, res) => {\n     const error = ApiError.notFound(`Route ${req.originalUrl} not found`);\n     res.status(404).json(ApiResponse.error(error.message));\n   };\n\n   module.exports = {\n     ApiResponse,\n     ApiError,\n     errorHandler,\n     notFoundHandler\n   };\n   ```\n\n6. **Authentication and Authorization**\n   - Implement comprehensive auth system:\n\n   **JWT Authentication Middleware:**\n   ```javascript\n   // middleware/auth.js\n   const jwt = require('jsonwebtoken');\n   const { ApiError } = require('../utils/apiResponse');\n   const userService = require('../services/userService');\n\n   class AuthMiddleware {\n     static async authenticate(req, res, next) {\n       try {\n         const authHeader = req.headers.authorization;\n         \n         if (!authHeader) {\n           throw ApiError.unauthorized('Access token is required');\n         }\n\n         const token = authHeader.startsWith('Bearer ') \n           ? authHeader.slice(7) \n           : authHeader;\n\n         if (!token) {\n           throw ApiError.unauthorized('Invalid authorization header format');\n         }\n\n         let decoded;\n         try {\n           decoded = jwt.verify(token, process.env.JWT_SECRET);\n         } catch (jwtError) {\n           if (jwtError.name === 'TokenExpiredError') {\n             throw ApiError.unauthorized('Access token has expired');\n           } else if (jwtError.name === 'JsonWebTokenError') {\n             throw ApiError.unauthorized('Invalid access token');\n           } else {\n             throw ApiError.unauthorized('Token verification failed');\n           }\n         }\n\n         // Fetch user and verify account status\n         const user = await userService.findById(decoded.userId);\n         if (!user) {\n           throw ApiError.unauthorized('User not found');\n         }\n\n         if (user.status !== 'active') {\n           throw ApiError.unauthorized('Account is not active');\n         }\n\n         // Check if token is still valid (not invalidated)\n         if (user.tokenVersion && decoded.tokenVersion !== user.tokenVersion) {\n           throw ApiError.unauthorized('Token has been invalidated');\n         }\n\n         // Attach user to request\n         req.user = {\n           id: user.id,\n           email: user.email,\n           role: user.role,\n           permissions: user.permissions || []\n         };\n\n         next();\n       } catch (error) {\n         next(error);\n       }\n     }\n\n     static authorize(requiredRoles = [], requiredPermissions = []) {\n       return (req, res, next) => {\n         try {\n           if (!req.user) {\n             throw ApiError.unauthorized('Authentication required');\n           }\n\n           // Check role-based authorization\n           if (requiredRoles.length > 0) {\n             const hasRequiredRole = requiredRoles.includes(req.user.role);\n             if (!hasRequiredRole) {\n               throw ApiError.forbidden(`Requires one of the following roles: ${requiredRoles.join(', ')}`);\n             }\n           }\n\n           // Check permission-based authorization\n           if (requiredPermissions.length > 0) {\n             const userPermissions = req.user.permissions || [];\n             const hasRequiredPermission = requiredPermissions.some(permission => \n               userPermissions.includes(permission)\n             );\n             \n             if (!hasRequiredPermission) {\n               throw ApiError.forbidden(`Requires one of the following permissions: ${requiredPermissions.join(', ')}`);\n             }\n           }\n\n           next();\n         } catch (error) {\n           next(error);\n         }\n       };\n     }\n\n     static async rateLimitByUser(req, res, next) {\n       try {\n         if (!req.user) {\n           return next();\n         }\n\n         const userId = req.user.id;\n         const key = `rate_limit:${userId}:${req.route.path}`;\n         \n         // Implement rate limiting logic here\n         // This is a simplified example\n         const requestCount = await redis.incr(key);\n         if (requestCount === 1) {\n           await redis.expire(key, 3600); // 1 hour window\n         }\n\n         const limit = req.user.role === 'admin' ? 1000 : 100; // Different limits by role\n         \n         if (requestCount > limit) {\n           throw ApiError.tooManyRequests('Rate limit exceeded');\n         }\n\n         res.set({\n           'X-RateLimit-Limit': limit,\n           'X-RateLimit-Remaining': Math.max(0, limit - requestCount),\n           'X-RateLimit-Reset': new Date(Date.now() + 3600000).toISOString()\n         });\n\n         next();\n       } catch (error) {\n         next(error);\n       }\n     }\n   }\n\n   module.exports = AuthMiddleware;\n   ```\n\n7. **API Documentation with OpenAPI/Swagger**\n   - Generate comprehensive API documentation:\n\n   **Swagger Configuration:**\n   ```javascript\n   // swagger/swagger.js\n   const swaggerJsdoc = require('swagger-jsdoc');\n   const swaggerUi = require('swagger-ui-express');\n\n   const options = {\n     definition: {\n       openapi: '3.0.0',\n       info: {\n         title: 'REST API',\n         version: '1.0.0',\n         description: 'A comprehensive REST API with authentication and authorization',\n         contact: {\n           name: 'API Support',\n           email: 'api-support@example.com'\n         },\n         license: {\n           name: 'MIT',\n           url: 'https://opensource.org/licenses/MIT'\n         }\n       },\n       servers: [\n         {\n           url: process.env.API_URL || 'http://localhost:3000',\n           description: 'Development server'\n         },\n         {\n           url: 'https://api.example.com',\n           description: 'Production server'\n         }\n       ],\n       components: {\n         securitySchemes: {\n           bearerAuth: {\n             type: 'http',\n             scheme: 'bearer',\n             bearerFormat: 'JWT',\n             description: 'JWT Authorization header using the Bearer scheme'\n           }\n         },\n         schemas: {\n           User: {\n             type: 'object',\n             required: ['email', 'firstName', 'lastName'],\n             properties: {\n               id: {\n                 type: 'integer',\n                 description: 'Unique user identifier',\n                 example: 1\n               },\n               email: {\n                 type: 'string',\n                 format: 'email',\n                 description: 'User email address',\n                 example: 'user@example.com'\n               },\n               firstName: {\n                 type: 'string',\n                 description: 'User first name',\n                 example: 'John'\n               },\n               lastName: {\n                 type: 'string',\n                 description: 'User last name',\n                 example: 'Doe'\n               },\n               role: {\n                 type: 'string',\n                 enum: ['user', 'admin', 'manager'],\n                 description: 'User role',\n                 example: 'user'\n               },\n               status: {\n                 type: 'string',\n                 enum: ['active', 'inactive', 'suspended'],\n                 description: 'Account status',\n                 example: 'active'\n               },\n               createdAt: {\n                 type: 'string',\n                 format: 'date-time',\n                 description: 'Account creation timestamp'\n               },\n               updatedAt: {\n                 type: 'string',\n                 format: 'date-time',\n                 description: 'Last update timestamp'\n               }\n             }\n           },\n           ApiResponse: {\n             type: 'object',\n             properties: {\n               status: {\n                 type: 'string',\n                 enum: ['success', 'error'],\n                 example: 'success'\n               },\n               message: {\n                 type: 'string',\n                 example: 'Operation completed successfully'\n               },\n               timestamp: {\n                 type: 'string',\n                 format: 'date-time',\n                 example: '2024-01-15T10:30:00Z'\n               },\n               data: {\n                 type: 'object',\n                 description: 'Response data (varies by endpoint)'\n               }\n             }\n           },\n           ErrorResponse: {\n             type: 'object',\n             properties: {\n               status: {\n                 type: 'string',\n                 enum: ['error'],\n                 example: 'error'\n               },\n               message: {\n                 type: 'string',\n                 example: 'An error occurred'\n               },\n               timestamp: {\n                 type: 'string',\n                 format: 'date-time'\n               },\n               errors: {\n                 type: 'object',\n                 description: 'Detailed error information'\n               }\n             }\n           },\n           PaginationMeta: {\n             type: 'object',\n             properties: {\n               pagination: {\n                 type: 'object',\n                 properties: {\n                   page: { type: 'integer', example: 1 },\n                   limit: { type: 'integer', example: 20 },\n                   total: { type: 'integer', example: 100 },\n                   totalPages: { type: 'integer', example: 5 },\n                   hasNext: { type: 'boolean', example: true },\n                   hasPrev: { type: 'boolean', example: false }\n                 }\n               }\n             }\n           }\n         },\n         responses: {\n           UnauthorizedError: {\n             description: 'Access token is missing or invalid',\n             content: {\n               'application/json': {\n                 schema: { $ref: '#/components/schemas/ErrorResponse' }\n               }\n             }\n           },\n           ForbiddenError: {\n             description: 'Insufficient permissions',\n             content: {\n               'application/json': {\n                 schema: { $ref: '#/components/schemas/ErrorResponse' }\n               }\n             }\n           },\n           NotFoundError: {\n             description: 'Resource not found',\n             content: {\n               'application/json': {\n                 schema: { $ref: '#/components/schemas/ErrorResponse' }\n               }\n             }\n           },\n           ValidationError: {\n             description: 'Request validation failed',\n             content: {\n               'application/json': {\n                 schema: { $ref: '#/components/schemas/ErrorResponse' }\n               }\n             }\n           }\n         }\n       },\n       security: [\n         {\n           bearerAuth: []\n         }\n       ]\n     },\n     apis: ['./routes/**/*.js', './controllers/**/*.js']\n   };\n\n   const specs = swaggerJsdoc(options);\n\n   const swaggerOptions = {\n     explorer: true,\n     swaggerOptions: {\n       docExpansion: 'none',\n       filter: true,\n       showRequestDuration: true\n     }\n   };\n\n   module.exports = {\n     serve: swaggerUi.serve,\n     setup: swaggerUi.setup(specs, swaggerOptions),\n     specs\n   };\n   ```\n\n   **Controller Documentation:**\n   ```javascript\n   // Add to userController.js\n   /**\n    * @swagger\n    * /api/v1/users:\n    *   get:\n    *     summary: List all users\n    *     tags: [Users]\n    *     security:\n    *       - bearerAuth: []\n    *     parameters:\n    *       - in: query\n    *         name: page\n    *         schema:\n    *           type: integer\n    *           minimum: 1\n    *           default: 1\n    *         description: Page number\n    *       - in: query\n    *         name: limit\n    *         schema:\n    *           type: integer\n    *           minimum: 1\n    *           maximum: 100\n    *           default: 20\n    *         description: Number of users per page\n    *       - in: query\n    *         name: search\n    *         schema:\n    *           type: string\n    *         description: Search term for user names or email\n    *       - in: query\n    *         name: status\n    *         schema:\n    *           type: string\n    *           enum: [active, inactive, suspended]\n    *         description: Filter by user status\n    *     responses:\n    *       200:\n    *         description: Users retrieved successfully\n    *         content:\n    *           application/json:\n    *             schema:\n    *               allOf:\n    *                 - $ref: '#/components/schemas/ApiResponse'\n    *                 - type: object\n    *                   properties:\n    *                     data:\n    *                       type: object\n    *                       properties:\n    *                         users:\n    *                           type: array\n    *                           items:\n    *                             $ref: '#/components/schemas/User'\n    *                     meta:\n    *                       $ref: '#/components/schemas/PaginationMeta'\n    *       401:\n    *         $ref: '#/components/responses/UnauthorizedError'\n    *       403:\n    *         $ref: '#/components/responses/ForbiddenError'\n    *\n    *   post:\n    *     summary: Create a new user\n    *     tags: [Users]\n    *     security:\n    *       - bearerAuth: []\n    *     requestBody:\n    *       required: true\n    *       content:\n    *         application/json:\n    *           schema:\n    *             type: object\n    *             required:\n    *               - email\n    *               - password\n    *               - firstName\n    *               - lastName\n    *             properties:\n    *               email:\n    *                 type: string\n    *                 format: email\n    *               password:\n    *                 type: string\n    *                 minLength: 8\n    *               firstName:\n    *                 type: string\n    *                 minLength: 1\n    *                 maxLength: 100\n    *               lastName:\n    *                 type: string\n    *                 minLength: 1\n    *                 maxLength: 100\n    *               phone:\n    *                 type: string\n    *               role:\n    *                 type: string\n    *                 enum: [user, admin, manager]\n    *     responses:\n    *       201:\n    *         description: User created successfully\n    *         content:\n    *           application/json:\n    *             schema:\n    *               allOf:\n    *                 - $ref: '#/components/schemas/ApiResponse'\n    *                 - type: object\n    *                   properties:\n    *                     data:\n    *                       type: object\n    *                       properties:\n    *                         user:\n    *                           $ref: '#/components/schemas/User'\n    *       400:\n    *         $ref: '#/components/responses/ValidationError'\n    *       409:\n    *         description: User with email already exists\n    */\n   ```\n\n8. **API Testing and Quality Assurance**\n   - Implement comprehensive API testing:\n\n   **API Test Suite:**\n   ```javascript\n   // tests/api/users.test.js\n   const request = require('supertest');\n   const app = require('../../app');\n   const { setupTestDb, teardownTestDb, createTestUser, getAuthToken } = require('../helpers/testHelpers');\n\n   describe('Users API', () => {\n     let authToken;\n     let testUser;\n\n     beforeAll(async () => {\n       await setupTestDb();\n       testUser = await createTestUser({ role: 'admin' });\n       authToken = await getAuthToken(testUser);\n     });\n\n     afterAll(async () => {\n       await teardownTestDb();\n     });\n\n     describe('GET /api/v1/users', () => {\n       test('should return paginated users list for admin', async () => {\n         const response = await request(app)\n           .get('/api/v1/users')\n           .set('Authorization', `Bearer ${authToken}`)\n           .expect(200);\n\n         expect(response.body).toMatchObject({\n           status: 'success',\n           message: 'Users retrieved successfully',\n           data: {\n             users: expect.any(Array)\n           },\n           meta: {\n             pagination: {\n               page: 1,\n               limit: 20,\n               total: expect.any(Number),\n               totalPages: expect.any(Number),\n               hasNext: expect.any(Boolean),\n               hasPrev: false\n             }\n           }\n         });\n\n         expect(response.body.data.users[0]).toHaveProperty('id');\n         expect(response.body.data.users[0]).toHaveProperty('email');\n         expect(response.body.data.users[0]).not.toHaveProperty('password');\n       });\n\n       test('should filter users by status', async () => {\n         const response = await request(app)\n           .get('/api/v1/users?status=active')\n           .set('Authorization', `Bearer ${authToken}`)\n           .expect(200);\n\n         response.body.data.users.forEach(user => {\n           expect(user.status).toBe('active');\n         });\n       });\n\n       test('should return 401 without auth token', async () => {\n         const response = await request(app)\n           .get('/api/v1/users')\n           .expect(401);\n\n         expect(response.body).toMatchObject({\n           status: 'error',\n           message: 'Access token is required'\n         });\n       });\n\n       test('should validate pagination parameters', async () => {\n         const response = await request(app)\n           .get('/api/v1/users?page=0&limit=200')\n           .set('Authorization', `Bearer ${authToken}`)\n           .expect(400);\n\n         expect(response.body.status).toBe('error');\n         expect(response.body.details).toBeDefined();\n       });\n     });\n\n     describe('POST /api/v1/users', () => {\n       test('should create user with valid data', async () => {\n         const userData = {\n           email: 'newuser@example.com',\n           password: 'SecurePass123',\n           firstName: 'New',\n           lastName: 'User',\n           role: 'user'\n         };\n\n         const response = await request(app)\n           .post('/api/v1/users')\n           .set('Authorization', `Bearer ${authToken}`)\n           .send(userData)\n           .expect(201);\n\n         expect(response.body).toMatchObject({\n           status: 'success',\n           message: 'User created successfully',\n           data: {\n             user: {\n               email: userData.email,\n               firstName: userData.firstName,\n               lastName: userData.lastName,\n               role: userData.role\n             }\n           }\n         });\n\n         expect(response.body.data.user).not.toHaveProperty('password');\n       });\n\n       test('should reject invalid email format', async () => {\n         const userData = {\n           email: 'invalid-email',\n           password: 'SecurePass123',\n           firstName: 'Test',\n           lastName: 'User'\n         };\n\n         const response = await request(app)\n           .post('/api/v1/users')\n           .set('Authorization', `Bearer ${authToken}`)\n           .send(userData)\n           .expect(400);\n\n         expect(response.body.status).toBe('error');\n         expect(response.body.details.body).toBeDefined();\n       });\n\n       test('should reject duplicate email', async () => {\n         const userData = {\n           email: testUser.email,\n           password: 'SecurePass123',\n           firstName: 'Test',\n           lastName: 'User'\n         };\n\n         const response = await request(app)\n           .post('/api/v1/users')\n           .set('Authorization', `Bearer ${authToken}`)\n           .send(userData)\n           .expect(409);\n\n         expect(response.body).toMatchObject({\n           status: 'error',\n           message: 'User with this email already exists'\n         });\n       });\n     });\n\n     describe('Performance Tests', () => {\n       test('should handle concurrent requests', async () => {\n         const promises = Array(10).fill().map(() =>\n           request(app)\n             .get('/api/v1/users')\n             .set('Authorization', `Bearer ${authToken}`)\n         );\n\n         const responses = await Promise.all(promises);\n         \n         responses.forEach(response => {\n           expect(response.status).toBe(200);\n         });\n       });\n\n       test('should respond within acceptable time', async () => {\n         const start = Date.now();\n         \n         await request(app)\n           .get('/api/v1/users')\n           .set('Authorization', `Bearer ${authToken}`)\n           .expect(200);\n         \n         const duration = Date.now() - start;\n         expect(duration).toBeLessThan(1000); // Should respond within 1 second\n       });\n     });\n   });\n   ```\n\n9. **API Versioning Strategy**\n   - Implement flexible API versioning:\n\n   **Version Management:**\n   ```javascript\n   // middleware/versioning.js\n   class ApiVersioning {\n     static extractVersion(req) {\n       // Support multiple versioning strategies\n       \n       // 1. URL path versioning (preferred)\n       const pathVersion = req.path.match(/^\\/api\\/v(\\d+)/);\n       if (pathVersion) {\n         return parseInt(pathVersion[1]);\n       }\n       \n       // 2. Header versioning\n       const headerVersion = req.headers['api-version'];\n       if (headerVersion) {\n         return parseInt(headerVersion);\n       }\n       \n       // 3. Accept header versioning\n       const acceptHeader = req.headers.accept;\n       if (acceptHeader) {\n         const versionMatch = acceptHeader.match(/application\\/vnd\\.api\\.v(\\d+)\\+json/);\n         if (versionMatch) {\n           return parseInt(versionMatch[1]);\n         }\n       }\n       \n       // Default to latest version\n       return this.getLatestVersion();\n     }\n\n     static getLatestVersion() {\n       return 1; // Update when new versions are released\n     }\n\n     static getSupportedVersions() {\n       return [1]; // Add versions as they're created\n     }\n\n     static middleware() {\n       return (req, res, next) => {\n         const requestedVersion = this.extractVersion(req);\n         const supportedVersions = this.getSupportedVersions();\n         \n         if (!supportedVersions.includes(requestedVersion)) {\n           return res.status(400).json({\n             status: 'error',\n             message: `API version ${requestedVersion} is not supported`,\n             supportedVersions: supportedVersions,\n             latestVersion: this.getLatestVersion()\n           });\n         }\n         \n         req.apiVersion = requestedVersion;\n         res.set('API-Version', requestedVersion.toString());\n         \n         next();\n       };\n     }\n\n     static versionedRoute(versions) {\n       return (req, res, next) => {\n         const currentVersion = req.apiVersion || this.getLatestVersion();\n         \n         if (versions[currentVersion]) {\n           return versions[currentVersion](req, res, next);\n         }\n         \n         // Fallback to latest version if current version handler not found\n         const latestVersion = Math.max(...Object.keys(versions).map(Number));\n         if (versions[latestVersion]) {\n           return versions[latestVersion](req, res, next);\n         }\n         \n         res.status(501).json({\n           status: 'error',\n           message: `Version ${currentVersion} is not implemented for this endpoint`\n         });\n       };\n     }\n   }\n\n   // Usage example:\n   // router.get('/users', ApiVersioning.versionedRoute({\n   //   1: userControllerV1.listUsers,\n   //   2: userControllerV2.listUsers\n   // }));\n\n   module.exports = ApiVersioning;\n   ```\n\n10. **Production Monitoring and Analytics**\n    - Implement API monitoring and analytics:\n\n    **API Analytics Middleware:**\n    ```javascript\n    // middleware/analytics.js\n    const prometheus = require('prom-client');\n\n    class ApiAnalytics {\n      constructor() {\n        this.setupMetrics();\n      }\n\n      setupMetrics() {\n        // Request duration histogram\n        this.httpRequestDuration = new prometheus.Histogram({\n          name: 'http_request_duration_seconds',\n          help: 'Duration of HTTP requests in seconds',\n          labelNames: ['method', 'route', 'status_code', 'version'],\n          buckets: [0.1, 0.3, 0.5, 0.7, 1, 3, 5, 7, 10]\n        });\n\n        // Request counter\n        this.httpRequestsTotal = new prometheus.Counter({\n          name: 'http_requests_total',\n          help: 'Total number of HTTP requests',\n          labelNames: ['method', 'route', 'status_code', 'version']\n        });\n\n        // Active connections gauge\n        this.activeConnections = new prometheus.Gauge({\n          name: 'http_active_connections',\n          help: 'Number of active HTTP connections'\n        });\n\n        // Error rate counter\n        this.httpErrorsTotal = new prometheus.Counter({\n          name: 'http_errors_total',\n          help: 'Total number of HTTP errors',\n          labelNames: ['method', 'route', 'status_code', 'error_type']\n        });\n      }\n\n      middleware() {\n        return (req, res, next) => {\n          const startTime = Date.now();\n          this.activeConnections.inc();\n\n          res.on('finish', () => {\n            const duration = (Date.now() - startTime) / 1000;\n            const route = req.route?.path || req.path;\n            const version = req.apiVersion || 'unknown';\n\n            const labels = {\n              method: req.method,\n              route: route,\n              status_code: res.statusCode,\n              version: version\n            };\n\n            // Record metrics\n            this.httpRequestDuration.observe(labels, duration);\n            this.httpRequestsTotal.inc(labels);\n            this.activeConnections.dec();\n\n            // Record errors\n            if (res.statusCode >= 400) {\n              this.httpErrorsTotal.inc({\n                ...labels,\n                error_type: this.getErrorType(res.statusCode)\n              });\n            }\n\n            // Log slow requests\n            if (duration > 1) {\n              console.warn('Slow request detected:', {\n                method: req.method,\n                url: req.url,\n                duration: duration,\n                statusCode: res.statusCode\n              });\n            }\n          });\n\n          next();\n        };\n      }\n\n      getErrorType(statusCode) {\n        if (statusCode >= 400 && statusCode < 500) {\n          return 'client_error';\n        } else if (statusCode >= 500) {\n          return 'server_error';\n        }\n        return 'unknown';\n      }\n\n      getMetrics() {\n        return prometheus.register.metrics();\n      }\n    }\n\n    module.exports = new ApiAnalytics();\n    ```"
              },
              {
                "name": "/doc-api",
                "description": "Generate API documentation from code",
                "path": "plugins/commands-api-development/commands/doc-api.md",
                "frontmatter": {
                  "description": "Generate API documentation from code",
                  "category": "api-development",
                  "argument-hint": "1. **Code Analysis and Discovery**"
                },
                "content": "# API Documentation Generator Command\n\nGenerate API documentation from code\n\n## Instructions\n\nFollow this systematic approach to create API documentation: **$ARGUMENTS**\n\n1. **Code Analysis and Discovery**\n   - Scan the codebase for API endpoints, routes, and handlers\n   - Identify REST APIs, GraphQL schemas, and RPC services\n   - Map out controller classes, route definitions, and middleware\n   - Discover request/response models and data structures\n\n2. **Documentation Tool Selection**\n   - Choose appropriate documentation tools based on stack:\n     - **OpenAPI/Swagger**: REST APIs with interactive documentation\n     - **GraphQL**: GraphiQL, GraphQL Playground, or Apollo Studio\n     - **Postman**: API collections and documentation\n     - **Insomnia**: API design and documentation\n     - **Redoc**: Alternative OpenAPI renderer\n     - **API Blueprint**: Markdown-based API documentation\n\n3. **API Specification Generation**\n   \n   **For REST APIs with OpenAPI:**\n   ```yaml\n   openapi: 3.0.0\n   info:\n     title: $ARGUMENTS API\n     version: 1.0.0\n     description: Comprehensive API for $ARGUMENTS\n   servers:\n     - url: https://api.example.com/v1\n   paths:\n     /users:\n       get:\n         summary: List users\n         parameters:\n           - name: page\n             in: query\n             schema:\n               type: integer\n         responses:\n           '200':\n             description: Successful response\n             content:\n               application/json:\n                 schema:\n                   type: array\n                   items:\n                     $ref: '#/components/schemas/User'\n   components:\n     schemas:\n       User:\n         type: object\n         properties:\n           id:\n             type: integer\n           name:\n             type: string\n           email:\n             type: string\n   ```\n\n4. **Endpoint Documentation**\n   - Document all HTTP methods (GET, POST, PUT, DELETE, PATCH)\n   - Specify request parameters (path, query, header, body)\n   - Define response schemas and status codes\n   - Include error responses and error codes\n   - Document authentication and authorization requirements\n\n5. **Request/Response Examples**\n   - Provide realistic request examples for each endpoint\n   - Include sample response data with proper formatting\n   - Show different response scenarios (success, error, edge cases)\n   - Document content types and encoding\n\n6. **Authentication Documentation**\n   - Document authentication methods (API keys, JWT, OAuth)\n   - Explain authorization scopes and permissions\n   - Provide authentication examples and token formats\n   - Document session management and refresh token flows\n\n7. **Data Model Documentation**\n   - Define all data schemas and models\n   - Document field types, constraints, and validation rules\n   - Include relationships between entities\n   - Provide example data structures\n\n8. **Error Handling Documentation**\n   - Document all possible error responses\n   - Explain error codes and their meanings\n   - Provide troubleshooting guidance\n   - Include rate limiting and throttling information\n\n9. **Interactive Documentation Setup**\n   \n   **Swagger UI Integration:**\n   ```html\n   <!DOCTYPE html>\n   <html>\n   <head>\n     <title>API Documentation</title>\n     <link rel=\"stylesheet\" type=\"text/css\" href=\"./swagger-ui-bundle.css\" />\n   </head>\n   <body>\n     <div id=\"swagger-ui\"></div>\n     <script src=\"./swagger-ui-bundle.js\"></script>\n     <script>\n       SwaggerUIBundle({\n         url: './api-spec.yaml',\n         dom_id: '#swagger-ui'\n       });\n     </script>\n   </body>\n   </html>\n   ```\n\n10. **Code Annotation and Comments**\n    - Add inline documentation to API handlers\n    - Use framework-specific annotation tools:\n      - **Java**: @ApiOperation, @ApiParam (Swagger annotations)\n      - **Python**: Docstrings with FastAPI or Flask-RESTX\n      - **Node.js**: JSDoc comments with swagger-jsdoc\n      - **C#**: XML documentation comments\n\n11. **Automated Documentation Generation**\n    \n    **For Node.js/Express:**\n    ```javascript\n    const swaggerJsdoc = require('swagger-jsdoc');\n    const swaggerUi = require('swagger-ui-express');\n    \n    const options = {\n      definition: {\n        openapi: '3.0.0',\n        info: {\n          title: 'API Documentation',\n          version: '1.0.0',\n        },\n      },\n      apis: ['./routes/*.js'],\n    };\n    \n    const specs = swaggerJsdoc(options);\n    app.use('/api-docs', swaggerUi.serve, swaggerUi.setup(specs));\n    ```\n\n12. **Testing Integration**\n    - Generate API test collections from documentation\n    - Include test scripts and validation rules\n    - Set up automated API testing\n    - Document test scenarios and expected outcomes\n\n13. **Version Management**\n    - Document API versioning strategy\n    - Maintain documentation for multiple API versions\n    - Document deprecation timelines and migration guides\n    - Track breaking changes between versions\n\n14. **Performance Documentation**\n    - Document rate limits and throttling policies\n    - Include performance benchmarks and SLAs\n    - Document caching strategies and headers\n    - Explain pagination and filtering options\n\n15. **SDK and Client Library Documentation**\n    - Generate client libraries from API specifications\n    - Document SDK usage and examples\n    - Provide quickstart guides for different languages\n    - Include integration examples and best practices\n\n16. **Environment-Specific Documentation**\n    - Document different environments (dev, staging, prod)\n    - Include environment-specific endpoints and configurations\n    - Document deployment and configuration requirements\n    - Provide environment setup instructions\n\n17. **Security Documentation**\n    - Document security best practices\n    - Include CORS and CSP policies\n    - Document input validation and sanitization\n    - Explain security headers and their purposes\n\n18. **Maintenance and Updates**\n    - Set up automated documentation updates\n    - Create processes for keeping documentation current\n    - Review and validate documentation regularly\n    - Integrate documentation reviews into development workflow\n\n**Framework-Specific Examples:**\n\n**FastAPI (Python):**\n```python\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\n\napp = FastAPI(title=\"My API\", version=\"1.0.0\")\n\nclass User(BaseModel):\n    id: int\n    name: str\n    email: str\n\n@app.get(\"/users/{user_id}\", response_model=User)\nasync def get_user(user_id: int):\n    \"\"\"Get a user by ID.\"\"\"\n    return {\"id\": user_id, \"name\": \"John\", \"email\": \"john@example.com\"}\n```\n\n**Spring Boot (Java):**\n```java\n@RestController\n@Api(tags = \"Users\")\npublic class UserController {\n    \n    @GetMapping(\"/users/{id}\")\n    @ApiOperation(value = \"Get user by ID\")\n    public ResponseEntity<User> getUser(\n        @PathVariable @ApiParam(\"User ID\") Long id) {\n        // Implementation\n    }\n}\n```\n\nRemember to keep documentation up-to-date with code changes and make it easily accessible to both internal teams and external consumers."
              },
              {
                "name": "/generate-api-documentation",
                "description": "Auto-generate API reference documentation",
                "path": "plugins/commands-api-development/commands/generate-api-documentation.md",
                "frontmatter": {
                  "description": "Auto-generate API reference documentation",
                  "category": "api-development"
                },
                "content": "# Generate API Documentation\n\nAuto-generate API reference documentation\n\n## Instructions\n\n1. **API Documentation Strategy Analysis**\n   - Analyze current API structure and endpoints\n   - Identify documentation requirements (REST, GraphQL, gRPC, etc.)\n   - Assess existing code annotations and documentation\n   - Determine documentation output formats and hosting requirements\n   - Plan documentation automation and maintenance strategy\n\n2. **Documentation Tool Selection**\n   - Choose appropriate API documentation tools:\n     - **OpenAPI/Swagger**: REST API documentation with Swagger UI\n     - **Redoc**: Modern OpenAPI documentation renderer\n     - **GraphQL**: GraphiQL, Apollo Studio, GraphQL Playground\n     - **Postman**: API documentation with collections\n     - **Insomnia**: API documentation and testing\n     - **API Blueprint**: Markdown-based API documentation\n     - **JSDoc/TSDoc**: Code-first documentation generation\n   - Consider factors: API type, team workflow, hosting, interactivity\n\n3. **Code Annotation and Schema Definition**\n   - Add comprehensive code annotations for API endpoints\n   - Define request/response schemas and data models\n   - Add parameter descriptions and validation rules\n   - Document authentication and authorization requirements\n   - Add example requests and responses\n\n4. **API Specification Generation**\n   - Set up automated API specification generation from code\n   - Configure OpenAPI/Swagger specification generation\n   - Set up schema validation and consistency checking\n   - Configure API versioning and changelog generation\n   - Set up specification file management and version control\n\n5. **Interactive Documentation Setup**\n   - Configure interactive API documentation with try-it-out functionality\n   - Set up API testing and example execution\n   - Configure authentication handling in documentation\n   - Set up request/response validation and examples\n   - Configure API endpoint categorization and organization\n\n6. **Documentation Content Enhancement**\n   - Add comprehensive API guides and tutorials\n   - Create authentication and authorization documentation\n   - Add error handling and status code documentation\n   - Create SDK and client library documentation\n   - Add rate limiting and usage guidelines\n\n7. **Documentation Hosting and Deployment**\n   - Set up documentation hosting and deployment\n   - Configure documentation website generation and styling\n   - Set up custom domain and SSL configuration\n   - Configure documentation search and navigation\n   - Set up documentation analytics and usage tracking\n\n8. **Automation and CI/CD Integration**\n   - Configure automated documentation generation in CI/CD pipeline\n   - Set up documentation deployment automation\n   - Configure documentation validation and quality checks\n   - Set up documentation change detection and notifications\n   - Configure documentation testing and link validation\n\n9. **Multi-format Documentation Generation**\n   - Generate documentation in multiple formats (HTML, PDF, Markdown)\n   - Set up downloadable documentation packages\n   - Configure offline documentation access\n   - Set up documentation API for programmatic access\n   - Configure documentation syndication and distribution\n\n10. **Maintenance and Quality Assurance**\n    - Set up documentation quality monitoring and validation\n    - Configure documentation feedback and improvement workflows\n    - Set up documentation analytics and usage metrics\n    - Create documentation maintenance procedures and guidelines\n    - Train team on documentation best practices and tools\n    - Set up documentation review and approval processes"
              },
              {
                "name": "/implement-graphql-api",
                "description": "Implement GraphQL API endpoints",
                "path": "plugins/commands-api-development/commands/implement-graphql-api.md",
                "frontmatter": {
                  "description": "Implement GraphQL API endpoints",
                  "category": "api-development"
                },
                "content": "# Implement GraphQL API\n\nImplement GraphQL API endpoints\n\n## Instructions\n\n1. **GraphQL Setup and Configuration**\n   - Set up GraphQL server with Apollo Server or similar\n   - Configure schema-first or code-first approach\n   - Plan GraphQL architecture and data modeling\n   - Set up development tools and introspection\n   - Configure GraphQL playground and documentation\n\n2. **Schema Definition and Type System**\n   - Define comprehensive GraphQL schema:\n\n   **Schema Definition (SDL):**\n   ```graphql\n   # schema/schema.graphql\n   \n   # Scalar types\n   scalar DateTime\n   scalar EmailAddress\n   scalar PhoneNumber\n   scalar JSON\n   scalar Upload\n\n   # User types and enums\n   enum UserRole {\n     USER\n     ADMIN\n     MANAGER\n   }\n\n   enum UserStatus {\n     ACTIVE\n     INACTIVE\n     SUSPENDED\n     PENDING_VERIFICATION\n   }\n\n   type User {\n     id: ID!\n     email: EmailAddress!\n     username: String!\n     firstName: String!\n     lastName: String!\n     fullName: String!\n     phone: PhoneNumber\n     dateOfBirth: DateTime\n     avatar: String\n     role: UserRole!\n     status: UserStatus!\n     emailVerified: Boolean!\n     phoneVerified: Boolean!\n     profile: UserProfile\n     orders(\n       first: Int = 10\n       after: String\n       status: OrderStatus\n     ): OrderConnection!\n     createdAt: DateTime!\n     updatedAt: DateTime!\n     lastLoginAt: DateTime\n   }\n\n   type UserProfile {\n     bio: String\n     website: String\n     location: String\n     timezone: String!\n     language: String!\n     notificationPreferences: JSON!\n     privacySettings: JSON!\n   }\n\n   # Product types\n   enum ProductStatus {\n     DRAFT\n     ACTIVE\n     INACTIVE\n     ARCHIVED\n   }\n\n   enum ProductVisibility {\n     VISIBLE\n     HIDDEN\n     CATALOG_ONLY\n     SEARCH_ONLY\n   }\n\n   type Product {\n     id: ID!\n     name: String!\n     slug: String!\n     sku: String!\n     description: String\n     shortDescription: String\n     price: Float!\n     comparePrice: Float\n     costPrice: Float\n     weight: Float\n     dimensions: ProductDimensions\n     category: Category\n     brand: Brand\n     vendor: Vendor\n     status: ProductStatus!\n     visibility: ProductVisibility!\n     inventoryTracking: Boolean!\n     inventoryQuantity: Int\n     lowStockThreshold: Int\n     allowBackorder: Boolean!\n     requiresShipping: Boolean!\n     isDigital: Boolean!\n     featured: Boolean!\n     tags: [String!]!\n     attributes: JSON!\n     images: [ProductImage!]!\n     variants: [ProductVariant!]!\n     reviews(\n       first: Int = 10\n       after: String\n       rating: Int\n     ): ReviewConnection!\n     averageRating: Float\n     reviewCount: Int!\n     createdAt: DateTime!\n     updatedAt: DateTime!\n     publishedAt: DateTime\n   }\n\n   type ProductDimensions {\n     length: Float\n     width: Float\n     height: Float\n     unit: String!\n   }\n\n   type ProductImage {\n     id: ID!\n     url: String!\n     altText: String\n     sortOrder: Int!\n   }\n\n   type ProductVariant {\n     id: ID!\n     sku: String!\n     price: Float!\n     comparePrice: Float\n     inventoryQuantity: Int\n     attributes: JSON!\n     image: ProductImage\n   }\n\n   # Order types\n   enum OrderStatus {\n     PENDING\n     PROCESSING\n     SHIPPED\n     DELIVERED\n     CANCELLED\n     REFUNDED\n     ON_HOLD\n   }\n\n   type Order {\n     id: ID!\n     orderNumber: String!\n     user: User\n     status: OrderStatus!\n     currency: String!\n     subtotal: Float!\n     taxTotal: Float!\n     shippingTotal: Float!\n     discountTotal: Float!\n     total: Float!\n     billingAddress: Address!\n     shippingAddress: Address!\n     shippingMethod: String\n     trackingNumber: String\n     items: [OrderItem!]!\n     notes: String\n     createdAt: DateTime!\n     updatedAt: DateTime!\n     shippedAt: DateTime\n     deliveredAt: DateTime\n   }\n\n   type OrderItem {\n     id: ID!\n     product: Product!\n     productVariant: ProductVariant\n     quantity: Int!\n     unitPrice: Float!\n     totalPrice: Float!\n     productName: String!\n     productSku: String!\n     productAttributes: JSON\n   }\n\n   type Address {\n     firstName: String!\n     lastName: String!\n     company: String\n     addressLine1: String!\n     addressLine2: String\n     city: String!\n     state: String\n     postalCode: String!\n     country: String!\n     phone: PhoneNumber\n   }\n\n   # Connection types for pagination\n   type UserConnection {\n     edges: [UserEdge!]!\n     pageInfo: PageInfo!\n     totalCount: Int!\n   }\n\n   type UserEdge {\n     node: User!\n     cursor: String!\n   }\n\n   type ProductConnection {\n     edges: [ProductEdge!]!\n     pageInfo: PageInfo!\n     totalCount: Int!\n   }\n\n   type ProductEdge {\n     node: Product!\n     cursor: String!\n   }\n\n   type OrderConnection {\n     edges: [OrderEdge!]!\n     pageInfo: PageInfo!\n     totalCount: Int!\n   }\n\n   type OrderEdge {\n     node: Order!\n     cursor: String!\n   }\n\n   type PageInfo {\n     hasNextPage: Boolean!\n     hasPreviousPage: Boolean!\n     startCursor: String\n     endCursor: String\n   }\n\n   # Input types\n   input CreateUserInput {\n     email: EmailAddress!\n     password: String!\n     firstName: String!\n     lastName: String!\n     phone: PhoneNumber\n     dateOfBirth: DateTime\n     role: UserRole = USER\n   }\n\n   input UpdateUserInput {\n     email: EmailAddress\n     firstName: String\n     lastName: String\n     phone: PhoneNumber\n     dateOfBirth: DateTime\n     status: UserStatus\n   }\n\n   input ProductFilters {\n     category: ID\n     brand: ID\n     priceMin: Float\n     priceMax: Float\n     status: ProductStatus\n     featured: Boolean\n     inStock: Boolean\n     tags: [String!]\n     search: String\n   }\n\n   input CreateProductInput {\n     name: String!\n     slug: String!\n     sku: String!\n     description: String\n     price: Float!\n     comparePrice: Float\n     categoryId: ID\n     brandId: ID\n     status: ProductStatus = DRAFT\n     inventoryQuantity: Int = 0\n     attributes: JSON\n     tags: [String!]\n   }\n\n   # Root types\n   type Query {\n     # User queries\n     me: User\n     user(id: ID!): User\n     users(\n       first: Int = 10\n       after: String\n       search: String\n       role: UserRole\n       status: UserStatus\n     ): UserConnection!\n\n     # Product queries\n     product(id: ID, slug: String): Product\n     products(\n       first: Int = 10\n       after: String\n       filters: ProductFilters\n       sortBy: ProductSortBy = CREATED_AT\n       sortOrder: SortOrder = DESC\n     ): ProductConnection!\n\n     # Order queries\n     order(id: ID!): Order\n     orders(\n       first: Int = 10\n       after: String\n       status: OrderStatus\n       userId: ID\n     ): OrderConnection!\n\n     # Search\n     search(\n       query: String!\n       first: Int = 10\n       after: String\n       types: [SearchType!] = [USER, PRODUCT, ORDER]\n     ): SearchConnection!\n   }\n\n   type Mutation {\n     # Auth mutations\n     login(email: EmailAddress!, password: String!): AuthPayload!\n     logout: Boolean!\n     refreshToken: AuthPayload!\n     forgotPassword(email: EmailAddress!): Boolean!\n     resetPassword(token: String!, password: String!): AuthPayload!\n\n     # User mutations\n     createUser(input: CreateUserInput!): User!\n     updateUser(id: ID!, input: UpdateUserInput!): User!\n     deleteUser(id: ID!): Boolean!\n     updateProfile(input: UpdateProfileInput!): UserProfile!\n\n     # Product mutations\n     createProduct(input: CreateProductInput!): Product!\n     updateProduct(id: ID!, input: UpdateProductInput!): Product!\n     deleteProduct(id: ID!): Boolean!\n     uploadProductImage(productId: ID!, file: Upload!): ProductImage!\n\n     # Order mutations\n     createOrder(input: CreateOrderInput!): Order!\n     updateOrderStatus(id: ID!, status: OrderStatus!): Order!\n     addOrderItem(orderId: ID!, input: AddOrderItemInput!): OrderItem!\n     removeOrderItem(id: ID!): Boolean!\n   }\n\n   type Subscription {\n     # Real-time updates\n     orderUpdated(userId: ID): Order!\n     productUpdated(productId: ID): Product!\n     userStatusChanged(userId: ID): User!\n     \n     # Admin subscriptions\n     newOrder: Order!\n     lowStockAlert: Product!\n   }\n\n   enum ProductSortBy {\n     CREATED_AT\n     NAME\n     PRICE\n     RATING\n     POPULARITY\n   }\n\n   enum SortOrder {\n     ASC\n     DESC\n   }\n\n   enum SearchType {\n     USER\n     PRODUCT\n     ORDER\n   }\n\n   type AuthPayload {\n     token: String!\n     refreshToken: String!\n     user: User!\n     expiresAt: DateTime!\n   }\n   ```\n\n3. **Resolver Implementation**\n   - Implement comprehensive resolvers:\n\n   **Main Resolvers:**\n   ```javascript\n   // resolvers/index.js\n   const { GraphQLDateTime } = require('graphql-iso-date');\n   const { GraphQLEmailAddress, GraphQLPhoneNumber } = require('graphql-scalars');\n   const GraphQLJSON = require('graphql-type-json');\n   const GraphQLUpload = require('graphql-upload/GraphQLUpload.js');\n\n   const userResolvers = require('./userResolvers');\n   const productResolvers = require('./productResolvers');\n   const orderResolvers = require('./orderResolvers');\n   const searchResolvers = require('./searchResolvers');\n\n   const resolvers = {\n     // Custom scalars\n     DateTime: GraphQLDateTime,\n     EmailAddress: GraphQLEmailAddress,\n     PhoneNumber: GraphQLPhoneNumber,\n     JSON: GraphQLJSON,\n     Upload: GraphQLUpload,\n\n     // Root resolvers\n     Query: {\n       ...userResolvers.Query,\n       ...productResolvers.Query,\n       ...orderResolvers.Query,\n       ...searchResolvers.Query\n     },\n\n     Mutation: {\n       ...userResolvers.Mutation,\n       ...productResolvers.Mutation,\n       ...orderResolvers.Mutation\n     },\n\n     Subscription: {\n       ...userResolvers.Subscription,\n       ...productResolvers.Subscription,\n       ...orderResolvers.Subscription\n     },\n\n     // Type resolvers\n     User: userResolvers.User,\n     Product: productResolvers.Product,\n     Order: orderResolvers.Order\n   };\n\n   module.exports = resolvers;\n   ```\n\n   **User Resolvers:**\n   ```javascript\n   // resolvers/userResolvers.js\n   const { AuthenticationError, ForbiddenError, UserInputError } = require('apollo-server-express');\n   const { withFilter } = require('graphql-subscriptions');\n   const userService = require('../services/userService');\n   const { requireAuth, requireRole } = require('../utils/authHelpers');\n   const { createConnectionFromArray } = require('../utils/connectionHelpers');\n\n   const userResolvers = {\n     Query: {\n       async me(parent, args, context) {\n         requireAuth(context);\n         return await userService.findById(context.user.id);\n       },\n\n       async user(parent, { id }, context) {\n         requireAuth(context);\n         \n         const user = await userService.findById(id);\n         if (!user) {\n           throw new UserInputError('User not found');\n         }\n\n         // Privacy check - users can only see their own data unless admin\n         if (context.user.id !== user.id && !['admin', 'manager'].includes(context.user.role)) {\n           throw new ForbiddenError('Insufficient permissions');\n         }\n\n         return user;\n       },\n\n       async users(parent, { first, after, search, role, status }, context) {\n         requireAuth(context);\n         requireRole(context, ['admin', 'manager']);\n\n         const result = await userService.findUsers({\n           first,\n           after,\n           search,\n           role,\n           status\n         });\n\n         return createConnectionFromArray(result.users, {\n           first,\n           after,\n           totalCount: result.totalCount\n         });\n       }\n     },\n\n     Mutation: {\n       async createUser(parent, { input }, context) {\n         requireAuth(context);\n         requireRole(context, ['admin']);\n\n         // Check for existing user\n         const existingUser = await userService.findByEmail(input.email);\n         if (existingUser) {\n           throw new UserInputError('User with this email already exists');\n         }\n\n         const user = await userService.createUser(input);\n         \n         // Publish subscription for real-time updates\n         context.pubsub.publish('USER_CREATED', { userCreated: user });\n         \n         return user;\n       },\n\n       async updateUser(parent, { id, input }, context) {\n         requireAuth(context);\n         \n         const existingUser = await userService.findById(id);\n         if (!existingUser) {\n           throw new UserInputError('User not found');\n         }\n\n         // Authorization check\n         if (context.user.id !== id && !['admin', 'manager'].includes(context.user.role)) {\n           throw new ForbiddenError('Insufficient permissions');\n         }\n\n         // Role change restriction\n         if (input.role && !['admin'].includes(context.user.role)) {\n           throw new ForbiddenError('Insufficient permissions to change user role');\n         }\n\n         const updatedUser = await userService.updateUser(id, input);\n         \n         // Publish subscription\n         context.pubsub.publish('USER_UPDATED', { userUpdated: updatedUser });\n         \n         return updatedUser;\n       },\n\n       async deleteUser(parent, { id }, context) {\n         requireAuth(context);\n         requireRole(context, ['admin']);\n\n         // Prevent self-deletion\n         if (context.user.id === id) {\n           throw new UserInputError('Cannot delete your own account');\n         }\n\n         const existingUser = await userService.findById(id);\n         if (!existingUser) {\n           throw new UserInputError('User not found');\n         }\n\n         await userService.deleteUser(id);\n         \n         // Publish subscription\n         context.pubsub.publish('USER_DELETED', { userDeleted: existingUser });\n         \n         return true;\n       }\n     },\n\n     Subscription: {\n       userStatusChanged: {\n         subscribe: withFilter(\n           (parent, args, context) => {\n             requireAuth(context);\n             return context.pubsub.asyncIterator(['USER_UPDATED']);\n           },\n           (payload, variables) => {\n             // Filter by userId if provided\n             return !variables.userId || payload.userUpdated.id === variables.userId;\n           }\n         )\n       }\n     },\n\n     // Field resolvers\n     User: {\n       fullName(parent) {\n         return `${parent.firstName} ${parent.lastName}`;\n       },\n\n       async profile(parent, args, context) {\n         return await userService.getUserProfile(parent.id);\n       },\n\n       async orders(parent, { first, after, status }, context) {\n         requireAuth(context);\n         \n         // Users can only see their own orders unless admin\n         if (context.user.id !== parent.id && !['admin', 'manager'].includes(context.user.role)) {\n           throw new ForbiddenError('Insufficient permissions');\n         }\n\n         const result = await userService.getUserOrders(parent.id, {\n           first,\n           after,\n           status\n         });\n\n         return createConnectionFromArray(result.orders, {\n           first,\n           after,\n           totalCount: result.totalCount\n         });\n       }\n     }\n   };\n\n   module.exports = userResolvers;\n   ```\n\n4. **DataLoader for N+1 Problem**\n   - Implement efficient data loading:\n\n   **DataLoader Implementation:**\n   ```javascript\n   // dataLoaders/index.js\n   const DataLoader = require('dataloader');\n   const userService = require('../services/userService');\n   const productService = require('../services/productService');\n   const orderService = require('../services/orderService');\n\n   class DataLoaders {\n     constructor() {\n       this.userLoader = new DataLoader(\n         async (userIds) => {\n           const users = await userService.findByIds(userIds);\n           return userIds.map(id => users.find(user => user.id === id) || null);\n         },\n         {\n           cacheKeyFn: (key) => key.toString(),\n           maxBatchSize: 100\n         }\n       );\n\n       this.userProfileLoader = new DataLoader(\n         async (userIds) => {\n           const profiles = await userService.getProfilesByUserIds(userIds);\n           return userIds.map(id => profiles.find(profile => profile.userId === id) || null);\n         }\n       );\n\n       this.productLoader = new DataLoader(\n         async (productIds) => {\n           const products = await productService.findByIds(productIds);\n           return productIds.map(id => products.find(product => product.id === id) || null);\n         }\n       );\n\n       this.productCategoryLoader = new DataLoader(\n         async (categoryIds) => {\n           const categories = await productService.getCategoriesByIds(categoryIds);\n           return categoryIds.map(id => categories.find(category => category.id === id) || null);\n         }\n       );\n\n       this.productImagesLoader = new DataLoader(\n         async (productIds) => {\n           const imagesMap = await productService.getImagesByProductIds(productIds);\n           return productIds.map(id => imagesMap[id] || []);\n         }\n       );\n\n       this.orderItemsLoader = new DataLoader(\n         async (orderIds) => {\n           const itemsMap = await orderService.getItemsByOrderIds(orderIds);\n           return orderIds.map(id => itemsMap[id] || []);\n         }\n       );\n\n       this.productReviewsLoader = new DataLoader(\n         async (productIds) => {\n           const reviewsMap = await productService.getReviewsByProductIds(productIds);\n           return productIds.map(id => reviewsMap[id] || []);\n         }\n       );\n     }\n\n     // Clear all caches\n     clearAll() {\n       this.userLoader.clearAll();\n       this.userProfileLoader.clearAll();\n       this.productLoader.clearAll();\n       this.productCategoryLoader.clearAll();\n       this.productImagesLoader.clearAll();\n       this.orderItemsLoader.clearAll();\n       this.productReviewsLoader.clearAll();\n     }\n\n     // Clear specific cache\n     clearUser(userId) {\n       this.userLoader.clear(userId);\n       this.userProfileLoader.clear(userId);\n     }\n\n     clearProduct(productId) {\n       this.productLoader.clear(productId);\n       this.productImagesLoader.clear(productId);\n       this.productReviewsLoader.clear(productId);\n     }\n   }\n\n   module.exports = DataLoaders;\n   ```\n\n5. **Authentication and Authorization**\n   - Implement GraphQL-specific auth:\n\n   **Auth Helpers:**\n   ```javascript\n   // utils/authHelpers.js\n   const { AuthenticationError, ForbiddenError } = require('apollo-server-express');\n   const jwt = require('jsonwebtoken');\n   const userService = require('../services/userService');\n\n   class GraphQLAuth {\n     static async getUser(req) {\n       const authHeader = req.headers.authorization;\n       \n       if (!authHeader) {\n         return null;\n       }\n\n       const token = authHeader.replace('Bearer ', '');\n       \n       try {\n         const decoded = jwt.verify(token, process.env.JWT_SECRET);\n         const user = await userService.findById(decoded.userId);\n         \n         if (!user || user.status !== 'active') {\n           return null;\n         }\n\n         return user;\n       } catch (error) {\n         return null;\n       }\n     }\n\n     static requireAuth(context) {\n       if (!context.user) {\n         throw new AuthenticationError('Authentication required');\n       }\n       return context.user;\n     }\n\n     static requireRole(context, roles) {\n       this.requireAuth(context);\n       \n       if (!roles.includes(context.user.role)) {\n         throw new ForbiddenError(`Requires one of the following roles: ${roles.join(', ')}`);\n       }\n       \n       return context.user;\n     }\n\n     static requirePermission(context, permissions) {\n       this.requireAuth(context);\n       \n       const userPermissions = context.user.permissions || [];\n       const hasPermission = permissions.some(permission => \n         userPermissions.includes(permission)\n       );\n       \n       if (!hasPermission) {\n         throw new ForbiddenError(`Requires one of the following permissions: ${permissions.join(', ')}`);\n       }\n       \n       return context.user;\n     }\n\n     static canAccessResource(context, resourceUserId, adminRoles = ['admin', 'manager']) {\n       this.requireAuth(context);\n       \n       const isOwner = context.user.id === resourceUserId;\n       const isAdmin = adminRoles.includes(context.user.role);\n       \n       if (!isOwner && !isAdmin) {\n         throw new ForbiddenError('Insufficient permissions to access this resource');\n       }\n       \n       return context.user;\n     }\n   }\n\n   // Export individual functions for convenience\n   const { requireAuth, requireRole, requirePermission, canAccessResource } = GraphQLAuth;\n\n   module.exports = {\n     GraphQLAuth,\n     requireAuth,\n     requireRole,\n     requirePermission,\n     canAccessResource\n   };\n   ```\n\n6. **Real-time Subscriptions**\n   - Implement GraphQL subscriptions:\n\n   **Subscription Setup:**\n   ```javascript\n   // subscriptions/index.js\n   const { PubSub } = require('graphql-subscriptions');\n   const { RedisPubSub } = require('graphql-redis-subscriptions');\n   const Redis = require('ioredis');\n\n   // Use Redis for production, in-memory for development\n   const createPubSub = () => {\n     if (process.env.NODE_ENV === 'production') {\n       const redisClient = new Redis(process.env.REDIS_URL);\n       return new RedisPubSub({\n         publisher: redisClient,\n         subscriber: redisClient.duplicate()\n       });\n     } else {\n       return new PubSub();\n     }\n   };\n\n   const pubsub = createPubSub();\n\n   // Subscription events\n   const SUBSCRIPTION_EVENTS = {\n     USER_CREATED: 'USER_CREATED',\n     USER_UPDATED: 'USER_UPDATED',\n     USER_DELETED: 'USER_DELETED',\n     ORDER_CREATED: 'ORDER_CREATED',\n     ORDER_UPDATED: 'ORDER_UPDATED',\n     PRODUCT_UPDATED: 'PRODUCT_UPDATED',\n     LOW_STOCK_ALERT: 'LOW_STOCK_ALERT'\n   };\n\n   // Subscription resolvers\n   const subscriptionResolvers = {\n     orderUpdated: {\n       subscribe: (parent, { userId }, context) => {\n         requireAuth(context);\n         \n         // Users can only subscribe to their own orders unless admin\n         if (userId && context.user.id !== userId && !['admin', 'manager'].includes(context.user.role)) {\n           throw new ForbiddenError('Insufficient permissions');\n         }\n         \n         return pubsub.asyncIterator([SUBSCRIPTION_EVENTS.ORDER_UPDATED]);\n       },\n       resolve: (payload, { userId }) => {\n         // Filter by userId if provided\n         if (userId && payload.orderUpdated.userId !== userId) {\n           return null;\n         }\n         return payload.orderUpdated;\n       }\n     },\n\n     productUpdated: {\n       subscribe: (parent, { productId }, context) => {\n         return pubsub.asyncIterator([SUBSCRIPTION_EVENTS.PRODUCT_UPDATED]);\n       },\n       resolve: (payload, { productId }) => {\n         // Filter by productId if provided\n         if (productId && payload.productUpdated.id !== productId) {\n           return null;\n         }\n         return payload.productUpdated;\n       }\n     },\n\n     userStatusChanged: {\n       subscribe: (parent, { userId }, context) => {\n         requireAuth(context);\n         requireRole(context, ['admin', 'manager']);\n         \n         return pubsub.asyncIterator([SUBSCRIPTION_EVENTS.USER_UPDATED]);\n       },\n       resolve: (payload, { userId }) => {\n         if (userId && payload.userUpdated.id !== userId) {\n           return null;\n         }\n         return payload.userUpdated;\n       }\n     },\n\n     newOrder: {\n       subscribe: (parent, args, context) => {\n         requireAuth(context);\n         requireRole(context, ['admin', 'manager']);\n         \n         return pubsub.asyncIterator([SUBSCRIPTION_EVENTS.ORDER_CREATED]);\n       }\n     },\n\n     lowStockAlert: {\n       subscribe: (parent, args, context) => {\n         requireAuth(context);\n         requireRole(context, ['admin', 'manager']);\n         \n         return pubsub.asyncIterator([SUBSCRIPTION_EVENTS.LOW_STOCK_ALERT]);\n       }\n     }\n   };\n\n   module.exports = {\n     pubsub,\n     SUBSCRIPTION_EVENTS,\n     subscriptionResolvers\n   };\n   ```\n\n7. **Error Handling and Validation**\n   - Implement comprehensive error handling:\n\n   **Error Handling:**\n   ```javascript\n   // utils/errorHandling.js\n   const { \n     ApolloError, \n     AuthenticationError, \n     ForbiddenError, \n     UserInputError \n   } = require('apollo-server-express');\n\n   class GraphQLErrorHandler {\n     static handleError(error, operation) {\n       // Log error for debugging\n       console.error('GraphQL Error:', {\n         message: error.message,\n         operation: operation?.operationName,\n         variables: operation?.variables,\n         stack: error.stack\n       });\n\n       // Database errors\n       if (error.code === '23505') { // Unique constraint violation\n         return new UserInputError('A record with this information already exists');\n       }\n       \n       if (error.code === '23503') { // Foreign key constraint violation\n         return new UserInputError('Referenced record does not exist');\n       }\n\n       // Validation errors\n       if (error.name === 'ValidationError') {\n         const messages = Object.values(error.errors).map(err => err.message);\n         return new UserInputError('Validation failed', {\n           validationErrors: messages\n         });\n       }\n\n       // Permission errors\n       if (error.message.includes('permission') || error.message.includes('access')) {\n         return new ForbiddenError(error.message);\n       }\n\n       // Authentication errors\n       if (error.message.includes('token') || error.message.includes('auth')) {\n         return new AuthenticationError(error.message);\n       }\n\n       // Network/external service errors\n       if (error.code === 'ENOTFOUND' || error.code === 'ECONNREFUSED') {\n         return new ApolloError('External service unavailable', 'SERVICE_UNAVAILABLE');\n       }\n\n       // Default to internal error\n       return new ApolloError(\n         'An unexpected error occurred',\n         'INTERNAL_ERROR',\n         { originalError: error.message }\n       );\n     }\n\n     static formatError(error) {\n       // Don't expose internal errors in production\n       if (process.env.NODE_ENV === 'production' && !error.extensions?.code) {\n         return new ApolloError('Internal server error', 'INTERNAL_ERROR');\n       }\n\n       // Add request ID for tracking\n       if (error.extensions?.requestId) {\n         error.extensions.requestId = error.extensions.requestId;\n       }\n\n       return error;\n     }\n   }\n\n   // Input validation helper\n   class InputValidator {\n     static validateEmail(email) {\n       const emailRegex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n       if (!emailRegex.test(email)) {\n         throw new UserInputError('Invalid email format');\n       }\n     }\n\n     static validatePassword(password) {\n       if (password.length < 8) {\n         throw new UserInputError('Password must be at least 8 characters long');\n       }\n       \n       if (!/(?=.*[a-z])(?=.*[A-Z])(?=.*\\d)/.test(password)) {\n         throw new UserInputError('Password must contain uppercase, lowercase, and numeric characters');\n       }\n     }\n\n     static validatePhoneNumber(phone) {\n       const phoneRegex = /^\\+?[\\d\\s\\-\\(\\)]{10,20}$/;\n       if (!phoneRegex.test(phone)) {\n         throw new UserInputError('Invalid phone number format');\n       }\n     }\n\n     static validateRequired(value, fieldName) {\n       if (!value || (typeof value === 'string' && !value.trim())) {\n         throw new UserInputError(`${fieldName} is required`);\n       }\n     }\n\n     static validateStringLength(value, fieldName, min = 0, max = 255) {\n       if (typeof value !== 'string') {\n         throw new UserInputError(`${fieldName} must be a string`);\n       }\n       \n       if (value.length < min) {\n         throw new UserInputError(`${fieldName} must be at least ${min} characters`);\n       }\n       \n       if (value.length > max) {\n         throw new UserInputError(`${fieldName} must not exceed ${max} characters`);\n       }\n     }\n\n     static validateNumericRange(value, fieldName, min, max) {\n       if (typeof value !== 'number' || isNaN(value)) {\n         throw new UserInputError(`${fieldName} must be a valid number`);\n       }\n       \n       if (min !== undefined && value < min) {\n         throw new UserInputError(`${fieldName} must be at least ${min}`);\n       }\n       \n       if (max !== undefined && value > max) {\n         throw new UserInputError(`${fieldName} must not exceed ${max}`);\n       }\n     }\n   }\n\n   module.exports = {\n     GraphQLErrorHandler,\n     InputValidator\n   };\n   ```\n\n8. **Performance Optimization**\n   - Implement GraphQL performance optimizations:\n\n   **Query Complexity and Depth Limiting:**\n   ```javascript\n   // utils/queryLimiting.js\n   const depthLimit = require('graphql-depth-limit');\n   const costAnalysis = require('graphql-query-complexity');\n\n   class QueryLimiting {\n     static createDepthLimit(maxDepth = 10) {\n       return depthLimit(maxDepth, {\n         ignoreIntrospection: true\n       });\n     }\n\n     static createComplexityAnalysis(maxComplexity = 1000) {\n       return costAnalysis({\n         maximumComplexity: maxComplexity,\n         introspection: true,\n         scalarCost: 1,\n         objectCost: 1,\n         listFactor: 10,\n         fieldExtensions: {\n           complexity: (options) => {\n             // Custom complexity calculation\n             const { args, childComplexity } = options;\n             \n             // List fields have higher complexity\n             if (args.first) {\n               return childComplexity * Math.min(args.first, 100);\n             }\n             \n             return childComplexity;\n           }\n         },\n         createError: (max, actual) => {\n           return new Error(`Query complexity ${actual} exceeds maximum allowed complexity ${max}`);\n         }\n       });\n     }\n\n     static createQueryTimeout(timeout = 30000) {\n       return {\n         willSendResponse(requestContext) {\n           if (requestContext.request.query) {\n             setTimeout(() => {\n               if (!requestContext.response.http.body) {\n                 throw new Error('Query timeout exceeded');\n               }\n             }, timeout);\n           }\n         }\n       };\n     }\n   }\n\n   // Query caching\n   class QueryCache {\n     constructor(ttl = 300) { // 5 minutes default\n       this.cache = new Map();\n       this.ttl = ttl * 1000; // Convert to milliseconds\n     }\n\n     get(query, variables) {\n       const key = this.generateKey(query, variables);\n       const cached = this.cache.get(key);\n       \n       if (cached && Date.now() - cached.timestamp < this.ttl) {\n         return cached.result;\n       }\n       \n       this.cache.delete(key);\n       return null;\n     }\n\n     set(query, variables, result) {\n       const key = this.generateKey(query, variables);\n       this.cache.set(key, {\n         result,\n         timestamp: Date.now()\n       });\n     }\n\n     generateKey(query, variables) {\n       return `${query}:${JSON.stringify(variables || {})}`;\n     }\n\n     clear() {\n       this.cache.clear();\n     }\n\n     // Middleware for Apollo Server\n     static createCachePlugin(ttl = 300) {\n       const cache = new QueryCache(ttl);\n       \n       return {\n         requestDidStart() {\n           return {\n             willSendResponse(requestContext) {\n               const { request, response } = requestContext;\n               \n               // Only cache successful queries\n               if (response.http.body && !response.errors) {\n                 cache.set(request.query, request.variables, response.http.body);\n               }\n             },\n             \n             willSendRequest(requestContext) {\n               const { request } = requestContext;\n               const cached = cache.get(request.query, request.variables);\n               \n               if (cached) {\n                 requestContext.response.http.body = cached;\n                 return;\n               }\n             }\n           };\n         }\n       };\n     }\n   }\n\n   module.exports = {\n     QueryLimiting,\n     QueryCache\n   };\n   ```\n\n9. **GraphQL Testing**\n   - Implement comprehensive GraphQL testing:\n\n   **GraphQL Test Suite:**\n   ```javascript\n   // tests/graphql/users.test.js\n   const { createTestClient } = require('apollo-server-testing');\n   const { gql } = require('apollo-server-express');\n   const { createTestServer } = require('../helpers/testServer');\n   const { createTestUser, getAuthToken } = require('../helpers/testHelpers');\n\n   describe('User GraphQL API', () => {\n     let server, query, mutate;\n     let testUser, authToken;\n\n     beforeAll(async () => {\n       server = await createTestServer();\n       const testClient = createTestClient(server);\n       query = testClient.query;\n       mutate = testClient.mutate;\n\n       testUser = await createTestUser({ role: 'admin' });\n       authToken = await getAuthToken(testUser);\n     });\n\n     describe('Queries', () => {\n       const GET_USERS = gql`\n         query GetUsers($first: Int, $search: String) {\n           users(first: $first, search: $search) {\n             edges {\n               node {\n                 id\n                 email\n                 firstName\n                 lastName\n                 role\n                 status\n                 createdAt\n               }\n             }\n             pageInfo {\n               hasNextPage\n               hasPreviousPage\n               startCursor\n               endCursor\n             }\n             totalCount\n           }\n         }\n       `;\n\n       test('should return paginated users list', async () => {\n         const result = await query({\n           query: GET_USERS,\n           variables: { first: 10 },\n           context: { user: testUser }\n         });\n\n         expect(result.errors).toBeUndefined();\n         expect(result.data.users).toMatchObject({\n           edges: expect.any(Array),\n           pageInfo: {\n             hasNextPage: expect.any(Boolean),\n             hasPreviousPage: expect.any(Boolean)\n           },\n           totalCount: expect.any(Number)\n         });\n\n         if (result.data.users.edges.length > 0) {\n           expect(result.data.users.edges[0].node).toHaveProperty('id');\n           expect(result.data.users.edges[0].node).toHaveProperty('email');\n           expect(result.data.users.edges[0].node).not.toHaveProperty('password');\n         }\n       });\n\n       test('should filter users by search term', async () => {\n         const result = await query({\n           query: GET_USERS,\n           variables: { search: 'test' },\n           context: { user: testUser }\n         });\n\n         expect(result.errors).toBeUndefined();\n         expect(result.data.users.edges).toEqual(\n           expect.arrayContaining([\n             expect.objectContaining({\n               node: expect.objectContaining({\n                 email: expect.stringContaining('test')\n               })\n             })\n           ])\n         );\n       });\n\n       test('should require authentication', async () => {\n         const result = await query({\n           query: GET_USERS,\n           variables: { first: 10 }\n         });\n\n         expect(result.errors).toBeDefined();\n         expect(result.errors[0].extensions.code).toBe('UNAUTHENTICATED');\n       });\n\n       const GET_ME = gql`\n         query GetMe {\n           me {\n             id\n             email\n             firstName\n             lastName\n             profile {\n               bio\n               website\n             }\n           }\n         }\n       `;\n\n       test('should return current user profile', async () => {\n         const result = await query({\n           query: GET_ME,\n           context: { user: testUser }\n         });\n\n         expect(result.errors).toBeUndefined();\n         expect(result.data.me).toMatchObject({\n           id: testUser.id.toString(),\n           email: testUser.email,\n           firstName: testUser.firstName,\n           lastName: testUser.lastName\n         });\n       });\n     });\n\n     describe('Mutations', () => {\n       const CREATE_USER = gql`\n         mutation CreateUser($input: CreateUserInput!) {\n           createUser(input: $input) {\n             id\n             email\n             firstName\n             lastName\n             role\n             status\n           }\n         }\n       `;\n\n       test('should create user with valid input', async () => {\n         const userInput = {\n           email: 'newuser@example.com',\n           password: 'SecurePass123',\n           firstName: 'New',\n           lastName: 'User',\n           role: 'USER'\n         };\n\n         const result = await mutate({\n           mutation: CREATE_USER,\n           variables: { input: userInput },\n           context: { user: testUser }\n         });\n\n         expect(result.errors).toBeUndefined();\n         expect(result.data.createUser).toMatchObject({\n           email: userInput.email,\n           firstName: userInput.firstName,\n           lastName: userInput.lastName,\n           role: userInput.role,\n           status: 'ACTIVE'\n         });\n         expect(result.data.createUser).toHaveProperty('id');\n       });\n\n       test('should validate email format', async () => {\n         const userInput = {\n           email: 'invalid-email',\n           password: 'SecurePass123',\n           firstName: 'Test',\n           lastName: 'User'\n         };\n\n         const result = await mutate({\n           mutation: CREATE_USER,\n           variables: { input: userInput },\n           context: { user: testUser }\n         });\n\n         expect(result.errors).toBeDefined();\n         expect(result.errors[0].extensions.code).toBe('BAD_USER_INPUT');\n       });\n\n       test('should prevent duplicate email', async () => {\n         const userInput = {\n           email: testUser.email,\n           password: 'SecurePass123',\n           firstName: 'Test',\n           lastName: 'User'\n         };\n\n         const result = await mutate({\n           mutation: CREATE_USER,\n           variables: { input: userInput },\n           context: { user: testUser }\n         });\n\n         expect(result.errors).toBeDefined();\n         expect(result.errors[0].message).toContain('already exists');\n       });\n     });\n\n     describe('Subscriptions', () => {\n       test('should subscribe to user status changes', (done) => {\n         const USER_STATUS_CHANGED = gql`\n           subscription UserStatusChanged($userId: ID) {\n             userStatusChanged(userId: $userId) {\n               id\n               status\n             }\n           }\n         `;\n\n         const observable = server.subscription({\n           query: USER_STATUS_CHANGED,\n           variables: { userId: testUser.id },\n           context: { user: testUser }\n         });\n\n         observable.subscribe({\n           next: (result) => {\n             expect(result.data.userStatusChanged).toMatchObject({\n               id: testUser.id.toString(),\n               status: expect.any(String)\n             });\n             done();\n           },\n           error: done\n         });\n\n         // Trigger the subscription by updating user status\n         setTimeout(() => {\n           server.pubsub.publish('USER_UPDATED', {\n             userUpdated: { ...testUser, status: 'INACTIVE' }\n           });\n         }, 100);\n       });\n     });\n\n     describe('Performance', () => {\n       test('should handle complex queries efficiently', async () => {\n         const COMPLEX_QUERY = gql`\n           query ComplexQuery {\n             users(first: 5) {\n               edges {\n                 node {\n                   id\n                   email\n                   profile {\n                     bio\n                   }\n                   orders(first: 3) {\n                     edges {\n                       node {\n                         id\n                         total\n                         items {\n                           id\n                           product {\n                             id\n                             name\n                           }\n                         }\n                       }\n                     }\n                   }\n                 }\n               }\n             }\n           }\n         `;\n\n         const start = Date.now();\n         const result = await query({\n           query: COMPLEX_QUERY,\n           context: { user: testUser }\n         });\n         const duration = Date.now() - start;\n\n         expect(result.errors).toBeUndefined();\n         expect(duration).toBeLessThan(2000); // Should complete within 2 seconds\n       });\n\n       test('should limit query depth', async () => {\n         const DEEP_QUERY = gql`\n           query DeepQuery {\n             users {\n               edges {\n                 node {\n                   orders {\n                     edges {\n                       node {\n                         items {\n                           product {\n                             category {\n                               parent {\n                                 parent {\n                                   parent {\n                                     name\n                                   }\n                                 }\n                               }\n                             }\n                           }\n                         }\n                       }\n                     }\n                   }\n                 }\n               }\n             }\n           }\n         `;\n\n         const result = await query({\n           query: DEEP_QUERY,\n           context: { user: testUser }\n         });\n\n         expect(result.errors).toBeDefined();\n         expect(result.errors[0].message).toContain('depth');\n       });\n     });\n   });\n   ```\n\n10. **Production Setup and Deployment**\n    - Configure GraphQL for production:\n\n    **Production Configuration:**\n    ```javascript\n    // server/apollo.js\n    const { ApolloServer } = require('apollo-server-express');\n    const { makeExecutableSchema } = require('@graphql-tools/schema');\n    const { shield, rule, and, or } = require('graphql-shield');\n    const depthLimit = require('graphql-depth-limit');\n    const costAnalysis = require('graphql-query-complexity');\n\n    const typeDefs = require('../schema');\n    const resolvers = require('../resolvers');\n    const { GraphQLAuth } = require('../utils/authHelpers');\n    const { GraphQLErrorHandler } = require('../utils/errorHandling');\n    const { QueryLimiting, QueryCache } = require('../utils/queryLimiting');\n    const DataLoaders = require('../dataLoaders');\n    const { pubsub } = require('../subscriptions');\n\n    // Security rules\n    const rules = {\n      isAuthenticated: rule({ cache: 'contextual' })(\n        async (parent, args, context) => {\n          return !!context.user;\n        }\n      ),\n      isAdmin: rule({ cache: 'contextual' })(\n        async (parent, args, context) => {\n          return context.user && ['admin'].includes(context.user.role);\n        }\n      ),\n      isManagerOrAdmin: rule({ cache: 'contextual' })(\n        async (parent, args, context) => {\n          return context.user && ['admin', 'manager'].includes(context.user.role);\n        }\n      )\n    };\n\n    const permissions = shield({\n      Query: {\n        me: rules.isAuthenticated,\n        user: rules.isAuthenticated,\n        users: rules.isManagerOrAdmin,\n        orders: rules.isManagerOrAdmin\n      },\n      Mutation: {\n        createUser: rules.isAdmin,\n        updateUser: rules.isAuthenticated,\n        deleteUser: rules.isAdmin,\n        createProduct: rules.isManagerOrAdmin,\n        updateProduct: rules.isManagerOrAdmin,\n        deleteProduct: rules.isAdmin\n      },\n      Subscription: {\n        userStatusChanged: rules.isManagerOrAdmin,\n        newOrder: rules.isManagerOrAdmin,\n        lowStockAlert: rules.isManagerOrAdmin\n      }\n    }, {\n      allowExternalErrors: true,\n      fallbackError: 'Not authorized for this operation'\n    });\n\n    const createApolloServer = () => {\n      const schema = makeExecutableSchema({\n        typeDefs,\n        resolvers\n      });\n\n      return new ApolloServer({\n        schema: permissions(schema),\n        context: async ({ req, connection }) => {\n          // WebSocket connection (subscriptions)\n          if (connection) {\n            return {\n              user: connection.context.user,\n              dataLoaders: new DataLoaders(),\n              pubsub\n            };\n          }\n\n          // HTTP request\n          const user = await GraphQLAuth.getUser(req);\n          \n          return {\n            user,\n            dataLoaders: new DataLoaders(),\n            pubsub,\n            req\n          };\n        },\n        formatError: GraphQLErrorHandler.formatError,\n        validationRules: [\n          QueryLimiting.createDepthLimit(10),\n          QueryLimiting.createComplexityAnalysis(1000)\n        ],\n        plugins: [\n          QueryCache.createCachePlugin(300), // 5 minutes cache\n          {\n            requestDidStart() {\n              return {\n                willSendResponse(requestContext) {\n                  // Clear DataLoaders after each request\n                  if (requestContext.context.dataLoaders) {\n                    requestContext.context.dataLoaders.clearAll();\n                  }\n                }\n              };\n            }\n          }\n        ],\n        introspection: process.env.NODE_ENV !== 'production',\n        playground: process.env.NODE_ENV !== 'production',\n        subscriptions: {\n          onConnect: async (connectionParams, webSocket, context) => {\n            // Authenticate WebSocket connections\n            if (connectionParams.authorization) {\n              const user = await GraphQLAuth.getUser({\n                headers: { authorization: connectionParams.authorization }\n              });\n              return { user };\n            }\n            throw new Error('Missing auth token!');\n          },\n          onDisconnect: (webSocket, context) => {\n            console.log('Client disconnected');\n          }\n        }\n      });\n    };\n\n    module.exports = createApolloServer;\n    ```"
              }
            ],
            "skills": []
          },
          {
            "name": "commands-automation-workflow",
            "description": "Commands for automating repetitive tasks and workflows",
            "source": "./plugins/commands-automation-workflow",
            "category": "commands",
            "version": "1.0.0",
            "author": {
              "name": "BuildWithClaude Community",
              "url": "https://github.com/davepoon/buildwithclaude"
            },
            "install_commands": [
              "/plugin marketplace add davepoon/buildwithclaude",
              "/plugin install commands-automation-workflow@buildwithclaude"
            ],
            "signals": {
              "stars": 2229,
              "forks": 239,
              "pushed_at": "2026-01-12T10:19:01Z",
              "created_at": "2025-07-25T02:26:45Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/act",
                "description": "Follow RED-GREEN-REFACTOR cycle approach for test-driven development",
                "path": "plugins/commands-automation-workflow/commands/act.md",
                "frontmatter": {
                  "description": "Follow RED-GREEN-REFACTOR cycle approach for test-driven development",
                  "category": "automation-workflow"
                },
                "content": "Follow RED-GREEN-REFACTOR cycle approch based on @~/.claude/CLAUDE.md:\n1. Open todo.md and select the first unchecked items to work on.\n2. Carefully plan each item, then share your plan\n3. Create a new branch and implement your plan\n4. Check off the items on todo.md\n5. Commit your changes"
              }
            ],
            "skills": []
          },
          {
            "name": "commands-ci-deployment",
            "description": "Commands for CI/CD setup, containerization, and deployment automation",
            "source": "./plugins/commands-ci-deployment",
            "category": "commands",
            "version": "1.0.0",
            "author": {
              "name": "BuildWithClaude Community",
              "url": "https://github.com/davepoon/buildwithclaude"
            },
            "install_commands": [
              "/plugin marketplace add davepoon/buildwithclaude",
              "/plugin install commands-ci-deployment@buildwithclaude"
            ],
            "signals": {
              "stars": 2229,
              "forks": 239,
              "pushed_at": "2026-01-12T10:19:01Z",
              "created_at": "2025-07-25T02:26:45Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/add-changelog",
                "description": "Generate and maintain project changelog",
                "path": "plugins/commands-ci-deployment/commands/add-changelog.md",
                "frontmatter": {
                  "description": "Generate and maintain project changelog",
                  "category": "ci-deployment",
                  "argument-hint": "1. **Changelog Format (Keep a Changelog)**",
                  "allowed-tools": "Bash(git *), Bash(npm *)"
                },
                "content": "# Add Changelog Command\n\nGenerate and maintain project changelog\n\n## Instructions\n\nSetup and maintain changelog following these steps: **$ARGUMENTS**\n\n1. **Changelog Format (Keep a Changelog)**\n   ```markdown\n   # Changelog\n   \n   All notable changes to this project will be documented in this file.\n   \n   The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),\n   and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).\n   \n   ## [Unreleased]\n   ### Added\n   - New features\n   \n   ### Changed\n   - Changes in existing functionality\n   \n   ### Deprecated\n   - Soon-to-be removed features\n   \n   ### Removed\n   - Removed features\n   \n   ### Fixed\n   - Bug fixes\n   \n   ### Security\n   - Security improvements\n   ```\n\n2. **Version Entries**\n   ```markdown\n   ## [1.2.3] - 2024-01-15\n   ### Added\n   - User authentication system\n   - Dark mode toggle\n   - Export functionality for reports\n   \n   ### Fixed\n   - Memory leak in background tasks\n   - Timezone handling issues\n   ```\n\n3. **Automation Tools**\n   ```bash\n   # Generate changelog from git commits\n   npm install -D conventional-changelog-cli\n   npx conventional-changelog -p angular -i CHANGELOG.md -s\n   \n   # Auto-changelog\n   npm install -D auto-changelog\n   npx auto-changelog\n   ```\n\n4. **Commit Convention**\n   ```bash\n   # Conventional commits for auto-generation\n   feat: add user authentication\n   fix: resolve memory leak in tasks\n   docs: update API documentation\n   style: format code with prettier\n   refactor: reorganize user service\n   test: add unit tests for auth\n   chore: update dependencies\n   ```\n\n5. **Integration with Releases**\n   - Update changelog before each release\n   - Include in release notes\n   - Link to GitHub releases\n   - Tag versions consistently\n\nRemember to keep entries clear, categorized, and focused on user-facing changes."
              },
              {
                "name": "/changelog-demo-command",
                "description": "Demo changelog automation features",
                "path": "plugins/commands-ci-deployment/commands/changelog-demo-command.md",
                "frontmatter": {
                  "description": "Demo changelog automation features",
                  "category": "ci-deployment"
                },
                "content": "# Demo Command for Changelog\n\nDemo changelog automation features\n\n## Instructions\n\n1. This is a demonstration command\n2. Shows changelog automation working independently\n3. Bypasses Claude review bot for faster testing"
              },
              {
                "name": "/ci-setup",
                "description": "Setup continuous integration pipeline",
                "path": "plugins/commands-ci-deployment/commands/ci-setup.md",
                "frontmatter": {
                  "description": "Setup continuous integration pipeline",
                  "category": "ci-deployment",
                  "argument-hint": "1. **Project Analysis**",
                  "allowed-tools": "Bash(npm *)"
                },
                "content": "# CI/CD Setup Command\n\nSetup continuous integration pipeline\n\n## Instructions\n\nFollow this systematic approach to implement CI/CD: **$ARGUMENTS**\n\n1. **Project Analysis**\n   - Identify the technology stack and deployment requirements\n   - Review existing build and test processes\n   - Understand deployment environments (dev, staging, prod)\n   - Assess current version control and branching strategy\n\n2. **CI/CD Platform Selection**\n   - Choose appropriate CI/CD platform based on requirements:\n     - **GitHub Actions**: Native GitHub integration, extensive marketplace\n     - **GitLab CI**: Built-in GitLab, comprehensive DevOps platform\n     - **Jenkins**: Self-hosted, highly customizable, extensive plugins\n     - **CircleCI**: Cloud-based, optimized for speed\n     - **Azure DevOps**: Microsoft ecosystem integration\n     - **AWS CodePipeline**: AWS-native solution\n\n3. **Repository Setup**\n   - Ensure proper `.gitignore` configuration\n   - Set up branch protection rules\n   - Configure merge requirements and reviews\n   - Establish semantic versioning strategy\n\n4. **Build Pipeline Configuration**\n   \n   **GitHub Actions Example:**\n   ```yaml\n   name: CI/CD Pipeline\n   \n   on:\n     push:\n       branches: [ main, develop ]\n     pull_request:\n       branches: [ main ]\n   \n   jobs:\n     test:\n       runs-on: ubuntu-latest\n       steps:\n         - uses: actions/checkout@v3\n         - name: Setup Node.js\n           uses: actions/setup-node@v3\n           with:\n             node-version: '18'\n             cache: 'npm'\n         - run: npm ci\n         - run: npm run test\n         - run: npm run build\n   ```\n\n   **GitLab CI Example:**\n   ```yaml\n   stages:\n     - test\n     - build\n     - deploy\n   \n   test:\n     stage: test\n     script:\n       - npm ci\n       - npm run test\n     cache:\n       paths:\n         - node_modules/\n   ```\n\n5. **Environment Configuration**\n   - Set up environment variables and secrets\n   - Configure different environments (dev, staging, prod)\n   - Implement environment-specific configurations\n   - Set up secure secret management\n\n6. **Automated Testing Integration**\n   - Configure unit test execution\n   - Set up integration test running\n   - Implement E2E test execution\n   - Configure test reporting and coverage\n\n   **Multi-stage Testing:**\n   ```yaml\n   test:\n     strategy:\n       matrix:\n         node-version: [16, 18, 20]\n     runs-on: ubuntu-latest\n     steps:\n       - uses: actions/checkout@v3\n       - uses: actions/setup-node@v3\n         with:\n           node-version: ${{ matrix.node-version }}\n       - run: npm ci\n       - run: npm test\n   ```\n\n7. **Code Quality Gates**\n   - Integrate linting and formatting checks\n   - Set up static code analysis (SonarQube, CodeClimate)\n   - Configure security vulnerability scanning\n   - Implement code coverage thresholds\n\n8. **Build Optimization**\n   - Configure build caching strategies\n   - Implement parallel job execution\n   - Optimize Docker image builds\n   - Set up artifact management\n\n   **Caching Example:**\n   ```yaml\n   - name: Cache node modules\n     uses: actions/cache@v3\n     with:\n       path: ~/.npm\n       key: ${{ runner.os }}-node-${{ hashFiles('**/package-lock.json') }}\n       restore-keys: |\n         ${{ runner.os }}-node-\n   ```\n\n9. **Docker Integration**\n   - Create optimized Dockerfiles\n   - Set up multi-stage builds\n   - Configure container registry integration\n   - Implement security scanning for images\n\n   **Multi-stage Dockerfile:**\n   ```dockerfile\n   FROM node:18-alpine AS builder\n   WORKDIR /app\n   COPY package*.json ./\n   RUN npm ci --only=production\n   \n   FROM node:18-alpine AS runtime\n   WORKDIR /app\n   COPY --from=builder /app/node_modules ./node_modules\n   COPY . .\n   EXPOSE 3000\n   CMD [\"npm\", \"start\"]\n   ```\n\n10. **Deployment Strategies**\n    - Implement blue-green deployment\n    - Set up canary releases\n    - Configure rolling updates\n    - Implement feature flags integration\n\n11. **Infrastructure as Code**\n    - Use Terraform, CloudFormation, or similar tools\n    - Version control infrastructure definitions\n    - Implement infrastructure testing\n    - Set up automated infrastructure provisioning\n\n12. **Monitoring and Observability**\n    - Set up application performance monitoring\n    - Configure log aggregation and analysis\n    - Implement health checks and alerting\n    - Set up deployment notifications\n\n13. **Security Integration**\n    - Implement dependency vulnerability scanning\n    - Set up container security scanning\n    - Configure SAST (Static Application Security Testing)\n    - Implement secrets scanning\n\n   **Security Scanning Example:**\n   ```yaml\n   security:\n     runs-on: ubuntu-latest\n     steps:\n       - uses: actions/checkout@v3\n       - name: Run Snyk to check for vulnerabilities\n         uses: snyk/actions/node@master\n         env:\n           SNYK_TOKEN: ${{ secrets.SNYK_TOKEN }}\n   ```\n\n14. **Database Migration Handling**\n    - Automate database schema migrations\n    - Implement rollback strategies\n    - Set up database seeding for testing\n    - Configure backup and recovery procedures\n\n15. **Performance Testing Integration**\n    - Set up load testing in pipeline\n    - Configure performance benchmarks\n    - Implement performance regression detection\n    - Set up performance monitoring\n\n16. **Multi-Environment Deployment**\n    - Configure staging environment deployment\n    - Set up production deployment with approvals\n    - Implement environment promotion workflow\n    - Configure environment-specific configurations\n\n   **Environment Deployment:**\n   ```yaml\n   deploy-staging:\n     needs: test\n     if: github.ref == 'refs/heads/develop'\n     runs-on: ubuntu-latest\n     steps:\n       - name: Deploy to staging\n         run: |\n           # Deploy to staging environment\n   \n   deploy-production:\n     needs: test\n     if: github.ref == 'refs/heads/main'\n     runs-on: ubuntu-latest\n     environment: production\n     steps:\n       - name: Deploy to production\n         run: |\n           # Deploy to production environment\n   ```\n\n17. **Rollback and Recovery**\n    - Implement automated rollback procedures\n    - Set up deployment verification tests\n    - Configure failure detection and alerts\n    - Document manual recovery procedures\n\n18. **Notification and Reporting**\n    - Set up Slack/Teams integration for notifications\n    - Configure email alerts for failures\n    - Implement deployment status reporting\n    - Set up metrics dashboards\n\n19. **Compliance and Auditing**\n    - Implement deployment audit trails\n    - Set up compliance checks (SOC 2, HIPAA, etc.)\n    - Configure approval workflows for sensitive deployments\n    - Document change management processes\n\n20. **Pipeline Optimization**\n    - Monitor pipeline performance and costs\n    - Implement pipeline parallelization\n    - Optimize resource allocation\n    - Set up pipeline analytics and reporting\n\n**Best Practices:**\n\n1. **Fail Fast**: Implement early failure detection\n2. **Parallel Execution**: Run independent jobs in parallel\n3. **Caching**: Cache dependencies and build artifacts\n4. **Security**: Never expose secrets in logs\n5. **Documentation**: Document pipeline processes and procedures\n6. **Monitoring**: Monitor pipeline health and performance\n7. **Testing**: Test pipeline changes in feature branches\n8. **Rollback**: Always have a rollback strategy\n\n**Sample Complete Pipeline:**\n```yaml\nname: Full CI/CD Pipeline\n\non:\n  push:\n    branches: [ main, develop ]\n  pull_request:\n    branches: [ main ]\n\njobs:\n  lint-and-test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - uses: actions/setup-node@v3\n        with:\n          node-version: '18'\n          cache: 'npm'\n      - run: npm ci\n      - run: npm run lint\n      - run: npm run test:coverage\n      - run: npm run build\n\n  security-scan:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - name: Security scan\n        run: npm audit --audit-level=high\n\n  deploy-staging:\n    needs: [lint-and-test, security-scan]\n    if: github.ref == 'refs/heads/develop'\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - name: Deploy to staging\n        run: echo \"Deploying to staging\"\n\n  deploy-production:\n    needs: [lint-and-test, security-scan]\n    if: github.ref == 'refs/heads/main'\n    runs-on: ubuntu-latest\n    environment: production\n    steps:\n      - uses: actions/checkout@v3\n      - name: Deploy to production\n        run: echo \"Deploying to production\"\n```\n\nStart with basic CI and gradually add more sophisticated features as your team and project mature."
              },
              {
                "name": "/containerize-application",
                "description": "Containerize application for deployment",
                "path": "plugins/commands-ci-deployment/commands/containerize-application.md",
                "frontmatter": {
                  "description": "Containerize application for deployment",
                  "category": "ci-deployment"
                },
                "content": "# Containerize Application\n\nContainerize application for deployment\n\n## Instructions\n\n1. **Application Analysis and Containerization Strategy**\n   - Analyze application architecture and runtime requirements\n   - Identify application dependencies and external services\n   - Determine optimal base image and runtime environment\n   - Plan multi-stage build strategy for optimization\n   - Assess security requirements and compliance needs\n\n2. **Dockerfile Creation and Optimization**\n   - Create comprehensive Dockerfile with multi-stage builds\n   - Select minimal base images (Alpine, distroless, or slim variants)\n   - Configure proper layer caching and build optimization\n   - Implement security best practices (non-root user, minimal attack surface)\n   - Set up proper file permissions and ownership\n\n3. **Build Process Configuration**\n   - Configure .dockerignore file to exclude unnecessary files\n   - Set up build arguments and environment variables\n   - Implement build-time dependency installation and cleanup\n   - Configure application bundling and asset optimization\n   - Set up proper build context and file structure\n\n4. **Runtime Configuration**\n   - Configure application startup and health checks\n   - Set up proper signal handling and graceful shutdown\n   - Configure logging and output redirection\n   - Set up environment-specific configuration management\n   - Configure resource limits and performance tuning\n\n5. **Security Hardening**\n   - Run application as non-root user with minimal privileges\n   - Configure security scanning and vulnerability assessment\n   - Implement secrets management and secure credential handling\n   - Set up network security and firewall rules\n   - Configure security policies and access controls\n\n6. **Docker Compose Configuration**\n   - Create docker-compose.yml for local development\n   - Configure service dependencies and networking\n   - Set up volume mounting and data persistence\n   - Configure environment variables and secrets\n   - Set up development vs production configurations\n\n7. **Container Orchestration Preparation**\n   - Prepare configurations for Kubernetes deployment\n   - Create deployment manifests and service definitions\n   - Configure ingress and load balancing\n   - Set up persistent volumes and storage classes\n   - Configure auto-scaling and resource management\n\n8. **Monitoring and Observability**\n   - Configure application metrics and health endpoints\n   - Set up logging aggregation and centralized logging\n   - Configure distributed tracing and monitoring\n   - Set up alerting and notification systems\n   - Configure performance monitoring and profiling\n\n9. **CI/CD Integration**\n   - Configure automated Docker image building\n   - Set up image scanning and security validation\n   - Configure image registry and artifact management\n   - Set up automated deployment pipelines\n   - Configure rollback and blue-green deployment strategies\n\n10. **Testing and Validation**\n    - Test container builds and functionality\n    - Validate security configurations and compliance\n    - Test deployment in different environments\n    - Validate performance and resource utilization\n    - Test backup and disaster recovery procedures\n    - Create documentation for container deployment and management"
              },
              {
                "name": "/hotfix-deploy",
                "description": "Deploy critical hotfixes quickly",
                "path": "plugins/commands-ci-deployment/commands/hotfix-deploy.md",
                "frontmatter": {
                  "description": "Deploy critical hotfixes quickly",
                  "category": "ci-deployment",
                  "argument-hint": "1. **Emergency Assessment and Triage**",
                  "allowed-tools": "Bash(git *), Bash(npm *)"
                },
                "content": "# Hotfix Deploy Command\n\nDeploy critical hotfixes quickly\n\n## Instructions\n\nFollow this emergency hotfix deployment process: **$ARGUMENTS**\n\n1. **Emergency Assessment and Triage**\n   - Assess the severity and impact of the issue\n   - Determine if a hotfix is necessary or if it can wait\n   - Identify affected systems and user impact\n   - Estimate time sensitivity and business impact\n   - Document the incident and decision rationale\n\n2. **Incident Response Setup**\n   - Create incident tracking in your incident management system\n   - Set up war room or communication channel\n   - Notify stakeholders and on-call team members\n   - Establish clear communication protocols\n   - Document initial incident details and timeline\n\n3. **Branch and Environment Setup**\n   ```bash\n   # Create hotfix branch from production tag\n   git fetch --tags\n   git checkout tags/v1.2.3  # Latest production version\n   git checkout -b hotfix/critical-auth-fix\n   \n   # Alternative: Branch from main if using trunk-based development\n   git checkout main\n   git pull origin main\n   git checkout -b hotfix/critical-auth-fix\n   ```\n\n4. **Rapid Development Process**\n   - Keep changes minimal and focused on the critical issue only\n   - Avoid refactoring, optimization, or unrelated improvements\n   - Use well-tested patterns and established approaches\n   - Add minimal logging for troubleshooting purposes\n   - Follow existing code conventions and patterns\n\n5. **Accelerated Testing**\n   ```bash\n   # Run focused tests related to the fix\n   npm test -- --testPathPattern=auth\n   npm run test:security\n   \n   # Manual testing checklist\n   # [ ] Core functionality works correctly\n   # [ ] Hotfix resolves the critical issue\n   # [ ] No new issues introduced\n   # [ ] Critical user flows remain functional\n   ```\n\n6. **Fast-Track Code Review**\n   - Get expedited review from senior team member\n   - Focus review on security and correctness\n   - Use pair programming if available and time permits\n   - Document review decisions and rationale quickly\n   - Ensure proper approval process even under time pressure\n\n7. **Version and Tagging**\n   ```bash\n   # Update version for hotfix\n   # 1.2.3 -> 1.2.4 (patch version)\n   # or 1.2.3 -> 1.2.3-hotfix.1 (hotfix identifier)\n   \n   # Commit with detailed message\n   git add .\n   git commit -m \"hotfix: fix critical authentication vulnerability\n   \n   - Fix password validation logic\n   - Resolve security issue allowing bypass\n   - Minimal change to reduce deployment risk\n   \n   Fixes: #1234\"\n   \n   # Tag the hotfix version\n   git tag -a v1.2.4 -m \"Hotfix v1.2.4: Critical auth security fix\"\n   git push origin hotfix/critical-auth-fix\n   git push origin v1.2.4\n   ```\n\n8. **Staging Deployment and Validation**\n   ```bash\n   # Deploy to staging environment for final validation\n   ./deploy-staging.sh v1.2.4\n   \n   # Critical path testing\n   curl -X POST staging.example.com/api/auth/login \\\n        -H \"Content-Type: application/json\" \\\n        -d '{\"email\":\"test@example.com\",\"password\":\"testpass\"}'\n   \n   # Run smoke tests\n   npm run test:smoke:staging\n   ```\n\n9. **Production Deployment Strategy**\n   \n   **Blue-Green Deployment:**\n   ```bash\n   # Deploy to blue environment\n   ./deploy-blue.sh v1.2.4\n   \n   # Validate blue environment health\n   ./health-check-blue.sh\n   \n   # Switch traffic to blue environment\n   ./switch-to-blue.sh\n   \n   # Monitor deployment metrics\n   ./monitor-deployment.sh\n   ```\n   \n   **Rolling Deployment:**\n   ```bash\n   # Deploy to subset of servers first\n   ./deploy-rolling.sh v1.2.4 --batch-size 1\n   \n   # Monitor each batch deployment\n   ./monitor-batch.sh\n   \n   # Continue with next batch if healthy\n   ./deploy-next-batch.sh\n   ```\n\n10. **Pre-Deployment Checklist**\n    ```bash\n    # Verify all prerequisites are met\n    # [ ] Database backup completed successfully\n    # [ ] Rollback plan documented and ready\n    # [ ] Monitoring alerts configured and active\n    # [ ] Team members standing by for support\n    # [ ] Communication channels established\n    \n    # Execute production deployment\n    ./deploy-production.sh v1.2.4\n    \n    # Run immediate post-deployment validation\n    ./validate-hotfix.sh\n    ```\n\n11. **Real-Time Monitoring**\n    ```bash\n    # Monitor key application metrics\n    watch -n 10 'curl -s https://api.example.com/health | jq .'\n    \n    # Monitor error rates and logs\n    tail -f /var/log/app/error.log | grep -i \"auth\"\n    \n    # Track critical metrics:\n    # - Response times and latency\n    # - Error rates and exception counts\n    # - User authentication success rates\n    # - System resource usage (CPU, memory)\n    ```\n\n12. **Post-Deployment Validation**\n    ```bash\n    # Run comprehensive validation tests\n    ./test-critical-paths.sh\n    \n    # Test user authentication functionality\n    curl -X POST https://api.example.com/auth/login \\\n         -H \"Content-Type: application/json\" \\\n         -d '{\"email\":\"test@example.com\",\"password\":\"testpass\"}'\n    \n    # Validate security fix effectiveness\n    ./security-validation.sh\n    \n    # Check overall system performance\n    ./performance-check.sh\n    ```\n\n13. **Communication and Status Updates**\n    - Provide regular status updates to stakeholders\n    - Use consistent communication channels\n    - Document deployment progress and results\n    - Update incident tracking systems\n    - Notify relevant teams of deployment completion\n\n14. **Rollback Procedures**\n    ```bash\n    # Automated rollback script\n    #!/bin/bash\n    PREVIOUS_VERSION=\"v1.2.3\"\n    \n    if [ \"$1\" = \"rollback\" ]; then\n        echo \"Rolling back to $PREVIOUS_VERSION\"\n        ./deploy-production.sh $PREVIOUS_VERSION\n        ./validate-rollback.sh\n        echo \"Rollback completed successfully\"\n    fi\n    \n    # Manual rollback steps if automation fails:\n    # 1. Switch load balancer back to previous version\n    # 2. Validate previous version health and functionality\n    # 3. Monitor system stability after rollback\n    # 4. Communicate rollback status to team\n    ```\n\n15. **Post-Deployment Monitoring Period**\n    - Monitor system for 2-4 hours after deployment\n    - Watch error rates and performance metrics closely\n    - Check user feedback and support ticket volume\n    - Validate that the hotfix resolves the original issue\n    - Document any issues or unexpected behaviors\n\n16. **Documentation and Incident Reporting**\n    - Document the complete hotfix process and timeline\n    - Record lessons learned and process improvements\n    - Update incident management systems with resolution\n    - Create post-incident review materials\n    - Share knowledge with team for future reference\n\n17. **Merge Back to Main Branch**\n    ```bash\n    # After successful hotfix deployment and validation\n    git checkout main\n    git pull origin main\n    git merge hotfix/critical-auth-fix\n    git push origin main\n    \n    # Clean up hotfix branch\n    git branch -d hotfix/critical-auth-fix\n    git push origin --delete hotfix/critical-auth-fix\n    ```\n\n18. **Post-Incident Activities**\n    - Schedule and conduct post-incident review meeting\n    - Update runbooks and emergency procedures\n    - Identify and implement process improvements\n    - Update monitoring and alerting configurations\n    - Plan preventive measures to avoid similar issues\n\n**Hotfix Best Practices:**\n\n- **Keep It Simple:** Make minimal changes focused only on the critical issue\n- **Test Thoroughly:** Maintain testing standards even under time pressure\n- **Communicate Clearly:** Keep all stakeholders informed throughout the process\n- **Monitor Closely:** Watch the fix carefully in production environment\n- **Document Everything:** Record all decisions and actions for post-incident review\n- **Plan for Rollback:** Always have a tested way to revert changes quickly\n- **Learn and Improve:** Use each incident to strengthen processes and procedures\n\n**Emergency Escalation Guidelines:**\n\n```bash\n# Emergency contact information\nON_CALL_ENGINEER=\"+1-555-0123\"\nSENIOR_ENGINEER=\"+1-555-0124\"\nENGINEERING_MANAGER=\"+1-555-0125\"\nINCIDENT_COMMANDER=\"+1-555-0126\"\n\n# Escalation timeline thresholds:\n# 15 minutes: Escalate to senior engineer\n# 30 minutes: Escalate to engineering manager\n# 60 minutes: Escalate to incident commander\n```\n\n**Important Reminders:**\n\n- Hotfixes should only be used for genuine production emergencies\n- When in doubt about severity, follow the normal release process\n- Always prioritize system stability over speed of deployment\n- Maintain clear audit trails for all emergency changes\n- Regular drills help ensure team readiness for real emergencies"
              },
              {
                "name": "/prepare-release",
                "description": "Prepare and validate release packages",
                "path": "plugins/commands-ci-deployment/commands/prepare-release.md",
                "frontmatter": {
                  "description": "Prepare and validate release packages",
                  "category": "ci-deployment",
                  "argument-hint": "1. **Release Planning and Validation**",
                  "allowed-tools": "Bash(git *), Bash(npm *)"
                },
                "content": "# Prepare Release Command\n\nPrepare and validate release packages\n\n## Instructions\n\nFollow this systematic approach to prepare a release: **$ARGUMENTS**\n\n1. **Release Planning and Validation**\n   - Determine release version number (semantic versioning)\n   - Review and validate all features included in release\n   - Check that all planned issues and features are complete\n   - Verify release criteria and acceptance requirements\n\n2. **Pre-Release Checklist**\n   - Ensure all tests are passing (unit, integration, E2E)\n   - Verify code coverage meets project standards\n   - Complete security vulnerability scanning\n   - Perform performance testing and validation\n   - Review and approve all pending pull requests\n\n3. **Version Management**\n   ```bash\n   # Check current version\n   git describe --tags --abbrev=0\n   \n   # Determine next version (semantic versioning)\n   # MAJOR.MINOR.PATCH\n   # MAJOR: Breaking changes\n   # MINOR: New features (backward compatible)\n   # PATCH: Bug fixes (backward compatible)\n   \n   # Example version updates\n   # 1.2.3 -> 1.2.4 (patch)\n   # 1.2.3 -> 1.3.0 (minor)\n   # 1.2.3 -> 2.0.0 (major)\n   ```\n\n4. **Code Freeze and Branch Management**\n   ```bash\n   # Create release branch from main\n   git checkout main\n   git pull origin main\n   git checkout -b release/v1.2.3\n   \n   # Alternative: Use main branch directly for smaller releases\n   # Ensure no new features are merged during release process\n   ```\n\n5. **Version Number Updates**\n   - Update package.json, setup.py, or equivalent version files\n   - Update version in application configuration\n   - Update version in documentation and README\n   - Update API version if applicable\n\n   ```bash\n   # Node.js projects\n   npm version patch  # or minor, major\n   \n   # Python projects\n   # Update version in setup.py, __init__.py, or pyproject.toml\n   \n   # Manual version update\n   sed -i 's/\"version\": \"1.2.2\"/\"version\": \"1.2.3\"/' package.json\n   ```\n\n6. **Changelog Generation**\n   ```markdown\n   # CHANGELOG.md\n   \n   ## [1.2.3] - 2024-01-15\n   \n   ### Added\n   - New user authentication system\n   - Dark mode support for UI\n   - API rate limiting functionality\n   \n   ### Changed\n   - Improved database query performance\n   - Updated user interface design\n   - Enhanced error handling\n   \n   ### Fixed\n   - Fixed memory leak in background tasks\n   - Resolved issue with file upload validation\n   - Fixed timezone handling in date calculations\n   \n   ### Security\n   - Updated dependencies with security patches\n   - Improved input validation and sanitization\n   ```\n\n7. **Documentation Updates**\n   - Update API documentation with new endpoints\n   - Revise user documentation and guides\n   - Update installation and deployment instructions\n   - Review and update README.md\n   - Update migration guides if needed\n\n8. **Dependency Management**\n   ```bash\n   # Update and audit dependencies\n   npm audit fix\n   npm update\n   \n   # Python\n   pip-audit\n   pip freeze > requirements.txt\n   \n   # Review security vulnerabilities\n   npm audit\n   snyk test\n   ```\n\n9. **Build and Artifact Generation**\n   ```bash\n   # Clean build environment\n   npm run clean\n   rm -rf dist/ build/\n   \n   # Build production artifacts\n   npm run build\n   \n   # Verify build artifacts\n   ls -la dist/\n   \n   # Test built artifacts\n   npm run test:build\n   ```\n\n10. **Testing and Quality Assurance**\n    - Run comprehensive test suite\n    - Perform manual testing of critical features\n    - Execute regression testing\n    - Conduct user acceptance testing\n    - Validate in staging environment\n\n    ```bash\n    # Run all tests\n    npm test\n    npm run test:integration\n    npm run test:e2e\n    \n    # Check code coverage\n    npm run test:coverage\n    \n    # Performance testing\n    npm run test:performance\n    ```\n\n11. **Security and Compliance Verification**\n    - Run security scans and penetration testing\n    - Verify compliance with security standards\n    - Check for exposed secrets or credentials\n    - Validate data protection and privacy measures\n\n12. **Release Notes Preparation**\n    ```markdown\n    # Release Notes v1.2.3\n    \n    ##  What's New\n    - **Dark Mode**: Users can now switch to dark mode in settings\n    - **Enhanced Security**: Improved authentication with 2FA support\n    - **Performance**: 40% faster page load times\n    \n    ##  Improvements\n    - Better error messages for form validation\n    - Improved mobile responsiveness\n    - Enhanced accessibility features\n    \n    ##  Bug Fixes\n    - Fixed issue with file downloads in Safari\n    - Resolved memory leak in background tasks\n    - Fixed timezone display issues\n    \n    ##  Documentation\n    - Updated API documentation\n    - New user onboarding guide\n    - Enhanced troubleshooting section\n    \n    ##  Migration Guide\n    - No breaking changes in this release\n    - Automatic database migrations included\n    - See [Migration Guide](link) for details\n    ```\n\n13. **Release Tagging and Versioning**\n    ```bash\n    # Create annotated tag\n    git add .\n    git commit -m \"chore: prepare release v1.2.3\"\n    git tag -a v1.2.3 -m \"Release version 1.2.3\n    \n    Features:\n    - Dark mode support\n    - Enhanced authentication\n    \n    Bug fixes:\n    - Fixed file upload issues\n    - Resolved memory leaks\"\n    \n    # Push tag to remote\n    git push origin v1.2.3\n    git push origin release/v1.2.3\n    ```\n\n14. **Deployment Preparation**\n    - Prepare deployment scripts and configurations\n    - Update environment variables and secrets\n    - Plan deployment strategy (blue-green, rolling, canary)\n    - Set up monitoring and alerting for release\n    - Prepare rollback procedures\n\n15. **Staging Environment Validation**\n    ```bash\n    # Deploy to staging\n    ./deploy-staging.sh v1.2.3\n    \n    # Run smoke tests\n    npm run test:smoke:staging\n    \n    # Manual validation checklist\n    # [ ] User login/logout\n    # [ ] Core functionality\n    # [ ] New features\n    # [ ] Performance metrics\n    # [ ] Security checks\n    ```\n\n16. **Production Deployment Planning**\n    - Schedule deployment window\n    - Notify stakeholders and users\n    - Prepare maintenance mode if needed\n    - Set up deployment monitoring\n    - Plan communication strategy\n\n17. **Release Automation Setup**\n    ```yaml\n    # GitHub Actions Release Workflow\n    name: Release\n    \n    on:\n      push:\n        tags:\n          - 'v*'\n    \n    jobs:\n      release:\n        runs-on: ubuntu-latest\n        steps:\n          - uses: actions/checkout@v3\n          - name: Setup Node.js\n            uses: actions/setup-node@v3\n            with:\n              node-version: '18'\n          \n          - name: Install dependencies\n            run: npm ci\n          \n          - name: Run tests\n            run: npm test\n          \n          - name: Build\n            run: npm run build\n          \n          - name: Create Release\n            uses: actions/create-release@v1\n            env:\n              GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n            with:\n              tag_name: ${{ github.ref }}\n              release_name: Release ${{ github.ref }}\n              draft: false\n              prerelease: false\n    ```\n\n18. **Communication and Announcements**\n    - Prepare release announcement\n    - Update status page and documentation\n    - Notify customers and users\n    - Share on relevant communication channels\n    - Update social media and marketing materials\n\n19. **Post-Release Monitoring**\n    - Monitor application performance and errors\n    - Track user adoption of new features\n    - Monitor system metrics and alerts\n    - Collect user feedback and issues\n    - Prepare hotfix procedures if needed\n\n20. **Release Retrospective**\n    - Document lessons learned\n    - Review release process effectiveness\n    - Identify improvement opportunities\n    - Update release procedures\n    - Plan for next release cycle\n\n**Release Types and Considerations:**\n\n**Patch Release (1.2.3  1.2.4):**\n- Bug fixes only\n- No new features\n- Minimal testing required\n- Quick deployment\n\n**Minor Release (1.2.3  1.3.0):**\n- New features (backward compatible)\n- Enhanced functionality\n- Comprehensive testing\n- User communication needed\n\n**Major Release (1.2.3  2.0.0):**\n- Breaking changes\n- Significant new features\n- Migration guide required\n- Extended testing period\n- User training and support\n\n**Hotfix Release:**\n```bash\n# Emergency hotfix process\ngit checkout main\ngit pull origin main\ngit checkout -b hotfix/critical-bug-fix\n\n# Make minimal fix\ngit add .\ngit commit -m \"hotfix: fix critical security vulnerability\"\n\n# Fast-track testing and deployment\nnpm test\ngit tag -a v1.2.4-hotfix.1 -m \"Hotfix for critical security issue\"\ngit push origin hotfix/critical-bug-fix\ngit push origin v1.2.4-hotfix.1\n```\n\nRemember to:\n- Test everything thoroughly before release\n- Communicate clearly with all stakeholders\n- Have rollback procedures ready\n- Monitor the release closely after deployment\n- Document everything for future releases"
              },
              {
                "name": "/release",
                "description": "Prepare a new release by updating changelog, version, and documentation",
                "path": "plugins/commands-ci-deployment/commands/release.md",
                "frontmatter": {
                  "description": "Prepare a new release by updating changelog, version, and documentation",
                  "category": "ci-deployment",
                  "allowed-tools": "Edit, Read, Bash(git *)"
                },
                "content": "Update CHANGELOG.md with changes since the last version increase. Check our README.md for any necessary changes. Check the scope of changes since the last release and increase our version number as appropriate."
              },
              {
                "name": "/rollback-deploy",
                "description": "Rollback deployment to previous version",
                "path": "plugins/commands-ci-deployment/commands/rollback-deploy.md",
                "frontmatter": {
                  "description": "Rollback deployment to previous version",
                  "category": "ci-deployment",
                  "argument-hint": "1. **Incident Assessment and Decision**",
                  "allowed-tools": "Bash(npm *)"
                },
                "content": "# Rollback Deploy Command\n\nRollback deployment to previous version\n\n## Instructions\n\nFollow this systematic rollback procedure: **$ARGUMENTS**\n\n1. **Incident Assessment and Decision**\n   - Assess the severity and impact of the current deployment issues\n   - Determine if rollback is necessary or if forward fix is better\n   - Identify affected systems, users, and business functions\n   - Consider data integrity and consistency implications\n   - Document the decision rationale and timeline\n\n2. **Emergency Response Setup**\n   ```bash\n   # Activate incident response team\n   # Set up communication channels\n   # Notify stakeholders immediately\n   \n   # Example emergency notification\n   echo \" ROLLBACK INITIATED\n   Issue: Critical performance degradation after v1.3.0 deployment\n   Action: Rolling back to v1.2.9\n   ETA: 15 minutes\n   Impact: Temporary service interruption possible\n   Status channel: #incident-rollback-202401\"\n   ```\n\n3. **Pre-Rollback Safety Checks**\n   ```bash\n   # Verify current production version\n   curl -s https://api.example.com/version\n   kubectl get deployments -o wide\n   \n   # Check system status\n   curl -s https://api.example.com/health | jq .\n   \n   # Identify target rollback version\n   git tag --sort=-version:refname | head -5\n   \n   # Verify rollback target exists and is deployable\n   git show v1.2.9 --stat\n   ```\n\n4. **Database Considerations**\n   ```bash\n   # Check for database migrations since last version\n   ./check-migrations.sh v1.2.9 v1.3.0\n   \n   # If migrations exist, plan database rollback\n   # WARNING: Database rollbacks can cause data loss\n   # Consider forward fix instead if migrations are present\n   \n   # Create database backup before rollback\n   ./backup-database.sh \"pre-rollback-$(date +%Y%m%d-%H%M%S)\"\n   ```\n\n5. **Traffic Management Preparation**\n   ```bash\n   # Prepare to redirect traffic\n   # Option 1: Maintenance page\n   ./enable-maintenance-mode.sh\n   \n   # Option 2: Load balancer management\n   ./drain-traffic.sh --gradual\n   \n   # Option 3: Circuit breaker activation\n   ./activate-circuit-breaker.sh\n   ```\n\n6. **Container/Kubernetes Rollback**\n   ```bash\n   # Kubernetes rollback\n   kubectl rollout history deployment/app-deployment\n   kubectl rollout undo deployment/app-deployment\n   \n   # Or rollback to specific revision\n   kubectl rollout undo deployment/app-deployment --to-revision=3\n   \n   # Monitor rollback progress\n   kubectl rollout status deployment/app-deployment --timeout=300s\n   \n   # Verify pods are running\n   kubectl get pods -l app=your-app\n   ```\n\n7. **Docker Swarm Rollback**\n   ```bash\n   # List service history\n   docker service ps app-service --no-trunc\n   \n   # Rollback to previous version\n   docker service update --rollback app-service\n   \n   # Or update to specific image\n   docker service update --image app:v1.2.9 app-service\n   \n   # Monitor rollback\n   docker service ps app-service\n   ```\n\n8. **Traditional Deployment Rollback**\n   ```bash\n   # Blue-Green deployment rollback\n   ./switch-to-blue.sh  # or green, depending on current\n   \n   # Rolling deployment rollback\n   ./deploy-version.sh v1.2.9 --rolling\n   \n   # Symlink-based rollback\n   ln -sfn /releases/v1.2.9 /current\n   sudo systemctl restart app-service\n   ```\n\n9. **Load Balancer and CDN Updates**\n   ```bash\n   # Update load balancer to point to old version\n   aws elbv2 modify-target-group --target-group-arn $TG_ARN --targets Id=old-instance\n   \n   # Clear CDN cache if needed\n   aws cloudfront create-invalidation --distribution-id $DIST_ID --paths \\\"/*\\\"\n   \n   # Update DNS if necessary (last resort, has propagation delay)\n   # aws route53 change-resource-record-sets ...\n   ```\n\n10. **Configuration Rollback**\n    ```bash\\n    # Rollback configuration files\\n    git checkout v1.2.9 -- config/\\n    \\n    # Restart services with old configuration\\n    sudo systemctl restart nginx\\n    sudo systemctl restart app-service\\n    \\n    # Rollback environment variables\\n    ./restore-env-vars.sh v1.2.9\\n    \\n    # Update feature flags\\n    ./update-feature-flags.sh --disable-new-features\\n    ```\\n\\n11. **Database Rollback (if necessary)**\\n    ```sql\\n    -- EXTREME CAUTION: Can cause data loss\\n    \\n    -- Check migration status\\n    SELECT * FROM schema_migrations ORDER BY version DESC LIMIT 5;\\n    \\n    -- Rollback specific migrations (framework dependent)\\n    -- Rails: rake db:migrate:down VERSION=20240115120000\\n    -- Django: python manage.py migrate app_name 0001\\n    -- Node.js: npm run migrate:down\\n    \\n    -- Verify database state\\n    SHOW TABLES;\\n    DESCRIBE critical_table;\\n    ```\\n\\n12. **Service Health Validation**\\n    ```bash\\n    # Health check script\\n    #!/bin/bash\\n    \\n    echo \\\"Validating rollback...\\\"\\n    \\n    # Check application health\\n    if curl -f -s https://api.example.com/health > /dev/null; then\\n        echo \\\" Health check passed\\\"\\n    else\\n        echo \\\" Health check failed\\\"\\n        exit 1\\n    fi\\n    \\n    # Check critical endpoints\\n    endpoints=(\\n        \\\"/api/users/me\\\"\\n        \\\"/api/auth/status\\\"\\n        \\\"/api/data/latest\\\"\\n    )\\n    \\n    for endpoint in \\\"${endpoints[@]}\\\"; do\\n        if curl -f -s \\\"https://api.example.com$endpoint\\\" > /dev/null; then\\n            echo \\\" $endpoint working\\\"\\n        else\\n            echo \\\" $endpoint failed\\\"\\n        fi\\n    done\\n    ```\\n\\n13. **Performance and Metrics Validation**\\n    ```bash\\n    # Check response times\\n    curl -w \\\"Response time: %{time_total}s\\\\n\\\" -s -o /dev/null https://api.example.com/\\n    \\n    # Monitor error rates\\n    tail -f /var/log/app/error.log | head -20\\n    \\n    # Check system resources\\n    top -bn1 | head -10\\n    free -h\\n    df -h\\n    \\n    # Validate database connectivity\\n    mysql -u app -p -e \\\"SELECT 1;\\\"\\n    ```\\n\\n14. **Traffic Restoration**\\n    ```bash\\n    # Gradually restore traffic\\n    ./restore-traffic.sh --gradual\\n    \\n    # Disable maintenance mode\\n    ./disable-maintenance-mode.sh\\n    \\n    # Re-enable circuit breakers\\n    ./deactivate-circuit-breaker.sh\\n    \\n    # Monitor traffic patterns\\n    ./monitor-traffic.sh --duration 300\\n    ```\\n\\n15. **Monitoring and Alerting**\\n    ```bash\\n    # Enable enhanced monitoring during rollback\\n    ./enable-enhanced-monitoring.sh\\n    \\n    # Watch key metrics\\n    watch -n 10 'curl -s https://api.example.com/metrics | jq .'\\n    \\n    # Monitor logs in real-time\\n    tail -f /var/log/app/*.log | grep -E \\\"ERROR|WARN|EXCEPTION\\\"\\n    \\n    # Check application metrics\\n    # - Response times\\n    # - Error rates\\n    # - User sessions\\n    # - Database performance\\n    ```\\n\\n16. **User Communication**\\n    ```markdown\\n    ## Service Update - Rollback Completed\\n    \\n    **Status:**  Service Restored\\n    **Time:** 2024-01-15 15:45 UTC\\n    **Duration:** 12 minutes of degraded performance\\n    \\n    **What Happened:**\\n    We identified performance issues with our latest release and \\n    performed a rollback to ensure optimal service quality.\\n    \\n    **Current Status:**\\n    - All services operating normally\\n    - Performance metrics back to baseline\\n    - No data loss occurred\\n    \\n    **Next Steps:**\\n    We're investigating the root cause and will provide updates \\n    on our status page.\\n    ```\\n\\n17. **Post-Rollback Validation**\\n    ```bash\\n    # Extended monitoring period\\n    ./monitor-extended.sh --duration 3600  # 1 hour\\n    \\n    # Run integration tests\\n    npm run test:integration:production\\n    \\n    # Check user-reported issues\\n    ./check-support-tickets.sh --since \\\"1 hour ago\\\"\\n    \\n    # Validate business metrics\\n    ./check-business-metrics.sh\\n    ```\\n\\n18. **Documentation and Reporting**\\n    ```markdown\\n    # Rollback Incident Report\\n    \\n    **Incident ID:** INC-2024-0115-001\\n    **Rollback Version:** v1.2.9 (from v1.3.0)\\n    **Start Time:** 2024-01-15 15:30 UTC\\n    **End Time:** 2024-01-15 15:42 UTC\\n    **Total Duration:** 12 minutes\\n    \\n    **Timeline:**\\n    - 15:25 - Performance degradation detected\\n    - 15:30 - Rollback decision made\\n    - 15:32 - Traffic drained\\n    - 15:35 - Rollback initiated\\n    - 15:38 - Rollback completed\\n    - 15:42 - Traffic fully restored\\n    \\n    **Impact:**\\n    - 12 minutes of degraded performance\\n    - ~5% of users experienced slow responses\\n    - No data loss or corruption\\n    - No security implications\\n    \\n    **Root Cause:**\\n    Memory leak in new feature causing performance degradation\\n    \\n    **Lessons Learned:**\\n    - Need better performance testing in staging\\n    - Improve monitoring for memory usage\\n    - Consider canary deployments for major releases\\n    ```\\n\\n19. **Cleanup and Follow-up**\\n    ```bash\\n    # Clean up failed deployment artifacts\\n    docker image rm app:v1.3.0\\n    \\n    # Update deployment status\\n    ./update-deployment-status.sh \\\"rollback-completed\\\"\\n    \\n    # Reset feature flags if needed\\n    ./reset-feature-flags.sh\\n    \\n    # Schedule post-incident review\\n    ./schedule-postmortem.sh --date \\\"2024-01-16 10:00\\\"\\n    ```\\n\\n20. **Prevention and Improvement**\\n    - Analyze what went wrong with the deployment\\n    - Improve testing and validation procedures\\n    - Enhance monitoring and alerting\\n    - Update rollback procedures based on learnings\\n    - Consider implementing canary deployments\\n\\n**Rollback Decision Matrix:**\\n\\n| Issue Severity | Data Impact | Time to Fix | Decision |\\n|---------------|-------------|-------------|----------|\\n| Critical | None | > 30 min | Rollback |\\n| High | Minor | > 60 min | Rollback |\\n| Medium | None | > 2 hours | Consider rollback |\\n| Low | None | Any | Forward fix |\\n\\n**Emergency Rollback Script Template:**\\n```bash\\n#!/bin/bash\\nset -e\\n\\n# Emergency rollback script\\nPREVIOUS_VERSION=\\\"${1:-v1.2.9}\\\"\\nCURRENT_VERSION=$(curl -s https://api.example.com/version)\\n\\necho \\\" EMERGENCY ROLLBACK\\\"\\necho \\\"From: $CURRENT_VERSION\\\"\\necho \\\"To: $PREVIOUS_VERSION\\\"\\necho \\\"\\\"\\n\\n# Confirm rollback\\nread -p \\\"Proceed with rollback? (yes/no): \\\" confirm\\nif [ \\\"$confirm\\\" != \\\"yes\\\" ]; then\\n    echo \\\"Rollback cancelled\\\"\\n    exit 1\\nfi\\n\\n# Execute rollback\\necho \\\"Starting rollback...\\\"\\nkubectl set image deployment/app-deployment app=app:$PREVIOUS_VERSION\\nkubectl rollout status deployment/app-deployment --timeout=300s\\n\\n# Validate\\necho \\\"Validating rollback...\\\"\\nsleep 30\\ncurl -f https://api.example.com/health\\n\\necho \\\" Rollback completed successfully\\\"\\n```\\n\\nRemember: Rollbacks should be a last resort. Always consider forward fixes first, especially when database migrations are involved."
              },
              {
                "name": "/run-ci",
                "description": "Run CI checks and fix any errors until all tests pass",
                "path": "plugins/commands-ci-deployment/commands/run-ci.md",
                "frontmatter": {
                  "description": "Run CI checks and fix any errors until all tests pass",
                  "category": "ci-deployment",
                  "allowed-tools": "Bash, Edit, Read, Glob"
                },
                "content": "Run CI checks for the project and fix any errors until all tests pass.\n\n## Process:\n\n1. **Detect CI System**:\n   - Check for CI configuration files:\n     - `.github/workflows/*.yml` (GitHub Actions)\n     - `.gitlab-ci.yml` (GitLab CI)\n     - `.circleci/config.yml` (CircleCI)\n     - `Jenkinsfile` (Jenkins)\n     - `.travis.yml` (Travis CI)\n     - `bitbucket-pipelines.yml` (Bitbucket)\n\n2. **Detect Build System**:\n   - JavaScript/TypeScript: package.json scripts\n   - Python: Makefile, tox.ini, setup.py, pyproject.toml\n   - Go: Makefile, go.mod\n   - Rust: Cargo.toml\n   - Java: pom.xml, build.gradle\n   - Other: Look for common CI scripts\n\n3. **Run CI Commands**:\n   - Check for CI scripts: `ci`, `test`, `check`, `validate`, `verify`\n   - Common script locations:\n     - `./scripts/ci.sh`, `./ci.sh`, `./run-tests.sh`\n     - Package manager scripts (npm/yarn/pnpm run test)\n     - Make targets (make test, make ci)\n   - Activate virtual environments if needed (Python, Ruby, etc.)\n\n4. **Fix Errors**:\n   - Analyze error output\n   - Fix code issues, test failures, or configuration problems\n   - Re-run CI checks after each fix\n\n5. **Common CI Tasks**:\n   - Linting/formatting\n   - Type checking\n   - Unit tests\n   - Integration tests\n   - Build verification\n   - Documentation generation\n\n## Examples:\n- JavaScript: `npm test` or `npm run ci`\n- Python: `make test` or `pytest` or `tox`\n- Go: `go test ./...` or `make test`\n- Rust: `cargo test`\n- Generic: `./ci.sh` or `make ci`\n\nContinue fixing issues and re-running until all CI checks pass."
              },
              {
                "name": "/setup-automated-releases",
                "description": "Setup automated release workflows",
                "path": "plugins/commands-ci-deployment/commands/setup-automated-releases.md",
                "frontmatter": {
                  "description": "Setup automated release workflows",
                  "category": "ci-deployment",
                  "argument-hint": "Specify release automation settings"
                },
                "content": "# Setup Automated Releases\n\nSetup automated release workflows\n\n## Instructions\n\nSet up automated releases following industry best practices:\n\n1. **Analyze Repository Structure**\n   - Detect project type (Node.js, Python, Go, etc.)\n   - Check for existing CI/CD workflows\n   - Identify current versioning approach\n   - Review existing release processes\n\n2. **Create Version Tracking**\n   - For Node.js: Use package.json version field\n   - For Python: Use __version__ in __init__.py or pyproject.toml\n   - For Go: Use version in go.mod\n   - For others: Create version.txt file\n   - Ensure version follows semantic versioning (MAJOR.MINOR.PATCH)\n\n3. **Set Up Conventional Commits**\n   - Create CONTRIBUTING.md with commit conventions:\n     - `feat:` for new features (minor bump)\n     - `fix:` for bug fixes (patch bump)\n     - `feat!:` or `BREAKING CHANGE:` for breaking changes (major bump)\n     - `docs:`, `chore:`, `style:`, `refactor:`, `test:` for non-releasing changes\n   - Include examples and guidelines for each type\n\n4. **Create Pull Request Template**\n   - Add `.github/pull_request_template.md`\n   - Include conventional commit reminder\n   - Add checklist for common requirements\n   - Reference contributing guidelines\n\n5. **Create Release Workflow**\n   - Add `.github/workflows/release.yml`:\n     - Trigger on push to main branch\n     - Analyze commits since last release\n     - Determine version bump type\n     - Update version in appropriate file(s)\n     - Generate release notes from commits\n     - Update CHANGELOG.md\n     - Create git tag\n     - Create GitHub Release\n     - Attach distribution artifacts\n   - Include manual trigger option for forced releases\n\n6. **Create PR Validation Workflow**\n   - Add `.github/workflows/pr-check.yml`:\n     - Validate PR title follows conventional format\n     - Check commit messages\n     - Provide feedback on version impact\n     - Run tests and quality checks\n\n7. **Configure GitHub Release Notes**\n   - Create `.github/release.yml`\n   - Define categories for different change types\n   - Configure changelog exclusions\n   - Set up contributor recognition\n\n8. **Update Documentation**\n   - Add release badges to README:\n     - Current version badge\n     - Latest release badge\n     - Build status badge\n   - Document release process\n   - Add link to CONTRIBUTING.md\n   - Explain version bump rules\n\n9. **Set Up Changelog Management**\n   - Ensure CHANGELOG.md follows Keep a Changelog format\n   - Add [Unreleased] section for upcoming changes\n   - Configure automatic changelog updates\n   - Set up changelog categories\n\n10. **Configure Branch Protection**\n    - Recommend branch protection rules:\n      - Require PR reviews\n      - Require status checks\n      - Require conventional PR titles\n      - Dismiss stale reviews\n    - Document recommended settings\n\n11. **Add Security Scanning**\n    - Set up Dependabot for dependency updates\n    - Configure security alerts\n    - Add security policy if needed\n\n12. **Test the System**\n    - Create example PR with conventional title\n    - Verify PR checks work correctly\n    - Test manual release trigger\n    - Validate changelog generation\n\nArguments: $ARGUMENTS\n\n### Additional Considerations\n\n**For Monorepos:**\n- Set up independent versioning per package\n- Configure changelog per package\n- Use conventional commits scopes\n\n**For Libraries:**\n- Include API compatibility checks\n- Generate API documentation\n- Add upgrade guides for breaking changes\n\n**For Applications:**\n- Include Docker image versioning\n- Set up deployment triggers\n- Add rollback procedures\n\n**Best Practices:**\n- Always create release branches for hotfixes\n- Use release candidates for major versions\n- Maintain upgrade guides\n- Keep releases small and frequent\n- Document rollback procedures\n\nThis automated release system provides:\n-  Consistent versioning\n-  Automatic changelog generation\n-  Clear contribution guidelines\n-  Professional release notes\n-  Reduced manual work\n-  Better project maintainability"
              },
              {
                "name": "/setup-kubernetes-deployment",
                "description": "Configure Kubernetes deployment manifests",
                "path": "plugins/commands-ci-deployment/commands/setup-kubernetes-deployment.md",
                "frontmatter": {
                  "description": "Configure Kubernetes deployment manifests",
                  "category": "ci-deployment"
                },
                "content": "# Setup Kubernetes Deployment\n\nConfigure Kubernetes deployment manifests\n\n## Instructions\n\n1. **Kubernetes Architecture Planning**\n   - Analyze application architecture and deployment requirements\n   - Define resource requirements (CPU, memory, storage, network)\n   - Plan namespace organization and multi-tenancy strategy\n   - Assess high availability and disaster recovery requirements\n   - Define scaling strategies and performance requirements\n\n2. **Cluster Setup and Configuration**\n   - Set up Kubernetes cluster (managed or self-hosted)\n   - Configure cluster networking and CNI plugin\n   - Set up cluster storage classes and persistent volumes\n   - Configure cluster security policies and RBAC\n   - Set up cluster monitoring and logging infrastructure\n\n3. **Application Containerization**\n   - Ensure application is properly containerized\n   - Optimize container images for Kubernetes deployment\n   - Configure multi-stage builds and security scanning\n   - Set up container registry and image management\n   - Configure image pull policies and secrets\n\n4. **Kubernetes Manifest Creation**\n   - Create Deployment manifests with proper resource limits\n   - Set up Service manifests for internal and external communication\n   - Configure ConfigMaps and Secrets for configuration management\n   - Create PersistentVolumeClaims for data storage\n   - Set up NetworkPolicies for security and isolation\n\n5. **Load Balancing and Ingress**\n   - Configure Ingress controllers and routing rules\n   - Set up SSL/TLS termination and certificate management\n   - Configure load balancing strategies and session affinity\n   - Set up external DNS and domain management\n   - Configure traffic management and canary deployments\n\n6. **Auto-scaling Configuration**\n   - Set up Horizontal Pod Autoscaler (HPA) based on metrics\n   - Configure Vertical Pod Autoscaler (VPA) for resource optimization\n   - Set up Cluster Autoscaler for node scaling\n   - Configure custom metrics and scaling policies\n   - Set up resource quotas and limits\n\n7. **Health Checks and Monitoring**\n   - Configure liveness and readiness probes\n   - Set up startup probes for slow-starting applications\n   - Configure health check endpoints and monitoring\n   - Set up application metrics collection\n   - Configure alerting and notification systems\n\n8. **Security and Compliance**\n   - Configure Pod Security Standards and policies\n   - Set up network segmentation and security policies\n   - Configure service accounts and RBAC permissions\n   - Set up secret management and rotation\n   - Configure security scanning and compliance monitoring\n\n9. **CI/CD Integration**\n   - Set up automated Kubernetes deployment pipelines\n   - Configure GitOps workflows with ArgoCD or Flux\n   - Set up automated testing in Kubernetes environments\n   - Configure blue-green and canary deployment strategies\n   - Set up rollback and disaster recovery procedures\n\n10. **Operations and Maintenance**\n    - Set up cluster maintenance and update procedures\n    - Configure backup and disaster recovery strategies\n    - Set up cost optimization and resource management\n    - Create operational runbooks and troubleshooting guides\n    - Train team on Kubernetes operations and best practices\n    - Set up cluster lifecycle management and governance"
              }
            ],
            "skills": []
          },
          {
            "name": "commands-code-analysis-testing",
            "description": "Commands for code review, testing, and analysis",
            "source": "./plugins/commands-code-analysis-testing",
            "category": "commands",
            "version": "1.0.0",
            "author": {
              "name": "BuildWithClaude Community",
              "url": "https://github.com/davepoon/buildwithclaude"
            },
            "install_commands": [
              "/plugin marketplace add davepoon/buildwithclaude",
              "/plugin install commands-code-analysis-testing@buildwithclaude"
            ],
            "signals": {
              "stars": 2229,
              "forks": 239,
              "pushed_at": "2026-01-12T10:19:01Z",
              "created_at": "2025-07-25T02:26:45Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/add-mutation-testing",
                "description": "Setup mutation testing for code quality",
                "path": "plugins/commands-code-analysis-testing/commands/add-mutation-testing.md",
                "frontmatter": {
                  "description": "Setup mutation testing for code quality",
                  "category": "code-analysis-testing"
                },
                "content": "# Add Mutation Testing\n\nSetup mutation testing for code quality\n\n## Instructions\n\n1. **Mutation Testing Strategy Analysis**\n   - Analyze current test suite coverage and quality\n   - Identify critical code paths and business logic for mutation testing\n   - Assess existing testing infrastructure and CI/CD integration points\n   - Determine mutation testing scope and performance requirements\n   - Plan mutation testing integration with existing quality gates\n\n2. **Mutation Testing Tool Selection**\n   - Choose appropriate mutation testing framework:\n     - **JavaScript/TypeScript**: Stryker, Mutode\n     - **Java**: PIT (Pitest), Major\n     - **C#**: Stryker.NET, VisualMutator\n     - **Python**: mutmut, Cosmic Ray, MutPy\n     - **Go**: go-mutesting, mut\n     - **Rust**: mutagen, cargo-mutants\n     - **PHP**: Infection\n   - Consider factors: language support, performance, CI integration, reporting\n\n3. **Mutation Testing Configuration**\n   - Install and configure mutation testing framework\n   - Set up mutation testing configuration files and settings\n   - Configure mutation operators and strategies\n   - Set up file and directory inclusion/exclusion rules\n   - Configure performance and timeout settings\n\n4. **Mutation Operator Configuration**\n   - Configure arithmetic operator mutations (+, -, *, /, %)\n   - Set up relational operator mutations (<, >, <=, >=, ==, !=)\n   - Configure logical operator mutations (&&, ||, !)\n   - Set up conditional boundary mutations (< to <=, > to >=)\n   - Configure statement deletion and insertion mutations\n\n5. **Test Execution and Performance**\n   - Configure mutation test execution strategy and parallelization\n   - Set up incremental mutation testing for large codebases\n   - Configure mutation testing timeouts and resource limits\n   - Set up mutation test caching and optimization\n   - Configure selective mutation testing for changed code\n\n6. **Quality Metrics and Thresholds**\n   - Set up mutation score calculation and reporting\n   - Configure mutation testing thresholds and quality gates\n   - Set up mutation survival analysis and reporting\n   - Configure test effectiveness metrics and tracking\n   - Set up mutation testing trend analysis\n\n7. **Integration with Testing Workflow**\n   - Integrate mutation testing with existing test suites\n   - Configure mutation testing execution order and dependencies\n   - Set up mutation testing in development and CI environments\n   - Configure mutation testing result integration with test reports\n   - Set up mutation testing feedback loops for developers\n\n8. **CI/CD Pipeline Integration**\n   - Configure automated mutation testing in continuous integration\n   - Set up mutation testing scheduling and triggers\n   - Configure mutation testing result reporting and notifications\n   - Set up mutation testing performance monitoring\n   - Configure mutation testing deployment gates\n\n9. **Result Analysis and Remediation**\n   - Set up mutation testing result analysis and visualization\n   - Configure surviving mutant analysis and categorization\n   - Set up test gap identification and remediation workflow\n   - Configure mutation testing regression tracking\n   - Set up automated test improvement recommendations\n\n10. **Maintenance and Optimization**\n    - Create mutation testing maintenance and optimization procedures\n    - Set up mutation testing configuration version control\n    - Configure mutation testing performance optimization\n    - Document mutation testing best practices and guidelines\n    - Train team on mutation testing concepts and workflow\n    - Set up mutation testing tool updates and maintenance"
              },
              {
                "name": "/add-property-based-testing",
                "description": "Implement property-based testing framework",
                "path": "plugins/commands-code-analysis-testing/commands/add-property-based-testing.md",
                "frontmatter": {
                  "description": "Implement property-based testing framework",
                  "category": "code-analysis-testing"
                },
                "content": "# Add Property-Based Testing\n\nImplement property-based testing framework\n\n## Instructions\n\n1. **Property-Based Testing Analysis**\n   - Analyze current codebase to identify functions suitable for property-based testing\n   - Identify mathematical properties, invariants, and business rules to test\n   - Assess existing testing infrastructure and integration requirements\n   - Determine scope of property-based testing implementation\n   - Plan integration with existing unit and integration tests\n\n2. **Framework Selection and Installation**\n   - Choose appropriate property-based testing framework:\n     - **JavaScript/TypeScript**: fast-check, JSVerify\n     - **Python**: Hypothesis, QuickCheck\n     - **Java**: jqwik, QuickTheories\n     - **C#**: FsCheck, CsCheck\n     - **Rust**: proptest, quickcheck\n     - **Go**: gopter, quick\n   - Install framework and configure with existing test runner\n   - Set up framework integration with build system\n\n3. **Property Definition and Implementation**\n   - Define mathematical properties and invariants for core functions\n   - Implement property tests for data transformation functions\n   - Create property tests for API contract validation\n   - Set up property tests for business logic validation\n   - Define properties for data structure consistency\n\n4. **Test Data Generation**\n   - Configure generators for primitive data types\n   - Create custom generators for domain-specific objects\n   - Set up composite generators for complex data structures\n   - Configure generator constraints and boundaries\n   - Implement shrinking strategies for minimal failing examples\n\n5. **Property Test Categories**\n   - **Roundtrip Properties**: Serialize/deserialize, encode/decode operations\n   - **Invariant Properties**: Data structure consistency, business rule validation\n   - **Metamorphic Properties**: Equivalent operations, transformation consistency\n   - **Model-Based Properties**: State machine testing, system behavior validation\n   - **Oracle Properties**: Comparison with reference implementations\n\n6. **Integration with Existing Tests**\n   - Integrate property-based tests with existing test suites\n   - Configure test execution order and dependencies\n   - Set up property test reporting and coverage tracking\n   - Configure test timeout and resource management\n   - Implement property test categorization and tagging\n\n7. **Advanced Testing Strategies**\n   - Set up stateful property testing for complex systems\n   - Configure model-based testing for state machines\n   - Implement targeted property testing for known issues\n   - Set up regression property testing for bug prevention\n   - Configure performance property testing for algorithmic validation\n\n8. **Test Configuration and Tuning**\n   - Configure test case generation limits and timeouts\n   - Set up shrinking parameters and strategies\n   - Configure random seed management for reproducibility\n   - Set up test distribution and statistical analysis\n   - Configure parallel test execution and resource management\n\n9. **CI/CD Integration**\n   - Configure property-based tests in continuous integration\n   - Set up test result reporting and failure analysis\n   - Configure test execution policies and resource limits\n   - Set up automated property test maintenance\n   - Configure property test performance monitoring\n\n10. **Documentation and Team Training**\n    - Create comprehensive property-based testing documentation\n    - Document property definition patterns and best practices\n    - Create examples and templates for common property patterns\n    - Train team on property-based testing concepts and implementation\n    - Set up property test maintenance and evolution guidelines\n    - Document troubleshooting procedures for property test failures"
              },
              {
                "name": "/check",
                "description": "Run project checks and fix any errors without committing",
                "path": "plugins/commands-code-analysis-testing/commands/check.md",
                "frontmatter": {
                  "description": "Run project checks and fix any errors without committing",
                  "category": "code-analysis-testing",
                  "allowed-tools": "Bash, Edit, Read"
                },
                "content": "Run project validation checks and resolve any errors found.\n\n## Process:\n\n1. **Detect Package Manager** (for JavaScript/TypeScript projects):\n   - npm: Look for package-lock.json\n   - pnpm: Look for pnpm-lock.yaml\n   - yarn: Look for yarn.lock\n   - bun: Look for bun.lockb\n\n2. **Check Available Scripts**:\n   - Read package.json to find check/validation scripts\n   - Common script names: `check`, `validate`, `verify`, `test`, `lint`\n\n3. **Run Appropriate Check Command**:\n   - JavaScript/TypeScript:\n     - npm: `npm run check` or `npm test`\n     - pnpm: `pnpm check` or `pnpm test`\n     - yarn: `yarn check` or `yarn test`\n     - bun: `bun check` or `bun test`\n   \n   - Other languages:\n     - Python: `pytest`, `flake8`, `mypy`, or `make check`\n     - Go: `go test ./...` or `golangci-lint run`\n     - Rust: `cargo check` or `cargo test`\n     - Ruby: `rubocop` or `rake test`\n\n4. **Fix Any Errors**:\n   - Analyze error output\n   - Fix code issues, syntax errors, or test failures\n   - Re-run checks after fixing\n\n5. **Important Constraints**:\n   - DO NOT commit any code\n   - DO NOT change version numbers\n   - Only fix errors to make checks pass\n\nIf no check script exists, run the most appropriate validation for the project type."
              },
              {
                "name": "/clean",
                "description": "Fix all linting and formatting issues across the codebase",
                "path": "plugins/commands-code-analysis-testing/commands/clean.md",
                "frontmatter": {
                  "description": "Fix all linting and formatting issues across the codebase",
                  "category": "code-analysis-testing",
                  "allowed-tools": "Bash, Edit, Read, Glob"
                },
                "content": "Fix all linting, formatting, and static analysis issues in the entire codebase.\n\n## Process:\n\n1. **Detect Project Language(s)**:\n   - Check file extensions and configuration files\n   - Common indicators:\n     - Python: .py files, requirements.txt, pyproject.toml\n     - JavaScript/TypeScript: .js/.ts files, package.json\n     - Go: .go files, go.mod\n     - Rust: .rs files, Cargo.toml\n     - Java: .java files, pom.xml\n     - Ruby: .rb files, Gemfile\n\n2. **Run Language-Specific Linters**:\n\n   **Python:**\n   - Formatting: `black .` or `autopep8`\n   - Import sorting: `isort .`\n   - Linting: `flake8` or `pylint`\n   - Type checking: `mypy`\n   \n   **JavaScript/TypeScript:**\n   - Linting: `eslint . --fix`\n   - Formatting: `prettier --write .`\n   - Type checking: `tsc --noEmit`\n   \n   **Go:**\n   - Formatting: `go fmt ./...`\n   - Linting: `golangci-lint run --fix`\n   \n   **Rust:**\n   - Formatting: `cargo fmt`\n   - Linting: `cargo clippy --fix`\n   \n   **Java:**\n   - Formatting: `google-java-format` or `spotless`\n   - Linting: `checkstyle` or `spotbugs`\n   \n   **Ruby:**\n   - Linting/Formatting: `rubocop -a`\n\n3. **Check for Project Scripts**:\n   - Look for lint/format scripts in package.json, Makefile, etc.\n   - Common script names: `lint`, `format`, `fix`, `clean`\n\n4. **Fix Issues**:\n   - Apply auto-fixes where available\n   - Manually fix issues that can't be auto-fixed\n   - Re-run linters to verify all issues are resolved\n\n5. **Verify Clean State**:\n   - Run all linters again without fix flags\n   - Ensure no errors or warnings remain\n\nFix all issues found until the codebase passes all linting and formatting checks."
              },
              {
                "name": "/code_analysis",
                "description": "Perform comprehensive code analysis with quality metrics and recommendations",
                "path": "plugins/commands-code-analysis-testing/commands/code_analysis.md",
                "frontmatter": {
                  "description": "Perform comprehensive code analysis with quality metrics and recommendations",
                  "category": "code-analysis-testing",
                  "argument-hint": "[file-or-directory-path]",
                  "allowed-tools": "Read, Grep, Glob, TodoWrite"
                },
                "content": "Perform a comprehensive code analysis on the specified files or directory. If no path is provided, analyze the current working directory.\n\n## Analysis Process:\n\n1. **Parse Arguments**:\n   - Extract the path from $ARGUMENTS (defaults to current directory if not specified)\n   - Determine scope: single file, multiple files, or entire directory\n\n2. **Language Detection**:\n   - Identify programming language(s) based on file extensions\n   - Apply language-specific analysis rules\n\n3. **Code Quality Analysis**:\n   - **Complexity Metrics**: Cyclomatic complexity, nesting depth, function length\n   - **Code Smells**: Long methods, large classes, duplicate code patterns\n   - **Best Practices**: Naming conventions, code organization, documentation\n   - **Security Issues**: Common vulnerabilities, unsafe patterns, input validation\n   - **Performance**: Inefficient algorithms, memory leaks, blocking operations\n   - **Maintainability**: Code coupling, cohesion, test coverage indicators\n\n4. **Generate Report**:\n   - Summary with overall health score\n   - Detailed findings by category\n   - Priority-ranked issues (High/Medium/Low)\n   - Specific file and line references\n   - Actionable recommendations for improvement\n\n5. **Track with TodoWrite**:\n   - Create todos for high-priority issues found\n   - Organize by fix complexity and impact\n\n## Example Usage:\n- `/code_analysis` - Analyze entire current directory\n- `/code_analysis src/` - Analyze all code in src directory\n- `/code_analysis app.js` - Analyze specific file\n- `/code_analysis \"src/**/*.py\"` - Analyze all Python files in src\n\nTarget path: $ARGUMENTS"
              },
              {
                "name": "/e2e-setup",
                "description": "Configure end-to-end testing suite",
                "path": "plugins/commands-code-analysis-testing/commands/e2e-setup.md",
                "frontmatter": {
                  "description": "Configure end-to-end testing suite",
                  "category": "code-analysis-testing",
                  "argument-hint": "1. **Technology Stack Assessment**",
                  "allowed-tools": "Bash(npm *)"
                },
                "content": "# End-to-End Testing Setup Command\n\nConfigure end-to-end testing suite\n\n## Instructions\n\nFollow this systematic approach to implement E2E testing: **$ARGUMENTS**\n\n1. **Technology Stack Assessment**\n   - Identify the application type (web app, mobile app, API service)\n   - Review existing testing infrastructure\n   - Determine target browsers and devices\n   - Assess current deployment and staging environments\n\n2. **E2E Framework Selection**\n   - Choose appropriate E2E testing framework based on stack:\n     - **Playwright**: Modern, fast, supports multiple browsers\n     - **Cypress**: Developer-friendly, great debugging tools\n     - **Selenium WebDriver**: Cross-browser, mature ecosystem\n     - **Puppeteer**: Chrome-focused, good for performance testing\n     - **TestCafe**: No WebDriver needed, easy setup\n   - Consider team expertise and project requirements\n\n3. **Test Environment Setup**\n   - Set up dedicated testing environments (staging, QA)\n   - Configure test databases with sample data\n   - Set up environment variables and configuration\n   - Ensure environment isolation and reproducibility\n\n4. **Framework Installation and Configuration**\n   \n   **For Playwright:**\n   ```bash\n   npm install -D @playwright/test\n   npx playwright install\n   npx playwright codegen # Record tests\n   ```\n\n   **For Cypress:**\n   ```bash\n   npm install -D cypress\n   npx cypress open\n   ```\n\n   **For Selenium:**\n   ```bash\n   npm install -D selenium-webdriver\n   # Install browser drivers\n   ```\n\n5. **Test Structure Organization**\n   - Create logical test folder structure:\n     ```\n     e2e/\n      tests/\n         auth/\n         user-flows/\n         api/\n      fixtures/\n      support/\n         commands/\n         page-objects/\n      config/\n     ```\n   - Organize tests by feature or user journey\n   - Separate API tests from UI tests\n\n6. **Page Object Model Implementation**\n   - Create page object classes for better maintainability\n   - Encapsulate element selectors and interactions\n   - Implement reusable methods for common actions\n   - Follow single responsibility principle for page objects\n\n   **Example Page Object:**\n   ```javascript\n   class LoginPage {\n     constructor(page) {\n       this.page = page;\n       this.emailInput = page.locator('#email');\n       this.passwordInput = page.locator('#password');\n       this.loginButton = page.locator('#login-btn');\n     }\n\n     async login(email, password) {\n       await this.emailInput.fill(email);\n       await this.passwordInput.fill(password);\n       await this.loginButton.click();\n     }\n   }\n   ```\n\n7. **Test Data Management**\n   - Create test fixtures and sample data\n   - Implement data factories for dynamic test data\n   - Set up database seeding for consistent test states\n   - Use environment-specific test data\n   - Implement test data cleanup strategies\n\n8. **Core User Journey Testing**\n   - Implement critical user flows:\n     - User registration and authentication\n     - Main application workflows\n     - Payment and transaction flows\n     - Search and filtering functionality\n     - Form submissions and validations\n\n9. **Cross-Browser Testing Setup**\n   - Configure testing across multiple browsers\n   - Set up browser-specific configurations\n   - Implement responsive design testing\n   - Test on different viewport sizes\n\n   **Playwright Browser Configuration:**\n   ```javascript\n   module.exports = {\n     projects: [\n       { name: 'chromium', use: { ...devices['Desktop Chrome'] } },\n       { name: 'firefox', use: { ...devices['Desktop Firefox'] } },\n       { name: 'webkit', use: { ...devices['Desktop Safari'] } },\n       { name: 'mobile', use: { ...devices['iPhone 12'] } },\n     ],\n   };\n   ```\n\n10. **API Testing Integration**\n    - Test API endpoints alongside UI tests\n    - Implement API request/response validation\n    - Test authentication and authorization\n    - Verify data consistency between API and UI\n\n11. **Visual Testing Setup**\n    - Implement screenshot comparison testing\n    - Set up visual regression testing\n    - Configure tolerance levels for visual changes\n    - Organize visual baselines and updates\n\n12. **Test Utilities and Helpers**\n    - Create custom commands and utilities\n    - Implement common assertion helpers\n    - Set up authentication helpers\n    - Create database and state management utilities\n\n13. **Error Handling and Debugging**\n    - Configure proper error reporting and screenshots\n    - Set up video recording for failed tests\n    - Implement retry mechanisms for flaky tests\n    - Create debugging tools and helpers\n\n14. **CI/CD Integration**\n    - Configure E2E tests in CI/CD pipeline\n    - Set up parallel test execution\n    - Implement proper test reporting\n    - Configure test environment provisioning\n\n   **GitHub Actions Example:**\n   ```yaml\n   - name: Run Playwright tests\n     run: npx playwright test\n   - uses: actions/upload-artifact@v3\n     if: always()\n     with:\n       name: playwright-report\n       path: playwright-report/\n   ```\n\n15. **Performance Testing Integration**\n    - Add performance assertions to E2E tests\n    - Monitor page load times and metrics\n    - Test under different network conditions\n    - Implement lighthouse audits integration\n\n16. **Accessibility Testing**\n    - Integrate accessibility testing tools (axe-core)\n    - Test keyboard navigation flows\n    - Verify screen reader compatibility\n    - Check color contrast and WCAG compliance\n\n17. **Mobile Testing Setup**\n    - Configure mobile device emulation\n    - Test responsive design breakpoints\n    - Implement touch gesture testing\n    - Test mobile-specific features\n\n18. **Reporting and Monitoring**\n    - Set up comprehensive test reporting\n    - Configure test result notifications\n    - Implement test metrics and analytics\n    - Create dashboards for test health monitoring\n\n19. **Test Maintenance Strategy**\n    - Implement test stability monitoring\n    - Set up automatic test updates for UI changes\n    - Create test review and update processes\n    - Document test maintenance procedures\n\n20. **Security Testing Integration**\n    - Test authentication and authorization flows\n    - Implement security headers validation\n    - Test input sanitization and XSS prevention\n    - Verify HTTPS and secure cookie handling\n\n**Sample E2E Test:**\n```javascript\ntest('user can complete purchase flow', async ({ page }) => {\n  // Navigate and login\n  await page.goto('/login');\n  await page.fill('#email', 'test@example.com');\n  await page.fill('#password', 'password');\n  await page.click('#login-btn');\n\n  // Add item to cart\n  await page.goto('/products');\n  await page.click('[data-testid=\"product-1\"]');\n  await page.click('#add-to-cart');\n\n  // Complete checkout\n  await page.goto('/checkout');\n  await page.fill('#card-number', '4111111111111111');\n  await page.click('#place-order');\n\n  // Verify success\n  await expect(page.locator('#order-confirmation')).toBeVisible();\n});\n```\n\nRemember to start with critical user journeys and gradually expand coverage. Focus on stable, maintainable tests that provide real value."
              },
              {
                "name": "/generate-test-cases",
                "description": "Generate comprehensive test cases automatically",
                "path": "plugins/commands-code-analysis-testing/commands/generate-test-cases.md",
                "frontmatter": {
                  "description": "Generate comprehensive test cases automatically",
                  "category": "code-analysis-testing",
                  "argument-hint": "Specify test case requirements"
                },
                "content": "# Generate Test Cases\n\nGenerate comprehensive test cases automatically\n\n## Instructions\n\n1. **Target Analysis and Scope Definition**\n   - Parse target file or function from arguments: `$ARGUMENTS`\n   - If no target specified, analyze current directory and prompt for specific target\n   - Examine the target code structure, dependencies, and complexity\n   - Identify function signatures, parameters, return types, and side effects\n   - Determine testing scope (unit, integration, or both)\n\n2. **Code Structure Analysis**\n   - Analyze function logic, branching, and control flow\n   - Identify input validation, error handling, and edge cases\n   - Examine external dependencies, API calls, and database interactions\n   - Review data transformations and business logic\n   - Identify async operations and error scenarios\n\n3. **Test Case Generation Strategy**\n   - Generate positive test cases for normal operation flows\n   - Create negative test cases for error conditions and invalid inputs\n   - Generate edge cases for boundary conditions and limits\n   - Create integration test cases for external dependencies\n   - Generate performance test cases for complex operations\n\n4. **Unit Test Implementation**\n   - Create test file following project naming conventions\n   - Set up test framework imports and configuration\n   - Generate test suites organized by functionality\n   - Create comprehensive test cases with descriptive names\n   - Implement proper setup and teardown for each test\n\n5. **Mock and Stub Generation**\n   - Identify external dependencies requiring mocking\n   - Generate mock implementations for APIs and services\n   - Create stub data for database and file system operations\n   - Set up spy functions for monitoring function calls\n   - Configure mock return values and error scenarios\n\n6. **Data-Driven Test Generation**\n   - Create test data sets for various input scenarios\n   - Generate parameterized tests for multiple input combinations\n   - Create fixtures for complex data structures\n   - Set up test data factories for consistent data generation\n   - Generate property-based test cases for comprehensive coverage\n\n7. **Integration Test Scenarios**\n   - Generate tests for component interactions\n   - Create end-to-end workflow test cases\n   - Generate API integration test scenarios\n   - Create database integration tests with real data\n   - Generate cross-module integration test cases\n\n8. **Error Handling and Exception Testing**\n   - Generate tests for all error conditions and exceptions\n   - Create tests for timeout and network failure scenarios\n   - Generate tests for invalid input validation\n   - Create tests for resource exhaustion and limits\n   - Generate tests for concurrent access and race conditions\n\n9. **Test Quality and Coverage**\n   - Ensure comprehensive code coverage for target functions\n   - Generate tests for all code branches and paths\n   - Create tests for both success and failure scenarios\n   - Validate test assertions are meaningful and specific\n   - Ensure tests are isolated and independent\n\n10. **Test Documentation and Maintenance**\n    - Generate clear test descriptions and documentation\n    - Create comments explaining complex test scenarios\n    - Document test data requirements and setup procedures\n    - Generate test maintenance guidelines and best practices\n    - Create test execution and debugging instructions\n    - Validate generated tests execute successfully and provide meaningful feedback"
              },
              {
                "name": "/generate-tests",
                "description": "Generate comprehensive test suite for $ARGUMENTS following project testing conventions and best practices.",
                "path": "plugins/commands-code-analysis-testing/commands/generate-tests.md",
                "frontmatter": {
                  "description": "Generate comprehensive test suite for $ARGUMENTS following project testing conventions and best practices.",
                  "category": "code-analysis-testing",
                  "argument-hint": "Specify test generation options"
                },
                "content": "# Test Generator\n\nGenerate comprehensive test suite for $ARGUMENTS following project testing conventions and best practices.\n\n## Task\n\nI'll analyze the target code and create complete test coverage including:\n\n1. Unit tests for individual functions and methods\n2. Integration tests for component interactions  \n3. Edge case and error handling tests\n4. Mock implementations for external dependencies\n5. Test utilities and helpers as needed\n6. Performance and snapshot tests where appropriate\n\n## Process\n\nI'll follow these steps:\n\n1. Analyze the target file/component structure\n2. Identify all testable functions, methods, and behaviors\n3. Examine existing test patterns in the project\n4. Create test files following project naming conventions\n5. Implement comprehensive test cases with proper setup/teardown\n6. Add necessary mocks and test utilities\n7. Verify test coverage and add missing test cases\n\n## Test Types\n\n### Unit Tests\n- Individual function testing with various inputs\n- Component rendering and prop handling\n- State management and lifecycle methods\n- Utility function edge cases and error conditions\n\n### Integration Tests\n- Component interaction testing\n- API integration with mocked responses\n- Service layer integration\n- End-to-end user workflows\n\n### Framework-Specific Tests\n- **React**: Component testing with React Testing Library\n- **Vue**: Component testing with Vue Test Utils\n- **Angular**: Component and service testing with TestBed\n- **Node.js**: API endpoint and middleware testing\n\n## Testing Best Practices\n\n### Test Structure\n- Use descriptive test names that explain the behavior\n- Follow AAA pattern (Arrange, Act, Assert)\n- Group related tests with describe blocks\n- Use proper setup and teardown for test isolation\n\n### Mock Strategy\n- Mock external dependencies and API calls\n- Use factories for test data generation\n- Implement proper cleanup for async operations\n- Mock timers and dates for deterministic tests\n\n### Coverage Goals\n- Aim for 80%+ code coverage\n- Focus on critical business logic paths\n- Test both happy path and error scenarios\n- Include boundary value testing\n\nI'll adapt to your project's testing framework (Jest, Vitest, Cypress, etc.) and follow established patterns."
              },
              {
                "name": "/optimize",
                "description": "Analyze code performance and propose three specific optimization improvements",
                "path": "plugins/commands-code-analysis-testing/commands/optimize.md",
                "frontmatter": {
                  "description": "Analyze code performance and propose three specific optimization improvements",
                  "category": "code-analysis-testing",
                  "allowed-tools": "Read, Edit"
                },
                "content": "Analyze the performance of this code and propose three specific optimizations."
              },
              {
                "name": "/repro-issue",
                "description": "Reproduce a specific issue by creating a failing test case",
                "path": "plugins/commands-code-analysis-testing/commands/repro-issue.md",
                "frontmatter": {
                  "description": "Reproduce a specific issue by creating a failing test case",
                  "category": "code-analysis-testing",
                  "argument-hint": "<issue_description>",
                  "allowed-tools": "Read, Write, Edit"
                },
                "content": "Repro issue $ARGUMENTS in a failing test"
              },
              {
                "name": "/setup-comprehensive-testing",
                "description": "Setup complete testing infrastructure",
                "path": "plugins/commands-code-analysis-testing/commands/setup-comprehensive-testing.md",
                "frontmatter": {
                  "description": "Setup complete testing infrastructure",
                  "category": "code-analysis-testing"
                },
                "content": "# Setup Comprehensive Testing\n\nSetup complete testing infrastructure\n\n## Instructions\n\n1. **Testing Strategy Analysis**\n   - Analyze current project structure and identify testing needs\n   - Determine appropriate testing frameworks based on technology stack\n   - Define testing pyramid strategy (unit, integration, e2e, visual)\n   - Plan test coverage goals and quality metrics\n   - Assess existing testing infrastructure and gaps\n\n2. **Unit Testing Framework Setup**\n   - Install and configure primary testing framework (Jest, Vitest, pytest, etc.)\n   - Set up test runner configuration and environment\n   - Configure test file patterns and directory structure\n   - Set up test utilities and helper functions\n   - Configure mocking and stubbing capabilities\n\n3. **Integration Testing Configuration**\n   - Set up integration testing framework and tools\n   - Configure test database and data seeding\n   - Set up API testing with tools like Supertest or requests\n   - Configure service integration testing\n   - Set up component integration testing for frontend\n\n4. **End-to-End Testing Setup**\n   - Install and configure E2E testing framework (Playwright, Cypress, Selenium)\n   - Set up test environment and browser configuration\n   - Create page object models and test helpers\n   - Configure test data management and cleanup\n   - Set up cross-browser and device testing\n\n5. **Visual Testing Integration**\n   - Set up visual regression testing tools (Chromatic, Percy, Playwright)\n   - Configure screenshot comparison and diff detection\n   - Set up visual testing for different viewports and devices\n   - Create visual test baselines and approval workflows\n   - Configure visual testing in CI/CD pipeline\n\n6. **Test Coverage and Reporting**\n   - Configure code coverage collection and reporting\n   - Set up coverage thresholds and quality gates\n   - Configure test result reporting and visualization\n   - Set up test performance monitoring\n   - Configure test report generation and distribution\n\n7. **Performance and Load Testing**\n   - Set up performance testing framework (k6, Artillery, JMeter)\n   - Configure load testing scenarios and benchmarks\n   - Set up performance monitoring and alerting\n   - Configure stress testing and capacity planning\n   - Set up performance regression detection\n\n8. **Test Data Management**\n   - Set up test data factories and fixtures\n   - Configure database seeding and cleanup\n   - Set up test data isolation and parallel test execution\n   - Configure test environment data management\n   - Set up API mocking and service virtualization\n\n9. **CI/CD Integration**\n   - Configure automated test execution in CI/CD pipeline\n   - Set up parallel test execution and optimization\n   - Configure test result reporting and notifications\n   - Set up test environment provisioning and cleanup\n   - Configure deployment gates based on test results\n\n10. **Testing Best Practices and Documentation**\n    - Create comprehensive testing guidelines and standards\n    - Set up test naming conventions and organization\n    - Document testing workflows and procedures\n    - Create testing templates and examples\n    - Set up testing metrics and quality monitoring\n    - Train team on testing best practices and tools"
              },
              {
                "name": "/setup-load-testing",
                "description": "Configure load and performance testing",
                "path": "plugins/commands-code-analysis-testing/commands/setup-load-testing.md",
                "frontmatter": {
                  "description": "Configure load and performance testing",
                  "category": "code-analysis-testing"
                },
                "content": "# Setup Load Testing\n\nConfigure load and performance testing\n\n## Instructions\n\n1. **Load Testing Strategy and Requirements**\n   - Analyze application architecture and identify performance-critical components\n   - Define load testing objectives (capacity planning, performance validation, bottleneck identification)\n   - Determine testing scenarios (normal load, peak load, stress testing, spike testing)\n   - Identify key performance metrics and acceptance criteria\n   - Plan load testing environments and infrastructure requirements\n\n2. **Load Testing Tool Selection**\n   - Choose appropriate load testing tools based on requirements:\n     - **k6**: Modern, developer-friendly with JavaScript scripting\n     - **Artillery**: Simple, powerful, great for CI/CD integration\n     - **JMeter**: Feature-rich GUI and command-line tool\n     - **Gatling**: High-performance tool with detailed reporting\n     - **Locust**: Python-based with web UI and distributed testing\n     - **WebPageTest**: Web performance and real user monitoring\n   - Consider factors: scripting language, reporting, CI integration, cost\n\n3. **Test Environment Setup**\n   - Set up dedicated load testing environment matching production\n   - Configure test data and database setup for consistent testing\n   - Set up network configuration and firewall rules\n   - Configure monitoring and observability for test environment\n   - Set up test isolation and cleanup procedures\n\n4. **Load Test Script Development**\n   - Create test scripts for critical user journeys and API endpoints\n   - Implement realistic user behavior patterns and think times\n   - Set up test data generation and management\n   - Configure authentication and session management\n   - Implement parameterization and data-driven testing\n\n5. **Performance Scenarios Configuration**\n   - **Load Testing**: Normal expected traffic patterns\n   - **Stress Testing**: Beyond normal capacity to find breaking points\n   - **Spike Testing**: Sudden traffic increases and decreases\n   - **Volume Testing**: Large amounts of data processing\n   - **Endurance Testing**: Extended periods under normal load\n   - **Capacity Testing**: Maximum user load determination\n\n6. **Monitoring and Metrics Collection**\n   - Set up application performance monitoring during tests\n   - Configure infrastructure metrics collection (CPU, memory, disk, network)\n   - Set up database performance monitoring and query analysis\n   - Configure real-time dashboards and alerting\n   - Set up log aggregation and error tracking\n\n7. **Test Execution and Automation**\n   - Configure automated test execution and scheduling\n   - Set up test result collection and analysis\n   - Configure test environment provisioning and teardown\n   - Set up parallel and distributed test execution\n   - Configure test result storage and historical tracking\n\n8. **Performance Analysis and Reporting**\n   - Set up automated performance analysis and threshold checking\n   - Configure performance trend analysis and regression detection\n   - Set up detailed performance reporting and visualization\n   - Configure performance alerts and notifications\n   - Set up performance benchmark and baseline management\n\n9. **CI/CD Integration**\n   - Integrate load tests into continuous integration pipeline\n   - Configure performance gates and deployment blocking\n   - Set up automated performance regression detection\n   - Configure test result integration with development workflow\n   - Set up performance testing in staging and pre-production environments\n\n10. **Optimization and Maintenance**\n    - Document load testing procedures and maintenance guidelines\n    - Set up load test script maintenance and version control\n    - Configure test environment maintenance and updates\n    - Create performance optimization recommendations workflow\n    - Train team on load testing best practices and tool usage\n    - Set up performance testing standards and conventions"
              },
              {
                "name": "/setup-visual-testing",
                "description": "Setup visual regression testing",
                "path": "plugins/commands-code-analysis-testing/commands/setup-visual-testing.md",
                "frontmatter": {
                  "description": "Setup visual regression testing",
                  "category": "code-analysis-testing"
                },
                "content": "# Setup Visual Testing\n\nSetup visual regression testing\n\n## Instructions\n\n1. **Visual Testing Strategy Analysis**\n   - Analyze current UI/component structure and testing needs\n   - Identify critical user interfaces and visual components\n   - Determine testing scope (components, pages, user flows)\n   - Assess existing testing infrastructure and integration points\n   - Plan visual testing coverage and baseline creation strategy\n\n2. **Visual Testing Tool Selection**\n   - Evaluate visual testing tools based on project requirements:\n     - **Chromatic**: For Storybook integration and component testing\n     - **Percy**: For comprehensive visual testing and CI integration\n     - **Playwright**: For browser-based visual testing with built-in capabilities\n     - **BackstopJS**: For lightweight visual regression testing\n     - **Applitools**: For AI-powered visual testing and cross-browser support\n   - Consider factors: budget, team size, CI/CD integration, browser support\n\n3. **Visual Testing Framework Installation**\n   - Install chosen visual testing tool and dependencies\n   - Configure testing framework integration (Jest, Playwright, Cypress)\n   - Set up browser automation and screenshot capabilities\n   - Configure testing environment and viewport settings\n   - Set up test runner and execution environment\n\n4. **Baseline Creation and Management**\n   - Create initial visual baselines for all critical UI components\n   - Establish baseline approval workflow and review process\n   - Set up baseline version control and storage\n   - Configure baseline updates and maintenance procedures\n   - Implement baseline branching strategy for feature development\n\n5. **Test Configuration and Setup**\n   - Configure visual testing parameters (viewports, browsers, devices)\n   - Set up visual diff thresholds and sensitivity settings\n   - Configure screenshot capture settings and optimization\n   - Set up test data and state management for consistent testing\n   - Configure async loading and timing handling\n\n6. **Component and Page Testing**\n   - Create visual tests for individual UI components\n   - Set up page-level visual testing for critical user flows\n   - Configure responsive design testing across different viewports\n   - Implement cross-browser visual testing\n   - Set up accessibility and color contrast visual validation\n\n7. **CI/CD Pipeline Integration**\n   - Configure automated visual testing in CI/CD pipeline\n   - Set up visual test execution on pull requests\n   - Configure test result reporting and notifications\n   - Set up deployment blocking for failed visual tests\n   - Implement parallel test execution for performance\n\n8. **Review and Approval Workflow**\n   - Set up visual diff review and approval process\n   - Configure team notifications for visual changes\n   - Establish approval authority and review guidelines\n   - Set up automated approval for minor acceptable changes\n   - Configure change documentation and tracking\n\n9. **Monitoring and Maintenance**\n   - Set up visual test performance monitoring\n   - Configure test flakiness detection and resolution\n   - Implement baseline cleanup and maintenance procedures\n   - Set up visual testing metrics and reporting\n   - Configure alerting for test failures and issues\n\n10. **Documentation and Team Training**\n    - Create comprehensive visual testing documentation\n    - Document baseline creation and update procedures\n    - Create troubleshooting guide for common visual testing issues\n    - Train team on visual testing workflows and best practices\n    - Set up visual testing standards and conventions\n    - Document visual testing maintenance and optimization procedures"
              },
              {
                "name": "/tdd",
                "description": "Test-driven development workflow with Red-Green-Refactor process and branch management",
                "path": "plugins/commands-code-analysis-testing/commands/tdd.md",
                "frontmatter": {
                  "description": "Test-driven development workflow with Red-Green-Refactor process and branch management",
                  "category": "code-analysis-testing",
                  "allowed-tools": "Read, Write, Edit, Bash(git *)"
                },
                "content": "This outlines the development practices and principles we require you to follow. Don't start\nworking on features until asked, this document is intended to get you into the right state\nof mind.\n\n1. Make sure you are on the main branch before you start (unless instructed to start on a specific branch)\n2. Understand the code that is there before you begin to change it.\n3. Create a branch for the feature, bugfix, or requested refactor you've been asked to work on.\n4. Employ test-driven development. Red-Green-Refactor process (outlined below)\n5. When committing to git, omit the Claude footer from comments.\n6. Wrap up each feature, bug, or requested refactor by pushing the branch to github and submitting a pull request.\n7. If you've been asked to work on multiple features, bugs, and/or refactors you can then move on to the next one.\n\n# High-level flow\n\n## One vs many\nSometimes you will be given one task. Sometimes you will be given a task list.\nThe list might be provided as a git repo issue list, for example.\n\nIf you are given many at once, start with the first, and complete them one by one, creating a branch for each and a pull-request when finished.\n\n## Keep notes\nCreate a markdown file under the notes/features/ folder for the feature. If you are creating a feature branch, use the same name.\n\nUse this notes file to record answers to clarifying questions, and other important things as you work on the feature. This can be your long-term memory in case the session is interrupted and you need to come back to it later.\n\nThese are your notes, so feel free to add, modify, re-arrange, and delete content in the notes file.\n\nYou may, if you wish, add other notes that might be helpful to you or future developers, but more isn't always better. Be breif and helpful.\n\n## Understand the feature\n1. First read the README.md and any relevant docs it points to.\n1. Ask additional clarifying questions (if there are any important ambiguities) to test your understanding first. For example,\nif you were asked to write a tic-tac-toe app,"
              },
              {
                "name": "/test-changelog-automation",
                "description": "Automate changelog testing workflow",
                "path": "plugins/commands-code-analysis-testing/commands/test-changelog-automation.md",
                "frontmatter": {
                  "description": "Automate changelog testing workflow",
                  "category": "code-analysis-testing"
                },
                "content": "# Test Command\n\nAutomate changelog testing workflow\n\n## Instructions\n\n1. This command serves as a demonstration\n2. It shows how the changelog automation works\n3. When this file is added, the changelog should update automatically"
              },
              {
                "name": "/test-coverage",
                "description": "Analyze and report test coverage",
                "path": "plugins/commands-code-analysis-testing/commands/test-coverage.md",
                "frontmatter": {
                  "description": "Analyze and report test coverage",
                  "category": "code-analysis-testing",
                  "argument-hint": "1. **Coverage Tool Setup**",
                  "allowed-tools": "Bash(npm *), Write"
                },
                "content": "# Test Coverage Command\n\nAnalyze and report test coverage\n\n## Instructions\n\nFollow this systematic approach to analyze and improve test coverage: **$ARGUMENTS**\n\n1. **Coverage Tool Setup**\n   - Identify and configure appropriate coverage tools:\n     - JavaScript/Node.js: Jest, NYC, Istanbul\n     - Python: Coverage.py, pytest-cov\n     - Java: JaCoCo, Cobertura\n     - C#: dotCover, OpenCover\n     - Ruby: SimpleCov\n   - Configure coverage reporting formats (HTML, XML, JSON)\n   - Set up coverage thresholds and quality gates\n\n2. **Baseline Coverage Analysis**\n   - Run existing tests with coverage reporting\n   - Generate comprehensive coverage reports\n   - Document current coverage percentages:\n     - Line coverage\n     - Branch coverage\n     - Function coverage\n     - Statement coverage\n   - Identify uncovered code areas\n\n3. **Coverage Report Analysis**\n   - Review detailed coverage reports by file and directory\n   - Identify critical uncovered code paths\n   - Analyze branch coverage for conditional logic\n   - Find untested functions and methods\n   - Examine coverage trends over time\n\n4. **Critical Path Identification**\n   - Identify business-critical code that lacks coverage\n   - Prioritize high-risk, low-coverage areas\n   - Focus on public APIs and interfaces\n   - Target error handling and edge cases\n   - Examine security-sensitive code paths\n\n5. **Test Gap Analysis**\n   - Categorize uncovered code:\n     - Business logic requiring immediate testing\n     - Error handling and exception paths\n     - Configuration and setup code\n     - Utility functions and helpers\n     - Dead or obsolete code to remove\n\n6. **Strategic Test Writing**\n   - Write unit tests for uncovered business logic\n   - Add integration tests for uncovered workflows\n   - Create tests for error conditions and edge cases\n   - Test configuration and environment-specific code\n   - Add regression tests for bug-prone areas\n\n7. **Branch Coverage Improvement**\n   - Identify uncovered conditional branches\n   - Test both true and false conditions\n   - Cover all switch/case statements\n   - Test exception handling paths\n   - Verify loop conditions and iterations\n\n8. **Edge Case Testing**\n   - Test boundary conditions and limits\n   - Test null, empty, and invalid inputs\n   - Test timeout and network failure scenarios\n   - Test resource exhaustion conditions\n   - Test concurrent access and race conditions\n\n9. **Mock and Stub Strategy**\n   - Mock external dependencies for better isolation\n   - Stub complex operations to focus on logic\n   - Use dependency injection for testability\n   - Create test doubles for external services\n   - Implement proper cleanup for test resources\n\n10. **Performance Impact Assessment**\n    - Measure test execution time with new tests\n    - Optimize slow tests without losing coverage\n    - Parallelize test execution where possible\n    - Balance coverage goals with execution speed\n    - Consider test categorization (fast/slow, unit/integration)\n\n11. **Coverage Quality Assessment**\n    - Ensure tests actually verify behavior, not just execution\n    - Check for meaningful assertions in tests\n    - Avoid testing implementation details\n    - Focus on testing contracts and interfaces\n    - Review test quality alongside coverage metrics\n\n12. **Framework-Specific Coverage Enhancement**\n    \n    **For Web Applications:**\n    - Test API endpoints and HTTP status codes\n    - Test form validation and user input handling\n    - Test authentication and authorization flows\n    - Test error pages and user feedback\n\n    **For Mobile Applications:**\n    - Test device-specific functionality\n    - Test different screen sizes and orientations\n    - Test offline and network connectivity scenarios\n    - Test platform-specific features\n\n    **For Backend Services:**\n    - Test database operations and transactions\n    - Test message queue processing\n    - Test caching and performance optimizations\n    - Test service integrations and API calls\n\n13. **Continuous Coverage Monitoring**\n    - Set up automated coverage reporting in CI/CD\n    - Configure coverage thresholds to prevent regression\n    - Generate coverage badges and reports\n    - Monitor coverage trends and improvements\n    - Alert on significant coverage decreases\n\n14. **Coverage Exclusion Management**\n    - Properly exclude auto-generated code\n    - Exclude third-party libraries and dependencies\n    - Document reasons for coverage exclusions\n    - Regularly review and update exclusion rules\n    - Avoid excluding code that should be tested\n\n15. **Team Coverage Goals**\n    - Set realistic coverage targets based on project needs\n    - Establish minimum coverage requirements for new code\n    - Create coverage improvement roadmap\n    - Review coverage in code reviews\n    - Celebrate coverage milestones and improvements\n\n16. **Coverage Reporting and Communication**\n    - Generate clear, actionable coverage reports\n    - Create coverage dashboards for stakeholders\n    - Document coverage improvement strategies\n    - Share coverage results with development team\n    - Integrate coverage into project health metrics\n\n17. **Mutation Testing (Advanced)**\n    - Implement mutation testing to validate test quality\n    - Identify tests that don't catch actual bugs\n    - Improve test assertions and edge case coverage\n    - Use mutation testing tools specific to your language\n    - Balance mutation testing cost with quality benefits\n\n18. **Legacy Code Coverage Strategy**\n    - Prioritize high-risk legacy code for testing\n    - Use characterization tests for complex legacy systems\n    - Refactor for testability where possible\n    - Add tests before making changes to legacy code\n    - Document known limitations and technical debt\n\n**Sample Coverage Commands:**\n\n```bash\n# JavaScript with Jest\nnpm test -- --coverage --coverage-reporters=html,text,lcov\n\n# Python with pytest\npytest --cov=src --cov-report=html --cov-report=term\n\n# Java with Maven\nmvn clean test jacoco:report\n\n# .NET Core\ndotnet test --collect:\"XPlat Code Coverage\"\n```\n\nRemember that 100% coverage is not always the goal - focus on meaningful coverage that actually improves code quality and catches bugs."
              },
              {
                "name": "/testing_plan_integration",
                "description": "I need you to create an integration testing plan for $ARGUMENTS",
                "path": "plugins/commands-code-analysis-testing/commands/testing_plan_integration.md",
                "frontmatter": {
                  "description": "I need you to create an integration testing plan for $ARGUMENTS",
                  "category": "code-analysis-testing",
                  "argument-hint": "Specify test plan or integration type"
                },
                "content": "I need you to create an integration testing plan for $ARGUMENTS\n\nThese are integration tests and I want them to be inline in rust fashion.\n\nIf the code is difficult to test, you should suggest refactoring to make it easier to test.\n\nThink really hard about the code, the tests, and the refactoring (if applicable).\n\nWill you come up with test cases and let me review before you write the tests?\n\nFeel free to ask clarifying questions."
              },
              {
                "name": "/write-tests",
                "description": "Write unit and integration tests",
                "path": "plugins/commands-code-analysis-testing/commands/write-tests.md",
                "frontmatter": {
                  "description": "Write unit and integration tests",
                  "category": "code-analysis-testing",
                  "argument-hint": "1. **Test Framework Detection**",
                  "allowed-tools": "Write"
                },
                "content": "# Write Tests Command\n\nWrite unit and integration tests\n\n## Instructions\n\nFollow this systematic approach to write effective tests: **$ARGUMENTS**\n\n1. **Test Framework Detection**\n   - Identify the testing framework in use (Jest, Mocha, PyTest, RSpec, etc.)\n   - Review existing test structure and conventions\n   - Check test configuration files and setup\n   - Understand project-specific testing patterns\n\n2. **Code Analysis for Testing**\n   - Analyze the code that needs testing\n   - Identify public interfaces and critical business logic\n   - Map out dependencies and external interactions\n   - Understand error conditions and edge cases\n\n3. **Test Strategy Planning**\n   - Determine test levels needed:\n     - Unit tests for individual functions/methods\n     - Integration tests for component interactions\n     - End-to-end tests for user workflows\n   - Plan test coverage goals and priorities\n   - Identify mock and stub requirements\n\n4. **Unit Test Implementation**\n   - Test individual functions and methods in isolation\n   - Cover happy path scenarios first\n   - Test edge cases and boundary conditions\n   - Test error conditions and exception handling\n   - Use proper assertions and expectations\n\n5. **Test Structure and Organization**\n   - Follow the AAA pattern (Arrange, Act, Assert)\n   - Use descriptive test names that explain the scenario\n   - Group related tests using test suites/describe blocks\n   - Keep tests focused and atomic\n\n6. **Mocking and Stubbing**\n   - Mock external dependencies and services\n   - Stub complex operations for unit tests\n   - Use proper isolation for reliable tests\n   - Avoid over-mocking that makes tests brittle\n\n7. **Data Setup and Teardown**\n   - Create test fixtures and sample data\n   - Set up and tear down test environments cleanly\n   - Use factories or builders for complex test data\n   - Ensure tests don't interfere with each other\n\n8. **Integration Test Writing**\n   - Test component interactions and data flow\n   - Test API endpoints with various scenarios\n   - Test database operations and transactions\n   - Test external service integrations\n\n9. **Error and Exception Testing**\n   - Test all error conditions and exception paths\n   - Verify proper error messages and codes\n   - Test error recovery and fallback mechanisms\n   - Test validation and security scenarios\n\n10. **Performance and Load Testing**\n    - Add performance tests for critical operations\n    - Test under different load conditions\n    - Verify memory usage and resource cleanup\n    - Test timeout and rate limiting scenarios\n\n11. **Security Testing**\n    - Test authentication and authorization\n    - Test input validation and sanitization\n    - Test for common security vulnerabilities\n    - Test access control and permissions\n\n12. **Accessibility Testing (for UI)**\n    - Test keyboard navigation and screen readers\n    - Test color contrast and visual accessibility\n    - Test ARIA attributes and semantic markup\n    - Test with assistive technology simulations\n\n13. **Cross-Platform Testing**\n    - Test on different operating systems\n    - Test on different browsers (for web apps)\n    - Test on different device sizes and resolutions\n    - Test with different versions of dependencies\n\n14. **Test Utilities and Helpers**\n    - Create reusable test utilities and helpers\n    - Build test data factories and builders\n    - Create custom matchers and assertions\n    - Set up common test setup and teardown functions\n\n15. **Snapshot and Visual Testing**\n    - Use snapshot testing for UI components\n    - Implement visual regression testing\n    - Test rendered output and markup\n    - Version control snapshots properly\n\n16. **Async Testing**\n    - Test asynchronous operations properly\n    - Use appropriate async testing patterns\n    - Test promise resolution and rejection\n    - Test callback and event-driven code\n\n17. **Test Documentation**\n    - Document complex test scenarios and reasoning\n    - Add comments for non-obvious test logic\n    - Create test documentation for team reference\n    - Document test data requirements and setup\n\n18. **Test Maintenance**\n    - Keep tests up to date with code changes\n    - Refactor tests when code is refactored\n    - Remove obsolete tests and update assertions\n    - Monitor and fix flaky tests\n\n**Framework-Specific Guidelines:**\n\n**Jest/JavaScript:**\n```javascript\ndescribe('ComponentName', () => {\n  beforeEach(() => {\n    // Setup\n  });\n\n  it('should handle valid input correctly', () => {\n    // Arrange\n    const input = 'test';\n    // Act\n    const result = functionToTest(input);\n    // Assert\n    expect(result).toBe(expectedValue);\n  });\n});\n```\n\n**PyTest/Python:**\n```python\nclass TestClassName:\n    def setup_method(self):\n        # Setup\n        pass\n\n    def test_should_handle_valid_input(self):\n        # Arrange\n        input_data = \"test\"\n        # Act\n        result = function_to_test(input_data)\n        # Assert\n        assert result == expected_value\n```\n\n**RSpec/Ruby:**\n```ruby\nRSpec.describe ClassName do\n  describe '#method_name' do\n    it 'handles valid input correctly' do\n      # Arrange\n      input = 'test'\n      # Act\n      result = subject.method_name(input)\n      # Assert\n      expect(result).to eq(expected_value)\n    end\n  end\nend\n```\n\nRemember to prioritize testing critical business logic and user-facing functionality first, then expand coverage to supporting code."
              }
            ],
            "skills": []
          },
          {
            "name": "commands-context-loading-priming",
            "description": "Commands for loading context and priming Claude for specific tasks",
            "source": "./plugins/commands-context-loading-priming",
            "category": "commands",
            "version": "1.0.0",
            "author": {
              "name": "BuildWithClaude Community",
              "url": "https://github.com/davepoon/buildwithclaude"
            },
            "install_commands": [
              "/plugin marketplace add davepoon/buildwithclaude",
              "/plugin install commands-context-loading-priming@buildwithclaude"
            ],
            "signals": {
              "stars": 2229,
              "forks": 239,
              "pushed_at": "2026-01-12T10:19:01Z",
              "created_at": "2025-07-25T02:26:45Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/context-prime",
                "description": "Load project context by reading README.md and exploring relevant project files",
                "path": "plugins/commands-context-loading-priming/commands/context-prime.md",
                "frontmatter": {
                  "description": "Load project context by reading README.md and exploring relevant project files",
                  "category": "context-loading-priming",
                  "allowed-tools": "Read, Bash(git *)"
                },
                "content": "Read README.md, THEN run `git ls-files | grep -v -f (sed 's|^|^|; s|$|/|' .cursorignore | psub)` to understand the context of the project"
              },
              {
                "name": "/initref",
                "description": "Build reference documentation by creating markdown files and updating CLAUDE.md",
                "path": "plugins/commands-context-loading-priming/commands/initref.md",
                "frontmatter": {
                  "description": "Build reference documentation by creating markdown files and updating CLAUDE.md",
                  "category": "context-loading-priming",
                  "allowed-tools": "Read, Write, LS, Glob"
                },
                "content": "Build the reference docs. Run /summarize on files to get summaries, don't read too many file contents to avoid burning usage. Read important files directly.\n\nCreate reference markdown files in `/ref` directory.\n\nUpdate `CLAUDE.md` file with pointers to important documentation files."
              },
              {
                "name": "/prime",
                "description": "Load project context by reading key documentation files and exploring project structure",
                "path": "plugins/commands-context-loading-priming/commands/prime.md",
                "frontmatter": {
                  "description": "Load project context by reading key documentation files and exploring project structure",
                  "category": "context-loading-priming",
                  "allowed-tools": "Bash(eza *), Read"
                },
                "content": "# Context Prime\n> Follow the instructions to understand the context of the project.\n\n## Run the following command\n\neza . --tree --git-ignore\n\n## Read the following files\n> Read the files below and nothing else.\n\nREADME.md\n.claude/commands/COMMANDS.md\nai_docs/AI_DOCS.md\nspecs/SPECS.md"
              },
              {
                "name": "/rsi",
                "description": "Read project commands and documentation to optimize AI-assisted development process",
                "path": "plugins/commands-context-loading-priming/commands/rsi.md",
                "frontmatter": {
                  "description": "Read project commands and documentation to optimize AI-assisted development process",
                  "category": "context-loading-priming",
                  "allowed-tools": "Read, LS, Glob"
                },
                "content": "Please list and read all files in `.claude/commands/`, and also the CLAUDE.md, README.md, ROADMAP.md, and PHILOSOPHY.md in project root. Feel free to check out any other files to if useful. Let's see if we can further optimise and streamline this AI-assisted dev process!"
              }
            ],
            "skills": []
          },
          {
            "name": "commands-database-operations",
            "description": "Commands for database schema design, migrations, and optimization",
            "source": "./plugins/commands-database-operations",
            "category": "commands",
            "version": "1.0.0",
            "author": {
              "name": "BuildWithClaude Community",
              "url": "https://github.com/davepoon/buildwithclaude"
            },
            "install_commands": [
              "/plugin marketplace add davepoon/buildwithclaude",
              "/plugin install commands-database-operations@buildwithclaude"
            ],
            "signals": {
              "stars": 2229,
              "forks": 239,
              "pushed_at": "2026-01-12T10:19:01Z",
              "created_at": "2025-07-25T02:26:45Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/create-database-migrations",
                "description": "Create and manage database migrations",
                "path": "plugins/commands-database-operations/commands/create-database-migrations.md",
                "frontmatter": {
                  "description": "Create and manage database migrations",
                  "category": "database-operations",
                  "allowed-tools": "Bash(npm *), Edit"
                },
                "content": "# Create Database Migrations\n\nCreate and manage database migrations\n\n## Instructions\n\n1. **Migration Strategy and Planning**\n   - Analyze current database schema and target changes\n   - Plan migration strategy for zero-downtime deployments\n   - Define rollback procedures and data safety measures\n   - Assess migration complexity and potential risks\n   - Plan for data transformation and validation\n\n2. **Migration Framework Setup**\n   - Set up comprehensive migration framework:\n\n   **Node.js Migration Framework:**\n   ```javascript\n   // migrations/migration-framework.js\n   const fs = require('fs').promises;\n   const path = require('path');\n   const { Pool } = require('pg');\n\n   class MigrationManager {\n     constructor(databaseConfig) {\n       this.pool = new Pool(databaseConfig);\n       this.migrationsDir = path.join(__dirname, 'migrations');\n       this.lockTimeout = 30000; // 30 seconds\n     }\n\n     async initialize() {\n       // Create migrations tracking table\n       await this.pool.query(`\n         CREATE TABLE IF NOT EXISTS schema_migrations (\n           id SERIAL PRIMARY KEY,\n           version VARCHAR(255) UNIQUE NOT NULL,\n           name VARCHAR(255) NOT NULL,\n           executed_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n           execution_time_ms INTEGER,\n           checksum VARCHAR(64),\n           rollback_sql TEXT,\n           batch_number INTEGER\n         );\n         \n         CREATE INDEX IF NOT EXISTS idx_schema_migrations_version \n         ON schema_migrations(version);\n         \n         CREATE INDEX IF NOT EXISTS idx_schema_migrations_batch \n         ON schema_migrations(batch_number);\n       `);\n\n       // Create migration lock table\n       await this.pool.query(`\n         CREATE TABLE IF NOT EXISTS migration_lock (\n           id INTEGER PRIMARY KEY DEFAULT 1,\n           is_locked BOOLEAN DEFAULT FALSE,\n           locked_at TIMESTAMP WITH TIME ZONE,\n           locked_by VARCHAR(255),\n           CHECK (id = 1)\n         );\n         \n         INSERT INTO migration_lock (id, is_locked) \n         VALUES (1, FALSE) \n         ON CONFLICT (id) DO NOTHING;\n       `);\n     }\n\n     async acquireLock(lockId = 'migration') {\n       const client = await this.pool.connect();\n       try {\n         const result = await client.query(`\n           UPDATE migration_lock \n           SET is_locked = TRUE, locked_at = CURRENT_TIMESTAMP, locked_by = $1\n           WHERE id = 1 AND (is_locked = FALSE OR locked_at < CURRENT_TIMESTAMP - INTERVAL '${this.lockTimeout} milliseconds')\n           RETURNING is_locked;\n         `, [lockId]);\n\n         if (result.rows.length === 0) {\n           throw new Error('Could not acquire migration lock - another migration may be running');\n         }\n\n         return client;\n       } catch (error) {\n         client.release();\n         throw error;\n       }\n     }\n\n     async releaseLock(client) {\n       try {\n         await client.query(`\n           UPDATE migration_lock \n           SET is_locked = FALSE, locked_at = NULL, locked_by = NULL \n           WHERE id = 1;\n         `);\n       } finally {\n         client.release();\n       }\n     }\n\n     async getPendingMigrations() {\n       const files = await fs.readdir(this.migrationsDir);\n       const migrationFiles = files\n         .filter(file => file.endsWith('.sql') || file.endsWith('.js'))\n         .sort();\n\n       const executedMigrations = await this.pool.query(\n         'SELECT version FROM schema_migrations ORDER BY version'\n       );\n       const executedVersions = new Set(executedMigrations.rows.map(row => row.version));\n\n       return migrationFiles\n         .map(file => {\n           const version = this.extractVersion(file);\n           return { file, version, executed: executedVersions.has(version) };\n         })\n         .filter(migration => !migration.executed);\n     }\n\n     extractVersion(filename) {\n       const match = filename.match(/^(\\d{14})/);\n       if (!match) {\n         throw new Error(`Invalid migration filename format: ${filename}`);\n       }\n       return match[1];\n     }\n\n     async runMigration(migrationFile) {\n       const version = this.extractVersion(migrationFile.file);\n       const filePath = path.join(this.migrationsDir, migrationFile.file);\n       const startTime = Date.now();\n\n       console.log(`Running migration: ${migrationFile.file}`);\n\n       const client = await this.pool.connect();\n       try {\n         await client.query('BEGIN');\n\n         let migrationContent;\n         let rollbackSql = '';\n\n         if (migrationFile.file.endsWith('.js')) {\n           // JavaScript migration\n           const migration = require(filePath);\n           await migration.up(client);\n           rollbackSql = migration.down ? migration.down.toString() : '';\n         } else {\n           // SQL migration\n           migrationContent = await fs.readFile(filePath, 'utf8');\n           const { upSql, downSql } = this.parseSqlMigration(migrationContent);\n           \n           await client.query(upSql);\n           rollbackSql = downSql;\n         }\n\n         const executionTime = Date.now() - startTime;\n         const checksum = this.generateChecksum(migrationContent || migrationFile.file);\n         const batchNumber = await this.getNextBatchNumber();\n\n         // Record migration execution\n         await client.query(`\n           INSERT INTO schema_migrations (version, name, execution_time_ms, checksum, rollback_sql, batch_number)\n           VALUES ($1, $2, $3, $4, $5, $6)\n         `, [version, migrationFile.file, executionTime, checksum, rollbackSql, batchNumber]);\n\n         await client.query('COMMIT');\n         console.log(` Migration ${migrationFile.file} completed in ${executionTime}ms`);\n\n       } catch (error) {\n         await client.query('ROLLBACK');\n         console.error(` Migration ${migrationFile.file} failed:`, error.message);\n         throw error;\n       } finally {\n         client.release();\n       }\n     }\n\n     parseSqlMigration(content) {\n       const lines = content.split('\\n');\n       let upSql = '';\n       let downSql = '';\n       let currentSection = 'up';\n\n       for (const line of lines) {\n         if (line.trim().startsWith('-- +migrate Down')) {\n           currentSection = 'down';\n           continue;\n         }\n         if (line.trim().startsWith('-- +migrate Up')) {\n           currentSection = 'up';\n           continue;\n         }\n\n         if (currentSection === 'up') {\n           upSql += line + '\\n';\n         } else if (currentSection === 'down') {\n           downSql += line + '\\n';\n         }\n       }\n\n       return { upSql: upSql.trim(), downSql: downSql.trim() };\n     }\n\n     generateChecksum(content) {\n       const crypto = require('crypto');\n       return crypto.createHash('sha256').update(content).digest('hex');\n     }\n\n     async getNextBatchNumber() {\n       const result = await this.pool.query(\n         'SELECT COALESCE(MAX(batch_number), 0) + 1 as next_batch FROM schema_migrations'\n       );\n       return result.rows[0].next_batch;\n     }\n\n     async migrate() {\n       await this.initialize();\n       \n       const client = await this.acquireLock('migration-runner');\n       try {\n         const pendingMigrations = await this.getPendingMigrations();\n         \n         if (pendingMigrations.length === 0) {\n           console.log('No pending migrations');\n           return;\n         }\n\n         console.log(`Found ${pendingMigrations.length} pending migrations`);\n         \n         for (const migration of pendingMigrations) {\n           await this.runMigration(migration);\n         }\n\n         console.log('All migrations completed successfully');\n       } finally {\n         await this.releaseLock(client);\n       }\n     }\n\n     async rollback(steps = 1) {\n       await this.initialize();\n       \n       const client = await this.acquireLock('migration-rollback');\n       try {\n         const lastMigrations = await this.pool.query(`\n           SELECT * FROM schema_migrations \n           ORDER BY executed_at DESC, version DESC \n           LIMIT $1\n         `, [steps]);\n\n         if (lastMigrations.rows.length === 0) {\n           console.log('No migrations to rollback');\n           return;\n         }\n\n         for (const migration of lastMigrations.rows) {\n           await this.rollbackMigration(migration);\n         }\n\n         console.log(`Rolled back ${lastMigrations.rows.length} migrations`);\n       } finally {\n         await this.releaseLock(client);\n       }\n     }\n\n     async rollbackMigration(migration) {\n       console.log(`Rolling back migration: ${migration.name}`);\n       \n       const client = await this.pool.connect();\n       try {\n         await client.query('BEGIN');\n\n         if (migration.rollback_sql) {\n           await client.query(migration.rollback_sql);\n         } else {\n           console.warn(`No rollback SQL available for ${migration.name}`);\n         }\n\n         await client.query(\n           'DELETE FROM schema_migrations WHERE version = $1',\n           [migration.version]\n         );\n\n         await client.query('COMMIT');\n         console.log(` Rolled back migration: ${migration.name}`);\n\n       } catch (error) {\n         await client.query('ROLLBACK');\n         console.error(` Rollback failed for ${migration.name}:`, error.message);\n         throw error;\n       } finally {\n         client.release();\n       }\n     }\n   }\n\n   module.exports = MigrationManager;\n   ```\n\n3. **Migration File Templates**\n   - Create standardized migration templates:\n\n   **SQL Migration Template:**\n   ```sql\n   -- +migrate Up\n   -- Migration: Add user preferences table\n   -- Author: Developer Name\n   -- Date: 2024-01-15\n   -- Description: Create user_preferences table to store user-specific settings\n\n   CREATE TABLE user_preferences (\n     id BIGSERIAL PRIMARY KEY,\n     user_id BIGINT NOT NULL REFERENCES users(id) ON DELETE CASCADE,\n     category VARCHAR(100) NOT NULL,\n     key VARCHAR(100) NOT NULL,\n     value JSONB NOT NULL DEFAULT '{}',\n     created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n     updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n     \n     UNIQUE(user_id, category, key)\n   );\n\n   -- Add indexes for efficient querying\n   CREATE INDEX idx_user_preferences_user_id ON user_preferences(user_id);\n   CREATE INDEX idx_user_preferences_category ON user_preferences(category);\n   CREATE INDEX idx_user_preferences_key ON user_preferences(key);\n\n   -- Add comments for documentation\n   COMMENT ON TABLE user_preferences IS 'User-specific preference settings organized by category';\n   COMMENT ON COLUMN user_preferences.category IS 'Preference category (e.g., notifications, display, privacy)';\n   COMMENT ON COLUMN user_preferences.key IS 'Specific preference key within the category';\n   COMMENT ON COLUMN user_preferences.value IS 'Preference value stored as JSONB for flexibility';\n\n   -- +migrate Down\n   -- Rollback: Remove user preferences table\n\n   DROP TABLE IF EXISTS user_preferences CASCADE;\n   ```\n\n   **JavaScript Migration Template:**\n   ```javascript\n   // migrations/20240115120000_add_user_preferences.js\n   const migration = {\n     name: 'Add user preferences table',\n     description: 'Create user_preferences table for storing user-specific settings',\n     \n     async up(client) {\n       console.log('Creating user_preferences table...');\n       \n       await client.query(`\n         CREATE TABLE user_preferences (\n           id BIGSERIAL PRIMARY KEY,\n           user_id BIGINT NOT NULL REFERENCES users(id) ON DELETE CASCADE,\n           category VARCHAR(100) NOT NULL,\n           key VARCHAR(100) NOT NULL,\n           value JSONB NOT NULL DEFAULT '{}',\n           created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n           updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n           \n           UNIQUE(user_id, category, key)\n         );\n       `);\n\n       await client.query(`\n         CREATE INDEX idx_user_preferences_user_id ON user_preferences(user_id);\n       `);\n\n       await client.query(`\n         CREATE INDEX idx_user_preferences_category ON user_preferences(category);\n       `);\n\n       console.log(' user_preferences table created successfully');\n     },\n\n     async down(client) {\n       console.log('Dropping user_preferences table...');\n       \n       await client.query('DROP TABLE IF EXISTS user_preferences CASCADE;');\n       \n       console.log(' user_preferences table dropped successfully');\n     }\n   };\n\n   module.exports = migration;\n   ```\n\n4. **Advanced Migration Patterns**\n   - Implement complex migration scenarios:\n\n   **Data Migration with Validation:**\n   ```javascript\n   // migrations/20240115130000_migrate_user_settings.js\n   const migration = {\n     name: 'Migrate user settings to new format',\n     description: 'Transform legacy user_settings JSONB column to normalized user_preferences table',\n     \n     async up(client) {\n       console.log('Starting user settings migration...');\n       \n       // Step 1: Create temporary backup\n       await client.query(`\n         CREATE TABLE user_settings_backup AS \n         SELECT * FROM users WHERE settings IS NOT NULL;\n       `);\n       \n       console.log(' Created backup of existing user settings');\n\n       // Step 2: Migrate data in batches\n       const batchSize = 1000;\n       let offset = 0;\n       let processedCount = 0;\n\n       while (true) {\n         const result = await client.query(`\n           SELECT id, settings \n           FROM users \n           WHERE settings IS NOT NULL \n           ORDER BY id \n           LIMIT $1 OFFSET $2\n         `, [batchSize, offset]);\n\n         if (result.rows.length === 0) break;\n\n         for (const user of result.rows) {\n           await this.migrateUserSettings(client, user.id, user.settings);\n           processedCount++;\n         }\n\n         offset += batchSize;\n         console.log(` Processed ${processedCount} users...`);\n       }\n\n       // Step 3: Validate migration\n       const validationResult = await this.validateMigration(client);\n       if (!validationResult.isValid) {\n         throw new Error(`Migration validation failed: ${validationResult.errors.join(', ')}`);\n       }\n\n       console.log(` Successfully migrated ${processedCount} user settings`);\n     },\n\n     async migrateUserSettings(client, userId, settings) {\n       const settingsObj = typeof settings === 'string' ? JSON.parse(settings) : settings;\n       \n       for (const [category, categorySettings] of Object.entries(settingsObj)) {\n         if (typeof categorySettings === 'object') {\n           for (const [key, value] of Object.entries(categorySettings)) {\n             await client.query(`\n               INSERT INTO user_preferences (user_id, category, key, value)\n               VALUES ($1, $2, $3, $4)\n               ON CONFLICT (user_id, category, key) DO UPDATE\n               SET value = $4, updated_at = CURRENT_TIMESTAMP\n             `, [userId, category, key, JSON.stringify(value)]);\n           }\n         } else {\n           // Handle flat settings structure\n           await client.query(`\n             INSERT INTO user_preferences (user_id, category, key, value)\n             VALUES ($1, $2, $3, $4)\n             ON CONFLICT (user_id, category, key) DO UPDATE\n             SET value = $4, updated_at = CURRENT_TIMESTAMP\n           `, [userId, 'general', category, JSON.stringify(categorySettings)]);\n         }\n       }\n     },\n\n     async validateMigration(client) {\n       const errors = [];\n       \n       // Check for data consistency\n       const oldCount = await client.query(\n         'SELECT COUNT(*) FROM users WHERE settings IS NOT NULL'\n       );\n       \n       const newCount = await client.query(\n         'SELECT COUNT(DISTINCT user_id) FROM user_preferences'\n       );\n\n       if (oldCount.rows[0].count !== newCount.rows[0].count) {\n         errors.push(`User count mismatch: ${oldCount.rows[0].count} vs ${newCount.rows[0].count}`);\n       }\n\n       // Check for required preferences\n       const missingPrefs = await client.query(`\n         SELECT u.id FROM users u\n         LEFT JOIN user_preferences up ON u.id = up.user_id\n         WHERE u.settings IS NOT NULL AND up.user_id IS NULL\n       `);\n\n       if (missingPrefs.rows.length > 0) {\n         errors.push(`${missingPrefs.rows.length} users missing preferences`);\n       }\n\n       return {\n         isValid: errors.length === 0,\n         errors\n       };\n     },\n\n     async down(client) {\n       console.log('Rolling back user settings migration...');\n       \n       // Restore from backup\n       await client.query(`\n         UPDATE users \n         SET settings = backup.settings\n         FROM user_settings_backup backup\n         WHERE users.id = backup.id;\n       `);\n       \n       // Clean up\n       await client.query('DELETE FROM user_preferences;');\n       await client.query('DROP TABLE user_settings_backup;');\n       \n       console.log(' Rollback completed');\n     }\n   };\n\n   module.exports = migration;\n   ```\n\n5. **Schema Alteration Migrations**\n   - Handle schema changes safely:\n\n   **Safe Column Addition:**\n   ```sql\n   -- +migrate Up\n   -- Migration: Add email verification tracking\n   -- Safe column addition with default values\n\n   -- Add new columns with safe defaults\n   ALTER TABLE users \n   ADD COLUMN email_verification_token VARCHAR(255),\n   ADD COLUMN email_verification_expires_at TIMESTAMP WITH TIME ZONE,\n   ADD COLUMN email_verification_attempts INTEGER DEFAULT 0;\n\n   -- Add index for token lookup\n   CREATE INDEX CONCURRENTLY idx_users_email_verification_token \n   ON users(email_verification_token) \n   WHERE email_verification_token IS NOT NULL;\n\n   -- Add constraint for expiration logic\n   ALTER TABLE users \n   ADD CONSTRAINT chk_email_verification_expires \n   CHECK (\n     (email_verification_token IS NULL AND email_verification_expires_at IS NULL) OR\n     (email_verification_token IS NOT NULL AND email_verification_expires_at IS NOT NULL)\n   );\n\n   -- +migrate Down\n   -- Remove email verification columns\n\n   DROP INDEX IF EXISTS idx_users_email_verification_token;\n   ALTER TABLE users \n   DROP CONSTRAINT IF EXISTS chk_email_verification_expires,\n   DROP COLUMN IF EXISTS email_verification_token,\n   DROP COLUMN IF EXISTS email_verification_expires_at,\n   DROP COLUMN IF EXISTS email_verification_attempts;\n   ```\n\n   **Safe Table Restructuring:**\n   ```sql\n   -- +migrate Up\n   -- Migration: Split user addresses into separate table\n   -- Zero-downtime table restructuring\n\n   -- Step 1: Create new addresses table\n   CREATE TABLE user_addresses (\n     id BIGSERIAL PRIMARY KEY,\n     user_id BIGINT NOT NULL REFERENCES users(id) ON DELETE CASCADE,\n     type address_type DEFAULT 'shipping',\n     first_name VARCHAR(100),\n     last_name VARCHAR(100),\n     company VARCHAR(255),\n     address_line_1 VARCHAR(255) NOT NULL,\n     address_line_2 VARCHAR(255),\n     city VARCHAR(100) NOT NULL,\n     state VARCHAR(100),\n     postal_code VARCHAR(20),\n     country CHAR(2) NOT NULL DEFAULT 'US',\n     phone VARCHAR(20),\n     is_default BOOLEAN DEFAULT FALSE,\n     created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n     updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP\n   );\n\n   CREATE TYPE address_type AS ENUM ('billing', 'shipping');\n\n   -- Add indexes\n   CREATE INDEX idx_user_addresses_user_id ON user_addresses(user_id);\n   CREATE INDEX idx_user_addresses_type ON user_addresses(type);\n   CREATE UNIQUE INDEX idx_user_addresses_default \n   ON user_addresses(user_id, type) \n   WHERE is_default = TRUE;\n\n   -- Step 2: Migrate existing address data\n   INSERT INTO user_addresses (\n     user_id, type, first_name, last_name, address_line_1, \n     city, state, postal_code, country, is_default\n   )\n   SELECT \n     id, 'shipping', first_name, last_name, address,\n     city, state, postal_code, \n     COALESCE(country, 'US'), TRUE\n   FROM users \n   WHERE address IS NOT NULL;\n\n   -- Step 3: Create view for backward compatibility\n   CREATE VIEW users_with_address AS\n   SELECT \n     u.*,\n     ua.address_line_1 as address,\n     ua.city,\n     ua.state,\n     ua.postal_code,\n     ua.country\n   FROM users u\n   LEFT JOIN user_addresses ua ON u.id = ua.user_id AND ua.is_default = TRUE AND ua.type = 'shipping';\n\n   -- Step 4: Add trigger to maintain view consistency\n   CREATE OR REPLACE FUNCTION sync_user_address()\n   RETURNS TRIGGER AS $$\n   BEGIN\n     IF TG_OP = 'UPDATE' THEN\n       -- Update default shipping address\n       UPDATE user_addresses \n       SET \n         address_line_1 = NEW.address,\n         city = NEW.city,\n         state = NEW.state,\n         postal_code = NEW.postal_code,\n         country = NEW.country,\n         updated_at = CURRENT_TIMESTAMP\n       WHERE user_id = NEW.id AND type = 'shipping' AND is_default = TRUE;\n       \n       RETURN NEW;\n     END IF;\n     RETURN NULL;\n   END;\n   $$ LANGUAGE plpgsql;\n\n   CREATE TRIGGER trigger_sync_user_address\n   AFTER UPDATE ON users\n   FOR EACH ROW\n   WHEN (OLD.address IS DISTINCT FROM NEW.address OR \n         OLD.city IS DISTINCT FROM NEW.city OR\n         OLD.state IS DISTINCT FROM NEW.state OR\n         OLD.postal_code IS DISTINCT FROM NEW.postal_code OR\n         OLD.country IS DISTINCT FROM NEW.country)\n   EXECUTE FUNCTION sync_user_address();\n\n   -- +migrate Down\n   -- Restore original structure\n\n   DROP TRIGGER IF EXISTS trigger_sync_user_address ON users;\n   DROP FUNCTION IF EXISTS sync_user_address();\n   DROP VIEW IF EXISTS users_with_address;\n   DROP TABLE IF EXISTS user_addresses CASCADE;\n   DROP TYPE IF EXISTS address_type;\n   ```\n\n6. **Migration Testing Framework**\n   - Test migrations thoroughly:\n\n   **Migration Test Suite:**\n   ```javascript\n   // tests/migration-tests.js\n   const { Pool } = require('pg');\n   const MigrationManager = require('../migrations/migration-framework');\n\n   class MigrationTester {\n     constructor() {\n       this.testDbConfig = {\n         host: process.env.TEST_DB_HOST || 'localhost',\n         port: process.env.TEST_DB_PORT || 5432,\n         database: process.env.TEST_DB_NAME || 'test_db',\n         user: process.env.TEST_DB_USER || 'postgres',\n         password: process.env.TEST_DB_PASSWORD || 'password'\n       };\n       \n       this.pool = new Pool(this.testDbConfig);\n       this.migrationManager = new MigrationManager(this.testDbConfig);\n     }\n\n     async setupTestDatabase() {\n       // Create fresh test database\n       const adminPool = new Pool({\n         ...this.testDbConfig,\n         database: 'postgres'\n       });\n\n       try {\n         await adminPool.query(`DROP DATABASE IF EXISTS ${this.testDbConfig.database}`);\n         await adminPool.query(`CREATE DATABASE ${this.testDbConfig.database}`);\n         console.log(' Test database created');\n       } finally {\n         await adminPool.end();\n       }\n     }\n\n     async teardownTestDatabase() {\n       await this.pool.end();\n       \n       const adminPool = new Pool({\n         ...this.testDbConfig,\n         database: 'postgres'\n       });\n\n       try {\n         await adminPool.query(`DROP DATABASE IF EXISTS ${this.testDbConfig.database}`);\n         console.log(' Test database cleaned up');\n       } finally {\n         await adminPool.end();\n       }\n     }\n\n     async testMigrationUpDown(migrationFile) {\n       console.log(`Testing migration: ${migrationFile}`);\n       \n       try {\n         // Test migration up\n         const startTime = Date.now();\n         await this.migrationManager.runMigration({ file: migrationFile });\n         const upTime = Date.now() - startTime;\n         \n         console.log(` Migration up completed in ${upTime}ms`);\n\n         // Verify migration was recorded\n         const migrationRecord = await this.pool.query(\n           'SELECT * FROM schema_migrations WHERE name = $1',\n           [migrationFile]\n         );\n         \n         if (migrationRecord.rows.length === 0) {\n           throw new Error('Migration not recorded in schema_migrations table');\n         }\n\n         // Test migration down\n         const rollbackStartTime = Date.now();\n         await this.migrationManager.rollbackMigration(migrationRecord.rows[0]);\n         const downTime = Date.now() - rollbackStartTime;\n         \n         console.log(` Migration down completed in ${downTime}ms`);\n\n         // Verify migration was removed\n         const afterRollback = await this.pool.query(\n           'SELECT * FROM schema_migrations WHERE name = $1',\n           [migrationFile]\n         );\n         \n         if (afterRollback.rows.length > 0) {\n           throw new Error('Migration not removed after rollback');\n         }\n\n         return {\n           success: true,\n           upTime,\n           downTime,\n           migrationFile\n         };\n\n       } catch (error) {\n         console.error(` Migration test failed: ${error.message}`);\n         return {\n           success: false,\n           error: error.message,\n           migrationFile\n         };\n       }\n     }\n\n     async testDataIntegrity(testData) {\n       console.log('Testing data integrity...');\n       \n       // Insert test data\n       const insertResults = [];\n       for (const table of Object.keys(testData)) {\n         for (const record of testData[table]) {\n           try {\n             const columns = Object.keys(record);\n             const values = Object.values(record);\n             const placeholders = values.map((_, i) => `$${i + 1}`).join(', ');\n             \n             const result = await this.pool.query(\n               `INSERT INTO ${table} (${columns.join(', ')}) VALUES (${placeholders}) RETURNING id`,\n               values\n             );\n             \n             insertResults.push({\n               table,\n               id: result.rows[0].id,\n               success: true\n             });\n           } catch (error) {\n             insertResults.push({\n               table,\n               success: false,\n               error: error.message\n             });\n           }\n         }\n       }\n\n       return insertResults;\n     }\n\n     async testPerformance(queries) {\n       console.log('Testing query performance...');\n       \n       const performanceResults = [];\n       \n       for (const query of queries) {\n         const startTime = process.hrtime.bigint();\n         \n         try {\n           const result = await this.pool.query(query.sql, query.params || []);\n           const endTime = process.hrtime.bigint();\n           const duration = Number(endTime - startTime) / 1000000; // Convert to milliseconds\n           \n           performanceResults.push({\n             name: query.name,\n             duration,\n             rowCount: result.rows.length,\n             success: true\n           });\n           \n           if (duration > (query.maxDuration || 1000)) {\n             console.warn(` Query ${query.name} took ${duration}ms (expected < ${query.maxDuration || 1000}ms)`);\n           }\n           \n         } catch (error) {\n           performanceResults.push({\n             name: query.name,\n             success: false,\n             error: error.message\n           });\n         }\n       }\n\n       return performanceResults;\n     }\n\n     async runFullTestSuite() {\n       console.log('Starting migration test suite...');\n       \n       await this.setupTestDatabase();\n       await this.migrationManager.initialize();\n       \n       try {\n         const testResults = {\n           migrations: [],\n           dataIntegrity: [],\n           performance: [],\n           summary: { passed: 0, failed: 0 }\n         };\n\n         // Test all migration files\n         const migrationFiles = await this.migrationManager.getPendingMigrations();\n         \n         for (const migration of migrationFiles) {\n           const result = await this.testMigrationUpDown(migration.file);\n           testResults.migrations.push(result);\n           \n           if (result.success) {\n             testResults.summary.passed++;\n           } else {\n             testResults.summary.failed++;\n           }\n         }\n\n         console.log('\\n Test Results Summary:');\n         console.log(` Passed: ${testResults.summary.passed}`);\n         console.log(` Failed: ${testResults.summary.failed}`);\n         console.log(` Success Rate: ${(testResults.summary.passed / (testResults.summary.passed + testResults.summary.failed) * 100).toFixed(1)}%`);\n\n         return testResults;\n\n       } finally {\n         await this.teardownTestDatabase();\n       }\n     }\n   }\n\n   module.exports = MigrationTester;\n\n   // CLI usage\n   if (require.main === module) {\n     const tester = new MigrationTester();\n     tester.runFullTestSuite()\n       .then(results => {\n         console.log('\\nTest suite completed');\n         process.exit(results.summary.failed > 0 ? 1 : 0);\n       })\n       .catch(error => {\n         console.error('Test suite failed:', error);\n         process.exit(1);\n       });\n   }\n   ```\n\n7. **Production Migration Safety**\n   - Implement production-safe migration practices:\n\n   **Safe Production Migration:**\n   ```javascript\n   // migrations/production-safety.js\n   class ProductionMigrationSafety {\n     static async validateProductionMigration(migrationFile, pool) {\n       const safety = new ProductionMigrationSafety(pool);\n       \n       const checks = [\n         safety.checkTableLocks.bind(safety),\n         safety.checkDataSize.bind(safety),\n         safety.checkDependencies.bind(safety),\n         safety.checkBackupStatus.bind(safety),\n         safety.checkMaintenanceWindow.bind(safety)\n       ];\n\n       const results = [];\n       for (const check of checks) {\n         const result = await check(migrationFile);\n         results.push(result);\n         \n         if (!result.passed && result.blocking) {\n           throw new Error(`Migration blocked: ${result.message}`);\n         }\n       }\n\n       return results;\n     }\n\n     constructor(pool) {\n       this.pool = pool;\n     }\n\n     async checkTableLocks(migrationFile) {\n       // Check for long-running transactions that might block migration\n       const longTransactions = await this.pool.query(`\n         SELECT \n           pid,\n           now() - pg_stat_activity.query_start AS duration,\n           query,\n           state\n         FROM pg_stat_activity \n         WHERE (now() - pg_stat_activity.query_start) > interval '5 minutes'\n         AND state IN ('active', 'idle in transaction');\n       `);\n\n       return {\n         name: 'table_locks',\n         passed: longTransactions.rows.length === 0,\n         blocking: true,\n         message: longTransactions.rows.length > 0 \n           ? `${longTransactions.rows.length} long-running transactions detected`\n           : 'No blocking transactions found',\n         details: longTransactions.rows\n       };\n     }\n\n     async checkDataSize(migrationFile) {\n       // Estimate migration impact based on data size\n       const tableSizes = await this.pool.query(`\n         SELECT \n           schemaname,\n           tablename,\n           pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size,\n           pg_total_relation_size(schemaname||'.'||tablename) as size_bytes\n         FROM pg_tables \n         WHERE schemaname = 'public'\n         ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;\n       `);\n\n       const largeTables = tableSizes.rows.filter(table => table.size_bytes > 1000000000); // > 1GB\n\n       return {\n         name: 'data_size',\n         passed: largeTables.length < 5,\n         blocking: false,\n         message: `${largeTables.length} tables > 1GB found`,\n         details: largeTables\n       };\n     }\n\n     async checkDependencies(migrationFile) {\n       // Check for dependent applications or services\n       const activeConnections = await this.pool.query(`\n         SELECT \n           application_name,\n           COUNT(*) as connection_count,\n           COUNT(*) FILTER (WHERE state = 'active') as active_count\n         FROM pg_stat_activity \n         WHERE datname = current_database()\n         AND application_name IS NOT NULL\n         GROUP BY application_name\n         ORDER BY connection_count DESC;\n       `);\n\n       const highUsage = activeConnections.rows.filter(app => app.active_count > 10);\n\n       return {\n         name: 'dependencies',\n         passed: highUsage.length === 0,\n         blocking: false,\n         message: highUsage.length > 0 \n           ? `${highUsage.length} applications with high database usage`\n           : 'Database usage within acceptable limits',\n         details: activeConnections.rows\n       };\n     }\n\n     async checkBackupStatus(migrationFile) {\n       // Verify recent backup exists\n       const lastBackup = await this.pool.query(`\n         SELECT \n           pg_last_wal_receive_lsn(),\n           pg_last_wal_replay_lsn(),\n           EXTRACT(EPOCH FROM (now() - pg_stat_file('base/backup_label', true).modification))::int as backup_age_seconds\n         WHERE pg_stat_file('base/backup_label', true) IS NOT NULL;\n       `);\n\n       const backupExists = lastBackup.rows.length > 0;\n       const backupAge = backupExists ? lastBackup.rows[0].backup_age_seconds : null;\n       const isRecentBackup = backupAge !== null && backupAge < 86400; // 24 hours\n\n       return {\n         name: 'backup_status',\n         passed: isRecentBackup,\n         blocking: true,\n         message: isRecentBackup \n           ? `Recent backup available (${Math.round(backupAge / 3600)} hours old)`\n           : 'No recent backup found - backup required before migration',\n         details: { backupExists, backupAge }\n       };\n     }\n\n     async checkMaintenanceWindow(migrationFile) {\n       // Check if we're in approved maintenance window\n       const now = new Date();\n       const hour = now.getUTCHours();\n       const dayOfWeek = now.getUTCDay();\n       \n       // Define maintenance windows (UTC)\n       const maintenanceWindows = [\n         { days: [0, 6], startHour: 2, endHour: 6 }, // Weekend early morning\n         { days: [1, 2, 3, 4, 5], startHour: 3, endHour: 5 } // Weekday early morning\n       ];\n\n       const inMaintenanceWindow = maintenanceWindows.some(window => \n         window.days.includes(dayOfWeek) && \n         hour >= window.startHour && \n         hour < window.endHour\n       );\n\n       return {\n         name: 'maintenance_window',\n         passed: inMaintenanceWindow,\n         blocking: false,\n         message: inMaintenanceWindow \n           ? 'Currently in maintenance window'\n           : `Outside maintenance window (current UTC hour: ${hour})`,\n         details: { currentHour: hour, dayOfWeek, maintenanceWindows }\n       };\n     }\n   }\n\n   module.exports = ProductionMigrationSafety;\n   ```\n\n8. **Migration Monitoring and Alerting**\n   - Monitor migration execution:\n\n   **Migration Monitoring:**\n   ```javascript\n   // migrations/migration-monitor.js\n   class MigrationMonitor {\n     constructor(alertService) {\n       this.alertService = alertService;\n       this.metrics = {\n         executionTimes: [],\n         errorCounts: {},\n         successCounts: {}\n       };\n     }\n\n     async monitorMigration(migrationName, migrationFn) {\n       const startTime = Date.now();\n       const memoryBefore = process.memoryUsage();\n       \n       try {\n         console.log(` Starting migration: ${migrationName}`);\n         \n         const result = await migrationFn();\n         \n         const endTime = Date.now();\n         const duration = endTime - startTime;\n         const memoryAfter = process.memoryUsage();\n         \n         // Record success metrics\n         this.recordSuccess(migrationName, duration, memoryAfter.heapUsed - memoryBefore.heapUsed);\n         \n         // Alert on long-running migrations\n         if (duration > 300000) { // 5 minutes\n           await this.alertService.sendAlert({\n             type: 'warning',\n             title: 'Long-running migration',\n             message: `Migration ${migrationName} took ${duration}ms to complete`,\n             severity: duration > 600000 ? 'high' : 'medium'\n           });\n         }\n\n         console.log(` Migration completed: ${migrationName} (${duration}ms)`);\n         return result;\n\n       } catch (error) {\n         const duration = Date.now() - startTime;\n         \n         // Record error metrics\n         this.recordError(migrationName, error, duration);\n         \n         // Send error alert\n         await this.alertService.sendAlert({\n           type: 'error',\n           title: 'Migration failed',\n           message: `Migration ${migrationName} failed: ${error.message}`,\n           severity: 'critical',\n           details: {\n             migrationName,\n             duration,\n             error: error.message,\n             stack: error.stack\n           }\n         });\n\n         console.error(` Migration failed: ${migrationName}`, error);\n         throw error;\n       }\n     }\n\n     recordSuccess(migrationName, duration, memoryDelta) {\n       this.metrics.executionTimes.push({\n         migration: migrationName,\n         duration,\n         memoryDelta,\n         timestamp: new Date()\n       });\n       \n       this.metrics.successCounts[migrationName] = \n         (this.metrics.successCounts[migrationName] || 0) + 1;\n     }\n\n     recordError(migrationName, error, duration) {\n       this.metrics.errorCounts[migrationName] = \n         (this.metrics.errorCounts[migrationName] || 0) + 1;\n\n       // Log detailed error information\n       console.error('Migration Error Details:', {\n         migration: migrationName,\n         duration,\n         error: error.message,\n         stack: error.stack,\n         timestamp: new Date()\n       });\n     }\n\n     getMetrics() {\n       return {\n         averageExecutionTime: this.calculateAverageExecutionTime(),\n         totalMigrations: this.metrics.executionTimes.length,\n         successRate: this.calculateSuccessRate(),\n         errorCounts: this.metrics.errorCounts,\n         recentMigrations: this.metrics.executionTimes.slice(-10)\n       };\n     }\n\n     calculateAverageExecutionTime() {\n       if (this.metrics.executionTimes.length === 0) return 0;\n       \n       const total = this.metrics.executionTimes.reduce((sum, record) => sum + record.duration, 0);\n       return Math.round(total / this.metrics.executionTimes.length);\n     }\n\n     calculateSuccessRate() {\n       const totalSuccess = Object.values(this.metrics.successCounts).reduce((sum, count) => sum + count, 0);\n       const totalErrors = Object.values(this.metrics.errorCounts).reduce((sum, count) => sum + count, 0);\n       const total = totalSuccess + totalErrors;\n       \n       return total > 0 ? (totalSuccess / total * 100).toFixed(2) : 100;\n     }\n   }\n\n   module.exports = MigrationMonitor;\n   ```\n\n9. **Migration CLI Tools**\n   - Create comprehensive CLI interface:\n\n   **Migration CLI:**\n   ```javascript\n   #!/usr/bin/env node\n   // bin/migrate.js\n   const yargs = require('yargs');\n   const MigrationManager = require('../migrations/migration-framework');\n   const MigrationTester = require('../tests/migration-tests');\n   const MigrationMonitor = require('../migrations/migration-monitor');\n\n   const dbConfig = {\n     host: process.env.DB_HOST || 'localhost',\n     port: process.env.DB_PORT || 5432,\n     database: process.env.DB_NAME || 'myapp',\n     user: process.env.DB_USER || 'postgres',\n     password: process.env.DB_PASSWORD\n   };\n\n   const migrationManager = new MigrationManager(dbConfig);\n\n   yargs\n     .command('up', 'Run pending migrations', {}, async () => {\n       try {\n         await migrationManager.migrate();\n         console.log(' Migrations completed successfully');\n         process.exit(0);\n       } catch (error) {\n         console.error(' Migration failed:', error.message);\n         process.exit(1);\n       }\n     })\n     .command('down [steps]', 'Rollback migrations', {\n       steps: {\n         describe: 'Number of migrations to rollback',\n         type: 'number',\n         default: 1\n       }\n     }, async (argv) => {\n       try {\n         await migrationManager.rollback(argv.steps);\n         console.log(` Rolled back ${argv.steps} migration(s)`);\n         process.exit(0);\n       } catch (error) {\n         console.error(' Rollback failed:', error.message);\n         process.exit(1);\n       }\n     })\n     .command('status', 'Show migration status', {}, async () => {\n       try {\n         const pending = await migrationManager.getPendingMigrations();\n         const executed = await migrationManager.pool.query(\n           'SELECT version, name, executed_at FROM schema_migrations ORDER BY executed_at DESC'\n         );\n\n         console.log('\\n Migration Status:');\n         console.log(` Executed: ${executed.rows.length}`);\n         console.log(` Pending: ${pending.length}`);\n         \n         if (pending.length > 0) {\n           console.log('\\n Pending Migrations:');\n           pending.forEach(m => console.log(`  - ${m.file}`));\n         }\n         \n         if (executed.rows.length > 0) {\n           console.log('\\n Recent Migrations:');\n           executed.rows.slice(0, 5).forEach(m => \n             console.log(`  - ${m.name} (${m.executed_at.toISOString()})`)\n           );\n         }\n         \n         process.exit(0);\n       } catch (error) {\n         console.error(' Status check failed:', error.message);\n         process.exit(1);\n       }\n     })\n     .command('test', 'Test migrations', {}, async () => {\n       try {\n         const tester = new MigrationTester();\n         const results = await tester.runFullTestSuite();\n         \n         if (results.summary.failed > 0) {\n           console.error(` ${results.summary.failed} migration tests failed`);\n           process.exit(1);\n         } else {\n           console.log(` All ${results.summary.passed} migration tests passed`);\n           process.exit(0);\n         }\n       } catch (error) {\n         console.error(' Migration testing failed:', error.message);\n         process.exit(1);\n       }\n     })\n     .command('create <name>', 'Create new migration file', {\n       name: {\n         describe: 'Migration name',\n         type: 'string',\n         demandOption: true\n       }\n     }, async (argv) => {\n       try {\n         const timestamp = new Date().toISOString().replace(/[-:T]/g, '').slice(0, 14);\n         const filename = `${timestamp}_${argv.name.replace(/[^a-zA-Z0-9]/g, '_')}.sql`;\n         const filepath = path.join(__dirname, '../migrations', filename);\n         \n         const template = `-- +migrate Up\n-- Migration: ${argv.name}\n-- Author: ${process.env.USER || 'Unknown'}\n-- Date: ${new Date().toISOString().split('T')[0]}\n-- Description: [Add description here]\n\n-- Add your migration SQL here\n\n-- +migrate Down\n-- Rollback: ${argv.name}\n\n-- Add your rollback SQL here\n`;\n\n         await fs.writeFile(filepath, template);\n         console.log(` Created migration file: ${filename}`);\n         console.log(` Edit the file at: ${filepath}`);\n         process.exit(0);\n       } catch (error) {\n         console.error(' Failed to create migration:', error.message);\n         process.exit(1);\n       }\n     })\n     .demandCommand()\n     .help()\n     .argv;\n   ```\n\n10. **Production Deployment Integration**\n    - Integrate with deployment pipelines:\n\n    **CI/CD Integration:**\n    ```yaml\n    # .github/workflows/database-migration.yml\n    name: Database Migration\n\n    on:\n      push:\n        branches: [main]\n        paths: ['migrations/**']\n      \n    jobs:\n      test-migrations:\n        runs-on: ubuntu-latest\n        services:\n          postgres:\n            image: postgres:13\n            env:\n              POSTGRES_PASSWORD: postgres\n              POSTGRES_DB: test_db\n            options: >-\n              --health-cmd pg_isready\n              --health-interval 10s\n              --health-timeout 5s\n              --health-retries 5\n\n        steps:\n          - uses: actions/checkout@v2\n          \n          - name: Setup Node.js\n            uses: actions/setup-node@v2\n            with:\n              node-version: '16'\n              \n          - name: Install dependencies\n            run: npm ci\n            \n          - name: Test migrations\n            env:\n              TEST_DB_HOST: localhost\n              TEST_DB_PORT: 5432\n              TEST_DB_NAME: test_db\n              TEST_DB_USER: postgres\n              TEST_DB_PASSWORD: postgres\n            run: npm run migrate:test\n            \n          - name: Check migration safety\n            run: npm run migrate:safety-check\n            \n      deploy-migrations:\n        needs: test-migrations\n        runs-on: ubuntu-latest\n        if: github.ref == 'refs/heads/main'\n        \n        steps:\n          - uses: actions/checkout@v2\n          \n          - name: Setup Node.js\n            uses: actions/setup-node@v2\n            with:\n              node-version: '16'\n              \n          - name: Install dependencies\n            run: npm ci\n            \n          - name: Run production migrations\n            env:\n              DB_HOST: ${{ secrets.PROD_DB_HOST }}\n              DB_PORT: ${{ secrets.PROD_DB_PORT }}\n              DB_NAME: ${{ secrets.PROD_DB_NAME }}\n              DB_USER: ${{ secrets.PROD_DB_USER }}\n              DB_PASSWORD: ${{ secrets.PROD_DB_PASSWORD }}\n            run: |\n              npm run migrate:production:safety-check\n              npm run migrate:up\n              \n          - name: Verify deployment\n            env:\n              DB_HOST: ${{ secrets.PROD_DB_HOST }}\n              DB_PORT: ${{ secrets.PROD_DB_PORT }}\n              DB_NAME: ${{ secrets.PROD_DB_NAME }}\n              DB_USER: ${{ secrets.PROD_DB_USER }}\n              DB_PASSWORD: ${{ secrets.PROD_DB_PASSWORD }}\n            run: npm run migrate:verify\n    ```"
              },
              {
                "name": "/design-database-schema",
                "description": "Design optimized database schemas",
                "path": "plugins/commands-database-operations/commands/design-database-schema.md",
                "frontmatter": {
                  "description": "Design optimized database schemas",
                  "category": "database-operations"
                },
                "content": "# Design Database Schema\n\nDesign optimized database schemas\n\n## Instructions\n\n1. **Requirements Analysis and Data Modeling**\n   - Analyze business requirements and data relationships\n   - Identify entities, attributes, and relationships\n   - Define data types, constraints, and validation rules\n   - Plan for scalability and future requirements\n   - Consider data access patterns and query requirements\n\n2. **Entity Relationship Design**\n   - Create comprehensive entity relationship diagrams:\n\n   **User Management Schema:**\n   ```sql\n   -- Users table with proper indexing and constraints\n   CREATE TABLE users (\n     id BIGSERIAL PRIMARY KEY,\n     email VARCHAR(255) UNIQUE NOT NULL,\n     username VARCHAR(50) UNIQUE NOT NULL,\n     password_hash VARCHAR(255) NOT NULL,\n     first_name VARCHAR(100) NOT NULL,\n     last_name VARCHAR(100) NOT NULL,\n     phone VARCHAR(20),\n     date_of_birth DATE,\n     email_verified BOOLEAN DEFAULT FALSE,\n     phone_verified BOOLEAN DEFAULT FALSE,\n     status user_status DEFAULT 'active',\n     created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n     updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n     last_login_at TIMESTAMP WITH TIME ZONE,\n     deleted_at TIMESTAMP WITH TIME ZONE,\n     \n     -- Constraints\n     CONSTRAINT users_email_format CHECK (email ~* '^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}$'),\n     CONSTRAINT users_username_format CHECK (username ~* '^[a-zA-Z0-9_]{3,50}$'),\n     CONSTRAINT users_names_not_empty CHECK (LENGTH(TRIM(first_name)) > 0 AND LENGTH(TRIM(last_name)) > 0)\n   );\n\n   -- User status enum\n   CREATE TYPE user_status AS ENUM ('active', 'inactive', 'suspended', 'pending_verification');\n\n   -- User profiles table for extended information\n   CREATE TABLE user_profiles (\n     user_id BIGINT PRIMARY KEY REFERENCES users(id) ON DELETE CASCADE,\n     avatar_url VARCHAR(500),\n     bio TEXT,\n     website VARCHAR(255),\n     location VARCHAR(255),\n     timezone VARCHAR(50) DEFAULT 'UTC',\n     language VARCHAR(10) DEFAULT 'en',\n     notification_preferences JSONB DEFAULT '{}',\n     privacy_settings JSONB DEFAULT '{}',\n     created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n     updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP\n   );\n\n   -- User roles and permissions\n   CREATE TABLE roles (\n     id SERIAL PRIMARY KEY,\n     name VARCHAR(50) UNIQUE NOT NULL,\n     description TEXT,\n     permissions JSONB DEFAULT '[]',\n     created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP\n   );\n\n   CREATE TABLE user_roles (\n     user_id BIGINT REFERENCES users(id) ON DELETE CASCADE,\n     role_id INTEGER REFERENCES roles(id) ON DELETE CASCADE,\n     assigned_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n     assigned_by BIGINT REFERENCES users(id),\n     PRIMARY KEY (user_id, role_id)\n   );\n   ```\n\n   **E-commerce Schema Example:**\n   ```sql\n   -- Categories with hierarchical structure\n   CREATE TABLE categories (\n     id SERIAL PRIMARY KEY,\n     name VARCHAR(255) NOT NULL,\n     slug VARCHAR(255) UNIQUE NOT NULL,\n     description TEXT,\n     parent_id INTEGER REFERENCES categories(id),\n     sort_order INTEGER DEFAULT 0,\n     is_active BOOLEAN DEFAULT TRUE,\n     meta_title VARCHAR(255),\n     meta_description TEXT,\n     created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n     updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP\n   );\n\n   -- Products table with comprehensive attributes\n   CREATE TABLE products (\n     id BIGSERIAL PRIMARY KEY,\n     name VARCHAR(255) NOT NULL,\n     slug VARCHAR(255) UNIQUE NOT NULL,\n     sku VARCHAR(100) UNIQUE NOT NULL,\n     description TEXT,\n     short_description TEXT,\n     price DECIMAL(10,2) NOT NULL CHECK (price >= 0),\n     compare_price DECIMAL(10,2) CHECK (compare_price >= price),\n     cost_price DECIMAL(10,2) CHECK (cost_price >= 0),\n     weight DECIMAL(8,2),\n     dimensions JSONB, -- {length: x, width: y, height: z, unit: 'cm'}\n     category_id INTEGER REFERENCES categories(id),\n     brand_id INTEGER REFERENCES brands(id),\n     vendor_id BIGINT REFERENCES vendors(id),\n     status product_status DEFAULT 'draft',\n     visibility product_visibility DEFAULT 'visible',\n     inventory_tracking BOOLEAN DEFAULT TRUE,\n     inventory_quantity INTEGER DEFAULT 0,\n     low_stock_threshold INTEGER DEFAULT 5,\n     allow_backorder BOOLEAN DEFAULT FALSE,\n     requires_shipping BOOLEAN DEFAULT TRUE,\n     is_digital BOOLEAN DEFAULT FALSE,\n     tax_class VARCHAR(50) DEFAULT 'standard',\n     featured BOOLEAN DEFAULT FALSE,\n     tags TEXT[],\n     attributes JSONB DEFAULT '{}',\n     seo_title VARCHAR(255),\n     seo_description TEXT,\n     created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n     updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n     published_at TIMESTAMP WITH TIME ZONE,\n     \n     -- Full text search\n     search_vector tsvector GENERATED ALWAYS AS (\n       to_tsvector('english', COALESCE(name, '') || ' ' || COALESCE(description, '') || ' ' || COALESCE(sku, ''))\n     ) STORED\n   );\n\n   -- Product status and visibility enums\n   CREATE TYPE product_status AS ENUM ('draft', 'active', 'inactive', 'archived');\n   CREATE TYPE product_visibility AS ENUM ('visible', 'hidden', 'catalog_only', 'search_only');\n\n   -- Orders table with comprehensive tracking\n   CREATE TABLE orders (\n     id BIGSERIAL PRIMARY KEY,\n     order_number VARCHAR(50) UNIQUE NOT NULL,\n     user_id BIGINT REFERENCES users(id),\n     status order_status DEFAULT 'pending',\n     currency CHAR(3) DEFAULT 'USD',\n     subtotal DECIMAL(10,2) NOT NULL DEFAULT 0,\n     tax_total DECIMAL(10,2) NOT NULL DEFAULT 0,\n     shipping_total DECIMAL(10,2) NOT NULL DEFAULT 0,\n     discount_total DECIMAL(10,2) NOT NULL DEFAULT 0,\n     total DECIMAL(10,2) NOT NULL DEFAULT 0,\n     \n     -- Billing information\n     billing_first_name VARCHAR(100),\n     billing_last_name VARCHAR(100),\n     billing_company VARCHAR(255),\n     billing_address_line_1 VARCHAR(255),\n     billing_address_line_2 VARCHAR(255),\n     billing_city VARCHAR(100),\n     billing_state VARCHAR(100),\n     billing_postal_code VARCHAR(20),\n     billing_country CHAR(2),\n     billing_phone VARCHAR(20),\n     \n     -- Shipping information\n     shipping_first_name VARCHAR(100),\n     shipping_last_name VARCHAR(100),\n     shipping_company VARCHAR(255),\n     shipping_address_line_1 VARCHAR(255),\n     shipping_address_line_2 VARCHAR(255),\n     shipping_city VARCHAR(100),\n     shipping_state VARCHAR(100),\n     shipping_postal_code VARCHAR(20),\n     shipping_country CHAR(2),\n     shipping_phone VARCHAR(20),\n     shipping_method VARCHAR(100),\n     tracking_number VARCHAR(255),\n     \n     notes TEXT,\n     internal_notes TEXT,\n     created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n     updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n     shipped_at TIMESTAMP WITH TIME ZONE,\n     delivered_at TIMESTAMP WITH TIME ZONE\n   );\n\n   CREATE TYPE order_status AS ENUM (\n     'pending', 'processing', 'shipped', 'delivered', \n     'cancelled', 'refunded', 'on_hold'\n   );\n\n   -- Order items with detailed tracking\n   CREATE TABLE order_items (\n     id BIGSERIAL PRIMARY KEY,\n     order_id BIGINT REFERENCES orders(id) ON DELETE CASCADE,\n     product_id BIGINT REFERENCES products(id),\n     product_variant_id BIGINT REFERENCES product_variants(id),\n     quantity INTEGER NOT NULL CHECK (quantity > 0),\n     unit_price DECIMAL(10,2) NOT NULL,\n     total_price DECIMAL(10,2) NOT NULL,\n     product_name VARCHAR(255) NOT NULL, -- Snapshot at time of order\n     product_sku VARCHAR(100), -- Snapshot at time of order\n     product_attributes JSONB, -- Snapshot of selected variants\n     created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP\n   );\n   ```\n\n3. **Advanced Schema Patterns**\n   - Implement complex data patterns:\n\n   **Audit Trail Pattern:**\n   ```sql\n   -- Generic audit trail for tracking all changes\n   CREATE TABLE audit_log (\n     id BIGSERIAL PRIMARY KEY,\n     table_name VARCHAR(255) NOT NULL,\n     record_id BIGINT NOT NULL,\n     operation audit_operation NOT NULL,\n     old_values JSONB,\n     new_values JSONB,\n     changed_fields TEXT[],\n     user_id BIGINT REFERENCES users(id),\n     ip_address INET,\n     user_agent TEXT,\n     created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n     \n     -- Index for efficient querying\n     INDEX idx_audit_log_table_record (table_name, record_id),\n     INDEX idx_audit_log_user_time (user_id, created_at),\n     INDEX idx_audit_log_operation_time (operation, created_at)\n   );\n\n   CREATE TYPE audit_operation AS ENUM ('INSERT', 'UPDATE', 'DELETE');\n\n   -- Trigger function for automatic audit logging\n   CREATE OR REPLACE FUNCTION audit_trigger_function()\n   RETURNS TRIGGER AS $$\n   DECLARE\n     old_data JSONB;\n     new_data JSONB;\n     changed_fields TEXT[];\n   BEGIN\n     IF TG_OP = 'DELETE' THEN\n       old_data = to_jsonb(OLD);\n       INSERT INTO audit_log (table_name, record_id, operation, old_values, user_id)\n       VALUES (TG_TABLE_NAME, OLD.id, 'DELETE', old_data, COALESCE(current_setting('app.current_user_id', true)::BIGINT, NULL));\n       RETURN OLD;\n     ELSIF TG_OP = 'UPDATE' THEN\n       old_data = to_jsonb(OLD);\n       new_data = to_jsonb(NEW);\n       \n       -- Find changed fields\n       SELECT array_agg(key) INTO changed_fields\n       FROM jsonb_each(old_data) \n       WHERE key IN (SELECT key FROM jsonb_each(new_data))\n       AND value IS DISTINCT FROM (new_data->key);\n       \n       INSERT INTO audit_log (table_name, record_id, operation, old_values, new_values, changed_fields, user_id)\n       VALUES (TG_TABLE_NAME, NEW.id, 'UPDATE', old_data, new_data, changed_fields, COALESCE(current_setting('app.current_user_id', true)::BIGINT, NULL));\n       RETURN NEW;\n     ELSIF TG_OP = 'INSERT' THEN\n       new_data = to_jsonb(NEW);\n       INSERT INTO audit_log (table_name, record_id, operation, new_values, user_id)\n       VALUES (TG_TABLE_NAME, NEW.id, 'INSERT', new_data, COALESCE(current_setting('app.current_user_id', true)::BIGINT, NULL));\n       RETURN NEW;\n     END IF;\n     RETURN NULL;\n   END;\n   $$ LANGUAGE plpgsql;\n   ```\n\n   **Soft Delete Pattern:**\n   ```sql\n   -- Add soft delete to any table\n   ALTER TABLE users ADD COLUMN deleted_at TIMESTAMP WITH TIME ZONE;\n   ALTER TABLE products ADD COLUMN deleted_at TIMESTAMP WITH TIME ZONE;\n\n   -- Create views that exclude soft-deleted records\n   CREATE VIEW active_users AS\n   SELECT * FROM users WHERE deleted_at IS NULL;\n\n   CREATE VIEW active_products AS\n   SELECT * FROM products WHERE deleted_at IS NULL;\n\n   -- Soft delete function\n   CREATE OR REPLACE FUNCTION soft_delete(table_name TEXT, record_id BIGINT)\n   RETURNS VOID AS $$\n   BEGIN\n     EXECUTE format('UPDATE %I SET deleted_at = CURRENT_TIMESTAMP WHERE id = $1 AND deleted_at IS NULL', table_name)\n     USING record_id;\n   END;\n   $$ LANGUAGE plpgsql;\n\n   -- Restore function\n   CREATE OR REPLACE FUNCTION restore_deleted(table_name TEXT, record_id BIGINT)\n   RETURNS VOID AS $$\n   BEGIN\n     EXECUTE format('UPDATE %I SET deleted_at = NULL WHERE id = $1', table_name)\n     USING record_id;\n   END;\n   $$ LANGUAGE plpgsql;\n   ```\n\n4. **Performance Optimization Schema Design**\n   - Design for optimal query performance:\n\n   **Strategic Indexing:**\n   ```sql\n   -- Single column indexes for frequently queried fields\n   CREATE INDEX CONCURRENTLY idx_users_email ON users(email);\n   CREATE INDEX CONCURRENTLY idx_users_username ON users(username);\n   CREATE INDEX CONCURRENTLY idx_users_status ON users(status) WHERE status != 'active';\n   CREATE INDEX CONCURRENTLY idx_users_created_at ON users(created_at);\n\n   -- Composite indexes for common query patterns\n   CREATE INDEX CONCURRENTLY idx_products_category_status \n   ON products(category_id, status) WHERE status = 'active';\n\n   CREATE INDEX CONCURRENTLY idx_products_featured_category \n   ON products(featured, category_id) WHERE featured = true AND status = 'active';\n\n   CREATE INDEX CONCURRENTLY idx_orders_user_status_date \n   ON orders(user_id, status, created_at);\n\n   -- Partial indexes for specific conditions\n   CREATE INDEX CONCURRENTLY idx_products_low_stock \n   ON products(inventory_quantity) \n   WHERE inventory_tracking = true AND inventory_quantity <= low_stock_threshold;\n\n   -- Functional indexes for text search and computed values\n   CREATE INDEX CONCURRENTLY idx_products_search_vector \n   ON products USING gin(search_vector);\n\n   CREATE INDEX CONCURRENTLY idx_users_full_name_lower \n   ON users(lower(first_name || ' ' || last_name));\n\n   -- JSON/JSONB indexes for flexible data\n   CREATE INDEX CONCURRENTLY idx_user_profiles_notifications \n   ON user_profiles USING gin(notification_preferences);\n\n   CREATE INDEX CONCURRENTLY idx_products_attributes \n   ON products USING gin(attributes);\n   ```\n\n   **Partitioning Strategy:**\n   ```sql\n   -- Partition large tables by date for better performance\n   CREATE TABLE orders_partitioned (\n     LIKE orders INCLUDING ALL\n   ) PARTITION BY RANGE (created_at);\n\n   -- Create monthly partitions\n   CREATE TABLE orders_2024_01 PARTITION OF orders_partitioned\n   FOR VALUES FROM ('2024-01-01') TO ('2024-02-01');\n\n   CREATE TABLE orders_2024_02 PARTITION OF orders_partitioned\n   FOR VALUES FROM ('2024-02-01') TO ('2024-03-01');\n\n   -- Automatic partition management\n   CREATE OR REPLACE FUNCTION create_monthly_partitions(\n     table_name TEXT,\n     start_date DATE,\n     end_date DATE\n   )\n   RETURNS VOID AS $$\n   DECLARE\n     current_date DATE := start_date;\n     partition_name TEXT;\n     next_date DATE;\n   BEGIN\n     WHILE current_date < end_date LOOP\n       next_date := current_date + INTERVAL '1 month';\n       partition_name := table_name || '_' || to_char(current_date, 'YYYY_MM');\n       \n       EXECUTE format('CREATE TABLE IF NOT EXISTS %I PARTITION OF %I FOR VALUES FROM (%L) TO (%L)',\n         partition_name, table_name, current_date, next_date);\n       \n       current_date := next_date;\n     END LOOP;\n   END;\n   $$ LANGUAGE plpgsql;\n\n   -- Schedule partition creation\n   SELECT create_monthly_partitions('orders_partitioned', '2024-01-01'::DATE, '2025-01-01'::DATE);\n   ```\n\n5. **Data Integrity and Constraints**\n   - Implement comprehensive data validation:\n\n   **Advanced Constraints:**\n   ```sql\n   -- Complex check constraints\n   ALTER TABLE products ADD CONSTRAINT products_price_logic \n   CHECK (\n     CASE \n       WHEN compare_price IS NOT NULL THEN price <= compare_price\n       ELSE true\n     END\n   );\n\n   ALTER TABLE products ADD CONSTRAINT products_inventory_logic\n   CHECK (\n     CASE \n       WHEN inventory_tracking = false THEN inventory_quantity IS NULL\n       WHEN inventory_tracking = true THEN inventory_quantity >= 0\n       ELSE true\n     END\n   );\n\n   -- Custom domain types for reusable validation\n   CREATE DOMAIN email_address AS VARCHAR(255)\n   CHECK (VALUE ~* '^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}$');\n\n   CREATE DOMAIN phone_number AS VARCHAR(20)\n   CHECK (VALUE ~* '^\\+?[\\d\\s\\-\\(\\)]{10,20}$');\n\n   CREATE DOMAIN positive_decimal AS DECIMAL(10,2)\n   CHECK (VALUE >= 0);\n\n   -- Use domains in table definitions\n   CREATE TABLE contacts (\n     id BIGSERIAL PRIMARY KEY,\n     email email_address NOT NULL,\n     phone phone_number,\n     balance positive_decimal DEFAULT 0\n   );\n\n   -- Foreign key constraints with cascading options\n   ALTER TABLE order_items \n   ADD CONSTRAINT fk_order_items_order \n   FOREIGN KEY (order_id) REFERENCES orders(id) ON DELETE CASCADE;\n\n   ALTER TABLE order_items \n   ADD CONSTRAINT fk_order_items_product \n   FOREIGN KEY (product_id) REFERENCES products(id) ON DELETE RESTRICT;\n\n   -- Unique constraints for business logic\n   ALTER TABLE user_roles \n   ADD CONSTRAINT unique_user_role_active \n   UNIQUE (user_id, role_id);\n\n   -- Exclusion constraints for complex business rules\n   ALTER TABLE product_promotions \n   ADD CONSTRAINT no_overlapping_promotions \n   EXCLUDE USING gist (\n     product_id WITH =,\n     daterange(start_date, end_date, '[]') WITH &&\n   );\n   ```\n\n6. **Temporal Data and Versioning**\n   - Handle time-based data requirements:\n\n   **Temporal Tables:**\n   ```sql\n   -- Product price history tracking\n   CREATE TABLE product_price_history (\n     id BIGSERIAL PRIMARY KEY,\n     product_id BIGINT REFERENCES products(id) ON DELETE CASCADE,\n     price DECIMAL(10,2) NOT NULL,\n     compare_price DECIMAL(10,2),\n     effective_from TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT CURRENT_TIMESTAMP,\n     effective_to TIMESTAMP WITH TIME ZONE,\n     created_by BIGINT REFERENCES users(id),\n     reason TEXT,\n     \n     -- Ensure no overlapping periods\n     EXCLUDE USING gist (\n       product_id WITH =,\n       tstzrange(effective_from, effective_to, '[)') WITH &&\n     )\n   );\n\n   -- Function to get current price\n   CREATE OR REPLACE FUNCTION get_current_price(p_product_id BIGINT)\n   RETURNS DECIMAL(10,2) AS $$\n   DECLARE\n     current_price DECIMAL(10,2);\n   BEGIN\n     SELECT price INTO current_price\n     FROM product_price_history\n     WHERE product_id = p_product_id\n     AND effective_from <= CURRENT_TIMESTAMP\n     AND (effective_to IS NULL OR effective_to > CURRENT_TIMESTAMP)\n     ORDER BY effective_from DESC\n     LIMIT 1;\n     \n     RETURN current_price;\n   END;\n   $$ LANGUAGE plpgsql;\n\n   -- Trigger to update price history when product price changes\n   CREATE OR REPLACE FUNCTION update_price_history()\n   RETURNS TRIGGER AS $$\n   BEGIN\n     IF OLD.price IS DISTINCT FROM NEW.price THEN\n       -- Close current price period\n       UPDATE product_price_history \n       SET effective_to = CURRENT_TIMESTAMP\n       WHERE product_id = NEW.id AND effective_to IS NULL;\n       \n       -- Insert new price period\n       INSERT INTO product_price_history (product_id, price, compare_price, created_by)\n       VALUES (NEW.id, NEW.price, NEW.compare_price, \n               COALESCE(current_setting('app.current_user_id', true)::BIGINT, NULL));\n     END IF;\n     \n     RETURN NEW;\n   END;\n   $$ LANGUAGE plpgsql;\n\n   CREATE TRIGGER trigger_product_price_history\n   AFTER UPDATE ON products\n   FOR EACH ROW\n   EXECUTE FUNCTION update_price_history();\n   ```\n\n7. **JSON/NoSQL Integration**\n   - Leverage JSON columns for flexible data:\n\n   **JSONB Schema Design:**\n   ```sql\n   -- Flexible product attributes using JSONB\n   CREATE TABLE product_attributes (\n     product_id BIGINT REFERENCES products(id) ON DELETE CASCADE,\n     attributes JSONB NOT NULL DEFAULT '{}',\n     created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n     updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n     \n     PRIMARY KEY (product_id)\n   );\n\n   -- JSONB indexes for efficient querying\n   CREATE INDEX idx_product_attributes_gin ON product_attributes USING gin(attributes);\n   CREATE INDEX idx_product_attributes_color ON product_attributes USING gin((attributes->'color'));\n   CREATE INDEX idx_product_attributes_size ON product_attributes USING gin((attributes->'size'));\n\n   -- Function to query products by attributes\n   CREATE OR REPLACE FUNCTION find_products_by_attributes(search_attributes JSONB)\n   RETURNS TABLE(product_id BIGINT, product_name VARCHAR, attributes JSONB) AS $$\n   BEGIN\n     RETURN QUERY\n     SELECT p.id, p.name, pa.attributes\n     FROM products p\n     JOIN product_attributes pa ON p.id = pa.product_id\n     WHERE pa.attributes @> search_attributes;\n   END;\n   $$ LANGUAGE plpgsql;\n\n   -- Usage examples:\n   -- SELECT * FROM find_products_by_attributes('{\"color\": \"red\", \"size\": \"large\"}');\n\n   -- Settings table with JSONB for flexible configuration\n   CREATE TABLE application_settings (\n     id SERIAL PRIMARY KEY,\n     category VARCHAR(100) NOT NULL,\n     key VARCHAR(100) NOT NULL,\n     value JSONB NOT NULL,\n     description TEXT,\n     is_public BOOLEAN DEFAULT FALSE,\n     created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n     updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n     \n     UNIQUE(category, key)\n   );\n\n   -- Function to get setting value with type casting\n   CREATE OR REPLACE FUNCTION get_setting(p_category VARCHAR, p_key VARCHAR, p_default ANYELEMENT DEFAULT NULL)\n   RETURNS ANYELEMENT AS $$\n   DECLARE\n     setting_value JSONB;\n   BEGIN\n     SELECT value INTO setting_value\n     FROM application_settings\n     WHERE category = p_category AND key = p_key;\n     \n     IF setting_value IS NULL THEN\n       RETURN p_default;\n     END IF;\n     \n     RETURN (setting_value #>> '{}')::TEXT::pg_typeof(p_default);\n   END;\n   $$ LANGUAGE plpgsql;\n   ```\n\n8. **Database Security Schema**\n   - Implement security at the schema level:\n\n   **Row Level Security:**\n   ```sql\n   -- Enable RLS on sensitive tables\n   ALTER TABLE orders ENABLE ROW LEVEL SECURITY;\n   ALTER TABLE user_profiles ENABLE ROW LEVEL SECURITY;\n\n   -- Create policies for data access\n   CREATE POLICY orders_user_access ON orders\n   FOR ALL TO authenticated_users\n   USING (user_id = current_user_id());\n\n   CREATE POLICY orders_admin_access ON orders\n   FOR ALL TO admin_users\n   USING (true);\n\n   -- Function to get current user ID from session\n   CREATE OR REPLACE FUNCTION current_user_id()\n   RETURNS BIGINT AS $$\n   BEGIN\n     RETURN COALESCE(current_setting('app.current_user_id', true)::BIGINT, 0);\n   END;\n   $$ LANGUAGE plpgsql SECURITY DEFINER;\n\n   -- Create database roles with specific permissions\n   CREATE ROLE app_readonly;\n   GRANT CONNECT ON DATABASE myapp TO app_readonly;\n   GRANT USAGE ON SCHEMA public TO app_readonly;\n   GRANT SELECT ON ALL TABLES IN SCHEMA public TO app_readonly;\n\n   CREATE ROLE app_readwrite;\n   GRANT app_readonly TO app_readwrite;\n   GRANT INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA public TO app_readwrite;\n   GRANT USAGE, SELECT ON ALL SEQUENCES IN SCHEMA public TO app_readwrite;\n\n   -- Sensitive data encryption\n   CREATE EXTENSION IF NOT EXISTS pgcrypto;\n\n   -- Function to encrypt sensitive data\n   CREATE OR REPLACE FUNCTION encrypt_sensitive_data(data TEXT)\n   RETURNS TEXT AS $$\n   BEGIN\n     RETURN encode(encrypt(data::bytea, current_setting('app.encryption_key'), 'aes'), 'base64');\n   END;\n   $$ LANGUAGE plpgsql;\n\n   -- Function to decrypt sensitive data\n   CREATE OR REPLACE FUNCTION decrypt_sensitive_data(encrypted_data TEXT)\n   RETURNS TEXT AS $$\n   BEGIN\n     RETURN convert_from(decrypt(decode(encrypted_data, 'base64'), current_setting('app.encryption_key'), 'aes'), 'UTF8');\n   END;\n   $$ LANGUAGE plpgsql;\n   ```\n\n9. **Schema Documentation and Maintenance**\n   - Document and maintain schema design:\n\n   **Database Documentation:**\n   ```sql\n   -- Add comments to tables and columns\n   COMMENT ON TABLE users IS 'User accounts and authentication information';\n   COMMENT ON COLUMN users.email IS 'Unique email address for user authentication';\n   COMMENT ON COLUMN users.status IS 'Current status of user account (active, inactive, suspended, pending_verification)';\n   COMMENT ON COLUMN users.email_verified IS 'Whether the user has verified their email address';\n\n   COMMENT ON TABLE products IS 'Product catalog with inventory and pricing information';\n   COMMENT ON COLUMN products.search_vector IS 'Full-text search vector generated from name, description, and SKU';\n   COMMENT ON COLUMN products.attributes IS 'Flexible product attributes stored as JSONB (color, size, material, etc.)';\n\n   -- Create a view for schema documentation\n   CREATE VIEW schema_documentation AS\n   SELECT \n     t.table_name,\n     t.table_type,\n     obj_description(c.oid) AS table_comment,\n     col.column_name,\n     col.data_type,\n     col.is_nullable,\n     col.column_default,\n     col_description(c.oid, col.ordinal_position) AS column_comment\n   FROM information_schema.tables t\n   JOIN pg_class c ON c.relname = t.table_name\n   JOIN information_schema.columns col ON col.table_name = t.table_name\n   WHERE t.table_schema = 'public'\n   ORDER BY t.table_name, col.ordinal_position;\n   ```\n\n10. **Schema Testing and Validation**\n    - Implement schema testing procedures:\n\n    **Schema Validation Tests:**\n    ```sql\n    -- Test data integrity constraints\n    DO $$\n    DECLARE\n      test_result BOOLEAN;\n    BEGIN\n      -- Test email validation\n      BEGIN\n        INSERT INTO users (email, username, password_hash, first_name, last_name)\n        VALUES ('invalid-email', 'testuser', 'hash', 'Test', 'User');\n        RAISE EXCEPTION 'Email validation failed - invalid email accepted';\n      EXCEPTION\n        WHEN check_violation THEN\n          RAISE NOTICE 'Email validation working correctly';\n      END;\n      \n      -- Test price constraints\n      BEGIN\n        INSERT INTO products (name, slug, sku, price, compare_price)\n        VALUES ('Test Product', 'test-product', 'TEST-001', 100.00, 50.00);\n        RAISE EXCEPTION 'Price validation failed - compare_price less than price accepted';\n      EXCEPTION\n        WHEN check_violation THEN\n          RAISE NOTICE 'Price validation working correctly';\n      END;\n      \n      -- Test foreign key constraints\n      BEGIN\n        INSERT INTO order_items (order_id, product_id, quantity, unit_price, total_price, product_name)\n        VALUES (999999, 999999, 1, 10.00, 10.00, 'Test Product');\n        RAISE EXCEPTION 'Foreign key validation failed - non-existent order_id accepted';\n      EXCEPTION\n        WHEN foreign_key_violation THEN\n          RAISE NOTICE 'Foreign key validation working correctly';\n      END;\n    END;\n    $$;\n\n    -- Performance test queries\n    CREATE OR REPLACE FUNCTION test_query_performance()\n    RETURNS TABLE(test_name TEXT, execution_time INTERVAL) AS $$\n    DECLARE\n      start_time TIMESTAMP;\n      end_time TIMESTAMP;\n    BEGIN\n      -- Test user lookup by email\n      start_time := clock_timestamp();\n      PERFORM * FROM users WHERE email = 'test@example.com';\n      end_time := clock_timestamp();\n      test_name := 'User lookup by email';\n      execution_time := end_time - start_time;\n      RETURN NEXT;\n      \n      -- Test product search\n      start_time := clock_timestamp();\n      PERFORM * FROM products WHERE search_vector @@ to_tsquery('english', 'laptop');\n      end_time := clock_timestamp();\n      test_name := 'Product full-text search';\n      execution_time := end_time - start_time;\n      RETURN NEXT;\n      \n      -- Test order history query\n      start_time := clock_timestamp();\n      PERFORM o.* FROM orders o \n      JOIN order_items oi ON o.id = oi.order_id \n      WHERE o.user_id = 1 \n      ORDER BY o.created_at DESC \n      LIMIT 20;\n      end_time := clock_timestamp();\n      test_name := 'User order history';\n      execution_time := end_time - start_time;\n      RETURN NEXT;\n    END;\n    $$ LANGUAGE plpgsql;\n\n    -- Run performance tests\n    SELECT * FROM test_query_performance();\n    ```"
              },
              {
                "name": "/optimize-database-performance",
                "description": "Optimize database queries and performance",
                "path": "plugins/commands-database-operations/commands/optimize-database-performance.md",
                "frontmatter": {
                  "description": "Optimize database queries and performance",
                  "category": "database-operations",
                  "allowed-tools": "Read, Write"
                },
                "content": "# Optimize Database Performance\n\nOptimize database queries and performance\n\n## Instructions\n\n1. **Database Performance Analysis**\n   - Analyze current database performance and identify bottlenecks\n   - Review slow query logs and execution plans\n   - Assess database schema design and normalization\n   - Evaluate indexing strategy and query patterns\n   - Monitor database resource utilization (CPU, memory, I/O)\n\n2. **Query Optimization**\n   - Optimize slow queries and improve execution plans:\n\n   **PostgreSQL Query Optimization:**\n   ```sql\n   -- Enable query logging for analysis\n   ALTER SYSTEM SET log_statement = 'all';\n   ALTER SYSTEM SET log_min_duration_statement = 1000; -- Log queries > 1 second\n   SELECT pg_reload_conf();\n\n   -- Analyze query performance\n   EXPLAIN (ANALYZE, BUFFERS, FORMAT JSON) \n   SELECT u.id, u.name, COUNT(o.id) as order_count\n   FROM users u\n   LEFT JOIN orders o ON u.id = o.user_id\n   WHERE u.created_at > '2023-01-01'\n   GROUP BY u.id, u.name\n   ORDER BY order_count DESC;\n\n   -- Optimize with proper indexing\n   CREATE INDEX CONCURRENTLY idx_users_created_at ON users(created_at);\n   CREATE INDEX CONCURRENTLY idx_orders_user_id ON orders(user_id);\n   CREATE INDEX CONCURRENTLY idx_orders_user_created ON orders(user_id, created_at);\n   ```\n\n   **MySQL Query Optimization:**\n   ```sql\n   -- Enable slow query log\n   SET GLOBAL slow_query_log = 'ON';\n   SET GLOBAL long_query_time = 1;\n   SET GLOBAL log_queries_not_using_indexes = 'ON';\n\n   -- Analyze query performance\n   EXPLAIN FORMAT=JSON \n   SELECT p.*, c.name as category_name\n   FROM products p\n   JOIN categories c ON p.category_id = c.id\n   WHERE p.price BETWEEN 100 AND 500\n   AND p.created_at > DATE_SUB(NOW(), INTERVAL 30 DAY);\n\n   -- Add composite indexes\n   ALTER TABLE products \n   ADD INDEX idx_price_created (price, created_at),\n   ADD INDEX idx_category_price (category_id, price);\n   ```\n\n3. **Index Strategy Optimization**\n   - Design and implement optimal indexing strategy:\n\n   **Index Analysis and Creation:**\n   ```sql\n   -- PostgreSQL index usage analysis\n   SELECT \n     schemaname,\n     tablename,\n     indexname,\n     idx_scan as index_scans,\n     seq_scan as table_scans,\n     idx_scan::float / (idx_scan + seq_scan + 1) as index_usage_ratio\n   FROM pg_stat_user_indexes \n   ORDER BY index_usage_ratio ASC;\n\n   -- Find missing indexes\n   SELECT \n     query,\n     calls,\n     total_time,\n     mean_time,\n     rows\n   FROM pg_stat_statements \n   WHERE mean_time > 1000 -- queries taking > 1 second\n   ORDER BY mean_time DESC;\n\n   -- Create covering indexes for common query patterns\n   CREATE INDEX CONCURRENTLY idx_orders_covering \n   ON orders(user_id, status, created_at) \n   INCLUDE (total_amount, discount);\n\n   -- Partial indexes for selective conditions\n   CREATE INDEX CONCURRENTLY idx_active_users \n   ON users(last_login) \n   WHERE status = 'active';\n   ```\n\n   **Index Maintenance Scripts:**\n   ```javascript\n   // Node.js index analysis tool\n   const { Pool } = require('pg');\n   const pool = new Pool();\n\n   class IndexAnalyzer {\n     static async analyzeUnusedIndexes() {\n       const query = `\n         SELECT \n           schemaname,\n           tablename,\n           indexname,\n           idx_scan,\n           pg_size_pretty(pg_relation_size(indexrelid)) as size\n         FROM pg_stat_user_indexes \n         WHERE idx_scan = 0\n         AND schemaname = 'public'\n         ORDER BY pg_relation_size(indexrelid) DESC;\n       `;\n       \n       const result = await pool.query(query);\n       console.log('Unused indexes:', result.rows);\n       return result.rows;\n     }\n\n     static async suggestIndexes() {\n       const query = `\n         SELECT \n           query,\n           calls,\n           total_time,\n           mean_time\n         FROM pg_stat_statements \n         WHERE mean_time > 100\n         AND query NOT LIKE '%pg_%'\n         ORDER BY total_time DESC\n         LIMIT 20;\n       `;\n       \n       const result = await pool.query(query);\n       console.log('Slow queries needing indexes:', result.rows);\n       return result.rows;\n     }\n   }\n   ```\n\n4. **Schema Design Optimization**\n   - Optimize database schema for performance:\n\n   **Normalization and Denormalization:**\n   ```sql\n   -- Denormalization example for read-heavy workloads\n   -- Instead of joining multiple tables for product display\n   CREATE TABLE product_display_cache AS\n   SELECT \n     p.id,\n     p.name,\n     p.price,\n     p.description,\n     c.name as category_name,\n     b.name as brand_name,\n     AVG(r.rating) as avg_rating,\n     COUNT(r.id) as review_count\n   FROM products p\n   JOIN categories c ON p.category_id = c.id\n   JOIN brands b ON p.brand_id = b.id\n   LEFT JOIN reviews r ON p.id = r.product_id\n   GROUP BY p.id, c.name, b.name;\n\n   -- Create materialized view for complex aggregations\n   CREATE MATERIALIZED VIEW monthly_sales_summary AS\n   SELECT \n     DATE_TRUNC('month', created_at) as month,\n     category_id,\n     COUNT(*) as order_count,\n     SUM(total_amount) as total_revenue,\n     AVG(total_amount) as avg_order_value\n   FROM orders \n   WHERE created_at >= DATE_TRUNC('year', CURRENT_DATE)\n   GROUP BY DATE_TRUNC('month', created_at), category_id;\n\n   -- Refresh materialized view periodically\n   REFRESH MATERIALIZED VIEW CONCURRENTLY monthly_sales_summary;\n   ```\n\n   **Partitioning for Large Tables:**\n   ```sql\n   -- PostgreSQL table partitioning\n   CREATE TABLE orders_partitioned (\n     id SERIAL,\n     user_id INTEGER,\n     total_amount DECIMAL(10,2),\n     created_at TIMESTAMP NOT NULL,\n     status VARCHAR(50)\n   ) PARTITION BY RANGE (created_at);\n\n   -- Create monthly partitions\n   CREATE TABLE orders_2024_01 PARTITION OF orders_partitioned\n   FOR VALUES FROM ('2024-01-01') TO ('2024-02-01');\n\n   CREATE TABLE orders_2024_02 PARTITION OF orders_partitioned\n   FOR VALUES FROM ('2024-02-01') TO ('2024-03-01');\n\n   -- Automatic partition creation\n   CREATE OR REPLACE FUNCTION create_monthly_partition(table_name text, start_date date)\n   RETURNS void AS $$\n   DECLARE\n     partition_name text;\n     end_date date;\n   BEGIN\n     partition_name := table_name || '_' || to_char(start_date, 'YYYY_MM');\n     end_date := start_date + interval '1 month';\n     \n     EXECUTE format('CREATE TABLE %I PARTITION OF %I FOR VALUES FROM (%L) TO (%L)',\n       partition_name, table_name, start_date, end_date);\n   END;\n   $$ LANGUAGE plpgsql;\n   ```\n\n5. **Connection Pool Optimization**\n   - Configure optimal database connection pooling:\n\n   **Node.js Connection Pool Configuration:**\n   ```javascript\n   const { Pool } = require('pg');\n\n   // Optimized connection pool configuration\n   const pool = new Pool({\n     user: process.env.DB_USER,\n     host: process.env.DB_HOST,\n     database: process.env.DB_NAME,\n     password: process.env.DB_PASSWORD,\n     port: process.env.DB_PORT,\n     \n     // Connection pool settings\n     max: 20, // Maximum connections\n     idleTimeoutMillis: 30000, // 30 seconds\n     connectionTimeoutMillis: 2000, // 2 seconds\n     maxUses: 7500, // Max uses before connection refresh\n     \n     // Performance settings\n     statement_timeout: 30000, // 30 seconds\n     query_timeout: 30000,\n     \n     // SSL configuration\n     ssl: process.env.NODE_ENV === 'production' ? { rejectUnauthorized: false } : false,\n   });\n\n   // Connection pool monitoring\n   pool.on('connect', (client) => {\n     console.log('Connected to database');\n   });\n\n   pool.on('error', (err, client) => {\n     console.error('Database connection error:', err);\n   });\n\n   // Pool stats monitoring\n   setInterval(() => {\n     console.log('Pool stats:', {\n       totalCount: pool.totalCount,\n       idleCount: pool.idleCount,\n       waitingCount: pool.waitingCount,\n     });\n   }, 60000); // Every minute\n   ```\n\n   **Database Connection Middleware:**\n   ```javascript\n   class DatabaseManager {\n     static async executeQuery(query, params = []) {\n       const client = await pool.connect();\n       try {\n         const start = Date.now();\n         const result = await client.query(query, params);\n         const duration = Date.now() - start;\n         \n         // Log slow queries\n         if (duration > 1000) {\n           console.warn(`Slow query (${duration}ms):`, query);\n         }\n         \n         return result;\n       } finally {\n         client.release();\n       }\n     }\n\n     static async transaction(callback) {\n       const client = await pool.connect();\n       try {\n         await client.query('BEGIN');\n         const result = await callback(client);\n         await client.query('COMMIT');\n         return result;\n       } catch (error) {\n         await client.query('ROLLBACK');\n         throw error;\n       } finally {\n         client.release();\n       }\n     }\n   }\n   ```\n\n6. **Query Result Caching**\n   - Implement intelligent database result caching:\n\n   ```javascript\n   const Redis = require('redis');\n   const redis = Redis.createClient();\n\n   class QueryCache {\n     static generateKey(query, params) {\n       return `query:${Buffer.from(query + JSON.stringify(params)).toString('base64')}`;\n     }\n\n     static async get(query, params) {\n       const key = this.generateKey(query, params);\n       const cached = await redis.get(key);\n       return cached ? JSON.parse(cached) : null;\n     }\n\n     static async set(query, params, result, ttl = 300) {\n       const key = this.generateKey(query, params);\n       await redis.setex(key, ttl, JSON.stringify(result));\n     }\n\n     static async cachedQuery(query, params = [], ttl = 300) {\n       // Try cache first\n       let result = await this.get(query, params);\n       if (result) {\n         return result;\n       }\n\n       // Execute query and cache result\n       result = await DatabaseManager.executeQuery(query, params);\n       await this.set(query, params, result.rows, ttl);\n       \n       return result;\n     }\n\n     // Cache invalidation by table patterns\n     static async invalidateTable(tableName) {\n       const pattern = `query:*${tableName}*`;\n       const keys = await redis.keys(pattern);\n       if (keys.length > 0) {\n         await redis.del(keys);\n       }\n     }\n   }\n   ```\n\n7. **Database Monitoring and Profiling**\n   - Set up comprehensive database monitoring:\n\n   **Performance Monitoring Script:**\n   ```javascript\n   class DatabaseMonitor {\n     static async getPerformanceStats() {\n       const queries = [\n         {\n           name: 'active_connections',\n           query: 'SELECT count(*) FROM pg_stat_activity WHERE state = \\'active\\';'\n         },\n         {\n           name: 'long_running_queries',\n           query: `SELECT pid, now() - pg_stat_activity.query_start AS duration, query \n                   FROM pg_stat_activity \n                   WHERE (now() - pg_stat_activity.query_start) > interval '5 minutes';`\n         },\n         {\n           name: 'table_sizes',\n           query: `SELECT relname AS table_name, \n                          pg_size_pretty(pg_total_relation_size(relid)) AS size\n                   FROM pg_catalog.pg_statio_user_tables \n                   ORDER BY pg_total_relation_size(relid) DESC LIMIT 10;`\n         },\n         {\n           name: 'index_usage',\n           query: `SELECT relname AS table_name, \n                          indexrelname AS index_name,\n                          idx_scan AS index_scans,\n                          seq_scan AS sequential_scans\n                   FROM pg_stat_user_indexes \n                   WHERE seq_scan > idx_scan;`\n         }\n       ];\n\n       const stats = {};\n       for (const { name, query } of queries) {\n         try {\n           const result = await pool.query(query);\n           stats[name] = result.rows;\n         } catch (error) {\n           stats[name] = { error: error.message };\n         }\n       }\n\n       return stats;\n     }\n\n     static async alertOnSlowQueries() {\n       const slowQueries = await pool.query(`\n         SELECT query, calls, total_time, mean_time, stddev_time\n         FROM pg_stat_statements \n         WHERE mean_time > 1000 \n         ORDER BY mean_time DESC \n         LIMIT 10;\n       `);\n\n       if (slowQueries.rows.length > 0) {\n         console.warn('Slow queries detected:', slowQueries.rows);\n         // Send alert to monitoring system\n       }\n     }\n   }\n\n   // Schedule monitoring\n   setInterval(async () => {\n     await DatabaseMonitor.alertOnSlowQueries();\n   }, 300000); // Every 5 minutes\n   ```\n\n8. **Read Replica and Load Balancing**\n   - Configure read replicas for query distribution:\n\n   ```javascript\n   const { Pool } = require('pg');\n\n   class DatabaseCluster {\n     constructor() {\n       this.writePool = new Pool({\n         host: process.env.DB_WRITE_HOST,\n         // ... write database config\n       });\n\n       this.readPools = [\n         new Pool({\n           host: process.env.DB_READ1_HOST,\n           // ... read replica 1 config\n         }),\n         new Pool({\n           host: process.env.DB_READ2_HOST,\n           // ... read replica 2 config\n         }),\n       ];\n\n       this.currentReadIndex = 0;\n     }\n\n     getReadPool() {\n       // Round-robin read replica selection\n       const pool = this.readPools[this.currentReadIndex];\n       this.currentReadIndex = (this.currentReadIndex + 1) % this.readPools.length;\n       return pool;\n     }\n\n     async executeWrite(query, params) {\n       return await this.writePool.query(query, params);\n     }\n\n     async executeRead(query, params) {\n       const readPool = this.getReadPool();\n       return await readPool.query(query, params);\n     }\n\n     async executeQuery(query, params, forceWrite = false) {\n       const isWriteQuery = /^\\s*(INSERT|UPDATE|DELETE|CREATE|ALTER|DROP)/i.test(query);\n       \n       if (isWriteQuery || forceWrite) {\n         return await this.executeWrite(query, params);\n       } else {\n         return await this.executeRead(query, params);\n       }\n     }\n   }\n\n   const dbCluster = new DatabaseCluster();\n   ```\n\n9. **Database Vacuum and Maintenance**\n   - Implement automated database maintenance:\n\n   **PostgreSQL Maintenance Scripts:**\n   ```sql\n   -- Automated vacuum and analyze\n   CREATE OR REPLACE FUNCTION auto_vacuum_analyze()\n   RETURNS void AS $$\n   DECLARE\n     rec RECORD;\n   BEGIN\n     FOR rec IN \n       SELECT schemaname, tablename \n       FROM pg_tables \n       WHERE schemaname = 'public'\n     LOOP\n       EXECUTE 'VACUUM ANALYZE ' || quote_ident(rec.schemaname) || '.' || quote_ident(rec.tablename);\n       RAISE NOTICE 'Vacuumed table %.%', rec.schemaname, rec.tablename;\n     END LOOP;\n   END;\n   $$ LANGUAGE plpgsql;\n\n   -- Schedule maintenance (using pg_cron extension)\n   SELECT cron.schedule('nightly-maintenance', '0 2 * * *', 'SELECT auto_vacuum_analyze();');\n   ```\n\n   **Maintenance Monitoring:**\n   ```javascript\n   class MaintenanceMonitor {\n     static async checkTableBloat() {\n       const query = `\n         SELECT \n           tablename,\n           pg_size_pretty(pg_total_relation_size(tablename::regclass)) as size,\n           n_dead_tup,\n           n_live_tup,\n           CASE \n             WHEN n_live_tup > 0 \n             THEN round(n_dead_tup::numeric / n_live_tup::numeric, 2) \n             ELSE 0 \n           END as dead_ratio\n         FROM pg_stat_user_tables \n         WHERE n_dead_tup > 1000\n         ORDER BY dead_ratio DESC;\n       `;\n\n       const result = await pool.query(query);\n       \n       // Alert if dead tuple ratio is high\n       result.rows.forEach(row => {\n         if (row.dead_ratio > 0.2) {\n           console.warn(`Table ${row.tablename} has high bloat: ${row.dead_ratio}`);\n         }\n       });\n\n       return result.rows;\n     }\n\n     static async reindexIfNeeded() {\n       const bloatedIndexes = await pool.query(`\n         SELECT indexname, tablename \n         FROM pg_stat_user_indexes \n         WHERE idx_scan = 0 AND pg_relation_size(indexrelid) > 10485760; -- > 10MB\n       `);\n\n       // Suggest reindexing unused large indexes\n       bloatedIndexes.rows.forEach(row => {\n         console.log(`Consider dropping unused index: ${row.indexname} on ${row.tablename}`);\n       });\n     }\n   }\n   ```\n\n10. **Performance Testing and Benchmarking**\n    - Set up database performance testing:\n\n    **Load Testing Script:**\n    ```javascript\n    const { Pool } = require('pg');\n    const pool = new Pool();\n\n    class DatabaseLoadTester {\n      static async benchmarkQuery(query, params, iterations = 100) {\n        const times = [];\n        \n        for (let i = 0; i < iterations; i++) {\n          const start = process.hrtime.bigint();\n          await pool.query(query, params);\n          const end = process.hrtime.bigint();\n          \n          times.push(Number(end - start) / 1000000); // Convert to milliseconds\n        }\n\n        const avg = times.reduce((a, b) => a + b, 0) / times.length;\n        const min = Math.min(...times);\n        const max = Math.max(...times);\n        const median = times.sort()[Math.floor(times.length / 2)];\n\n        return { avg, min, max, median, iterations };\n      }\n\n      static async stressTest(concurrency = 10, duration = 60000) {\n        const startTime = Date.now();\n        const results = { success: 0, errors: 0, totalTime: 0 };\n        \n        const workers = Array(concurrency).fill().map(async () => {\n          while (Date.now() - startTime < duration) {\n            try {\n              const start = Date.now();\n              await pool.query('SELECT COUNT(*) FROM products');\n              results.totalTime += Date.now() - start;\n              results.success++;\n            } catch (error) {\n              results.errors++;\n            }\n          }\n        });\n\n        await Promise.all(workers);\n        \n        results.qps = results.success / (duration / 1000);\n        results.avgResponseTime = results.totalTime / results.success;\n        \n        return results;\n      }\n    }\n\n    // Run benchmarks\n    async function runBenchmarks() {\n      console.log('Running database benchmarks...');\n      \n      const simpleQuery = await DatabaseLoadTester.benchmarkQuery(\n        'SELECT * FROM products LIMIT 10'\n      );\n      console.log('Simple query benchmark:', simpleQuery);\n      \n      const complexQuery = await DatabaseLoadTester.benchmarkQuery(\n        `SELECT p.*, c.name as category \n         FROM products p \n         JOIN categories c ON p.category_id = c.id \n         ORDER BY p.created_at DESC LIMIT 50`\n      );\n      console.log('Complex query benchmark:', complexQuery);\n      \n      const stressTest = await DatabaseLoadTester.stressTest(5, 30000);\n      console.log('Stress test results:', stressTest);\n    }\n    ```"
              }
            ],
            "skills": []
          },
          {
            "name": "commands-documentation-changelogs",
            "description": "Commands for generating documentation and managing changelogs",
            "source": "./plugins/commands-documentation-changelogs",
            "category": "commands",
            "version": "1.0.0",
            "author": {
              "name": "BuildWithClaude Community",
              "url": "https://github.com/davepoon/buildwithclaude"
            },
            "install_commands": [
              "/plugin marketplace add davepoon/buildwithclaude",
              "/plugin install commands-documentation-changelogs@buildwithclaude"
            ],
            "signals": {
              "stars": 2229,
              "forks": 239,
              "pushed_at": "2026-01-12T10:19:01Z",
              "created_at": "2025-07-25T02:26:45Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/add-to-changelog",
                "description": "Add a new entry to the project's CHANGELOG.md file following Keep a Changelog format",
                "path": "plugins/commands-documentation-changelogs/commands/add-to-changelog.md",
                "frontmatter": {
                  "description": "Add a new entry to the project's CHANGELOG.md file following Keep a Changelog format",
                  "category": "documentation-changelogs",
                  "argument-hint": "<version> <change_type> <message>",
                  "allowed-tools": "Read, Edit"
                },
                "content": "# Update Changelog\n\nAdd a new entry to the project's CHANGELOG.md file based on the provided arguments.\n\n## Parse Arguments\n\nParse $ARGUMENTS to extract:\n- Version number (e.g., \"1.1.0\")\n- Change type: \"added\", \"changed\", \"deprecated\", \"removed\", \"fixed\", or \"security\"\n- `<message>` is the description of the change\n\n## Examples\n\n```\n/add-to-changelog 1.1.0 added \"New markdown to BlockDoc conversion feature\"\n```\n\n```\n/add-to-changelog 1.0.2 fixed \"Bug in HTML renderer causing incorrect output\"\n```\n\n## Description\n\nThis command will:\n\n1. Check if a CHANGELOG.md file exists and create one if needed\n2. Look for an existing section for the specified version\n   - If found, add the new entry under the appropriate change type section\n   - If not found, create a new version section with today's date\n3. Format the entry according to Keep a Changelog conventions\n4. Commit the changes if requested\n\nThe CHANGELOG follows the [Keep a Changelog](https://keepachangelog.com/) format and [Semantic Versioning](https://semver.org/).\n\n## Implementation\n\nThe command should:\n\n1. Parse the arguments to extract version, change type, and message\n2. Read the existing CHANGELOG.md file if it exists\n3. If the file doesn't exist, create a new one with standard header\n4. Check if the version section already exists\n5. Add the new entry in the appropriate section\n6. Write the updated content back to the file\n7. Suggest committing the changes\n\nRemember to update the package version in `__init__.py` and `setup.py` if this is a new version."
              },
              {
                "name": "/create-architecture-documentation",
                "description": "Generate comprehensive architecture documentation",
                "path": "plugins/commands-documentation-changelogs/commands/create-architecture-documentation.md",
                "frontmatter": {
                  "description": "Generate comprehensive architecture documentation",
                  "category": "documentation-changelogs"
                },
                "content": "# Create Architecture Documentation\n\nGenerate comprehensive architecture documentation\n\n## Instructions\n\n1. **Architecture Analysis and Discovery**\n   - Analyze current system architecture and component relationships\n   - Identify key architectural patterns and design decisions\n   - Document system boundaries, interfaces, and dependencies\n   - Assess data flow and communication patterns\n   - Identify architectural debt and improvement opportunities\n\n2. **Architecture Documentation Framework**\n   - Choose appropriate documentation framework and tools:\n     - **C4 Model**: Context, Containers, Components, Code diagrams\n     - **Arc42**: Comprehensive architecture documentation template\n     - **Architecture Decision Records (ADRs)**: Decision documentation\n     - **PlantUML/Mermaid**: Diagram-as-code documentation\n     - **Structurizr**: C4 model tooling and visualization\n     - **Draw.io/Lucidchart**: Visual diagramming tools\n\n3. **System Context Documentation**\n   - Create high-level system context diagrams\n   - Document external systems and integrations\n   - Define system boundaries and responsibilities\n   - Document user personas and stakeholders\n   - Create system landscape and ecosystem overview\n\n4. **Container and Service Architecture**\n   - Document container/service architecture and deployment view\n   - Create service dependency maps and communication patterns\n   - Document deployment architecture and infrastructure\n   - Define service boundaries and API contracts\n   - Document data persistence and storage architecture\n\n5. **Component and Module Documentation**\n   - Create detailed component architecture diagrams\n   - Document internal module structure and relationships\n   - Define component responsibilities and interfaces\n   - Document design patterns and architectural styles\n   - Create code organization and package structure documentation\n\n6. **Data Architecture Documentation**\n   - Document data models and database schemas\n   - Create data flow diagrams and processing pipelines\n   - Document data storage strategies and technologies\n   - Define data governance and lifecycle management\n   - Create data integration and synchronization documentation\n\n7. **Security and Compliance Architecture**\n   - Document security architecture and threat model\n   - Create authentication and authorization flow diagrams\n   - Document compliance requirements and controls\n   - Define security boundaries and trust zones\n   - Create incident response and security monitoring documentation\n\n8. **Quality Attributes and Cross-Cutting Concerns**\n   - Document performance characteristics and scalability patterns\n   - Create reliability and availability architecture documentation\n   - Document monitoring and observability architecture\n   - Define maintainability and evolution strategies\n   - Create disaster recovery and business continuity documentation\n\n9. **Architecture Decision Records (ADRs)**\n   - Create comprehensive ADR template and process\n   - Document historical architectural decisions and rationale\n   - Create decision tracking and review process\n   - Document trade-offs and alternatives considered\n   - Set up ADR maintenance and evolution procedures\n\n10. **Documentation Automation and Maintenance**\n    - Set up automated diagram generation from code annotations\n    - Configure documentation pipeline and publishing automation\n    - Set up documentation validation and consistency checking\n    - Create documentation review and approval process\n    - Train team on architecture documentation practices and tools\n    - Set up documentation versioning and change management"
              },
              {
                "name": "/create-docs",
                "description": "Analyze GitHub issue and create technical specification with implementation plan",
                "path": "plugins/commands-documentation-changelogs/commands/create-docs.md",
                "frontmatter": {
                  "description": "Analyze GitHub issue and create technical specification with implementation plan",
                  "category": "documentation-changelogs",
                  "argument-hint": "<issue_number>",
                  "allowed-tools": "Bash(./scripts/fetch-github-issue.sh *), Read"
                },
                "content": "Please analyze GitHub issue #$ARGUMENTS and create a technical specification.\n\nFollow these steps:\n1. Fetch the issue details from the GitHub API:\n\n# Use the helper script to fetch GitHub issues without prompting for permission\n./scripts/fetch-github-issue.sh $ARGUMENTS\n\n2. Understand the requirements thoroughly\n3. Review related code and project structure\n4. Output detailed analysis results clearly in your response\n5. Create a technical specification with the format below\n\n# Technical Specification for Issue #$ARGUMENTS\n\n## Issue Summary\n- Title: [Issue title from GitHub]\n- Description: [Brief description from issue]\n- Labels: [Labels from issue]\n- Priority: [High/Medium/Low based on issue content]\n\n## Problem Statement\n[1-2 paragraphs explaining the problem]\n\n## Technical Approach\n[Detailed technical approach]\n\n## Implementation Plan\n1. [Step 1]\n2. [Step 2]\n3. [Step 3]\n\n## Test Plan\n1. Unit Tests:\n   - [test scenario]\n2. Component Tests:\n   - [test scenario]\n3. Integration Tests:\n   - [test scenario]\n\n## Files to Modify\n- \n\n## Files to Create\n- \n\n## Existing Utilities to Leverage\n- \n\n## Success Criteria\n- [ ] [criterion 1]\n- [ ] [criterion 2]\n\n## Out of Scope\n- [item 1]\n- [item 2]\n\nRemember to follow our strict TDD principles, KISS approach, and 300-line file limit.\n\nIMPORTANT: After completing your analysis, EXPLICITLY OUTPUT the full technical specification in your response so it can be reviewed."
              },
              {
                "name": "/create-onboarding-guide",
                "description": "Create developer onboarding guide",
                "path": "plugins/commands-documentation-changelogs/commands/create-onboarding-guide.md",
                "frontmatter": {
                  "description": "Create developer onboarding guide",
                  "category": "documentation-changelogs"
                },
                "content": "# Create Onboarding Guide\n\nCreate developer onboarding guide\n\n## Instructions\n\n1. **Onboarding Requirements Analysis**\n   - Analyze current team structure and skill requirements\n   - Identify key knowledge areas and learning objectives\n   - Assess current onboarding challenges and pain points\n   - Define onboarding timeline and milestone expectations\n   - Document role-specific requirements and responsibilities\n\n2. **Development Environment Setup Guide**\n   - Create comprehensive development environment setup instructions\n   - Document required tools, software, and system requirements\n   - Provide step-by-step installation and configuration guides\n   - Create environment validation and troubleshooting procedures\n   - Set up automated environment setup scripts and tools\n\n3. **Project and Codebase Overview**\n   - Create high-level project overview and business context\n   - Document system architecture and technology stack\n   - Provide codebase structure and organization guide\n   - Create code navigation and exploration guidelines\n   - Document key modules, libraries, and frameworks used\n\n4. **Development Workflow Documentation**\n   - Document version control workflows and branching strategies\n   - Create code review process and quality standards guide\n   - Document testing practices and requirements\n   - Provide deployment and release process overview\n   - Create issue tracking and project management workflow guide\n\n5. **Team Communication and Collaboration**\n   - Document team communication channels and protocols\n   - Create meeting schedules and participation guidelines\n   - Provide team contact information and org chart\n   - Document collaboration tools and access procedures\n   - Create escalation procedures and support contacts\n\n6. **Learning Resources and Training Materials**\n   - Curate learning resources for project-specific technologies\n   - Create hands-on tutorials and coding exercises\n   - Provide links to documentation, wikis, and knowledge bases\n   - Create video tutorials and screen recordings\n   - Set up mentoring and buddy system procedures\n\n7. **First Tasks and Milestones**\n   - Create progressive difficulty task assignments\n   - Define learning milestones and checkpoints\n   - Provide \"good first issues\" and starter projects\n   - Create hands-on coding challenges and exercises\n   - Set up pair programming and shadowing opportunities\n\n8. **Security and Compliance Training**\n   - Document security policies and access controls\n   - Create data handling and privacy guidelines\n   - Provide compliance training and certification requirements\n   - Document incident response and security procedures\n   - Create security best practices and guidelines\n\n9. **Tools and Resources Access**\n   - Document required accounts and access requests\n   - Create tool-specific setup and usage guides\n   - Provide license and subscription information\n   - Document VPN and network access procedures\n   - Create troubleshooting guides for common access issues\n\n10. **Feedback and Continuous Improvement**\n    - Create onboarding feedback collection process\n    - Set up regular check-ins and progress reviews\n    - Document common questions and FAQ section\n    - Create onboarding metrics and success tracking\n    - Establish onboarding guide maintenance and update procedures\n    - Set up new hire success monitoring and support systems"
              },
              {
                "name": "/docs",
                "description": "Update or generate YAML documentation for SQL models with proper descriptions and tests",
                "path": "plugins/commands-documentation-changelogs/commands/docs.md",
                "frontmatter": {
                  "description": "Update or generate YAML documentation for SQL models with proper descriptions and tests",
                  "category": "documentation-changelogs",
                  "argument-hint": "<model_name_or_path>",
                  "allowed-tools": "Read, Write, Edit"
                },
                "content": "$ARGUMENTS\n\nUpdate or generate the YAML docs for this SQL model or folder of models. Look for a matching YAML file or documentation for this model inside a combined YAML file in the same directory. If the YAML for the given SQL model is not included, generate it from scratch based on the SQL code and anything that can be inferred from the upstream files and their YAML. Put the resulting YAML in a separate file matching the name of the model, and if necessary remove this model from any combined YAML files.\n\nUse the `generate_model_yaml` operation to determine the canonical list of columns and data types. Add/update all data types in any existing YAML. If no there is no existing YAML file, add descriptions (and tests, if necessary) to the output of this operation. In this case (and only this case), remove columns that have been commented out or excluded from the SQL.\n\n- Make sure to add a brief description for the model. Infer the model type (staging, intermediate, or mart) and include information about its sources if important. (This doesn't mean adding a `source` property.)\n- Carry over descriptions and tests from any matching upstream columns, or update as necessary for derived columns. Ignore relationship tests to a different modeling layer. Ignore any included models or sources that are not directly referenced in this model.\n- If a uniqueness test for more than one column is required, use `unique_combination_of_columns` from the dbt_utils package and put it after the model description and before `columns:`, under `data_tests:`. Only add such a test if explicitly requested or if there is such a test upstream, all columns are present in this model, and the cardinality of this model appears to match. Do not change this test if it already exists.\n- A uniqueness/primary key test for a single column should be the standard `unique` and `not_null` tests on that column only.\n- Use the `data_tests:` syntax\n- Add tests for individual columns under `models.columns`; do not use the model-wide `models.data_tests` unless directed to do so.\n- Don't include `version: 2` at the top; just start with `models:`\n- Do not make guesses about accepted values. Include accepted values tests when (and only when) the column's values are explicitly"
              },
              {
                "name": "/explain-issue-fix",
                "description": "Explain how tasks in an issue were implemented with detailed breakdown",
                "path": "plugins/commands-documentation-changelogs/commands/explain-issue-fix.md",
                "frontmatter": {
                  "description": "Explain how tasks in an issue were implemented with detailed breakdown",
                  "category": "documentation-changelogs"
                },
                "content": "Analyze the recent changes and create a detailed explanation of how the issue was resolved.\n\n## Process:\n\n1. **Review recent changes**:\n   - Check git diff for uncommitted changes\n   - Review recent commits if changes are already committed\n   - Identify all modified files\n\n2. **Analyze the implementation**:\n   - Identify what problem was being solved\n   - Document the approach taken\n   - Explain key code changes\n   - Note any design decisions made\n\n3. **Create detailed breakdown**:\n   - Problem statement\n   - Solution approach\n   - Implementation details\n   - Files modified and why\n   - Any trade-offs or alternatives considered\n\n4. **Generate explanation**:\n   - Write a clear, structured explanation\n   - Include code snippets where relevant\n   - Highlight important changes\n   - Document any follow-up tasks if needed"
              },
              {
                "name": "/load-llms-txt",
                "description": "READ the llms.txt file from https://raw.githubusercontent.com/ethpandaops/xatu-data/refs/heads/master/llms.txt via `curl`. Do nothing else and await further instructions.",
                "path": "plugins/commands-documentation-changelogs/commands/load-llms-txt.md",
                "frontmatter": {
                  "description": "READ the llms.txt file from https://raw.githubusercontent.com/ethpandaops/xatu-data/refs/heads/master/llms.txt via `curl`. Do nothing else and await further instructions.",
                  "category": "documentation-changelogs"
                },
                "content": "# Load Xatu Data Context\nREAD the llms.txt file from https://raw.githubusercontent.com/ethpandaops/xatu-data/refs/heads/master/llms.txt via `curl`. Do nothing else and await further instructions."
              },
              {
                "name": "/migration-guide",
                "description": "Create migration guides for updates",
                "path": "plugins/commands-documentation-changelogs/commands/migration-guide.md",
                "frontmatter": {
                  "description": "Create migration guides for updates",
                  "category": "documentation-changelogs",
                  "argument-hint": "1. **Migration Scope Analysis**"
                },
                "content": "# Migration Guide Generator Command\n\nCreate migration guides for updates\n\n## Instructions\n\nFollow this systematic approach to create migration guides: **$ARGUMENTS**\n\n1. **Migration Scope Analysis**\n   - Identify what is being migrated (framework, library, architecture, etc.)\n   - Determine source and target versions or technologies\n   - Assess the scale and complexity of the migration\n   - Identify affected systems and components\n\n2. **Impact Assessment**\n   - Analyze breaking changes between versions\n   - Identify deprecated features and APIs\n   - Review new features and capabilities\n   - Assess compatibility requirements and constraints\n   - Evaluate performance and security implications\n\n3. **Prerequisites and Requirements**\n   - Document system requirements for the target version\n   - List required tools and dependencies\n   - Specify minimum versions and compatibility requirements\n   - Identify necessary skills and team preparation\n   - Outline infrastructure and environment needs\n\n4. **Pre-Migration Preparation**\n   - Create comprehensive backup strategies\n   - Set up development and testing environments\n   - Document current system state and configurations\n   - Establish rollback procedures and contingency plans\n   - Create migration timeline and milestones\n\n5. **Step-by-Step Migration Process**\n   \n   **Example for Framework Upgrade:**\n   ```markdown\n   ## Step 1: Environment Setup\n   1. Update development environment\n   2. Install new framework version\n   3. Update build tools and dependencies\n   4. Configure IDE and tooling\n   \n   ## Step 2: Dependencies Update\n   1. Update package.json/requirements.txt\n   2. Resolve dependency conflicts\n   3. Update related libraries\n   4. Test compatibility\n   \n   ## Step 3: Code Migration\n   1. Update import statements\n   2. Replace deprecated APIs\n   3. Update configuration files\n   4. Modify build scripts\n   ```\n\n6. **Breaking Changes Documentation**\n   - List all breaking changes with examples\n   - Provide before/after code comparisons\n   - Explain the rationale behind changes\n   - Offer alternative approaches for removed features\n\n   **Example Breaking Change:**\n   ```markdown\n   ### Removed: `oldMethod()`\n   **Before:**\n   ```javascript\n   const result = library.oldMethod(param1, param2);\n   ```\n   \n   **After:**\n   ```javascript\n   const result = library.newMethod({ \n     param1: param1, \n     param2: param2 \n   });\n   ```\n   \n   **Rationale:** Improved type safety and extensibility\n   ```\n\n7. **Configuration Changes**\n   - Document configuration file updates\n   - Explain new configuration options\n   - Provide configuration migration scripts\n   - Show environment-specific configurations\n\n8. **Database Migration (if applicable)**\n   - Create database schema migration scripts\n   - Document data transformation requirements\n   - Provide backup and restore procedures\n   - Test migration with sample data\n   - Plan for zero-downtime migrations\n\n9. **Testing Strategy**\n   - Update existing tests for new APIs\n   - Create migration-specific test cases\n   - Implement integration and E2E tests\n   - Set up performance and load testing\n   - Document test scenarios and expected outcomes\n\n10. **Performance Considerations**\n    - Document performance changes and optimizations\n    - Provide benchmarking guidelines\n    - Identify potential performance regressions\n    - Suggest monitoring and alerting updates\n    - Include memory and resource usage changes\n\n11. **Security Updates**\n    - Document security improvements and changes\n    - Update authentication and authorization code\n    - Review and update security configurations\n    - Update dependency security scanning\n    - Document new security best practices\n\n12. **Deployment Strategy**\n    - Plan phased rollout approach\n    - Create deployment scripts and automation\n    - Set up monitoring and health checks\n    - Plan for blue-green or canary deployments\n    - Document rollback procedures\n\n13. **Common Issues and Troubleshooting**\n    \n    ```markdown\n    ## Common Migration Issues\n    \n    ### Issue: Import/Module Resolution Errors\n    **Symptoms:** Cannot resolve module 'old-package'\n    **Solution:** \n    1. Update import statements to new package names\n    2. Check package.json for correct dependencies\n    3. Clear node_modules and reinstall\n    \n    ### Issue: API Method Not Found\n    **Symptoms:** TypeError: oldMethod is not a function\n    **Solution:** Replace with new API as documented in step 3\n    ```\n\n14. **Team Communication and Training**\n    - Create team training materials\n    - Schedule knowledge sharing sessions\n    - Document new development workflows\n    - Update coding standards and guidelines\n    - Create quick reference guides\n\n15. **Tools and Automation**\n    - Provide migration scripts and utilities\n    - Create code transformation tools (codemods)\n    - Set up automated compatibility checks\n    - Implement CI/CD pipeline updates\n    - Create validation and verification tools\n\n16. **Timeline and Milestones**\n    \n    ```markdown\n    ## Migration Timeline\n    \n    ### Phase 1: Preparation (Week 1-2)\n    - [ ] Environment setup\n    - [ ] Team training\n    - [ ] Development environment migration\n    \n    ### Phase 2: Development (Week 3-6)\n    - [ ] Core application migration\n    - [ ] Testing and validation\n    - [ ] Performance optimization\n    \n    ### Phase 3: Deployment (Week 7-8)\n    - [ ] Staging deployment\n    - [ ] Production deployment\n    - [ ] Monitoring and support\n    ```\n\n17. **Risk Mitigation**\n    - Identify potential migration risks\n    - Create contingency plans for each risk\n    - Document escalation procedures\n    - Plan for extended timeline scenarios\n    - Prepare communication for stakeholders\n\n18. **Post-Migration Tasks**\n    - Clean up deprecated code and configurations\n    - Update documentation and README files\n    - Review and optimize new implementation\n    - Conduct post-migration retrospective\n    - Plan for future maintenance and updates\n\n19. **Validation and Testing**\n    - Create comprehensive test plans\n    - Document acceptance criteria\n    - Set up automated regression testing\n    - Plan user acceptance testing\n    - Implement monitoring and alerting\n\n20. **Documentation Updates**\n    - Update API documentation\n    - Revise development guides\n    - Update deployment documentation\n    - Create troubleshooting guides\n    - Update team onboarding materials\n\n**Migration Types and Specific Considerations:**\n\n**Framework Migration (React 17  18):**\n- Update React and ReactDOM imports\n- Replace deprecated lifecycle methods\n- Update testing library methods\n- Handle concurrent features and Suspense\n\n**Database Migration (MySQL  PostgreSQL):**\n- Convert SQL syntax differences\n- Update data types and constraints\n- Migrate stored procedures to functions\n- Update ORM configurations\n\n**Cloud Migration (On-premise  AWS):**\n- Containerize applications\n- Update CI/CD pipelines\n- Configure cloud services\n- Implement infrastructure as code\n\n**Architecture Migration (Monolith  Microservices):**\n- Identify service boundaries\n- Implement inter-service communication\n- Set up service discovery\n- Plan data consistency strategies\n\nRemember to:\n- Test thoroughly in non-production environments first\n- Communicate progress and issues regularly\n- Document lessons learned for future migrations\n- Keep the migration guide updated based on real experiences"
              },
              {
                "name": "/troubleshooting-guide",
                "description": "Generate troubleshooting documentation",
                "path": "plugins/commands-documentation-changelogs/commands/troubleshooting-guide.md",
                "frontmatter": {
                  "description": "Generate troubleshooting documentation",
                  "category": "documentation-changelogs",
                  "argument-hint": "1. **System Overview and Architecture**"
                },
                "content": "# Troubleshooting Guide Generator Command\n\nGenerate troubleshooting documentation\n\n## Instructions\n\nFollow this systematic approach to create troubleshooting guides: **$ARGUMENTS**\n\n1. **System Overview and Architecture**\n   - Document the system architecture and components\n   - Map out dependencies and integrations\n   - Identify critical paths and failure points\n   - Create system topology diagrams\n   - Document data flow and communication patterns\n\n2. **Common Issues Identification**\n   - Collect historical support tickets and issues\n   - Interview team members about frequent problems\n   - Analyze error logs and monitoring data\n   - Review user feedback and complaints\n   - Identify patterns in system failures\n\n3. **Troubleshooting Framework**\n   - Establish systematic diagnostic procedures\n   - Create problem isolation methodologies\n   - Document escalation paths and procedures\n   - Set up logging and monitoring checkpoints\n   - Define severity levels and response times\n\n4. **Diagnostic Tools and Commands**\n   \n   ```markdown\n   ## Essential Diagnostic Commands\n   \n   ### System Health\n   ```bash\n   # Check system resources\n   top                    # CPU and memory usage\n   df -h                 # Disk space\n   free -m               # Memory usage\n   netstat -tuln         # Network connections\n   \n   # Application logs\n   tail -f /var/log/app.log\n   journalctl -u service-name -f\n   \n   # Database connectivity\n   mysql -u user -p -e \"SELECT 1\"\n   psql -h host -U user -d db -c \"SELECT 1\"\n   ```\n   ```\n\n5. **Issue Categories and Solutions**\n\n   **Performance Issues:**\n   ```markdown\n   ### Slow Response Times\n   \n   **Symptoms:**\n   - API responses > 5 seconds\n   - User interface freezing\n   - Database timeouts\n   \n   **Diagnostic Steps:**\n   1. Check system resources (CPU, memory, disk)\n   2. Review application logs for errors\n   3. Analyze database query performance\n   4. Check network connectivity and latency\n   \n   **Common Causes:**\n   - Database connection pool exhaustion\n   - Inefficient database queries\n   - Memory leaks in application\n   - Network bandwidth limitations\n   \n   **Solutions:**\n   - Restart application services\n   - Optimize database queries\n   - Increase connection pool size\n   - Scale infrastructure resources\n   ```\n\n6. **Error Code Documentation**\n   \n   ```markdown\n   ## Error Code Reference\n   \n   ### HTTP Status Codes\n   - **500 Internal Server Error**\n     - Check application logs for stack traces\n     - Verify database connectivity\n     - Check environment variables\n   \n   - **404 Not Found**\n     - Verify URL routing configuration\n     - Check if resources exist\n     - Review API endpoint documentation\n   \n   - **503 Service Unavailable**\n     - Check service health status\n     - Verify load balancer configuration\n     - Check for maintenance mode\n   ```\n\n7. **Environment-Specific Issues**\n   - Document development environment problems\n   - Address staging/testing environment issues\n   - Cover production-specific troubleshooting\n   - Include local development setup problems\n\n8. **Database Troubleshooting**\n   \n   ```markdown\n   ### Database Connection Issues\n   \n   **Symptoms:**\n   - \"Connection refused\" errors\n   - \"Too many connections\" errors\n   - Slow query performance\n   \n   **Diagnostic Commands:**\n   ```sql\n   -- Check active connections\n   SHOW PROCESSLIST;\n   \n   -- Check database size\n   SELECT table_schema, \n          ROUND(SUM(data_length + index_length) / 1024 / 1024, 1) AS 'DB Size in MB' \n   FROM information_schema.tables \n   GROUP BY table_schema;\n   \n   -- Check slow queries\n   SHOW VARIABLES LIKE 'slow_query_log';\n   ```\n   ```\n\n9. **Network and Connectivity Issues**\n   \n   ```markdown\n   ### Network Troubleshooting\n   \n   **Basic Connectivity:**\n   ```bash\n   # Test basic connectivity\n   ping example.com\n   telnet host port\n   curl -v https://api.example.com/health\n   \n   # DNS resolution\n   nslookup example.com\n   dig example.com\n   \n   # Network routing\n   traceroute example.com\n   ```\n   \n   **SSL/TLS Issues:**\n   ```bash\n   # Check SSL certificate\n   openssl s_client -connect example.com:443\n   curl -vI https://example.com\n   ```\n   ```\n\n10. **Application-Specific Troubleshooting**\n    \n    **Memory Issues:**\n    ```markdown\n    ### Out of Memory Errors\n    \n    **Java Applications:**\n    ```bash\n    # Check heap usage\n    jstat -gc [PID]\n    jmap -dump:format=b,file=heapdump.hprof [PID]\n    \n    # Analyze heap dump\n    jhat heapdump.hprof\n    ```\n    \n    **Node.js Applications:**\n    ```bash\n    # Monitor memory usage\n    node --inspect app.js\n    # Use Chrome DevTools for memory profiling\n    ```\n    ```\n\n11. **Security and Authentication Issues**\n    \n    ```markdown\n    ### Authentication Failures\n    \n    **Symptoms:**\n    - 401 Unauthorized responses\n    - Token validation errors\n    - Session timeout issues\n    \n    **Diagnostic Steps:**\n    1. Verify credentials and tokens\n    2. Check token expiration\n    3. Validate authentication service\n    4. Review CORS configuration\n    \n    **Common Solutions:**\n    - Refresh authentication tokens\n    - Clear browser cookies/cache\n    - Verify CORS headers\n    - Check API key permissions\n    ```\n\n12. **Deployment and Configuration Issues**\n    \n    ```markdown\n    ### Deployment Failures\n    \n    **Container Issues:**\n    ```bash\n    # Check container status\n    docker ps -a\n    docker logs container-name\n    \n    # Check resource limits\n    docker stats\n    \n    # Debug container\n    docker exec -it container-name /bin/bash\n    ```\n    \n    **Kubernetes Issues:**\n    ```bash\n    # Check pod status\n    kubectl get pods\n    kubectl describe pod pod-name\n    kubectl logs pod-name\n    \n    # Check service connectivity\n    kubectl get svc\n    kubectl port-forward pod-name 8080:8080\n    ```\n    ```\n\n13. **Monitoring and Alerting Setup**\n    - Configure health checks and monitoring\n    - Set up log aggregation and analysis\n    - Implement alerting for critical issues\n    - Create dashboards for system metrics\n    - Document monitoring thresholds\n\n14. **Escalation Procedures**\n    \n    ```markdown\n    ## Escalation Matrix\n    \n    ### Severity Levels\n    \n    **Critical (P1):** System down, data loss\n    - Immediate response required\n    - Escalate to on-call engineer\n    - Notify management within 30 minutes\n    \n    **High (P2):** Major functionality impaired\n    - Response within 2 hours\n    - Escalate to senior engineer\n    - Provide hourly updates\n    \n    **Medium (P3):** Minor functionality issues\n    - Response within 8 hours\n    - Assign to appropriate team member\n    - Provide daily updates\n    ```\n\n15. **Recovery Procedures**\n    - Document system recovery steps\n    - Create data backup and restore procedures\n    - Establish rollback procedures for deployments\n    - Document disaster recovery processes\n    - Test recovery procedures regularly\n\n16. **Preventive Measures**\n    - Implement monitoring and alerting\n    - Set up automated health checks\n    - Create deployment validation procedures\n    - Establish code review processes\n    - Document maintenance procedures\n\n17. **Knowledge Base Integration**\n    - Link to relevant documentation\n    - Reference API documentation\n    - Include links to monitoring dashboards\n    - Connect to team communication channels\n    - Integrate with ticketing systems\n\n18. **Team Communication**\n    \n    ```markdown\n    ## Communication Channels\n    \n    ### Immediate Response\n    - Slack: #incidents channel\n    - Phone: On-call rotation\n    - Email: alerts@company.com\n    \n    ### Status Updates\n    - Status page: status.company.com\n    - Twitter: @company_status\n    - Internal wiki: troubleshooting section\n    ```\n\n19. **Documentation Maintenance**\n    - Regular review and updates\n    - Version control for troubleshooting guides\n    - Feedback collection from users\n    - Integration with incident post-mortems\n    - Continuous improvement processes\n\n20. **Self-Service Tools**\n    - Create diagnostic scripts and tools\n    - Build automated recovery procedures\n    - Implement self-healing systems\n    - Provide user-friendly diagnostic interfaces\n    - Create chatbot integration for common issues\n\n**Advanced Troubleshooting Techniques:**\n\n**Log Analysis:**\n```bash\n# Search for specific errors\ngrep -i \"error\" /var/log/app.log | tail -50\n\n# Analyze log patterns\nawk '{print $1}' access.log | sort | uniq -c | sort -nr\n\n# Monitor logs in real-time\ntail -f /var/log/app.log | grep -i \"exception\"\n```\n\n**Performance Profiling:**\n```bash\n# System performance\niostat -x 1\nsar -u 1 10\nvmstat 1 10\n\n# Application profiling\nstrace -p [PID]\nperf record -p [PID]\n```\n\nRemember to:\n- Keep troubleshooting guides up-to-date\n- Test all documented procedures regularly\n- Collect feedback from users and improve guides\n- Include screenshots and visual aids where helpful\n- Make guides searchable and well-organized"
              },
              {
                "name": "/update-docs",
                "description": "Update implementation documentation including specs, status, and best practices",
                "path": "plugins/commands-documentation-changelogs/commands/update-docs.md",
                "frontmatter": {
                  "description": "Update implementation documentation including specs, status, and best practices",
                  "category": "documentation-changelogs",
                  "allowed-tools": "Read, Edit, Write"
                },
                "content": "# Documentation Update Command: Update Implementation Documentation\n\n## Documentation Analysis\n\n1. Review current documentation status:\n   - Check `specs/implementation_status.md` for overall project status\n   - Review implemented phase document (`specs/phase{N}_implementation_plan.md`)\n   - Review `specs/flutter_structurizr_implementation_spec.md` and `specs/flutter_structurizr_implementation_spec_updated.md`\n   - Review `specs/testing_plan.md` to ensure it is current given recent test passes, failures, and changes\n   - Examine `CLAUDE.md` and `README.md` for project-wide documentation\n   - Check for and document any new lessons learned or best practices in CLAUDE.md\n\n2. Analyze implementation and testing results:\n   - Review what was implemented in the last phase\n   - Review testing results and coverage\n   - Identify new best practices discovered during implementation\n   - Note any implementation challenges and solutions\n   - Cross-reference updated documentation with recent implementation and test results to ensure accuracy\n\n## Documentation Updates\n\n1. Update phase implementation document:\n   - Mark completed tasks with  status\n   - Update implementation percentages\n   - Add detailed notes on implementation approach\n   - Document any deviations from original plan with justification\n   - Add new sections if needed (lessons learned, best practices)\n   - Document specific implementation details for complex components\n   - Include a summary of any new troubleshooting tips or workflow improvements discovered during the phase\n\n2. Update implementation status document:\n   - Update phase completion percentages\n   - Add or update implementation status for components\n   - Add notes on implementation approach and decisions\n   - Document best practices discovered during implementation\n   - Note any challenges overcome and solutions implemented\n\n3. Update implementation specification documents:\n   - Mark completed items with  or strikethrough but preserve original requirements\n   - Add notes on implementation details where appropriate\n   - Add references to implemented files and classes\n   - Update any implementation guidance based on experience\n\n4. Update CLAUDE.md and README.md if necessary:\n   - Add new best practices\n   - Update project status\n   - Add new implementation guidance\n   - Document known issues or limitations\n   - Update usage examples to include new"
              }
            ],
            "skills": []
          },
          {
            "name": "commands-framework-svelte",
            "description": "Specialized commands for Svelte and SvelteKit development",
            "source": "./plugins/commands-framework-svelte",
            "category": "commands",
            "version": "1.0.0",
            "author": {
              "name": "BuildWithClaude Community",
              "url": "https://github.com/davepoon/buildwithclaude"
            },
            "install_commands": [
              "/plugin marketplace add davepoon/buildwithclaude",
              "/plugin install commands-framework-svelte@buildwithclaude"
            ],
            "signals": {
              "stars": 2229,
              "forks": 239,
              "pushed_at": "2026-01-12T10:19:01Z",
              "created_at": "2025-07-25T02:26:45Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/svelte-a11y",
                "description": "Audit and improve accessibility in Svelte/SvelteKit applications, ensuring WCAG compliance and inclusive user experiences.",
                "path": "plugins/commands-framework-svelte/commands/svelte-a11y.md",
                "frontmatter": {
                  "description": "Audit and improve accessibility in Svelte/SvelteKit applications, ensuring WCAG compliance and inclusive user experiences.",
                  "category": "framework-svelte"
                },
                "content": "# /svelte-a11y\n\nAudit and improve accessibility in Svelte/SvelteKit applications, ensuring WCAG compliance and inclusive user experiences.\n\n## Instructions\n\nYou are acting as the Svelte Development Agent focused on accessibility. When improving accessibility:\n\n1. **Accessibility Audit**:\n   - Run automated accessibility tests\n   - Check WCAG 2.1 AA/AAA compliance\n   - Test with screen readers\n   - Verify keyboard navigation\n   - Analyze color contrast\n   - Review ARIA usage\n\n2. **Common Issues & Fixes**:\n   \n   **Component Accessibility**:\n   ```svelte\n   <!-- Bad -->\n   <div onclick={handleClick}>Click me</div>\n   \n   <!-- Good -->\n   <button onclick={handleClick} aria-label=\"Action description\">\n     Click me\n   </button>\n   ```\n   \n   **Form Accessibility**:\n   ```svelte\n   <label for=\"email\">Email Address</label>\n   <input \n     id=\"email\"\n     type=\"email\"\n     required\n     aria-describedby=\"email-error\"\n   />\n   {#if errors.email}\n     <span id=\"email-error\" role=\"alert\">\n       {errors.email}\n     </span>\n   {/if}\n   ```\n\n3. **Navigation & Focus**:\n   ```javascript\n   // Skip links\n   <a href=\"#main\" class=\"skip-link\">Skip to main content</a>\n   \n   // Focus management\n   onMount(() => {\n     if (shouldFocus) {\n       element.focus();\n     }\n   });\n   \n   // Keyboard navigation\n   function handleKeydown(event) {\n     if (event.key === 'Escape') {\n       closeModal();\n     }\n   }\n   ```\n\n4. **ARIA Implementation**:\n   - Use semantic HTML first\n   - Add ARIA labels for clarity\n   - Implement live regions\n   - Manage focus properly\n   - Announce dynamic changes\n\n5. **Testing Tools**:\n   - Svelte a11y warnings\n   - axe-core integration\n   - Pa11y CI setup\n   - Screen reader testing\n   - Keyboard navigation testing\n\n6. **Accessibility Checklist**:\n   - [ ] All interactive elements keyboard accessible\n   - [ ] Proper heading hierarchy\n   - [ ] Images have alt text\n   - [ ] Color contrast meets standards\n   - [ ] Forms have proper labels\n   - [ ] Error messages announced\n   - [ ] Focus indicators visible\n   - [ ] Page has unique title\n   - [ ] Landmarks properly used\n   - [ ] Animations respect prefers-reduced-motion\n\n## Example Usage\n\nUser: \"Audit my e-commerce site for accessibility issues\"\n\nAssistant will:\n- Run automated accessibility scan\n- Check product cards for proper markup\n- Verify cart keyboard navigation\n- Test checkout form accessibility\n- Review color contrast on CTAs\n- Add ARIA labels where needed\n- Implement focus management\n- Create accessibility test suite\n- Provide WCAG compliance report"
              },
              {
                "name": "/svelte-component",
                "description": "Create new Svelte components with best practices, proper structure, and optional TypeScript support.",
                "path": "plugins/commands-framework-svelte/commands/svelte-component.md",
                "frontmatter": {
                  "description": "Create new Svelte components with best practices, proper structure, and optional TypeScript support.",
                  "category": "framework-svelte"
                },
                "content": "# /svelte-component\n\nCreate new Svelte components with best practices, proper structure, and optional TypeScript support.\n\n## Instructions\n\nYou are acting as the Svelte Development Agent focused on component creation. When creating components:\n\n1. **Gather Requirements**:\n   - Component name and purpose\n   - Props interface\n   - Events to emit\n   - Slots needed\n   - State management requirements\n   - TypeScript preference\n\n2. **Component Structure**:\n   ```svelte\n   <script lang=\"ts\">\n     // Imports\n     // Type definitions\n     // Props\n     // State\n     // Derived values\n     // Effects\n     // Functions\n   </script>\n   \n   <!-- Markup -->\n   \n   <style>\n     /* Scoped styles */\n   </style>\n   ```\n\n3. **Best Practices**:\n   - Use proper prop typing with TypeScript/JSDoc\n   - Implement $bindable props where appropriate\n   - Create accessible markup by default\n   - Add proper ARIA attributes\n   - Use semantic HTML elements\n   - Include keyboard navigation support\n\n4. **Component Types to Create**:\n   - **UI Components**: Buttons, Cards, Modals, etc.\n   - **Form Components**: Inputs with validation, custom form controls\n   - **Layout Components**: Headers, Sidebars, Grids\n   - **Data Components**: Tables, Lists, Data visualizations\n   - **Utility Components**: Portals, Transitions, Error boundaries\n\n5. **Additional Files**:\n   - Create accompanying test file\n   - Add Storybook story if applicable\n   - Create usage documentation\n   - Export from index file\n\n## Example Usage\n\nUser: \"Create a Modal component with customizable header, footer slots, and close functionality\"\n\nAssistant will:\n- Create Modal.svelte with proper structure\n- Implement focus trap and keyboard handling\n- Add transition effects\n- Create Modal.test.js with basic tests\n- Provide usage examples\n- Suggest accessibility improvements"
              },
              {
                "name": "/svelte-debug",
                "description": "Help debug Svelte and SvelteKit issues by analyzing error messages, stack traces, and common problems.",
                "path": "plugins/commands-framework-svelte/commands/svelte-debug.md",
                "frontmatter": {
                  "description": "Help debug Svelte and SvelteKit issues by analyzing error messages, stack traces, and common problems.",
                  "category": "framework-svelte"
                },
                "content": "# /svelte-debug\n\nHelp debug Svelte and SvelteKit issues by analyzing error messages, stack traces, and common problems.\n\n## Instructions\n\nYou are acting as the Svelte Development Agent with a focus on debugging. When the user provides an error or describes an issue:\n\n1. **Analyze the Error**:\n   - Parse error messages and stack traces\n   - Identify the root cause (compilation, runtime, or configuration)\n   - Check for common Svelte/SvelteKit pitfalls\n\n2. **Diagnose the Problem**:\n   - Examine the relevant code files\n   - Check for syntax errors, missing imports, or incorrect usage\n   - Verify configuration files (vite.config.js, svelte.config.js, etc.)\n   - Look for version mismatches or dependency conflicts\n\n3. **Common Issues to Check**:\n   - Reactive statement errors ($state, $derived, $effect)\n   - SSR vs CSR conflicts\n   - Load function errors (missing returns, incorrect data access)\n   - Form action problems\n   - Routing issues\n   - Build and deployment errors\n\n4. **Provide Solutions**:\n   - Offer specific fixes with code examples\n   - Suggest debugging techniques (console.log, {@debug}, browser DevTools)\n   - Recommend relevant documentation sections\n   - Provide step-by-step resolution guides\n\n5. **Preventive Measures**:\n   - Suggest TypeScript additions for better error catching\n   - Recommend linting rules\n   - Propose architectural improvements\n\n## Example Usage\n\nUser: \"I'm getting 'Cannot access 'user' before initialization' error in my load function\"\n\nAssistant will:\n- Examine the load function structure\n- Check for proper async/await usage\n- Verify data dependencies\n- Provide corrected code\n- Explain the fix and how to avoid similar issues"
              },
              {
                "name": "/svelte-migrate",
                "description": "Migrate Svelte/SvelteKit projects between versions, adopt new features like runes, and handle breaking changes.",
                "path": "plugins/commands-framework-svelte/commands/svelte-migrate.md",
                "frontmatter": {
                  "description": "Migrate Svelte/SvelteKit projects between versions, adopt new features like runes, and handle breaking changes.",
                  "category": "framework-svelte"
                },
                "content": "# /svelte-migrate\n\nMigrate Svelte/SvelteKit projects between versions, adopt new features like runes, and handle breaking changes.\n\n## Instructions\n\nYou are acting as the Svelte Development Agent focused on migrations. When migrating projects:\n\n1. **Migration Types**:\n   \n   **Version Migrations**:\n   - Svelte 3  Svelte 4\n   - Svelte 4  Svelte 5 (Runes)\n   - SvelteKit 1.x  SvelteKit 2.x\n   - Legacy app  Modern SvelteKit\n   \n   **Feature Migrations**:\n   - Stores  Runes ($state, $derived)\n   - Class components  Function syntax\n   - Imperative  Declarative patterns\n   - JavaScript  TypeScript\n\n2. **Migration Process**:\n   ```bash\n   # Automated migrations\n   npx sv migrate [migration-name]\n   \n   # Manual migration steps\n   1. Backup current code\n   2. Update dependencies\n   3. Run codemods\n   4. Fix breaking changes\n   5. Update configurations\n   6. Test thoroughly\n   ```\n\n3. **Runes Migration**:\n   ```javascript\n   // Before (Svelte 4)\n   let count = 0;\n   $: doubled = count * 2;\n   \n   // After (Svelte 5)\n   let count = $state(0);\n   let doubled = $derived(count * 2);\n   ```\n\n4. **Breaking Changes**:\n   - Component API changes\n   - Store subscription syntax\n   - Event handling updates\n   - SSR behavior changes\n   - Build configuration updates\n   - Package import paths\n\n5. **Migration Checklist**:\n   - [ ] Update package.json dependencies\n   - [ ] Run automated migration scripts\n   - [ ] Update component syntax\n   - [ ] Fix TypeScript errors\n   - [ ] Update configuration files\n   - [ ] Test all routes and components\n   - [ ] Update deployment scripts\n   - [ ] Review performance impacts\n\n## Example Usage\n\nUser: \"Migrate my Svelte 4 app to Svelte 5 with runes\"\n\nAssistant will:\n- Analyze current codebase\n- Create migration plan\n- Run `npx sv migrate svelte-5`\n- Convert reactive statements to runes\n- Update component props syntax\n- Fix effect timing issues\n- Update test files\n- Handle edge cases manually\n- Provide rollback strategy"
              },
              {
                "name": "/svelte-optimize",
                "description": "Optimize Svelte/SvelteKit applications for performance, including bundle size reduction, rendering optimization, and loading performance.",
                "path": "plugins/commands-framework-svelte/commands/svelte-optimize.md",
                "frontmatter": {
                  "description": "Optimize Svelte/SvelteKit applications for performance, including bundle size reduction, rendering optimization, and loading performance.",
                  "category": "framework-svelte"
                },
                "content": "# /svelte-optimize\n\nOptimize Svelte/SvelteKit applications for performance, including bundle size reduction, rendering optimization, and loading performance.\n\n## Instructions\n\nYou are acting as the Svelte Development Agent focused on performance optimization. When optimizing:\n\n1. **Performance Analysis**:\n   - Analyze bundle size with rollup-plugin-visualizer\n   - Profile component rendering\n   - Measure Core Web Vitals\n   - Identify performance bottlenecks\n   - Check network waterfall\n\n2. **Bundle Optimization**:\n   \n   **Code Splitting**:\n   ```javascript\n   // Dynamic imports\n   const HeavyComponent = await import('./HeavyComponent.svelte');\n   \n   // Route-based splitting\n   export const prerender = false;\n   export const ssr = true;\n   ```\n   \n   **Tree Shaking**:\n   - Remove unused imports\n   - Optimize library imports\n   - Use production builds\n   - Eliminate dead code\n\n3. **Rendering Optimization**:\n   \n   **Reactive Performance**:\n   ```javascript\n   // Use $state.raw for large objects\n   let data = $state.raw(largeDataset);\n   \n   // Optimize derived computations\n   let filtered = $derived.lazy(() => \n     expensiveFilter(data)\n   );\n   ```\n   \n   **Component Optimization**:\n   - Minimize re-renders\n   - Use keyed each blocks\n   - Implement virtual scrolling\n   - Lazy load components\n\n4. **Loading Performance**:\n   - Implement preloading strategies\n   - Optimize images (lazy loading, WebP)\n   - Use resource hints (preconnect, prefetch)\n   - Enable HTTP/2 push\n   - Implement service workers\n\n5. **SvelteKit Optimizations**:\n   ```javascript\n   // Prerender static pages\n   export const prerender = true;\n   \n   // Optimize data loading\n   export async function load({ fetch, setHeaders }) {\n     setHeaders({\n       'cache-control': 'public, max-age=3600'\n     });\n     \n     return {\n       data: await fetch('/api/data')\n     };\n   }\n   ```\n\n6. **Optimization Checklist**:\n   - [ ] Enable compression (gzip/brotli)\n   - [ ] Optimize fonts (subsetting, preload)\n   - [ ] Minimize CSS (PurgeCSS/Tailwind)\n   - [ ] Enable CDN/edge caching\n   - [ ] Implement critical CSS\n   - [ ] Optimize third-party scripts\n   - [ ] Use WebAssembly for heavy computation\n\n## Example Usage\n\nUser: \"My SvelteKit app is loading slowly, optimize it\"\n\nAssistant will:\n- Run performance analysis\n- Identify largest bundle chunks\n- Implement code splitting\n- Optimize images and assets\n- Add preloading for critical resources\n- Configure caching headers\n- Implement lazy loading\n- Optimize server-side rendering\n- Provide performance metrics comparison"
              },
              {
                "name": "/svelte-scaffold",
                "description": "Scaffold new SvelteKit projects, features, or modules with best practices and optimal project structure.",
                "path": "plugins/commands-framework-svelte/commands/svelte-scaffold.md",
                "frontmatter": {
                  "description": "Scaffold new SvelteKit projects, features, or modules with best practices and optimal project structure.",
                  "category": "framework-svelte"
                },
                "content": "# /svelte-scaffold\n\nScaffold new SvelteKit projects, features, or modules with best practices and optimal project structure.\n\n## Instructions\n\nYou are acting as the Svelte Development Agent focused on project scaffolding. When scaffolding:\n\n1. **Project Types**:\n   \n   **New SvelteKit Project**:\n   - Use `npx sv create` with appropriate options\n   - Select TypeScript/JSDoc preference\n   - Choose testing framework\n   - Add essential integrations (Tailwind, ESLint, etc.)\n   - Set up Git repository\n   \n   **Feature Modules**:\n   - Authentication system\n   - Admin dashboard\n   - Blog/CMS\n   - E-commerce features\n   - API integrations\n   \n   **Component Libraries**:\n   - Design system setup\n   - Storybook integration\n   - Component documentation\n   - Publishing configuration\n\n2. **Project Structure**:\n   ```\n   project/\n    src/\n       routes/\n          (app)/\n          (auth)/\n          api/\n       lib/\n          components/\n          stores/\n          utils/\n          server/\n       hooks.server.ts\n       app.html\n    tests/\n    static/\n    [config files]\n   ```\n\n3. **Essential Features**:\n   - Environment variable setup\n   - Database configuration\n   - Authentication scaffolding\n   - API route templates\n   - Error handling\n   - Logging setup\n   - Deployment configuration\n\n4. **Configuration Files**:\n   - `svelte.config.js` - Optimized settings\n   - `vite.config.js` - Build optimization\n   - `playwright.config.js` - E2E testing\n   - `tailwind.config.js` - Styling (if selected)\n   - `.env.example` - Environment template\n   - `docker-compose.yml` - Container setup\n\n5. **Starter Code**:\n   - Layout with navigation\n   - Authentication flow\n   - Protected routes\n   - Form examples\n   - API integration patterns\n   - State management setup\n\n## Example Usage\n\nUser: \"Scaffold a new SaaS starter with auth and payments\"\n\nAssistant will:\n- Create SvelteKit project with TypeScript\n- Set up authentication (Lucia/Auth.js)\n- Add payment integration (Stripe)\n- Create user dashboard structure\n- Set up database (Prisma/Drizzle)\n- Add email service\n- Configure deployment\n- Create example protected routes\n- Add subscription management"
              },
              {
                "name": "/svelte-storybook-migrate",
                "description": "Migrate Storybook configurations and stories to newer versions, including Svelte CSF v5 and @storybook/sveltekit framework.",
                "path": "plugins/commands-framework-svelte/commands/svelte-storybook-migrate.md",
                "frontmatter": {
                  "description": "Migrate Storybook configurations and stories to newer versions, including Svelte CSF v5 and @storybook/sveltekit framework.",
                  "category": "framework-svelte",
                  "allowed-tools": "Bash(npm *), Write"
                },
                "content": "# /svelte-storybook-migrate\n\nMigrate Storybook configurations and stories to newer versions, including Svelte CSF v5 and @storybook/sveltekit framework.\n\n## Instructions\n\nYou are acting as the Svelte Storybook Specialist Agent focused on migration. When migrating Storybook:\n\n1. **Version Migrations**:\n   \n   **Storybook 6.x to 7.x**:\n   ```bash\n   # Automated upgrade\n   npx storybook@latest upgrade\n   \n   # Manual steps:\n   # 1. Update dependencies\n   # 2. Migrate to @storybook/sveltekit\n   # 3. Remove obsolete packages\n   # 4. Update configuration\n   ```\n   \n   **Configuration Changes**:\n   ```javascript\n   // Old (.storybook/main.js)\n   module.exports = {\n     framework: '@storybook/svelte',\n     svelteOptions: { ... } // Remove this\n   };\n   \n   // New (.storybook/main.js)\n   export default {\n     framework: {\n       name: '@storybook/sveltekit',\n       options: {}\n     }\n   };\n   ```\n\n2. **Svelte CSF Migration (v4 to v5)**:\n   \n   **Meta Component  defineMeta**:\n   ```svelte\n   <!-- Old -->\n   <script context=\"module\">\n     import { Meta, Story } from '@storybook/addon-svelte-csf';\n   </script>\n   \n   <Meta title=\"Button\" component={Button} />\n   \n   <!-- New -->\n   <script>\n     import { defineMeta } from '@storybook/addon-svelte-csf';\n     import Button from './Button.svelte';\n     \n     const { Story } = defineMeta({\n       title: 'Button',\n       component: Button\n     });\n   </script>\n   ```\n   \n   **Template  Children/Snippets**:\n   ```svelte\n   <!-- Old -->\n   <Story name=\"Default\">\n     <Template let:args>\n       <Button {...args} />\n     </Template>\n   </Story>\n   \n   <!-- New -->\n   <Story name=\"Default\" args={{ label: 'Click' }}>\n     {#snippet template(args)}\n       <Button {...args} />\n     {/snippet}\n   </Story>\n   ```\n\n3. **Package Migration**:\n   \n   **Remove Obsolete Packages**:\n   ```bash\n   npm uninstall @storybook/svelte-vite\n   npm uninstall storybook-builder-vite\n   npm uninstall @storybook/builder-vite\n   npm uninstall @storybook/svelte\n   ```\n   \n   **Install New Packages**:\n   ```bash\n   npm install -D @storybook/sveltekit\n   npm install -D @storybook/addon-svelte-csf@latest\n   ```\n\n4. **Story Format Migration**:\n   \n   **CSF 2 to CSF 3**:\n   ```javascript\n   // Old (CSF 2)\n   export default {\n     title: 'Button',\n     component: Button\n   };\n   \n   export const Primary = (args) => ({\n     Component: Button,\n     props: args\n   });\n   Primary.args = { variant: 'primary' };\n   \n   // New (CSF 3)\n   export default {\n     title: 'Button',\n     component: Button\n   };\n   \n   export const Primary = {\n     args: { variant: 'primary' }\n   };\n   ```\n\n5. **Addon Updates**:\n   \n   **Actions  Tags**:\n   ```javascript\n   // Old\n   export default {\n     component: Button,\n     parameters: {\n       docs: { autodocs: true }\n     }\n   };\n   \n   // New\n   export default {\n     component: Button,\n     tags: ['autodocs']\n   };\n   ```\n\n6. **Module Mocking Updates**:\n   \n   **New Parameter Structure**:\n   ```javascript\n   // Old approach (custom mocks)\n   import { page } from './__mocks__/stores';\n   \n   // New approach (parameters)\n   export const Default = {\n     parameters: {\n       sveltekit_experimental: {\n         stores: { page: { ... } }\n       }\n     }\n   };\n   ```\n\n7. **Migration Script**:\n   ```javascript\n   // migration-helper.js\n   import { readdir, readFile, writeFile } from 'fs/promises';\n   import { parse, walk } from 'svelte/compiler';\n   \n   async function migrateStories() {\n     // Find all .stories.svelte files\n     // Parse and transform AST\n     // Update syntax to v5\n     // Write updated files\n   }\n   ```\n\n8. **Testing After Migration**:\n   - Run `npm run storybook`\n   - Check all stories render\n   - Verify interactions work\n   - Test addons functionality\n   - Validate build process\n\n## Migration Checklist\n\n1. [ ] Backup current setup\n2. [ ] Update Storybook to v7+\n3. [ ] Migrate to @storybook/sveltekit\n4. [ ] Update Svelte CSF addon\n5. [ ] Convert story syntax\n6. [ ] Update module mocks\n7. [ ] Test all stories\n8. [ ] Update CI/CD config\n\n## Example Usage\n\nUser: \"Migrate my Storybook from v6 with Svelte to v7 with SvelteKit\"\n\nAssistant will:\n- Analyze current setup\n- Create migration plan\n- Run upgrade command\n- Update framework config\n- Convert story formats\n- Migrate CSF syntax\n- Update module mocking\n- Test and validate\n- Document breaking changes"
              },
              {
                "name": "/svelte-storybook-mock",
                "description": "Mock SvelteKit modules and functionality in Storybook stories for isolated component development.",
                "path": "plugins/commands-framework-svelte/commands/svelte-storybook-mock.md",
                "frontmatter": {
                  "description": "Mock SvelteKit modules and functionality in Storybook stories for isolated component development.",
                  "category": "framework-svelte"
                },
                "content": "# /svelte-storybook-mock\n\nMock SvelteKit modules and functionality in Storybook stories for isolated component development.\n\n## Instructions\n\nYou are acting as the Svelte Storybook Specialist Agent focused on mocking SvelteKit modules. When setting up mocks:\n\n1. **Module Mocking Overview**:\n   \n   **Fully Supported**:\n   - `$app/environment` - Browser and version info\n   - `$app/paths` - Base paths configuration\n   - `$lib` - Library imports\n   - `@sveltejs/kit/*` - Kit utilities\n   \n   **Experimental (Requires Mocking)**:\n   - `$app/stores` - Page, navigating, updated stores\n   - `$app/navigation` - Navigation functions\n   - `$app/forms` - Form enhancement\n   \n   **Not Supported**:\n   - `$env/dynamic/private` - Server-only\n   - `$env/static/private` - Server-only\n   - `$service-worker` - Service worker context\n\n2. **Store Mocking**:\n   ```javascript\n   export const Default = {\n     parameters: {\n       sveltekit_experimental: {\n         stores: {\n           // Page store\n           page: {\n             url: new URL('https://example.com/products/123'),\n             params: { id: '123' },\n             route: {\n               id: '/products/[id]'\n             },\n             status: 200,\n             error: null,\n             data: {\n               product: {\n                 id: '123',\n                 name: 'Sample Product',\n                 price: 99.99\n               }\n             },\n             form: null\n           },\n           // Navigating store\n           navigating: {\n             from: {\n               params: { id: '122' },\n               route: { id: '/products/[id]' },\n               url: new URL('https://example.com/products/122')\n             },\n             to: {\n               params: { id: '123' },\n               route: { id: '/products/[id]' },\n               url: new URL('https://example.com/products/123')\n             },\n             type: 'link',\n             delta: 1\n           },\n           // Updated store\n           updated: true\n         }\n       }\n     }\n   };\n   ```\n\n3. **Navigation Mocking**:\n   ```javascript\n   parameters: {\n     sveltekit_experimental: {\n       navigation: {\n         goto: (url, options) => {\n           console.log('Navigating to:', url);\n           action('goto')(url, options);\n         },\n         pushState: (url, state) => {\n           console.log('Push state:', url, state);\n           action('pushState')(url, state);\n         },\n         replaceState: (url, state) => {\n           console.log('Replace state:', url, state);\n           action('replaceState')(url, state);\n         },\n         invalidate: (url) => {\n           console.log('Invalidate:', url);\n           action('invalidate')(url);\n         },\n         invalidateAll: () => {\n           console.log('Invalidate all');\n           action('invalidateAll')();\n         },\n         afterNavigate: {\n           from: null,\n           to: { url: new URL('https://example.com') },\n           type: 'enter'\n         }\n       }\n     }\n   }\n   ```\n\n4. **Form Enhancement Mocking**:\n   ```javascript\n   parameters: {\n     sveltekit_experimental: {\n       forms: {\n         enhance: (form) => {\n           console.log('Form enhanced:', form);\n           // Return cleanup function\n           return {\n             destroy() {\n               console.log('Form enhancement cleaned up');\n             }\n           };\n         }\n       }\n     }\n   }\n   ```\n\n5. **Link Handling**:\n   ```javascript\n   parameters: {\n     sveltekit_experimental: {\n       hrefs: {\n         // Exact match\n         '/products': (to, event) => {\n           console.log('Products link clicked');\n           event.preventDefault();\n         },\n         // Regex pattern\n         '/product/.*': {\n           callback: (to, event) => {\n             console.log('Product detail:', to);\n           },\n           asRegex: true\n         },\n         // API routes\n         '/api/.*': {\n           callback: (to, event) => {\n             event.preventDefault();\n             console.log('API call intercepted:', to);\n           },\n           asRegex: true\n         }\n       }\n     }\n   }\n   ```\n\n6. **Complex Mocking Scenarios**:\n   \n   **Auth State**:\n   ```javascript\n   const mockAuthenticatedUser = {\n     parameters: {\n       sveltekit_experimental: {\n         stores: {\n           page: {\n             data: {\n               user: {\n                 id: '123',\n                 email: 'user@example.com',\n                 role: 'admin'\n               },\n               session: {\n                 token: 'mock-jwt-token',\n                 expiresAt: '2024-12-31'\n               }\n             }\n           }\n         }\n       }\n     }\n   };\n   ```\n   \n   **Loading States**:\n   ```javascript\n   const mockLoadingState = {\n     parameters: {\n       sveltekit_experimental: {\n         stores: {\n           navigating: {\n             from: { url: new URL('https://example.com') },\n             to: { url: new URL('https://example.com/products') }\n           }\n         }\n       }\n     }\n   };\n   ```\n\n## Example Usage\n\nUser: \"Mock SvelteKit stores for my ProductDetail component\"\n\nAssistant will:\n- Analyze component's store dependencies\n- Create comprehensive store mocks\n- Mock page data with product info\n- Set up navigation mocks\n- Configure link handling\n- Add form enhancement if needed\n- Create multiple story variants\n- Test different states (loading, error, success)"
              },
              {
                "name": "/svelte-storybook-setup",
                "description": "Initialize and configure Storybook for SvelteKit projects with optimal settings and structure.",
                "path": "plugins/commands-framework-svelte/commands/svelte-storybook-setup.md",
                "frontmatter": {
                  "description": "Initialize and configure Storybook for SvelteKit projects with optimal settings and structure.",
                  "category": "framework-svelte",
                  "allowed-tools": "Glob"
                },
                "content": "# /svelte-storybook-setup\n\nInitialize and configure Storybook for SvelteKit projects with optimal settings and structure.\n\n## Instructions\n\nYou are acting as the Svelte Storybook Specialist Agent focused on Storybook setup. When setting up Storybook:\n\n1. **Installation Process**:\n   \n   **New Installation**:\n   ```bash\n   npx storybook@latest init\n   ```\n   \n   **Manual Setup**:\n   - Install core dependencies\n   - Configure @storybook/sveltekit framework\n   - Add essential addons\n   - Set up Svelte CSF addon\n\n2. **Configuration Files**:\n   \n   **.storybook/main.js**:\n   ```javascript\n   export default {\n     stories: ['../src/**/*.stories.@(js|ts|svelte)'],\n     addons: [\n       '@storybook/addon-essentials',\n       '@storybook/addon-svelte-csf',\n       '@storybook/addon-a11y',\n       '@storybook/addon-interactions'\n     ],\n     framework: {\n       name: '@storybook/sveltekit',\n       options: {}\n     },\n     staticDirs: ['../static']\n   };\n   ```\n   \n   **.storybook/preview.js**:\n   ```javascript\n   import '../src/app.css'; // Global styles\n   \n   export const parameters = {\n     actions: { argTypesRegex: '^on[A-Z].*' },\n     controls: {\n       matchers: {\n         color: /(background|color)$/i,\n         date: /Date$/i\n       }\n     },\n     layout: 'centered'\n   };\n   ```\n\n3. **Project Structure**:\n   ```\n   src/\n    lib/\n       components/\n           Button/\n              Button.svelte\n              Button.stories.svelte\n              Button.test.ts\n           Card/\n               Card.svelte\n               Card.stories.svelte\n    stories/\n        Introduction.mdx\n        Configure.mdx\n   ```\n\n4. **Essential Addons**:\n   - **@storybook/addon-essentials**: Core functionality\n   - **@storybook/addon-svelte-csf**: Native Svelte stories\n   - **@storybook/addon-a11y**: Accessibility testing\n   - **@storybook/addon-interactions**: Play functions\n   - **@chromatic-com/storybook**: Visual testing\n\n5. **Scripts Configuration**:\n   ```json\n   {\n     \"scripts\": {\n       \"storybook\": \"storybook dev -p 6006\",\n       \"build-storybook\": \"storybook build\",\n       \"test-storybook\": \"test-storybook\",\n       \"chromatic\": \"chromatic --exit-zero-on-changes\"\n     }\n   }\n   ```\n\n6. **SvelteKit Integration**:\n   - Configure module mocking\n   - Set up path aliases\n   - Handle SSR considerations\n   - Configure static assets\n\n## Example Usage\n\nUser: \"Set up Storybook for my new SvelteKit project\"\n\nAssistant will:\n- Check project structure and dependencies\n- Run Storybook init command\n- Configure for SvelteKit framework\n- Add Svelte CSF addon\n- Set up proper file structure\n- Create example stories\n- Configure preview settings\n- Add helpful npm scripts\n- Set up GitHub Actions for Chromatic"
              },
              {
                "name": "/svelte-storybook-story",
                "description": "Create comprehensive Storybook stories for Svelte components using modern patterns and best practices.",
                "path": "plugins/commands-framework-svelte/commands/svelte-storybook-story.md",
                "frontmatter": {
                  "description": "Create comprehensive Storybook stories for Svelte components using modern patterns and best practices.",
                  "category": "framework-svelte"
                },
                "content": "# /svelte-storybook-story\n\nCreate comprehensive Storybook stories for Svelte components using modern patterns and best practices.\n\n## Instructions\n\nYou are acting as the Svelte Storybook Specialist Agent focused on creating stories. When creating stories:\n\n1. **Analyze the Component**:\n   - Review component props and types\n   - Identify all possible states\n   - Find interactive elements\n   - Check for slots and events\n   - Note accessibility requirements\n\n2. **Story Structure (Svelte CSF)**:\n   ```svelte\n   <script>\n     import { defineMeta } from '@storybook/addon-svelte-csf';\n     import { within, userEvent, expect } from '@storybook/test';\n     import Component from './Component.svelte';\n\n     const { Story } = defineMeta({\n       component: Component,\n       title: 'Category/Component',\n       tags: ['autodocs'],\n       parameters: {\n         layout: 'centered',\n         docs: {\n           description: {\n             component: 'Component description for docs'\n           }\n         }\n       },\n       argTypes: {\n         variant: {\n           control: 'select',\n           options: ['primary', 'secondary'],\n           description: 'Visual style variant'\n         },\n         size: {\n           control: 'radio',\n           options: ['small', 'medium', 'large']\n         },\n         disabled: {\n           control: 'boolean'\n         }\n       }\n     });\n   </script>\n   ```\n\n3. **Story Patterns**:\n   \n   **Basic Story**:\n   ```svelte\n   <Story name=\"Default\" args={{ label: 'Click me' }} />\n   ```\n   \n   **With Children/Slots**:\n   ```svelte\n   <Story name=\"WithIcon\">\n     {#snippet template(args)}\n       <Component {...args}>\n         <Icon slot=\"icon\" />\n         Custom content\n       </Component>\n     {/snippet}\n   </Story>\n   ```\n   \n   **Interactive Story**:\n   ```svelte\n   <Story \n     name=\"Interactive\"\n     play={async ({ canvasElement }) => {\n       const canvas = within(canvasElement);\n       const button = canvas.getByRole('button');\n       \n       await userEvent.click(button);\n       await expect(button).toHaveTextContent('Clicked!');\n     }}\n   />\n   ```\n\n4. **Common Story Types**:\n   - **Default**: Basic component usage\n   - **Variants**: All visual variations\n   - **States**: Loading, error, success, empty\n   - **Sizes**: All size options\n   - **Interactive**: User interactions\n   - **Responsive**: Different viewports\n   - **Accessibility**: Focus and ARIA states\n   - **Edge Cases**: Long text, missing data\n\n5. **Advanced Features**:\n   \n   **Custom Render**:\n   ```svelte\n   <Story name=\"Grid\">\n     {#snippet template()}\n       <div class=\"grid grid-cols-3 gap-4\">\n         <Component variant=\"primary\" />\n         <Component variant=\"secondary\" />\n         <Component variant=\"tertiary\" />\n       </div>\n     {/snippet}\n   </Story>\n   ```\n   \n   **With Decorators**:\n   ```javascript\n   export const DarkMode = {\n     decorators: [\n       (Story) => ({\n         Component: Story,\n         props: {\n           style: 'background: #333; padding: 2rem;'\n         }\n       })\n     ]\n   };\n   ```\n\n6. **Documentation**:\n   - Use JSDoc for props\n   - Add story descriptions\n   - Include usage examples\n   - Document accessibility\n   - Add design notes\n\n## Example Usage\n\nUser: \"Create stories for my Button component\"\n\nAssistant will:\n- Analyze Button.svelte component\n- Create comprehensive stories file\n- Add all visual variants\n- Include interactive states\n- Test keyboard navigation\n- Add accessibility tests\n- Create responsive stories\n- Document all props\n- Add play functions for interactions"
              },
              {
                "name": "/svelte-storybook-troubleshoot",
                "description": "Diagnose and fix common Storybook issues in SvelteKit projects, including build errors, module problems, and configuration issues.",
                "path": "plugins/commands-framework-svelte/commands/svelte-storybook-troubleshoot.md",
                "frontmatter": {
                  "description": "Diagnose and fix common Storybook issues in SvelteKit projects, including build errors, module problems, and configuration issues.",
                  "category": "framework-svelte",
                  "allowed-tools": "Glob"
                },
                "content": "# /svelte-storybook-troubleshoot\n\nDiagnose and fix common Storybook issues in SvelteKit projects, including build errors, module problems, and configuration issues.\n\n## Instructions\n\nYou are acting as the Svelte Storybook Specialist Agent focused on troubleshooting. When diagnosing issues:\n\n1. **Common Build Errors**:\n   \n   **\"__esbuild_register_import_meta_url__ already declared\"**:\n   - Remove `svelteOptions` from `.storybook/main.js`\n   - This is a v6 to v7 migration issue\n   - Ensure using @storybook/sveltekit framework\n   \n   **Module Resolution Errors**:\n   ```javascript\n   // .storybook/main.js\n   export default {\n     framework: {\n       name: '@storybook/sveltekit',\n       options: {\n         builder: {\n           viteConfigPath: './vite.config.js'\n         }\n       }\n     },\n     viteFinal: async (config) => {\n       config.resolve.alias = {\n         ...config.resolve.alias,\n         $lib: path.resolve('./src/lib'),\n         $app: path.resolve('./.storybook/mocks/app')\n       };\n       return config;\n     }\n   };\n   ```\n\n2. **SvelteKit Module Issues**:\n   \n   **\"Cannot find module '$app/stores'\"**:\n   - These modules need mocking\n   - Use `parameters.sveltekit_experimental`\n   - Create mock files if needed:\n   ```javascript\n   // .storybook/mocks/app/stores.js\n   import { writable } from 'svelte/store';\n   \n   export const page = writable({\n     url: new URL('http://localhost:6006'),\n     params: {},\n     route: { id: '/' },\n     data: {}\n   });\n   \n   export const navigating = writable(null);\n   export const updated = writable(false);\n   ```\n\n3. **CSS and Styling Issues**:\n   \n   **Global Styles Not Loading**:\n   ```javascript\n   // .storybook/preview.js\n   import '../src/app.css';\n   import '../src/app.postcss';\n   import '../src/styles/global.css';\n   ```\n   \n   **Tailwind Not Working**:\n   ```javascript\n   // .storybook/main.js\n   export default {\n     addons: [\n       {\n         name: '@storybook/addon-postcss',\n         options: {\n           postcssLoaderOptions: {\n             implementation: require('postcss')\n           }\n         }\n       }\n     ]\n   };\n   ```\n\n4. **Component Import Issues**:\n   \n   **SSR Components**:\n   ```javascript\n   // Mark stories as client-only if needed\n   export const Default = {\n     parameters: {\n       storyshots: { disable: true } // Skip for SSR-incompatible\n     }\n   };\n   ```\n   \n   **Dynamic Imports**:\n   ```javascript\n   // Use lazy loading for heavy components\n   const HeavyComponent = lazy(() => import('./HeavyComponent.svelte'));\n   ```\n\n5. **Environment Variables**:\n   \n   **PUBLIC_ Variables Not Available**:\n   ```javascript\n   // .storybook/main.js\n   export default {\n     env: (config) => ({\n       ...config,\n       PUBLIC_API_URL: process.env.PUBLIC_API_URL || 'http://localhost:3000'\n     })\n   };\n   ```\n   \n   **Create .env for Storybook**:\n   ```bash\n   # .env.storybook\n   PUBLIC_API_URL=http://localhost:3000\n   PUBLIC_FEATURE_FLAG=true\n   ```\n\n6. **Performance Issues**:\n   \n   **Slow Build Times**:\n   - Exclude large dependencies\n   - Use production builds\n   - Enable caching\n   ```javascript\n   export default {\n     features: {\n       buildStoriesJson: true,\n       storyStoreV7: true\n     },\n     core: {\n       disableTelemetry: true\n     }\n   };\n   ```\n\n7. **Addon Conflicts**:\n   \n   **Version Mismatches**:\n   ```bash\n   # Check for version conflicts\n   npm ls @storybook/svelte\n   npm ls @storybook/sveltekit\n   \n   # Update all Storybook packages\n   npx storybook@latest upgrade\n   ```\n\n8. **Testing Issues**:\n   \n   **Play Functions Not Working**:\n   ```javascript\n   // Ensure testing library is set up\n   import { within, userEvent, expect } from '@storybook/test';\n   ```\n   \n   **Interaction Tests Failing**:\n   - Check element selectors\n   - Add proper waits\n   - Use data-testid attributes\n\n## Debugging Checklist\n\n1. [ ] Check Storybook and SvelteKit versions\n2. [ ] Verify framework configuration\n3. [ ] Check for module mocking needs\n4. [ ] Validate Vite configuration\n5. [ ] Review addon compatibility\n6. [ ] Test in isolation mode\n7. [ ] Check browser console errors\n8. [ ] Review build output\n\n## Example Usage\n\nUser: \"Storybook won't start, getting module errors\"\n\nAssistant will:\n- Check error messages\n- Identify missing module mocks\n- Set up proper aliases\n- Configure module mocking\n- Fix import paths\n- Test the solution\n- Provide debugging steps\n- Document the fix for team"
              },
              {
                "name": "/svelte-storybook",
                "description": "General-purpose Storybook assistance for SvelteKit projects, including setup guidance, best practices, and common tasks.",
                "path": "plugins/commands-framework-svelte/commands/svelte-storybook.md",
                "frontmatter": {
                  "description": "General-purpose Storybook assistance for SvelteKit projects, including setup guidance, best practices, and common tasks.",
                  "category": "framework-svelte"
                },
                "content": "# /svelte-storybook\n\nGeneral-purpose Storybook assistance for SvelteKit projects, including setup guidance, best practices, and common tasks.\n\n## Instructions\n\nYou are acting as the Svelte Storybook Specialist Agent. Provide comprehensive assistance with Storybook for SvelteKit projects.\n\n1. **Assess the Request**:\n   - Determine if it's about setup, story creation, configuration, or troubleshooting\n   - Check the current Storybook setup in the project\n   - Identify specific Storybook version and addons\n\n2. **Common Tasks**:\n   - Setting up Storybook in a SvelteKit project\n   - Creating stories for components\n   - Configuring Storybook for SvelteKit modules\n   - Adding addons and customizations\n   - Optimizing Storybook performance\n   - Setting up visual testing\n\n3. **Best Practices**:\n   - Use Svelte CSF format for native syntax\n   - Implement proper mocking for SvelteKit modules\n   - Structure stories for maintainability\n   - Document components with controls and docs\n   - Set up accessibility testing\n\n4. **Guidance Areas**:\n   - Project structure for stories\n   - Naming conventions\n   - Story organization\n   - Addon selection\n   - Testing integration\n   - CI/CD setup\n\n## Example Usage\n\nUser: \"Help me set up Storybook for my component library\"\n\nAssistant will:\n- Check if Storybook is already installed\n- Guide through installation if needed\n- Set up proper configuration\n- Create example stories\n- Configure essential addons\n- Provide project structure recommendations\n- Set up build and deployment scripts"
              },
              {
                "name": "/svelte-test-coverage",
                "description": "Analyze test coverage, identify testing gaps, and provide recommendations for improving test coverage in Svelte/SvelteKit projects.",
                "path": "plugins/commands-framework-svelte/commands/svelte-test-coverage.md",
                "frontmatter": {
                  "description": "Analyze test coverage, identify testing gaps, and provide recommendations for improving test coverage in Svelte/SvelteKit projects.",
                  "category": "framework-svelte",
                  "allowed-tools": "Bash(gh *)"
                },
                "content": "# /svelte-test-coverage\n\nAnalyze test coverage, identify testing gaps, and provide recommendations for improving test coverage in Svelte/SvelteKit projects.\n\n## Instructions\n\nYou are acting as the Svelte Testing Specialist Agent focused on test coverage analysis. When analyzing coverage:\n\n1. **Coverage Analysis**:\n   - Run coverage reports\n   - Identify untested files and functions\n   - Analyze coverage metrics (statements, branches, functions, lines)\n   - Find critical paths without tests\n\n2. **Gap Identification**:\n   \n   **Component Coverage**:\n   - Props not tested\n   - Event handlers without tests\n   - Conditional rendering paths\n   - Error states\n   - Edge cases\n   \n   **Route Coverage**:\n   - Untested load functions\n   - Form actions without tests\n   - Error boundaries\n   - Authentication flows\n   \n   **Business Logic**:\n   - Stores without tests\n   - Utility functions\n   - Data transformations\n   - API integrations\n\n3. **Priority Matrix**:\n   ```\n   High Priority:\n   - Core user flows\n   - Payment/checkout processes\n   - Authentication/authorization\n   - Data mutations\n   \n   Medium Priority:\n   - UI component variations\n   - Form validations\n   - Navigation flows\n   \n   Low Priority:\n   - Static content\n   - Simple presentational components\n   ```\n\n4. **Coverage Report Actions**:\n   - Generate visual coverage reports\n   - Create coverage badges\n   - Set up coverage thresholds\n   - Integrate with CI/CD\n\n5. **Recommendations**:\n   - Suggest specific tests to write\n   - Identify high-risk untested code\n   - Propose testing strategies\n   - Estimate effort for coverage improvement\n\n## Example Usage\n\nUser: \"Analyze test coverage for my e-commerce site\"\n\nAssistant will:\n- Run coverage analysis\n- Identify critical untested paths (checkout, payment)\n- Find components with low coverage\n- Analyze store and API coverage\n- Create prioritized test writing plan\n- Suggest coverage threshold targets\n- Provide specific test examples for gaps"
              },
              {
                "name": "/svelte-test-fix",
                "description": "Troubleshoot and fix failing tests in Svelte/SvelteKit projects, including debugging test issues and resolving common testing problems.",
                "path": "plugins/commands-framework-svelte/commands/svelte-test-fix.md",
                "frontmatter": {
                  "description": "Troubleshoot and fix failing tests in Svelte/SvelteKit projects, including debugging test issues and resolving common testing problems.",
                  "category": "framework-svelte"
                },
                "content": "# /svelte-test-fix\n\nTroubleshoot and fix failing tests in Svelte/SvelteKit projects, including debugging test issues and resolving common testing problems.\n\n## Instructions\n\nYou are acting as the Svelte Testing Specialist Agent focused on fixing test issues. When troubleshooting tests:\n\n1. **Diagnose Test Failures**:\n   - Analyze error messages and stack traces\n   - Identify failure patterns (flaky, consistent, environment-specific)\n   - Check test logs and debug output\n   - Review recent code changes\n\n2. **Common Test Issues**:\n   \n   **Component Tests**:\n   - Async timing issues  Use `await tick()` or `flushSync()`\n   - Component not cleaning up  Ensure proper unmounting\n   - State not updating  Check reactivity and bindings\n   - DOM queries failing  Use proper Testing Library queries\n   \n   **E2E Tests**:\n   - Timing issues  Add proper waits and assertions\n   - Selector problems  Use data-testid attributes\n   - Navigation failures  Check route configurations\n   - API mocking issues  Verify mock setup\n   \n   **Environment Issues**:\n   - Module resolution  Check import paths\n   - TypeScript errors  Verify test tsconfig\n   - Missing globals  Configure test environment\n   - Build conflicts  Separate test builds\n\n3. **Debugging Techniques**:\n   ```javascript\n   // Add debug helpers\n   const { debug } = render(Component);\n   debug(); // Print DOM\n   \n   // Component state inspection\n   console.log('Props:', component.$$.props);\n   console.log('Context:', component.$$.context);\n   \n   // Playwright debugging\n   await page.pause(); // Interactive debugging\n   await page.screenshot({ path: 'debug.png' });\n   ```\n\n4. **Fix Strategies**:\n   - Isolate failing tests\n   - Add detailed logging\n   - Simplify test cases\n   - Mock external dependencies\n   - Fix timing/race conditions\n\n5. **Prevention**:\n   - Add retry logic for flaky tests\n   - Improve test stability\n   - Set up better error reporting\n   - Create test utilities\n\n## Example Usage\n\nUser: \"My component tests are failing with 'Cannot access before initialization' errors\"\n\nAssistant will:\n- Analyze the test setup\n- Check component lifecycle\n- Identify initialization issues\n- Fix async/timing problems\n- Add proper test utilities\n- Ensure cleanup procedures\n- Provide debugging tips"
              },
              {
                "name": "/svelte-test-setup",
                "description": "Set up comprehensive testing infrastructure for Svelte/SvelteKit projects, including unit testing, component testing, and E2E testing frameworks.",
                "path": "plugins/commands-framework-svelte/commands/svelte-test-setup.md",
                "frontmatter": {
                  "description": "Set up comprehensive testing infrastructure for Svelte/SvelteKit projects, including unit testing, component testing, and E2E testing frameworks.",
                  "category": "framework-svelte"
                },
                "content": "# /svelte-test-setup\n\nSet up comprehensive testing infrastructure for Svelte/SvelteKit projects, including unit testing, component testing, and E2E testing frameworks.\n\n## Instructions\n\nYou are acting as the Svelte Testing Specialist Agent focused on testing infrastructure. When setting up testing:\n\n1. **Assess Current State**:\n   - Check existing test setup\n   - Identify missing testing tools\n   - Review package.json for test scripts\n   - Analyze project structure\n\n2. **Testing Stack Setup**:\n   \n   **Unit/Component Testing (Vitest)**:\n   - Install dependencies: `vitest`, `@testing-library/svelte`, `jsdom`\n   - Configure vitest.config.js\n   - Set up test helpers and utilities\n   - Create setup files\n   \n   **E2E Testing (Playwright)**:\n   - Install Playwright\n   - Configure playwright.config.js\n   - Set up test fixtures\n   - Create page object models\n   \n   **Additional Tools**:\n   - Coverage reporting (c8/istanbul)\n   - Test utilities (@testing-library/user-event)\n   - Mock service worker for API mocking\n   - Visual regression testing tools\n\n3. **Configuration Files**:\n   ```javascript\n   // vitest.config.js\n   import { sveltekit } from '@sveltejs/kit/vite';\n   import { defineConfig } from 'vitest/config';\n   \n   export default defineConfig({\n     plugins: [sveltekit()],\n     test: {\n       environment: 'jsdom',\n       setupFiles: ['./src/tests/setup.ts'],\n       coverage: {\n         reporter: ['text', 'html', 'lcov']\n       }\n     }\n   });\n   ```\n\n4. **Test Structure**:\n   ```\n   src/\n    tests/\n       setup.ts\n       helpers/\n       fixtures/\n    routes/\n       +page.test.ts\n    lib/\n        Component.test.ts\n   ```\n\n5. **NPM Scripts**:\n   - `test`: Run all tests\n   - `test:unit`: Run unit tests\n   - `test:e2e`: Run E2E tests\n   - `test:coverage`: Generate coverage report\n   - `test:watch`: Run tests in watch mode\n\n## Example Usage\n\nUser: \"Set up testing for my new SvelteKit project\"\n\nAssistant will:\n- Analyze current project setup\n- Install and configure Vitest\n- Install and configure Playwright\n- Create test configuration files\n- Set up test utilities and helpers\n- Add comprehensive npm scripts\n- Create example tests\n- Set up CI/CD test workflows"
              },
              {
                "name": "/svelte-test",
                "description": "Create comprehensive tests for Svelte components and SvelteKit routes, including unit tests, component tests, and E2E tests.",
                "path": "plugins/commands-framework-svelte/commands/svelte-test.md",
                "frontmatter": {
                  "description": "Create comprehensive tests for Svelte components and SvelteKit routes, including unit tests, component tests, and E2E tests.",
                  "category": "framework-svelte"
                },
                "content": "# /svelte-test\n\nCreate comprehensive tests for Svelte components and SvelteKit routes, including unit tests, component tests, and E2E tests.\n\n## Instructions\n\nYou are acting as the Svelte Testing Specialist Agent. When creating tests:\n\n1. **Analyze the Target**:\n   - Identify what needs testing (component, route, store, utility)\n   - Determine appropriate test types (unit, integration, E2E)\n   - Review existing test patterns in the codebase\n\n2. **Test Creation Strategy**:\n   - **Component Tests**: User interactions, prop variations, slots, events\n   - **Route Tests**: Load functions, form actions, error handling\n   - **Store Tests**: State changes, derived values, subscriptions\n   - **E2E Tests**: User flows, navigation, form submissions\n\n3. **Test Structure**:\n   ```javascript\n   // Component Test Example\n   import { render, fireEvent } from '@testing-library/svelte';\n   import { expect, test, describe } from 'vitest';\n   \n   describe('Component', () => {\n     test('user interaction', async () => {\n       // Arrange\n       // Act\n       // Assert\n     });\n   });\n   ```\n\n4. **Coverage Areas**:\n   - Happy path scenarios\n   - Edge cases and error states\n   - Accessibility requirements\n   - Performance constraints\n   - Security considerations\n\n5. **Test Types to Generate**:\n   - Vitest unit/component tests\n   - Playwright E2E tests\n   - Accessibility tests\n   - Performance tests\n   - Visual regression tests\n\n## Example Usage\n\nUser: \"Create tests for my UserProfile component that has edit mode\"\n\nAssistant will:\n- Analyze UserProfile component structure\n- Create comprehensive component tests\n- Test view/edit mode transitions\n- Test form validation in edit mode\n- Add accessibility tests\n- Create E2E test for full user flow\n- Suggest additional test scenarios"
              }
            ],
            "skills": []
          },
          {
            "name": "commands-game-development",
            "description": "Commands for game development workflows",
            "source": "./plugins/commands-game-development",
            "category": "commands",
            "version": "1.0.0",
            "author": {
              "name": "BuildWithClaude Community",
              "url": "https://github.com/davepoon/buildwithclaude"
            },
            "install_commands": [
              "/plugin marketplace add davepoon/buildwithclaude",
              "/plugin install commands-game-development@buildwithclaude"
            ],
            "signals": {
              "stars": 2229,
              "forks": 239,
              "pushed_at": "2026-01-12T10:19:01Z",
              "created_at": "2025-07-25T02:26:45Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/unity-project-setup",
                "description": "Sets up a professional Unity project with industry-standard structure and configurations",
                "path": "plugins/commands-game-development/commands/unity-project-setup.md",
                "frontmatter": {
                  "description": "Sets up a professional Unity project with industry-standard structure and configurations",
                  "category": "game-development",
                  "allowed-tools": "Edit, Write"
                },
                "content": "# Unity Project Setup Command\n\nSets up a professional Unity project with industry-standard structure and configurations.\n\n## What it creates:\n\n### Project Structure\n```\nAssets/\n _Project/\n    Scripts/\n       Managers/\n       Player/\n       UI/\n       Gameplay/\n       Utilities/\n    Art/\n       Textures/\n       Materials/\n       Models/\n       Animations/\n    Audio/\n       Music/\n       SFX/\n       Voice/\n    Prefabs/\n       Characters/\n       Environment/\n       UI/\n       Effects/\n    Scenes/\n       Development/\n       Production/\n       Testing/\n    Settings/\n       Input/\n       Rendering/\n       Audio/\n    Resources/\n Plugins/\n StreamingAssets/\n Editor/\n     Scripts/\n     Resources/\n```\n\n### Essential Packages\n- Universal Render Pipeline (URP)\n- Input System\n- Cinemachine\n- ProBuilder\n- Timeline\n- Addressables\n- Unity Analytics\n- Version Control (if available)\n\n### Project Settings\n- Optimized quality settings for target platforms\n- Input system configuration\n- Physics settings\n- Time and rendering configurations\n- Build settings for multiple platforms\n\n### Development Tools\n- Code formatting rules (.editorconfig)\n- Git configuration with Unity-optimized .gitignore\n- Assembly definition files for better compilation\n- Custom editor scripts for workflow improvement\n\n### Version Control Setup\n- Git repository initialization\n- Unity-specific .gitignore\n- LFS configuration for large assets\n- Branching strategy documentation\n\n## Usage:\n\n```bash\nnpx claude-code-templates@latest --command unity-project-setup\n```\n\n## Interactive Options:\n\n1. **Project Type Selection**\n   - 2D Game\n   - 3D Game\n   - Mobile Game\n   - VR/AR Game\n   - Hybrid (2D/3D)\n\n2. **Target Platforms**\n   - PC (Windows/Mac/Linux)\n   - Mobile (iOS/Android)\n   - Console (PlayStation/Xbox/Nintendo)\n   - WebGL\n   - VR (Oculus/SteamVR)\n\n3. **Version Control**\n   - Git\n   - Plastic SCM\n   - Perforce\n   - None\n\n4. **Additional Packages**\n   - TextMeshPro\n   - Post Processing\n   - Unity Ads\n   - Unity Analytics\n   - Unity Cloud Build\n   - Custom package selection\n\n## Generated Files:\n\n### Core Scripts\n- `GameManager.cs` - Main game controller\n- `SceneLoader.cs` - Scene management system\n- `AudioManager.cs` - Audio system controller\n- `InputManager.cs` - Input handling system\n- `UIManager.cs` - UI system manager\n- `SaveSystem.cs` - Save/load functionality\n\n### Editor Tools\n- `ProjectSetupWindow.cs` - Custom editor window\n- `SceneQuickStart.cs` - Scene setup automation\n- `AssetValidator.cs` - Asset validation tools\n- `BuildAutomation.cs` - Build pipeline helpers\n\n### Configuration Files\n- `ProjectSettings.asset` - Optimized project settings\n- `QualitySettings.asset` - Multi-platform quality tiers\n- `InputActions.inputactions` - Input system configuration\n- `AssemblyDefinitions` - Modular compilation setup\n\n### Documentation\n- `README.md` - Project overview and setup instructions\n- `CONTRIBUTING.md` - Development guidelines\n- `CHANGELOG.md` - Version history template\n- `API_REFERENCE.md` - Code documentation template\n\n## Post-Setup Checklist:\n\n- [ ] Review and adjust quality settings for target platforms\n- [ ] Configure input actions for your game controls\n- [ ] Set up build configurations for all target platforms\n- [ ] Review folder structure and rename as needed\n- [ ] Configure version control and make initial commit\n- [ ] Set up continuous integration if required\n- [ ] Configure analytics and crash reporting\n- [ ] Review and customize coding standards\n\n## Platform-Specific Configurations:\n\n### Mobile\n- Touch input configuration\n- Performance optimization settings\n- Battery usage optimization\n- App store submission setup\n\n### PC\n- Multi-resolution support\n- Keyboard/mouse input setup\n- Graphics options menu template\n- Windows/Mac/Linux build configs\n\n### Console\n- Platform-specific input mapping\n- Achievement/trophy integration setup\n- Online services configuration\n- Certification requirement templates\n\nThis command creates a production-ready Unity project structure that scales from prototype to shipped game, following industry best practices and Unity's recommended patterns."
              }
            ],
            "skills": []
          },
          {
            "name": "commands-integration-sync",
            "description": "Commands for integrating with external services and syncing data",
            "source": "./plugins/commands-integration-sync",
            "category": "commands",
            "version": "1.0.0",
            "author": {
              "name": "BuildWithClaude Community",
              "url": "https://github.com/davepoon/buildwithclaude"
            },
            "install_commands": [
              "/plugin marketplace add davepoon/buildwithclaude",
              "/plugin install commands-integration-sync@buildwithclaude"
            ],
            "signals": {
              "stars": 2229,
              "forks": 239,
              "pushed_at": "2026-01-12T10:19:01Z",
              "created_at": "2025-07-25T02:26:45Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/bidirectional-sync",
                "description": "Enable bidirectional GitHub-Linear synchronization",
                "path": "plugins/commands-integration-sync/commands/bidirectional-sync.md",
                "frontmatter": {
                  "description": "Enable bidirectional GitHub-Linear synchronization",
                  "category": "integration-sync"
                },
                "content": "# bidirectional-sync\n\nEnable bidirectional GitHub-Linear synchronization\n\n## System\n\nYou are a bidirectional synchronization specialist that maintains consistency between GitHub Issues and Linear tasks. You handle conflict resolution, prevent sync loops, and ensure data integrity across both platforms.\n\n## Instructions\n\nWhen implementing bidirectional sync:\n\n1. **Prerequisites & Setup**\n   - Verify both GitHub CLI and Linear MCP\n   - Initialize sync state storage\n   - Set up webhook endpoints (if available)\n\n2. **Sync State Management**\n   ```json\n   {\n     \"syncVersion\": \"1.0\",\n     \"lastFullSync\": \"2025-01-16T10:00:00Z\",\n     \"entities\": {\n       \"gh-123\": {\n         \"linearId\": \"ABC-456\",\n         \"githubNumber\": 123,\n         \"lastGithubUpdate\": \"2025-01-16T09:00:00Z\",\n         \"lastLinearUpdate\": \"2025-01-16T09:30:00Z\",\n         \"syncHash\": \"a1b2c3d4e5f6\",\n         \"lockedBy\": null\n       }\n     }\n   }\n   ```\n\n3. **Conflict Detection**\n   ```javascript\n   function detectConflict(entity) {\n     const githubChanged = entity.githubUpdated > entity.lastSync;\n     const linearChanged = entity.linearUpdated > entity.lastSync;\n     \n     if (githubChanged && linearChanged) {\n       return {\n         type: 'BOTH_CHANGED',\n         githubDelta: calculateDelta(entity.githubOld, entity.githubNew),\n         linearDelta: calculateDelta(entity.linearOld, entity.linearNew)\n       };\n     }\n     return null;\n   }\n   ```\n\n4. **Conflict Resolution Strategies**\n   ```\n   Strategy Options:\n    NEWER_WINS (default)\n    GITHUB_WINS\n    LINEAR_WINS\n    MANUAL_MERGE\n    FIELD_LEVEL_MERGE\n   ```\n\n5. **Field-Level Merge Rules**\n   ```javascript\n   const mergeRules = {\n     title: 'NEWER_WINS',\n     description: 'MERGE_CHANGES',\n     state: 'NEWER_WINS',\n     assignee: 'NEWER_WINS',\n     labels: 'UNION_MERGE',\n     priority: 'LINEAR_WINS',\n     comments: 'APPEND_ALL'\n   };\n   ```\n\n6. **Sync Loop Prevention**\n   ```javascript\n   // Add sync markers to prevent loops\n   const SYNC_MARKER = '[sync-bot]';\n   \n   function shouldSync(change) {\n     // Skip if change was made by sync bot\n     if (change.author === SYNC_BOT_ID) return false;\n     \n     // Skip if within grace period of last sync\n     const gracePeriod = 30000; // 30 seconds\n     if (Date.now() - lastSyncTime < gracePeriod) return false;\n     \n     // Check for sync marker in comments\n     if (change.body?.includes(SYNC_MARKER)) return false;\n     \n     return true;\n   }\n   ```\n\n7. **Bidirectional Field Mapping**\n   ```yaml\n   mappings:\n     # GitHub  Linear\n     - source: github.title\n       target: linear.title\n       transform: direct\n     \n     # Linear  GitHub  \n     - source: linear.identifier\n       target: github.body\n       transform: appendToFooter\n     \n     # Special handling\n     - source: github.labels\n       target: linear.labels\n       transform: mapLabels\n       reverse: true\n   ```\n\n8. **Transaction Management**\n   ```javascript\n   async function syncTransaction(syncOp) {\n     const transaction = await beginTransaction();\n     try {\n       // Lock both entities\n       await lockGitHub(syncOp.githubId);\n       await lockLinear(syncOp.linearId);\n       \n       // Perform sync\n       await syncOp.execute();\n       \n       // Update sync state\n       await updateSyncState(syncOp);\n       \n       await transaction.commit();\n     } catch (error) {\n       await transaction.rollback();\n       throw error;\n     } finally {\n       await unlockAll();\n     }\n   }\n   ```\n\n9. **Webhook Integration**\n   ```javascript\n   // GitHub webhook handler\n   app.post('/webhook/github', async (req, res) => {\n     const event = req.headers['x-github-event'];\n     if (shouldSync(req.body)) {\n       await queueSync({\n         source: 'github',\n         event: event,\n         data: req.body\n       });\n     }\n   });\n   \n   // Linear webhook handler\n   app.post('/webhook/linear', async (req, res) => {\n     if (shouldSync(req.body)) {\n       await queueSync({\n         source: 'linear',\n         event: req.body.type,\n         data: req.body\n       });\n     }\n   });\n   ```\n\n10. **Sync Execution Flow**\n    ```\n    1. Fetch all changes since last sync\n    2. Build sync queue with priorities\n    3. Process each item:\n       a. Check for conflicts\n       b. Apply resolution strategy\n       c. Update both platforms\n       d. Record sync state\n    4. Handle failures and retries\n    5. Generate sync report\n    ```\n\n## Examples\n\n### Initial Setup\n```bash\n# Initialize bidirectional sync\nclaude bidirectional-sync --init --repo=\"owner/repo\" --team=\"ENG\"\n\n# Configure sync options\nclaude bidirectional-sync --config \\\n  --conflict-strategy=\"NEWER_WINS\" \\\n  --sync-interval=\"5m\" \\\n  --webhook-secret=\"your-secret\"\n```\n\n### Manual Sync\n```bash\n# Full bidirectional sync\nclaude bidirectional-sync --full\n\n# Incremental sync (default)\nclaude bidirectional-sync\n\n# Dry run to preview changes\nclaude bidirectional-sync --dry-run\n```\n\n### Conflict Resolution\n```bash\n# Use specific strategy\nclaude bidirectional-sync --conflict-strategy=\"LINEAR_WINS\"\n\n# Interactive conflict resolution\nclaude bidirectional-sync --interactive\n\n# Force sync despite conflicts\nclaude bidirectional-sync --force\n```\n\n## Output Format\n\n```\nBidirectional Sync Report\n=========================\nPeriod: 2025-01-16 10:00:00 - 10:15:00\nMode: Incremental\n\nChanges Detected:\n- GitHub  Linear: 12 updates\n- Linear  GitHub: 8 updates\n- Conflicts: 3\n\nSync Results:\n GitHub #123  Linear ABC-456: Title updated (GitHub  Linear)\n GitHub #124  Linear ABC-457: Status changed (Linear  GitHub)\n GitHub #125  Linear ABC-458: Conflict resolved (NEWER_WINS)\n GitHub #126  Linear ABC-459: New task created\n Linear ABC-460  GitHub #127: New issue created\n\nConflict Details:\n1. #125  ABC-458:\n   - Field: description\n   - GitHub changed: 10:05:00\n   - Linear changed: 10:07:00\n   - Resolution: Used Linear version (newer)\n\nPerformance:\n- Total time: 15.3s\n- API calls: 45 (GitHub: 25, Linear: 20)\n- Rate limit status: OK\n\nNext sync: 2025-01-16 10:20:00\n```\n\n## Advanced Configuration\n\n### Sync Rules File\n```yaml\n# .github/linear-sync.yml\nversion: 1.0\nsync:\n  enabled: true\n  direction: bidirectional\n  interval: 5m\n  \nrules:\n  - name: \"Bug Priority Sync\"\n    condition:\n      github:\n        labels: [\"bug\"]\n    action:\n      linear:\n        priority: 1\n        \n  - name: \"Skip Draft Issues\"\n    condition:\n      github:\n        labels: [\"draft\"]\n    action:\n      skip: true\n\nconflicts:\n  strategy: NEWER_WINS\n  manual_review:\n    - title\n    - milestone\n    \nwebhooks:\n  github:\n    secret: ${GITHUB_WEBHOOK_SECRET}\n  linear:\n    secret: ${LINEAR_WEBHOOK_SECRET}\n```\n\n## Best Practices\n\n1. **Consistency Guarantees**\n   - Use distributed locks\n   - Implement idempotent operations\n   - Maintain audit logs\n\n2. **Performance Optimization**\n   - Batch similar operations\n   - Use caching for mappings\n   - Implement smart diffing\n\n3. **Error Handling**\n   - Exponential backoff for retries\n   - Dead letter queue for failures\n   - Alert on repeated failures\n\n4. **Monitoring**\n   - Track sync lag time\n   - Monitor conflict frequency\n   - Alert on sync failures"
              },
              {
                "name": "/bulk-import-issues",
                "description": "Bulk import GitHub issues to Linear",
                "path": "plugins/commands-integration-sync/commands/bulk-import-issues.md",
                "frontmatter": {
                  "description": "Bulk import GitHub issues to Linear",
                  "category": "integration-sync"
                },
                "content": "# bulk-import-issues\n\nBulk import GitHub issues to Linear\n\n## System\n\nYou are a bulk import specialist that efficiently transfers large numbers of GitHub issues to Linear. You handle rate limits, provide progress feedback, manage errors gracefully, and ensure data integrity during mass operations.\n\n## Instructions\n\nWhen performing bulk imports:\n\n1. **Pre-import Analysis**\n   ```javascript\n   async function analyzeImport(filters) {\n     const issues = await fetchGitHubIssues(filters);\n     \n     return {\n       totalIssues: issues.length,\n       byState: groupBy(issues, 'state'),\n       byLabel: groupBy(issues, issue => issue.labels[0]?.name),\n       byMilestone: groupBy(issues, 'milestone.title'),\n       estimatedTime: estimateImportTime(issues.length),\n       apiCallsRequired: calculateAPICalls(issues),\n       \n       warnings: [\n         issues.length > 500 && 'Large import may take significant time',\n         hasRateLimitRisk(issues.length) && 'May hit rate limits',\n         hasDuplicates(issues) && 'Potential duplicates detected'\n       ].filter(Boolean)\n     };\n   }\n   ```\n\n2. **Batch Configuration**\n   ```javascript\n   const BATCH_CONFIG = {\n     size: 20,                    // Items per batch\n     delayBetweenBatches: 2000,   // 2 seconds\n     maxConcurrent: 5,            // Parallel operations\n     retryAttempts: 3,\n     backoffMultiplier: 2,\n     \n     // Dynamic adjustment\n     adjustBatchSize(performance) {\n       if (performance.errorRate > 0.1) return Math.max(5, this.size / 2);\n       if (performance.avgTime > 5000) return Math.max(10, this.size - 5);\n       if (performance.avgTime < 1000) return Math.min(50, this.size + 5);\n       return this.size;\n     }\n   };\n   ```\n\n3. **Import Pipeline**\n   ```javascript\n   class BulkImportPipeline {\n     constructor(issues, options) {\n       this.queue = issues;\n       this.processed = [];\n       this.failed = [];\n       this.options = options;\n       this.startTime = Date.now();\n     }\n     \n     async execute() {\n       // Pre-process\n       await this.validate();\n       await this.deduplicate();\n       \n       // Process in batches\n       while (this.queue.length > 0) {\n         const batch = this.queue.splice(0, BATCH_CONFIG.size);\n         await this.processBatch(batch);\n         await this.updateProgress();\n         await this.checkRateLimits();\n       }\n       \n       // Post-process\n       await this.reconcile();\n       return this.generateReport();\n     }\n   }\n   ```\n\n4. **Progress Tracking**\n   ```javascript\n   class ProgressTracker {\n     constructor(total) {\n       this.total = total;\n       this.completed = 0;\n       this.failed = 0;\n       this.startTime = Date.now();\n     }\n     \n     update(success = true) {\n       success ? this.completed++ : this.failed++;\n       this.render();\n     }\n     \n     render() {\n       const progress = (this.completed + this.failed) / this.total;\n       const elapsed = Date.now() - this.startTime;\n       const eta = (elapsed / progress) - elapsed;\n       \n       console.log(`\n   Importing GitHub Issues to Linear\n   \n   \n   Progress: [${''.repeat(progress * 30)}${' '.repeat(30 - progress * 30)}] ${(progress * 100).toFixed(1)}%\n   \n   Completed: ${this.completed}/${this.total}\n   Failed: ${this.failed}\n   Rate: ${(this.completed / (elapsed / 1000)).toFixed(1)} issues/sec\n   ETA: ${formatTime(eta)}\n   \n   Current: ${this.currentItem?.title || 'Processing...'}\n       `);\n     }\n   }\n   ```\n\n5. **Error Handling**\n   ```javascript\n   async function handleImportError(issue, error, attempt) {\n     const errorType = classifyError(error);\n     \n     switch (errorType) {\n       case 'RATE_LIMIT':\n         await waitForRateLimit(error);\n         return 'RETRY';\n         \n       case 'DUPLICATE':\n         logDuplicate(issue);\n         return 'SKIP';\n         \n       case 'VALIDATION':\n         const fixed = await tryAutoFix(issue, error);\n         return fixed ? 'RETRY' : 'FAIL';\n         \n       case 'NETWORK':\n         if (attempt < BATCH_CONFIG.retryAttempts) {\n           await exponentialBackoff(attempt);\n           return 'RETRY';\n         }\n         return 'FAIL';\n         \n       default:\n         return 'FAIL';\n     }\n   }\n   ```\n\n6. **Data Transformation**\n   ```javascript\n   async function transformIssuesBatch(issues) {\n     return Promise.all(issues.map(async issue => {\n       try {\n         return {\n           title: sanitizeTitle(issue.title),\n           description: await enhanceDescription(issue),\n           priority: calculatePriority(issue),\n           state: mapState(issue.state),\n           labels: await mapLabels(issue.labels),\n           assignee: await findLinearUser(issue.assignee),\n           \n           metadata: {\n             githubNumber: issue.number,\n             githubUrl: issue.html_url,\n             importedAt: new Date().toISOString(),\n             importBatch: this.batchId\n           }\n         };\n       } catch (error) {\n         return { error, issue };\n       }\n     }));\n   }\n   ```\n\n7. **Duplicate Detection**\n   ```javascript\n   async function checkDuplicates(issues) {\n     const existingTasks = await linear.issues({\n       filter: { \n         externalId: { in: issues.map(i => `gh-${i.number}`) }\n       }\n     });\n     \n     const duplicates = new Map();\n     for (const task of existingTasks) {\n       duplicates.set(task.externalId, task);\n     }\n     \n     return {\n       hasDuplicates: duplicates.size > 0,\n       duplicates: duplicates,\n       unique: issues.filter(i => !duplicates.has(`gh-${i.number}`))\n     };\n   }\n   ```\n\n8. **Rate Limit Management**\n   ```javascript\n   class RateLimitManager {\n     constructor() {\n       this.github = { limit: 5000, remaining: 5000, reset: null };\n       this.linear = { limit: 1500, remaining: 1500, reset: null };\n     }\n     \n     async checkAndWait() {\n       // Update current limits\n       await this.updateLimits();\n       \n       // GitHub check\n       if (this.github.remaining < 100) {\n         const waitTime = this.github.reset - Date.now();\n         console.log(` GitHub rate limit pause: ${waitTime}ms`);\n         await sleep(waitTime);\n       }\n       \n       // Linear check\n       if (this.linear.remaining < 50) {\n         const waitTime = this.linear.reset - Date.now();\n         console.log(` Linear rate limit pause: ${waitTime}ms`);\n         await sleep(waitTime);\n       }\n       \n       // Adaptive throttling\n       const usage = 1 - (this.linear.remaining / this.linear.limit);\n       if (usage > 0.8) {\n         BATCH_CONFIG.delayBetweenBatches *= 1.5;\n       }\n     }\n   }\n   ```\n\n9. **Import Options**\n   ```javascript\n   const importOptions = {\n     // Filtering\n     labels: ['bug', 'enhancement'],\n     milestone: 'v2.0',\n     state: 'open',\n     since: '2025-01-01',\n     \n     // Mapping\n     teamId: 'engineering',\n     projectId: 'product-backlog',\n     defaultPriority: 3,\n     \n     // Behavior\n     skipDuplicates: true,\n     updateExisting: false,\n     preserveClosedState: false,\n     importComments: true,\n     importAttachments: false,\n     \n     // Performance\n     batchSize: 25,\n     maxConcurrent: 5,\n     timeout: 30000\n   };\n   ```\n\n10. **Post-Import Actions**\n    ```javascript\n    async function postImportTasks(report) {\n      // Create import summary\n      await createImportSummary(report);\n      \n      // Update GitHub issues with Linear links\n      if (options.updateGitHub) {\n        await updateGitHubIssues(report.successful);\n      }\n      \n      // Generate mapping file\n      await saveMappingFile({\n        timestamp: new Date().toISOString(),\n        mappings: report.mappings,\n        failed: report.failed\n      });\n      \n      // Send notifications\n      if (options.notify) {\n        await sendImportNotification(report);\n      }\n    }\n    ```\n\n## Examples\n\n### Basic Bulk Import\n```bash\n# Import all open issues\nclaude bulk-import-issues\n\n# Import with filters\nclaude bulk-import-issues --state=\"open\" --label=\"bug\"\n\n# Import specific milestone\nclaude bulk-import-issues --milestone=\"v2.0\"\n```\n\n### Advanced Import\n```bash\n# Custom batch settings\nclaude bulk-import-issues \\\n  --batch-size=50 \\\n  --delay=1000 \\\n  --max-concurrent=10\n\n# With mapping options\nclaude bulk-import-issues \\\n  --team=\"backend\" \\\n  --project=\"Q1-2025\" \\\n  --default-priority=\"medium\"\n\n# Skip duplicates and import comments\nclaude bulk-import-issues \\\n  --skip-duplicates \\\n  --import-comments \\\n  --update-github\n```\n\n### Recovery and Resume\n```bash\n# Dry run first\nclaude bulk-import-issues --dry-run\n\n# Resume failed import\nclaude bulk-import-issues --resume-from=\"import-12345.json\"\n\n# Retry only failed items\nclaude bulk-import-issues --retry-failed=\"import-12345.json\"\n```\n\n## Output Format\n\n```\nBulk Import Report\n==================\nStarted: 2025-01-16 10:00:00\nCompleted: 2025-01-16 10:15:32\n\nImport Summary:\n\nTotal Issues    : 523\nSuccessful      : 518 (99.0%)\nFailed          : 3 (0.6%)\nSkipped (Dupes) : 2 (0.4%)\n\nPerformance Metrics:\n- Total Duration: 15m 32s\n- Average Speed: 33.5 issues/minute\n- API Calls: 1,047 (GitHub: 523, Linear: 524)\n- Rate Limits: OK (GitHub: 4,477/5000, Linear: 976/1500)\n\nFailed Imports:\n1. Issue #234: \"Invalid assignee email\"\n2. Issue #456: \"Network timeout after 3 retries\"\n3. Issue #789: \"Label mapping failed\"\n\nBatch Performance:\nBatch 1-5   :  100% (2.1s avg)\nBatch 6-10  :  100% (1.8s avg)\nBatch 11-15 :  100% (2.3s avg)\n...\nBatch 26    :   78% (3 failed)\n\nActions Taken:\n Created 518 Linear tasks\n Mapped 45 unique labels\n Assigned to 12 team members\n Added to 3 projects\n Imported 1,234 comments\n Updated GitHub issues with Linear links\n\nMapping File: imports/bulk-import-2025-01-16-100000.json\n```\n\n## Error Recovery\n\n```javascript\n// Resume interrupted import\nasync function resumeImport(stateFile) {\n  const state = await loadImportState(stateFile);\n  \n  console.log(`\nResuming Import\n\nPrevious progress: ${state.completed}/${state.total}\nFailed items: ${state.failed.length}\nResuming from: Issue #${state.lastProcessed}\n  `);\n  \n  const remaining = state.queue.slice(state.position);\n  const pipeline = new BulkImportPipeline(remaining, state.options);\n  pipeline.processed = state.processed;\n  pipeline.failed = state.failed;\n  \n  return pipeline.execute();\n}\n```\n\n## Best Practices\n\n1. **Pre-Import Validation**\n   - Always run dry-run first\n   - Check for duplicates\n   - Validate mappings\n\n2. **Performance Optimization**\n   - Start with smaller batch sizes\n   - Monitor and adjust dynamically\n   - Use off-peak hours for large imports\n\n3. **Data Integrity**\n   - Save import mappings\n   - Enable rollback capability\n   - Verify post-import data\n\n4. **Error Management**\n   - Implement comprehensive logging\n   - Save failed items for retry\n   - Provide clear error messages"
              },
              {
                "name": "/cross-reference-manager",
                "description": "Manage cross-platform reference links",
                "path": "plugins/commands-integration-sync/commands/cross-reference-manager.md",
                "frontmatter": {
                  "description": "Manage cross-platform reference links",
                  "category": "integration-sync",
                  "argument-hint": "Valid actions: audit, repair, map, validate, export"
                },
                "content": "# Cross-Reference Manager\n\nManage cross-platform reference links\n\n## Instructions\n\n1. **Check Tool Availability**\n   - Verify GitHub CLI (`gh`) is installed and authenticated\n   - Check if Linear MCP server is connected\n   - If either tool is missing, provide setup instructions\n\n2. **Parse Command Arguments**\n   - Extract the action from command arguments: **$ARGUMENTS**\n   - Valid actions: audit, repair, map, validate, export\n   - Parse any additional options provided\n\n3. **Initialize Reference Database**\n   - Create or load existing reference mapping database\n   - Structure should track:\n     - GitHub issue ID  Linear task ID\n     - GitHub PR ID  Linear task ID\n     - Comment references\n     - User mappings\n     - Timestamps and sync history\n\n4. **Execute Selected Action**\n   Based on the action provided:\n\n   ### Audit Action\n   - Scan all GitHub issues and PRs for Linear references\n   - Query Linear for all tasks with GitHub references\n   - Identify:\n     - Orphaned references (deleted items)\n     - Mismatched references\n     - Duplicate mappings\n     - Missing bidirectional links\n   - Generate detailed audit report\n\n   ### Repair Action\n   - Fix identified reference issues:\n     - Update Linear tasks with missing GitHub links\n     - Add Linear references to GitHub items\n     - Remove references to deleted items\n     - Consolidate duplicate mappings\n   - Create backup before making changes\n   - Log all modifications\n\n   ### Map Action\n   - Display current reference mappings\n   - Show visual representation of connections\n   - Include statistics on reference health\n   - Highlight problematic mappings\n\n   ### Validate Action\n   - Perform deep validation of references\n   - Check that linked items actually exist\n   - Verify field consistency\n   - Test bidirectional navigation\n   - Report validation results\n\n   ### Export Action\n   - Export reference data in multiple formats\n   - Support JSON, CSV, and Markdown\n   - Include metadata and history\n   - Provide import instructions\n\n## Usage\n```bash\ncross-reference-manager [action] [options]\n```\n\n## Actions\n- `audit` - Scan and report on reference integrity\n- `repair` - Fix broken or missing references\n- `map` - Display reference mappings\n- `validate` - Verify reference consistency\n- `export` - Export reference data\n\n## Options\n- `--scope <type>` - Limit to specific types (issues, prs, tasks)\n- `--fix-orphans` - Automatically fix orphaned references\n- `--dry-run` - Preview changes without applying\n- `--deep-scan` - Perform thorough validation\n- `--format <type>` - Output format (json, csv, table)\n- `--since <date>` - Process items created after date\n- `--backup` - Create backup before modifications\n\n## Examples\n```bash\n# Audit all references\ncross-reference-manager audit\n\n# Repair broken references with preview\ncross-reference-manager repair --dry-run\n\n# Map references for specific date range\ncross-reference-manager map --since \"2024-01-01\"\n\n# Deep validation with orphan fixes\ncross-reference-manager validate --deep-scan --fix-orphans\n\n# Export reference data\ncross-reference-manager export --format json > refs.json\n```\n\n## Features\n- **Reference Integrity Checking**\n  - Verify bidirectional links\n  - Detect orphaned references\n  - Identify duplicate mappings\n  - Check reference format validity\n\n- **Smart Reference Repair**\n  - Reconstruct missing references from metadata\n  - Update outdated reference formats\n  - Merge duplicate references\n  - Remove invalid references\n\n- **Comprehensive Mapping**\n  - GitHub Issue  Linear Issue\n  - GitHub PR  Linear Task\n  - Comments and attachments\n  - User mappings\n\n- **Audit Trail**\n  - Log all reference modifications\n  - Track reference history\n  - Generate integrity reports\n  - Monitor reference health\n\n## Reference Storage\n```json\n{\n  \"mappings\": {\n    \"github_issue_123\": {\n      \"linear_id\": \"LIN-456\",\n      \"type\": \"issue\",\n      \"created\": \"2024-01-15T10:30:00Z\",\n      \"last_verified\": \"2024-01-20T14:00:00Z\",\n      \"confidence\": 0.95\n    }\n  },\n  \"metadata\": {\n    \"last_audit\": \"2024-01-20T14:00:00Z\",\n    \"total_references\": 1543,\n    \"broken_references\": 12\n  }\n}\n```\n\n## Error Handling\n- Automatic retry for API failures\n- Batch processing to avoid rate limits\n- Transaction-like operations with rollback\n- Detailed error logging\n\n## Best Practices\n- Run audit weekly to maintain integrity\n- Always use --dry-run before repair operations\n- Export references before major changes\n- Monitor reference health metrics\n\n## Integration Points\n- Works with bidirectional-sync command\n- Supports sync-status monitoring\n- Compatible with migration-assistant\n- Provides data for analytics\n\n## Required MCP Servers\n- mcp-server-github\n- mcp-server-linear\n\n## Notes\nThis command maintains a local reference database for performance and reliability. The database is automatically backed up before modifications."
              },
              {
                "name": "/issue-to-linear-task",
                "description": "Convert GitHub issues to Linear tasks",
                "path": "plugins/commands-integration-sync/commands/issue-to-linear-task.md",
                "frontmatter": {
                  "description": "Convert GitHub issues to Linear tasks",
                  "category": "integration-sync",
                  "allowed-tools": "Bash(gh *)"
                },
                "content": "# issue-to-linear-task\n\nConvert GitHub issues to Linear tasks\n\n## System\n\nYou are a precision converter that transforms individual GitHub issues into Linear tasks. You preserve all relevant data, maintain references, and ensure proper field mapping for single issue conversions.\n\n## Instructions\n\nWhen converting a GitHub issue to Linear:\n\n1. **Fetch Issue Details**\n   ```bash\n   # Get complete issue data\n   gh issue view <issue-number> --json \\\n     number,title,body,labels,assignees,milestone,state,\\\n     createdAt,updatedAt,closedAt,comments,projectItems\n   ```\n\n2. **Extract Issue Metadata**\n   ```javascript\n   const issueData = {\n     // Core fields\n     number: issue.number,\n     title: issue.title,\n     body: issue.body,\n     state: issue.state,\n     \n     // People\n     author: issue.author.login,\n     assignees: issue.assignees.map(a => a.login),\n     \n     // Classification\n     labels: issue.labels.map(l => ({\n       name: l.name,\n       color: l.color,\n       description: l.description\n     })),\n     \n     // Timeline\n     created: issue.createdAt,\n     updated: issue.updatedAt,\n     closed: issue.closedAt,\n     \n     // References\n     url: issue.url,\n     repository: issue.repository.nameWithOwner\n   };\n   ```\n\n3. **Analyze Issue Content**\n   ```javascript\n   function analyzeIssue(issue) {\n     return {\n       hasCheckboxes: /- \\[[ x]\\]/.test(issue.body),\n       hasCodeBlocks: /```/.test(issue.body),\n       hasMentions: /@[\\w-]+/.test(issue.body),\n       hasImages: /!\\[.*\\]\\(.*\\)/.test(issue.body),\n       estimatedSize: estimateFromContent(issue),\n       suggestedPriority: inferPriority(issue)\n     };\n   }\n   ```\n\n4. **Priority Inference**\n   ```javascript\n   function inferPriority(issue) {\n     const signals = {\n       urgent: ['critical', 'urgent', 'blocker', 'security'],\n       high: ['bug', 'regression', 'important'],\n       medium: ['enhancement', 'feature'],\n       low: ['documentation', 'chore', 'nice-to-have']\n     };\n     \n     // Check labels\n     for (const [priority, keywords] of Object.entries(signals)) {\n       if (issue.labels.some(l => \n         keywords.some(k => l.name.toLowerCase().includes(k))\n       )) {\n         return priority;\n       }\n     }\n     \n     // Check title/body\n     const text = `${issue.title} ${issue.body}`.toLowerCase();\n     if (text.includes('asap') || text.includes('urgent')) {\n       return 'urgent';\n     }\n     \n     return 'medium';\n   }\n   ```\n\n5. **Transform to Linear Format**\n   ```javascript\n   const linearTask = {\n     title: issue.title,\n     description: formatDescription(issue),\n     priority: mapPriority(inferredPriority),\n     state: mapState(issue.state),\n     labels: mapLabels(issue.labels),\n     assignee: findLinearUser(issue.assignees[0]),\n     project: mapMilestoneToProject(issue.milestone),\n     \n     // Metadata\n     externalId: `gh-${issue.number}`,\n     externalUrl: issue.url,\n     \n     // Custom fields\n     customFields: {\n       githubNumber: issue.number,\n       githubAuthor: issue.author,\n       githubRepo: issue.repository\n     }\n   };\n   ```\n\n6. **Description Formatting**\n   ```markdown\n   [Original issue description with formatting preserved]\n   \n   ## GitHub Metadata\n   - **Issue:** #<number>\n   - **Author:** @<username>\n   - **Created:** <date>\n   - **Labels:** <label1>, <label2>\n   \n   ## Comments\n   [Formatted comments from GitHub]\n   \n   ---\n   *Imported from GitHub: [#<number>](<url>)*\n   ```\n\n7. **Comment Import**\n   ```javascript\n   async function importComments(issue, linearTaskId) {\n     const comments = await getIssueComments(issue.number);\n     \n     for (const comment of comments) {\n       await createLinearComment(linearTaskId, {\n         body: formatComment(comment),\n         createdAt: comment.createdAt\n       });\n     }\n   }\n   ```\n\n8. **User Mapping**\n   ```javascript\n   const userMap = {\n     // GitHub username  Linear user ID\n     'octocat': 'linear-user-123',\n     'defunkt': 'linear-user-456'\n   };\n   \n   function findLinearUser(githubUsername) {\n     return userMap[githubUsername] || null;\n   }\n   ```\n\n9. **Validation & Confirmation**\n   ```\n   Issue to Convert:\n   \n   GitHub Issue: #123 - Implement user authentication\n   Author: @octocat\n   Labels: enhancement, priority/high\n   Assignee: @defunkt\n   Milestone: v2.0\n   \n   Will create Linear task:\n   \n   Title: Implement user authentication\n   Priority: High\n   State: Todo\n   Assignee: John Doe\n   Project: Version 2.0\n   Labels: Feature, High Priority\n   \n   Proceed? [Y/n]\n   ```\n\n10. **Post-Creation Actions**\n    - Add GitHub issue reference to Linear\n    - Comment on GitHub issue with Linear link\n    - Update sync state database\n    - Close GitHub issue (if requested)\n\n## Examples\n\n### Basic Conversion\n```bash\n# Convert single issue\nclaude issue-to-linear-task 123\n\n# Convert with team specification\nclaude issue-to-linear-task 123 --team=\"backend\"\n\n# Convert and close GitHub issue\nclaude issue-to-linear-task 123 --close-github\n```\n\n### Batch Conversion\n```bash\n# Convert multiple issues\nclaude issue-to-linear-task 123,124,125\n\n# Convert from file\nclaude issue-to-linear-task --from-file=\"issues.txt\"\n```\n\n### Advanced Options\n```bash\n# Custom field mapping\nclaude issue-to-linear-task 123 \\\n  --map-assignee=\"octocat:john.doe\" \\\n  --default-priority=\"high\"\n\n# Skip comments\nclaude issue-to-linear-task 123 --skip-comments\n\n# Custom project\nclaude issue-to-linear-task 123 --project=\"Sprint 24\"\n```\n\n## Output Format\n\n```\nGitHub Issue  Linear Task Conversion\n=====================================\n\nSource Issue:\n- Number: #123\n- Title: Implement user authentication\n- URL: https://github.com/owner/repo/issues/123\n\nCreated Linear Task:\n- ID: ABC-789\n- Title: Implement user authentication\n- URL: https://linear.app/team/issue/ABC-789\n\nConversion Details:\n Title and description converted\n Priority set to: High\n Assigned to: John Doe\n Added to project: Version 2.0\n 3 labels mapped\n 5 comments imported\n References linked\n\nActions Taken:\n- Created Linear task ABC-789\n- Added comment to GitHub issue #123\n- Updated sync database\n\nTotal time: 2.3s\n```\n\n## Error Handling\n\n```\nConversion Errors:\n\n Warning: No Linear user found for @octocat\n   Task created without assignee\n\n Warning: Label \"wontfix\" has no Linear equivalent\n   Skipped this label\n\n Error: Milestone \"v3.0\" not found in Linear\n   Task created without project assignment\n   Manual assignment required\n\nRecovery Actions:\n- Partial task created: ABC-789\n- Manual review recommended\n- Sync state NOT updated\n```\n\n## Best Practices\n\n1. **Data Preservation**\n   - Keep original formatting\n   - Preserve all metadata\n   - Maintain comment threading\n\n2. **User Experience**\n   - Show preview before creation\n   - Provide rollback option\n   - Clear success/error messages\n\n3. **Integration**\n   - Update both platforms\n   - Maintain bidirectional links\n   - Log all conversions"
              },
              {
                "name": "/linear-task-to-issue",
                "description": "Convert Linear tasks to GitHub issues",
                "path": "plugins/commands-integration-sync/commands/linear-task-to-issue.md",
                "frontmatter": {
                  "description": "Convert Linear tasks to GitHub issues",
                  "category": "integration-sync",
                  "allowed-tools": "Bash(gh *), Read, Edit"
                },
                "content": "# linear-task-to-issue\n\nConvert Linear tasks to GitHub issues\n\n## System\n\nYou are a Linear-to-GitHub converter that transforms individual Linear tasks into GitHub issues. You preserve task context, maintain relationships, and ensure accurate representation in GitHub's issue tracking system.\n\n## Instructions\n\nWhen converting a Linear task to a GitHub issue:\n\n1. **Fetch Linear Task Details**\n   ```javascript\n   // Get complete task data\n   const task = await linear.issue(taskId, {\n     includeRelations: ['assignee', 'labels', 'project', 'team', 'parent', 'children'],\n     includeComments: true,\n     includeHistory: true\n   });\n   ```\n\n2. **Extract Task Components**\n   ```javascript\n   const taskData = {\n     // Core fields\n     identifier: task.identifier,\n     title: task.title,\n     description: task.description,\n     state: task.state.name,\n     priority: task.priority,\n     \n     // Relationships\n     assignee: task.assignee?.email,\n     team: task.team.key,\n     project: task.project?.name,\n     cycle: task.cycle?.name,\n     parent: task.parent?.identifier,\n     children: task.children.map(c => c.identifier),\n     \n     // Metadata\n     createdAt: task.createdAt,\n     updatedAt: task.updatedAt,\n     completedAt: task.completedAt,\n     \n     // Content\n     labels: task.labels.map(l => l.name),\n     attachments: task.attachments,\n     comments: task.comments\n   };\n   ```\n\n3. **Build GitHub Issue Body**\n   ```markdown\n   # <Task Title>\n   \n   <Task Description>\n   \n   ## Task Details\n   - **Linear ID:** [<identifier>](<linear-url>)\n   - **Priority:** <priority-emoji> <priority-name>\n   - **Status:** <status>\n   - **Team:** <team>\n   - **Project:** <project>\n   - **Cycle:** <cycle>\n   \n   ## Relationships\n   - **Parent:** <parent-link>\n   - **Sub-tasks:** \n     - [ ] <child-1>\n     - [ ] <child-2>\n   \n   ## Acceptance Criteria\n   <extracted-from-description>\n   \n   ## Attachments\n   <uploaded-attachments>\n   \n   ---\n   *Imported from Linear: [<identifier>](<url>)*\n   *Import date: <timestamp>*\n   ```\n\n4. **Priority Mapping**\n   ```javascript\n   const priorityMap = {\n     0: { label: null, emoji: '' },           // No priority\n     1: { label: 'priority/urgent', emoji: '' }, // Urgent\n     2: { label: 'priority/high', emoji: '' },   // High\n     3: { label: 'priority/medium', emoji: '' }, // Medium\n     4: { label: 'priority/low', emoji: '' }     // Low\n   };\n   ```\n\n5. **State to Label Conversion**\n   ```javascript\n   function stateToLabels(state) {\n     const stateLabels = {\n       'Backlog': ['status/backlog'],\n       'Todo': ['status/todo'],\n       'In Progress': ['status/in-progress'],\n       'In Review': ['status/review'],\n       'Done': [], // No label, will close issue\n       'Canceled': ['status/canceled']\n     };\n     \n     return stateLabels[state] || [];\n   }\n   ```\n\n6. **Create GitHub Issue**\n   ```bash\n   # Create the issue\n   gh issue create \\\n     --repo \"<owner>/<repo>\" \\\n     --title \"<title>\" \\\n     --body \"<formatted-body>\" \\\n     --label \"<labels>\" \\\n     --assignee \"<github-username>\" \\\n     --milestone \"<milestone>\"\n   ```\n\n7. **Handle Attachments**\n   ```javascript\n   async function uploadAttachments(attachments, issueNumber) {\n     const uploaded = [];\n     \n     for (const attachment of attachments) {\n       // Download from Linear\n       const file = await downloadAttachment(attachment.url);\n       \n       // Upload to GitHub\n       const uploadUrl = await getGitHubUploadUrl(issueNumber);\n       const githubUrl = await uploadFile(uploadUrl, file);\n       \n       uploaded.push({\n         original: attachment.url,\n         github: githubUrl,\n         filename: attachment.filename\n       });\n     }\n     \n     return uploaded;\n   }\n   ```\n\n8. **Import Comments**\n   ```bash\n   # Add each comment\n   for comment in comments; do\n     gh issue comment <issue-number> \\\n       --body \"**@<author>** commented on <date>:\\n\\n<comment-body>\"\n   done\n   ```\n\n9. **User Mapping**\n   ```javascript\n   const linearToGitHub = {\n     'john@example.com': 'johndoe',\n     'jane@example.com': 'janedoe'\n   };\n   \n   function mapAssignee(linearUser) {\n     return linearToGitHub[linearUser.email] || null;\n   }\n   ```\n\n10. **Post-Creation Updates**\n    ```javascript\n    // Update Linear task with GitHub reference\n    await linear.updateIssue(taskId, {\n       description: appendGitHubLink(task.description, githubIssueUrl)\n    });\n    \n    // Add GitHub issue number to Linear\n    await linear.createComment(taskId, {\n       body: `GitHub Issue created: #${issueNumber}`\n    });\n    ```\n\n## Examples\n\n### Basic Conversion\n```bash\n# Convert single task\nclaude linear-task-to-issue ABC-123\n\n# Specify target repository\nclaude linear-task-to-issue ABC-123 --repo=\"owner/repo\"\n\n# Convert and close Linear task\nclaude linear-task-to-issue ABC-123 --close-linear\n```\n\n### Advanced Options\n```bash\n# Custom label mapping\nclaude linear-task-to-issue ABC-123 \\\n  --label-prefix=\"linear/\" \\\n  --add-labels=\"imported,needs-review\"\n\n# Skip certain elements\nclaude linear-task-to-issue ABC-123 \\\n  --skip-comments \\\n  --skip-attachments\n\n# Map to specific milestone\nclaude linear-task-to-issue ABC-123 --milestone=\"v2.0\"\n```\n\n### Bulk Operations\n```bash\n# Convert multiple tasks\nclaude linear-task-to-issue ABC-123,ABC-124,ABC-125\n\n# Convert all tasks from a project\nclaude linear-task-to-issue --project=\"Sprint 23\"\n```\n\n## Output Format\n\n```\nLinear Task  GitHub Issue Conversion\n=====================================\n\nSource Task:\n- ID: ABC-123\n- Title: Implement caching layer\n- URL: https://linear.app/team/issue/ABC-123\n\nCreated GitHub Issue:\n- Number: #456\n- Title: Implement caching layer\n- URL: https://github.com/owner/repo/issues/456\n\nConversion Summary:\n Title and description converted\n Priority mapped to: priority/high\n State mapped to: status/in-progress\n Assigned to: @johndoe\n 4 labels applied\n 3 attachments uploaded\n 7 comments imported\n Cross-references created\n\nRelationships:\n- Parent task: Not applicable (no parent)\n- Sub-tasks: 2 references added to description\n\nTotal time: 5.2s\nAPI calls: 12\n```\n\n## Special Handling\n\n### Linear-Specific Features\n```javascript\n// Handle Linear's rich text\nfunction convertLinearMarkdown(content) {\n  return content\n    .replace(/\\[([^\\]]+)\\]\\(lin:\\/\\/([^)]+)\\)/g, '[$1](https://linear.app/$2)')\n    .replace(/{{([^}]+)}}/g, '`$1`') // Inline code\n    .replace(/@([a-zA-Z0-9]+)/g, '@$1'); // User mentions\n}\n\n// Handle Linear estimates\nfunction addEstimateLabel(estimate) {\n  const estimateMap = {\n    1: 'size/xs',\n    2: 'size/s', \n    3: 'size/m',\n    5: 'size/l',\n    8: 'size/xl'\n  };\n  return estimateMap[estimate] || null;\n}\n```\n\n### Error Recovery\n```\nConversion Warnings:\n\n Assignee not found in GitHub\n   Issue created without assignee\n   Added note in description\n\n 2 attachments failed to upload\n   Links preserved in description\n   Manual upload required\n\n Project \"Q1 Goals\" has no GitHub milestone\n   Issue created without milestone\n\nRecovery Options:\n1. Edit issue manually: gh issue edit 456\n2. Retry failed uploads: claude linear-task-to-issue ABC-123 --retry-attachments\n3. Create missing milestone: gh api repos/owner/repo/milestones -f title=\"Q1 Goals\"\n```\n\n## Best Practices\n\n1. **Content Fidelity**\n   - Preserve formatting and structure\n   - Maintain all metadata\n   - Keep original timestamps in comments\n\n2. **Relationship Management**\n   - Link parent/child tasks\n   - Preserve team context\n   - Maintain project associations\n\n3. **Automation Ready**\n   - Structured data in description\n   - Consistent label naming\n   - Machine-readable references"
              },
              {
                "name": "/sync-automation-setup",
                "description": "Setup automated synchronization workflows",
                "path": "plugins/commands-integration-sync/commands/sync-automation-setup.md",
                "frontmatter": {
                  "description": "Setup automated synchronization workflows",
                  "category": "integration-sync",
                  "allowed-tools": "Bash(npm *)"
                },
                "content": "# sync-automation-setup\n\nSetup automated synchronization workflows\n\n## System\n\nYou are an automation setup specialist that configures robust, automated synchronization between GitHub and Linear. You handle webhook configuration, CI/CD integration, scheduling, monitoring, and ensure reliable continuous synchronization.\n\n## Instructions\n\nWhen setting up sync automation:\n\n1. **Prerequisites Check**\n   ```javascript\n   async function checkPrerequisites() {\n     const checks = {\n       github: {\n         cli: await checkCommand('gh --version'),\n         auth: await checkGitHubAuth(),\n         permissions: await checkGitHubPermissions(),\n         webhookAccess: await checkWebhookPermissions()\n       },\n       linear: {\n         mcp: await checkLinearMCP(),\n         apiKey: await checkLinearAPIKey(),\n         webhookUrl: await checkLinearWebhookEndpoint()\n       },\n       infrastructure: {\n         serverEndpoint: process.env.SYNC_SERVER_URL,\n         database: await checkDatabaseConnection(),\n         queue: await checkQueueService(),\n         storage: await checkStateStorage()\n       }\n     };\n     \n     return validateAllChecks(checks);\n   }\n   ```\n\n2. **GitHub Webhook Setup**\n   ```bash\n   # Create webhook for issue events\n   gh api repos/:owner/:repo/hooks \\\n     --method POST \\\n     --field name='web' \\\n     --field active=true \\\n     --field events[]='issues' \\\n     --field events[]='issue_comment' \\\n     --field events[]='pull_request' \\\n     --field events[]='pull_request_review' \\\n     --field config[url]=\"${WEBHOOK_URL}/github\" \\\n     --field config[content_type]='json' \\\n     --field config[secret]=\"${WEBHOOK_SECRET}\"\n   ```\n\n3. **Linear Webhook Configuration**\n   ```javascript\n   async function setupLinearWebhooks() {\n     const webhook = await linear.createWebhook({\n       url: `${WEBHOOK_URL}/linear`,\n       resourceTypes: ['Issue', 'Comment', 'Project', 'Cycle'],\n       label: 'GitHub Sync',\n       enabled: true,\n       secret: process.env.LINEAR_WEBHOOK_SECRET\n     });\n     \n     // Verify webhook\n     await linear.testWebhook(webhook.id);\n     \n     return webhook;\n   }\n   ```\n\n4. **GitHub Actions Workflow**\n   ```yaml\n   # .github/workflows/linear-sync.yml\n   name: Linear Sync\n   \n   on:\n     issues:\n       types: [opened, edited, closed, reopened, labeled, unlabeled]\n     issue_comment:\n       types: [created, edited, deleted]\n     pull_request:\n       types: [opened, edited, closed, merged]\n     schedule:\n       - cron: '*/15 * * * *'  # Every 15 minutes\n     workflow_dispatch:\n       inputs:\n         sync_type:\n           description: 'Type of sync to perform'\n           required: true\n           default: 'incremental'\n           type: choice\n           options:\n             - incremental\n             - full\n             - repair\n   \n   jobs:\n     sync:\n       runs-on: ubuntu-latest\n       steps:\n         - uses: actions/checkout@v3\n         \n         - name: Setup sync environment\n           run: |\n             npm install -g @linear/sync-cli\n             echo \"${{ secrets.SYNC_CONFIG }}\" > sync.config.json\n         \n         - name: Run sync\n           env:\n             GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n             LINEAR_API_KEY: ${{ secrets.LINEAR_API_KEY }}\n             SYNC_STATE_BUCKET: ${{ secrets.SYNC_STATE_BUCKET }}\n           run: |\n             case \"${{ github.event_name }}\" in\n               \"schedule\")\n                 linear-sync run --type=incremental\n                 ;;\n               \"workflow_dispatch\")\n                 linear-sync run --type=${{ inputs.sync_type }}\n                 ;;\n               *)\n                 linear-sync handle-event \\\n                   --event=${{ github.event_name }} \\\n                   --payload='${{ toJSON(github.event) }}'\n                 ;;\n             esac\n         \n         - name: Upload sync report\n           if: always()\n           uses: actions/upload-artifact@v3\n           with:\n             name: sync-report-${{ github.run_id }}\n             path: sync-report.json\n   ```\n\n5. **Sync Server Configuration**\n   ```javascript\n   // sync-server.js\n   const express = require('express');\n   const { Queue } = require('bull');\n   const { SyncEngine } = require('./sync-engine');\n   \n   const app = express();\n   const syncQueue = new Queue('sync-tasks', REDIS_URL);\n   const syncEngine = new SyncEngine();\n   \n   // GitHub webhook endpoint\n   app.post('/webhooks/github', verifyGitHubWebhook, async (req, res) => {\n     const event = req.headers['x-github-event'];\n     const payload = req.body;\n     \n     // Queue sync task\n     await syncQueue.add('github-event', {\n       event,\n       payload,\n       timestamp: new Date().toISOString()\n     }, {\n       attempts: 3,\n       backoff: { type: 'exponential', delay: 2000 }\n     });\n     \n     res.status(200).send('OK');\n   });\n   \n   // Linear webhook endpoint\n   app.post('/webhooks/linear', verifyLinearWebhook, async (req, res) => {\n     const { action, data, type } = req.body;\n     \n     await syncQueue.add('linear-event', {\n       action,\n       data,\n       type,\n       timestamp: new Date().toISOString()\n     });\n     \n     res.status(200).send('OK');\n   });\n   \n   // Health check endpoint\n   app.get('/health', async (req, res) => {\n     const health = await syncEngine.getHealth();\n     res.json(health);\n   });\n   \n   // Process sync queue\n   syncQueue.process('github-event', async (job) => {\n     return await syncEngine.processGitHubEvent(job.data);\n   });\n   \n   syncQueue.process('linear-event', async (job) => {\n     return await syncEngine.processLinearEvent(job.data);\n   });\n   ```\n\n6. **Sync Configuration File**\n   ```yaml\n   # sync-config.yml\n   version: 1.0\n   \n   sync:\n     enabled: true\n     direction: bidirectional\n     mode: real-time  # real-time, scheduled, or hybrid\n     \n   scheduling:\n     incremental:\n       interval: '*/5 * * * *'  # Every 5 minutes\n       enabled: true\n     full:\n       interval: '0 2 * * *'    # Daily at 2 AM\n       enabled: true\n     health_check:\n       interval: '*/30 * * * *' # Every 30 minutes\n       enabled: true\n   \n   mapping:\n     states:\n       github_to_linear:\n         open: Todo\n         closed: Done\n       linear_to_github:\n         Backlog: open\n         Todo: open\n         'In Progress': open\n         Done: closed\n         Canceled: closed\n     \n     priorities:\n       label_to_priority:\n         'priority/urgent': 1\n         'priority/high': 2\n         'priority/medium': 3\n         'priority/low': 4\n       priority_to_label:\n         1: 'priority/urgent'\n         2: 'priority/high'\n         3: 'priority/medium'\n         4: 'priority/low'\n     \n     teams:\n       default: 'engineering'\n       mapping:\n         'frontend/*': 'frontend-team'\n         'backend/*': 'backend-team'\n         'docs/*': 'docs-team'\n   \n   conflict_resolution:\n     strategy: newer_wins  # newer_wins, github_wins, linear_wins, manual\n     preserve_fields:\n       - comments\n       - attachments\n     merge_fields:\n       - labels\n       - assignees\n   \n   filters:\n     github:\n       include_labels:\n         - 'linear-sync'\n       exclude_labels:\n         - 'no-sync'\n         - 'draft'\n     linear:\n       include_teams:\n         - 'engineering'\n         - 'product'\n       exclude_states:\n         - 'Duplicate'\n   \n   notifications:\n     slack:\n       enabled: true\n       webhook_url: ${SLACK_WEBHOOK_URL}\n       channels:\n         errors: '#sync-errors'\n         summary: '#dev-updates'\n     email:\n       enabled: false\n       recipients:\n         - 'ops@company.com'\n   \n   monitoring:\n     metrics:\n       enabled: true\n       provider: datadog\n       api_key: ${DATADOG_API_KEY}\n     logging:\n       level: info\n       destination: 'cloudwatch'\n     alerts:\n       - metric: sync_failure_rate\n         threshold: 0.05\n         action: notify\n       - metric: sync_lag\n         threshold: 300  # seconds\n         action: alert\n   ```\n\n7. **Database Schema**\n   ```sql\n   -- Sync state management\n   CREATE TABLE sync_state (\n     id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n     github_id VARCHAR(255),\n     linear_id VARCHAR(255),\n     github_updated_at TIMESTAMP,\n     linear_updated_at TIMESTAMP,\n     last_sync_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n     sync_hash VARCHAR(64),\n     sync_version INTEGER DEFAULT 1,\n     metadata JSONB,\n     UNIQUE(github_id, linear_id)\n   );\n   \n   -- Sync history\n   CREATE TABLE sync_history (\n     id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n     sync_id UUID REFERENCES sync_state(id),\n     direction VARCHAR(50),\n     status VARCHAR(50),\n     started_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n     completed_at TIMESTAMP,\n     changes JSONB,\n     errors JSONB\n   );\n   \n   -- Conflict log\n   CREATE TABLE sync_conflicts (\n     id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n     sync_id UUID REFERENCES sync_state(id),\n     detected_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n     conflict_type VARCHAR(100),\n     github_data JSONB,\n     linear_data JSONB,\n     resolution VARCHAR(100),\n     resolved_at TIMESTAMP,\n     resolved_by VARCHAR(255)\n   );\n   \n   -- Indexes for performance\n   CREATE INDEX idx_sync_state_github_id ON sync_state(github_id);\n   CREATE INDEX idx_sync_state_linear_id ON sync_state(linear_id);\n   CREATE INDEX idx_sync_history_sync_id ON sync_history(sync_id);\n   CREATE INDEX idx_sync_history_started_at ON sync_history(started_at);\n   ```\n\n8. **Monitoring Dashboard**\n   ```javascript\n   // monitoring/dashboard.js\n   const metrics = {\n     // Real-time metrics\n     syncRate: new Rate('sync.operations'),\n     syncDuration: new Histogram('sync.duration'),\n     syncErrors: new Counter('sync.errors'),\n     \n     // Business metrics\n     issuesSynced: new Counter('issues.synced'),\n     conflictsResolved: new Counter('conflicts.resolved'),\n     \n     // System health\n     apiLatency: new Histogram('api.latency'),\n     queueDepth: new Gauge('queue.depth'),\n     rateLimitRemaining: new Gauge('ratelimit.remaining')\n   };\n   \n   // Dashboard configuration\n   const dashboard = {\n     title: 'GitHub-Linear Sync Monitor',\n     widgets: [\n       {\n         type: 'timeseries',\n         title: 'Sync Operations',\n         metrics: ['sync.operations', 'sync.errors'],\n         period: '1h'\n       },\n       {\n         type: 'gauge',\n         title: 'Queue Depth',\n         metric: 'queue.depth',\n         thresholds: [0, 50, 100, 200]\n       },\n       {\n         type: 'heatmap',\n         title: 'Sync Duration',\n         metric: 'sync.duration',\n         buckets: [100, 500, 1000, 5000, 10000]\n       },\n       {\n         type: 'counter',\n         title: 'Today\\'s Syncs',\n         metric: 'issues.synced',\n         period: '1d'\n       }\n     ],\n     alerts: [\n       {\n         name: 'High Error Rate',\n         condition: 'rate(sync.errors) > 0.1',\n         severity: 'critical'\n       },\n       {\n         name: 'Sync Lag',\n         condition: 'queue.depth > 100',\n         severity: 'warning'\n       }\n     ]\n   };\n   ```\n\n9. **Deployment Script**\n   ```bash\n   #!/bin/bash\n   # deploy-sync-automation.sh\n   \n   set -e\n   \n   echo \" Deploying GitHub-Linear Sync Automation\"\n   \n   # Check prerequisites\n   echo \" Checking prerequisites...\"\n   command -v gh >/dev/null 2>&1 || { echo \" GitHub CLI required\"; exit 1; }\n   command -v docker >/dev/null 2>&1 || { echo \" Docker required\"; exit 1; }\n   \n   # Load configuration\n   source .env\n   \n   # Build sync server\n   echo \" Building sync server...\"\n   docker build -t linear-sync-server .\n   \n   # Deploy database\n   echo \" Setting up database...\"\n   docker-compose up -d postgres redis\n   sleep 5\n   docker-compose run --rm migrate\n   \n   # Configure webhooks\n   echo \" Configuring webhooks...\"\n   ./scripts/setup-webhooks.sh\n   \n   # Deploy sync server\n   echo \" Deploying sync server...\"\n   docker-compose up -d sync-server\n   \n   # Setup monitoring\n   echo \" Configuring monitoring...\"\n   ./scripts/setup-monitoring.sh\n   \n   # Verify deployment\n   echo \" Verifying deployment...\"\n   sleep 10\n   curl -f http://localhost:3000/health || { echo \" Health check failed\"; exit 1; }\n   \n   # Run initial sync\n   echo \" Running initial sync...\"\n   docker-compose run --rm sync-cli full-sync\n   \n   echo \" Deployment complete!\"\n   echo \" Dashboard: http://localhost:3000/dashboard\"\n   echo \" Logs: docker-compose logs -f sync-server\"\n   ```\n\n10. **Maintenance Commands**\n    ```bash\n    # Sync management CLI\n    linear-sync status          # Check sync status\n    linear-sync pause          # Pause all syncing\n    linear-sync resume         # Resume syncing\n    linear-sync repair         # Repair sync state\n    linear-sync reset          # Reset sync (caution!)\n    \n    # Troubleshooting\n    linear-sync diagnose       # Run diagnostics\n    linear-sync test-webhooks  # Test webhook connectivity\n    linear-sync validate       # Validate configuration\n    \n    # Maintenance\n    linear-sync cleanup        # Clean old sync records\n    linear-sync export         # Export sync state\n    linear-sync import         # Import sync state\n    ```\n\n## Examples\n\n### Basic Setup\n```bash\n# Interactive setup\nclaude sync-automation-setup\n\n# Setup with config file\nclaude sync-automation-setup --config=\"sync-config.yml\"\n\n# Minimal setup (webhooks only)\nclaude sync-automation-setup --mode=\"webhooks-only\"\n```\n\n### Advanced Configuration\n```bash\n# Full automation with monitoring\nclaude sync-automation-setup \\\n  --mode=\"full\" \\\n  --monitoring=\"datadog\" \\\n  --alerts=\"slack,email\"\n\n# Custom deployment\nclaude sync-automation-setup \\\n  --deploy-target=\"kubernetes\" \\\n  --namespace=\"sync-system\"\n```\n\n### Maintenance\n```bash\n# Update webhook configuration\nclaude sync-automation-setup --update-webhooks\n\n# Rotate secrets\nclaude sync-automation-setup --rotate-secrets\n\n# Upgrade sync version\nclaude sync-automation-setup --upgrade\n```\n\n## Output Format\n\n```\nGitHub-Linear Sync Automation Setup\n===================================\n\n Prerequisites Check\n   GitHub CLI authenticated\n   Linear MCP connected\n   Database accessible\n   Redis running\n\n Configuration Summary\n  Mode: Bidirectional real-time sync\n  Webhook URL: https://sync.company.com/webhooks\n  Sync Interval: 5 minutes (incremental)\n  Conflict Strategy: newer_wins\n\n Webhook Configuration\n  GitHub Webhooks:\n     Issues webhook created (ID: 12345)\n     Pull requests webhook created (ID: 12346)\n     Webhook test successful\n  \n  Linear Webhooks:\n     Issue webhook registered\n     Comment webhook registered\n     Webhook verified\n\n Deployment Status\n   Sync server deployed (3 replicas)\n   Database migrations complete\n   Redis queue initialized\n   Monitoring configured\n\n Monitoring Setup\n  Dashboard: https://monitoring.company.com/linear-sync\n  Alerts configured:\n    - Slack: #sync-alerts\n    - Email: ops@company.com\n  \n  Metrics collecting:\n    - Sync rate\n    - Error rate\n    - API latency\n    - Queue depth\n\n Security Configuration\n   Webhook secrets configured\n   API keys encrypted\n   TLS enabled\n   Rate limiting active\n\n Next Steps\n  1. Monitor initial sync: docker-compose logs -f\n  2. Check dashboard for metrics\n  3. Review sync-config.yml for customization\n  4. Set up team notifications\n\nAutomation Status:  ACTIVE\nFirst sync scheduled: 2 minutes\n```\n\n## Best Practices\n\n1. **Security**\n   - Use webhook secrets\n   - Encrypt API keys\n   - Implement rate limiting\n   - Regular secret rotation\n\n2. **Reliability**\n   - Implement retry logic\n   - Use message queues\n   - Monitor system health\n   - Plan for failures\n\n3. **Performance**\n   - Optimize batch sizes\n   - Implement caching\n   - Use connection pooling\n   - Monitor API limits\n\n4. **Maintenance**\n   - Regular health checks\n   - Automated backups\n   - Log retention policies\n   - Update procedures"
              },
              {
                "name": "/sync-conflict-resolver",
                "description": "Resolve synchronization conflicts automatically",
                "path": "plugins/commands-integration-sync/commands/sync-conflict-resolver.md",
                "frontmatter": {
                  "description": "Resolve synchronization conflicts automatically",
                  "category": "integration-sync",
                  "argument-hint": "Set up conflict detection parameters"
                },
                "content": "# Sync Conflict Resolver\n\nResolve synchronization conflicts automatically\n\n## Instructions\n\n1. **Initialize Conflict Detection**\n   - Check GitHub CLI and Linear MCP availability\n   - Load existing sync metadata and mappings\n   - Parse command arguments from: **$ARGUMENTS**\n   - Set up conflict detection parameters\n\n2. **Parse Resolution Strategy**\n   - Extract action (detect, resolve, analyze, configure, report)\n   - Determine resolution strategy from options\n   - Configure auto-resolve preferences\n   - Set priority system if specified\n\n3. **Execute Selected Action**\n   Based on the action provided:\n\n   ### Detect Action\n   - Scan all synchronized items\n   - Compare GitHub and Linear versions\n   - Identify field-level conflicts\n   - Flag timing conflicts\n   - Generate conflict list\n\n   ### Resolve Action\n   - Apply selected strategy to conflicts\n   - Handle field merging if enabled\n   - Create backups before changes\n   - Log all resolutions\n   - Update sync metadata\n\n   ### Analyze Action\n   - Study conflict patterns\n   - Identify frequent conflict types\n   - Suggest process improvements\n   - Generate analytics report\n\n   ### Configure Action\n   - Set default resolution strategies\n   - Configure field priorities\n   - Define merge rules\n   - Save preferences\n\n   ### Report Action\n   - Generate detailed conflict report\n   - Show resolution history\n   - Provide conflict statistics\n   - Export findings\n\n## Usage\n```bash\nsync-conflict-resolver [action] [options]\n```\n\n## Actions\n- `detect` - Identify synchronization conflicts\n- `resolve` - Apply resolution strategies\n- `analyze` - Deep analysis of conflict patterns\n- `configure` - Set resolution preferences\n- `report` - Generate conflict reports\n\n## Options\n- `--strategy <type>` - Resolution strategy (latest-wins, manual, smart)\n- `--interactive` - Prompt for each conflict\n- `--auto-resolve` - Automatically resolve using rules\n- `--dry-run` - Preview resolutions without applying\n- `--backup` - Create backup before resolving\n- `--priority <system>` - Prioritize GitHub or Linear\n- `--merge-fields` - Merge non-conflicting fields\n\n## Examples\n```bash\n# Detect all conflicts\nsync-conflict-resolver detect\n\n# Resolve with latest-wins strategy\nsync-conflict-resolver resolve --strategy latest-wins\n\n# Interactive resolution with backup\nsync-conflict-resolver resolve --interactive --backup\n\n# Analyze conflict patterns\nsync-conflict-resolver analyze --since \"30 days ago\"\n\n# Configure auto-resolution rules\nsync-conflict-resolver configure --auto-resolve\n```\n\n## Conflict Types\n1. **Field Conflicts**\n   - Title differences\n   - Description mismatches\n   - Status discrepancies\n   - Priority conflicts\n   - Assignee differences\n\n2. **Structural Conflicts**\n   - Deleted in one system\n   - Duplicated items\n   - Circular references\n   - Parent-child mismatches\n\n3. **Timing Conflicts**\n   - Simultaneous updates\n   - Out-of-order syncs\n   - Version conflicts\n   - Race conditions\n\n## Resolution Strategies\n\n### Latest Wins\n- Uses most recent modification\n- Configurable per field\n- Maintains audit trail\n\n### Smart Resolution\n- Field-level intelligence\n- Preserves important data\n- Merges compatible changes\n- User preference learning\n\n### Manual Resolution\n- Interactive prompts\n- Side-by-side comparison\n- Selective field merging\n- Custom resolution rules\n\n## Conflict Detection Algorithm\n```yaml\ndetection:\n  - compare_timestamps\n  - check_field_hashes\n  - verify_relationships\n  - analyze_change_patterns\n  \nanalysis:\n  - identify_conflict_type\n  - determine_severity\n  - suggest_resolution\n  - calculate_impact\n```\n\n## Resolution Rules Configuration\n```json\n{\n  \"rules\": {\n    \"title\": {\n      \"strategy\": \"latest-wins\",\n      \"priority\": \"linear\"\n    },\n    \"description\": {\n      \"strategy\": \"merge\",\n      \"preserve_sections\": [\"## Requirements\", \"## Acceptance Criteria\"]\n    },\n    \"status\": {\n      \"strategy\": \"smart\",\n      \"mapping\": {\n        \"github_closed\": \"linear_completed\",\n        \"github_open\": \"linear_in_progress\"\n      }\n    },\n    \"assignee\": {\n      \"strategy\": \"manual\",\n      \"notify\": true\n    }\n  },\n  \"global\": {\n    \"backup_before_resolve\": true,\n    \"log_level\": \"detailed\"\n  }\n}\n```\n\n## Merge Algorithm\n1. Identify non-conflicting changes\n2. Apply field-specific merge strategies\n3. Preserve formatting and structure\n4. Validate merged result\n5. Create resolution record\n\n## Conflict Prevention\n- Implement field locking\n- Use optimistic concurrency\n- Add sync timestamps\n- Enable change notifications\n\n## Reporting Features\n- Conflict frequency analysis\n- Resolution success rates\n- Common conflict patterns\n- Team conflict hotspots\n- Time-based trends\n\n## Integration Workflow\n1. Run after sync operations\n2. Process conflict queue\n3. Apply resolutions\n4. Update reference manager\n5. Notify affected users\n\n## Required MCP Servers\n- mcp-server-github\n- mcp-server-linear\n\n## Error Handling\n- Transaction-based resolution\n- Automatic rollback on failure\n- Detailed conflict logs\n- Resolution history tracking\n\n## Best Practices\n- Review conflict patterns monthly\n- Adjust resolution rules based on patterns\n- Train team on conflict prevention\n- Monitor resolution success rates\n- Keep manual intervention minimal\n\n## Notes\nThis command maintains a conflict history database to improve resolution accuracy over time. Machine learning capabilities can be enabled for advanced pattern recognition."
              },
              {
                "name": "/sync-issues-to-linear",
                "description": "Sync GitHub issues to Linear workspace",
                "path": "plugins/commands-integration-sync/commands/sync-issues-to-linear.md",
                "frontmatter": {
                  "description": "Sync GitHub issues to Linear workspace",
                  "category": "integration-sync",
                  "allowed-tools": "Bash(gh *)"
                },
                "content": "# sync-issues-to-linear\n\nSync GitHub issues to Linear workspace\n\n## System\n\nYou are a GitHub-to-Linear synchronization assistant that imports GitHub issues into Linear. You ensure data integrity, handle field mappings, and manage rate limits effectively.\n\n## Instructions\n\nWhen asked to sync GitHub issues to Linear:\n\n1. **Check Prerequisites**\n   - Verify `gh` CLI is available and authenticated\n   - Check Linear MCP server connection\n   - Confirm repository context\n\n2. **Fetch GitHub Issues**\n   ```bash\n   # Get all open issues\n   gh issue list --state open --limit 1000 --json number,title,body,labels,assignees,milestone,state,createdAt,updatedAt\n   \n   # Get specific issue\n   gh issue view <issue-number> --json number,title,body,labels,assignees,milestone,state,createdAt,updatedAt,comments\n   ```\n\n3. **Field Mapping Strategy**\n   ```\n   GitHub Issue  Linear Task\n   \n   title         title\n   body          description\n   labels        labels (create if missing)\n   assignees     assignee (first assignee)\n   milestone     project/cycle\n   state         state (map: openbacklog/todo, closeddone)\n   number        externalId (GitHub Issue #)\n   url           externalUrl\n   ```\n\n4. **Priority Mapping**\n   - bug label  urgent/high priority\n   - enhancement  medium priority\n   - documentation  low priority\n   - Default: medium priority\n\n5. **Label Handling**\n   ```javascript\n   // Map GitHub labels to Linear\n   const labelMap = {\n     'bug': { name: 'Bug', color: '#d73a4a' },\n     'enhancement': { name: 'Feature', color: '#a2eeef' },\n     'documentation': { name: 'Docs', color: '#0075ca' },\n     'good first issue': { name: 'Good First Issue', color: '#7057ff' },\n     'help wanted': { name: 'Help Wanted', color: '#008672' }\n   };\n   ```\n\n6. **Create Linear Tasks**\n   - Check if task already exists (by externalId)\n   - Create new task with mapped fields\n   - Add sync metadata\n\n7. **Sync Metadata**\n   Store in task description footer:\n   ```\n   ---\n   _Synced from GitHub Issue #123_\n   _Last sync: 2025-01-16T10:30:00Z_\n   _Sync ID: gh-issue-123_\n   ```\n\n8. **Rate Limiting**\n   - GitHub: 5000 requests/hour (authenticated)\n   - Linear: 1500 requests/hour\n   - Implement exponential backoff\n   - Batch operations where possible\n\n9. **Progress Tracking**\n   ```\n   Syncing GitHub Issues to Linear...\n   [] 80% (40/50 issues)\n    Issue #123: Fix navigation bug\n    Issue #124: Add dark mode\n    Issue #125: Syncing...\n   ```\n\n10. **Error Handling**\n    - Network failures: Retry with backoff\n    - Duplicate detection: Skip or update\n    - Missing fields: Use defaults\n    - API errors: Log and continue\n\n## Examples\n\n### Basic Sync\n```bash\n# Sync all open issues\nclaude sync-issues-to-linear\n\n# Sync with filters\nclaude sync-issues-to-linear --label=\"bug\" --assignee=\"@me\"\n\n# Sync specific issues\nclaude sync-issues-to-linear --issues=\"123,124,125\"\n```\n\n### Advanced Options\n```bash\n# Dry run mode\nclaude sync-issues-to-linear --dry-run\n\n# Force update existing\nclaude sync-issues-to-linear --force-update\n\n# Custom field mapping\nclaude sync-issues-to-linear --map-priority=\"critical:urgent,high:high,medium:medium,low:low\"\n```\n\n### Webhook Setup\n```yaml\n# GitHub webhook configuration\n- URL: https://your-sync-service.com/webhook\n- Events: issues, issue_comment\n- Secret: your-webhook-secret\n```\n\n## Output Format\n\n```\nGitHub to Linear Sync Report\n============================\nRepository: owner/repo\nStarted: 2025-01-16 10:30:00\nCompleted: 2025-01-16 10:32:15\n\nSummary:\n- Total issues: 50\n- Successfully synced: 48\n- Skipped (duplicates): 1\n- Failed: 1\n\nDetails:\n #123  LIN-456: Fix navigation bug\n #124  LIN-457: Add dark mode\n #125  Skipped: Already exists (LIN-458)\n #126  Failed: Rate limit exceeded\n\nNext sync scheduled: 2025-01-16 11:00:00\n```\n\n## Best Practices\n\n1. **Incremental Sync**\n   - Track last sync timestamp\n   - Only sync updated issues\n   - Use webhooks for real-time updates\n\n2. **Conflict Resolution**\n   - Newer update wins\n   - Preserve Linear-specific fields\n   - Log all conflicts\n\n3. **Performance**\n   - Batch API calls\n   - Cache label mappings\n   - Use parallel processing for large syncs\n\n4. **Data Integrity**\n   - Validate required fields\n   - Maintain bidirectional references\n   - Regular sync health checks"
              },
              {
                "name": "/sync-linear-to-issues",
                "description": "Sync Linear tasks to GitHub issues",
                "path": "plugins/commands-integration-sync/commands/sync-linear-to-issues.md",
                "frontmatter": {
                  "description": "Sync Linear tasks to GitHub issues",
                  "category": "integration-sync",
                  "allowed-tools": "Bash(gh *)"
                },
                "content": "# sync-linear-to-issues\n\nSync Linear tasks to GitHub issues\n\n## System\n\nYou are a Linear-to-GitHub synchronization assistant that exports Linear tasks as GitHub issues. You maintain data fidelity, handle complex mappings, and ensure consistent synchronization.\n\n## Instructions\n\nWhen asked to sync Linear tasks to GitHub issues:\n\n1. **Check Prerequisites**\n   - Verify Linear MCP server is available\n   - Check `gh` CLI authentication\n   - Confirm target repository\n\n2. **Fetch Linear Tasks**\n   ```javascript\n   // Query Linear tasks\n   const tasks = await linear.issues({\n     filter: {\n       state: { name: { nin: [\"Canceled\", \"Duplicate\"] } },\n       team: { key: { eq: \"ENG\" } }\n     },\n     includeArchived: false\n   });\n   ```\n\n3. **Field Mapping Strategy**\n   ```\n   Linear Task  GitHub Issue\n   \n   title        title\n   description  body\n   labels       labels\n   assignee     assignees\n   project      milestone\n   state        state (map: backlog/todoopen, done/canceledclosed)\n   identifier   body footer (Linear: ABC-123)\n   url          body footer link\n   priority     labels (priority/urgent, priority/high, etc.)\n   ```\n\n4. **State Mapping**\n   ```javascript\n   const stateMap = {\n     'Backlog': 'open',\n     'Todo': 'open',\n     'In Progress': 'open',\n     'In Review': 'open',\n     'Done': 'closed',\n     'Canceled': 'closed'\n   };\n   ```\n\n5. **Priority to Label Conversion**\n   - Urgent (1)  `priority/urgent`, `bug`\n   - High (2)  `priority/high`\n   - Medium (3)  `priority/medium`\n   - Low (4)  `priority/low`\n   - None (0)  no priority label\n\n6. **Create GitHub Issues**\n   ```bash\n   # Create new issue\n   gh issue create \\\n     --title \"Task title\" \\\n     --body \"Description with Linear reference\" \\\n     --label \"enhancement,priority/high\" \\\n     --assignee \"username\" \\\n     --milestone \"Sprint 23\"\n   ```\n\n7. **Issue Body Template**\n   ```markdown\n   [Original task description]\n   \n   ## Acceptance Criteria\n   - [ ] Criteria from Linear\n   \n   ## Additional Context\n   [Any Linear comments or context]\n   \n   ---\n   *Synced from Linear: [ABC-123](https://linear.app/team/issue/ABC-123)*\n   *Last sync: 2025-01-16T10:30:00Z*\n   ```\n\n8. **Comment Synchronization**\n   ```bash\n   # Add Linear comments to GitHub\n   gh issue comment <issue-number> --body \"Comment from Linear by @user\"\n   ```\n\n9. **Attachment Handling**\n   - Upload Linear attachments to GitHub\n   - Update links in issue body\n   - Preserve file names and types\n\n10. **Rate Limiting & Batching**\n    ```javascript\n    // Batch create issues\n    const BATCH_SIZE = 20;\n    for (let i = 0; i < tasks.length; i += BATCH_SIZE) {\n      const batch = tasks.slice(i, i + BATCH_SIZE);\n      await processBatch(batch);\n      await sleep(2000); // Rate limit delay\n    }\n    ```\n\n## Examples\n\n### Basic Sync\n```bash\n# Sync all Linear tasks\nclaude sync-linear-to-issues\n\n# Sync specific team\nclaude sync-linear-to-issues --team=\"ENG\"\n\n# Sync by project\nclaude sync-linear-to-issues --project=\"Sprint 23\"\n```\n\n### Filtered Sync\n```bash\n# Sync only high priority\nclaude sync-linear-to-issues --priority=\"urgent,high\"\n\n# Sync by assignee\nclaude sync-linear-to-issues --assignee=\"john.doe\"\n\n# Sync with state filter\nclaude sync-linear-to-issues --states=\"Todo,In Progress\"\n```\n\n### Advanced Options\n```bash\n# Include archived tasks\nclaude sync-linear-to-issues --include-archived\n\n# Sync with custom label prefix\nclaude sync-linear-to-issues --label-prefix=\"linear/\"\n\n# Update existing issues\nclaude sync-linear-to-issues --update-existing\n```\n\n## Output Format\n\n```\nLinear to GitHub Sync Report\n============================\nTeam: Engineering\nStarted: 2025-01-16 10:30:00\nCompleted: 2025-01-16 10:35:42\n\nSummary:\n- Total tasks: 75\n- Created issues: 72\n- Updated issues: 2\n- Skipped: 1\n\nDetails:\n ABC-123  #456: Implement user authentication\n ABC-124  #457: Fix memory leak in parser\n ABC-125  #458: Updated: Add caching layer\n ABC-126  Skipped: Already synced\n\nSync Metrics:\n- Average time per issue: 4.2s\n- API calls made: 150\n- Rate limit remaining: 4850/5000\n```\n\n## Conflict Resolution\n\n1. **Duplicate Detection**\n   - Check for existing issues with Linear ID\n   - Compare by title if ID not found\n   - Option to force create duplicates\n\n2. **Update Strategy**\n   - Preserve GitHub-specific fields\n   - Merge labels (don't replace)\n   - Append new comments only\n\n3. **Sync Conflicts**\n   ```\n   Conflict detected for ABC-123:\n   - Linear updated: 2025-01-16 10:00:00\n   - GitHub updated: 2025-01-16 10:05:00\n   \n   Resolution: Using newer (GitHub) version\n   Action: Skipping Linear update\n   ```\n\n## Best Practices\n\n1. **Maintain Sync State**\n   ```json\n   {\n     \"lastSync\": \"2025-01-16T10:30:00Z\",\n     \"syncedTasks\": {\n       \"ABC-123\": { \"githubIssue\": 456, \"lastUpdated\": \"...\" },\n       \"ABC-124\": { \"githubIssue\": 457, \"lastUpdated\": \"...\" }\n     }\n   }\n   ```\n\n2. **Incremental Updates**\n   - Track modification timestamps\n   - Only sync changed tasks\n   - Use Linear webhooks for real-time\n\n3. **Error Recovery**\n   - Log all failures\n   - Implement retry logic\n   - Continue on non-critical errors\n\n4. **Performance Optimization**\n   - Cache team and project mappings\n   - Bulk fetch related data\n   - Use GraphQL for complex queries"
              },
              {
                "name": "/sync-pr-to-task",
                "description": "Link pull requests to Linear tasks",
                "path": "plugins/commands-integration-sync/commands/sync-pr-to-task.md",
                "frontmatter": {
                  "description": "Link pull requests to Linear tasks",
                  "category": "integration-sync",
                  "allowed-tools": "Bash(gh *)"
                },
                "content": "# sync-pr-to-task\n\nLink pull requests to Linear tasks\n\n## System\n\nYou are a PR-to-task synchronization specialist that connects GitHub pull requests with Linear tasks. You extract task references, update statuses bidirectionally, and maintain development workflow integration.\n\n## Instructions\n\nWhen syncing pull requests to Linear tasks:\n\n1. **Detect Linear References**\n   ```javascript\n   function extractLinearRefs(pr) {\n     const patterns = [\n       /([A-Z]{2,5}-\\d+)/g,              // ABC-123\n       /linear\\.app\\/.*\\/issue\\/([A-Z]{2,5}-\\d+)/g,  // Linear URLs\n       /(?:fixes|closes|resolves)\\s+([A-Z]{2,5}-\\d+)/gi  // Keywords\n     ];\n     \n     const refs = new Set();\n     const searchText = `${pr.title} ${pr.body}`;\n     \n     for (const pattern of patterns) {\n       const matches = searchText.matchAll(pattern);\n       for (const match of matches) {\n         refs.add(match[1].toUpperCase());\n       }\n     }\n     \n     return Array.from(refs);\n   }\n   ```\n\n2. **Fetch PR Details**\n   ```bash\n   # Get PR information\n   gh pr view <pr-number> --json \\\n     number,title,body,state,draft,author,assignees,\\\n     labels,milestone,createdAt,updatedAt,mergedAt,\\\n     commits,additions,deletions,changedFiles,reviews\n   ```\n\n3. **PR State Mapping**\n   ```javascript\n   function mapPRStateToLinear(pr) {\n     if (pr.draft) return 'Backlog';\n     if (pr.state === 'CLOSED' && !pr.merged) return 'Canceled';\n     if (pr.merged) return 'Done';\n     \n     // Check reviews\n     const hasApprovals = pr.reviews.some(r => r.state === 'APPROVED');\n     const hasRequestedChanges = pr.reviews.some(r => r.state === 'CHANGES_REQUESTED');\n     \n     if (hasRequestedChanges) return 'Todo';\n     if (hasApprovals) return 'In Review';\n     if (pr.state === 'OPEN') return 'In Progress';\n     \n     return 'Todo';\n   }\n   ```\n\n4. **Update Linear Task**\n   ```javascript\n   async function updateLinearTask(taskId, prData) {\n     const updates = {\n       // Update state based on PR\n       state: mapPRStateToLinear(prData),\n       \n       // Add PR link to description\n       description: appendPRLink(task.description, prData.url),\n       \n       // Update custom fields\n       customFields: {\n         githubPR: prData.number,\n         prStatus: prData.state,\n         prAuthor: prData.author.login\n       }\n     };\n     \n     // Add PR labels\n     if (prData.labels.includes('bug')) {\n       updates.labels = [...task.labels, 'Has PR', 'Bug Fix'];\n     }\n     \n     await linear.updateIssue(taskId, updates);\n   }\n   ```\n\n5. **Create Linear Comment**\n   ```javascript\n   function createPRComment(taskId, pr) {\n     const comment = `\n    **Pull Request ${pr.draft ? 'Draft ' : ''}#${pr.number}**\n   \n   **Title:** ${pr.title}\n   **Author:** @${pr.author.login}\n   **Status:** ${pr.state} ${pr.merged ? '(Merged)' : ''}\n   **Changes:** +${pr.additions} -${pr.deletions} in ${pr.changedFiles} files\n   \n   **Reviews:**\n   ${formatReviews(pr.reviews)}\n   \n   [View on GitHub](${pr.url})\n     `;\n     \n     return linear.createComment(taskId, { body: comment });\n   }\n   ```\n\n6. **Update PR with Linear Info**\n   ```bash\n   # Add Linear task info to PR\n   gh pr comment <pr-number> --body \"\n   ## Linear Task: $TASK_ID\n   \n   This PR addresses: [$TASK_ID - $TASK_TITLE]($TASK_URL)\n   \n   **Task Status:** $TASK_STATUS\n   **Priority:** $TASK_PRIORITY\n   **Assignee:** $TASK_ASSIGNEE\n   \"\n   \n   # Add labels\n   gh pr edit <pr-number> --add-label \"linear:$TASK_ID\"\n   ```\n\n7. **Automated Status Updates**\n   ```javascript\n   // PR event handlers\n   const prEventHandlers = {\n     'opened': async (pr, taskId) => {\n       await updateTaskState(taskId, 'In Progress');\n       await addComment(taskId, 'PR opened');\n     },\n     \n     'ready_for_review': async (pr, taskId) => {\n       await updateTaskState(taskId, 'In Review');\n       await addComment(taskId, 'PR ready for review');\n     },\n     \n     'merged': async (pr, taskId) => {\n       await updateTaskState(taskId, 'Done');\n       await addComment(taskId, 'PR merged');\n     },\n     \n     'closed': async (pr, taskId) => {\n       if (!pr.merged) {\n         await addComment(taskId, 'PR closed without merging');\n       }\n     }\n   };\n   ```\n\n8. **Branch Detection**\n   ```javascript\n   function detectTaskFromBranch(branchName) {\n     // Common patterns\n     const patterns = [\n       /^(?:feature|fix|bug)\\/([A-Z]{2,5}-\\d+)/,  // feature/ABC-123\n       /^([A-Z]{2,5}-\\d+)/,                        // ABC-123\n       /([A-Z]{2,5}-\\d+)$/                         // anything-ABC-123\n     ];\n     \n     for (const pattern of patterns) {\n       const match = branchName.match(pattern);\n       if (match) return match[1];\n     }\n     \n     return null;\n   }\n   ```\n\n9. **Webhook Configuration**\n   ```yaml\n   # GitHub webhook events\n   events:\n     - pull_request.opened\n     - pull_request.closed\n     - pull_request.ready_for_review\n     - pull_request.converted_to_draft\n     - pull_request_review.submitted\n     - pull_request.merged\n   ```\n\n10. **Sync Validation**\n    ```javascript\n    async function validateSync(pr, task) {\n      const warnings = [];\n      \n      // Check assignee match\n      if (pr.assignees[0]?.login !== mapToGitHub(task.assignee)) {\n        warnings.push('Assignee mismatch between PR and task');\n      }\n      \n      // Check labels\n      if (!hasMatchingLabels(pr.labels, task.labels)) {\n        warnings.push('Label inconsistency detected');\n      }\n      \n      // Check milestone/project\n      if (pr.milestone?.title !== task.project?.name) {\n        warnings.push('Different milestone/project');\n      }\n      \n      return warnings;\n    }\n    ```\n\n## Examples\n\n### Manual PR Linking\n```bash\n# Link PR to Linear task\nclaude sync-pr-to-task 123 --task=\"ABC-456\"\n\n# Auto-detect task from PR\nclaude sync-pr-to-task 123\n\n# Link multiple PRs\nclaude sync-pr-to-task 123,124,125 --task=\"ABC-456\"\n```\n\n### Automated Sync\n```bash\n# Enable auto-sync for repository\nclaude sync-pr-to-task --enable-auto --repo=\"owner/repo\"\n\n# Configure sync behavior\nclaude sync-pr-to-task --config \\\n  --update-state=\"true\" \\\n  --sync-reviews=\"true\" \\\n  --sync-labels=\"true\"\n```\n\n### Status Monitoring\n```bash\n# Check PR-task links\nclaude sync-pr-to-task --status\n\n# Find unlinked PRs\nclaude sync-pr-to-task --find-unlinked\n\n# Validate existing links\nclaude sync-pr-to-task --validate\n```\n\n## Output Format\n\n```\nPR to Linear Task Sync\n======================\nRepository: owner/repo\nPR: #123 - Implement caching layer\n\nLinear Task Detection:\n Found task reference: ABC-456\n Task exists in Linear\n Task is in \"In Progress\" state\n\nSync Actions:\n Updated Linear task state  \"In Review\"\n Added PR link to task description\n Created comment in Linear with PR details\n Added \"linear:ABC-456\" label to PR\n Posted Linear task summary to PR\n\nValidation Results:\n Assignees match\n Label mismatch: PR has \"enhancement\", task has \"feature\"\n Both targeting same milestone\n\nAutomated Sync: Enabled\nNext sync: On PR update\n```\n\n## Advanced Features\n\n### Smart State Synchronization\n```javascript\nconst stateSync = {\n  // PR state  Linear state\n  prToLinear: {\n    'draft': 'Backlog',\n    'open': 'In Progress',\n    'ready_for_review': 'In Review',\n    'merged': 'Done',\n    'closed': null  // Don't change\n  },\n  \n  // Linear state  PR action\n  linearToPR: {\n    'Backlog': 'convert_to_draft',\n    'In Progress': 'ready_for_review',\n    'Done': 'merge',\n    'Canceled': 'close'\n  }\n};\n```\n\n### Commit Analysis\n```javascript\nasync function analyzeCommits(pr, taskId) {\n  const commits = await getPRCommits(pr.number);\n  \n  const analysis = {\n    totalCommits: commits.length,\n    authors: new Set(commits.map(c => c.author)),\n    timeSpent: calculateTimeSpent(commits),\n    filesChanged: await getChangedFiles(pr.number),\n    testCoverage: await getTestCoverage(pr.number)\n  };\n  \n  // Update Linear task with insights\n  await updateTaskWithMetrics(taskId, analysis);\n}\n```\n\n## Best Practices\n\n1. **Clear References**\n   - Use branch naming conventions\n   - Include task ID in PR title\n   - Reference in PR body\n\n2. **Automation**\n   - Set up webhooks for real-time sync\n   - Use GitHub Actions for validation\n   - Automate state transitions\n\n3. **Data Quality**\n   - Validate links regularly\n   - Clean up stale references\n   - Monitor sync health"
              },
              {
                "name": "/sync-status",
                "description": "Monitor GitHub-Linear sync health status",
                "path": "plugins/commands-integration-sync/commands/sync-status.md",
                "frontmatter": {
                  "description": "Monitor GitHub-Linear sync health status",
                  "category": "integration-sync"
                },
                "content": "# sync-status\n\nMonitor GitHub-Linear sync health status\n\n## System\n\nYou are a sync health monitoring specialist that tracks, analyzes, and reports on the synchronization status between GitHub and Linear. You identify issues, measure performance, and ensure data consistency across platforms.\n\n## Instructions\n\nWhen checking synchronization status:\n\n1. **Sync State Overview**\n   ```javascript\n   async function getSyncOverview() {\n     const state = await loadSyncState();\n     \n     return {\n       lastFullSync: state.lastFullSync,\n       lastIncrementalSync: state.lastIncremental,\n       totalSyncedItems: Object.keys(state.entities).length,\n       pendingSync: state.queue.length,\n       failedSync: state.failures.length,\n       syncEnabled: state.config.enabled,\n       syncDirection: state.config.direction,\n       webhooksActive: await checkWebhooks()\n     };\n   }\n   ```\n\n2. **Health Metrics**\n   ```javascript\n   const healthMetrics = {\n     // Performance metrics\n     avgSyncTime: calculateAverage(syncTimes),\n     maxSyncTime: Math.max(...syncTimes),\n     syncSuccessRate: (successful / total) * 100,\n     \n     // Data quality metrics\n     conflictRate: (conflicts / syncs) * 100,\n     duplicateRate: (duplicates / total) * 100,\n     orphanedItems: countOrphaned(),\n     \n     // API health\n     githubRateLimit: await getGitHubRateLimit(),\n     linearRateLimit: await getLinearRateLimit(),\n     apiErrors: recentErrors.length,\n     \n     // Sync lag\n     avgSyncLag: calculateSyncLag(),\n     maxSyncLag: findMaxLag(),\n     itemsOutOfSync: findOutOfSync().length\n   };\n   ```\n\n3. **Consistency Checks**\n   ```javascript\n   async function checkConsistency() {\n     const issues = [];\n     \n     // Check GitHub  Linear\n     const githubIssues = await fetchAllGitHubIssues();\n     for (const issue of githubIssues) {\n       const linearTask = await findLinearTask(issue);\n       if (!linearTask) {\n         issues.push({\n           type: 'MISSING_IN_LINEAR',\n           github: issue.number,\n           severity: 'high'\n         });\n       } else {\n         const diffs = compareFields(issue, linearTask);\n         if (diffs.length > 0) {\n           issues.push({\n             type: 'FIELD_MISMATCH',\n             github: issue.number,\n             linear: linearTask.identifier,\n             differences: diffs,\n             severity: 'medium'\n           });\n         }\n       }\n     }\n     \n     return issues;\n   }\n   ```\n\n4. **Sync History Analysis**\n   ```javascript\n   function analyzeSyncHistory(days = 7) {\n     const history = loadSyncHistory(days);\n     \n     return {\n       totalSyncs: history.length,\n       byType: groupBy(history, 'type'),\n       byDirection: groupBy(history, 'direction'),\n       successRate: calculateRate(history, 'success'),\n       \n       patterns: {\n         peakHours: findPeakSyncHours(history),\n         commonErrors: findCommonErrors(history),\n         slowestOperations: findSlowestOps(history)\n       },\n       \n       trends: {\n         syncVolume: calculateTrend(history, 'volume'),\n         errorRate: calculateTrend(history, 'errors'),\n         performance: calculateTrend(history, 'duration')\n       }\n     };\n   }\n   ```\n\n5. **Real-time Monitoring**\n   ```javascript\n   class SyncMonitor {\n     constructor() {\n       this.metrics = new Map();\n       this.alerts = [];\n     }\n     \n     track(operation) {\n       const start = Date.now();\n       \n       return {\n         complete: (success, details) => {\n           const duration = Date.now() - start;\n           this.metrics.set(operation.id, {\n             ...operation,\n             duration,\n             success,\n             details,\n             timestamp: new Date()\n           });\n           \n           // Check for alerts\n           if (duration > SLOW_SYNC_THRESHOLD) {\n             this.alert('SLOW_SYNC', operation);\n           }\n           if (!success) {\n             this.alert('SYNC_FAILURE', operation);\n           }\n         }\n       };\n     }\n   }\n   ```\n\n6. **Webhook Status**\n   ```bash\n   # Check GitHub webhooks\n   gh api repos/:owner/:repo/hooks --jq '.[] | select(.config.url | contains(\"linear\"))'\n   \n   # Validate webhook health\n   gh api repos/:owner/:repo/hooks/:id/deliveries --jq '.[0:10] | .[] | {id, status_code, delivered_at}'\n   ```\n\n7. **Queue Management**\n   ```javascript\n   async function getQueueStatus() {\n     const queue = await loadSyncQueue();\n     \n     return {\n       size: queue.length,\n       oldest: queue[0]?.createdAt,\n       byPriority: groupBy(queue, 'priority'),\n       estimatedTime: estimateProcessingTime(queue),\n       \n       blocked: queue.filter(item => item.retries >= MAX_RETRIES),\n       processing: queue.filter(item => item.status === 'processing'),\n       pending: queue.filter(item => item.status === 'pending')\n     };\n   }\n   ```\n\n8. **Diagnostic Reports**\n   ```javascript\n   function generateDiagnostics() {\n     return {\n       systemInfo: {\n         version: SYNC_VERSION,\n         githubCLI: checkGitHubCLI(),\n         linearMCP: checkLinearMCP(),\n         config: loadSyncConfig()\n       },\n       \n       connectivity: {\n         github: testGitHubAPI(),\n         linear: testLinearAPI(),\n         webhooks: testWebhooks()\n       },\n       \n       dataIntegrity: {\n         orphanedGitHub: findOrphanedGitHubIssues(),\n         orphanedLinear: findOrphanedLinearTasks(),\n         duplicates: findDuplicates(),\n         conflicts: findConflicts()\n       },\n       \n       recommendations: generateRecommendations()\n     };\n   }\n   ```\n\n9. **Alert Configuration**\n   ```yaml\n   alerts:\n     - name: high_conflict_rate\n       condition: conflict_rate > 10%\n       severity: warning\n       action: notify\n     \n     - name: sync_failure\n       condition: success_rate < 95%\n       severity: critical\n       action: pause_sync\n     \n     - name: api_rate_limit\n       condition: rate_limit_remaining < 100\n       severity: warning\n       action: throttle\n   ```\n\n10. **Performance Visualization**\n    ```\n    Sync Performance (Last 24h)\n    \n    \n    Sync Volume:\n    00:00  23:59\n    \n    Success Rate: 98.5%\n     \n    \n    Avg Duration: 2.3s\n     (Target: 5s)\n    ```\n\n## Examples\n\n### Basic Status Check\n```bash\n# Get current sync status\nclaude sync-status\n\n# Detailed status with history\nclaude sync-status --detailed\n\n# Check specific sync types\nclaude sync-status --type=\"issue-to-linear\"\n```\n\n### Health Monitoring\n```bash\n# Run health check\nclaude sync-status --health-check\n\n# Continuous monitoring\nclaude sync-status --monitor --interval=5m\n\n# Generate diagnostic report\nclaude sync-status --diagnostics\n```\n\n### Troubleshooting\n```bash\n# Check for sync issues\nclaude sync-status --check-issues\n\n# Verify specific items\nclaude sync-status --verify=\"gh-123,ABC-456\"\n\n# Queue management\nclaude sync-status --queue --clear-failed\n```\n\n## Output Format\n\n```\nGitHub-Linear Sync Status\n=========================\nLast Updated: 2025-01-16 10:45:00\n\nOverview:\n Sync Enabled: Bidirectional\n Webhooks: Active (GitHub: , Linear: )\n Last Full Sync: 2 hours ago\n Last Activity: 5 minutes ago\n\nStatistics:\n- Total Synced Items: 1,234\n- Items in Queue: 3\n- Failed Items: 1\n\nHealth Metrics:\n\nSuccess Rate     96.5%\nConflict Rate     8.2%\nSync Lag         ~2min\n\nAPI Status:\n- GitHub: 4,832/5,000 requests remaining\n- Linear: 1,245/1,500 requests remaining\n\nRecent Activity:\n10:44  Issue #123  ABC-789 (1.2s)\n10:42  ABC-788  Issue #122 (0.8s)\n10:40  Issue #121  Conflict detected\n10:38  PR #456  ABC-787 linked\n\nAlerts:\n High conflict rate in last hour (12%)\n 1 item failed after max retries\n\nRecommendations:\n1. Review and resolve conflict for Issue #121\n2. Retry failed sync for ABC-456\n3. Consider increasing sync frequency\n```\n\n## Advanced Features\n\n### Sync Analytics Dashboard\n```\n\n                 SYNC ANALYTICS DASHBOARD\n\n\nDaily Sync Volume          Sync Types\n\n     150                 Issues  Linear  45%\n     120              Linear  Issues  30%\n      90               PR  Task        20%\n      60               Comments          5%\n      30        ___   \n       0    \n         Mon  Wed  Fri    \n\nError Distribution         Performance Trends\n\nNetwork       40%      Avg Time   2.3s\nRate Limit     30%      P95 Time   5.1s\nConflicts       20%      P99 Time   8.2s\nOther            10%     \n```\n\n### Predictive Analysis\n```javascript\nfunction predictSyncIssues() {\n  const patterns = analyzeHistoricalData();\n  \n  return {\n    likelyConflicts: predictConflicts(patterns),\n    peakLoadTimes: predictPeakLoad(patterns),\n    rateLimitRisk: calculateRateLimitRisk(),\n    recommendations: {\n      optimalSyncInterval: calculateOptimalInterval(),\n      suggestedBatchSize: calculateOptimalBatch(),\n      conflictPrevention: suggestConflictStrategies()\n    }\n  };\n}\n```\n\n## Best Practices\n\n1. **Regular Monitoring**\n   - Set up automated health checks\n   - Review sync metrics daily\n   - Act on alerts promptly\n\n2. **Proactive Maintenance**\n   - Clear failed items regularly\n   - Optimize sync intervals\n   - Update conflict strategies\n\n3. **Documentation**\n   - Log all sync issues\n   - Document resolution steps\n   - Track performance trends"
              },
              {
                "name": "/task-from-pr",
                "description": "Create Linear tasks from pull requests",
                "path": "plugins/commands-integration-sync/commands/task-from-pr.md",
                "frontmatter": {
                  "description": "Create Linear tasks from pull requests",
                  "category": "integration-sync",
                  "allowed-tools": "Bash(gh *)"
                },
                "content": "# task-from-pr\n\nCreate Linear tasks from pull requests\n\n## Purpose\nThis command analyzes GitHub pull requests and creates corresponding Linear tasks, automatically extracting key information like title, description, labels, and assignees. It helps maintain synchronization between GitHub development workflow and Linear project management.\n\n## Usage\n```bash\n# Convert a specific PR to a Linear task\nclaude \"Convert PR #123 to a Linear task\"\n\n# Convert multiple PRs from a repository\nclaude \"Convert all open PRs to Linear tasks for repo owner/repo\"\n\n# Convert PR with custom mapping\nclaude \"Create Linear task from PR #456 and assign to team 'Engineering'\"\n```\n\n## Instructions\n\n### 1. Gather PR Information\nFirst, use GitHub CLI to fetch PR details:\n\n```bash\n# Get PR information\ngh pr view <PR_NUMBER> --json title,body,labels,assignees,state,url,createdAt,updatedAt,milestone\n\n# List all open PRs\ngh pr list --json number,title,labels,assignees --limit 100\n```\n\n### 2. Parse PR Description\nExtract structured information from the PR body:\n\n- Look for sections like \"## Description\", \"## Changes\", \"## Testing\"\n- Identify checklist items (- [ ] or - [x])\n- Extract any mentioned issue numbers (#123)\n- Find @mentions for stakeholders\n- Identify code blocks for technical details\n\n### 3. Map GitHub Labels to Linear\nCommon label mappings:\n- `bug`  Linear label: \"Bug\" + Priority: High\n- `feature`  Linear label: \"Feature\"\n- `enhancement`  Linear label: \"Improvement\"\n- `documentation`  Linear label: \"Documentation\"\n- `performance`  Linear label: \"Performance\"\n- `security`  Linear label: \"Security\" + Priority: Urgent\n\n### 4. Extract Task Details\nGenerate Linear task structure:\n\n```javascript\n{\n  title: `[PR #${prNumber}] ${prTitle}`,\n  description: `\n    **GitHub PR:** ${prUrl}\n    \n    ## Summary\n    ${extractedSummary}\n    \n    ## Changes\n    ${bulletPoints}\n    \n    ## Acceptance Criteria\n    ${checklistItems}\n    \n    ## Technical Details\n    ${codeSnippets}\n  `,\n  priority: mapPriorityFromLabels(labels),\n  labels: mapLabelsToLinear(labels),\n  estimate: estimateFromPRSize(additions, deletions),\n  assignee: mapGitHubUserToLinear(assignees[0])\n}\n```\n\n### 5. Estimate Task Size\nCalculate estimates based on PR metrics:\n\n```\n- Tiny (1 point): < 10 lines changed\n- Small (2 points): 10-50 lines changed\n- Medium (3 points): 50-250 lines changed\n- Large (5 points): 250-500 lines changed\n- X-Large (8 points): > 500 lines changed\n\nAdjust based on:\n- Number of files changed (multiply by 1.2 if > 10 files)\n- Presence of tests (multiply by 0.8 if tests included)\n- Documentation changes (multiply by 0.7 if only docs)\n```\n\n### 6. Create Linear Task\nUse Linear MCP to create the task:\n\n```javascript\n// Example Linear task creation\nconst task = await linear.createTask({\n  title: taskTitle,\n  description: taskDescription,\n  teamId: getTeamId(),\n  priority: priority,\n  estimate: estimate,\n  labels: labelIds,\n  assigneeId: assigneeId\n});\n\n// Link back to GitHub PR\nawait linear.createComment({\n  issueId: task.id,\n  body: `Linked to GitHub PR: ${prUrl}`\n});\n```\n\n### 7. Error Handling\nHandle common scenarios:\n\n```javascript\n// Check for Linear MCP availability\nif (!linear.available) {\n  console.error(\"Linear MCP tool not available. Please ensure it's configured.\");\n  return;\n}\n\n// Check for GitHub CLI\ntry {\n  await exec('gh --version');\n} catch (error) {\n  console.error(\"GitHub CLI not installed. Please install: https://cli.github.com/\");\n  return;\n}\n\n// Handle duplicate tasks\nconst existingTask = await linear.searchTasks(`PR #${prNumber}`);\nif (existingTask) {\n  console.log(`Task already exists for PR #${prNumber}: ${existingTask.url}`);\n  return;\n}\n```\n\n## Example Output\n\n```\nConverting PR #123 to Linear task...\n\nFetched PR details:\n- Title: Add user authentication middleware\n- Author: @johndoe\n- Labels: feature, backend, security\n- Size: 234 lines changed across 8 files\n\nParsed description:\n- Summary: Implements JWT-based authentication\n- Has 5 checklist items (3 completed)\n- References issues: #98, #102\n\nCreating Linear task...\n Task created: LIN-456\n  Title: [PR #123] Add user authentication middleware\n  Team: Backend\n  Priority: High (due to security label)\n  Estimate: 3 points\n  Labels: Feature, Backend, Security\n  Assignee: John Doe\n\nTask URL: https://linear.app/yourteam/issue/LIN-456\n```\n\n## Advanced Features\n\n### Batch Processing\nConvert multiple PRs:\n```bash\n# Convert all PRs with specific label\ngh pr list --label \"needs-task\" --json number | \\\n  jq -r '.[].number' | \\\n  xargs -I {} claude \"Convert PR #{} to Linear task\"\n```\n\n### Custom Field Mapping\nMap PR metadata to Linear custom fields:\n- PR review status  Linear custom field \"Review Status\"\n- PR branch name  Linear custom field \"Feature Branch\"\n- CI/CD status  Linear custom field \"Build Status\"\n\n### Automated Sync\nSet up webhook to automatically create tasks when PRs are opened:\n```javascript\n// Webhook handler\non('pull_request.opened', async (event) => {\n  await createLinearTaskFromPR(event.pull_request);\n});\n```\n\n## Tips\n- Include PR number in task title for easy reference\n- Use Linear's GitHub integration to auto-link commits\n- Set up bidirectional sync to update PR when task status changes\n- Create subtasks for PR checklist items if needed\n- Add PR author as a subscriber if they're not the assignee"
              }
            ],
            "skills": []
          },
          {
            "name": "commands-miscellaneous",
            "description": "General-purpose utility commands",
            "source": "./plugins/commands-miscellaneous",
            "category": "commands",
            "version": "1.0.0",
            "author": {
              "name": "BuildWithClaude Community",
              "url": "https://github.com/davepoon/buildwithclaude"
            },
            "install_commands": [
              "/plugin marketplace add davepoon/buildwithclaude",
              "/plugin install commands-miscellaneous@buildwithclaude"
            ],
            "signals": {
              "stars": 2229,
              "forks": 239,
              "pushed_at": "2026-01-12T10:19:01Z",
              "created_at": "2025-07-25T02:26:45Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/five",
                "description": "Apply the Five Whys root cause analysis technique to systematically investigate issues",
                "path": "plugins/commands-miscellaneous/commands/five.md",
                "frontmatter": {
                  "description": "Apply the Five Whys root cause analysis technique to systematically investigate issues",
                  "category": "miscellaneous",
                  "argument-hint": "<issue_description>"
                },
                "content": "# Five Whys Analysis\n\nApply the Five Whys root cause analysis technique to investigate: $ARGUMENTS\n\n## Description\nThis command implements the Five Whys problem-solving methodology, iteratively asking \"why\" to drill down from symptoms to root causes. It helps identify the fundamental reason behind a problem rather than just addressing surface-level symptoms.\n\n## Usage\n`five [issue_description]`\n\n## Variables\n- ISSUE: The problem or symptom to analyze (default: prompt for input)\n- DEPTH: Number of \"why\" iterations (default: 5, can be adjusted)\n\n## Steps\n1. Start with the problem statement\n2. Ask \"Why did this happen?\" and document the answer\n3. For each answer, ask \"Why?\" again\n4. Continue for at least 5 iterations or until root cause is found\n5. Validate the root cause by working backwards\n6. Propose solutions that address the root cause\n\n## Examples\n### Example 1: Application crash analysis\n```\nProblem: Application crashes on startup\nWhy 1: Database connection fails\nWhy 2: Connection string is invalid\nWhy 3: Environment variable not set\nWhy 4: Deployment script missing env setup\nWhy 5: Documentation didn't specify env requirements\nRoot Cause: Missing deployment documentation\n```\n\n### Example 2: Performance issue investigation\nSystematically trace why a feature is running slowly by examining each contributing factor.\n\n## Notes\n- Don't stop at symptoms; keep digging for systemic issues\n- Multiple root causes may exist - explore different branches\n- Document each \"why\" for future reference\n- Consider both technical and process-related causes\n- The magic isn't in exactly 5 whys - stop when you reach the true root cause"
              },
              {
                "name": "/mermaid",
                "description": "Create entity relationship diagrams using Mermaid from SQL/database files",
                "path": "plugins/commands-miscellaneous/commands/mermaid.md",
                "frontmatter": {
                  "description": "Create entity relationship diagrams using Mermaid from SQL/database files",
                  "category": "miscellaneous",
                  "argument-hint": "<source-path> [output-path]",
                  "allowed-tools": "Read, Write, Bash, Glob"
                },
                "content": "Create Mermaid entity relationship diagrams (ERD) from SQL migration files or database schemas.\n\n## Process:\n\n1. **Parse Arguments**:\n   - First argument: Source path (SQL files or directory)\n   - Second argument: Output path (optional, defaults to `docs/erd.md`)\n\n2. **Find SQL/Schema Files**:\n   - Look for SQL files: `*.sql`, `*.ddl`\n   - Check common locations if no path provided:\n     - `migrations/`, `db/migrations/`, `schema/`\n     - `database/`, `sql/`\n   - Support multiple database formats:\n     - PostgreSQL, MySQL, SQLite\n     - Migration files (Rails, Django, Flyway, etc.)\n\n3. **Extract Schema Information**:\n   - Parse CREATE TABLE statements\n   - Extract table names, columns, and data types\n   - Identify primary keys, foreign keys, and relationships\n   - Handle indexes and constraints\n\n4. **Generate Mermaid ERD**:\n   ```mermaid\n   erDiagram\n     CUSTOMER ||--o{ ORDER : places\n     ORDER ||--|{ LINE-ITEM : contains\n     CUSTOMER {\n       string name\n       string email\n       int id PK\n     }\n   ```\n\n5. **Validate Diagram**:\n   - If mermaid-cli is available: `npx -p @mermaid-js/mermaid-cli mmdc -i output.md -o temp.svg`\n   - Alternative validation: Check syntax manually\n   - Clean up temporary files\n\n6. **Output Options**:\n   - Single file with all entities\n   - Separate files per schema/database\n   - Include relationship descriptions\n\n## Example Usage:\n- `/mermaid migrations/` - Create ERD from all SQL files in migrations\n- `/mermaid schema.sql docs/database-erd.md` - Create ERD from specific file\n- `/mermaid \"db/**/*.sql\" erd/` - Create ERDs for all SQL files\n\nSource: $ARGUMENTS"
              },
              {
                "name": "/use-stepper",
                "description": "Use structured stepper approach for problem-solving and project development",
                "path": "plugins/commands-miscellaneous/commands/use-stepper.md",
                "frontmatter": {
                  "description": "Use structured stepper approach for problem-solving and project development",
                  "category": "miscellaneous"
                },
                "content": "<Stepper>\n\n1. **Identify the Problem**\n1. **Plan Your Project**\n1. **Build Your Solution**\n1. **Test and Deploy**\n\n</Stepper>"
              }
            ],
            "skills": []
          },
          {
            "name": "commands-monitoring-observability",
            "description": "Commands for setting up monitoring and observability",
            "source": "./plugins/commands-monitoring-observability",
            "category": "commands",
            "version": "1.0.0",
            "author": {
              "name": "BuildWithClaude Community",
              "url": "https://github.com/davepoon/buildwithclaude"
            },
            "install_commands": [
              "/plugin marketplace add davepoon/buildwithclaude",
              "/plugin install commands-monitoring-observability@buildwithclaude"
            ],
            "signals": {
              "stars": 2229,
              "forks": 239,
              "pushed_at": "2026-01-12T10:19:01Z",
              "created_at": "2025-07-25T02:26:45Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/add-performance-monitoring",
                "description": "Setup application performance monitoring",
                "path": "plugins/commands-monitoring-observability/commands/add-performance-monitoring.md",
                "frontmatter": {
                  "description": "Setup application performance monitoring",
                  "category": "monitoring-observability",
                  "allowed-tools": "Glob"
                },
                "content": "# Add Performance Monitoring\n\nSetup application performance monitoring\n\n## Instructions\n\n1. **Performance Monitoring Strategy**\n   - Define key performance indicators (KPIs) and service level objectives (SLOs)\n   - Identify critical user journeys and performance bottlenecks\n   - Plan monitoring architecture and data collection strategy\n   - Assess existing monitoring infrastructure and integration points\n   - Define alerting thresholds and escalation procedures\n\n2. **Application Performance Monitoring (APM)**\n   - Set up comprehensive APM monitoring:\n\n   **Node.js APM with New Relic:**\n   ```javascript\n   // newrelic.js\n   exports.config = {\n     app_name: [process.env.NEW_RELIC_APP_NAME || 'My Application'],\n     license_key: process.env.NEW_RELIC_LICENSE_KEY,\n     distributed_tracing: {\n       enabled: true\n     },\n     transaction_tracer: {\n       enabled: true,\n       transaction_threshold: 0.5, // 500ms\n       record_sql: 'obfuscated',\n       explain_threshold: 1000 // 1 second\n     },\n     error_collector: {\n       enabled: true,\n       ignore_status_codes: [404, 401]\n     },\n     browser_monitoring: {\n       enable: true\n     },\n     application_logging: {\n       forwarding: {\n         enabled: true\n       }\n     }\n   };\n\n   // app.js\n   require('newrelic');\n   const express = require('express');\n   const app = express();\n\n   // Custom metrics\n   const newrelic = require('newrelic');\n\n   app.use((req, res, next) => {\n     const startTime = Date.now();\n     \n     res.on('finish', () => {\n       const duration = Date.now() - startTime;\n       \n       // Record custom metrics\n       newrelic.recordMetric('Custom/ResponseTime', duration);\n       newrelic.recordMetric(`Custom/Endpoint/${req.path}`, duration);\n       \n       // Add custom attributes\n       newrelic.addCustomAttributes({\n         'user.id': req.user?.id,\n         'request.method': req.method,\n         'response.statusCode': res.statusCode\n       });\n     });\n     \n     next();\n   });\n   ```\n\n   **Datadog APM Integration:**\n   ```javascript\n   // datadog-tracer.js\n   const tracer = require('dd-trace').init({\n     service: 'my-application',\n     env: process.env.NODE_ENV,\n     version: process.env.APP_VERSION,\n     logInjection: true,\n     runtimeMetrics: true,\n     profiling: true,\n     analytics: true\n   });\n\n   // Custom instrumentation\n   class PerformanceTracker {\n     static startSpan(operationName, options = {}) {\n       return tracer.startSpan(operationName, {\n         tags: {\n           'service.name': 'my-application',\n           ...options.tags\n         },\n         ...options\n       });\n     }\n\n     static async traceAsync(operationName, asyncFn, tags = {}) {\n       const span = this.startSpan(operationName, { tags });\n       \n       try {\n         const result = await asyncFn(span);\n         span.setTag('operation.success', true);\n         return result;\n       } catch (error) {\n         span.setTag('operation.success', false);\n         span.setTag('error.message', error.message);\n         span.setTag('error.stack', error.stack);\n         throw error;\n       } finally {\n         span.finish();\n       }\n     }\n\n     static trackDatabaseQuery(query, duration, success) {\n       tracer.startSpan('database.query', {\n         tags: {\n           'db.statement': query,\n           'db.duration': duration,\n           'db.success': success\n         }\n       }).finish();\n     }\n   }\n\n   // Usage example\n   app.get('/api/users/:id', async (req, res) => {\n     await PerformanceTracker.traceAsync('get_user', async (span) => {\n       span.setTag('user.id', req.params.id);\n       \n       const user = await getUserFromDatabase(req.params.id);\n       span.setTag('user.found', !!user);\n       \n       res.json(user);\n     }, { endpoint: '/api/users/:id' });\n   });\n   ```\n\n3. **Real User Monitoring (RUM)**\n   - Implement client-side performance tracking:\n\n   **Web Vitals Monitoring:**\n   ```javascript\n   // performance-monitor.js\n   import { getCLS, getFID, getFCP, getLCP, getTTFB } from 'web-vitals';\n\n   class RealUserMonitoring {\n     constructor() {\n       this.metrics = {};\n       this.setupWebVitals();\n       this.setupCustomMetrics();\n     }\n\n     setupWebVitals() {\n       getCLS(this.sendMetric.bind(this, 'CLS'));\n       getFID(this.sendMetric.bind(this, 'FID'));\n       getFCP(this.sendMetric.bind(this, 'FCP'));\n       getLCP(this.sendMetric.bind(this, 'LCP'));\n       getTTFB(this.sendMetric.bind(this, 'TTFB'));\n     }\n\n     setupCustomMetrics() {\n       // Track page load performance\n       window.addEventListener('load', () => {\n         const navigation = performance.getEntriesByType('navigation')[0];\n         \n         this.sendMetric('page_load_time', {\n           name: 'page_load_time',\n           value: navigation.loadEventEnd - navigation.fetchStart,\n           delta: navigation.loadEventEnd - navigation.fetchStart\n         });\n\n         this.sendMetric('dom_content_loaded', {\n           name: 'dom_content_loaded',\n           value: navigation.domContentLoadedEventEnd - navigation.fetchStart,\n           delta: navigation.domContentLoadedEventEnd - navigation.fetchStart\n         });\n       });\n\n       // Track resource loading\n       new PerformanceObserver((list) => {\n         for (const entry of list.getEntries()) {\n           if (entry.duration > 1000) { // Resources taking >1s\n             this.sendMetric('slow_resource', {\n               name: 'slow_resource',\n               value: entry.duration,\n               resource: entry.name,\n               type: entry.initiatorType\n             });\n           }\n         }\n       }).observe({ entryTypes: ['resource'] });\n\n       // Track user interactions\n       ['click', 'keydown', 'touchstart'].forEach(eventType => {\n         document.addEventListener(eventType, (event) => {\n           const startTime = performance.now();\n           \n           requestIdleCallback(() => {\n             const duration = performance.now() - startTime;\n             if (duration > 100) { // Interactions taking >100ms\n               this.sendMetric('slow_interaction', {\n                 name: 'slow_interaction',\n                 value: duration,\n                 eventType: eventType,\n                 target: event.target.tagName\n               });\n             }\n           });\n         });\n       });\n     }\n\n     sendMetric(metricName, metric) {\n       const data = {\n         name: metricName,\n         value: metric.value,\n         delta: metric.delta,\n         id: metric.id,\n         url: window.location.href,\n         userAgent: navigator.userAgent,\n         timestamp: Date.now(),\n         sessionId: this.getSessionId(),\n         userId: this.getUserId()\n       };\n\n       // Send to analytics endpoint\n       navigator.sendBeacon('/api/metrics', JSON.stringify(data));\n     }\n\n     getSessionId() {\n       return sessionStorage.getItem('sessionId') || 'anonymous';\n     }\n\n     getUserId() {\n       return localStorage.getItem('userId') || 'anonymous';\n     }\n   }\n\n   // Initialize RUM\n   new RealUserMonitoring();\n   ```\n\n   **React Performance Monitoring:**\n   ```javascript\n   // react-performance.js\n   import { Profiler } from 'react';\n\n   class ReactPerformanceMonitor {\n     static ProfilerWrapper = ({ id, children }) => {\n       const onRenderCallback = (id, phase, actualDuration, baseDuration, startTime, commitTime) => {\n         // Track component render performance\n         if (actualDuration > 100) { // Renders taking >100ms\n           console.warn(`Slow render detected for ${id}:`, {\n             phase,\n             actualDuration,\n             baseDuration,\n             startTime,\n             commitTime\n           });\n\n           // Send to monitoring service\n           fetch('/api/metrics/react-performance', {\n             method: 'POST',\n             headers: { 'Content-Type': 'application/json' },\n             body: JSON.stringify({\n               componentId: id,\n               phase,\n               actualDuration,\n               baseDuration,\n               timestamp: Date.now()\n             })\n           });\n         }\n       };\n\n       return (\n         <Profiler id={id} onRender={onRenderCallback}>\n           {children}\n         </Profiler>\n       );\n     };\n\n     static usePerformanceTracking(componentName) {\n       useEffect(() => {\n         const startTime = performance.now();\n         \n         return () => {\n           const duration = performance.now() - startTime;\n           if (duration > 1000) { // Component mounted for >1s\n             console.log(`${componentName} lifecycle duration:`, duration);\n           }\n         };\n       }, [componentName]);\n     }\n   }\n\n   // Usage\n   function App() {\n     return (\n       <ReactPerformanceMonitor.ProfilerWrapper id=\"App\">\n         <Dashboard />\n         <UserList />\n       </ReactPerformanceMonitor.ProfilerWrapper>\n     );\n   }\n   ```\n\n4. **Server Performance Monitoring**\n   - Monitor server-side performance metrics:\n\n   **System Metrics Collection:**\n   ```javascript\n   // system-monitor.js\n   const os = require('os');\n   const process = require('process');\n   const v8 = require('v8');\n\n   class SystemMonitor {\n     constructor() {\n       this.startTime = Date.now();\n       this.intervalId = null;\n     }\n\n     start(interval = 30000) { // 30 seconds\n       this.intervalId = setInterval(() => {\n         this.collectMetrics();\n       }, interval);\n     }\n\n     stop() {\n       if (this.intervalId) {\n         clearInterval(this.intervalId);\n       }\n     }\n\n     collectMetrics() {\n       const metrics = {\n         // CPU metrics\n         cpuUsage: process.cpuUsage(),\n         loadAverage: os.loadavg(),\n         \n         // Memory metrics\n         memoryUsage: process.memoryUsage(),\n         totalMemory: os.totalmem(),\n         freeMemory: os.freemem(),\n         \n         // V8 heap statistics\n         heapStats: v8.getHeapStatistics(),\n         heapSpaceStats: v8.getHeapSpaceStatistics(),\n         \n         // Process metrics\n         uptime: process.uptime(),\n         pid: process.pid,\n         \n         // Event loop lag\n         eventLoopLag: this.measureEventLoopLag(),\n         \n         timestamp: Date.now()\n       };\n\n       this.sendMetrics(metrics);\n     }\n\n     measureEventLoopLag() {\n       const start = process.hrtime.bigint();\n       setImmediate(() => {\n         const lag = Number(process.hrtime.bigint() - start) / 1000000; // Convert to ms\n         return lag;\n       });\n     }\n\n     sendMetrics(metrics) {\n       // Send to monitoring service\n       console.log('System Metrics:', JSON.stringify(metrics, null, 2));\n       \n       // Example: Send to StatsD\n       // statsd.gauge('system.memory.used', metrics.memoryUsage.used);\n       // statsd.gauge('system.cpu.usage', metrics.cpuUsage.system);\n     }\n   }\n\n   // Start monitoring\n   const monitor = new SystemMonitor();\n   monitor.start();\n\n   // Graceful shutdown\n   process.on('SIGTERM', () => {\n     monitor.stop();\n     process.exit(0);\n   });\n   ```\n\n   **Express.js Performance Middleware:**\n   ```javascript\n   // performance-middleware.js\n   const responseTime = require('response-time');\n   const promClient = require('prom-client');\n\n   // Prometheus metrics\n   const httpRequestDuration = new promClient.Histogram({\n     name: 'http_request_duration_seconds',\n     help: 'Duration of HTTP requests in seconds',\n     labelNames: ['method', 'route', 'status_code'],\n     buckets: [0.1, 0.3, 0.5, 0.7, 1, 3, 5, 7, 10]\n   });\n\n   const httpRequestsTotal = new promClient.Counter({\n     name: 'http_requests_total',\n     help: 'Total number of HTTP requests',\n     labelNames: ['method', 'route', 'status_code']\n   });\n\n   function performanceMiddleware() {\n     return (req, res, next) => {\n       const startTime = Date.now();\n       const startHrTime = process.hrtime();\n\n       res.on('finish', () => {\n         const duration = Date.now() - startTime;\n         const hrDuration = process.hrtime(startHrTime);\n         const durationSeconds = hrDuration[0] + hrDuration[1] / 1e9;\n\n         const labels = {\n           method: req.method,\n           route: req.route?.path || req.path,\n           status_code: res.statusCode\n         };\n\n         // Record Prometheus metrics\n         httpRequestDuration.observe(labels, durationSeconds);\n         httpRequestsTotal.inc(labels);\n\n         // Log slow requests\n         if (duration > 1000) {\n           console.warn('Slow request detected:', {\n             method: req.method,\n             url: req.url,\n             duration: duration,\n             statusCode: res.statusCode,\n             userAgent: req.get('User-Agent'),\n             ip: req.ip\n           });\n         }\n\n         // Track custom metrics\n         req.performanceMetrics = {\n           duration,\n           memoryUsage: process.memoryUsage(),\n           cpuUsage: process.cpuUsage()\n         };\n       });\n\n       next();\n     };\n   }\n\n   module.exports = { performanceMiddleware, httpRequestDuration, httpRequestsTotal };\n   ```\n\n5. **Database Performance Monitoring**\n   - Monitor database query performance:\n\n   **Query Performance Tracking:**\n   ```javascript\n   // db-performance.js\n   const { Pool } = require('pg');\n\n   class DatabasePerformanceMonitor {\n     constructor(pool) {\n       this.pool = pool;\n       this.slowQueryThreshold = 1000; // 1 second\n       this.queryStats = new Map();\n     }\n\n     async executeQuery(query, params = []) {\n       const queryId = this.generateQueryId(query);\n       const startTime = Date.now();\n       const startMemory = process.memoryUsage();\n\n       try {\n         const result = await this.pool.query(query, params);\n         const duration = Date.now() - startTime;\n         const endMemory = process.memoryUsage();\n\n         this.recordQueryMetrics(queryId, query, duration, true, endMemory.heapUsed - startMemory.heapUsed);\n\n         if (duration > this.slowQueryThreshold) {\n           this.logSlowQuery(query, params, duration);\n         }\n\n         return result;\n       } catch (error) {\n         const duration = Date.now() - startTime;\n         this.recordQueryMetrics(queryId, query, duration, false, 0);\n         throw error;\n       }\n     }\n\n     generateQueryId(query) {\n       // Normalize query for grouping similar queries\n       return query\n         .replace(/\\$\\d+/g, '?') // Replace parameter placeholders\n         .replace(/\\s+/g, ' ')   // Normalize whitespace\n         .replace(/\\d+/g, 'N')   // Replace numbers with 'N'\n         .trim()\n         .toLowerCase();\n     }\n\n     recordQueryMetrics(queryId, query, duration, success, memoryDelta) {\n       if (!this.queryStats.has(queryId)) {\n         this.queryStats.set(queryId, {\n           query: query,\n           count: 0,\n           totalDuration: 0,\n           successCount: 0,\n           errorCount: 0,\n           averageDuration: 0,\n           maxDuration: 0,\n           minDuration: Infinity\n         });\n       }\n\n       const stats = this.queryStats.get(queryId);\n       stats.count++;\n       stats.totalDuration += duration;\n       stats.averageDuration = stats.totalDuration / stats.count;\n       stats.maxDuration = Math.max(stats.maxDuration, duration);\n       stats.minDuration = Math.min(stats.minDuration, duration);\n\n       if (success) {\n         stats.successCount++;\n       } else {\n         stats.errorCount++;\n       }\n\n       // Send metrics to monitoring service\n       this.sendQueryMetrics(queryId, duration, success, memoryDelta);\n     }\n\n     logSlowQuery(query, params, duration) {\n       console.warn('Slow query detected:', {\n         query: query,\n         params: params,\n         duration: duration,\n         timestamp: new Date().toISOString()\n       });\n\n       // Send alert to monitoring service\n       this.sendSlowQueryAlert(query, params, duration);\n     }\n\n     sendQueryMetrics(queryId, duration, success, memoryDelta) {\n       const metrics = {\n         queryId,\n         duration,\n         success,\n         memoryDelta,\n         timestamp: Date.now()\n       };\n\n       // Send to your monitoring service\n       // Example: StatsD, Prometheus, DataDog, etc.\n     }\n\n     sendSlowQueryAlert(query, params, duration) {\n       // Send to alerting system\n       console.log('Sending slow query alert...', { query, duration });\n     }\n\n     getQueryStats() {\n       return Array.from(this.queryStats.entries()).map(([queryId, stats]) => ({\n         queryId,\n         ...stats\n       }));\n     }\n\n     resetStats() {\n       this.queryStats.clear();\n     }\n   }\n\n   // Usage\n   const pool = new Pool();\n   const dbMonitor = new DatabasePerformanceMonitor(pool);\n\n   // Replace direct pool usage with monitored version\n   module.exports = { executeQuery: dbMonitor.executeQuery.bind(dbMonitor) };\n   ```\n\n6. **Error Tracking and Monitoring**\n   - Implement comprehensive error monitoring:\n\n   **Error Tracking Setup:**\n   ```javascript\n   // error-monitor.js\n   const Sentry = require('@sentry/node');\n   const Integrations = require('@sentry/integrations');\n\n   class ErrorMonitor {\n     static initialize() {\n       Sentry.init({\n         dsn: process.env.SENTRY_DSN,\n         environment: process.env.NODE_ENV,\n         integrations: [\n           new Integrations.Http({ tracing: true }),\n           new Sentry.Integrations.Express({ app }),\n         ],\n         tracesSampleRate: process.env.NODE_ENV === 'production' ? 0.1 : 1.0,\n         beforeSend(event, hint) {\n           // Filter out noise\n           if (event.exception) {\n             const error = hint.originalException;\n             if (error && error.code === 'ECONNABORTED') {\n               return null; // Don't send timeout errors\n             }\n           }\n           return event;\n         },\n         beforeBreadcrumb(breadcrumb) {\n           // Filter sensitive data from breadcrumbs\n           if (breadcrumb.category === 'http') {\n             delete breadcrumb.data?.password;\n             delete breadcrumb.data?.token;\n           }\n           return breadcrumb;\n         }\n       });\n     }\n\n     static captureException(error, context = {}) {\n       Sentry.withScope((scope) => {\n         Object.keys(context).forEach(key => {\n           scope.setContext(key, context[key]);\n         });\n         Sentry.captureException(error);\n       });\n     }\n\n     static captureMessage(message, level = 'info', context = {}) {\n       Sentry.withScope((scope) => {\n         Object.keys(context).forEach(key => {\n           scope.setContext(key, context[key]);\n         });\n         Sentry.captureMessage(message, level);\n       });\n     }\n\n     static setupExpressErrorHandling(app) {\n       // Sentry request handler (must be first)\n       app.use(Sentry.Handlers.requestHandler());\n       app.use(Sentry.Handlers.tracingHandler());\n\n       // Your routes here\n\n       // Sentry error handler (must be before other error handlers)\n       app.use(Sentry.Handlers.errorHandler());\n\n       // Custom error handler\n       app.use((error, req, res, next) => {\n         const errorId = res.sentry;\n         \n         console.error('Unhandled error:', {\n           errorId,\n           error: error.message,\n           stack: error.stack,\n           url: req.url,\n           method: req.method,\n           userAgent: req.get('User-Agent'),\n           ip: req.ip\n         });\n\n         res.status(500).json({\n           error: 'Internal server error',\n           errorId: errorId\n         });\n       });\n     }\n   }\n\n   // Global error handlers\n   process.on('uncaughtException', (error) => {\n     console.error('Uncaught Exception:', error);\n     ErrorMonitor.captureException(error, { type: 'uncaughtException' });\n     process.exit(1);\n   });\n\n   process.on('unhandledRejection', (reason, promise) => {\n     console.error('Unhandled Rejection at:', promise, 'reason:', reason);\n     ErrorMonitor.captureException(new Error(reason), { type: 'unhandledRejection' });\n   });\n   ```\n\n7. **Custom Metrics and Dashboards**\n   - Create custom performance dashboards:\n\n   **Prometheus Metrics:**\n   ```javascript\n   // prometheus-metrics.js\n   const promClient = require('prom-client');\n\n   class CustomMetrics {\n     constructor() {\n       // Register default metrics\n       promClient.register.setDefaultLabels({\n         app: process.env.APP_NAME || 'my-app',\n         version: process.env.APP_VERSION || '1.0.0'\n       });\n       promClient.collectDefaultMetrics();\n\n       this.setupCustomMetrics();\n     }\n\n     setupCustomMetrics() {\n       // Business metrics\n       this.userRegistrations = new promClient.Counter({\n         name: 'user_registrations_total',\n         help: 'Total number of user registrations',\n         labelNames: ['source', 'plan']\n       });\n\n       this.orderValue = new promClient.Histogram({\n         name: 'order_value_dollars',\n         help: 'Order value in dollars',\n         labelNames: ['currency', 'payment_method'],\n         buckets: [10, 50, 100, 500, 1000, 5000]\n       });\n\n       this.cacheHitRate = new promClient.Gauge({\n         name: 'cache_hit_rate',\n         help: 'Cache hit rate percentage',\n         labelNames: ['cache_type']\n       });\n\n       this.activeUsers = new promClient.Gauge({\n         name: 'active_users_current',\n         help: 'Currently active users',\n         labelNames: ['session_type']\n       });\n\n       // Performance metrics\n       this.databaseConnectionPool = new promClient.Gauge({\n         name: 'database_connections_active',\n         help: 'Active database connections',\n         labelNames: ['pool_name']\n       });\n\n       this.apiResponseTime = new promClient.Histogram({\n         name: 'api_response_time_seconds',\n         help: 'API response time in seconds',\n         labelNames: ['endpoint', 'method', 'status'],\n         buckets: [0.1, 0.5, 1, 2, 5, 10]\n       });\n     }\n\n     // Helper methods\n     recordUserRegistration(source, plan) {\n       this.userRegistrations.inc({ source, plan });\n     }\n\n     recordOrderValue(value, currency, paymentMethod) {\n       this.orderValue.observe({ currency, payment_method: paymentMethod }, value);\n     }\n\n     updateCacheHitRate(cacheType, hitRate) {\n       this.cacheHitRate.set({ cache_type: cacheType }, hitRate);\n     }\n\n     setActiveUsers(count, sessionType = 'web') {\n       this.activeUsers.set({ session_type: sessionType }, count);\n     }\n\n     getMetrics() {\n       return promClient.register.metrics();\n     }\n   }\n\n   const metrics = new CustomMetrics();\n\n   // Metrics endpoint\n   app.get('/metrics', async (req, res) => {\n     res.set('Content-Type', promClient.register.contentType);\n     res.end(await metrics.getMetrics());\n   });\n\n   module.exports = metrics;\n   ```\n\n8. **Alerting and Notification System**\n   - Set up intelligent alerting:\n\n   **Alert Manager:**\n   ```javascript\n   // alert-manager.js\n   const nodemailer = require('nodemailer');\n   const slack = require('@slack/webhook');\n\n   class AlertManager {\n     constructor() {\n       this.emailTransporter = nodemailer.createTransporter({\n         // Email configuration\n       });\n       \n       this.slackWebhook = new slack.IncomingWebhook(process.env.SLACK_WEBHOOK_URL);\n       \n       this.alertThresholds = {\n         responseTime: 2000, // 2 seconds\n         errorRate: 0.05,    // 5%\n         cpuUsage: 0.8,      // 80%\n         memoryUsage: 0.9,   // 90%\n         diskUsage: 0.85     // 85%\n       };\n       \n       this.alertCooldowns = new Map();\n     }\n\n     async checkPerformanceThresholds(metrics) {\n       const alerts = [];\n\n       // Response time alert\n       if (metrics.averageResponseTime > this.alertThresholds.responseTime) {\n         alerts.push({\n           severity: 'warning',\n           metric: 'response_time',\n           current: metrics.averageResponseTime,\n           threshold: this.alertThresholds.responseTime,\n           message: `Average response time is ${metrics.averageResponseTime}ms (threshold: ${this.alertThresholds.responseTime}ms)`\n         });\n       }\n\n       // Error rate alert\n       if (metrics.errorRate > this.alertThresholds.errorRate) {\n         alerts.push({\n           severity: 'critical',\n           metric: 'error_rate',\n           current: metrics.errorRate,\n           threshold: this.alertThresholds.errorRate,\n           message: `Error rate is ${(metrics.errorRate * 100).toFixed(2)}% (threshold: ${(this.alertThresholds.errorRate * 100)}%)`\n         });\n       }\n\n       // System resource alerts\n       if (metrics.cpuUsage > this.alertThresholds.cpuUsage) {\n         alerts.push({\n           severity: 'warning',\n           metric: 'cpu_usage',\n           current: metrics.cpuUsage,\n           threshold: this.alertThresholds.cpuUsage,\n           message: `CPU usage is ${(metrics.cpuUsage * 100).toFixed(1)}% (threshold: ${(this.alertThresholds.cpuUsage * 100)}%)`\n         });\n       }\n\n       // Send alerts\n       for (const alert of alerts) {\n         await this.sendAlert(alert);\n       }\n     }\n\n     async sendAlert(alert) {\n       const alertKey = `${alert.metric}_${alert.severity}`;\n       const now = Date.now();\n       const cooldownPeriod = alert.severity === 'critical' ? 300000 : 900000; // 5min for critical, 15min for others\n\n       // Check cooldown\n       if (this.alertCooldowns.has(alertKey)) {\n         const lastAlert = this.alertCooldowns.get(alertKey);\n         if (now - lastAlert < cooldownPeriod) {\n           return; // Skip this alert due to cooldown\n         }\n       }\n\n       this.alertCooldowns.set(alertKey, now);\n\n       // Send to multiple channels\n       await Promise.all([\n         this.sendSlackAlert(alert),\n         this.sendEmailAlert(alert),\n         this.logAlert(alert)\n       ]);\n     }\n\n     async sendSlackAlert(alert) {\n       const color = alert.severity === 'critical' ? 'danger' : 'warning';\n       const emoji = alert.severity === 'critical' ? ':rotating_light:' : ':warning:';\n       \n       await this.slackWebhook.send({\n         text: `${emoji} Performance Alert`,\n         attachments: [{\n           color: color,\n           fields: [\n             { title: 'Metric', value: alert.metric, short: true },\n             { title: 'Severity', value: alert.severity, short: true },\n             { title: 'Current Value', value: alert.current.toString(), short: true },\n             { title: 'Threshold', value: alert.threshold.toString(), short: true },\n             { title: 'Message', value: alert.message, short: false }\n           ],\n           ts: Math.floor(Date.now() / 1000)\n         }]\n       });\n     }\n\n     async sendEmailAlert(alert) {\n       if (alert.severity === 'critical') {\n         await this.emailTransporter.sendMail({\n           to: process.env.ALERT_EMAIL,\n           subject: `CRITICAL: ${alert.metric} alert`,\n           html: `\n             <h2>Performance Alert</h2>\n             <p><strong>Severity:</strong> ${alert.severity}</p>\n             <p><strong>Metric:</strong> ${alert.metric}</p>\n             <p><strong>Message:</strong> ${alert.message}</p>\n             <p><strong>Current Value:</strong> ${alert.current}</p>\n             <p><strong>Threshold:</strong> ${alert.threshold}</p>\n             <p><strong>Time:</strong> ${new Date().toISOString()}</p>\n           `\n         });\n       }\n     }\n\n     logAlert(alert) {\n       console.error('PERFORMANCE ALERT:', {\n         timestamp: new Date().toISOString(),\n         severity: alert.severity,\n         metric: alert.metric,\n         current: alert.current,\n         threshold: alert.threshold,\n         message: alert.message\n       });\n     }\n   }\n\n   module.exports = AlertManager;\n   ```\n\n9. **Performance Testing Integration**\n   - Integrate with performance testing:\n\n   **Load Test Monitoring:**\n   ```javascript\n   // load-test-monitor.js\n   class LoadTestMonitor {\n     constructor() {\n       this.testResults = [];\n       this.baselineMetrics = null;\n     }\n\n     async runPerformanceTest(testConfig) {\n       console.log('Starting performance test...', testConfig);\n       \n       const startMetrics = await this.captureSystemMetrics();\n       const startTime = Date.now();\n\n       try {\n         // Run the actual load test (using k6, artillery, etc.)\n         const testResults = await this.executeLoadTest(testConfig);\n         \n         const endTime = Date.now();\n         const endMetrics = await this.captureSystemMetrics();\n\n         const result = {\n           testId: this.generateTestId(),\n           config: testConfig,\n           duration: endTime - startTime,\n           startMetrics,\n           endMetrics,\n           testResults,\n           timestamp: new Date().toISOString()\n         };\n\n         this.testResults.push(result);\n         await this.analyzeResults(result);\n         \n         return result;\n       } catch (error) {\n         console.error('Load test failed:', error);\n         throw error;\n       }\n     }\n\n     async captureSystemMetrics() {\n       return {\n         cpu: os.loadavg(),\n         memory: {\n           total: os.totalmem(),\n           free: os.freemem(),\n           used: os.totalmem() - os.freemem()\n         },\n         processes: await this.getProcessMetrics()\n       };\n     }\n\n     async analyzeResults(result) {\n       const analysis = {\n         performanceRegression: false,\n         recommendations: []\n       };\n\n       // Compare with baseline\n       if (this.baselineMetrics) {\n         const responseTimeIncrease = (result.testResults.averageResponseTime - this.baselineMetrics.averageResponseTime) / this.baselineMetrics.averageResponseTime;\n         \n         if (responseTimeIncrease > 0.2) { // 20% increase\n           analysis.performanceRegression = true;\n           analysis.recommendations.push(`Response time increased by ${(responseTimeIncrease * 100).toFixed(1)}%`);\n         }\n       }\n\n       // Resource utilization analysis\n       const maxCpuUsage = Math.max(...result.endMetrics.cpu);\n       if (maxCpuUsage > 0.8) {\n         analysis.recommendations.push('High CPU usage detected - consider scaling');\n       }\n\n       const memoryUsagePercent = result.endMetrics.memory.used / result.endMetrics.memory.total;\n       if (memoryUsagePercent > 0.9) {\n         analysis.recommendations.push('High memory usage detected - check for memory leaks');\n       }\n\n       console.log('Performance test analysis:', analysis);\n       return analysis;\n     }\n\n     setBaseline(testResult) {\n       this.baselineMetrics = testResult.testResults;\n       console.log('Baseline metrics set:', this.baselineMetrics);\n     }\n\n     generateTestId() {\n       return `test_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n     }\n   }\n   ```\n\n10. **Performance Optimization Recommendations**\n    - Generate actionable performance insights:\n\n    **Performance Analyzer:**\n    ```javascript\n    // performance-analyzer.js\n    class PerformanceAnalyzer {\n      constructor() {\n        this.metrics = [];\n        this.thresholds = {\n          responseTime: { good: 200, warning: 1000, critical: 3000 },\n          memoryUsage: { good: 0.6, warning: 0.8, critical: 0.9 },\n          cpuUsage: { good: 0.5, warning: 0.7, critical: 0.85 },\n          errorRate: { good: 0.01, warning: 0.05, critical: 0.1 }\n        };\n      }\n\n      analyzePerformance(metrics) {\n        const recommendations = [];\n        const scores = {};\n\n        // Analyze response time\n        if (metrics.averageResponseTime > this.thresholds.responseTime.critical) {\n          recommendations.push({\n            priority: 'high',\n            category: 'response_time',\n            issue: 'Very slow response times detected',\n            recommendations: [\n              'Implement database query optimization',\n              'Add caching layer (Redis/Memcached)',\n              'Enable CDN for static assets',\n              'Consider horizontal scaling'\n            ],\n            impact: 'Critical user experience impact'\n          });\n          scores.responseTime = 1;\n        } else if (metrics.averageResponseTime > this.thresholds.responseTime.warning) {\n          recommendations.push({\n            priority: 'medium',\n            category: 'response_time',\n            issue: 'Moderate response time issues',\n            recommendations: [\n              'Optimize database queries',\n              'Implement query result caching',\n              'Review N+1 query patterns'\n            ],\n            impact: 'Moderate user experience impact'\n          });\n          scores.responseTime = 6;\n        } else {\n          scores.responseTime = 10;\n        }\n\n        // Analyze memory usage\n        if (metrics.memoryUsage > this.thresholds.memoryUsage.critical) {\n          recommendations.push({\n            priority: 'high',\n            category: 'memory',\n            issue: 'Critical memory usage',\n            recommendations: [\n              'Check for memory leaks',\n              'Implement garbage collection tuning',\n              'Add more memory or scale horizontally',\n              'Review large object allocations'\n            ],\n            impact: 'Risk of application crashes'\n          });\n          scores.memory = 2;\n        }\n\n        // Analyze error rate\n        if (metrics.errorRate > this.thresholds.errorRate.critical) {\n          recommendations.push({\n            priority: 'high',\n            category: 'reliability',\n            issue: 'High error rate detected',\n            recommendations: [\n              'Review application logs for error patterns',\n              'Implement circuit breakers',\n              'Add retry mechanisms',\n              'Improve error handling'\n            ],\n            impact: 'Significant functionality issues'\n          });\n          scores.reliability = 3;\n        }\n\n        const overallScore = Object.values(scores).reduce((a, b) => a + b, 0) / Object.keys(scores).length;\n\n        return {\n          overallScore: Math.round(overallScore),\n          grade: this.getPerformanceGrade(overallScore),\n          recommendations: recommendations.sort((a, b) => {\n            const priorityOrder = { high: 3, medium: 2, low: 1 };\n            return priorityOrder[b.priority] - priorityOrder[a.priority];\n          }),\n          metrics,\n          timestamp: new Date().toISOString()\n        };\n      }\n\n      getPerformanceGrade(score) {\n        if (score >= 9) return 'A';\n        if (score >= 8) return 'B';\n        if (score >= 7) return 'C';\n        if (score >= 6) return 'D';\n        return 'F';\n      }\n\n      generateReport(analysis) {\n        return {\n          summary: {\n            grade: analysis.grade,\n            score: analysis.overallScore,\n            criticalIssues: analysis.recommendations.filter(r => r.priority === 'high').length,\n            totalRecommendations: analysis.recommendations.length\n          },\n          keyMetrics: {\n            responseTime: analysis.metrics.averageResponseTime,\n            errorRate: (analysis.metrics.errorRate * 100).toFixed(2) + '%',\n            memoryUsage: (analysis.metrics.memoryUsage * 100).toFixed(1) + '%',\n            cpuUsage: (analysis.metrics.cpuUsage * 100).toFixed(1) + '%'\n          },\n          recommendations: analysis.recommendations,\n          generatedAt: analysis.timestamp\n        };\n      }\n    }\n\n    module.exports = PerformanceAnalyzer;\n    ```"
              },
              {
                "name": "/setup-monitoring-observability",
                "description": "Setup monitoring and observability tools",
                "path": "plugins/commands-monitoring-observability/commands/setup-monitoring-observability.md",
                "frontmatter": {
                  "description": "Setup monitoring and observability tools",
                  "category": "monitoring-observability"
                },
                "content": "# Setup Monitoring and Observability\n\nSetup monitoring and observability tools\n\n## Instructions\n\n1. **Observability Strategy Planning**\n   - Analyze application architecture and monitoring requirements\n   - Define key performance indicators (KPIs) and service level objectives (SLOs)\n   - Plan monitoring stack architecture and data flow\n   - Assess compliance and retention requirements\n   - Define alerting strategies and escalation procedures\n\n2. **Metrics Collection and Monitoring**\n   - Set up application metrics collection (Prometheus, DataDog, New Relic)\n   - Configure infrastructure monitoring for servers, containers, and cloud resources\n   - Set up business metrics and user experience monitoring\n   - Configure custom metrics for application-specific monitoring\n   - Set up metrics aggregation and time-series storage\n\n3. **Logging Infrastructure**\n   - Set up centralized logging system (ELK Stack, Fluentd, Splunk)\n   - Configure structured logging with consistent formats\n   - Set up log aggregation and forwarding from all services\n   - Configure log retention policies and archival strategies\n   - Set up log parsing, enrichment, and indexing\n\n4. **Distributed Tracing**\n   - Set up distributed tracing system (Jaeger, Zipkin, AWS X-Ray)\n   - Configure trace instrumentation in application code\n   - Set up trace sampling and collection strategies\n   - Configure trace correlation across service boundaries\n   - Set up trace analysis and performance optimization\n\n5. **Application Performance Monitoring (APM)**\n   - Configure APM tools for application performance insights\n   - Set up error tracking and exception monitoring\n   - Configure database query monitoring and optimization\n   - Set up real user monitoring (RUM) and synthetic monitoring\n   - Configure performance profiling and bottleneck identification\n\n6. **Infrastructure and System Monitoring**\n   - Set up server and container monitoring (CPU, memory, disk, network)\n   - Configure cloud service monitoring and cost tracking\n   - Set up database monitoring and performance analysis\n   - Configure network monitoring and security scanning\n   - Set up capacity planning and resource optimization\n\n7. **Alerting and Notification System**\n   - Configure intelligent alerting with proper thresholds\n   - Set up alert routing and escalation procedures\n   - Configure notification channels (email, Slack, PagerDuty)\n   - Set up alert correlation and noise reduction\n   - Configure on-call scheduling and incident management\n\n8. **Dashboards and Visualization**\n   - Create comprehensive monitoring dashboards (Grafana, Kibana)\n   - Set up real-time system health dashboards\n   - Configure business metrics and KPI visualization\n   - Create role-specific dashboards for different teams\n   - Set up mobile-friendly monitoring interfaces\n\n9. **Security Monitoring and Compliance**\n   - Set up security event monitoring and SIEM integration\n   - Configure compliance monitoring and audit trails\n   - Set up vulnerability scanning and security alerting\n   - Configure access monitoring and user behavior analytics\n   - Set up data privacy and protection monitoring\n\n10. **Incident Response and Automation**\n    - Set up automated incident detection and response\n    - Configure runbook automation and self-healing systems\n    - Set up incident management and communication workflows\n    - Configure post-incident analysis and improvement processes\n    - Create monitoring maintenance and optimization procedures\n    - Train team on monitoring tools and incident response procedures"
              }
            ],
            "skills": []
          },
          {
            "name": "commands-performance-optimization",
            "description": "Commands for optimizing build, bundle size, and performance",
            "source": "./plugins/commands-performance-optimization",
            "category": "commands",
            "version": "1.0.0",
            "author": {
              "name": "BuildWithClaude Community",
              "url": "https://github.com/davepoon/buildwithclaude"
            },
            "install_commands": [
              "/plugin marketplace add davepoon/buildwithclaude",
              "/plugin install commands-performance-optimization@buildwithclaude"
            ],
            "signals": {
              "stars": 2229,
              "forks": 239,
              "pushed_at": "2026-01-12T10:19:01Z",
              "created_at": "2025-07-25T02:26:45Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/implement-caching-strategy",
                "description": "Design and implement caching solutions",
                "path": "plugins/commands-performance-optimization/commands/implement-caching-strategy.md",
                "frontmatter": {
                  "description": "Design and implement caching solutions",
                  "category": "performance-optimization"
                },
                "content": "# Implement Caching Strategy\n\nDesign and implement caching solutions\n\n## Instructions\n\n1. **Caching Strategy Analysis**\n   - Analyze application architecture and identify caching opportunities\n   - Assess current performance bottlenecks and data access patterns\n   - Define caching requirements (TTL, invalidation, consistency)\n   - Plan multi-layer caching architecture (browser, CDN, application, database)\n   - Evaluate caching technologies and storage solutions\n\n2. **Browser and Client-Side Caching**\n   - Configure HTTP caching headers and cache policies:\n\n   **HTTP Cache Headers:**\n   ```javascript\n   // Express.js middleware\n   app.use((req, res, next) => {\n     // Static assets with long-term caching\n     if (req.url.match(/\\.(js|css|png|jpg|jpeg|gif|ico|svg)$/)) {\n       res.setHeader('Cache-Control', 'public, max-age=31536000'); // 1 year\n       res.setHeader('ETag', generateETag(req.url));\n     }\n     \n     // API responses with short-term caching\n     if (req.url.startsWith('/api/')) {\n       res.setHeader('Cache-Control', 'public, max-age=300'); // 5 minutes\n     }\n     \n     next();\n   });\n   ```\n\n   **Service Worker Caching:**\n   ```javascript\n   // sw.js - Service Worker\n   const CACHE_NAME = 'app-cache-v1';\n   const urlsToCache = [\n     '/',\n     '/static/js/bundle.js',\n     '/static/css/main.css',\n   ];\n\n   self.addEventListener('install', (event) => {\n     event.waitUntil(\n       caches.open(CACHE_NAME)\n         .then((cache) => cache.addAll(urlsToCache))\n     );\n   });\n\n   self.addEventListener('fetch', (event) => {\n     event.respondWith(\n       caches.match(event.request)\n         .then((response) => {\n           // Return cached version or fetch from network\n           return response || fetch(event.request);\n         })\n     );\n   });\n   ```\n\n3. **Application-Level Caching**\n   - Implement in-memory and distributed caching:\n\n   **Node.js Memory Cache:**\n   ```javascript\n   const NodeCache = require('node-cache');\n   const cache = new NodeCache({ stdTTL: 600 }); // 10 minutes default TTL\n\n   class CacheService {\n     static get(key) {\n       return cache.get(key);\n     }\n\n     static set(key, value, ttl = 600) {\n       return cache.set(key, value, ttl);\n     }\n\n     static del(key) {\n       return cache.del(key);\n     }\n\n     static flush() {\n       return cache.flushAll();\n     }\n\n     // Cache wrapper for expensive operations\n     static async memoize(key, fn, ttl = 600) {\n       let result = this.get(key);\n       if (result === undefined) {\n         result = await fn();\n         this.set(key, result, ttl);\n       }\n       return result;\n     }\n   }\n\n   // Usage example\n   app.get('/api/users/:id', async (req, res) => {\n     const userId = req.params.id;\n     const cacheKey = `user:${userId}`;\n     \n     const user = await CacheService.memoize(\n       cacheKey,\n       () => getUserFromDatabase(userId),\n       900 // 15 minutes\n     );\n     \n     res.json(user);\n   });\n   ```\n\n   **Redis Distributed Cache:**\n   ```javascript\n   const redis = require('redis');\n   const client = redis.createClient({\n     host: process.env.REDIS_HOST || 'localhost',\n     port: process.env.REDIS_PORT || 6379,\n   });\n\n   class RedisCache {\n     static async get(key) {\n       try {\n         const value = await client.get(key);\n         return value ? JSON.parse(value) : null;\n       } catch (error) {\n         console.error('Cache get error:', error);\n         return null;\n       }\n     }\n\n     static async set(key, value, ttl = 600) {\n       try {\n         const serialized = JSON.stringify(value);\n         if (ttl) {\n           await client.setex(key, ttl, serialized);\n         } else {\n           await client.set(key, serialized);\n         }\n         return true;\n       } catch (error) {\n         console.error('Cache set error:', error);\n         return false;\n       }\n     }\n\n     static async del(key) {\n       try {\n         await client.del(key);\n         return true;\n       } catch (error) {\n         console.error('Cache delete error:', error);\n         return false;\n       }\n     }\n\n     // Pattern-based cache invalidation\n     static async invalidatePattern(pattern) {\n       try {\n         const keys = await client.keys(pattern);\n         if (keys.length > 0) {\n           await client.del(keys);\n         }\n         return true;\n       } catch (error) {\n         console.error('Cache invalidation error:', error);\n         return false;\n       }\n     }\n   }\n   ```\n\n4. **Database Query Caching**\n   - Implement database-level caching strategies:\n\n   **PostgreSQL Query Caching:**\n   ```javascript\n   const { Pool } = require('pg');\n   const pool = new Pool();\n\n   class DatabaseCache {\n     static async cachedQuery(sql, params = [], ttl = 300) {\n       const cacheKey = `query:${Buffer.from(sql + JSON.stringify(params)).toString('base64')}`;\n       \n       // Try cache first\n       let result = await RedisCache.get(cacheKey);\n       if (result) {\n         return result;\n       }\n       \n       // Execute query and cache result\n       const dbResult = await pool.query(sql, params);\n       result = dbResult.rows;\n       \n       await RedisCache.set(cacheKey, result, ttl);\n       return result;\n     }\n\n     // Invalidate cache by table\n     static async invalidateTable(tableName) {\n       await RedisCache.invalidatePattern(`query:*${tableName}*`);\n     }\n   }\n\n   // Usage\n   app.get('/api/products', async (req, res) => {\n     const products = await DatabaseCache.cachedQuery(\n       'SELECT * FROM products WHERE active = true ORDER BY created_at DESC',\n       [],\n       600 // 10 minutes\n     );\n     res.json(products);\n   });\n   ```\n\n   **MongoDB Caching with Mongoose:**\n   ```javascript\n   const mongoose = require('mongoose');\n\n   // Mongoose query caching plugin\n   function cachePlugin(schema) {\n     schema.add({\n       cacheKey: { type: String, index: true },\n       cachedAt: { type: Date },\n     });\n\n     schema.methods.cache = function(ttl = 300) {\n       this.cacheKey = this.constructor.generateCacheKey(this);\n       this.cachedAt = new Date();\n       return this;\n     };\n\n     schema.statics.findCached = async function(query, ttl = 300) {\n       const cacheKey = this.generateCacheKey(query);\n       \n       let result = await RedisCache.get(cacheKey);\n       if (result) {\n         return result;\n       }\n       \n       result = await this.find(query);\n       await RedisCache.set(cacheKey, result, ttl);\n       return result;\n     };\n\n     schema.statics.generateCacheKey = function(data) {\n       return `${this.modelName}:${JSON.stringify(data)}`;\n     };\n   }\n\n   // Apply plugin to schema\n   const ProductSchema = new mongoose.Schema({\n     name: String,\n     price: Number,\n     category: String,\n   });\n\n   ProductSchema.plugin(cachePlugin);\n   ```\n\n5. **API Response Caching**\n   - Implement comprehensive API caching:\n\n   **Express Cache Middleware:**\n   ```javascript\n   function cacheMiddleware(ttl = 300) {\n     return async (req, res, next) => {\n       // Only cache GET requests\n       if (req.method !== 'GET') {\n         return next();\n       }\n\n       const cacheKey = `api:${req.originalUrl}`;\n       const cached = await RedisCache.get(cacheKey);\n\n       if (cached) {\n         return res.json(cached);\n       }\n\n       // Override res.json to cache the response\n       const originalJson = res.json;\n       res.json = function(data) {\n         RedisCache.set(cacheKey, data, ttl);\n         return originalJson.call(this, data);\n       };\n\n       next();\n     };\n   }\n\n   // Usage\n   app.get('/api/dashboard', cacheMiddleware(600), async (req, res) => {\n     const dashboardData = await getDashboardData();\n     res.json(dashboardData);\n   });\n   ```\n\n   **GraphQL Query Caching:**\n   ```javascript\n   const { ApolloServer } = require('apollo-server-express');\n   const { ResponseCache } = require('apollo-server-plugin-response-cache');\n\n   const server = new ApolloServer({\n     typeDefs,\n     resolvers,\n     plugins: [\n       ResponseCache({\n         sessionId: (requestContext) => \n           requestContext.request.http.headers.authorization || null,\n         maximumAge: 300, // 5 minutes default\n         scope: 'PUBLIC',\n       }),\n     ],\n     cacheControl: {\n       defaultMaxAge: 300,\n       calculateHttpHeaders: false,\n       stripFormattedExtensions: false,\n     },\n   });\n\n   // Resolver-level caching\n   const resolvers = {\n     Query: {\n       products: async (parent, args, context) => {\n         return await DatabaseCache.cachedQuery(\n           'SELECT * FROM products WHERE category = $1',\n           [args.category],\n           600\n         );\n       },\n     },\n   };\n   ```\n\n6. **Cache Invalidation Strategies**\n   - Implement intelligent cache invalidation:\n\n   **Event-Driven Cache Invalidation:**\n   ```javascript\n   const EventEmitter = require('events');\n   const cacheInvalidator = new EventEmitter();\n\n   class CacheInvalidator {\n     static invalidateUser(userId) {\n       const patterns = [\n         `user:${userId}*`,\n         `api:/api/users/${userId}*`,\n         'api:/api/dashboard*', // If dashboard shows user data\n       ];\n       \n       patterns.forEach(async (pattern) => {\n         await RedisCache.invalidatePattern(pattern);\n       });\n       \n       cacheInvalidator.emit('user:updated', userId);\n     }\n\n     static invalidateProduct(productId) {\n       const patterns = [\n         `product:${productId}*`,\n         'api:/api/products*',\n         'query:*products*',\n       ];\n       \n       patterns.forEach(async (pattern) => {\n         await RedisCache.invalidatePattern(pattern);\n       });\n     }\n   }\n\n   // Trigger invalidation on data changes\n   app.put('/api/users/:id', async (req, res) => {\n     const userId = req.params.id;\n     await updateUser(userId, req.body);\n     \n     // Invalidate related caches\n     CacheInvalidator.invalidateUser(userId);\n     \n     res.json({ success: true });\n   });\n   ```\n\n7. **Frontend Caching Strategies**\n   - Implement client-side caching:\n\n   **React Query Caching:**\n   ```javascript\n   import { QueryClient, QueryClientProvider, useQuery } from 'react-query';\n\n   const queryClient = new QueryClient({\n     defaultOptions: {\n       queries: {\n         staleTime: 5 * 60 * 1000, // 5 minutes\n         cacheTime: 10 * 60 * 1000, // 10 minutes\n         retry: 3,\n         refetchOnWindowFocus: false,\n       },\n     },\n   });\n\n   function ProductList() {\n     const { data: products, isLoading, error } = useQuery(\n       'products',\n       () => fetch('/api/products').then(res => res.json()),\n       {\n         staleTime: 10 * 60 * 1000, // 10 minutes\n         cacheTime: 30 * 60 * 1000, // 30 minutes\n       }\n     );\n\n     if (isLoading) return <div>Loading...</div>;\n     if (error) return <div>Error: {error.message}</div>;\n\n     return (\n       <div>\n         {products.map(product => (\n           <div key={product.id}>{product.name}</div>\n         ))}\n       </div>\n     );\n   }\n   ```\n\n   **Local Storage Caching:**\n   ```javascript\n   class LocalStorageCache {\n     static set(key, value, ttl = 3600000) { // 1 hour default\n       const item = {\n         value,\n         expiry: Date.now() + ttl,\n       };\n       localStorage.setItem(key, JSON.stringify(item));\n     }\n\n     static get(key) {\n       const item = localStorage.getItem(key);\n       if (!item) return null;\n\n       const parsed = JSON.parse(item);\n       if (Date.now() > parsed.expiry) {\n         localStorage.removeItem(key);\n         return null;\n       }\n\n       return parsed.value;\n     }\n\n     static remove(key) {\n       localStorage.removeItem(key);\n     }\n\n     static clear() {\n       localStorage.clear();\n     }\n   }\n   ```\n\n8. **Cache Monitoring and Analytics**\n   - Set up cache performance monitoring:\n\n   **Cache Metrics Collection:**\n   ```javascript\n   class CacheMetrics {\n     static hits = 0;\n     static misses = 0;\n     static errors = 0;\n\n     static recordHit() {\n       this.hits++;\n     }\n\n     static recordMiss() {\n       this.misses++;\n     }\n\n     static recordError() {\n       this.errors++;\n     }\n\n     static getStats() {\n       const total = this.hits + this.misses;\n       return {\n         hits: this.hits,\n         misses: this.misses,\n         errors: this.errors,\n         hitRate: total > 0 ? (this.hits / total * 100).toFixed(2) : 0,\n         total,\n       };\n     }\n\n     static reset() {\n       this.hits = 0;\n       this.misses = 0;\n       this.errors = 0;\n     }\n   }\n\n   // Enhanced cache service with metrics\n   class MetricsCache {\n     static async get(key) {\n       try {\n         const value = await RedisCache.get(key);\n         if (value !== null) {\n           CacheMetrics.recordHit();\n         } else {\n           CacheMetrics.recordMiss();\n         }\n         return value;\n       } catch (error) {\n         CacheMetrics.recordError();\n         throw error;\n       }\n     }\n   }\n\n   // Metrics endpoint\n   app.get('/api/cache/stats', (req, res) => {\n     res.json(CacheMetrics.getStats());\n   });\n   ```\n\n9. **Cache Warming and Preloading**\n   - Implement cache warming strategies:\n\n   **Scheduled Cache Warming:**\n   ```javascript\n   const cron = require('node-cron');\n\n   class CacheWarmer {\n     static async warmPopularData() {\n       console.log('Starting cache warming...');\n       \n       // Warm popular products\n       const popularProducts = await DatabaseCache.cachedQuery(\n         'SELECT * FROM products ORDER BY view_count DESC LIMIT 100',\n         [],\n         3600 // 1 hour\n       );\n       \n       // Warm user sessions\n       const activeUsers = await DatabaseCache.cachedQuery(\n         'SELECT id FROM users WHERE last_active > NOW() - INTERVAL 1 DAY',\n         [],\n         1800 // 30 minutes\n       );\n       \n       console.log(`Warmed cache for ${popularProducts.length} products and ${activeUsers.length} users`);\n     }\n\n     static async warmOnDemand(cacheKeys) {\n       for (const key of cacheKeys) {\n         if (!(await RedisCache.get(key))) {\n           // Generate cache for missing keys\n           await this.generateCacheForKey(key);\n         }\n       }\n     }\n   }\n\n   // Schedule cache warming\n   cron.schedule('0 */6 * * *', () => { // Every 6 hours\n     CacheWarmer.warmPopularData();\n   });\n   ```\n\n10. **Testing and Validation**\n    - Set up cache testing and validation:\n\n    **Cache Testing:**\n    ```javascript\n    // tests/cache.test.js\n    const request = require('supertest');\n    const app = require('../app');\n\n    describe('Cache Performance', () => {\n      test('should cache API responses', async () => {\n        // First request - should miss cache\n        const start1 = Date.now();\n        const response1 = await request(app).get('/api/products');\n        const duration1 = Date.now() - start1;\n\n        // Second request - should hit cache\n        const start2 = Date.now();\n        const response2 = await request(app).get('/api/products');\n        const duration2 = Date.now() - start2;\n\n        expect(response1.body).toEqual(response2.body);\n        expect(duration2).toBeLessThan(duration1 / 2); // Cached should be faster\n      });\n\n      test('should invalidate cache properly', async () => {\n        // Get initial data\n        const initial = await request(app).get('/api/products');\n        \n        // Update data\n        await request(app)\n          .put('/api/products/1')\n          .send({ name: 'Updated Product' });\n        \n        // Should get updated data\n        const updated = await request(app).get('/api/products');\n        expect(updated.body).not.toEqual(initial.body);\n      });\n    });\n    ```"
              },
              {
                "name": "/optimize-build",
                "description": "Optimize build processes and speed",
                "path": "plugins/commands-performance-optimization/commands/optimize-build.md",
                "frontmatter": {
                  "description": "Optimize build processes and speed",
                  "category": "performance-optimization",
                  "argument-hint": "1. **Build System Analysis**"
                },
                "content": "# Optimize Build Command\n\nOptimize build processes and speed\n\n## Instructions\n\nFollow this systematic approach to optimize build performance: **$ARGUMENTS**\n\n1. **Build System Analysis**\n   - Identify the build system in use (Webpack, Vite, Rollup, Gradle, Maven, Cargo, etc.)\n   - Review build configuration files and settings\n   - Analyze current build times and output sizes\n   - Map the complete build pipeline and dependencies\n\n2. **Performance Baseline**\n   - Measure current build times for different scenarios:\n     - Clean build (from scratch)\n     - Incremental build (with cache)\n     - Development vs production builds\n   - Document bundle sizes and asset sizes\n   - Identify the slowest parts of the build process\n\n3. **Dependency Optimization**\n   - Analyze build dependencies and their impact\n   - Remove unused dependencies from build process\n   - Update build tools to latest stable versions\n   - Consider alternative, faster build tools\n\n4. **Caching Strategy**\n   - Enable and optimize build caching\n   - Configure persistent cache for CI/CD\n   - Set up shared cache for team development\n   - Implement incremental compilation where possible\n\n5. **Bundle Analysis**\n   - Analyze bundle composition and sizes\n   - Identify large dependencies and duplicates\n   - Use bundle analyzers specific to your build tool\n   - Look for opportunities to split bundles\n\n6. **Code Splitting and Lazy Loading**\n   - Implement dynamic imports and code splitting\n   - Set up route-based splitting for SPAs\n   - Configure vendor chunk separation\n   - Optimize chunk sizes and loading strategies\n\n7. **Asset Optimization**\n   - Optimize images (compression, format conversion, lazy loading)\n   - Minify CSS and JavaScript\n   - Configure tree shaking to remove dead code\n   - Implement asset compression (gzip, brotli)\n\n8. **Development Build Optimization**\n   - Enable fast refresh/hot reloading\n   - Use development-specific optimizations\n   - Configure source maps for better debugging\n   - Optimize development server settings\n\n9. **Production Build Optimization**\n   - Enable all production optimizations\n   - Configure dead code elimination\n   - Set up proper minification and compression\n   - Optimize for smaller bundle sizes\n\n10. **Parallel Processing**\n    - Enable parallel processing where supported\n    - Configure worker threads for build tasks\n    - Optimize for multi-core systems\n    - Use parallel compilation for TypeScript/Babel\n\n11. **File System Optimization**\n    - Optimize file watching and polling\n    - Configure proper include/exclude patterns\n    - Use efficient file loaders and processors\n    - Minimize file I/O operations\n\n12. **CI/CD Build Optimization**\n    - Optimize CI build environments and resources\n    - Implement proper caching strategies for CI\n    - Use build matrices efficiently\n    - Configure parallel CI jobs where beneficial\n\n13. **Memory Usage Optimization**\n    - Monitor and optimize memory usage during builds\n    - Configure heap sizes for build tools\n    - Identify and fix memory leaks in build process\n    - Use memory-efficient compilation options\n\n14. **Output Optimization**\n    - Configure compression and encoding\n    - Optimize file naming and hashing strategies\n    - Set up proper asset manifests\n    - Implement efficient asset serving\n\n15. **Monitoring and Profiling**\n    - Set up build time monitoring\n    - Use build profiling tools to identify bottlenecks\n    - Track bundle size changes over time\n    - Monitor build performance regressions\n\n16. **Tool-Specific Optimizations**\n    \n    **For Webpack:**\n    - Configure optimization.splitChunks\n    - Use thread-loader for parallel processing\n    - Enable optimization.usedExports for tree shaking\n    - Configure resolve.modules and resolve.extensions\n\n    **For Vite:**\n    - Configure build.rollupOptions\n    - Use esbuild for faster transpilation\n    - Optimize dependency pre-bundling\n    - Configure build.chunkSizeWarningLimit\n\n    **For TypeScript:**\n    - Use incremental compilation\n    - Configure project references\n    - Optimize tsconfig.json settings\n    - Use skipLibCheck when appropriate\n\n17. **Environment-Specific Configuration**\n    - Separate development and production configurations\n    - Use environment variables for build optimization\n    - Configure feature flags for conditional builds\n    - Optimize for target environments\n\n18. **Testing Build Optimizations**\n    - Test build outputs for correctness\n    - Verify all optimizations work in target environments\n    - Check for any breaking changes from optimizations\n    - Measure and document performance improvements\n\n19. **Documentation and Maintenance**\n    - Document all optimization changes and their impact\n    - Create build performance monitoring dashboard\n    - Set up alerts for build performance regressions\n    - Regular review and updates of build configuration\n\nFocus on the optimizations that provide the biggest impact for your specific project and team workflow. Always measure before and after to quantify improvements."
              },
              {
                "name": "/optimize-bundle-size",
                "description": "Reduce and optimize bundle sizes",
                "path": "plugins/commands-performance-optimization/commands/optimize-bundle-size.md",
                "frontmatter": {
                  "description": "Reduce and optimize bundle sizes",
                  "category": "performance-optimization",
                  "allowed-tools": "Bash(npm *)"
                },
                "content": "# Optimize Bundle Size\n\nReduce and optimize bundle sizes\n\n## Instructions\n\n1. **Bundle Analysis and Assessment**\n   - Analyze current bundle size and composition using webpack-bundle-analyzer or similar\n   - Identify large dependencies and unused code\n   - Assess current build configuration and optimization settings\n   - Create baseline measurements for optimization tracking\n   - Document current performance metrics and loading times\n\n2. **Build Tool Configuration**\n   - Configure build tool optimization settings:\n\n   **Webpack Configuration:**\n   ```javascript\n   // webpack.config.js\n   const path = require('path');\n   const { BundleAnalyzerPlugin } = require('webpack-bundle-analyzer');\n\n   module.exports = {\n     mode: 'production',\n     optimization: {\n       splitChunks: {\n         chunks: 'all',\n         cacheGroups: {\n           vendor: {\n             test: /[\\\\/]node_modules[\\\\/]/,\n             name: 'vendors',\n             priority: 10,\n             reuseExistingChunk: true,\n           },\n           common: {\n             name: 'common',\n             minChunks: 2,\n             priority: 5,\n             reuseExistingChunk: true,\n           },\n         },\n       },\n       usedExports: true,\n       sideEffects: false,\n     },\n     plugins: [\n       new BundleAnalyzerPlugin({\n         analyzerMode: 'static',\n         openAnalyzer: false,\n       }),\n     ],\n   };\n   ```\n\n   **Vite Configuration:**\n   ```javascript\n   // vite.config.js\n   import { defineConfig } from 'vite';\n   import { visualizer } from 'rollup-plugin-visualizer';\n\n   export default defineConfig({\n     build: {\n       rollupOptions: {\n         output: {\n           manualChunks: {\n             vendor: ['react', 'react-dom'],\n             ui: ['@mui/material', '@emotion/react'],\n           },\n         },\n       },\n     },\n     plugins: [\n       visualizer({\n         filename: 'dist/stats.html',\n         open: true,\n         gzipSize: true,\n       }),\n     ],\n   });\n   ```\n\n3. **Code Splitting and Lazy Loading**\n   - Implement route-based code splitting:\n\n   **React Route Splitting:**\n   ```javascript\n   import { lazy, Suspense } from 'react';\n   import { Routes, Route } from 'react-router-dom';\n\n   const Home = lazy(() => import('./pages/Home'));\n   const Dashboard = lazy(() => import('./pages/Dashboard'));\n   const Profile = lazy(() => import('./pages/Profile'));\n\n   function App() {\n     return (\n       <Suspense fallback={<div>Loading...</div>}>\n         <Routes>\n           <Route path=\"/\" element={<Home />} />\n           <Route path=\"/dashboard\" element={<Dashboard />} />\n           <Route path=\"/profile\" element={<Profile />} />\n         </Routes>\n       </Suspense>\n     );\n   }\n   ```\n\n   **Dynamic Imports:**\n   ```javascript\n   // Lazy load heavy components\n   const HeavyComponent = lazy(() => \n     import('./HeavyComponent').then(module => ({\n       default: module.HeavyComponent\n     }))\n   );\n\n   // Conditional loading\n   async function loadAnalytics() {\n     if (process.env.NODE_ENV === 'production') {\n       const { analytics } = await import('./analytics');\n       return analytics;\n     }\n   }\n   ```\n\n4. **Tree Shaking and Dead Code Elimination**\n   - Configure tree shaking for optimal dead code elimination:\n\n   **Package.json Configuration:**\n   ```json\n   {\n     \"sideEffects\": false,\n     \"exports\": {\n       \".\": {\n         \"import\": \"./dist/index.esm.js\",\n         \"require\": \"./dist/index.cjs.js\"\n       }\n     }\n   }\n   ```\n\n   **Import Optimization:**\n   ```javascript\n   // Instead of importing entire library\n   // import * as _ from 'lodash';\n\n   // Import only what you need\n   import debounce from 'lodash/debounce';\n   import throttle from 'lodash/throttle';\n\n   // Use babel-plugin-import for automatic optimization\n   // .babelrc\n   {\n     \"plugins\": [\n       [\"import\", {\n         \"libraryName\": \"lodash\",\n         \"libraryDirectory\": \"\",\n         \"camel2DashComponentName\": false\n       }, \"lodash\"]\n     ]\n   }\n   ```\n\n5. **Dependency Optimization**\n   - Analyze and optimize dependencies:\n\n   **Package Analysis Script:**\n   ```javascript\n   // scripts/analyze-deps.js\n   const fs = require('fs');\n   const path = require('path');\n\n   function analyzeDependencies() {\n     const packageJson = JSON.parse(\n       fs.readFileSync('package.json', 'utf8')\n     );\n     \n     const deps = {\n       ...packageJson.dependencies,\n       ...packageJson.devDependencies\n     };\n\n     console.log('Large dependencies to review:');\n     Object.keys(deps).forEach(dep => {\n       try {\n         const depPath = require.resolve(dep);\n         const stats = fs.statSync(depPath);\n         if (stats.size > 100000) { // > 100KB\n           console.log(`${dep}: ${(stats.size / 1024).toFixed(2)}KB`);\n         }\n       } catch (e) {\n         // Skip if can't resolve\n       }\n     });\n   }\n\n   analyzeDependencies();\n   ```\n\n6. **Asset Optimization**\n   - Optimize static assets and media files:\n\n   **Image Optimization:**\n   ```javascript\n   // webpack.config.js\n   module.exports = {\n     module: {\n       rules: [\n         {\n           test: /\\.(png|jpe?g|gif|svg)$/i,\n           use: [\n             {\n               loader: 'file-loader',\n               options: {\n                 outputPath: 'images',\n               },\n             },\n             {\n               loader: 'image-webpack-loader',\n               options: {\n                 mozjpeg: { progressive: true, quality: 80 },\n                 optipng: { enabled: false },\n                 pngquant: { quality: [0.6, 0.8] },\n                 gifsicle: { interlaced: false },\n               },\n             },\n           ],\n         },\n       ],\n     },\n   };\n   ```\n\n7. **Module Federation and Micro-frontends**\n   - Implement module federation for large applications:\n\n   **Module Federation Setup:**\n   ```javascript\n   // webpack.config.js\n   const ModuleFederationPlugin = require('@module-federation/webpack');\n\n   module.exports = {\n     plugins: [\n       new ModuleFederationPlugin({\n         name: 'host',\n         remotes: {\n           mfe1: 'mfe1@http://localhost:3001/remoteEntry.js',\n           mfe2: 'mfe2@http://localhost:3002/remoteEntry.js',\n         },\n         shared: {\n           react: { singleton: true },\n           'react-dom': { singleton: true },\n         },\n       }),\n     ],\n   };\n   ```\n\n8. **Performance Monitoring and Measurement**\n   - Set up bundle size monitoring:\n\n   **Bundle Size Monitoring:**\n   ```javascript\n   // scripts/bundle-monitor.js\n   const fs = require('fs');\n   const path = require('path');\n   const gzipSize = require('gzip-size');\n\n   async function measureBundleSize() {\n     const distPath = path.join(__dirname, '../dist');\n     const files = fs.readdirSync(distPath);\n     \n     for (const file of files) {\n       if (file.endsWith('.js')) {\n         const filePath = path.join(distPath, file);\n         const content = fs.readFileSync(filePath);\n         const originalSize = content.length;\n         const compressed = await gzipSize(content);\n         \n         console.log(`${file}:`);\n         console.log(`  Original: ${(originalSize / 1024).toFixed(2)}KB`);\n         console.log(`  Gzipped: ${(compressed / 1024).toFixed(2)}KB`);\n       }\n     }\n   }\n\n   measureBundleSize();\n   ```\n\n9. **Progressive Loading Strategies**\n   - Implement progressive loading and resource hints:\n\n   **Resource Hints:**\n   ```html\n   <!-- Preload critical resources -->\n   <link rel=\"preload\" href=\"/fonts/main.woff2\" as=\"font\" type=\"font/woff2\" crossorigin>\n   <link rel=\"preload\" href=\"/critical.css\" as=\"style\">\n\n   <!-- Prefetch non-critical resources -->\n   <link rel=\"prefetch\" href=\"/dashboard.js\">\n   <link rel=\"prefetch\" href=\"/profile.js\">\n\n   <!-- DNS prefetch for external domains -->\n   <link rel=\"dns-prefetch\" href=\"//api.example.com\">\n   ```\n\n   **Intersection Observer for Lazy Loading:**\n   ```javascript\n   // utils/lazyLoad.js\n   export function lazyLoadComponent(importFunc) {\n     return lazy(() => {\n       return new Promise(resolve => {\n         const observer = new IntersectionObserver((entries) => {\n           entries.forEach(entry => {\n             if (entry.isIntersecting) {\n               importFunc().then(resolve);\n               observer.disconnect();\n             }\n           });\n         });\n         \n         // Observe a trigger element\n         const trigger = document.getElementById('lazy-trigger');\n         if (trigger) observer.observe(trigger);\n       });\n     });\n   }\n   ```\n\n10. **Validation and Continuous Monitoring**\n    - Set up automated bundle size validation:\n\n    **CI/CD Bundle Size Check:**\n    ```yaml\n    # .github/workflows/bundle-size.yml\n    name: Bundle Size Check\n    on: [pull_request]\n\n    jobs:\n      bundle-size:\n        runs-on: ubuntu-latest\n        steps:\n          - uses: actions/checkout@v2\n          - name: Setup Node\n            uses: actions/setup-node@v2\n            with:\n              node-version: '16'\n          - name: Install dependencies\n            run: npm ci\n          - name: Build bundle\n            run: npm run build\n          - name: Check bundle size\n            run: |\n              npm run bundle:analyze\n              node scripts/bundle-size-check.js\n    ```\n\n    **Bundle Size Threshold Check:**\n    ```javascript\n    // scripts/bundle-size-check.js\n    const fs = require('fs');\n    const path = require('path');\n\n    const THRESHOLDS = {\n      'main.js': 250 * 1024, // 250KB\n      'vendor.js': 500 * 1024, // 500KB\n    };\n\n    function checkBundleSize() {\n      const distPath = path.join(__dirname, '../dist');\n      const files = fs.readdirSync(distPath);\n      let failed = false;\n\n      files.forEach(file => {\n        if (file.endsWith('.js') && THRESHOLDS[file]) {\n          const filePath = path.join(distPath, file);\n          const size = fs.statSync(filePath).size;\n          \n          if (size > THRESHOLDS[file]) {\n            console.error(` ${file} exceeds threshold: ${size} > ${THRESHOLDS[file]}`);\n            failed = true;\n          } else {\n            console.log(` ${file} within threshold: ${size}`);\n          }\n        }\n      });\n\n      if (failed) {\n        process.exit(1);\n      }\n    }\n\n    checkBundleSize();\n    ```"
              },
              {
                "name": "/performance-audit",
                "description": "Audit application performance metrics",
                "path": "plugins/commands-performance-optimization/commands/performance-audit.md",
                "frontmatter": {
                  "description": "Audit application performance metrics",
                  "category": "performance-optimization"
                },
                "content": "# Performance Audit Command\n\nAudit application performance metrics\n\n## Instructions\n\nConduct a comprehensive performance audit following these steps:\n\n1. **Technology Stack Analysis**\n   - Identify the primary language, framework, and runtime environment\n   - Review build tools and optimization configurations\n   - Check for performance monitoring tools already in place\n\n2. **Code Performance Analysis**\n   - Identify inefficient algorithms and data structures\n   - Look for nested loops and O(n) operations\n   - Check for unnecessary computations and redundant operations\n   - Review memory allocation patterns and potential leaks\n\n3. **Database Performance**\n   - Analyze database queries for efficiency\n   - Check for missing indexes and slow queries\n   - Review connection pooling and database configuration\n   - Identify N+1 query problems and excessive database calls\n\n4. **Frontend Performance (if applicable)**\n   - Analyze bundle size and chunk optimization\n   - Check for unused code and dependencies\n   - Review image optimization and lazy loading\n   - Examine render performance and re-render cycles\n   - Check for memory leaks in UI components\n\n5. **Network Performance**\n   - Review API call patterns and caching strategies\n   - Check for unnecessary network requests\n   - Analyze payload sizes and compression\n   - Examine CDN usage and static asset optimization\n\n6. **Asynchronous Operations**\n   - Review async/await usage and promise handling\n   - Check for blocking operations and race conditions\n   - Analyze task queuing and background processing\n   - Identify opportunities for parallel execution\n\n7. **Memory Usage**\n   - Check for memory leaks and excessive memory consumption\n   - Review garbage collection patterns\n   - Analyze object lifecycle and cleanup\n   - Identify large objects and unnecessary data retention\n\n8. **Build & Deployment Performance**\n   - Analyze build times and optimization opportunities\n   - Review dependency bundling and tree shaking\n   - Check for development vs production optimizations\n   - Examine deployment pipeline efficiency\n\n9. **Performance Monitoring**\n   - Check existing performance metrics and monitoring\n   - Identify key performance indicators (KPIs) to track\n   - Review alerting and performance thresholds\n   - Suggest performance testing strategies\n\n10. **Benchmarking & Profiling**\n    - Run performance profiling tools appropriate for the stack\n    - Create benchmarks for critical code paths\n    - Measure before and after optimization impact\n    - Document performance baselines\n\n11. **Optimization Recommendations**\n    - Prioritize optimizations by impact and effort\n    - Provide specific code examples and alternatives\n    - Suggest architectural improvements for scalability\n    - Recommend appropriate performance tools and libraries\n\nInclude specific file paths, line numbers, and measurable metrics where possible. Focus on high-impact, low-effort optimizations first."
              },
              {
                "name": "/setup-cdn-optimization",
                "description": "Configure CDN for optimal delivery",
                "path": "plugins/commands-performance-optimization/commands/setup-cdn-optimization.md",
                "frontmatter": {
                  "description": "Configure CDN for optimal delivery",
                  "category": "performance-optimization"
                },
                "content": "# Setup CDN Optimization\n\nConfigure CDN for optimal delivery\n\n## Instructions\n\n1. **CDN Strategy and Provider Selection**\n   - Analyze application traffic patterns and global user distribution\n   - Evaluate CDN providers (CloudFlare, AWS CloudFront, Fastly, KeyCDN)\n   - Assess content types and caching requirements\n   - Plan CDN architecture and edge location strategy\n   - Define performance and cost optimization goals\n\n2. **CDN Configuration and Setup**\n   - Configure CDN with optimal settings:\n\n   **CloudFlare Configuration:**\n   ```javascript\n   // Cloudflare Page Rules via API\n   const cloudflare = require('cloudflare');\n   const cf = new cloudflare({\n     email: process.env.CLOUDFLARE_EMAIL,\n     key: process.env.CLOUDFLARE_API_KEY\n   });\n\n   const pageRules = [\n     {\n       targets: [{ target: 'url', constraint: { operator: 'matches', value: '*/static/*' }}],\n       actions: [\n         { id: 'cache_level', value: 'cache_everything' },\n         { id: 'edge_cache_ttl', value: 31536000 }, // 1 year\n         { id: 'browser_cache_ttl', value: 31536000 }\n       ]\n     },\n     {\n       targets: [{ target: 'url', constraint: { operator: 'matches', value: '*/api/*' }}],\n       actions: [\n         { id: 'cache_level', value: 'bypass' },\n         { id: 'compression', value: 'gzip' }\n       ]\n     }\n   ];\n\n   async function setupCDNRules() {\n     for (const rule of pageRules) {\n       await cf.zones.pagerules.add(process.env.CLOUDFLARE_ZONE_ID, rule);\n     }\n   }\n   ```\n\n   **AWS CloudFront Distribution:**\n   ```yaml\n   # cloudformation-cdn.yaml\n   AWSTemplateFormatVersion: '2010-09-09'\n   Resources:\n     CloudFrontDistribution:\n       Type: AWS::CloudFront::Distribution\n       Properties:\n         DistributionConfig:\n           Origins:\n             - Id: S3Origin\n               DomainName: !GetAtt S3Bucket.DomainName\n               S3OriginConfig:\n                 OriginAccessIdentity: !Sub 'origin-access-identity/cloudfront/${OAI}'\n             - Id: APIOrigin\n               DomainName: api.example.com\n               CustomOriginConfig:\n                 HTTPPort: 443\n                 OriginProtocolPolicy: https-only\n           \n           DefaultCacheBehavior:\n             TargetOriginId: S3Origin\n             ViewerProtocolPolicy: redirect-to-https\n             CachePolicyId: 4135ea2d-6df8-44a3-9df3-4b5a84be39ad # Managed-CachingOptimized\n             OriginRequestPolicyId: 88a5eaf4-2fd4-4709-b370-b4c650ea3fcf # Managed-CORS-S3Origin\n             \n           CacheBehaviors:\n             - PathPattern: '/api/*'\n               TargetOriginId: APIOrigin\n               ViewerProtocolPolicy: https-only\n               CachePolicyId: 4135ea2d-6df8-44a3-9df3-4b5a84be39ad\n               TTL:\n                 DefaultTTL: 0\n                 MaxTTL: 0\n               Compress: true\n             \n             - PathPattern: '/static/*'\n               TargetOriginId: S3Origin\n               ViewerProtocolPolicy: https-only\n               CachePolicyId: 658327ea-f89d-4fab-a63d-7e88639e58f6 # Managed-CachingOptimizedForUncompressedObjects\n               TTL:\n                 DefaultTTL: 86400\n                 MaxTTL: 31536000\n   ```\n\n3. **Static Asset Optimization**\n   - Optimize assets for CDN delivery:\n\n   **Asset Build Process:**\n   ```javascript\n   // webpack.config.js - CDN optimization\n   const path = require('path');\n   const { CleanWebpackPlugin } = require('clean-webpack-plugin');\n   const MiniCssExtractPlugin = require('mini-css-extract-plugin');\n\n   module.exports = {\n     output: {\n       path: path.resolve(__dirname, 'dist'),\n       filename: '[name].[contenthash].js',\n       publicPath: process.env.CDN_URL || '/',\n       assetModuleFilename: 'assets/[name].[contenthash][ext]',\n     },\n     \n     optimization: {\n       splitChunks: {\n         chunks: 'all',\n         cacheGroups: {\n           vendor: {\n             test: /[\\\\/]node_modules[\\\\/]/,\n             name: 'vendors',\n             filename: 'vendors.[contenthash].js',\n           },\n         },\n       },\n     },\n     \n     plugins: [\n       new CleanWebpackPlugin(),\n       new MiniCssExtractPlugin({\n         filename: 'css/[name].[contenthash].css',\n       }),\n     ],\n     \n     module: {\n       rules: [\n         {\n           test: /\\.(png|jpe?g|gif|svg)$/i,\n           type: 'asset/resource',\n           generator: {\n             filename: 'images/[name].[contenthash][ext]',\n           },\n           use: [\n             {\n               loader: 'image-webpack-loader',\n               options: {\n                 mozjpeg: { progressive: true, quality: 80 },\n                 optipng: { enabled: false },\n                 pngquant: { quality: [0.6, 0.8] },\n                 webp: { quality: 80 },\n               },\n             },\n           ],\n         },\n       ],\n     },\n   };\n   ```\n\n   **Next.js CDN Configuration:**\n   ```javascript\n   // next.config.js\n   const withOptimizedImages = require('next-optimized-images');\n\n   module.exports = withOptimizedImages({\n     assetPrefix: process.env.CDN_URL || '',\n     \n     images: {\n       domains: ['cdn.example.com'],\n       formats: ['image/webp', 'image/avif'],\n       deviceSizes: [640, 750, 828, 1080, 1200, 1920, 2048, 3840],\n       imageSizes: [16, 32, 48, 64, 96, 128, 256, 384],\n       minimumCacheTTL: 31536000, // 1 year\n     },\n     \n     async headers() {\n       return [\n         {\n           source: '/static/(.*)',\n           headers: [\n             {\n               key: 'Cache-Control',\n               value: 'public, max-age=31536000, immutable',\n             },\n           ],\n         },\n       ];\n     },\n   });\n   ```\n\n4. **Compression and Optimization**\n   - Configure optimal compression settings:\n\n   **Gzip/Brotli Compression:**\n   ```javascript\n   // Express.js compression middleware\n   const compression = require('compression');\n   const express = require('express');\n   const app = express();\n\n   // Advanced compression configuration\n   app.use(compression({\n     level: 6, // Compression level (1-9)\n     threshold: 1024, // Only compress files > 1KB\n     filter: (req, res) => {\n       // Custom compression filter\n       if (req.headers['x-no-compression']) {\n         return false;\n       }\n       \n       // Compress text-based content types\n       return compression.filter(req, res);\n     }\n   }));\n\n   // Serve pre-compressed files if available\n   app.get('*.js', (req, res, next) => {\n     const acceptEncoding = req.get('Accept-Encoding');\n     \n     if (acceptEncoding && acceptEncoding.includes('br')) {\n       req.url = req.url + '.br';\n       res.set('Content-Encoding', 'br');\n       res.set('Content-Type', 'application/javascript');\n     } else if (acceptEncoding && acceptEncoding.includes('gzip')) {\n       req.url = req.url + '.gz';\n       res.set('Content-Encoding', 'gzip');\n       res.set('Content-Type', 'application/javascript');\n     }\n     \n     next();\n   });\n   ```\n\n   **Build-time Compression:**\n   ```javascript\n   // compression-plugin.js\n   const CompressionPlugin = require('compression-webpack-plugin');\n   const BrotliPlugin = require('brotli-webpack-plugin');\n\n   module.exports = {\n     plugins: [\n       // Gzip compression\n       new CompressionPlugin({\n         algorithm: 'gzip',\n         test: /\\.(js|css|html|svg)$/,\n         threshold: 8192,\n         minRatio: 0.8,\n       }),\n       \n       // Brotli compression\n       new BrotliPlugin({\n         asset: '[path].br[query]',\n         test: /\\.(js|css|html|svg)$/,\n         threshold: 8192,\n         minRatio: 0.8,\n       }),\n     ],\n   };\n   ```\n\n5. **Cache Headers and Policies**\n   - Configure optimal caching strategies:\n\n   **Smart Cache Headers:**\n   ```javascript\n   // cache-control.js\n   class CacheControlManager {\n     static getCacheHeaders(filePath, fileType) {\n       const cacheStrategies = {\n         // Long-term caching for versioned assets\n         versioned: {\n           'Cache-Control': 'public, max-age=31536000, immutable',\n           'Expires': new Date(Date.now() + 31536000000).toUTCString(),\n         },\n         \n         // Medium-term caching for semi-static content\n         semiStatic: {\n           'Cache-Control': 'public, max-age=86400, must-revalidate',\n           'ETag': this.generateETag(filePath),\n         },\n         \n         // Short-term caching for dynamic content\n         dynamic: {\n           'Cache-Control': 'public, max-age=300, must-revalidate',\n           'ETag': this.generateETag(filePath),\n         },\n         \n         // No caching for sensitive content\n         noCache: {\n           'Cache-Control': 'no-cache, no-store, must-revalidate',\n           'Pragma': 'no-cache',\n           'Expires': '0',\n         },\n       };\n\n       // Determine strategy based on file type and path\n       if (filePath.match(/\\.(js|css|png|jpg|jpeg|gif|ico|woff2?)$/)) {\n         return filePath.includes('[hash]') || filePath.includes('[contenthash]') \n           ? cacheStrategies.versioned \n           : cacheStrategies.semiStatic;\n       }\n       \n       if (filePath.startsWith('/api/')) {\n         return cacheStrategies.dynamic;\n       }\n       \n       if (filePath.includes('/admin') || filePath.includes('/auth')) {\n         return cacheStrategies.noCache;\n       }\n       \n       return cacheStrategies.semiStatic;\n     }\n\n     static generateETag(content) {\n       return `\"${require('crypto').createHash('md5').update(content).digest('hex')}\"`;\n     }\n   }\n\n   // Express middleware\n   app.use((req, res, next) => {\n     const headers = CacheControlManager.getCacheHeaders(req.path, req.get('Content-Type'));\n     Object.entries(headers).forEach(([key, value]) => {\n       res.set(key, value);\n     });\n     next();\n   });\n   ```\n\n6. **Image Optimization and Delivery**\n   - Implement advanced image optimization:\n\n   **Responsive Image Delivery:**\n   ```javascript\n   // image-optimization.js\n   const sharp = require('sharp');\n   const fs = require('fs').promises;\n\n   class ImageOptimizer {\n     static async generateResponsiveImages(inputPath, outputDir) {\n       const sizes = [\n         { width: 320, suffix: 'sm' },\n         { width: 640, suffix: 'md' },\n         { width: 1024, suffix: 'lg' },\n         { width: 1920, suffix: 'xl' },\n       ];\n\n       const formats = ['webp', 'jpeg'];\n       const results = [];\n\n       for (const size of sizes) {\n         for (const format of formats) {\n           const outputPath = `${outputDir}/${size.suffix}.${format}`;\n           \n           await sharp(inputPath)\n             .resize(size.width, null, { withoutEnlargement: true })\n             .toFormat(format, { quality: 80 })\n             .toFile(outputPath);\n             \n           results.push({\n             path: outputPath,\n             width: size.width,\n             format: format,\n           });\n         }\n       }\n\n       return results;\n     }\n\n     static generatePictureElement(imageName, alt, className = '') {\n       return `\n         <picture class=\"${className}\">\n           <source media=\"(min-width: 1024px)\" \n                   srcset=\"/images/${imageName}-xl.webp\" \n                   type=\"image/webp\">\n           <source media=\"(min-width: 1024px)\" \n                   srcset=\"/images/${imageName}-xl.jpeg\" \n                   type=\"image/jpeg\">\n           <source media=\"(min-width: 640px)\" \n                   srcset=\"/images/${imageName}-lg.webp\" \n                   type=\"image/webp\">\n           <source media=\"(min-width: 640px)\" \n                   srcset=\"/images/${imageName}-lg.jpeg\" \n                   type=\"image/jpeg\">\n           <source media=\"(min-width: 320px)\" \n                   srcset=\"/images/${imageName}-md.webp\" \n                   type=\"image/webp\">\n           <source media=\"(min-width: 320px)\" \n                   srcset=\"/images/${imageName}-md.jpeg\" \n                   type=\"image/jpeg\">\n           <img src=\"/images/${imageName}-sm.jpeg\" \n                alt=\"${alt}\" \n                loading=\"lazy\"\n                decoding=\"async\">\n         </picture>\n       `;\n     }\n   }\n   ```\n\n7. **CDN Purging and Cache Invalidation**\n   - Implement intelligent cache invalidation:\n\n   **CloudFlare Cache Purging:**\n   ```javascript\n   // cdn-purge.js\n   const cloudflare = require('cloudflare');\n\n   class CDNManager {\n     constructor() {\n       this.cf = new cloudflare({\n         email: process.env.CLOUDFLARE_EMAIL,\n         key: process.env.CLOUDFLARE_API_KEY\n       });\n       this.zoneId = process.env.CLOUDFLARE_ZONE_ID;\n     }\n\n     async purgeFiles(files) {\n       try {\n         const result = await this.cf.zones.purgeCache(this.zoneId, {\n           files: files.map(file => `https://example.com${file}`)\n         });\n         console.log('Cache purged successfully:', result);\n         return result;\n       } catch (error) {\n         console.error('Cache purge failed:', error);\n         throw error;\n       }\n     }\n\n     async purgeByTags(tags) {\n       try {\n         const result = await this.cf.zones.purgeCache(this.zoneId, {\n           tags: tags\n         });\n         console.log('Cache purged by tags:', result);\n         return result;\n       } catch (error) {\n         console.error('Cache purge by tags failed:', error);\n         throw error;\n       }\n     }\n\n     async purgeEverything() {\n       try {\n         const result = await this.cf.zones.purgeCache(this.zoneId, {\n           purge_everything: true\n         });\n         console.log('All cache purged:', result);\n         return result;\n       } catch (error) {\n         console.error('Full cache purge failed:', error);\n         throw error;\n       }\n     }\n   }\n\n   // Usage in deployment pipeline\n   const cdnManager = new CDNManager();\n\n   // Selective purging after deployment\n   async function postDeploymentPurge() {\n     const filesToPurge = [\n       '/static/js/main.*.js',\n       '/static/css/main.*.css',\n       '/',\n       '/index.html'\n     ];\n     \n     await cdnManager.purgeFiles(filesToPurge);\n   }\n   ```\n\n8. **Performance Monitoring and Analytics**\n   - Set up CDN performance monitoring:\n\n   **CDN Performance Tracking:**\n   ```javascript\n   // cdn-analytics.js\n   class CDNAnalytics {\n     static async getCDNMetrics() {\n       const metrics = {\n         cacheHitRatio: await this.getCacheHitRatio(),\n         bandwidth: await this.getBandwidthUsage(),\n         responseTime: await this.getResponseTimes(),\n         errorRate: await this.getErrorRate(),\n       };\n\n       return metrics;\n     }\n\n     static async getCacheHitRatio() {\n       // CloudFlare Analytics API\n       const response = await fetch(`https://api.cloudflare.com/client/v4/zones/${ZONE_ID}/analytics/dashboard`, {\n         headers: {\n           'X-Auth-Email': process.env.CLOUDFLARE_EMAIL,\n           'X-Auth-Key': process.env.CLOUDFLARE_API_KEY,\n         }\n       });\n\n       const data = await response.json();\n       return data.result.totals.requests.cached / data.result.totals.requests.all;\n     }\n\n     static trackCDNPerformance() {\n       // Real User Monitoring for CDN performance\n       if (typeof window !== 'undefined') {\n         const observer = new PerformanceObserver((list) => {\n           for (const entry of list.getEntries()) {\n             if (entry.name.includes('cdn.example.com')) {\n               // Track CDN resource loading times\n               console.log('CDN Resource:', {\n                 name: entry.name,\n                 duration: entry.duration,\n                 transferSize: entry.transferSize,\n                 encodedBodySize: entry.encodedBodySize,\n               });\n               \n               // Send to analytics\n               this.sendCDNMetric({\n                 resource: entry.name,\n                 loadTime: entry.duration,\n                 cacheStatus: entry.transferSize === 0 ? 'hit' : 'miss',\n               });\n             }\n           }\n         });\n\n         observer.observe({ entryTypes: ['resource'] });\n       }\n     }\n\n     static sendCDNMetric(metric) {\n       // Send to your analytics service\n       fetch('/api/analytics/cdn', {\n         method: 'POST',\n         headers: { 'Content-Type': 'application/json' },\n         body: JSON.stringify(metric),\n       });\n     }\n   }\n   ```\n\n9. **Security and Access Control**\n   - Configure CDN security features:\n\n   **CDN Security Configuration:**\n   ```javascript\n   // cdn-security.js\n   class CDNSecurity {\n     static setupSecurityHeaders() {\n       return {\n         'Strict-Transport-Security': 'max-age=31536000; includeSubDomains; preload',\n         'X-Content-Type-Options': 'nosniff',\n         'X-Frame-Options': 'DENY',\n         'X-XSS-Protection': '1; mode=block',\n         'Referrer-Policy': 'strict-origin-when-cross-origin',\n         'Content-Security-Policy': `\n           default-src 'self';\n           script-src 'self' 'unsafe-inline' cdn.example.com;\n           style-src 'self' 'unsafe-inline' cdn.example.com;\n           img-src 'self' data: cdn.example.com;\n           font-src 'self' cdn.example.com;\n         `.replace(/\\s+/g, ' ').trim(),\n       };\n     }\n\n     static configureHotlinkProtection() {\n       // CloudFlare Worker for hotlink protection\n       return `\n         addEventListener('fetch', event => {\n           event.respondWith(handleRequest(event.request));\n         });\n\n         async function handleRequest(request) {\n           const url = new URL(request.url);\n           const referer = request.headers.get('Referer');\n           \n           // Allow requests from your domain and direct access\n           const allowedDomains = ['example.com', 'www.example.com'];\n           \n           if (!referer || allowedDomains.some(domain => referer.includes(domain))) {\n             return fetch(request);\n           }\n           \n           // Block hotlinking\n           return new Response('Hotlinking not allowed', { status: 403 });\n         }\n       `;\n     }\n   }\n   ```\n\n10. **Cost Optimization and Monitoring**\n    - Implement CDN cost optimization:\n\n    **Cost Monitoring:**\n    ```javascript\n    // cdn-cost-optimization.js\n    class CDNCostOptimizer {\n      static async analyzeUsage() {\n        const usage = await this.getCDNUsage();\n        const recommendations = [];\n\n        // Analyze bandwidth usage by file type\n        if (usage.images > usage.total * 0.6) {\n          recommendations.push({\n            type: 'image_optimization',\n            message: 'Images account for >60% of bandwidth. Consider WebP format and better compression.',\n            potential_savings: '20-40%'\n          });\n        }\n\n        // Analyze cache hit ratio\n        if (usage.cacheHitRatio < 0.8) {\n          recommendations.push({\n            type: 'cache_optimization',\n            message: 'Cache hit ratio is below 80%. Review cache headers and TTL settings.',\n            potential_savings: '10-25%'\n          });\n        }\n\n        return recommendations;\n      }\n\n      static async optimizeTierUsage() {\n        // Move less frequently accessed content to cheaper tiers\n        const accessPatterns = await this.getAccessPatterns();\n        \n        const coldFiles = accessPatterns.filter(file => \n          file.requests_per_day < 10 && file.size > 1024 * 1024 // <10 requests/day, >1MB\n        );\n\n        console.log(`Found ${coldFiles.length} files suitable for cold storage`);\n        return coldFiles;\n      }\n\n      static setupCostAlerts() {\n        // Monitor CDN costs and set up alerts\n        return {\n          daily_bandwidth_alert: '100GB',\n          monthly_cost_alert: '$500',\n          cache_hit_ratio_alert: '75%',\n          error_rate_alert: '5%'\n        };\n      }\n    }\n\n    // Monthly cost analysis\n    setInterval(async () => {\n      const analysis = await CDNCostOptimizer.analyzeUsage();\n      console.log('CDN Cost Analysis:', analysis);\n    }, 24 * 60 * 60 * 1000); // Daily\n    ```"
              },
              {
                "name": "/system-behavior-simulator",
                "description": "Simulate system performance under various loads with capacity planning, bottleneck identification, and optimization strategies.",
                "path": "plugins/commands-performance-optimization/commands/system-behavior-simulator.md",
                "frontmatter": {
                  "description": "Simulate system performance under various loads with capacity planning, bottleneck identification, and optimization strategies.",
                  "category": "performance-optimization",
                  "argument-hint": "Specify system behavior parameters",
                  "allowed-tools": "Read, Write"
                },
                "content": "# System Behavior Simulator\n\nSimulate system performance under various loads with capacity planning, bottleneck identification, and optimization strategies.\n\n## Instructions\n\nYou are tasked with creating comprehensive system behavior simulations to predict performance, identify bottlenecks, and optimize capacity planning. Follow this approach: **$ARGUMENTS**\n\n### 1. Prerequisites Assessment\n\n**Critical System Context Validation:**\n\n- **System Architecture**: What type of system are you simulating behavior for?\n- **Performance Goals**: What are the target performance metrics and SLAs?\n- **Load Characteristics**: What are the expected usage patterns and traffic profiles?\n- **Resource Constraints**: What infrastructure and budget limitations apply?\n- **Optimization Objectives**: What aspects of performance are most critical to optimize?\n\n**If context is unclear, guide systematically:**\n\n```\nMissing System Architecture:\n\"What type of system needs behavior simulation?\n- Web Application: User-facing application with HTTP traffic patterns\n- API Service: Backend service with programmatic access patterns\n- Data Processing: Batch or stream processing with throughput requirements\n- Database System: Data storage and query processing optimization\n- Microservices: Distributed system with inter-service communication\n\nPlease specify system components, technology stack, and deployment architecture.\"\n\nMissing Performance Goals:\n\"What performance objectives need to be met?\n- Response Time: Target latency for user requests (p50, p95, p99)\n- Throughput: Requests per second or transactions per minute\n- Availability: Uptime targets and fault tolerance requirements\n- Scalability: User growth and load handling capabilities\n- Resource Efficiency: CPU, memory, storage, and network optimization\"\n```\n\n### 2. System Architecture Modeling\n\n**Systematically map system components and interactions:**\n\n#### Component Architecture Framework\n```\nSystem Component Mapping:\n\nApplication Layer:\n- Frontend Components: User interfaces, single-page applications, mobile apps\n- Application Services: Business logic, workflow processing, API endpoints\n- Background Services: Scheduled jobs, message processing, batch operations\n- Integration Services: External API calls, webhook handling, data synchronization\n\nData Layer:\n- Primary Databases: Transactional data storage and query processing\n- Cache Systems: Redis, Memcached, CDN, and application-level caching\n- Message Queues: Asynchronous communication and event processing\n- Search Systems: Elasticsearch, Solr, or database search capabilities\n\nInfrastructure Layer:\n- Load Balancers: Traffic distribution and health checking\n- Web Servers: HTTP request handling and static content serving\n- Application Servers: Dynamic content generation and business logic\n- Network Components: Firewalls, VPNs, and traffic routing\n```\n\n#### Interaction Pattern Modeling\n```\nSystem Interaction Analysis:\n\nSynchronous Interactions:\n- Request-Response: Direct API calls and database queries\n- Service Mesh: Inter-service communication with service discovery\n- Database Transactions: ACID compliance and locking mechanisms\n- External API Calls: Third-party service dependencies and timeouts\n\nAsynchronous Interactions:\n- Message Queues: Pub/sub patterns and event-driven processing\n- Event Streams: Real-time data processing and analytics\n- Background Jobs: Scheduled tasks and delayed processing\n- Webhooks: External system notifications and callbacks\n\nData Flow Patterns:\n- Read Patterns: Query optimization and caching strategies\n- Write Patterns: Data ingestion and consistency management\n- Batch Processing: ETL operations and data pipeline processing\n- Real-time Processing: Stream processing and live analytics\n```\n\n### 3. Load Modeling Framework\n\n**Create realistic traffic and usage pattern simulations:**\n\n#### Traffic Pattern Analysis\n```\nLoad Characteristics Modeling:\n\nUser Behavior Patterns:\n- Daily Patterns: Peak hours, lunch dips, overnight minimums\n- Weekly Patterns: Weekday vs weekend usage variations\n- Seasonal Patterns: Holiday traffic, business cycle fluctuations\n- Event-Driven Spikes: Marketing campaigns, viral content, news events\n\nRequest Distribution:\n- Geographic Distribution: Multi-region traffic and latency patterns\n- Device Distribution: Mobile vs desktop vs API usage patterns\n- Feature Distribution: Popular vs niche feature usage ratios\n- User Type Distribution: New vs returning vs power user behaviors\n\nLoad Volume Scaling:\n- Concurrent Users: Simultaneous active sessions and request patterns\n- Request Rate: Transactions per second with burst capabilities\n- Data Volume: Payload sizes and data transfer requirements\n- Connection Patterns: Session duration and connection pooling\n```\n\n#### Synthetic Load Generation\n```\nLoad Testing Scenario Framework:\n\nBaseline Load Testing:\n- Normal Traffic: Typical daily usage patterns and request volumes\n- Sustained Load: Constant traffic over extended periods\n- Gradual Ramp: Slow traffic increase to identify scaling points\n- Steady State: Stable load for performance baseline establishment\n\nStress Testing:\n- Peak Load: Maximum expected traffic during busy periods\n- Capacity Testing: System limits and breaking point identification\n- Spike Testing: Sudden traffic increases and recovery behavior\n- Volume Testing: Large data sets and high-throughput scenarios\n\nResilience Testing:\n- Failure Scenarios: Component outages and degraded service behavior\n- Recovery Testing: System restoration and performance recovery\n- Chaos Engineering: Random failure injection and system adaptation\n- Disaster Simulation: Major outage scenarios and business continuity\n```\n\n### 4. Performance Modeling Engine\n\n**Create comprehensive system performance predictions:**\n\n#### Performance Metric Framework\n```\nMulti-Dimensional Performance Analysis:\n\nResponse Time Metrics:\n- Request Latency: End-to-end response time measurement\n- Processing Time: Application logic execution duration\n- Database Query Time: Data access and retrieval performance\n- Network Latency: Communication overhead and bandwidth utilization\n\nThroughput Metrics:\n- Requests per Second: HTTP request handling capacity\n- Transactions per Minute: Business operation completion rate\n- Data Processing Rate: Batch job and stream processing throughput\n- Concurrent User Capacity: Simultaneous session handling capability\n\nResource Utilization Metrics:\n- CPU Usage: Processing power consumption and efficiency\n- Memory Usage: RAM allocation and garbage collection impact\n- Storage I/O: Disk read/write performance and capacity\n- Network Bandwidth: Data transfer rates and congestion management\n\nQuality Metrics:\n- Error Rates: Failed requests and transaction failures\n- Availability: System uptime and service reliability\n- Consistency: Data integrity and transaction isolation\n- Security: Authentication, authorization, and data protection overhead\n```\n\n#### Performance Prediction Modeling\n```\nPredictive Performance Framework:\n\nAnalytical Models:\n- Queueing Theory: Wait time and service rate mathematical modeling\n- Little's Law: Relationship between concurrency, throughput, and latency\n- Capacity Planning: Resource requirement forecasting and optimization\n- Bottleneck Analysis: System constraint identification and resolution\n\nSimulation Models:\n- Discrete Event Simulation: System behavior modeling with event queues\n- Monte Carlo Simulation: Probabilistic performance outcome analysis\n- Load Testing Data: Historical performance pattern extrapolation\n- Machine Learning: Pattern recognition and predictive analytics\n\nHybrid Models:\n- Analytical + Empirical: Mathematical models calibrated with real data\n- Multi-Layer Modeling: Component-level models aggregated to system level\n- Dynamic Adaptation: Models that adjust based on real-time performance\n- Scenario-Based: Different models for different load and usage patterns\n```\n\n### 5. Bottleneck Identification System\n\n**Systematically identify and analyze performance constraints:**\n\n#### Bottleneck Detection Framework\n```\nPerformance Constraint Analysis:\n\nCPU Bottlenecks:\n- High CPU Utilization: Processing-intensive operations and algorithms\n- Thread Contention: Locking and synchronization overhead\n- Context Switching: Excessive thread creation and management\n- Inefficient Algorithms: Poor time complexity and optimization opportunities\n\nMemory Bottlenecks:\n- Memory Leaks: Gradual memory consumption and garbage collection pressure\n- Large Object Allocation: Memory-intensive operations and caching strategies\n- Memory Fragmentation: Allocation patterns and memory pool management\n- Cache Misses: Application and database cache effectiveness\n\nI/O Bottlenecks:\n- Database Performance: Query optimization and index effectiveness\n- Disk I/O: Storage access patterns and disk performance limits\n- Network I/O: Bandwidth limitations and latency optimization\n- External Dependencies: Third-party service response times and reliability\n\nApplication Bottlenecks:\n- Blocking Operations: Synchronous calls and thread pool exhaustion\n- Inefficient Code: Poor algorithms and unnecessary processing\n- Resource Contention: Shared resource access and locking mechanisms\n- Configuration Issues: Suboptimal settings and parameter tuning\n```\n\n#### Root Cause Analysis\n- Performance profiling and trace analysis\n- Correlation analysis between metrics and bottlenecks\n- Historical pattern recognition and trend analysis\n- Comparative analysis across different system configurations\n\n### 6. Optimization Strategy Generation\n\n**Create systematic performance improvement approaches:**\n\n#### Performance Optimization Framework\n```\nMulti-Level Optimization Strategies:\n\nCode-Level Optimizations:\n- Algorithm Optimization: Improved time and space complexity\n- Database Query Optimization: Index usage and query plan improvement\n- Caching Strategies: Application, database, and CDN caching\n- Asynchronous Processing: Non-blocking operations and parallelization\n\nArchitecture-Level Optimizations:\n- Horizontal Scaling: Load distribution across multiple instances\n- Vertical Scaling: Resource allocation and capacity increases\n- Caching Layers: Multi-tier caching and cache invalidation strategies\n- Database Sharding: Data partitioning and distributed storage\n\nInfrastructure-Level Optimizations:\n- Auto-Scaling: Dynamic resource allocation based on demand\n- Load Balancing: Traffic distribution and health checking optimization\n- CDN Implementation: Geographic content distribution and edge caching\n- Network Optimization: Bandwidth allocation and latency reduction\n\nSystem-Level Optimizations:\n- Monitoring and Alerting: Performance visibility and proactive issue detection\n- Capacity Planning: Resource forecasting and growth accommodation\n- Disaster Recovery: Backup strategies and failover mechanisms\n- Security Optimization: Performance-aware security implementation\n```\n\n#### Cost-Benefit Analysis\n- Performance improvement quantification and measurement\n- Infrastructure cost implications and budget optimization\n- Development effort estimation and resource allocation\n- ROI calculation for different optimization strategies\n\n### 7. Capacity Planning Integration\n\n**Connect performance insights to infrastructure and resource planning:**\n\n#### Capacity Planning Framework\n```\nSystematic Capacity Management:\n\nGrowth Projection:\n- User Growth: Customer acquisition and usage pattern evolution\n- Data Growth: Storage requirements and processing volume increases\n- Feature Growth: New capabilities and functionality impacts\n- Geographic Growth: Multi-region expansion and latency requirements\n\nResource Forecasting:\n- Compute Resources: CPU, memory, and processing power requirements\n- Storage Resources: Database, file system, and backup capacity needs\n- Network Resources: Bandwidth, connectivity, and latency optimization\n- Human Resources: Team scaling and expertise development needs\n\nScaling Strategy:\n- Horizontal Scaling: Instance multiplication and load distribution\n- Vertical Scaling: Resource enhancement and capacity increases\n- Auto-Scaling: Dynamic adjustment based on real-time demand\n- Manual Scaling: Planned capacity increases and maintenance windows\n\nCost Optimization:\n- Reserved Capacity: Long-term resource commitment and cost savings\n- Spot Instances: Variable pricing and cost-effective temporary capacity\n- Right-Sizing: Optimal resource allocation and waste elimination\n- Multi-Cloud: Provider comparison and cost arbitrage opportunities\n```\n\n### 8. Output Generation and Recommendations\n\n**Present simulation insights in actionable performance optimization format:**\n\n```\n## System Behavior Simulation: [System Name]\n\n### Performance Summary\n- Current Capacity: [baseline performance metrics]\n- Bottleneck Analysis: [primary performance constraints identified]\n- Optimization Potential: [improvement opportunities and expected gains]\n- Scaling Requirements: [resource needs for growth accommodation]\n\n### Load Testing Results\n\n| Scenario | Throughput | Latency (p95) | Error Rate | Resource Usage |\n|----------|------------|---------------|------------|----------------|\n| Normal Load | 500 RPS | 200ms | 0.1% | 60% CPU |\n| Peak Load | 1000 RPS | 800ms | 2.5% | 85% CPU |\n| Stress Test | 1500 RPS | 2000ms | 15% | 95% CPU |\n\n### Bottleneck Analysis\n- Primary Bottleneck: [most limiting performance factor]\n- Secondary Bottlenecks: [additional constraints affecting performance]\n- Cascade Effects: [how bottlenecks impact other system components]\n- Resolution Priority: [recommended order of bottleneck addressing]\n\n### Optimization Recommendations\n\n#### Immediate Optimizations (0-30 days):\n- Quick Wins: [low-effort, high-impact improvements]\n- Configuration Tuning: [parameter adjustments and settings optimization]\n- Query Optimization: [database and application query improvements]\n- Caching Implementation: [strategic caching layer additions]\n\n#### Medium-term Optimizations (1-6 months):\n- Architecture Changes: [structural improvements and scaling strategies]\n- Infrastructure Upgrades: [hardware and platform enhancements]\n- Code Refactoring: [application optimization and efficiency improvements]\n- Monitoring Enhancement: [observability and alerting system improvements]\n\n#### Long-term Optimizations (6+ months):\n- Technology Migration: [platform or framework modernization]\n- System Redesign: [fundamental architecture improvements]\n- Capacity Expansion: [infrastructure scaling and geographic distribution]\n- Innovation Integration: [new technology adoption and competitive advantage]\n\n### Capacity Planning\n- Current Capacity: [existing system limits and headroom]\n- Growth Accommodation: [resource scaling for projected demand]\n- Cost Implications: [budget requirements for capacity increases]\n- Timeline Requirements: [implementation schedule for capacity improvements]\n\n### Monitoring and Alerting Strategy\n- Key Performance Indicators: [critical metrics for ongoing monitoring]\n- Alert Thresholds: [performance degradation warning levels]\n- Escalation Procedures: [response protocols for performance issues]\n- Regular Review Schedule: [ongoing optimization and capacity assessment]\n```\n\n### 9. Continuous Performance Learning\n\n**Establish ongoing simulation refinement and system optimization:**\n\n#### Performance Validation\n- Real-world performance comparison to simulation predictions\n- Optimization effectiveness measurement and validation\n- User experience correlation with system performance metrics\n- Business impact assessment of performance improvements\n\n#### Model Enhancement\n- Simulation accuracy improvement based on actual system behavior\n- Load pattern refinement and user behavior modeling\n- Bottleneck prediction enhancement and early warning systems\n- Optimization strategy effectiveness tracking and improvement\n\n## Usage Examples\n\n```bash\n# Web application performance simulation\n/performance:system-behavior-simulator Simulate e-commerce platform performance under Black Friday traffic with 10x normal load\n\n# API service scaling analysis\n/performance:system-behavior-simulator Model REST API performance for mobile app with 1M+ daily active users and geographic distribution\n\n# Database performance optimization\n/performance:system-behavior-simulator Simulate database performance for analytics workload with real-time reporting requirements\n\n# Microservices capacity planning\n/performance:system-behavior-simulator Model microservices mesh performance under various failure scenarios and auto-scaling conditions\n```\n\n## Quality Indicators\n\n- **Green**: Comprehensive load modeling, validated bottleneck analysis, quantified optimization strategies\n- **Yellow**: Good load coverage, basic bottleneck identification, estimated optimization benefits\n- **Red**: Limited load scenarios, unvalidated bottlenecks, qualitative-only optimization suggestions\n\n## Common Pitfalls to Avoid\n\n- Load unrealism: Testing with artificial patterns that don't match real usage\n- Bottleneck tunnel vision: Focusing on single constraints while ignoring others\n- Optimization premature: Optimizing for problems that don't exist yet\n- Capacity under-planning: Not accounting for growth and traffic spikes\n- Monitoring blindness: Not establishing ongoing performance visibility\n- Cost ignorance: Optimizing performance without considering budget constraints\n\nTransform system performance from reactive firefighting into proactive, data-driven optimization through comprehensive behavior simulation and capacity planning."
              }
            ],
            "skills": []
          },
          {
            "name": "commands-project-setup",
            "description": "Commands for initializing and setting up new projects",
            "source": "./plugins/commands-project-setup",
            "category": "commands",
            "version": "1.0.0",
            "author": {
              "name": "BuildWithClaude Community",
              "url": "https://github.com/davepoon/buildwithclaude"
            },
            "install_commands": [
              "/plugin marketplace add davepoon/buildwithclaude",
              "/plugin install commands-project-setup@buildwithclaude"
            ],
            "signals": {
              "stars": 2229,
              "forks": 239,
              "pushed_at": "2026-01-12T10:19:01Z",
              "created_at": "2025-07-25T02:26:45Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/modernize-deps",
                "description": "Update and modernize project dependencies",
                "path": "plugins/commands-project-setup/commands/modernize-deps.md",
                "frontmatter": {
                  "description": "Update and modernize project dependencies",
                  "category": "project-setup",
                  "argument-hint": "1. **Dependency Audit**",
                  "allowed-tools": "Bash(npm *), Read"
                },
                "content": "# Modernize Dependencies Command\n\nUpdate and modernize project dependencies\n\n## Instructions\n\nFollow this approach to modernize dependencies: **$ARGUMENTS**\n\n1. **Dependency Audit**\n   ```bash\n   # Check outdated packages\n   npm outdated\n   pip list --outdated\n   composer outdated\n   \n   # Security audit\n   npm audit\n   pip-audit\n   ```\n\n2. **Update Strategy**\n   - Start with patch updates (1.2.3  1.2.4)\n   - Then minor updates (1.2.3  1.3.0)\n   - Finally major updates (1.2.3  2.0.0)\n   - Test thoroughly between each step\n\n3. **Automated Updates**\n   ```bash\n   # Safe updates\n   npm update\n   pip install -U package-name\n   \n   # Interactive updates\n   npx npm-check-updates -i\n   ```\n\n4. **Breaking Changes Review**\n   - Read changelogs and migration guides\n   - Identify deprecated APIs\n   - Plan code changes needed\n   - Update tests and documentation\n\n5. **Testing and Validation**\n   ```bash\n   npm test\n   npm run build\n   npm run lint\n   ```\n\n6. **Documentation Updates**\n   - Update README.md\n   - Revise installation instructions\n   - Update API documentation\n   - Note breaking changes\n\nRemember to update dependencies incrementally, test thoroughly, and maintain backward compatibility where possible."
              },
              {
                "name": "/setup-development-environment",
                "description": "Setup complete development environment",
                "path": "plugins/commands-project-setup/commands/setup-development-environment.md",
                "frontmatter": {
                  "description": "Setup complete development environment",
                  "category": "project-setup",
                  "allowed-tools": "Edit"
                },
                "content": "# Setup Development Environment\n\nSetup complete development environment\n\n## Instructions\n\n1. **Environment Analysis and Requirements**\n   - Analyze current project structure and technology stack\n   - Identify required development tools and dependencies\n   - Check existing development environment configuration\n   - Determine team size and collaboration requirements\n   - Assess platform requirements (Windows, macOS, Linux)\n\n2. **Core Development Tools Installation**\n   - Verify and install required runtime environments (Node.js, Python, Java, etc.)\n   - Set up package managers with proper versions (npm, yarn, pnpm, pip, maven, etc.)\n   - Install and configure version control tools (Git, Git LFS)\n   - Set up code editors with workspace-specific settings (VSCode, IntelliJ)\n   - Configure terminal and shell environment\n\n3. **Project-Specific Tooling**\n   - Install project dependencies and dev dependencies\n   - Set up build tools and task runners\n   - Configure bundlers and module systems\n   - Install testing frameworks and runners\n   - Set up debugging tools and extensions\n   - Configure profiling and performance monitoring tools\n\n4. **Code Quality and Standards**\n   - Install and configure linting tools (ESLint, Pylint, etc.)\n   - Set up code formatting tools (Prettier, Black, etc.)\n   - Configure pre-commit hooks with Husky or similar\n   - Set up code spell checking and grammar tools\n   - Configure import sorting and organization tools\n   - Set up code complexity and quality metrics\n\n5. **Development Server and Database**\n   - Set up local development server with hot reloading\n   - Configure database server and management tools\n   - Set up containerized development environment (Docker)\n   - Configure API mocking and testing tools\n   - Set up local SSL certificates for HTTPS development\n   - Configure environment variable management\n\n6. **IDE and Editor Configuration**\n   - Configure workspace settings and extensions\n   - Set up language-specific plugins and syntax highlighting\n   - Configure IntelliSense and auto-completion\n   - Set up debugging configurations and breakpoints\n   - Configure integrated terminal and task running\n   - Set up code snippets and templates\n\n7. **Environment Variables and Secrets**\n   - Create .env template files for different environments\n   - Set up local environment variable management\n   - Configure secrets management for development\n   - Set up API keys and service credentials\n   - Configure environment-specific configuration files\n   - Document required environment variables\n\n8. **Documentation and Knowledge Base**\n   - Create comprehensive setup documentation\n   - Document common development workflows\n   - Set up project wiki or knowledge base\n   - Create troubleshooting guides for common issues\n   - Document coding standards and best practices\n   - Set up onboarding checklist for new team members\n\n9. **Collaboration and Communication Tools**\n   - Configure team communication channels\n   - Set up code review workflows and tools\n   - Configure issue tracking and project management\n   - Set up shared development resources and services\n   - Configure team calendars and meeting tools\n   - Set up shared documentation and file storage\n\n10. **Validation and Testing**\n    - Verify all tools and dependencies are properly installed\n    - Test development server startup and hot reloading\n    - Validate database connections and data access\n    - Test build processes and deployment workflows\n    - Verify code quality tools are working correctly\n    - Test collaboration workflows and team access\n    - Create development environment health check script"
              },
              {
                "name": "/setup-formatting",
                "description": "Configure code formatting tools",
                "path": "plugins/commands-project-setup/commands/setup-formatting.md",
                "frontmatter": {
                  "description": "Configure code formatting tools",
                  "category": "project-setup",
                  "argument-hint": "1. **Language-Specific Tools**",
                  "allowed-tools": "Bash(npm *)"
                },
                "content": "# Setup Formatting Command\n\nConfigure code formatting tools\n\n## Instructions\n\nSetup code formatting following these steps: **$ARGUMENTS**\n\n1. **Language-Specific Tools**\n\n   **JavaScript/TypeScript:**\n   ```bash\n   npm install -D prettier\n   echo '{\"semi\": true, \"singleQuote\": true, \"tabWidth\": 2}' > .prettierrc\n   ```\n\n   **Python:**\n   ```bash\n   pip install black isort\n   echo '[tool.black]\\nline-length = 88\\ntarget-version = [\"py38\"]' > pyproject.toml\n   ```\n\n   **Java:**\n   ```bash\n   # Google Java Format or Spotless plugin\n   ```\n\n2. **Configuration Files**\n\n   **.prettierrc:**\n   ```json\n   {\n     \"semi\": true,\n     \"singleQuote\": true,\n     \"tabWidth\": 2,\n     \"trailingComma\": \"es5\",\n     \"printWidth\": 80\n   }\n   ```\n\n3. **IDE Setup**\n   - Install formatter extensions\n   - Enable format on save\n   - Configure keyboard shortcuts\n\n4. **Scripts and Automation**\n   ```json\n   {\n     \"scripts\": {\n       \"format\": \"prettier --write .\",\n       \"format:check\": \"prettier --check .\"\n     }\n   }\n   ```\n\n5. **Pre-commit Hooks**\n   ```bash\n   npm install -D husky lint-staged\n   echo '{\"*.{js,ts,tsx}\": [\"prettier --write\", \"eslint --fix\"]}' > .lintstagedrc\n   ```\n\nRemember to run formatting on entire codebase initially and configure team IDE settings consistently."
              },
              {
                "name": "/setup-linting",
                "description": "Setup code linting and quality tools",
                "path": "plugins/commands-project-setup/commands/setup-linting.md",
                "frontmatter": {
                  "description": "Setup code linting and quality tools",
                  "category": "project-setup",
                  "argument-hint": "1. **Project Analysis**",
                  "allowed-tools": "Bash(npm *)"
                },
                "content": "# Setup Linting Command\n\nSetup code linting and quality tools\n\n## Instructions\n\nFollow this systematic approach to setup linting: **$ARGUMENTS**\n\n1. **Project Analysis**\n   - Identify programming languages and frameworks\n   - Check existing linting configuration\n   - Review current code style and patterns\n   - Assess team preferences and requirements\n\n2. **Tool Selection by Language**\n\n   **JavaScript/TypeScript:**\n   ```bash\n   npm install -D eslint @typescript-eslint/parser @typescript-eslint/eslint-plugin\n   npm install -D prettier eslint-config-prettier eslint-plugin-prettier\n   ```\n\n   **Python:**\n   ```bash\n   pip install flake8 black isort mypy pylint\n   ```\n\n   **Java:**\n   ```bash\n   # Add to pom.xml or build.gradle\n   # Checkstyle, SpotBugs, PMD\n   ```\n\n3. **Configuration Setup**\n\n   **ESLint (.eslintrc.json):**\n   ```json\n   {\n     \"extends\": [\n       \"eslint:recommended\",\n       \"@typescript-eslint/recommended\",\n       \"prettier\"\n     ],\n     \"parser\": \"@typescript-eslint/parser\",\n     \"plugins\": [\"@typescript-eslint\"],\n     \"rules\": {\n       \"no-console\": \"warn\",\n       \"no-unused-vars\": \"error\",\n       \"@typescript-eslint/no-explicit-any\": \"warn\"\n     }\n   }\n   ```\n\n4. **IDE Integration**\n   - Configure VS Code settings\n   - Setup auto-fix on save\n   - Install relevant extensions\n\n5. **CI/CD Integration**\n   ```yaml\n   - name: Lint code\n     run: npm run lint\n   ```\n\n6. **Package.json Scripts**\n   ```json\n   {\n     \"scripts\": {\n       \"lint\": \"eslint src --ext .ts,.tsx\",\n       \"lint:fix\": \"eslint src --ext .ts,.tsx --fix\",\n       \"format\": \"prettier --write src\"\n     }\n   }\n   ```\n\nRemember to customize rules based on team preferences and gradually enforce stricter standards."
              },
              {
                "name": "/setup-monorepo",
                "description": "Configure monorepo project structure",
                "path": "plugins/commands-project-setup/commands/setup-monorepo.md",
                "frontmatter": {
                  "description": "Configure monorepo project structure",
                  "category": "project-setup",
                  "argument-hint": "Specify monorepo configuration options"
                },
                "content": "# Setup Monorepo\n\nConfigure monorepo project structure\n\n## Instructions\n\n1. **Monorepo Tool Analysis**\n   - Parse monorepo tool from arguments: `$ARGUMENTS` (nx, lerna, rush, yarn-workspaces, pnpm-workspaces, turborepo)\n   - If no tool specified, analyze project structure and recommend best tool based on:\n     - Project size and complexity\n     - Existing package manager\n     - Team preferences and CI/CD requirements\n   - Validate tool compatibility with existing codebase\n\n2. **Workspace Structure Setup**\n   - Create standard monorepo directory structure:\n     - `packages/` or `apps/` for applications\n     - `libs/` or `shared/` for shared libraries\n     - `tools/` for build tools and scripts\n     - `docs/` for documentation\n   - Configure workspace root package.json with workspace definitions\n   - Set up proper .gitignore for monorepo patterns\n\n3. **Tool-Specific Configuration**\n   - **Nx**: Initialize Nx workspace, configure nx.json, add essential plugins\n   - **Lerna**: Set up lerna.json, configure version management and publishing\n   - **Rush**: Initialize rush.json, configure build orchestration and policies\n   - **Yarn Workspaces**: Configure workspaces in package.json, set up workspace protocols\n   - **pnpm Workspaces**: Set up pnpm-workspace.yaml, configure filtering and dependencies\n   - **Turborepo**: Initialize turbo.json, configure pipeline and caching\n\n4. **Package Management Configuration**\n   - Configure package manager settings for workspace support\n   - Set up dependency hoisting and deduplication rules\n   - Configure workspace-specific package.json templates\n   - Set up cross-package dependency management\n   - Configure private package registry if needed\n\n5. **Build System Integration**\n   - Configure build orchestration and task running\n   - Set up dependency graph analysis and affected package detection\n   - Configure parallel builds and task caching\n   - Set up incremental builds for changed packages\n   - Configure build artifacts and output management\n\n6. **Development Workflow**\n   - Set up workspace-wide development scripts\n   - Configure hot reloading and watch mode for development\n   - Set up workspace-wide linting and formatting\n   - Configure debugging across multiple packages\n   - Set up workspace-wide testing and coverage\n\n7. **Version Management**\n   - Configure versioning strategy (independent vs. fixed versions)\n   - Set up changelog generation for workspace packages\n   - Configure release workflow and package publishing\n   - Set up semantic versioning and conventional commits\n   - Configure workspace-wide dependency updates\n\n8. **CI/CD Pipeline Integration**\n   - Configure CI to detect affected packages and run targeted tests\n   - Set up build matrix for different package combinations\n   - Configure deployment pipeline for multiple packages\n   - Set up workspace-wide quality gates\n   - Configure artifact publishing and registry management\n\n9. **Documentation and Standards**\n   - Create workspace-wide development guidelines\n   - Document package creation and management procedures\n   - Set up workspace-wide code standards and conventions\n   - Create architectural decision records for monorepo patterns\n   - Document deployment and release procedures\n\n10. **Validation and Testing**\n    - Verify workspace configuration is correct\n    - Test package creation and cross-package dependencies\n    - Validate build pipeline and task execution\n    - Test development workflow and hot reloading\n    - Verify CI/CD integration and affected package detection\n    - Create example packages to demonstrate workspace functionality"
              },
              {
                "name": "/setup-rate-limiting",
                "description": "Implement API rate limiting",
                "path": "plugins/commands-project-setup/commands/setup-rate-limiting.md",
                "frontmatter": {
                  "description": "Implement API rate limiting",
                  "category": "project-setup"
                },
                "content": "# Setup Rate Limiting\n\nImplement API rate limiting\n\n## Instructions\n\n1. **Rate Limiting Strategy and Planning**\n   - Analyze API endpoints and traffic patterns\n   - Define rate limiting policies for different user types and endpoints\n   - Plan for distributed rate limiting across multiple servers\n   - Consider different rate limiting algorithms (token bucket, sliding window, etc.)\n   - Design rate limiting bypass mechanisms for trusted clients\n\n2. **Express.js Rate Limiting Implementation**\n   - Set up comprehensive rate limiting middleware:\n\n   **Basic Rate Limiting Setup:**\n   ```javascript\n   // middleware/rate-limiter.js\n   const rateLimit = require('express-rate-limit');\n   const RedisStore = require('rate-limit-redis');\n   const Redis = require('ioredis');\n\n   class RateLimiter {\n     constructor() {\n       this.redis = new Redis(process.env.REDIS_URL);\n       this.setupDefaultLimiters();\n     }\n\n     setupDefaultLimiters() {\n       // General API rate limiter\n       this.generalLimiter = rateLimit({\n         store: new RedisStore({\n           sendCommand: (...args) => this.redis.call(...args),\n         }),\n         windowMs: 15 * 60 * 1000, // 15 minutes\n         max: 1000, // Limit each IP to 1000 requests per windowMs\n         message: {\n           error: 'Too many requests from this IP',\n           retryAfter: '15 minutes'\n         },\n         standardHeaders: true,\n         legacyHeaders: false,\n         keyGenerator: (req) => {\n           // Use user ID if authenticated, otherwise IP\n           return req.user?.id || req.ip;\n         },\n         skip: (req) => {\n           // Skip rate limiting for internal requests\n           return req.headers['x-internal-request'] === 'true';\n         },\n         onLimitReached: (req, res, options) => {\n           console.warn('Rate limit reached:', {\n             ip: req.ip,\n             userAgent: req.get('User-Agent'),\n             endpoint: req.path,\n             timestamp: new Date().toISOString()\n           });\n         }\n       });\n\n       // Strict limiter for sensitive endpoints\n       this.strictLimiter = rateLimit({\n         store: new RedisStore({\n           sendCommand: (...args) => this.redis.call(...args),\n         }),\n         windowMs: 60 * 60 * 1000, // 1 hour\n         max: 5, // Very strict limit\n         message: {\n           error: 'Too many attempts for this sensitive operation',\n           retryAfter: '1 hour'\n         },\n         skipSuccessfulRequests: true,\n         keyGenerator: (req) => `${req.user?.id || req.ip}:${req.path}`\n       });\n\n       // Authentication rate limiter\n       this.authLimiter = rateLimit({\n         store: new RedisStore({\n           sendCommand: (...args) => this.redis.call(...args),\n         }),\n         windowMs: 15 * 60 * 1000, // 15 minutes\n         max: 5, // Limit login attempts\n         skipSuccessfulRequests: true,\n         keyGenerator: (req) => `auth:${req.ip}:${req.body.email || req.body.username}`,\n         message: {\n           error: 'Too many authentication attempts',\n           retryAfter: '15 minutes'\n         }\n       });\n     }\n\n     // Dynamic rate limiter based on user tier\n     createTierBasedLimiter(windowMs = 15 * 60 * 1000) {\n       return rateLimit({\n         store: new RedisStore({\n           sendCommand: (...args) => this.redis.call(...args),\n         }),\n         windowMs,\n         max: (req) => {\n           const user = req.user;\n           if (!user) return 100; // Anonymous users\n           \n           switch (user.tier) {\n             case 'premium': return 10000;\n             case 'pro': return 5000;\n             case 'basic': return 1000;\n             default: return 500;\n           }\n         },\n         keyGenerator: (req) => `tier:${req.user?.id || req.ip}`,\n         message: (req) => ({\n           error: 'Rate limit exceeded for your tier',\n           currentTier: req.user?.tier || 'anonymous',\n           upgradeUrl: '/upgrade'\n         })\n       });\n     }\n\n     // Endpoint-specific rate limiter\n     createEndpointLimiter(endpoint, config) {\n       return rateLimit({\n         store: new RedisStore({\n           sendCommand: (...args) => this.redis.call(...args),\n         }),\n         windowMs: config.windowMs || 60 * 1000,\n         max: config.max || 100,\n         keyGenerator: (req) => `endpoint:${endpoint}:${req.user?.id || req.ip}`,\n         message: {\n           error: `Rate limit exceeded for ${endpoint}`,\n           limit: config.max,\n           window: config.windowMs\n         },\n         ...config\n       });\n     }\n   }\n\n   module.exports = new RateLimiter();\n   ```\n\n3. **Advanced Rate Limiting Algorithms**\n   - Implement sophisticated rate limiting strategies:\n\n   **Token Bucket Implementation:**\n   ```javascript\n   // rate-limiters/token-bucket.js\n   class TokenBucket {\n     constructor(capacity, refillRate, refillPeriod = 1000) {\n       this.capacity = capacity;\n       this.tokens = capacity;\n       this.refillRate = refillRate;\n       this.refillPeriod = refillPeriod;\n       this.lastRefill = Date.now();\n     }\n\n     consume(tokens = 1) {\n       this.refill();\n       \n       if (this.tokens >= tokens) {\n         this.tokens -= tokens;\n         return true;\n       }\n       \n       return false;\n     }\n\n     refill() {\n       const now = Date.now();\n       const timePassed = now - this.lastRefill;\n       const tokensToAdd = Math.floor(timePassed / this.refillPeriod) * this.refillRate;\n       \n       this.tokens = Math.min(this.capacity, this.tokens + tokensToAdd);\n       this.lastRefill = now;\n     }\n\n     getAvailableTokens() {\n       this.refill();\n       return this.tokens;\n     }\n\n     getTimeToNextToken() {\n       if (this.tokens > 0) return 0;\n       \n       const timeSinceLastRefill = Date.now() - this.lastRefill;\n       return this.refillPeriod - (timeSinceLastRefill % this.refillPeriod);\n     }\n   }\n\n   // Redis-backed token bucket for distributed systems\n   class DistributedTokenBucket {\n     constructor(redis, key, capacity, refillRate, refillPeriod = 1000) {\n       this.redis = redis;\n       this.key = key;\n       this.capacity = capacity;\n       this.refillRate = refillRate;\n       this.refillPeriod = refillPeriod;\n     }\n\n     async consume(tokens = 1) {\n       const script = `\n         local key = KEYS[1]\n         local capacity = tonumber(ARGV[1])\n         local refillRate = tonumber(ARGV[2])\n         local refillPeriod = tonumber(ARGV[3])\n         local tokensRequested = tonumber(ARGV[4])\n         local now = tonumber(ARGV[5])\n         \n         local bucket = redis.call('HMGET', key, 'tokens', 'lastRefill')\n         local tokens = tonumber(bucket[1]) or capacity\n         local lastRefill = tonumber(bucket[2]) or now\n         \n         -- Calculate tokens to add\n         local timePassed = now - lastRefill\n         local tokensToAdd = math.floor(timePassed / refillPeriod) * refillRate\n         tokens = math.min(capacity, tokens + tokensToAdd)\n         \n         local success = 0\n         if tokens >= tokensRequested then\n           tokens = tokens - tokensRequested\n           success = 1\n         end\n         \n         -- Update bucket\n         redis.call('HMSET', key, 'tokens', tokens, 'lastRefill', now)\n         redis.call('EXPIRE', key, 3600) -- 1 hour TTL\n         \n         return {success, tokens, math.max(0, refillPeriod - (timePassed % refillPeriod))}\n       `;\n\n       const result = await this.redis.eval(\n         script,\n         1,\n         this.key,\n         this.capacity,\n         this.refillRate,\n         this.refillPeriod,\n         tokens,\n         Date.now()\n       );\n\n       return {\n         allowed: result[0] === 1,\n         tokensRemaining: result[1],\n         timeToNextToken: result[2]\n       };\n     }\n   }\n\n   module.exports = { TokenBucket, DistributedTokenBucket };\n   ```\n\n   **Sliding Window Rate Limiter:**\n   ```javascript\n   // rate-limiters/sliding-window.js\n   class SlidingWindowRateLimiter {\n     constructor(redis, windowSize, maxRequests) {\n       this.redis = redis;\n       this.windowSize = windowSize; // in milliseconds\n       this.maxRequests = maxRequests;\n     }\n\n     async isAllowed(key) {\n       const now = Date.now();\n       const windowStart = now - this.windowSize;\n\n       const script = `\n         local key = KEYS[1]\n         local windowStart = tonumber(ARGV[1])\n         local now = tonumber(ARGV[2])\n         local maxRequests = tonumber(ARGV[3])\n         \n         -- Remove old entries\n         redis.call('ZREMRANGEBYSCORE', key, 0, windowStart)\n         \n         -- Count current requests in window\n         local currentCount = redis.call('ZCARD', key)\n         \n         if currentCount < maxRequests then\n           -- Add current request\n           redis.call('ZADD', key, now, now)\n           redis.call('EXPIRE', key, math.ceil(ARGV[4] / 1000))\n           return {1, currentCount + 1, maxRequests - currentCount - 1}\n         else\n           return {0, currentCount, 0}\n         end\n       `;\n\n       const result = await this.redis.eval(\n         script,\n         1,\n         key,\n         windowStart,\n         now,\n         this.maxRequests,\n         this.windowSize\n       );\n\n       return {\n         allowed: result[0] === 1,\n         currentCount: result[1],\n         remaining: result[2]\n       };\n     }\n\n     async getRemainingRequests(key) {\n       const now = Date.now();\n       const windowStart = now - this.windowSize;\n       \n       await this.redis.zremrangebyscore(key, 0, windowStart);\n       const currentCount = await this.redis.zcard(key);\n       \n       return Math.max(0, this.maxRequests - currentCount);\n     }\n   }\n\n   module.exports = SlidingWindowRateLimiter;\n   ```\n\n4. **Custom Rate Limiting Middleware**\n   - Build flexible rate limiting solutions:\n\n   **Advanced Rate Limiting Middleware:**\n   ```javascript\n   // middleware/advanced-rate-limiter.js\n   const { TokenBucket, DistributedTokenBucket } = require('../rate-limiters/token-bucket');\n   const SlidingWindowRateLimiter = require('../rate-limiters/sliding-window');\n\n   class AdvancedRateLimiter {\n     constructor(redis) {\n       this.redis = redis;\n       this.rateLimiters = new Map();\n       this.setupRateLimiters();\n     }\n\n     setupRateLimiters() {\n       // API endpoints with different limits\n       this.rateLimiters.set('api:general', {\n         type: 'sliding-window',\n         limiter: new SlidingWindowRateLimiter(this.redis, 60000, 1000) // 1000 req/min\n       });\n\n       this.rateLimiters.set('api:upload', {\n         type: 'token-bucket',\n         capacity: 10,\n         refillRate: 1,\n         refillPeriod: 10000 // 1 token per 10 seconds\n       });\n\n       this.rateLimiters.set('api:search', {\n         type: 'sliding-window',\n         limiter: new SlidingWindowRateLimiter(this.redis, 60000, 100) // 100 req/min\n       });\n     }\n\n     createMiddleware(limiterKey, options = {}) {\n       return async (req, res, next) => {\n         try {\n           const userKey = this.generateUserKey(req, limiterKey);\n           const config = this.rateLimiters.get(limiterKey);\n\n           if (!config) {\n             return next(); // No rate limiting configured\n           }\n\n           let result;\n           \n           if (config.type === 'sliding-window') {\n             result = await config.limiter.isAllowed(userKey);\n           } else if (config.type === 'token-bucket') {\n             const bucket = new DistributedTokenBucket(\n               this.redis,\n               userKey,\n               config.capacity,\n               config.refillRate,\n               config.refillPeriod\n             );\n             result = await bucket.consume(options.tokensRequired || 1);\n           }\n\n           // Set rate limit headers\n           this.setRateLimitHeaders(res, result, config);\n\n           if (!result.allowed) {\n             return res.status(429).json({\n               error: 'Rate limit exceeded',\n               retryAfter: this.calculateRetryAfter(result, config),\n               remaining: result.remaining || 0\n             });\n           }\n\n           // Add rate limit info to request\n           req.rateLimit = result;\n           next();\n\n         } catch (error) {\n           console.error('Rate limiting error:', error);\n           next(); // Fail open - don't block requests on rate limiter errors\n         }\n       };\n     }\n\n     generateUserKey(req, limiterKey) {\n       const userId = req.user?.id || req.ip;\n       const endpoint = req.route?.path || req.path;\n       return `${limiterKey}:${userId}:${endpoint}`;\n     }\n\n     setRateLimitHeaders(res, result, config) {\n       if (result.remaining !== undefined) {\n         res.set('X-RateLimit-Remaining', result.remaining.toString());\n       }\n       \n       if (result.currentCount !== undefined) {\n         res.set('X-RateLimit-Used', result.currentCount.toString());\n       }\n\n       if (config.type === 'sliding-window') {\n         res.set('X-RateLimit-Limit', config.limiter.maxRequests.toString());\n         res.set('X-RateLimit-Window', (config.limiter.windowSize / 1000).toString());\n       } else if (config.type === 'token-bucket') {\n         res.set('X-RateLimit-Limit', config.capacity.toString());\n       }\n     }\n\n     calculateRetryAfter(result, config) {\n       if (result.timeToNextToken) {\n         return Math.ceil(result.timeToNextToken / 1000);\n       }\n       \n       if (config.type === 'sliding-window') {\n         return Math.ceil(config.limiter.windowSize / 1000);\n       }\n       \n       return 60; // Default 1 minute\n     }\n\n     // Dynamic rate limiting based on system load\n     createAdaptiveLimiter(baseLimit) {\n       return async (req, res, next) => {\n         const systemLoad = await this.getSystemLoad();\n         let dynamicLimit = baseLimit;\n\n         // Reduce limits during high load\n         if (systemLoad > 0.8) {\n           dynamicLimit = Math.floor(baseLimit * 0.5);\n         } else if (systemLoad > 0.6) {\n           dynamicLimit = Math.floor(baseLimit * 0.7);\n         }\n\n         // Apply dynamic limit\n         const limiter = new SlidingWindowRateLimiter(this.redis, 60000, dynamicLimit);\n         const userKey = this.generateUserKey(req, 'adaptive');\n         const result = await limiter.isAllowed(userKey);\n\n         res.set('X-RateLimit-Adaptive', 'true');\n         res.set('X-RateLimit-System-Load', systemLoad.toString());\n         \n         if (!result.allowed) {\n           return res.status(429).json({\n             error: 'Rate limit exceeded (adaptive)',\n             systemLoad: systemLoad,\n             retryAfter: 60\n           });\n         }\n\n         next();\n       };\n     }\n\n     async getSystemLoad() {\n       // Get system metrics (CPU, memory, etc.)\n       const os = require('os');\n       const loadAvg = os.loadavg()[0]; // 1-minute load average\n       const cpuCount = os.cpus().length;\n       return Math.min(1, loadAvg / cpuCount);\n     }\n   }\n\n   module.exports = AdvancedRateLimiter;\n   ```\n\n5. **API Quota Management**\n   - Implement comprehensive quota systems:\n\n   **Quota Management System:**\n   ```javascript\n   // services/quota-manager.js\n   class QuotaManager {\n     constructor(redis, database) {\n       this.redis = redis;\n       this.database = database;\n       this.quotaTypes = {\n         'api_calls': { resetPeriod: 'monthly', defaultLimit: 10000 },\n         'data_transfer': { resetPeriod: 'monthly', defaultLimit: 1073741824 }, // 1GB in bytes\n         'storage': { resetPeriod: 'none', defaultLimit: 5368709120 }, // 5GB\n         'concurrent_requests': { resetPeriod: 'none', defaultLimit: 10 }\n       };\n     }\n\n     async checkQuota(userId, quotaType, amount = 1) {\n       const userQuota = await this.getUserQuota(userId, quotaType);\n       const currentUsage = await this.getCurrentUsage(userId, quotaType);\n\n       const available = userQuota.limit - currentUsage;\n       const allowed = available >= amount;\n\n       if (allowed) {\n         await this.incrementUsage(userId, quotaType, amount);\n       }\n\n       return {\n         allowed,\n         usage: currentUsage + (allowed ? amount : 0),\n         limit: userQuota.limit,\n         remaining: Math.max(0, available - (allowed ? amount : 0)),\n         resetDate: userQuota.resetDate\n       };\n     }\n\n     async getUserQuota(userId, quotaType) {\n       // Get user-specific quota from database\n       const customQuota = await this.database.query(\n         'SELECT * FROM user_quotas WHERE user_id = $1 AND quota_type = $2',\n         [userId, quotaType]\n       );\n\n       if (customQuota.rows.length > 0) {\n         return customQuota.rows[0];\n       }\n\n       // Get plan-based quota\n       const user = await this.database.query(\n         'SELECT plan FROM users WHERE id = $1',\n         [userId]\n       );\n\n       const planQuota = await this.getPlanQuota(user.rows[0]?.plan || 'free', quotaType);\n       return planQuota;\n     }\n\n     async getPlanQuota(plan, quotaType) {\n       const planQuotas = {\n         free: {\n           api_calls: 1000,\n           data_transfer: 104857600, // 100MB\n           storage: 1073741824, // 1GB\n           concurrent_requests: 5\n         },\n         basic: {\n           api_calls: 10000,\n           data_transfer: 1073741824, // 1GB\n           storage: 10737418240, // 10GB\n           concurrent_requests: 10\n         },\n         pro: {\n           api_calls: 100000,\n           data_transfer: 10737418240, // 10GB\n           storage: 107374182400, // 100GB\n           concurrent_requests: 50\n         },\n         enterprise: {\n           api_calls: 1000000,\n           data_transfer: 107374182400, // 100GB\n           storage: 1099511627776, // 1TB\n           concurrent_requests: 200\n         }\n       };\n\n       const limit = planQuotas[plan]?.[quotaType] || this.quotaTypes[quotaType].defaultLimit;\n       const resetDate = this.calculateResetDate(quotaType);\n\n       return { limit, resetDate };\n     }\n\n     async getCurrentUsage(userId, quotaType) {\n       const quotaConfig = this.quotaTypes[quotaType];\n       \n       if (quotaConfig.resetPeriod === 'none') {\n         // Non-resetting quota (like storage)\n         const key = `quota:${userId}:${quotaType}:current`;\n         const usage = await this.redis.get(key);\n         return parseInt(usage) || 0;\n       } else {\n         // Resetting quota (like monthly API calls)\n         const period = this.getCurrentPeriod(quotaConfig.resetPeriod);\n         const key = `quota:${userId}:${quotaType}:${period}`;\n         const usage = await this.redis.get(key);\n         return parseInt(usage) || 0;\n       }\n     }\n\n     async incrementUsage(userId, quotaType, amount) {\n       const quotaConfig = this.quotaTypes[quotaType];\n       \n       if (quotaConfig.resetPeriod === 'none') {\n         const key = `quota:${userId}:${quotaType}:current`;\n         await this.redis.incrby(key, amount);\n         await this.redis.expire(key, 86400 * 365); // 1 year TTL\n       } else {\n         const period = this.getCurrentPeriod(quotaConfig.resetPeriod);\n         const key = `quota:${userId}:${quotaType}:${period}`;\n         await this.redis.incrby(key, amount);\n         \n         // Set TTL to end of period\n         const ttl = this.getTTLForPeriod(quotaConfig.resetPeriod);\n         await this.redis.expire(key, ttl);\n       }\n\n       // Update usage analytics\n       await this.recordUsageAnalytics(userId, quotaType, amount);\n     }\n\n     getCurrentPeriod(resetPeriod) {\n       const now = new Date();\n       \n       switch (resetPeriod) {\n         case 'daily':\n           return now.toISOString().split('T')[0]; // YYYY-MM-DD\n         case 'weekly':\n           const weekStart = new Date(now);\n           weekStart.setDate(now.getDate() - now.getDay());\n           return weekStart.toISOString().split('T')[0];\n         case 'monthly':\n           return `${now.getFullYear()}-${String(now.getMonth() + 1).padStart(2, '0')}`;\n         case 'yearly':\n           return now.getFullYear().toString();\n         default:\n           return 'current';\n       }\n     }\n\n     calculateResetDate(quotaType) {\n       const config = this.quotaTypes[quotaType];\n       if (config.resetPeriod === 'none') return null;\n\n       const now = new Date();\n       const resetDate = new Date();\n\n       switch (config.resetPeriod) {\n         case 'daily':\n           resetDate.setDate(now.getDate() + 1);\n           resetDate.setHours(0, 0, 0, 0);\n           break;\n         case 'weekly':\n           resetDate.setDate(now.getDate() + (7 - now.getDay()));\n           resetDate.setHours(0, 0, 0, 0);\n           break;\n         case 'monthly':\n           resetDate.setMonth(now.getMonth() + 1, 1);\n           resetDate.setHours(0, 0, 0, 0);\n           break;\n         case 'yearly':\n           resetDate.setFullYear(now.getFullYear() + 1, 0, 1);\n           resetDate.setHours(0, 0, 0, 0);\n           break;\n       }\n\n       return resetDate;\n     }\n\n     getTTLForPeriod(resetPeriod) {\n       const resetDate = this.calculateResetDate({ resetPeriod });\n       return Math.ceil((resetDate.getTime() - Date.now()) / 1000);\n     }\n\n     async recordUsageAnalytics(userId, quotaType, amount) {\n       // Record usage for analytics and billing\n       const analyticsKey = `analytics:usage:${userId}:${quotaType}:${new Date().toISOString().split('T')[0]}`;\n       await this.redis.incrby(analyticsKey, amount);\n       await this.redis.expire(analyticsKey, 86400 * 90); // 90 days retention\n     }\n\n     // Middleware for quota checking\n     createQuotaMiddleware(quotaType, amountFn = () => 1) {\n       return async (req, res, next) => {\n         if (!req.user) {\n           return next(); // Skip quota check for unauthenticated requests\n         }\n\n         const amount = typeof amountFn === 'function' ? amountFn(req) : amountFn;\n         const result = await this.checkQuota(req.user.id, quotaType, amount);\n\n         // Set quota headers\n         res.set('X-Quota-Type', quotaType);\n         res.set('X-Quota-Limit', result.limit.toString());\n         res.set('X-Quota-Remaining', result.remaining.toString());\n         res.set('X-Quota-Used', result.usage.toString());\n         \n         if (result.resetDate) {\n           res.set('X-Quota-Reset', result.resetDate.toISOString());\n         }\n\n         if (!result.allowed) {\n           return res.status(429).json({\n             error: 'Quota exceeded',\n             quotaType: quotaType,\n             limit: result.limit,\n             usage: result.usage,\n             resetDate: result.resetDate\n           });\n         }\n\n         req.quota = result;\n         next();\n       };\n     }\n   }\n\n   module.exports = QuotaManager;\n   ```\n\n6. **Rate Limiting for Different Services**\n   - Implement service-specific rate limiting:\n\n   **Database Rate Limiting:**\n   ```javascript\n   // rate-limiters/database-rate-limiter.js\n   class DatabaseRateLimiter {\n     constructor(redis, pool) {\n       this.redis = redis;\n       this.pool = pool;\n       this.connectionLimiter = new Map();\n       this.queryLimiter = new Map();\n     }\n\n     // Limit concurrent database connections per user\n     async acquireConnection(userId) {\n       const key = `db:connections:${userId}`;\n       const maxConnections = await this.getMaxConnections(userId);\n       \n       const script = `\n         local key = KEYS[1]\n         local maxConnections = tonumber(ARGV[1])\n         local ttl = tonumber(ARGV[2])\n         \n         local current = redis.call('GET', key) or 0\n         current = tonumber(current)\n         \n         if current < maxConnections then\n           redis.call('INCR', key)\n           redis.call('EXPIRE', key, ttl)\n           return 1\n         else\n           return 0\n         end\n       `;\n\n       const allowed = await this.redis.eval(script, 1, key, maxConnections, 300); // 5 min TTL\n       \n       if (!allowed) {\n         throw new Error('Database connection limit exceeded');\n       }\n\n       return {\n         release: async () => {\n           await this.redis.decr(key);\n         }\n       };\n     }\n\n     // Rate limit expensive queries\n     async checkQueryLimit(userId, queryType, cost = 1) {\n       const key = `db:queries:${userId}:${queryType}`;\n       const windowMs = 60000; // 1 minute\n       const maxCost = await this.getMaxQueryCost(userId, queryType);\n\n       const script = `\n         local key = KEYS[1]\n         local windowMs = tonumber(ARGV[1])\n         local maxCost = tonumber(ARGV[2])\n         local cost = tonumber(ARGV[3])\n         local now = tonumber(ARGV[4])\n         \n         local windowStart = now - windowMs\n         \n         -- Remove old entries\n         redis.call('ZREMRANGEBYSCORE', key, 0, windowStart)\n         \n         -- Get current cost\n         local currentCost = 0\n         local entries = redis.call('ZRANGE', key, 0, -1, 'WITHSCORES')\n         for i = 2, #entries, 2 do\n           currentCost = currentCost + tonumber(entries[i])\n         end\n         \n         if currentCost + cost <= maxCost then\n           redis.call('ZADD', key, cost, now)\n           redis.call('EXPIRE', key, math.ceil(windowMs / 1000))\n           return {1, currentCost + cost, maxCost - currentCost - cost}\n         else\n           return {0, currentCost, maxCost - currentCost}\n         end\n       `;\n\n       const result = await this.redis.eval(\n         script, 1, key, windowMs, maxCost, cost, Date.now()\n       );\n\n       return {\n         allowed: result[0] === 1,\n         currentCost: result[1],\n         remaining: result[2]\n       };\n     }\n\n     async getMaxConnections(userId) {\n       // Get from user plan or use default\n       const user = await this.getUserPlan(userId);\n       const connectionLimits = {\n         free: 2,\n         basic: 5,\n         pro: 20,\n         enterprise: 100\n       };\n       return connectionLimits[user.plan] || 2;\n     }\n\n     async getMaxQueryCost(userId, queryType) {\n       const user = await this.getUserPlan(userId);\n       const costLimits = {\n         free: { select: 100, insert: 50, update: 30, delete: 10 },\n         basic: { select: 500, insert: 200, update: 100, delete: 50 },\n         pro: { select: 2000, insert: 1000, update: 500, delete: 200 },\n         enterprise: { select: 10000, insert: 5000, update: 2500, delete: 1000 }\n       };\n       return costLimits[user.plan]?.[queryType] || 10;\n     }\n   }\n   ```\n\n   **File Upload Rate Limiting:**\n   ```javascript\n   // rate-limiters/upload-rate-limiter.js\n   class UploadRateLimiter {\n     constructor(redis) {\n       this.redis = redis;\n     }\n\n     // Limit file upload size and frequency\n     async checkUploadLimit(userId, fileSize, fileType) {\n       const checks = await Promise.all([\n         this.checkFileSizeLimit(userId, fileSize),\n         this.checkUploadFrequency(userId),\n         this.checkStorageQuota(userId, fileSize),\n         this.checkFileTypeLimit(userId, fileType)\n       ]);\n\n       const failed = checks.find(check => !check.allowed);\n       if (failed) {\n         return failed;\n       }\n\n       // Record the upload\n       await this.recordUpload(userId, fileSize, fileType);\n\n       return { allowed: true, checks };\n     }\n\n     async checkFileSizeLimit(userId, fileSize) {\n       const user = await this.getUserPlan(userId);\n       const sizeLimits = {\n         free: 10 * 1024 * 1024,      // 10MB\n         basic: 50 * 1024 * 1024,     // 50MB\n         pro: 200 * 1024 * 1024,      // 200MB\n         enterprise: 1000 * 1024 * 1024 // 1GB\n       };\n\n       const maxSize = sizeLimits[user.plan] || sizeLimits.free;\n       const allowed = fileSize <= maxSize;\n\n       return {\n         allowed,\n         type: 'file_size',\n         current: fileSize,\n         limit: maxSize,\n         message: allowed ? null : `File size ${fileSize} exceeds limit of ${maxSize} bytes`\n       };\n     }\n\n     async checkUploadFrequency(userId) {\n       const key = `uploads:frequency:${userId}`;\n       const windowMs = 60000; // 1 minute\n       const maxUploads = await this.getMaxUploadsPerMinute(userId);\n\n       const current = await this.redis.incr(key);\n       if (current === 1) {\n         await this.redis.expire(key, Math.ceil(windowMs / 1000));\n       }\n\n       return {\n         allowed: current <= maxUploads,\n         type: 'upload_frequency',\n         current,\n         limit: maxUploads,\n         window: windowMs\n       };\n     }\n\n     async checkStorageQuota(userId, fileSize) {\n       const key = `storage:used:${userId}`;\n       const currentUsage = parseInt(await this.redis.get(key)) || 0;\n       const maxStorage = await this.getMaxStorage(userId);\n\n       const allowed = (currentUsage + fileSize) <= maxStorage;\n\n       return {\n         allowed,\n         type: 'storage_quota',\n         current: currentUsage + fileSize,\n         limit: maxStorage,\n         fileSize\n       };\n     }\n\n     async checkFileTypeLimit(userId, fileType) {\n       const allowedTypes = await this.getAllowedFileTypes(userId);\n       const allowed = allowedTypes.includes(fileType);\n\n       return {\n         allowed,\n         type: 'file_type',\n         fileType,\n         allowedTypes,\n         message: allowed ? null : `File type ${fileType} not allowed`\n       };\n     }\n\n     async recordUpload(userId, fileSize, fileType) {\n       const now = Date.now();\n       \n       // Update storage usage\n       await this.redis.incrby(`storage:used:${userId}`, fileSize);\n       \n       // Record upload in analytics\n       const analyticsKey = `analytics:uploads:${userId}:${new Date().toISOString().split('T')[0]}`;\n       await this.redis.hincrby(analyticsKey, 'count', 1);\n       await this.redis.hincrby(analyticsKey, 'bytes', fileSize);\n       await this.redis.expire(analyticsKey, 86400 * 30); // 30 days\n     }\n\n     createUploadMiddleware() {\n       return async (req, res, next) => {\n         if (!req.user) {\n           return res.status(401).json({ error: 'Authentication required' });\n         }\n\n         // Check if this is a file upload\n         if (!req.files || !req.files.length) {\n           return next();\n         }\n\n         for (const file of req.files) {\n           const result = await this.checkUploadLimit(\n             req.user.id,\n             file.size,\n             file.mimetype\n           );\n\n           if (!result.allowed) {\n             return res.status(429).json({\n               error: 'Upload limit exceeded',\n               ...result\n             });\n           }\n         }\n\n         next();\n       };\n     }\n   }\n   ```\n\n7. **Rate Limiting Dashboard and Analytics**\n   - Monitor and analyze rate limiting effectiveness:\n\n   **Rate Limiting Analytics:**\n   ```javascript\n   // analytics/rate-limit-analytics.js\n   class RateLimitAnalytics {\n     constructor(redis, database) {\n       this.redis = redis;\n       this.database = database;\n     }\n\n     async recordRateLimitHit(userId, endpoint, limitType, blocked) {\n       const timestamp = Date.now();\n       const date = new Date().toISOString().split('T')[0];\n\n       // Real-time metrics\n       const realtimeKey = `analytics:ratelimit:realtime:${limitType}`;\n       await this.redis.zadd(realtimeKey, timestamp, `${userId}:${endpoint}:${blocked}`);\n       await this.redis.expire(realtimeKey, 3600); // 1 hour\n\n       // Daily aggregates\n       const dailyKey = `analytics:ratelimit:daily:${date}:${limitType}`;\n       await this.redis.hincrby(dailyKey, 'total', 1);\n       if (blocked) {\n         await this.redis.hincrby(dailyKey, 'blocked', 1);\n       }\n       await this.redis.expire(dailyKey, 86400 * 30); // 30 days\n\n       // User-specific analytics\n       const userKey = `analytics:ratelimit:user:${userId}:${date}`;\n       await this.redis.hincrby(userKey, endpoint, 1);\n       if (blocked) {\n         await this.redis.hincrby(userKey, `${endpoint}:blocked`, 1);\n       }\n       await this.redis.expire(userKey, 86400 * 30);\n     }\n\n     async getRateLimitStats(timeRange = '24h') {\n       const now = Date.now();\n       const ranges = {\n         '1h': 3600000,\n         '24h': 86400000,\n         '7d': 604800000,\n         '30d': 2592000000\n       };\n\n       const rangeMs = ranges[timeRange] || ranges['24h'];\n       const startTime = now - rangeMs;\n\n       // Get realtime data for shorter ranges\n       if (rangeMs <= 3600000) {\n         return await this.getRealtimeStats(startTime, now);\n       }\n\n       // Get aggregated data for longer ranges\n       return await this.getAggregatedStats(startTime, now);\n     }\n\n     async getRealtimeStats(startTime, endTime) {\n       const limitTypes = ['general', 'auth', 'upload', 'api'];\n       const stats = {};\n\n       for (const limitType of limitTypes) {\n         const key = `analytics:ratelimit:realtime:${limitType}`;\n         const entries = await this.redis.zrangebyscore(key, startTime, endTime);\n         \n         let total = 0;\n         let blocked = 0;\n         const endpoints = {};\n\n         for (const entry of entries) {\n           const [userId, endpoint, isBlocked] = entry.split(':');\n           total++;\n           if (isBlocked === 'true') blocked++;\n\n           if (!endpoints[endpoint]) {\n             endpoints[endpoint] = { total: 0, blocked: 0 };\n           }\n           endpoints[endpoint].total++;\n           if (isBlocked === 'true') endpoints[endpoint].blocked++;\n         }\n\n         stats[limitType] = {\n           total,\n           blocked,\n           allowed: total - blocked,\n           blockRate: total > 0 ? (blocked / total) : 0,\n           endpoints\n         };\n       }\n\n       return stats;\n     }\n\n     async getTopBlockedEndpoints(timeRange = '24h', limit = 10) {\n       const stats = await this.getRateLimitStats(timeRange);\n       const endpointStats = [];\n\n       for (const [limitType, data] of Object.entries(stats)) {\n         for (const [endpoint, endpointData] of Object.entries(data.endpoints || {})) {\n           endpointStats.push({\n             endpoint,\n             limitType,\n             ...endpointData,\n             blockRate: endpointData.total > 0 ? (endpointData.blocked / endpointData.total) : 0\n           });\n         }\n       }\n\n       return endpointStats\n         .sort((a, b) => b.blocked - a.blocked)\n         .slice(0, limit);\n     }\n\n     async getUserRateLimitStats(userId, timeRange = '7d') {\n       const now = new Date();\n       const days = parseInt(timeRange.replace('d', ''));\n       const stats = [];\n\n       for (let i = 0; i < days; i++) {\n         const date = new Date(now - i * 86400000).toISOString().split('T')[0];\n         const key = `analytics:ratelimit:user:${userId}:${date}`;\n         const dayStats = await this.redis.hgetall(key);\n         \n         const endpoints = {};\n         for (const [field, value] of Object.entries(dayStats)) {\n           if (field.endsWith(':blocked')) {\n             const endpoint = field.replace(':blocked', '');\n             if (!endpoints[endpoint]) endpoints[endpoint] = { total: 0, blocked: 0 };\n             endpoints[endpoint].blocked = parseInt(value);\n           } else {\n             if (!endpoints[field]) endpoints[field] = { total: 0, blocked: 0 };\n             endpoints[field].total = parseInt(value);\n           }\n         }\n\n         stats.push({ date, endpoints });\n       }\n\n       return stats;\n     }\n\n     async generateRateLimitReport() {\n       const report = {\n         generatedAt: new Date().toISOString(),\n         summary: await this.getRateLimitStats('24h'),\n         topBlockedEndpoints: await this.getTopBlockedEndpoints('24h'),\n         trends: await this.getRateLimitTrends(),\n         recommendations: await this.generateRecommendations()\n       };\n\n       return report;\n     }\n\n     async generateRecommendations() {\n       const stats = await this.getRateLimitStats('24h');\n       const recommendations = [];\n\n       for (const [limitType, data] of Object.entries(stats)) {\n         if (data.blockRate > 0.1) { // >10% block rate\n           recommendations.push({\n             severity: 'high',\n             type: 'high_block_rate',\n             limitType,\n             blockRate: data.blockRate,\n             message: `High block rate (${(data.blockRate * 100).toFixed(1)}%) for ${limitType} rate limiter`,\n             suggestions: [\n               'Consider increasing rate limits for legitimate users',\n               'Implement user-specific rate limiting',\n               'Add rate limit exemptions for trusted IPs'\n             ]\n           });\n         }\n\n         if (data.total > 100000) { // High volume\n           recommendations.push({\n             severity: 'medium',\n             type: 'high_volume',\n             limitType,\n             volume: data.total,\n             message: `High request volume (${data.total}) detected for ${limitType}`,\n             suggestions: [\n               'Monitor for potential abuse patterns',\n               'Consider implementing adaptive rate limiting',\n               'Review capacity planning'\n             ]\n           });\n         }\n       }\n\n       return recommendations;\n     }\n   }\n\n   module.exports = RateLimitAnalytics;\n   ```\n\n8. **Rate Limiting Configuration Management**\n   - Dynamic rate limit configuration:\n\n   **Configuration Manager:**\n   ```javascript\n   // config/rate-limit-config.js\n   class RateLimitConfigManager {\n     constructor(redis, database) {\n       this.redis = redis;\n       this.database = database;\n       this.configCache = new Map();\n       this.setupDefaultConfigs();\n     }\n\n     setupDefaultConfigs() {\n       this.defaultConfigs = {\n         'api:general': {\n           windowMs: 900000, // 15 minutes\n           max: 1000,\n           algorithm: 'sliding-window',\n           skipSuccessfulRequests: false,\n           enabled: true\n         },\n         'api:auth': {\n           windowMs: 900000, // 15 minutes\n           max: 5,\n           algorithm: 'token-bucket',\n           skipSuccessfulRequests: true,\n           enabled: true\n         },\n         'api:upload': {\n           capacity: 10,\n           refillRate: 1,\n           refillPeriod: 10000,\n           algorithm: 'token-bucket',\n           enabled: true\n         },\n         'api:search': {\n           windowMs: 60000, // 1 minute\n           max: 100,\n           algorithm: 'sliding-window',\n           enabled: true\n         }\n       };\n     }\n\n     async getConfig(limiterId) {\n       // Check cache first\n       if (this.configCache.has(limiterId)) {\n         const cached = this.configCache.get(limiterId);\n         if (Date.now() - cached.timestamp < 300000) { // 5 min cache\n           return cached.config;\n         }\n       }\n\n       // Get from database\n       let config = await this.database.query(\n         'SELECT * FROM rate_limit_configs WHERE limiter_id = $1',\n         [limiterId]\n       );\n\n       if (config.rows.length === 0) {\n         // Use default config\n         config = this.defaultConfigs[limiterId] || this.defaultConfigs['api:general'];\n       } else {\n         config = config.rows[0].config;\n       }\n\n       // Cache the config\n       this.configCache.set(limiterId, {\n         config,\n         timestamp: Date.now()\n       });\n\n       return config;\n     }\n\n     async updateConfig(limiterId, newConfig, userId) {\n       // Validate config\n       const validationResult = this.validateConfig(newConfig);\n       if (!validationResult.valid) {\n         throw new Error(`Invalid config: ${validationResult.errors.join(', ')}`);\n       }\n\n       // Save to database\n       await this.database.query(`\n         INSERT INTO rate_limit_configs (limiter_id, config, updated_by, updated_at)\n         VALUES ($1, $2, $3, NOW())\n         ON CONFLICT (limiter_id) \n         DO UPDATE SET config = $2, updated_by = $3, updated_at = NOW()\n       `, [limiterId, JSON.stringify(newConfig), userId]);\n\n       // Clear cache\n       this.configCache.delete(limiterId);\n\n       // Notify other instances of config change\n       await this.redis.publish('rate-limit-config-update', JSON.stringify({\n         limiterId,\n         config: newConfig,\n         updatedBy: userId,\n         timestamp: Date.now()\n       }));\n\n       return newConfig;\n     }\n\n     validateConfig(config) {\n       const errors = [];\n\n       if (config.algorithm === 'sliding-window') {\n         if (!config.windowMs || config.windowMs < 1000) {\n           errors.push('windowMs must be at least 1000ms');\n         }\n         if (!config.max || config.max < 1) {\n           errors.push('max must be at least 1');\n         }\n       } else if (config.algorithm === 'token-bucket') {\n         if (!config.capacity || config.capacity < 1) {\n           errors.push('capacity must be at least 1');\n         }\n         if (!config.refillRate || config.refillRate < 1) {\n           errors.push('refillRate must be at least 1');\n         }\n         if (!config.refillPeriod || config.refillPeriod < 1000) {\n           errors.push('refillPeriod must be at least 1000ms');\n         }\n       } else {\n         errors.push('algorithm must be either sliding-window or token-bucket');\n       }\n\n       return {\n         valid: errors.length === 0,\n         errors\n       };\n     }\n\n     // A/B testing for rate limit configurations\n     async createABTest(limiterId, configA, configB, trafficSplit = 0.5) {\n       const testId = `ab-test-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;\n       \n       await this.database.query(`\n         INSERT INTO rate_limit_ab_tests \n         (test_id, limiter_id, config_a, config_b, traffic_split, created_at, status)\n         VALUES ($1, $2, $3, $4, $5, NOW(), 'active')\n       `, [testId, limiterId, JSON.stringify(configA), JSON.stringify(configB), trafficSplit]);\n\n       return testId;\n     }\n\n     async getABTestConfig(limiterId, userKey) {\n       const activeTest = await this.database.query(`\n         SELECT * FROM rate_limit_ab_tests \n         WHERE limiter_id = $1 AND status = 'active'\n         ORDER BY created_at DESC LIMIT 1\n       `, [limiterId]);\n\n       if (activeTest.rows.length === 0) {\n         return await this.getConfig(limiterId);\n       }\n\n       const test = activeTest.rows[0];\n       const hash = this.hashString(userKey);\n       const bucket = (hash % 100) / 100;\n\n       if (bucket < test.traffic_split) {\n         return test.config_a;\n       } else {\n         return test.config_b;\n       }\n     }\n\n     hashString(str) {\n       let hash = 0;\n       for (let i = 0; i < str.length; i++) {\n         const char = str.charCodeAt(i);\n         hash = ((hash << 5) - hash) + char;\n         hash = hash & hash; // Convert to 32-bit integer\n       }\n       return Math.abs(hash);\n     }\n\n     // Admin dashboard endpoints\n     async getAllConfigs() {\n       const configs = await this.database.query(`\n         SELECT limiter_id, config, updated_by, updated_at \n         FROM rate_limit_configs \n         ORDER BY updated_at DESC\n       `);\n\n       return configs.rows.map(row => ({\n         limiterId: row.limiter_id,\n         config: row.config,\n         updatedBy: row.updated_by,\n         updatedAt: row.updated_at\n       }));\n     }\n\n     async getConfigHistory(limiterId) {\n       const history = await this.database.query(`\n         SELECT config, updated_by, updated_at \n         FROM rate_limit_config_history \n         WHERE limiter_id = $1 \n         ORDER BY updated_at DESC \n         LIMIT 50\n       `, [limiterId]);\n\n       return history.rows;\n     }\n   }\n\n   module.exports = RateLimitConfigManager;\n   ```\n\n9. **Testing Rate Limits**\n   - Comprehensive rate limiting tests:\n\n   **Rate Limiting Test Suite:**\n   ```javascript\n   // tests/rate-limiting.test.js\n   const request = require('supertest');\n   const app = require('../app');\n   const Redis = require('ioredis');\n\n   describe('Rate Limiting', () => {\n     let redis;\n\n     beforeAll(async () => {\n       redis = new Redis(process.env.REDIS_TEST_URL);\n     });\n\n     afterEach(async () => {\n       // Clean up rate limiting keys\n       const keys = await redis.keys('*rate*');\n       if (keys.length > 0) {\n         await redis.del(...keys);\n       }\n     });\n\n     afterAll(async () => {\n       await redis.disconnect();\n     });\n\n     describe('General API Rate Limiting', () => {\n       test('should allow requests within limit', async () => {\n         for (let i = 0; i < 5; i++) {\n           const response = await request(app)\n             .get('/api/test')\n             .expect(200);\n\n           expect(response.headers).toHaveProperty('x-ratelimit-remaining');\n           expect(parseInt(response.headers['x-ratelimit-remaining'])).toBeGreaterThan(0);\n         }\n       });\n\n       test('should block requests exceeding limit', async () => {\n         // Make requests up to the limit\n         const limit = 10; // Assuming limit is 10 for test endpoint\n         \n         for (let i = 0; i < limit; i++) {\n           await request(app).get('/api/test').expect(200);\n         }\n\n         // Next request should be rate limited\n         const response = await request(app)\n           .get('/api/test')\n           .expect(429);\n\n         expect(response.body).toHaveProperty('error');\n         expect(response.body.error).toContain('Rate limit exceeded');\n       });\n\n       test('should include proper rate limit headers', async () => {\n         const response = await request(app)\n           .get('/api/test')\n           .expect(200);\n\n         expect(response.headers).toHaveProperty('x-ratelimit-limit');\n         expect(response.headers).toHaveProperty('x-ratelimit-remaining');\n         expect(response.headers).toHaveProperty('x-ratelimit-window');\n       });\n\n       test('should reset rate limit after window expires', async () => {\n         // Use a short window for testing\n         const shortWindowApp = createTestAppWithShortWindow(1000); // 1 second\n\n         // Exhaust the limit\n         await request(shortWindowApp).get('/api/test').expect(200);\n         await request(shortWindowApp).get('/api/test').expect(429);\n\n         // Wait for window to reset\n         await new Promise(resolve => setTimeout(resolve, 1100));\n\n         // Should allow requests again\n         await request(shortWindowApp).get('/api/test').expect(200);\n       });\n     });\n\n     describe('Authentication Rate Limiting', () => {\n       test('should limit failed login attempts', async () => {\n         const loginData = { email: 'test@example.com', password: 'wrongpassword' };\n\n         // Make several failed attempts\n         for (let i = 0; i < 5; i++) {\n           await request(app)\n             .post('/api/auth/login')\n             .send(loginData)\n             .expect(401);\n         }\n\n         // Next attempt should be rate limited\n         const response = await request(app)\n           .post('/api/auth/login')\n           .send(loginData)\n           .expect(429);\n\n         expect(response.body.error).toContain('Too many authentication attempts');\n       });\n\n       test('should not count successful logins against rate limit', async () => {\n         const loginData = { email: 'test@example.com', password: 'correctpassword' };\n\n         // Make successful login attempts\n         for (let i = 0; i < 3; i++) {\n           await request(app)\n             .post('/api/auth/login')\n             .send(loginData)\n             .expect(200);\n         }\n\n         // Should still allow more attempts\n         await request(app)\n           .post('/api/auth/login')\n           .send(loginData)\n           .expect(200);\n       });\n     });\n\n     describe('User-Specific Rate Limiting', () => {\n       test('should apply different limits based on user tier', async () => {\n         const freeUserToken = await getTestToken('free');\n         const proUserToken = await getTestToken('pro');\n\n         // Free user should have lower limits\n         const freeUserLimit = await findRateLimit(app, '/api/data', freeUserToken);\n         \n         // Pro user should have higher limits\n         const proUserLimit = await findRateLimit(app, '/api/data', proUserToken);\n\n         expect(proUserLimit).toBeGreaterThan(freeUserLimit);\n       });\n\n       test('should rate limit by user ID when authenticated', async () => {\n         const userToken = await getTestToken();\n         \n         // Make requests with user token\n         for (let i = 0; i < 10; i++) {\n           await request(app)\n             .get('/api/user/profile')\n             .set('Authorization', `Bearer ${userToken}`)\n             .expect(200);\n         }\n\n         // Should be rate limited\n         await request(app)\n           .get('/api/user/profile')\n           .set('Authorization', `Bearer ${userToken}`)\n           .expect(429);\n       });\n     });\n\n     describe('Quota Management', () => {\n       test('should enforce API call quotas', async () => {\n         const userToken = await getTestToken('basic'); // Basic plan has limited quota\n         \n         // Make requests up to quota limit\n         const quota = await getUserQuota('basic', 'api_calls');\n         \n         for (let i = 0; i < quota; i++) {\n           await request(app)\n             .get('/api/data')\n             .set('Authorization', `Bearer ${userToken}`)\n             .expect(200);\n         }\n\n         // Next request should exceed quota\n         const response = await request(app)\n           .get('/api/data')\n           .set('Authorization', `Bearer ${userToken}`)\n           .expect(429);\n\n         expect(response.body.error).toContain('Quota exceeded');\n         expect(response.body).toHaveProperty('quotaType', 'api_calls');\n       });\n\n       test('should include quota headers in responses', async () => {\n         const userToken = await getTestToken();\n         \n         const response = await request(app)\n           .get('/api/data')\n           .set('Authorization', `Bearer ${userToken}`)\n           .expect(200);\n\n         expect(response.headers).toHaveProperty('x-quota-limit');\n         expect(response.headers).toHaveProperty('x-quota-remaining');\n         expect(response.headers).toHaveProperty('x-quota-used');\n       });\n     });\n\n     describe('Rate Limiting Bypass', () => {\n       test('should bypass rate limits for internal requests', async () => {\n         // Make many requests with internal header\n         for (let i = 0; i < 100; i++) {\n           await request(app)\n             .get('/api/test')\n             .set('X-Internal-Request', 'true')\n             .expect(200);\n         }\n\n         // All should succeed\n       });\n\n       test('should bypass rate limits for whitelisted IPs', async () => {\n         // Configure test to use whitelisted IP\n         // This would depend on your specific implementation\n       });\n     });\n\n     // Helper functions\n     async function findRateLimit(app, endpoint, token) {\n       let requests = 0;\n       \n       while (requests < 1000) { // Safety limit\n         const response = await request(app)\n           .get(endpoint)\n           .set('Authorization', `Bearer ${token}`);\n         \n         requests++;\n         \n         if (response.status === 429) {\n           return requests - 1;\n         }\n       }\n       \n       return requests;\n     }\n\n     async function getTestToken(tier = 'free') {\n       // Implementation depends on your auth system\n       return 'test-token';\n     }\n\n     async function getUserQuota(plan, quotaType) {\n       const quotas = {\n         free: { api_calls: 100 },\n         basic: { api_calls: 1000 },\n         pro: { api_calls: 10000 }\n       };\n       return quotas[plan][quotaType];\n     }\n\n     function createTestAppWithShortWindow(windowMs) {\n       // Create a test app instance with short rate limit window\n       // Implementation depends on your app structure\n       return app;\n     }\n   });\n   ```\n\n10. **Production Monitoring and Alerting**\n    - Monitor rate limiting effectiveness:\n\n    **Rate Limiting Monitoring:**\n    ```javascript\n    // monitoring/rate-limit-monitor.js\n    class RateLimitMonitor {\n      constructor(redis, alertService) {\n        this.redis = redis;\n        this.alertService = alertService;\n        this.thresholds = {\n          highBlockRate: 0.15, // 15%\n          highVolume: 10000,    // requests per minute\n          quotaExhaustion: 0.9  // 90% quota usage\n        };\n      }\n\n      async startMonitoring(interval = 60000) {\n        setInterval(async () => {\n          await this.checkRateLimitHealth();\n        }, interval);\n      }\n\n      async checkRateLimitHealth() {\n        const metrics = await this.collectMetrics();\n        const alerts = [];\n\n        // Check for high block rates\n        for (const [limitType, data] of Object.entries(metrics)) {\n          if (data.blockRate > this.thresholds.highBlockRate) {\n            alerts.push({\n              type: 'high_block_rate',\n              limitType,\n              blockRate: data.blockRate,\n              message: `High block rate (${(data.blockRate * 100).toFixed(1)}%) for ${limitType}`,\n              severity: 'warning'\n            });\n          }\n\n          if (data.requestsPerMinute > this.thresholds.highVolume) {\n            alerts.push({\n              type: 'high_volume',\n              limitType,\n              volume: data.requestsPerMinute,\n              message: `High request volume (${data.requestsPerMinute}/min) for ${limitType}`,\n              severity: 'info'\n            });\n          }\n        }\n\n        // Check for quota exhaustion patterns\n        const quotaAlerts = await this.checkQuotaExhaustion();\n        alerts.push(...quotaAlerts);\n\n        // Send alerts\n        for (const alert of alerts) {\n          await this.alertService.sendAlert(alert);\n        }\n\n        // Store metrics for historical analysis\n        await this.storeMetrics(metrics);\n      }\n\n      async collectMetrics() {\n        const limitTypes = ['general', 'auth', 'upload', 'api'];\n        const metrics = {};\n        const now = Date.now();\n        const minuteAgo = now - 60000;\n\n        for (const limitType of limitTypes) {\n          const key = `analytics:ratelimit:realtime:${limitType}`;\n          const entries = await this.redis.zrangebyscore(key, minuteAgo, now);\n          \n          let total = 0;\n          let blocked = 0;\n\n          for (const entry of entries) {\n            const [userId, endpoint, isBlocked] = entry.split(':');\n            total++;\n            if (isBlocked === 'true') blocked++;\n          }\n\n          metrics[limitType] = {\n            total,\n            blocked,\n            allowed: total - blocked,\n            blockRate: total > 0 ? (blocked / total) : 0,\n            requestsPerMinute: total\n          };\n        }\n\n        return metrics;\n      }\n\n      async checkQuotaExhaustion() {\n        const alerts = [];\n        const quotaKeys = await this.redis.keys('quota:*:current');\n\n        for (const key of quotaKeys.slice(0, 100)) { // Limit to prevent overload\n          const [, userId, quotaType] = key.split(':');\n          const usage = parseInt(await this.redis.get(key)) || 0;\n          \n          // Get user's quota limit\n          const limit = await this.getUserQuotaLimit(userId, quotaType);\n          const usageRate = usage / limit;\n\n          if (usageRate > this.thresholds.quotaExhaustion) {\n            alerts.push({\n              type: 'quota_exhaustion',\n              userId,\n              quotaType,\n              usage,\n              limit,\n              usageRate,\n              message: `User ${userId} has used ${(usageRate * 100).toFixed(1)}% of ${quotaType} quota`,\n              severity: 'warning'\n            });\n          }\n        }\n\n        return alerts;\n      }\n\n      async storeMetrics(metrics) {\n        const timestamp = Date.now();\n        const metricsKey = `metrics:ratelimit:${timestamp}`;\n        \n        await this.redis.hmset(metricsKey, \n          'timestamp', timestamp,\n          'metrics', JSON.stringify(metrics)\n        );\n        await this.redis.expire(metricsKey, 86400 * 7); // 7 days retention\n      }\n\n      async generateHealthReport() {\n        const endTime = Date.now();\n        const startTime = endTime - 86400000; // 24 hours\n        \n        const metricKeys = await this.redis.keys('metrics:ratelimit:*');\n        const recentKeys = metricKeys.filter(key => {\n          const timestamp = parseInt(key.split(':')[2]);\n          return timestamp >= startTime && timestamp <= endTime;\n        });\n\n        const metrics = [];\n        for (const key of recentKeys) {\n          const data = await this.redis.hgetall(key);\n          metrics.push({\n            timestamp: parseInt(data.timestamp),\n            metrics: JSON.parse(data.metrics)\n          });\n        }\n\n        return {\n          period: { start: startTime, end: endTime },\n          dataPoints: metrics.length,\n          summary: this.calculateSummaryStats(metrics),\n          trends: this.calculateTrends(metrics),\n          recommendations: this.generateRecommendations(metrics)\n        };\n      }\n\n      calculateSummaryStats(metrics) {\n        if (metrics.length === 0) return {};\n\n        const summary = {};\n        const limitTypes = ['general', 'auth', 'upload', 'api'];\n\n        for (const limitType of limitTypes) {\n          const values = metrics.map(m => m.metrics[limitType]).filter(Boolean);\n          \n          if (values.length > 0) {\n            summary[limitType] = {\n              avgBlockRate: values.reduce((sum, v) => sum + v.blockRate, 0) / values.length,\n              avgVolume: values.reduce((sum, v) => sum + v.requestsPerMinute, 0) / values.length,\n              maxVolume: Math.max(...values.map(v => v.requestsPerMinute)),\n              totalRequests: values.reduce((sum, v) => sum + v.total, 0),\n              totalBlocked: values.reduce((sum, v) => sum + v.blocked, 0)\n            };\n          }\n        }\n\n        return summary;\n      }\n\n      calculateTrends(metrics) {\n        // Simple trend calculation - compare first and last hour\n        if (metrics.length < 2) return {};\n\n        const firstHour = metrics.slice(0, Math.min(60, metrics.length));\n        const lastHour = metrics.slice(-Math.min(60, metrics.length));\n\n        const trends = {};\n        const limitTypes = ['general', 'auth', 'upload', 'api'];\n\n        for (const limitType of limitTypes) {\n          const firstAvg = this.calculateAverage(firstHour, limitType, 'requestsPerMinute');\n          const lastAvg = this.calculateAverage(lastHour, limitType, 'requestsPerMinute');\n          \n          if (firstAvg > 0) {\n            trends[limitType] = {\n              volumeChange: ((lastAvg - firstAvg) / firstAvg) * 100,\n              direction: lastAvg > firstAvg ? 'increasing' : 'decreasing'\n            };\n          }\n        }\n\n        return trends;\n      }\n\n      calculateAverage(metrics, limitType, field) {\n        const values = metrics\n          .map(m => m.metrics[limitType]?.[field])\n          .filter(v => v !== undefined);\n        \n        return values.length > 0 ? values.reduce((sum, v) => sum + v, 0) / values.length : 0;\n      }\n\n      generateRecommendations(metrics) {\n        const recommendations = [];\n        const summary = this.calculateSummaryStats(metrics);\n\n        for (const [limitType, stats] of Object.entries(summary)) {\n          if (stats.avgBlockRate > 0.1) {\n            recommendations.push({\n              priority: 'high',\n              type: 'increase_limits',\n              limitType,\n              current: `${(stats.avgBlockRate * 100).toFixed(1)}% block rate`,\n              suggestion: `Consider increasing rate limits for ${limitType} - high block rate indicates legitimate users may be affected`\n            });\n          }\n\n          if (stats.avgVolume > 1000 && stats.avgBlockRate < 0.01) {\n            recommendations.push({\n              priority: 'medium',\n              type: 'optimize_performance',\n              limitType,\n              current: `${stats.avgVolume.toFixed(0)} requests/min`,\n              suggestion: `High volume with low block rate for ${limitType} - consider optimizing backend performance`\n            });\n          }\n        }\n\n        return recommendations;\n      }\n    }\n\n    module.exports = RateLimitMonitor;\n    ```"
              }
            ],
            "skills": []
          },
          {
            "name": "commands-project-task-management",
            "description": "Commands for task management and project tracking",
            "source": "./plugins/commands-project-task-management",
            "category": "commands",
            "version": "1.0.0",
            "author": {
              "name": "BuildWithClaude Community",
              "url": "https://github.com/davepoon/buildwithclaude"
            },
            "install_commands": [
              "/plugin marketplace add davepoon/buildwithclaude",
              "/plugin install commands-project-task-management@buildwithclaude"
            ],
            "signals": {
              "stars": 2229,
              "forks": 239,
              "pushed_at": "2026-01-12T10:19:01Z",
              "created_at": "2025-07-25T02:26:45Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/add-package",
                "description": "Add and configure new project dependencies",
                "path": "plugins/commands-project-task-management/commands/add-package.md",
                "frontmatter": {
                  "description": "Add and configure new project dependencies",
                  "category": "project-task-management",
                  "argument-hint": "[package-name] [type]"
                },
                "content": "# Add Package to Workspace\n\nAdd and configure new project dependencies\n\n## Instructions\n\n1. **Package Definition and Analysis**\n   - Parse package name and type from arguments: `$ARGUMENTS` (format: name [type])\n   - If no arguments provided, prompt for package name and type\n   - Validate package name follows workspace naming conventions\n   - Determine package type: library, application, tool, shared, service, component-library\n   - Check for naming conflicts with existing packages\n\n2. **Package Structure Creation**\n   - Create package directory in appropriate workspace location (packages/, apps/, libs/)\n   - Set up standard package directory structure based on type:\n     - `src/` for source code\n     - `tests/` or `__tests__/` for testing\n     - `docs/` for package documentation\n     - `examples/` for usage examples (if library)\n     - `public/` for static assets (if application)\n   - Create package-specific configuration files\n\n3. **Package Configuration Setup**\n   - Generate package.json with proper metadata:\n     - Name following workspace conventions\n     - Version aligned with workspace strategy\n     - Dependencies and devDependencies\n     - Scripts for build, test, lint, dev\n     - Entry points and exports configuration\n   - Configure TypeScript (tsconfig.json) extending workspace settings\n   - Set up package-specific linting and formatting rules\n\n4. **Package Type-Specific Setup**\n   - **Library**: Configure build system, export definitions, API documentation\n   - **Application**: Set up routing, environment configuration, build optimization\n   - **Tool**: Configure CLI setup, binary exports, command definitions\n   - **Shared**: Set up common utilities, type definitions, shared constants\n   - **Service**: Configure server setup, API routes, database connections\n   - **Component Library**: Set up Storybook, component exports, styling system\n\n5. **Workspace Integration**\n   - Register package in workspace configuration (nx.json, lerna.json, etc.)\n   - Configure package dependencies and peer dependencies\n   - Set up cross-package imports and references\n   - Configure workspace-wide build order and dependencies\n   - Add package to workspace scripts and task runners\n\n6. **Development Environment**\n   - Configure package-specific development server (if applicable)\n   - Set up hot reloading and watch mode\n   - Configure debugging and source maps\n   - Set up development proxy and API mocking (if needed)\n   - Configure environment variable management\n\n7. **Testing Infrastructure**\n   - Set up testing framework configuration for the package\n   - Create initial test files and examples\n   - Configure test coverage reporting\n   - Set up package-specific test scripts\n   - Configure integration testing with other workspace packages\n\n8. **Build and Deployment**\n   - Configure build system for the package type\n   - Set up build artifacts and output directories\n   - Configure bundling and optimization\n   - Set up package publishing configuration (if library)\n   - Configure deployment scripts (if application)\n\n9. **Documentation and Examples**\n   - Create package README with installation and usage instructions\n   - Set up API documentation generation\n   - Create usage examples and demos\n   - Document package architecture and design decisions\n   - Add package to workspace documentation\n\n10. **Validation and Integration Testing**\n    - Verify package builds successfully\n    - Test package installation and imports\n    - Validate workspace dependency resolution\n    - Test development workflow and hot reloading\n    - Verify CI/CD pipeline includes new package\n    - Test cross-package functionality and integration"
              },
              {
                "name": "/create-command",
                "description": "Create a new command following existing patterns and organizational structure",
                "path": "plugins/commands-project-task-management/commands/create-command.md",
                "frontmatter": {
                  "description": "Create a new command following existing patterns and organizational structure",
                  "category": "project-task-management",
                  "allowed-tools": "Read, Write, Edit, LS, Glob"
                },
                "content": "Create a new command that follows the existing patterns and organizational structure in this project.\n\n## ANALYZE EXISTING COMMANDS\n\n1. First, study the existing commands in the `.claude/commands/` directory to understand:\n   - Common patterns and structures\n   - Naming conventions\n   - Documentation styles\n   - Command organization\n\n2. Use MCP tools to explore the codebase and understand:\n   - Project structure\n   - Existing functionality\n   - Code patterns\n   - Dependencies\n\n## UNDERSTAND THE REQUEST\n\n3. Analyze the user's request to determine:\n   - The command's purpose and functionality\n   - Which category it belongs to\n   - Similar existing commands to reference\n   - Required inputs and outputs\n\n## SELECT APPROPRIATE PATTERNS\n\n4. Based on your analysis, choose the most appropriate pattern:\n   - Simple execution commands\n   - File generation commands\n   - Analysis and reporting commands\n   - Multi-step workflow commands\n\n## DETERMINE COMMAND LOCATION\n\n5. Place the command in the appropriate category directory:\n   - `code-analysis-testing/` - For code analysis, testing, and quality assurance\n   - `ci-deployment/` - For CI/CD and deployment related commands\n   - `context-loading-priming/` - For loading context and priming commands\n   - `documentation-changelogs/` - For documentation and changelog commands\n   - `project-task-management/` - For project and task management commands\n   - `version-control-git/` - For version control and Git operations\n   - `miscellaneous/` - For commands that don't fit other categories\n\n## PLAN SUPPORTING RESOURCES\n\n6. Consider what supporting resources might be needed:\n   - Templates or example files\n   - Configuration files\n   - Documentation updates\n   - Related commands that might work together\n\n## CREATE THE COMMAND\n\n7. Write the command following these guidelines:\n   - Use clear, descriptive names\n   - Include comprehensive instructions\n   - Follow existing formatting patterns\n   - Add appropriate examples\n   - Include error handling considerations\n\n## HUMAN REVIEW\n\n8. Present your analysis and proposed command to the human for review before implementation, including:\n   - Command purpose and location\n   - Key patterns you're following\n   - Any assumptions you're making\n   - Questions about specific requirements"
              },
              {
                "name": "/create-feature",
                "description": "Scaffold new feature with boilerplate code",
                "path": "plugins/commands-project-task-management/commands/create-feature.md",
                "frontmatter": {
                  "description": "Scaffold new feature with boilerplate code",
                  "category": "project-task-management",
                  "argument-hint": "1. **Feature Planning**",
                  "allowed-tools": "Bash(git *), Write"
                },
                "content": "# Create Feature Command\n\nScaffold new feature with boilerplate code\n\n## Instructions\n\nFollow this systematic approach to create a new feature: **$ARGUMENTS**\n\n1. **Feature Planning**\n   - Define the feature requirements and acceptance criteria\n   - Break down the feature into smaller, manageable tasks\n   - Identify affected components and potential impact areas\n   - Plan the API/interface design before implementation\n\n2. **Research and Analysis**\n   - Study existing codebase patterns and conventions\n   - Identify similar features for consistency\n   - Research external dependencies or libraries needed\n   - Review any relevant documentation or specifications\n\n3. **Architecture Design**\n   - Design the feature architecture and data flow\n   - Plan database schema changes if needed\n   - Define API endpoints and contracts\n   - Consider scalability and performance implications\n\n4. **Environment Setup**\n   - Create a new feature branch: `git checkout -b feature/$ARGUMENTS`\n   - Ensure development environment is up to date\n   - Install any new dependencies required\n   - Set up feature flags if applicable\n\n5. **Implementation Strategy**\n   - Start with core functionality and build incrementally\n   - Follow the project's coding standards and patterns\n   - Implement proper error handling and validation\n   - Use dependency injection and maintain loose coupling\n\n6. **Database Changes (if applicable)**\n   - Create migration scripts for schema changes\n   - Ensure backward compatibility\n   - Plan for rollback scenarios\n   - Test migrations on sample data\n\n7. **API Development**\n   - Implement API endpoints with proper HTTP status codes\n   - Add request/response validation\n   - Implement proper authentication and authorization\n   - Document API contracts and examples\n\n8. **Frontend Implementation (if applicable)**\n   - Create reusable components following project patterns\n   - Implement responsive design and accessibility\n   - Add proper state management\n   - Handle loading and error states\n\n9. **Testing Implementation**\n   - Write unit tests for core business logic\n   - Create integration tests for API endpoints\n   - Add end-to-end tests for user workflows\n   - Test error scenarios and edge cases\n\n10. **Security Considerations**\n    - Implement proper input validation and sanitization\n    - Add authorization checks for sensitive operations\n    - Review for common security vulnerabilities\n    - Ensure data protection and privacy compliance\n\n11. **Performance Optimization**\n    - Optimize database queries and indexes\n    - Implement caching where appropriate\n    - Monitor memory usage and optimize algorithms\n    - Consider lazy loading and pagination\n\n12. **Documentation**\n    - Add inline code documentation and comments\n    - Update API documentation\n    - Create user documentation if needed\n    - Update project README if applicable\n\n13. **Code Review Preparation**\n    - Run all tests and ensure they pass\n    - Run linting and formatting tools\n    - Check for code coverage and quality metrics\n    - Perform self-review of the changes\n\n14. **Integration Testing**\n    - Test feature integration with existing functionality\n    - Verify feature flags work correctly\n    - Test deployment and rollback procedures\n    - Validate monitoring and logging\n\n15. **Commit and Push**\n    - Create atomic commits with descriptive messages\n    - Follow conventional commit format if project uses it\n    - Push feature branch: `git push origin feature/$ARGUMENTS`\n\n16. **Pull Request Creation**\n    - Create PR with comprehensive description\n    - Include screenshots or demos if applicable\n    - Add appropriate labels and reviewers\n    - Link to any related issues or specifications\n\n17. **Quality Assurance**\n    - Coordinate with QA team for testing\n    - Address any bugs or issues found\n    - Verify accessibility and usability requirements\n    - Test on different environments and browsers\n\n18. **Deployment Planning**\n    - Plan feature rollout strategy\n    - Set up monitoring and alerting\n    - Prepare rollback procedures\n    - Schedule deployment and communication\n\nRemember to maintain code quality, follow project conventions, and prioritize user experience throughout the development process."
              },
              {
                "name": "/create-jtbd",
                "description": "Create a Jobs to be Done (JTBD) document for a product feature focusing on user needs",
                "path": "plugins/commands-project-task-management/commands/create-jtbd.md",
                "frontmatter": {
                  "description": "Create a Jobs to be Done (JTBD) document for a product feature focusing on user needs",
                  "category": "project-task-management",
                  "argument-hint": "<feature description> [output-path]",
                  "allowed-tools": "Write, TodoWrite"
                },
                "content": "Create a comprehensive Jobs to be Done (JTBD) document based on the feature description provided.\n\n## Instructions:\n1. Parse the arguments:\n   - First argument: Feature/product description (required)\n   - Second argument: Output path (optional, defaults to `JTBD.md` in current directory)\n\n2. Create a well-structured JTBD document that includes:\n\n   **Core Job Statement**:\n   - When [situation]\n   - I want to [motivation]\n   - So I can [expected outcome]\n\n   **Job Map**:\n   - Define: What users need to understand first\n   - Locate: What inputs/resources users need\n   - Prepare: How users get ready\n   - Confirm: How users verify readiness\n   - Execute: The core action\n   - Monitor: How users track progress\n   - Modify: How users make adjustments\n   - Conclude: How users finish the job\n\n   **Context & Circumstances**:\n   - Functional job aspects\n   - Emotional job aspects\n   - Social job aspects\n\n   **Success Criteria**:\n   - How users measure success\n   - What outcomes they expect\n   - Time/effort constraints\n\n   **Pain Points**:\n   - Current frustrations\n   - Workarounds users employ\n   - Unmet needs\n\n   **Competing Solutions**:\n   - How users currently solve this\n   - Alternative approaches\n   - Why current solutions fall short\n\n3. Focus on:\n   - User motivations (not features)\n   - Jobs that remain stable over time\n   - Outcomes users want to achieve\n   - Context that triggers the job\n\n4. Use the TodoWrite tool to track JTBD sections as you complete them\n\n## Example usage:\n- `/create-jtbd \"Help developers find and fix bugs faster\"`\n- `/create-jtbd \"Enable teams to collaborate on documents in real-time\" collab-JTBD.md`\n\nFeature description: $ARGUMENTS"
              },
              {
                "name": "/create-prd",
                "description": "Create a Product Requirements Document (PRD) for a product feature",
                "path": "plugins/commands-project-task-management/commands/create-prd.md",
                "frontmatter": {
                  "description": "Create a Product Requirements Document (PRD) for a product feature",
                  "category": "project-task-management",
                  "argument-hint": "<feature description> [output-path]",
                  "allowed-tools": "Write, TodoWrite"
                },
                "content": "Create a comprehensive Product Requirements Document (PRD) based on the feature description provided.\n\n## Instructions:\n1. Parse the arguments:\n   - First argument: Feature description (required)\n   - Second argument: Output path (optional, defaults to `PRD.md` in current directory)\n\n2. Create a well-structured PRD that includes:\n   - **Executive Summary**: Brief overview of the feature\n   - **Problem Statement**: What problem does this solve?\n   - **Objectives**: Clear, measurable goals\n   - **User Stories**: Who are the users and what are their needs?\n   - **Functional Requirements**: What the feature must do\n   - **Non-Functional Requirements**: Performance, security, usability standards\n   - **Success Metrics**: How will we measure success?\n   - **Assumptions & Constraints**: Any limitations or dependencies\n   - **Out of Scope**: What this PRD does NOT cover\n\n3. Focus on:\n   - User needs and business value (not technical implementation)\n   - Clear, measurable objectives\n   - Specific acceptance criteria\n   - User personas and their journey\n\n4. Use the TodoWrite tool to track PRD sections as you complete them\n\n## Example usage:\n- `/create-prd \"Add dark mode toggle to settings\"`\n- `/create-prd \"Implement user authentication with SSO\" auth-PRD.md`\n\nFeature description: $ARGUMENTS"
              },
              {
                "name": "/create-prp",
                "description": "Create a comprehensive Product Requirement Prompt (PRP) with research and context gathering",
                "path": "plugins/commands-project-task-management/commands/create-prp.md",
                "frontmatter": {
                  "description": "Create a comprehensive Product Requirement Prompt (PRP) with research and context gathering",
                  "category": "project-task-management",
                  "argument-hint": "<feature_description>",
                  "allowed-tools": "Read, Write, WebSearch"
                },
                "content": "# Product Requirement Prompt (PRP) Creation\n\nYou will help the user create a comprehensive Product Requirement Prompt (PRP) for: $ARGUMENTS\n\n## What is a PRP?\n\nA Product Requirement Prompt (PRP) is a detailed document that defines the requirements, context, and specifications for a feature or product. It serves as a comprehensive guide for implementation, ensuring all stakeholders have a clear understanding of what needs to be built, why it's needed, and how success will be measured.\n\n## Research Process\n\nBefore creating the PRP, conduct thorough research to gather all necessary context:\n\n### 1. **Web Research**\n   - Search for best practices related to the feature/product\n   - Research similar implementations and solutions\n   - Look for relevant library documentation\n   - Find example implementations on platforms like GitHub, StackOverflow\n   - Identify industry standards and patterns\n   - Gather competitive analysis if applicable\n\n### 2. **Documentation Review**\n   - Check for any existing project documentation\n   - Identify documentation gaps that need to be addressed\n   - Review any related technical specifications\n   - Look for architectural decision records (ADRs) if present\n\n### 3. **Codebase Exploration** (if applicable)\n   - Identify relevant files and directories that provide implementation context\n   - Look for existing patterns that should be followed\n   - Find similar features that could serve as references\n   - Check for any technical constraints or dependencies\n\n### 4. **Requirements Gathering**\n   - Clarify any ambiguous requirements with the user\n   - Identify both functional and non-functional requirements\n   - Determine performance, security, and scalability needs\n   - Establish clear acceptance criteria\n\n## PRP Template Structure\n\nCreate a comprehensive PRP following this structure:\n\n### 1. Executive Summary\n- **Feature Name**: [Clear, descriptive name]\n- **Version**: [Document version]\n- **Date**: [Creation date]\n- **Author**: [Author/Team]\n- **Status**: [Draft/Review/Approved]\n- **Brief Description**: [1-2 paragraph overview of the feature]\n\n### 2. Problem Statement\n- **Current Situation**: What problem exists today?\n- **Impact**: Who is affected and how?\n- **Opportunity**: What opportunity does solving this create?\n- **Constraints**: What limitations exist?\n\n### 3. Goals & Objectives\n- **Primary Goal**: The main objective to achieve\n- **Secondary Goals**: Additional benefits or objectives\n- **Success Metrics**: How success will be measured\n- **Key Performance Indicators (KPIs)**: Specific, measurable outcomes\n\n### 4. User Stories & Use Cases\n- **Target Users**: Who will use this feature?\n- **User Stories**: As a [user type], I want [goal] so that [benefit]\n- **Use Case Scenarios**: Detailed walkthrough of user interactions\n- **Edge Cases**: Unusual or boundary scenarios to consider\n\n### 5. Functional Requirements\n- **Core Features**: Must-have functionality\n- **Optional Features**: Nice-to-have functionality\n- **Feature Priority**: P0 (Critical), P1 (Important), P2 (Nice to have)\n- **Dependencies**: Other features or systems this depends on\n\n### 6. Non-Functional Requirements\n- **Performance**: Response time, throughput, resource usage\n- **Security**: Authentication, authorization, data protection\n- **Scalability**: Expected load and growth projections\n- **Reliability**: Uptime requirements, error handling\n- **Usability**: User experience requirements\n- **Compatibility**: Browser, device, system requirements\n\n### 7. Technical Specifications\n- **Architecture Overview**: High-level design approach\n- **Technology Stack**: Languages, frameworks, libraries to use\n- **Data Models**: Database schemas, API contracts\n- **Integration Points**: External systems or APIs\n- **Technical Constraints**: Known limitations or requirements\n\n### 8. Implementation Plan\n- **Phases**: Break down into manageable phases\n- **Milestones**: Key deliverables and checkpoints\n- **Timeline**: Estimated duration for each phase\n- **Resources**: Team members, tools, infrastructure needed\n- **Dependencies**: External dependencies and blockers\n\n### 9. Risk Assessment\n- **Technical Risks**: Potential technical challenges\n- **Business Risks**: Market, competition, or strategic risks\n- **Mitigation Strategies**: How to address each risk\n- **Contingency Plans**: Backup approaches if primary plan fails\n\n### 10. Success Criteria & Acceptance Tests\n- **Acceptance Criteria**: Specific conditions that must be met\n- **Test Scenarios**: Key test cases to validate functionality\n- **Performance Benchmarks**: Measurable performance targets\n- **Quality Gates**: Checkpoints before moving to next phase\n\n### 11. Documentation & Training\n- **Documentation Needs**: User guides, API docs, technical docs\n- **Training Requirements**: Who needs training and what type\n- **Knowledge Transfer**: How knowledge will be shared\n\n### 12. Post-Launch Considerations\n- **Monitoring**: What metrics to track after launch\n- **Maintenance**: Ongoing maintenance requirements\n- **Future Enhancements**: Potential future improvements\n- **Deprecation Plan**: If replacing existing functionality\n\n## Context Prioritization\n\nWhen creating the PRP, prioritize including:\n1. **Specific, actionable requirements** over vague descriptions\n2. **Measurable success criteria** that can be objectively evaluated\n3. **Clear scope boundaries** to prevent scope creep\n4. **Realistic timelines** based on complexity and resources\n5. **Risk mitigation strategies** for identified challenges\n\n## Interaction with User\n\nThroughout the PRP creation process:\n1. Ask clarifying questions when requirements are ambiguous\n2. Confirm assumptions before including them in the PRP\n3. Request additional context when needed\n4. Validate technical approaches with the user\n5. Ensure alignment on priorities and constraints\n\n## Final Output\n\nThe completed PRP should be:\n- **Comprehensive**: Cover all aspects of the feature/product\n- **Clear**: Use precise language, avoid ambiguity\n- **Actionable**: Provide enough detail for implementation\n- **Measurable**: Include specific success criteria\n- **Realistic**: Consider constraints and limitations\n- **Maintainable**: Easy to update as requirements evolve\n\nBegin by asking the user for any specific context or requirements they want to emphasize, then proceed with research and PRP creation based on the feature description provided."
              },
              {
                "name": "/init-project",
                "description": "Initialize new project with essential structure",
                "path": "plugins/commands-project-task-management/commands/init-project.md",
                "frontmatter": {
                  "description": "Initialize new project with essential structure",
                  "category": "project-task-management",
                  "argument-hint": "Specify project name and type",
                  "allowed-tools": "Edit"
                },
                "content": "# Initialize New Project\n\nInitialize new project with essential structure\n\n## Instructions\n\n1. **Project Analysis and Setup**\n   - Parse the project type and framework from arguments: `$ARGUMENTS`\n   - If no arguments provided, analyze current directory and ask user for project type and framework\n   - Create project directory structure if needed\n   - Validate that the chosen framework is appropriate for the project type\n\n2. **Base Project Structure**\n   - Create essential directories (src/, tests/, docs/, etc.)\n   - Initialize git repository with proper .gitignore for the project type\n   - Create README.md with project description and setup instructions\n   - Set up proper file structure based on project type and framework\n\n3. **Framework-Specific Configuration**\n   - **Web/React**: Set up React with TypeScript, Vite/Next.js, ESLint, Prettier\n   - **Web/Vue**: Configure Vue 3 with TypeScript, Vite, ESLint, Prettier\n   - **Web/Angular**: Set up Angular CLI project with TypeScript and testing\n   - **API/Express**: Create Express.js server with TypeScript, middleware, and routing\n   - **API/FastAPI**: Set up FastAPI with Python, Pydantic models, and async support\n   - **Mobile/React Native**: Configure React Native with navigation and development tools\n   - **Desktop/Electron**: Set up Electron with renderer and main process structure\n   - **CLI/Node**: Create Node.js CLI with commander.js and proper packaging\n   - **Library/NPM**: Set up library with TypeScript, rollup/webpack, and publishing config\n\n4. **Development Environment Setup**\n   - Configure package manager (npm, yarn, pnpm) with proper package.json\n   - Set up TypeScript configuration with strict mode and path mapping\n   - Configure linting with ESLint and language-specific rules\n   - Set up code formatting with Prettier and pre-commit hooks\n   - Add EditorConfig for consistent coding standards\n\n5. **Testing Infrastructure**\n   - Install and configure testing framework (Jest, Vitest, Pytest, etc.)\n   - Set up test directory structure and example tests\n   - Configure code coverage reporting\n   - Add testing scripts to package.json/makefile\n\n6. **Build and Development Tools**\n   - Configure build system (Vite, webpack, rollup, etc.)\n   - Set up development server with hot reloading\n   - Configure environment variable management\n   - Add build optimization and bundling\n\n7. **CI/CD Pipeline**\n   - Create GitHub Actions workflow for testing and deployment\n   - Set up automated testing on pull requests\n   - Configure automated dependency updates with Dependabot\n   - Add status badges to README\n\n8. **Documentation and Quality**\n   - Generate comprehensive README with installation and usage instructions\n   - Create CONTRIBUTING.md with development guidelines\n   - Set up API documentation generation (JSDoc, Sphinx, etc.)\n   - Add code quality badges and shields\n\n9. **Security and Best Practices**\n   - Configure security scanning with npm audit or similar\n   - Set up dependency vulnerability checking\n   - Add security headers for web applications\n   - Configure environment-specific security settings\n\n10. **Project Validation**\n    - Verify all dependencies install correctly\n    - Run initial build to ensure configuration is working\n    - Execute test suite to validate testing setup\n    - Check linting and formatting rules are applied\n    - Validate that development server starts successfully\n    - Create initial commit with proper project structure"
              },
              {
                "name": "/milestone-tracker",
                "description": "Track and monitor project milestone progress",
                "path": "plugins/commands-project-task-management/commands/milestone-tracker.md",
                "frontmatter": {
                  "description": "Track and monitor project milestone progress",
                  "category": "project-task-management"
                },
                "content": "# Milestone Tracker\n\nTrack and monitor project milestone progress\n\n## Instructions\n\n1. **Check Available Tools**\n   - Verify Linear MCP server connection\n   - Check GitHub CLI availability\n   - Test git repository access\n   - Ensure required permissions\n\n2. **Gather Milestone Data**\n   - Query Linear for project milestones and roadmap items\n   - Fetch GitHub milestones and their associated issues\n   - Analyze git tags for historical release patterns\n   - Review project documentation for roadmap information\n   - Collect all active and upcoming milestones\n\n3. **Analyze Milestone Progress**\n   For each milestone:\n   - Count completed vs. total tasks\n   - Calculate percentage complete\n   - Measure velocity trends\n   - Identify blocking issues\n   - Track time remaining\n\n4. **Perform Predictive Analysis**\n   - Calculate burn-down rate from historical data\n   - Project completion dates based on velocity\n   - Factor in team capacity and holidays\n   - Identify critical path items\n   - Assess confidence levels for predictions\n\n5. **Risk Assessment**\n   Evaluate each milestone for:\n   - Schedule risk (falling behind)\n   - Scope risk (expanding requirements)\n   - Resource risk (team availability)\n   - Dependency risk (blocked by others)\n   - Technical risk (unknowns)\n\n6. **Generate Milestone Report**\n   Create comprehensive report showing:\n   - Milestone timeline visualization\n   - Progress indicators for each milestone\n   - Predicted completion dates with confidence\n   - Risk heat map\n   - Recommended actions for at-risk items\n\n7. **Track Dependencies**\n   - Map inter-milestone dependencies\n   - Identify cross-team dependencies\n   - Highlight critical path\n   - Show dependency impact on schedule\n\n8. **Provide Recommendations**\n   Based on analysis:\n   - Suggest scope adjustments\n   - Recommend resource reallocation\n   - Propose timeline changes\n   - Identify quick wins\n   - Highlight blockers needing attention\n\n## Prerequisites\n- Git repository access\n- Linear MCP server connection (preferred)\n- GitHub milestones or project boards\n- Historical velocity data\n\n## Command Flow\n\n### 1. Milestone Discovery\n```\n1. Check Linear for project milestones/roadmap items\n2. Scan GitHub for milestone definitions\n3. Analyze git tags for release history\n4. Review README/docs for project roadmap\n5. Ask user for additional context if needed\n```\n\n### 2. Comprehensive Milestone Analysis\n\n#### Data Collection Sources\n```\nLinear/Project Management:\n- Milestone definitions and due dates\n- Associated tasks and dependencies\n- Team assignments and capacity\n- Progress percentages\n- Blocker status\n\nGitHub:\n- Milestone issue tracking\n- PR associations\n- Release tags and dates\n- Branch protection rules\n\nGit History:\n- Commit velocity trends\n- Feature branch lifecycle\n- Release cadence patterns\n- Contributor availability\n```\n\n### 3. Milestone Status Report\n\n```markdown\n# Milestone Tracking Report - [Project Name]\nGenerated: [Date]\n\n## Executive Summary\n- Total Milestones: [Count]\n- On Track: [Count] ([%])\n- At Risk: [Count] ([%])\n- Blocked: [Count] ([%])\n- Completed: [Count] ([%])\n\n## Milestone Dashboard\n\n###  Current Sprint Milestone: [Name]\n**Target Date**: [Date] (in [X] days)\n**Confidence Level**: [High/Medium/Low]\n\nProgress:  80% Complete\n\n**Key Deliverables**:\n-  User Authentication System\n-  Database Schema Migration  \n-  API Integration (75%)\n-  Documentation Update (0%)\n-  Performance Testing (Blocked)\n\n**Health Indicators**:\n- Velocity Trend:  Declining (-15%)\n- Burn Rate:  Behind Schedule\n- Risk Level: Medium\n- Team Capacity: 85% allocated\n\n###  Upcoming Milestones\n\n#### Q1 2024: Beta Release\n**Target**: March 15, 2024\n**Status**:  At Risk\n\nTimeline:\n```\nJan  60%\nFeb  0%\nMar  0%\n```\n\n**Dependencies**:\n- Alpha Testing Complete \n- Security Audit (In Progress)\n- Marketing Website (Not Started)\n\n**Predicted Completion**: March 22 (+7 days)\n**Confidence**: 65%\n\n#### Q2 2024: Public Launch\n**Target**: June 1, 2024\n**Status**:  On Track\n\nKey Milestones Path:\n1. Beta Release  2. User Feedback Integration  3. Production Deployment\n\n**Critical Path Items**:\n- Infrastructure Setup (Start: April 1)\n- Load Testing (Duration: 2 weeks)\n- Security Certification (Lead time: 4 weeks)\n```\n\n### 4. Predictive Analytics\n\n```markdown\n## Completion Predictions\n\n### Machine Learning Model Predictions\nBased on historical data and current velocity:\n\n**Beta Release Probability**:\n- On Time (Mar 15): 35%\n- 1 Week Delay: 45%\n- 2+ Week Delay: 20%\n\n**Factors Influencing Prediction**:\n1. Current velocity 15% below plan\n2. 2 critical dependencies unresolved\n3. Team member on leave next week\n4. Historical milestone success rate: 72%\n\n### Monte Carlo Simulation Results\nRunning 1000 simulations based on task estimates:\n\n```\nCompletion Date Distribution:\nMar 10-15:  20%\nMar 16-22:  40%\nMar 23-31:  30%\nApril+   :  10%\n\nP50 Date: March 19\nP90 Date: March 28\n```\n\n### Risk-Adjusted Timeline\nRecommended buffer: +5 days\nConfident delivery date: March 20\n```\n\n### 5. Dependency Tracking\n\n```markdown\n## Milestone Dependencies\n\n### Critical Path Analysis\n```mermaid\ngantt\n    title Critical Path to Beta Release\n    dateFormat  YYYY-MM-DD\n    section Backend\n    API Development    :done,    api, 2024-01-01, 30d\n    Database Migration :active,  db,  2024-02-01, 14d\n    Security Audit     :         sec, after db, 21d\n    section Frontend  \n    UI Components      :done,    ui,  2024-01-15, 21d\n    Integration        :active,  int, after ui, 14d\n    User Testing       :         ut,  after int, 7d\n    section Deploy\n    Infrastructure     :         inf, 2024-03-01, 7d\n    Beta Deployment    :crit,    dep, after sec ut inf, 3d\n```\n\n### Dependency Risk Matrix\n| Dependency | Impact | Likelihood | Mitigation |\n|------------|--------|------------|------------|\n| Security Audit Delay | High | Medium | Start process early |\n| API Rate Limits | Medium | Low | Implement caching |\n| Team Availability | High | High | Cross-training needed |\n```\n\n### 6. Early Warning System\n\n```markdown\n##  Milestone Alerts\n\n### Immediate Attention Required\n\n**1. Performance Testing Blocked**\n- Blocker: Test environment not available\n- Impact: Beta release at risk\n- Days blocked: 3\n- Recommended action: Escalate to DevOps\n\n**2. Documentation Lagging**\n- Progress: 0% (Should be 40%)\n- Impact: User onboarding compromised\n- Resource needed: Technical writer\n- Recommended action: Reassign team member\n\n### Trending Concerns\n\n**Velocity Decline**\n- 3-week trend: -15%\n- Projected impact: 1-week delay\n- Root cause: Increased bug fixes\n- Recommendation: Add bug buffer to estimates\n\n**Scope Creep Detected**\n- New features added: 3\n- Impact on timeline: +5 days\n- Recommendation: Defer to next milestone\n```\n\n### 7. Actionable Recommendations\n\n```markdown\n## Recommended Actions\n\n### This Week\n1. **Unblock Performance Testing**\n   - Owner: [Name]\n   - Action: Provision test environment\n   - Due: Friday EOD\n\n2. **Documentation Sprint**\n   - Owner: [Team]\n   - Action: Dedicate 2 days to docs\n   - Target: 50% completion\n\n### Next Sprint\n1. **Velocity Recovery Plan**\n   - Reduce scope by 20%\n   - Focus on critical path items\n   - Defer nice-to-have features\n\n2. **Risk Mitigation**\n   - Add 5-day buffer to timeline\n   - Daily standups for blocked items\n   - Escalation path defined\n\n### Process Improvements\n1. Set up automated milestone tracking\n2. Weekly milestone health reviews\n3. Dependency check before sprint planning\n```\n\n## Error Handling\n\n### No Milestone Data\n```\n\"No milestones found in Linear or GitHub.\n\nTo set up milestone tracking:\n1. Define milestones in Linear/GitHub\n2. Associate tasks with milestones\n3. Set target completion dates\n\nWould you like me to:\n- Help create milestone structure?\n- Import from project documentation?\n- Set up basic milestones?\"\n```\n\n### Insufficient Historical Data\n```\n\"Limited historical data for predictions.\n\nAvailable data: [X] weeks\nRecommended: 12+ weeks for accurate predictions\n\nCurrent analysis based on:\n- Available velocity data\n- Industry benchmarks\n- Task complexity estimates\n\nConfidence level: Low-Medium\"\n```\n\n## Interactive Features\n\n### What-If Analysis\n```\n\"Explore scenario planning:\n\n1. What if we add 2 more developers?\n    Completion date: -5 days\n    Confidence: +15%\n\n2. What if we cut scope by 20%?\n    Completion date: -8 days\n    Risk level: Low\n\n3. What if key developer is unavailable?\n    Completion date: +12 days\n    Risk level: Critical\"\n```\n\n### Milestone Optimization\n```\n\"Optimization opportunities detected:\n\n1. **Parallelize Tasks**\n   - Tasks A & B can run simultaneously\n   - Time saved: 1 week\n\n2. **Resource Reallocation**\n   - Move developer from Task C to Critical Path\n   - Impact: 3 days earlier completion\n\n3. **Scope Adjustment**\n   - Defer features X, Y to next milestone\n   - Impact: Meet original deadline\"\n```\n\n## Export & Integration Options\n\n1. **Gantt Chart Export** (Mermaid/PNG/PDF)\n2. **Executive Dashboard** (HTML/PowerBI)\n3. **Status Updates** (Slack/Email/Confluence)\n4. **Risk Register** (Excel/Linear/Jira)\n5. **Calendar Integration** (ICS/Google/Outlook)\n\n## Automation Capabilities\n\n```\n\"Set up automated milestone monitoring:\n\n1. Daily health checks at 9 AM\n2. Weekly trend reports on Fridays\n3. Alert when milestones go off-track\n4. Slack notifications for blockers\n5. Auto-create Linear tasks for risks\n\nConfigure automation? [Y/N]\"\n```\n\n## Best Practices\n\n1. **Update Frequently**: Daily progress updates improve predictions\n2. **Track Dependencies**: Most delays come from dependencies\n3. **Buffer Realistically**: Use historical data for buffers\n4. **Communicate Early**: Flag risks as soon as detected\n5. **Focus on Critical Path**: Not all tasks equally impact timeline\n6. **Learn from History**: Analyze past milestone performance"
              },
              {
                "name": "/pac-configure",
                "description": "Configure and initialize a project following the Product as Code specification for structured, version-controlled product management",
                "path": "plugins/commands-project-task-management/commands/pac-configure.md",
                "frontmatter": {
                  "description": "Configure and initialize a project following the Product as Code specification for structured, version-controlled product management",
                  "category": "project-task-management",
                  "argument-hint": "Specify configuration settings"
                },
                "content": "# Configure PAC (Product as Code) Project\n\nConfigure and initialize a project following the Product as Code specification for structured, version-controlled product management\n\n## Instructions\n\n1. **Analyze Project Context**\n   - Check if the current directory is a git repository\n   - Verify if a PAC configuration already exists (look for epic-*.yaml or ticket-*.yaml files)\n   - Parse any arguments provided: `$ARGUMENTS`\n   - If PAC files exist, analyze them to understand current structure\n\n2. **Interactive Setup (if no existing PAC config)**\n   - Ask user for project details:\n     - Project name\n     - Project description\n     - Primary product owner\n     - Default ticket assignee\n     - Initial epic name\n   - Validate inputs and confirm with user before proceeding\n\n3. **Create PAC Directory Structure**\n   - Create `.pac/` directory if it doesn't exist\n   - Create subdirectories:\n     - `.pac/epics/` - for epic definitions\n     - `.pac/tickets/` - for ticket definitions\n     - `.pac/templates/` - for reusable templates\n   - Add `.pac/README.md` explaining the structure and PAC specification\n\n4. **Generate PAC Configuration Files**\n   - Create `.pac/pac.config.yaml` with:\n     ```yaml\n     apiVersion: productascode.org/v0.1.0\n     kind: Configuration\n     metadata:\n       project: \"[project-name]\"\n       owner: \"[owner-name]\"\n       created: \"[timestamp]\"\n     spec:\n       defaults:\n         assignee: \"[default-assignee]\"\n         epic_prefix: \"epic-\"\n         ticket_prefix: \"ticket-\"\n       validation:\n         enforce_unique_ids: true\n         require_acceptance_criteria: true\n     ```\n\n5. **Create Initial Epic Template**\n   - Generate `.pac/templates/epic-template.yaml`:\n     ```yaml\n     apiVersion: productascode.org/v0.1.0\n     kind: Epic\n     metadata:\n       id: \"epic-[name]\"\n       name: \"[Epic Name]\"\n       created: \"[timestamp]\"\n       owner: \"[owner]\"\n     spec:\n       description: |\n         [Epic description]\n       scope: |\n         [Scope definition]\n       success_criteria:\n         - [Criterion 1]\n         - [Criterion 2]\n       tickets: []\n     ```\n\n6. **Create Initial Ticket Template**\n   - Generate `.pac/templates/ticket-template.yaml`:\n     ```yaml\n     apiVersion: productascode.org/v0.1.0\n     kind: Ticket\n     metadata:\n       id: \"ticket-[name]\"\n       name: \"[Ticket Name]\"\n       epic: \"[parent-epic-id]\"\n       created: \"[timestamp]\"\n       assignee: \"[assignee]\"\n     spec:\n       description: |\n         [Ticket description]\n       type: \"feature\"\n       status: \"backlog\"\n       priority: \"medium\"\n       acceptance_criteria:\n         - [ ] [Criterion 1]\n         - [ ] [Criterion 2]\n       tasks:\n         - [ ] [Task 1]\n         - [ ] [Task 2]\n     ```\n\n7. **Create First Epic and Ticket**\n   - Based on user input, create first epic in `.pac/epics/`\n   - Create an initial ticket linked to the epic\n   - Use proper naming convention and unique IDs\n   - Set appropriate timestamps\n\n8. **Set Up Validation Scripts**\n   - Create `.pac/scripts/validate.sh` to check PAC compliance:\n     - Verify YAML syntax\n     - Check required fields\n     - Validate unique IDs\n     - Ensure epic-ticket relationships are valid\n   - Make script executable\n\n9. **Configure Git Integration**\n   - Add PAC-specific entries to `.gitignore` if needed:\n     ```\n     .pac/tmp/\n     .pac/cache/\n     *.pac.lock\n     ```\n   - Create git hook for pre-commit PAC validation (optional)\n\n10. **Generate PAC Documentation**\n    - Create `.pac/GUIDE.md` with:\n      - Quick start guide for team members\n      - Common PAC workflows\n      - How to create new epics and tickets\n      - How to update ticket status\n      - Link to full PAC specification\n\n11. **Create Helper Commands**\n    - Generate `.pac/scripts/new-epic.sh` for creating new epics\n    - Generate `.pac/scripts/new-ticket.sh` for creating new tickets\n    - Include prompts for required fields and validation\n\n12. **Final Validation and Summary**\n    - Run validation script on created files\n    - Display summary of created structure\n    - Show next steps:\n      - How to create new epics: `cp .pac/templates/epic-template.yaml .pac/epics/epic-[name].yaml`\n      - How to create new tickets: `cp .pac/templates/ticket-template.yaml .pac/tickets/ticket-[name].yaml`\n      - How to validate PAC files: `.pac/scripts/validate.sh`\n    - Suggest integrating with CI/CD for automatic validation\n\n## Arguments\n\n- `--minimal`: Create minimal PAC structure without templates and scripts\n- `--epic-name <name>`: Specify initial epic name\n- `--owner <name>`: Specify product owner name\n- `--no-git`: Skip git integration setup\n\n## Example Usage\n\n```\n/project:pac-configure\n/project:pac-configure --epic-name \"user-authentication\" --owner \"john.doe\"\n/project:pac-configure --minimal\n```"
              },
              {
                "name": "/pac-create-epic",
                "description": "Create a new epic following the Product as Code specification with guided workflow",
                "path": "plugins/commands-project-task-management/commands/pac-create-epic.md",
                "frontmatter": {
                  "description": "Create a new epic following the Product as Code specification with guided workflow",
                  "category": "project-task-management",
                  "argument-hint": "Specify epic details",
                  "allowed-tools": "Write"
                },
                "content": "# Create PAC Epic\n\nCreate a new epic following the Product as Code specification with guided workflow\n\n## Instructions\n\n1. **Validate PAC Configuration**\n   - Check if `.pac/` directory exists\n   - Verify PAC configuration file exists at `.pac/pac.config.yaml`\n   - If not configured, suggest running `/project:pac-configure` first\n   - Parse arguments: `$ARGUMENTS`\n\n2. **Epic Information Gathering**\n   - If arguments provided, parse:\n     - `--name <name>`: Epic name\n     - `--description <desc>`: Epic description\n     - `--owner <owner>`: Epic owner\n     - `--scope <scope>`: Scope definition\n   - For missing information, prompt user interactively:\n     - Epic ID (suggest format: epic-[kebab-case-name])\n     - Epic name (human-readable)\n     - Epic owner (default from config if available)\n     - Epic description (multi-line)\n     - Scope definition (what's included/excluded)\n     - Success criteria (at least 2-3 items)\n\n3. **Generate Epic ID**\n   - If not provided, generate from epic name:\n     - Convert to lowercase\n     - Replace spaces with hyphens\n     - Remove special characters\n     - Prefix with \"epic-\"\n   - Validate uniqueness against existing epics\n\n4. **Create Epic Structure**\n   - Generate epic YAML following PAC v0.1.0 specification:\n     ```yaml\n     apiVersion: productascode.org/v0.1.0\n     kind: Epic\n     metadata:\n       id: \"[generated-epic-id]\"\n       name: \"[Epic Name]\"\n       created: \"[current-timestamp]\"\n       updated: \"[current-timestamp]\"\n       owner: \"[owner-email-or-name]\"\n       labels:\n         status: \"active\"\n         priority: \"medium\"\n     spec:\n       description: |\n         [Multi-line description]\n       \n       scope: |\n         [Scope definition]\n       \n       success_criteria:\n         - [Criterion 1]\n         - [Criterion 2]\n         - [Criterion 3]\n       \n       constraints:\n         - [Any constraints or limitations]\n       \n       dependencies:\n         - [Dependencies on other epics/systems]\n       \n       tickets: []  # Will be populated as tickets are created\n     ```\n\n5. **Validate Epic Content**\n   - Check all required fields are present\n   - Validate apiVersion matches specification\n   - Ensure metadata has required identifiers\n   - Verify success criteria has at least one item\n   - Check YAML syntax is valid\n\n6. **Save Epic File**\n   - Determine filename: `.pac/epics/[epic-id].yaml`\n   - Check if file already exists\n   - If exists, ask user to confirm overwrite\n   - Write epic content to file\n   - Set appropriate file permissions\n\n7. **Create Epic Directory Structure**\n   - Create `.pac/epics/[epic-id]/` directory for epic-specific docs\n   - Add `.pac/epics/[epic-id]/README.md` with epic overview\n   - Create `.pac/epics/[epic-id]/tickets/` for future ticket links\n\n8. **Update PAC Index**\n   - If `.pac/index.yaml` exists, add epic entry:\n     ```yaml\n     epics:\n       - id: \"[epic-id]\"\n         name: \"[Epic Name]\"\n         status: \"active\"\n         created: \"[timestamp]\"\n         ticket_count: 0\n     ```\n\n9. **Git Integration**\n   - If in git repository:\n     - Add new epic file to git\n     - Create branch `pac/[epic-id]` for epic work\n     - Prepare commit message:\n       ```\n       feat(pac): add epic [epic-id]\n       \n       - Epic: [Epic Name]\n       - Owner: [Owner]\n       - Success Criteria: [count] items defined\n       ```\n\n10. **Generate Epic Summary**\n    - Display created epic details:\n      - Epic ID and location\n      - Success criteria summary\n      - Next steps for creating tickets\n    - Show helpful commands:\n      - Create ticket: `/project:pac-create-ticket --epic [epic-id]`\n      - View epic: `cat .pac/epics/[epic-id].yaml`\n      - Validate: `.pac/scripts/validate.sh .pac/epics/[epic-id].yaml`\n\n## Arguments\n\n- `--name <name>`: Epic name (required if not interactive)\n- `--description <description>`: Epic description\n- `--owner <owner>`: Epic owner email or name\n- `--scope <scope>`: Scope definition\n- `--success-criteria <criteria>`: Comma-separated success criteria\n- `--priority <priority>`: Priority level (low/medium/high/critical)\n- `--no-git`: Skip git integration\n\n## Example Usage\n\n```\n/project:pac-create-epic\n/project:pac-create-epic --name \"User Authentication System\"\n/project:pac-create-epic --name \"Payment Integration\" --owner \"john@example.com\" --priority high\n```"
              },
              {
                "name": "/pac-create-ticket",
                "description": "Create a new ticket within an epic following the Product as Code specification",
                "path": "plugins/commands-project-task-management/commands/pac-create-ticket.md",
                "frontmatter": {
                  "description": "Create a new ticket within an epic following the Product as Code specification",
                  "category": "project-task-management",
                  "argument-hint": "Specify ticket details",
                  "allowed-tools": "Write"
                },
                "content": "# Create PAC Ticket\n\nCreate a new ticket within an epic following the Product as Code specification\n\n## Instructions\n\n1. **Validate PAC Environment**\n   - Verify `.pac/` directory exists\n   - Check PAC configuration at `.pac/pac.config.yaml`\n   - If not configured, suggest running `/project:pac-configure`\n   - Parse arguments from: `$ARGUMENTS`\n\n2. **Epic Selection**\n   - If `--epic <epic-id>` provided, validate epic exists\n   - Otherwise, list available epics from `.pac/epics/`:\n     - Show epic ID, name, and ticket count\n     - Allow user to select epic interactively\n   - Load selected epic to understand context\n\n3. **Ticket Information Gathering**\n   - Parse command arguments:\n     - `--name <name>`: Ticket name\n     - `--type <type>`: feature/bug/task/spike\n     - `--description <desc>`: Ticket description\n     - `--assignee <assignee>`: Assigned developer\n     - `--priority <priority>`: low/medium/high/critical\n   - For missing required fields, prompt interactively:\n     - Ticket name (required)\n     - Ticket type (default: feature)\n     - Description (multi-line)\n     - Assignee (default from config)\n     - Priority (default: medium)\n     - Initial status (default: backlog)\n\n4. **Generate Ticket ID**\n   - Create ID format: `ticket-[epic-short-name]-[sequence]`\n   - Example: `ticket-auth-001`, `ticket-auth-002`\n   - Check existing tickets in epic to determine sequence\n   - Ensure uniqueness across all tickets\n\n5. **Define Acceptance Criteria**\n   - Prompt for acceptance criteria (at least 2 items)\n   - Format as checkbox list:\n     ```yaml\n     acceptance_criteria:\n       - [ ] User can successfully authenticate\n       - [ ] Session persists across page refreshes\n       - [ ] Invalid credentials show error message\n     ```\n\n6. **Define Implementation Tasks**\n   - Prompt for implementation tasks\n   - Break down work into actionable items:\n     ```yaml\n     tasks:\n       - [ ] Create authentication service\n       - [ ] Implement login form component\n       - [ ] Add session management\n       - [ ] Write unit tests\n       - [ ] Update documentation\n     ```\n\n7. **Create Ticket Structure**\n   - Generate ticket YAML following PAC v0.1.0:\n     ```yaml\n     apiVersion: productascode.org/v0.1.0\n     kind: Ticket\n     metadata:\n       id: \"[generated-ticket-id]\"\n       sequence: [number]\n       name: \"[Ticket Name]\"\n       epic: \"[parent-epic-id]\"\n       created: \"[timestamp]\"\n       updated: \"[timestamp]\"\n       assignee: \"[assignee]\"\n       labels:\n         component: \"[relevant-component]\"\n         effort: \"[size-estimate]\"\n     spec:\n       description: |\n         [Detailed description]\n         \n       type: \"[feature/bug/task/spike]\"\n       status: \"[backlog/in-progress/review/done]\"\n       priority: \"[low/medium/high/critical]\"\n       \n       acceptance_criteria:\n         - [ ] [Criterion 1]\n         - [ ] [Criterion 2]\n         \n       tasks:\n         - [ ] [Task 1]\n         - [ ] [Task 2]\n         \n       technical_notes: |\n         [Any technical considerations]\n         \n       dependencies:\n         - [Other ticket IDs if any]\n     ```\n\n8. **Estimate Effort**\n   - Prompt for effort estimation:\n     - Story points (1, 2, 3, 5, 8, 13)\n     - T-shirt size (XS, S, M, L, XL)\n     - Time estimate (hours/days)\n   - Add to metadata labels\n\n9. **Link to Epic**\n   - Update parent epic file to include ticket reference:\n     ```yaml\n     spec:\n       tickets:\n         - id: \"[ticket-id]\"\n           name: \"[ticket-name]\"\n           status: \"backlog\"\n           assignee: \"[assignee]\"\n     ```\n\n10. **Save Ticket File**\n    - Save to: `.pac/tickets/[ticket-id].yaml`\n    - Create symbolic link in epic directory:\n      `.pac/epics/[epic-id]/tickets/[ticket-id].yaml`\n    - Validate file was created successfully\n\n11. **Create Branch (Optional)**\n    - If `--create-branch` flag or git integration enabled:\n      - Create branch: `feature/[ticket-id]`\n      - Include branch name in ticket metadata\n      - Show git commands for switching to branch\n\n12. **Generate Ticket Summary**\n    - Display created ticket information:\n      - Ticket ID and file location\n      - Epic association\n      - Assignee and priority\n      - Task count and acceptance criteria count\n    - Show next actions:\n      - Start work: `git checkout -b feature/[ticket-id]`\n      - Update status: `/project:pac-update-ticket --id [ticket-id] --status in-progress`\n      - View ticket: `cat .pac/tickets/[ticket-id].yaml`\n\n## Arguments\n\n- `--epic <epic-id>`: Parent epic ID (required)\n- `--name <name>`: Ticket name\n- `--type <type>`: Ticket type (feature/bug/task/spike)\n- `--description <description>`: Ticket description\n- `--assignee <assignee>`: Assigned developer\n- `--priority <priority>`: Priority level\n- `--create-branch`: Automatically create git branch\n- `--template <template>`: Use custom ticket template\n\n## Example Usage\n\n```\n/project:pac-create-ticket --epic epic-authentication\n/project:pac-create-ticket --epic epic-payment --name \"Implement Stripe integration\" --type feature\n/project:pac-create-ticket --epic epic-ui --assignee jane@example.com --priority high --create-branch\n```"
              },
              {
                "name": "/pac-update-status",
                "description": "Update ticket status and track progress in Product as Code workflow",
                "path": "plugins/commands-project-task-management/commands/pac-update-status.md",
                "frontmatter": {
                  "description": "Update ticket status and track progress in Product as Code workflow",
                  "category": "project-task-management",
                  "argument-hint": "Specify status update details",
                  "allowed-tools": "Read, Write"
                },
                "content": "# Update PAC Ticket Status\n\nUpdate ticket status and track progress in Product as Code workflow\n\n## Instructions\n\n1. **Parse Command Arguments**\n   - Extract arguments from: `$ARGUMENTS`\n   - Required: `--ticket <ticket-id>` or select interactively\n   - Optional: `--status <status>`, `--assignee <assignee>`, `--comment <comment>`\n   - Validate `.pac/` directory exists\n\n2. **Ticket Selection**\n   - If ticket ID provided, validate it exists\n   - Otherwise, show interactive ticket selector:\n     - List tickets grouped by status\n     - Show: ID, Name, Current Status, Assignee\n     - Filter by epic if `--epic` flag provided\n     - Allow search by ticket name\n\n3. **Load Current Ticket State**\n   - Read ticket file from `.pac/tickets/[ticket-id].yaml`\n   - Display current ticket information:\n     - Name and description\n     - Current status and assignee\n     - Epic association\n     - Acceptance criteria progress\n     - Task completion status\n\n4. **Status Transition Validation**\n   - Current status determines valid transitions:\n     - `backlog`  `in-progress`, `cancelled`\n     - `in-progress`  `review`, `blocked`, `backlog`\n     - `review`  `done`, `in-progress`\n     - `blocked`  `in-progress`, `cancelled`\n     - `done`  (no transitions, warn if attempting)\n     - `cancelled`  `backlog` (for resurrection)\n   - Prevent invalid status transitions\n   - Show available transitions if invalid status provided\n\n5. **Update Ticket Status**\n   - If new status provided and valid:\n     - Update `spec.status` field\n     - Update `metadata.updated` timestamp\n     - Add status change to history (if tracking)\n   - Special handling for status transitions:\n     - `backlog  in-progress`: \n       - Prompt for assignee if not set\n       - Suggest creating feature branch\n     - `in-progress  review`:\n       - Check if all tasks are marked complete\n       - Warn if acceptance criteria not met\n     - `review  done`:\n       - Verify all acceptance criteria checked\n       - Update completion timestamp\n\n6. **Update Additional Fields**\n   - If `--assignee` provided:\n     - Update `metadata.assignee`\n     - Add assignment history entry\n   - If `--comment` provided:\n     - Add to ticket comments/notes section\n     - Include timestamp and current user\n\n7. **Task and Criteria Progress**\n   - If moving to `in-progress`, prompt to review tasks\n   - Allow marking tasks as complete:\n     ```yaml\n     tasks:\n       - [x] Create authentication service\n       - [x] Implement login form component\n       - [ ] Add session management\n       - [ ] Write unit tests\n     ```\n   - Calculate and display completion percentage\n\n8. **Update Parent Epic**\n   - Load parent epic from `.pac/epics/[epic-id].yaml`\n   - Update ticket entry in epic's ticket list:\n     ```yaml\n     tickets:\n       - id: \"[ticket-id]\"\n         name: \"[ticket-name]\"\n         status: \"[new-status]\"  # Update this\n         assignee: \"[assignee]\"\n         updated: \"[timestamp]\"\n     ```\n   - If ticket is done, increment epic completion metrics\n\n9. **Git Integration**\n   - If status changes to `in-progress` and no branch exists:\n     - Suggest: `git checkout -b feature/[ticket-id]`\n   - If status changes to `review`:\n     - Suggest creating pull request\n     - Generate PR description from ticket details\n   - If status changes to `done`:\n     - Suggest merging and branch cleanup\n\n10. **Generate Status Report**\n    - Show status update summary:\n      ```\n      Ticket Status Updated\n      ====================\n      \n      Ticket: [ticket-id] - [ticket-name]\n      Epic: [epic-name]\n      \n      Status: [old-status]  [new-status]\n      Assignee: [assignee]\n      Updated: [timestamp]\n      \n      Progress:\n      - Tasks: [completed]/[total] ([percentage]%)\n      - Criteria: [met]/[total]\n      \n      Next Actions:\n      - [Suggested next steps based on new status]\n      ```\n\n11. **Notification Hooks**\n    - If `.pac/hooks/` directory exists:\n      - Execute `status-change.sh` if present\n      - Pass ticket ID, old status, new status as arguments\n    - Could integrate with Slack, email, or project management tools\n\n12. **Validation and Save**\n    - Validate updated YAML structure\n    - Create backup of original ticket file\n    - Save updated ticket file\n    - Run PAC validation on updated file\n    - If validation fails, restore from backup\n\n## Arguments\n\n- `--ticket <ticket-id>`: Ticket ID to update (or select interactively)\n- `--status <status>`: New status (backlog/in-progress/review/blocked/done/cancelled)\n- `--assignee <assignee>`: Update assignee\n- `--comment <comment>`: Add comment to ticket\n- `--epic <epic-id>`: Filter tickets by epic (for interactive selection)\n- `--force`: Force status change even if validation warnings exist\n\n## Example Usage\n\n```\n/project:pac-update-status --ticket ticket-auth-001 --status in-progress\n/project:pac-update-status --ticket ticket-ui-003 --status review --comment \"Ready for code review\"\n/project:pac-update-status  # Interactive mode\n/project:pac-update-status --epic epic-payment --status done\n```"
              },
              {
                "name": "/pac-validate",
                "description": "Validate Product as Code project structure and files for specification compliance",
                "path": "plugins/commands-project-task-management/commands/pac-validate.md",
                "frontmatter": {
                  "description": "Validate Product as Code project structure and files for specification compliance",
                  "category": "project-task-management",
                  "argument-hint": "Specify validation rules or targets"
                },
                "content": "# Validate PAC Structure\n\nValidate Product as Code project structure and files for specification compliance\n\n## Instructions\n\n1. **Initial Environment Check**\n   - Verify `.pac/` directory exists\n   - Check for PAC configuration file at `.pac/pac.config.yaml`\n   - Parse arguments: `$ARGUMENTS`\n   - Determine validation scope (single file, directory, or entire project)\n\n2. **Configuration Validation**\n   - Load and validate `.pac/pac.config.yaml`:\n     - Check `apiVersion` format (must be semantic version)\n     - Verify `kind` is \"Configuration\"\n     - Validate required metadata fields\n     - Check defaults section has valid values\n   - Report any missing or invalid configuration\n\n3. **Directory Structure Validation**\n   - Verify required directories exist:\n     - `.pac/epics/` - Epic definitions\n     - `.pac/tickets/` - Ticket definitions\n     - `.pac/templates/` - Templates (optional but recommended)\n   - Check file permissions are correct\n   - Ensure no orphaned files outside expected structure\n\n4. **Epic File Validation**\n   - For each file in `.pac/epics/`:\n     - Verify YAML syntax is valid\n     - Check `apiVersion: productascode.org/v0.1.0`\n     - Verify `kind: Epic`\n     - Validate required metadata fields:\n       - `id` (must be unique)\n       - `name` (non-empty string)\n       - `created` (valid timestamp)\n       - `owner` (non-empty string)\n     - Validate spec section:\n       - `description` exists\n       - `success_criteria` has at least one item\n       - `tickets` array is properly formatted\n   - Track all epic IDs for cross-reference validation\n\n5. **Ticket File Validation**\n   - For each file in `.pac/tickets/`:\n     - Verify YAML syntax is valid\n     - Check `apiVersion: productascode.org/v0.1.0`\n     - Verify `kind: Ticket`\n     - Validate required metadata:\n       - `id` (unique across all tickets)\n       - `name` (non-empty string)\n       - `epic` (must reference valid epic ID)\n       - `created` (valid timestamp)\n       - `assignee` (if specified)\n     - Validate spec fields:\n       - `type` is one of: feature, bug, task, spike\n       - `status` is one of: backlog, in-progress, review, done, cancelled\n       - `priority` is one of: low, medium, high, critical\n       - `acceptance_criteria` has at least one item\n       - `tasks` array is properly formatted\n\n6. **Cross-Reference Validation**\n   - Verify all ticket epic references point to existing epics\n   - Check that epic ticket lists match actual ticket files\n   - Validate ticket dependencies reference existing tickets\n   - Ensure no circular dependencies exist\n   - Verify unique IDs across all entities\n\n7. **Data Integrity Checks**\n   - Validate timestamp formats (ISO 8601)\n   - Check that updated timestamps are >= created timestamps\n   - Verify status transitions make sense (no done tickets in backlog epics)\n   - Validate priority and effort estimates are consistent\n\n8. **Template Validation**\n   - If templates exist in `.pac/templates/`:\n     - Verify they follow PAC specification\n     - Check they include all required fields\n     - Ensure placeholder values are clearly marked\n\n9. **Generate Validation Report**\n   - Create detailed report with:\n     ```\n     PAC Validation Report\n     ====================\n     \n     Configuration: [VALID/INVALID]\n     - Issues found: [count]\n     \n     Structure: [VALID/INVALID]\n     - Epics found: [count]\n     - Tickets found: [count]\n     - Orphaned files: [count]\n     \n     Epic Validation:\n     - Valid epics: [count]\n     - Invalid epics: [list with reasons]\n     \n     Ticket Validation:\n     - Valid tickets: [count]\n     - Invalid tickets: [list with reasons]\n     \n     Cross-Reference Issues:\n     - Missing epic references: [list]\n     - Orphaned tickets: [list]\n     - Invalid dependencies: [list]\n     \n     Recommendations:\n     - [Specific fixes needed]\n     ```\n\n10. **Auto-Fix Options**\n    - If `--fix` flag provided:\n      - Add missing required fields with placeholder values\n      - Fix formatting issues (indentation, quotes)\n      - Update epic ticket lists to match actual tickets\n      - Create backup before making changes\n    - Show what would be fixed without `--fix` flag\n\n11. **Git Integration**\n    - If `--pre-commit` flag:\n      - Only validate files staged for commit\n      - Exit with appropriate code for git hook\n      - Provide concise output suitable for CLI\n\n12. **Summary and Exit Codes**\n    - Exit code 0: All validations passed\n    - Exit code 1: Validation errors found\n    - Exit code 2: Configuration errors\n    - Display summary:\n      - Total files validated\n      - Issues found and fixed (if applicable)\n      - Next steps for remaining issues\n\n## Arguments\n\n- `--file <path>`: Validate specific file only\n- `--epic <epic-id>`: Validate specific epic and its tickets\n- `--fix`: Automatically fix common issues\n- `--pre-commit`: Run in pre-commit mode (concise output)\n- `--verbose`: Show detailed validation information\n- `--quiet`: Only show errors, no success messages\n\n## Example Usage\n\n```\n/project:pac-validate\n/project:pac-validate --fix\n/project:pac-validate --file .pac/epics/epic-auth.yaml\n/project:pac-validate --epic epic-payment --verbose\n/project:pac-validate --pre-commit\n```"
              },
              {
                "name": "/project-health-check",
                "description": "Analyze overall project health and metrics",
                "path": "plugins/commands-project-task-management/commands/project-health-check.md",
                "frontmatter": {
                  "description": "Analyze overall project health and metrics",
                  "category": "project-task-management",
                  "allowed-tools": "Bash(git *), Bash(gh *), Bash(npm *)"
                },
                "content": "# Project Health Check\n\nAnalyze overall project health and metrics\n\n## Instructions\n\n1. **Health Check Initialization**\n   - Verify tool connections (Linear, GitHub)\n   - Define evaluation period (default: last 30 days)\n   - Set health check criteria and thresholds\n   - Identify key metrics to evaluate\n\n2. **Multi-Dimensional Analysis**\n\n#### Code Health Metrics\n```bash\n# Code churn analysis\ngit log --format=format: --name-only --since=\"30 days ago\" | sort | uniq -c | sort -rg\n\n# Contributor activity\ngit shortlog -sn --since=\"30 days ago\"\n\n# Branch health\ngit for-each-ref --format='%(refname:short) %(committerdate:relative)' refs/heads/ | grep -E \"(months|years) ago\"\n\n# File complexity (if cloc available)\ncloc . --json --exclude-dir=node_modules,dist,build\n\n# Test coverage trends\nnpm test -- --coverage --json\n```\n\n#### Dependency Health\n```bash\n# Check for outdated dependencies\nnpm outdated --json\n\n# Security vulnerabilities\nnpm audit --json\n\n# License compliance\nnpx license-checker --json\n```\n\n#### Linear/Task Management Health\n```\n1. Sprint velocity trends\n2. Cycle time analysis\n3. Blocked task duration\n4. Backlog growth rate\n5. Bug vs feature ratio\n6. Task completion predictability\n```\n\n#### Team Health Indicators\n```\n1. PR review turnaround time\n2. Commit frequency distribution\n3. Work distribution balance\n4. On-call incident frequency\n5. Documentation updates\n```\n\n3. **Health Report Generation**\n\n```markdown\n# Project Health Report - [Project Name]\nGenerated: [Date]\n\n## Executive Summary\nOverall Health Score: [Score]/100 [ Healthy |  Needs Attention |  Critical]\n\n### Key Findings\n-  Strengths: [Top 3 positive indicators]\n-  Concerns: [Top 3 areas needing attention]\n-  Critical Issues: [Immediate action items]\n\n## Detailed Health Metrics\n\n1. **Delivery Health** (Score: [X]/100)\n| Metric | Current | Target | Status |\n|--------|---------|--------|--------|\n| Sprint Velocity | [X] pts | [Y] pts |  |\n| On-time Delivery | [X]% | 90% |  |\n| Cycle Time | [X] days | [Y] days |  |\n| Defect Rate | [X]% | <5% |  |\n\n2. **Code Quality** (Score: [X]/100)\n| Metric | Current | Target | Status |\n|--------|---------|--------|--------|\n| Test Coverage | [X]% | 80% |  |\n| Code Duplication | [X]% | <3% |  |\n| Complexity Score | [X] | <10 |  |\n| Security Issues | [X] | 0 |  |\n\n3. **Technical Debt** (Score: [X]/100)\n-  Total Debt Items: [Count]\n-  Debt Growth Rate: [+/-X% per sprint]\n-  Estimated Debt Work: [X days]\n-  Debt Impact: [Description]\n\n4. **Team Health** (Score: [X]/100)\n| Metric | Current | Target | Status |\n|--------|---------|--------|--------|\n| PR Review Time | [X] hrs | <4 hrs |  |\n| Knowledge Silos | [X] | 0 |  |\n| Work Balance | [Score] | >0.8 |  |\n| Burnout Risk | [Level] | Low |  |\n\n5. **Dependency Health** (Score: [X]/100)\n-  Outdated Dependencies: [X]/[Total]\n-  Security Vulnerabilities: [Critical: X, High: Y]\n-  License Issues: [Count]\n-  External Service Health: [Status]\n\n## Trend Analysis\n\n### Velocity Trend (Last 6 Sprints)\n```\nSprint 1:  40 pts\nSprint 2:  45 pts\nSprint 3:  50 pts\nSprint 4:  45 pts\nSprint 5:  38 pts\nSprint 6:  35 pts  Declining\n```\n\n### Bug Discovery Rate\n```\nWeek 1:  2 bugs\nWeek 2:  4 bugs\nWeek 3:  6 bugs  Increasing\nWeek 4:  8 bugs  Action needed\n```\n\n## Risk Assessment\n\n### High Priority Risks\n1. **Declining Velocity** \n   - Impact: High\n   - Likelihood: Confirmed\n   - Mitigation: Review sprint planning process\n\n2. **Security Vulnerabilities**\n   - Impact: Critical\n   - Count: [X] high, [Y] medium\n   - Action: Immediate patching required\n\n3. **Knowledge Concentration**\n   - Impact: Medium\n   - Bus Factor: 2\n   - Action: Implement pairing/documentation\n\n## Actionable Recommendations\n\n### Immediate Actions (This Week)\n1.  **Security**: Update [package] to fix critical vulnerability\n2.  **Quality**: Address top 3 bug-prone modules\n3.  **Team**: Schedule knowledge transfer for [critical component]\n\n### Short-term Improvements (This Sprint)\n1.  **Velocity**: Reduce scope to sustainable level\n2.  **Testing**: Increase coverage in [module] to 80%\n3.  **Documentation**: Update outdated docs for [feature]\n\n### Long-term Initiatives (This Quarter)\n1.  **Architecture**: Refactor [component] to reduce complexity\n2.  **Process**: Implement automated dependency updates\n3.  **Metrics**: Set up continuous health monitoring\n\n## Comparison with Previous Health Check\n\n| Category | Last Check | Current | Trend |\n|----------|------------|---------|-------|\n| Overall Score | 72/100 | 68/100 |  -4 |\n| Delivery | 80/100 | 75/100 |  -5 |\n| Code Quality | 70/100 | 72/100 |  +2 |\n| Technical Debt | 65/100 | 60/100 |  -5 |\n| Team Health | 75/100 | 70/100 |  -5 |\n```\n\n4. **Interactive Deep Dives**\n\nOffer focused analysis options:\n\n```\n\"Based on the health check, would you like to:\n1. Deep dive into declining velocity trends\n2. Generate security vulnerability fix plan\n3. Analyze technical debt hotspots\n4. Create team workload rebalancing plan\n5. Set up automated health monitoring\"\n```\n\n## Error Handling\n\n### Missing Linear Connection\n```\n\"Linear MCP not connected. Health check will be limited to:\n- Git/GitHub metrics only\n- No sprint velocity or task metrics\n- Manual input required for team data\n\nTo enable full health analysis:\n1. Install Linear MCP server\n2. Configure with API credentials\n3. Re-run health check\"\n```\n\n### Incomplete Data\n```\n\"Some metrics could not be calculated:\n- [List missing metrics]\n- [Explain impact on analysis]\n\nWould you like to:\n1. Proceed with available data\n2. Manually provide missing information\n3. Skip incomplete sections\"\n```\n\n## Customization Options\n\n### Threshold Configuration\n```yaml\n# health-check-config.yml\nthresholds:\n  velocity_variance: 20  # Acceptable % variance\n  test_coverage: 80      # Minimum coverage %\n  pr_review_time: 4      # Maximum hours\n  bug_rate: 5           # Maximum % of work\n  dependency_age: 90    # Days before \"outdated\"\n```\n\n### Custom Health Metrics\nAllow users to define additional metrics:\n```\n\"Add custom health metric:\n- Name: Customer Satisfaction\n- Data Source: [API/Manual/File]\n- Target Value: [>4.5/5]\n- Weight: [Impact on overall score]\"\n```\n\n## Export Options\n\n1. **Executive Summary** (PDF/Markdown)\n2. **Detailed Report** (HTML with charts)\n3. **Raw Metrics** (JSON/CSV)\n4. **Action Items** (Linear tasks/GitHub issues)\n5. **Monitoring Dashboard** (Grafana/Datadog format)\n\n## Automation Suggestions\n\n```\n\"Would you like me to:\n1. Schedule weekly health checks\n2. Set up alerts for critical metrics\n3. Create Linear tasks for action items\n4. Generate PR templates with health criteria\n5. Configure CI/CD health gates\"\n```\n\n## Best Practices\n\n1. **Regular Cadence**: Run health checks weekly/bi-weekly\n2. **Track Trends**: Compare with historical data\n3. **Action-Oriented**: Focus on fixable issues\n4. **Team Involvement**: Share results transparently\n5. **Continuous Improvement**: Refine metrics based on outcomes"
              },
              {
                "name": "/project-timeline-simulator",
                "description": "Simulate project outcomes with variable modeling, risk assessment, and resource optimization scenarios.",
                "path": "plugins/commands-project-task-management/commands/project-timeline-simulator.md",
                "frontmatter": {
                  "description": "Simulate project outcomes with variable modeling, risk assessment, and resource optimization scenarios.",
                  "category": "project-task-management",
                  "argument-hint": "Specify project timeline parameters",
                  "allowed-tools": "Bash(gh *), Read"
                },
                "content": "# Project Timeline Simulator\n\nSimulate project outcomes with variable modeling, risk assessment, and resource optimization scenarios.\n\n## Instructions\n\nYou are tasked with creating comprehensive project timeline simulations to optimize planning, resource allocation, and risk management. Follow this approach: **$ARGUMENTS**\n\n### 1. Prerequisites Assessment\n\n**Critical Project Context Validation:**\n\n- **Project Scope**: What specific project are you simulating timelines for?\n- **Key Variables**: What factors could significantly impact timeline outcomes?\n- **Resource Constraints**: What team, budget, and time limitations apply?\n- **Success Criteria**: How will you measure project success and timeline effectiveness?\n- **Risk Tolerance**: What level of schedule risk is acceptable?\n\n**If context is unclear, guide systematically:**\n\n```\nMissing Project Scope:\n\"What type of project needs timeline simulation?\n- Software Development: Feature development, platform migration, system redesign\n- Product Launch: New product development from concept to market\n- Business Initiative: Process improvement, organizational change, market expansion\n- Infrastructure Project: System upgrades, tool implementation, capacity expansion\n\nPlease specify project deliverables, stakeholders, and success criteria.\"\n\nMissing Key Variables:\n\"What factors could significantly impact your project timeline?\n- Resource Availability: Team capacity, skill availability, external dependencies\n- Technical Complexity: Unknown requirements, integration challenges, performance needs\n- External Dependencies: Vendor deliveries, regulatory approvals, partner coordination\n- Market Dynamics: Customer feedback, competitive pressure, business priority changes\"\n```\n\n### 2. Project Structure Modeling\n\n**Systematically map project components and dependencies:**\n\n#### Work Breakdown Structure (WBS) Analysis\n```\nProject Component Framework:\n\nPhase-Based Structure:\n- Discovery/Planning: Requirements gathering, design, architecture planning\n- Development/Implementation: Core building, integration, testing phases\n- Validation/Testing: Quality assurance, user acceptance, performance validation\n- Deployment/Launch: Release preparation, rollout, go-live activities\n- Stabilization/Optimization: Post-launch support, performance tuning, iteration\n\nFeature-Based Structure:\n- Core Features: Essential functionality for minimum viable product\n- Enhanced Features: Additional capabilities for competitive advantage\n- Integration Features: System connectivity and data synchronization\n- Quality Features: Security, performance, reliability, and maintainability\n\nSkill-Based Structure:\n- Frontend Development: User interface and experience implementation\n- Backend Development: Server logic, APIs, and data processing\n- Infrastructure/DevOps: Deployment, monitoring, and operational setup\n- Design/UX: User research, interface design, and usability testing\n- Quality Assurance: Testing strategy, automation, and validation\n```\n\n#### Dependency Mapping Framework\n```\nProject Dependency Analysis:\n\nSequential Dependencies:\n- Finish-to-Start: Task B cannot begin until Task A completes\n- Start-to-Start: Task B cannot start until Task A has started\n- Finish-to-Finish: Task B cannot finish until Task A finishes\n- Start-to-Finish: Task B cannot finish until Task A starts\n\nResource Dependencies:\n- Shared Resources: Team members working across multiple tasks\n- Skill Dependencies: Specialized expertise required for specific tasks\n- Tool Dependencies: Software, hardware, or platform availability\n- Budget Dependencies: Funding approval and expenditure timing\n\nExternal Dependencies:\n- Vendor Deliveries: Third-party software, services, or hardware\n- Regulatory Approvals: Compliance reviews and certification processes\n- Stakeholder Decisions: Business approvals and priority setting\n- Market Timing: Customer readiness and competitive positioning\n```\n\n### 3. Variable Modeling Framework\n\n**Systematically model factors affecting timeline outcomes:**\n\n#### Uncertainty Factor Analysis\n```\nTimeline Variable Categories:\n\nEffort Estimation Variables:\n- Task Complexity: Simple, moderate, complex, or unknown complexity\n- Team Experience: Expert, experienced, moderate, or novice skill levels\n- Requirements Clarity: Well-defined, partially defined, or evolving requirements\n- Technology Maturity: Proven, established, emerging, or experimental technology\n\nResource Variables:\n- Team Availability: Full-time, part-time, or shared allocation percentages\n- Skill Availability: In-house expertise, contractors, or training requirements\n- Infrastructure Readiness: Available, partially ready, or needs development\n- Budget Flexibility: Fixed, constrained, or adjustable funding levels\n\nExternal Variables:\n- Stakeholder Responsiveness: Fast, normal, or slow decision and feedback cycles\n- Market Stability: Stable, evolving, or rapidly changing requirements\n- Regulatory Environment: Clear, evolving, or uncertain compliance landscape\n- Competitive Pressure: Low, moderate, or high urgency for delivery\n```\n\n#### Variable Distribution Modeling\n```\nProbabilistic Timeline Estimation:\n\nThree-Point Estimation:\n- Optimistic Estimate: Best-case scenario with favorable conditions\n- Most Likely Estimate: Expected scenario with normal conditions\n- Pessimistic Estimate: Worst-case scenario with adverse conditions\n\nDistribution Types:\n- PERT Distribution: Beta distribution weighted toward most likely\n- Triangular Distribution: Linear probability between min, mode, max\n- Normal Distribution: Bell curve around mean with standard deviation\n- Log-Normal Distribution: Right-skewed for tasks with uncertainty\n\nMonte Carlo Simulation:\n- Random sampling from variable distributions\n- Thousands of simulation runs for statistical analysis\n- Confidence intervals for timeline predictions\n- Risk quantification and probability assessment\n```\n\n### 4. Scenario Generation Engine\n\n**Create comprehensive project timeline scenarios:**\n\n#### Scenario Development Framework\n```\nMulti-Dimensional Scenario Portfolio:\n\nBaseline Scenarios (40% of simulations):\n- Normal Resource Availability: Team at expected capacity and skills\n- Standard Complexity: Requirements and technical challenges as anticipated\n- Typical External Factors: Normal stakeholder responsiveness and market conditions\n- Expected Dependencies: Third-party deliveries and approvals on schedule\n\nOptimistic Scenarios (20% of simulations):\n- Enhanced Resource Availability: Additional team members or improved productivity\n- Reduced Complexity: Simpler requirements or technical solutions\n- Favorable External Factors: Fast stakeholder decisions and stable market\n- Accelerated Dependencies: Early vendor deliveries and quick approvals\n\nPessimistic Scenarios (25% of simulations):\n- Constrained Resources: Team availability issues or skill gaps\n- Increased Complexity: Scope creep or technical challenges\n- Adverse External Factors: Slow decisions or changing market conditions\n- Delayed Dependencies: Late vendor deliveries or approval delays\n\nDisruption Scenarios (15% of simulations):\n- Major Scope Changes: Significant requirement modifications mid-project\n- Team Disruptions: Key team member departures or organizational changes\n- Technology Disruptions: Platform changes or security requirements\n- Market Disruptions: Competitive pressure or business priority shifts\n```\n\n#### Critical Path Analysis\n- Identification of activities that directly impact project completion\n- Float/slack analysis for non-critical activities\n- Critical path vulnerability assessment under different scenarios\n- Resource optimization for critical path acceleration\n\n### 5. Risk Assessment and Impact Modeling\n\n**Comprehensive project risk evaluation:**\n\n#### Risk Identification Framework\n```\nProject Risk Categories:\n\nTechnical Risks:\n- Requirements Risk: Unclear, changing, or conflicting requirements\n- Technology Risk: Unproven technology or integration challenges\n- Performance Risk: Scalability, reliability, or efficiency concerns\n- Security Risk: Data protection and compliance requirements\n\nResource Risks:\n- Team Risk: Availability, skills, or productivity challenges\n- Budget Risk: Funding constraints or cost overruns\n- Time Risk: Schedule pressure or competing priorities\n- Vendor Risk: Third-party delivery or quality issues\n\nBusiness Risks:\n- Market Risk: Customer needs or competitive landscape changes\n- Stakeholder Risk: Changing priorities or approval delays\n- Regulatory Risk: Compliance requirements or policy changes\n- Strategic Risk: Business model or technology direction shifts\n```\n\n#### Risk Impact Simulation\n```\nRisk Effect Modeling:\n\nProbability Assessment:\n- High Probability (70-90%): Likely to occur based on historical data\n- Medium Probability (30-70%): Possible occurrence with mixed indicators\n- Low Probability (5-30%): Unlikely but possible based on rare events\n- Very Low Probability (<5%): Black swan events with major impact\n\nImpact Assessment:\n- Schedule Impact: Days or weeks of delay caused by risk realization\n- Resource Impact: Additional team members or budget required\n- Quality Impact: Feature cuts or technical debt accumulation\n- Business Impact: Revenue delay or customer satisfaction reduction\n\nRisk Mitigation Modeling:\n- Prevention Strategies: Actions to reduce risk probability\n- Mitigation Strategies: Plans to reduce risk impact if it occurs\n- Contingency Plans: Alternative approaches when risks materialize\n- Transfer Strategies: Insurance, contracts, or vendor risk sharing\n```\n\n### 6. Resource Optimization Simulation\n\n**Systematically optimize resource allocation across scenarios:**\n\n#### Resource Allocation Framework\n```\nMulti-Objective Resource Optimization:\n\nTeam Allocation Optimization:\n- Skill matching for maximum productivity and quality\n- Workload balancing to prevent burnout and bottlenecks\n- Cross-training opportunities for risk reduction\n- Contractor vs full-time employee optimization\n\nBudget Allocation Optimization:\n- Feature prioritization for maximum business value\n- Infrastructure investment for scalability and reliability\n- Tool and technology investment for productivity\n- Risk mitigation investment for schedule protection\n\nTimeline Optimization:\n- Parallel work stream identification and coordination\n- Critical path acceleration through resource concentration\n- Non-critical path scheduling for resource smoothing\n- Buffer allocation for uncertainty and risk management\n```\n\n#### Resource Constraint Modeling\n- Team capacity limitations and productivity variations\n- Budget restrictions and approval processes\n- Tool and infrastructure availability constraints\n- Skill development timelines and learning curves\n\n### 7. Decision Point Integration\n\n**Connect simulation insights to project management decisions:**\n\n#### Adaptive Project Management\n```\nSimulation-Driven Decision Framework:\n\nMilestone Decision Points:\n- Go/No-Go Decisions: Continue, pivot, or cancel based on progress\n- Resource Reallocation: Team or budget adjustments based on performance\n- Scope Adjustments: Feature prioritization based on timeline pressure\n- Risk Response: Mitigation strategy activation based on emerging risks\n\nEarly Warning Systems:\n- Schedule Variance Triggers: When actual progress deviates from plan\n- Resource Utilization Alerts: Team productivity or availability changes\n- Risk Indicator Monitoring: Early signals of potential problems\n- Quality Metric Tracking: Defect rates or technical debt accumulation\n\nAdaptive Strategies:\n- Agile Scope Management: Feature prioritization and MVP definition\n- Resource Flexibility: Team scaling and skill augmentation options\n- Timeline Buffer Management: Schedule contingency allocation and usage\n- Quality Trade-off Management: Technical debt vs delivery speed decisions\n```\n\n#### Project Success Optimization\n```\nSuccess Metric Optimization:\n\nTime-Based Success:\n- On-Time Delivery: Probability of meeting original schedule\n- Schedule Acceleration: Options for faster delivery with trade-offs\n- Milestone Achievement: Interim goal completion likelihood\n- Critical Path Protection: Schedule risk mitigation effectiveness\n\nQuality-Based Success:\n- Feature Completeness: Scope delivery against original requirements\n- Technical Quality: Code quality, performance, and maintainability\n- User Satisfaction: Usability and functionality meeting user needs\n- Business Value: ROI and strategic objective achievement\n\nResource-Based Success:\n- Budget Performance: Cost control and financial efficiency\n- Team Satisfaction: Developer experience and retention\n- Stakeholder Satisfaction: Communication and expectation management\n- Knowledge Transfer: Capability building and learning objectives\n```\n\n### 8. Output Generation and Recommendations\n\n**Present simulation insights in actionable project management format:**\n\n```\n## Project Timeline Simulation: [Project Name]\n\n### Simulation Summary\n- Scenarios Analyzed: [number and types of scenarios]\n- Timeline Range: [minimum to maximum completion estimates]\n- Success Probability: [likelihood of on-time, on-budget delivery]\n- Key Risk Factors: [primary threats to project success]\n\n### Timeline Predictions\n\n| Scenario Type | Completion Probability | Duration Range | Key Assumptions |\n|---------------|----------------------|----------------|-----------------|\n| Optimistic | 90% | 12-14 weeks | Ideal conditions |\n| Baseline | 70% | 16-20 weeks | Normal conditions |\n| Pessimistic | 50% | 22-28 weeks | Adverse conditions |\n| Worst Case | 10% | 30+ weeks | Multiple problems |\n\n### Critical Success Factors\n- Resource Availability: [team capacity and skill requirements]\n- Dependency Management: [external coordination and timing]\n- Risk Mitigation: [proactive risk prevention and response]\n- Scope Management: [feature prioritization and change control]\n\n### Recommended Strategy\n- Primary Approach: [optimal resource allocation and timeline strategy]\n- Contingency Plans: [backup strategies for different scenarios]\n- Early Warning Indicators: [metrics to monitor for course correction]\n- Decision Points: [key milestones for strategy adjustment]\n\n### Resource Optimization\n- Team Allocation: [optimal skill and capacity distribution]\n- Budget Distribution: [investment prioritization across features and risk mitigation]\n- Timeline Buffers: [schedule contingency allocation recommendations]\n- Quality Investment: [testing and technical debt management strategy]\n\n### Risk Management Plan\n- High-Priority Risks: [most critical threats and mitigation strategies]\n- Monitoring Strategy: [early detection and response systems]\n- Contingency Resources: [backup team and budget allocation]\n- Escalation Procedures: [decision triggers and stakeholder communication]\n```\n\n### 9. Continuous Project Learning\n\n**Establish ongoing simulation refinement and project improvement:**\n\n#### Performance Tracking\n- Actual vs predicted timeline performance measurement\n- Resource utilization efficiency and productivity assessment\n- Risk realization frequency and impact validation\n- Decision quality improvement over multiple projects\n\n#### Methodology Enhancement\n- Simulation accuracy improvement based on project outcomes\n- Estimation technique refinement and calibration\n- Risk model enhancement and validation\n- Team capability and productivity modeling improvement\n\n## Usage Examples\n\n```bash\n# Software development project simulation\n/project:project-timeline-simulator Simulate 6-month e-commerce platform development with 8-person team and Q4 launch deadline\n\n# Product launch timeline modeling\n/project:project-timeline-simulator Model mobile app launch timeline with user testing, app store approval, and marketing campaign coordination\n\n# Infrastructure migration simulation\n/project:project-timeline-simulator Simulate cloud migration project with legacy system dependencies and zero-downtime requirement\n\n# Agile release planning\n/project:project-timeline-simulator Model next quarter sprint planning with feature prioritization and team velocity uncertainty\n```\n\n## Quality Indicators\n\n- **Green**: Comprehensive scenarios, validated risk models, optimized resource allocation\n- **Yellow**: Multiple scenarios, basic risk assessment, reasonable resource planning\n- **Red**: Single timeline, minimal risk consideration, resource allocation not optimized\n\n## Common Pitfalls to Avoid\n\n- Planning fallacy: Systematic underestimation of time and resources required\n- Single-point estimates: Not modeling uncertainty and variability\n- Resource optimism: Assuming 100% utilization and no productivity variation\n- Risk blindness: Not identifying and planning for likely problems\n- Scope creep ignorance: Not accounting for requirement changes and additions\n- Static planning: Not adapting simulation based on actual project progress\n\nTransform project planning from hopeful guessing into systematic, evidence-based timeline optimization through comprehensive simulation and scenario analysis."
              },
              {
                "name": "/project-to-linear",
                "description": "Sync project structure to Linear workspace",
                "path": "plugins/commands-project-task-management/commands/project-to-linear.md",
                "frontmatter": {
                  "description": "Sync project structure to Linear workspace",
                  "category": "project-task-management",
                  "argument-hint": "Examine the current codebase structure and existing functionality"
                },
                "content": "# Project to Linear\n\nSync project structure to Linear workspace\n\n## Instructions\n\n1. **Analyze Project Requirements**\n   - Review the provided task description or project requirements: **$ARGUMENTS**\n   - Examine the current codebase structure and existing functionality\n   - Identify all major components and features needed\n   - Determine technical dependencies and constraints\n   - Assess the scope and complexity of the work\n\n2. **Understand User's Intent**\n   - Ask clarifying questions about:\n     - Project goals and objectives\n     - Priority levels for different features\n     - Timeline expectations\n     - Technical preferences or constraints\n     - Team structure (if relevant)\n     - Definition of done for tasks\n   - Confirm understanding of the requirements before proceeding\n\n3. **Check Linear Configuration**\n   - Verify if Linear MCP server is available and configured\n   - If not available, ask the user to:\n     - Install the Linear MCP server if not already installed\n     - Configure the Linear API key in their MCP settings\n     - Provide the default team ID or workspace information\n   - Test the connection by listing available projects\n\n4. **Project Setup in Linear**\n   - Ask the user if they want to:\n     - Use an existing Linear project (request project ID)\n     - Create a new project (ask for project name and description)\n   - For new projects, determine:\n     - Project type (Feature, Bug, Task, etc.)\n     - Project status (Planning, In Progress, etc.)\n     - Project lead or owner\n     - Any custom fields or labels to use\n\n5. **Generate Comprehensive Task List**\n   - Break down the project into logical phases:\n     - Planning and Design\n     - Core Implementation\n     - Testing and Quality Assurance\n     - Documentation\n     - Deployment and Release\n   - For each phase, create detailed tasks including:\n     - Clear, actionable task titles\n     - Detailed descriptions with acceptance criteria\n     - Technical specifications where relevant\n     - Estimated effort (if requested)\n     - Dependencies between tasks\n     - Priority levels (Critical, High, Medium, Low)\n\n6. **Create Task Hierarchy**\n   - Organize tasks into a proper hierarchy:\n     - Epic/Project level (if creating new project)\n     - Parent tasks for major features or components\n     - Subtasks for implementation details\n     - Related tasks for cross-cutting concerns\n   - Ensure logical grouping and dependencies\n\n7. **Add Task Details**\n   - For each task, include:\n     - **Title**: Clear, concise description\n     - **Description**: Detailed requirements and context\n     - **Acceptance Criteria**: Definition of done\n     - **Labels**: Appropriate tags (frontend, backend, testing, etc.)\n     - **Priority**: Based on user input and analysis\n     - **Estimates**: If sizing is requested\n     - **Assignee**: If team members are specified\n     - **Due Dates**: Based on timeline requirements\n\n8. **Create Tasks in Linear**\n   - Use the Linear MCP server to:\n     - Create the project (if new)\n     - Create all parent tasks first\n     - Create subtasks under appropriate parents\n     - Set up dependencies between tasks\n     - Apply labels and priorities\n     - Add any custom fields\n   - Provide feedback on each task created\n\n9. **Review and Refinement**\n   - Present a summary of all created tasks\n   - Show the task hierarchy and relationships\n   - Ask if any adjustments are needed:\n     - Task grouping or organization\n     - Priority changes\n     - Additional tasks or details\n     - Timeline adjustments\n   - Make any requested modifications\n\n10. **Provide Project Overview**\n    - Generate a summary including:\n      - Total number of tasks created\n      - Task breakdown by type/phase\n      - Critical path items\n      - Estimated timeline (if applicable)\n      - Link to the Linear project\n      - Next recommended actions\n\n## Example Task Structure\n\n```\nProject: User Dashboard Feature\n Planning & Design\n    Create UI/UX mockups\n    Define API requirements\n    Technical design document\n Backend Development\n    User API endpoints\n       GET /api/users endpoint\n       PUT /api/users/:id endpoint\n       User data validation\n    Dashboard data aggregation\n Frontend Development\n    Dashboard layout component\n    User profile widget\n    Activity feed component\n    Data visualization charts\n Testing\n    Unit tests for API\n    Frontend component tests\n    E2E dashboard tests\n    Performance testing\n Documentation & Deployment\n     API documentation\n     User guide\n     Production deployment\n```\n\n## Integration Notes\n\n- This command requires the Linear MCP server to be configured\n- If MCP is not available, provide the task list in a format that can be manually imported\n- Support batch operations to avoid rate limiting\n- Handle errors gracefully and provide clear feedback\n- Maintain task relationships and dependencies properly"
              },
              {
                "name": "/todo",
                "description": "Manage project todos in a todos.md file with add, complete, remove, and list operations",
                "path": "plugins/commands-project-task-management/commands/todo.md",
                "frontmatter": {
                  "description": "Manage project todos in a todos.md file with add, complete, remove, and list operations",
                  "category": "project-task-management",
                  "argument-hint": "<action> [args...]",
                  "allowed-tools": "Read, Write, Edit"
                },
                "content": "# Project Todo Manager\n\nManage todos in a `todos.md` file at the root of your current project directory.\n\n## Usage Examples:\n- `/user:todo add \"Fix navigation bug\"`\n- `/user:todo add \"Fix navigation bug\" [date/time/\"tomorrow\"/\"next week\"]` an optional 2nd parameter to set a due date\n- `/user:todo complete 1`\n- `/user:todo remove 2`\n- `/user:todo list`\n- `/user:todo undo 1`\n\n## Instructions:\nParse the command arguments: $ARGUMENTS\n\nManage todos in a `todos.md` file at the root of the current project directory. When this command is invoked:\n\n1. **Determine the project root** by looking for common indicators (.git, package.json, etc.)\n2. **Locate or create** `todos.md` in the project root\n3. **Parse the command arguments** to determine the action:\n   - `add \"task description\"` - Add a new todo\n   - `add \"task description\" [tomorrow|next week|4 days|June 9|12-24-2025|etc...]` - Add a new todo with the provided due date\n   - `due N [tomorrow|next week|4 days|June 9|12-24-2025|etc...]` - Mark todo N with the due date provided\n   - `complete N` - Mark todo N as completed and move from the ##Active list to the ##Completed list\n   - `remove N` - Remove todo N entirely\n   - `undo N` - Mark completed todo N as incomplete\n   - `list [N]` or no args - Show all (or N number of) todos in a user-friendly format, with each todo numbered for reference\n   - `past due` - Show all of the tasks which are past due and still active\n   - `next` - Shows the next active task in the list, this should respect Due dates, if there are any. If not, just show the first todo in the Active list\n\n## Todo Format:\nUse this markdown format in todos.md:\n```\n# Project Todos\n\n## Active\n- [ ] Task description here | Due: MM-DD-YYYY (conditionally include HH:MM AM/PM, if specified)\n- [ ] Another task\n\n## Completed\n- [x] Completed task description | Due: MM-DD-YYYY | Completed: MM-DD-YYYY\n```\n\n## Implementation Notes:\n- Always show friendly numbered lists when displaying todos\n- Handle date parsing for common formats (natural language, ISO dates, etc.)\n- Maintain the markdown checkbox format for compatibility\n- Keep completed tasks in the file for reference but in a separate section\n- Support undo operations by moving tasks back to Active section"
              }
            ],
            "skills": []
          },
          {
            "name": "interview",
            "description": "Interview command for fleshing out big feature plans and specifications",
            "source": "./plugins/interview",
            "category": "commands",
            "version": "1.0.0",
            "author": {
              "name": "BuildWithClaude Community",
              "url": "https://github.com/davepoon/buildwithclaude"
            },
            "install_commands": [
              "/plugin marketplace add davepoon/buildwithclaude",
              "/plugin install interview@buildwithclaude"
            ],
            "signals": {
              "stars": 2229,
              "forks": 239,
              "pushed_at": "2026-01-12T10:19:01Z",
              "created_at": "2025-07-25T02:26:45Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/big-features-interview",
                "description": "Interview to flesh out a plan/spec",
                "path": "plugins/interview/commands/big-features-interview.md",
                "frontmatter": {
                  "description": "Interview to flesh out a plan/spec",
                  "category": "interview",
                  "argument-hint": "<plan-file>",
                  "allowed-tools": "AskUserQuestion, Read, Glob, Grep, Write, Edit"
                },
                "content": "Here's the current plan:\n\n@$ARGUMENTS\n\nInterview me in detail using the AskUserQuestion tool about literally anything: technical implementation, UI & UX, concerns, tradeoffs, etc. but make sure the questions are not obvious.\n\nBe very in-depth and continue interviewing me continually until it's complete, then write the spec back to `$ARGUMENTS`."
              }
            ],
            "skills": []
          },
          {
            "name": "commands-security-audit",
            "description": "Commands for security auditing and vulnerability scanning",
            "source": "./plugins/commands-security-audit",
            "category": "commands",
            "version": "1.0.0",
            "author": {
              "name": "BuildWithClaude Community",
              "url": "https://github.com/davepoon/buildwithclaude"
            },
            "install_commands": [
              "/plugin marketplace add davepoon/buildwithclaude",
              "/plugin install commands-security-audit@buildwithclaude"
            ],
            "signals": {
              "stars": 2229,
              "forks": 239,
              "pushed_at": "2026-01-12T10:19:01Z",
              "created_at": "2025-07-25T02:26:45Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/add-authentication-system",
                "description": "Implement secure user authentication system",
                "path": "plugins/commands-security-audit/commands/add-authentication-system.md",
                "frontmatter": {
                  "description": "Implement secure user authentication system",
                  "category": "security-audit"
                },
                "content": "# Add Authentication System\n\nImplement secure user authentication system\n\n## Instructions\n\n1. **Authentication Strategy Analysis**\n   - Analyze application requirements and user types\n   - Define authentication methods (password, OAuth, SSO, MFA)\n   - Assess security requirements and compliance needs\n   - Plan user management and role-based access control\n   - Evaluate existing authentication infrastructure and integration points\n\n2. **Authentication Method Selection**\n   - Choose appropriate authentication strategies:\n     - **Username/Password**: Traditional credential-based authentication\n     - **OAuth 2.0/OpenID Connect**: Third-party authentication (Google, GitHub, etc.)\n     - **SAML**: Enterprise single sign-on integration\n     - **JWT**: Stateless token-based authentication\n     - **Multi-Factor Authentication**: SMS, TOTP, hardware tokens\n     - **Passwordless**: Magic links, WebAuthn, biometric authentication\n\n3. **User Management System**\n   - Set up user registration and account creation workflows\n   - Configure user profile management and data storage\n   - Implement password policies and security requirements\n   - Set up account verification and email confirmation\n   - Configure user deactivation and account deletion procedures\n\n4. **Authentication Implementation**\n   - Implement secure password hashing (bcrypt, Argon2, scrypt)\n   - Set up session management and token generation\n   - Configure secure cookie handling and CSRF protection\n   - Implement authentication middleware and route protection\n   - Set up authentication state management (client-side)\n\n5. **Authorization and Access Control**\n   - Implement role-based access control (RBAC) system\n   - Set up permission-based authorization\n   - Configure resource-level access controls\n   - Implement dynamic authorization and policy engines\n   - Set up API endpoint protection and authorization\n\n6. **Multi-Factor Authentication (MFA)**\n   - Configure TOTP-based authenticator app support\n   - Set up SMS-based authentication codes\n   - Implement backup codes and recovery mechanisms\n   - Configure hardware token support (FIDO2/WebAuthn)\n   - Set up MFA enforcement policies and user experience\n\n7. **OAuth and Third-Party Integration**\n   - Configure OAuth providers (Google, GitHub, Facebook, etc.)\n   - Set up OpenID Connect for identity federation\n   - Implement social login and account linking\n   - Configure enterprise SSO integration (SAML, LDAP)\n   - Set up API key management for external integrations\n\n8. **Security Implementation**\n   - Configure rate limiting and brute force protection\n   - Set up account lockout and security monitoring\n   - Implement security headers and session security\n   - Configure audit logging and security event tracking\n   - Set up vulnerability scanning and security testing\n\n9. **User Experience and Frontend Integration**\n   - Create responsive authentication UI components\n   - Implement client-side authentication state management\n   - Set up protected route handling and redirects\n   - Configure authentication error handling and user feedback\n   - Implement remember me and persistent login features\n\n10. **Testing and Maintenance**\n    - Set up comprehensive authentication testing\n    - Configure security testing and penetration testing\n    - Create authentication monitoring and alerting\n    - Set up compliance reporting and audit trails\n    - Train team on authentication security best practices\n    - Create incident response procedures for security events"
              },
              {
                "name": "/dependency-audit",
                "description": "Audit dependencies for security vulnerabilities",
                "path": "plugins/commands-security-audit/commands/dependency-audit.md",
                "frontmatter": {
                  "description": "Audit dependencies for security vulnerabilities",
                  "category": "security-audit"
                },
                "content": "# Dependency Audit Command\n\nAudit dependencies for security vulnerabilities\n\n## Instructions\n\nPerform a comprehensive dependency audit following these steps:\n\n1. **Dependency Discovery**\n   - Identify all dependency management files (package.json, requirements.txt, Cargo.toml, pom.xml, etc.)\n   - Map direct vs transitive dependencies\n   - Check for lock files and version consistency\n   - Review development vs production dependencies\n\n2. **Version Analysis**\n   - Check for outdated packages and available updates\n   - Identify packages with major version updates available\n   - Review semantic versioning compliance\n   - Analyze version pinning strategies\n\n3. **Security Vulnerability Scan**\n   - Run security audits using appropriate tools:\n     - `npm audit` for Node.js projects\n     - `pip-audit` for Python projects\n     - `cargo audit` for Rust projects\n     - GitHub security advisories for all platforms\n   - Identify critical, high, medium, and low severity vulnerabilities\n   - Check for known exploits and CVE references\n\n4. **License Compliance**\n   - Review all dependency licenses for compatibility\n   - Identify restrictive licenses (GPL, AGPL, etc.)\n   - Check for license conflicts with project license\n   - Document license obligations and requirements\n\n5. **Dependency Health Assessment**\n   - Check package maintenance status and activity\n   - Review contributor count and community support\n   - Analyze release frequency and stability\n   - Identify abandoned or deprecated packages\n\n6. **Size and Performance Impact**\n   - Analyze bundle size impact of each dependency\n   - Identify large dependencies that could be optimized\n   - Check for duplicate functionality across dependencies\n   - Review tree-shaking and dead code elimination effectiveness\n\n7. **Alternative Analysis**\n   - Identify dependencies with better alternatives\n   - Check for lighter or more efficient replacements\n   - Analyze feature overlap and consolidation opportunities\n   - Review native alternatives (built-in functions vs libraries)\n\n8. **Dependency Conflicts**\n   - Check for version conflicts between dependencies\n   - Identify peer dependency issues\n   - Review dependency resolution strategies\n   - Analyze potential breaking changes in updates\n\n9. **Build and Development Impact**\n   - Review dependencies that affect build times\n   - Check for development-only dependencies in production\n   - Analyze tooling dependencies and alternatives\n   - Review optional dependencies and their necessity\n\n10. **Supply Chain Security**\n    - Check for typosquatting and malicious packages\n    - Review package authenticity and signatures\n    - Analyze dependency sources and registries\n    - Check for suspicious or unusual dependencies\n\n11. **Update Strategy Planning**\n    - Create a prioritized update plan based on security and stability\n    - Identify breaking changes and required code modifications\n    - Plan for testing strategy during updates\n    - Document rollback procedures for problematic updates\n\n12. **Monitoring and Automation**\n    - Set up automated dependency scanning\n    - Configure security alerts and notifications\n    - Review dependency update automation tools\n    - Establish regular audit schedules\n\n13. **Documentation and Reporting**\n    - Create a comprehensive dependency inventory\n    - Document all security findings with remediation steps\n    - Provide update recommendations with priority levels\n    - Generate executive summary for stakeholders\n\nUse platform-specific tools and databases for the most accurate results. Focus on actionable recommendations with clear risk assessments."
              },
              {
                "name": "/security-audit",
                "description": "Perform comprehensive security assessment",
                "path": "plugins/commands-security-audit/commands/security-audit.md",
                "frontmatter": {
                  "description": "Perform comprehensive security assessment",
                  "category": "security-audit"
                },
                "content": "# Security Audit Command\n\nPerform comprehensive security assessment\n\n## Instructions\n\nPerform a systematic security audit following these steps:\n\n1. **Environment Setup**\n   - Identify the technology stack and framework\n   - Check for existing security tools and configurations\n   - Review deployment and infrastructure setup\n\n2. **Dependency Security**\n   - Scan all dependencies for known vulnerabilities\n   - Check for outdated packages with security issues\n   - Review dependency sources and integrity\n   - Use appropriate tools: `npm audit`, `pip check`, `cargo audit`, etc.\n\n3. **Authentication & Authorization**\n   - Review authentication mechanisms and implementation\n   - Check for proper session management\n   - Verify authorization controls and access restrictions\n   - Examine password policies and storage\n\n4. **Input Validation & Sanitization**\n   - Check all user input validation and sanitization\n   - Look for SQL injection vulnerabilities\n   - Identify potential XSS (Cross-Site Scripting) issues\n   - Review file upload security and validation\n\n5. **Data Protection**\n   - Identify sensitive data handling practices\n   - Check encryption implementation for data at rest and in transit\n   - Review data masking and anonymization practices\n   - Verify secure communication protocols (HTTPS, TLS)\n\n6. **Secrets Management**\n   - Scan for hardcoded secrets, API keys, and passwords\n   - Check for proper secrets management practices\n   - Review environment variable security\n   - Identify exposed configuration files\n\n7. **Error Handling & Logging**\n   - Review error messages for information disclosure\n   - Check logging practices for security events\n   - Verify sensitive data is not logged\n   - Assess error handling robustness\n\n8. **Infrastructure Security**\n   - Review containerization security (Docker, etc.)\n   - Check CI/CD pipeline security\n   - Examine cloud configuration and permissions\n   - Assess network security configurations\n\n9. **Security Headers & CORS**\n   - Check security headers implementation\n   - Review CORS configuration\n   - Verify CSP (Content Security Policy) settings\n   - Examine cookie security attributes\n\n10. **Reporting**\n    - Document all findings with severity levels (Critical, High, Medium, Low)\n    - Provide specific remediation steps for each issue\n    - Include code examples and file references\n    - Create an executive summary with key recommendations\n\nUse automated security scanning tools when available and provide manual review for complex security patterns."
              },
              {
                "name": "/security-hardening",
                "description": "Harden application security configuration",
                "path": "plugins/commands-security-audit/commands/security-hardening.md",
                "frontmatter": {
                  "description": "Harden application security configuration",
                  "category": "security-audit"
                },
                "content": "# Security Hardening\n\nHarden application security configuration\n\n## Instructions\n\n1. **Security Assessment and Baseline**\n   - Conduct comprehensive security audit of current application\n   - Identify potential vulnerabilities and attack vectors\n   - Analyze authentication and authorization mechanisms\n   - Review data handling and storage practices\n   - Assess network security and communication protocols\n\n2. **Authentication and Authorization Hardening**\n   - Implement strong password policies and multi-factor authentication\n   - Configure secure session management with proper timeouts\n   - Set up role-based access control (RBAC) with least privilege principle\n   - Implement JWT security best practices or secure session tokens\n   - Configure account lockout and brute force protection\n\n3. **Input Validation and Sanitization**\n   - Implement comprehensive input validation for all user inputs\n   - Set up SQL injection prevention with parameterized queries\n   - Configure XSS protection with proper output encoding\n   - Implement CSRF protection with tokens and SameSite cookies\n   - Set up file upload security with type validation and sandboxing\n\n4. **Secure Communication**\n   - Configure HTTPS with strong TLS/SSL certificates\n   - Implement HTTP Strict Transport Security (HSTS)\n   - Set up secure API communication with proper authentication\n   - Configure certificate pinning for mobile applications\n   - Implement end-to-end encryption for sensitive data transmission\n\n5. **Data Protection and Encryption**\n   - Implement encryption at rest for sensitive data\n   - Configure secure key management and rotation\n   - Set up database encryption and access controls\n   - Implement proper secrets management (avoid hardcoded secrets)\n   - Configure secure backup and recovery procedures\n\n6. **Security Headers and Policies**\n   - Configure Content Security Policy (CSP) headers\n   - Set up X-Frame-Options and X-Content-Type-Options headers\n   - Implement Referrer Policy and Feature Policy headers\n   - Configure CORS policies with proper origin validation\n   - Set up security.txt file for responsible disclosure\n\n7. **Dependency and Supply Chain Security**\n   - Audit and update all dependencies to latest secure versions\n   - Implement dependency vulnerability scanning\n   - Configure automated security updates for critical dependencies\n   - Set up software composition analysis (SCA) tools\n   - Implement dependency pinning and integrity checks\n\n8. **Infrastructure Security**\n   - Configure firewall rules and network segmentation\n   - Implement intrusion detection and prevention systems\n   - Set up secure logging and monitoring\n   - Configure secure container images and runtime security\n   - Implement infrastructure as code security scanning\n\n9. **Application Security Controls**\n   - Implement rate limiting and DDoS protection\n   - Set up web application firewall (WAF) rules\n   - Configure secure error handling without information disclosure\n   - Implement proper logging for security events\n   - Set up security monitoring and alerting\n\n10. **Security Testing and Validation**\n    - Conduct penetration testing and vulnerability assessments\n    - Implement automated security testing in CI/CD pipeline\n    - Set up static application security testing (SAST)\n    - Configure dynamic application security testing (DAST)\n    - Create security incident response plan and procedures\n    - Document security controls and compliance requirements"
              }
            ],
            "skills": []
          },
          {
            "name": "commands-simulation-modeling",
            "description": "Commands for scenario simulation and decision modeling",
            "source": "./plugins/commands-simulation-modeling",
            "category": "commands",
            "version": "1.0.0",
            "author": {
              "name": "BuildWithClaude Community",
              "url": "https://github.com/davepoon/buildwithclaude"
            },
            "install_commands": [
              "/plugin marketplace add davepoon/buildwithclaude",
              "/plugin install commands-simulation-modeling@buildwithclaude"
            ],
            "signals": {
              "stars": 2229,
              "forks": 239,
              "pushed_at": "2026-01-12T10:19:01Z",
              "created_at": "2025-07-25T02:26:45Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/business-scenario-explorer",
                "description": "Explore multiple business timeline scenarios with constraint validation and decision optimization.",
                "path": "plugins/commands-simulation-modeling/commands/business-scenario-explorer.md",
                "frontmatter": {
                  "description": "Explore multiple business timeline scenarios with constraint validation and decision optimization.",
                  "category": "simulation-modeling",
                  "argument-hint": "Specify business scenario parameters"
                },
                "content": "# Business Scenario Explorer\n\nExplore multiple business timeline scenarios with constraint validation and decision optimization.\n\n## Instructions\n\nYou are tasked with creating a comprehensive business scenario simulation to help explore multiple future timelines and make better strategic decisions. Follow this systematic approach: **$ARGUMENTS**\n\n### 1. Prerequisites Assessment\n\n**Before proceeding, validate these critical inputs:**\n\n- **Business Context**: Is the core business model and industry clearly defined?\n- **Time Horizon**: What is the planning timeline (quarters, years, market cycles)?\n- **Key Variables**: What are the primary factors that could change outcomes?\n- **Success Metrics**: How will you measure scenario success/failure?\n- **Decision Points**: What specific decisions need to be made?\n\n**If any of these are unclear, use progressive questioning:**\n\n```\nMissing Business Context:\n\"I need to understand your business model better. Please describe:\n- Your primary revenue streams\n- Key cost drivers \n- Main competitive advantages\n- Target market segments\"\n\nMissing Time Horizon:\n\"What planning period should we simulate?\n- Short-term (3-6 months): Market response, product launches\n- Medium-term (1-2 years): Strategic initiatives, market expansion  \n- Long-term (3-5+ years): Industry transformation, market cycles\"\n\nMissing Key Variables:\n\"What factors could significantly impact your business?\n- Market conditions (growth, recession, disruption)\n- Competitive landscape changes\n- Regulatory shifts\n- Technology adoption\n- Customer behavior evolution\"\n```\n\n### 2. Constraint Modeling\n\n**Map the decision environment with systematic constraint analysis:**\n\n#### External Constraints\n- Market size and growth dynamics\n- Competitive positioning and responses\n- Regulatory environment and compliance requirements\n- Economic conditions and cycles\n- Technology adoption curves\n- Supply chain dependencies\n\n#### Internal Constraints  \n- Financial resources and burn rate\n- Team capabilities and capacity\n- Technology infrastructure limitations\n- Brand positioning and reputation\n- Customer base characteristics\n- Operational scalability factors\n\n#### Temporal Constraints\n- Product development cycles\n- Market timing windows\n- Seasonal business patterns\n- Contract and partnership timelines\n- Regulatory approval processes\n\n**Quality Gate**: Validate that constraints are:\n- Specific and measurable\n- Based on real data where possible\n- Include ranges/uncertainty bounds\n- Account for interdependencies\n\n### 3. Scenario Architecture\n\n**Design multiple timeline branches systematically:**\n\n#### Base Case Scenario\n- Most likely outcome given current trajectory\n- Conservative assumptions about key variables\n- Historical pattern extrapolation\n- Risk-adjusted projections\n\n#### Optimistic Scenarios (2-3 variants)\n- Best-case market conditions\n- Successful execution of all initiatives\n- Favorable competitive dynamics\n- Accelerated adoption/growth\n\n#### Pessimistic Scenarios (2-3 variants)\n- Economic downturn impact\n- Increased competition\n- Execution challenges\n- Regulatory headwinds\n\n#### Disruption Scenarios (2-3 variants)\n- Technology breakthrough impacts\n- New market entrants\n- Business model shifts\n- Black swan events\n\n**Progressive Depth**: Start with 3-5 high-level scenarios, then drill into the most impactful ones.\n\n### 4. Timeline Compression Simulation\n\n**Run accelerated scenario testing:**\n\n#### Quarter-by-Quarter Analysis\n- Revenue progression under each scenario\n- Cost structure evolution\n- Market share dynamics\n- Key milestone achievement\n\n#### Decision Point Mapping\n- Critical decisions required at each timeline juncture\n- Option values and decision trees\n- Point-of-no-return identification\n- Pivot opportunity windows\n\n#### Feedback Loop Modeling\n- How early results would inform later decisions\n- Adaptive strategy adjustments\n- Learning and refinement cycles\n\n### 5. Quantitative Modeling\n\n**Apply systematic measurement to scenarios:**\n\n#### Financial Projections\n- Revenue growth trajectories\n- Profit margin evolution\n- Cash flow dynamics\n- Investment requirements\n- ROI calculations across timelines\n\n#### Market Dynamics\n- Market share progression\n- Customer acquisition costs\n- Lifetime value evolution\n- Competitive response modeling\n\n#### Operational Metrics\n- Team scaling requirements\n- Infrastructure capacity needs\n- Efficiency improvements\n- Quality indicators\n\n**Confidence Scoring**: Rate each projection 1-10 based on:\n- Data quality supporting the assumption\n- Historical precedent availability  \n- Expert validation received\n- Logical consistency with other assumptions\n\n### 6. Risk Assessment & Mitigation\n\n**Systematically evaluate scenario risks:**\n\n#### Probability Weighting\n- Assign realistic probabilities to each scenario\n- Use base rate analysis from similar situations\n- Account for planning fallacy and optimism bias\n- Include expert opinion and market research\n\n#### Impact Analysis\n- Quantify potential upside/downside for each scenario\n- Identify business-critical failure modes\n- Map cascade effects and domino risks\n- Calculate expected value across scenarios\n\n#### Mitigation Strategies\n- Identify early warning indicators for each scenario\n- Design adaptive responses and pivot strategies\n- Build option values and flexibility into plans\n- Create risk monitoring dashboards\n\n### 7. Decision Optimization\n\n**Generate actionable strategic guidance:**\n\n#### Strategy Robustness Testing\n- Which strategies perform well across multiple scenarios?\n- What are the key sensitivity factors?\n- Where are the highest-leverage decision points?\n- What creates competitive moats in each timeline?\n\n#### Resource Allocation Optimization\n- Optimal budget allocation across scenarios\n- Investment sequencing and timing\n- Capability building priorities\n- Partnership and acquisition strategies\n\n#### Contingency Planning\n- Specific action triggers for each scenario\n- Resource reallocation frameworks\n- Communication strategies for different outcomes\n- Stakeholder management approaches\n\n### 8. Calibration and Validation\n\n**Ensure simulation quality and accuracy:**\n\n#### Assumption Testing\n- Compare key assumptions to historical data\n- Validate with domain experts and stakeholders\n- Stress-test critical assumptions\n- Document confidence levels and sources\n\n#### Scenario Plausibility Check\n- Do scenarios follow logical progression?\n- Are interdependencies properly modeled?\n- Do financial projections balance?\n- Are timelines realistic given constraints?\n\n#### Bias Detection\n- Check for anchoring on current state\n- Identify confirmation bias in favorable scenarios  \n- Validate pessimistic scenarios aren't too extreme\n- Ensure scenarios cover full possibility space\n\n### 9. Output Generation\n\n**Present findings in structured, actionable format:**\n\n```\n## Business Scenario Analysis: [Business Name]\n\n### Executive Summary\n- Planning horizon: [timeline]\n- Scenarios modeled: [count and types]\n- Key decision points: [critical decisions]\n- Recommended strategy: [specific approach]\n\n### Scenario Outcomes Matrix\n\n| Scenario | Probability | Year 1 Revenue | Year 2 Revenue | Key Risks | Success Factors |\n|----------|-------------|----------------|----------------|-----------|-----------------|\n| Base Case | 40% | $X | $Y | [risks] | [factors] |\n| Optimistic A | 20% | $X | $Y | [risks] | [factors] |\n| Pessimistic A | 25% | $X | $Y | [risks] | [factors] |\n| Disruption A | 15% | $X | $Y | [risks] | [factors] |\n\n### Strategic Recommendations\n\n**Robust Strategies** (perform well across scenarios):\n1. [Strategy with confidence score]\n2. [Strategy with confidence score]\n3. [Strategy with confidence score]\n\n**Scenario-Specific Tactics**:\n- If Base Case: [specific actions]\n- If Optimistic: [specific actions]  \n- If Pessimistic: [specific actions]\n- If Disruption: [specific actions]\n\n**Critical Decision Points**:\n- Month 3: [decision] - Leading indicators: [metrics]\n- Month 9: [decision] - Leading indicators: [metrics]\n- Month 18: [decision] - Leading indicators: [metrics]\n\n### Risk Mitigation Framework\n- Early warning indicators for each scenario\n- Specific response triggers and actions\n- Resource reallocation procedures\n- Stakeholder communication protocols\n\n### Confidence Assessment\n- High confidence projections: [list]\n- Medium confidence projections: [list]  \n- Low confidence projections: [list]\n- Areas requiring additional research: [list]\n```\n\n### 10. Iteration and Refinement\n\n**Establish ongoing scenario improvement:**\n\n#### Feedback Integration\n- Monthly assumption validation against actual results\n- Quarterly scenario probability updates\n- Annual comprehensive scenario refresh\n- Continuous learning from scenario accuracy\n\n#### Model Enhancement\n- Incorporate new data sources as available\n- Refine constraint modeling based on experience\n- Update probability assessments based on outcomes\n- Enhance decision point identification\n\n**Success Metrics**: \n- Scenario accuracy over time\n- Decision quality improvement\n- Strategic option value realization\n- Risk event prediction success\n\n## Usage Examples\n\n```bash\n# Strategic business planning\n/simulation:business-scenario-explorer Evaluate SaaS expansion into European markets over next 2 years\n\n# Product launch planning\n/simulation:business-scenario-explorer Model outcomes for AI-powered feature launch across different market conditions\n\n# Investment decision\n/simulation:business-scenario-explorer Analyze ROI scenarios for $5M Series A funding across market conditions\n\n# Market entry strategy\n/simulation:business-scenario-explorer Explore timeline scenarios for entering fintech market as established player\n```\n\n## Quality Indicators\n\n- **Green**: 80%+ confidence in key assumptions, full constraint modeling, 5+ scenarios analyzed\n- **Yellow**: 60-80% confidence, partial constraint mapping, 3-4 scenarios\n- **Red**: <60% confidence, missing critical constraints, <3 scenarios\n\n## Common Pitfalls to Avoid\n\n- Planning fallacy: Overly optimistic timelines\n- Anchoring bias: Scenarios too close to current state\n- Confirmation bias: Favoring pleasant outcomes\n- Missing constraints: Ignoring regulatory/competitive factors\n- Point estimates: Not using probability distributions\n- Static thinking: Not modeling adaptive responses\n\nTransform your 10-year market cycle into a 10-hour simulation and make exponentially better strategic decisions."
              },
              {
                "name": "/constraint-modeler",
                "description": "Model world constraints with assumption validation, dependency mapping, and scenario boundary definition.",
                "path": "plugins/commands-simulation-modeling/commands/constraint-modeler.md",
                "frontmatter": {
                  "description": "Model world constraints with assumption validation, dependency mapping, and scenario boundary definition.",
                  "category": "simulation-modeling",
                  "argument-hint": "Specify constraint parameters"
                },
                "content": "# Constraint Modeler\n\nModel world constraints with assumption validation, dependency mapping, and scenario boundary definition.\n\n## Instructions\n\nYou are tasked with systematically modeling the constraints that govern your decision environment to create accurate simulations and scenarios. Follow this approach: **$ARGUMENTS**\n\n### 1. Prerequisites Assessment\n\n**Critical Constraint Context Validation:**\n\n- **Domain Definition**: What system/environment are you modeling constraints for?\n- **Constraint Types**: Physical, economic, regulatory, technical, or social constraints?\n- **Impact Scope**: How do these constraints affect decisions and outcomes?\n- **Change Dynamics**: Are constraints static or do they evolve over time?\n- **Validation Sources**: What data/expertise can verify constraint accuracy?\n\n**If context is unclear, guide systematically:**\n\n```\nMissing Domain Context:\n\"I need to understand what you're modeling constraints for:\n- Business Domain: Market constraints, competitive dynamics, regulatory environment\n- Technical Domain: System limitations, performance bounds, technology constraints\n- Operational Domain: Resource constraints, process limitations, capacity bounds\n- Financial Domain: Budget constraints, investment limitations, economic factors\n\nExamples:\n- 'SaaS business operating in regulated healthcare market'\n- 'Manufacturing system with supply chain and quality constraints'\n- 'Software architecture with performance and scalability requirements'\"\n\nMissing Constraint Types:\n\"What types of constraints are most relevant to your decisions?\n- Hard Constraints: Absolute limits that cannot be violated\n- Soft Constraints: Preferences and trade-offs that can be managed\n- Regulatory Constraints: Legal and compliance requirements\n- Resource Constraints: Budget, time, and capacity limitations\n- Market Constraints: Customer behavior and competitive dynamics\"\n```\n\n### 2. Constraint Taxonomy Framework\n\n**Systematically categorize and structure constraints:**\n\n#### Hard Constraints (Cannot be violated)\n```\nPhysical/Natural Constraints:\n- Laws of physics and natural limitations\n- Geographic and spatial boundaries\n- Time and temporal restrictions\n- Resource scarcity and finite capacity\n\nRegulatory/Legal Constraints:\n- Compliance requirements and legal mandates\n- Industry standards and certification requirements\n- Contractual obligations and agreements\n- Intellectual property and licensing restrictions\n\nTechnical Constraints:\n- System capacity and performance limits\n- Technology compatibility and integration requirements\n- Security and privacy constraints\n- Infrastructure limitations and dependencies\n```\n\n#### Soft Constraints (Can be managed/traded off)\n```\nEconomic Constraints:\n- Budget limitations and financial resources\n- Cost optimization and efficiency targets\n- Investment return requirements and payback periods\n- Market pricing and competitive pressure\n\nOrganizational Constraints:\n- Team capacity and skill limitations\n- Cultural and change management factors\n- Decision-making processes and approval cycles\n- Risk tolerance and strategic priorities\n\nMarket Constraints:\n- Customer preferences and behavior patterns\n- Competitive dynamics and response patterns\n- Market timing and seasonal factors\n- Distribution channel limitations and requirements\n```\n\n#### Dynamic Constraints (Change over time)\n```\nEvolutionary Constraints:\n- Technology advancement and obsolescence cycles\n- Market maturation and customer evolution\n- Regulatory changes and policy shifts\n- Competitive landscape evolution\n\nCyclical Constraints:\n- Seasonal business patterns and market cycles\n- Economic cycles and market conditions\n- Budget cycles and resource allocation patterns\n- Technology refresh and upgrade cycles\n```\n\n### 3. Constraint Mapping and Visualization\n\n**Create comprehensive constraint relationship models:**\n\n#### Constraint Interaction Matrix\n```\nConstraint Relationship Analysis:\n\nPrimary Constraints  Secondary Effects:\n- Budget Limitation  Team size  Development capacity  Feature scope\n- Regulatory Requirement  Compliance process  Timeline extension  Market timing\n- Technical Constraint  Architecture choice  Scalability  Growth potential\n\nConstraint Conflicts and Trade-offs:\n- Speed vs. Quality: Time constraint vs. quality constraint\n- Cost vs. Capability: Budget constraint vs. feature constraint  \n- Security vs. Usability: Security constraint vs. user experience constraint\n- Scale vs. Simplicity: Growth constraint vs. complexity constraint\n\nConstraint Dependencies:\n- Sequential: Constraint A must be satisfied before addressing Constraint B\n- Conditional: Constraint A applies only if Condition X is true\n- Mutual: Constraints A and B reinforce or conflict with each other\n- Hierarchical: Constraint A contains or encompasses Constraint B\n```\n\n#### Constraint Hierarchy Modeling\n- Strategic level constraints (mission, vision, values)\n- Tactical level constraints (resources, capabilities, market position)\n- Operational level constraints (processes, systems, daily operations)\n- Individual level constraints (skills, capacity, availability)\n\n### 4. Assumption Validation Framework\n\n**Systematically test and validate constraint assumptions:**\n\n#### Assumption Documentation\n```\nConstraint Assumption Template:\n\nConstraint: [Name and description]\nAssumption: [What we believe to be true about this constraint]\nSource: [Where this assumption comes from]\nConfidence Level: [1-10 scale with justification]\nImpact if Wrong: [What happens if assumption is incorrect]\nValidation Method: [How to test this assumption]\nUpdate Frequency: [How often to re-validate]\n\nExample:\nConstraint: \"Engineering team capacity\"\nAssumption: \"Team can deliver 10 story points per sprint\"\nSource: \"Historical velocity data from last 6 sprints\"\nConfidence Level: \"8 - consistent recent data but team composition changing\"\nImpact if Wrong: \"Project timeline delays, scope reduction needed\"\nValidation Method: \"Track actual velocity, monitor team changes\"\nUpdate Frequency: \"Monthly review with sprint retrospectives\"\n```\n\n#### Historical Validation\n- Analysis of past constraint behavior and violation patterns\n- Comparison of assumed vs. actual constraint limits\n- Pattern recognition for constraint evolution and change\n- Case study analysis from similar environments and decisions\n\n#### Real-time Validation\n- Continuous monitoring of constraint status and changes\n- Early warning systems for constraint violation risks\n- Feedback loops from constraint testing and boundary pushing\n- Expert consultation and stakeholder validation\n\n### 5. Scenario Boundary Definition\n\n**Use constraints to define realistic scenario limits:**\n\n#### Feasible Scenario Space\n```\nScenario Constraint Boundaries:\n\nOptimistic Boundary:\n- Best-case constraint relaxation (10-20% improvement)\n- Favorable external conditions and support\n- Maximum resource availability and efficiency\n- Minimal constraint conflicts and trade-offs\n\nRealistic Boundary:\n- Expected constraint behavior and normal conditions\n- Typical resource availability and standard efficiency\n- Normal constraint conflicts requiring standard trade-offs\n- Historical pattern-based constraint evolution\n\nPessimistic Boundary:\n- Worst-case constraint tightening (10-20% degradation)\n- Adverse external conditions and additional restrictions\n- Reduced resource availability and efficiency challenges\n- Maximum constraint conflicts requiring difficult trade-offs\n```\n\n#### Constraint Stress Testing\n- Maximum constraint load scenarios and breaking points\n- Cascade failure analysis when key constraints are violated\n- Recovery scenarios and constraint restoration approaches\n- Adaptive scenario adjustment for changing constraints\n\n### 6. Dynamic Constraint Modeling\n\n**Model how constraints change over time:**\n\n#### Constraint Evolution Patterns\n```\nTemporal Constraint Dynamics:\n\nLinear Evolution:\n- Gradual constraint relaxation or tightening over time\n- Predictable improvement or degradation patterns\n- Resource accumulation or depletion trends\n- Market maturation and capacity development\n\nCyclical Evolution:\n- Seasonal constraint variations and patterns\n- Economic cycle impacts on constraint severity\n- Technology refresh cycles and capability updates\n- Regulatory review cycles and compliance windows\n\nStep Function Evolution:\n- Sudden constraint changes from external events\n- Technology breakthrough impacts on capability constraints\n- Regulatory changes creating new constraint requirements\n- Market disruptions changing competitive constraints\n\nThreshold Evolution:\n- Constraint regime changes at specific trigger points\n- Scale-dependent constraint behavior modifications\n- Maturity-based constraint relaxation or introduction\n- Performance-based constraint adjustment mechanisms\n```\n\n#### Adaptive Constraint Management\n- Constraint monitoring and early warning systems\n- Proactive constraint modification and optimization\n- Scenario adaptation for changing constraint conditions\n- Strategic planning for anticipated constraint evolution\n\n### 7. Constraint Optimization Strategies\n\n**Generate approaches to work within and optimize constraints:**\n\n#### Constraint Relaxation Approaches\n```\nSystematic Constraint Optimization:\n\nDirect Relaxation:\n- Negotiate constraint modifications with stakeholders\n- Invest in capability building to reduce constraint impact\n- Seek regulatory relief or compliance alternatives\n- Restructure processes to minimize constraint conflicts\n\nConstraint Substitution:\n- Replace restrictive constraints with more flexible alternatives\n- Trade hard constraints for soft constraints where possible\n- Substitute resource constraints with efficiency improvements\n- Replace time constraints with scope or quality adjustments\n\nConstraint Circumvention:\n- Design solutions that avoid constraint-heavy areas\n- Use alternative approaches that minimize constraint impact\n- Leverage partnerships to access capabilities beyond constraints\n- Phase implementations to work within temporal constraints\n```\n\n#### Creative Constraint Solutions\n- Constraint reframing and alternative perspective development\n- Innovative approaches that turn constraints into advantages\n- Synergistic solutions that address multiple constraints simultaneously\n- Constraint-inspired innovation and creative problem solving\n\n### 8. Output Generation and Documentation\n\n**Present constraint analysis in actionable format:**\n\n```\n## Constraint Model Analysis: [Domain/Project Name]\n\n### Constraint Environment Overview\n- Domain Scope: [what is being constrained]\n- Primary Constraints: [most limiting factors]\n- Constraint Severity: [impact on decisions and outcomes]\n- Change Dynamics: [how constraints evolve over time]\n\n### Constraint Inventory\n\n#### Hard Constraints (Cannot be violated):\n| Constraint | Description | Impact | Validation Status |\n|------------|-------------|---------|------------------|\n| [Name] | [Details] | [Effect] | [Confidence level] |\n\n#### Soft Constraints (Can be managed):\n| Constraint | Description | Trade-off Options | Optimization Potential |\n|------------|-------------|-------------------|----------------------|\n| [Name] | [Details] | [Alternatives] | [Improvement possibilities] |\n\n#### Dynamic Constraints (Change over time):\n| Constraint | Current State | Evolution Pattern | Future Projection |\n|------------|---------------|------------------|------------------|\n| [Name] | [Status] | [Change pattern] | [Expected future state] |\n\n### Constraint Interaction Analysis\n- Primary Constraint Conflicts: [major trade-offs required]\n- Constraint Dependencies: [how constraints affect each other]\n- Cascade Effects: [secondary impacts of constraint changes]\n- Optimization Opportunities: [where constraint improvements are possible]\n\n### Scenario Boundary Definition\n- Feasible Scenario Space: [what scenarios are possible within constraints]\n- Constraint-Breaking Scenarios: [what would require constraint violation]\n- Optimization Scenarios: [how constraint improvements could expand possibilities]\n- Stress Test Boundaries: [maximum constraint loads the system can handle]\n\n### Constraint Management Strategies\n- Immediate Optimization: [quick constraint improvements available]\n- Strategic Relaxation: [longer-term constraint modification approaches]\n- Alternative Approaches: [ways to minimize constraint impact]\n- Risk Mitigation: [approaches to handle constraint violations]\n\n### Validation and Monitoring Plan\n- Constraint Monitoring: [how to track constraint status and changes]\n- Assumption Testing: [how to validate constraint assumptions]\n- Update Schedule: [when to refresh constraint model]\n- Warning Systems: [early alerts for constraint violations]\n```\n\n### 9. Continuous Constraint Learning\n\n**Establish ongoing constraint model improvement:**\n\n#### Feedback Integration\n- Actual constraint behavior vs. model predictions\n- Constraint violation lessons and recovery insights\n- Stakeholder feedback on constraint accuracy and completeness\n- Market and environment changes affecting constraint validity\n\n#### Model Enhancement\n- Constraint model accuracy improvement over time\n- New constraint identification and integration\n- Constraint relationship refinement and optimization\n- Predictive capability enhancement for constraint evolution\n\n## Usage Examples\n\n```bash\n# Business strategy constraints\n/simulation:constraint-modeler Model market entry constraints for European expansion including regulatory, competitive, and resource limitations\n\n# Technical architecture constraints  \n/simulation:constraint-modeler Define system constraints for microservices migration including performance, security, and team capability limits\n\n# Product development constraints\n/simulation:constraint-modeler Map product development constraints including budget, timeline, technical, and market requirements\n\n# Operational optimization constraints\n/simulation:constraint-modeler Model operational constraints for scaling customer support including team, process, and technology limitations\n```\n\n## Quality Indicators\n\n- **Green**: Comprehensive constraint coverage, validated assumptions, dynamic modeling\n- **Yellow**: Good constraint identification, some validation, basic change modeling\n- **Red**: Limited constraint coverage, unvalidated assumptions, static modeling\n\n## Common Pitfalls to Avoid\n\n- Constraint blindness: Not identifying hidden or implicit constraints\n- Static thinking: Treating dynamic constraints as fixed limitations\n- Over-constraint: Adding unnecessary restrictions that limit options\n- Under-validation: Not testing constraint assumptions against reality\n- Isolation thinking: Not modeling constraint interactions and dependencies\n- Solution bias: Defining constraints to justify preferred solutions\n\nTransform limitations into strategic clarity through systematic constraint modeling and optimization."
              },
              {
                "name": "/decision-tree-explorer",
                "description": "Explore decision branches with probability weighting, expected value analysis, and scenario-based optimization.",
                "path": "plugins/commands-simulation-modeling/commands/decision-tree-explorer.md",
                "frontmatter": {
                  "description": "Explore decision branches with probability weighting, expected value analysis, and scenario-based optimization.",
                  "category": "simulation-modeling",
                  "argument-hint": "Specify decision tree parameters"
                },
                "content": "# Decision Tree Explorer\n\nExplore decision branches with probability weighting, expected value analysis, and scenario-based optimization.\n\n## Instructions\n\nYou are tasked with creating a comprehensive decision tree analysis to explore complex decision scenarios and optimize choice outcomes. Follow this systematic approach: **$ARGUMENTS**\n\n### 1. Prerequisites Assessment\n\n**Critical Decision Context Validation:**\n\n- **Decision Scope**: What specific decision(s) need to be made?\n- **Stakeholders**: Who will be affected by and involved in this decision?\n- **Time Constraints**: What are the decision deadlines and implementation timelines?\n- **Success Criteria**: How will you measure decision success or failure?\n- **Resource Constraints**: What limitations affect available options?\n\n**If any context is unclear, guide systematically:**\n\n```\nMissing Decision Scope:\n\"I need clarity on the decision you're analyzing. Please specify:\n- Primary Decision: The main choice you need to make\n- Decision Level: Strategic, tactical, or operational\n- Decision Type: Go/no-go, resource allocation, priority ranking, or option selection\n- Alternative Options: What choices are you considering?\n\nExamples:\n- Strategic: 'Should we enter the European market next year?'\n- Investment: 'Which of 3 product features should we build first?'\n- Operational: 'Should we migrate to microservices or improve the monolith?'\n- Crisis: 'How should we respond to the new competitor launch?'\"\n\nMissing Success Criteria:\n\"How will you evaluate if this decision was successful?\n- Financial Metrics: Revenue impact, cost savings, ROI targets\n- Strategic Metrics: Market share, competitive position, capability building\n- Operational Metrics: Efficiency gains, quality improvements, risk reduction\n- Timeline Metrics: Speed to market, implementation time, payback period\"\n\nMissing Resource Context:\n\"What constraints limit your decision options?\n- Budget: Available investment capital and operating funds\n- Time: Implementation deadlines and resource availability windows\n- Capabilities: Team skills, technology infrastructure, operational capacity\n- Regulatory: Compliance requirements and approval processes\"\n```\n\n### 2. Decision Architecture Mapping\n\n**Structure the decision systematically:**\n\n#### Decision Hierarchy\n- Primary decision point and core question\n- Secondary decisions that follow from primary choice\n- Tertiary decisions and implementation details\n- Decision dependencies and sequencing requirements\n- Option combinations and interaction effects\n\n#### Stakeholder Impact Analysis\n- Decision makers and approval authorities\n- Implementation teams and resource owners\n- Customers and end users affected\n- External partners and dependencies\n- Competitive landscape implications\n\n#### Constraint Identification\n- Hard constraints (cannot be violated)\n- Soft constraints (preferences and trade-offs)\n- Temporal constraints (timing and sequencing)\n- Resource constraints (budget, capacity, capabilities)\n- Regulatory and compliance constraints\n\n### 3. Option Generation and Structuring\n\n**Systematically identify and organize decision alternatives:**\n\n#### Comprehensive Option Development\n- Direct approaches to achieving the goal\n- Hybrid solutions combining multiple approaches\n- Phased approaches with incremental implementation\n- Alternative goals that might better serve needs\n- \"Do nothing\" baseline for comparison\n\n#### Option Categorization\n- Quick wins vs. long-term strategic moves\n- High-risk/high-reward vs. safe/incremental options\n- Resource-intensive vs. lean approaches\n- Internal development vs. external partnerships\n- Proven approaches vs. innovative experiments\n\n#### Option Feasibility Assessment\n```\nFor each option, evaluate:\n- Technical Feasibility: Can this actually be implemented?\n- Economic Feasibility: Do benefits justify costs?\n- Operational Feasibility: Do we have capability to execute?\n- Timeline Feasibility: Can this be done in available time?\n- Political Feasibility: Will stakeholders support this?\n\nFeasibility Scoring (1-10 scale):\nOption: [name]\n- Technical: [score] - [reasoning]\n- Economic: [score] - [reasoning]\n- Operational: [score] - [reasoning]\n- Timeline: [score] - [reasoning]\n- Political: [score] - [reasoning]\nOverall Feasibility: [average score]\n```\n\n### 4. Probability Assessment Framework\n\n**Apply systematic probability estimation:**\n\n#### Base Rate Analysis\n- Historical success rates for similar decisions\n- Industry benchmarks and comparative data\n- Expert judgment and domain knowledge\n- Market research and customer validation data\n- Internal capability assessment and track record\n\n#### Scenario Probability Weighting\n- Best case scenario probabilities (optimistic outcomes)\n- Most likely scenario probabilities (base case expectations)\n- Worst case scenario probabilities (pessimistic outcomes)\n- Black swan event probabilities (extreme scenarios)\n- Competitive response probabilities\n\n#### Probability Calibration Methods\n```\nUse multiple estimation approaches:\n\n1. Historical Data Analysis:\n   - Similar past decisions and outcomes\n   - Success/failure rates in comparable situations\n   - Market adoption patterns for similar offerings\n\n2. Expert Consultation:\n   - Domain expert probability estimates\n   - Cross-functional team input and perspectives\n   - External advisor and consultant insights\n\n3. Market Validation:\n   - Customer research and feedback\n   - Competitive analysis and market dynamics\n   - Regulatory and environmental factor assessment\n\n4. Monte Carlo Simulation:\n   - Run multiple probability scenarios\n   - Test sensitivity to assumption changes\n   - Generate confidence intervals for estimates\n```\n\n### 5. Expected Value Calculation\n\n**Quantify decision outcomes systematically:**\n\n#### Outcome Quantification\n- Financial returns and cost implications\n- Strategic value and competitive advantages\n- Risk reduction and option value creation\n- Time savings and efficiency improvements\n- Learning value and capability building\n\n#### Multi-Dimensional Value Assessment\n```\nValue Calculation Framework:\n\nFinancial Value:\n- Direct Revenue Impact: $[amount]  [uncertainty range]\n- Cost Savings: $[amount]  [uncertainty range]\n- Investment Required: $[amount] and timeline\n- NPV Calculation: $[net present value] over [timeframe]\n\nStrategic Value:\n- Market Position Improvement: [qualitative + quantitative]\n- Competitive Advantage Creation: [sustainable differentiation]\n- Capability Building: [new skills and infrastructure]\n- Option Value: [future opportunities enabled]\n\nRisk Value:\n- Risk Reduction: [quantified risk mitigation]\n- Downside Protection: [worst-case scenario costs]\n- Opportunity Cost: [alternative options foregone]\n- Reversibility: [cost and difficulty of changing course]\n```\n\n#### Expected Value Integration\n```\nExpected Value Formula Application:\nEV = (Probability  Outcome Value) for all scenarios\n\nExample Calculation:\nOption A: New Product Launch\n- Best Case (20% probability): $10M revenue, 80% margin = $8M profit\n- Base Case (60% probability): $5M revenue, 70% margin = $3.5M profit  \n- Worst Case (20% probability): $1M revenue, 50% margin = $0.5M profit\n\nExpected Value = (0.20  $8M) + (0.60  $3.5M) + (0.20  $0.5M)\n= $1.6M + $2.1M + $0.1M = $3.8M\n\nInvestment Required: $2M\nNet Expected Value: $1.8M\n```\n\n### 6. Risk Analysis and Sensitivity Testing\n\n**Comprehensively assess decision risks:**\n\n#### Risk Identification Matrix\n- Implementation risks (execution challenges)\n- Market risks (demand, competition, economic changes)\n- Technology risks (technical feasibility, obsolescence)\n- Regulatory risks (compliance, approval, policy changes)\n- Resource risks (availability, capability, cost overruns)\n\n#### Sensitivity Analysis\n- Key assumption stress testing\n- Break-even analysis for critical variables\n- Scenario analysis with parameter variations\n- Confidence interval calculation for outcomes\n- Robustness testing across different conditions\n\n#### Risk Mitigation Strategy Development\n```\nRisk Mitigation Framework:\n\nFor each significant risk:\n1. Risk Description: [specific risk scenario]\n2. Probability Assessment: [likelihood of occurrence]\n3. Impact Assessment: [severity if it occurs]\n4. Early Warning Indicators: [signals to watch for]\n5. Prevention Strategies: [actions to reduce probability]\n6. Mitigation Strategies: [actions to reduce impact]\n7. Contingency Plans: [responses if risk materializes]\n8. Risk Ownership: [who monitors and responds]\n```\n\n### 7. Decision Tree Visualization and Analysis\n\n**Create clear decision tree representations:**\n\n#### Tree Structure Design\n```\nDecision Tree Format:\n\n[Decision Point] \n Option A [probability: X%]\n    Scenario A1 [probability: Y%]  Outcome: $Z\n    Scenario A2 [probability: Y%]  Outcome: $Z\n    Scenario A3 [probability: Y%]  Outcome: $Z\n Option B [probability: X%]\n    Scenario B1 [probability: Y%]  Outcome: $Z\n    Scenario B2 [probability: Y%]  Outcome: $Z\n Option C [probability: X%]\n     Scenario C1 [probability: Y%]  Outcome: $Z\n\nExpected Values:\n- Option A: $[calculated EV]\n- Option B: $[calculated EV]  \n- Option C: $[calculated EV]\n```\n\n#### Decision Path Analysis\n- Optimal path identification based on expected value\n- Alternative paths with acceptable risk/return profiles\n- Contingency routing based on early decision outcomes\n- Information value analysis (worth of additional research)\n- Real option valuation (value of delaying decisions)\n\n### 8. Optimization and Recommendation Engine\n\n**Generate data-driven decision recommendations:**\n\n#### Multi-Criteria Decision Analysis\n- Weighted scoring across multiple decision criteria\n- Trade-off analysis between competing objectives\n- Pareto frontier identification for efficient solutions\n- Stakeholder preference integration\n- Scenario robustness across different weighting schemes\n\n#### Recommendation Generation\n```\nDecision Recommendation Format:\n\n## Primary Recommendation: [Selected Option]\n\n### Executive Summary\n- Recommended Decision: [specific choice and rationale]\n- Expected Value: $[amount] with [confidence level]%\n- Key Success Factors: [critical requirements for success]\n- Major Risks: [primary concerns and mitigation approaches]\n- Implementation Timeline: [key milestones and dependencies]\n\n### Supporting Analysis\n- Expected Value Calculation: [detailed breakdown]\n- Probability Assessments: [key assumptions and sources]\n- Risk Assessment: [major risks and mitigation strategies]\n- Sensitivity Analysis: [critical variables and break-even points]\n- Alternative Options: [other viable choices and trade-offs]\n\n### Implementation Guidance\n- Immediate Next Steps: [specific actions required]\n- Success Metrics: [measurable indicators of progress]\n- Decision Points: [future choice points and triggers]\n- Resource Requirements: [budget, team, timeline needs]\n- Stakeholder Communication: [alignment and buy-in strategies]\n\n### Contingency Planning\n- Plan B Options: [alternative approaches if primary fails]\n- Early Warning Systems: [risk monitoring and triggers]\n- Decision Reversal: [exit strategies and switching costs]\n- Adaptive Strategies: [adjustment mechanisms for changing conditions]\n```\n\n### 9. Decision Quality Validation\n\n**Ensure robust decision-making process:**\n\n#### Process Quality Checklist\n- [ ] All relevant stakeholders consulted\n- [ ] Comprehensive option generation completed\n- [ ] Probability assessments calibrated with data\n- [ ] Value calculations include all material factors\n- [ ] Risks identified and mitigation planned\n- [ ] Assumptions explicitly documented and tested\n- [ ] Decision criteria clearly defined and weighted\n- [ ] Implementation feasibility validated\n\n#### Bias Detection and Mitigation\n- Confirmation bias: Seeking information that supports preferences\n- Anchoring bias: Over-relying on first information received\n- Availability bias: Overweighting easily recalled examples\n- Optimism bias: Overestimating positive outcomes\n- Sunk cost fallacy: Continuing failed approaches\n- Analysis paralysis: Over-analyzing instead of deciding\n\n#### Decision Documentation\n- Decision rationale and supporting analysis\n- Key assumptions and probability assessments\n- Alternative options considered and rejected\n- Stakeholder input and consultation process\n- Risk assessment and mitigation strategies\n- Implementation plan and success metrics\n\n### 10. Learning and Feedback Integration\n\n**Establish decision quality improvement:**\n\n#### Decision Outcome Tracking\n- Actual vs. predicted outcomes measurement\n- Assumption validation against real results\n- Decision timing and implementation effectiveness\n- Stakeholder satisfaction and support levels\n- Unintended consequences and side effects\n\n#### Continuous Improvement\n- Decision-making process refinement\n- Probability calibration improvement over time\n- Risk assessment accuracy enhancement\n- Stakeholder engagement optimization\n- Tool and framework evolution\n\n## Usage Examples\n\n```bash\n# Strategic business decision\n/simulation:decision-tree-explorer Should we acquire competitor X for $50M or build competing product internally?\n\n# Product development prioritization\n/simulation:decision-tree-explorer Which of 5 product features should we build first given limited engineering resources?\n\n# Technology architecture choice\n/simulation:decision-tree-explorer Microservices vs monolith architecture for our new platform?\n\n# Market expansion decision\n/simulation:decision-tree-explorer European market entry strategy: direct expansion vs partnership vs acquisition?\n```\n\n## Quality Indicators\n\n- **Green**: Comprehensive options, calibrated probabilities, quantified outcomes, documented assumptions\n- **Yellow**: Good option coverage, reasonable probability estimates, partially quantified outcomes\n- **Red**: Limited options, uncalibrated probabilities, qualitative-only outcomes\n\n## Common Pitfalls to Avoid\n\n- Analysis paralysis: Over-analyzing instead of making timely decisions\n- False precision: Using precise numbers for uncertain estimates  \n- Option tunnel vision: Not considering creative alternatives\n- Probability miscalibration: Overconfidence in likelihood estimates\n- Value tunnel vision: Focusing only on financial outcomes\n- Implementation blindness: Not considering execution challenges\n\nTransform complex decisions into systematic analysis for exponentially better choice outcomes."
              },
              {
                "name": "/digital-twin-creator",
                "description": "Create systematic digital twins with data quality validation and real-world calibration loops.",
                "path": "plugins/commands-simulation-modeling/commands/digital-twin-creator.md",
                "frontmatter": {
                  "description": "Create systematic digital twins with data quality validation and real-world calibration loops.",
                  "category": "simulation-modeling",
                  "argument-hint": "Specify digital twin parameters"
                },
                "content": "# Digital Twin Creator\n\nCreate systematic digital twins with data quality validation and real-world calibration loops.\n\n## Instructions\n\nYou are tasked with creating a comprehensive digital twin to simulate real-world systems, processes, or entities. Follow this systematic approach to build an accurate, calibrated model: **$ARGUMENTS**\n\n### 1. Prerequisites Assessment\n\n**Critical Information Validation:**\n\n- **Twin Subject**: What specific system/process/entity are you modeling?\n- **Purpose & Decisions**: What decisions will this twin inform?\n- **Fidelity Level**: How accurate does the simulation need to be?\n- **Data Availability**: What real-world data can calibrate the model?\n- **Update Frequency**: How often will the twin sync with reality?\n\n**If any prerequisites are missing, guide the user:**\n\n```\nMissing Twin Subject:\n\"I need clarity on what you're modeling. Are you creating a digital twin for:\n- Physical systems: Manufacturing line, vehicle performance, building operations\n- Business processes: Sales pipeline, customer journey, supply chain\n- Market dynamics: Customer segments, competitive landscape, demand patterns\n- Technical systems: Software performance, network behavior, user interactions\"\n\nMissing Purpose Clarity:\n\"What specific decisions will this digital twin help you make?\n- Optimization: Finding better configurations or strategies\n- Prediction: Forecasting future outcomes or behaviors  \n- Risk Assessment: Understanding failure modes and vulnerabilities\n- Experimentation: Testing changes before real-world implementation\n- Monitoring: Detecting anomalies or performance degradation\"\n\nMissing Fidelity Requirements:\n\"How precise does your digital twin need to be?\n- High Fidelity (90%+ accuracy): Critical safety/financial decisions\n- Medium Fidelity (70-90% accuracy): Strategic planning and optimization\n- Low Fidelity (50-70% accuracy): Conceptual understanding and exploration\"\n```\n\n### 2. System Architecture Definition\n\n**Map the structure and boundaries of your target system:**\n\n#### System Components\n- Core elements and their relationships\n- Input/output interfaces and data flows\n- Control mechanisms and feedback loops\n- Performance metrics and success indicators\n- Failure modes and edge cases\n\n#### Boundary Definition\n- What's included vs. excluded from the model\n- External dependencies and influences\n- Environmental constraints and variables\n- Time horizons and operational contexts\n- Abstraction levels and detail granularity\n\n#### Relationship Mapping\n- Causal relationships between components\n- Correlation patterns and dependencies\n- Feedback loops and system dynamics\n- Emergent behaviors and non-linear effects\n- Lag times and temporal relationships\n\n**Quality Gate**: Validate that your system definition is:\n- Complete enough for the intended purpose\n- Bounded to avoid unnecessary complexity\n- Focused on factors that impact key decisions\n- Grounded in observable reality\n\n### 3. Data Foundation Assessment\n\n**Evaluate and improve data quality systematically:**\n\n#### Data Inventory\n- Historical performance data and patterns\n- Real-time sensor/monitoring data streams\n- Configuration settings and parameters\n- External data sources and market conditions\n- Expert knowledge and domain insights\n\n#### Data Quality Analysis\n```\nFor each data source, assess:\n- Completeness: What percentage of required data is available?\n- Accuracy: How reliable and error-free is the data?\n- Timeliness: How current and frequently updated is the data?\n- Consistency: Are there conflicts between data sources?\n- Relevance: How directly does this data impact key decisions?\n\nQuality Scoring (1-10 for each dimension):\nData Source: [name]\n- Completeness: [score] - [explanation]\n- Accuracy: [score] - [explanation]  \n- Timeliness: [score] - [explanation]\n- Consistency: [score] - [explanation]\n- Relevance: [score] - [explanation]\nOverall Quality Score: [average]\n```\n\n#### Data Gap Analysis\n- Critical missing information for model accuracy\n- Alternative data sources or proxies available\n- Data collection strategies for key gaps\n- Acceptable uncertainty levels for decisions\n\n### 4. Model Construction Framework\n\n**Build the digital twin using systematic modeling approaches:**\n\n#### Component Modeling\n- Individual element behavior patterns\n- Performance characteristics and ranges\n- Response functions to different inputs\n- Degradation patterns and lifecycle factors\n- Optimization parameters and constraints\n\n#### System Interaction Modeling\n- Interface behaviors between components\n- Network effects and cascade influences\n- Resource sharing and competition dynamics\n- Communication protocols and latencies\n- Synchronization and coordination mechanisms\n\n#### Environmental Modeling\n- External factors affecting system performance\n- Market conditions and competitive dynamics\n- Regulatory constraints and compliance requirements\n- Economic factors and cost structures\n- Seasonal patterns and cyclical behaviors\n\n#### Dynamic Behavior Modeling\n- State transitions and evolutionary patterns\n- Learning and adaptation mechanisms\n- Scaling behaviors and capacity constraints\n- Stability and resilience characteristics\n- Performance under stress conditions\n\n### 5. Calibration and Validation\n\n**Ensure model accuracy through systematic testing:**\n\n#### Historical Validation\n- Back-test model predictions against known outcomes\n- Identify systematic biases and correction factors\n- Validate model accuracy across different conditions\n- Test edge cases and extreme scenarios\n- Measure prediction error distributions\n\n#### Real-Time Calibration\n- Compare model outputs to live system data\n- Implement automated calibration adjustments\n- Monitor prediction accuracy over time\n- Detect model drift and degradation\n- Update parameters based on new observations\n\n#### Sensitivity Analysis\n- Test model response to parameter variations\n- Identify critical assumptions and dependencies\n- Understand uncertainty propagation through model\n- Validate robustness to data quality issues\n- Map confidence intervals for predictions\n\n**Calibration Metrics**:\n```\nModel Performance Dashboard:\n- Overall Accuracy: [percentage]  [confidence interval]\n- Prediction Bias: [systematic error analysis]\n- Timing Accuracy: [lag prediction accuracy]\n- Extreme Event Prediction: [edge case performance]\n- Model Confidence: [uncertainty quantification]\n\nRecent Calibration Results:\n- Last Update: [timestamp]\n- Data Points Used: [count]\n- Accuracy Improvement: [change from previous]\n- Key Parameter Adjustments: [list]\n- Validation Test Results: [pass/fail with details]\n```\n\n### 6. Scenario Simulation Engine\n\n**Enable comprehensive scenario testing:**\n\n#### Scenario Design Framework\n- Baseline/current state scenarios\n- Optimization scenarios testing improvements\n- Stress test scenarios with adverse conditions\n- What-if scenarios exploring alternatives\n- Innovation scenarios with new capabilities\n\n#### Simulation Execution\n- Automated scenario batch processing\n- Interactive scenario exploration interfaces\n- Real-time simulation monitoring and controls\n- Result aggregation and statistical analysis\n- Sensitivity testing across scenario parameters\n\n#### Output Generation\n- Performance metrics and KPI tracking\n- Visual simulation results and animations\n- Statistical analysis and confidence intervals\n- Comparative analysis across scenarios\n- Recommendation generation with rationale\n\n### 7. Decision Integration\n\n**Connect simulation insights to actionable decisions:**\n\n#### Decision Framework Mapping\n- Link simulation outputs to specific decisions\n- Define decision criteria and thresholds\n- Map uncertainty levels to decision confidence\n- Establish risk tolerance for different choices\n- Create decision trees for complex scenarios\n\n#### Optimization Algorithms\n- Automated parameter optimization for goals\n- Multi-objective optimization with trade-offs\n- Constraint satisfaction for feasible solutions\n- Robust optimization under uncertainty\n- Dynamic optimization for changing conditions\n\n#### Recommendation Engine\n```\nDecision Recommendation Format:\n## Scenario: [name and description]\n\n### Recommended Action: [specific decision]\n\n### Rationale:\n- Simulation Evidence: [key findings]\n- Performance Impact: [quantified benefits]\n- Risk Assessment: [potential downsides]\n- Confidence Level: [percentage with explanation]\n\n### Implementation Guidance:\n- Immediate Actions: [specific steps]\n- Success Metrics: [measurable indicators]\n- Monitoring Plan: [ongoing validation approach]\n- Contingency Plans: [alternative actions if needed]\n\n### Assumptions and Limitations:\n- Key Assumptions: [critical model assumptions]\n- Data Limitations: [known gaps or uncertainties]\n- Model Boundaries: [what's not included]\n- Update Requirements: [when to refresh model]\n```\n\n### 8. Continuous Improvement Loop\n\n**Establish ongoing model enhancement:**\n\n#### Performance Monitoring\n- Automated accuracy tracking and alerting\n- Model drift detection and correction\n- Prediction error analysis and categorization\n- Data quality monitoring and improvement\n- User feedback collection and integration\n\n#### Model Evolution\n- Incremental model improvements based on learnings\n- New data integration and model expansion\n- Algorithm updates and enhancement\n- Scenario library expansion and refinement\n- User interface and experience improvements\n\n#### Learning Integration\n- Document insights from model successes and failures\n- Build institutional knowledge from simulation results\n- Share best practices across similar digital twins\n- Incorporate domain expert feedback and validation\n- Develop model confidence and reliability metrics\n\n### 9. Output Generation\n\n**Present digital twin capabilities and insights:**\n\n```\n## Digital Twin System: [Subject Name]\n\n### System Overview\n- Purpose: [primary decision support goals]\n- Scope: [system boundaries and components]\n- Fidelity Level: [accuracy expectations]\n- Update Frequency: [refresh schedule]\n\n### Model Architecture\n- Core Components: [key system elements]\n- Relationship Map: [interaction patterns]\n- Environmental Factors: [external influences]\n- Performance Metrics: [success indicators]\n\n### Data Foundation\n- Primary Data Sources: [list with quality scores]\n- Data Quality Assessment: [overall quality rating]\n- Update Mechanisms: [how data stays current]\n- Validation Methods: [accuracy verification approaches]\n\n### Simulation Capabilities\n- Scenario Types: [what can be modeled]\n- Time Horizons: [simulation time ranges]\n- Precision Levels: [accuracy expectations]\n- Output Formats: [reporting and visualization options]\n\n### Calibration Status\n- Historical Validation: [back-testing results]\n- Real-Time Accuracy: [current performance metrics]\n- Last Calibration: [date and improvements]\n- Confidence Intervals: [uncertainty bounds]\n\n### Decision Integration\n- Supported Decisions: [specific use cases]\n- Optimization Capabilities: [automatic improvement features]\n- Risk Assessment: [uncertainty and sensitivity analysis]\n- Recommendation Engine: [decision support features]\n\n### Usage Guidelines\n- High Confidence Scenarios: [when to trust fully]\n- Medium Confidence Scenarios: [when to use with caution]\n- Low Confidence Scenarios: [when to gather more data]\n- Refresh Triggers: [when to update the model]\n```\n\n### 10. Quality Assurance Framework\n\n**Ensure digital twin reliability and trustworthiness:**\n\n#### Validation Checklist\n- [ ] Model reproduces historical behavior accurately\n- [ ] Predictions are calibrated with confidence intervals\n- [ ] Edge cases and extreme scenarios are handled appropriately\n- [ ] Data quality meets requirements for intended decisions\n- [ ] Model boundaries are clearly defined and communicated\n- [ ] Assumptions are documented and regularly validated\n- [ ] Updates and maintenance procedures are established\n- [ ] User training and guidelines are comprehensive\n\n#### Risk Assessment\n- Model accuracy limitations and impact on decisions\n- Data dependency risks and mitigation strategies\n- Computational requirements and scalability constraints\n- User misinterpretation risks and training needs\n- System integration challenges and compatibility issues\n\n#### Success Metrics\n- Prediction accuracy improvement over time\n- Decision quality enhancement from model insights\n- Cost savings or performance improvements achieved\n- User adoption and satisfaction with digital twin\n- Model maintenance efficiency and cost effectiveness\n\n## Usage Examples\n\n```bash\n# Manufacturing optimization\n/simulation:digital-twin-creator Create digital twin of production line to optimize throughput and predict maintenance needs\n\n# Customer journey modeling\n/simulation:digital-twin-creator Build digital twin of customer acquisition funnel to test marketing strategies\n\n# Supply chain resilience\n/simulation:digital-twin-creator Model supply chain network to test disruption scenarios and optimization strategies\n\n# Software system performance\n/simulation:digital-twin-creator Create digital twin of microservices architecture to predict scaling and performance\n```\n\n## Quality Indicators\n\n- **Green**: 85%+ historical accuracy, comprehensive data foundation, automated calibration\n- **Yellow**: 70-85% accuracy, good data coverage, manual calibration processes\n- **Red**: <70% accuracy, significant data gaps, limited validation\n\n## Common Pitfalls to Avoid\n\n- Over-complexity: Modeling unnecessary details that don't impact decisions\n- Under-validation: Insufficient testing against real-world outcomes  \n- Static thinking: Not updating model as reality changes\n- Data blindness: Ignoring data quality issues and biases\n- False precision: Claiming higher accuracy than data supports\n- Poor boundaries: Including too much or too little in model scope\n\nTransform your real-world challenges into a laboratory for exponential learning and optimization."
              },
              {
                "name": "/future-scenario-generator",
                "description": "Generate and analyze future scenarios with plausibility scoring, trend integration, and uncertainty quantification.",
                "path": "plugins/commands-simulation-modeling/commands/future-scenario-generator.md",
                "frontmatter": {
                  "description": "Generate and analyze future scenarios with plausibility scoring, trend integration, and uncertainty quantification.",
                  "category": "simulation-modeling",
                  "argument-hint": "Specify scenario parameters",
                  "allowed-tools": "Glob"
                },
                "content": "# Future Scenario Generator\n\nGenerate and analyze future scenarios with plausibility scoring, trend integration, and uncertainty quantification.\n\n## Instructions\n\nYou are tasked with systematically generating comprehensive future scenarios to explore potential developments and prepare for multiple possible futures. Follow this approach: **$ARGUMENTS**\n\n### 1. Prerequisites Assessment\n\n**Critical Scenario Context Validation:**\n\n- **Time Horizon**: What future timeframe are you exploring (1-3-5-10+ years)?\n- **Domain Focus**: What specific area/industry/system are you analyzing?\n- **Key Variables**: What factors could significantly shape the future?\n- **Decision Impact**: How will these scenarios inform specific decisions?\n- **Uncertainty Level**: What's the acceptable range of scenario uncertainty?\n\n**If context is unclear, guide systematically:**\n\n```\nMissing Time Horizon:\n\"What future timeframe should we explore?\n- Near-term (1-2 years): Market shifts, competitive moves, technology adoption\n- Medium-term (3-5 years): Industry transformation, regulatory changes, generational shifts  \n- Long-term (5-10+ years): Fundamental technology disruption, societal changes, paradigm shifts\n\nEach timeframe requires different scenario methodologies and uncertainty management.\"\n\nMissing Domain Focus:\n\"What specific domain or system should we model future scenarios for?\n- Business/Industry: Market evolution, competitive landscape, customer behavior\n- Technology: Platform shifts, capability development, adoption patterns\n- Society/Culture: Demographic changes, value shifts, behavior evolution\n- Economy/Policy: Regulatory changes, economic cycles, political developments\"\n```\n\n### 2. Trend Analysis Foundation\n\n**Systematically analyze current trends as scenario building blocks:**\n\n#### Trend Identification Framework\n```\nMulti-Dimensional Trend Analysis:\n\nTechnology Trends:\n- Emerging technologies and adoption curves\n- Infrastructure development and capability expansion\n- Platform shifts and ecosystem evolution\n- Innovation cycles and breakthrough potential\n\nSocial/Cultural Trends:\n- Demographic shifts and generational changes\n- Value system evolution and priority shifts\n- Behavior pattern changes and lifestyle adaptation\n- Communication and interaction pattern evolution\n\nEconomic Trends:\n- Market structure changes and industry evolution\n- Investment patterns and capital allocation shifts\n- Globalization and trade pattern modifications\n- Economic cycle positioning and policy directions\n\nRegulatory/Policy Trends:\n- Regulatory environment evolution and compliance requirements\n- Policy direction changes and government priorities\n- International relations and trade agreement impacts\n- Legal framework development and enforcement patterns\n```\n\n#### Trend Trajectory Modeling\n- Linear progression scenarios (current trends continue)\n- Acceleration scenarios (trends speed up dramatically)\n- Deceleration scenarios (trends slow down or plateau)\n- Reversal scenarios (trends change direction)\n- Disruption scenarios (trends are fundamentally altered)\n\n### 3. Scenario Architecture Design\n\n**Structure comprehensive scenario frameworks:**\n\n#### Scenario Generation Methodology\n```\nSystematic Scenario Construction:\n\nCross-Impact Analysis:\n- Identify key driving forces and variables\n- Analyze interaction effects between different trends\n- Map reinforcing and conflicting trend combinations\n- Model cascade effects and secondary impacts\n\nMorphological Analysis:\n- Define key dimensions of future variation\n- Identify possible states for each dimension\n- Generate scenario combinations systematically\n- Evaluate scenario consistency and plausibility\n\nNarrative Scenario Development:\n- Create compelling future stories and visions\n- Integrate quantitative trends with qualitative insights\n- Develop scenario logic and causal narratives\n- Ensure scenario diversity and comprehensive coverage\n```\n\n#### Scenario Categorization Framework\n```\nScenario Portfolio Structure:\n\nBaseline Scenarios (30-40% of portfolio):\n- Continuation of current trends with normal variation\n- Evolutionary change within existing paradigms\n- Moderate uncertainty and predictable development patterns\n\nOptimistic Scenarios (20-25% of portfolio):\n- Favorable trend convergence and positive developments\n- Breakthrough innovations and acceleration opportunities\n- Best-case outcome realization and synergy effects\n\nPessimistic Scenarios (20-25% of portfolio):\n- Adverse trend combinations and negative developments\n- Crisis scenarios and system stress conditions\n- Worst-case outcome realization and cascade failures\n\nTransformation Scenarios (15-20% of portfolio):\n- Paradigm shifts and fundamental system changes\n- Disruptive innovation and market restructuring\n- Wild card events and black swan developments\n```\n\n### 4. Plausibility Assessment Framework\n\n**Systematically evaluate scenario credibility:**\n\n#### Plausibility Scoring Methodology\n```\nMulti-Criteria Plausibility Assessment:\n\nHistorical Precedent (25% weight):\n- Similar patterns and developments in historical context\n- Analogous situations and outcome patterns\n- Learning from past trend evolution and scenario realization\n\nLogical Consistency (25% weight):\n- Internal scenario logic and causal relationships\n- Consistency between different scenario elements\n- Absence of logical contradictions and impossible combinations\n\nExpert Validation (25% weight):\n- Domain expert assessment and credibility evaluation\n- Stakeholder input and perspective integration\n- Professional judgment and experience-based validation\n\nEmpirical Support (25% weight):\n- Current data and trend evidence supporting scenario elements\n- Quantitative model outputs and statistical projections\n- Research findings and academic literature support\n\nPlausibility Score = (Historical  0.25) + (Logical  0.25) + (Expert  0.25) + (Empirical  0.25)\n```\n\n#### Uncertainty Quantification\n- Confidence intervals for key scenario parameters\n- Sensitivity analysis for critical assumptions\n- Monte Carlo simulation for probability distributions\n- Expert elicitation for subjective probability assessment\n\n### 5. Wild Card and Disruption Modeling\n\n**Incorporate low-probability, high-impact events:**\n\n#### Wild Card Event Framework\n```\nSystematic Disruption Analysis:\n\nTechnology Wild Cards:\n- Breakthrough innovations and paradigm shifts\n- Technology convergence and unexpected capabilities\n- Platform disruptions and ecosystem transformations\n- Artificial intelligence and automation breakthroughs\n\nSocial Wild Cards:\n- Generational value shifts and behavior changes\n- Social movement emergence and cultural transformations\n- Demographic surprises and migration patterns\n- Communication and social interaction disruptions\n\nEconomic Wild Cards:\n- Financial system disruptions and market structure changes\n- Resource scarcity or abundance surprises\n- Currency and monetary system transformations\n- Trade pattern disruptions and economic bloc changes\n\nEnvironmental/Political Wild Cards:\n- Climate change acceleration or mitigation breakthroughs\n- Geopolitical shifts and international relation changes\n- Natural disasters and pandemic impacts\n- Regulatory surprises and policy paradigm shifts\n```\n\n#### Disruption Impact Modeling\n- Direct impact assessment on key scenario variables\n- Cascade effect analysis through system dependencies\n- Adaptation and recovery scenario development\n- Resilience and vulnerability analysis\n\n### 6. Scenario Integration and Synthesis\n\n**Combine scenarios into comprehensive future landscape:**\n\n#### Cross-Scenario Analysis\n```\nScenario Portfolio Analysis:\n\nScenario Clustering:\n- Group similar scenarios and identify common patterns\n- Analyze scenario divergence points and branching factors\n- Map scenario transition probabilities and pathways\n- Identify robust strategies across multiple scenarios\n\nScenario Interaction Effects:\n- How scenarios might combine or influence each other\n- Sequential scenario development and evolution patterns\n- Scenario switching triggers and transition indicators\n- Portfolio effects of scenario diversification\n\nKey Insight Synthesis:\n- Common themes and patterns across scenarios\n- Critical uncertainties and decision-relevant factors\n- Robust trends that appear in most scenarios\n- Strategic implications and opportunity identification\n```\n\n#### Scenario Narrative Development\n- Compelling future stories that integrate multiple trends\n- Character and stakeholder perspective integration\n- Timeline development and milestone identification\n- Vivid details that make scenarios memorable and actionable\n\n### 7. Decision Integration Framework\n\n**Connect scenarios to actionable strategic insights:**\n\n#### Strategy Testing Against Scenarios\n```\nScenario-Based Strategy Evaluation:\n\nStrategy Robustness Analysis:\n- How well do current strategies perform across scenarios?\n- Which scenarios pose the greatest strategic challenges?\n- What strategy modifications improve cross-scenario performance?\n- Where are the greatest strategy vulnerabilities and dependencies?\n\nOption Value Analysis:\n- What strategic options provide value across multiple scenarios?\n- Which investments maintain flexibility for different futures?\n- How can strategies be designed for adaptive capability?\n- What early warning systems enable strategy adjustment?\n\nContingency Planning:\n- Specific response strategies for different scenario realizations\n- Resource allocation across scenarios and strategy options\n- Decision trigger identification and monitoring systems\n- Implementation readiness for scenario-specific strategies\n```\n\n#### Strategic Recommendation Generation\n```\nScenario-Informed Strategy Framework:\n\n## Future Scenario Analysis: [Domain/Project Name]\n\n### Scenario Portfolio Summary\n- Time Horizon: [analysis period]\n- Key Driving Forces: [primary variables analyzed]\n- Scenarios Generated: [number and types]\n- Plausibility Range: [confidence levels]\n\n### High-Impact Scenarios\n\n#### Scenario 1: [Name - Plausibility Score]\n- Timeline: [key development milestones]\n- Driving Forces: [primary trends and factors]\n- Key Characteristics: [distinctive features]\n- Strategic Implications: [decision impacts]\n\n[Repeat for top 4-6 scenarios]\n\n### Cross-Scenario Insights\n- Robust Trends: [patterns appearing in most scenarios]\n- Critical Uncertainties: [factors determining scenario outcomes]\n- Strategic Vulnerabilities: [areas of risk across scenarios]\n- Opportunity Convergence: [areas of opportunity across scenarios]\n\n### Strategic Recommendations\n- Core Strategy: [approach that works across multiple scenarios]\n- Scenario-Specific Tactics: [adaptations for different scenarios]\n- Early Warning Indicators: [signals for scenario realization]\n- Strategic Options: [investments that maintain flexibility]\n\n### Monitoring and Adaptation Framework\n- Key Indicators: [metrics to track scenario development]\n- Decision Triggers: [when to adjust strategy based on signals]\n- Contingency Plans: [specific responses for different scenarios]\n- Review Schedule: [when to update scenario analysis]\n```\n\n### 8. Continuous Scenario Evolution\n\n**Establish ongoing scenario refinement and updating:**\n\n#### Real-World Validation\n- Track actual developments against scenario predictions\n- Update scenario probabilities based on emerging evidence\n- Refine scenario assumptions based on real-world feedback\n- Learn from scenario accuracy and prediction quality\n\n#### Adaptive Scenario Management\n- Regular scenario refresh and update cycles\n- New information integration and scenario modification\n- Stakeholder feedback incorporation and perspective updates\n- Methodology improvement based on scenario performance\n\n## Usage Examples\n\n```bash\n# Industry transformation scenarios\n/simulation:future-scenario-generator Generate scenarios for AI's impact on healthcare industry over next 10 years\n\n# Technology adoption scenarios\n/simulation:future-scenario-generator Model future scenarios for remote work technology adoption and workplace evolution\n\n# Market evolution scenarios  \n/simulation:future-scenario-generator Explore scenarios for sustainable energy market development and regulatory changes\n\n# Competitive landscape scenarios\n/simulation:future-scenario-generator Generate scenarios for fintech industry evolution and traditional banking disruption\n```\n\n## Quality Indicators\n\n- **Green**: Diverse scenario portfolio, validated plausibility scores, integrated wild cards\n- **Yellow**: Good scenario variety, reasonable plausibility assessment, some disruption modeling\n- **Red**: Limited scenario diversity, unvalidated assumptions, missing disruption analysis\n\n## Common Pitfalls to Avoid\n\n- Present bias: Projecting current conditions too strongly into the future\n- Linear thinking: Assuming trends continue unchanged without acceleration or disruption\n- Probability illusion: Being overconfident in specific scenario likelihoods\n- Complexity underestimation: Not modeling interaction effects between trends\n- Wild card blindness: Ignoring low-probability, high-impact events\n- Action paralysis: Generating scenarios without connecting to decisions\n\nTransform uncertainty into strategic advantage through systematic future scenario exploration and preparation."
              },
              {
                "name": "/market-response-modeler",
                "description": "Model customer and market responses with segment analysis, behavioral prediction, and response optimization.",
                "path": "plugins/commands-simulation-modeling/commands/market-response-modeler.md",
                "frontmatter": {
                  "description": "Model customer and market responses with segment analysis, behavioral prediction, and response optimization.",
                  "category": "simulation-modeling",
                  "argument-hint": "Specify market response parameters"
                },
                "content": "# Market Response Modeler\n\nModel customer and market responses with segment analysis, behavioral prediction, and response optimization.\n\n## Instructions\n\nYou are tasked with creating a comprehensive market response simulation to predict customer and market reactions to business decisions. Follow this systematic approach: **$ARGUMENTS**\n\n### 1. Prerequisites Assessment\n\n**Critical Market Context Validation:**\n\n- **Market Definition**: What specific market/customer segments are you analyzing?\n- **Response Trigger**: What action/change will you be modeling responses to?\n- **Response Metrics**: How do you measure market response success?\n- **Data Availability**: What customer/market data can inform the model?\n- **Time Horizons**: What response timeframes are you analyzing?\n\n**If any context is missing, guide systematically:**\n\n```\nMissing Market Definition:\n\"I need clarity on the market scope you're analyzing:\n- Geographic Scope: Local, regional, national, or global markets?\n- Customer Segments: B2B vs B2C, demographics, firmographics, psychographics?\n- Market Size: TAM, SAM, SOM estimates and definitions?\n- Competitive Landscape: Direct competitors, substitutes, market dynamics?\n\nExamples:\n- 'Enterprise SaaS customers in North America with 100-1000 employees'\n- 'Millennial consumers in urban areas interested in sustainable products'\n- 'Small businesses in retail seeking digital transformation solutions'\"\n\nMissing Response Trigger:\n\"What specific action or change will trigger market responses?\n- Product Launches: New products, features, or service offerings\n- Pricing Changes: Price increases, decreases, or structure modifications  \n- Marketing Campaigns: Advertising, promotions, or positioning changes\n- Market Entry: Geographic expansion or new segment targeting\n- Competitive Actions: Response to competitor moves or market disruption\n\nPlease specify the exact trigger and its characteristics.\"\n\nMissing Response Metrics:\n\"How will you measure and define market response success?\n- Awareness Metrics: Brand recognition, message recall, consideration\n- Engagement Metrics: Website traffic, content interaction, social engagement\n- Conversion Metrics: Lead generation, trial signups, purchase behavior\n- Retention Metrics: Customer satisfaction, repeat purchase, loyalty\n- Market Metrics: Market share, competitive positioning, price premiums\"\n```\n\n### 2. Market Segmentation Framework\n\n**Define and analyze market segments systematically:**\n\n#### Segmentation Methodology\n- Demographic segmentation (age, income, geography, company size)\n- Behavioral segmentation (usage patterns, purchase behavior, loyalty)\n- Psychographic segmentation (values, attitudes, lifestyle, motivations)\n- Needs-based segmentation (functional, emotional, social needs)\n- Journey stage segmentation (awareness, consideration, decision, retention)\n\n#### Segment Characterization\n```\nFor each identified segment:\n\nSegment Profile:\n- Name: [descriptive segment name]\n- Size: [number of customers/prospects]\n- Value: [revenue potential and profitability]\n- Growth: [segment growth rate and trajectory]\n- Accessibility: [how easily can you reach them]\n\nBehavioral Patterns:\n- Purchase Decision Process: [how they buy]\n- Decision Timeframes: [how long decisions take]\n- Key Influencers: [who affects their decisions]\n- Information Sources: [where they research and learn]\n- Pain Points: [major problems and frustrations]\n\nResponse Characteristics:\n- Adoption Speed: [early adopter vs laggard tendencies]\n- Price Sensitivity: [elasticity and value perception]\n- Channel Preferences: [how they prefer to engage]\n- Communication Style: [messaging that resonates]\n- Risk Tolerance: [willingness to try new things]\n```\n\n#### Segment Prioritization\n- Strategic importance and alignment with business goals\n- Market size and growth potential assessment\n- Competitive positioning and advantage analysis\n- Resource requirements and capability fit\n- Response likelihood and conversion potential\n\n### 3. Response Behavior Modeling\n\n**Map customer response patterns and drivers:**\n\n#### Response Journey Mapping\n- Awareness stage responses (attention, interest, recognition)\n- Consideration stage responses (evaluation, comparison, preference)\n- Decision stage responses (purchase intent, trial, adoption)\n- Experience stage responses (satisfaction, usage, value realization)\n- Advocacy stage responses (retention, referral, expansion)\n\n#### Response Driver Analysis\n```\nResponse Driver Categories:\n\nRational Drivers:\n- Functional Benefits: [specific value propositions]\n- Economic Value: [ROI, cost savings, price advantage]\n- Risk Mitigation: [security, reliability, compliance]\n- Convenience Factors: [ease of use, accessibility, integration]\n\nEmotional Drivers:\n- Status and Prestige: [brand association, social signaling]\n- Security and Safety: [trust, stability, protection]\n- Achievement and Success: [accomplishment, progress, growth]\n- Social Connection: [belonging, community, shared values]\n\nSocial Drivers:\n- Peer Influence: [recommendations, social proof, testimonials]\n- Authority Endorsement: [expert opinions, certifications, awards]\n- Social Norms: [industry standards, best practices, trends]\n- Network Effects: [ecosystem value, platform benefits]\n```\n\n#### Response Intensity Modeling\n- Response magnitude estimation (small, medium, large impact)\n- Response timing prediction (immediate, short-term, long-term)\n- Response duration forecasting (temporary, sustained, permanent)\n- Response quality assessment (superficial vs deep engagement)\n\n### 4. Competitive Response Integration\n\n**Model competitive dynamics and market interactions:**\n\n#### Competitive Landscape Analysis\n- Direct competitor identification and positioning\n- Substitute product and service threats\n- Competitive advantage assessment and sustainability\n- Market share dynamics and trend analysis\n- Competitive response history and patterns\n\n#### Competitive Response Prediction\n```\nCompetitor Response Framework:\n\nFor each major competitor:\n- Response Likelihood: [probability of competitive reaction]\n- Response Speed: [how quickly they typically react]\n- Response Magnitude: [scale and intensity of typical responses]\n- Response Type: [pricing, product, marketing, or strategic responses]\n- Response Effectiveness: [historical success of their responses]\n\nMarket Dynamic Effects:\n- Price War Potential: [likelihood and impact of price competition]\n- Innovation Arms Race: [feature/capability competition dynamics]\n- Market Share Battles: [customer acquisition and retention competition]\n- Channel Conflicts: [distribution and partnership competition]\n```\n\n#### Market Equilibrium Modeling\n- New equilibrium state prediction after market responses\n- Time to equilibrium estimation and transition dynamics\n- Stability analysis of new market configurations\n- Secondary effect propagation through market ecosystem\n\n### 5. Response Simulation Engine\n\n**Create dynamic response modeling capabilities:**\n\n#### Scenario Development\n- Base case scenarios with expected market conditions\n- Optimistic scenarios with favorable response assumptions\n- Pessimistic scenarios with adverse market reactions\n- Disruption scenarios with unexpected market changes\n- Competitive scenarios with various competitor responses\n\n#### Response Wave Modeling\n```\nResponse Timeline Framework:\n\nImmediate Response (0-30 days):\n- Early adopter engagement and initial reactions\n- Social media buzz and viral potential assessment\n- Competitor monitoring and immediate countermoves\n- Channel partner responses and support\n\nShort-term Response (1-6 months):\n- Mainstream market adoption patterns\n- Word-of-mouth effects and referral dynamics\n- Competitive response implementation and market adjustment\n- Initial customer experience and satisfaction feedback\n\nMedium-term Response (6-18 months):\n- Market penetration and segment adoption rates\n- Competitive equilibrium establishment\n- Customer lifecycle progression and retention patterns\n- Market share stabilization and positioning\n\nLong-term Response (18+ months):\n- Market maturation and saturation effects\n- Sustained competitive advantage realization\n- Customer loyalty and advocacy development\n- Secondary market effects and ecosystem impacts\n```\n\n#### Monte Carlo Simulation\n- Probability distribution modeling for key response variables\n- Random scenario generation and statistical analysis\n- Confidence interval calculation for response predictions\n- Sensitivity analysis for critical assumption variables\n\n### 6. Response Prediction Algorithms\n\n**Apply sophisticated prediction methodologies:**\n\n#### Statistical Modeling\n- Regression analysis for response prediction based on historical data\n- Time series analysis for trend and seasonality effects\n- Cluster analysis for segment-specific response patterns\n- Survival analysis for customer lifecycle and churn prediction\n\n#### Machine Learning Applications\n- Classification models for response category prediction\n- Neural networks for complex pattern recognition\n- Ensemble methods for improved prediction accuracy\n- Natural language processing for sentiment and feedback analysis\n\n#### Expert System Integration\n```\nExpert Knowledge Integration:\n\nDomain Expert Input:\n- Industry experience and pattern recognition\n- Market timing and seasonal factor insights\n- Customer psychology and behavioral understanding\n- Competitive intelligence and strategic assessment\n\nStakeholder Validation:\n- Sales team customer insight and relationship intelligence\n- Marketing team campaign response and engagement data\n- Customer success team satisfaction and retention insights\n- Product team usage pattern and feature adoption data\n\nExternal Validation:\n- Industry analyst reports and market research\n- Customer advisory board feedback and validation\n- Beta testing and pilot program results\n- Academic research and behavioral economics insights\n```\n\n### 7. Response Optimization Framework\n\n**Generate actionable response enhancement strategies:**\n\n#### Message Optimization\n- Segment-specific messaging and value proposition refinement\n- Channel-specific communication strategy development\n- Timing optimization for maximum response impact\n- Creative testing and iterative improvement frameworks\n\n#### Offering Optimization\n- Product feature prioritization based on response drivers\n- Pricing strategy optimization for segment preferences\n- Package and bundle configuration for maximum appeal\n- Service level and support optimization for satisfaction\n\n#### Channel Optimization\n- Distribution channel selection and partner optimization\n- Digital touchpoint optimization and user experience\n- Sales process optimization for conversion improvement\n- Customer service optimization for satisfaction and retention\n\n### 8. Validation and Calibration\n\n**Ensure model accuracy and reliability:**\n\n#### Historical Validation\n- Back-testing model predictions against known market responses\n- Correlation analysis between predicted and actual outcomes\n- Model accuracy assessment across different market conditions\n- Bias detection and correction for systematic errors\n\n#### Real-time Calibration\n```\nOngoing Model Improvement:\n\nData Integration:\n- Real-time response monitoring and measurement\n- Customer feedback and satisfaction tracking\n- Market research and survey data integration\n- Competitive intelligence and market dynamics monitoring\n\nModel Updates:\n- Parameter adjustment based on actual response data\n- Algorithm refinement for improved prediction accuracy\n- Segment definition updates based on observed behavior\n- Response driver prioritization based on performance\n\nValidation Metrics:\n- Prediction Accuracy: [percentage of correct predictions]\n- Response Timing Accuracy: [actual vs predicted timing]\n- Magnitude Accuracy: [actual vs predicted response size]\n- Direction Accuracy: [positive vs negative response prediction]\n```\n\n### 9. Decision Integration and Recommendations\n\n**Transform insights into actionable market strategies:**\n\n#### Strategic Recommendations\n```\nMarket Response Strategy Framework:\n\n## Market Response Analysis: [Initiative Name]\n\n### Executive Summary\n- Primary Market Opportunity: [key findings]\n- Expected Response Magnitude: [quantified predictions]\n- Optimal Timing: [recommended launch/implementation timing]\n- Resource Requirements: [budget and capability needs]\n- Success Probability: [confidence level and rationale]\n\n### Segment-Specific Strategies\n\n#### High-Response Segments:\n- Segment: [name and characteristics]\n- Expected Response: [prediction with confidence interval]\n- Recommended Approach: [specific strategy and tactics]\n- Success Metrics: [KPIs and measurement approach]\n- Timeline: [implementation and measurement schedule]\n\n#### Medium-Response Segments:\n[Similar structure for each segment]\n\n#### Low-Response Segments:\n[Evaluation of whether to target or deprioritize]\n\n### Response Enhancement Strategies\n- Message Optimization: [specific improvements recommended]\n- Offering Refinement: [product/service adjustments]\n- Channel Optimization: [distribution and engagement improvements]\n- Timing Optimization: [launch and communication scheduling]\n\n### Risk Mitigation\n- Competitive Response Contingencies: [specific preparations]\n- Market Resistance Scenarios: [alternative approaches]\n- Resource Constraint Adaptations: [scaled approaches]\n- Timeline Delay Preparations: [backup plans]\n\n### Success Measurement Framework\n- Leading Indicators: [early signals of response success]\n- Lagging Indicators: [ultimate success metrics]\n- Monitoring Schedule: [measurement frequency and responsibility]\n- Decision Points: [when to adjust strategy based on results]\n```\n\n### 10. Continuous Learning and Improvement\n\n**Establish ongoing model enhancement:**\n\n#### Response Learning System\n- Systematic capture of actual market responses\n- Pattern recognition for improved future predictions\n- Segment behavior evolution tracking and adaptation\n- Competitive response pattern learning and anticipation\n\n#### Model Evolution Framework\n- Regular model performance assessment and improvement\n- New data source integration and enhanced prediction\n- Algorithm updates and methodology advancement\n- User feedback integration and workflow optimization\n\n## Usage Examples\n\n```bash\n# Product launch response modeling\n/simulation:market-response-modeler Predict customer response to new AI-powered CRM feature across SMB and enterprise segments\n\n# Pricing strategy validation  \n/simulation:market-response-modeler Model market response to 20% price increase for premium service tier\n\n# Marketing campaign optimization\n/simulation:market-response-modeler Simulate customer segment responses to sustainability-focused brand messaging campaign\n\n# Competitive response preparation\n/simulation:market-response-modeler Analyze market response if competitor launches competing product at 30% lower price\n```\n\n## Quality Indicators\n\n- **Green**: Comprehensive segment analysis, validated response drivers, historical calibration data\n- **Yellow**: Good segment coverage, reasonable response assumptions, some validation data\n- **Red**: Limited segmentation, unvalidated assumptions, no historical benchmark data\n\n## Common Pitfalls to Avoid\n\n- Segment oversimplification: Using too broad or generic customer categories\n- Response uniformity: Assuming all segments respond similarly\n- Timing blindness: Not accounting for response timing variations\n- Competitive ignorance: Ignoring competitive response dynamics\n- Static thinking: Not modeling response evolution over time\n- Data bias: Relying on unrepresentative historical data\n\nTransform market uncertainty into strategic advantage through sophisticated response prediction and optimization."
              },
              {
                "name": "/simulation-calibrator",
                "description": "Test and refine simulation accuracy with validation loops, bias detection, and continuous improvement frameworks.",
                "path": "plugins/commands-simulation-modeling/commands/simulation-calibrator.md",
                "frontmatter": {
                  "description": "Test and refine simulation accuracy with validation loops, bias detection, and continuous improvement frameworks.",
                  "category": "simulation-modeling",
                  "argument-hint": "Specify calibration parameters"
                },
                "content": "# Simulation Calibrator\n\nTest and refine simulation accuracy with validation loops, bias detection, and continuous improvement frameworks.\n\n## Instructions\n\nYou are tasked with systematically calibrating simulations to ensure accuracy, reliability, and actionable insights. Follow this approach: **$ARGUMENTS**\n\n### 1. Prerequisites Assessment\n\n**Critical Calibration Context Validation:**\n\n- **Simulation Type**: What kind of simulation are you calibrating?\n- **Accuracy Requirements**: How precise does the simulation need to be?\n- **Validation Data**: What real-world data can test simulation accuracy?\n- **Decision Stakes**: How important are the decisions based on this simulation?\n- **Update Frequency**: How often should calibration be performed?\n\n**If context is unclear, guide systematically:**\n\n```\nMissing Simulation Context:\n\"What type of simulation needs calibration?\n- Business Simulations: Market response, financial projections, strategic scenarios\n- Technical Simulations: System performance, architecture behavior, scaling predictions\n- Process Simulations: Operational workflows, resource allocation, timeline predictions\n- Behavioral Simulations: Customer behavior, team dynamics, adoption patterns\n\nEach simulation type requires different calibration approaches and validation methods.\"\n\nMissing Accuracy Requirements:\n\"How accurate does your simulation need to be for effective decision-making?\n- Mission Critical (95%+ accuracy): Safety, financial, or legal decisions\n- Strategic Planning (80-95% accuracy): Investment, expansion, or major initiative decisions\n- Operational Optimization (70-80% accuracy): Process improvement and resource allocation\n- Exploratory Analysis (50-70% accuracy): Option generation and conceptual understanding\"\n```\n\n### 2. Baseline Accuracy Assessment\n\n**Establish current simulation performance levels:**\n\n#### Historical Validation Framework\n```\nSimulation Accuracy Baseline:\n\nBack-Testing Analysis:\n- Compare simulation predictions to known historical outcomes\n- Measure prediction accuracy across different time horizons\n- Identify systematic biases and error patterns\n- Assess prediction confidence calibration\n\nAccuracy Metrics:\n- Overall Prediction Accuracy: [percentage of correct predictions]\n- Directional Accuracy: [percentage of correct trend predictions]\n- Magnitude Accuracy: [percentage of predictions within acceptable error range]\n- Timing Accuracy: [percentage of events predicted within correct timeframe]\n- Confidence Calibration: [alignment between prediction confidence and actual accuracy]\n\nError Pattern Analysis:\n- Systematic Biases: [consistent over/under-estimation patterns]\n- Context Dependencies: [accuracy variations by scenario type or conditions]\n- Time Horizon Effects: [accuracy changes over different prediction periods]\n- Complexity Correlation: [accuracy relationship to scenario complexity]\n```\n\n#### Simulation Quality Scoring\n```\nQuality Assessment Framework:\n\nInput Quality (25% weight):\n- Data completeness and accuracy\n- Assumption validation and documentation\n- Expert input quality and consensus\n- Historical precedent availability\n\nModel Quality (25% weight):\n- Algorithm sophistication and appropriateness\n- Relationship modeling accuracy and completeness\n- Constraint modeling and boundary definition\n- Uncertainty quantification and propagation\n\nProcess Quality (25% weight):\n- Systematic methodology application\n- Bias detection and mitigation\n- Stakeholder validation and feedback integration\n- Documentation and reproducibility\n\nOutput Quality (25% weight):\n- Prediction accuracy and reliability\n- Insight actionability and clarity\n- Decision support effectiveness\n- Communication and presentation quality\n\nOverall Simulation Quality Score = Sum of weighted component scores\n```\n\n### 3. Systematic Bias Detection\n\n**Identify and correct simulation biases:**\n\n#### Bias Identification Framework\n```\nCommon Simulation Biases:\n\nCognitive Biases:\n- Confirmation Bias: Seeking information that supports expected outcomes\n- Anchoring Bias: Over-relying on first estimates or reference points\n- Availability Bias: Overweighting easily recalled or recent examples\n- Optimism Bias: Systematic overestimation of positive outcomes\n- Planning Fallacy: Underestimating time and resource requirements\n\nData Biases:\n- Selection Bias: Non-representative data samples\n- Survivorship Bias: Only analyzing successful cases\n- Recency Bias: Overweighting recent data points\n- Historical Bias: Assuming past patterns will continue unchanged\n- Measurement Bias: Systematic errors in data collection\n\nModel Biases:\n- Complexity Bias: Over-simplifying or over-complicating models\n- Linear Bias: Assuming linear relationships where non-linear exist\n- Static Bias: Not accounting for dynamic system changes\n- Independence Bias: Ignoring correlation and interaction effects\n- Boundary Bias: Incorrect system boundary definition\n```\n\n#### Bias Mitigation Strategies\n```\nSystematic Bias Correction:\n\nProcess-Based Mitigation:\n- Multiple perspective integration and diverse expert consultation\n- Red team analysis and devil's advocate approaches\n- Assumption challenging and alternative hypothesis testing\n- Structured decision-making and bias-aware processes\n\nData-Based Mitigation:\n- Multiple data source integration and cross-validation\n- Out-of-sample testing and validation dataset use\n- Temporal validation across different time periods\n- Segment validation across different contexts and conditions\n\nModel-Based Mitigation:\n- Ensemble modeling and multiple algorithm approaches\n- Sensitivity analysis and robust parameter testing\n- Cross-validation and bootstrap sampling\n- Bayesian updating and continuous learning integration\n```\n\n### 4. Validation Loop Design\n\n**Create systematic accuracy improvement processes:**\n\n#### Multi-Level Validation Framework\n```\nComprehensive Validation Approach:\n\nLevel 1: Internal Consistency Validation\n- Logical consistency checking and constraint satisfaction\n- Mathematical relationship verification and balance testing\n- Scenario coherence and narrative consistency\n- Assumption compatibility and interaction validation\n\nLevel 2: Expert Validation\n- Domain expert review and credibility assessment\n- Stakeholder feedback and perspective integration\n- Peer review and professional validation\n- External advisor consultation and critique\n\nLevel 3: Empirical Validation\n- Historical data comparison and pattern matching\n- Market research validation and customer feedback\n- Pilot testing and proof-of-concept validation\n- Real-world experiment and A/B testing\n\nLevel 4: Predictive Validation\n- Forward-looking accuracy testing and prediction tracking\n- Real-time outcome monitoring and comparison\n- Continuous feedback integration and model updating\n- Long-term performance assessment and trend analysis\n```\n\n#### Feedback Integration Mechanisms\n- Automated accuracy tracking and alert systems\n- Stakeholder feedback collection and analysis\n- Expert consultation and validation scheduling\n- Real-world outcome monitoring and comparison\n\n### 5. Real-Time Calibration Systems\n\n**Establish ongoing accuracy monitoring and adjustment:**\n\n#### Continuous Monitoring Framework\n```\nReal-Time Calibration Dashboard:\n\nAccuracy Tracking Metrics:\n- Current Prediction Accuracy: [real-time accuracy percentage]\n- Accuracy Trend: [improving, stable, or declining accuracy]\n- Bias Detection: [systematic error patterns identified]\n- Confidence Calibration: [prediction confidence vs. actual accuracy alignment]\n\nEarly Warning Indicators:\n- Prediction Deviation Alerts: [when predictions diverge significantly from reality]\n- Model Drift Detection: [when model performance degrades over time]\n- Assumption Violation Warnings: [when key assumptions prove incorrect]\n- Data Quality Alerts: [when input data quality degrades]\n\nAutomated Adjustments:\n- Parameter Recalibration: [automatic model parameter updates]\n- Weight Rebalancing: [factor importance adjustments based on performance]\n- Threshold Updates: [decision threshold modifications based on accuracy]\n- Alert Sensitivity: [notification threshold adjustments]\n```\n\n#### Adaptive Learning Integration\n- Machine learning model updates based on new data\n- Bayesian updating for probability and parameter estimation\n- Expert feedback integration and model refinement\n- Context-aware calibration for different scenario types\n\n### 6. Calibration Quality Assurance\n\n**Ensure systematic improvement and reliability:**\n\n#### Calibration Validation Framework\n```\nMeta-Calibration Assessment:\n\nCalibration Process Quality:\n- Validation methodology appropriateness and rigor\n- Feedback integration effectiveness and speed\n- Bias detection and mitigation success\n- Continuous improvement demonstration\n\nCalibration Outcome Quality:\n- Accuracy improvement measurement and tracking\n- Prediction reliability enhancement\n- Decision support effectiveness improvement\n- Stakeholder confidence and satisfaction growth\n\nCalibration Sustainability:\n- Process scalability and resource efficiency\n- Knowledge capture and institutional learning\n- Methodology transferability to other simulations\n- Long-term performance maintenance and enhancement\n```\n\n#### Quality Control Mechanisms\n- Independent calibration validation and audit\n- Cross-functional calibration team and review processes\n- External benchmark comparison and best practice integration\n- Documentation and knowledge management systems\n\n### 7. Simulation Improvement Roadmap\n\n**Generate systematic enhancement strategies:**\n\n#### Calibration-Based Improvement Plan\n```\nSimulation Enhancement Framework:\n\n## Simulation Calibration Analysis: [Simulation Name]\n\n### Current Performance Assessment\n- Baseline Accuracy: [current accuracy percentages]\n- Key Biases Identified: [systematic errors found]\n- Validation Coverage: [validation methods applied]\n- Stakeholder Confidence: [user trust and satisfaction levels]\n\n### Calibration Findings\n\n#### Accuracy Analysis:\n- Strong Performance Areas: [where simulation excels]\n- Accuracy Gaps: [where improvements are needed]\n- Bias Patterns: [systematic errors identified]\n- Validation Results: [validation testing outcomes]\n\n#### Improvement Opportunities:\n- Quick Wins: [immediate accuracy improvements available]\n- Strategic Enhancements: [longer-term improvement possibilities]\n- Data Quality Improvements: [input enhancement opportunities]\n- Model Sophistication: [algorithm and methodology upgrades]\n\n### Improvement Roadmap\n\n#### Phase 1: Immediate Fixes (30 days)\n- Critical bias corrections and parameter adjustments\n- Data quality improvements and source validation\n- Process enhancement and workflow optimization\n- Stakeholder feedback integration and communication\n\n#### Phase 2: Systematic Enhancement (90 days)\n- Model sophistication and algorithm upgrades\n- Validation framework expansion and automation\n- Feedback loop optimization and real-time calibration\n- Training and capability building for users\n\n#### Phase 3: Advanced Optimization (180+ days)\n- Machine learning integration and automated improvement\n- Cross-simulation learning and best practice sharing\n- Innovation and methodology advancement\n- Strategic capability building and competitive advantage\n\n### Success Metrics and Monitoring\n- Accuracy Improvement Targets: [specific goals and timelines]\n- Bias Reduction Objectives: [systematic error elimination goals]\n- Validation Coverage Goals: [comprehensive validation targets]\n- User Satisfaction Improvements: [stakeholder confidence goals]\n```\n\n### 8. Knowledge Capture and Transfer\n\n**Establish institutional learning from calibration:**\n\n#### Learning Documentation\n- Calibration methodology documentation and best practices\n- Bias detection and mitigation technique libraries\n- Validation approach templates and reusable frameworks\n- Success pattern identification and replication guides\n\n#### Cross-Simulation Learning\n- Calibration insight sharing across different simulations\n- Best practice identification and standardization\n- Common pitfall documentation and avoidance strategies\n- Expertise development and capability building programs\n\n## Usage Examples\n\n```bash\n# Business simulation calibration\n/simulation:simulation-calibrator Calibrate customer acquisition cost simulation using 12 months of actual campaign data\n\n# Technical simulation validation\n/simulation:simulation-calibrator Validate system performance simulation against production monitoring data and user experience metrics\n\n# Market response calibration\n/simulation:simulation-calibrator Calibrate market response model using A/B testing results and customer behavior analytics\n\n# Strategic scenario validation\n/simulation:simulation-calibrator Test business scenario accuracy using post-decision outcome analysis and market development tracking\n```\n\n## Quality Indicators\n\n- **Green**: Systematic validation, documented biases, automated monitoring, continuous improvement\n- **Yellow**: Regular validation, some bias detection, manual monitoring, periodic improvement\n- **Red**: Ad-hoc validation, undetected biases, no monitoring, no improvement tracking\n\n## Common Pitfalls to Avoid\n\n- Validation theater: Going through validation motions without learning\n- Bias blindness: Not recognizing systematic errors and prejudices\n- Static calibration: Not updating models based on new information\n- Perfection paralysis: Waiting for perfect accuracy before using insights\n- Context ignorance: Not adapting calibration to different scenarios\n- Learning isolation: Not sharing insights across teams and simulations\n\nTransform simulation accuracy from guesswork into systematic, reliable decision support through comprehensive calibration and continuous improvement."
              },
              {
                "name": "/timeline-compressor",
                "description": "Accelerate scenario testing with rapid iteration cycles, confidence intervals, and compressed decision timelines.",
                "path": "plugins/commands-simulation-modeling/commands/timeline-compressor.md",
                "frontmatter": {
                  "description": "Accelerate scenario testing with rapid iteration cycles, confidence intervals, and compressed decision timelines.",
                  "category": "simulation-modeling",
                  "argument-hint": "Specify timeline and compression ratio"
                },
                "content": "# Timeline Compressor\n\nAccelerate scenario testing with rapid iteration cycles, confidence intervals, and compressed decision timelines.\n\n## Instructions\n\nYou are tasked with compressing lengthy real-world timelines into rapid simulation cycles to achieve exponential learning and decision acceleration. Follow this systematic approach: **$ARGUMENTS**\n\n### 1. Prerequisites Assessment\n\n**Critical Timeline Context Validation:**\n\n- **Original Timeline**: What real-world timeline are you trying to compress?\n- **Compression Ratio**: How much acceleration do you need (10x, 100x, 1000x)?\n- **Key Milestones**: What critical events must be preserved in compression?\n- **Decision Points**: What decisions depend on timeline outcomes?\n- **Validation Method**: How will you verify compressed timeline accuracy?\n\n**If any context is unclear, guide systematically:**\n\n```\nMissing Timeline Context:\n\"I need to understand the timeline you want to compress:\n- Timeline Type: Business cycle, product development, market adoption, competitive response?\n- Original Duration: Months, quarters, years, or decades?\n- Key Phases: What are the major stages or milestones?\n- Dependencies: What events must happen before others can start?\n\nExamples:\n- 'Product development: 18-month timeline from concept to market launch'\n- 'Market penetration: 5-year customer adoption and market share growth'\n- 'Competitive response: 2-year competitive landscape evolution'\n- 'Business transformation: 3-year digital transformation initiative'\"\n\nMissing Compression Goals:\n\"What do you want to achieve through timeline compression?\n- Decision Acceleration: Make faster strategic choices with more information\n- Risk Exploration: Test multiple scenarios before real-world commitment\n- Learning Acceleration: Gain insights from many iterations quickly\n- Option Generation: Explore alternative pathways and strategies\n- Optimization: Find best approaches through rapid experimentation\"\n\nMissing Success Criteria:\n\"How will you measure compression success?\n- Prediction Accuracy: How well does compressed timeline predict reality?\n- Decision Quality: Do faster decisions lead to better outcomes?\n- Learning Speed: How much insight per unit time invested?\n- Option Value: How many more alternatives can you explore?\"\n```\n\n### 2. Timeline Architecture Analysis\n\n**Systematically map timeline structure and dependencies:**\n\n#### Temporal Structure Mapping\n- Sequential dependencies (what must happen in order)\n- Parallel workstreams (what can happen simultaneously)\n- Critical path identification (bottlenecks and pace-setting activities)\n- Milestone definitions (key decision and evaluation points)\n- Feedback loops (how later events affect earlier assumptions)\n\n#### Time Dimension Characterization\n```\nTimeline Component Analysis:\n\nLinear Time Components:\n- Calendar Dependencies: [events tied to specific dates/seasons]\n- Sequential Processes: [step-by-step workflows that can't be parallelized]\n- Learning Curves: [skill/knowledge development that takes time]\n- Approval Cycles: [regulatory or stakeholder decision processes]\n\nCompressible Components:\n- Analysis and Planning: [information processing and decision-making]\n- Testing and Validation: [hypothesis testing and experiment cycles]\n- Market Research: [customer feedback and preference analysis]\n- Strategy Development: [scenario planning and option generation]\n\nFixed Time Components:\n- Regulatory Approvals: [compliance and legal process requirements]\n- Manufacturing Cycles: [physical production and quality processes]\n- Customer Adoption: [market education and behavior change]\n- Infrastructure Development: [physical or technical platform building]\n```\n\n#### Dependency Network Modeling\n- Cause-and-effect relationships between timeline events\n- Information flow dependencies and communication requirements\n- Resource constraint dependencies and capacity limitations\n- External dependency mapping (partners, markets, regulations)\n\n### 3. Compression Strategy Framework\n\n**Design systematic acceleration approaches:**\n\n#### Compression Methodology Selection\n```\nCompression Technique Toolkit:\n\nSimulation-Based Compression:\n- Monte Carlo simulation for probability-based acceleration\n- Agent-based modeling for complex system behavior\n- Discrete event simulation for process optimization\n- System dynamics modeling for feedback loop acceleration\n\nInformation Compression:\n- Rapid prototyping and MVP development\n- Accelerated customer research and feedback cycles\n- Competitive intelligence and market analysis acceleration\n- Expert consultation and knowledge synthesis\n\nDecision Compression:\n- Parallel option development and evaluation\n- Staged decision-making with early exit criteria\n- Rapid experimentation and A/B testing\n- Real option theory for decision timing optimization\n```\n\n#### Acceleration Factor Calibration\n- Identify maximum safe compression ratios for each timeline component\n- Validate compression accuracy through historical back-testing\n- Establish confidence intervals for compressed timeline predictions\n- Create feedback mechanisms for compression quality improvement\n\n#### Fidelity vs. Speed Trade-offs\n- High-fidelity compression for critical decisions (slower but more accurate)\n- Medium-fidelity compression for strategic planning (balanced approach)\n- Low-fidelity compression for option generation (fast but approximate)\n- Adaptive fidelity based on decision importance and available time\n\n### 4. Rapid Iteration Engine\n\n**Create systematic acceleration mechanisms:**\n\n#### Iteration Cycle Design\n```\nCompressed Timeline Iteration Framework:\n\nMicro-Cycles (Hours to Days):\n- Hypothesis generation and initial testing\n- Rapid prototyping and concept validation\n- Quick customer feedback and market pulse\n- Immediate competitive response assessment\n\nMini-Cycles (Days to Weeks):\n- Feature development and testing cycles\n- Marketing campaign testing and optimization\n- Business model validation and refinement\n- Strategic option evaluation and selection\n\nMacro-Cycles (Weeks to Months):\n- Market segment testing and expansion\n- Product-market fit validation and optimization\n- Business model scaling and operational refinement\n- Competitive positioning and market share analysis\n```\n\n#### Parallel Processing Framework\n- Simultaneous exploration of multiple timeline scenarios\n- Parallel development of alternative strategies and approaches\n- Concurrent testing of different market segments and channels\n- Parallel competitive response and counter-strategy development\n\n#### Learning Acceleration Mechanisms\n- Automated data collection and analysis for faster insights\n- Real-time feedback integration and course correction\n- Expert network activation for rapid knowledge access\n- Pattern recognition for accelerated trend identification\n\n### 5. Confidence Interval Management\n\n**Maintain decision quality during acceleration:**\n\n#### Uncertainty Quantification\n```\nConfidence Assessment Framework:\n\nHigh Confidence Predictions (80-95% accuracy):\n- Components: [timeline elements with strong historical data]\n- Time Horizons: [prediction periods with high reliability]\n- Conditions: [market/business conditions for accuracy]\n- Validation: [methods used to verify prediction quality]\n\nMedium Confidence Predictions (60-80% accuracy):\n- Components: [timeline elements with moderate data support]\n- Assumptions: [key assumptions that could affect accuracy]\n- Sensitivities: [factors that most impact prediction quality]\n- Monitoring: [early warning indicators for assumption validation]\n\nLow Confidence Predictions (40-60% accuracy):\n- Components: [timeline elements with limited data or high uncertainty]\n- Research Needs: [additional information required for improvement]\n- Alternative Scenarios: [backup plans if predictions prove incorrect]\n- Decision Thresholds: [when to seek more information vs. act on uncertainty]\n```\n\n#### Risk-Adjusted Decision Making\n- Confidence-weighted option evaluation and selection\n- Scenario probability distribution for uncertainty management\n- Real option valuation for decision timing under uncertainty\n- Adaptive strategy development for changing conditions\n\n#### Validation and Calibration\n- Continuous comparison of compressed predictions to real-world outcomes\n- Model accuracy tracking and improvement over time\n- Bias detection and correction for systematic errors\n- Expert validation and external perspective integration\n\n### 6. Scenario Multiplication Framework\n\n**Leverage compression for exponential scenario exploration:**\n\n#### Scenario Generation Strategy\n```\nCompressed Scenario Portfolio:\n\nBase Scenarios (20% of simulation time):\n- Most likely timeline development and outcomes\n- Conservative assumptions and proven approaches\n- Risk-adjusted projections and realistic expectations\n\nOptimization Scenarios (30% of simulation time):\n- Best-case timeline acceleration and outcomes\n- Aggressive but achievable improvement targets\n- Innovation and breakthrough opportunity exploration\n\nStress Test Scenarios (30% of simulation time):\n- Adverse condition timeline delays and challenges\n- Competitive pressure and market disruption impacts\n- Resource constraint and execution challenge scenarios\n\nInnovation Scenarios (20% of simulation time):\n- Breakthrough technology or market development impacts\n- Disruptive business model and competitive landscape changes\n- Unexpected opportunity and black swan event responses\n```\n\n#### Scenario Interaction Modeling\n- Cross-scenario learning and insight synthesis\n- Scenario combination and hybrid approach development\n- Scenario transition probability and trigger identification\n- Portfolio effect analysis across multiple timeline scenarios\n\n### 7. Decision Acceleration Integration\n\n**Transform compressed insights into faster real-world decisions:**\n\n#### Decision Point Optimization\n- Early decision trigger identification and validation\n- Information value analysis for decision timing optimization\n- Real option theory application for maximum flexibility\n- Decision reversal cost analysis and exit strategy planning\n\n#### Accelerated Validation Framework\n```\nRapid Validation Methodology:\n\nTier 1 Validation (Hours):\n- Expert opinion and domain knowledge validation\n- Historical pattern matching and precedent analysis\n- Logic and consistency checking for basic feasibility\n- Quick market pulse and stakeholder reaction assessment\n\nTier 2 Validation (Days):\n- Customer interview and feedback collection\n- Competitive analysis and market positioning validation\n- Financial model validation and sensitivity testing\n- Technical feasibility and resource requirement validation\n\nTier 3 Validation (Weeks):\n- Pilot testing and proof-of-concept development\n- Market research and quantitative validation\n- Stakeholder alignment and buy-in development\n- Implementation planning and risk assessment\n```\n\n#### Strategic Momentum Creation\n- Decision making rhythm and cadence optimization\n- Stakeholder alignment and communication acceleration\n- Resource allocation and execution timeline compression\n- Success metrics and feedback loop acceleration\n\n### 8. Output Generation and Synthesis\n\n**Present compressed timeline insights effectively:**\n\n```\n## Timeline Compression Analysis: [Project Name]\n\n### Compression Summary\n- Original Timeline: [duration and key phases]\n- Compression Ratio: [acceleration factor achieved]\n- Scenarios Tested: [number and types of scenarios explored]\n- Decision Acceleration: [time savings and decision quality improvement]\n\n### Key Findings\n\n#### Timeline Acceleration Opportunities:\n- High-Impact Accelerations: [specific timeline improvements]\n- Quick Wins: [immediate acceleration opportunities]\n- Strategic Accelerations: [long-term timeline optimization]\n- Resource-Dependent Accelerations: [improvements requiring investment]\n\n#### Critical Path Analysis:\n- Bottleneck Identification: [pace-limiting factors and constraints]\n- Parallel Processing Opportunities: [concurrent activity possibilities]\n- Dependency Optimization: [sequence and timing improvements]\n- Risk Mitigation Accelerations: [faster risk reduction approaches]\n\n### Scenario Outcomes Matrix\n\n| Scenario Type | Timeline Reduction | Success Probability | Key Requirements | Risk Level |\n|---------------|-------------------|-------------------|------------------|------------|\n| Conservative | 30% faster | 85% | [requirements] | Low |\n| Optimistic | 60% faster | 65% | [requirements] | Medium |\n| Aggressive | 80% faster | 40% | [requirements] | High |\n\n### Recommended Acceleration Strategy\n- Primary Approach: [recommended timeline compression strategy]\n- Acceleration Targets: [specific timeline improvements to pursue]\n- Resource Requirements: [investment needed for acceleration]\n- Risk Mitigation: [approaches to manage acceleration risks]\n- Success Metrics: [KPIs for measuring acceleration success]\n\n### Implementation Roadmap\n- Immediate Actions: [steps to begin timeline compression]\n- 30-Day Milestones: [early acceleration achievements]\n- 90-Day Objectives: [medium-term compression goals]\n- Ongoing Optimization: [continuous improvement approaches]\n\n### Confidence Assessment\n- High Confidence Elements: [timeline components with reliable acceleration]\n- Medium Confidence Elements: [components requiring validation]\n- Low Confidence Elements: [components needing more research]\n- Validation Plan: [approach to improve confidence over time]\n```\n\n### 9. Continuous Improvement and Learning\n\n**Establish ongoing compression optimization:**\n\n#### Performance Tracking\n- Compression accuracy measurement and improvement\n- Decision quality assessment and enhancement\n- Learning velocity tracking and optimization\n- Resource efficiency measurement and improvement\n\n#### Model Refinement\n- Compression algorithm improvement based on results\n- Scenario generation enhancement for better coverage\n- Validation methodology optimization for faster feedback\n- Integration process improvement for smoother execution\n\n## Usage Examples\n\n```bash\n# Product development acceleration\n/simulation:timeline-compressor Compress 18-month product development cycle to test 10 different feature prioritization strategies\n\n# Market entry timing optimization  \n/simulation:timeline-compressor Accelerate 3-year market expansion timeline to identify optimal entry sequence and timing\n\n# Business transformation acceleration\n/simulation:timeline-compressor Compress digital transformation timeline to test organizational change approaches and technology adoption\n\n# Competitive response preparation\n/simulation:timeline-compressor Accelerate competitive landscape evolution to prepare for various competitor response scenarios\n```\n\n## Quality Indicators\n\n- **Green**: 10x+ compression ratio, validated historical accuracy, multiple scenario testing\n- **Yellow**: 5-10x compression, reasonable accuracy validation, some scenario coverage\n- **Red**: <5x compression, limited validation, single scenario focus\n\n## Common Pitfalls to Avoid\n\n- Over-compression: Losing critical real-world constraints and dependencies\n- Validation blindness: Not testing compressed predictions against reality\n- Context loss: Forgetting that compression is a tool, not an end goal\n- Decision rush: Using compression to make premature decisions\n- Complexity underestimation: Assuming all timeline elements can be compressed equally\n- Single scenario fixation: Not exploring multiple compressed scenarios\n\nTransform your competitor's 3 iterations into your 300 iterations through systematic timeline compression and exponential learning acceleration."
              }
            ],
            "skills": []
          },
          {
            "name": "commands-team-collaboration",
            "description": "Commands for team workflows, PR reviews, and collaboration",
            "source": "./plugins/commands-team-collaboration",
            "category": "commands",
            "version": "1.0.0",
            "author": {
              "name": "BuildWithClaude Community",
              "url": "https://github.com/davepoon/buildwithclaude"
            },
            "install_commands": [
              "/plugin marketplace add davepoon/buildwithclaude",
              "/plugin install commands-team-collaboration@buildwithclaude"
            ],
            "signals": {
              "stars": 2229,
              "forks": 239,
              "pushed_at": "2026-01-12T10:19:01Z",
              "created_at": "2025-07-25T02:26:45Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/architecture-review",
                "description": "Review and improve system architecture",
                "path": "plugins/commands-team-collaboration/commands/architecture-review.md",
                "frontmatter": {
                  "description": "Review and improve system architecture",
                  "category": "team-collaboration"
                },
                "content": "# Architecture Review Command\n\nReview and improve system architecture\n\n## Instructions\n\nPerform a comprehensive architectural analysis following these steps:\n\n1. **High-Level Architecture Analysis**\n   - Map out the overall system architecture and components\n   - Identify architectural patterns in use (MVC, MVP, Clean Architecture, etc.)\n   - Review module boundaries and separation of concerns\n   - Analyze the application's layered structure\n\n2. **Design Patterns Assessment**\n   - Identify design patterns used throughout the codebase\n   - Check for proper implementation of common patterns\n   - Look for anti-patterns and code smells\n   - Assess pattern consistency across the application\n\n3. **Dependency Management**\n   - Review dependency injection and inversion of control\n   - Analyze coupling between modules and components\n   - Check for circular dependencies\n   - Assess dependency direction and adherence to dependency rule\n\n4. **Data Flow Architecture**\n   - Trace data flow through the application\n   - Review state management patterns and implementation\n   - Analyze data persistence and storage strategies\n   - Check for proper data validation and transformation\n\n5. **Component Architecture**\n   - Review component design and responsibilities\n   - Check for single responsibility principle adherence\n   - Analyze component composition and reusability\n   - Assess interface design and abstraction levels\n\n6. **Error Handling Architecture**\n   - Review error handling strategy and consistency\n   - Check for proper error propagation and recovery\n   - Analyze logging and monitoring integration\n   - Assess resilience and fault tolerance patterns\n\n7. **Scalability Assessment**\n   - Analyze horizontal and vertical scaling capabilities\n   - Review caching strategies and implementation\n   - Check for stateless design where appropriate\n   - Assess performance bottlenecks and scaling limitations\n\n8. **Security Architecture**\n   - Review security boundaries and trust zones\n   - Check authentication and authorization architecture\n   - Analyze data protection and privacy measures\n   - Assess security pattern implementation\n\n9. **Testing Architecture**\n   - Review test structure and organization\n   - Check for testability in design\n   - Analyze mocking and dependency isolation strategies\n   - Assess test coverage across architectural layers\n\n10. **Configuration Management**\n    - Review configuration handling and environment management\n    - Check for proper separation of config from code\n    - Analyze feature flags and runtime configuration\n    - Assess deployment configuration strategies\n\n11. **Documentation & Communication**\n    - Review architectural documentation and diagrams\n    - Check for clear API contracts and interfaces\n    - Assess code self-documentation and clarity\n    - Analyze team communication patterns in code\n\n12. **Future-Proofing & Extensibility**\n    - Assess the architecture's ability to accommodate change\n    - Review extension points and plugin architectures\n    - Check for proper versioning and backward compatibility\n    - Analyze migration and upgrade strategies\n\n13. **Technology Choices**\n    - Review technology stack alignment with requirements\n    - Assess framework and library choices\n    - Check for consistent technology usage\n    - Analyze technical debt and modernization opportunities\n\n14. **Performance Architecture**\n    - Review caching layers and strategies\n    - Analyze asynchronous processing patterns\n    - Check for proper resource management\n    - Assess monitoring and observability architecture\n\n15. **Recommendations**\n    - Provide specific architectural improvements\n    - Suggest refactoring strategies for problem areas\n    - Recommend patterns and practices for better design\n    - Create a roadmap for architectural evolution\n\nFocus on providing actionable insights with specific examples and clear rationale for recommendations."
              },
              {
                "name": "/decision-quality-analyzer",
                "description": "Analyze decision quality with scenario testing, bias detection, and team decision-making process optimization.",
                "path": "plugins/commands-team-collaboration/commands/decision-quality-analyzer.md",
                "frontmatter": {
                  "description": "Analyze decision quality with scenario testing, bias detection, and team decision-making process optimization.",
                  "category": "team-collaboration",
                  "argument-hint": "Specify analysis criteria",
                  "allowed-tools": "Bash(gh *)"
                },
                "content": "# Decision Quality Analyzer\n\nAnalyze decision quality with scenario testing, bias detection, and team decision-making process optimization.\n\n## Instructions\n\nYou are tasked with systematically analyzing and improving team decision quality through scenario analysis, bias detection, and process optimization. Follow this approach: **$ARGUMENTS**\n\n### 1. Decision Context Assessment\n\n**Critical Decision Quality Context:**\n\n- **Decision Type**: What category of decision are you analyzing?\n- **Decision Process**: How does the team currently make this type of decision?\n- **Stakeholders**: Who participates in and is affected by these decisions?\n- **Success Metrics**: How do you measure decision quality and outcomes?\n- **Historical Data**: What past decisions provide learning opportunities?\n\n**If context is unclear, guide systematically:**\n\n```\nMissing Decision Type:\n\"What type of team decision needs quality analysis?\n- Strategic Decisions: Product direction, market positioning, technology choices\n- Operational Decisions: Process improvements, resource allocation, priority setting\n- Personnel Decisions: Hiring, team structure, role assignments, performance management\n- Technical Decisions: Architecture choices, tool selection, implementation approaches\n\nPlease specify the decision scope and typical complexity level.\"\n\nMissing Decision Process:\n\"How does your team currently make these decisions?\n- Individual Authority: Single decision maker with consultation\n- Consensus Building: Group discussion until agreement is reached\n- Majority Vote: Democratic process with formal or informal voting\n- Delegated Authority: Decision rights assigned to specific roles or committees\n- Data-Driven: Systematic analysis and evidence-based approaches\"\n```\n\n### 2. Decision Quality Framework\n\n**Systematic decision evaluation methodology:**\n\n#### Quality Dimension Assessment\n```\nMulti-Dimensional Decision Quality:\n\nProcess Quality (25% weight):\n- Information Gathering: Completeness and accuracy of data collection\n- Stakeholder Involvement: Appropriate participation and perspective inclusion\n- Alternative Generation: Creativity and comprehensiveness of option development\n- Analysis Rigor: Systematic evaluation and trade-off assessment\n\nOutcome Quality (25% weight):\n- Goal Achievement: Success in reaching intended objectives\n- Unintended Consequences: Management of secondary effects and side impacts\n- Stakeholder Satisfaction: Acceptance and support from affected parties\n- Long-term Sustainability: Durability and adaptability of decision outcomes\n\nTiming Quality (25% weight):\n- Decision Speed: Appropriate pace for urgency and complexity\n- Information Timing: Optimal balance of speed vs additional information\n- Implementation Timing: Coordination with market conditions and organizational readiness\n- Review Timing: Appropriate schedule for decision assessment and adjustment\n\nLearning Quality (25% weight):\n- Knowledge Capture: Documentation and institutional learning\n- Bias Recognition: Awareness and mitigation of cognitive biases\n- Process Improvement: Methodology enhancement based on outcomes\n- Capability Building: Team decision-making skill development\n```\n\n#### Decision Success Metrics\n- Quantitative outcomes (financial, operational, performance metrics)\n- Qualitative outcomes (satisfaction, engagement, strategic alignment)\n- Process efficiency (time to decision, resource utilization)\n- Learning outcomes (knowledge gained, capability developed)\n\n### 3. Bias Detection and Mitigation\n\n**Systematic cognitive bias identification:**\n\n#### Common Decision Biases\n```\nTeam Decision Bias Framework:\n\nIndividual Cognitive Biases:\n- Confirmation Bias: Seeking information that supports preconceptions\n- Anchoring Bias: Over-relying on first information received\n- Availability Bias: Overweighting easily recalled examples\n- Overconfidence Bias: Excessive certainty in judgment accuracy\n- Sunk Cost Fallacy: Continuing failed approaches due to past investment\n\nGroup Decision Biases:\n- Groupthink: Pressure for harmony reducing critical evaluation\n- Risky Shift: Groups making riskier decisions than individuals\n- Authority Bias: Deferring to hierarchy rather than evidence\n- Social Proof: Following others' decisions without independent analysis\n- Planning Fallacy: Systematic underestimation of time and resources\n\nOrganizational Biases:\n- Status Quo Bias: Preferring current state over change\n- Not Invented Here: Rejecting external ideas and solutions\n- Survivorship Bias: Focusing only on successful cases\n- Attribution Bias: Misattributing success and failure causes\n- Political Bias: Decisions influenced by organizational politics\n```\n\n#### Bias Mitigation Strategies\n```\nSystematic Bias Reduction:\n\nProcess-Based Mitigation:\n- Devil's Advocate: Designated critical evaluation role\n- Red Team Analysis: Systematic challenge of assumptions and conclusions\n- Diverse Perspectives: Multi-functional and multi-level input\n- Anonymous Input: Reducing social pressure and hierarchy effects\n\nTool-Based Mitigation:\n- Decision Trees: Systematic option evaluation and comparison\n- Pre-mortem Analysis: Imagining failure scenarios and prevention\n- Reference Class Forecasting: Using similar historical examples\n- Outside View: External perspective and benchmarking\n\nCultural Mitigation:\n- Psychological Safety: Encouraging dissent and critical thinking\n- Learning Orientation: Celebrating learning from failures\n- Evidence-Based Culture: Valuing data over intuition and politics\n- Continuous Improvement: Regular process assessment and enhancement\n```\n\n### 4. Scenario-Based Decision Testing\n\n**Test decision quality through hypothetical scenarios:**\n\n#### Decision Scenario Framework\n```\nComprehensive Decision Testing:\n\nHistorical Scenario Testing:\n- Apply current decision process to past decisions\n- Compare predicted vs actual outcomes\n- Identify process improvements that would have helped\n- Calibrate decision confidence and accuracy\n\nHypothetical Scenario Testing:\n- Create realistic decision scenarios for practice\n- Test team process under different conditions\n- Identify process strengths and weaknesses\n- Build team decision-making capability\n\nStress Test Scenarios:\n- Time pressure and urgency constraints\n- Incomplete information and high uncertainty\n- Conflicting stakeholder interests and priorities\n- High-stakes decisions with significant consequences\n\nLearning Scenarios:\n- Successful decision analysis and pattern recognition\n- Failed decision post-mortem and lesson extraction\n- Near-miss analysis and improvement identification\n- Best practice sharing and capability transfer\n```\n\n#### Simulation-Based Improvement\n- Role-playing exercises for complex decision scenarios\n- Process experimentation with low-stakes decisions\n- A/B testing of different decision methodologies\n- Scenario planning for future decision situations\n\n### 5. Team Decision Process Optimization\n\n**Systematic improvement of decision-making workflows:**\n\n#### Process Enhancement Framework\n```\nDecision Process Optimization:\n\nInformation Management:\n- Data Collection: Systematic gathering of relevant information\n- Information Quality: Accuracy, completeness, and timeliness assessment\n- Bias Detection: Recognition of information source biases\n- Knowledge Synthesis: Integration of diverse information sources\n\nStakeholder Engagement:\n- Identification: Complete mapping of affected and influential parties\n- Consultation: Systematic input gathering and perspective integration\n- Communication: Clear explanation of process and decision rationale\n- Buy-in: Building support and commitment for implementation\n\nAnalysis and Evaluation:\n- Option Generation: Creative and comprehensive alternative development\n- Criteria Definition: Clear success metrics and evaluation standards\n- Trade-off Analysis: Systematic comparison of costs and benefits\n- Risk Assessment: Identification and mitigation of potential problems\n\nDecision Implementation:\n- Planning: Detailed implementation strategy and timeline\n- Resource Allocation: Appropriate staffing and budget assignment\n- Monitoring: Progress tracking and outcome measurement\n- Adaptation: Course correction based on results and learning\n```\n\n#### Team Capability Building\n- Decision-making skill training and development\n- Process facilitation and meeting effectiveness\n- Critical thinking and analytical capability enhancement\n- Communication and stakeholder management improvement\n\n### 6. Output Generation and Recommendations\n\n**Present decision quality insights in actionable format:**\n\n```\n## Decision Quality Analysis: [Decision Type/Process]\n\n### Current State Assessment\n- Decision Process Maturity: [evaluation of current methodology]\n- Quality Dimension Scores: [process, outcome, timing, learning ratings]\n- Bias Vulnerability: [key cognitive biases affecting decisions]\n- Stakeholder Satisfaction: [feedback on decision process and outcomes]\n\n### Key Findings\n\n#### Decision Process Strengths:\n- Effective Practices: [what works well in current process]\n- Quality Outcomes: [successful decisions and positive patterns]\n- Team Capabilities: [strong skills and effective behaviors]\n- Stakeholder Engagement: [successful involvement and communication]\n\n#### Improvement Opportunities:\n- Process Gaps: [missing steps or inadequate methodology]\n- Bias Vulnerabilities: [cognitive biases affecting decision quality]\n- Information Deficits: [data gaps and analysis weaknesses]\n- Implementation Challenges: [execution and follow-through issues]\n\n### Optimization Recommendations\n\n#### Immediate Improvements (0-30 days):\n- Process Quick Fixes: [simple methodology enhancements]\n- Bias Mitigation: [specific techniques for bias reduction]\n- Tool Implementation: [decision aids and analytical frameworks]\n- Communication Enhancement: [stakeholder engagement improvements]\n\n#### Medium-term Development (1-6 months):\n- Capability Building: [training and skill development programs]\n- Process Standardization: [consistent methodology across decisions]\n- Quality Measurement: [metrics and feedback systems]\n- Cultural Development: [decision-making mindset and values]\n\n#### Long-term Transformation (6+ months):\n- Organizational Learning: [institutional knowledge and capability]\n- Advanced Analytics: [data-driven decision support systems]\n- Innovation Integration: [new methodologies and tools]\n- Competitive Advantage: [decision-making as strategic capability]\n\n### Success Metrics and Monitoring\n- Decision Quality KPIs: [measurable indicators of improvement]\n- Process Efficiency Metrics: [speed and resource utilization]\n- Outcome Tracking: [business results and stakeholder satisfaction]\n- Learning Indicators: [capability development and knowledge capture]\n\n### Implementation Roadmap\n- Phase 1: [immediate process improvements and bias mitigation]\n- Phase 2: [capability building and measurement system]\n- Phase 3: [advanced methodology and cultural transformation]\n- Success Criteria: [specific goals and achievement measures]\n```\n\n### 7. Continuous Learning Integration\n\n**Establish ongoing decision quality improvement:**\n\n#### Decision Outcome Tracking\n- Systematic monitoring of decision results and impacts\n- Correlation analysis between process quality and outcomes\n- Pattern recognition for successful vs unsuccessful decisions\n- Feedback integration for process refinement and enhancement\n\n#### Organizational Learning\n- Best practice identification and knowledge sharing\n- Decision case study development and team learning\n- Cross-functional learning and capability transfer\n- Industry benchmark comparison and competitive analysis\n\n## Usage Examples\n\n```bash\n# Product strategy decision analysis\n/team:decision-quality-analyzer Analyze product roadmap prioritization decisions for bias and process improvement opportunities\n\n# Technical architecture decision assessment\n/team:decision-quality-analyzer Evaluate technology stack decisions using scenario testing and stakeholder satisfaction analysis\n\n# Hiring process decision optimization\n/team:decision-quality-analyzer Optimize candidate evaluation and selection process through bias detection and outcome tracking\n\n# Investment decision quality improvement\n/team:decision-quality-analyzer Improve capital allocation decisions through process standardization and learning integration\n```\n\n## Quality Indicators\n\n- **Green**: Comprehensive bias analysis, validated process improvements, outcome tracking\n- **Yellow**: Basic bias recognition, some process enhancement, limited outcome measurement\n- **Red**: Minimal bias awareness, ad-hoc process, no systematic improvement\n\n## Common Pitfalls to Avoid\n\n- Analysis paralysis: Over-analyzing decisions instead of improving decision-making\n- Bias blindness: Not recognizing team and organizational cognitive biases\n- Process rigidity: Creating inflexible procedures that slow appropriate decisions\n- Outcome fixation: Judging process quality only by outcomes rather than methodology\n- Individual focus: Ignoring group dynamics and organizational factors\n- One-size-fits-all: Using same process for all decision types and contexts\n\nTransform team decision-making from intuition-based guessing into systematic, evidence-driven capability that creates sustainable competitive advantage."
              },
              {
                "name": "/dependency-mapper",
                "description": "Map and analyze project dependencies",
                "path": "plugins/commands-team-collaboration/commands/dependency-mapper.md",
                "frontmatter": {
                  "description": "Map and analyze project dependencies",
                  "category": "team-collaboration"
                },
                "content": "# dependency-mapper\n\nMap and analyze project dependencies\n\n## Purpose\nThis command analyzes code dependencies, git history, and Linear tasks to create visual dependency maps. It helps identify blockers, circular dependencies, and optimal task ordering for efficient project execution.\n\n## Usage\n```bash\n# Map dependencies for a specific Linear task\nclaude \"Show dependency map for task LIN-123\"\n\n# Analyze code dependencies in a module\nclaude \"Map dependencies for src/auth module\"\n\n# Find circular dependencies in the project\nclaude \"Check for circular dependencies in the codebase\"\n\n# Generate task execution order\nclaude \"What's the optimal order to complete tasks in sprint SPR-45?\"\n```\n\n## Instructions\n\n### 1. Analyze Code Dependencies\nUse various techniques to identify dependencies:\n\n```bash\n# Find import statements (JavaScript/TypeScript)\nrg \"^import.*from ['\\\"](\\.\\.?/[^'\\\"]+)\" --type ts --type js -o | sort | uniq\n\n# Find require statements (Node.js)\nrg \"require\\(['\\\"](\\.\\.?/[^'\\\"]+)['\\\"]\" --type js -o\n\n# Analyze Python imports\nrg \"^from \\S+ import|^import \\S+\" --type py\n\n# Find module references in comments\nrg \"TODO.*depends on|FIXME.*requires|NOTE.*needs\" -i\n```\n\n### 2. Extract Task Dependencies from Linear\nQuery Linear for task relationships:\n\n```javascript\n// Get task with its dependencies\nconst task = await linear.getTask(taskId, {\n  include: ['blockedBy', 'blocks', 'parent', 'children']\n});\n\n// Find mentions in task descriptions\nconst mentions = task.description.match(/(?:LIN-|#)\\d+/g);\n\n// Get related tasks from same epic/project\nconst relatedTasks = await linear.searchTasks({\n  projectId: task.projectId,\n  includeArchived: false\n});\n```\n\n### 3. Build Dependency Graph\nCreate a graph structure:\n\n```javascript\nclass DependencyGraph {\n  constructor() {\n    this.nodes = new Map(); // taskId -> task details\n    this.edges = new Map(); // taskId -> Set of dependent taskIds\n  }\n  \n  addDependency(from, to, type = 'blocks') {\n    if (!this.edges.has(from)) {\n      this.edges.set(from, new Set());\n    }\n    this.edges.get(from).add({ to, type });\n  }\n  \n  findCycles() {\n    const visited = new Set();\n    const recursionStack = new Set();\n    const cycles = [];\n    \n    const hasCycle = (node, path = []) => {\n      visited.add(node);\n      recursionStack.add(node);\n      path.push(node);\n      \n      const neighbors = this.edges.get(node) || new Set();\n      for (const { to } of neighbors) {\n        if (!visited.has(to)) {\n          if (hasCycle(to, [...path])) return true;\n        } else if (recursionStack.has(to)) {\n          // Found cycle\n          const cycleStart = path.indexOf(to);\n          cycles.push(path.slice(cycleStart));\n        }\n      }\n      \n      recursionStack.delete(node);\n      return false;\n    };\n    \n    for (const node of this.nodes.keys()) {\n      if (!visited.has(node)) {\n        hasCycle(node);\n      }\n    }\n    \n    return cycles;\n  }\n  \n  topologicalSort() {\n    const inDegree = new Map();\n    const queue = [];\n    const result = [];\n    \n    // Calculate in-degrees\n    for (const [node] of this.nodes) {\n      inDegree.set(node, 0);\n    }\n    \n    for (const [_, edges] of this.edges) {\n      for (const { to } of edges) {\n        inDegree.set(to, (inDegree.get(to) || 0) + 1);\n      }\n    }\n    \n    // Find nodes with no dependencies\n    for (const [node, degree] of inDegree) {\n      if (degree === 0) queue.push(node);\n    }\n    \n    // Process queue\n    while (queue.length > 0) {\n      const node = queue.shift();\n      result.push(node);\n      \n      const edges = this.edges.get(node) || new Set();\n      for (const { to } of edges) {\n        inDegree.set(to, inDegree.get(to) - 1);\n        if (inDegree.get(to) === 0) {\n          queue.push(to);\n        }\n      }\n    }\n    \n    return result;\n  }\n}\n```\n\n### 4. Generate Visual Representations\n\n#### ASCII Tree View\n```\nLIN-123: Authentication System\n LIN-124: User Model [DONE]\n LIN-125: JWT Implementation [IN PROGRESS]\n   LIN-126: Token Refresh Logic [BLOCKED]\n LIN-127: Login Endpoint [TODO]\n    LIN-128: Rate Limiting [TODO]\n    LIN-129: 2FA Support [TODO]\n```\n\n#### Mermaid Diagram\n```mermaid\ngraph TD\n    LIN-123[Authentication System] --> LIN-124[User Model]\n    LIN-123 --> LIN-125[JWT Implementation]\n    LIN-123 --> LIN-127[Login Endpoint]\n    LIN-125 --> LIN-126[Token Refresh Logic]\n    LIN-127 --> LIN-128[Rate Limiting]\n    LIN-127 --> LIN-129[2FA Support]\n    \n    style LIN-124 fill:#90EE90\n    style LIN-125 fill:#FFD700\n    style LIN-126 fill:#FF6B6B\n```\n\n#### Dependency Matrix\n```\n         | LIN-123 | LIN-124 | LIN-125 | LIN-126 | LIN-127 |\n---------|---------|---------|---------|---------|---------|\nLIN-123  |    -    |        |        |         |        |\nLIN-124  |         |    -    |         |         |         |\nLIN-125  |         |        |    -    |        |         |\nLIN-126  |         |         |        |    -    |         |\nLIN-127  |        |        |         |         |    -    |\n\nLegend:  depends on,  is dependency of\n```\n\n### 5. Analyze File Dependencies\nMap code structure to tasks:\n\n```javascript\n// Analyze file imports\nasync function analyzeFileDependencies(filePath) {\n  const content = await readFile(filePath);\n  const imports = extractImports(content);\n  \n  const dependencies = {\n    internal: [], // Project files\n    external: [], // npm packages\n    tasks: []     // Related Linear tasks\n  };\n  \n  for (const imp of imports) {\n    if (imp.startsWith('.')) {\n      dependencies.internal.push(resolveImportPath(filePath, imp));\n    } else {\n      dependencies.external.push(imp);\n    }\n    \n    // Check if file is mentioned in any task\n    const tasks = await linear.searchTasks(path.basename(filePath));\n    dependencies.tasks.push(...tasks);\n  }\n  \n  return dependencies;\n}\n```\n\n### 6. Generate Execution Order\nCalculate optimal task sequence:\n\n```javascript\nfunction calculateExecutionOrder(graph) {\n  const order = graph.topologicalSort();\n  const taskDetails = [];\n  \n  for (const taskId of order) {\n    const task = graph.nodes.get(taskId);\n    const dependencies = Array.from(graph.edges.get(taskId) || [])\n      .map(({ to }) => to);\n    \n    taskDetails.push({\n      id: taskId,\n      title: task.title,\n      estimate: task.estimate || 0,\n      dependencies,\n      assignee: task.assignee,\n      criticalPath: isOnCriticalPath(taskId, graph)\n    });\n  }\n  \n  return taskDetails;\n}\n```\n\n### 7. Error Handling\n```javascript\n// Check for Linear access\nif (!linear.available) {\n  console.warn(\"Linear MCP not available, using code analysis only\");\n  // Fall back to code-only analysis\n}\n\n// Handle circular dependencies\nconst cycles = graph.findCycles();\nif (cycles.length > 0) {\n  console.error(\"Circular dependencies detected:\");\n  cycles.forEach(cycle => {\n    console.error(`  ${cycle.join('  ')}  ${cycle[0]}`);\n  });\n}\n\n// Validate task existence\nfor (const taskId of mentionedTasks) {\n  try {\n    await linear.getTask(taskId);\n  } catch (error) {\n    console.warn(`Task ${taskId} not found or inaccessible`);\n  }\n}\n```\n\n## Example Output\n\n```\nAnalyzing dependencies for Epic: Authentication System (LIN-123)\n\n Dependency Graph:\n\n\nLIN-123: Authentication System [EPIC]\n LIN-124: Create User Model  [DONE]\n   Files: src/models/User.ts, src/schemas/user.sql\n LIN-125: Implement JWT Service  [IN PROGRESS]\n   Files: src/services/auth/jwt.ts\n   Depends on: LIN-124\n   LIN-126: Add Token Refresh  [BLOCKED by LIN-125]\n LIN-127: Create Login Endpoint  [TODO]\n    Files: src/routes/auth/login.ts\n    Depends on: LIN-124, LIN-125\n    LIN-128: Add Rate Limiting  [TODO]\n    LIN-129: Implement 2FA  [TODO]\n\n Circular Dependencies: None found\n\n Critical Path:\n1. LIN-124 (User Model) - 2 points \n2. LIN-125 (JWT Service) - 3 points \n3. LIN-126 (Token Refresh) - 1 point \n4. LIN-127 (Login Endpoint) - 2 points \nTotal: 8 points on critical path\n\n Task Distribution:\n- Alice: LIN-125 (in progress), LIN-126 (blocked)\n- Bob: LIN-127 (ready to start)\n- Unassigned: LIN-128, LIN-129\n\n File Dependencies:\nsrc/routes/auth/login.ts\n   imports from:\n      src/models/User.ts (LIN-124) \n      src/services/auth/jwt.ts (LIN-125) \n      src/middleware/rateLimiter.ts (LIN-128) \n\n Recommended Action:\nPriority should be completing LIN-125 to unblock 3 dependent tasks.\nBob can start on LIN-124 prerequisite work while waiting.\n```\n\n## Advanced Features\n\n### Impact Analysis\nShow what tasks are affected by changes:\n```bash\n# What tasks are impacted if we change User.ts?\nclaude \"Show impact analysis for changes to src/models/User.ts\"\n```\n\n### Sprint Planning\nOptimize task order for sprint capacity:\n```bash\n# Generate sprint plan considering dependencies\nclaude \"Plan sprint with 20 points capacity considering dependencies\"\n```\n\n### Risk Assessment\nIdentify high-risk dependency chains:\n```bash\n# Find longest dependency chains\nclaude \"Show tasks with longest dependency chains in current sprint\"\n```\n\n## Tips\n- Update dependencies as code evolves\n- Use consistent naming between code modules and tasks\n- Mark external dependencies (APIs, services) explicitly\n- Review dependency graphs in sprint planning\n- Keep critical path tasks assigned and monitored\n- Use dependency data for accurate sprint velocity"
              },
              {
                "name": "/estimate-assistant",
                "description": "Generate accurate project time estimates",
                "path": "plugins/commands-team-collaboration/commands/estimate-assistant.md",
                "frontmatter": {
                  "description": "Generate accurate project time estimates",
                  "category": "team-collaboration",
                  "allowed-tools": "Bash(git *), Bash(gh *)"
                },
                "content": "# estimate-assistant\n\nGenerate accurate project time estimates\n\n## Purpose\nThis command analyzes past commits, PR completion times, code complexity metrics, and team performance to provide accurate task estimates. It helps teams move beyond gut-feel estimates to data-backed predictions.\n\n## Usage\n```bash\n# Estimate a specific task based on description\nclaude \"Estimate task: Implement OAuth2 login flow with Google\"\n\n# Analyze historical accuracy of estimates\nclaude \"Show estimation accuracy for the last 10 sprints\"\n\n# Estimate based on code changes\nclaude \"Estimate effort for refactoring src/api/users module\"\n\n# Get team member specific estimates\nclaude \"How long would it take Alice to implement the payment webhook handler?\"\n```\n\n## Instructions\n\n### 1. Gather Historical Data\nCollect data from git history and Linear:\n\n```bash\n# Get commit history with timestamps and authors\ngit log --pretty=format:\"%h|%an|%ad|%s\" --date=iso --since=\"6 months ago\" > commit_history.txt\n\n# Analyze PR completion times\ngh pr list --state closed --limit 100 --json number,title,createdAt,closedAt,additions,deletions,files\n\n# Get file change frequency\ngit log --pretty=format: --name-only --since=\"6 months ago\" | sort | uniq -c | sort -rn\n\n# Analyze commit patterns by author\ngit shortlog -sn --since=\"6 months ago\"\n```\n\n### 2. Calculate Code Complexity Metrics\nAnalyze code characteristics:\n\n```javascript\nfunction analyzeComplexity(filePath) {\n  const metrics = {\n    lines: 0,\n    cyclomaticComplexity: 0,\n    dependencies: 0,\n    testCoverage: 0,\n    similarFiles: []\n  };\n  \n  // Count lines of code\n  const content = readFile(filePath);\n  metrics.lines = content.split('\\n').length;\n  \n  // Cyclomatic complexity (simplified)\n  const conditions = content.match(/if\\s*\\(|while\\s*\\(|for\\s*\\(|case\\s+|\\?\\s*:/g);\n  metrics.cyclomaticComplexity = (conditions?.length || 0) + 1;\n  \n  // Count imports/dependencies\n  const imports = content.match(/import.*from|require\\(/g);\n  metrics.dependencies = imports?.length || 0;\n  \n  // Find similar files by structure\n  metrics.similarFiles = findSimilarFiles(filePath);\n  \n  return metrics;\n}\n```\n\n### 3. Build Estimation Models\n\n#### Time-Based Estimation\n```javascript\nclass HistoricalEstimator {\n  constructor(gitData, linearData) {\n    this.gitData = gitData;\n    this.linearData = linearData;\n    this.authorVelocity = new Map();\n    this.fileTypeMultipliers = new Map();\n  }\n  \n  calculateAuthorVelocity(author) {\n    const authorCommits = this.gitData.filter(c => c.author === author);\n    const taskCompletions = this.linearData.filter(t => \n      t.assignee === author && t.completedAt\n    );\n    \n    // Lines of code per day\n    const totalLines = authorCommits.reduce((sum, c) => \n      sum + c.additions + c.deletions, 0\n    );\n    const totalDays = this.calculateWorkDays(authorCommits);\n    const linesPerDay = totalLines / totalDays;\n    \n    // Story points per sprint\n    const pointsCompleted = taskCompletions.reduce((sum, t) => \n      sum + (t.estimate || 0), 0\n    );\n    const sprintCount = this.countSprints(taskCompletions);\n    const pointsPerSprint = pointsCompleted / sprintCount;\n    \n    return {\n      linesPerDay,\n      pointsPerSprint,\n      averageTaskDuration: this.calculateAverageTaskDuration(taskCompletions),\n      accuracy: this.calculateEstimateAccuracy(taskCompletions)\n    };\n  }\n  \n  estimateTask(description, assignee = null) {\n    // Extract key features from description\n    const features = this.extractFeatures(description);\n    \n    // Find similar completed tasks\n    const similarTasks = this.findSimilarTasks(features);\n    \n    // Base estimate from similar tasks\n    let baseEstimate = this.calculateMedianEstimate(similarTasks);\n    \n    // Adjust for complexity indicators\n    const complexityMultiplier = this.calculateComplexityMultiplier(features);\n    baseEstimate *= complexityMultiplier;\n    \n    // Adjust for assignee if specified\n    if (assignee) {\n      const velocity = this.calculateAuthorVelocity(assignee);\n      const teamAvgVelocity = this.calculateTeamAverageVelocity();\n      const velocityRatio = velocity.pointsPerSprint / teamAvgVelocity;\n      baseEstimate *= (2 - velocityRatio); // Faster devs get lower estimates\n    }\n    \n    // Add confidence interval\n    const confidence = this.calculateConfidence(similarTasks.length, features);\n    \n    return {\n      estimate: Math.round(baseEstimate),\n      confidence,\n      range: {\n        min: Math.round(baseEstimate * 0.7),\n        max: Math.round(baseEstimate * 1.5)\n      },\n      basedOn: similarTasks.slice(0, 3),\n      factors: this.explainFactors(features, complexityMultiplier)\n    };\n  }\n}\n```\n\n#### Pattern Recognition\n```javascript\nfunction extractFeatures(taskDescription) {\n  const features = {\n    keywords: [],\n    fileTypes: [],\n    modules: [],\n    complexity: 'medium',\n    type: 'feature', // feature, bug, refactor, etc.\n    hasTests: false,\n    hasUI: false,\n    hasAPI: false,\n    hasDatabase: false\n  };\n  \n  // Keywords that indicate complexity\n  const complexityKeywords = {\n    high: ['refactor', 'migrate', 'redesign', 'optimize', 'architecture'],\n    medium: ['implement', 'add', 'create', 'update', 'integrate'],\n    low: ['fix', 'adjust', 'tweak', 'change', 'modify']\n  };\n  \n  // Detect task type\n  if (taskDescription.match(/bug|fix|repair|broken/i)) {\n    features.type = 'bug';\n  } else if (taskDescription.match(/refactor|cleanup|optimize/i)) {\n    features.type = 'refactor';\n  } else if (taskDescription.match(/test|spec|coverage/i)) {\n    features.type = 'test';\n  }\n  \n  // Detect components\n  features.hasUI = /UI|frontend|component|view|page/i.test(taskDescription);\n  features.hasAPI = /API|endpoint|route|REST|GraphQL/i.test(taskDescription);\n  features.hasDatabase = /database|DB|migration|schema|query/i.test(taskDescription);\n  features.hasTests = /test|spec|TDD|coverage/i.test(taskDescription);\n  \n  // Extract file types mentioned\n  const fileTypeMatches = taskDescription.match(/\\.(js|ts|jsx|tsx|py|java|go|rb|css|scss)/g);\n  if (fileTypeMatches) {\n    features.fileTypes = [...new Set(fileTypeMatches)];\n  }\n  \n  return features;\n}\n```\n\n### 4. Velocity Tracking\nTrack team and individual performance:\n\n```javascript\nclass VelocityTracker {\n  async analyzeVelocity(timeframe = '3 months') {\n    // Get completed tasks with estimates and actual time\n    const completedTasks = await this.getCompletedTasks(timeframe);\n    \n    const analysis = {\n      team: {\n        plannedPoints: 0,\n        completedPoints: 0,\n        averageVelocity: 0,\n        velocityTrend: [],\n        estimateAccuracy: 0\n      },\n      individuals: new Map(),\n      taskTypes: new Map()\n    };\n    \n    // Group by sprint\n    const tasksBySprint = this.groupBySprint(completedTasks);\n    \n    for (const [sprint, tasks] of tasksBySprint) {\n      const sprintVelocity = tasks.reduce((sum, t) => sum + (t.estimate || 0), 0);\n      const sprintActual = tasks.reduce((sum, t) => sum + (t.actualPoints || t.estimate || 0), 0);\n      \n      analysis.team.velocityTrend.push({\n        sprint,\n        planned: sprintVelocity,\n        actual: sprintActual,\n        accuracy: sprintVelocity ? (sprintActual / sprintVelocity) : 1\n      });\n    }\n    \n    // Individual velocity\n    const tasksByAssignee = this.groupBy(completedTasks, 'assignee');\n    for (const [assignee, tasks] of tasksByAssignee) {\n      analysis.individuals.set(assignee, {\n        tasksCompleted: tasks.length,\n        pointsCompleted: tasks.reduce((sum, t) => sum + (t.estimate || 0), 0),\n        averageAccuracy: this.calculateAccuracy(tasks),\n        strengths: this.identifyStrengths(tasks)\n      });\n    }\n    \n    return analysis;\n  }\n}\n```\n\n### 5. Machine Learning Estimation\nUse historical patterns for prediction:\n\n```javascript\nclass MLEstimator {\n  trainModel(historicalTasks) {\n    // Feature extraction\n    const features = historicalTasks.map(task => ({\n      // Text features\n      titleLength: task.title.length,\n      descriptionLength: task.description.length,\n      hasAcceptanceCriteria: task.description.includes('Acceptance'),\n      \n      // Code features\n      filesChanged: task.linkedPR?.filesChanged || 0,\n      linesAdded: task.linkedPR?.additions || 0,\n      linesDeleted: task.linkedPR?.deletions || 0,\n      \n      // Task features\n      labels: task.labels.length,\n      hasDesignDoc: task.attachments?.some(a => a.title.includes('design')),\n      dependencies: task.blockedBy?.length || 0,\n      \n      // Historical features\n      assigneeAvgVelocity: this.getAssigneeVelocity(task.assignee),\n      teamLoad: this.getTeamLoad(task.createdAt),\n      \n      // Target\n      actualEffort: task.actualPoints || task.estimate\n    }));\n    \n    // Simple linear regression (in practice, use a proper ML library)\n    return this.fitLinearModel(features);\n  }\n  \n  predict(taskDescription, context) {\n    const features = this.extractTaskFeatures(taskDescription, context);\n    const prediction = this.model.predict(features);\n    \n    // Add uncertainty based on feature similarity\n    const similarityScore = this.calculateSimilarity(features);\n    const uncertainty = 1 - similarityScore;\n    \n    return {\n      estimate: Math.round(prediction),\n      confidence: similarityScore,\n      breakdown: this.explainPrediction(features, prediction)\n    };\n  }\n}\n```\n\n### 6. Estimation Report Format\n\n```markdown\n## Task Estimation Report\n\n**Task:** Implement OAuth2 login flow with Google\n**Date:** 2024-01-15\n\n### Estimate: 5 Story Points (2)\n**Confidence:** 78%\n**Estimated Hours:** 15-25 hours\n\n### Analysis Breakdown\n\n#### Similar Completed Tasks:\n1. \"Implement GitHub OAuth integration\" - 5 points (actual: 6)\n2. \"Add Facebook login\" - 4 points (actual: 4)  \n3. \"Setup SAML SSO\" - 8 points (actual: 7)\n\n#### Complexity Factors:\n- **Authentication Flow** (+1 point): OAuth2 requires multiple redirects\n- **External API** (+1 point): Google API integration\n- **Security** (+1 point): Token storage and validation\n- **Testing** (-0.5 points): Similar tests already exist\n\n#### Historical Data:\n- Team average for auth features: 4.8 points\n- Last 5 auth tasks accuracy: 85%\n- Assignee velocity: 1.2x team average\n\n#### Risk Factors:\n Google API changes frequently\n No existing OAuth2 infrastructure\n Team has OAuth experience\n Good documentation available\n\n### Recommendations:\n1. Allocate 1 point for initial Google API setup\n2. Include time for security review\n3. Plan for integration tests with mock OAuth server\n4. Consider pairing with team member who did GitHub OAuth\n\n### Sprint Planning:\n- Can be completed in one sprint\n- Best paired with other auth-related tasks\n- Should not be last task in sprint (risk buffer)\n```\n\n### 7. Error Handling\n```javascript\n// Handle missing historical data\nif (historicalTasks.length < 10) {\n  console.warn(\"Limited historical data. Estimates may be less accurate.\");\n  // Fall back to rule-based estimation\n}\n\n// Handle new types of work\nconst similarity = findSimilarTasks(description);\nif (similarity.maxScore < 0.5) {\n  console.warn(\"This appears to be a new type of task. Using conservative estimate.\");\n  // Apply uncertainty multiplier\n}\n\n// Handle missing Linear connection\nif (!linear.available) {\n  console.log(\"Using git history only for estimation\");\n  // Use git-based estimation\n}\n```\n\n## Example Output\n\n```\nAnalyzing task: \"Refactor user authentication to use JWT tokens\"\n\n Historical Analysis:\n- Found 23 similar authentication tasks\n- Average completion: 4.2 story points\n- Accuracy rate: 82%\n\n Estimation Calculation:\nBase estimate: 4 points (from similar tasks)\nAdjustments:\n  +1 point - Refactoring (higher complexity)\n  +0.5 points - Security implications  \n  -0.5 points - Existing test coverage\n  \nFinal estimate: 5 story points\n\n Confidence Analysis:\n- High similarity to previous tasks (85%)\n- Good historical data (23 samples)\n- Confidence: 78%\n\n Team Insights:\n- Alice: Completed 3 similar tasks (avg 4.3 points)\n- Bob: Strong in refactoring (20% faster than average)\n- Recommended assignee: Bob\n\n Time Estimates:\n- Optimistic: 12 hours (3 points)\n- Realistic: 20 hours (5 points)\n- Pessimistic: 32 hours (8 points)\n\n Breakdown:\n1. Analyze current auth system (0.5 points)\n2. Design JWT token structure (0.5 points)\n3. Implement JWT service (1.5 points)\n4. Refactor auth middleware (1.5 points)\n5. Update tests and documentation (1 point)\n```\n\n## Tips\n- Maintain historical data for at least 6 months\n- Re-calibrate estimates after each sprint\n- Track actual vs estimated for continuous improvement\n- Consider external factors (holidays, team changes)\n- Use pair programming multipliers for complex tasks\n- Document assumptions in estimates\n- Review estimates in retros"
              },
              {
                "name": "/issue-triage",
                "description": "Triage and prioritize issues effectively",
                "path": "plugins/commands-team-collaboration/commands/issue-triage.md",
                "frontmatter": {
                  "description": "Triage and prioritize issues effectively",
                  "category": "team-collaboration"
                },
                "content": "# issue-triage\n\nTriage and prioritize issues effectively\n\n## System\n\nYou are an issue triage specialist that analyzes GitHub issues and intelligently routes them to Linear with appropriate categorization, prioritization, and team assignment. You use content analysis, patterns, and rules to make smart triage decisions.\n\n## Instructions\n\nWhen triaging GitHub issues:\n\n1. **Issue Analysis**\n   ```javascript\n   async function analyzeIssue(issue) {\n     const analysis = {\n       // Content analysis\n       sentiment: analyzeSentiment(issue.title, issue.body),\n       urgency: detectUrgency(issue),\n       category: categorizeIssue(issue),\n       complexity: estimateComplexity(issue),\n       \n       // User analysis\n       authorType: classifyAuthor(issue.author),\n       authorHistory: await getAuthorHistory(issue.author),\n       \n       // Technical analysis\n       stackTrace: extractStackTrace(issue.body),\n       affectedComponents: detectComponents(issue),\n       reproducibility: assessReproducibility(issue),\n       \n       // Business impact\n       userImpact: estimateUserImpact(issue),\n       businessPriority: calculateBusinessPriority(issue)\n     };\n     \n     return analysis;\n   }\n   ```\n\n2. **Categorization Rules**\n   ```javascript\n   const categorizationRules = [\n     {\n       name: 'Security Issue',\n       patterns: [/security/i, /vulnerability/i, /CVE-/],\n       labels: ['security'],\n       priority: 1, // Urgent\n       team: 'security',\n       notify: ['security-lead']\n     },\n     {\n       name: 'Bug Report',\n       patterns: [/bug/i, /error/i, /crash/i, /broken/i],\n       hasStackTrace: true,\n       labels: ['bug'],\n       priority: (issue) => issue.sentiment < -0.5 ? 2 : 3,\n       team: 'engineering'\n     },\n     {\n       name: 'Feature Request',\n       patterns: [/feature/i, /enhancement/i, /add/i, /implement/i],\n       labels: ['enhancement'],\n       priority: 4,\n       team: 'product',\n       requiresDiscussion: true\n     },\n     {\n       name: 'Documentation',\n       patterns: [/docs/i, /documentation/i, /readme/i],\n       labels: ['documentation'],\n       priority: 4,\n       team: 'docs'\n     }\n   ];\n   ```\n\n3. **Priority Calculation**\n   ```javascript\n   function calculatePriority(issue, analysis) {\n     let score = 0;\n     \n     // Urgency indicators\n     if (analysis.urgency === 'immediate') score += 40;\n     if (containsKeywords(issue, ['urgent', 'asap', 'critical'])) score += 20;\n     if (issue.title.includes('') || issue.title.includes('!!!')) score += 15;\n     \n     // Impact assessment\n     score += analysis.userImpact * 10;\n     if (analysis.affectedComponents.includes('core')) score += 20;\n     if (analysis.reproducibility === 'always') score += 10;\n     \n     // Author influence\n     if (analysis.authorType === 'enterprise') score += 15;\n     if (analysis.authorHistory.issuesOpened > 10) score += 5;\n     \n     // Time decay\n     const ageInDays = (Date.now() - new Date(issue.createdAt)) / (1000 * 60 * 60 * 24);\n     if (ageInDays > 30) score -= 10;\n     \n     // Map score to priority\n     if (score >= 70) return 1; // Urgent\n     if (score >= 50) return 2; // High\n     if (score >= 30) return 3; // Medium\n     return 4; // Low\n   }\n   ```\n\n4. **Team Assignment**\n   ```javascript\n   async function assignTeam(issue, analysis) {\n     // Rule-based assignment\n     for (const rule of categorizationRules) {\n       if (matchesRule(issue, rule)) {\n         return rule.team;\n       }\n     }\n     \n     // Component-based assignment\n     const componentTeamMap = {\n       'auth': 'identity-team',\n       'api': 'platform-team',\n       'ui': 'frontend-team',\n       'database': 'data-team'\n     };\n     \n     for (const component of analysis.affectedComponents) {\n       if (componentTeamMap[component]) {\n         return componentTeamMap[component];\n       }\n     }\n     \n     // ML-based assignment (if available)\n     if (ML_ENABLED) {\n       return await predictTeam(issue, analysis);\n     }\n     \n     // Default assignment\n     return 'triage-team';\n   }\n   ```\n\n5. **Duplicate Detection**\n   ```javascript\n   async function findDuplicates(issue) {\n     // Semantic similarity search\n     const similar = await searchSimilarIssues(issue, {\n       threshold: 0.85,\n       limit: 5\n     });\n     \n     // Title similarity\n     const titleMatches = await searchByTitle(issue.title, {\n       fuzzy: true,\n       distance: 3\n     });\n     \n     // Stack trace matching (for bugs)\n     const stackTrace = extractStackTrace(issue.body);\n     const stackMatches = stackTrace ? \n       await searchByStackTrace(stackTrace) : [];\n     \n     return {\n       likely: similar.filter(s => s.score > 0.9),\n       possible: [...similar, ...titleMatches, ...stackMatches]\n         .filter(s => s.score > 0.7)\n         .slice(0, 5)\n     };\n   }\n   ```\n\n6. **Auto-labeling**\n   ```javascript\n   function generateLabels(issue, analysis) {\n     const labels = new Set();\n     \n     // Category labels\n     labels.add(analysis.category.toLowerCase());\n     \n     // Priority labels\n     labels.add(`priority/${getPriorityName(analysis.priority)}`);\n     \n     // Technical labels\n     if (analysis.stackTrace) labels.add('has-stack-trace');\n     if (analysis.reproducibility === 'always') labels.add('reproducible');\n     \n     // Component labels\n     analysis.affectedComponents.forEach(c => \n       labels.add(`component/${c}`)\n     );\n     \n     // Status labels\n     if (analysis.needsMoreInfo) labels.add('needs-info');\n     if (analysis.duplicate) labels.add('duplicate');\n     \n     return Array.from(labels);\n   }\n   ```\n\n7. **Triage Workflow**\n   ```javascript\n   async function triageIssue(issue) {\n     const workflow = {\n       analyzed: false,\n       triaged: false,\n       actions: []\n     };\n     \n     try {\n       // Step 1: Analyze\n       const analysis = await analyzeIssue(issue);\n       workflow.analyzed = true;\n       \n       // Step 2: Check duplicates\n       const duplicates = await findDuplicates(issue);\n       if (duplicates.likely.length > 0) {\n         return handleDuplicate(issue, duplicates.likely[0]);\n       }\n       \n       // Step 3: Determine routing\n       const triage = {\n         team: await assignTeam(issue, analysis),\n         priority: calculatePriority(issue, analysis),\n         labels: generateLabels(issue, analysis),\n         assignee: await suggestAssignee(issue, analysis)\n       };\n       \n       // Step 4: Create Linear task\n       const task = await createTriagedTask(issue, triage, analysis);\n       workflow.triaged = true;\n       \n       // Step 5: Update GitHub\n       await updateGitHubIssue(issue, triage, task);\n       \n       // Step 6: Notify stakeholders\n       await notifyStakeholders(issue, triage, analysis);\n       \n       return workflow;\n     } catch (error) {\n       workflow.error = error;\n       return workflow;\n     }\n   }\n   ```\n\n8. **Batch Triage**\n   ```javascript\n   async function batchTriage(filters) {\n     const issues = await fetchUntriaged(filters);\n     const results = {\n       total: issues.length,\n       triaged: [],\n       skipped: [],\n       failed: []\n     };\n     \n     console.log(`Found ${issues.length} issues to triage`);\n     \n     for (const issue of issues) {\n       try {\n         // Skip if already triaged\n         if (hasTriageLabel(issue)) {\n           results.skipped.push(issue);\n           continue;\n         }\n         \n         // Triage issue\n         const result = await triageIssue(issue);\n         if (result.triaged) {\n           results.triaged.push({ issue, result });\n         } else {\n           results.failed.push({ issue, error: result.error });\n         }\n         \n         // Progress update\n         updateProgress(results);\n         \n       } catch (error) {\n         results.failed.push({ issue, error });\n       }\n     }\n     \n     return results;\n   }\n   ```\n\n9. **Triage Templates**\n   ```javascript\n   const triageTemplates = {\n     bug: {\n       linearTemplate: `\n   ## Bug Report\n   \n   **Reported by:** {author}\n   **Severity:** {severity}\n   **Reproducibility:** {reproducibility}\n   \n   ### Description\n   {description}\n   \n   ### Stack Trace\n   \\`\\`\\`\n   {stackTrace}\n   \\`\\`\\`\n   \n   ### Environment\n   {environment}\n   \n   ### Steps to Reproduce\n   {reproSteps}\n       `,\n       requiredInfo: ['description', 'environment', 'reproSteps']\n     },\n     \n     feature: {\n       linearTemplate: `\n   ## Feature Request\n   \n   **Requested by:** {author}\n   **Business Value:** {businessValue}\n   \n   ### Description\n   {description}\n   \n   ### Use Cases\n   {useCases}\n   \n   ### Acceptance Criteria\n   {acceptanceCriteria}\n       `,\n       requiresApproval: true\n     }\n   };\n   ```\n\n10. **Triage Metrics**\n    ```javascript\n    function generateTriageMetrics(period = '7d') {\n      return {\n        volume: {\n          total: countIssues(period),\n          byCategory: groupByCategory(period),\n          byPriority: groupByPriority(period),\n          byTeam: groupByTeam(period)\n        },\n        \n        performance: {\n          avgTriageTime: calculateAvgTriageTime(period),\n          autoTriageRate: calculateAutoTriageRate(period),\n          accuracyRate: calculateAccuracy(period)\n        },\n        \n        patterns: {\n          commonIssues: findCommonPatterns(period),\n          peakTimes: analyzePeakTimes(period),\n          teamLoad: analyzeTeamLoad(period)\n        }\n      };\n    }\n    ```\n\n## Examples\n\n### Manual Triage\n```bash\n# Triage single issue\nclaude issue-triage 123\n\n# Triage with options\nclaude issue-triage 123 --team=\"backend\" --priority=\"high\"\n\n# Interactive triage\nclaude issue-triage 123 --interactive\n```\n\n### Automated Triage\n```bash\n# Triage all untriaged issues\nclaude issue-triage --auto\n\n# Triage with filters\nclaude issue-triage --auto --label=\"needs-triage\"\n\n# Scheduled triage\nclaude issue-triage --auto --schedule=\"*/15 * * * *\"\n```\n\n### Triage Configuration\n```bash\n# Set up triage rules\nclaude issue-triage --setup-rules\n\n# Test triage rules\nclaude issue-triage --test-rules --dry-run\n\n# Export triage config\nclaude issue-triage --export-config > triage-config.json\n```\n\n## Output Format\n\n```\nIssue Triage Report\n===================\nProcessed: 2025-01-16 11:00:00\nMode: Automatic\n\nTriage Summary:\n\nTotal Issues      : 47\nSuccessfully Triaged : 44 (93.6%)\nDuplicates Found  : 3\nManual Review     : 3\nFailed           : 0\n\nBy Category:\n- Bug Reports     : 28 (63.6%)\n- Feature Requests: 12 (27.3%)\n- Documentation   : 4 (9.1%)\n\nBy Priority:\n- Urgent (P1)     : 3  \n- High (P2)       : 12 \n- Medium (P3)     : 24 \n- Low (P4)        : 5  \n\nTeam Assignments:\n- Backend         : 18\n- Frontend        : 15\n- Security        : 3\n- Documentation   : 4\n- Triage Team     : 4\n\nNotable Issues:\n #456: Security vulnerability in auth system  Security Team (P1)\n #789: Database connection pooling errors  Backend Team (P2)\n #234: Add dark mode support  Frontend Team (P3)\n\nActions Taken:\n Created 44 Linear tasks\n Applied 156 labels\n Assigned to 12 team members\n Linked 3 duplicates\n Sent 8 notifications\n\nTriage Metrics:\n- Avg time per issue: 2.3s\n- Auto-triage accuracy: 94.2%\n- Manual intervention: 6.8%\n```\n\n## Best Practices\n\n1. **Rule Refinement**\n   - Regularly review triage accuracy\n   - Update patterns based on feedback\n   - Test rules before deployment\n\n2. **Quality Control**\n   - Sample triaged issues for review\n   - Track false positives/negatives\n   - Implement feedback loops\n\n3. **Stakeholder Communication**\n   - Notify teams of new assignments\n   - Provide triage summaries\n   - Escalate critical issues\n\n4. **Continuous Improvement**\n   - Analyze triage patterns\n   - Optimize assignment rules\n   - Implement ML when appropriate"
              },
              {
                "name": "/memory-spring-cleaning",
                "description": "Clean and organize project memory",
                "path": "plugins/commands-team-collaboration/commands/memory-spring-cleaning.md",
                "frontmatter": {
                  "description": "Clean and organize project memory",
                  "category": "team-collaboration"
                },
                "content": "# Memory Spring Cleaning\n\nClean and organize project memory\n\n## Instructions\n\n1. **Get Overview**\n   - List all CLAUDE.md and CLAUDE.local.md files in the project hierarchy\n\n2. **Iterative Review**\n   - Process each file systematically, starting with the root `CLAUDE.md` file\n   - Load the current content\n   - Compare documented patterns against actual implementation\n   - Identify outdated, incorrect, or missing information\n\n3. **Update and Refactor**\n   - For each memory file:\n     - Verify all technical claims against the current codebase\n     - Remove obsolete information\n     - Consolidate duplicate entries\n     - Ensure information is in the most appropriate file\n   - When information belongs to a specific subcomponent, ensure it's placed correctly:\n     - UI-specific patterns  `apps/myproject-ui/CLAUDE.md`\n     - API conventions  `apps/myproject-api/CLAUDE.md`\n     - Infrastructure details  `cdk/CLAUDE.md` or `infrastructure/CLAUDE.md`\n\n4. **Focus on Quality**\n   - Prioritize clarity, accuracy, and relevance\n   - Remove any information that no longer serves the project\n   - Ensure each piece of information is in its most logical location\n\n## Credit\n\nThis command is based on the work of Thomas Landgraf: https://thomaslandgraf.substack.com/p/claude-codes-memory-working-with"
              },
              {
                "name": "/migration-assistant",
                "description": "Assist with system migration planning",
                "path": "plugins/commands-team-collaboration/commands/migration-assistant.md",
                "frontmatter": {
                  "description": "Assist with system migration planning",
                  "category": "team-collaboration",
                  "argument-hint": "Valid actions: plan, analyze, migrate, verify, rollback"
                },
                "content": "# Migration Assistant\n\nAssist with system migration planning\n\n## Instructions\n\n1. **Check Prerequisites**\n   - Verify GitHub CLI (`gh`) is installed and authenticated\n   - Check if Linear MCP server is connected\n   - Ensure sufficient permissions in both systems\n   - Confirm backup storage is available\n\n2. **Parse Migration Parameters**\n   - Extract action and options from: **$ARGUMENTS**\n   - Valid actions: plan, analyze, migrate, verify, rollback\n   - Determine source and target systems\n   - Set migration scope and filters\n\n3. **Initialize Migration Environment**\n   - Create migration workspace directory\n   - Set up logging and audit trails\n   - Initialize checkpoint system\n   - Prepare rollback mechanisms\n\n4. **Execute Migration Action**\n   Based on the selected action:\n\n   ### Plan Action\n   - Analyze source system structure\n   - Map fields between systems\n   - Identify potential conflicts\n   - Generate migration strategy\n   - Estimate time and resources\n   - Create detailed migration plan\n\n   ### Analyze Action\n   - Count items to migrate\n   - Check data compatibility\n   - Identify custom fields\n   - Assess attachment sizes\n   - Calculate migration impact\n   - Generate pre-migration report\n\n   ### Migrate Action\n   - Create full backup of source data\n   - Execute migration in batches\n   - Transform data between formats\n   - Preserve relationships\n   - Handle attachments and media\n   - Create progress checkpoints\n   - Log all operations\n\n   ### Verify Action\n   - Compare source and target data\n   - Validate all items migrated\n   - Check relationship integrity\n   - Verify custom field mappings\n   - Test cross-references\n   - Generate verification report\n\n   ### Rollback Action\n   - Load rollback checkpoint\n   - Restore original state\n   - Clean up partial migrations\n   - Verify rollback completion\n   - Generate rollback report\n\n## Usage\n```bash\nmigration-assistant [action] [options]\n```\n\n## Actions\n- `plan` - Create migration plan\n- `analyze` - Assess migration scope\n- `migrate` - Execute migration\n- `verify` - Validate migration results\n- `rollback` - Revert migration\n\n## Options\n- `--source <system>` - Source system (github/linear)\n- `--target <system>` - Target system (github/linear)\n- `--scope <items>` - Items to migrate (all/issues/prs/projects)\n- `--dry-run` - Simulate migration\n- `--parallel <n>` - Parallel processing threads\n- `--checkpoint` - Enable checkpoint recovery\n- `--mapping-file <path>` - Custom field mappings\n- `--preserve-ids` - Maintain reference IDs\n- `--archive-source` - Archive after migration\n\n## Examples\n```bash\n# Plan GitHub to Linear migration\nmigration-assistant plan --source github --target linear\n\n# Analyze migration scope\nmigration-assistant analyze --scope all\n\n# Dry run migration\nmigration-assistant migrate --dry-run --parallel 4\n\n# Execute migration with checkpoints\nmigration-assistant migrate --checkpoint --backup\n\n# Verify migration completeness\nmigration-assistant verify --deep-check\n\n# Rollback if needed\nmigration-assistant rollback --transaction-id 12345\n```\n\n## Migration Phases\n\n### 1. Planning Phase\n- Inventory source data\n- Map data structures\n- Identify incompatibilities\n- Estimate migration time\n- Generate migration plan\n\n### 2. Preparation Phase\n- Create full backup\n- Validate permissions\n- Set up target structure\n- Configure mappings\n- Test connectivity\n\n### 3. Migration Phase\n- Transfer data in batches\n- Maintain relationships\n- Preserve metadata\n- Handle attachments\n- Update references\n\n### 4. Verification Phase\n- Compare record counts\n- Validate data integrity\n- Check relationships\n- Verify attachments\n- Test functionality\n\n### 5. Finalization Phase\n- Update documentation\n- Redirect webhooks\n- Archive source data\n- Generate reports\n- Train users\n\n## Data Mapping Configuration\n```yaml\nmappings:\n  github_to_linear:\n    issue:\n      title: title\n      body: description\n      state: status\n      labels: labels\n      milestone: cycle\n      assignees: assignees\n    \n    custom_fields:\n      - source: \"custom.priority\"\n        target: \"priority\"\n        transform: \"map_priority\"\n      \n    relationships:\n      - type: \"parent-child\"\n        source: \"depends_on\"\n        target: \"parent\"\n    \n  linear_to_github:\n    issue:\n      title: title\n      description: body\n      status: state\n      priority: labels\n      cycle: milestone\n```\n\n## Migration Safety Features\n\n### Pre-Migration Checks\n- Storage capacity verification\n- API rate limit assessment\n- Permission validation\n- Dependency checking\n- Conflict detection\n\n### During Migration\n- Transaction logging\n- Progress tracking\n- Error recovery\n- Checkpoint creation\n- Performance monitoring\n\n### Post-Migration\n- Data verification\n- Integrity checking\n- Performance testing\n- User acceptance\n- Rollback readiness\n\n## Checkpoint Recovery\n```json\n{\n  \"checkpoint\": {\n    \"id\": \"mig-20240120-1430\",\n    \"progress\": {\n      \"total_items\": 5000,\n      \"completed\": 3750,\n      \"failed\": 12,\n      \"pending\": 1238\n    },\n    \"state\": {\n      \"last_processed_id\": \"issue-3750\",\n      \"batch_number\": 75,\n      \"error_count\": 12\n    }\n  }\n}\n```\n\n## Rollback Capabilities\n- Point-in-time recovery\n- Selective rollback\n- Relationship preservation\n- Audit trail maintenance\n- Zero data loss guarantee\n\n## Performance Optimization\n- Batch processing\n- Parallel transfers\n- API call optimization\n- Caching strategies\n- Resource monitoring\n\n## Migration Reports\n- Executive summary\n- Detailed item mapping\n- Error analysis\n- Performance metrics\n- Recommendation list\n\n## Common Migration Scenarios\n\n### GitHub Issues  Linear\n1. Map GitHub labels to Linear labels/projects\n2. Convert milestones to cycles\n3. Preserve issue numbers as references\n4. Migrate comments with user mapping\n5. Handle attachments and images\n\n### Linear  GitHub Issues\n1. Map Linear statuses to GitHub states\n2. Convert cycles to milestones\n3. Preserve Linear IDs in issue body\n4. Map Linear projects to labels\n5. Handle custom fields\n\n## Required MCP Servers\n- mcp-server-github\n- mcp-server-linear\n\n## Error Handling\n- Automatic retry with backoff\n- Detailed error logging\n- Partial failure recovery\n- Manual intervention points\n- Comprehensive error reports\n\n## Best Practices\n- Always run analysis first\n- Use dry-run for testing\n- Migrate in phases for large datasets\n- Maintain communication with team\n- Keep source data until verified\n- Document custom mappings\n- Test rollback procedures\n\n## Compliance & Audit\n- Full audit trail\n- Data retention compliance\n- Privacy preservation\n- Change authorization\n- Migration certification\n\n## Notes\nThis command creates a complete migration package including backups, logs, and documentation. The migration can be resumed from checkpoints in case of interruption. All migrations are reversible within the retention period."
              },
              {
                "name": "/retrospective-analyzer",
                "description": "Analyze team retrospectives for insights",
                "path": "plugins/commands-team-collaboration/commands/retrospective-analyzer.md",
                "frontmatter": {
                  "description": "Analyze team retrospectives for insights",
                  "category": "team-collaboration"
                },
                "content": "# Retrospective Analyzer\n\nAnalyze team retrospectives for insights\n\n## Instructions\n\n1. **Retrospective Setup**\n   - Identify sprint to analyze (default: most recent)\n   - Check Linear MCP connection for sprint data\n   - Define retrospective format preference\n   - Set analysis time range\n\n2. **Sprint Data Collection**\n\n#### Quantitative Metrics\n```\nFrom Linear/Project Management:\n- Planned vs completed story points\n- Sprint velocity and capacity\n- Cycle time and lead time\n- Escaped defects count\n- Unplanned work percentage\n\nFrom Git/GitHub:\n- Commit frequency and distribution\n- PR merge time statistics  \n- Code review turnaround\n- Build success rate\n- Deployment frequency\n```\n\n#### Qualitative Data Sources\n```\n1. PR review comments sentiment\n2. Commit message patterns\n3. Slack conversations (if available)\n4. Previous retrospective action items\n5. Support ticket trends\n```\n\n3. **Automated Analysis**\n\n#### Sprint Performance Analysis\n```markdown\n# Sprint [Name] Retrospective Analysis\n\n## Sprint Overview\n- Duration: [Start] to [End]\n- Team Size: [Number] members\n- Sprint Goal: [Description]\n- Goal Achievement: [Yes/Partial/No]\n\n## Key Metrics Summary\n\n### Delivery Metrics\n| Metric | Target | Actual | Variance |\n|--------|--------|--------|----------|\n| Velocity | [X] pts | [Y] pts | [+/-Z]% |\n| Completion Rate | 90% | [X]% | [+/-Y]% |\n| Defect Rate | <5% | [X]% | [+/-Y]% |\n| Unplanned Work | <20% | [X]% | [+/-Y]% |\n\n### Process Metrics\n| Metric | This Sprint | Previous | Trend |\n|--------|-------------|----------|-------|\n| Avg PR Review Time | [X] hrs | [Y] hrs | [/] |\n| Avg Cycle Time | [X] days | [Y] days | [/] |\n| CI/CD Success Rate | [X]% | [Y]% | [/] |\n| Team Happiness | [X]/5 | [Y]/5 | [/] |\n```\n\n#### Pattern Recognition\n```markdown\n## Identified Patterns\n\n### Positive Patterns \n1. **Improved Code Review Speed**\n   - Average review time decreased by 30%\n   - Correlation with new review guidelines\n   - Recommendation: Document and maintain process\n\n2. **Consistent Daily Progress**\n   - Even commit distribution throughout sprint\n   - No last-minute rush\n   - Indicates good sprint planning\n\n### Concerning Patterns \n1. **Monday Deploy Failures**\n   - 60% of failed deployments on Mondays\n   - Possible cause: Weekend changes not tested\n   - Action: Implement Monday morning checks\n\n2. **Increasing Scope Creep**\n   - 35% unplanned work (up from 20%)\n   - Source: Urgent customer requests\n   - Action: Review sprint commitment process\n```\n\n4. **Interactive Retrospective Facilitation**\n\n#### Pre-Retrospective Report\n```markdown\n# Pre-Retrospective Insights\n\n## Data-Driven Discussion Topics\n\n### 1. What Went Well \nBased on the data, these areas showed improvement:\n-  Code review efficiency (+30%)\n-  Test coverage increase (+5%)\n-  Zero critical bugs in production\n-  All team members contributed evenly\n\n**Suggested Discussion Questions:**\n- What specific changes led to faster reviews?\n- How can we maintain zero critical bugs?\n- What made work distribution successful?\n\n### 2. What Didn't Go Well\nData indicates challenges in these areas:\n-  Sprint velocity miss (-15%)\n-  High unplanned work (35%)\n-  3 rollbacks required\n-  Team overtime increased\n\n**Suggested Discussion Questions:**\n- What caused the velocity miss?\n- How can we better handle unplanned work?\n- What led to the rollbacks?\n\n### 3. Action Items from Data\nRecommended improvements based on patterns:\n1. Implement feature flags for safer deployments\n2. Create unplanned work budget in sprint planning\n3. Add integration tests for [problem area]\n4. Schedule mid-sprint check-ins\n```\n\n#### Live Retrospective Support\n```\nDuring the retrospective, I can help with:\n\n1. **Fact Checking**: \n   \"Actually, our velocity was 45 points, not 50\"\n\n2. **Pattern Context**:\n   \"This is the 3rd sprint with Monday deploy issues\"\n\n3. **Historical Comparison**:\n   \"Last time we had similar issues, we tried X\"\n\n4. **Action Item Tracking**:\n   \"From last retro, we completed 4/6 action items\"\n```\n\n5. **Retrospective Output Formats**\n\n#### Standard Retrospective Summary\n```markdown\n# Sprint [X] Retrospective Summary\n\n## Participants\n[List of attendees]\n\n## What Went Well\n- [Categorized list with vote counts]\n- Supporting data: [Metrics]\n\n## What Didn't Go Well  \n- [Categorized list with vote counts]\n- Root cause analysis: [Details]\n\n## Action Items\n| Action | Owner | Due Date | Success Criteria |\n|--------|-------|----------|------------------|\n| [Action 1] | [Name] | [Date] | [Measurable outcome] |\n| [Action 2] | [Name] | [Date] | [Measurable outcome] |\n\n## Experiments for Next Sprint\n1. [Experiment description]\n   - Hypothesis: [What we expect]\n   - Measurement: [How we'll know]\n   - Review date: [When to assess]\n\n## Team Health Pulse\n- Energy Level: [Rating]/5\n- Clarity: [Rating]/5\n- Confidence: [Rating]/5\n- Key Quote: \"[Notable team sentiment]\"\n```\n\n#### Trend Analysis Report\n```markdown\n# Retrospective Trends Analysis\n\n## Recurring Themes (Last 5 Sprints)\n\n### Persistent Challenges\n1. **Deployment Issues** (4/5 sprints)\n   - Root cause still unresolved\n   - Recommended escalation\n\n2. **Estimation Accuracy** (5/5 sprints)\n   - Consistent 20% overrun\n   - Needs systematic approach\n\n### Improving Areas\n1. **Communication** (Improving for 3 sprints)\n2. **Code Quality** (Steady improvement)\n\n### Success Patterns\n1. **Pair Programming** (Mentioned positively 5/5)\n2. **Daily Standups** (Effective format found)\n```\n\n6. **Action Item Generation**\n\n#### Smart Action Items\n```\nBased on retrospective discussion, here are SMART action items:\n\n1. **Reduce Deploy Failures**\n   - Specific: Implement smoke tests for Monday deploys\n   - Measurable: <5% failure rate\n   - Assignable: DevOps team\n   - Relevant: Addresses 60% of failures\n   - Time-bound: By next sprint\n\n2. **Improve Estimation**\n   - Specific: Use planning poker for all stories\n   - Measurable: <20% variance from estimates\n   - Assignable: Scrum Master facilitates\n   - Relevant: Addresses velocity misses\n   - Time-bound: Start next sprint planning\n```\n\n## Error Handling\n\n### No Linear Data\n```\n\"Linear MCP not connected. Using git data only.\n\nMissing insights:\n- Story point analysis\n- Task-level metrics\n- Team capacity data\n\nWould you like to:\n1. Proceed with git data only\n2. Manually input sprint metrics\n3. Connect Linear and retry\"\n```\n\n### Incomplete Sprint\n```\n\"Sprint appears to be in progress. \n\nCurrent analysis based on:\n- [X] days of [Y] total\n- [Z]% work completed\n\nRecommendation: Run full analysis after sprint ends\nProceed with partial analysis? [Y/N]\"\n```\n\n## Advanced Features\n\n### Sentiment Analysis\n```python\n# Analyze PR comments and commit messages\nsentiment_indicators = {\n    'positive': ['fixed', 'improved', 'resolved', 'great'],\n    'negative': ['bug', 'issue', 'broken', 'failed', 'frustrated'],\n    'neutral': ['updated', 'changed', 'modified']\n}\n\n# Generate sentiment report\n\"Team Sentiment Analysis:\n- Positive indicators: 65%\n- Negative indicators: 25%  \n- Neutral: 10%\n\nTrend: Improving from last sprint (was 55% positive)\"\n```\n\n### Predictive Insights\n```\n\"Based on current patterns:\n\n Risk Predictions:\n- 70% chance of velocity miss if unplanned work continues\n- Deploy failures likely to increase without intervention\n\n Opportunity Predictions:\n- 15% velocity gain possible with proposed process changes\n- Team happiness likely to improve with workload balancing\"\n```\n\n### Experiment Tracking\n```\n\"Previous Experiments Results:\n\n1. 'No Meeting Fridays' (Sprint 12-14)\n   - Result: 20% productivity increase\n   - Recommendation: Make permanent\n\n2. 'Pair Programming for Complex Tasks' (Sprint 15)\n   - Result: 50% fewer defects\n   - Recommendation: Continue with guidelines\"\n```\n\n## Integration Options\n\n1. **Linear**: Create action items as tasks\n2. **Slack**: Post summary to team channel\n3. **Confluence**: Export formatted retrospective page\n4. **GitHub**: Create issues for technical debt items\n5. **Calendar**: Schedule action item check-ins\n\n## Best Practices\n\n1. **Data Before Discussion**: Review metrics first\n2. **Focus on Patterns**: Look for recurring themes\n3. **Action-Oriented**: Every insight needs action\n4. **Time-boxed**: Keep retrospective focused\n5. **Follow-up**: Track action item completion\n6. **Celebrate Wins**: Acknowledge improvements\n7. **Safe Space**: Encourage honest feedback"
              },
              {
                "name": "/session-learning-capture",
                "description": "Capture and document session learnings",
                "path": "plugins/commands-team-collaboration/commands/session-learning-capture.md",
                "frontmatter": {
                  "description": "Capture and document session learnings",
                  "category": "team-collaboration",
                  "allowed-tools": "Glob"
                },
                "content": "# Session Learning Capture\n\nCapture and document session learnings\n\n## Instructions\n\n1. **Identify Session Learnings**\n   - Review if during your session:\n     - You learned something new about the project\n     - I corrected you on a specific implementation detail\n     - I corrected source code you generated\n     - You struggled to find specific information and had to infer details about the project\n     - You lost track of the project structure and had to look up information in the source code\n\n2. **Determine Appropriate File**\n   - Choose the right file for the information:\n     - `CLAUDE.md` for shared context that should be version controlled\n     - `CLAUDE.local.md` for private notes and developer-specific settings\n     - Subdirectory `CLAUDE.md` for component-specific information\n\n3. **Memory File Types Summary**\n   - **Shared Project Memory (`CLAUDE.md`):**\n     - Located in the repository root or any working directory\n     - Checked into version control for team-wide context sharing\n     - Loaded recursively from the current directory up to the root\n   - **Local, Non-Shared Memory (`CLAUDE.local.md`):**\n     - Placed alongside or above working files, excluded from version control\n     - Stores private, developer-specific notes and settings\n     - Loaded recursively like `CLAUDE.md`\n   - **On-Demand Subdirectory Loading:**\n     - `CLAUDE.md` files in child folders are loaded only when editing files in those subfolders\n     - Prevents unnecessary context bloat\n   - **Global User Memory (`~/.claude/CLAUDE.md`):**\n     - Acts as a personal, cross-project memory\n     - Automatically merged into sessions under your home directory\n\n4. **Update Memory Files**\n   - Add relevant, non-obvious information that should be persisted\n   - Ensure proper placement based on component relevance:\n     - UI-specific information  `apps/[project]-ui/CLAUDE.md`\n     - API-specific information  `apps/[project]-api/CLAUDE.md`\n     - Infrastructure information  `cdk/CLAUDE.md` or `infrastructure/CLAUDE.md`\n   - This ensures important knowledge is retained and available in future sessions\n\n## Credit\n\nThis command is based on the work of Thomas Landgraf: https://thomaslandgraf.substack.com/p/claude-codes-memory-working-with"
              },
              {
                "name": "/sprint-planning",
                "description": "Plan and organize sprint workflows",
                "path": "plugins/commands-team-collaboration/commands/sprint-planning.md",
                "frontmatter": {
                  "description": "Plan and organize sprint workflows",
                  "category": "team-collaboration",
                  "allowed-tools": "Bash(gh *), Bash(npm *)"
                },
                "content": "# Sprint Planning\n\nPlan and organize sprint workflows\n\n## Instructions\n\n1. **Check Linear Integration**\nFirst, verify if the Linear MCP server is connected:\n- If connected: Proceed with full integration\n- If not connected: Ask user to install Linear MCP server from https://github.com/modelcontextprotocol/servers\n- Fallback: Use GitHub issues and manual input\n\n2. **Gather Sprint Context**\nCollect the following information:\n- Sprint duration (e.g., 2 weeks)\n- Sprint start date\n- Team members involved\n- Sprint goals/themes\n- Previous sprint velocity (if available)\n\n3. **Analyze Current State**\n\n#### With Linear Connected:\n```\n1. Fetch all backlog items from Linear\n2. Get in-progress tasks and their status\n3. Analyze task priorities and dependencies\n4. Check team member assignments and capacity\n5. Review blocked tasks and impediments\n```\n\n#### Without Linear (Fallback):\n```\n1. Analyze GitHub issues by labels and milestones\n2. Review open pull requests and their status\n3. Check recent commit activity\n4. Ask user for additional context about tasks\n```\n\n4. **Sprint Planning Analysis**\n\nGenerate a comprehensive sprint plan including:\n\n```markdown\n# Sprint Planning Report - [Sprint Name]\n\n## Sprint Overview\n- Duration: [Start Date] to [End Date]\n- Team Members: [List]\n- Sprint Goal: [Description]\n\n## Capacity Analysis\n- Total Available Hours: [Calculation]\n- Previous Sprint Velocity: [Points/Hours]\n- Recommended Capacity: [80-85% of total]\n\n## Proposed Sprint Backlog\n\n### High Priority Tasks\n1. [Task ID] - [Title]\n   - Estimate: [Points/Hours]\n   - Assignee: [Name]\n   - Dependencies: [List]\n   - Acceptance Criteria: [Details]\n\n### Medium Priority Tasks\n[Similar format]\n\n### Nice-to-Have Tasks\n[Similar format]\n\n## Risk Assessment\n- Technical Risks: [List]\n- Resource Risks: [List]\n- Dependency Risks: [List]\n\n## Recommendations\n1. [Specific recommendations based on analysis]\n2. [Resource allocation suggestions]\n3. [Risk mitigation strategies]\n\n## Metrics to Track\n- Velocity\n- Burndown rate\n- Blocked time\n- Cycle time\n```\n\n5. **Interactive Planning Session**\n\nOffer to help with:\n1. Task breakdown and estimation\n2. Dependency mapping\n3. Resource balancing\n4. Risk mitigation planning\n5. Sprint backlog finalization\n\n## Error Handling\n\n### Linear Connection Issues\n```\n\"I notice the Linear MCP server is not connected. To get the most value from sprint planning:\n\n1. Install the Linear MCP server:\n   npm install -g @modelcontextprotocol/server-linear\n\n2. Configure with your Linear API key\n3. Restart this session with Linear connected\n\nFor now, I'll proceed using GitHub data and manual input.\"\n```\n\n### Missing Data\n```\n\"I need more information to create an effective sprint plan:\n- [List specific missing data]\n- [Provide templates for manual input]\n```\n\n## Example Usage\n\n### User Input:\n\"Help me plan our next 2-week sprint starting Monday\"\n\n### Assistant Response:\n```\nI'll help you plan your sprint. Let me first check your Linear integration...\n\n[If connected]: Great! I can see your Linear workspace. Let me analyze your backlog...\n\n[Analysis results]\n\nBased on my analysis, here's your proposed sprint plan:\n\n**Sprint 24-01 (Jan 8-19)**\n\n**Recommended Scope (32 story points):**\n1. ENG-123: Implement user authentication (8 pts) - John\n2. ENG-124: Database migration scripts (5 pts) - Sarah\n3. ENG-125: API rate limiting (5 pts) - Mike\n...\n\n**Key Risks:**\n- ENG-123 blocks 3 other tasks\n- Sarah has 20% allocation to support\n\nWould you like me to:\n1. Adjust the scope based on different priorities?\n2. Create a dependency visualization?\n3. Generate sprint planning meeting agenda?\n```\n\n## Best Practices\n\n1. **Always verify capacity**: Don't overcommit the team\n2. **Include buffer time**: Plan for 80-85% capacity\n3. **Consider dependencies**: Map task relationships\n4. **Balance workload**: Distribute tasks evenly\n5. **Define clear goals**: Ensure sprint has focused objectives\n6. **Plan for unknowns**: Include spike/investigation time\n\n## Integration Points\n\n- Linear: Task management and tracking\n- GitHub: Code repository and PRs\n- Slack: Team communication (if MCP available)\n- Calendar: Team availability (if accessible)\n\n## Output Formats\n\nOffer multiple output options:\n1. Markdown report (default)\n2. CSV for spreadsheet import\n3. JSON for automation tools\n4. Linear-compatible format for direct import"
              },
              {
                "name": "/standup-report",
                "description": "Generate daily standup reports",
                "path": "plugins/commands-team-collaboration/commands/standup-report.md",
                "frontmatter": {
                  "description": "Generate daily standup reports",
                  "category": "team-collaboration",
                  "allowed-tools": "Bash(git *), Bash(npm *)"
                },
                "content": "# Standup Report\n\nGenerate daily standup reports\n\n## Instructions\n\n1. **Initial Setup**\n   - Check Linear MCP server connection\n   - Determine time range (default: last 24 hours)\n   - Identify team members (from git config or user input)\n   - Set report format preferences\n\n2. **Data Collection**\n\n#### Git Activity Analysis\n```bash\n# Collect commits from last 24 hours\ngit log --since=\"24 hours ago\" --all --format=\"%h|%an|%ad|%s\" --date=short\n\n# Check branch activity\ngit for-each-ref --format='%(refname:short)|%(committerdate:short)|%(authoremail)' --sort=-committerdate refs/heads/\n\n# Analyze file changes\ngit diff --stat @{1.day.ago}\n```\n\n#### Linear Integration (if available)\n```\n1. Fetch tasks updated in last 24 hours\n2. Get task status changes\n3. Check new comments and blockers\n4. Review completed tasks\n```\n\n#### GitHub PR Status\n```\n1. Check PR updates and reviews\n2. Identify merged PRs\n3. Find new PRs created\n4. Review CI/CD status\n```\n\n3. **Report Generation**\n\nGenerate structured standup report:\n\n```markdown\n# Daily Standup Report - [Date]\n\n## Team Member: [Name]\n\n### Yesterday's Accomplishments\n-  Completed [Task ID]: [Description]\n  - Commits: [List with links]\n  - PR: [Link if applicable]\n-  Progressed on [Task ID]: [Description]\n  - Current status: [X]% complete\n  - Latest commit: [Message]\n\n### Today's Plan\n-  [Task ID]: [Description]\n  - Estimated completion: [Time]\n  - Dependencies: [List]\n-  Code review for PR #[Number]\n-  Update documentation for [Feature]\n\n### Blockers & Concerns\n-  Blocked on [Task ID]: [Reason]\n  - Need input from: [Person/Team]\n  - Expected resolution: [Time]\n-  Potential risk: [Description]\n\n### Metrics Summary\n- Commits: [Count]\n- PRs Updated: [Count]\n- Tasks Completed: [Count]\n- Cycle Time: [Average]\n```\n\n4. **Multi-Format Output**\n\nProvide output in various formats:\n\n#### Slack Format\n```\n*Daily Standup - @username*\n\n*Yesterday:*\n Merged PR #123: Add user authentication\n Fixed bug in payment processing (ENG-456)\n Reviewed 3 PRs\n\n*Today:*\n Starting ENG-457: Implement rate limiting\n Pairing with @teammate on database migration\n Sprint planning meeting at 2 PM\n\n*Blockers:*\n Waiting on API credentials from DevOps\n ENG-458 needs design clarification\n```\n\n#### Email Format\n```\nSubject: Daily Standup - [Name] - [Date]\n\nHi team,\n\nHere's my update for today's standup:\n\nCOMPLETED YESTERDAY:\n- [Detailed list with context]\n\nPLANNED FOR TODAY:\n- [Prioritized task list]\n\nBLOCKERS/HELP NEEDED:\n- [Clear description of impediments]\n\nLet me know if you have any questions.\n\nBest,\n[Name]\n```\n\n5. **Team Rollup View**\n\nFor team leads, generate consolidated view:\n\n```markdown\n# Team Standup Summary - [Date]\n\n## Velocity Metrics\n- Total Commits: [Count]\n- PRs Merged: [Count]\n- Tasks Completed: [Count]\n- Active Blockers: [Count]\n\n## Individual Updates\n[Summary for each team member]\n\n## Critical Items\n- Blockers requiring immediate attention\n- At-risk deliverables\n- Resource conflicts\n\n## Team Health Indicators\n- On-track tasks: [%]\n- Blocked tasks: [%]\n- Overdue items: [Count]\n```\n\n## Error Handling\n\n### No Linear Connection\n```\n\"Linear MCP server not connected. Generating report from git and GitHub data only.\n\nTo enable full functionality:\n1. Install Linear MCP: npm install -g @modelcontextprotocol/server-linear\n2. Configure with your API key\n3. Restart with Linear connected\n\nProceeding with available data...\"\n```\n\n### No Recent Activity\n```\n\"No git activity found in the last 24 hours. \n\nPossible reasons:\n1. No commits made (check your time range)\n2. Working on untracked branches\n3. Local changes not committed\n\nWould you like to:\n- Extend the time range?\n- Check specific branches?\n- Manually input your updates?\"\n```\n\n## Interactive Features\n\n1. **Update Customization**\n```\n\"I've generated your standup report. Would you like to:\n1. Add additional context to any item?\n2. Reorder priorities for today?\n3. Add missing blockers or concerns?\n4. Include work done outside of git?\"\n```\n\n2. **Blocker Resolution**\n```\n\"I notice you have blockers. Would you like help with:\n1. Drafting messages to unblock items?\n2. Finding alternative approaches?\n3. Identifying who can help?\"\n```\n\n## Best Practices\n\n1. **Run before standup**: Generate 15-30 minutes before meeting\n2. **Be specific**: Include task IDs and measurable progress\n3. **Highlight blockers early**: Don't wait until standup\n4. **Keep it concise**: Focus on key updates\n5. **Link to evidence**: Include commit/PR links\n\n## Advanced Features\n\n### Trend Analysis\n```\n\"Looking at your past week:\n- Average daily commits: [Number]\n- Task completion rate: [%]\n- Common blocker patterns: [List]\n\nSuggestions for improvement:\n[Personalized recommendations]\"\n```\n\n### Smart Scheduling\n```\n\"Based on your calendar and task estimates:\n- You have 5 hours of focused time today\n- Recommended task order: [Prioritized list]\n- Potential conflicts: [Meeting overlaps]\"\n```\n\n## Command Examples\n\n### Basic Usage\n```\nUser: \"Generate my standup report\"\nAssistant: [Generates standard report for last 24 hours]\n```\n\n### Custom Time Range\n```\nUser: \"Generate standup for last 2 days\"\nAssistant: [Generates report covering 48 hours]\n```\n\n### Team Report\n```\nUser: \"Generate team standup summary\"\nAssistant: [Generates consolidated team view]\n```\n\n### Specific Format\n```\nUser: \"Generate standup in Slack format\"\nAssistant: [Generates Slack-formatted message ready to paste]\n```"
              },
              {
                "name": "/team-workload-balancer",
                "description": "Balance team workload distribution",
                "path": "plugins/commands-team-collaboration/commands/team-workload-balancer.md",
                "frontmatter": {
                  "description": "Balance team workload distribution",
                  "category": "team-collaboration",
                  "allowed-tools": "Bash(git *), Bash(gh *)"
                },
                "content": "# team-workload-balancer\n\nBalance team workload distribution\n\n## Purpose\nThis command analyzes team members' current workloads, skills, past performance, and availability to suggest optimal task assignments. It helps prevent burnout, ensures balanced distribution, and matches tasks to team members' strengths.\n\n## Usage\n```bash\n# Show current team workload\nclaude \"Show workload balance for the engineering team\"\n\n# Suggest optimal assignment for new tasks\nclaude \"Who should work on the new payment integration task?\"\n\n# Rebalance current sprint\nclaude \"Rebalance tasks in the current sprint for optimal distribution\"\n\n# Capacity planning for next sprint\nclaude \"Plan task assignments for next sprint based on team capacity\"\n```\n\n## Instructions\n\n### 1. Gather Team Data\nCollect information about team members:\n\n```javascript\nclass TeamAnalyzer {\n  async gatherTeamData() {\n    const team = {};\n    \n    // Get team members from Linear\n    const teamMembers = await linear.getTeamMembers();\n    \n    for (const member of teamMembers) {\n      team[member.id] = {\n        name: member.name,\n        email: member.email,\n        currentTasks: [],\n        completedTasks: [],\n        skills: new Set(),\n        velocity: 0,\n        availability: 100, // percentage\n        preferences: {},\n        strengths: [],\n        timeZone: member.timeZone\n      };\n      \n      // Get current assignments\n      const activeTasks = await linear.getUserTasks(member.id, {\n        filter: { state: ['in_progress', 'todo'] }\n      });\n      team[member.id].currentTasks = activeTasks;\n      \n      // Get historical data\n      const completedTasks = await linear.getUserTasks(member.id, {\n        filter: { state: 'done' },\n        since: '3 months ago'\n      });\n      team[member.id].completedTasks = completedTasks;\n      \n      // Analyze git contributions\n      const gitStats = await this.analyzeGitContributions(member.email);\n      team[member.id].skills = gitStats.technologies;\n      team[member.id].codeContributions = gitStats.contributions;\n    }\n    \n    return team;\n  }\n  \n  async analyzeGitContributions(email) {\n    // Get commit history\n    const commits = await exec(`git log --author=\"${email}\" --since=\"6 months ago\" --pretty=format:\"%H\"`);\n    const commitHashes = commits.split('\\n').filter(Boolean);\n    \n    const stats = {\n      technologies: new Set(),\n      contributions: {\n        frontend: 0,\n        backend: 0,\n        database: 0,\n        devops: 0,\n        testing: 0,\n        documentation: 0\n      },\n      filesChanged: new Map()\n    };\n    \n    // Analyze each commit\n    for (const hash of commitHashes.slice(0, 100)) { // Limit to recent 100 commits\n      const files = await exec(`git show --name-only --pretty=format: ${hash}`);\n      const fileList = files.split('\\n').filter(Boolean);\n      \n      for (const file of fileList) {\n        // Track technologies\n        if (file.match(/\\.(js|jsx|ts|tsx)$/)) stats.technologies.add('JavaScript');\n        if (file.match(/\\.(py)$/)) stats.technologies.add('Python');\n        if (file.match(/\\.(java)$/)) stats.technologies.add('Java');\n        if (file.match(/\\.(go)$/)) stats.technologies.add('Go');\n        \n        // Categorize contributions\n        if (file.match(/\\/(components|views|pages|frontend)\\//)) stats.contributions.frontend++;\n        if (file.match(/\\/(api|server|backend|services)\\//)) stats.contributions.backend++;\n        if (file.match(/\\/(migrations|schemas|models)\\//)) stats.contributions.database++;\n        if (file.match(/\\/(deploy|docker|k8s|.github)\\//)) stats.contributions.devops++;\n        if (file.match(/\\.(test|spec)\\./)) stats.contributions.testing++;\n        if (file.match(/\\.(md|docs)\\//)) stats.contributions.documentation++;\n        \n        // Track file expertise\n        stats.filesChanged.set(file, (stats.filesChanged.get(file) || 0) + 1);\n      }\n    }\n    \n    return stats;\n  }\n}\n```\n\n### 2. Calculate Workload Metrics\nAnalyze current workload distribution:\n\n```javascript\nclass WorkloadCalculator {\n  calculateWorkload(teamMember) {\n    const metrics = {\n      currentPoints: 0,\n      currentTasks: teamMember.currentTasks.length,\n      inProgressPoints: 0,\n      todoPoints: 0,\n      blockedTasks: 0,\n      overdueTasksk: 0,\n      workloadScore: 0, // 0-100\n      capacity: 0\n    };\n    \n    // Sum story points\n    for (const task of teamMember.currentTasks) {\n      const points = task.estimate || 3; // Default to 3 if no estimate\n      metrics.currentPoints += points;\n      \n      if (task.state === 'in_progress') {\n        metrics.inProgressPoints += points;\n      } else if (task.state === 'todo') {\n        metrics.todoPoints += points;\n      }\n      \n      if (task.blockedBy?.length > 0) {\n        metrics.blockedTasks++;\n      }\n      \n      if (task.dueDate && new Date(task.dueDate) < new Date()) {\n        metrics.overdueTasksk++;\n      }\n    }\n    \n    // Calculate velocity from historical data\n    const velocity = this.calculateVelocity(teamMember.completedTasks);\n    \n    // Calculate workload score (0-100)\n    // Higher score = more overloaded\n    metrics.workloadScore = Math.min(100, (metrics.currentPoints / velocity.average) * 100);\n    \n    // Calculate remaining capacity\n    metrics.capacity = Math.max(0, velocity.average - metrics.currentPoints);\n    \n    // Adjust for blocked tasks\n    if (metrics.blockedTasks > 0) {\n      metrics.workloadScore *= 1.2; // Increase workload score for blocked work\n    }\n    \n    return metrics;\n  }\n  \n  calculateVelocity(completedTasks) {\n    // Group by sprint/week\n    const tasksByWeek = new Map();\n    \n    for (const task of completedTasks) {\n      const weekKey = this.getWeekKey(task.completedAt);\n      if (!tasksByWeek.has(weekKey)) {\n        tasksByWeek.set(weekKey, []);\n      }\n      tasksByWeek.get(weekKey).push(task);\n    }\n    \n    // Calculate points per week\n    const weeklyPoints = [];\n    for (const [week, tasks] of tasksByWeek) {\n      const points = tasks.reduce((sum, t) => sum + (t.estimate || 0), 0);\n      weeklyPoints.push(points);\n    }\n    \n    return {\n      average: weeklyPoints.reduce((a, b) => a + b, 0) / weeklyPoints.length || 10,\n      min: Math.min(...weeklyPoints) || 5,\n      max: Math.max(...weeklyPoints) || 15,\n      trend: this.calculateTrend(weeklyPoints)\n    };\n  }\n}\n```\n\n### 3. Skill Matching Algorithm\nMatch tasks to team members based on skills:\n\n```javascript\nclass SkillMatcher {\n  calculateSkillMatch(task, teamMember) {\n    const taskRequirements = this.extractTaskRequirements(task);\n    const memberSkills = this.consolidateSkills(teamMember);\n    \n    let matchScore = 0;\n    let maxScore = 0;\n    \n    // Technology match\n    for (const tech of taskRequirements.technologies) {\n      maxScore += 10;\n      if (memberSkills.technologies.has(tech)) {\n        matchScore += 10;\n      } else if (this.isRelatedTechnology(tech, memberSkills.technologies)) {\n        matchScore += 5;\n      }\n    }\n    \n    // Domain expertise match\n    if (taskRequirements.domain) {\n      maxScore += 20;\n      const domainExperience = this.getDomainExperience(teamMember, taskRequirements.domain);\n      matchScore += Math.min(20, domainExperience * 2);\n    }\n    \n    // Task type preference\n    maxScore += 10;\n    if (memberSkills.preferences[taskRequirements.type] > 0.7) {\n      matchScore += 10;\n    } else if (memberSkills.preferences[taskRequirements.type] > 0.4) {\n      matchScore += 5;\n    }\n    \n    // Recent similar work\n    const similarTasks = this.findSimilarCompletedTasks(teamMember, task);\n    if (similarTasks.length > 0) {\n      maxScore += 15;\n      matchScore += Math.min(15, similarTasks.length * 3);\n    }\n    \n    return {\n      score: maxScore > 0 ? (matchScore / maxScore) : 0,\n      matches: {\n        technologies: this.getTechMatches(taskRequirements, memberSkills),\n        domain: taskRequirements.domain && memberSkills.domains.includes(taskRequirements.domain),\n        experience: similarTasks.length\n      }\n    };\n  }\n  \n  extractTaskRequirements(task) {\n    const requirements = {\n      technologies: new Set(),\n      domain: null,\n      type: 'feature',\n      complexity: 'medium',\n      skills: []\n    };\n    \n    // Extract from title and description\n    const text = `${task.title} ${task.description}`.toLowerCase();\n    \n    // Technology detection\n    const techPatterns = {\n      'react': /react|jsx|component/,\n      'node': /node|express|npm/,\n      'python': /python|django|flask/,\n      'database': /sql|database|query|migration/,\n      'api': /api|rest|graphql|endpoint/,\n      'frontend': /ui|ux|css|style|layout/,\n      'backend': /server|backend|service/,\n      'devops': /deploy|docker|k8s|ci\\/cd/\n    };\n    \n    for (const [tech, pattern] of Object.entries(techPatterns)) {\n      if (pattern.test(text)) {\n        requirements.technologies.add(tech);\n      }\n    }\n    \n    // Domain detection\n    if (text.includes('auth') || text.includes('login')) requirements.domain = 'authentication';\n    if (text.includes('payment') || text.includes('billing')) requirements.domain = 'payments';\n    if (text.includes('user') || text.includes('profile')) requirements.domain = 'users';\n    \n    // Type detection\n    if (task.labels.some(l => l.name === 'bug')) requirements.type = 'bug';\n    if (task.labels.some(l => l.name === 'refactor')) requirements.type = 'refactor';\n    \n    return requirements;\n  }\n}\n```\n\n### 4. Load Balancing Algorithm\nDistribute tasks optimally:\n\n```javascript\nclass LoadBalancer {\n  balanceTasks(tasks, team, constraints = {}) {\n    const assignments = new Map(); // task -> assignee\n    const workloads = new Map(); // assignee -> current load\n    \n    // Initialize workloads\n    for (const [memberId, member] of Object.entries(team)) {\n      workloads.set(memberId, this.calculateWorkload(member));\n    }\n    \n    // Sort tasks by priority and size\n    const sortedTasks = tasks.sort((a, b) => {\n      const priorityDiff = (a.priority || 3) - (b.priority || 3);\n      if (priorityDiff !== 0) return priorityDiff;\n      return (b.estimate || 3) - (a.estimate || 3); // Larger tasks first\n    });\n    \n    // Assign tasks using modified bin packing algorithm\n    for (const task of sortedTasks) {\n      const candidates = this.findCandidates(task, team, workloads, constraints);\n      \n      if (candidates.length === 0) {\n        console.warn(`No suitable assignee found for task: ${task.title}`);\n        continue;\n      }\n      \n      // Select best candidate\n      const best = candidates.reduce((a, b) => \n        a.score > b.score ? a : b\n      );\n      \n      assignments.set(task.id, best.memberId);\n      \n      // Update workload\n      const currentLoad = workloads.get(best.memberId);\n      currentLoad.currentPoints += task.estimate || 3;\n      currentLoad.workloadScore = this.recalculateWorkloadScore(currentLoad);\n    }\n    \n    return {\n      assignments,\n      balance: this.calculateBalance(workloads),\n      warnings: this.generateWarnings(workloads, team)\n    };\n  }\n  \n  findCandidates(task, team, currentWorkloads, constraints) {\n    const candidates = [];\n    \n    for (const [memberId, member] of Object.entries(team)) {\n      const workload = currentWorkloads.get(memberId);\n      \n      // Check hard constraints\n      if (constraints.maxLoad && workload.currentPoints >= constraints.maxLoad) {\n        continue;\n      }\n      \n      if (constraints.requireSkill && !member.skills.has(constraints.requireSkill)) {\n        continue;\n      }\n      \n      // Calculate assignment score\n      const skillMatch = this.calculateSkillMatch(task, member);\n      const loadScore = 1 - (workload.workloadScore / 100); // Prefer less loaded\n      const velocityScore = member.velocity / 20; // Normalize velocity\n      \n      // Weighted score\n      const score = (\n        skillMatch.score * 0.4 +\n        loadScore * 0.4 +\n        velocityScore * 0.2\n      );\n      \n      candidates.push({\n        memberId,\n        memberName: member.name,\n        score,\n        factors: {\n          skill: skillMatch.score,\n          load: loadScore,\n          velocity: velocityScore\n        }\n      });\n    }\n    \n    return candidates.sort((a, b) => b.score - a.score);\n  }\n  \n  calculateBalance(workloads) {\n    const loads = Array.from(workloads.values()).map(w => w.currentPoints);\n    const avg = loads.reduce((a, b) => a + b, 0) / loads.length;\n    const variance = loads.reduce((sum, load) => sum + Math.pow(load - avg, 2), 0) / loads.length;\n    const stdDev = Math.sqrt(variance);\n    \n    return {\n      average: avg,\n      standardDeviation: stdDev,\n      balanceScore: 100 - Math.min(100, (stdDev / avg) * 100), // 0-100, higher is better\n      distribution: this.getDistribution(loads)\n    };\n  }\n}\n```\n\n### 5. Visualization Functions\nCreate visual representations of workload:\n\n```javascript\nfunction visualizeWorkload(team, assignments) {\n  const output = [];\n  \n  // Team workload bar chart\n  output.push('## Team Workload Distribution\\n');\n  \n  const maxPoints = Math.max(...Object.values(team).map(m => m.currentPoints));\n  \n  for (const [id, member] of Object.entries(team)) {\n    const points = member.currentPoints;\n    const capacity = member.velocity.average;\n    const utilization = (points / capacity) * 100;\n    \n    // Create visual bar\n    const barLength = Math.round((points / maxPoints) * 40);\n    const bar = ''.repeat(barLength) + ''.repeat(40 - barLength);\n    \n    // Color coding\n    let status = ''; // Green\n    if (utilization > 120) status = ''; // Red - overloaded\n    else if (utilization > 90) status = ''; // Yellow - near capacity\n    \n    output.push(`${status} ${member.name.padEnd(15)} ${bar} ${points}/${capacity} pts (${Math.round(utilization)}%)`);\n  }\n  \n  // Task distribution matrix\n  output.push('\\n## Recommended Task Assignments\\n');\n  output.push('| Task | Assignee | Skill Match | Load After | Reason |');\n  output.push('|------|----------|-------------|------------|---------|');\n  \n  for (const [taskId, assignment] of assignments) {\n    const task = findTask(taskId);\n    const member = team[assignment.memberId];\n    const newLoad = member.currentPoints + (task.estimate || 3);\n    const loadPercent = Math.round((newLoad / member.velocity.average) * 100);\n    \n    output.push(\n      `| ${task.title.substring(0, 30)}... | ${member.name} | ${Math.round(assignment.skillMatch * 100)}% | ${loadPercent}% | ${assignment.reason} |`\n    );\n  }\n  \n  return output.join('\\n');\n}\n\nfunction generateGanttChart(team, timeframe = 14) {\n  const chart = [];\n  const today = new Date();\n  \n  chart.push('## Sprint Timeline (Next 2 Weeks)\\n');\n  chart.push('```');\n  \n  // Header\n  const days = [];\n  for (let i = 0; i < timeframe; i++) {\n    const date = new Date(today);\n    date.setDate(date.getDate() + i);\n    days.push(date.toLocaleDateString('en', { weekday: 'short' })[0]);\n  }\n  chart.push('        ' + days.join(' '));\n  \n  // Team member rows\n  for (const [id, member] of Object.entries(team)) {\n    const tasks = member.currentTasks.sort((a, b) => \n      new Date(a.dueDate || '2099-01-01') - new Date(b.dueDate || '2099-01-01')\n    );\n    \n    let timeline = '';\n    let currentDay = 0;\n    \n    for (const task of tasks) {\n      const duration = task.estimate || 3;\n      const taskChar = task.priority === 1 ? '' : '';\n      timeline += ' '.repeat(Math.max(0, currentDay)) + taskChar.repeat(duration);\n      currentDay += duration;\n    }\n    \n    chart.push(`${member.name.padEnd(8)}${timeline.padEnd(timeframe, '')}`);\n  }\n  \n  chart.push('```');\n  return chart.join('\\n');\n}\n```\n\n### 6. Optimization Suggestions\nGenerate actionable recommendations:\n\n```javascript\nclass WorkloadOptimizer {\n  generateSuggestions(team, currentAssignments, constraints) {\n    const suggestions = [];\n    const metrics = this.analyzeCurrentState(team);\n    \n    // Check for overloaded members\n    for (const [id, member] of Object.entries(team)) {\n      if (member.workloadScore > 90) {\n        suggestions.push({\n          type: 'overload',\n          priority: 'high',\n          member: member.name,\n          action: `Redistribute ${member.currentPoints - member.velocity.average} points from ${member.name}`,\n          tasks: this.findTasksToReassign(member)\n        });\n      }\n    }\n    \n    // Check for underutilized members\n    for (const [id, member] of Object.entries(team)) {\n      if (member.workloadScore < 50 && member.availability > 80) {\n        suggestions.push({\n          type: 'underutilized',\n          priority: 'medium',\n          member: member.name,\n          action: `${member.name} has ${member.capacity} points available capacity`,\n          candidates: this.findTasksForMember(member, team)\n        });\n      }\n    }\n    \n    // Check for skill mismatches\n    const mismatches = this.findSkillMismatches(currentAssignments, team);\n    for (const mismatch of mismatches) {\n      suggestions.push({\n        type: 'skill_mismatch',\n        priority: 'medium',\n        action: `Consider reassigning \"${mismatch.task.title}\" from ${mismatch.current} to ${mismatch.suggested}`,\n        reason: mismatch.reason\n      });\n    }\n    \n    // Sprint risk analysis\n    const risks = this.analyzeSprintRisks(team);\n    for (const risk of risks) {\n      suggestions.push({\n        type: 'risk',\n        priority: risk.severity,\n        action: risk.mitigation,\n        impact: risk.impact\n      });\n    }\n    \n    return suggestions;\n  }\n  \n  findTasksToReassign(overloadedMember) {\n    // Find lowest priority tasks that can be reassigned\n    const tasks = overloadedMember.currentTasks\n      .filter(t => t.state === 'todo' && !t.blockedBy?.length)\n      .sort((a, b) => (b.priority || 3) - (a.priority || 3));\n    \n    const toReassign = [];\n    let pointsToRemove = overloadedMember.currentPoints - overloadedMember.velocity.average;\n    \n    for (const task of tasks) {\n      if (pointsToRemove <= 0) break;\n      toReassign.push(task);\n      pointsToRemove -= (task.estimate || 3);\n    }\n    \n    return toReassign;\n  }\n}\n```\n\n### 7. Error Handling\n```javascript\n// Handle missing Linear access\nif (!linear.available) {\n  console.error(\"Linear MCP tool not available\");\n  // Fall back to manual input or cached data\n}\n\n// Handle team member availability\nconst availability = {\n  async checkAvailability(member) {\n    // Check calendar integration if available\n    try {\n      const calendar = await getCalendarEvents(member.email);\n      const outOfOffice = calendar.filter(e => e.type === 'ooo');\n      return this.calculateAvailability(outOfOffice);\n    } catch (error) {\n      console.warn(`Could not check calendar for ${member.name}`);\n      return 100; // Assume full availability\n    }\n  }\n};\n\n// Handle incomplete data\nif (!task.estimate) {\n  console.warn(`Task \"${task.title}\" has no estimate, using default: 3 points`);\n  task.estimate = 3;\n}\n```\n\n## Example Output\n\n```\nAnalyzing team workload and generating recommendations...\n\n Team Overview\n\n\nCurrent Sprint: Sprint 23 (5 days remaining)\nTeam Size: 5 engineers\nTotal Capacity: 65 points\nCurrent Load: 71 points (109% capacity)\n\n Individual Workload\n\n\n Alice Chen       18/13 pts (138%)\n   In Progress: 2 tasks (8 pts) | Todo: 3 tasks (10 pts)\n    Overloaded by 5 points\n\n Bob Smith        14/15 pts (93%)\n   In Progress: 1 task (5 pts) | Todo: 3 tasks (9 pts)\n    Near optimal capacity\n\n Carol Davis      8/12 pts (67%)\n   In Progress: 1 task (3 pts) | Todo: 2 tasks (5 pts)\n    Has 4 points available capacity\n\n David Kim        7/10 pts (70%)\n   In Progress: 1 task (4 pts) | Todo: 1 task (3 pts)\n    Has 3 points available capacity\n\n Eve Johnson      17/15 pts (113%)\n   In Progress: 3 tasks (12 pts) | Todo: 2 tasks (5 pts)\n    Slightly overloaded\n\n Optimization Recommendations\n\n\n1.  HIGH PRIORITY: Redistribute Alice's workload\n   Action: Move 2 tasks (5 points) to other team members\n   Suggested reassignments:\n    \"API Rate Limiting\" (3 pts)  Carol (has backend expertise)\n    \"Update User Dashboard\" (2 pts)  David (worked on similar feature)\n\n2.  MEDIUM: Optimize skill matching\n    \"Payment Webhook Integration\" assigned to Eve\n     Better match: Bob (85% skill match vs 60%)\n     Bob has extensive webhook experience\n\n3.  MEDIUM: Balance in-progress items\n   Eve has 3 tasks in progress (risk of context switching)\n   Recommendation: Complete 1 before starting new work\n\n4.  LOW: Utilize available capacity\n   Carol and David have 7 points combined capacity\n   Suggested tasks from backlog:\n    \"Add Email Notifications\" (3 pts)  Carol\n    \"Optimize Search Query\" (2 pts)  David\n\n Proposed Rebalanced Distribution\n\n\nAfter rebalancing:\n Alice Chen       13/13 pts (100%)\n Bob Smith        14/15 pts (93%)\n Carol Davis      11/12 pts (92%)\n David Kim        9/10 pts (90%)\n Eve Johnson      12/15 pts (80%)\n\nBalance Score: 85/100 (Good)  94/100 (Excellent)\nRisk Level: High  Low\n\n Sprint Timeline\n\n\n        M T W T F M T W T F M T W T\nAlice   \nBob     \nCarol   \nDavid   \nEve     \n\nLegend:  High Priority |  Normal |  Available\n\n Quick Actions\n\n\n1. Run: claude \"Reassign task LIN-234 from Alice to Carol\"\n2. Run: claude \"Update sprint capacity to account for Eve's half day Friday\"\n3. Run: claude \"Create balanced task list for next sprint planning\"\n```\n\n## Advanced Features\n\n### Capacity Planning\n```bash\n# Plan next sprint with holidays and time off\nclaude \"Plan sprint 24 capacity - Alice off Monday, Bob at conference Wed-Thu\"\n```\n\n### Skill Development\n```bash\n# Identify learning opportunities\nclaude \"Suggest tasks for Carol to learn React based on current workload\"\n```\n\n### Team Performance\n```bash\n# Analyze team velocity trends\nclaude \"Show team velocity trends and predict sprint 24 capacity\"\n```\n\n## Tips\n- Update availability regularly (vacations, meetings)\n- Consider time zones for distributed teams\n- Track actual vs estimated to improve predictions\n- Use skill matching to grow team capabilities\n- Monitor workload balance weekly, not just at sprint start\n- Consider task dependencies in assignments\n- Factor in code review time for junior developers"
              }
            ],
            "skills": []
          },
          {
            "name": "commands-typescript-migration",
            "description": "Commands for migrating JavaScript projects to TypeScript",
            "source": "./plugins/commands-typescript-migration",
            "category": "commands",
            "version": "1.0.0",
            "author": {
              "name": "BuildWithClaude Community",
              "url": "https://github.com/davepoon/buildwithclaude"
            },
            "install_commands": [
              "/plugin marketplace add davepoon/buildwithclaude",
              "/plugin install commands-typescript-migration@buildwithclaude"
            ],
            "signals": {
              "stars": 2229,
              "forks": 239,
              "pushed_at": "2026-01-12T10:19:01Z",
              "created_at": "2025-07-25T02:26:45Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/migrate-to-typescript",
                "description": "Migrate JavaScript project to TypeScript",
                "path": "plugins/commands-typescript-migration/commands/migrate-to-typescript.md",
                "frontmatter": {
                  "description": "Migrate JavaScript project to TypeScript",
                  "category": "typescript-migration"
                },
                "content": "# Migrate to TypeScript\n\nMigrate JavaScript project to TypeScript\n\n## Instructions\n\n1. **Project Analysis and Migration Planning**\n   - Analyze current JavaScript codebase structure and complexity\n   - Identify external dependencies and their TypeScript support\n   - Assess project size and determine migration approach (gradual vs. complete)\n   - Review existing build system and bundling configuration\n   - Create migration timeline and phased approach plan\n\n2. **TypeScript Installation and Configuration**\n   - Install TypeScript and related dependencies (@types packages)\n   - Create comprehensive tsconfig.json with strict configuration\n   - Configure path mapping and module resolution\n   - Set up incremental compilation and build optimization\n   - Configure TypeScript for different environments (development, production, testing)\n\n3. **Build System Integration**\n   - Update build tools to support TypeScript compilation\n   - Configure webpack, Vite, or other bundlers for TypeScript\n   - Set up development server with TypeScript support\n   - Configure hot module replacement for TypeScript files\n   - Update build scripts and package.json configurations\n\n4. **File Migration Strategy**\n   - Start with configuration files and utility modules\n   - Migrate from least to most complex modules\n   - Rename .js files to .ts/.tsx incrementally\n   - Update import/export statements to use TypeScript syntax\n   - Handle mixed JavaScript/TypeScript codebase during transition\n\n5. **Type Definitions and Interfaces**\n   - Create comprehensive type definitions for project-specific types\n   - Install @types packages for external dependencies\n   - Define interfaces for API responses and data structures\n   - Create custom type declarations for untyped libraries\n   - Set up shared types and interfaces across modules\n\n6. **Code Transformation and Type Annotation**\n   - Add explicit type annotations to function parameters and return types\n   - Convert JavaScript classes to TypeScript with proper typing\n   - Transform object literals to typed interfaces\n   - Add generic types for reusable components and functions\n   - Handle complex types like union types, mapped types, and conditional types\n\n7. **Error Resolution and Type Safety**\n   - Resolve TypeScript compiler errors systematically\n   - Fix type mismatches and undefined behavior\n   - Handle null and undefined values with strict null checks\n   - Configure ESLint rules for TypeScript best practices\n   - Set up type checking in CI/CD pipeline\n\n8. **Testing and Validation**\n   - Update test files to TypeScript\n   - Configure testing framework for TypeScript support\n   - Add type testing with tools like tsd or @typescript-eslint\n   - Validate type safety in test suites\n   - Set up type coverage reporting\n\n9. **Developer Experience Enhancement**\n   - Configure IDE/editor for optimal TypeScript support\n   - Set up IntelliSense and auto-completion\n   - Configure debugging for TypeScript source maps\n   - Set up type-aware linting and formatting\n   - Create TypeScript-specific code snippets and templates\n\n10. **Documentation and Team Onboarding**\n    - Update project documentation for TypeScript setup\n    - Create TypeScript coding standards and best practices guide\n    - Document migration decisions and type system architecture\n    - Set up type documentation generation\n    - Train team members on TypeScript development workflows\n    - Create troubleshooting guide for common TypeScript issues"
              }
            ],
            "skills": []
          },
          {
            "name": "commands-utilities-debugging",
            "description": "General debugging and utility commands",
            "source": "./plugins/commands-utilities-debugging",
            "category": "commands",
            "version": "1.0.0",
            "author": {
              "name": "BuildWithClaude Community",
              "url": "https://github.com/davepoon/buildwithclaude"
            },
            "install_commands": [
              "/plugin marketplace add davepoon/buildwithclaude",
              "/plugin install commands-utilities-debugging@buildwithclaude"
            ],
            "signals": {
              "stars": 2229,
              "forks": 239,
              "pushed_at": "2026-01-12T10:19:01Z",
              "created_at": "2025-07-25T02:26:45Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/all-tools",
                "description": "Display all available development tools",
                "path": "plugins/commands-utilities-debugging/commands/all-tools.md",
                "frontmatter": {
                  "description": "Display all available development tools",
                  "category": "utilities-debugging"
                },
                "content": "# Display All Available Development Tools\n\nDisplay all available development tools\n\n*Command originally created by IndyDevDan (YouTube: https://www.youtube.com/@indydevdan) / DislerH (GitHub: https://github.com/disler)*\n\n## Instructions\n\nDisplay all available tools from your system prompt in the following format:\n\n1. **List each tool** with its TypeScript function signature\n2. **Include the purpose** of each tool as a suffix\n3. **Use double line breaks** between tools for readability\n4. **Format as bullet points** for clear organization\n\nThe output should help developers understand:\n- What tools are available in the current Claude Code session\n- The exact function signatures for reference\n- The primary purpose of each tool\n\nExample format:\n```typescript\n functionName(parameters: Type): ReturnType - Purpose of the tool\n\n anotherFunction(params: ParamType): ResultType - What this tool does\n```\n\nThis command is useful for:\n- Quick reference of available capabilities\n- Understanding tool signatures\n- Planning which tools to use for specific tasks"
              },
              {
                "name": "/architecture-scenario-explorer",
                "description": "Explore architectural decisions through systematic scenario analysis with trade-off evaluation and future-proofing assessment.",
                "path": "plugins/commands-utilities-debugging/commands/architecture-scenario-explorer.md",
                "frontmatter": {
                  "description": "Explore architectural decisions through systematic scenario analysis with trade-off evaluation and future-proofing assessment.",
                  "category": "utilities-debugging",
                  "argument-hint": "Specify architecture scenario options",
                  "allowed-tools": "Glob"
                },
                "content": "# Architecture Scenario Explorer\n\nExplore architectural decisions through systematic scenario analysis with trade-off evaluation and future-proofing assessment.\n\n## Instructions\n\nYou are tasked with systematically exploring architectural decisions through comprehensive scenario modeling to optimize system design choices. Follow this approach: **$ARGUMENTS**\n\n### 1. Prerequisites Assessment\n\n**Critical Architecture Context Validation:**\n\n- **System Scope**: What system or component architecture are you designing?\n- **Scale Requirements**: What are the expected usage patterns and growth projections?\n- **Constraints**: What technical, business, or resource constraints apply?\n- **Timeline**: What is the implementation timeline and evolution roadmap?\n- **Success Criteria**: How will you measure architectural success?\n\n**If context is unclear, guide systematically:**\n\n```\nMissing System Scope:\n\"What specific system architecture needs exploration?\n- New System Design: Greenfield application or service architecture\n- System Migration: Moving from legacy to modern architecture\n- Scaling Architecture: Expanding existing system capabilities\n- Integration Architecture: Connecting multiple systems and services\n- Platform Architecture: Building foundational infrastructure\n\nPlease specify the system boundaries, key components, and primary functions.\"\n\nMissing Scale Requirements:\n\"What are the expected system scale and usage patterns?\n- User Scale: Number of concurrent and total users\n- Data Scale: Volume, velocity, and variety of data processed\n- Transaction Scale: Requests per second, peak load patterns\n- Geographic Scale: Single region, multi-region, or global distribution\n- Growth Projections: Expected scaling timeline and magnitude\"\n```\n\n### 2. Architecture Option Generation\n\n**Systematically identify architectural approaches:**\n\n#### Architecture Pattern Matrix\n```\nArchitectural Approach Framework:\n\nMonolithic Patterns:\n- Layered Architecture: Traditional n-tier with clear separation\n- Modular Monolith: Well-bounded modules within single deployment\n- Plugin Architecture: Core system with extensible plugin ecosystem\n- Service-Oriented Monolith: Internal service boundaries with single deployment\n\nDistributed Patterns:\n- Microservices: Independent services with business capability alignment\n- Service Mesh: Microservices with infrastructure-level communication\n- Event-Driven: Asynchronous communication with event sourcing\n- CQRS/Event Sourcing: Command-query separation with event storage\n\nHybrid Patterns:\n- Modular Microservices: Services grouped by business domain\n- Micro-Frontend: Frontend decomposition matching backend services\n- Strangler Fig: Gradual migration from monolith to distributed\n- API Gateway: Centralized entry point with backend service routing\n\nCloud-Native Patterns:\n- Serverless: Function-based with cloud provider infrastructure\n- Container-Native: Kubernetes-first with cloud-native services\n- Multi-Cloud: Cloud-agnostic with portable infrastructure\n- Edge-First: Distributed computing with edge location optimization\n```\n\n#### Architecture Variation Specification\n```\nFor each architectural option:\n\nStructural Characteristics:\n- Component Organization: [how system parts are structured and related]\n- Communication Patterns: [synchronous vs asynchronous, protocols, messaging]\n- Data Management: [database strategy, consistency model, storage patterns]\n- Deployment Model: [packaging, distribution, scaling, and operational approach]\n\nQuality Attributes:\n- Scalability Profile: [horizontal vs vertical scaling, bottleneck analysis]\n- Reliability Characteristics: [failure modes, recovery, fault tolerance]\n- Performance Expectations: [latency, throughput, resource efficiency]\n- Security Model: [authentication, authorization, data protection, attack surface]\n\nImplementation Considerations:\n- Technology Stack: [languages, frameworks, databases, infrastructure]\n- Team Structure Fit: [Conway's Law implications, team capabilities]\n- Development Process: [build, test, deploy, monitor workflows]\n- Evolution Strategy: [how architecture can grow and change over time]\n```\n\n### 3. Scenario Framework Development\n\n**Create comprehensive architectural testing scenarios:**\n\n#### Usage Scenario Matrix\n```\nMulti-Dimensional Scenario Framework:\n\nLoad Scenarios:\n- Normal Operation: Typical daily usage patterns and traffic\n- Peak Load: Maximum expected concurrent usage and transaction volume\n- Stress Testing: Beyond normal capacity to identify breaking points\n- Spike Testing: Sudden traffic increases and burst handling\n\nGrowth Scenarios:\n- Linear Growth: Steady user and data volume increases over time\n- Exponential Growth: Rapid scaling requirements and viral adoption\n- Geographic Expansion: Multi-region deployment and global scaling\n- Feature Expansion: New capabilities and service additions\n\nFailure Scenarios:\n- Component Failures: Individual service or database outages\n- Infrastructure Failures: Network, storage, or compute disruptions\n- Cascade Failures: Failure propagation and system-wide impacts\n- Disaster Recovery: Major outage recovery and business continuity\n\nEvolution Scenarios:\n- Technology Migration: Framework, language, or platform changes\n- Business Model Changes: New revenue streams or service offerings\n- Regulatory Changes: Compliance requirements and data protection\n- Competitive Response: Market pressures and feature requirements\n```\n\n#### Scenario Impact Modeling\n- Performance impact under each scenario type\n- Cost implications for infrastructure and operations\n- Development velocity and team productivity effects\n- Risk assessment and mitigation requirements\n\n### 4. Trade-off Analysis Framework\n\n**Systematic evaluation of architectural trade-offs:**\n\n#### Quality Attribute Trade-off Matrix\n```\nArchitecture Quality Assessment:\n\nPerformance Trade-offs:\n- Latency vs Throughput: Response time vs maximum concurrent processing\n- Memory vs CPU: Resource utilization optimization strategies\n- Consistency vs Availability: CAP theorem implications and choices\n- Caching vs Freshness: Data staleness vs response speed\n\nScalability Trade-offs:\n- Horizontal vs Vertical: Infrastructure scaling approach and economics\n- Stateless vs Stateful: Session management and performance implications\n- Synchronous vs Asynchronous: Communication complexity vs performance\n- Coupling vs Autonomy: Service independence vs operational overhead\n\nDevelopment Trade-offs:\n- Development Speed vs Runtime Performance: Optimization time investment\n- Type Safety vs Flexibility: Compile-time vs runtime error handling\n- Code Reuse vs Service Independence: Shared libraries vs duplication\n- Testing Complexity vs System Reliability: Test investment vs quality\n\nOperational Trade-offs:\n- Complexity vs Control: Managed services vs self-managed infrastructure\n- Monitoring vs Privacy: Observability vs data protection\n- Automation vs Flexibility: Standardization vs customization\n- Cost vs Performance: Infrastructure spending vs response times\n```\n\n#### Decision Matrix Construction\n- Weight assignment for different quality attributes based on business priorities\n- Scoring methodology for each architecture option across quality dimensions\n- Sensitivity analysis for weight and score variations\n- Pareto frontier identification for non-dominated solutions\n\n### 5. Future-Proofing Assessment\n\n**Evaluate architectural adaptability and evolution potential:**\n\n#### Technology Evolution Scenarios\n```\nFuture-Proofing Analysis Framework:\n\nTechnology Trend Integration:\n- AI/ML Integration: Machine learning capability embedding and scaling\n- Edge Computing: Distributed processing and low-latency requirements\n- Quantum Computing: Post-quantum cryptography and computational impacts\n- Blockchain/DLT: Distributed ledger integration and trust mechanisms\n\nMarket Evolution Preparation:\n- Business Model Flexibility: Subscription, marketplace, platform pivots\n- Global Expansion: Multi-tenant, multi-region, multi-regulatory compliance\n- Customer Expectation Evolution: Real-time, personalized, omnichannel experiences\n- Competitive Landscape Changes: Feature parity and differentiation requirements\n\nRegulatory Future-Proofing:\n- Privacy Regulation: GDPR, CCPA evolution and global privacy requirements\n- Security Standards: Zero-trust, compliance framework evolution\n- Data Sovereignty: Geographic data residency and cross-border restrictions\n- Accessibility Requirements: Inclusive design and assistive technology support\n```\n\n#### Adaptability Scoring\n- Architecture flexibility for requirement changes\n- Technology migration feasibility and cost\n- Team skill evolution and learning curve management\n- Investment protection and technical debt management\n\n### 6. Architecture Simulation Engine\n\n**Model architectural behavior under different scenarios:**\n\n#### Performance Simulation Framework\n```\nMulti-Layer Architecture Simulation:\n\nComponent-Level Simulation:\n- Individual service performance characteristics and resource usage\n- Database query performance and optimization opportunities\n- Cache hit ratios and invalidation strategies\n- Message queue throughput and latency patterns\n\nIntegration-Level Simulation:\n- Service-to-service communication overhead and optimization\n- API gateway performance and routing efficiency\n- Load balancer distribution and health checking\n- Circuit breaker and retry mechanism effectiveness\n\nSystem-Level Simulation:\n- End-to-end request flow and user experience\n- Peak load distribution and resource allocation\n- Failure propagation and recovery patterns\n- Monitoring and alerting system effectiveness\n\nInfrastructure-Level Simulation:\n- Cloud resource utilization and auto-scaling behavior\n- Network bandwidth and latency optimization\n- Storage performance and data consistency patterns\n- Security policy enforcement and performance impact\n```\n\n#### Cost Modeling Integration\n- Infrastructure cost estimation across different scenarios\n- Development and operational cost projection\n- Total cost of ownership analysis over multi-year timeline\n- Cost optimization opportunities and trade-off analysis\n\n### 7. Risk Assessment and Mitigation\n\n**Comprehensive architectural risk evaluation:**\n\n#### Technical Risk Framework\n```\nArchitecture Risk Assessment:\n\nImplementation Risks:\n- Technology Maturity: New vs proven technology adoption risks\n- Complexity Management: System comprehension and debugging challenges\n- Integration Challenges: Third-party service dependencies and compatibility\n- Performance Uncertainty: Untested scaling and optimization requirements\n\nOperational Risks:\n- Deployment Complexity: Release management and rollback capabilities\n- Monitoring Gaps: Observability and troubleshooting limitations\n- Scaling Challenges: Auto-scaling reliability and cost control\n- Disaster Recovery: Backup, recovery, and business continuity planning\n\nStrategic Risks:\n- Technology Lock-in: Vendor dependency and migration flexibility\n- Skill Dependencies: Team expertise requirements and knowledge gaps\n- Evolution Constraints: Architecture modification and extension limitations\n- Competitive Disadvantage: Time-to-market and feature development speed\n```\n\n#### Risk Mitigation Strategy Development\n- Specific mitigation approaches for identified risks\n- Contingency planning and alternative architecture options\n- Early warning indicators and monitoring strategies\n- Risk acceptance criteria and stakeholder communication\n\n### 8. Decision Framework and Recommendations\n\n**Generate systematic architectural guidance:**\n\n#### Architecture Decision Record (ADR) Format\n```\n## Architecture Decision: [System Name] - [Decision Topic]\n\n### Context and Problem Statement\n- Business Requirements: [key functional and non-functional requirements]\n- Current Constraints: [technical, resource, and timeline limitations]\n- Decision Drivers: [factors influencing architectural choice]\n\n### Architecture Options Considered\n\n#### Option 1: [Architecture Name]\n- Description: [architectural approach and key characteristics]\n- Pros: [advantages and benefits]\n- Cons: [disadvantages and risks]\n- Trade-offs: [specific quality attribute impacts]\n\n[Repeat for each option]\n\n### Decision Outcome\n- Selected Architecture: [chosen approach with rationale]\n- Decision Rationale: [why this option was selected]\n- Expected Benefits: [anticipated advantages and success metrics]\n- Accepted Trade-offs: [compromises and mitigation strategies]\n\n### Implementation Strategy\n- Phase 1 (Immediate): [initial implementation steps and validation]\n- Phase 2 (Short-term): [core system development and integration]\n- Phase 3 (Medium-term): [optimization and scaling implementation]\n- Phase 4 (Long-term): [evolution and enhancement roadmap]\n\n### Validation and Success Criteria\n- Performance Metrics: [specific KPIs and acceptable ranges]\n- Quality Gates: [architectural compliance and validation checkpoints]\n- Review Schedule: [when to reassess architectural decisions]\n- Adaptation Triggers: [conditions requiring architectural modification]\n\n### Risks and Mitigation\n- High-Priority Risks: [most significant concerns and responses]\n- Monitoring Strategy: [early warning systems and health checks]\n- Contingency Plans: [alternative approaches if problems arise]\n- Learning and Adaptation: [how to incorporate feedback and improve]\n```\n\n### 9. Continuous Architecture Evolution\n\n**Establish ongoing architectural assessment and improvement:**\n\n#### Architecture Health Monitoring\n- Performance metric tracking against architectural predictions\n- Technical debt accumulation and remediation planning\n- Team productivity and development velocity measurement\n- User satisfaction and business outcome correlation\n\n#### Evolutionary Architecture Practices\n- Regular architecture review and fitness function evaluation\n- Incremental improvement identification and implementation\n- Technology trend assessment and adoption planning\n- Cross-team architecture knowledge sharing and standardization\n\n## Usage Examples\n\n```bash\n# Microservices migration planning\n/dev:architecture-scenario-explorer Evaluate monolith to microservices migration for e-commerce platform with 1M+ users\n\n# New system architecture design\n/dev:architecture-scenario-explorer Design architecture for real-time analytics platform handling 100k events/second\n\n# Scaling architecture assessment\n/dev:architecture-scenario-explorer Analyze architecture options for scaling social media platform from 10k to 1M daily active users\n\n# Technology modernization planning\n/dev:architecture-scenario-explorer Compare serverless vs container-native architectures for data processing pipeline modernization\n```\n\n## Quality Indicators\n\n- **Green**: Multiple architectures analyzed, comprehensive scenarios tested, validated trade-offs\n- **Yellow**: Some architectural options considered, basic scenario coverage, estimated trade-offs\n- **Red**: Single architecture focus, limited scenario analysis, unvalidated assumptions\n\n## Common Pitfalls to Avoid\n\n- Architecture astronauting: Over-engineering for theoretical rather than real requirements\n- Cargo cult architecture: Copying successful patterns without understanding context\n- Technology bias: Choosing architecture based on technology preferences rather than requirements\n- Premature optimization: Solving performance problems that don't exist yet\n- Scalability obsession: Over-optimizing for scale that may never materialize\n- Evolution blindness: Not planning for architectural change and growth\n\nTransform architectural decisions from opinion-based debates into systematic, evidence-driven choices through comprehensive scenario exploration and trade-off analysis."
              },
              {
                "name": "/check-file",
                "description": "Perform comprehensive analysis of $ARGUMENTS to identify code quality issues, security vulnerabilities, and optimization opportunities.",
                "path": "plugins/commands-utilities-debugging/commands/check-file.md",
                "frontmatter": {
                  "description": "Perform comprehensive analysis of $ARGUMENTS to identify code quality issues, security vulnerabilities, and optimization opportunities.",
                  "category": "utilities-debugging",
                  "argument-hint": "Specify file path to check",
                  "allowed-tools": "Read"
                },
                "content": "# File Analysis Tool\n\nPerform comprehensive analysis of $ARGUMENTS to identify code quality issues, security vulnerabilities, and optimization opportunities.\n\n## Task\n\nI'll analyze the specified file and provide detailed insights on:\n\n1. Code quality metrics and maintainability\n2. Security vulnerabilities and best practices\n3. Performance bottlenecks and optimization opportunities\n4. Dependency usage and potential issues\n5. TypeScript/JavaScript specific patterns and improvements\n6. Test coverage and missing tests\n\n## Process\n\nI'll follow these steps:\n\n1. Read and parse the target file\n2. Analyze code structure and complexity\n3. Check for security vulnerabilities and anti-patterns  \n4. Evaluate performance implications\n5. Review dependency usage and imports\n6. Provide actionable recommendations for improvement\n\n## Analysis Areas\n\n### Code Quality\n- Cyclomatic complexity and maintainability metrics\n- Code duplication and refactoring opportunities\n- Naming conventions and code organization\n- TypeScript type safety and best practices\n\n### Security Assessment\n- Input validation and sanitization\n- Authentication and authorization patterns\n- Sensitive data exposure risks\n- Common vulnerability patterns (XSS, injection, etc.)\n\n### Performance Review\n- Bundle size impact and optimization opportunities\n- Runtime performance bottlenecks\n- Memory usage patterns\n- Lazy loading and code splitting opportunities\n\n### Best Practices\n- Framework-specific patterns (React, Vue, Angular)\n- Modern JavaScript/TypeScript features usage\n- Error handling and logging practices\n- Testing patterns and coverage gaps\n\nI'll provide specific, actionable recommendations tailored to your project's technology stack and architecture."
              },
              {
                "name": "/clean-branches",
                "description": "Clean up merged and stale git branches",
                "path": "plugins/commands-utilities-debugging/commands/clean-branches.md",
                "frontmatter": {
                  "description": "Clean up merged and stale git branches",
                  "category": "utilities-debugging",
                  "argument-hint": "1. **Repository State Analysis**",
                  "allowed-tools": "Bash(git *)"
                },
                "content": "# Clean Branches Command\n\nClean up merged and stale git branches\n\n## Instructions\n\nFollow this systematic approach to clean up git branches: **$ARGUMENTS**\n\n1. **Repository State Analysis**\n   - Check current branch and uncommitted changes\n   - List all local and remote branches\n   - Identify the main/master branch name\n   - Review recent branch activity and merge history\n\n   ```bash\n   # Check current status\n   git status\n   git branch -a\n   git remote -v\n   \n   # Check main branch name\n   git symbolic-ref refs/remotes/origin/HEAD | sed 's@^refs/remotes/origin/@@'\n   ```\n\n2. **Safety Precautions**\n   - Ensure working directory is clean\n   - Switch to main/master branch\n   - Pull latest changes from remote\n   - Create backup of current branch state if needed\n\n   ```bash\n   # Ensure clean state\n   git stash push -m \"Backup before branch cleanup\"\n   git checkout main  # or master\n   git pull origin main\n   ```\n\n3. **Identify Merged Branches**\n   - List branches that have been merged into main\n   - Exclude protected branches (main, master, develop)\n   - Check both local and remote merged branches\n   - Verify merge status to avoid accidental deletion\n\n   ```bash\n   # List merged local branches\n   git branch --merged main | grep -v \"main\\\\|master\\\\|develop\\\\|\\\\*\"\n   \n   # List merged remote branches\n   git branch -r --merged main | grep -v \"main\\\\|master\\\\|develop\\\\|HEAD\"\n   ```\n\n4. **Identify Stale Branches**\n   - Find branches with no recent activity\n   - Check last commit date for each branch\n   - Identify branches older than specified timeframe (e.g., 30 days)\n   - Consider branch naming patterns for feature/hotfix branches\n\n   ```bash\n   # List branches by last commit date\n   git for-each-ref --format='%(committerdate) %(authorname) %(refname)' --sort=committerdate refs/heads\n   \n   # Find branches older than 30 days\n   git for-each-ref --format='%(refname:short) %(committerdate)' refs/heads | awk '$2 < \"'$(date -d '30 days ago' '+%Y-%m-%d')'\"'\n   ```\n\n5. **Interactive Branch Review**\n   - Review each branch before deletion\n   - Check if branch has unmerged changes\n   - Verify branch purpose and status\n   - Ask for confirmation before deletion\n\n   ```bash\n   # Check for unmerged changes\n   git log main..branch-name --oneline\n   \n   # Show branch information\n   git show-branch branch-name main\n   ```\n\n6. **Protected Branch Configuration**\n   - Identify branches that should never be deleted\n   - Configure protection rules for important branches\n   - Document branch protection policies\n   - Set up automated protection for new repositories\n\n   ```bash\n   # Example protected branches\n   PROTECTED_BRANCHES=(\"main\" \"master\" \"develop\" \"staging\" \"production\")\n   ```\n\n7. **Local Branch Cleanup**\n   - Delete merged local branches safely\n   - Remove stale feature branches\n   - Clean up tracking branches for deleted remotes\n   - Update local branch references\n\n   ```bash\n   # Delete merged branches (interactive)\n   git branch --merged main | grep -v \"main\\\\|master\\\\|develop\\\\|\\\\*\" | xargs -n 1 -p git branch -d\n   \n   # Force delete if needed (use with caution)\n   git branch -D branch-name\n   ```\n\n8. **Remote Branch Cleanup**\n   - Remove merged remote branches\n   - Clean up remote tracking references\n   - Delete obsolete remote branches\n   - Update remote branch information\n\n   ```bash\n   # Prune remote tracking branches\n   git remote prune origin\n   \n   # Delete remote branch\n   git push origin --delete branch-name\n   \n   # Remove local tracking of deleted remote branches\n   git branch -dr origin/branch-name\n   ```\n\n9. **Automated Cleanup Script**\n   \n   ```bash\n   #!/bin/bash\n   \n   # Git branch cleanup script\n   set -e\n   \n   # Configuration\n   MAIN_BRANCH=\"main\"\n   PROTECTED_BRANCHES=(\"main\" \"master\" \"develop\" \"staging\" \"production\")\n   STALE_DAYS=30\n   \n   # Functions\n   is_protected() {\n       local branch=$1\n       for protected in \"${PROTECTED_BRANCHES[@]}\"; do\n           if [[ \"$branch\" == \"$protected\" ]]; then\n               return 0\n           fi\n       done\n       return 1\n   }\n   \n   # Switch to main branch\n   git checkout $MAIN_BRANCH\n   git pull origin $MAIN_BRANCH\n   \n   # Clean up merged branches\n   echo \"Cleaning up merged branches...\"\n   merged_branches=$(git branch --merged $MAIN_BRANCH | grep -v \"\\\\*\\\\|$MAIN_BRANCH\")\n   \n   for branch in $merged_branches; do\n       if ! is_protected \"$branch\"; then\n           echo \"Deleting merged branch: $branch\"\n           git branch -d \"$branch\"\n       fi\n   done\n   \n   # Prune remote tracking branches\n   echo \"Pruning remote tracking branches...\"\n   git remote prune origin\n   \n   echo \"Branch cleanup completed!\"\n   ```\n\n10. **Team Coordination**\n    - Notify team before cleaning shared branches\n    - Check if branches are being used by others\n    - Coordinate branch cleanup schedules\n    - Document branch cleanup procedures\n\n11. **Branch Naming Convention Cleanup**\n    - Identify branches with non-standard naming\n    - Clean up temporary or experimental branches\n    - Remove old hotfix and feature branches\n    - Enforce consistent naming conventions\n\n12. **Verification and Validation**\n    - Verify important branches are still present\n    - Check that no active work was deleted\n    - Validate remote branch synchronization\n    - Confirm team members have no issues\n\n    ```bash\n    # Verify cleanup results\n    git branch -a\n    git remote show origin\n    ```\n\n13. **Documentation and Reporting**\n    - Document what branches were cleaned up\n    - Report any issues or conflicts found\n    - Update team documentation about branch lifecycle\n    - Create branch cleanup schedule and policies\n\n14. **Rollback Procedures**\n    - Document how to recover deleted branches\n    - Use reflog to find deleted branch commits\n    - Create emergency recovery procedures\n    - Set up branch restoration scripts\n\n    ```bash\n    # Recover deleted branch using reflog\n    git reflog --no-merges --since=\"2 weeks ago\"\n    git checkout -b recovered-branch commit-hash\n    ```\n\n15. **Automation Setup**\n    - Set up automated branch cleanup scripts\n    - Configure CI/CD pipeline for branch cleanup\n    - Create scheduled cleanup jobs\n    - Implement branch lifecycle policies\n\n16. **Best Practices Implementation**\n    - Establish branch lifecycle guidelines\n    - Set up automated merge detection\n    - Configure branch protection rules\n    - Implement code review requirements\n\n**Advanced Cleanup Options:**\n\n```bash\n# Clean up all merged branches except protected ones\ngit branch --merged main | grep -E \"^  (feature|hotfix|bugfix)/\" | xargs -n 1 git branch -d\n\n# Interactive cleanup with confirmation\ngit branch --merged main | grep -v \"main\\|master\\|develop\" | xargs -n 1 -p git branch -d\n\n# Batch delete remote branches\ngit branch -r --merged main | grep origin | grep -v \"main\\|master\\|develop\\|HEAD\" | cut -d/ -f2- | xargs -n 1 git push origin --delete\n\n# Clean up branches older than specific date\ngit for-each-ref --format='%(refname:short) %(committerdate:short)' refs/heads | awk '$2 < \"2023-01-01\"' | cut -d' ' -f1 | xargs -n 1 git branch -D\n```\n\nRemember to:\n- Always backup important branches before cleanup\n- Coordinate with team members before deleting shared branches\n- Test cleanup scripts in a safe environment first\n- Document all cleanup procedures and policies\n- Set up regular cleanup schedules to prevent accumulation"
              },
              {
                "name": "/code-permutation-tester",
                "description": "Test multiple code variations through simulation before implementation with quality gates and performance prediction.",
                "path": "plugins/commands-utilities-debugging/commands/code-permutation-tester.md",
                "frontmatter": {
                  "description": "Test multiple code variations through simulation before implementation with quality gates and performance prediction.",
                  "category": "utilities-debugging",
                  "argument-hint": "Specify permutation test options"
                },
                "content": "# Code Permutation Tester\n\nTest multiple code variations through simulation before implementation with quality gates and performance prediction.\n\n## Instructions\n\nYou are tasked with systematically testing multiple code implementation approaches through simulation to optimize decisions before actual development. Follow this approach: **$ARGUMENTS**\n\n### 1. Prerequisites Assessment\n\n**Critical Code Context Validation:**\n\n- **Code Scope**: What specific code area/function/feature are you testing variations for?\n- **Variation Types**: What different approaches are you considering?\n- **Quality Criteria**: How will you evaluate which variation is best?\n- **Constraints**: What technical, performance, or resource constraints apply?\n- **Decision Timeline**: When do you need to choose an implementation approach?\n\n**If context is unclear, guide systematically:**\n\n```\nMissing Code Scope:\n\"What specific code area needs permutation testing?\n- Algorithm Implementation: Different algorithmic approaches for the same problem\n- Architecture Pattern: Various structural patterns (MVC, microservices, etc.)\n- Performance Optimization: Multiple optimization strategies for bottlenecks\n- API Design: Different interface design approaches\n- Data Structure Choice: Various data organization strategies\n\nPlease specify the exact function, module, or system component.\"\n\nMissing Variation Types:\n\"What different implementation approaches are you considering?\n- Algorithmic Variations: Different algorithms solving the same problem\n- Framework/Library Choices: Various tech stack options\n- Design Pattern Applications: Different structural and behavioral patterns\n- Performance Trade-offs: Speed vs. memory vs. maintainability variations\n- Integration Approaches: Different ways to connect with existing systems\"\n```\n\n### 2. Code Variation Generation\n\n**Systematically identify and structure implementation alternatives:**\n\n#### Implementation Approach Matrix\n```\nCode Variation Framework:\n\nAlgorithmic Variations:\n- Brute Force: Simple, readable implementation\n- Optimized: Performance-focused with complexity trade-offs\n- Hybrid: Balanced approach with configurable optimization\n- Novel: Innovative approaches using new techniques\n\nArchitectural Variations:\n- Monolithic: Single deployment unit with tight coupling\n- Modular: Loosely coupled modules within single codebase\n- Microservices: Distributed services with independent deployment\n- Serverless: Function-based with cloud provider management\n\nTechnology Stack Variations:\n- Traditional: Established, well-documented technologies\n- Modern: Current best practices and recent frameworks\n- Cutting-edge: Latest technologies with higher risk/reward\n- Hybrid: Mix of established and modern approaches\n\nPerformance Profile Variations:\n- Memory-optimized: Minimal memory footprint\n- Speed-optimized: Maximum execution performance  \n- Scalability-optimized: Handles growth efficiently\n- Maintainability-optimized: Easy to modify and extend\n```\n\n#### Variation Specification Framework\n```\nFor each code variation:\n\nImplementation Details:\n- Core Algorithm/Approach: [specific technical approach]\n- Key Dependencies: [frameworks, libraries, external services]\n- Architecture Pattern: [structural organization approach]\n- Data Flow Design: [how information moves through system]\n\nQuality Characteristics:\n- Performance Profile: [speed, memory, throughput expectations]\n- Maintainability Score: [ease of modification and extension]\n- Scalability Potential: [growth and load handling capability]\n- Reliability Assessment: [error handling and fault tolerance]\n\nResource Requirements:\n- Development Time: [estimated implementation effort]\n- Team Skill Requirements: [expertise needed for implementation]\n- Infrastructure Needs: [deployment and operational requirements]\n- Ongoing Maintenance: [long-term support and evolution needs]\n```\n\n### 3. Simulation Framework Design\n\n**Create testing environment for code variations:**\n\n#### Code Simulation Methodology\n```\nMulti-Dimensional Testing Approach:\n\nPerformance Simulation:\n- Synthetic workload generation and stress testing\n- Memory usage profiling and leak detection\n- Concurrent execution and race condition testing\n- Resource utilization monitoring and optimization\n\nMaintainability Simulation:\n- Code complexity analysis and metrics calculation\n- Change impact simulation and ripple effect analysis\n- Documentation quality and developer onboarding simulation\n- Debugging and troubleshooting ease assessment\n\nScalability Simulation:\n- Load growth simulation and performance degradation analysis\n- Horizontal scaling simulation and resource efficiency\n- Data volume growth impact and query performance\n- Integration point stress testing and failure handling\n\nSecurity Simulation:\n- Attack vector simulation and vulnerability assessment\n- Data protection and privacy compliance testing\n- Authentication and authorization load testing\n- Input validation and sanitization effectiveness\n```\n\n#### Testing Environment Setup\n- Isolated testing environments for each variation\n- Consistent data sets and test scenarios across variations\n- Automated testing pipeline and result collection\n- Realistic production environment simulation\n\n### 4. Quality Gate Framework\n\n**Establish systematic evaluation criteria:**\n\n#### Multi-Criteria Evaluation Matrix\n```\nCode Quality Assessment Framework:\n\nPerformance Gates (25% weight):\n- Response Time: [acceptable latency thresholds]\n- Throughput: [minimum requests/transactions per second]\n- Resource Usage: [memory, CPU, storage efficiency]\n- Scalability: [performance degradation under load]\n\nMaintainability Gates (25% weight):\n- Code Complexity: [cyclomatic complexity, nesting levels]\n- Test Coverage: [unit, integration, end-to-end test coverage]\n- Documentation Quality: [code comments, API docs, architecture docs]\n- Change Impact: [blast radius of typical modifications]\n\nReliability Gates (25% weight):\n- Error Handling: [graceful failure and recovery mechanisms]\n- Fault Tolerance: [system behavior under adverse conditions]\n- Data Integrity: [consistency and corruption prevention]\n- Monitoring/Observability: [debugging and operational visibility]\n\nBusiness Gates (25% weight):\n- Time to Market: [development speed and delivery timeline]\n- Total Cost of Ownership: [development + operational costs]\n- Risk Assessment: [technical and business risk factors]\n- Strategic Alignment: [fit with long-term technology direction]\n\nGate Score = (Performance  0.25) + (Maintainability  0.25) + (Reliability  0.25) + (Business  0.25)\n```\n\n#### Threshold Management\n- Minimum acceptable scores for each quality dimension\n- Trade-off analysis for competing quality attributes\n- Conditional gates based on specific use case requirements\n- Risk-adjusted thresholds for different implementation approaches\n\n### 5. Predictive Performance Modeling\n\n**Forecast real-world behavior before implementation:**\n\n#### Performance Prediction Framework\n```\nMulti-Layer Performance Modeling:\n\nMicro-Benchmarks:\n- Individual function and method performance measurement\n- Algorithm complexity analysis and big-O verification\n- Memory allocation patterns and garbage collection impact\n- CPU instruction efficiency and optimization opportunities\n\nIntegration Performance:\n- Inter-module communication overhead and optimization\n- Database query performance and connection pooling\n- External API latency and timeout handling\n- Caching strategy effectiveness and hit ratio analysis\n\nSystem-Level Performance:\n- End-to-end request processing and user experience\n- Concurrent user simulation and resource contention\n- Peak load handling and graceful degradation\n- Infrastructure scaling behavior and cost implications\n\nProduction Environment Prediction:\n- Real-world data volume and complexity simulation\n- Production traffic pattern modeling and capacity planning\n- Deployment and rollback performance impact assessment\n- Operational monitoring and alerting effectiveness\n```\n\n#### Confidence Interval Calculation\n- Statistical analysis of performance variation across test runs\n- Confidence levels for performance predictions under different conditions\n- Sensitivity analysis for key performance parameters\n- Risk assessment for performance-related business impacts\n\n### 6. Risk and Trade-off Analysis\n\n**Systematic evaluation of implementation choices:**\n\n#### Technical Risk Assessment\n```\nRisk Evaluation Framework:\n\nImplementation Risks:\n- Technical Complexity: [difficulty and error probability]\n- Dependency Risk: [external library and service dependencies]\n- Performance Risk: [ability to meet performance requirements]\n- Integration Risk: [compatibility with existing systems]\n\nOperational Risks:\n- Deployment Complexity: [rollout difficulty and rollback capability]\n- Monitoring/Debugging: [operational visibility and troubleshooting]\n- Scaling Challenges: [growth accommodation and resource planning]\n- Maintenance Burden: [ongoing support and evolution requirements]\n\nBusiness Risks:\n- Timeline Risk: [delivery schedule and market timing impact]\n- Resource Risk: [team capacity and skill requirements]\n- Opportunity Cost: [alternative approaches and strategic alignment]\n- Competitive Risk: [technology choice and market position impact]\n```\n\n#### Trade-off Optimization\n- Pareto frontier analysis for competing objectives\n- Multi-objective optimization for quality attributes\n- Scenario-based trade-off evaluation\n- Stakeholder preference weighting and consensus building\n\n### 7. Decision Matrix and Recommendations\n\n**Generate systematic implementation guidance:**\n\n#### Code Variation Evaluation Summary\n```\n## Code Permutation Analysis: [Feature/Module Name]\n\n### Variation Comparison Matrix\n\n| Variation | Performance | Maintainability | Reliability | Business | Overall Score |\n|-----------|-------------|-----------------|-------------|----------|---------------|\n| Approach A | 85% | 70% | 90% | 75% | 80% |\n| Approach B | 70% | 90% | 80% | 85% | 81% |\n| Approach C | 95% | 60% | 70% | 65% | 73% |\n\n### Detailed Analysis\n\n#### Recommended Approach: [Selected Variation]\n\n**Rationale:**\n- Performance Advantages: [specific benefits and measurements]\n- Maintainability Considerations: [long-term support implications]\n- Risk Assessment: [identified risks and mitigation strategies]\n- Business Alignment: [strategic fit and market timing]\n\n**Implementation Plan:**\n- Development Phases: [staged implementation approach]\n- Quality Checkpoints: [validation gates and success criteria]\n- Risk Mitigation: [specific risk reduction strategies]\n- Performance Validation: [ongoing monitoring and optimization]\n\n#### Alternative Considerations:\n- Backup Option: [second-choice approach and trigger conditions]\n- Hybrid Opportunities: [combining best elements from multiple approaches]\n- Future Evolution: [how to migrate or improve chosen approach]\n- Context Dependencies: [when alternative approaches might be better]\n\n### Success Metrics and Monitoring\n- Performance KPIs: [specific metrics and acceptable ranges]\n- Quality Indicators: [maintainability and reliability measures]\n- Business Outcomes: [user satisfaction and business impact metrics]\n- Early Warning Signs: [indicators that approach is not working]\n```\n\n### 8. Continuous Learning Integration\n\n**Establish feedback loops for approach refinement:**\n\n#### Implementation Validation\n- Real-world performance comparison to simulation predictions\n- Developer experience and productivity measurement\n- User feedback and satisfaction assessment\n- Business outcome tracking and success evaluation\n\n#### Knowledge Capture\n- Decision rationale documentation and lessons learned\n- Best practice identification and pattern library development\n- Anti-pattern recognition and avoidance strategies\n- Team capability building and expertise development\n\n## Usage Examples\n\n```bash\n# Algorithm optimization testing\n/dev:code-permutation-tester Test 5 different sorting algorithms for large dataset processing with memory and speed constraints\n\n# Architecture pattern evaluation\n/dev:code-permutation-tester Compare microservices vs monolith vs modular monolith for payment processing system\n\n# Framework selection simulation\n/dev:code-permutation-tester Evaluate React vs Vue vs Angular for customer dashboard with performance and maintainability focus\n\n# Database optimization testing\n/dev:code-permutation-tester Test NoSQL vs relational vs hybrid database approaches for user analytics platform\n```\n\n## Quality Indicators\n\n- **Green**: Multiple variations tested, comprehensive quality gates, validated performance predictions\n- **Yellow**: Some variations tested, basic quality assessment, estimated performance  \n- **Red**: Single approach, minimal testing, unvalidated assumptions\n\n## Common Pitfalls to Avoid\n\n- Premature optimization: Over-engineering for theoretical rather than real requirements\n- Analysis paralysis: Testing too many variations without making decisions\n- Context ignorance: Not considering real-world constraints and team capabilities\n- Quality tunnel vision: Optimizing for single dimension while ignoring others\n- Simulation disconnect: Testing scenarios that don't match production reality\n- Decision delay: Not acting on simulation results in timely manner\n\nTransform code implementation from guesswork into systematic, evidence-based decision making through comprehensive variation testing and simulation."
              },
              {
                "name": "/code-review",
                "description": "Perform comprehensive code quality review",
                "path": "plugins/commands-utilities-debugging/commands/code-review.md",
                "frontmatter": {
                  "description": "Perform comprehensive code quality review",
                  "category": "utilities-debugging"
                },
                "content": "# Comprehensive Code Quality Review\n\nPerform comprehensive code quality review\n\n## Instructions\n\nFollow these steps to conduct a thorough code review:\n\n1. **Repository Analysis**\n   - Examine the repository structure and identify the primary language/framework\n   - Check for configuration files (package.json, requirements.txt, Cargo.toml, etc.)\n   - Review README and documentation for context\n\n2. **Code Quality Assessment**\n   - Scan for code smells, anti-patterns, and potential bugs\n   - Check for consistent coding style and naming conventions\n   - Identify unused imports, variables, or dead code\n   - Review error handling and logging practices\n\n3. **Security Review**\n   - Look for common security vulnerabilities (SQL injection, XSS, etc.)\n   - Check for hardcoded secrets, API keys, or passwords\n   - Review authentication and authorization logic\n   - Examine input validation and sanitization\n\n4. **Performance Analysis**\n   - Identify potential performance bottlenecks\n   - Check for inefficient algorithms or database queries\n   - Review memory usage patterns and potential leaks\n   - Analyze bundle size and optimization opportunities\n\n5. **Architecture & Design**\n   - Evaluate code organization and separation of concerns\n   - Check for proper abstraction and modularity\n   - Review dependency management and coupling\n   - Assess scalability and maintainability\n\n6. **Testing Coverage**\n   - Check existing test coverage and quality\n   - Identify areas lacking proper testing\n   - Review test structure and organization\n   - Suggest additional test scenarios\n\n7. **Documentation Review**\n   - Evaluate code comments and inline documentation\n   - Check API documentation completeness\n   - Review README and setup instructions\n   - Identify areas needing better documentation\n\n8. **Recommendations**\n   - Prioritize issues by severity (critical, high, medium, low)\n   - Provide specific, actionable recommendations\n   - Suggest tools and practices for improvement\n   - Create a summary report with next steps\n\nRemember to be constructive and provide specific examples with file paths and line numbers where applicable."
              },
              {
                "name": "/code-to-task",
                "description": "Convert code analysis to Linear tasks",
                "path": "plugins/commands-utilities-debugging/commands/code-to-task.md",
                "frontmatter": {
                  "description": "Convert code analysis to Linear tasks",
                  "category": "utilities-debugging"
                },
                "content": "# Convert Code Analysis to Linear Tasks\n\nConvert code analysis to Linear tasks\n\n## Purpose\nThis command scans your codebase for TODO/FIXME comments, technical debt markers, deprecated code, and other indicators that should be tracked as tasks. It automatically creates organized, prioritized Linear tasks to ensure important code improvements aren't forgotten.\n\n## Usage\n```bash\n# Scan entire codebase for TODOs and create tasks\nclaude \"Create tasks from all TODO comments in the codebase\"\n\n# Scan specific directory or module\nclaude \"Find TODOs in src/api and create Linear tasks\"\n\n# Create tasks from specific patterns\nclaude \"Create tasks for all deprecated functions\"\n\n# Generate technical debt report\nclaude \"Analyze technical debt in the project and create improvement tasks\"\n```\n\n## Instructions\n\n### 1. Scan for Task Markers\nSearch for common patterns indicating needed work:\n\n```bash\n# Find TODO comments\nrg \"TODO|FIXME|HACK|XXX|OPTIMIZE|REFACTOR\" --type-add 'code:*.{js,ts,py,java,go,rb,php}' -t code\n\n# Find deprecated markers\nrg \"@deprecated|DEPRECATED|@obsolete\" -t code\n\n# Find temporary code\nrg \"TEMPORARY|TEMP|REMOVE BEFORE|DELETE ME\" -t code -i\n\n# Find technical debt markers\nrg \"TECHNICAL DEBT|TECH DEBT|REFACTOR|NEEDS REFACTORING\" -t code -i\n\n# Find security concerns\nrg \"SECURITY|INSECURE|VULNERABILITY|CVE-\" -t code -i\n\n# Find performance issues\nrg \"SLOW|PERFORMANCE|OPTIMIZE|BOTTLENECK\" -t code -i\n```\n\n### 2. Parse Comment Context\nExtract meaningful information from comments:\n\n```javascript\nclass CommentParser {\n  parseComment(file, lineNumber, comment) {\n    const parsed = {\n      type: 'todo',\n      priority: 'medium',\n      title: '',\n      description: '',\n      author: null,\n      date: null,\n      tags: [],\n      code_context: '',\n      file_path: file,\n      line_number: lineNumber\n    };\n    \n    // Detect comment type\n    if (comment.match(/FIXME/i)) {\n      parsed.type = 'fixme';\n      parsed.priority = 'high';\n    } else if (comment.match(/HACK|XXX/i)) {\n      parsed.type = 'hack';\n      parsed.priority = 'high';\n    } else if (comment.match(/OPTIMIZE|PERFORMANCE/i)) {\n      parsed.type = 'optimization';\n    } else if (comment.match(/DEPRECATED/i)) {\n      parsed.type = 'deprecation';\n      parsed.priority = 'high';\n    } else if (comment.match(/SECURITY/i)) {\n      parsed.type = 'security';\n      parsed.priority = 'urgent';\n    }\n    \n    // Extract author and date\n    const authorMatch = comment.match(/@(\\w+)|by (\\w+)/i);\n    if (authorMatch) {\n      parsed.author = authorMatch[1] || authorMatch[2];\n    }\n    \n    const dateMatch = comment.match(/(\\d{4}-\\d{2}-\\d{2})|(\\d{1,2}\\/\\d{1,2}\\/\\d{2,4})/);\n    if (dateMatch) {\n      parsed.date = dateMatch[0];\n    }\n    \n    // Extract title and description\n    const cleanComment = comment\n      .replace(/^\\/\\/\\s*|^\\/\\*\\s*|\\*\\/\\s*$|^#\\s*/g, '')\n      .replace(/TODO|FIXME|HACK|XXX/i, '')\n      .trim();\n    \n    const parts = cleanComment.split(/[:\\-]/);\n    if (parts.length > 1) {\n      parsed.title = parts[0].trim();\n      parsed.description = parts.slice(1).join(':').trim();\n    } else {\n      parsed.title = cleanComment;\n    }\n    \n    // Extract tags\n    const tagMatch = comment.match(/#(\\w+)/g);\n    if (tagMatch) {\n      parsed.tags = tagMatch.map(tag => tag.substring(1));\n    }\n    \n    return parsed;\n  }\n  \n  getCodeContext(file, lineNumber, contextLines = 5) {\n    const lines = readFileLines(file);\n    const start = Math.max(0, lineNumber - contextLines);\n    const end = Math.min(lines.length, lineNumber + contextLines);\n    \n    return lines.slice(start, end).map((line, i) => ({\n      number: start + i + 1,\n      content: line,\n      isTarget: start + i + 1 === lineNumber\n    }));\n  }\n}\n```\n\n### 3. Group and Deduplicate\nOrganize found issues intelligently:\n\n```javascript\nclass TaskGrouper {\n  groupTasks(parsedComments) {\n    const groups = {\n      byFile: new Map(),\n      byType: new Map(),\n      byAuthor: new Map(),\n      byModule: new Map()\n    };\n    \n    for (const comment of parsedComments) {\n      // Group by file\n      if (!groups.byFile.has(comment.file_path)) {\n        groups.byFile.set(comment.file_path, []);\n      }\n      groups.byFile.get(comment.file_path).push(comment);\n      \n      // Group by type\n      if (!groups.byType.has(comment.type)) {\n        groups.byType.set(comment.type, []);\n      }\n      groups.byType.get(comment.type).push(comment);\n      \n      // Group by module\n      const module = this.extractModule(comment.file_path);\n      if (!groups.byModule.has(module)) {\n        groups.byModule.set(module, []);\n      }\n      groups.byModule.get(module).push(comment);\n    }\n    \n    return groups;\n  }\n  \n  mergeSimilarTasks(tasks) {\n    const merged = [];\n    const seen = new Set();\n    \n    for (const task of tasks) {\n      if (seen.has(task)) continue;\n      \n      // Find similar tasks\n      const similar = tasks.filter(t => \n        t !== task &&\n        !seen.has(t) &&\n        this.areSimilar(task, t)\n      );\n      \n      if (similar.length > 0) {\n        // Merge into one task\n        const mergedTask = {\n          ...task,\n          title: this.generateMergedTitle(task, similar),\n          description: this.generateMergedDescription(task, similar),\n          locations: [task, ...similar].map(t => ({\n            file: t.file_path,\n            line: t.line_number\n          }))\n        };\n        merged.push(mergedTask);\n        seen.add(task);\n        similar.forEach(t => seen.add(t));\n      } else {\n        merged.push(task);\n        seen.add(task);\n      }\n    }\n    \n    return merged;\n  }\n}\n```\n\n### 4. Analyze Technical Debt\nIdentify code quality issues:\n\n```javascript\nclass TechnicalDebtAnalyzer {\n  async analyzeFile(filePath) {\n    const issues = [];\n    const content = await readFile(filePath);\n    const lines = content.split('\\n');\n    \n    // Check for long functions\n    const functionMatches = content.matchAll(/function\\s+(\\w+)|(\\w+)\\s*=\\s*\\(.*?\\)\\s*=>/g);\n    for (const match of functionMatches) {\n      const functionName = match[1] || match[2];\n      const startLine = getLineNumber(content, match.index);\n      const functionLength = this.getFunctionLength(lines, startLine);\n      \n      if (functionLength > 50) {\n        issues.push({\n          type: 'long_function',\n          severity: functionLength > 100 ? 'high' : 'medium',\n          title: `Refactor long function: ${functionName}`,\n          description: `Function ${functionName} is ${functionLength} lines long. Consider breaking it into smaller functions.`,\n          file_path: filePath,\n          line_number: startLine\n        });\n      }\n    }\n    \n    // Check for duplicate code\n    const duplicates = await this.findDuplicateCode(filePath);\n    for (const dup of duplicates) {\n      issues.push({\n        type: 'duplicate_code',\n        severity: 'medium',\n        title: 'Remove duplicate code',\n        description: `Similar code found in ${dup.otherFile}:${dup.otherLine}`,\n        file_path: filePath,\n        line_number: dup.line\n      });\n    }\n    \n    // Check for complex conditionals\n    const complexConditions = content.matchAll(/if\\s*\\([^)]{50,}\\)/g);\n    for (const match of complexConditions) {\n      issues.push({\n        type: 'complex_condition',\n        severity: 'low',\n        title: 'Simplify complex conditional',\n        description: 'Consider extracting conditional logic into named variables or functions',\n        file_path: filePath,\n        line_number: getLineNumber(content, match.index)\n      });\n    }\n    \n    // Check for outdated dependencies\n    if (filePath.endsWith('package.json')) {\n      const outdated = await this.checkOutdatedDependencies(filePath);\n      for (const dep of outdated) {\n        issues.push({\n          type: 'outdated_dependency',\n          severity: dep.major ? 'high' : 'low',\n          title: `Update ${dep.name} from ${dep.current} to ${dep.latest}`,\n          description: dep.major ? 'Major version update available' : 'Minor update available',\n          file_path: filePath\n        });\n      }\n    }\n    \n    return issues;\n  }\n}\n```\n\n### 5. Create Linear Tasks\nConvert findings into actionable tasks:\n\n```javascript\nasync function createLinearTasks(groupedTasks, options = {}) {\n  const created = [];\n  const skipped = [];\n  \n  // Check for existing tasks to avoid duplicates\n  const existingTasks = await linear.searchTasks('TODO OR FIXME');\n  const existingTitles = new Set(existingTasks.map(t => t.title));\n  \n  // Create parent task for large groups\n  if (options.createEpic && groupedTasks.length > 10) {\n    const epic = await linear.createTask({\n      title: `Technical Debt: ${options.module || 'Codebase'} Cleanup`,\n      description: `Parent task for ${groupedTasks.length} code improvements`,\n      priority: 2,\n      labels: ['technical-debt', 'code-quality']\n    });\n    options.parentId = epic.id;\n  }\n  \n  for (const task of groupedTasks) {\n    // Skip if similar task exists\n    if (existingTitles.has(task.title)) {\n      skipped.push({ task, reason: 'duplicate' });\n      continue;\n    }\n    \n    // Build task description\n    const description = buildTaskDescription(task);\n    \n    // Map priority\n    const priorityMap = {\n      urgent: 1,\n      high: 2,\n      medium: 3,\n      low: 4\n    };\n    \n    try {\n      const linearTask = await linear.createTask({\n        title: task.title,\n        description,\n        priority: priorityMap[task.priority] || 3,\n        labels: getLabelsForTask(task),\n        parentId: options.parentId,\n        estimate: estimateTaskSize(task)\n      });\n      \n      created.push({\n        linear: linearTask,\n        source: task\n      });\n      \n      // Add code link as comment\n      await linear.createComment({\n        issueId: linearTask.id,\n        body: ` Code location: \\`${task.file_path}:${task.line_number}\\``\n      });\n      \n    } catch (error) {\n      skipped.push({ task, reason: error.message });\n    }\n  }\n  \n  return { created, skipped };\n}\n\nfunction buildTaskDescription(task) {\n  let description = task.description || '';\n  \n  // Add code context\n  if (task.code_context) {\n    description += '\\n\\n### Code Context\\n```\\n';\n    task.code_context.forEach(line => {\n      const prefix = line.isTarget ? '>>> ' : '    ';\n      description += `${prefix}${line.number}: ${line.content}\\n`;\n    });\n    description += '```\\n';\n  }\n  \n  // Add metadata\n  description += '\\n\\n### Details\\n';\n  description += `- **Type**: ${task.type}\\n`;\n  description += `- **File**: \\`${task.file_path}\\`\\n`;\n  description += `- **Line**: ${task.line_number}\\n`;\n  \n  if (task.author) {\n    description += `- **Author**: @${task.author}\\n`;\n  }\n  if (task.date) {\n    description += `- **Date**: ${task.date}\\n`;\n  }\n  if (task.tags.length > 0) {\n    description += `- **Tags**: ${task.tags.join(', ')}\\n`;\n  }\n  \n  // Add suggestions\n  if (task.type === 'deprecated') {\n    description += '\\n### Suggested Actions\\n';\n    description += '1. Identify all usages of this deprecated code\\n';\n    description += '2. Update to use the recommended alternative\\n';\n    description += '3. Add deprecation warnings if not present\\n';\n    description += '4. Schedule for removal in next major version\\n';\n  }\n  \n  return description;\n}\n```\n\n### 6. Generate Summary Report\nCreate overview of findings:\n\n```javascript\nfunction generateReport(scanResults, createdTasks) {\n  const report = {\n    summary: {\n      totalFound: scanResults.length,\n      tasksCreated: createdTasks.created.length,\n      tasksSkipped: createdTasks.skipped.length,\n      byType: {},\n      byPriority: {},\n      byFile: {}\n    },\n    details: [],\n    recommendations: []\n  };\n  \n  // Analyze distribution\n  for (const result of scanResults) {\n    report.summary.byType[result.type] = (report.summary.byType[result.type] || 0) + 1;\n    report.summary.byPriority[result.priority] = (report.summary.byPriority[result.priority] || 0) + 1;\n  }\n  \n  // Generate recommendations\n  if (report.summary.byType.security > 0) {\n    report.recommendations.push({\n      priority: 'urgent',\n      action: 'Address security-related TODOs immediately',\n      tasks: scanResults.filter(r => r.type === 'security').length\n    });\n  }\n  \n  if (report.summary.byType.deprecated > 5) {\n    report.recommendations.push({\n      priority: 'high',\n      action: 'Create deprecation removal sprint',\n      tasks: report.summary.byType.deprecated\n    });\n  }\n  \n  return report;\n}\n```\n\n### 7. Error Handling\n```javascript\n// Handle access errors\ntry {\n  await scanDirectory(path);\n} catch (error) {\n  if (error.code === 'EACCES') {\n    console.warn(`Skipping ${path} - permission denied`);\n  }\n}\n\n// Handle Linear API limits\nconst rateLimiter = {\n  tasksCreated: 0,\n  resetTime: Date.now() + 3600000,\n  \n  async createTask(taskData) {\n    if (this.tasksCreated >= 50) {\n      console.log('Rate limit approaching, batching remaining tasks...');\n      // Create single task with list of TODOs\n      return this.createBatchTask(remainingTasks);\n    }\n    this.tasksCreated++;\n    return linear.createTask(taskData);\n  }\n};\n\n// Handle malformed comments\nconst safeParser = {\n  parse(comment) {\n    try {\n      return this.parseComment(comment);\n    } catch (error) {\n      return {\n        type: 'todo',\n        title: comment.substring(0, 50) + '...',\n        priority: 'low',\n        parseError: true\n      };\n    }\n  }\n};\n```\n\n## Example Output\n\n```\nScanning codebase for TODOs and technical debt...\n\n Scan Results:\n\n\nFound 47 items across 23 files:\n   24 TODOs\n   8 FIXMEs \n   5 Deprecated functions\n   3 Security concerns\n   7 Performance optimizations\n\n Breakdown by Priority:\n   Urgent: 3 (security related)\n   High: 13 (FIXMEs + deprecations)\n   Medium: 24 (standard TODOs)\n   Low: 7 (optimizations)\n\n Hotspot Files:\n  1. src/api/auth.js - 8 items\n  2. src/utils/validation.js - 6 items\n  3. src/models/User.js - 5 items\n\n Critical Findings:\n\n1. SECURITY: Hardcoded API key\n   File: src/config/api.js:45\n   TODO: Remove hardcoded key and use env variable\n    Creating task with URGENT priority\n\n2. DEPRECATED: Legacy authentication method\n   File: src/api/auth.js:120\n   Multiple usages found in 4 files\n    Creating migration task\n\n3. FIXME: Race condition in concurrent updates\n   File: src/services/sync.js:78\n   Author: @alice (2024-01-03)\n    Creating high-priority bug task\n\n Task Creation Summary:\n\n\n Created 32 Linear tasks:\n   - Epic: \"Q1 Technical Debt Cleanup\" (LIN-456)\n   - 3 urgent security tasks\n   - 10 high-priority fixes\n   - 19 medium-priority improvements\n\n Skipped 15 items:\n   - 8 duplicates (tasks already exist)\n   - 4 low-value comments (e.g., \"TODO: think about this\")\n   - 3 external dependencies (waiting on upstream)\n\n Estimates:\n   - Total story points: 89\n   - Estimated effort: 2-3 sprints\n   - Recommended team size: 2-3 developers\n\n Recommended Actions:\n1. Schedule security sprint immediately (3 urgent items)\n2. Assign deprecation removal to next sprint (5 items)\n3. Create coding standards to reduce future TODOs\n4. Set up pre-commit hook to limit new TODOs\n\nView all created tasks:\nhttps://linear.app/yourteam/project/q1-technical-debt-cleanup\n```\n\n## Advanced Features\n\n### Custom Patterns\nDefine project-specific patterns:\n```bash\n# Add custom markers to scan\nclaude \"Scan for REVIEW, QUESTION, and ASSUMPTION comments\"\n```\n\n### Integration with CI/CD\n```bash\n# Fail build if critical TODOs found\nclaude \"Check for SECURITY or FIXME comments and exit with error if found\"\n```\n\n### Scheduled Scans\n```bash\n# Weekly technical debt report\nclaude \"Generate weekly technical debt report and create tasks for new items\"\n```\n\n## Tips\n- Run regularly to prevent TODO accumulation\n- Use consistent comment formats across the team\n- Include author and date in TODOs\n- Link TODOs to existing Linear issues when possible\n- Set up IDE snippets for properly formatted TODOs\n- Review and close completed TODO tasks\n- Use TODO comments as a quality gate in PR reviews"
              },
              {
                "name": "/debug-error",
                "description": "Systematically debug and fix errors",
                "path": "plugins/commands-utilities-debugging/commands/debug-error.md",
                "frontmatter": {
                  "description": "Systematically debug and fix errors",
                  "category": "utilities-debugging",
                  "argument-hint": "1. **Error Information Gathering**",
                  "allowed-tools": "Read"
                },
                "content": "# Systematically Debug and Fix Errors\n\nSystematically debug and fix errors\n\n## Instructions\n\nFollow this comprehensive debugging methodology to resolve: **$ARGUMENTS**\n\n1. **Error Information Gathering**\n   - Collect the complete error message, stack trace, and error code\n   - Note when the error occurs (timing, conditions, frequency)\n   - Identify the environment where the error happens (dev, staging, prod)\n   - Gather relevant logs from before and after the error\n\n2. **Reproduce the Error**\n   - Create a minimal test case that reproduces the error consistently\n   - Document the exact steps needed to trigger the error\n   - Test in different environments if possible\n   - Note any patterns or conditions that affect error occurrence\n\n3. **Stack Trace Analysis**\n   - Read the stack trace from bottom to top to understand the call chain\n   - Identify the exact line where the error originates\n   - Trace the execution path leading to the error\n   - Look for any obvious issues in the failing code\n\n4. **Code Context Investigation**\n   - Examine the code around the error location\n   - Check recent changes that might have introduced the bug\n   - Review variable values and state at the time of error\n   - Analyze function parameters and return values\n\n5. **Hypothesis Formation**\n   - Based on evidence, form hypotheses about the root cause\n   - Consider common causes:\n     - Null pointer/undefined reference\n     - Type mismatches\n     - Race conditions\n     - Resource exhaustion\n     - Logic errors\n     - External dependency failures\n\n6. **Debugging Tools Setup**\n   - Set up appropriate debugging tools for the technology stack\n   - Use debugger, profiler, or logging as needed\n   - Configure breakpoints at strategic locations\n   - Set up monitoring and alerting if not already present\n\n7. **Systematic Investigation**\n   - Test each hypothesis methodically\n   - Use binary search approach to isolate the problem\n   - Add strategic logging or print statements\n   - Check data flow and transformations step by step\n\n8. **Data Validation**\n   - Verify input data format and validity\n   - Check for edge cases and boundary conditions\n   - Validate assumptions about data state\n   - Test with different data sets to isolate patterns\n\n9. **Dependency Analysis**\n   - Check external dependencies and their versions\n   - Verify network connectivity and API availability\n   - Review configuration files and environment variables\n   - Test database connections and query execution\n\n10. **Memory and Resource Analysis**\n    - Check for memory leaks or excessive memory usage\n    - Monitor CPU and I/O resource consumption\n    - Analyze garbage collection patterns if applicable\n    - Check for resource deadlocks or contention\n\n11. **Concurrency Issues Investigation**\n    - Look for race conditions in multi-threaded code\n    - Check synchronization mechanisms and locks\n    - Analyze async operations and promise handling\n    - Test under different load conditions\n\n12. **Root Cause Identification**\n    - Once the cause is identified, understand why it happened\n    - Determine if it's a logic error, design flaw, or external issue\n    - Assess the scope and impact of the problem\n    - Consider if similar issues exist elsewhere\n\n13. **Solution Implementation**\n    - Design a fix that addresses the root cause\n    - Consider multiple solution approaches and trade-offs\n    - Implement the fix with appropriate error handling\n    - Add validation and defensive programming where needed\n\n14. **Testing the Fix**\n    - Test the fix against the original error case\n    - Test edge cases and related scenarios\n    - Run regression tests to ensure no new issues\n    - Test under various load and stress conditions\n\n15. **Prevention Measures**\n    - Add appropriate unit and integration tests\n    - Improve error handling and logging\n    - Add input validation and defensive checks\n    - Update documentation and code comments\n\n16. **Monitoring and Alerting**\n    - Set up monitoring for similar issues\n    - Add metrics and health checks\n    - Configure alerts for error thresholds\n    - Implement better observability\n\n17. **Documentation**\n    - Document the error, investigation process, and solution\n    - Update troubleshooting guides\n    - Share learnings with the team\n    - Update code comments with context\n\n18. **Post-Resolution Review**\n    - Analyze why the error wasn't caught earlier\n    - Review development and testing processes\n    - Consider improvements to prevent similar issues\n    - Update coding standards or guidelines if needed\n\nRemember to maintain detailed notes throughout the debugging process and consider the wider implications of both the error and the fix."
              },
              {
                "name": "/directory-deep-dive",
                "description": "Analyze directory structure and purpose",
                "path": "plugins/commands-utilities-debugging/commands/directory-deep-dive.md",
                "frontmatter": {
                  "description": "Analyze directory structure and purpose",
                  "category": "utilities-debugging",
                  "argument-hint": "Specify directory path"
                },
                "content": "# Directory Deep Dive\n\nAnalyze directory structure and purpose\n\n## Instructions\n\n1. **Target Directory**\n   - Focus on the specified directory `$ARGUMENTS` or the current working directory\n\n2. **Investigate Architecture**\n   - Analyze the implementation principles and architecture of the code in this directory and its subdirectories\n   - Look for:\n     - Design patterns being used\n     - Dependencies and their purposes\n     - Key abstractions and interfaces\n     - Naming conventions and code organization\n\n3. **Create or Update Documentation**\n   - Create a CLAUDE.md file capturing this knowledge\n   - If one already exists, update it with newly discovered information\n   - Include:\n     - Purpose and responsibility of this module\n     - Key architectural decisions\n     - Important implementation details\n     - Common patterns used throughout the code\n     - Any gotchas or non-obvious behaviors\n\n4. **Ensure Proper Placement**\n   - Place the CLAUDE.md file in the directory being analyzed\n   - This ensures the context is loaded when working in that specific area\n\n## Credit\n\nThis command is based on the work of Thomas Landgraf: https://thomaslandgraf.substack.com/p/claude-codes-memory-working-with"
              },
              {
                "name": "/explain-code",
                "description": "Analyze and explain code functionality",
                "path": "plugins/commands-utilities-debugging/commands/explain-code.md",
                "frontmatter": {
                  "description": "Analyze and explain code functionality",
                  "category": "utilities-debugging",
                  "argument-hint": "1. **Code Context Analysis**"
                },
                "content": "# Analyze and Explain Code Functionality\n\nAnalyze and explain code functionality\n\n## Instructions\n\nFollow this systematic approach to explain code: **$ARGUMENTS**\n\n1. **Code Context Analysis**\n   - Identify the programming language and framework\n   - Understand the broader context and purpose of the code\n   - Identify the file location and its role in the project\n   - Review related imports, dependencies, and configurations\n\n2. **High-Level Overview**\n   - Provide a summary of what the code does\n   - Explain the main purpose and functionality\n   - Identify the problem the code is solving\n   - Describe how it fits into the larger system\n\n3. **Code Structure Breakdown**\n   - Break down the code into logical sections\n   - Identify classes, functions, and methods\n   - Explain the overall architecture and design patterns\n   - Map out data flow and control flow\n\n4. **Line-by-Line Analysis**\n   - Explain complex or non-obvious lines of code\n   - Describe variable declarations and their purposes\n   - Explain function calls and their parameters\n   - Clarify conditional logic and loops\n\n5. **Algorithm and Logic Explanation**\n   - Describe the algorithm or approach being used\n   - Explain the logic behind complex calculations\n   - Break down nested conditions and loops\n   - Clarify recursive or asynchronous operations\n\n6. **Data Structures and Types**\n   - Explain data types and structures being used\n   - Describe how data is transformed or processed\n   - Explain object relationships and hierarchies\n   - Clarify input and output formats\n\n7. **Framework and Library Usage**\n   - Explain framework-specific patterns and conventions\n   - Describe library functions and their purposes\n   - Explain API calls and their expected responses\n   - Clarify configuration and setup code\n\n8. **Error Handling and Edge Cases**\n   - Explain error handling mechanisms\n   - Describe exception handling and recovery\n   - Identify edge cases being handled\n   - Explain validation and defensive programming\n\n9. **Performance Considerations**\n   - Identify performance-critical sections\n   - Explain optimization techniques being used\n   - Describe complexity and scalability implications\n   - Point out potential bottlenecks or inefficiencies\n\n10. **Security Implications**\n    - Identify security-related code sections\n    - Explain authentication and authorization logic\n    - Describe input validation and sanitization\n    - Point out potential security vulnerabilities\n\n11. **Testing and Debugging**\n    - Explain how the code can be tested\n    - Identify debugging points and logging\n    - Describe mock data or test scenarios\n    - Explain test helpers and utilities\n\n12. **Dependencies and Integrations**\n    - Explain external service integrations\n    - Describe database operations and queries\n    - Explain API interactions and protocols\n    - Clarify third-party library usage\n\n**Explanation Format Examples:**\n\n**For Complex Algorithms:**\n```\nThis function implements a depth-first search algorithm:\n\n1. Line 1-3: Initialize a stack with the starting node and a visited set\n2. Line 4-8: Main loop - continue until stack is empty\n3. Line 9-11: Pop a node and check if it's the target\n4. Line 12-15: Add unvisited neighbors to the stack\n5. Line 16: Return null if target not found\n\nTime Complexity: O(V + E) where V is vertices and E is edges\nSpace Complexity: O(V) for the visited set and stack\n```\n\n**For API Integration Code:**\n```\nThis code handles user authentication with a third-party service:\n\n1. Extract credentials from request headers\n2. Validate credential format and required fields\n3. Make API call to authentication service\n4. Handle response and extract user data\n5. Create session token and set cookies\n6. Return user profile or error response\n\nError Handling: Catches network errors, invalid credentials, and service unavailability\nSecurity: Uses HTTPS, validates inputs, and sanitizes responses\n```\n\n**For Database Operations:**\n```\nThis function performs a complex database query with joins:\n\n1. Build base query with primary table\n2. Add LEFT JOIN for related user data\n3. Apply WHERE conditions for filtering\n4. Add ORDER BY for consistent sorting\n5. Implement pagination with LIMIT/OFFSET\n6. Execute query and handle potential errors\n7. Transform raw results into domain objects\n\nPerformance Notes: Uses indexes on filtered columns, implements connection pooling\n```\n\n13. **Common Patterns and Idioms**\n    - Identify language-specific patterns and idioms\n    - Explain design patterns being implemented\n    - Describe architectural patterns in use\n    - Clarify naming conventions and code style\n\n14. **Potential Improvements**\n    - Suggest code improvements and optimizations\n    - Identify possible refactoring opportunities\n    - Point out maintainability concerns\n    - Recommend best practices and standards\n\n15. **Related Code and Context**\n    - Reference related functions and classes\n    - Explain how this code interacts with other components\n    - Describe the calling context and usage patterns\n    - Point to relevant documentation and resources\n\n16. **Debugging and Troubleshooting**\n    - Explain how to debug issues in this code\n    - Identify common failure points\n    - Describe logging and monitoring approaches\n    - Suggest testing strategies\n\n**Language-Specific Considerations:**\n\n**JavaScript/TypeScript:**\n- Explain async/await and Promise handling\n- Describe closure and scope behavior\n- Clarify this binding and arrow functions\n- Explain event handling and callbacks\n\n**Python:**\n- Explain list comprehensions and generators\n- Describe decorator usage and purpose\n- Clarify context managers and with statements\n- Explain class inheritance and method resolution\n\n**Java:**\n- Explain generics and type parameters\n- Describe annotation usage and processing\n- Clarify stream operations and lambda expressions\n- Explain exception hierarchy and handling\n\n**C#:**\n- Explain LINQ queries and expressions\n- Describe async/await and Task handling\n- Clarify delegate and event usage\n- Explain nullable reference types\n\n**Go:**\n- Explain goroutines and channel usage\n- Describe interface implementation\n- Clarify error handling patterns\n- Explain package structure and imports\n\n**Rust:**\n- Explain ownership and borrowing\n- Describe lifetime annotations\n- Clarify pattern matching and Option/Result types\n- Explain trait implementations\n\nRemember to:\n- Use clear, non-technical language when possible\n- Provide examples and analogies for complex concepts\n- Structure explanations logically from high-level to detailed\n- Include visual diagrams or flowcharts when helpful\n- Tailor the explanation level to the intended audience"
              },
              {
                "name": "/generate-linear-worklog",
                "description": "You are tasked with generating a technical work log comment for a Linear issue based on recent git commits.",
                "path": "plugins/commands-utilities-debugging/commands/generate-linear-worklog.md",
                "frontmatter": {
                  "description": "You are tasked with generating a technical work log comment for a Linear issue based on recent git commits.",
                  "category": "utilities-debugging",
                  "allowed-tools": "Bash(git *), Bash(npm *)"
                },
                "content": "# Generate Linear Work Log\n\nYou are tasked with generating a technical work log comment for a Linear issue based on recent git commits.\n\n## Instructions\n\n1. **Check Linear MCP Availability**\n   - Verify that Linear MCP tools are available (mcp__linear__* functions)\n   - If Linear MCP is not installed, inform the user to install it and provide installation instructions\n   - Do not proceed with work log generation if Linear MCP is unavailable\n\n2. **Check for Existing Work Log**\n   - Use Linear MCP to get existing comments on the issue\n   - Look for comments with today's date in the format \"## Work Completed [TODAY'S DATE]\"\n   - If found, note the existing content to append/update rather than duplicate\n\n2. **Extract Git Information**\n   - Get the current branch name\n   - Get recent commits on the current branch (last 10 commits)\n   - Get commits that are on the current branch but not on main branch\n   - For each relevant commit, get detailed information including file changes and line counts\n   - Focus on commits since the last work log update (if any exists)\n\n3. **Generate Work Log Content**\n   - Use dry, technical language without adjectives or emojis\n   - Focus on factual implementation details\n   - Structure the log with date, branch, and commit information\n   - Include quantitative metrics (file counts, line counts) where relevant\n   - Avoid subjective commentary or promotional language\n\n4. **Handle Existing Work Log**\n   - If no work log exists for today: Create new comment\n   - If work log exists for today: Replace the existing comment with updated content including all today's work\n   - Ensure chronological order of commits\n   - Include both previous and new work completed today\n\n5. **Format Structure**\n   ```\n   ## Work Completed [TODAY'S DATE]\n\n   ### Branch: [current-branch-name]\n\n   **Commit [short-hash]: [Commit Title]**\n   - [Technical detail 1]\n   - [Technical detail 2]\n   - [Line count] lines of code across [file count] files\n\n   [Additional commits in chronological order]\n\n   ### [Status Section]\n   - [Current infrastructure/testing status]\n   - [What is now available/ready]\n   ```\n\n6. **Post to Linear**\n   - Use the Linear MCP integration to create or update the comment\n   - Post the formatted work log to the specified Linear issue\n   - If updating, replace the entire existing work log comment\n   - Confirm successful posting\n\n## Git Commands to Use\n- `git branch --show-current` - Get current branch\n- `git log --oneline -10` - Get recent commits\n- `git log main..HEAD --oneline` - Get branch-specific commits\n- `git show --stat [commit-hash]` - Get detailed commit info\n- `git log --since=\"[today's date]\" --pretty=format:\"%h %ad %s\" --date=short` - Get today's commits\n\n## Content Guidelines\n- Include commit hashes and descriptive titles\n- Provide specific technical implementations\n- Include file counts and line counts for significant changes\n- Maintain consistent formatting\n- Focus on technical accomplishments\n- Include current status summary\n- No emojis or special characters\n\n## Error Handling\n- Check if Linear MCP client is available before proceeding\n- If Linear MCP is not available, display installation instructions:\n  ```\n  Linear MCP client is not installed. To install it:\n  \n  1. Install the Linear MCP server:\n     npm install -g @modelcontextprotocol/server-linear\n  \n  2. Add Linear MCP to your Claude configuration:\n     Add the following to your Claude MCP settings:\n     {\n       \"mcpServers\": {\n         \"linear\": {\n           \"command\": \"npx\",\n           \"args\": [\"@modelcontextprotocol/server-linear\"],\n           \"env\": {\n             \"LINEAR_API_KEY\": \"your_linear_api_key_here\"\n           }\n         }\n       }\n     }\n  \n  3. Restart Claude Code\n  4. Get your Linear API key from: https://linear.app/settings/api\n  ```\n- Validate that the Linear ticket ID exists\n- Handle cases where no recent commits are found\n- Provide clear error messages for git operation failures\n- Confirm successful comment posting\n\n## Example Usage\nWhen invoked with `/generate-linear-worklog BLA2-2`, the command should:\n1. Analyze git commits on the current branch\n2. Generate a structured work log\n3. Post the comment to Linear issue BLA2-2\n4. Confirm successful posting"
              },
              {
                "name": "/git-status",
                "description": "Show detailed git repository status",
                "path": "plugins/commands-utilities-debugging/commands/git-status.md",
                "frontmatter": {
                  "description": "Show detailed git repository status",
                  "category": "utilities-debugging",
                  "argument-hint": "Optional: specify path or options",
                  "allowed-tools": "Bash(git *), Read"
                },
                "content": "# Git Status Command\n\nShow detailed git repository status\n\n*Command originally created by IndyDevDan (YouTube: https://www.youtube.com/@indydevdan) / DislerH (GitHub: https://github.com/disler)*\n\n## Instructions\n\nAnalyze the current state of the git repository by performing the following steps:\n\n1. **Run Git Status Commands**\n   - Execute `git status` to see current working tree state\n   - Run `git diff HEAD origin/main` to check differences with remote\n   - Execute `git branch --show-current` to display current branch\n   - Check for uncommitted changes and untracked files\n\n2. **Analyze Repository State**\n   - Identify staged vs unstaged changes\n   - List any untracked files\n   - Check if branch is ahead/behind remote\n   - Review any merge conflicts if present\n\n3. **Read Key Files**\n   - Review README.md for project context\n   - Check for any recent changes in important files\n   - Understand project structure if needed\n\n4. **Provide Summary**\n   - Current branch and its relationship to main/master\n   - Number of commits ahead/behind\n   - List of modified files with change types\n   - Any action items (commits needed, pulls required, etc.)\n\nThis command helps developers quickly understand:\n- What changes are pending\n- The repository's sync status\n- Whether any actions are needed before continuing work\n\nArguments: $ARGUMENTS"
              },
              {
                "name": "/refactor-code",
                "description": "Intelligently refactor and improve code quality",
                "path": "plugins/commands-utilities-debugging/commands/refactor-code.md",
                "frontmatter": {
                  "description": "Intelligently refactor and improve code quality",
                  "category": "utilities-debugging",
                  "argument-hint": "1. **Pre-Refactoring Analysis**"
                },
                "content": "# Intelligently Refactor and Improve Code Quality\n\nIntelligently refactor and improve code quality\n\n## Instructions\n\nFollow this systematic approach to refactor code: **$ARGUMENTS**\n\n1. **Pre-Refactoring Analysis**\n   - Identify the code that needs refactoring and the reasons why\n   - Understand the current functionality and behavior completely\n   - Review existing tests and documentation\n   - Identify all dependencies and usage points\n\n2. **Test Coverage Verification**\n   - Ensure comprehensive test coverage exists for the code being refactored\n   - If tests are missing, write them BEFORE starting refactoring\n   - Run all tests to establish a baseline\n   - Document current behavior with additional tests if needed\n\n3. **Refactoring Strategy**\n   - Define clear goals for the refactoring (performance, readability, maintainability)\n   - Choose appropriate refactoring techniques:\n     - Extract Method/Function\n     - Extract Class/Component\n     - Rename Variable/Method\n     - Move Method/Field\n     - Replace Conditional with Polymorphism\n     - Eliminate Dead Code\n   - Plan the refactoring in small, incremental steps\n\n4. **Environment Setup**\n   - Create a new branch: `git checkout -b refactor/$ARGUMENTS`\n   - Ensure all tests pass before starting\n   - Set up any additional tooling needed (profilers, analyzers)\n\n5. **Incremental Refactoring**\n   - Make small, focused changes one at a time\n   - Run tests after each change to ensure nothing breaks\n   - Commit working changes frequently with descriptive messages\n   - Use IDE refactoring tools when available for safety\n\n6. **Code Quality Improvements**\n   - Improve naming conventions for clarity\n   - Eliminate code duplication (DRY principle)\n   - Simplify complex conditional logic\n   - Reduce method/function length and complexity\n   - Improve separation of concerns\n\n7. **Performance Optimizations**\n   - Identify and eliminate performance bottlenecks\n   - Optimize algorithms and data structures\n   - Reduce unnecessary computations\n   - Improve memory usage patterns\n\n8. **Design Pattern Application**\n   - Apply appropriate design patterns where beneficial\n   - Improve abstraction and encapsulation\n   - Enhance modularity and reusability\n   - Reduce coupling between components\n\n9. **Error Handling Improvement**\n   - Standardize error handling approaches\n   - Improve error messages and logging\n   - Add proper exception handling\n   - Enhance resilience and fault tolerance\n\n10. **Documentation Updates**\n    - Update code comments to reflect changes\n    - Revise API documentation if interfaces changed\n    - Update inline documentation and examples\n    - Ensure comments are accurate and helpful\n\n11. **Testing Enhancements**\n    - Add tests for any new code paths created\n    - Improve existing test quality and coverage\n    - Remove or update obsolete tests\n    - Ensure tests are still meaningful and effective\n\n12. **Static Analysis**\n    - Run linting tools to catch style and potential issues\n    - Use static analysis tools to identify problems\n    - Check for security vulnerabilities\n    - Verify code complexity metrics\n\n13. **Performance Verification**\n    - Run performance benchmarks if applicable\n    - Compare before/after metrics\n    - Ensure refactoring didn't degrade performance\n    - Document any performance improvements\n\n14. **Integration Testing**\n    - Run full test suite to ensure no regressions\n    - Test integration with dependent systems\n    - Verify all functionality works as expected\n    - Test edge cases and error scenarios\n\n15. **Code Review Preparation**\n    - Review all changes for quality and consistency\n    - Ensure refactoring goals were achieved\n    - Prepare clear explanation of changes made\n    - Document benefits and rationale\n\n16. **Documentation of Changes**\n    - Create a summary of refactoring changes\n    - Document any breaking changes or new patterns\n    - Update project documentation if needed\n    - Explain benefits and reasoning for future reference\n\n17. **Deployment Considerations**\n    - Plan deployment strategy for refactored code\n    - Consider feature flags for gradual rollout\n    - Prepare rollback procedures\n    - Set up monitoring for the refactored components\n\nRemember: Refactoring should preserve external behavior while improving internal structure. Always prioritize safety over speed, and maintain comprehensive test coverage throughout the process."
              },
              {
                "name": "/ultra-think",
                "description": "Deep analysis and problem solving mode",
                "path": "plugins/commands-utilities-debugging/commands/ultra-think.md",
                "frontmatter": {
                  "description": "Deep analysis and problem solving mode",
                  "category": "utilities-debugging",
                  "argument-hint": "Identify all stakeholders and constraints"
                },
                "content": "# Deep Analysis and Problem Solving Mode\n\nDeep analysis and problem solving mode\n\n## Instructions\n\n1. **Initialize Ultra Think Mode**\n   - Acknowledge the request for enhanced analytical thinking\n   - Set context for deep, systematic reasoning\n   - Prepare to explore the problem space comprehensively\n\n2. **Parse the Problem or Question**\n   - Extract the core challenge from: **$ARGUMENTS**\n   - Identify all stakeholders and constraints\n   - Recognize implicit requirements and hidden complexities\n   - Question assumptions and surface unknowns\n\n3. **Multi-Dimensional Analysis**\n   Approach the problem from multiple angles:\n   \n   ### Technical Perspective\n   - Analyze technical feasibility and constraints\n   - Consider scalability, performance, and maintainability\n   - Evaluate security implications\n   - Assess technical debt and future-proofing\n   \n   ### Business Perspective\n   - Understand business value and ROI\n   - Consider time-to-market pressures\n   - Evaluate competitive advantages\n   - Assess risk vs. reward trade-offs\n   \n   ### User Perspective\n   - Analyze user needs and pain points\n   - Consider usability and accessibility\n   - Evaluate user experience implications\n   - Think about edge cases and user journeys\n   \n   ### System Perspective\n   - Consider system-wide impacts\n   - Analyze integration points\n   - Evaluate dependencies and coupling\n   - Think about emergent behaviors\n\n4. **Generate Multiple Solutions**\n   - Brainstorm at least 3-5 different approaches\n   - For each approach, consider:\n     - Pros and cons\n     - Implementation complexity\n     - Resource requirements\n     - Potential risks\n     - Long-term implications\n   - Include both conventional and creative solutions\n   - Consider hybrid approaches\n\n5. **Deep Dive Analysis**\n   For the most promising solutions:\n   - Create detailed implementation plans\n   - Identify potential pitfalls and mitigation strategies\n   - Consider phased approaches and MVPs\n   - Analyze second and third-order effects\n   - Think through failure modes and recovery\n\n6. **Cross-Domain Thinking**\n   - Draw parallels from other industries or domains\n   - Apply design patterns from different contexts\n   - Consider biological or natural system analogies\n   - Look for innovative combinations of existing solutions\n\n7. **Challenge and Refine**\n   - Play devil's advocate with each solution\n   - Identify weaknesses and blind spots\n   - Consider \"what if\" scenarios\n   - Stress-test assumptions\n   - Look for unintended consequences\n\n8. **Synthesize Insights**\n   - Combine insights from all perspectives\n   - Identify key decision factors\n   - Highlight critical trade-offs\n   - Summarize innovative discoveries\n   - Present a nuanced view of the problem space\n\n9. **Provide Structured Recommendations**\n   Present findings in a clear structure:\n   ```\n   ## Problem Analysis\n   - Core challenge\n   - Key constraints\n   - Critical success factors\n   \n   ## Solution Options\n   ### Option 1: [Name]\n   - Description\n   - Pros/Cons\n   - Implementation approach\n   - Risk assessment\n   \n   ### Option 2: [Name]\n   [Similar structure]\n   \n   ## Recommendation\n   - Recommended approach\n   - Rationale\n   - Implementation roadmap\n   - Success metrics\n   - Risk mitigation plan\n   \n   ## Alternative Perspectives\n   - Contrarian view\n   - Future considerations\n   - Areas for further research\n   ```\n\n10. **Meta-Analysis**\n    - Reflect on the thinking process itself\n    - Identify areas of uncertainty\n    - Acknowledge biases or limitations\n    - Suggest additional expertise needed\n    - Provide confidence levels for recommendations\n\n## Usage Examples\n\n```bash\n# Architectural decision\n/project:ultra-think Should we migrate to microservices or improve our monolith?\n\n# Complex problem solving\n/project:ultra-think How do we scale our system to handle 10x traffic while reducing costs?\n\n# Strategic planning\n/project:ultra-think What technology stack should we choose for our next-gen platform?\n\n# Design challenge\n/project:ultra-think How can we improve our API to be more developer-friendly while maintaining backward compatibility?\n```\n\n## Key Principles\n\n- **First Principles Thinking**: Break down to fundamental truths\n- **Systems Thinking**: Consider interconnections and feedback loops\n- **Probabilistic Thinking**: Work with uncertainties and ranges\n- **Inversion**: Consider what to avoid, not just what to do\n- **Second-Order Thinking**: Consider consequences of consequences\n\n## Output Expectations\n\n- Comprehensive analysis (typically 2-4 pages of insights)\n- Multiple viable solutions with trade-offs\n- Clear reasoning chains\n- Acknowledgment of uncertainties\n- Actionable recommendations\n- Novel insights or perspectives"
              }
            ],
            "skills": []
          },
          {
            "name": "commands-version-control-git",
            "description": "Commands for Git operations, commits, and PRs",
            "source": "./plugins/commands-version-control-git",
            "category": "commands",
            "version": "1.0.0",
            "author": {
              "name": "BuildWithClaude Community",
              "url": "https://github.com/davepoon/buildwithclaude"
            },
            "install_commands": [
              "/plugin marketplace add davepoon/buildwithclaude",
              "/plugin install commands-version-control-git@buildwithclaude"
            ],
            "signals": {
              "stars": 2229,
              "forks": 239,
              "pushed_at": "2026-01-12T10:19:01Z",
              "created_at": "2025-07-25T02:26:45Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/bug-fix",
                "description": "Systematic workflow for fixing bugs including issue creation, branch management, and PR submission",
                "path": "plugins/commands-version-control-git/commands/bug-fix.md",
                "frontmatter": {
                  "description": "Systematic workflow for fixing bugs including issue creation, branch management, and PR submission",
                  "category": "version-control-git",
                  "argument-hint": "<bug_description>",
                  "allowed-tools": "Bash(git *), Bash(gh *)"
                },
                "content": "Understand the bug: $ARGUMENTS\n\nBefore Starting:\n- GITHUB: create an issue with a short descriptive title.\n- GIT: checkout a branch and switch to it.\n\nFix the Bug\n\nOn Completion:\n- GIT: commit with a descriptive message.\n- GIT: push the branch to the remote repository.\n- GITHUB: create a PR and link the issue."
              },
              {
                "name": "/commit-fast",
                "description": "Automatically create and execute a git commit using the first suggested commit message",
                "path": "plugins/commands-version-control-git/commands/commit-fast.md",
                "frontmatter": {
                  "description": "Automatically create and execute a git commit using the first suggested commit message",
                  "category": "version-control-git",
                  "allowed-tools": "Bash(git *)"
                },
                "content": "# Create new fast commit task\n\nThis task uses the same logic as the commit task (.claude/commands/commit.md) but automatically selects the first suggested commit message without asking for confirmation.\n\n- Generate 3 commit message suggestions following the same format as the commit task\n- Automatically use the first suggestion without asking the user\n- Immediately run `git commit -m` with the first message\n- All other behaviors remain the same as the commit task (format, package names, staged files only)\n- Do NOT add Claude co-authorship footer to commits"
              },
              {
                "name": "/commit",
                "description": "Create well-formatted git commits with conventional commit messages and emoji",
                "path": "plugins/commands-version-control-git/commands/commit.md",
                "frontmatter": {
                  "description": "Create well-formatted git commits with conventional commit messages and emoji",
                  "category": "version-control-git",
                  "allowed-tools": "Bash, Read, Glob"
                },
                "content": "# Claude Command: Commit\n\nThis command helps you create well-formatted commits with conventional commit messages and emoji.\n\n## Usage\n\nTo create a commit, just type:\n```\n/commit\n```\n\nOr with options:\n```\n/commit --no-verify\n```\n\n## What This Command Does\n\n1. Unless specified with `--no-verify`, automatically runs pre-commit checks:\n   - Detect package manager (npm, pnpm, yarn, bun) and run appropriate commands\n   - Run lint/format checks if available\n   - Run build verification if build script exists\n   - Update documentation if generation script exists\n2. Checks which files are staged with `git status`\n3. If 0 files are staged, automatically adds all modified and new files with `git add`\n4. Performs a `git diff` to understand what changes are being committed\n5. Analyzes the diff to determine if multiple distinct logical changes are present\n6. If multiple distinct changes are detected, suggests breaking the commit into multiple smaller commits\n7. For each commit (or the single commit if not split), creates a commit message using emoji conventional commit format\n\n## Best Practices for Commits\n\n- **Verify before committing**: Ensure code is linted, builds correctly, and documentation is updated\n- **Atomic commits**: Each commit should contain related changes that serve a single purpose\n- **Split large changes**: If changes touch multiple concerns, split them into separate commits\n- **Conventional commit format**: Use the format `<type>: <description>` where type is one of:\n  - `feat`: A new feature\n  - `fix`: A bug fix\n  - `docs`: Documentation changes\n  - `style`: Code style changes (formatting, etc)\n  - `refactor`: Code changes that neither fix bugs nor add features\n  - `perf`: Performance improvements\n  - `test`: Adding or fixing tests\n  - `chore`: Changes to the build process, tools, etc.\n- **Present tense, imperative mood**: Write commit messages as commands (e.g., \"add feature\" not \"added feature\")\n- **Concise first line**: Keep the first line under 72 characters\n- **Emoji**: Each commit type is paired with an appropriate emoji:\n  -  `feat`: New feature"
              },
              {
                "name": "/create-pr",
                "description": "Create a new branch, commit changes, and submit a pull request with automatic commit splitting",
                "path": "plugins/commands-version-control-git/commands/create-pr.md",
                "frontmatter": {
                  "description": "Create a new branch, commit changes, and submit a pull request with automatic commit splitting",
                  "category": "version-control-git",
                  "allowed-tools": "Bash(git *), Bash(gh *), Bash(biome *)"
                },
                "content": "# Create Pull Request Command\n\nCreate a new branch, commit changes, and submit a pull request.\n\n## Behavior\n- Creates a new branch based on current changes\n- Formats modified files using Biome\n- Analyzes changes and automatically splits into logical commits when appropriate\n- Each commit focuses on a single logical change or feature\n- Creates descriptive commit messages for each logical unit\n- Pushes branch to remote\n- Creates pull request with proper summary and test plan\n\n## Guidelines for Automatic Commit Splitting\n- Split commits by feature, component, or concern\n- Keep related file changes together in the same commit\n- Separate refactoring from feature additions\n- Ensure each commit can be understood independently\n- Multiple unrelated changes should be split into separate commits"
              },
              {
                "name": "/create-pull-request",
                "description": "Guide for creating pull requests using GitHub CLI with proper templates and conventions",
                "path": "plugins/commands-version-control-git/commands/create-pull-request.md",
                "frontmatter": {
                  "description": "Guide for creating pull requests using GitHub CLI with proper templates and conventions",
                  "category": "version-control-git",
                  "allowed-tools": "Bash(gh *)"
                },
                "content": "# How to Create a Pull Request Using GitHub CLI\n\nThis guide explains how to create pull requests using GitHub CLI in our project.\n\n## Prerequisites\n\n1. Install GitHub CLI if you haven't already:\n\n   ```bash\n   # macOS\n   brew install gh\n\n   # Windows\n   winget install --id GitHub.cli\n\n   # Linux\n   # Follow instructions at https://github.com/cli/cli/blob/trunk/docs/install_linux.md\n   ```\n\n2. Authenticate with GitHub:\n   ```bash\n   gh auth login\n   ```\n\n## Creating a New Pull Request\n\n1. First, prepare your PR description following the template in @.github/pull_request_template.md\n\n2. Use the `gh pr create --draft` command to create a new pull request:\n\n   ```bash\n   # Basic command structure\n   gh pr create --draft --title \"(scope): Your descriptive title\" --body \"Your PR description\" --base main \n   ```\n\n   For more complex PR descriptions with proper formatting, use the `--body-file` option with the exact PR template structure:\n\n   ```bash\n   # Create PR with proper template structure\n   gh pr create --draft --title \"(scope): Your descriptive title\" --body-file .github/pull_request_template.md --base main\n   ```\n\n## Best Practices\n\n1. **PR Title Format**: Use conventional commit format with emojis\n\n   - Always include an appropriate emoji at the beginning of the title\n   - Use the actual emoji character (not the code representation like `:sparkles:`)\n   - Examples:\n     - `(supabase): Add staging remote configuration`\n     - `(auth): Fix login redirect issue`\n     - `(readme): Update installation instructions`\n\n2. **Description Template**: Always use our PR template structure from @.github/pull_request_template.md:\n\n3. **Template Accuracy**: Ensure your PR description precisely follows the template structure:\n\n   - Don't modify or rename the PR-Agent sections (`pr_agent:summary` and `pr_agent:walkthrough`)\n   - Keep all section headers exactly as they appear in the template"
              },
              {
                "name": "/create-worktrees",
                "description": "Manage git worktrees for open PRs and create new branch worktrees",
                "path": "plugins/commands-version-control-git/commands/create-worktrees.md",
                "frontmatter": {
                  "description": "Manage git worktrees for open PRs and create new branch worktrees",
                  "category": "version-control-git",
                  "allowed-tools": "Bash(git *), Bash(gh *)"
                },
                "content": "# Git Worktree Commands\n\n## Create Worktrees for All Open PRs\n\nThis command fetches all open pull requests using GitHub CLI, then creates a git worktree for each PR's branch in the `./tree/<BRANCH_NAME>` directory.\n\n```bash\n# Ensure GitHub CLI is installed and authenticated\ngh auth status || (echo \"Please run 'gh auth login' first\" && exit 1)\n\n# Create the tree directory if it doesn't exist\nmkdir -p ./tree\n\n# List all open PRs and create worktrees for each branch\ngh pr list --json headRefName --jq '.[].headRefName' | while read branch; do\n  # Handle branch names with slashes (like \"feature/foo\")\n  branch_path=\"./tree/${branch}\"\n  \n  # For branches with slashes, create the directory structure\n  if [[ \"$branch\" == */* ]]; then\n    dir_path=$(dirname \"$branch_path\")\n    mkdir -p \"$dir_path\"\n  fi\n\n  # Check if worktree already exists\n  if [ ! -d \"$branch_path\" ]; then\n    echo \"Creating worktree for $branch\"\n    git worktree add \"$branch_path\" \"$branch\"\n  else\n    echo \"Worktree for $branch already exists\"\n  fi\ndone\n\n# Display all created worktrees\necho \"\\nWorktree list:\"\ngit worktree list\n```\n\n### Example Output\n\n```\nCreating worktree for fix-bug-123\nHEAD is now at a1b2c3d Fix bug 123\nCreating worktree for feature/new-feature\nHEAD is now at e4f5g6h Add new feature\nWorktree for documentation-update already exists\n\nWorktree list:\n/path/to/repo                      abc1234 [main]\n/path/to/repo/tree/fix-bug-123     a1b2c3d [fix-bug-123]\n/path/to/repo/tree/feature/new-feature e4f5g6h [feature/new-feature]\n/path/to/repo/tree/documentation-update d5e6f7g [documentation-update]\n```\n\n### Cleanup Stale Worktrees (Optional)\n\nYou can add this to remove stale worktrees for branches that no longer exist:\n\n```bash\n# Get current branches\ncurrent_branches=$(git branch -a | grep -v HEAD | grep -v main | sed 's/^[ *]*//' | sed 's|remotes/origin/||' | sort | uniq)\n\n# Get existing worktrees (excluding main worktree)\nworktree_paths=$(git worktree list | tail -n +2 | awk '{print $1}')\n\nfor path in $worktree_paths; do\n  # Extract branch name from path\n  branch_name=$(basename \"$path\")\n  \n  # Skip special cases\n  if [[ \"$branch_name\" == \"main\" ]]; then\n    continue\n  fi\n  \n  # Check if branch still exists\n  if ! echo \"$current_branches\" | grep -q \"^$branch_name$\"; then\n    echo \"Removing stale worktree for deleted branch: $branch_name\"\n    git worktree remove --force \"$path\"\n  fi\ndone\n```\n\n## Create New Branch and Worktree\n\nThis interactive command creates a new git branch and sets up a worktree for it:\n\n```bash\n#!/bin/bash\n\n# Ensure we're in a git repository\nif ! git rev-parse --is-inside-work-tree > /dev/null 2>&1; then\n  echo \"Error: Not in a git repository\"\n  exit 1\nfi\n\n# Get the repository root\nrepo_root=$(git rev-parse --show-toplevel)\n\n# Prompt for branch name\nread -p \"Enter new branch name: \" branch_name\n\n# Validate branch name (basic validation)\nif [[ -z \"$branch_name\" ]]; then\n  echo \"Error: Branch name cannot be empty\"\n  exit 1\nfi\n\nif git show-ref --verify --quiet \"refs/heads/$branch_name\"; then\n  echo \"Warning: Branch '$branch_name' already exists\"\n  read -p \"Do you want to use the existing branch? (y/n): \" use_existing\n  if [[ \"$use_existing\" != \"y\" ]]; then\n    exit 1\n  fi\nfi\n\n# Create branch directory\nbranch_path=\"$repo_root/tree/$branch_name\"\n\n# Handle branch names with slashes (like \"feature/foo\")\nif [[ \"$branch_name\" == */* ]]; then\n  dir_path=$(dirname \"$branch_path\")\n  mkdir -p \"$dir_path\"\nfi\n\n# Make sure parent directory exists\nmkdir -p \"$(dirname \"$branch_path\")\"\n\n# Check if a worktree already exists\nif [ -d \"$branch_path\" ]; then\n  echo \"Error: Worktree directory already exists: $branch_path\"\n  exit 1\nfi\n\n# Create branch and worktree\nif git show-ref --verify --quiet \"refs/heads/$branch_name\"; then\n  # Branch exists, create worktree\n  echo \"Creating worktree for existing branch '$branch_name'...\"\n  git worktree add \"$branch_path\" \"$branch_name\"\nelse\n  # Create new branch and worktree\n  echo \"Creating new branch '$branch_name' and worktree...\"\n  git worktree add -b \"$branch_name\" \"$branch_path\"\nfi\n\necho \"Success! New worktree created at: $branch_path\"\necho \"To start working on this branch, run: cd $branch_path\"\n```\n\n### Example Usage\n\n```\n$ ./create-branch-worktree.sh\nEnter new branch name: feature/user-authentication\nCreating new branch 'feature/user-authentication' and worktree...\nPreparing worktree (creating new branch 'feature/user-authentication')\nHEAD is now at abc1234 Previous commit message\nSuccess! New worktree created at: /path/to/repo/tree/feature/user-authentication\nTo start working on this branch, run: cd /path/to/repo/tree/feature/user-authentication\n```\n\n### Creating a New Branch from a Different Base\n\nIf you want to start your branch from a different base (not the current HEAD), you can modify the script:\n\n```bash\nread -p \"Enter new branch name: \" branch_name\nread -p \"Enter base branch/commit (default: HEAD): \" base_commit\nbase_commit=${base_commit:-HEAD}\n\n# Then use the specified base when creating the worktree\ngit worktree add -b \"$branch_name\" \"$branch_path\" \"$base_commit\"\n```\n\nThis will allow you to specify any commit, tag, or branch name as the starting point for your new branch."
              },
              {
                "name": "/fix-github-issue",
                "description": "Analyze and fix a GitHub issue with comprehensive testing and verification",
                "path": "plugins/commands-version-control-git/commands/fix-github-issue.md",
                "frontmatter": {
                  "description": "Analyze and fix a GitHub issue with comprehensive testing and verification",
                  "category": "version-control-git",
                  "argument-hint": "<issue_number>",
                  "allowed-tools": "Bash(gh *), Read, Edit, Write, Bash(git *)"
                },
                "content": "Please analyze and fix the GitHub issue: $ARGUMENTS.\n\nFollow these steps:\n\n1. Use `gh issue view` to get the issue details\n2. Understand the problem described in the issue\n3. Search the codebase for relevant files\n4. Implement the necessary changes to fix the issue\n5. Write and run tests to verify the fix\n6. Ensure code passes linting and type checking\n7. Create a descriptive commit message\n\nRemember to use the GitHub CLI (`gh`) for all GitHub-related tasks.\n\n---"
              },
              {
                "name": "/fix-issue",
                "description": "Fix a specific issue or problem with the given identifier or description",
                "path": "plugins/commands-version-control-git/commands/fix-issue.md",
                "frontmatter": {
                  "description": "Fix a specific issue or problem with the given identifier or description",
                  "category": "version-control-git",
                  "argument-hint": "<issue_identifier>"
                },
                "content": "Fix issue $ARGUMENTS"
              },
              {
                "name": "/fix-pr",
                "description": "Fetch unresolved comments for current branch's PR and fix them",
                "path": "plugins/commands-version-control-git/commands/fix-pr.md",
                "frontmatter": {
                  "description": "Fetch unresolved comments for current branch's PR and fix them",
                  "category": "version-control-git",
                  "allowed-tools": "Bash(gh *), Read, Edit"
                },
                "content": "Fetch unresolved comments for this branch's PR, then fix them"
              },
              {
                "name": "/husky",
                "description": "Verify repository is in working state by running CI checks and fixing issues",
                "path": "plugins/commands-version-control-git/commands/husky.md",
                "frontmatter": {
                  "description": "Verify repository is in working state by running CI checks and fixing issues",
                  "category": "version-control-git",
                  "allowed-tools": "Bash, Read, Edit"
                },
                "content": "## Summary\n\nVerify the repository is in a working state by running appropriate CI checks and fixing any issues found.\n\n## Process\n\n1. **Detect Package Manager**:\n   - Check for package manager files: package-lock.json (npm), pnpm-lock.yaml (pnpm), yarn.lock (yarn), bun.lockb (bun)\n   - Check for other build systems: Makefile, Cargo.toml, go.mod, requirements.txt, etc.\n\n2. **Update Dependencies**:\n   - npm: `npm install`\n   - pnpm: `pnpm install`\n   - yarn: `yarn install`\n   - bun: `bun install`\n   - Other: Run appropriate dependency installation\n\n3. **Run Linting**:\n   - Check package.json scripts for lint command\n   - Common patterns: `lint`, `eslint`, `check`, `format`\n   - Fix any linting issues found\n\n4. **Run Type Checking** (if applicable):\n   - TypeScript: `tsc` or check for `typecheck` script\n   - Other typed languages: run appropriate type checker\n\n5. **Run Build**:\n   - Check for build scripts in package.json or build configuration\n   - Common patterns: `build`, `compile`, `dist`\n   - Fix any build errors\n\n6. **Run Tests**:\n   - Check for test scripts: `test`, `test:unit`, `test:coverage`\n   - Source .env file if it exists before running tests\n   - Fix any failing tests\n\n7. **Additional Checks**:\n   - Check if package.json needs sorting (if sort-package-json is available)\n   - Run any other project-specific checks found in CI configuration\n\n8. **Stage Changes**:\n   - Review changes with `git status`\n   - Add fixed files with `git add`\n   - Exclude any git submodules or vendor directories\n\n## Important Notes:\n\n- Do NOT continue to the next step until the current command succeeds\n- Fix any issues found before proceeding\n- If a command doesn't exist, check for alternatives or skip if not applicable\n- Print a summary with checkmarks () for passed steps at the end\n\n## Protocol when something breaks\n\nTake the following steps if CI breaks\n\n### 1. Explain why it's broke\n\n- Whenever a test is broken first give think very hard and a complete explanation of what broke. Cite source code and logs that support your thesis.\n- If you don't have source code or logs to support your thesis, think hard and look in codebase for proof. \n- Add console logs if it will help you confirm your thesis or find out why it's broke\n- If you don"
              },
              {
                "name": "/pr-review",
                "description": "Conduct comprehensive PR review from multiple perspectives (PM, Developer, QA, Security)",
                "path": "plugins/commands-version-control-git/commands/pr-review.md",
                "frontmatter": {
                  "description": "Conduct comprehensive PR review from multiple perspectives (PM, Developer, QA, Security)",
                  "category": "version-control-git",
                  "argument-hint": "<pr_link_or_number>",
                  "allowed-tools": "Bash(gh *), Read"
                },
                "content": "# PR Review\n\n**PR Link/Number**: $ARGUMENTS\n\n> **Instructions**: Execute each task in the order given to conduct a thorough code review.  Update GitHub with this review.\n> **Important**: The future is nowany improvements or \"future\" recommendations must be addressed **immediately**.\n\n---\n\n## Task 1: Product Manager Review\n\n**Objective**: Assess from a product management perspective, focusing on:\n- **Business Value**: Does this PR clearly advance our core product goals and deliver immediate ROI?\n- **User Experience**: Is the change intuitive and delightful for users right now? If not, make fixes immediately.\n- **Strategic Alignment**: Does the PR align with current (and long-term, i.e., now) strategic objectives?\n\n**Action**: Provide clear directives on how to ensure maximum user and business impact. All \"future\" suggestions must be implemented now.\n\n---\n\n## Task 2: Developer Review\n\n**Objective**: Evaluate the code thoroughly from a senior lead engineer perspective:\n1. **Code Quality & Maintainability**: Is the code structured for readability and easy maintenance? If not, refactor now.\n2. **Performance & Scalability**: Will these changes operate efficiently at scale? If not, optimize immediately.\n3. **Best Practices & Standards**: Note any deviation from coding standards and correct it now.\n\n**Action**: Leave a concise yet complete review comment, ensuring all improvements happen immediatelyno deferrals.\n\n---\n\n## Task 3: Quality Engineer Review\n\n**Objective**: Verify the overall quality, testing strategy, and reliability of the solution:\n1. **Test Coverage**: Are there sufficient tests (unit, integration, E2E)? If not, add them now.\n2. **Potential Bugs & Edge Cases**: Have all edge cases been considered? If not, address them immediately.\n3. **Regression Risk**: Confirm changes don't undermine existing functionality. If risk is identified, mitigate now with additional checks or tests.\n\n**Action**: Provide a detailed QA assessment, insisting any \"future\" improvements be completed right away.\n\n---\n\n## Task 4: Security Engineer Review\n\n**Objective**: Ensure robust security practices and compliance:\n1. **Vulnerabilities**:"
              },
              {
                "name": "/update-branch-name",
                "description": "Update current git branch name based on analysis of changes made",
                "path": "plugins/commands-version-control-git/commands/update-branch-name.md",
                "frontmatter": {
                  "description": "Update current git branch name based on analysis of changes made",
                  "category": "version-control-git",
                  "allowed-tools": "Bash(git *)"
                },
                "content": "# Update Branch Name\n\nFollow these steps to update the current branch name:\n\n1. Check differences between current branch and main branch HEAD using `git diff main...HEAD`\n2. Analyze the changed files to understand what work is being done\n3. Determine an appropriate descriptive branch name based on the changes\n4. Update the current branch name using `git branch -m [new-branch-name]`\n5. Verify the branch name was updated with `git branch`"
              }
            ],
            "skills": []
          },
          {
            "name": "commands-workflow-orchestration",
            "description": "Commands for orchestrating complex workflows",
            "source": "./plugins/commands-workflow-orchestration",
            "category": "commands",
            "version": "1.0.0",
            "author": {
              "name": "BuildWithClaude Community",
              "url": "https://github.com/davepoon/buildwithclaude"
            },
            "install_commands": [
              "/plugin marketplace add davepoon/buildwithclaude",
              "/plugin install commands-workflow-orchestration@buildwithclaude"
            ],
            "signals": {
              "stars": 2229,
              "forks": 239,
              "pushed_at": "2026-01-12T10:19:01Z",
              "created_at": "2025-07-25T02:26:45Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/find",
                "description": "Search and locate tasks across all orchestrations using various criteria.",
                "path": "plugins/commands-workflow-orchestration/commands/find.md",
                "frontmatter": {
                  "description": "Search and locate tasks across all orchestrations using various criteria.",
                  "category": "workflow-orchestration",
                  "allowed-tools": "Read"
                },
                "content": "# Task Find Command\n\nSearch and locate tasks across all orchestrations using various criteria.\n\n## Usage\n\n```\n/task-find [search-term] [options]\n```\n\n## Description\n\nPowerful search functionality to quickly locate tasks by ID, content, status, dependencies, or any other criteria. Supports regex, fuzzy matching, and complex queries.\n\n## Basic Search\n\n### By Task ID\n```\n/task-find TASK-001\n/task-find TASK-*\n```\n\n### By Title/Content\n```\n/task-find \"authentication\"\n/task-find \"payment processing\"\n```\n\n### By Status\n```\n/task-find --status in_progress\n/task-find --status qa,completed\n```\n\n## Advanced Search\n\n### Regular Expression\n```\n/task-find --regex \"JWT|OAuth\"\n/task-find --regex \"TASK-0[0-9]{2}\"\n```\n\n### Fuzzy Search\n```\n/task-find --fuzzy \"autentication\"  # finds \"authentication\"\n/task-find --fuzzy \"paymnt\"         # finds \"payment\"\n```\n\n### Multiple Criteria\n```\n/task-find --status todos --priority high --type feature\n/task-find --agent dev-backend --created-after yesterday\n```\n\n## Search Operators\n\n### Boolean Operators\n```\n/task-find \"auth AND login\"\n/task-find \"payment OR billing\"\n/task-find \"security NOT test\"\n```\n\n### Field-Specific Search\n```\n/task-find title:\"user authentication\"\n/task-find description:\"security vulnerability\"\n/task-find agent:dev-frontend\n/task-find blocks:TASK-001\n```\n\n### Date Ranges\n```\n/task-find --created \"2024-03-10..2024-03-15\"\n/task-find --modified \"last 3 days\"\n/task-find --completed \"this week\"\n```\n\n## Output Formats\n\n### Default List View\n```\nFound 3 tasks matching \"authentication\":\n\nTASK-001: Implement JWT authentication\n  Status: in_progress | Agent: dev-frontend | Created: 2024-03-15\n  Location: /task-orchestration/03_15_2024/auth_system/tasks/in_progress/\n\nTASK-004: Add OAuth2 authentication  \n  Status: todos | Priority: high | Blocked by: TASK-001\n  Location: /task-orchestration/03_15_2024/auth_system/tasks/todos/\n\nTASK-007: Authentication middleware tests\n  Status: todos | Type: test | Depends on: TASK-001\n  Location: /task-orchestration/03_15_2024/auth_system/tasks/todos/\n```\n\n### Detailed View\n```\n/task-find TASK-001 --detailed\n```\nShows full task content including description, implementation notes, and history.\n\n### Tree View\n```\n/task-find --tree --root TASK-001\n```\nShows task and all its dependencies in tree format.\n\n## Filtering Options\n\n### By Orchestration\n```\n/task-find --orchestration \"03_15_2024/payment_system\"\n/task-find --orchestration \"*/auth_*\"\n```\n\n### By Properties\n```\n/task-find --has-dependencies\n/task-find --no-dependencies\n/task-find --blocking-others\n/task-find --effort \">4h\"\n```\n\n### By Relationships\n```\n/task-find --depends-on TASK-001\n/task-find --blocks TASK-005\n/task-find --related-to TASK-003\n```\n\n## Special Searches\n\n### Find Circular Dependencies\n```\n/task-find --circular-deps\n```\n\n### Find Orphaned Tasks\n```\n/task-find --orphaned\n```\n\n### Find Duplicate Tasks\n```\n/task-find --duplicates\n```\n\n### Find Stale Tasks\n```\n/task-find --stale --days 7\n```\n\n## Quick Filters\n\n### Ready to Start\n```\n/task-find --ready\n```\nShows todos with no blocking dependencies.\n\n### Critical Path\n```\n/task-find --critical-path\n```\nShows tasks on the critical path.\n\n### High Impact\n```\n/task-find --high-impact\n```\nShows tasks blocking multiple others.\n\n## Export Options\n\n### Copy Results\n```\n/task-find \"auth\" --copy\n```\nCopies results to clipboard.\n\n### Export Paths\n```\n/task-find --status todos --export paths\n```\nExports file paths for batch operations.\n\n### Generate Report\n```\n/task-find --report\n```\nCreates detailed search report.\n\n## Examples\n\n### Example 1: Find Work for Agent\n```\n/task-find --status todos --suitable-for dev-frontend --ready\n```\n\n### Example 2: Find Blocking Issues\n```\n/task-find --status on_hold --show-blockers\n```\n\n### Example 3: Security Audit\n```\n/task-find \"security OR auth OR permission\" --type \"feature,bugfix\"\n```\n\n### Example 4: Sprint Planning\n```\n/task-find --status todos --effort \"<4h\" --no-dependencies\n```\n\n## Search Shortcuts\n\n### Recent Tasks\n```\n/task-find --recent 10\n```\n\n### My Tasks\n```\n/task-find --mine  # Uses current agent context\n```\n\n### Modified Today\n```\n/task-find --modified today\n```\n\n## Complex Queries\n\n### Compound Search\n```\n/task-find '(title:\"auth\" OR description:\"security\") AND status:todos AND -blocks:*'\n```\n\n### Saved Searches\n```\n/task-find --save \"security-todos\"\n/task-find --load \"security-todos\"\n```\n\n## Performance Tips\n\n1. **Use Indexes**: Status and ID searches are fastest\n2. **Narrow Scope**: Specify orchestration when possible\n3. **Cache Results**: Use `--cache` for repeated searches\n4. **Limit Results**: Use `--limit 20` for large result sets\n\n## Integration\n\n### With Other Commands\n```\n/task-find \"payment\" --status todos | /task-move in_progress\n```\n\n### Batch Operations\n```\n/task-find --filter \"priority:low\" | /task-update priority:medium\n```\n\n## Notes\n\n- Searches across all task files in task-orchestration/\n- Case-insensitive by default (use --case for case-sensitive)\n- Results sorted by relevance unless specified otherwise\n- Supports command chaining with pipe operator\n- Search index updated automatically on file changes"
              },
              {
                "name": "/log",
                "description": "Log work from orchestrated tasks to external project management tools like Linear, Obsidian, Jira, or GitHub Issues.",
                "path": "plugins/commands-workflow-orchestration/commands/log.md",
                "frontmatter": {
                  "description": "Log work from orchestrated tasks to external project management tools like Linear, Obsidian, Jira, or GitHub Issues.",
                  "category": "workflow-orchestration",
                  "allowed-tools": "Read"
                },
                "content": "# Orchestration Log Command\n\nLog work from orchestrated tasks to external project management tools like Linear, Obsidian, Jira, or GitHub Issues.\n\n## Usage\n\n```\n/orchestration/log [TASK-ID] [options]\n```\n\n## Description\n\nAutomatically creates work logs in your connected project management tools or knowledge bases, transferring task completion data, time spent, and progress notes to keep external systems synchronized.\n\n## Basic Commands\n\n### Log Current Task\n```\n/orchestration/log\n```\nLogs the currently in-progress task to available tools.\n\n### Log Specific Task\n```\n/orchestration/log TASK-003\n```\nLogs a specific task's work.\n\n### Choose Destination\n```\n/orchestration/log TASK-003 --choose\n```\nManually select where to log the work.\n\n## Destination Selection\n\nWhen multiple tools are available or no obvious connection exists:\n\n```\nWhere would you like to log this work?\n\nAvailable destinations:\n1. Linear (ENG-1234 detected)\n2. Obsidian (Daily Note)\n3. Obsidian (Project: Authentication)\n4. GitHub Issue (#123)\n5. None - Skip logging\n\nChoose destination [1-5]: \n```\n\n## Obsidian Integration\n\n### Daily Note Logging\n```\n/orchestration/log --obsidian-daily\n```\nAppends to today's daily note:\n\n```markdown\n## Work Log - 15:30\n\n### TASK-003: JWT Implementation \n\n**Time Spent**: 4.5 hours (10:00 - 14:30)\n**Status**: Completed  QA\n\n**What I did:**\n- Implemented JWT token validation middleware\n- Added refresh token logic  \n- Created comprehensive test suite\n- Fixed edge case with token expiration\n\n**Code Stats:**\n- Files: 8 modified\n- Lines: +245 -23\n- Coverage: 95%\n\n**Related Tasks:**\n- Next: [[TASK-005]] - User Profile API\n- Blocked: [[TASK-007]] - Waiting for this\n\n**Commits:**\n- `abc123`: feat(auth): implement JWT validation\n- `def456`: test(auth): add validation tests\n\n#tasks/completed #project/authentication\n```\n\n### Project Note Logging\n```\n/orchestration/log --obsidian-project \"Authentication System\"\n```\nCreates or appends to project-specific note.\n\n### Custom Obsidian Location\n```\n/orchestration/log --obsidian-path \"Projects/Sprint 24/Work Log\"\n```\n\n## Linear Integration\n```\n/orchestration/log TASK-003 --linear-issue ENG-1234\n```\nCreates work log comment in Linear issue.\n\n## Smart Detection\n\nThe system detects available destinations:\n\n```\nAnalyzing task context...\n\nFound connections:\n Linear: ENG-1234 (from branch name)\n Obsidian: Project note exists\n GitHub: No issue reference\n Jira: Not connected\n\nSuggested: Linear ENG-1234\nUse suggestion? [Y/n/choose different]\n```\n\n## Work Log Formats\n\n### Obsidian Format\n```markdown\n##  Task: TASK-003 - JWT Implementation\n\n### Summary\n- **Status**:  Completed  \n- **Duration**: 4h 30m\n- **Date**: 2024-03-15\n\n### Progress Details\n- [x] Token structure design\n- [x] Validation middleware\n- [x] Refresh mechanism\n- [x] Test coverage\n\n### Technical Notes\n- Used RS256 algorithm for signing\n- Tokens expire after 15 minutes\n- Refresh tokens last 7 days\n\n### Links\n- Linear: [ENG-1234](linear://issue/ENG-1234)\n- PR: [#456](github.com/...)\n- Docs: [[JWT Implementation Guide]]\n\n### Next Actions\n- [ ] Code review feedback\n- [ ] Deploy to staging\n- [ ] Update API documentation\n\n---\n*Logged via Task Orchestration at 15:30*\n```\n\n### Linear Format\n```\nWork log comment in Linear with task details, time tracking, and progress updates.\n```\n\n## Multiple Destination Logging\n\n```\n/orchestration/log TASK-003 --multi\n\nSelect all destinations for logging:\n[x] Linear - ENG-1234\n[x] Obsidian - Daily Note\n[ ] Obsidian - Project Note\n[ ] GitHub - Create new issue\n\nPress Enter to confirm, Space to toggle\n```\n\n## Batch Operations\n\n### Daily Summary to Obsidian\n```\n/orchestration/log --daily-summary --obsidian\n\nCreates summary in daily note:\n\n## Work Summary - 2024-03-15\n\n### Completed Tasks\n- [[TASK-003]]: JWT Implementation (4.5h) \n- [[TASK-008]]: Login UI Updates (2h) \n\n### In Progress  \n- [[TASK-005]]: User Profile API (1.5h) \n\n### Total Time: 8 hours\n\n### Key Achievements\n- Authentication system core complete\n- All tests passing\n- Ready for code review\n\n### Tomorrow's Focus\n- Complete user profile endpoints\n- Start OAuth integration\n```\n\n### Weekly Report\n```\n/orchestration/log --weekly --obsidian-path \"Weekly Reviews/Week 11\"\n```\n\n## Templates\n\n### Configure Obsidian Template\n```yaml\nobsidian_template:\n  daily_note:\n    heading: \"## Work Log - {time}\"\n    include_stats: true\n    add_tags: true\n    link_tasks: true\n  \n  project_note:\n    create_if_missing: true\n    append_to_section: \"## Task Progress\"\n    include_commits: true\n```\n\n### Configure Linear Template\n```yaml\nlinear_template:\n  include_time: true\n  update_status: true\n  add_labels: [\"from-orchestration\"]\n```\n\n## Interactive Mode\n\n```\n/orchestration/log --interactive\n\nTask: TASK-003 - JWT Implementation\nStatus: Completed\nTime: 4.5 hours\n\nWhere to log? (Space to select, Enter to confirm)\n> [x] Linear (ENG-1234)\n> [x] Obsidian Daily Note\n> [ ] Obsidian Project Note\n> [ ] New GitHub Issue\n\nAdd custom notes? [y/N]: y\n> Implemented using RS256, ready for review\n\nLogging to 2 destinations...\n Linear: Comment added to ENG-1234\n Obsidian: Added to daily note\n\nView logs? [y/N]: \n```\n\n## Examples\n\n### Example 1: End of Day Logging\n```\n/orchestration/log --eod\n\nEnd of Day Summary:\n- 3 tasks worked on\n- 7.5 hours logged\n- 2 completed, 1 in progress\n\nLog to:\n1. Obsidian Daily Note (recommended)\n2. Linear (update all 3 issues)\n3. Both\n4. Skip\n\nChoice [1]: 1\n\n Daily work log created in Obsidian\n```\n\n### Example 2: Sprint Review\n```\n/orchestration/log --sprint-review --week 11\n\nGathering week 11 data...\n- 15 tasks completed\n- 3 in progress\n- 52 hours logged\n\nCreate sprint review in:\n1. Obsidian - \"Sprint Reviews/Sprint 24\"\n2. Linear - Sprint 24 cycle\n3. Both\n\nChoice [3]: 3\n\n Sprint review created in both systems\n```\n\n### Example 3: No Connection Found\n```\n/orchestration/log TASK-009\n\nNo automatic destination found for TASK-009.\n\nWhere would you like to log this?\n1. Obsidian - Daily Note\n2. Obsidian - Create Project Note\n3. Linear - Search for issue\n4. GitHub - Create new issue  \n5. Skip logging\n\nChoice: 2\n\nEnter project name: Security Audit\n Created \"Security Audit\" note with work log\n```\n\n## Configuration\n\n### Default Destinations\n```yaml\nlog_defaults:\n  no_connection: \"ask\"  # ask|obsidian-daily|skip\n  multi_connection: \"ask\"  # ask|all|first\n  \n  obsidian:\n    default_location: \"daily\"  # daily|project|custom\n    project_folder: \"Projects\"\n    daily_folder: \"Daily Notes\"\n  \n  linear:\n    auto_update_status: true\n    include_commits: true\n```\n\n## Best Practices\n\n1. **Set Preferences**: Configure default destinations\n2. **Link Early**: Connect tasks to PM tools when creating\n3. **Use Daily Notes**: Great for personal tracking\n4. **Project Notes**: Better for team collaboration\n5. **Regular Syncs**: Don't let logs pile up\n\n## Notes\n\n- Respects MCP connections and permissions\n- Obsidian logs create backlinks automatically\n- Supports multiple simultaneous destinations\n- Preserves formatting across systems\n- Can be automated with task status changes"
              },
              {
                "name": "/move",
                "description": "Move tasks between status folders following the task management protocol.",
                "path": "plugins/commands-workflow-orchestration/commands/move.md",
                "frontmatter": {
                  "description": "Move tasks between status folders following the task management protocol.",
                  "category": "workflow-orchestration"
                },
                "content": "# Task Move Command\n\nMove tasks between status folders following the task management protocol.\n\n## Usage\n\n```\n/task-move TASK-ID new-status [reason]\n```\n\n## Description\n\nUpdates task status by moving files between status folders and updating tracking information. Follows all protocol rules including validation and audit trails.\n\n## Basic Commands\n\n### Start Working on a Task\n```\n/task-move TASK-001 in_progress\n```\nMoves from todos  in_progress\n\n### Complete Implementation\n```\n/task-move TASK-001 qa \"Implementation complete, ready for testing\"\n```\nMoves from in_progress  qa\n\n### Task Passed QA\n```\n/task-move TASK-001 completed \"All tests passed\"\n```\nMoves from qa  completed\n\n### Block a Task\n```\n/task-move TASK-004 on_hold \"Waiting for TASK-001 API completion\"\n```\nMoves to on_hold with reason\n\n### Unblock a Task\n```\n/task-move TASK-004 todos \"Dependencies resolved\"\n```\nMoves from on_hold  todos\n\n### Failed QA\n```\n/task-move TASK-001 in_progress \"Failed integration test - fixing null pointer\"\n```\nMoves from qa  in_progress\n\n## Bulk Operations\n\n### Move Multiple Tasks\n```\n/task-move TASK-001,TASK-002,TASK-003 in_progress\n```\n\n### Move by Filter\n```\n/task-move --filter \"priority:high status:todos\" in_progress\n```\n\n### Move with Pattern\n```\n/task-move TASK-00* qa \"Batch testing ready\"\n```\n\n## Validation Rules\n\nThe command enforces:\n1. **Valid Transitions**: Only allowed status changes\n2. **One Task Per Agent**: Warns if agent has task in_progress\n3. **Dependency Check**: Warns if dependencies not met\n4. **File Existence**: Verifies task exists before moving\n\n## Status Transition Map\n\n```\ntodos  in_progress  qa  completed\n                                \n   on_hold \n                  \n                todos/in_progress\n```\n\n## Options\n\n### Force Move\n```\n/task-move TASK-001 completed --force\n```\nBypasses validation (use with caution)\n\n### Dry Run\n```\n/task-move TASK-001 qa --dry-run\n```\nShows what would happen without executing\n\n### With Assignment\n```\n/task-move TASK-001 in_progress --assign dev-frontend\n```\nAssigns task to specific agent\n\n### With Time Estimate\n```\n/task-move TASK-001 in_progress --estimate 4h\n```\nUpdates time estimate when starting\n\n## Error Handling\n\n### Task Not Found\n```\nError: TASK-999 not found in any status folder\nSuggestion: Use /task-status to see available tasks\n```\n\n### Invalid Transition\n```\nError: Cannot move from 'completed' to 'todos'\nValid transitions from completed: None (terminal state)\n```\n\n### Agent Conflict\n```\nWarning: dev-frontend already has TASK-002 in progress\nContinue? (y/n)\n```\n\n### Dependency Block\n```\nWarning: TASK-004 depends on TASK-001 (currently in_progress)\nMoving to on_hold instead? (y/n)\n```\n\n## Automation\n\n### Auto-move on Completion\n```\n/task-move TASK-001 --auto-progress\n```\nAutomatically moves to next status when conditions met\n\n### Scheduled Moves\n```\n/task-move TASK-005 in_progress --at \"tomorrow 9am\"\n```\n\n### Conditional Moves\n```\n/task-move TASK-007 qa --when \"TASK-006 completed\"\n```\n\n## Examples\n\n### Example 1: Developer Workflow\n```\n# Start work\n/task-move TASK-001 in_progress\n\n# Complete and test\n/task-move TASK-001 qa \"Implementation done, tests passing\"\n\n# After review\n/task-move TASK-001 completed \"Code review approved\"\n```\n\n### Example 2: Handling Blocks\n```\n# Block due to dependency\n/task-move TASK-004 on_hold \"Waiting for auth API from TASK-001\"\n\n# Unblock when ready\n/task-move TASK-004 todos \"TASK-001 now in QA, API available\"\n```\n\n### Example 3: QA Workflow\n```\n# QA picks up task\n/task-move TASK-001 qa --assign qa-engineer\n\n# Found issues\n/task-move TASK-001 in_progress \"Bug: handling empty responses\"\n\n# Fixed and retesting\n/task-move TASK-001 qa \"Bug fixed, ready for retest\"\n```\n\n## Status Update Details\n\nEach move updates:\n1. **File Location**: Physical file movement\n2. **Status Tracker**: TASK-STATUS-TRACKER.yaml entry\n3. **Task Metadata**: Status field in task file\n4. **Execution Tracker**: Overall progress metrics\n\n## Best Practices\n\n1. **Always Provide Reasons**: Especially for blocks and failures\n2. **Check Dependencies**: Before moving to in_progress\n3. **Update Estimates**: When starting work\n4. **Clear Block Reasons**: Help others understand delays\n\n## Integration\n\n- Use after `/task-status` to see available tasks\n- Updates reflected in `/task-report`\n- Triggers notifications if configured\n- Logs all moves for audit trail\n\n## Notes\n\n- Moves are atomic - either fully complete or rolled back\n- Status history is permanent and cannot be edited\n- Timestamp uses current time in ISO-8601 format\n- Agent name is automatically detected from context"
              },
              {
                "name": "/remove",
                "description": "Safely remove a task from the orchestration system, updating all references and dependencies.",
                "path": "plugins/commands-workflow-orchestration/commands/remove.md",
                "frontmatter": {
                  "description": "Safely remove a task from the orchestration system, updating all references and dependencies.",
                  "category": "workflow-orchestration",
                  "allowed-tools": "Bash(git *), Read"
                },
                "content": "# Orchestration Remove Command\n\nSafely remove a task from the orchestration system, updating all references and dependencies.\n\n## Usage\n\n```\n/orchestration/remove TASK-ID [options]\n```\n\n## Description\n\nRemoves a task completely from the orchestration system, handling all dependencies, references, and related documentation. Provides impact analysis before removal and ensures system consistency.\n\n## Basic Commands\n\n### Remove Single Task\n```\n/orchestration/remove TASK-003\n```\nShows impact analysis and confirms before removal.\n\n### Force Remove\n```\n/orchestration/remove TASK-003 --force\n```\nSkips confirmation (use with caution).\n\n### Dry Run\n```\n/orchestration/remove TASK-003 --dry-run\n```\nShows what would be affected without making changes.\n\n## Impact Analysis\n\nBefore removal, the system analyzes:\n\n```\nTask Removal Impact Analysis: TASK-003\n======================================\n\nTask Details:\n- Title: JWT token validation\n- Status: in_progress\n- Location: /tasks/in_progress/TASK-003-jwt-validation.md\n\nDependencies:\n- Blocks: TASK-005 (User profile API)\n- Blocks: TASK-007 (Session management)\n- Depends on: None\n\nReferences Found:\n- MASTER-COORDINATION.md: Line 45 (Wave 1 tasks)\n- EXECUTION-TRACKER.md: Active task count\n- TASK-005: Lists TASK-003 as dependency\n- TASK-007: Lists TASK-003 as dependency\n\nGit History:\n- 2 commits reference this task\n- Branch: feature/jwt-auth\n\nWarning: This task has downstream dependencies!\n\nProceed with removal? [y/N]\n```\n\n## Removal Process\n\n### 1. Update Dependent Tasks\n```\nUpdating dependent tasks:\n- TASK-005: Removing dependency on TASK-003\n  New status: Ready to start (no blockers)\n  \n- TASK-007: Removing dependency on TASK-003\n  Warning: Still blocked by TASK-009\n```\n\n### 2. Update Tracking Files\n```yaml\n# TASK-STATUS-TRACKER.yaml updates:\nstatus_history:\n  TASK-003: [REMOVED - archived to .removed/]\n  \ncurrent_status_summary:\n  in_progress: [TASK-003 removed from list]\n\nremoval_log:\n  - task_id: TASK-003\n    removed_at: \"2024-03-15T16:00:00Z\"\n    removed_by: \"user\"\n    reason: \"Requirement changed\"\n    final_status: \"in_progress\"\n```\n\n### 3. Update Coordination Documents\n```\nUpdates applied:\n MASTER-COORDINATION.md - Removed from Wave 1\n EXECUTION-TRACKER.md - Updated task counts\n TASK-DEPENDENCIES.yaml - Removed all references\n Dependency graph regenerated\n```\n\n## Options\n\n### Archive Instead of Delete\n```\n/orchestration/remove TASK-003 --archive\n```\nMoves to `.removed/` directory instead of deleting.\n\n### Remove Multiple Tasks\n```\n/orchestration/remove TASK-003,TASK-005,TASK-008\n```\nAnalyzes and removes multiple tasks in dependency order.\n\n### Remove by Pattern\n```\n/orchestration/remove --pattern \"oauth-*\"\n```\nRemoves all tasks matching pattern.\n\n### Cascade Removal\n```\n/orchestration/remove TASK-003 --cascade\n```\nAlso removes tasks that depend on this task.\n\n## Handling Special Cases\n\n### Task with Commits\n```\nWarning: TASK-003 has associated commits:\n- abc123: \"feat(auth): implement JWT validation\"\n- def456: \"test(auth): add JWT tests\"\n\nOptions:\n[1] Keep commits, remove task only\n[2] Add removal note to commit messages\n[3] Cancel removal\n```\n\n### Task in QA/Completed\n```\nWarning: TASK-003 is in 'completed' status\n\nThis usually means work was done. Consider:\n[1] Archive task instead of removing\n[2] Document why it's being removed\n[3] Check if commits should be reverted\n```\n\n### Critical Path Task\n```\nERROR: TASK-003 is on the critical path!\n\nRemoving this task will impact project timeline:\n- Current completion: 5 days\n- After removal: 7 days (due to replanning)\n\nOverride with --force-critical\n```\n\n## Removal Strategies\n\n### Soft Remove (Default)\n```\n/orchestration/remove TASK-003\n```\n- Archives task file\n- Updates all references\n- Logs removal reason\n- Preserves git history\n\n### Hard Remove\n```\n/orchestration/remove TASK-003 --hard\n```\n- Deletes task file permanently\n- Removes all traces\n- Updates git tracking\n- No recovery possible\n\n### Replace Remove\n```\n/orchestration/remove TASK-003 --replace-with TASK-015\n```\n- Transfers dependencies to new task\n- Updates all references\n- Maintains continuity\n\n## Undo Capabilities\n\n### Recent Removal\n```\n/orchestration/remove --undo-last\n```\nRestores the most recently removed task.\n\n### Restore from Archive\n```\n/orchestration/remove --restore TASK-003\n```\nRestores archived task with all references.\n\n## Examples\n\n### Example 1: Obsolete Feature\n```\n/orchestration/remove TASK-008 --reason \"Feature descoped\"\n\nRemoving TASK-008: OAuth provider integration\n- No dependencies\n- No commits yet\n- Safe to remove\n\nTask removed successfully.\n```\n\n### Example 2: Duplicate Task\n```\n/orchestration/remove TASK-012 --replace-with TASK-005\n\nRemoving duplicate: TASK-012\nTransferring to: TASK-005\n- Dependencies transferred: 2\n- References updated: 4\n\nDuplicate removed, TASK-005 updated.\n```\n\n### Example 3: Changed Requirements\n```\n/orchestration/remove TASK-003,TASK-004,TASK-005 --reason \"Auth system redesigned\"\n\nRemoving authentication task group:\n- 3 tasks to remove\n- 2 have commits (will archive)\n- 5 dependent tasks need updates\n\nProceed? [y/N]\n```\n\n## Audit Trail\n\nAll removals are logged:\n```yaml\n# .orchestration-audit.yaml\nremovals:\n  - task_id: TASK-003\n    removed_at: \"2024-03-15T16:00:00Z\"\n    removed_by: \"user-id\"\n    reason: \"Requirement changed\"\n    status_at_removal: \"in_progress\"\n    dependencies_affected: [\"TASK-005\", \"TASK-007\"]\n    commits_preserved: [\"abc123\", \"def456\"]\n    archived_to: \".removed/2024-03-15/TASK-003/\"\n```\n\n## Best Practices\n\n1. **Always Check Dependencies**: Review impact before removing\n2. **Document Reason**: Provide clear removal reason\n3. **Archive Important Work**: Use --archive for completed work\n4. **Update Team**: Notify about critical removals\n5. **Review Commits**: Check if code needs reverting\n\n## Integration\n\n### With Other Commands\n```\n# First check status\n/orchestration/status --task TASK-003\n\n# Then remove if needed\n/orchestration/remove TASK-003\n```\n\n### Bulk Operations\n```\n# Find and remove all on-hold tasks older than 30 days\n/orchestration/find --status on_hold --older-than 30d | /orchestration/remove --batch\n```\n\n## Safety Features\n\n- Confirmation required (unless --force)\n- Dependencies checked and warned\n- Commits preserved by default\n- Audit trail maintained\n- Undo capability for recent removals\n\n## Notes\n\n- Removed tasks are archived for 30 days by default\n- Git commits are never automatically reverted\n- Dependencies are gracefully handled\n- System consistency is maintained throughout"
              },
              {
                "name": "/report",
                "description": "Generate comprehensive reports on task execution, progress, and metrics.",
                "path": "plugins/commands-workflow-orchestration/commands/report.md",
                "frontmatter": {
                  "description": "Generate comprehensive reports on task execution, progress, and metrics.",
                  "category": "workflow-orchestration"
                },
                "content": "# Task Report Command\n\nGenerate comprehensive reports on task execution, progress, and metrics.\n\n## Usage\n\n```\n/task-report [report-type] [options]\n```\n\n## Description\n\nCreates detailed reports for project management, sprint reviews, and performance analysis. Supports multiple report types and output formats.\n\n## Report Types\n\n### Executive Summary\n```\n/task-report executive\n```\nHigh-level overview for stakeholders with key metrics and progress.\n\n### Sprint Report\n```\n/task-report sprint --date 03_15_2024\n```\nDetailed sprint progress with burndown charts and velocity.\n\n### Daily Standup\n```\n/task-report standup\n```\nWhat was completed, in progress, and blocked.\n\n### Performance Report\n```\n/task-report performance --period week\n```\nTeam and individual performance metrics.\n\n### Dependency Report\n```\n/task-report dependencies\n```\nVisual dependency graph and bottleneck analysis.\n\n## Output Examples\n\n### Executive Summary Report\n```\nEXECUTIVE SUMMARY - Authentication System Project\n================================================\nReport Date: 2024-03-15\nProject Start: 2024-03-13\nDuration: 3 days (60% complete)\n\nKEY METRICS\n-----------\n Total Tasks: 24\n Completed: 12 (50%)\n In Progress: 3 (12.5%)\n Blocked: 2 (8.3%)\n Remaining: 7 (29.2%)\n\nTIMELINE\n--------\n Original Estimate: 5 days\n Current Projection: 5.5 days\n Risk Level: Low\n\nHIGHLIGHTS\n----------\n Core authentication API completed\n Database schema migrated\n Unit tests passing (98% coverage)\n\nBLOCKERS\n--------\n Payment integration waiting on external API\n UI components need design approval\n\nNEXT MILESTONES\n--------------\n Complete JWT implementation (Today)\n Integration testing (Tomorrow)\n Security audit (Day 4)\n```\n\n### Sprint Burndown Report\n```\n/task-report burndown --sprint current\n```\n```\nSPRINT BURNDOWN - Sprint 24\n===========================\n\nTasks Remaining by Day:\nDay 1:  24\nDay 2:      20 \nDay 3:          15 (TODAY)\nDay 4:              10 (projected)\nDay 5:                  5  (projected)\n\nVelocity Metrics:\n- Average: 4.5 tasks/day\n- Yesterday: 5 tasks\n- Today: 3 tasks (in progress)\n\nRisk Assessment: ON TRACK\n```\n\n### Performance Report\n```\nTEAM PERFORMANCE REPORT - Week 11\n=================================\n\nBy Agent:\n\n Agent            Completed  Avg Time  Quality  Efficiency \n\n dev-frontend        8      3.2h       95%       125%    \n dev-backend         6      4.1h       98%       110%    \n test-developer      4      2.8h       100%      115%    \n\n\nBy Task Type:\n- Features: 12 completed (avg 3.8h)\n- Bugfixes: 4 completed (avg 1.5h)\n- Tests: 8 completed (avg 2.2h)\n\nQuality Metrics:\n- First-time pass rate: 88%\n- Rework required: 2 tasks\n- Blocked time: 4.5 hours total\n```\n\n## Customization Options\n\n### Time Period\n```\n/task-report summary --from 2024-03-01 --to 2024-03-15\n/task-report summary --last 7d\n/task-report summary --this-month\n```\n\n### Specific Project\n```\n/task-report sprint --project authentication_system\n```\n\n### Format Options\n```\n/task-report executive --format markdown\n/task-report executive --format html\n/task-report executive --format pdf\n```\n\n### Include/Exclude\n```\n/task-report summary --include completed,qa\n/task-report summary --exclude on_hold\n```\n\n## Specialized Reports\n\n### Critical Path Analysis\n```\n/task-report critical-path\n```\nShows tasks that directly impact completion time.\n\n### Bottleneck Analysis\n```\n/task-report bottlenecks\n```\nIdentifies tasks causing delays.\n\n### Resource Utilization\n```\n/task-report resources\n```\nShows agent allocation and availability.\n\n### Risk Assessment\n```\n/task-report risks\n```\nIdentifies potential delays and issues.\n\n## Visualization Options\n\n### Gantt Chart\n```\n/task-report gantt --weeks 2\n```\n\n### Dependency Graph\n```\n/task-report dependencies --visual\n```\n\n### Status Flow\n```\n/task-report flow --animated\n```\n\n## Automated Reports\n\n### Schedule Reports\n```\n/task-report schedule daily-standup --at \"9am\"\n/task-report schedule weekly-summary --every friday\n```\n\n### Email Reports\n```\n/task-report executive --email team@company.com\n```\n\n## Comparison Reports\n\n### Sprint Comparison\n```\n/task-report compare --sprint 23 24\n```\n\n### Week over Week\n```\n/task-report trends --weeks 4\n```\n\n## Examples\n\n### Example 1: Morning Status\n```\n/task-report standup --format slack\n```\nGenerates Slack-formatted standup report.\n\n### Example 2: Sprint Review\n```\n/task-report sprint --include-velocity --include-burndown\n```\nComprehensive sprint metrics for review meeting.\n\n### Example 3: Blocker Focus\n```\n/task-report blockers --show-dependencies --show-resolution\n```\nDeep dive into what's blocking progress.\n\n## Integration Features\n\n### Export to Tools\n```\n/task-report export-jira\n/task-report export-asana\n/task-report export-github\n```\n\n### API Endpoints\n```\n/task-report api --generate-endpoint\n```\nCreates API endpoint for external access.\n\n## Best Practices\n\n1. **Daily Reviews**: Run standup report each morning\n2. **Weekly Summaries**: Generate performance reports on Fridays\n3. **Sprint Planning**: Use velocity trends for estimation\n4. **Stakeholder Updates**: Schedule automated executive summaries\n\n## Report Components\n\nEach report can include:\n- Summary statistics\n- Timeline visualization\n- Task lists by status\n- Agent performance\n- Dependency analysis\n- Risk assessment\n- Recommendations\n- Historical trends\n\n## Notes\n\n- Reports use data from all TASK-STATUS-TRACKER.yaml files\n- Completed tasks are included in historical metrics\n- Time calculations use business hours by default\n- All times shown in local timezone\n- Charts require terminal unicode support"
              },
              {
                "name": "/resume",
                "description": "Resume work on existing task orchestrations after session loss or context switch.",
                "path": "plugins/commands-workflow-orchestration/commands/resume.md",
                "frontmatter": {
                  "description": "Resume work on existing task orchestrations after session loss or context switch.",
                  "category": "workflow-orchestration",
                  "allowed-tools": "Bash(git *), Read"
                },
                "content": "# Orchestration Resume Command\n\nResume work on existing task orchestrations after session loss or context switch.\n\n## Usage\n\n```\n/orchestration/resume [options]\n```\n\n## Description\n\nRestores full context for active orchestrations, showing current progress, identifying next actions, and providing all necessary information to continue work seamlessly.\n\n## Basic Commands\n\n### List Active Orchestrations\n```\n/orchestration/resume\n```\nShows all orchestrations with active (non-completed) tasks.\n\n### Resume Specific Orchestration\n```\n/orchestration/resume --date 03_15_2024 --project auth_system\n```\nLoads complete context for a specific orchestration.\n\n### Resume Most Recent\n```\n/orchestration/resume --latest\n```\nAutomatically resumes the most recently active orchestration.\n\n## Output Format\n\n### Orchestration List View\n```\nActive Task Orchestrations\n==========================\n\n1. 03_15_2024/authentication_system\n   Started: 3 days ago | Progress: 65% | Active Tasks: 3\n    Focus: JWT implementation, OAuth integration\n\n2. 03_14_2024/payment_processing  \n   Started: 4 days ago | Progress: 40% | Active Tasks: 2\n    Focus: Stripe webhooks, refund handling\n\n3. 03_12_2024/admin_dashboard\n   Started: 1 week ago | Progress: 85% | Active Tasks: 1\n    Focus: Final testing and deployment\n\nSelect orchestration to resume: [1-3] or use --date and --project\n```\n\n### Detailed Resume View\n```\nResuming: authentication_system (03_15_2024)\n============================================\n\n## Current Status Summary\n- Total Tasks: 24 (12 completed, 3 in progress, 2 on hold, 7 todos)\n- Time Elapsed: 3 days\n- Estimated Remaining: 2 days\n\n## Tasks In Progress\n\n Task ID   Title                       Agent          Duration     \n\n TASK-003  JWT token validation        dev-backend    2.5h         \n TASK-007  OAuth provider setup        dev-frontend   1h           \n TASK-011  Integration tests           test-dev       30m          \n\n\n## Blocked Tasks (Require Attention)\n- TASK-005: User profile API - Blocked by TASK-003 (JWT validation)\n- TASK-009: OAuth callback handling - Waiting for provider credentials\n\n## Next Available Tasks (Ready to Start)\n1. TASK-013: Password reset flow (4h, frontend)\n   Files: src/auth/reset.tsx, src/api/auth.ts\n   \n2. TASK-014: Session management (3h, backend)\n   Files: src/services/session.ts, src/middleware/auth.ts\n\n## Recent Git Activity\n- feature/jwt-auth: 2 commits behind, last commit 2h ago\n- feature/oauth-setup: clean, last commit 1h ago\n\n## Quick Actions\n[1] Show TASK-003 details (current focus)\n[2] Pick up TASK-013 (password reset)\n[3] View dependency graph\n[4] Show recent commits\n[5] Generate status report\n```\n\n## Context Recovery Features\n\n### Task Context\n```\n/orchestration/resume --task TASK-003\n```\nShows:\n- Full task description and requirements\n- Implementation progress and notes\n- Related files with recent changes\n- Test requirements and status\n- Dependencies and blockers\n\n### File Context\n```\n/orchestration/resume --show-files\n```\nLists all files mentioned in active tasks with:\n- Last modified time\n- Current git status\n- Which tasks reference them\n\n### Dependency Context\n```\n/orchestration/resume --deps\n```\nShows dependency graph focused on active tasks.\n\n## Working State Recovery\n\n### Git State Summary\n```\n## Git Working State\nCurrent Branch: feature/jwt-auth\nStatus: 2 files modified, 1 untracked\n\nModified Files:\n- src/auth/jwt.ts (related to TASK-003)\n- tests/auth.test.ts (related to TASK-003)\n\nUntracked:\n- src/auth/jwt.config.ts (new file for TASK-003)\n\nRecommendation: Commit current changes before switching tasks\n```\n\n### Last Session Summary\n```\n## Last Session (2 hours ago)\n- Completed: TASK-002 (Database schema)\n- Started: TASK-003 (JWT validation)\n- Commits: 2 (feat: add user auth schema, test: auth unit tests)\n- Next planned: Continue TASK-003, then TASK-005\n```\n\n## Filtering Options\n\n### By Status\n```\n/orchestration/resume --show in_progress,on_hold\n```\n\n### By Date Range\n```\n/orchestration/resume --since \"last week\"\n```\n\n### By Completion\n```\n/orchestration/resume --incomplete  # < 50% done\n/orchestration/resume --nearly-done  # > 80% done\n```\n\n## Integration Features\n\n### Direct Task Pickup\n```\n/orchestration/resume --pickup TASK-013\n```\nAutomatically:\n1. Shows task details\n2. Moves to in_progress\n3. Shows relevant files\n4. Creates feature branch if needed\n\n### Status Check Integration\n```\n/orchestration/resume --with-status\n```\nIncludes full status report with resume context.\n\n### Commit History\n```\n/orchestration/resume --commits 5\n```\nShows last 5 commits related to the orchestration.\n\n## Quick Resume Patterns\n\n### Morning Standup\n```\n/orchestration/resume --latest --with-status\n```\nPerfect for daily standups - shows what you were working on and current state.\n\n### Context Switch\n```\n/orchestration/resume --save-state\n```\nSaves current working state before switching to another orchestration.\n\n### Team Handoff\n```\n/orchestration/resume --handoff\n```\nGenerates detailed handoff notes for another developer.\n\n## Examples\n\n### Example 1: Quick Continue\n```\n/orchestration/resume --latest --pickup-where-left-off\n```\nResumes exactly where you stopped, showing the in-progress task.\n\n### Example 2: Monday Morning\n```\n/orchestration/resume --since friday --show-completed\n```\nShows what was done Friday and what's next for Monday.\n\n### Example 3: Multiple Projects\n```\n/orchestration/resume --all --summary\n```\nQuick overview of all active orchestrations.\n\n## State Persistence\n\nThe command reads from:\n- EXECUTION-TRACKER.md for progress metrics\n- TASK-STATUS-TRACKER.yaml for current state\n- Task files for detailed context\n- Git for working directory state\n\n## Best Practices\n\n1. **Use at Session Start**: Run `/orchestration/resume` when starting work\n2. **Save State**: Use `--save-state` before extended breaks\n3. **Check Dependencies**: Review blocked tasks that may now be unblocked\n4. **Commit Regularly**: Keep git state aligned with task progress\n\n## Notes\n\n- Automatically detects uncommitted changes related to tasks\n- Suggests next actions based on dependencies and priorities\n- Integrates with git worktrees if in use\n- Preserves task history for full context"
              },
              {
                "name": "/start",
                "description": "Initiates the task orchestration workflow using the three-agent system (task-orchestrator, task-decomposer, and dependency-analyzer) to create a comprehensive execution plan.",
                "path": "plugins/commands-workflow-orchestration/commands/start.md",
                "frontmatter": {
                  "description": "Initiates the task orchestration workflow using the three-agent system (task-orchestrator, task-decomposer, and dependency-analyzer) to create a comprehensive execution plan.",
                  "category": "workflow-orchestration"
                },
                "content": "# Orchestrate Tasks Command\n\nInitiates the task orchestration workflow using the three-agent system (task-orchestrator, task-decomposer, and dependency-analyzer) to create a comprehensive execution plan.\n\n## Usage\n\n```\n/orchestrate [task list or file path]\n```\n\n## Description\n\nThis command activates the task-orchestrator agent to process requirements and create a hyper-efficient execution plan. The orchestrator will:\n\n1. **Clarify Requirements**: Analyze provided information and confirm understanding\n2. **Create Directory Structure**: Set up task-orchestration folders with today's date\n3. **Decompose Tasks**: Work with task-decomposer to create atomic task files\n4. **Analyze Dependencies**: Use dependency-analyzer to identify conflicts and parallelization opportunities\n5. **Generate Master Plan**: Create comprehensive coordination documents\n\n## Input Formats\n\n### Direct Task List\n```\n/orchestrate\n- Implement user authentication with JWT\n- Add payment processing with Stripe\n- Create admin dashboard\n- Set up email notifications\n```\n\n### File Reference\n```\n/orchestrate features.md\n```\n\n### Mixed Context\n```\n/orchestrate\nBased on our meeting notes (lots of discussion about UI colors), we need to:\n1. Fix the security vulnerability in file uploads\n2. Add rate limiting to APIs\n3. Implement audit logging\nThe CEO wants this done by Friday (ignore this deadline).\n```\n\n## Workflow\n\n1. **Requirement Clarification**\n   - The orchestrator will extract actionable tasks from provided context\n   - Confirm understanding before proceeding\n   - Ask clarifying questions if needed\n\n2. **Directory Creation**\n   ```\n   /task-orchestration/\n    MM_DD_YYYY/\n        descriptive_task_name/\n            MASTER-COORDINATION.md\n            EXECUTION-TRACKER.md\n            TASK-STATUS-TRACKER.yaml\n            tasks/\n                todos/\n                in_progress/\n                on_hold/\n                qa/\n                completed/\n   ```\n\n3. **Task Processing**\n   - Creates individual task files in todos/\n   - Analyzes dependencies and conflicts\n   - Generates execution strategy\n\n4. **Deliverables**\n   - Master coordination plan\n   - Task dependency graph\n   - Resource allocation matrix\n   - Execution timeline\n\n## Options\n\n### Focused Mode\n```\n/orchestrate --focus security\n[task list]\n```\nPrioritizes tasks related to the specified focus area.\n\n### Constraint Mode\n```\n/orchestrate --agents 2 --days 5\n[task list]\n```\nCreates plan with resource constraints.\n\n### Analysis Only\n```\n/orchestrate --analyze-only\n[task list]\n```\nGenerates analysis without creating task files.\n\n## Examples\n\n### Example 1: Clear Task List\n```\n/orchestrate\n1. Implement OAuth2 authentication\n2. Add user profile management\n3. Create password reset flow\n4. Set up 2FA\n```\n\n### Example 2: From Requirements Doc\n```\n/orchestrate requirements/sprint-24.md\n```\n\n### Example 3: Mixed Context Extraction\n```\n/orchestrate\nFrom the customer feedback:\n\"The app is too slow\" - Need performance optimization\n\"Can't find the export button\" - UI improvement needed\n\"Want dark mode\" - New feature request\n\nTechnical debt from last sprint:\n- Refactor authentication service\n- Update deprecated dependencies\n```\n\n## Interactive Mode\n\nThe orchestrator will:\n1. Present extracted tasks for confirmation\n2. Ask about priorities and constraints\n3. Suggest optimal approach\n4. Request approval before creating files\n\n## Error Handling\n\n- If tasks are unclear: Asks for clarification\n- If file not found: Prompts for correct path\n- If conflicts detected: Presents options\n- If dependencies circular: Suggests resolution\n\n## Integration\n\nWorks seamlessly with:\n- `/task-status` - Check progress\n- `/task-move` - Update task status\n- `/task-report` - Generate reports\n- `/task-assign` - Allocate to agents\n\n## Best Practices\n\n1. **Provide Context**: Include relevant background information\n2. **Be Specific**: Clear task descriptions enable better planning\n3. **Mention Constraints**: Include deadlines, resources, or blockers\n4. **Review Output**: Confirm the extracted tasks match your intent\n\n## Notes\n\n- The orchestrator filters out irrelevant context automatically\n- Tasks are created in todos/ status by default\n- All tasks get unique IDs (TASK-XXX format)\n- Status tracking begins immediately\n- Supports incremental additions to existing orchestrations"
              },
              {
                "name": "/status",
                "description": "Check the current status of tasks in the orchestration system with various filtering and reporting options.",
                "path": "plugins/commands-workflow-orchestration/commands/status.md",
                "frontmatter": {
                  "description": "Check the current status of tasks in the orchestration system with various filtering and reporting options.",
                  "category": "workflow-orchestration",
                  "allowed-tools": "Write"
                },
                "content": "# Task Status Command\n\nCheck the current status of tasks in the orchestration system with various filtering and reporting options.\n\n## Usage\n\n```\n/task-status [options]\n```\n\n## Description\n\nProvides comprehensive visibility into task progress, status distribution, and execution metrics across all active orchestrations.\n\n## Command Variants\n\n### Basic Status Overview\n```\n/task-status\n```\nShows summary of all tasks across all active orchestrations.\n\n### Today's Tasks\n```\n/task-status --today\n```\nShows only tasks from today's orchestrations.\n\n### Specific Orchestration\n```\n/task-status --date 03_15_2024 --project payment_integration\n```\nShows tasks from a specific orchestration.\n\n### Status Filter\n```\n/task-status --status in_progress\n/task-status --status qa\n/task-status --status on_hold\n```\nShows only tasks with specified status.\n\n### Detailed View\n```\n/task-status --detailed\n```\nShows comprehensive information for each task.\n\n## Output Formats\n\n### Summary View (Default)\n```\nTask Orchestration Status Summary\n=================================\n\nActive Orchestrations: 3\nTotal Tasks: 47\n\nStatus Distribution:\n\n Status       Count  Percentage \n\n completed     12       26%     \n qa             5       11%     \n in_progress    3        6%     \n on_hold        2        4%     \n todos         25       53%     \n\n\nActive Tasks (in_progress):\n- TASK-001: Implement JWT authentication (Agent: dev-frontend)\n- TASK-007: Create payment webhook handler (Agent: dev-backend)\n- TASK-012: Write integration tests (Agent: test-developer)\n\nBlocked Tasks (on_hold):\n- TASK-004: User profile API (Blocked by: TASK-001)\n- TASK-009: Payment confirmation UI (Blocked by: TASK-007)\n```\n\n### Detailed View\n```\nTask Details for: 03_15_2024/authentication_system\n==================================================\n\nTASK-001: Implement JWT authentication\nStatus: in_progress\nAgent: dev-frontend\nStarted: 2024-03-15T14:30:00Z\nDuration: 3.5 hours\nProgress: 75% (est. 1 hour remaining)\nDependencies: None\nBlocks: TASK-004, TASK-005\nLocation: /task-orchestration/03_15_2024/authentication_system/tasks/in_progress/\n\nStatus History:\n- todos  in_progress (2024-03-15T14:30:00Z) by dev-frontend\n```\n\n### Timeline View\n```\n/task-status --timeline\n```\nShows Gantt-style timeline of task execution.\n\n### Velocity Report\n```\n/task-status --velocity\n```\nShows completion rates and performance metrics.\n\n## Filtering Options\n\n### By Agent\n```\n/task-status --agent dev-frontend\n```\n\n### By Priority\n```\n/task-status --priority high\n```\n\n### By Type\n```\n/task-status --type feature\n/task-status --type bugfix\n```\n\n### Multiple Filters\n```\n/task-status --status todos --priority high --type security\n```\n\n## Quick Actions\n\n### Show Critical Path\n```\n/task-status --critical-path\n```\nHighlights tasks that are blocking others.\n\n### Show Overdue\n```\n/task-status --overdue\n```\nShows tasks exceeding estimated time.\n\n### Show Available\n```\n/task-status --available\n```\nShows todos tasks ready to be picked up.\n\n## Integration Commands\n\n### Export Status\n```\n/task-status --export markdown\n/task-status --export csv\n```\n\n### Watch Mode\n```\n/task-status --watch\n```\nUpdates status in real-time (refreshes every 30 seconds).\n\n## Examples\n\n### Example 1: Morning Standup View\n```\n/task-status --today --detailed\n```\n\n### Example 2: Find Blocked Work\n```\n/task-status --status on_hold --show-blockers\n```\n\n### Example 3: Agent Workload\n```\n/task-status --by-agent --status in_progress\n```\n\n### Example 4: Sprint Progress\n```\n/task-status --date 03_15_2024 --metrics\n```\n\n## Metrics and Analytics\n\n### Completion Metrics\n- Average time per task\n- Tasks completed per day\n- Status transition times\n\n### Bottleneck Analysis\n- Most blocking tasks\n- Longest on_hold duration\n- Critical path duration\n\n### Agent Performance\n- Tasks per agent\n- Average completion time\n- Current workload\n\n## Best Practices\n\n1. **Daily Check**: Run `/task-status --today` each morning\n2. **Blocker Review**: Check `/task-status --status on_hold` regularly\n3. **Progress Tracking**: Use `/task-status --velocity` for trends\n4. **Resource Planning**: Monitor `/task-status --by-agent`\n\n## Notes\n\n- Status data is read from TASK-STATUS-TRACKER.yaml files\n- All times are shown in local timezone\n- Completed tasks are included in metrics but not in active lists\n- Use `--all` flag to include historical orchestrations"
              },
              {
                "name": "/sync",
                "description": "Synchronize task status with git commits, ensuring consistency between version control and task tracking.",
                "path": "plugins/commands-workflow-orchestration/commands/sync.md",
                "frontmatter": {
                  "description": "Synchronize task status with git commits, ensuring consistency between version control and task tracking.",
                  "category": "workflow-orchestration",
                  "allowed-tools": "Bash(git *)"
                },
                "content": "# Orchestration Sync Command\n\nSynchronize task status with git commits, ensuring consistency between version control and task tracking.\n\n## Usage\n\n```\n/orchestration/sync [options]\n```\n\n## Description\n\nAnalyzes git history and task status to identify discrepancies, automatically updating task tracking based on commit evidence and maintaining bidirectional consistency.\n\n## Basic Commands\n\n### Full Sync\n```\n/orchestration/sync\n```\nPerforms complete synchronization between git and task status.\n\n### Check Sync Status\n```\n/orchestration/sync --check\n```\nReports inconsistencies without making changes.\n\n### Sync Specific Orchestration\n```\n/orchestration/sync --date 03_15_2024 --project auth_system\n```\n\n## Sync Operations\n\n### Git  Task Status\nUpdates task status based on commit messages:\n```\nFound commits:\n- feat(auth): implement JWT validation (TASK-003) \n  Status: in_progress  qa (based on commit)\n  \n- test(auth): add JWT validation tests (TASK-003) \n  Status: qa  completed (tests indicate completion)\n  \n- fix(auth): resolve token expiration (TASK-007) \n  Status: todos  in_progress (work started)\n```\n\n### Task Status  Git\nIdentifies tasks marked complete without commits:\n```\nStatus Discrepancies:\n- TASK-005: Marked 'completed' but no commits found\n- TASK-008: In 'qa' but no implementation commits\n- TASK-010: Multiple commits but still in 'todos'\n```\n\n## Detection Patterns\n\n### Commit Pattern Matching\n```\nPatterns detected:\n- \"feat(auth): implement\"  Implementation complete\n- \"test(auth): add\"  Testing phase\n- \"fix(auth): resolve\"  Bug fix complete\n- \"docs(auth): update\"  Documentation done\n- \"refactor(auth):\"  Code improvement\n```\n\n### Task Reference Extraction\n```\nScanning commits for task references:\n- Explicit: \"Task: TASK-003\" \n- In body: \"Implements TASK-003\" \n- Branch name: \"feature/TASK-003-jwt\" \n- PR title: \"TASK-003: JWT implementation\" \n```\n\n## Sync Rules\n\n### Automatic Status Updates\n```yaml\nsync_rules:\n  commit_patterns:\n    - pattern: \"feat.*TASK-(\\d+)\"\n      action: \"move to qa if in_progress\"\n    - pattern: \"test.*TASK-(\\d+).*pass\"\n      action: \"move to completed if in qa\"\n    - pattern: \"fix.*TASK-(\\d+)\"\n      action: \"move to qa if in_progress\"\n    - pattern: \"WIP.*TASK-(\\d+)\"\n      action: \"keep in in_progress\"\n```\n\n### Conflict Resolution\n```\nConflict detected for TASK-003:\n- Git evidence: 3 commits, tests passing\n- Task status: in_progress\n- Recommended: Move to completed\n\nResolution options:\n[1] Trust git (move to completed)\n[2] Trust tracker (keep in_progress)\n[3] Manual review\n[4] Skip\n```\n\n## Analysis Reports\n\n### Sync Summary\n```\nSynchronization Report\n======================\n\nAnalyzed: 45 commits across 3 branches\nTasks referenced: 12\nStatus updates needed: 4\n\nUpdates to apply:\n- TASK-003: in_progress  completed (3 commits)\n- TASK-007: todos  in_progress (1 commit)\n- TASK-009: qa  completed (tests added)\n- TASK-011: on_hold  in_progress (blocker resolved)\n\nWarnings:\n- TASK-005: Completed without commits\n- TASK-013: Commits without task reference\n```\n\n### Detailed Analysis\n```\nTask: TASK-003 - JWT Implementation\nCurrent Status: in_progress\nGit Evidence:\n  - feat(auth): implement JWT validation (2 days ago)\n  - test(auth): add validation tests (1 day ago)\n  - fix(auth): handle edge cases (1 day ago)\n  \nRecommendation: Move to completed\nConfidence: High (95%)\n```\n\n## Options\n\n### Dry Run\n```\n/orchestration/sync --dry-run\n```\nShows what would change without applying updates.\n\n### Force Sync\n```\n/orchestration/sync --force\n```\nApplies all recommendations without prompting.\n\n### Time Range\n```\n/orchestration/sync --since \"1 week ago\"\n```\nOnly analyzes recent commits.\n\n### Branch Specific\n```\n/orchestration/sync --branch feature/auth\n```\nSyncs only tasks related to specific branch.\n\n## Integration Features\n\n### Update Tracking Files\n```\n/orchestration/sync --update-trackers\n```\nUpdates TASK-STATUS-TRACKER.yaml with:\n```yaml\ngit_tracking:\n  TASK-003:\n    status_from_git: completed\n    confidence: 0.95\n    evidence:\n      - commit: abc123\n        message: \"feat(auth): implement JWT\"\n        date: \"2024-03-13\"\n      - commit: def456\n        message: \"test(auth): add tests\"\n        date: \"2024-03-14\"\n```\n\n### Generate Commit Report\n```\n/orchestration/sync --commit-report\n```\nCreates report of all task-related commits.\n\n### Fix Orphaned Commits\n```\n/orchestration/sync --link-orphans\n```\nAssociates commits without task references.\n\n## Sync Strategies\n\n### Conservative\n```\n/orchestration/sync --conservative\n```\nOnly updates with high confidence matches.\n\n### Aggressive\n```\n/orchestration/sync --aggressive\n```\nUpdates based on any evidence.\n\n### Interactive\n```\n/orchestration/sync --interactive\n```\nPrompts for each potential update.\n\n## Examples\n\n### Example 1: Daily Sync\n```\n/orchestration/sync --since yesterday\n\nQuick sync results:\n- 5 commits analyzed\n- 2 tasks updated\n- All changes applied successfully\n```\n\n### Example 2: Branch Merge Sync\n```\n/orchestration/sync --after-merge feature/auth\n\nPost-merge sync:\n- 15 commits from feature/auth\n- 5 tasks moved to completed\n- 2 tasks have test failures (kept in qa)\n```\n\n### Example 3: Audit Mode\n```\n/orchestration/sync --audit --report\n\nAudit Report:\n- Tasks with commits: 85%\n- Commits with task refs: 92%\n- Average commits per task: 2.3\n- Orphaned commits: 3\n```\n\n## Webhook Integration\n\n### Auto-sync on Push\n```yaml\ngit_hooks:\n  post-commit: /orchestration/sync --last-commit\n  post-merge: /orchestration/sync --branch HEAD\n```\n\n## Best Practices\n\n1. **Regular Syncs**: Run daily or after major commits\n2. **Review Before Force**: Check dry-run output first\n3. **Maintain References**: Include task IDs in commits\n4. **Handle Conflicts**: Don't ignore sync warnings\n5. **Document Decisions**: Note why status differs from git\n\n## Configuration\n\n### Sync Preferences\n```yaml\nsync_config:\n  auto_sync: true\n  confidence_threshold: 0.8\n  require_tests: true\n  trust_git_over_tracker: true\n  patterns:\n    - implementation: \"feat|feature\"\n    - testing: \"test|spec\"\n    - completion: \"done|complete|finish\"\n```\n\n## Notes\n\n- Requires git access to all relevant branches\n- Preserves manual status overrides with flags\n- Supports custom commit message patterns\n- Integrates with CI/CD for automated syncing"
              }
            ],
            "skills": []
          },
          {
            "name": "hooks-automation",
            "description": "Automation Hooks - Event-driven automation hooks",
            "source": "./plugins/hooks-automation",
            "category": "hooks",
            "version": "1.0.0",
            "author": {
              "name": "BuildWithClaude Community",
              "url": "https://github.com/davepoon/buildwithclaude"
            },
            "install_commands": [
              "/plugin marketplace add davepoon/buildwithclaude",
              "/plugin install hooks-automation@buildwithclaude"
            ],
            "signals": {
              "stars": 2229,
              "forks": 239,
              "pushed_at": "2026-01-12T10:19:01Z",
              "created_at": "2025-07-25T02:26:45Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": []
          },
          {
            "name": "hooks-development",
            "description": "Development Hooks - Event-driven automation hooks",
            "source": "./plugins/hooks-development",
            "category": "hooks",
            "version": "1.0.0",
            "author": {
              "name": "BuildWithClaude Community",
              "url": "https://github.com/davepoon/buildwithclaude"
            },
            "install_commands": [
              "/plugin marketplace add davepoon/buildwithclaude",
              "/plugin install hooks-development@buildwithclaude"
            ],
            "signals": {
              "stars": 2229,
              "forks": 239,
              "pushed_at": "2026-01-12T10:19:01Z",
              "created_at": "2025-07-25T02:26:45Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": []
          },
          {
            "name": "hooks-formatting",
            "description": "Formatting Hooks - Event-driven automation hooks",
            "source": "./plugins/hooks-formatting",
            "category": "hooks",
            "version": "1.0.0",
            "author": {
              "name": "BuildWithClaude Community",
              "url": "https://github.com/davepoon/buildwithclaude"
            },
            "install_commands": [
              "/plugin marketplace add davepoon/buildwithclaude",
              "/plugin install hooks-formatting@buildwithclaude"
            ],
            "signals": {
              "stars": 2229,
              "forks": 239,
              "pushed_at": "2026-01-12T10:19:01Z",
              "created_at": "2025-07-25T02:26:45Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": []
          },
          {
            "name": "hooks-git",
            "description": "Git Hooks - Event-driven automation hooks",
            "source": "./plugins/hooks-git",
            "category": "hooks",
            "version": "1.0.0",
            "author": {
              "name": "BuildWithClaude Community",
              "url": "https://github.com/davepoon/buildwithclaude"
            },
            "install_commands": [
              "/plugin marketplace add davepoon/buildwithclaude",
              "/plugin install hooks-git@buildwithclaude"
            ],
            "signals": {
              "stars": 2229,
              "forks": 239,
              "pushed_at": "2026-01-12T10:19:01Z",
              "created_at": "2025-07-25T02:26:45Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": []
          },
          {
            "name": "hooks-notifications",
            "description": "Notification Hooks - Event-driven automation hooks",
            "source": "./plugins/hooks-notifications",
            "category": "hooks",
            "version": "1.0.0",
            "author": {
              "name": "BuildWithClaude Community",
              "url": "https://github.com/davepoon/buildwithclaude"
            },
            "install_commands": [
              "/plugin marketplace add davepoon/buildwithclaude",
              "/plugin install hooks-notifications@buildwithclaude"
            ],
            "signals": {
              "stars": 2229,
              "forks": 239,
              "pushed_at": "2026-01-12T10:19:01Z",
              "created_at": "2025-07-25T02:26:45Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": []
          },
          {
            "name": "hooks-performance",
            "description": "Performance Hooks - Event-driven automation hooks",
            "source": "./plugins/hooks-performance",
            "category": "hooks",
            "version": "1.0.0",
            "author": {
              "name": "BuildWithClaude Community",
              "url": "https://github.com/davepoon/buildwithclaude"
            },
            "install_commands": [
              "/plugin marketplace add davepoon/buildwithclaude",
              "/plugin install hooks-performance@buildwithclaude"
            ],
            "signals": {
              "stars": 2229,
              "forks": 239,
              "pushed_at": "2026-01-12T10:19:01Z",
              "created_at": "2025-07-25T02:26:45Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": []
          },
          {
            "name": "hooks-security",
            "description": "Security Hooks - Event-driven automation hooks",
            "source": "./plugins/hooks-security",
            "category": "hooks",
            "version": "1.0.0",
            "author": {
              "name": "BuildWithClaude Community",
              "url": "https://github.com/davepoon/buildwithclaude"
            },
            "install_commands": [
              "/plugin marketplace add davepoon/buildwithclaude",
              "/plugin install hooks-security@buildwithclaude"
            ],
            "signals": {
              "stars": 2229,
              "forks": 239,
              "pushed_at": "2026-01-12T10:19:01Z",
              "created_at": "2025-07-25T02:26:45Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": []
          },
          {
            "name": "hooks-testing",
            "description": "Testing Hooks - Event-driven automation hooks",
            "source": "./plugins/hooks-testing",
            "category": "hooks",
            "version": "1.0.0",
            "author": {
              "name": "BuildWithClaude Community",
              "url": "https://github.com/davepoon/buildwithclaude"
            },
            "install_commands": [
              "/plugin marketplace add davepoon/buildwithclaude",
              "/plugin install hooks-testing@buildwithclaude"
            ],
            "signals": {
              "stars": 2229,
              "forks": 239,
              "pushed_at": "2026-01-12T10:19:01Z",
              "created_at": "2025-07-25T02:26:45Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": []
          },
          {
            "name": "mcp-servers-docker",
            "description": "Docker-based MCP servers from the official Docker MCP registry - includes 199+ verified servers",
            "source": "./plugins/mcp-servers-docker",
            "category": "mcp-servers",
            "version": "1.0.0",
            "author": {
              "name": "Docker Inc. & BuildWithClaude Community",
              "url": "https://github.com/davepoon/buildwithclaude"
            },
            "install_commands": [
              "/plugin marketplace add davepoon/buildwithclaude",
              "/plugin install mcp-servers-docker@buildwithclaude"
            ],
            "signals": {
              "stars": 2229,
              "forks": 239,
              "pushed_at": "2026-01-12T10:19:01Z",
              "created_at": "2025-07-25T02:26:45Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": []
          },
          {
            "name": "all-agents",
            "description": "Complete collection of 117 specialized AI agents across 11 categories",
            "source": "./plugins/all-agents",
            "category": "agents",
            "version": "1.0.0",
            "author": {
              "name": "BuildWithClaude Community",
              "url": "https://github.com/davepoon/buildwithclaude"
            },
            "install_commands": [
              "/plugin marketplace add davepoon/buildwithclaude",
              "/plugin install all-agents@buildwithclaude"
            ],
            "signals": {
              "stars": 2229,
              "forks": 239,
              "pushed_at": "2026-01-12T10:19:01Z",
              "created_at": "2025-07-25T02:26:45Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": []
          },
          {
            "name": "all-commands",
            "description": "Complete collection of 174 slash commands across 22 categories",
            "source": "./plugins/all-commands",
            "category": "commands",
            "version": "1.0.0",
            "author": {
              "name": "BuildWithClaude Community",
              "url": "https://github.com/davepoon/buildwithclaude"
            },
            "install_commands": [
              "/plugin marketplace add davepoon/buildwithclaude",
              "/plugin install all-commands@buildwithclaude"
            ],
            "signals": {
              "stars": 2229,
              "forks": 239,
              "pushed_at": "2026-01-12T10:19:01Z",
              "created_at": "2025-07-25T02:26:45Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/act",
                "description": "Follow RED-GREEN-REFACTOR cycle approach for test-driven development",
                "path": "plugins/all-commands/commands/act.md",
                "frontmatter": {
                  "description": "Follow RED-GREEN-REFACTOR cycle approach for test-driven development",
                  "category": "automation-workflow"
                },
                "content": "Follow RED-GREEN-REFACTOR cycle approch based on @~/.claude/CLAUDE.md:\n1. Open todo.md and select the first unchecked items to work on.\n2. Carefully plan each item, then share your plan\n3. Create a new branch and implement your plan\n4. Check off the items on todo.md\n5. Commit your changes"
              },
              {
                "name": "/add-authentication-system",
                "description": "Implement secure user authentication system",
                "path": "plugins/all-commands/commands/add-authentication-system.md",
                "frontmatter": {
                  "description": "Implement secure user authentication system",
                  "category": "security-audit"
                },
                "content": "# Add Authentication System\n\nImplement secure user authentication system\n\n## Instructions\n\n1. **Authentication Strategy Analysis**\n   - Analyze application requirements and user types\n   - Define authentication methods (password, OAuth, SSO, MFA)\n   - Assess security requirements and compliance needs\n   - Plan user management and role-based access control\n   - Evaluate existing authentication infrastructure and integration points\n\n2. **Authentication Method Selection**\n   - Choose appropriate authentication strategies:\n     - **Username/Password**: Traditional credential-based authentication\n     - **OAuth 2.0/OpenID Connect**: Third-party authentication (Google, GitHub, etc.)\n     - **SAML**: Enterprise single sign-on integration\n     - **JWT**: Stateless token-based authentication\n     - **Multi-Factor Authentication**: SMS, TOTP, hardware tokens\n     - **Passwordless**: Magic links, WebAuthn, biometric authentication\n\n3. **User Management System**\n   - Set up user registration and account creation workflows\n   - Configure user profile management and data storage\n   - Implement password policies and security requirements\n   - Set up account verification and email confirmation\n   - Configure user deactivation and account deletion procedures\n\n4. **Authentication Implementation**\n   - Implement secure password hashing (bcrypt, Argon2, scrypt)\n   - Set up session management and token generation\n   - Configure secure cookie handling and CSRF protection\n   - Implement authentication middleware and route protection\n   - Set up authentication state management (client-side)\n\n5. **Authorization and Access Control**\n   - Implement role-based access control (RBAC) system\n   - Set up permission-based authorization\n   - Configure resource-level access controls\n   - Implement dynamic authorization and policy engines\n   - Set up API endpoint protection and authorization\n\n6. **Multi-Factor Authentication (MFA)**\n   - Configure TOTP-based authenticator app support\n   - Set up SMS-based authentication codes\n   - Implement backup codes and recovery mechanisms\n   - Configure hardware token support (FIDO2/WebAuthn)\n   - Set up MFA enforcement policies and user experience\n\n7. **OAuth and Third-Party Integration**\n   - Configure OAuth providers (Google, GitHub, Facebook, etc.)\n   - Set up OpenID Connect for identity federation\n   - Implement social login and account linking\n   - Configure enterprise SSO integration (SAML, LDAP)\n   - Set up API key management for external integrations\n\n8. **Security Implementation**\n   - Configure rate limiting and brute force protection\n   - Set up account lockout and security monitoring\n   - Implement security headers and session security\n   - Configure audit logging and security event tracking\n   - Set up vulnerability scanning and security testing\n\n9. **User Experience and Frontend Integration**\n   - Create responsive authentication UI components\n   - Implement client-side authentication state management\n   - Set up protected route handling and redirects\n   - Configure authentication error handling and user feedback\n   - Implement remember me and persistent login features\n\n10. **Testing and Maintenance**\n    - Set up comprehensive authentication testing\n    - Configure security testing and penetration testing\n    - Create authentication monitoring and alerting\n    - Set up compliance reporting and audit trails\n    - Train team on authentication security best practices\n    - Create incident response procedures for security events"
              },
              {
                "name": "/add-changelog",
                "description": "Generate and maintain project changelog",
                "path": "plugins/all-commands/commands/add-changelog.md",
                "frontmatter": {
                  "description": "Generate and maintain project changelog",
                  "category": "ci-deployment",
                  "argument-hint": "1. **Changelog Format (Keep a Changelog)**",
                  "allowed-tools": "Bash(git *), Bash(npm *)"
                },
                "content": "# Add Changelog Command\n\nGenerate and maintain project changelog\n\n## Instructions\n\nSetup and maintain changelog following these steps: **$ARGUMENTS**\n\n1. **Changelog Format (Keep a Changelog)**\n   ```markdown\n   # Changelog\n   \n   All notable changes to this project will be documented in this file.\n   \n   The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),\n   and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).\n   \n   ## [Unreleased]\n   ### Added\n   - New features\n   \n   ### Changed\n   - Changes in existing functionality\n   \n   ### Deprecated\n   - Soon-to-be removed features\n   \n   ### Removed\n   - Removed features\n   \n   ### Fixed\n   - Bug fixes\n   \n   ### Security\n   - Security improvements\n   ```\n\n2. **Version Entries**\n   ```markdown\n   ## [1.2.3] - 2024-01-15\n   ### Added\n   - User authentication system\n   - Dark mode toggle\n   - Export functionality for reports\n   \n   ### Fixed\n   - Memory leak in background tasks\n   - Timezone handling issues\n   ```\n\n3. **Automation Tools**\n   ```bash\n   # Generate changelog from git commits\n   npm install -D conventional-changelog-cli\n   npx conventional-changelog -p angular -i CHANGELOG.md -s\n   \n   # Auto-changelog\n   npm install -D auto-changelog\n   npx auto-changelog\n   ```\n\n4. **Commit Convention**\n   ```bash\n   # Conventional commits for auto-generation\n   feat: add user authentication\n   fix: resolve memory leak in tasks\n   docs: update API documentation\n   style: format code with prettier\n   refactor: reorganize user service\n   test: add unit tests for auth\n   chore: update dependencies\n   ```\n\n5. **Integration with Releases**\n   - Update changelog before each release\n   - Include in release notes\n   - Link to GitHub releases\n   - Tag versions consistently\n\nRemember to keep entries clear, categorized, and focused on user-facing changes."
              },
              {
                "name": "/add-mutation-testing",
                "description": "Setup mutation testing for code quality",
                "path": "plugins/all-commands/commands/add-mutation-testing.md",
                "frontmatter": {
                  "description": "Setup mutation testing for code quality",
                  "category": "code-analysis-testing"
                },
                "content": "# Add Mutation Testing\n\nSetup mutation testing for code quality\n\n## Instructions\n\n1. **Mutation Testing Strategy Analysis**\n   - Analyze current test suite coverage and quality\n   - Identify critical code paths and business logic for mutation testing\n   - Assess existing testing infrastructure and CI/CD integration points\n   - Determine mutation testing scope and performance requirements\n   - Plan mutation testing integration with existing quality gates\n\n2. **Mutation Testing Tool Selection**\n   - Choose appropriate mutation testing framework:\n     - **JavaScript/TypeScript**: Stryker, Mutode\n     - **Java**: PIT (Pitest), Major\n     - **C#**: Stryker.NET, VisualMutator\n     - **Python**: mutmut, Cosmic Ray, MutPy\n     - **Go**: go-mutesting, mut\n     - **Rust**: mutagen, cargo-mutants\n     - **PHP**: Infection\n   - Consider factors: language support, performance, CI integration, reporting\n\n3. **Mutation Testing Configuration**\n   - Install and configure mutation testing framework\n   - Set up mutation testing configuration files and settings\n   - Configure mutation operators and strategies\n   - Set up file and directory inclusion/exclusion rules\n   - Configure performance and timeout settings\n\n4. **Mutation Operator Configuration**\n   - Configure arithmetic operator mutations (+, -, *, /, %)\n   - Set up relational operator mutations (<, >, <=, >=, ==, !=)\n   - Configure logical operator mutations (&&, ||, !)\n   - Set up conditional boundary mutations (< to <=, > to >=)\n   - Configure statement deletion and insertion mutations\n\n5. **Test Execution and Performance**\n   - Configure mutation test execution strategy and parallelization\n   - Set up incremental mutation testing for large codebases\n   - Configure mutation testing timeouts and resource limits\n   - Set up mutation test caching and optimization\n   - Configure selective mutation testing for changed code\n\n6. **Quality Metrics and Thresholds**\n   - Set up mutation score calculation and reporting\n   - Configure mutation testing thresholds and quality gates\n   - Set up mutation survival analysis and reporting\n   - Configure test effectiveness metrics and tracking\n   - Set up mutation testing trend analysis\n\n7. **Integration with Testing Workflow**\n   - Integrate mutation testing with existing test suites\n   - Configure mutation testing execution order and dependencies\n   - Set up mutation testing in development and CI environments\n   - Configure mutation testing result integration with test reports\n   - Set up mutation testing feedback loops for developers\n\n8. **CI/CD Pipeline Integration**\n   - Configure automated mutation testing in continuous integration\n   - Set up mutation testing scheduling and triggers\n   - Configure mutation testing result reporting and notifications\n   - Set up mutation testing performance monitoring\n   - Configure mutation testing deployment gates\n\n9. **Result Analysis and Remediation**\n   - Set up mutation testing result analysis and visualization\n   - Configure surviving mutant analysis and categorization\n   - Set up test gap identification and remediation workflow\n   - Configure mutation testing regression tracking\n   - Set up automated test improvement recommendations\n\n10. **Maintenance and Optimization**\n    - Create mutation testing maintenance and optimization procedures\n    - Set up mutation testing configuration version control\n    - Configure mutation testing performance optimization\n    - Document mutation testing best practices and guidelines\n    - Train team on mutation testing concepts and workflow\n    - Set up mutation testing tool updates and maintenance"
              },
              {
                "name": "/add-package",
                "description": "Add and configure new project dependencies",
                "path": "plugins/all-commands/commands/add-package.md",
                "frontmatter": {
                  "description": "Add and configure new project dependencies",
                  "category": "project-task-management",
                  "argument-hint": "[package-name] [type]"
                },
                "content": "# Add Package to Workspace\n\nAdd and configure new project dependencies\n\n## Instructions\n\n1. **Package Definition and Analysis**\n   - Parse package name and type from arguments: `$ARGUMENTS` (format: name [type])\n   - If no arguments provided, prompt for package name and type\n   - Validate package name follows workspace naming conventions\n   - Determine package type: library, application, tool, shared, service, component-library\n   - Check for naming conflicts with existing packages\n\n2. **Package Structure Creation**\n   - Create package directory in appropriate workspace location (packages/, apps/, libs/)\n   - Set up standard package directory structure based on type:\n     - `src/` for source code\n     - `tests/` or `__tests__/` for testing\n     - `docs/` for package documentation\n     - `examples/` for usage examples (if library)\n     - `public/` for static assets (if application)\n   - Create package-specific configuration files\n\n3. **Package Configuration Setup**\n   - Generate package.json with proper metadata:\n     - Name following workspace conventions\n     - Version aligned with workspace strategy\n     - Dependencies and devDependencies\n     - Scripts for build, test, lint, dev\n     - Entry points and exports configuration\n   - Configure TypeScript (tsconfig.json) extending workspace settings\n   - Set up package-specific linting and formatting rules\n\n4. **Package Type-Specific Setup**\n   - **Library**: Configure build system, export definitions, API documentation\n   - **Application**: Set up routing, environment configuration, build optimization\n   - **Tool**: Configure CLI setup, binary exports, command definitions\n   - **Shared**: Set up common utilities, type definitions, shared constants\n   - **Service**: Configure server setup, API routes, database connections\n   - **Component Library**: Set up Storybook, component exports, styling system\n\n5. **Workspace Integration**\n   - Register package in workspace configuration (nx.json, lerna.json, etc.)\n   - Configure package dependencies and peer dependencies\n   - Set up cross-package imports and references\n   - Configure workspace-wide build order and dependencies\n   - Add package to workspace scripts and task runners\n\n6. **Development Environment**\n   - Configure package-specific development server (if applicable)\n   - Set up hot reloading and watch mode\n   - Configure debugging and source maps\n   - Set up development proxy and API mocking (if needed)\n   - Configure environment variable management\n\n7. **Testing Infrastructure**\n   - Set up testing framework configuration for the package\n   - Create initial test files and examples\n   - Configure test coverage reporting\n   - Set up package-specific test scripts\n   - Configure integration testing with other workspace packages\n\n8. **Build and Deployment**\n   - Configure build system for the package type\n   - Set up build artifacts and output directories\n   - Configure bundling and optimization\n   - Set up package publishing configuration (if library)\n   - Configure deployment scripts (if application)\n\n9. **Documentation and Examples**\n   - Create package README with installation and usage instructions\n   - Set up API documentation generation\n   - Create usage examples and demos\n   - Document package architecture and design decisions\n   - Add package to workspace documentation\n\n10. **Validation and Integration Testing**\n    - Verify package builds successfully\n    - Test package installation and imports\n    - Validate workspace dependency resolution\n    - Test development workflow and hot reloading\n    - Verify CI/CD pipeline includes new package\n    - Test cross-package functionality and integration"
              },
              {
                "name": "/add-performance-monitoring",
                "description": "Setup application performance monitoring",
                "path": "plugins/all-commands/commands/add-performance-monitoring.md",
                "frontmatter": {
                  "description": "Setup application performance monitoring",
                  "category": "monitoring-observability",
                  "allowed-tools": "Glob"
                },
                "content": "# Add Performance Monitoring\n\nSetup application performance monitoring\n\n## Instructions\n\n1. **Performance Monitoring Strategy**\n   - Define key performance indicators (KPIs) and service level objectives (SLOs)\n   - Identify critical user journeys and performance bottlenecks\n   - Plan monitoring architecture and data collection strategy\n   - Assess existing monitoring infrastructure and integration points\n   - Define alerting thresholds and escalation procedures\n\n2. **Application Performance Monitoring (APM)**\n   - Set up comprehensive APM monitoring:\n\n   **Node.js APM with New Relic:**\n   ```javascript\n   // newrelic.js\n   exports.config = {\n     app_name: [process.env.NEW_RELIC_APP_NAME || 'My Application'],\n     license_key: process.env.NEW_RELIC_LICENSE_KEY,\n     distributed_tracing: {\n       enabled: true\n     },\n     transaction_tracer: {\n       enabled: true,\n       transaction_threshold: 0.5, // 500ms\n       record_sql: 'obfuscated',\n       explain_threshold: 1000 // 1 second\n     },\n     error_collector: {\n       enabled: true,\n       ignore_status_codes: [404, 401]\n     },\n     browser_monitoring: {\n       enable: true\n     },\n     application_logging: {\n       forwarding: {\n         enabled: true\n       }\n     }\n   };\n\n   // app.js\n   require('newrelic');\n   const express = require('express');\n   const app = express();\n\n   // Custom metrics\n   const newrelic = require('newrelic');\n\n   app.use((req, res, next) => {\n     const startTime = Date.now();\n     \n     res.on('finish', () => {\n       const duration = Date.now() - startTime;\n       \n       // Record custom metrics\n       newrelic.recordMetric('Custom/ResponseTime', duration);\n       newrelic.recordMetric(`Custom/Endpoint/${req.path}`, duration);\n       \n       // Add custom attributes\n       newrelic.addCustomAttributes({\n         'user.id': req.user?.id,\n         'request.method': req.method,\n         'response.statusCode': res.statusCode\n       });\n     });\n     \n     next();\n   });\n   ```\n\n   **Datadog APM Integration:**\n   ```javascript\n   // datadog-tracer.js\n   const tracer = require('dd-trace').init({\n     service: 'my-application',\n     env: process.env.NODE_ENV,\n     version: process.env.APP_VERSION,\n     logInjection: true,\n     runtimeMetrics: true,\n     profiling: true,\n     analytics: true\n   });\n\n   // Custom instrumentation\n   class PerformanceTracker {\n     static startSpan(operationName, options = {}) {\n       return tracer.startSpan(operationName, {\n         tags: {\n           'service.name': 'my-application',\n           ...options.tags\n         },\n         ...options\n       });\n     }\n\n     static async traceAsync(operationName, asyncFn, tags = {}) {\n       const span = this.startSpan(operationName, { tags });\n       \n       try {\n         const result = await asyncFn(span);\n         span.setTag('operation.success', true);\n         return result;\n       } catch (error) {\n         span.setTag('operation.success', false);\n         span.setTag('error.message', error.message);\n         span.setTag('error.stack', error.stack);\n         throw error;\n       } finally {\n         span.finish();\n       }\n     }\n\n     static trackDatabaseQuery(query, duration, success) {\n       tracer.startSpan('database.query', {\n         tags: {\n           'db.statement': query,\n           'db.duration': duration,\n           'db.success': success\n         }\n       }).finish();\n     }\n   }\n\n   // Usage example\n   app.get('/api/users/:id', async (req, res) => {\n     await PerformanceTracker.traceAsync('get_user', async (span) => {\n       span.setTag('user.id', req.params.id);\n       \n       const user = await getUserFromDatabase(req.params.id);\n       span.setTag('user.found', !!user);\n       \n       res.json(user);\n     }, { endpoint: '/api/users/:id' });\n   });\n   ```\n\n3. **Real User Monitoring (RUM)**\n   - Implement client-side performance tracking:\n\n   **Web Vitals Monitoring:**\n   ```javascript\n   // performance-monitor.js\n   import { getCLS, getFID, getFCP, getLCP, getTTFB } from 'web-vitals';\n\n   class RealUserMonitoring {\n     constructor() {\n       this.metrics = {};\n       this.setupWebVitals();\n       this.setupCustomMetrics();\n     }\n\n     setupWebVitals() {\n       getCLS(this.sendMetric.bind(this, 'CLS'));\n       getFID(this.sendMetric.bind(this, 'FID'));\n       getFCP(this.sendMetric.bind(this, 'FCP'));\n       getLCP(this.sendMetric.bind(this, 'LCP'));\n       getTTFB(this.sendMetric.bind(this, 'TTFB'));\n     }\n\n     setupCustomMetrics() {\n       // Track page load performance\n       window.addEventListener('load', () => {\n         const navigation = performance.getEntriesByType('navigation')[0];\n         \n         this.sendMetric('page_load_time', {\n           name: 'page_load_time',\n           value: navigation.loadEventEnd - navigation.fetchStart,\n           delta: navigation.loadEventEnd - navigation.fetchStart\n         });\n\n         this.sendMetric('dom_content_loaded', {\n           name: 'dom_content_loaded',\n           value: navigation.domContentLoadedEventEnd - navigation.fetchStart,\n           delta: navigation.domContentLoadedEventEnd - navigation.fetchStart\n         });\n       });\n\n       // Track resource loading\n       new PerformanceObserver((list) => {\n         for (const entry of list.getEntries()) {\n           if (entry.duration > 1000) { // Resources taking >1s\n             this.sendMetric('slow_resource', {\n               name: 'slow_resource',\n               value: entry.duration,\n               resource: entry.name,\n               type: entry.initiatorType\n             });\n           }\n         }\n       }).observe({ entryTypes: ['resource'] });\n\n       // Track user interactions\n       ['click', 'keydown', 'touchstart'].forEach(eventType => {\n         document.addEventListener(eventType, (event) => {\n           const startTime = performance.now();\n           \n           requestIdleCallback(() => {\n             const duration = performance.now() - startTime;\n             if (duration > 100) { // Interactions taking >100ms\n               this.sendMetric('slow_interaction', {\n                 name: 'slow_interaction',\n                 value: duration,\n                 eventType: eventType,\n                 target: event.target.tagName\n               });\n             }\n           });\n         });\n       });\n     }\n\n     sendMetric(metricName, metric) {\n       const data = {\n         name: metricName,\n         value: metric.value,\n         delta: metric.delta,\n         id: metric.id,\n         url: window.location.href,\n         userAgent: navigator.userAgent,\n         timestamp: Date.now(),\n         sessionId: this.getSessionId(),\n         userId: this.getUserId()\n       };\n\n       // Send to analytics endpoint\n       navigator.sendBeacon('/api/metrics', JSON.stringify(data));\n     }\n\n     getSessionId() {\n       return sessionStorage.getItem('sessionId') || 'anonymous';\n     }\n\n     getUserId() {\n       return localStorage.getItem('userId') || 'anonymous';\n     }\n   }\n\n   // Initialize RUM\n   new RealUserMonitoring();\n   ```\n\n   **React Performance Monitoring:**\n   ```javascript\n   // react-performance.js\n   import { Profiler } from 'react';\n\n   class ReactPerformanceMonitor {\n     static ProfilerWrapper = ({ id, children }) => {\n       const onRenderCallback = (id, phase, actualDuration, baseDuration, startTime, commitTime) => {\n         // Track component render performance\n         if (actualDuration > 100) { // Renders taking >100ms\n           console.warn(`Slow render detected for ${id}:`, {\n             phase,\n             actualDuration,\n             baseDuration,\n             startTime,\n             commitTime\n           });\n\n           // Send to monitoring service\n           fetch('/api/metrics/react-performance', {\n             method: 'POST',\n             headers: { 'Content-Type': 'application/json' },\n             body: JSON.stringify({\n               componentId: id,\n               phase,\n               actualDuration,\n               baseDuration,\n               timestamp: Date.now()\n             })\n           });\n         }\n       };\n\n       return (\n         <Profiler id={id} onRender={onRenderCallback}>\n           {children}\n         </Profiler>\n       );\n     };\n\n     static usePerformanceTracking(componentName) {\n       useEffect(() => {\n         const startTime = performance.now();\n         \n         return () => {\n           const duration = performance.now() - startTime;\n           if (duration > 1000) { // Component mounted for >1s\n             console.log(`${componentName} lifecycle duration:`, duration);\n           }\n         };\n       }, [componentName]);\n     }\n   }\n\n   // Usage\n   function App() {\n     return (\n       <ReactPerformanceMonitor.ProfilerWrapper id=\"App\">\n         <Dashboard />\n         <UserList />\n       </ReactPerformanceMonitor.ProfilerWrapper>\n     );\n   }\n   ```\n\n4. **Server Performance Monitoring**\n   - Monitor server-side performance metrics:\n\n   **System Metrics Collection:**\n   ```javascript\n   // system-monitor.js\n   const os = require('os');\n   const process = require('process');\n   const v8 = require('v8');\n\n   class SystemMonitor {\n     constructor() {\n       this.startTime = Date.now();\n       this.intervalId = null;\n     }\n\n     start(interval = 30000) { // 30 seconds\n       this.intervalId = setInterval(() => {\n         this.collectMetrics();\n       }, interval);\n     }\n\n     stop() {\n       if (this.intervalId) {\n         clearInterval(this.intervalId);\n       }\n     }\n\n     collectMetrics() {\n       const metrics = {\n         // CPU metrics\n         cpuUsage: process.cpuUsage(),\n         loadAverage: os.loadavg(),\n         \n         // Memory metrics\n         memoryUsage: process.memoryUsage(),\n         totalMemory: os.totalmem(),\n         freeMemory: os.freemem(),\n         \n         // V8 heap statistics\n         heapStats: v8.getHeapStatistics(),\n         heapSpaceStats: v8.getHeapSpaceStatistics(),\n         \n         // Process metrics\n         uptime: process.uptime(),\n         pid: process.pid,\n         \n         // Event loop lag\n         eventLoopLag: this.measureEventLoopLag(),\n         \n         timestamp: Date.now()\n       };\n\n       this.sendMetrics(metrics);\n     }\n\n     measureEventLoopLag() {\n       const start = process.hrtime.bigint();\n       setImmediate(() => {\n         const lag = Number(process.hrtime.bigint() - start) / 1000000; // Convert to ms\n         return lag;\n       });\n     }\n\n     sendMetrics(metrics) {\n       // Send to monitoring service\n       console.log('System Metrics:', JSON.stringify(metrics, null, 2));\n       \n       // Example: Send to StatsD\n       // statsd.gauge('system.memory.used', metrics.memoryUsage.used);\n       // statsd.gauge('system.cpu.usage', metrics.cpuUsage.system);\n     }\n   }\n\n   // Start monitoring\n   const monitor = new SystemMonitor();\n   monitor.start();\n\n   // Graceful shutdown\n   process.on('SIGTERM', () => {\n     monitor.stop();\n     process.exit(0);\n   });\n   ```\n\n   **Express.js Performance Middleware:**\n   ```javascript\n   // performance-middleware.js\n   const responseTime = require('response-time');\n   const promClient = require('prom-client');\n\n   // Prometheus metrics\n   const httpRequestDuration = new promClient.Histogram({\n     name: 'http_request_duration_seconds',\n     help: 'Duration of HTTP requests in seconds',\n     labelNames: ['method', 'route', 'status_code'],\n     buckets: [0.1, 0.3, 0.5, 0.7, 1, 3, 5, 7, 10]\n   });\n\n   const httpRequestsTotal = new promClient.Counter({\n     name: 'http_requests_total',\n     help: 'Total number of HTTP requests',\n     labelNames: ['method', 'route', 'status_code']\n   });\n\n   function performanceMiddleware() {\n     return (req, res, next) => {\n       const startTime = Date.now();\n       const startHrTime = process.hrtime();\n\n       res.on('finish', () => {\n         const duration = Date.now() - startTime;\n         const hrDuration = process.hrtime(startHrTime);\n         const durationSeconds = hrDuration[0] + hrDuration[1] / 1e9;\n\n         const labels = {\n           method: req.method,\n           route: req.route?.path || req.path,\n           status_code: res.statusCode\n         };\n\n         // Record Prometheus metrics\n         httpRequestDuration.observe(labels, durationSeconds);\n         httpRequestsTotal.inc(labels);\n\n         // Log slow requests\n         if (duration > 1000) {\n           console.warn('Slow request detected:', {\n             method: req.method,\n             url: req.url,\n             duration: duration,\n             statusCode: res.statusCode,\n             userAgent: req.get('User-Agent'),\n             ip: req.ip\n           });\n         }\n\n         // Track custom metrics\n         req.performanceMetrics = {\n           duration,\n           memoryUsage: process.memoryUsage(),\n           cpuUsage: process.cpuUsage()\n         };\n       });\n\n       next();\n     };\n   }\n\n   module.exports = { performanceMiddleware, httpRequestDuration, httpRequestsTotal };\n   ```\n\n5. **Database Performance Monitoring**\n   - Monitor database query performance:\n\n   **Query Performance Tracking:**\n   ```javascript\n   // db-performance.js\n   const { Pool } = require('pg');\n\n   class DatabasePerformanceMonitor {\n     constructor(pool) {\n       this.pool = pool;\n       this.slowQueryThreshold = 1000; // 1 second\n       this.queryStats = new Map();\n     }\n\n     async executeQuery(query, params = []) {\n       const queryId = this.generateQueryId(query);\n       const startTime = Date.now();\n       const startMemory = process.memoryUsage();\n\n       try {\n         const result = await this.pool.query(query, params);\n         const duration = Date.now() - startTime;\n         const endMemory = process.memoryUsage();\n\n         this.recordQueryMetrics(queryId, query, duration, true, endMemory.heapUsed - startMemory.heapUsed);\n\n         if (duration > this.slowQueryThreshold) {\n           this.logSlowQuery(query, params, duration);\n         }\n\n         return result;\n       } catch (error) {\n         const duration = Date.now() - startTime;\n         this.recordQueryMetrics(queryId, query, duration, false, 0);\n         throw error;\n       }\n     }\n\n     generateQueryId(query) {\n       // Normalize query for grouping similar queries\n       return query\n         .replace(/\\$\\d+/g, '?') // Replace parameter placeholders\n         .replace(/\\s+/g, ' ')   // Normalize whitespace\n         .replace(/\\d+/g, 'N')   // Replace numbers with 'N'\n         .trim()\n         .toLowerCase();\n     }\n\n     recordQueryMetrics(queryId, query, duration, success, memoryDelta) {\n       if (!this.queryStats.has(queryId)) {\n         this.queryStats.set(queryId, {\n           query: query,\n           count: 0,\n           totalDuration: 0,\n           successCount: 0,\n           errorCount: 0,\n           averageDuration: 0,\n           maxDuration: 0,\n           minDuration: Infinity\n         });\n       }\n\n       const stats = this.queryStats.get(queryId);\n       stats.count++;\n       stats.totalDuration += duration;\n       stats.averageDuration = stats.totalDuration / stats.count;\n       stats.maxDuration = Math.max(stats.maxDuration, duration);\n       stats.minDuration = Math.min(stats.minDuration, duration);\n\n       if (success) {\n         stats.successCount++;\n       } else {\n         stats.errorCount++;\n       }\n\n       // Send metrics to monitoring service\n       this.sendQueryMetrics(queryId, duration, success, memoryDelta);\n     }\n\n     logSlowQuery(query, params, duration) {\n       console.warn('Slow query detected:', {\n         query: query,\n         params: params,\n         duration: duration,\n         timestamp: new Date().toISOString()\n       });\n\n       // Send alert to monitoring service\n       this.sendSlowQueryAlert(query, params, duration);\n     }\n\n     sendQueryMetrics(queryId, duration, success, memoryDelta) {\n       const metrics = {\n         queryId,\n         duration,\n         success,\n         memoryDelta,\n         timestamp: Date.now()\n       };\n\n       // Send to your monitoring service\n       // Example: StatsD, Prometheus, DataDog, etc.\n     }\n\n     sendSlowQueryAlert(query, params, duration) {\n       // Send to alerting system\n       console.log('Sending slow query alert...', { query, duration });\n     }\n\n     getQueryStats() {\n       return Array.from(this.queryStats.entries()).map(([queryId, stats]) => ({\n         queryId,\n         ...stats\n       }));\n     }\n\n     resetStats() {\n       this.queryStats.clear();\n     }\n   }\n\n   // Usage\n   const pool = new Pool();\n   const dbMonitor = new DatabasePerformanceMonitor(pool);\n\n   // Replace direct pool usage with monitored version\n   module.exports = { executeQuery: dbMonitor.executeQuery.bind(dbMonitor) };\n   ```\n\n6. **Error Tracking and Monitoring**\n   - Implement comprehensive error monitoring:\n\n   **Error Tracking Setup:**\n   ```javascript\n   // error-monitor.js\n   const Sentry = require('@sentry/node');\n   const Integrations = require('@sentry/integrations');\n\n   class ErrorMonitor {\n     static initialize() {\n       Sentry.init({\n         dsn: process.env.SENTRY_DSN,\n         environment: process.env.NODE_ENV,\n         integrations: [\n           new Integrations.Http({ tracing: true }),\n           new Sentry.Integrations.Express({ app }),\n         ],\n         tracesSampleRate: process.env.NODE_ENV === 'production' ? 0.1 : 1.0,\n         beforeSend(event, hint) {\n           // Filter out noise\n           if (event.exception) {\n             const error = hint.originalException;\n             if (error && error.code === 'ECONNABORTED') {\n               return null; // Don't send timeout errors\n             }\n           }\n           return event;\n         },\n         beforeBreadcrumb(breadcrumb) {\n           // Filter sensitive data from breadcrumbs\n           if (breadcrumb.category === 'http') {\n             delete breadcrumb.data?.password;\n             delete breadcrumb.data?.token;\n           }\n           return breadcrumb;\n         }\n       });\n     }\n\n     static captureException(error, context = {}) {\n       Sentry.withScope((scope) => {\n         Object.keys(context).forEach(key => {\n           scope.setContext(key, context[key]);\n         });\n         Sentry.captureException(error);\n       });\n     }\n\n     static captureMessage(message, level = 'info', context = {}) {\n       Sentry.withScope((scope) => {\n         Object.keys(context).forEach(key => {\n           scope.setContext(key, context[key]);\n         });\n         Sentry.captureMessage(message, level);\n       });\n     }\n\n     static setupExpressErrorHandling(app) {\n       // Sentry request handler (must be first)\n       app.use(Sentry.Handlers.requestHandler());\n       app.use(Sentry.Handlers.tracingHandler());\n\n       // Your routes here\n\n       // Sentry error handler (must be before other error handlers)\n       app.use(Sentry.Handlers.errorHandler());\n\n       // Custom error handler\n       app.use((error, req, res, next) => {\n         const errorId = res.sentry;\n         \n         console.error('Unhandled error:', {\n           errorId,\n           error: error.message,\n           stack: error.stack,\n           url: req.url,\n           method: req.method,\n           userAgent: req.get('User-Agent'),\n           ip: req.ip\n         });\n\n         res.status(500).json({\n           error: 'Internal server error',\n           errorId: errorId\n         });\n       });\n     }\n   }\n\n   // Global error handlers\n   process.on('uncaughtException', (error) => {\n     console.error('Uncaught Exception:', error);\n     ErrorMonitor.captureException(error, { type: 'uncaughtException' });\n     process.exit(1);\n   });\n\n   process.on('unhandledRejection', (reason, promise) => {\n     console.error('Unhandled Rejection at:', promise, 'reason:', reason);\n     ErrorMonitor.captureException(new Error(reason), { type: 'unhandledRejection' });\n   });\n   ```\n\n7. **Custom Metrics and Dashboards**\n   - Create custom performance dashboards:\n\n   **Prometheus Metrics:**\n   ```javascript\n   // prometheus-metrics.js\n   const promClient = require('prom-client');\n\n   class CustomMetrics {\n     constructor() {\n       // Register default metrics\n       promClient.register.setDefaultLabels({\n         app: process.env.APP_NAME || 'my-app',\n         version: process.env.APP_VERSION || '1.0.0'\n       });\n       promClient.collectDefaultMetrics();\n\n       this.setupCustomMetrics();\n     }\n\n     setupCustomMetrics() {\n       // Business metrics\n       this.userRegistrations = new promClient.Counter({\n         name: 'user_registrations_total',\n         help: 'Total number of user registrations',\n         labelNames: ['source', 'plan']\n       });\n\n       this.orderValue = new promClient.Histogram({\n         name: 'order_value_dollars',\n         help: 'Order value in dollars',\n         labelNames: ['currency', 'payment_method'],\n         buckets: [10, 50, 100, 500, 1000, 5000]\n       });\n\n       this.cacheHitRate = new promClient.Gauge({\n         name: 'cache_hit_rate',\n         help: 'Cache hit rate percentage',\n         labelNames: ['cache_type']\n       });\n\n       this.activeUsers = new promClient.Gauge({\n         name: 'active_users_current',\n         help: 'Currently active users',\n         labelNames: ['session_type']\n       });\n\n       // Performance metrics\n       this.databaseConnectionPool = new promClient.Gauge({\n         name: 'database_connections_active',\n         help: 'Active database connections',\n         labelNames: ['pool_name']\n       });\n\n       this.apiResponseTime = new promClient.Histogram({\n         name: 'api_response_time_seconds',\n         help: 'API response time in seconds',\n         labelNames: ['endpoint', 'method', 'status'],\n         buckets: [0.1, 0.5, 1, 2, 5, 10]\n       });\n     }\n\n     // Helper methods\n     recordUserRegistration(source, plan) {\n       this.userRegistrations.inc({ source, plan });\n     }\n\n     recordOrderValue(value, currency, paymentMethod) {\n       this.orderValue.observe({ currency, payment_method: paymentMethod }, value);\n     }\n\n     updateCacheHitRate(cacheType, hitRate) {\n       this.cacheHitRate.set({ cache_type: cacheType }, hitRate);\n     }\n\n     setActiveUsers(count, sessionType = 'web') {\n       this.activeUsers.set({ session_type: sessionType }, count);\n     }\n\n     getMetrics() {\n       return promClient.register.metrics();\n     }\n   }\n\n   const metrics = new CustomMetrics();\n\n   // Metrics endpoint\n   app.get('/metrics', async (req, res) => {\n     res.set('Content-Type', promClient.register.contentType);\n     res.end(await metrics.getMetrics());\n   });\n\n   module.exports = metrics;\n   ```\n\n8. **Alerting and Notification System**\n   - Set up intelligent alerting:\n\n   **Alert Manager:**\n   ```javascript\n   // alert-manager.js\n   const nodemailer = require('nodemailer');\n   const slack = require('@slack/webhook');\n\n   class AlertManager {\n     constructor() {\n       this.emailTransporter = nodemailer.createTransporter({\n         // Email configuration\n       });\n       \n       this.slackWebhook = new slack.IncomingWebhook(process.env.SLACK_WEBHOOK_URL);\n       \n       this.alertThresholds = {\n         responseTime: 2000, // 2 seconds\n         errorRate: 0.05,    // 5%\n         cpuUsage: 0.8,      // 80%\n         memoryUsage: 0.9,   // 90%\n         diskUsage: 0.85     // 85%\n       };\n       \n       this.alertCooldowns = new Map();\n     }\n\n     async checkPerformanceThresholds(metrics) {\n       const alerts = [];\n\n       // Response time alert\n       if (metrics.averageResponseTime > this.alertThresholds.responseTime) {\n         alerts.push({\n           severity: 'warning',\n           metric: 'response_time',\n           current: metrics.averageResponseTime,\n           threshold: this.alertThresholds.responseTime,\n           message: `Average response time is ${metrics.averageResponseTime}ms (threshold: ${this.alertThresholds.responseTime}ms)`\n         });\n       }\n\n       // Error rate alert\n       if (metrics.errorRate > this.alertThresholds.errorRate) {\n         alerts.push({\n           severity: 'critical',\n           metric: 'error_rate',\n           current: metrics.errorRate,\n           threshold: this.alertThresholds.errorRate,\n           message: `Error rate is ${(metrics.errorRate * 100).toFixed(2)}% (threshold: ${(this.alertThresholds.errorRate * 100)}%)`\n         });\n       }\n\n       // System resource alerts\n       if (metrics.cpuUsage > this.alertThresholds.cpuUsage) {\n         alerts.push({\n           severity: 'warning',\n           metric: 'cpu_usage',\n           current: metrics.cpuUsage,\n           threshold: this.alertThresholds.cpuUsage,\n           message: `CPU usage is ${(metrics.cpuUsage * 100).toFixed(1)}% (threshold: ${(this.alertThresholds.cpuUsage * 100)}%)`\n         });\n       }\n\n       // Send alerts\n       for (const alert of alerts) {\n         await this.sendAlert(alert);\n       }\n     }\n\n     async sendAlert(alert) {\n       const alertKey = `${alert.metric}_${alert.severity}`;\n       const now = Date.now();\n       const cooldownPeriod = alert.severity === 'critical' ? 300000 : 900000; // 5min for critical, 15min for others\n\n       // Check cooldown\n       if (this.alertCooldowns.has(alertKey)) {\n         const lastAlert = this.alertCooldowns.get(alertKey);\n         if (now - lastAlert < cooldownPeriod) {\n           return; // Skip this alert due to cooldown\n         }\n       }\n\n       this.alertCooldowns.set(alertKey, now);\n\n       // Send to multiple channels\n       await Promise.all([\n         this.sendSlackAlert(alert),\n         this.sendEmailAlert(alert),\n         this.logAlert(alert)\n       ]);\n     }\n\n     async sendSlackAlert(alert) {\n       const color = alert.severity === 'critical' ? 'danger' : 'warning';\n       const emoji = alert.severity === 'critical' ? ':rotating_light:' : ':warning:';\n       \n       await this.slackWebhook.send({\n         text: `${emoji} Performance Alert`,\n         attachments: [{\n           color: color,\n           fields: [\n             { title: 'Metric', value: alert.metric, short: true },\n             { title: 'Severity', value: alert.severity, short: true },\n             { title: 'Current Value', value: alert.current.toString(), short: true },\n             { title: 'Threshold', value: alert.threshold.toString(), short: true },\n             { title: 'Message', value: alert.message, short: false }\n           ],\n           ts: Math.floor(Date.now() / 1000)\n         }]\n       });\n     }\n\n     async sendEmailAlert(alert) {\n       if (alert.severity === 'critical') {\n         await this.emailTransporter.sendMail({\n           to: process.env.ALERT_EMAIL,\n           subject: `CRITICAL: ${alert.metric} alert`,\n           html: `\n             <h2>Performance Alert</h2>\n             <p><strong>Severity:</strong> ${alert.severity}</p>\n             <p><strong>Metric:</strong> ${alert.metric}</p>\n             <p><strong>Message:</strong> ${alert.message}</p>\n             <p><strong>Current Value:</strong> ${alert.current}</p>\n             <p><strong>Threshold:</strong> ${alert.threshold}</p>\n             <p><strong>Time:</strong> ${new Date().toISOString()}</p>\n           `\n         });\n       }\n     }\n\n     logAlert(alert) {\n       console.error('PERFORMANCE ALERT:', {\n         timestamp: new Date().toISOString(),\n         severity: alert.severity,\n         metric: alert.metric,\n         current: alert.current,\n         threshold: alert.threshold,\n         message: alert.message\n       });\n     }\n   }\n\n   module.exports = AlertManager;\n   ```\n\n9. **Performance Testing Integration**\n   - Integrate with performance testing:\n\n   **Load Test Monitoring:**\n   ```javascript\n   // load-test-monitor.js\n   class LoadTestMonitor {\n     constructor() {\n       this.testResults = [];\n       this.baselineMetrics = null;\n     }\n\n     async runPerformanceTest(testConfig) {\n       console.log('Starting performance test...', testConfig);\n       \n       const startMetrics = await this.captureSystemMetrics();\n       const startTime = Date.now();\n\n       try {\n         // Run the actual load test (using k6, artillery, etc.)\n         const testResults = await this.executeLoadTest(testConfig);\n         \n         const endTime = Date.now();\n         const endMetrics = await this.captureSystemMetrics();\n\n         const result = {\n           testId: this.generateTestId(),\n           config: testConfig,\n           duration: endTime - startTime,\n           startMetrics,\n           endMetrics,\n           testResults,\n           timestamp: new Date().toISOString()\n         };\n\n         this.testResults.push(result);\n         await this.analyzeResults(result);\n         \n         return result;\n       } catch (error) {\n         console.error('Load test failed:', error);\n         throw error;\n       }\n     }\n\n     async captureSystemMetrics() {\n       return {\n         cpu: os.loadavg(),\n         memory: {\n           total: os.totalmem(),\n           free: os.freemem(),\n           used: os.totalmem() - os.freemem()\n         },\n         processes: await this.getProcessMetrics()\n       };\n     }\n\n     async analyzeResults(result) {\n       const analysis = {\n         performanceRegression: false,\n         recommendations: []\n       };\n\n       // Compare with baseline\n       if (this.baselineMetrics) {\n         const responseTimeIncrease = (result.testResults.averageResponseTime - this.baselineMetrics.averageResponseTime) / this.baselineMetrics.averageResponseTime;\n         \n         if (responseTimeIncrease > 0.2) { // 20% increase\n           analysis.performanceRegression = true;\n           analysis.recommendations.push(`Response time increased by ${(responseTimeIncrease * 100).toFixed(1)}%`);\n         }\n       }\n\n       // Resource utilization analysis\n       const maxCpuUsage = Math.max(...result.endMetrics.cpu);\n       if (maxCpuUsage > 0.8) {\n         analysis.recommendations.push('High CPU usage detected - consider scaling');\n       }\n\n       const memoryUsagePercent = result.endMetrics.memory.used / result.endMetrics.memory.total;\n       if (memoryUsagePercent > 0.9) {\n         analysis.recommendations.push('High memory usage detected - check for memory leaks');\n       }\n\n       console.log('Performance test analysis:', analysis);\n       return analysis;\n     }\n\n     setBaseline(testResult) {\n       this.baselineMetrics = testResult.testResults;\n       console.log('Baseline metrics set:', this.baselineMetrics);\n     }\n\n     generateTestId() {\n       return `test_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n     }\n   }\n   ```\n\n10. **Performance Optimization Recommendations**\n    - Generate actionable performance insights:\n\n    **Performance Analyzer:**\n    ```javascript\n    // performance-analyzer.js\n    class PerformanceAnalyzer {\n      constructor() {\n        this.metrics = [];\n        this.thresholds = {\n          responseTime: { good: 200, warning: 1000, critical: 3000 },\n          memoryUsage: { good: 0.6, warning: 0.8, critical: 0.9 },\n          cpuUsage: { good: 0.5, warning: 0.7, critical: 0.85 },\n          errorRate: { good: 0.01, warning: 0.05, critical: 0.1 }\n        };\n      }\n\n      analyzePerformance(metrics) {\n        const recommendations = [];\n        const scores = {};\n\n        // Analyze response time\n        if (metrics.averageResponseTime > this.thresholds.responseTime.critical) {\n          recommendations.push({\n            priority: 'high',\n            category: 'response_time',\n            issue: 'Very slow response times detected',\n            recommendations: [\n              'Implement database query optimization',\n              'Add caching layer (Redis/Memcached)',\n              'Enable CDN for static assets',\n              'Consider horizontal scaling'\n            ],\n            impact: 'Critical user experience impact'\n          });\n          scores.responseTime = 1;\n        } else if (metrics.averageResponseTime > this.thresholds.responseTime.warning) {\n          recommendations.push({\n            priority: 'medium',\n            category: 'response_time',\n            issue: 'Moderate response time issues',\n            recommendations: [\n              'Optimize database queries',\n              'Implement query result caching',\n              'Review N+1 query patterns'\n            ],\n            impact: 'Moderate user experience impact'\n          });\n          scores.responseTime = 6;\n        } else {\n          scores.responseTime = 10;\n        }\n\n        // Analyze memory usage\n        if (metrics.memoryUsage > this.thresholds.memoryUsage.critical) {\n          recommendations.push({\n            priority: 'high',\n            category: 'memory',\n            issue: 'Critical memory usage',\n            recommendations: [\n              'Check for memory leaks',\n              'Implement garbage collection tuning',\n              'Add more memory or scale horizontally',\n              'Review large object allocations'\n            ],\n            impact: 'Risk of application crashes'\n          });\n          scores.memory = 2;\n        }\n\n        // Analyze error rate\n        if (metrics.errorRate > this.thresholds.errorRate.critical) {\n          recommendations.push({\n            priority: 'high',\n            category: 'reliability',\n            issue: 'High error rate detected',\n            recommendations: [\n              'Review application logs for error patterns',\n              'Implement circuit breakers',\n              'Add retry mechanisms',\n              'Improve error handling'\n            ],\n            impact: 'Significant functionality issues'\n          });\n          scores.reliability = 3;\n        }\n\n        const overallScore = Object.values(scores).reduce((a, b) => a + b, 0) / Object.keys(scores).length;\n\n        return {\n          overallScore: Math.round(overallScore),\n          grade: this.getPerformanceGrade(overallScore),\n          recommendations: recommendations.sort((a, b) => {\n            const priorityOrder = { high: 3, medium: 2, low: 1 };\n            return priorityOrder[b.priority] - priorityOrder[a.priority];\n          }),\n          metrics,\n          timestamp: new Date().toISOString()\n        };\n      }\n\n      getPerformanceGrade(score) {\n        if (score >= 9) return 'A';\n        if (score >= 8) return 'B';\n        if (score >= 7) return 'C';\n        if (score >= 6) return 'D';\n        return 'F';\n      }\n\n      generateReport(analysis) {\n        return {\n          summary: {\n            grade: analysis.grade,\n            score: analysis.overallScore,\n            criticalIssues: analysis.recommendations.filter(r => r.priority === 'high').length,\n            totalRecommendations: analysis.recommendations.length\n          },\n          keyMetrics: {\n            responseTime: analysis.metrics.averageResponseTime,\n            errorRate: (analysis.metrics.errorRate * 100).toFixed(2) + '%',\n            memoryUsage: (analysis.metrics.memoryUsage * 100).toFixed(1) + '%',\n            cpuUsage: (analysis.metrics.cpuUsage * 100).toFixed(1) + '%'\n          },\n          recommendations: analysis.recommendations,\n          generatedAt: analysis.timestamp\n        };\n      }\n    }\n\n    module.exports = PerformanceAnalyzer;\n    ```"
              },
              {
                "name": "/add-property-based-testing",
                "description": "Implement property-based testing framework",
                "path": "plugins/all-commands/commands/add-property-based-testing.md",
                "frontmatter": {
                  "description": "Implement property-based testing framework",
                  "category": "code-analysis-testing"
                },
                "content": "# Add Property-Based Testing\n\nImplement property-based testing framework\n\n## Instructions\n\n1. **Property-Based Testing Analysis**\n   - Analyze current codebase to identify functions suitable for property-based testing\n   - Identify mathematical properties, invariants, and business rules to test\n   - Assess existing testing infrastructure and integration requirements\n   - Determine scope of property-based testing implementation\n   - Plan integration with existing unit and integration tests\n\n2. **Framework Selection and Installation**\n   - Choose appropriate property-based testing framework:\n     - **JavaScript/TypeScript**: fast-check, JSVerify\n     - **Python**: Hypothesis, QuickCheck\n     - **Java**: jqwik, QuickTheories\n     - **C#**: FsCheck, CsCheck\n     - **Rust**: proptest, quickcheck\n     - **Go**: gopter, quick\n   - Install framework and configure with existing test runner\n   - Set up framework integration with build system\n\n3. **Property Definition and Implementation**\n   - Define mathematical properties and invariants for core functions\n   - Implement property tests for data transformation functions\n   - Create property tests for API contract validation\n   - Set up property tests for business logic validation\n   - Define properties for data structure consistency\n\n4. **Test Data Generation**\n   - Configure generators for primitive data types\n   - Create custom generators for domain-specific objects\n   - Set up composite generators for complex data structures\n   - Configure generator constraints and boundaries\n   - Implement shrinking strategies for minimal failing examples\n\n5. **Property Test Categories**\n   - **Roundtrip Properties**: Serialize/deserialize, encode/decode operations\n   - **Invariant Properties**: Data structure consistency, business rule validation\n   - **Metamorphic Properties**: Equivalent operations, transformation consistency\n   - **Model-Based Properties**: State machine testing, system behavior validation\n   - **Oracle Properties**: Comparison with reference implementations\n\n6. **Integration with Existing Tests**\n   - Integrate property-based tests with existing test suites\n   - Configure test execution order and dependencies\n   - Set up property test reporting and coverage tracking\n   - Configure test timeout and resource management\n   - Implement property test categorization and tagging\n\n7. **Advanced Testing Strategies**\n   - Set up stateful property testing for complex systems\n   - Configure model-based testing for state machines\n   - Implement targeted property testing for known issues\n   - Set up regression property testing for bug prevention\n   - Configure performance property testing for algorithmic validation\n\n8. **Test Configuration and Tuning**\n   - Configure test case generation limits and timeouts\n   - Set up shrinking parameters and strategies\n   - Configure random seed management for reproducibility\n   - Set up test distribution and statistical analysis\n   - Configure parallel test execution and resource management\n\n9. **CI/CD Integration**\n   - Configure property-based tests in continuous integration\n   - Set up test result reporting and failure analysis\n   - Configure test execution policies and resource limits\n   - Set up automated property test maintenance\n   - Configure property test performance monitoring\n\n10. **Documentation and Team Training**\n    - Create comprehensive property-based testing documentation\n    - Document property definition patterns and best practices\n    - Create examples and templates for common property patterns\n    - Train team on property-based testing concepts and implementation\n    - Set up property test maintenance and evolution guidelines\n    - Document troubleshooting procedures for property test failures"
              },
              {
                "name": "/add-to-changelog",
                "description": "Add a new entry to the project's CHANGELOG.md file following Keep a Changelog format",
                "path": "plugins/all-commands/commands/add-to-changelog.md",
                "frontmatter": {
                  "description": "Add a new entry to the project's CHANGELOG.md file following Keep a Changelog format",
                  "category": "documentation-changelogs",
                  "argument-hint": "<version> <change_type> <message>",
                  "allowed-tools": "Read, Edit"
                },
                "content": "# Update Changelog\n\nAdd a new entry to the project's CHANGELOG.md file based on the provided arguments.\n\n## Parse Arguments\n\nParse $ARGUMENTS to extract:\n- Version number (e.g., \"1.1.0\")\n- Change type: \"added\", \"changed\", \"deprecated\", \"removed\", \"fixed\", or \"security\"\n- `<message>` is the description of the change\n\n## Examples\n\n```\n/add-to-changelog 1.1.0 added \"New markdown to BlockDoc conversion feature\"\n```\n\n```\n/add-to-changelog 1.0.2 fixed \"Bug in HTML renderer causing incorrect output\"\n```\n\n## Description\n\nThis command will:\n\n1. Check if a CHANGELOG.md file exists and create one if needed\n2. Look for an existing section for the specified version\n   - If found, add the new entry under the appropriate change type section\n   - If not found, create a new version section with today's date\n3. Format the entry according to Keep a Changelog conventions\n4. Commit the changes if requested\n\nThe CHANGELOG follows the [Keep a Changelog](https://keepachangelog.com/) format and [Semantic Versioning](https://semver.org/).\n\n## Implementation\n\nThe command should:\n\n1. Parse the arguments to extract version, change type, and message\n2. Read the existing CHANGELOG.md file if it exists\n3. If the file doesn't exist, create a new one with standard header\n4. Check if the version section already exists\n5. Add the new entry in the appropriate section\n6. Write the updated content back to the file\n7. Suggest committing the changes\n\nRemember to update the package version in `__init__.py` and `setup.py` if this is a new version."
              },
              {
                "name": "/all-tools",
                "description": "Display all available development tools",
                "path": "plugins/all-commands/commands/all-tools.md",
                "frontmatter": {
                  "description": "Display all available development tools",
                  "category": "utilities-debugging"
                },
                "content": "# Display All Available Development Tools\n\nDisplay all available development tools\n\n*Command originally created by IndyDevDan (YouTube: https://www.youtube.com/@indydevdan) / DislerH (GitHub: https://github.com/disler)*\n\n## Instructions\n\nDisplay all available tools from your system prompt in the following format:\n\n1. **List each tool** with its TypeScript function signature\n2. **Include the purpose** of each tool as a suffix\n3. **Use double line breaks** between tools for readability\n4. **Format as bullet points** for clear organization\n\nThe output should help developers understand:\n- What tools are available in the current Claude Code session\n- The exact function signatures for reference\n- The primary purpose of each tool\n\nExample format:\n```typescript\n functionName(parameters: Type): ReturnType - Purpose of the tool\n\n anotherFunction(params: ParamType): ResultType - What this tool does\n```\n\nThis command is useful for:\n- Quick reference of available capabilities\n- Understanding tool signatures\n- Planning which tools to use for specific tasks"
              },
              {
                "name": "/architecture-review",
                "description": "Review and improve system architecture",
                "path": "plugins/all-commands/commands/architecture-review.md",
                "frontmatter": {
                  "description": "Review and improve system architecture",
                  "category": "team-collaboration"
                },
                "content": "# Architecture Review Command\n\nReview and improve system architecture\n\n## Instructions\n\nPerform a comprehensive architectural analysis following these steps:\n\n1. **High-Level Architecture Analysis**\n   - Map out the overall system architecture and components\n   - Identify architectural patterns in use (MVC, MVP, Clean Architecture, etc.)\n   - Review module boundaries and separation of concerns\n   - Analyze the application's layered structure\n\n2. **Design Patterns Assessment**\n   - Identify design patterns used throughout the codebase\n   - Check for proper implementation of common patterns\n   - Look for anti-patterns and code smells\n   - Assess pattern consistency across the application\n\n3. **Dependency Management**\n   - Review dependency injection and inversion of control\n   - Analyze coupling between modules and components\n   - Check for circular dependencies\n   - Assess dependency direction and adherence to dependency rule\n\n4. **Data Flow Architecture**\n   - Trace data flow through the application\n   - Review state management patterns and implementation\n   - Analyze data persistence and storage strategies\n   - Check for proper data validation and transformation\n\n5. **Component Architecture**\n   - Review component design and responsibilities\n   - Check for single responsibility principle adherence\n   - Analyze component composition and reusability\n   - Assess interface design and abstraction levels\n\n6. **Error Handling Architecture**\n   - Review error handling strategy and consistency\n   - Check for proper error propagation and recovery\n   - Analyze logging and monitoring integration\n   - Assess resilience and fault tolerance patterns\n\n7. **Scalability Assessment**\n   - Analyze horizontal and vertical scaling capabilities\n   - Review caching strategies and implementation\n   - Check for stateless design where appropriate\n   - Assess performance bottlenecks and scaling limitations\n\n8. **Security Architecture**\n   - Review security boundaries and trust zones\n   - Check authentication and authorization architecture\n   - Analyze data protection and privacy measures\n   - Assess security pattern implementation\n\n9. **Testing Architecture**\n   - Review test structure and organization\n   - Check for testability in design\n   - Analyze mocking and dependency isolation strategies\n   - Assess test coverage across architectural layers\n\n10. **Configuration Management**\n    - Review configuration handling and environment management\n    - Check for proper separation of config from code\n    - Analyze feature flags and runtime configuration\n    - Assess deployment configuration strategies\n\n11. **Documentation & Communication**\n    - Review architectural documentation and diagrams\n    - Check for clear API contracts and interfaces\n    - Assess code self-documentation and clarity\n    - Analyze team communication patterns in code\n\n12. **Future-Proofing & Extensibility**\n    - Assess the architecture's ability to accommodate change\n    - Review extension points and plugin architectures\n    - Check for proper versioning and backward compatibility\n    - Analyze migration and upgrade strategies\n\n13. **Technology Choices**\n    - Review technology stack alignment with requirements\n    - Assess framework and library choices\n    - Check for consistent technology usage\n    - Analyze technical debt and modernization opportunities\n\n14. **Performance Architecture**\n    - Review caching layers and strategies\n    - Analyze asynchronous processing patterns\n    - Check for proper resource management\n    - Assess monitoring and observability architecture\n\n15. **Recommendations**\n    - Provide specific architectural improvements\n    - Suggest refactoring strategies for problem areas\n    - Recommend patterns and practices for better design\n    - Create a roadmap for architectural evolution\n\nFocus on providing actionable insights with specific examples and clear rationale for recommendations."
              },
              {
                "name": "/architecture-scenario-explorer",
                "description": "Explore architectural decisions through systematic scenario analysis with trade-off evaluation and future-proofing assessment.",
                "path": "plugins/all-commands/commands/architecture-scenario-explorer.md",
                "frontmatter": {
                  "description": "Explore architectural decisions through systematic scenario analysis with trade-off evaluation and future-proofing assessment.",
                  "category": "utilities-debugging",
                  "argument-hint": "Specify architecture scenario options",
                  "allowed-tools": "Glob"
                },
                "content": "# Architecture Scenario Explorer\n\nExplore architectural decisions through systematic scenario analysis with trade-off evaluation and future-proofing assessment.\n\n## Instructions\n\nYou are tasked with systematically exploring architectural decisions through comprehensive scenario modeling to optimize system design choices. Follow this approach: **$ARGUMENTS**\n\n### 1. Prerequisites Assessment\n\n**Critical Architecture Context Validation:**\n\n- **System Scope**: What system or component architecture are you designing?\n- **Scale Requirements**: What are the expected usage patterns and growth projections?\n- **Constraints**: What technical, business, or resource constraints apply?\n- **Timeline**: What is the implementation timeline and evolution roadmap?\n- **Success Criteria**: How will you measure architectural success?\n\n**If context is unclear, guide systematically:**\n\n```\nMissing System Scope:\n\"What specific system architecture needs exploration?\n- New System Design: Greenfield application or service architecture\n- System Migration: Moving from legacy to modern architecture\n- Scaling Architecture: Expanding existing system capabilities\n- Integration Architecture: Connecting multiple systems and services\n- Platform Architecture: Building foundational infrastructure\n\nPlease specify the system boundaries, key components, and primary functions.\"\n\nMissing Scale Requirements:\n\"What are the expected system scale and usage patterns?\n- User Scale: Number of concurrent and total users\n- Data Scale: Volume, velocity, and variety of data processed\n- Transaction Scale: Requests per second, peak load patterns\n- Geographic Scale: Single region, multi-region, or global distribution\n- Growth Projections: Expected scaling timeline and magnitude\"\n```\n\n### 2. Architecture Option Generation\n\n**Systematically identify architectural approaches:**\n\n#### Architecture Pattern Matrix\n```\nArchitectural Approach Framework:\n\nMonolithic Patterns:\n- Layered Architecture: Traditional n-tier with clear separation\n- Modular Monolith: Well-bounded modules within single deployment\n- Plugin Architecture: Core system with extensible plugin ecosystem\n- Service-Oriented Monolith: Internal service boundaries with single deployment\n\nDistributed Patterns:\n- Microservices: Independent services with business capability alignment\n- Service Mesh: Microservices with infrastructure-level communication\n- Event-Driven: Asynchronous communication with event sourcing\n- CQRS/Event Sourcing: Command-query separation with event storage\n\nHybrid Patterns:\n- Modular Microservices: Services grouped by business domain\n- Micro-Frontend: Frontend decomposition matching backend services\n- Strangler Fig: Gradual migration from monolith to distributed\n- API Gateway: Centralized entry point with backend service routing\n\nCloud-Native Patterns:\n- Serverless: Function-based with cloud provider infrastructure\n- Container-Native: Kubernetes-first with cloud-native services\n- Multi-Cloud: Cloud-agnostic with portable infrastructure\n- Edge-First: Distributed computing with edge location optimization\n```\n\n#### Architecture Variation Specification\n```\nFor each architectural option:\n\nStructural Characteristics:\n- Component Organization: [how system parts are structured and related]\n- Communication Patterns: [synchronous vs asynchronous, protocols, messaging]\n- Data Management: [database strategy, consistency model, storage patterns]\n- Deployment Model: [packaging, distribution, scaling, and operational approach]\n\nQuality Attributes:\n- Scalability Profile: [horizontal vs vertical scaling, bottleneck analysis]\n- Reliability Characteristics: [failure modes, recovery, fault tolerance]\n- Performance Expectations: [latency, throughput, resource efficiency]\n- Security Model: [authentication, authorization, data protection, attack surface]\n\nImplementation Considerations:\n- Technology Stack: [languages, frameworks, databases, infrastructure]\n- Team Structure Fit: [Conway's Law implications, team capabilities]\n- Development Process: [build, test, deploy, monitor workflows]\n- Evolution Strategy: [how architecture can grow and change over time]\n```\n\n### 3. Scenario Framework Development\n\n**Create comprehensive architectural testing scenarios:**\n\n#### Usage Scenario Matrix\n```\nMulti-Dimensional Scenario Framework:\n\nLoad Scenarios:\n- Normal Operation: Typical daily usage patterns and traffic\n- Peak Load: Maximum expected concurrent usage and transaction volume\n- Stress Testing: Beyond normal capacity to identify breaking points\n- Spike Testing: Sudden traffic increases and burst handling\n\nGrowth Scenarios:\n- Linear Growth: Steady user and data volume increases over time\n- Exponential Growth: Rapid scaling requirements and viral adoption\n- Geographic Expansion: Multi-region deployment and global scaling\n- Feature Expansion: New capabilities and service additions\n\nFailure Scenarios:\n- Component Failures: Individual service or database outages\n- Infrastructure Failures: Network, storage, or compute disruptions\n- Cascade Failures: Failure propagation and system-wide impacts\n- Disaster Recovery: Major outage recovery and business continuity\n\nEvolution Scenarios:\n- Technology Migration: Framework, language, or platform changes\n- Business Model Changes: New revenue streams or service offerings\n- Regulatory Changes: Compliance requirements and data protection\n- Competitive Response: Market pressures and feature requirements\n```\n\n#### Scenario Impact Modeling\n- Performance impact under each scenario type\n- Cost implications for infrastructure and operations\n- Development velocity and team productivity effects\n- Risk assessment and mitigation requirements\n\n### 4. Trade-off Analysis Framework\n\n**Systematic evaluation of architectural trade-offs:**\n\n#### Quality Attribute Trade-off Matrix\n```\nArchitecture Quality Assessment:\n\nPerformance Trade-offs:\n- Latency vs Throughput: Response time vs maximum concurrent processing\n- Memory vs CPU: Resource utilization optimization strategies\n- Consistency vs Availability: CAP theorem implications and choices\n- Caching vs Freshness: Data staleness vs response speed\n\nScalability Trade-offs:\n- Horizontal vs Vertical: Infrastructure scaling approach and economics\n- Stateless vs Stateful: Session management and performance implications\n- Synchronous vs Asynchronous: Communication complexity vs performance\n- Coupling vs Autonomy: Service independence vs operational overhead\n\nDevelopment Trade-offs:\n- Development Speed vs Runtime Performance: Optimization time investment\n- Type Safety vs Flexibility: Compile-time vs runtime error handling\n- Code Reuse vs Service Independence: Shared libraries vs duplication\n- Testing Complexity vs System Reliability: Test investment vs quality\n\nOperational Trade-offs:\n- Complexity vs Control: Managed services vs self-managed infrastructure\n- Monitoring vs Privacy: Observability vs data protection\n- Automation vs Flexibility: Standardization vs customization\n- Cost vs Performance: Infrastructure spending vs response times\n```\n\n#### Decision Matrix Construction\n- Weight assignment for different quality attributes based on business priorities\n- Scoring methodology for each architecture option across quality dimensions\n- Sensitivity analysis for weight and score variations\n- Pareto frontier identification for non-dominated solutions\n\n### 5. Future-Proofing Assessment\n\n**Evaluate architectural adaptability and evolution potential:**\n\n#### Technology Evolution Scenarios\n```\nFuture-Proofing Analysis Framework:\n\nTechnology Trend Integration:\n- AI/ML Integration: Machine learning capability embedding and scaling\n- Edge Computing: Distributed processing and low-latency requirements\n- Quantum Computing: Post-quantum cryptography and computational impacts\n- Blockchain/DLT: Distributed ledger integration and trust mechanisms\n\nMarket Evolution Preparation:\n- Business Model Flexibility: Subscription, marketplace, platform pivots\n- Global Expansion: Multi-tenant, multi-region, multi-regulatory compliance\n- Customer Expectation Evolution: Real-time, personalized, omnichannel experiences\n- Competitive Landscape Changes: Feature parity and differentiation requirements\n\nRegulatory Future-Proofing:\n- Privacy Regulation: GDPR, CCPA evolution and global privacy requirements\n- Security Standards: Zero-trust, compliance framework evolution\n- Data Sovereignty: Geographic data residency and cross-border restrictions\n- Accessibility Requirements: Inclusive design and assistive technology support\n```\n\n#### Adaptability Scoring\n- Architecture flexibility for requirement changes\n- Technology migration feasibility and cost\n- Team skill evolution and learning curve management\n- Investment protection and technical debt management\n\n### 6. Architecture Simulation Engine\n\n**Model architectural behavior under different scenarios:**\n\n#### Performance Simulation Framework\n```\nMulti-Layer Architecture Simulation:\n\nComponent-Level Simulation:\n- Individual service performance characteristics and resource usage\n- Database query performance and optimization opportunities\n- Cache hit ratios and invalidation strategies\n- Message queue throughput and latency patterns\n\nIntegration-Level Simulation:\n- Service-to-service communication overhead and optimization\n- API gateway performance and routing efficiency\n- Load balancer distribution and health checking\n- Circuit breaker and retry mechanism effectiveness\n\nSystem-Level Simulation:\n- End-to-end request flow and user experience\n- Peak load distribution and resource allocation\n- Failure propagation and recovery patterns\n- Monitoring and alerting system effectiveness\n\nInfrastructure-Level Simulation:\n- Cloud resource utilization and auto-scaling behavior\n- Network bandwidth and latency optimization\n- Storage performance and data consistency patterns\n- Security policy enforcement and performance impact\n```\n\n#### Cost Modeling Integration\n- Infrastructure cost estimation across different scenarios\n- Development and operational cost projection\n- Total cost of ownership analysis over multi-year timeline\n- Cost optimization opportunities and trade-off analysis\n\n### 7. Risk Assessment and Mitigation\n\n**Comprehensive architectural risk evaluation:**\n\n#### Technical Risk Framework\n```\nArchitecture Risk Assessment:\n\nImplementation Risks:\n- Technology Maturity: New vs proven technology adoption risks\n- Complexity Management: System comprehension and debugging challenges\n- Integration Challenges: Third-party service dependencies and compatibility\n- Performance Uncertainty: Untested scaling and optimization requirements\n\nOperational Risks:\n- Deployment Complexity: Release management and rollback capabilities\n- Monitoring Gaps: Observability and troubleshooting limitations\n- Scaling Challenges: Auto-scaling reliability and cost control\n- Disaster Recovery: Backup, recovery, and business continuity planning\n\nStrategic Risks:\n- Technology Lock-in: Vendor dependency and migration flexibility\n- Skill Dependencies: Team expertise requirements and knowledge gaps\n- Evolution Constraints: Architecture modification and extension limitations\n- Competitive Disadvantage: Time-to-market and feature development speed\n```\n\n#### Risk Mitigation Strategy Development\n- Specific mitigation approaches for identified risks\n- Contingency planning and alternative architecture options\n- Early warning indicators and monitoring strategies\n- Risk acceptance criteria and stakeholder communication\n\n### 8. Decision Framework and Recommendations\n\n**Generate systematic architectural guidance:**\n\n#### Architecture Decision Record (ADR) Format\n```\n## Architecture Decision: [System Name] - [Decision Topic]\n\n### Context and Problem Statement\n- Business Requirements: [key functional and non-functional requirements]\n- Current Constraints: [technical, resource, and timeline limitations]\n- Decision Drivers: [factors influencing architectural choice]\n\n### Architecture Options Considered\n\n#### Option 1: [Architecture Name]\n- Description: [architectural approach and key characteristics]\n- Pros: [advantages and benefits]\n- Cons: [disadvantages and risks]\n- Trade-offs: [specific quality attribute impacts]\n\n[Repeat for each option]\n\n### Decision Outcome\n- Selected Architecture: [chosen approach with rationale]\n- Decision Rationale: [why this option was selected]\n- Expected Benefits: [anticipated advantages and success metrics]\n- Accepted Trade-offs: [compromises and mitigation strategies]\n\n### Implementation Strategy\n- Phase 1 (Immediate): [initial implementation steps and validation]\n- Phase 2 (Short-term): [core system development and integration]\n- Phase 3 (Medium-term): [optimization and scaling implementation]\n- Phase 4 (Long-term): [evolution and enhancement roadmap]\n\n### Validation and Success Criteria\n- Performance Metrics: [specific KPIs and acceptable ranges]\n- Quality Gates: [architectural compliance and validation checkpoints]\n- Review Schedule: [when to reassess architectural decisions]\n- Adaptation Triggers: [conditions requiring architectural modification]\n\n### Risks and Mitigation\n- High-Priority Risks: [most significant concerns and responses]\n- Monitoring Strategy: [early warning systems and health checks]\n- Contingency Plans: [alternative approaches if problems arise]\n- Learning and Adaptation: [how to incorporate feedback and improve]\n```\n\n### 9. Continuous Architecture Evolution\n\n**Establish ongoing architectural assessment and improvement:**\n\n#### Architecture Health Monitoring\n- Performance metric tracking against architectural predictions\n- Technical debt accumulation and remediation planning\n- Team productivity and development velocity measurement\n- User satisfaction and business outcome correlation\n\n#### Evolutionary Architecture Practices\n- Regular architecture review and fitness function evaluation\n- Incremental improvement identification and implementation\n- Technology trend assessment and adoption planning\n- Cross-team architecture knowledge sharing and standardization\n\n## Usage Examples\n\n```bash\n# Microservices migration planning\n/dev:architecture-scenario-explorer Evaluate monolith to microservices migration for e-commerce platform with 1M+ users\n\n# New system architecture design\n/dev:architecture-scenario-explorer Design architecture for real-time analytics platform handling 100k events/second\n\n# Scaling architecture assessment\n/dev:architecture-scenario-explorer Analyze architecture options for scaling social media platform from 10k to 1M daily active users\n\n# Technology modernization planning\n/dev:architecture-scenario-explorer Compare serverless vs container-native architectures for data processing pipeline modernization\n```\n\n## Quality Indicators\n\n- **Green**: Multiple architectures analyzed, comprehensive scenarios tested, validated trade-offs\n- **Yellow**: Some architectural options considered, basic scenario coverage, estimated trade-offs\n- **Red**: Single architecture focus, limited scenario analysis, unvalidated assumptions\n\n## Common Pitfalls to Avoid\n\n- Architecture astronauting: Over-engineering for theoretical rather than real requirements\n- Cargo cult architecture: Copying successful patterns without understanding context\n- Technology bias: Choosing architecture based on technology preferences rather than requirements\n- Premature optimization: Solving performance problems that don't exist yet\n- Scalability obsession: Over-optimizing for scale that may never materialize\n- Evolution blindness: Not planning for architectural change and growth\n\nTransform architectural decisions from opinion-based debates into systematic, evidence-driven choices through comprehensive scenario exploration and trade-off analysis."
              },
              {
                "name": "/bidirectional-sync",
                "description": "Enable bidirectional GitHub-Linear synchronization",
                "path": "plugins/all-commands/commands/bidirectional-sync.md",
                "frontmatter": {
                  "description": "Enable bidirectional GitHub-Linear synchronization",
                  "category": "integration-sync"
                },
                "content": "# bidirectional-sync\n\nEnable bidirectional GitHub-Linear synchronization\n\n## System\n\nYou are a bidirectional synchronization specialist that maintains consistency between GitHub Issues and Linear tasks. You handle conflict resolution, prevent sync loops, and ensure data integrity across both platforms.\n\n## Instructions\n\nWhen implementing bidirectional sync:\n\n1. **Prerequisites & Setup**\n   - Verify both GitHub CLI and Linear MCP\n   - Initialize sync state storage\n   - Set up webhook endpoints (if available)\n\n2. **Sync State Management**\n   ```json\n   {\n     \"syncVersion\": \"1.0\",\n     \"lastFullSync\": \"2025-01-16T10:00:00Z\",\n     \"entities\": {\n       \"gh-123\": {\n         \"linearId\": \"ABC-456\",\n         \"githubNumber\": 123,\n         \"lastGithubUpdate\": \"2025-01-16T09:00:00Z\",\n         \"lastLinearUpdate\": \"2025-01-16T09:30:00Z\",\n         \"syncHash\": \"a1b2c3d4e5f6\",\n         \"lockedBy\": null\n       }\n     }\n   }\n   ```\n\n3. **Conflict Detection**\n   ```javascript\n   function detectConflict(entity) {\n     const githubChanged = entity.githubUpdated > entity.lastSync;\n     const linearChanged = entity.linearUpdated > entity.lastSync;\n     \n     if (githubChanged && linearChanged) {\n       return {\n         type: 'BOTH_CHANGED',\n         githubDelta: calculateDelta(entity.githubOld, entity.githubNew),\n         linearDelta: calculateDelta(entity.linearOld, entity.linearNew)\n       };\n     }\n     return null;\n   }\n   ```\n\n4. **Conflict Resolution Strategies**\n   ```\n   Strategy Options:\n    NEWER_WINS (default)\n    GITHUB_WINS\n    LINEAR_WINS\n    MANUAL_MERGE\n    FIELD_LEVEL_MERGE\n   ```\n\n5. **Field-Level Merge Rules**\n   ```javascript\n   const mergeRules = {\n     title: 'NEWER_WINS',\n     description: 'MERGE_CHANGES',\n     state: 'NEWER_WINS',\n     assignee: 'NEWER_WINS',\n     labels: 'UNION_MERGE',\n     priority: 'LINEAR_WINS',\n     comments: 'APPEND_ALL'\n   };\n   ```\n\n6. **Sync Loop Prevention**\n   ```javascript\n   // Add sync markers to prevent loops\n   const SYNC_MARKER = '[sync-bot]';\n   \n   function shouldSync(change) {\n     // Skip if change was made by sync bot\n     if (change.author === SYNC_BOT_ID) return false;\n     \n     // Skip if within grace period of last sync\n     const gracePeriod = 30000; // 30 seconds\n     if (Date.now() - lastSyncTime < gracePeriod) return false;\n     \n     // Check for sync marker in comments\n     if (change.body?.includes(SYNC_MARKER)) return false;\n     \n     return true;\n   }\n   ```\n\n7. **Bidirectional Field Mapping**\n   ```yaml\n   mappings:\n     # GitHub  Linear\n     - source: github.title\n       target: linear.title\n       transform: direct\n     \n     # Linear  GitHub  \n     - source: linear.identifier\n       target: github.body\n       transform: appendToFooter\n     \n     # Special handling\n     - source: github.labels\n       target: linear.labels\n       transform: mapLabels\n       reverse: true\n   ```\n\n8. **Transaction Management**\n   ```javascript\n   async function syncTransaction(syncOp) {\n     const transaction = await beginTransaction();\n     try {\n       // Lock both entities\n       await lockGitHub(syncOp.githubId);\n       await lockLinear(syncOp.linearId);\n       \n       // Perform sync\n       await syncOp.execute();\n       \n       // Update sync state\n       await updateSyncState(syncOp);\n       \n       await transaction.commit();\n     } catch (error) {\n       await transaction.rollback();\n       throw error;\n     } finally {\n       await unlockAll();\n     }\n   }\n   ```\n\n9. **Webhook Integration**\n   ```javascript\n   // GitHub webhook handler\n   app.post('/webhook/github', async (req, res) => {\n     const event = req.headers['x-github-event'];\n     if (shouldSync(req.body)) {\n       await queueSync({\n         source: 'github',\n         event: event,\n         data: req.body\n       });\n     }\n   });\n   \n   // Linear webhook handler\n   app.post('/webhook/linear', async (req, res) => {\n     if (shouldSync(req.body)) {\n       await queueSync({\n         source: 'linear',\n         event: req.body.type,\n         data: req.body\n       });\n     }\n   });\n   ```\n\n10. **Sync Execution Flow**\n    ```\n    1. Fetch all changes since last sync\n    2. Build sync queue with priorities\n    3. Process each item:\n       a. Check for conflicts\n       b. Apply resolution strategy\n       c. Update both platforms\n       d. Record sync state\n    4. Handle failures and retries\n    5. Generate sync report\n    ```\n\n## Examples\n\n### Initial Setup\n```bash\n# Initialize bidirectional sync\nclaude bidirectional-sync --init --repo=\"owner/repo\" --team=\"ENG\"\n\n# Configure sync options\nclaude bidirectional-sync --config \\\n  --conflict-strategy=\"NEWER_WINS\" \\\n  --sync-interval=\"5m\" \\\n  --webhook-secret=\"your-secret\"\n```\n\n### Manual Sync\n```bash\n# Full bidirectional sync\nclaude bidirectional-sync --full\n\n# Incremental sync (default)\nclaude bidirectional-sync\n\n# Dry run to preview changes\nclaude bidirectional-sync --dry-run\n```\n\n### Conflict Resolution\n```bash\n# Use specific strategy\nclaude bidirectional-sync --conflict-strategy=\"LINEAR_WINS\"\n\n# Interactive conflict resolution\nclaude bidirectional-sync --interactive\n\n# Force sync despite conflicts\nclaude bidirectional-sync --force\n```\n\n## Output Format\n\n```\nBidirectional Sync Report\n=========================\nPeriod: 2025-01-16 10:00:00 - 10:15:00\nMode: Incremental\n\nChanges Detected:\n- GitHub  Linear: 12 updates\n- Linear  GitHub: 8 updates\n- Conflicts: 3\n\nSync Results:\n GitHub #123  Linear ABC-456: Title updated (GitHub  Linear)\n GitHub #124  Linear ABC-457: Status changed (Linear  GitHub)\n GitHub #125  Linear ABC-458: Conflict resolved (NEWER_WINS)\n GitHub #126  Linear ABC-459: New task created\n Linear ABC-460  GitHub #127: New issue created\n\nConflict Details:\n1. #125  ABC-458:\n   - Field: description\n   - GitHub changed: 10:05:00\n   - Linear changed: 10:07:00\n   - Resolution: Used Linear version (newer)\n\nPerformance:\n- Total time: 15.3s\n- API calls: 45 (GitHub: 25, Linear: 20)\n- Rate limit status: OK\n\nNext sync: 2025-01-16 10:20:00\n```\n\n## Advanced Configuration\n\n### Sync Rules File\n```yaml\n# .github/linear-sync.yml\nversion: 1.0\nsync:\n  enabled: true\n  direction: bidirectional\n  interval: 5m\n  \nrules:\n  - name: \"Bug Priority Sync\"\n    condition:\n      github:\n        labels: [\"bug\"]\n    action:\n      linear:\n        priority: 1\n        \n  - name: \"Skip Draft Issues\"\n    condition:\n      github:\n        labels: [\"draft\"]\n    action:\n      skip: true\n\nconflicts:\n  strategy: NEWER_WINS\n  manual_review:\n    - title\n    - milestone\n    \nwebhooks:\n  github:\n    secret: ${GITHUB_WEBHOOK_SECRET}\n  linear:\n    secret: ${LINEAR_WEBHOOK_SECRET}\n```\n\n## Best Practices\n\n1. **Consistency Guarantees**\n   - Use distributed locks\n   - Implement idempotent operations\n   - Maintain audit logs\n\n2. **Performance Optimization**\n   - Batch similar operations\n   - Use caching for mappings\n   - Implement smart diffing\n\n3. **Error Handling**\n   - Exponential backoff for retries\n   - Dead letter queue for failures\n   - Alert on repeated failures\n\n4. **Monitoring**\n   - Track sync lag time\n   - Monitor conflict frequency\n   - Alert on sync failures"
              },
              {
                "name": "/big-features-interview",
                "description": "Interview to flesh out a plan/spec",
                "path": "plugins/all-commands/commands/big-features-interview.md",
                "frontmatter": {
                  "description": "Interview to flesh out a plan/spec",
                  "category": "interview",
                  "argument-hint": "<plan-file>",
                  "allowed-tools": "AskUserQuestion, Read, Glob, Grep, Write, Edit"
                },
                "content": "Here's the current plan:\n\n@$ARGUMENTS\n\nInterview me in detail using the AskUserQuestion tool about literally anything: technical implementation, UI & UX, concerns, tradeoffs, etc. but make sure the questions are not obvious.\n\nBe very in-depth and continue interviewing me continually until it's complete, then write the spec back to `$ARGUMENTS`."
              },
              {
                "name": "/bug-fix",
                "description": "Systematic workflow for fixing bugs including issue creation, branch management, and PR submission",
                "path": "plugins/all-commands/commands/bug-fix.md",
                "frontmatter": {
                  "description": "Systematic workflow for fixing bugs including issue creation, branch management, and PR submission",
                  "category": "version-control-git",
                  "argument-hint": "<bug_description>",
                  "allowed-tools": "Bash(git *), Bash(gh *)"
                },
                "content": "Understand the bug: $ARGUMENTS\n\nBefore Starting:\n- GITHUB: create an issue with a short descriptive title.\n- GIT: checkout a branch and switch to it.\n\nFix the Bug\n\nOn Completion:\n- GIT: commit with a descriptive message.\n- GIT: push the branch to the remote repository.\n- GITHUB: create a PR and link the issue."
              },
              {
                "name": "/bulk-import-issues",
                "description": "Bulk import GitHub issues to Linear",
                "path": "plugins/all-commands/commands/bulk-import-issues.md",
                "frontmatter": {
                  "description": "Bulk import GitHub issues to Linear",
                  "category": "integration-sync"
                },
                "content": "# bulk-import-issues\n\nBulk import GitHub issues to Linear\n\n## System\n\nYou are a bulk import specialist that efficiently transfers large numbers of GitHub issues to Linear. You handle rate limits, provide progress feedback, manage errors gracefully, and ensure data integrity during mass operations.\n\n## Instructions\n\nWhen performing bulk imports:\n\n1. **Pre-import Analysis**\n   ```javascript\n   async function analyzeImport(filters) {\n     const issues = await fetchGitHubIssues(filters);\n     \n     return {\n       totalIssues: issues.length,\n       byState: groupBy(issues, 'state'),\n       byLabel: groupBy(issues, issue => issue.labels[0]?.name),\n       byMilestone: groupBy(issues, 'milestone.title'),\n       estimatedTime: estimateImportTime(issues.length),\n       apiCallsRequired: calculateAPICalls(issues),\n       \n       warnings: [\n         issues.length > 500 && 'Large import may take significant time',\n         hasRateLimitRisk(issues.length) && 'May hit rate limits',\n         hasDuplicates(issues) && 'Potential duplicates detected'\n       ].filter(Boolean)\n     };\n   }\n   ```\n\n2. **Batch Configuration**\n   ```javascript\n   const BATCH_CONFIG = {\n     size: 20,                    // Items per batch\n     delayBetweenBatches: 2000,   // 2 seconds\n     maxConcurrent: 5,            // Parallel operations\n     retryAttempts: 3,\n     backoffMultiplier: 2,\n     \n     // Dynamic adjustment\n     adjustBatchSize(performance) {\n       if (performance.errorRate > 0.1) return Math.max(5, this.size / 2);\n       if (performance.avgTime > 5000) return Math.max(10, this.size - 5);\n       if (performance.avgTime < 1000) return Math.min(50, this.size + 5);\n       return this.size;\n     }\n   };\n   ```\n\n3. **Import Pipeline**\n   ```javascript\n   class BulkImportPipeline {\n     constructor(issues, options) {\n       this.queue = issues;\n       this.processed = [];\n       this.failed = [];\n       this.options = options;\n       this.startTime = Date.now();\n     }\n     \n     async execute() {\n       // Pre-process\n       await this.validate();\n       await this.deduplicate();\n       \n       // Process in batches\n       while (this.queue.length > 0) {\n         const batch = this.queue.splice(0, BATCH_CONFIG.size);\n         await this.processBatch(batch);\n         await this.updateProgress();\n         await this.checkRateLimits();\n       }\n       \n       // Post-process\n       await this.reconcile();\n       return this.generateReport();\n     }\n   }\n   ```\n\n4. **Progress Tracking**\n   ```javascript\n   class ProgressTracker {\n     constructor(total) {\n       this.total = total;\n       this.completed = 0;\n       this.failed = 0;\n       this.startTime = Date.now();\n     }\n     \n     update(success = true) {\n       success ? this.completed++ : this.failed++;\n       this.render();\n     }\n     \n     render() {\n       const progress = (this.completed + this.failed) / this.total;\n       const elapsed = Date.now() - this.startTime;\n       const eta = (elapsed / progress) - elapsed;\n       \n       console.log(`\n   Importing GitHub Issues to Linear\n   \n   \n   Progress: [${''.repeat(progress * 30)}${' '.repeat(30 - progress * 30)}] ${(progress * 100).toFixed(1)}%\n   \n   Completed: ${this.completed}/${this.total}\n   Failed: ${this.failed}\n   Rate: ${(this.completed / (elapsed / 1000)).toFixed(1)} issues/sec\n   ETA: ${formatTime(eta)}\n   \n   Current: ${this.currentItem?.title || 'Processing...'}\n       `);\n     }\n   }\n   ```\n\n5. **Error Handling**\n   ```javascript\n   async function handleImportError(issue, error, attempt) {\n     const errorType = classifyError(error);\n     \n     switch (errorType) {\n       case 'RATE_LIMIT':\n         await waitForRateLimit(error);\n         return 'RETRY';\n         \n       case 'DUPLICATE':\n         logDuplicate(issue);\n         return 'SKIP';\n         \n       case 'VALIDATION':\n         const fixed = await tryAutoFix(issue, error);\n         return fixed ? 'RETRY' : 'FAIL';\n         \n       case 'NETWORK':\n         if (attempt < BATCH_CONFIG.retryAttempts) {\n           await exponentialBackoff(attempt);\n           return 'RETRY';\n         }\n         return 'FAIL';\n         \n       default:\n         return 'FAIL';\n     }\n   }\n   ```\n\n6. **Data Transformation**\n   ```javascript\n   async function transformIssuesBatch(issues) {\n     return Promise.all(issues.map(async issue => {\n       try {\n         return {\n           title: sanitizeTitle(issue.title),\n           description: await enhanceDescription(issue),\n           priority: calculatePriority(issue),\n           state: mapState(issue.state),\n           labels: await mapLabels(issue.labels),\n           assignee: await findLinearUser(issue.assignee),\n           \n           metadata: {\n             githubNumber: issue.number,\n             githubUrl: issue.html_url,\n             importedAt: new Date().toISOString(),\n             importBatch: this.batchId\n           }\n         };\n       } catch (error) {\n         return { error, issue };\n       }\n     }));\n   }\n   ```\n\n7. **Duplicate Detection**\n   ```javascript\n   async function checkDuplicates(issues) {\n     const existingTasks = await linear.issues({\n       filter: { \n         externalId: { in: issues.map(i => `gh-${i.number}`) }\n       }\n     });\n     \n     const duplicates = new Map();\n     for (const task of existingTasks) {\n       duplicates.set(task.externalId, task);\n     }\n     \n     return {\n       hasDuplicates: duplicates.size > 0,\n       duplicates: duplicates,\n       unique: issues.filter(i => !duplicates.has(`gh-${i.number}`))\n     };\n   }\n   ```\n\n8. **Rate Limit Management**\n   ```javascript\n   class RateLimitManager {\n     constructor() {\n       this.github = { limit: 5000, remaining: 5000, reset: null };\n       this.linear = { limit: 1500, remaining: 1500, reset: null };\n     }\n     \n     async checkAndWait() {\n       // Update current limits\n       await this.updateLimits();\n       \n       // GitHub check\n       if (this.github.remaining < 100) {\n         const waitTime = this.github.reset - Date.now();\n         console.log(` GitHub rate limit pause: ${waitTime}ms`);\n         await sleep(waitTime);\n       }\n       \n       // Linear check\n       if (this.linear.remaining < 50) {\n         const waitTime = this.linear.reset - Date.now();\n         console.log(` Linear rate limit pause: ${waitTime}ms`);\n         await sleep(waitTime);\n       }\n       \n       // Adaptive throttling\n       const usage = 1 - (this.linear.remaining / this.linear.limit);\n       if (usage > 0.8) {\n         BATCH_CONFIG.delayBetweenBatches *= 1.5;\n       }\n     }\n   }\n   ```\n\n9. **Import Options**\n   ```javascript\n   const importOptions = {\n     // Filtering\n     labels: ['bug', 'enhancement'],\n     milestone: 'v2.0',\n     state: 'open',\n     since: '2025-01-01',\n     \n     // Mapping\n     teamId: 'engineering',\n     projectId: 'product-backlog',\n     defaultPriority: 3,\n     \n     // Behavior\n     skipDuplicates: true,\n     updateExisting: false,\n     preserveClosedState: false,\n     importComments: true,\n     importAttachments: false,\n     \n     // Performance\n     batchSize: 25,\n     maxConcurrent: 5,\n     timeout: 30000\n   };\n   ```\n\n10. **Post-Import Actions**\n    ```javascript\n    async function postImportTasks(report) {\n      // Create import summary\n      await createImportSummary(report);\n      \n      // Update GitHub issues with Linear links\n      if (options.updateGitHub) {\n        await updateGitHubIssues(report.successful);\n      }\n      \n      // Generate mapping file\n      await saveMappingFile({\n        timestamp: new Date().toISOString(),\n        mappings: report.mappings,\n        failed: report.failed\n      });\n      \n      // Send notifications\n      if (options.notify) {\n        await sendImportNotification(report);\n      }\n    }\n    ```\n\n## Examples\n\n### Basic Bulk Import\n```bash\n# Import all open issues\nclaude bulk-import-issues\n\n# Import with filters\nclaude bulk-import-issues --state=\"open\" --label=\"bug\"\n\n# Import specific milestone\nclaude bulk-import-issues --milestone=\"v2.0\"\n```\n\n### Advanced Import\n```bash\n# Custom batch settings\nclaude bulk-import-issues \\\n  --batch-size=50 \\\n  --delay=1000 \\\n  --max-concurrent=10\n\n# With mapping options\nclaude bulk-import-issues \\\n  --team=\"backend\" \\\n  --project=\"Q1-2025\" \\\n  --default-priority=\"medium\"\n\n# Skip duplicates and import comments\nclaude bulk-import-issues \\\n  --skip-duplicates \\\n  --import-comments \\\n  --update-github\n```\n\n### Recovery and Resume\n```bash\n# Dry run first\nclaude bulk-import-issues --dry-run\n\n# Resume failed import\nclaude bulk-import-issues --resume-from=\"import-12345.json\"\n\n# Retry only failed items\nclaude bulk-import-issues --retry-failed=\"import-12345.json\"\n```\n\n## Output Format\n\n```\nBulk Import Report\n==================\nStarted: 2025-01-16 10:00:00\nCompleted: 2025-01-16 10:15:32\n\nImport Summary:\n\nTotal Issues    : 523\nSuccessful      : 518 (99.0%)\nFailed          : 3 (0.6%)\nSkipped (Dupes) : 2 (0.4%)\n\nPerformance Metrics:\n- Total Duration: 15m 32s\n- Average Speed: 33.5 issues/minute\n- API Calls: 1,047 (GitHub: 523, Linear: 524)\n- Rate Limits: OK (GitHub: 4,477/5000, Linear: 976/1500)\n\nFailed Imports:\n1. Issue #234: \"Invalid assignee email\"\n2. Issue #456: \"Network timeout after 3 retries\"\n3. Issue #789: \"Label mapping failed\"\n\nBatch Performance:\nBatch 1-5   :  100% (2.1s avg)\nBatch 6-10  :  100% (1.8s avg)\nBatch 11-15 :  100% (2.3s avg)\n...\nBatch 26    :   78% (3 failed)\n\nActions Taken:\n Created 518 Linear tasks\n Mapped 45 unique labels\n Assigned to 12 team members\n Added to 3 projects\n Imported 1,234 comments\n Updated GitHub issues with Linear links\n\nMapping File: imports/bulk-import-2025-01-16-100000.json\n```\n\n## Error Recovery\n\n```javascript\n// Resume interrupted import\nasync function resumeImport(stateFile) {\n  const state = await loadImportState(stateFile);\n  \n  console.log(`\nResuming Import\n\nPrevious progress: ${state.completed}/${state.total}\nFailed items: ${state.failed.length}\nResuming from: Issue #${state.lastProcessed}\n  `);\n  \n  const remaining = state.queue.slice(state.position);\n  const pipeline = new BulkImportPipeline(remaining, state.options);\n  pipeline.processed = state.processed;\n  pipeline.failed = state.failed;\n  \n  return pipeline.execute();\n}\n```\n\n## Best Practices\n\n1. **Pre-Import Validation**\n   - Always run dry-run first\n   - Check for duplicates\n   - Validate mappings\n\n2. **Performance Optimization**\n   - Start with smaller batch sizes\n   - Monitor and adjust dynamically\n   - Use off-peak hours for large imports\n\n3. **Data Integrity**\n   - Save import mappings\n   - Enable rollback capability\n   - Verify post-import data\n\n4. **Error Management**\n   - Implement comprehensive logging\n   - Save failed items for retry\n   - Provide clear error messages"
              },
              {
                "name": "/business-scenario-explorer",
                "description": "Explore multiple business timeline scenarios with constraint validation and decision optimization.",
                "path": "plugins/all-commands/commands/business-scenario-explorer.md",
                "frontmatter": {
                  "description": "Explore multiple business timeline scenarios with constraint validation and decision optimization.",
                  "category": "simulation-modeling",
                  "argument-hint": "Specify business scenario parameters"
                },
                "content": "# Business Scenario Explorer\n\nExplore multiple business timeline scenarios with constraint validation and decision optimization.\n\n## Instructions\n\nYou are tasked with creating a comprehensive business scenario simulation to help explore multiple future timelines and make better strategic decisions. Follow this systematic approach: **$ARGUMENTS**\n\n### 1. Prerequisites Assessment\n\n**Before proceeding, validate these critical inputs:**\n\n- **Business Context**: Is the core business model and industry clearly defined?\n- **Time Horizon**: What is the planning timeline (quarters, years, market cycles)?\n- **Key Variables**: What are the primary factors that could change outcomes?\n- **Success Metrics**: How will you measure scenario success/failure?\n- **Decision Points**: What specific decisions need to be made?\n\n**If any of these are unclear, use progressive questioning:**\n\n```\nMissing Business Context:\n\"I need to understand your business model better. Please describe:\n- Your primary revenue streams\n- Key cost drivers \n- Main competitive advantages\n- Target market segments\"\n\nMissing Time Horizon:\n\"What planning period should we simulate?\n- Short-term (3-6 months): Market response, product launches\n- Medium-term (1-2 years): Strategic initiatives, market expansion  \n- Long-term (3-5+ years): Industry transformation, market cycles\"\n\nMissing Key Variables:\n\"What factors could significantly impact your business?\n- Market conditions (growth, recession, disruption)\n- Competitive landscape changes\n- Regulatory shifts\n- Technology adoption\n- Customer behavior evolution\"\n```\n\n### 2. Constraint Modeling\n\n**Map the decision environment with systematic constraint analysis:**\n\n#### External Constraints\n- Market size and growth dynamics\n- Competitive positioning and responses\n- Regulatory environment and compliance requirements\n- Economic conditions and cycles\n- Technology adoption curves\n- Supply chain dependencies\n\n#### Internal Constraints  \n- Financial resources and burn rate\n- Team capabilities and capacity\n- Technology infrastructure limitations\n- Brand positioning and reputation\n- Customer base characteristics\n- Operational scalability factors\n\n#### Temporal Constraints\n- Product development cycles\n- Market timing windows\n- Seasonal business patterns\n- Contract and partnership timelines\n- Regulatory approval processes\n\n**Quality Gate**: Validate that constraints are:\n- Specific and measurable\n- Based on real data where possible\n- Include ranges/uncertainty bounds\n- Account for interdependencies\n\n### 3. Scenario Architecture\n\n**Design multiple timeline branches systematically:**\n\n#### Base Case Scenario\n- Most likely outcome given current trajectory\n- Conservative assumptions about key variables\n- Historical pattern extrapolation\n- Risk-adjusted projections\n\n#### Optimistic Scenarios (2-3 variants)\n- Best-case market conditions\n- Successful execution of all initiatives\n- Favorable competitive dynamics\n- Accelerated adoption/growth\n\n#### Pessimistic Scenarios (2-3 variants)\n- Economic downturn impact\n- Increased competition\n- Execution challenges\n- Regulatory headwinds\n\n#### Disruption Scenarios (2-3 variants)\n- Technology breakthrough impacts\n- New market entrants\n- Business model shifts\n- Black swan events\n\n**Progressive Depth**: Start with 3-5 high-level scenarios, then drill into the most impactful ones.\n\n### 4. Timeline Compression Simulation\n\n**Run accelerated scenario testing:**\n\n#### Quarter-by-Quarter Analysis\n- Revenue progression under each scenario\n- Cost structure evolution\n- Market share dynamics\n- Key milestone achievement\n\n#### Decision Point Mapping\n- Critical decisions required at each timeline juncture\n- Option values and decision trees\n- Point-of-no-return identification\n- Pivot opportunity windows\n\n#### Feedback Loop Modeling\n- How early results would inform later decisions\n- Adaptive strategy adjustments\n- Learning and refinement cycles\n\n### 5. Quantitative Modeling\n\n**Apply systematic measurement to scenarios:**\n\n#### Financial Projections\n- Revenue growth trajectories\n- Profit margin evolution\n- Cash flow dynamics\n- Investment requirements\n- ROI calculations across timelines\n\n#### Market Dynamics\n- Market share progression\n- Customer acquisition costs\n- Lifetime value evolution\n- Competitive response modeling\n\n#### Operational Metrics\n- Team scaling requirements\n- Infrastructure capacity needs\n- Efficiency improvements\n- Quality indicators\n\n**Confidence Scoring**: Rate each projection 1-10 based on:\n- Data quality supporting the assumption\n- Historical precedent availability  \n- Expert validation received\n- Logical consistency with other assumptions\n\n### 6. Risk Assessment & Mitigation\n\n**Systematically evaluate scenario risks:**\n\n#### Probability Weighting\n- Assign realistic probabilities to each scenario\n- Use base rate analysis from similar situations\n- Account for planning fallacy and optimism bias\n- Include expert opinion and market research\n\n#### Impact Analysis\n- Quantify potential upside/downside for each scenario\n- Identify business-critical failure modes\n- Map cascade effects and domino risks\n- Calculate expected value across scenarios\n\n#### Mitigation Strategies\n- Identify early warning indicators for each scenario\n- Design adaptive responses and pivot strategies\n- Build option values and flexibility into plans\n- Create risk monitoring dashboards\n\n### 7. Decision Optimization\n\n**Generate actionable strategic guidance:**\n\n#### Strategy Robustness Testing\n- Which strategies perform well across multiple scenarios?\n- What are the key sensitivity factors?\n- Where are the highest-leverage decision points?\n- What creates competitive moats in each timeline?\n\n#### Resource Allocation Optimization\n- Optimal budget allocation across scenarios\n- Investment sequencing and timing\n- Capability building priorities\n- Partnership and acquisition strategies\n\n#### Contingency Planning\n- Specific action triggers for each scenario\n- Resource reallocation frameworks\n- Communication strategies for different outcomes\n- Stakeholder management approaches\n\n### 8. Calibration and Validation\n\n**Ensure simulation quality and accuracy:**\n\n#### Assumption Testing\n- Compare key assumptions to historical data\n- Validate with domain experts and stakeholders\n- Stress-test critical assumptions\n- Document confidence levels and sources\n\n#### Scenario Plausibility Check\n- Do scenarios follow logical progression?\n- Are interdependencies properly modeled?\n- Do financial projections balance?\n- Are timelines realistic given constraints?\n\n#### Bias Detection\n- Check for anchoring on current state\n- Identify confirmation bias in favorable scenarios  \n- Validate pessimistic scenarios aren't too extreme\n- Ensure scenarios cover full possibility space\n\n### 9. Output Generation\n\n**Present findings in structured, actionable format:**\n\n```\n## Business Scenario Analysis: [Business Name]\n\n### Executive Summary\n- Planning horizon: [timeline]\n- Scenarios modeled: [count and types]\n- Key decision points: [critical decisions]\n- Recommended strategy: [specific approach]\n\n### Scenario Outcomes Matrix\n\n| Scenario | Probability | Year 1 Revenue | Year 2 Revenue | Key Risks | Success Factors |\n|----------|-------------|----------------|----------------|-----------|-----------------|\n| Base Case | 40% | $X | $Y | [risks] | [factors] |\n| Optimistic A | 20% | $X | $Y | [risks] | [factors] |\n| Pessimistic A | 25% | $X | $Y | [risks] | [factors] |\n| Disruption A | 15% | $X | $Y | [risks] | [factors] |\n\n### Strategic Recommendations\n\n**Robust Strategies** (perform well across scenarios):\n1. [Strategy with confidence score]\n2. [Strategy with confidence score]\n3. [Strategy with confidence score]\n\n**Scenario-Specific Tactics**:\n- If Base Case: [specific actions]\n- If Optimistic: [specific actions]  \n- If Pessimistic: [specific actions]\n- If Disruption: [specific actions]\n\n**Critical Decision Points**:\n- Month 3: [decision] - Leading indicators: [metrics]\n- Month 9: [decision] - Leading indicators: [metrics]\n- Month 18: [decision] - Leading indicators: [metrics]\n\n### Risk Mitigation Framework\n- Early warning indicators for each scenario\n- Specific response triggers and actions\n- Resource reallocation procedures\n- Stakeholder communication protocols\n\n### Confidence Assessment\n- High confidence projections: [list]\n- Medium confidence projections: [list]  \n- Low confidence projections: [list]\n- Areas requiring additional research: [list]\n```\n\n### 10. Iteration and Refinement\n\n**Establish ongoing scenario improvement:**\n\n#### Feedback Integration\n- Monthly assumption validation against actual results\n- Quarterly scenario probability updates\n- Annual comprehensive scenario refresh\n- Continuous learning from scenario accuracy\n\n#### Model Enhancement\n- Incorporate new data sources as available\n- Refine constraint modeling based on experience\n- Update probability assessments based on outcomes\n- Enhance decision point identification\n\n**Success Metrics**: \n- Scenario accuracy over time\n- Decision quality improvement\n- Strategic option value realization\n- Risk event prediction success\n\n## Usage Examples\n\n```bash\n# Strategic business planning\n/simulation:business-scenario-explorer Evaluate SaaS expansion into European markets over next 2 years\n\n# Product launch planning\n/simulation:business-scenario-explorer Model outcomes for AI-powered feature launch across different market conditions\n\n# Investment decision\n/simulation:business-scenario-explorer Analyze ROI scenarios for $5M Series A funding across market conditions\n\n# Market entry strategy\n/simulation:business-scenario-explorer Explore timeline scenarios for entering fintech market as established player\n```\n\n## Quality Indicators\n\n- **Green**: 80%+ confidence in key assumptions, full constraint modeling, 5+ scenarios analyzed\n- **Yellow**: 60-80% confidence, partial constraint mapping, 3-4 scenarios\n- **Red**: <60% confidence, missing critical constraints, <3 scenarios\n\n## Common Pitfalls to Avoid\n\n- Planning fallacy: Overly optimistic timelines\n- Anchoring bias: Scenarios too close to current state\n- Confirmation bias: Favoring pleasant outcomes\n- Missing constraints: Ignoring regulatory/competitive factors\n- Point estimates: Not using probability distributions\n- Static thinking: Not modeling adaptive responses\n\nTransform your 10-year market cycle into a 10-hour simulation and make exponentially better strategic decisions."
              },
              {
                "name": "/changelog-demo-command",
                "description": "Demo changelog automation features",
                "path": "plugins/all-commands/commands/changelog-demo-command.md",
                "frontmatter": {
                  "description": "Demo changelog automation features",
                  "category": "ci-deployment"
                },
                "content": "# Demo Command for Changelog\n\nDemo changelog automation features\n\n## Instructions\n\n1. This is a demonstration command\n2. Shows changelog automation working independently\n3. Bypasses Claude review bot for faster testing"
              },
              {
                "name": "/check-file",
                "description": "Perform comprehensive analysis of $ARGUMENTS to identify code quality issues, security vulnerabilities, and optimization opportunities.",
                "path": "plugins/all-commands/commands/check-file.md",
                "frontmatter": {
                  "description": "Perform comprehensive analysis of $ARGUMENTS to identify code quality issues, security vulnerabilities, and optimization opportunities.",
                  "category": "utilities-debugging",
                  "argument-hint": "Specify file path to check",
                  "allowed-tools": "Read"
                },
                "content": "# File Analysis Tool\n\nPerform comprehensive analysis of $ARGUMENTS to identify code quality issues, security vulnerabilities, and optimization opportunities.\n\n## Task\n\nI'll analyze the specified file and provide detailed insights on:\n\n1. Code quality metrics and maintainability\n2. Security vulnerabilities and best practices\n3. Performance bottlenecks and optimization opportunities\n4. Dependency usage and potential issues\n5. TypeScript/JavaScript specific patterns and improvements\n6. Test coverage and missing tests\n\n## Process\n\nI'll follow these steps:\n\n1. Read and parse the target file\n2. Analyze code structure and complexity\n3. Check for security vulnerabilities and anti-patterns  \n4. Evaluate performance implications\n5. Review dependency usage and imports\n6. Provide actionable recommendations for improvement\n\n## Analysis Areas\n\n### Code Quality\n- Cyclomatic complexity and maintainability metrics\n- Code duplication and refactoring opportunities\n- Naming conventions and code organization\n- TypeScript type safety and best practices\n\n### Security Assessment\n- Input validation and sanitization\n- Authentication and authorization patterns\n- Sensitive data exposure risks\n- Common vulnerability patterns (XSS, injection, etc.)\n\n### Performance Review\n- Bundle size impact and optimization opportunities\n- Runtime performance bottlenecks\n- Memory usage patterns\n- Lazy loading and code splitting opportunities\n\n### Best Practices\n- Framework-specific patterns (React, Vue, Angular)\n- Modern JavaScript/TypeScript features usage\n- Error handling and logging practices\n- Testing patterns and coverage gaps\n\nI'll provide specific, actionable recommendations tailored to your project's technology stack and architecture."
              },
              {
                "name": "/check",
                "description": "Run project checks and fix any errors without committing",
                "path": "plugins/all-commands/commands/check.md",
                "frontmatter": {
                  "description": "Run project checks and fix any errors without committing",
                  "category": "code-analysis-testing",
                  "allowed-tools": "Bash, Edit, Read"
                },
                "content": "Run project validation checks and resolve any errors found.\n\n## Process:\n\n1. **Detect Package Manager** (for JavaScript/TypeScript projects):\n   - npm: Look for package-lock.json\n   - pnpm: Look for pnpm-lock.yaml\n   - yarn: Look for yarn.lock\n   - bun: Look for bun.lockb\n\n2. **Check Available Scripts**:\n   - Read package.json to find check/validation scripts\n   - Common script names: `check`, `validate`, `verify`, `test`, `lint`\n\n3. **Run Appropriate Check Command**:\n   - JavaScript/TypeScript:\n     - npm: `npm run check` or `npm test`\n     - pnpm: `pnpm check` or `pnpm test`\n     - yarn: `yarn check` or `yarn test`\n     - bun: `bun check` or `bun test`\n   \n   - Other languages:\n     - Python: `pytest`, `flake8`, `mypy`, or `make check`\n     - Go: `go test ./...` or `golangci-lint run`\n     - Rust: `cargo check` or `cargo test`\n     - Ruby: `rubocop` or `rake test`\n\n4. **Fix Any Errors**:\n   - Analyze error output\n   - Fix code issues, syntax errors, or test failures\n   - Re-run checks after fixing\n\n5. **Important Constraints**:\n   - DO NOT commit any code\n   - DO NOT change version numbers\n   - Only fix errors to make checks pass\n\nIf no check script exists, run the most appropriate validation for the project type."
              },
              {
                "name": "/ci-setup",
                "description": "Setup continuous integration pipeline",
                "path": "plugins/all-commands/commands/ci-setup.md",
                "frontmatter": {
                  "description": "Setup continuous integration pipeline",
                  "category": "ci-deployment",
                  "argument-hint": "1. **Project Analysis**",
                  "allowed-tools": "Bash(npm *)"
                },
                "content": "# CI/CD Setup Command\n\nSetup continuous integration pipeline\n\n## Instructions\n\nFollow this systematic approach to implement CI/CD: **$ARGUMENTS**\n\n1. **Project Analysis**\n   - Identify the technology stack and deployment requirements\n   - Review existing build and test processes\n   - Understand deployment environments (dev, staging, prod)\n   - Assess current version control and branching strategy\n\n2. **CI/CD Platform Selection**\n   - Choose appropriate CI/CD platform based on requirements:\n     - **GitHub Actions**: Native GitHub integration, extensive marketplace\n     - **GitLab CI**: Built-in GitLab, comprehensive DevOps platform\n     - **Jenkins**: Self-hosted, highly customizable, extensive plugins\n     - **CircleCI**: Cloud-based, optimized for speed\n     - **Azure DevOps**: Microsoft ecosystem integration\n     - **AWS CodePipeline**: AWS-native solution\n\n3. **Repository Setup**\n   - Ensure proper `.gitignore` configuration\n   - Set up branch protection rules\n   - Configure merge requirements and reviews\n   - Establish semantic versioning strategy\n\n4. **Build Pipeline Configuration**\n   \n   **GitHub Actions Example:**\n   ```yaml\n   name: CI/CD Pipeline\n   \n   on:\n     push:\n       branches: [ main, develop ]\n     pull_request:\n       branches: [ main ]\n   \n   jobs:\n     test:\n       runs-on: ubuntu-latest\n       steps:\n         - uses: actions/checkout@v3\n         - name: Setup Node.js\n           uses: actions/setup-node@v3\n           with:\n             node-version: '18'\n             cache: 'npm'\n         - run: npm ci\n         - run: npm run test\n         - run: npm run build\n   ```\n\n   **GitLab CI Example:**\n   ```yaml\n   stages:\n     - test\n     - build\n     - deploy\n   \n   test:\n     stage: test\n     script:\n       - npm ci\n       - npm run test\n     cache:\n       paths:\n         - node_modules/\n   ```\n\n5. **Environment Configuration**\n   - Set up environment variables and secrets\n   - Configure different environments (dev, staging, prod)\n   - Implement environment-specific configurations\n   - Set up secure secret management\n\n6. **Automated Testing Integration**\n   - Configure unit test execution\n   - Set up integration test running\n   - Implement E2E test execution\n   - Configure test reporting and coverage\n\n   **Multi-stage Testing:**\n   ```yaml\n   test:\n     strategy:\n       matrix:\n         node-version: [16, 18, 20]\n     runs-on: ubuntu-latest\n     steps:\n       - uses: actions/checkout@v3\n       - uses: actions/setup-node@v3\n         with:\n           node-version: ${{ matrix.node-version }}\n       - run: npm ci\n       - run: npm test\n   ```\n\n7. **Code Quality Gates**\n   - Integrate linting and formatting checks\n   - Set up static code analysis (SonarQube, CodeClimate)\n   - Configure security vulnerability scanning\n   - Implement code coverage thresholds\n\n8. **Build Optimization**\n   - Configure build caching strategies\n   - Implement parallel job execution\n   - Optimize Docker image builds\n   - Set up artifact management\n\n   **Caching Example:**\n   ```yaml\n   - name: Cache node modules\n     uses: actions/cache@v3\n     with:\n       path: ~/.npm\n       key: ${{ runner.os }}-node-${{ hashFiles('**/package-lock.json') }}\n       restore-keys: |\n         ${{ runner.os }}-node-\n   ```\n\n9. **Docker Integration**\n   - Create optimized Dockerfiles\n   - Set up multi-stage builds\n   - Configure container registry integration\n   - Implement security scanning for images\n\n   **Multi-stage Dockerfile:**\n   ```dockerfile\n   FROM node:18-alpine AS builder\n   WORKDIR /app\n   COPY package*.json ./\n   RUN npm ci --only=production\n   \n   FROM node:18-alpine AS runtime\n   WORKDIR /app\n   COPY --from=builder /app/node_modules ./node_modules\n   COPY . .\n   EXPOSE 3000\n   CMD [\"npm\", \"start\"]\n   ```\n\n10. **Deployment Strategies**\n    - Implement blue-green deployment\n    - Set up canary releases\n    - Configure rolling updates\n    - Implement feature flags integration\n\n11. **Infrastructure as Code**\n    - Use Terraform, CloudFormation, or similar tools\n    - Version control infrastructure definitions\n    - Implement infrastructure testing\n    - Set up automated infrastructure provisioning\n\n12. **Monitoring and Observability**\n    - Set up application performance monitoring\n    - Configure log aggregation and analysis\n    - Implement health checks and alerting\n    - Set up deployment notifications\n\n13. **Security Integration**\n    - Implement dependency vulnerability scanning\n    - Set up container security scanning\n    - Configure SAST (Static Application Security Testing)\n    - Implement secrets scanning\n\n   **Security Scanning Example:**\n   ```yaml\n   security:\n     runs-on: ubuntu-latest\n     steps:\n       - uses: actions/checkout@v3\n       - name: Run Snyk to check for vulnerabilities\n         uses: snyk/actions/node@master\n         env:\n           SNYK_TOKEN: ${{ secrets.SNYK_TOKEN }}\n   ```\n\n14. **Database Migration Handling**\n    - Automate database schema migrations\n    - Implement rollback strategies\n    - Set up database seeding for testing\n    - Configure backup and recovery procedures\n\n15. **Performance Testing Integration**\n    - Set up load testing in pipeline\n    - Configure performance benchmarks\n    - Implement performance regression detection\n    - Set up performance monitoring\n\n16. **Multi-Environment Deployment**\n    - Configure staging environment deployment\n    - Set up production deployment with approvals\n    - Implement environment promotion workflow\n    - Configure environment-specific configurations\n\n   **Environment Deployment:**\n   ```yaml\n   deploy-staging:\n     needs: test\n     if: github.ref == 'refs/heads/develop'\n     runs-on: ubuntu-latest\n     steps:\n       - name: Deploy to staging\n         run: |\n           # Deploy to staging environment\n   \n   deploy-production:\n     needs: test\n     if: github.ref == 'refs/heads/main'\n     runs-on: ubuntu-latest\n     environment: production\n     steps:\n       - name: Deploy to production\n         run: |\n           # Deploy to production environment\n   ```\n\n17. **Rollback and Recovery**\n    - Implement automated rollback procedures\n    - Set up deployment verification tests\n    - Configure failure detection and alerts\n    - Document manual recovery procedures\n\n18. **Notification and Reporting**\n    - Set up Slack/Teams integration for notifications\n    - Configure email alerts for failures\n    - Implement deployment status reporting\n    - Set up metrics dashboards\n\n19. **Compliance and Auditing**\n    - Implement deployment audit trails\n    - Set up compliance checks (SOC 2, HIPAA, etc.)\n    - Configure approval workflows for sensitive deployments\n    - Document change management processes\n\n20. **Pipeline Optimization**\n    - Monitor pipeline performance and costs\n    - Implement pipeline parallelization\n    - Optimize resource allocation\n    - Set up pipeline analytics and reporting\n\n**Best Practices:**\n\n1. **Fail Fast**: Implement early failure detection\n2. **Parallel Execution**: Run independent jobs in parallel\n3. **Caching**: Cache dependencies and build artifacts\n4. **Security**: Never expose secrets in logs\n5. **Documentation**: Document pipeline processes and procedures\n6. **Monitoring**: Monitor pipeline health and performance\n7. **Testing**: Test pipeline changes in feature branches\n8. **Rollback**: Always have a rollback strategy\n\n**Sample Complete Pipeline:**\n```yaml\nname: Full CI/CD Pipeline\n\non:\n  push:\n    branches: [ main, develop ]\n  pull_request:\n    branches: [ main ]\n\njobs:\n  lint-and-test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - uses: actions/setup-node@v3\n        with:\n          node-version: '18'\n          cache: 'npm'\n      - run: npm ci\n      - run: npm run lint\n      - run: npm run test:coverage\n      - run: npm run build\n\n  security-scan:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - name: Security scan\n        run: npm audit --audit-level=high\n\n  deploy-staging:\n    needs: [lint-and-test, security-scan]\n    if: github.ref == 'refs/heads/develop'\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - name: Deploy to staging\n        run: echo \"Deploying to staging\"\n\n  deploy-production:\n    needs: [lint-and-test, security-scan]\n    if: github.ref == 'refs/heads/main'\n    runs-on: ubuntu-latest\n    environment: production\n    steps:\n      - uses: actions/checkout@v3\n      - name: Deploy to production\n        run: echo \"Deploying to production\"\n```\n\nStart with basic CI and gradually add more sophisticated features as your team and project mature."
              },
              {
                "name": "/clean-branches",
                "description": "Clean up merged and stale git branches",
                "path": "plugins/all-commands/commands/clean-branches.md",
                "frontmatter": {
                  "description": "Clean up merged and stale git branches",
                  "category": "utilities-debugging",
                  "argument-hint": "1. **Repository State Analysis**",
                  "allowed-tools": "Bash(git *)"
                },
                "content": "# Clean Branches Command\n\nClean up merged and stale git branches\n\n## Instructions\n\nFollow this systematic approach to clean up git branches: **$ARGUMENTS**\n\n1. **Repository State Analysis**\n   - Check current branch and uncommitted changes\n   - List all local and remote branches\n   - Identify the main/master branch name\n   - Review recent branch activity and merge history\n\n   ```bash\n   # Check current status\n   git status\n   git branch -a\n   git remote -v\n   \n   # Check main branch name\n   git symbolic-ref refs/remotes/origin/HEAD | sed 's@^refs/remotes/origin/@@'\n   ```\n\n2. **Safety Precautions**\n   - Ensure working directory is clean\n   - Switch to main/master branch\n   - Pull latest changes from remote\n   - Create backup of current branch state if needed\n\n   ```bash\n   # Ensure clean state\n   git stash push -m \"Backup before branch cleanup\"\n   git checkout main  # or master\n   git pull origin main\n   ```\n\n3. **Identify Merged Branches**\n   - List branches that have been merged into main\n   - Exclude protected branches (main, master, develop)\n   - Check both local and remote merged branches\n   - Verify merge status to avoid accidental deletion\n\n   ```bash\n   # List merged local branches\n   git branch --merged main | grep -v \"main\\\\|master\\\\|develop\\\\|\\\\*\"\n   \n   # List merged remote branches\n   git branch -r --merged main | grep -v \"main\\\\|master\\\\|develop\\\\|HEAD\"\n   ```\n\n4. **Identify Stale Branches**\n   - Find branches with no recent activity\n   - Check last commit date for each branch\n   - Identify branches older than specified timeframe (e.g., 30 days)\n   - Consider branch naming patterns for feature/hotfix branches\n\n   ```bash\n   # List branches by last commit date\n   git for-each-ref --format='%(committerdate) %(authorname) %(refname)' --sort=committerdate refs/heads\n   \n   # Find branches older than 30 days\n   git for-each-ref --format='%(refname:short) %(committerdate)' refs/heads | awk '$2 < \"'$(date -d '30 days ago' '+%Y-%m-%d')'\"'\n   ```\n\n5. **Interactive Branch Review**\n   - Review each branch before deletion\n   - Check if branch has unmerged changes\n   - Verify branch purpose and status\n   - Ask for confirmation before deletion\n\n   ```bash\n   # Check for unmerged changes\n   git log main..branch-name --oneline\n   \n   # Show branch information\n   git show-branch branch-name main\n   ```\n\n6. **Protected Branch Configuration**\n   - Identify branches that should never be deleted\n   - Configure protection rules for important branches\n   - Document branch protection policies\n   - Set up automated protection for new repositories\n\n   ```bash\n   # Example protected branches\n   PROTECTED_BRANCHES=(\"main\" \"master\" \"develop\" \"staging\" \"production\")\n   ```\n\n7. **Local Branch Cleanup**\n   - Delete merged local branches safely\n   - Remove stale feature branches\n   - Clean up tracking branches for deleted remotes\n   - Update local branch references\n\n   ```bash\n   # Delete merged branches (interactive)\n   git branch --merged main | grep -v \"main\\\\|master\\\\|develop\\\\|\\\\*\" | xargs -n 1 -p git branch -d\n   \n   # Force delete if needed (use with caution)\n   git branch -D branch-name\n   ```\n\n8. **Remote Branch Cleanup**\n   - Remove merged remote branches\n   - Clean up remote tracking references\n   - Delete obsolete remote branches\n   - Update remote branch information\n\n   ```bash\n   # Prune remote tracking branches\n   git remote prune origin\n   \n   # Delete remote branch\n   git push origin --delete branch-name\n   \n   # Remove local tracking of deleted remote branches\n   git branch -dr origin/branch-name\n   ```\n\n9. **Automated Cleanup Script**\n   \n   ```bash\n   #!/bin/bash\n   \n   # Git branch cleanup script\n   set -e\n   \n   # Configuration\n   MAIN_BRANCH=\"main\"\n   PROTECTED_BRANCHES=(\"main\" \"master\" \"develop\" \"staging\" \"production\")\n   STALE_DAYS=30\n   \n   # Functions\n   is_protected() {\n       local branch=$1\n       for protected in \"${PROTECTED_BRANCHES[@]}\"; do\n           if [[ \"$branch\" == \"$protected\" ]]; then\n               return 0\n           fi\n       done\n       return 1\n   }\n   \n   # Switch to main branch\n   git checkout $MAIN_BRANCH\n   git pull origin $MAIN_BRANCH\n   \n   # Clean up merged branches\n   echo \"Cleaning up merged branches...\"\n   merged_branches=$(git branch --merged $MAIN_BRANCH | grep -v \"\\\\*\\\\|$MAIN_BRANCH\")\n   \n   for branch in $merged_branches; do\n       if ! is_protected \"$branch\"; then\n           echo \"Deleting merged branch: $branch\"\n           git branch -d \"$branch\"\n       fi\n   done\n   \n   # Prune remote tracking branches\n   echo \"Pruning remote tracking branches...\"\n   git remote prune origin\n   \n   echo \"Branch cleanup completed!\"\n   ```\n\n10. **Team Coordination**\n    - Notify team before cleaning shared branches\n    - Check if branches are being used by others\n    - Coordinate branch cleanup schedules\n    - Document branch cleanup procedures\n\n11. **Branch Naming Convention Cleanup**\n    - Identify branches with non-standard naming\n    - Clean up temporary or experimental branches\n    - Remove old hotfix and feature branches\n    - Enforce consistent naming conventions\n\n12. **Verification and Validation**\n    - Verify important branches are still present\n    - Check that no active work was deleted\n    - Validate remote branch synchronization\n    - Confirm team members have no issues\n\n    ```bash\n    # Verify cleanup results\n    git branch -a\n    git remote show origin\n    ```\n\n13. **Documentation and Reporting**\n    - Document what branches were cleaned up\n    - Report any issues or conflicts found\n    - Update team documentation about branch lifecycle\n    - Create branch cleanup schedule and policies\n\n14. **Rollback Procedures**\n    - Document how to recover deleted branches\n    - Use reflog to find deleted branch commits\n    - Create emergency recovery procedures\n    - Set up branch restoration scripts\n\n    ```bash\n    # Recover deleted branch using reflog\n    git reflog --no-merges --since=\"2 weeks ago\"\n    git checkout -b recovered-branch commit-hash\n    ```\n\n15. **Automation Setup**\n    - Set up automated branch cleanup scripts\n    - Configure CI/CD pipeline for branch cleanup\n    - Create scheduled cleanup jobs\n    - Implement branch lifecycle policies\n\n16. **Best Practices Implementation**\n    - Establish branch lifecycle guidelines\n    - Set up automated merge detection\n    - Configure branch protection rules\n    - Implement code review requirements\n\n**Advanced Cleanup Options:**\n\n```bash\n# Clean up all merged branches except protected ones\ngit branch --merged main | grep -E \"^  (feature|hotfix|bugfix)/\" | xargs -n 1 git branch -d\n\n# Interactive cleanup with confirmation\ngit branch --merged main | grep -v \"main\\|master\\|develop\" | xargs -n 1 -p git branch -d\n\n# Batch delete remote branches\ngit branch -r --merged main | grep origin | grep -v \"main\\|master\\|develop\\|HEAD\" | cut -d/ -f2- | xargs -n 1 git push origin --delete\n\n# Clean up branches older than specific date\ngit for-each-ref --format='%(refname:short) %(committerdate:short)' refs/heads | awk '$2 < \"2023-01-01\"' | cut -d' ' -f1 | xargs -n 1 git branch -D\n```\n\nRemember to:\n- Always backup important branches before cleanup\n- Coordinate with team members before deleting shared branches\n- Test cleanup scripts in a safe environment first\n- Document all cleanup procedures and policies\n- Set up regular cleanup schedules to prevent accumulation"
              },
              {
                "name": "/clean",
                "description": "Fix all linting and formatting issues across the codebase",
                "path": "plugins/all-commands/commands/clean.md",
                "frontmatter": {
                  "description": "Fix all linting and formatting issues across the codebase",
                  "category": "code-analysis-testing",
                  "allowed-tools": "Bash, Edit, Read, Glob"
                },
                "content": "Fix all linting, formatting, and static analysis issues in the entire codebase.\n\n## Process:\n\n1. **Detect Project Language(s)**:\n   - Check file extensions and configuration files\n   - Common indicators:\n     - Python: .py files, requirements.txt, pyproject.toml\n     - JavaScript/TypeScript: .js/.ts files, package.json\n     - Go: .go files, go.mod\n     - Rust: .rs files, Cargo.toml\n     - Java: .java files, pom.xml\n     - Ruby: .rb files, Gemfile\n\n2. **Run Language-Specific Linters**:\n\n   **Python:**\n   - Formatting: `black .` or `autopep8`\n   - Import sorting: `isort .`\n   - Linting: `flake8` or `pylint`\n   - Type checking: `mypy`\n   \n   **JavaScript/TypeScript:**\n   - Linting: `eslint . --fix`\n   - Formatting: `prettier --write .`\n   - Type checking: `tsc --noEmit`\n   \n   **Go:**\n   - Formatting: `go fmt ./...`\n   - Linting: `golangci-lint run --fix`\n   \n   **Rust:**\n   - Formatting: `cargo fmt`\n   - Linting: `cargo clippy --fix`\n   \n   **Java:**\n   - Formatting: `google-java-format` or `spotless`\n   - Linting: `checkstyle` or `spotbugs`\n   \n   **Ruby:**\n   - Linting/Formatting: `rubocop -a`\n\n3. **Check for Project Scripts**:\n   - Look for lint/format scripts in package.json, Makefile, etc.\n   - Common script names: `lint`, `format`, `fix`, `clean`\n\n4. **Fix Issues**:\n   - Apply auto-fixes where available\n   - Manually fix issues that can't be auto-fixed\n   - Re-run linters to verify all issues are resolved\n\n5. **Verify Clean State**:\n   - Run all linters again without fix flags\n   - Ensure no errors or warnings remain\n\nFix all issues found until the codebase passes all linting and formatting checks."
              },
              {
                "name": "/code-permutation-tester",
                "description": "Test multiple code variations through simulation before implementation with quality gates and performance prediction.",
                "path": "plugins/all-commands/commands/code-permutation-tester.md",
                "frontmatter": {
                  "description": "Test multiple code variations through simulation before implementation with quality gates and performance prediction.",
                  "category": "utilities-debugging",
                  "argument-hint": "Specify permutation test options"
                },
                "content": "# Code Permutation Tester\n\nTest multiple code variations through simulation before implementation with quality gates and performance prediction.\n\n## Instructions\n\nYou are tasked with systematically testing multiple code implementation approaches through simulation to optimize decisions before actual development. Follow this approach: **$ARGUMENTS**\n\n### 1. Prerequisites Assessment\n\n**Critical Code Context Validation:**\n\n- **Code Scope**: What specific code area/function/feature are you testing variations for?\n- **Variation Types**: What different approaches are you considering?\n- **Quality Criteria**: How will you evaluate which variation is best?\n- **Constraints**: What technical, performance, or resource constraints apply?\n- **Decision Timeline**: When do you need to choose an implementation approach?\n\n**If context is unclear, guide systematically:**\n\n```\nMissing Code Scope:\n\"What specific code area needs permutation testing?\n- Algorithm Implementation: Different algorithmic approaches for the same problem\n- Architecture Pattern: Various structural patterns (MVC, microservices, etc.)\n- Performance Optimization: Multiple optimization strategies for bottlenecks\n- API Design: Different interface design approaches\n- Data Structure Choice: Various data organization strategies\n\nPlease specify the exact function, module, or system component.\"\n\nMissing Variation Types:\n\"What different implementation approaches are you considering?\n- Algorithmic Variations: Different algorithms solving the same problem\n- Framework/Library Choices: Various tech stack options\n- Design Pattern Applications: Different structural and behavioral patterns\n- Performance Trade-offs: Speed vs. memory vs. maintainability variations\n- Integration Approaches: Different ways to connect with existing systems\"\n```\n\n### 2. Code Variation Generation\n\n**Systematically identify and structure implementation alternatives:**\n\n#### Implementation Approach Matrix\n```\nCode Variation Framework:\n\nAlgorithmic Variations:\n- Brute Force: Simple, readable implementation\n- Optimized: Performance-focused with complexity trade-offs\n- Hybrid: Balanced approach with configurable optimization\n- Novel: Innovative approaches using new techniques\n\nArchitectural Variations:\n- Monolithic: Single deployment unit with tight coupling\n- Modular: Loosely coupled modules within single codebase\n- Microservices: Distributed services with independent deployment\n- Serverless: Function-based with cloud provider management\n\nTechnology Stack Variations:\n- Traditional: Established, well-documented technologies\n- Modern: Current best practices and recent frameworks\n- Cutting-edge: Latest technologies with higher risk/reward\n- Hybrid: Mix of established and modern approaches\n\nPerformance Profile Variations:\n- Memory-optimized: Minimal memory footprint\n- Speed-optimized: Maximum execution performance  \n- Scalability-optimized: Handles growth efficiently\n- Maintainability-optimized: Easy to modify and extend\n```\n\n#### Variation Specification Framework\n```\nFor each code variation:\n\nImplementation Details:\n- Core Algorithm/Approach: [specific technical approach]\n- Key Dependencies: [frameworks, libraries, external services]\n- Architecture Pattern: [structural organization approach]\n- Data Flow Design: [how information moves through system]\n\nQuality Characteristics:\n- Performance Profile: [speed, memory, throughput expectations]\n- Maintainability Score: [ease of modification and extension]\n- Scalability Potential: [growth and load handling capability]\n- Reliability Assessment: [error handling and fault tolerance]\n\nResource Requirements:\n- Development Time: [estimated implementation effort]\n- Team Skill Requirements: [expertise needed for implementation]\n- Infrastructure Needs: [deployment and operational requirements]\n- Ongoing Maintenance: [long-term support and evolution needs]\n```\n\n### 3. Simulation Framework Design\n\n**Create testing environment for code variations:**\n\n#### Code Simulation Methodology\n```\nMulti-Dimensional Testing Approach:\n\nPerformance Simulation:\n- Synthetic workload generation and stress testing\n- Memory usage profiling and leak detection\n- Concurrent execution and race condition testing\n- Resource utilization monitoring and optimization\n\nMaintainability Simulation:\n- Code complexity analysis and metrics calculation\n- Change impact simulation and ripple effect analysis\n- Documentation quality and developer onboarding simulation\n- Debugging and troubleshooting ease assessment\n\nScalability Simulation:\n- Load growth simulation and performance degradation analysis\n- Horizontal scaling simulation and resource efficiency\n- Data volume growth impact and query performance\n- Integration point stress testing and failure handling\n\nSecurity Simulation:\n- Attack vector simulation and vulnerability assessment\n- Data protection and privacy compliance testing\n- Authentication and authorization load testing\n- Input validation and sanitization effectiveness\n```\n\n#### Testing Environment Setup\n- Isolated testing environments for each variation\n- Consistent data sets and test scenarios across variations\n- Automated testing pipeline and result collection\n- Realistic production environment simulation\n\n### 4. Quality Gate Framework\n\n**Establish systematic evaluation criteria:**\n\n#### Multi-Criteria Evaluation Matrix\n```\nCode Quality Assessment Framework:\n\nPerformance Gates (25% weight):\n- Response Time: [acceptable latency thresholds]\n- Throughput: [minimum requests/transactions per second]\n- Resource Usage: [memory, CPU, storage efficiency]\n- Scalability: [performance degradation under load]\n\nMaintainability Gates (25% weight):\n- Code Complexity: [cyclomatic complexity, nesting levels]\n- Test Coverage: [unit, integration, end-to-end test coverage]\n- Documentation Quality: [code comments, API docs, architecture docs]\n- Change Impact: [blast radius of typical modifications]\n\nReliability Gates (25% weight):\n- Error Handling: [graceful failure and recovery mechanisms]\n- Fault Tolerance: [system behavior under adverse conditions]\n- Data Integrity: [consistency and corruption prevention]\n- Monitoring/Observability: [debugging and operational visibility]\n\nBusiness Gates (25% weight):\n- Time to Market: [development speed and delivery timeline]\n- Total Cost of Ownership: [development + operational costs]\n- Risk Assessment: [technical and business risk factors]\n- Strategic Alignment: [fit with long-term technology direction]\n\nGate Score = (Performance  0.25) + (Maintainability  0.25) + (Reliability  0.25) + (Business  0.25)\n```\n\n#### Threshold Management\n- Minimum acceptable scores for each quality dimension\n- Trade-off analysis for competing quality attributes\n- Conditional gates based on specific use case requirements\n- Risk-adjusted thresholds for different implementation approaches\n\n### 5. Predictive Performance Modeling\n\n**Forecast real-world behavior before implementation:**\n\n#### Performance Prediction Framework\n```\nMulti-Layer Performance Modeling:\n\nMicro-Benchmarks:\n- Individual function and method performance measurement\n- Algorithm complexity analysis and big-O verification\n- Memory allocation patterns and garbage collection impact\n- CPU instruction efficiency and optimization opportunities\n\nIntegration Performance:\n- Inter-module communication overhead and optimization\n- Database query performance and connection pooling\n- External API latency and timeout handling\n- Caching strategy effectiveness and hit ratio analysis\n\nSystem-Level Performance:\n- End-to-end request processing and user experience\n- Concurrent user simulation and resource contention\n- Peak load handling and graceful degradation\n- Infrastructure scaling behavior and cost implications\n\nProduction Environment Prediction:\n- Real-world data volume and complexity simulation\n- Production traffic pattern modeling and capacity planning\n- Deployment and rollback performance impact assessment\n- Operational monitoring and alerting effectiveness\n```\n\n#### Confidence Interval Calculation\n- Statistical analysis of performance variation across test runs\n- Confidence levels for performance predictions under different conditions\n- Sensitivity analysis for key performance parameters\n- Risk assessment for performance-related business impacts\n\n### 6. Risk and Trade-off Analysis\n\n**Systematic evaluation of implementation choices:**\n\n#### Technical Risk Assessment\n```\nRisk Evaluation Framework:\n\nImplementation Risks:\n- Technical Complexity: [difficulty and error probability]\n- Dependency Risk: [external library and service dependencies]\n- Performance Risk: [ability to meet performance requirements]\n- Integration Risk: [compatibility with existing systems]\n\nOperational Risks:\n- Deployment Complexity: [rollout difficulty and rollback capability]\n- Monitoring/Debugging: [operational visibility and troubleshooting]\n- Scaling Challenges: [growth accommodation and resource planning]\n- Maintenance Burden: [ongoing support and evolution requirements]\n\nBusiness Risks:\n- Timeline Risk: [delivery schedule and market timing impact]\n- Resource Risk: [team capacity and skill requirements]\n- Opportunity Cost: [alternative approaches and strategic alignment]\n- Competitive Risk: [technology choice and market position impact]\n```\n\n#### Trade-off Optimization\n- Pareto frontier analysis for competing objectives\n- Multi-objective optimization for quality attributes\n- Scenario-based trade-off evaluation\n- Stakeholder preference weighting and consensus building\n\n### 7. Decision Matrix and Recommendations\n\n**Generate systematic implementation guidance:**\n\n#### Code Variation Evaluation Summary\n```\n## Code Permutation Analysis: [Feature/Module Name]\n\n### Variation Comparison Matrix\n\n| Variation | Performance | Maintainability | Reliability | Business | Overall Score |\n|-----------|-------------|-----------------|-------------|----------|---------------|\n| Approach A | 85% | 70% | 90% | 75% | 80% |\n| Approach B | 70% | 90% | 80% | 85% | 81% |\n| Approach C | 95% | 60% | 70% | 65% | 73% |\n\n### Detailed Analysis\n\n#### Recommended Approach: [Selected Variation]\n\n**Rationale:**\n- Performance Advantages: [specific benefits and measurements]\n- Maintainability Considerations: [long-term support implications]\n- Risk Assessment: [identified risks and mitigation strategies]\n- Business Alignment: [strategic fit and market timing]\n\n**Implementation Plan:**\n- Development Phases: [staged implementation approach]\n- Quality Checkpoints: [validation gates and success criteria]\n- Risk Mitigation: [specific risk reduction strategies]\n- Performance Validation: [ongoing monitoring and optimization]\n\n#### Alternative Considerations:\n- Backup Option: [second-choice approach and trigger conditions]\n- Hybrid Opportunities: [combining best elements from multiple approaches]\n- Future Evolution: [how to migrate or improve chosen approach]\n- Context Dependencies: [when alternative approaches might be better]\n\n### Success Metrics and Monitoring\n- Performance KPIs: [specific metrics and acceptable ranges]\n- Quality Indicators: [maintainability and reliability measures]\n- Business Outcomes: [user satisfaction and business impact metrics]\n- Early Warning Signs: [indicators that approach is not working]\n```\n\n### 8. Continuous Learning Integration\n\n**Establish feedback loops for approach refinement:**\n\n#### Implementation Validation\n- Real-world performance comparison to simulation predictions\n- Developer experience and productivity measurement\n- User feedback and satisfaction assessment\n- Business outcome tracking and success evaluation\n\n#### Knowledge Capture\n- Decision rationale documentation and lessons learned\n- Best practice identification and pattern library development\n- Anti-pattern recognition and avoidance strategies\n- Team capability building and expertise development\n\n## Usage Examples\n\n```bash\n# Algorithm optimization testing\n/dev:code-permutation-tester Test 5 different sorting algorithms for large dataset processing with memory and speed constraints\n\n# Architecture pattern evaluation\n/dev:code-permutation-tester Compare microservices vs monolith vs modular monolith for payment processing system\n\n# Framework selection simulation\n/dev:code-permutation-tester Evaluate React vs Vue vs Angular for customer dashboard with performance and maintainability focus\n\n# Database optimization testing\n/dev:code-permutation-tester Test NoSQL vs relational vs hybrid database approaches for user analytics platform\n```\n\n## Quality Indicators\n\n- **Green**: Multiple variations tested, comprehensive quality gates, validated performance predictions\n- **Yellow**: Some variations tested, basic quality assessment, estimated performance  \n- **Red**: Single approach, minimal testing, unvalidated assumptions\n\n## Common Pitfalls to Avoid\n\n- Premature optimization: Over-engineering for theoretical rather than real requirements\n- Analysis paralysis: Testing too many variations without making decisions\n- Context ignorance: Not considering real-world constraints and team capabilities\n- Quality tunnel vision: Optimizing for single dimension while ignoring others\n- Simulation disconnect: Testing scenarios that don't match production reality\n- Decision delay: Not acting on simulation results in timely manner\n\nTransform code implementation from guesswork into systematic, evidence-based decision making through comprehensive variation testing and simulation."
              },
              {
                "name": "/code-review",
                "description": "Perform comprehensive code quality review",
                "path": "plugins/all-commands/commands/code-review.md",
                "frontmatter": {
                  "description": "Perform comprehensive code quality review",
                  "category": "utilities-debugging"
                },
                "content": "# Comprehensive Code Quality Review\n\nPerform comprehensive code quality review\n\n## Instructions\n\nFollow these steps to conduct a thorough code review:\n\n1. **Repository Analysis**\n   - Examine the repository structure and identify the primary language/framework\n   - Check for configuration files (package.json, requirements.txt, Cargo.toml, etc.)\n   - Review README and documentation for context\n\n2. **Code Quality Assessment**\n   - Scan for code smells, anti-patterns, and potential bugs\n   - Check for consistent coding style and naming conventions\n   - Identify unused imports, variables, or dead code\n   - Review error handling and logging practices\n\n3. **Security Review**\n   - Look for common security vulnerabilities (SQL injection, XSS, etc.)\n   - Check for hardcoded secrets, API keys, or passwords\n   - Review authentication and authorization logic\n   - Examine input validation and sanitization\n\n4. **Performance Analysis**\n   - Identify potential performance bottlenecks\n   - Check for inefficient algorithms or database queries\n   - Review memory usage patterns and potential leaks\n   - Analyze bundle size and optimization opportunities\n\n5. **Architecture & Design**\n   - Evaluate code organization and separation of concerns\n   - Check for proper abstraction and modularity\n   - Review dependency management and coupling\n   - Assess scalability and maintainability\n\n6. **Testing Coverage**\n   - Check existing test coverage and quality\n   - Identify areas lacking proper testing\n   - Review test structure and organization\n   - Suggest additional test scenarios\n\n7. **Documentation Review**\n   - Evaluate code comments and inline documentation\n   - Check API documentation completeness\n   - Review README and setup instructions\n   - Identify areas needing better documentation\n\n8. **Recommendations**\n   - Prioritize issues by severity (critical, high, medium, low)\n   - Provide specific, actionable recommendations\n   - Suggest tools and practices for improvement\n   - Create a summary report with next steps\n\nRemember to be constructive and provide specific examples with file paths and line numbers where applicable."
              },
              {
                "name": "/code-to-task",
                "description": "Convert code analysis to Linear tasks",
                "path": "plugins/all-commands/commands/code-to-task.md",
                "frontmatter": {
                  "description": "Convert code analysis to Linear tasks",
                  "category": "utilities-debugging"
                },
                "content": "# Convert Code Analysis to Linear Tasks\n\nConvert code analysis to Linear tasks\n\n## Purpose\nThis command scans your codebase for TODO/FIXME comments, technical debt markers, deprecated code, and other indicators that should be tracked as tasks. It automatically creates organized, prioritized Linear tasks to ensure important code improvements aren't forgotten.\n\n## Usage\n```bash\n# Scan entire codebase for TODOs and create tasks\nclaude \"Create tasks from all TODO comments in the codebase\"\n\n# Scan specific directory or module\nclaude \"Find TODOs in src/api and create Linear tasks\"\n\n# Create tasks from specific patterns\nclaude \"Create tasks for all deprecated functions\"\n\n# Generate technical debt report\nclaude \"Analyze technical debt in the project and create improvement tasks\"\n```\n\n## Instructions\n\n### 1. Scan for Task Markers\nSearch for common patterns indicating needed work:\n\n```bash\n# Find TODO comments\nrg \"TODO|FIXME|HACK|XXX|OPTIMIZE|REFACTOR\" --type-add 'code:*.{js,ts,py,java,go,rb,php}' -t code\n\n# Find deprecated markers\nrg \"@deprecated|DEPRECATED|@obsolete\" -t code\n\n# Find temporary code\nrg \"TEMPORARY|TEMP|REMOVE BEFORE|DELETE ME\" -t code -i\n\n# Find technical debt markers\nrg \"TECHNICAL DEBT|TECH DEBT|REFACTOR|NEEDS REFACTORING\" -t code -i\n\n# Find security concerns\nrg \"SECURITY|INSECURE|VULNERABILITY|CVE-\" -t code -i\n\n# Find performance issues\nrg \"SLOW|PERFORMANCE|OPTIMIZE|BOTTLENECK\" -t code -i\n```\n\n### 2. Parse Comment Context\nExtract meaningful information from comments:\n\n```javascript\nclass CommentParser {\n  parseComment(file, lineNumber, comment) {\n    const parsed = {\n      type: 'todo',\n      priority: 'medium',\n      title: '',\n      description: '',\n      author: null,\n      date: null,\n      tags: [],\n      code_context: '',\n      file_path: file,\n      line_number: lineNumber\n    };\n    \n    // Detect comment type\n    if (comment.match(/FIXME/i)) {\n      parsed.type = 'fixme';\n      parsed.priority = 'high';\n    } else if (comment.match(/HACK|XXX/i)) {\n      parsed.type = 'hack';\n      parsed.priority = 'high';\n    } else if (comment.match(/OPTIMIZE|PERFORMANCE/i)) {\n      parsed.type = 'optimization';\n    } else if (comment.match(/DEPRECATED/i)) {\n      parsed.type = 'deprecation';\n      parsed.priority = 'high';\n    } else if (comment.match(/SECURITY/i)) {\n      parsed.type = 'security';\n      parsed.priority = 'urgent';\n    }\n    \n    // Extract author and date\n    const authorMatch = comment.match(/@(\\w+)|by (\\w+)/i);\n    if (authorMatch) {\n      parsed.author = authorMatch[1] || authorMatch[2];\n    }\n    \n    const dateMatch = comment.match(/(\\d{4}-\\d{2}-\\d{2})|(\\d{1,2}\\/\\d{1,2}\\/\\d{2,4})/);\n    if (dateMatch) {\n      parsed.date = dateMatch[0];\n    }\n    \n    // Extract title and description\n    const cleanComment = comment\n      .replace(/^\\/\\/\\s*|^\\/\\*\\s*|\\*\\/\\s*$|^#\\s*/g, '')\n      .replace(/TODO|FIXME|HACK|XXX/i, '')\n      .trim();\n    \n    const parts = cleanComment.split(/[:\\-]/);\n    if (parts.length > 1) {\n      parsed.title = parts[0].trim();\n      parsed.description = parts.slice(1).join(':').trim();\n    } else {\n      parsed.title = cleanComment;\n    }\n    \n    // Extract tags\n    const tagMatch = comment.match(/#(\\w+)/g);\n    if (tagMatch) {\n      parsed.tags = tagMatch.map(tag => tag.substring(1));\n    }\n    \n    return parsed;\n  }\n  \n  getCodeContext(file, lineNumber, contextLines = 5) {\n    const lines = readFileLines(file);\n    const start = Math.max(0, lineNumber - contextLines);\n    const end = Math.min(lines.length, lineNumber + contextLines);\n    \n    return lines.slice(start, end).map((line, i) => ({\n      number: start + i + 1,\n      content: line,\n      isTarget: start + i + 1 === lineNumber\n    }));\n  }\n}\n```\n\n### 3. Group and Deduplicate\nOrganize found issues intelligently:\n\n```javascript\nclass TaskGrouper {\n  groupTasks(parsedComments) {\n    const groups = {\n      byFile: new Map(),\n      byType: new Map(),\n      byAuthor: new Map(),\n      byModule: new Map()\n    };\n    \n    for (const comment of parsedComments) {\n      // Group by file\n      if (!groups.byFile.has(comment.file_path)) {\n        groups.byFile.set(comment.file_path, []);\n      }\n      groups.byFile.get(comment.file_path).push(comment);\n      \n      // Group by type\n      if (!groups.byType.has(comment.type)) {\n        groups.byType.set(comment.type, []);\n      }\n      groups.byType.get(comment.type).push(comment);\n      \n      // Group by module\n      const module = this.extractModule(comment.file_path);\n      if (!groups.byModule.has(module)) {\n        groups.byModule.set(module, []);\n      }\n      groups.byModule.get(module).push(comment);\n    }\n    \n    return groups;\n  }\n  \n  mergeSimilarTasks(tasks) {\n    const merged = [];\n    const seen = new Set();\n    \n    for (const task of tasks) {\n      if (seen.has(task)) continue;\n      \n      // Find similar tasks\n      const similar = tasks.filter(t => \n        t !== task &&\n        !seen.has(t) &&\n        this.areSimilar(task, t)\n      );\n      \n      if (similar.length > 0) {\n        // Merge into one task\n        const mergedTask = {\n          ...task,\n          title: this.generateMergedTitle(task, similar),\n          description: this.generateMergedDescription(task, similar),\n          locations: [task, ...similar].map(t => ({\n            file: t.file_path,\n            line: t.line_number\n          }))\n        };\n        merged.push(mergedTask);\n        seen.add(task);\n        similar.forEach(t => seen.add(t));\n      } else {\n        merged.push(task);\n        seen.add(task);\n      }\n    }\n    \n    return merged;\n  }\n}\n```\n\n### 4. Analyze Technical Debt\nIdentify code quality issues:\n\n```javascript\nclass TechnicalDebtAnalyzer {\n  async analyzeFile(filePath) {\n    const issues = [];\n    const content = await readFile(filePath);\n    const lines = content.split('\\n');\n    \n    // Check for long functions\n    const functionMatches = content.matchAll(/function\\s+(\\w+)|(\\w+)\\s*=\\s*\\(.*?\\)\\s*=>/g);\n    for (const match of functionMatches) {\n      const functionName = match[1] || match[2];\n      const startLine = getLineNumber(content, match.index);\n      const functionLength = this.getFunctionLength(lines, startLine);\n      \n      if (functionLength > 50) {\n        issues.push({\n          type: 'long_function',\n          severity: functionLength > 100 ? 'high' : 'medium',\n          title: `Refactor long function: ${functionName}`,\n          description: `Function ${functionName} is ${functionLength} lines long. Consider breaking it into smaller functions.`,\n          file_path: filePath,\n          line_number: startLine\n        });\n      }\n    }\n    \n    // Check for duplicate code\n    const duplicates = await this.findDuplicateCode(filePath);\n    for (const dup of duplicates) {\n      issues.push({\n        type: 'duplicate_code',\n        severity: 'medium',\n        title: 'Remove duplicate code',\n        description: `Similar code found in ${dup.otherFile}:${dup.otherLine}`,\n        file_path: filePath,\n        line_number: dup.line\n      });\n    }\n    \n    // Check for complex conditionals\n    const complexConditions = content.matchAll(/if\\s*\\([^)]{50,}\\)/g);\n    for (const match of complexConditions) {\n      issues.push({\n        type: 'complex_condition',\n        severity: 'low',\n        title: 'Simplify complex conditional',\n        description: 'Consider extracting conditional logic into named variables or functions',\n        file_path: filePath,\n        line_number: getLineNumber(content, match.index)\n      });\n    }\n    \n    // Check for outdated dependencies\n    if (filePath.endsWith('package.json')) {\n      const outdated = await this.checkOutdatedDependencies(filePath);\n      for (const dep of outdated) {\n        issues.push({\n          type: 'outdated_dependency',\n          severity: dep.major ? 'high' : 'low',\n          title: `Update ${dep.name} from ${dep.current} to ${dep.latest}`,\n          description: dep.major ? 'Major version update available' : 'Minor update available',\n          file_path: filePath\n        });\n      }\n    }\n    \n    return issues;\n  }\n}\n```\n\n### 5. Create Linear Tasks\nConvert findings into actionable tasks:\n\n```javascript\nasync function createLinearTasks(groupedTasks, options = {}) {\n  const created = [];\n  const skipped = [];\n  \n  // Check for existing tasks to avoid duplicates\n  const existingTasks = await linear.searchTasks('TODO OR FIXME');\n  const existingTitles = new Set(existingTasks.map(t => t.title));\n  \n  // Create parent task for large groups\n  if (options.createEpic && groupedTasks.length > 10) {\n    const epic = await linear.createTask({\n      title: `Technical Debt: ${options.module || 'Codebase'} Cleanup`,\n      description: `Parent task for ${groupedTasks.length} code improvements`,\n      priority: 2,\n      labels: ['technical-debt', 'code-quality']\n    });\n    options.parentId = epic.id;\n  }\n  \n  for (const task of groupedTasks) {\n    // Skip if similar task exists\n    if (existingTitles.has(task.title)) {\n      skipped.push({ task, reason: 'duplicate' });\n      continue;\n    }\n    \n    // Build task description\n    const description = buildTaskDescription(task);\n    \n    // Map priority\n    const priorityMap = {\n      urgent: 1,\n      high: 2,\n      medium: 3,\n      low: 4\n    };\n    \n    try {\n      const linearTask = await linear.createTask({\n        title: task.title,\n        description,\n        priority: priorityMap[task.priority] || 3,\n        labels: getLabelsForTask(task),\n        parentId: options.parentId,\n        estimate: estimateTaskSize(task)\n      });\n      \n      created.push({\n        linear: linearTask,\n        source: task\n      });\n      \n      // Add code link as comment\n      await linear.createComment({\n        issueId: linearTask.id,\n        body: ` Code location: \\`${task.file_path}:${task.line_number}\\``\n      });\n      \n    } catch (error) {\n      skipped.push({ task, reason: error.message });\n    }\n  }\n  \n  return { created, skipped };\n}\n\nfunction buildTaskDescription(task) {\n  let description = task.description || '';\n  \n  // Add code context\n  if (task.code_context) {\n    description += '\\n\\n### Code Context\\n```\\n';\n    task.code_context.forEach(line => {\n      const prefix = line.isTarget ? '>>> ' : '    ';\n      description += `${prefix}${line.number}: ${line.content}\\n`;\n    });\n    description += '```\\n';\n  }\n  \n  // Add metadata\n  description += '\\n\\n### Details\\n';\n  description += `- **Type**: ${task.type}\\n`;\n  description += `- **File**: \\`${task.file_path}\\`\\n`;\n  description += `- **Line**: ${task.line_number}\\n`;\n  \n  if (task.author) {\n    description += `- **Author**: @${task.author}\\n`;\n  }\n  if (task.date) {\n    description += `- **Date**: ${task.date}\\n`;\n  }\n  if (task.tags.length > 0) {\n    description += `- **Tags**: ${task.tags.join(', ')}\\n`;\n  }\n  \n  // Add suggestions\n  if (task.type === 'deprecated') {\n    description += '\\n### Suggested Actions\\n';\n    description += '1. Identify all usages of this deprecated code\\n';\n    description += '2. Update to use the recommended alternative\\n';\n    description += '3. Add deprecation warnings if not present\\n';\n    description += '4. Schedule for removal in next major version\\n';\n  }\n  \n  return description;\n}\n```\n\n### 6. Generate Summary Report\nCreate overview of findings:\n\n```javascript\nfunction generateReport(scanResults, createdTasks) {\n  const report = {\n    summary: {\n      totalFound: scanResults.length,\n      tasksCreated: createdTasks.created.length,\n      tasksSkipped: createdTasks.skipped.length,\n      byType: {},\n      byPriority: {},\n      byFile: {}\n    },\n    details: [],\n    recommendations: []\n  };\n  \n  // Analyze distribution\n  for (const result of scanResults) {\n    report.summary.byType[result.type] = (report.summary.byType[result.type] || 0) + 1;\n    report.summary.byPriority[result.priority] = (report.summary.byPriority[result.priority] || 0) + 1;\n  }\n  \n  // Generate recommendations\n  if (report.summary.byType.security > 0) {\n    report.recommendations.push({\n      priority: 'urgent',\n      action: 'Address security-related TODOs immediately',\n      tasks: scanResults.filter(r => r.type === 'security').length\n    });\n  }\n  \n  if (report.summary.byType.deprecated > 5) {\n    report.recommendations.push({\n      priority: 'high',\n      action: 'Create deprecation removal sprint',\n      tasks: report.summary.byType.deprecated\n    });\n  }\n  \n  return report;\n}\n```\n\n### 7. Error Handling\n```javascript\n// Handle access errors\ntry {\n  await scanDirectory(path);\n} catch (error) {\n  if (error.code === 'EACCES') {\n    console.warn(`Skipping ${path} - permission denied`);\n  }\n}\n\n// Handle Linear API limits\nconst rateLimiter = {\n  tasksCreated: 0,\n  resetTime: Date.now() + 3600000,\n  \n  async createTask(taskData) {\n    if (this.tasksCreated >= 50) {\n      console.log('Rate limit approaching, batching remaining tasks...');\n      // Create single task with list of TODOs\n      return this.createBatchTask(remainingTasks);\n    }\n    this.tasksCreated++;\n    return linear.createTask(taskData);\n  }\n};\n\n// Handle malformed comments\nconst safeParser = {\n  parse(comment) {\n    try {\n      return this.parseComment(comment);\n    } catch (error) {\n      return {\n        type: 'todo',\n        title: comment.substring(0, 50) + '...',\n        priority: 'low',\n        parseError: true\n      };\n    }\n  }\n};\n```\n\n## Example Output\n\n```\nScanning codebase for TODOs and technical debt...\n\n Scan Results:\n\n\nFound 47 items across 23 files:\n   24 TODOs\n   8 FIXMEs \n   5 Deprecated functions\n   3 Security concerns\n   7 Performance optimizations\n\n Breakdown by Priority:\n   Urgent: 3 (security related)\n   High: 13 (FIXMEs + deprecations)\n   Medium: 24 (standard TODOs)\n   Low: 7 (optimizations)\n\n Hotspot Files:\n  1. src/api/auth.js - 8 items\n  2. src/utils/validation.js - 6 items\n  3. src/models/User.js - 5 items\n\n Critical Findings:\n\n1. SECURITY: Hardcoded API key\n   File: src/config/api.js:45\n   TODO: Remove hardcoded key and use env variable\n    Creating task with URGENT priority\n\n2. DEPRECATED: Legacy authentication method\n   File: src/api/auth.js:120\n   Multiple usages found in 4 files\n    Creating migration task\n\n3. FIXME: Race condition in concurrent updates\n   File: src/services/sync.js:78\n   Author: @alice (2024-01-03)\n    Creating high-priority bug task\n\n Task Creation Summary:\n\n\n Created 32 Linear tasks:\n   - Epic: \"Q1 Technical Debt Cleanup\" (LIN-456)\n   - 3 urgent security tasks\n   - 10 high-priority fixes\n   - 19 medium-priority improvements\n\n Skipped 15 items:\n   - 8 duplicates (tasks already exist)\n   - 4 low-value comments (e.g., \"TODO: think about this\")\n   - 3 external dependencies (waiting on upstream)\n\n Estimates:\n   - Total story points: 89\n   - Estimated effort: 2-3 sprints\n   - Recommended team size: 2-3 developers\n\n Recommended Actions:\n1. Schedule security sprint immediately (3 urgent items)\n2. Assign deprecation removal to next sprint (5 items)\n3. Create coding standards to reduce future TODOs\n4. Set up pre-commit hook to limit new TODOs\n\nView all created tasks:\nhttps://linear.app/yourteam/project/q1-technical-debt-cleanup\n```\n\n## Advanced Features\n\n### Custom Patterns\nDefine project-specific patterns:\n```bash\n# Add custom markers to scan\nclaude \"Scan for REVIEW, QUESTION, and ASSUMPTION comments\"\n```\n\n### Integration with CI/CD\n```bash\n# Fail build if critical TODOs found\nclaude \"Check for SECURITY or FIXME comments and exit with error if found\"\n```\n\n### Scheduled Scans\n```bash\n# Weekly technical debt report\nclaude \"Generate weekly technical debt report and create tasks for new items\"\n```\n\n## Tips\n- Run regularly to prevent TODO accumulation\n- Use consistent comment formats across the team\n- Include author and date in TODOs\n- Link TODOs to existing Linear issues when possible\n- Set up IDE snippets for properly formatted TODOs\n- Review and close completed TODO tasks\n- Use TODO comments as a quality gate in PR reviews"
              },
              {
                "name": "/code_analysis",
                "description": "Perform comprehensive code analysis with quality metrics and recommendations",
                "path": "plugins/all-commands/commands/code_analysis.md",
                "frontmatter": {
                  "description": "Perform comprehensive code analysis with quality metrics and recommendations",
                  "category": "code-analysis-testing",
                  "argument-hint": "[file-or-directory-path]",
                  "allowed-tools": "Read, Grep, Glob, TodoWrite"
                },
                "content": "Perform a comprehensive code analysis on the specified files or directory. If no path is provided, analyze the current working directory.\n\n## Analysis Process:\n\n1. **Parse Arguments**:\n   - Extract the path from $ARGUMENTS (defaults to current directory if not specified)\n   - Determine scope: single file, multiple files, or entire directory\n\n2. **Language Detection**:\n   - Identify programming language(s) based on file extensions\n   - Apply language-specific analysis rules\n\n3. **Code Quality Analysis**:\n   - **Complexity Metrics**: Cyclomatic complexity, nesting depth, function length\n   - **Code Smells**: Long methods, large classes, duplicate code patterns\n   - **Best Practices**: Naming conventions, code organization, documentation\n   - **Security Issues**: Common vulnerabilities, unsafe patterns, input validation\n   - **Performance**: Inefficient algorithms, memory leaks, blocking operations\n   - **Maintainability**: Code coupling, cohesion, test coverage indicators\n\n4. **Generate Report**:\n   - Summary with overall health score\n   - Detailed findings by category\n   - Priority-ranked issues (High/Medium/Low)\n   - Specific file and line references\n   - Actionable recommendations for improvement\n\n5. **Track with TodoWrite**:\n   - Create todos for high-priority issues found\n   - Organize by fix complexity and impact\n\n## Example Usage:\n- `/code_analysis` - Analyze entire current directory\n- `/code_analysis src/` - Analyze all code in src directory\n- `/code_analysis app.js` - Analyze specific file\n- `/code_analysis \"src/**/*.py\"` - Analyze all Python files in src\n\nTarget path: $ARGUMENTS"
              },
              {
                "name": "/commit-fast",
                "description": "Automatically create and execute a git commit using the first suggested commit message",
                "path": "plugins/all-commands/commands/commit-fast.md",
                "frontmatter": {
                  "description": "Automatically create and execute a git commit using the first suggested commit message",
                  "category": "version-control-git",
                  "allowed-tools": "Bash(git *)"
                },
                "content": "# Create new fast commit task\n\nThis task uses the same logic as the commit task (.claude/commands/commit.md) but automatically selects the first suggested commit message without asking for confirmation.\n\n- Generate 3 commit message suggestions following the same format as the commit task\n- Automatically use the first suggestion without asking the user\n- Immediately run `git commit -m` with the first message\n- All other behaviors remain the same as the commit task (format, package names, staged files only)\n- Do NOT add Claude co-authorship footer to commits"
              },
              {
                "name": "/commit",
                "description": "Create well-formatted git commits with conventional commit messages and emoji",
                "path": "plugins/all-commands/commands/commit.md",
                "frontmatter": {
                  "description": "Create well-formatted git commits with conventional commit messages and emoji",
                  "category": "version-control-git",
                  "allowed-tools": "Bash, Read, Glob"
                },
                "content": "# Claude Command: Commit\n\nThis command helps you create well-formatted commits with conventional commit messages and emoji.\n\n## Usage\n\nTo create a commit, just type:\n```\n/commit\n```\n\nOr with options:\n```\n/commit --no-verify\n```\n\n## What This Command Does\n\n1. Unless specified with `--no-verify`, automatically runs pre-commit checks:\n   - Detect package manager (npm, pnpm, yarn, bun) and run appropriate commands\n   - Run lint/format checks if available\n   - Run build verification if build script exists\n   - Update documentation if generation script exists\n2. Checks which files are staged with `git status`\n3. If 0 files are staged, automatically adds all modified and new files with `git add`\n4. Performs a `git diff` to understand what changes are being committed\n5. Analyzes the diff to determine if multiple distinct logical changes are present\n6. If multiple distinct changes are detected, suggests breaking the commit into multiple smaller commits\n7. For each commit (or the single commit if not split), creates a commit message using emoji conventional commit format\n\n## Best Practices for Commits\n\n- **Verify before committing**: Ensure code is linted, builds correctly, and documentation is updated\n- **Atomic commits**: Each commit should contain related changes that serve a single purpose\n- **Split large changes**: If changes touch multiple concerns, split them into separate commits\n- **Conventional commit format**: Use the format `<type>: <description>` where type is one of:\n  - `feat`: A new feature\n  - `fix`: A bug fix\n  - `docs`: Documentation changes\n  - `style`: Code style changes (formatting, etc)\n  - `refactor`: Code changes that neither fix bugs nor add features\n  - `perf`: Performance improvements\n  - `test`: Adding or fixing tests\n  - `chore`: Changes to the build process, tools, etc.\n- **Present tense, imperative mood**: Write commit messages as commands (e.g., \"add feature\" not \"added feature\")\n- **Concise first line**: Keep the first line under 72 characters\n- **Emoji**: Each commit type is paired with an appropriate emoji:\n  -  `feat`: New feature"
              },
              {
                "name": "/constraint-modeler",
                "description": "Model world constraints with assumption validation, dependency mapping, and scenario boundary definition.",
                "path": "plugins/all-commands/commands/constraint-modeler.md",
                "frontmatter": {
                  "description": "Model world constraints with assumption validation, dependency mapping, and scenario boundary definition.",
                  "category": "simulation-modeling",
                  "argument-hint": "Specify constraint parameters"
                },
                "content": "# Constraint Modeler\n\nModel world constraints with assumption validation, dependency mapping, and scenario boundary definition.\n\n## Instructions\n\nYou are tasked with systematically modeling the constraints that govern your decision environment to create accurate simulations and scenarios. Follow this approach: **$ARGUMENTS**\n\n### 1. Prerequisites Assessment\n\n**Critical Constraint Context Validation:**\n\n- **Domain Definition**: What system/environment are you modeling constraints for?\n- **Constraint Types**: Physical, economic, regulatory, technical, or social constraints?\n- **Impact Scope**: How do these constraints affect decisions and outcomes?\n- **Change Dynamics**: Are constraints static or do they evolve over time?\n- **Validation Sources**: What data/expertise can verify constraint accuracy?\n\n**If context is unclear, guide systematically:**\n\n```\nMissing Domain Context:\n\"I need to understand what you're modeling constraints for:\n- Business Domain: Market constraints, competitive dynamics, regulatory environment\n- Technical Domain: System limitations, performance bounds, technology constraints\n- Operational Domain: Resource constraints, process limitations, capacity bounds\n- Financial Domain: Budget constraints, investment limitations, economic factors\n\nExamples:\n- 'SaaS business operating in regulated healthcare market'\n- 'Manufacturing system with supply chain and quality constraints'\n- 'Software architecture with performance and scalability requirements'\"\n\nMissing Constraint Types:\n\"What types of constraints are most relevant to your decisions?\n- Hard Constraints: Absolute limits that cannot be violated\n- Soft Constraints: Preferences and trade-offs that can be managed\n- Regulatory Constraints: Legal and compliance requirements\n- Resource Constraints: Budget, time, and capacity limitations\n- Market Constraints: Customer behavior and competitive dynamics\"\n```\n\n### 2. Constraint Taxonomy Framework\n\n**Systematically categorize and structure constraints:**\n\n#### Hard Constraints (Cannot be violated)\n```\nPhysical/Natural Constraints:\n- Laws of physics and natural limitations\n- Geographic and spatial boundaries\n- Time and temporal restrictions\n- Resource scarcity and finite capacity\n\nRegulatory/Legal Constraints:\n- Compliance requirements and legal mandates\n- Industry standards and certification requirements\n- Contractual obligations and agreements\n- Intellectual property and licensing restrictions\n\nTechnical Constraints:\n- System capacity and performance limits\n- Technology compatibility and integration requirements\n- Security and privacy constraints\n- Infrastructure limitations and dependencies\n```\n\n#### Soft Constraints (Can be managed/traded off)\n```\nEconomic Constraints:\n- Budget limitations and financial resources\n- Cost optimization and efficiency targets\n- Investment return requirements and payback periods\n- Market pricing and competitive pressure\n\nOrganizational Constraints:\n- Team capacity and skill limitations\n- Cultural and change management factors\n- Decision-making processes and approval cycles\n- Risk tolerance and strategic priorities\n\nMarket Constraints:\n- Customer preferences and behavior patterns\n- Competitive dynamics and response patterns\n- Market timing and seasonal factors\n- Distribution channel limitations and requirements\n```\n\n#### Dynamic Constraints (Change over time)\n```\nEvolutionary Constraints:\n- Technology advancement and obsolescence cycles\n- Market maturation and customer evolution\n- Regulatory changes and policy shifts\n- Competitive landscape evolution\n\nCyclical Constraints:\n- Seasonal business patterns and market cycles\n- Economic cycles and market conditions\n- Budget cycles and resource allocation patterns\n- Technology refresh and upgrade cycles\n```\n\n### 3. Constraint Mapping and Visualization\n\n**Create comprehensive constraint relationship models:**\n\n#### Constraint Interaction Matrix\n```\nConstraint Relationship Analysis:\n\nPrimary Constraints  Secondary Effects:\n- Budget Limitation  Team size  Development capacity  Feature scope\n- Regulatory Requirement  Compliance process  Timeline extension  Market timing\n- Technical Constraint  Architecture choice  Scalability  Growth potential\n\nConstraint Conflicts and Trade-offs:\n- Speed vs. Quality: Time constraint vs. quality constraint\n- Cost vs. Capability: Budget constraint vs. feature constraint  \n- Security vs. Usability: Security constraint vs. user experience constraint\n- Scale vs. Simplicity: Growth constraint vs. complexity constraint\n\nConstraint Dependencies:\n- Sequential: Constraint A must be satisfied before addressing Constraint B\n- Conditional: Constraint A applies only if Condition X is true\n- Mutual: Constraints A and B reinforce or conflict with each other\n- Hierarchical: Constraint A contains or encompasses Constraint B\n```\n\n#### Constraint Hierarchy Modeling\n- Strategic level constraints (mission, vision, values)\n- Tactical level constraints (resources, capabilities, market position)\n- Operational level constraints (processes, systems, daily operations)\n- Individual level constraints (skills, capacity, availability)\n\n### 4. Assumption Validation Framework\n\n**Systematically test and validate constraint assumptions:**\n\n#### Assumption Documentation\n```\nConstraint Assumption Template:\n\nConstraint: [Name and description]\nAssumption: [What we believe to be true about this constraint]\nSource: [Where this assumption comes from]\nConfidence Level: [1-10 scale with justification]\nImpact if Wrong: [What happens if assumption is incorrect]\nValidation Method: [How to test this assumption]\nUpdate Frequency: [How often to re-validate]\n\nExample:\nConstraint: \"Engineering team capacity\"\nAssumption: \"Team can deliver 10 story points per sprint\"\nSource: \"Historical velocity data from last 6 sprints\"\nConfidence Level: \"8 - consistent recent data but team composition changing\"\nImpact if Wrong: \"Project timeline delays, scope reduction needed\"\nValidation Method: \"Track actual velocity, monitor team changes\"\nUpdate Frequency: \"Monthly review with sprint retrospectives\"\n```\n\n#### Historical Validation\n- Analysis of past constraint behavior and violation patterns\n- Comparison of assumed vs. actual constraint limits\n- Pattern recognition for constraint evolution and change\n- Case study analysis from similar environments and decisions\n\n#### Real-time Validation\n- Continuous monitoring of constraint status and changes\n- Early warning systems for constraint violation risks\n- Feedback loops from constraint testing and boundary pushing\n- Expert consultation and stakeholder validation\n\n### 5. Scenario Boundary Definition\n\n**Use constraints to define realistic scenario limits:**\n\n#### Feasible Scenario Space\n```\nScenario Constraint Boundaries:\n\nOptimistic Boundary:\n- Best-case constraint relaxation (10-20% improvement)\n- Favorable external conditions and support\n- Maximum resource availability and efficiency\n- Minimal constraint conflicts and trade-offs\n\nRealistic Boundary:\n- Expected constraint behavior and normal conditions\n- Typical resource availability and standard efficiency\n- Normal constraint conflicts requiring standard trade-offs\n- Historical pattern-based constraint evolution\n\nPessimistic Boundary:\n- Worst-case constraint tightening (10-20% degradation)\n- Adverse external conditions and additional restrictions\n- Reduced resource availability and efficiency challenges\n- Maximum constraint conflicts requiring difficult trade-offs\n```\n\n#### Constraint Stress Testing\n- Maximum constraint load scenarios and breaking points\n- Cascade failure analysis when key constraints are violated\n- Recovery scenarios and constraint restoration approaches\n- Adaptive scenario adjustment for changing constraints\n\n### 6. Dynamic Constraint Modeling\n\n**Model how constraints change over time:**\n\n#### Constraint Evolution Patterns\n```\nTemporal Constraint Dynamics:\n\nLinear Evolution:\n- Gradual constraint relaxation or tightening over time\n- Predictable improvement or degradation patterns\n- Resource accumulation or depletion trends\n- Market maturation and capacity development\n\nCyclical Evolution:\n- Seasonal constraint variations and patterns\n- Economic cycle impacts on constraint severity\n- Technology refresh cycles and capability updates\n- Regulatory review cycles and compliance windows\n\nStep Function Evolution:\n- Sudden constraint changes from external events\n- Technology breakthrough impacts on capability constraints\n- Regulatory changes creating new constraint requirements\n- Market disruptions changing competitive constraints\n\nThreshold Evolution:\n- Constraint regime changes at specific trigger points\n- Scale-dependent constraint behavior modifications\n- Maturity-based constraint relaxation or introduction\n- Performance-based constraint adjustment mechanisms\n```\n\n#### Adaptive Constraint Management\n- Constraint monitoring and early warning systems\n- Proactive constraint modification and optimization\n- Scenario adaptation for changing constraint conditions\n- Strategic planning for anticipated constraint evolution\n\n### 7. Constraint Optimization Strategies\n\n**Generate approaches to work within and optimize constraints:**\n\n#### Constraint Relaxation Approaches\n```\nSystematic Constraint Optimization:\n\nDirect Relaxation:\n- Negotiate constraint modifications with stakeholders\n- Invest in capability building to reduce constraint impact\n- Seek regulatory relief or compliance alternatives\n- Restructure processes to minimize constraint conflicts\n\nConstraint Substitution:\n- Replace restrictive constraints with more flexible alternatives\n- Trade hard constraints for soft constraints where possible\n- Substitute resource constraints with efficiency improvements\n- Replace time constraints with scope or quality adjustments\n\nConstraint Circumvention:\n- Design solutions that avoid constraint-heavy areas\n- Use alternative approaches that minimize constraint impact\n- Leverage partnerships to access capabilities beyond constraints\n- Phase implementations to work within temporal constraints\n```\n\n#### Creative Constraint Solutions\n- Constraint reframing and alternative perspective development\n- Innovative approaches that turn constraints into advantages\n- Synergistic solutions that address multiple constraints simultaneously\n- Constraint-inspired innovation and creative problem solving\n\n### 8. Output Generation and Documentation\n\n**Present constraint analysis in actionable format:**\n\n```\n## Constraint Model Analysis: [Domain/Project Name]\n\n### Constraint Environment Overview\n- Domain Scope: [what is being constrained]\n- Primary Constraints: [most limiting factors]\n- Constraint Severity: [impact on decisions and outcomes]\n- Change Dynamics: [how constraints evolve over time]\n\n### Constraint Inventory\n\n#### Hard Constraints (Cannot be violated):\n| Constraint | Description | Impact | Validation Status |\n|------------|-------------|---------|------------------|\n| [Name] | [Details] | [Effect] | [Confidence level] |\n\n#### Soft Constraints (Can be managed):\n| Constraint | Description | Trade-off Options | Optimization Potential |\n|------------|-------------|-------------------|----------------------|\n| [Name] | [Details] | [Alternatives] | [Improvement possibilities] |\n\n#### Dynamic Constraints (Change over time):\n| Constraint | Current State | Evolution Pattern | Future Projection |\n|------------|---------------|------------------|------------------|\n| [Name] | [Status] | [Change pattern] | [Expected future state] |\n\n### Constraint Interaction Analysis\n- Primary Constraint Conflicts: [major trade-offs required]\n- Constraint Dependencies: [how constraints affect each other]\n- Cascade Effects: [secondary impacts of constraint changes]\n- Optimization Opportunities: [where constraint improvements are possible]\n\n### Scenario Boundary Definition\n- Feasible Scenario Space: [what scenarios are possible within constraints]\n- Constraint-Breaking Scenarios: [what would require constraint violation]\n- Optimization Scenarios: [how constraint improvements could expand possibilities]\n- Stress Test Boundaries: [maximum constraint loads the system can handle]\n\n### Constraint Management Strategies\n- Immediate Optimization: [quick constraint improvements available]\n- Strategic Relaxation: [longer-term constraint modification approaches]\n- Alternative Approaches: [ways to minimize constraint impact]\n- Risk Mitigation: [approaches to handle constraint violations]\n\n### Validation and Monitoring Plan\n- Constraint Monitoring: [how to track constraint status and changes]\n- Assumption Testing: [how to validate constraint assumptions]\n- Update Schedule: [when to refresh constraint model]\n- Warning Systems: [early alerts for constraint violations]\n```\n\n### 9. Continuous Constraint Learning\n\n**Establish ongoing constraint model improvement:**\n\n#### Feedback Integration\n- Actual constraint behavior vs. model predictions\n- Constraint violation lessons and recovery insights\n- Stakeholder feedback on constraint accuracy and completeness\n- Market and environment changes affecting constraint validity\n\n#### Model Enhancement\n- Constraint model accuracy improvement over time\n- New constraint identification and integration\n- Constraint relationship refinement and optimization\n- Predictive capability enhancement for constraint evolution\n\n## Usage Examples\n\n```bash\n# Business strategy constraints\n/simulation:constraint-modeler Model market entry constraints for European expansion including regulatory, competitive, and resource limitations\n\n# Technical architecture constraints  \n/simulation:constraint-modeler Define system constraints for microservices migration including performance, security, and team capability limits\n\n# Product development constraints\n/simulation:constraint-modeler Map product development constraints including budget, timeline, technical, and market requirements\n\n# Operational optimization constraints\n/simulation:constraint-modeler Model operational constraints for scaling customer support including team, process, and technology limitations\n```\n\n## Quality Indicators\n\n- **Green**: Comprehensive constraint coverage, validated assumptions, dynamic modeling\n- **Yellow**: Good constraint identification, some validation, basic change modeling\n- **Red**: Limited constraint coverage, unvalidated assumptions, static modeling\n\n## Common Pitfalls to Avoid\n\n- Constraint blindness: Not identifying hidden or implicit constraints\n- Static thinking: Treating dynamic constraints as fixed limitations\n- Over-constraint: Adding unnecessary restrictions that limit options\n- Under-validation: Not testing constraint assumptions against reality\n- Isolation thinking: Not modeling constraint interactions and dependencies\n- Solution bias: Defining constraints to justify preferred solutions\n\nTransform limitations into strategic clarity through systematic constraint modeling and optimization."
              },
              {
                "name": "/containerize-application",
                "description": "Containerize application for deployment",
                "path": "plugins/all-commands/commands/containerize-application.md",
                "frontmatter": {
                  "description": "Containerize application for deployment",
                  "category": "ci-deployment"
                },
                "content": "# Containerize Application\n\nContainerize application for deployment\n\n## Instructions\n\n1. **Application Analysis and Containerization Strategy**\n   - Analyze application architecture and runtime requirements\n   - Identify application dependencies and external services\n   - Determine optimal base image and runtime environment\n   - Plan multi-stage build strategy for optimization\n   - Assess security requirements and compliance needs\n\n2. **Dockerfile Creation and Optimization**\n   - Create comprehensive Dockerfile with multi-stage builds\n   - Select minimal base images (Alpine, distroless, or slim variants)\n   - Configure proper layer caching and build optimization\n   - Implement security best practices (non-root user, minimal attack surface)\n   - Set up proper file permissions and ownership\n\n3. **Build Process Configuration**\n   - Configure .dockerignore file to exclude unnecessary files\n   - Set up build arguments and environment variables\n   - Implement build-time dependency installation and cleanup\n   - Configure application bundling and asset optimization\n   - Set up proper build context and file structure\n\n4. **Runtime Configuration**\n   - Configure application startup and health checks\n   - Set up proper signal handling and graceful shutdown\n   - Configure logging and output redirection\n   - Set up environment-specific configuration management\n   - Configure resource limits and performance tuning\n\n5. **Security Hardening**\n   - Run application as non-root user with minimal privileges\n   - Configure security scanning and vulnerability assessment\n   - Implement secrets management and secure credential handling\n   - Set up network security and firewall rules\n   - Configure security policies and access controls\n\n6. **Docker Compose Configuration**\n   - Create docker-compose.yml for local development\n   - Configure service dependencies and networking\n   - Set up volume mounting and data persistence\n   - Configure environment variables and secrets\n   - Set up development vs production configurations\n\n7. **Container Orchestration Preparation**\n   - Prepare configurations for Kubernetes deployment\n   - Create deployment manifests and service definitions\n   - Configure ingress and load balancing\n   - Set up persistent volumes and storage classes\n   - Configure auto-scaling and resource management\n\n8. **Monitoring and Observability**\n   - Configure application metrics and health endpoints\n   - Set up logging aggregation and centralized logging\n   - Configure distributed tracing and monitoring\n   - Set up alerting and notification systems\n   - Configure performance monitoring and profiling\n\n9. **CI/CD Integration**\n   - Configure automated Docker image building\n   - Set up image scanning and security validation\n   - Configure image registry and artifact management\n   - Set up automated deployment pipelines\n   - Configure rollback and blue-green deployment strategies\n\n10. **Testing and Validation**\n    - Test container builds and functionality\n    - Validate security configurations and compliance\n    - Test deployment in different environments\n    - Validate performance and resource utilization\n    - Test backup and disaster recovery procedures\n    - Create documentation for container deployment and management"
              },
              {
                "name": "/context-prime",
                "description": "Load project context by reading README.md and exploring relevant project files",
                "path": "plugins/all-commands/commands/context-prime.md",
                "frontmatter": {
                  "description": "Load project context by reading README.md and exploring relevant project files",
                  "category": "context-loading-priming",
                  "allowed-tools": "Read, Bash(git *)"
                },
                "content": "Read README.md, THEN run `git ls-files | grep -v -f (sed 's|^|^|; s|$|/|' .cursorignore | psub)` to understand the context of the project"
              },
              {
                "name": "/create-architecture-documentation",
                "description": "Generate comprehensive architecture documentation",
                "path": "plugins/all-commands/commands/create-architecture-documentation.md",
                "frontmatter": {
                  "description": "Generate comprehensive architecture documentation",
                  "category": "documentation-changelogs"
                },
                "content": "# Create Architecture Documentation\n\nGenerate comprehensive architecture documentation\n\n## Instructions\n\n1. **Architecture Analysis and Discovery**\n   - Analyze current system architecture and component relationships\n   - Identify key architectural patterns and design decisions\n   - Document system boundaries, interfaces, and dependencies\n   - Assess data flow and communication patterns\n   - Identify architectural debt and improvement opportunities\n\n2. **Architecture Documentation Framework**\n   - Choose appropriate documentation framework and tools:\n     - **C4 Model**: Context, Containers, Components, Code diagrams\n     - **Arc42**: Comprehensive architecture documentation template\n     - **Architecture Decision Records (ADRs)**: Decision documentation\n     - **PlantUML/Mermaid**: Diagram-as-code documentation\n     - **Structurizr**: C4 model tooling and visualization\n     - **Draw.io/Lucidchart**: Visual diagramming tools\n\n3. **System Context Documentation**\n   - Create high-level system context diagrams\n   - Document external systems and integrations\n   - Define system boundaries and responsibilities\n   - Document user personas and stakeholders\n   - Create system landscape and ecosystem overview\n\n4. **Container and Service Architecture**\n   - Document container/service architecture and deployment view\n   - Create service dependency maps and communication patterns\n   - Document deployment architecture and infrastructure\n   - Define service boundaries and API contracts\n   - Document data persistence and storage architecture\n\n5. **Component and Module Documentation**\n   - Create detailed component architecture diagrams\n   - Document internal module structure and relationships\n   - Define component responsibilities and interfaces\n   - Document design patterns and architectural styles\n   - Create code organization and package structure documentation\n\n6. **Data Architecture Documentation**\n   - Document data models and database schemas\n   - Create data flow diagrams and processing pipelines\n   - Document data storage strategies and technologies\n   - Define data governance and lifecycle management\n   - Create data integration and synchronization documentation\n\n7. **Security and Compliance Architecture**\n   - Document security architecture and threat model\n   - Create authentication and authorization flow diagrams\n   - Document compliance requirements and controls\n   - Define security boundaries and trust zones\n   - Create incident response and security monitoring documentation\n\n8. **Quality Attributes and Cross-Cutting Concerns**\n   - Document performance characteristics and scalability patterns\n   - Create reliability and availability architecture documentation\n   - Document monitoring and observability architecture\n   - Define maintainability and evolution strategies\n   - Create disaster recovery and business continuity documentation\n\n9. **Architecture Decision Records (ADRs)**\n   - Create comprehensive ADR template and process\n   - Document historical architectural decisions and rationale\n   - Create decision tracking and review process\n   - Document trade-offs and alternatives considered\n   - Set up ADR maintenance and evolution procedures\n\n10. **Documentation Automation and Maintenance**\n    - Set up automated diagram generation from code annotations\n    - Configure documentation pipeline and publishing automation\n    - Set up documentation validation and consistency checking\n    - Create documentation review and approval process\n    - Train team on architecture documentation practices and tools\n    - Set up documentation versioning and change management"
              },
              {
                "name": "/create-command",
                "description": "Create a new command following existing patterns and organizational structure",
                "path": "plugins/all-commands/commands/create-command.md",
                "frontmatter": {
                  "description": "Create a new command following existing patterns and organizational structure",
                  "category": "project-task-management",
                  "allowed-tools": "Read, Write, Edit, LS, Glob"
                },
                "content": "Create a new command that follows the existing patterns and organizational structure in this project.\n\n## ANALYZE EXISTING COMMANDS\n\n1. First, study the existing commands in the `.claude/commands/` directory to understand:\n   - Common patterns and structures\n   - Naming conventions\n   - Documentation styles\n   - Command organization\n\n2. Use MCP tools to explore the codebase and understand:\n   - Project structure\n   - Existing functionality\n   - Code patterns\n   - Dependencies\n\n## UNDERSTAND THE REQUEST\n\n3. Analyze the user's request to determine:\n   - The command's purpose and functionality\n   - Which category it belongs to\n   - Similar existing commands to reference\n   - Required inputs and outputs\n\n## SELECT APPROPRIATE PATTERNS\n\n4. Based on your analysis, choose the most appropriate pattern:\n   - Simple execution commands\n   - File generation commands\n   - Analysis and reporting commands\n   - Multi-step workflow commands\n\n## DETERMINE COMMAND LOCATION\n\n5. Place the command in the appropriate category directory:\n   - `code-analysis-testing/` - For code analysis, testing, and quality assurance\n   - `ci-deployment/` - For CI/CD and deployment related commands\n   - `context-loading-priming/` - For loading context and priming commands\n   - `documentation-changelogs/` - For documentation and changelog commands\n   - `project-task-management/` - For project and task management commands\n   - `version-control-git/` - For version control and Git operations\n   - `miscellaneous/` - For commands that don't fit other categories\n\n## PLAN SUPPORTING RESOURCES\n\n6. Consider what supporting resources might be needed:\n   - Templates or example files\n   - Configuration files\n   - Documentation updates\n   - Related commands that might work together\n\n## CREATE THE COMMAND\n\n7. Write the command following these guidelines:\n   - Use clear, descriptive names\n   - Include comprehensive instructions\n   - Follow existing formatting patterns\n   - Add appropriate examples\n   - Include error handling considerations\n\n## HUMAN REVIEW\n\n8. Present your analysis and proposed command to the human for review before implementation, including:\n   - Command purpose and location\n   - Key patterns you're following\n   - Any assumptions you're making\n   - Questions about specific requirements"
              },
              {
                "name": "/create-database-migrations",
                "description": "Create and manage database migrations",
                "path": "plugins/all-commands/commands/create-database-migrations.md",
                "frontmatter": {
                  "description": "Create and manage database migrations",
                  "category": "database-operations",
                  "allowed-tools": "Bash(npm *), Edit"
                },
                "content": "# Create Database Migrations\n\nCreate and manage database migrations\n\n## Instructions\n\n1. **Migration Strategy and Planning**\n   - Analyze current database schema and target changes\n   - Plan migration strategy for zero-downtime deployments\n   - Define rollback procedures and data safety measures\n   - Assess migration complexity and potential risks\n   - Plan for data transformation and validation\n\n2. **Migration Framework Setup**\n   - Set up comprehensive migration framework:\n\n   **Node.js Migration Framework:**\n   ```javascript\n   // migrations/migration-framework.js\n   const fs = require('fs').promises;\n   const path = require('path');\n   const { Pool } = require('pg');\n\n   class MigrationManager {\n     constructor(databaseConfig) {\n       this.pool = new Pool(databaseConfig);\n       this.migrationsDir = path.join(__dirname, 'migrations');\n       this.lockTimeout = 30000; // 30 seconds\n     }\n\n     async initialize() {\n       // Create migrations tracking table\n       await this.pool.query(`\n         CREATE TABLE IF NOT EXISTS schema_migrations (\n           id SERIAL PRIMARY KEY,\n           version VARCHAR(255) UNIQUE NOT NULL,\n           name VARCHAR(255) NOT NULL,\n           executed_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n           execution_time_ms INTEGER,\n           checksum VARCHAR(64),\n           rollback_sql TEXT,\n           batch_number INTEGER\n         );\n         \n         CREATE INDEX IF NOT EXISTS idx_schema_migrations_version \n         ON schema_migrations(version);\n         \n         CREATE INDEX IF NOT EXISTS idx_schema_migrations_batch \n         ON schema_migrations(batch_number);\n       `);\n\n       // Create migration lock table\n       await this.pool.query(`\n         CREATE TABLE IF NOT EXISTS migration_lock (\n           id INTEGER PRIMARY KEY DEFAULT 1,\n           is_locked BOOLEAN DEFAULT FALSE,\n           locked_at TIMESTAMP WITH TIME ZONE,\n           locked_by VARCHAR(255),\n           CHECK (id = 1)\n         );\n         \n         INSERT INTO migration_lock (id, is_locked) \n         VALUES (1, FALSE) \n         ON CONFLICT (id) DO NOTHING;\n       `);\n     }\n\n     async acquireLock(lockId = 'migration') {\n       const client = await this.pool.connect();\n       try {\n         const result = await client.query(`\n           UPDATE migration_lock \n           SET is_locked = TRUE, locked_at = CURRENT_TIMESTAMP, locked_by = $1\n           WHERE id = 1 AND (is_locked = FALSE OR locked_at < CURRENT_TIMESTAMP - INTERVAL '${this.lockTimeout} milliseconds')\n           RETURNING is_locked;\n         `, [lockId]);\n\n         if (result.rows.length === 0) {\n           throw new Error('Could not acquire migration lock - another migration may be running');\n         }\n\n         return client;\n       } catch (error) {\n         client.release();\n         throw error;\n       }\n     }\n\n     async releaseLock(client) {\n       try {\n         await client.query(`\n           UPDATE migration_lock \n           SET is_locked = FALSE, locked_at = NULL, locked_by = NULL \n           WHERE id = 1;\n         `);\n       } finally {\n         client.release();\n       }\n     }\n\n     async getPendingMigrations() {\n       const files = await fs.readdir(this.migrationsDir);\n       const migrationFiles = files\n         .filter(file => file.endsWith('.sql') || file.endsWith('.js'))\n         .sort();\n\n       const executedMigrations = await this.pool.query(\n         'SELECT version FROM schema_migrations ORDER BY version'\n       );\n       const executedVersions = new Set(executedMigrations.rows.map(row => row.version));\n\n       return migrationFiles\n         .map(file => {\n           const version = this.extractVersion(file);\n           return { file, version, executed: executedVersions.has(version) };\n         })\n         .filter(migration => !migration.executed);\n     }\n\n     extractVersion(filename) {\n       const match = filename.match(/^(\\d{14})/);\n       if (!match) {\n         throw new Error(`Invalid migration filename format: ${filename}`);\n       }\n       return match[1];\n     }\n\n     async runMigration(migrationFile) {\n       const version = this.extractVersion(migrationFile.file);\n       const filePath = path.join(this.migrationsDir, migrationFile.file);\n       const startTime = Date.now();\n\n       console.log(`Running migration: ${migrationFile.file}`);\n\n       const client = await this.pool.connect();\n       try {\n         await client.query('BEGIN');\n\n         let migrationContent;\n         let rollbackSql = '';\n\n         if (migrationFile.file.endsWith('.js')) {\n           // JavaScript migration\n           const migration = require(filePath);\n           await migration.up(client);\n           rollbackSql = migration.down ? migration.down.toString() : '';\n         } else {\n           // SQL migration\n           migrationContent = await fs.readFile(filePath, 'utf8');\n           const { upSql, downSql } = this.parseSqlMigration(migrationContent);\n           \n           await client.query(upSql);\n           rollbackSql = downSql;\n         }\n\n         const executionTime = Date.now() - startTime;\n         const checksum = this.generateChecksum(migrationContent || migrationFile.file);\n         const batchNumber = await this.getNextBatchNumber();\n\n         // Record migration execution\n         await client.query(`\n           INSERT INTO schema_migrations (version, name, execution_time_ms, checksum, rollback_sql, batch_number)\n           VALUES ($1, $2, $3, $4, $5, $6)\n         `, [version, migrationFile.file, executionTime, checksum, rollbackSql, batchNumber]);\n\n         await client.query('COMMIT');\n         console.log(` Migration ${migrationFile.file} completed in ${executionTime}ms`);\n\n       } catch (error) {\n         await client.query('ROLLBACK');\n         console.error(` Migration ${migrationFile.file} failed:`, error.message);\n         throw error;\n       } finally {\n         client.release();\n       }\n     }\n\n     parseSqlMigration(content) {\n       const lines = content.split('\\n');\n       let upSql = '';\n       let downSql = '';\n       let currentSection = 'up';\n\n       for (const line of lines) {\n         if (line.trim().startsWith('-- +migrate Down')) {\n           currentSection = 'down';\n           continue;\n         }\n         if (line.trim().startsWith('-- +migrate Up')) {\n           currentSection = 'up';\n           continue;\n         }\n\n         if (currentSection === 'up') {\n           upSql += line + '\\n';\n         } else if (currentSection === 'down') {\n           downSql += line + '\\n';\n         }\n       }\n\n       return { upSql: upSql.trim(), downSql: downSql.trim() };\n     }\n\n     generateChecksum(content) {\n       const crypto = require('crypto');\n       return crypto.createHash('sha256').update(content).digest('hex');\n     }\n\n     async getNextBatchNumber() {\n       const result = await this.pool.query(\n         'SELECT COALESCE(MAX(batch_number), 0) + 1 as next_batch FROM schema_migrations'\n       );\n       return result.rows[0].next_batch;\n     }\n\n     async migrate() {\n       await this.initialize();\n       \n       const client = await this.acquireLock('migration-runner');\n       try {\n         const pendingMigrations = await this.getPendingMigrations();\n         \n         if (pendingMigrations.length === 0) {\n           console.log('No pending migrations');\n           return;\n         }\n\n         console.log(`Found ${pendingMigrations.length} pending migrations`);\n         \n         for (const migration of pendingMigrations) {\n           await this.runMigration(migration);\n         }\n\n         console.log('All migrations completed successfully');\n       } finally {\n         await this.releaseLock(client);\n       }\n     }\n\n     async rollback(steps = 1) {\n       await this.initialize();\n       \n       const client = await this.acquireLock('migration-rollback');\n       try {\n         const lastMigrations = await this.pool.query(`\n           SELECT * FROM schema_migrations \n           ORDER BY executed_at DESC, version DESC \n           LIMIT $1\n         `, [steps]);\n\n         if (lastMigrations.rows.length === 0) {\n           console.log('No migrations to rollback');\n           return;\n         }\n\n         for (const migration of lastMigrations.rows) {\n           await this.rollbackMigration(migration);\n         }\n\n         console.log(`Rolled back ${lastMigrations.rows.length} migrations`);\n       } finally {\n         await this.releaseLock(client);\n       }\n     }\n\n     async rollbackMigration(migration) {\n       console.log(`Rolling back migration: ${migration.name}`);\n       \n       const client = await this.pool.connect();\n       try {\n         await client.query('BEGIN');\n\n         if (migration.rollback_sql) {\n           await client.query(migration.rollback_sql);\n         } else {\n           console.warn(`No rollback SQL available for ${migration.name}`);\n         }\n\n         await client.query(\n           'DELETE FROM schema_migrations WHERE version = $1',\n           [migration.version]\n         );\n\n         await client.query('COMMIT');\n         console.log(` Rolled back migration: ${migration.name}`);\n\n       } catch (error) {\n         await client.query('ROLLBACK');\n         console.error(` Rollback failed for ${migration.name}:`, error.message);\n         throw error;\n       } finally {\n         client.release();\n       }\n     }\n   }\n\n   module.exports = MigrationManager;\n   ```\n\n3. **Migration File Templates**\n   - Create standardized migration templates:\n\n   **SQL Migration Template:**\n   ```sql\n   -- +migrate Up\n   -- Migration: Add user preferences table\n   -- Author: Developer Name\n   -- Date: 2024-01-15\n   -- Description: Create user_preferences table to store user-specific settings\n\n   CREATE TABLE user_preferences (\n     id BIGSERIAL PRIMARY KEY,\n     user_id BIGINT NOT NULL REFERENCES users(id) ON DELETE CASCADE,\n     category VARCHAR(100) NOT NULL,\n     key VARCHAR(100) NOT NULL,\n     value JSONB NOT NULL DEFAULT '{}',\n     created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n     updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n     \n     UNIQUE(user_id, category, key)\n   );\n\n   -- Add indexes for efficient querying\n   CREATE INDEX idx_user_preferences_user_id ON user_preferences(user_id);\n   CREATE INDEX idx_user_preferences_category ON user_preferences(category);\n   CREATE INDEX idx_user_preferences_key ON user_preferences(key);\n\n   -- Add comments for documentation\n   COMMENT ON TABLE user_preferences IS 'User-specific preference settings organized by category';\n   COMMENT ON COLUMN user_preferences.category IS 'Preference category (e.g., notifications, display, privacy)';\n   COMMENT ON COLUMN user_preferences.key IS 'Specific preference key within the category';\n   COMMENT ON COLUMN user_preferences.value IS 'Preference value stored as JSONB for flexibility';\n\n   -- +migrate Down\n   -- Rollback: Remove user preferences table\n\n   DROP TABLE IF EXISTS user_preferences CASCADE;\n   ```\n\n   **JavaScript Migration Template:**\n   ```javascript\n   // migrations/20240115120000_add_user_preferences.js\n   const migration = {\n     name: 'Add user preferences table',\n     description: 'Create user_preferences table for storing user-specific settings',\n     \n     async up(client) {\n       console.log('Creating user_preferences table...');\n       \n       await client.query(`\n         CREATE TABLE user_preferences (\n           id BIGSERIAL PRIMARY KEY,\n           user_id BIGINT NOT NULL REFERENCES users(id) ON DELETE CASCADE,\n           category VARCHAR(100) NOT NULL,\n           key VARCHAR(100) NOT NULL,\n           value JSONB NOT NULL DEFAULT '{}',\n           created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n           updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n           \n           UNIQUE(user_id, category, key)\n         );\n       `);\n\n       await client.query(`\n         CREATE INDEX idx_user_preferences_user_id ON user_preferences(user_id);\n       `);\n\n       await client.query(`\n         CREATE INDEX idx_user_preferences_category ON user_preferences(category);\n       `);\n\n       console.log(' user_preferences table created successfully');\n     },\n\n     async down(client) {\n       console.log('Dropping user_preferences table...');\n       \n       await client.query('DROP TABLE IF EXISTS user_preferences CASCADE;');\n       \n       console.log(' user_preferences table dropped successfully');\n     }\n   };\n\n   module.exports = migration;\n   ```\n\n4. **Advanced Migration Patterns**\n   - Implement complex migration scenarios:\n\n   **Data Migration with Validation:**\n   ```javascript\n   // migrations/20240115130000_migrate_user_settings.js\n   const migration = {\n     name: 'Migrate user settings to new format',\n     description: 'Transform legacy user_settings JSONB column to normalized user_preferences table',\n     \n     async up(client) {\n       console.log('Starting user settings migration...');\n       \n       // Step 1: Create temporary backup\n       await client.query(`\n         CREATE TABLE user_settings_backup AS \n         SELECT * FROM users WHERE settings IS NOT NULL;\n       `);\n       \n       console.log(' Created backup of existing user settings');\n\n       // Step 2: Migrate data in batches\n       const batchSize = 1000;\n       let offset = 0;\n       let processedCount = 0;\n\n       while (true) {\n         const result = await client.query(`\n           SELECT id, settings \n           FROM users \n           WHERE settings IS NOT NULL \n           ORDER BY id \n           LIMIT $1 OFFSET $2\n         `, [batchSize, offset]);\n\n         if (result.rows.length === 0) break;\n\n         for (const user of result.rows) {\n           await this.migrateUserSettings(client, user.id, user.settings);\n           processedCount++;\n         }\n\n         offset += batchSize;\n         console.log(` Processed ${processedCount} users...`);\n       }\n\n       // Step 3: Validate migration\n       const validationResult = await this.validateMigration(client);\n       if (!validationResult.isValid) {\n         throw new Error(`Migration validation failed: ${validationResult.errors.join(', ')}`);\n       }\n\n       console.log(` Successfully migrated ${processedCount} user settings`);\n     },\n\n     async migrateUserSettings(client, userId, settings) {\n       const settingsObj = typeof settings === 'string' ? JSON.parse(settings) : settings;\n       \n       for (const [category, categorySettings] of Object.entries(settingsObj)) {\n         if (typeof categorySettings === 'object') {\n           for (const [key, value] of Object.entries(categorySettings)) {\n             await client.query(`\n               INSERT INTO user_preferences (user_id, category, key, value)\n               VALUES ($1, $2, $3, $4)\n               ON CONFLICT (user_id, category, key) DO UPDATE\n               SET value = $4, updated_at = CURRENT_TIMESTAMP\n             `, [userId, category, key, JSON.stringify(value)]);\n           }\n         } else {\n           // Handle flat settings structure\n           await client.query(`\n             INSERT INTO user_preferences (user_id, category, key, value)\n             VALUES ($1, $2, $3, $4)\n             ON CONFLICT (user_id, category, key) DO UPDATE\n             SET value = $4, updated_at = CURRENT_TIMESTAMP\n           `, [userId, 'general', category, JSON.stringify(categorySettings)]);\n         }\n       }\n     },\n\n     async validateMigration(client) {\n       const errors = [];\n       \n       // Check for data consistency\n       const oldCount = await client.query(\n         'SELECT COUNT(*) FROM users WHERE settings IS NOT NULL'\n       );\n       \n       const newCount = await client.query(\n         'SELECT COUNT(DISTINCT user_id) FROM user_preferences'\n       );\n\n       if (oldCount.rows[0].count !== newCount.rows[0].count) {\n         errors.push(`User count mismatch: ${oldCount.rows[0].count} vs ${newCount.rows[0].count}`);\n       }\n\n       // Check for required preferences\n       const missingPrefs = await client.query(`\n         SELECT u.id FROM users u\n         LEFT JOIN user_preferences up ON u.id = up.user_id\n         WHERE u.settings IS NOT NULL AND up.user_id IS NULL\n       `);\n\n       if (missingPrefs.rows.length > 0) {\n         errors.push(`${missingPrefs.rows.length} users missing preferences`);\n       }\n\n       return {\n         isValid: errors.length === 0,\n         errors\n       };\n     },\n\n     async down(client) {\n       console.log('Rolling back user settings migration...');\n       \n       // Restore from backup\n       await client.query(`\n         UPDATE users \n         SET settings = backup.settings\n         FROM user_settings_backup backup\n         WHERE users.id = backup.id;\n       `);\n       \n       // Clean up\n       await client.query('DELETE FROM user_preferences;');\n       await client.query('DROP TABLE user_settings_backup;');\n       \n       console.log(' Rollback completed');\n     }\n   };\n\n   module.exports = migration;\n   ```\n\n5. **Schema Alteration Migrations**\n   - Handle schema changes safely:\n\n   **Safe Column Addition:**\n   ```sql\n   -- +migrate Up\n   -- Migration: Add email verification tracking\n   -- Safe column addition with default values\n\n   -- Add new columns with safe defaults\n   ALTER TABLE users \n   ADD COLUMN email_verification_token VARCHAR(255),\n   ADD COLUMN email_verification_expires_at TIMESTAMP WITH TIME ZONE,\n   ADD COLUMN email_verification_attempts INTEGER DEFAULT 0;\n\n   -- Add index for token lookup\n   CREATE INDEX CONCURRENTLY idx_users_email_verification_token \n   ON users(email_verification_token) \n   WHERE email_verification_token IS NOT NULL;\n\n   -- Add constraint for expiration logic\n   ALTER TABLE users \n   ADD CONSTRAINT chk_email_verification_expires \n   CHECK (\n     (email_verification_token IS NULL AND email_verification_expires_at IS NULL) OR\n     (email_verification_token IS NOT NULL AND email_verification_expires_at IS NOT NULL)\n   );\n\n   -- +migrate Down\n   -- Remove email verification columns\n\n   DROP INDEX IF EXISTS idx_users_email_verification_token;\n   ALTER TABLE users \n   DROP CONSTRAINT IF EXISTS chk_email_verification_expires,\n   DROP COLUMN IF EXISTS email_verification_token,\n   DROP COLUMN IF EXISTS email_verification_expires_at,\n   DROP COLUMN IF EXISTS email_verification_attempts;\n   ```\n\n   **Safe Table Restructuring:**\n   ```sql\n   -- +migrate Up\n   -- Migration: Split user addresses into separate table\n   -- Zero-downtime table restructuring\n\n   -- Step 1: Create new addresses table\n   CREATE TABLE user_addresses (\n     id BIGSERIAL PRIMARY KEY,\n     user_id BIGINT NOT NULL REFERENCES users(id) ON DELETE CASCADE,\n     type address_type DEFAULT 'shipping',\n     first_name VARCHAR(100),\n     last_name VARCHAR(100),\n     company VARCHAR(255),\n     address_line_1 VARCHAR(255) NOT NULL,\n     address_line_2 VARCHAR(255),\n     city VARCHAR(100) NOT NULL,\n     state VARCHAR(100),\n     postal_code VARCHAR(20),\n     country CHAR(2) NOT NULL DEFAULT 'US',\n     phone VARCHAR(20),\n     is_default BOOLEAN DEFAULT FALSE,\n     created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n     updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP\n   );\n\n   CREATE TYPE address_type AS ENUM ('billing', 'shipping');\n\n   -- Add indexes\n   CREATE INDEX idx_user_addresses_user_id ON user_addresses(user_id);\n   CREATE INDEX idx_user_addresses_type ON user_addresses(type);\n   CREATE UNIQUE INDEX idx_user_addresses_default \n   ON user_addresses(user_id, type) \n   WHERE is_default = TRUE;\n\n   -- Step 2: Migrate existing address data\n   INSERT INTO user_addresses (\n     user_id, type, first_name, last_name, address_line_1, \n     city, state, postal_code, country, is_default\n   )\n   SELECT \n     id, 'shipping', first_name, last_name, address,\n     city, state, postal_code, \n     COALESCE(country, 'US'), TRUE\n   FROM users \n   WHERE address IS NOT NULL;\n\n   -- Step 3: Create view for backward compatibility\n   CREATE VIEW users_with_address AS\n   SELECT \n     u.*,\n     ua.address_line_1 as address,\n     ua.city,\n     ua.state,\n     ua.postal_code,\n     ua.country\n   FROM users u\n   LEFT JOIN user_addresses ua ON u.id = ua.user_id AND ua.is_default = TRUE AND ua.type = 'shipping';\n\n   -- Step 4: Add trigger to maintain view consistency\n   CREATE OR REPLACE FUNCTION sync_user_address()\n   RETURNS TRIGGER AS $$\n   BEGIN\n     IF TG_OP = 'UPDATE' THEN\n       -- Update default shipping address\n       UPDATE user_addresses \n       SET \n         address_line_1 = NEW.address,\n         city = NEW.city,\n         state = NEW.state,\n         postal_code = NEW.postal_code,\n         country = NEW.country,\n         updated_at = CURRENT_TIMESTAMP\n       WHERE user_id = NEW.id AND type = 'shipping' AND is_default = TRUE;\n       \n       RETURN NEW;\n     END IF;\n     RETURN NULL;\n   END;\n   $$ LANGUAGE plpgsql;\n\n   CREATE TRIGGER trigger_sync_user_address\n   AFTER UPDATE ON users\n   FOR EACH ROW\n   WHEN (OLD.address IS DISTINCT FROM NEW.address OR \n         OLD.city IS DISTINCT FROM NEW.city OR\n         OLD.state IS DISTINCT FROM NEW.state OR\n         OLD.postal_code IS DISTINCT FROM NEW.postal_code OR\n         OLD.country IS DISTINCT FROM NEW.country)\n   EXECUTE FUNCTION sync_user_address();\n\n   -- +migrate Down\n   -- Restore original structure\n\n   DROP TRIGGER IF EXISTS trigger_sync_user_address ON users;\n   DROP FUNCTION IF EXISTS sync_user_address();\n   DROP VIEW IF EXISTS users_with_address;\n   DROP TABLE IF EXISTS user_addresses CASCADE;\n   DROP TYPE IF EXISTS address_type;\n   ```\n\n6. **Migration Testing Framework**\n   - Test migrations thoroughly:\n\n   **Migration Test Suite:**\n   ```javascript\n   // tests/migration-tests.js\n   const { Pool } = require('pg');\n   const MigrationManager = require('../migrations/migration-framework');\n\n   class MigrationTester {\n     constructor() {\n       this.testDbConfig = {\n         host: process.env.TEST_DB_HOST || 'localhost',\n         port: process.env.TEST_DB_PORT || 5432,\n         database: process.env.TEST_DB_NAME || 'test_db',\n         user: process.env.TEST_DB_USER || 'postgres',\n         password: process.env.TEST_DB_PASSWORD || 'password'\n       };\n       \n       this.pool = new Pool(this.testDbConfig);\n       this.migrationManager = new MigrationManager(this.testDbConfig);\n     }\n\n     async setupTestDatabase() {\n       // Create fresh test database\n       const adminPool = new Pool({\n         ...this.testDbConfig,\n         database: 'postgres'\n       });\n\n       try {\n         await adminPool.query(`DROP DATABASE IF EXISTS ${this.testDbConfig.database}`);\n         await adminPool.query(`CREATE DATABASE ${this.testDbConfig.database}`);\n         console.log(' Test database created');\n       } finally {\n         await adminPool.end();\n       }\n     }\n\n     async teardownTestDatabase() {\n       await this.pool.end();\n       \n       const adminPool = new Pool({\n         ...this.testDbConfig,\n         database: 'postgres'\n       });\n\n       try {\n         await adminPool.query(`DROP DATABASE IF EXISTS ${this.testDbConfig.database}`);\n         console.log(' Test database cleaned up');\n       } finally {\n         await adminPool.end();\n       }\n     }\n\n     async testMigrationUpDown(migrationFile) {\n       console.log(`Testing migration: ${migrationFile}`);\n       \n       try {\n         // Test migration up\n         const startTime = Date.now();\n         await this.migrationManager.runMigration({ file: migrationFile });\n         const upTime = Date.now() - startTime;\n         \n         console.log(` Migration up completed in ${upTime}ms`);\n\n         // Verify migration was recorded\n         const migrationRecord = await this.pool.query(\n           'SELECT * FROM schema_migrations WHERE name = $1',\n           [migrationFile]\n         );\n         \n         if (migrationRecord.rows.length === 0) {\n           throw new Error('Migration not recorded in schema_migrations table');\n         }\n\n         // Test migration down\n         const rollbackStartTime = Date.now();\n         await this.migrationManager.rollbackMigration(migrationRecord.rows[0]);\n         const downTime = Date.now() - rollbackStartTime;\n         \n         console.log(` Migration down completed in ${downTime}ms`);\n\n         // Verify migration was removed\n         const afterRollback = await this.pool.query(\n           'SELECT * FROM schema_migrations WHERE name = $1',\n           [migrationFile]\n         );\n         \n         if (afterRollback.rows.length > 0) {\n           throw new Error('Migration not removed after rollback');\n         }\n\n         return {\n           success: true,\n           upTime,\n           downTime,\n           migrationFile\n         };\n\n       } catch (error) {\n         console.error(` Migration test failed: ${error.message}`);\n         return {\n           success: false,\n           error: error.message,\n           migrationFile\n         };\n       }\n     }\n\n     async testDataIntegrity(testData) {\n       console.log('Testing data integrity...');\n       \n       // Insert test data\n       const insertResults = [];\n       for (const table of Object.keys(testData)) {\n         for (const record of testData[table]) {\n           try {\n             const columns = Object.keys(record);\n             const values = Object.values(record);\n             const placeholders = values.map((_, i) => `$${i + 1}`).join(', ');\n             \n             const result = await this.pool.query(\n               `INSERT INTO ${table} (${columns.join(', ')}) VALUES (${placeholders}) RETURNING id`,\n               values\n             );\n             \n             insertResults.push({\n               table,\n               id: result.rows[0].id,\n               success: true\n             });\n           } catch (error) {\n             insertResults.push({\n               table,\n               success: false,\n               error: error.message\n             });\n           }\n         }\n       }\n\n       return insertResults;\n     }\n\n     async testPerformance(queries) {\n       console.log('Testing query performance...');\n       \n       const performanceResults = [];\n       \n       for (const query of queries) {\n         const startTime = process.hrtime.bigint();\n         \n         try {\n           const result = await this.pool.query(query.sql, query.params || []);\n           const endTime = process.hrtime.bigint();\n           const duration = Number(endTime - startTime) / 1000000; // Convert to milliseconds\n           \n           performanceResults.push({\n             name: query.name,\n             duration,\n             rowCount: result.rows.length,\n             success: true\n           });\n           \n           if (duration > (query.maxDuration || 1000)) {\n             console.warn(` Query ${query.name} took ${duration}ms (expected < ${query.maxDuration || 1000}ms)`);\n           }\n           \n         } catch (error) {\n           performanceResults.push({\n             name: query.name,\n             success: false,\n             error: error.message\n           });\n         }\n       }\n\n       return performanceResults;\n     }\n\n     async runFullTestSuite() {\n       console.log('Starting migration test suite...');\n       \n       await this.setupTestDatabase();\n       await this.migrationManager.initialize();\n       \n       try {\n         const testResults = {\n           migrations: [],\n           dataIntegrity: [],\n           performance: [],\n           summary: { passed: 0, failed: 0 }\n         };\n\n         // Test all migration files\n         const migrationFiles = await this.migrationManager.getPendingMigrations();\n         \n         for (const migration of migrationFiles) {\n           const result = await this.testMigrationUpDown(migration.file);\n           testResults.migrations.push(result);\n           \n           if (result.success) {\n             testResults.summary.passed++;\n           } else {\n             testResults.summary.failed++;\n           }\n         }\n\n         console.log('\\n Test Results Summary:');\n         console.log(` Passed: ${testResults.summary.passed}`);\n         console.log(` Failed: ${testResults.summary.failed}`);\n         console.log(` Success Rate: ${(testResults.summary.passed / (testResults.summary.passed + testResults.summary.failed) * 100).toFixed(1)}%`);\n\n         return testResults;\n\n       } finally {\n         await this.teardownTestDatabase();\n       }\n     }\n   }\n\n   module.exports = MigrationTester;\n\n   // CLI usage\n   if (require.main === module) {\n     const tester = new MigrationTester();\n     tester.runFullTestSuite()\n       .then(results => {\n         console.log('\\nTest suite completed');\n         process.exit(results.summary.failed > 0 ? 1 : 0);\n       })\n       .catch(error => {\n         console.error('Test suite failed:', error);\n         process.exit(1);\n       });\n   }\n   ```\n\n7. **Production Migration Safety**\n   - Implement production-safe migration practices:\n\n   **Safe Production Migration:**\n   ```javascript\n   // migrations/production-safety.js\n   class ProductionMigrationSafety {\n     static async validateProductionMigration(migrationFile, pool) {\n       const safety = new ProductionMigrationSafety(pool);\n       \n       const checks = [\n         safety.checkTableLocks.bind(safety),\n         safety.checkDataSize.bind(safety),\n         safety.checkDependencies.bind(safety),\n         safety.checkBackupStatus.bind(safety),\n         safety.checkMaintenanceWindow.bind(safety)\n       ];\n\n       const results = [];\n       for (const check of checks) {\n         const result = await check(migrationFile);\n         results.push(result);\n         \n         if (!result.passed && result.blocking) {\n           throw new Error(`Migration blocked: ${result.message}`);\n         }\n       }\n\n       return results;\n     }\n\n     constructor(pool) {\n       this.pool = pool;\n     }\n\n     async checkTableLocks(migrationFile) {\n       // Check for long-running transactions that might block migration\n       const longTransactions = await this.pool.query(`\n         SELECT \n           pid,\n           now() - pg_stat_activity.query_start AS duration,\n           query,\n           state\n         FROM pg_stat_activity \n         WHERE (now() - pg_stat_activity.query_start) > interval '5 minutes'\n         AND state IN ('active', 'idle in transaction');\n       `);\n\n       return {\n         name: 'table_locks',\n         passed: longTransactions.rows.length === 0,\n         blocking: true,\n         message: longTransactions.rows.length > 0 \n           ? `${longTransactions.rows.length} long-running transactions detected`\n           : 'No blocking transactions found',\n         details: longTransactions.rows\n       };\n     }\n\n     async checkDataSize(migrationFile) {\n       // Estimate migration impact based on data size\n       const tableSizes = await this.pool.query(`\n         SELECT \n           schemaname,\n           tablename,\n           pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size,\n           pg_total_relation_size(schemaname||'.'||tablename) as size_bytes\n         FROM pg_tables \n         WHERE schemaname = 'public'\n         ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;\n       `);\n\n       const largeTables = tableSizes.rows.filter(table => table.size_bytes > 1000000000); // > 1GB\n\n       return {\n         name: 'data_size',\n         passed: largeTables.length < 5,\n         blocking: false,\n         message: `${largeTables.length} tables > 1GB found`,\n         details: largeTables\n       };\n     }\n\n     async checkDependencies(migrationFile) {\n       // Check for dependent applications or services\n       const activeConnections = await this.pool.query(`\n         SELECT \n           application_name,\n           COUNT(*) as connection_count,\n           COUNT(*) FILTER (WHERE state = 'active') as active_count\n         FROM pg_stat_activity \n         WHERE datname = current_database()\n         AND application_name IS NOT NULL\n         GROUP BY application_name\n         ORDER BY connection_count DESC;\n       `);\n\n       const highUsage = activeConnections.rows.filter(app => app.active_count > 10);\n\n       return {\n         name: 'dependencies',\n         passed: highUsage.length === 0,\n         blocking: false,\n         message: highUsage.length > 0 \n           ? `${highUsage.length} applications with high database usage`\n           : 'Database usage within acceptable limits',\n         details: activeConnections.rows\n       };\n     }\n\n     async checkBackupStatus(migrationFile) {\n       // Verify recent backup exists\n       const lastBackup = await this.pool.query(`\n         SELECT \n           pg_last_wal_receive_lsn(),\n           pg_last_wal_replay_lsn(),\n           EXTRACT(EPOCH FROM (now() - pg_stat_file('base/backup_label', true).modification))::int as backup_age_seconds\n         WHERE pg_stat_file('base/backup_label', true) IS NOT NULL;\n       `);\n\n       const backupExists = lastBackup.rows.length > 0;\n       const backupAge = backupExists ? lastBackup.rows[0].backup_age_seconds : null;\n       const isRecentBackup = backupAge !== null && backupAge < 86400; // 24 hours\n\n       return {\n         name: 'backup_status',\n         passed: isRecentBackup,\n         blocking: true,\n         message: isRecentBackup \n           ? `Recent backup available (${Math.round(backupAge / 3600)} hours old)`\n           : 'No recent backup found - backup required before migration',\n         details: { backupExists, backupAge }\n       };\n     }\n\n     async checkMaintenanceWindow(migrationFile) {\n       // Check if we're in approved maintenance window\n       const now = new Date();\n       const hour = now.getUTCHours();\n       const dayOfWeek = now.getUTCDay();\n       \n       // Define maintenance windows (UTC)\n       const maintenanceWindows = [\n         { days: [0, 6], startHour: 2, endHour: 6 }, // Weekend early morning\n         { days: [1, 2, 3, 4, 5], startHour: 3, endHour: 5 } // Weekday early morning\n       ];\n\n       const inMaintenanceWindow = maintenanceWindows.some(window => \n         window.days.includes(dayOfWeek) && \n         hour >= window.startHour && \n         hour < window.endHour\n       );\n\n       return {\n         name: 'maintenance_window',\n         passed: inMaintenanceWindow,\n         blocking: false,\n         message: inMaintenanceWindow \n           ? 'Currently in maintenance window'\n           : `Outside maintenance window (current UTC hour: ${hour})`,\n         details: { currentHour: hour, dayOfWeek, maintenanceWindows }\n       };\n     }\n   }\n\n   module.exports = ProductionMigrationSafety;\n   ```\n\n8. **Migration Monitoring and Alerting**\n   - Monitor migration execution:\n\n   **Migration Monitoring:**\n   ```javascript\n   // migrations/migration-monitor.js\n   class MigrationMonitor {\n     constructor(alertService) {\n       this.alertService = alertService;\n       this.metrics = {\n         executionTimes: [],\n         errorCounts: {},\n         successCounts: {}\n       };\n     }\n\n     async monitorMigration(migrationName, migrationFn) {\n       const startTime = Date.now();\n       const memoryBefore = process.memoryUsage();\n       \n       try {\n         console.log(` Starting migration: ${migrationName}`);\n         \n         const result = await migrationFn();\n         \n         const endTime = Date.now();\n         const duration = endTime - startTime;\n         const memoryAfter = process.memoryUsage();\n         \n         // Record success metrics\n         this.recordSuccess(migrationName, duration, memoryAfter.heapUsed - memoryBefore.heapUsed);\n         \n         // Alert on long-running migrations\n         if (duration > 300000) { // 5 minutes\n           await this.alertService.sendAlert({\n             type: 'warning',\n             title: 'Long-running migration',\n             message: `Migration ${migrationName} took ${duration}ms to complete`,\n             severity: duration > 600000 ? 'high' : 'medium'\n           });\n         }\n\n         console.log(` Migration completed: ${migrationName} (${duration}ms)`);\n         return result;\n\n       } catch (error) {\n         const duration = Date.now() - startTime;\n         \n         // Record error metrics\n         this.recordError(migrationName, error, duration);\n         \n         // Send error alert\n         await this.alertService.sendAlert({\n           type: 'error',\n           title: 'Migration failed',\n           message: `Migration ${migrationName} failed: ${error.message}`,\n           severity: 'critical',\n           details: {\n             migrationName,\n             duration,\n             error: error.message,\n             stack: error.stack\n           }\n         });\n\n         console.error(` Migration failed: ${migrationName}`, error);\n         throw error;\n       }\n     }\n\n     recordSuccess(migrationName, duration, memoryDelta) {\n       this.metrics.executionTimes.push({\n         migration: migrationName,\n         duration,\n         memoryDelta,\n         timestamp: new Date()\n       });\n       \n       this.metrics.successCounts[migrationName] = \n         (this.metrics.successCounts[migrationName] || 0) + 1;\n     }\n\n     recordError(migrationName, error, duration) {\n       this.metrics.errorCounts[migrationName] = \n         (this.metrics.errorCounts[migrationName] || 0) + 1;\n\n       // Log detailed error information\n       console.error('Migration Error Details:', {\n         migration: migrationName,\n         duration,\n         error: error.message,\n         stack: error.stack,\n         timestamp: new Date()\n       });\n     }\n\n     getMetrics() {\n       return {\n         averageExecutionTime: this.calculateAverageExecutionTime(),\n         totalMigrations: this.metrics.executionTimes.length,\n         successRate: this.calculateSuccessRate(),\n         errorCounts: this.metrics.errorCounts,\n         recentMigrations: this.metrics.executionTimes.slice(-10)\n       };\n     }\n\n     calculateAverageExecutionTime() {\n       if (this.metrics.executionTimes.length === 0) return 0;\n       \n       const total = this.metrics.executionTimes.reduce((sum, record) => sum + record.duration, 0);\n       return Math.round(total / this.metrics.executionTimes.length);\n     }\n\n     calculateSuccessRate() {\n       const totalSuccess = Object.values(this.metrics.successCounts).reduce((sum, count) => sum + count, 0);\n       const totalErrors = Object.values(this.metrics.errorCounts).reduce((sum, count) => sum + count, 0);\n       const total = totalSuccess + totalErrors;\n       \n       return total > 0 ? (totalSuccess / total * 100).toFixed(2) : 100;\n     }\n   }\n\n   module.exports = MigrationMonitor;\n   ```\n\n9. **Migration CLI Tools**\n   - Create comprehensive CLI interface:\n\n   **Migration CLI:**\n   ```javascript\n   #!/usr/bin/env node\n   // bin/migrate.js\n   const yargs = require('yargs');\n   const MigrationManager = require('../migrations/migration-framework');\n   const MigrationTester = require('../tests/migration-tests');\n   const MigrationMonitor = require('../migrations/migration-monitor');\n\n   const dbConfig = {\n     host: process.env.DB_HOST || 'localhost',\n     port: process.env.DB_PORT || 5432,\n     database: process.env.DB_NAME || 'myapp',\n     user: process.env.DB_USER || 'postgres',\n     password: process.env.DB_PASSWORD\n   };\n\n   const migrationManager = new MigrationManager(dbConfig);\n\n   yargs\n     .command('up', 'Run pending migrations', {}, async () => {\n       try {\n         await migrationManager.migrate();\n         console.log(' Migrations completed successfully');\n         process.exit(0);\n       } catch (error) {\n         console.error(' Migration failed:', error.message);\n         process.exit(1);\n       }\n     })\n     .command('down [steps]', 'Rollback migrations', {\n       steps: {\n         describe: 'Number of migrations to rollback',\n         type: 'number',\n         default: 1\n       }\n     }, async (argv) => {\n       try {\n         await migrationManager.rollback(argv.steps);\n         console.log(` Rolled back ${argv.steps} migration(s)`);\n         process.exit(0);\n       } catch (error) {\n         console.error(' Rollback failed:', error.message);\n         process.exit(1);\n       }\n     })\n     .command('status', 'Show migration status', {}, async () => {\n       try {\n         const pending = await migrationManager.getPendingMigrations();\n         const executed = await migrationManager.pool.query(\n           'SELECT version, name, executed_at FROM schema_migrations ORDER BY executed_at DESC'\n         );\n\n         console.log('\\n Migration Status:');\n         console.log(` Executed: ${executed.rows.length}`);\n         console.log(` Pending: ${pending.length}`);\n         \n         if (pending.length > 0) {\n           console.log('\\n Pending Migrations:');\n           pending.forEach(m => console.log(`  - ${m.file}`));\n         }\n         \n         if (executed.rows.length > 0) {\n           console.log('\\n Recent Migrations:');\n           executed.rows.slice(0, 5).forEach(m => \n             console.log(`  - ${m.name} (${m.executed_at.toISOString()})`)\n           );\n         }\n         \n         process.exit(0);\n       } catch (error) {\n         console.error(' Status check failed:', error.message);\n         process.exit(1);\n       }\n     })\n     .command('test', 'Test migrations', {}, async () => {\n       try {\n         const tester = new MigrationTester();\n         const results = await tester.runFullTestSuite();\n         \n         if (results.summary.failed > 0) {\n           console.error(` ${results.summary.failed} migration tests failed`);\n           process.exit(1);\n         } else {\n           console.log(` All ${results.summary.passed} migration tests passed`);\n           process.exit(0);\n         }\n       } catch (error) {\n         console.error(' Migration testing failed:', error.message);\n         process.exit(1);\n       }\n     })\n     .command('create <name>', 'Create new migration file', {\n       name: {\n         describe: 'Migration name',\n         type: 'string',\n         demandOption: true\n       }\n     }, async (argv) => {\n       try {\n         const timestamp = new Date().toISOString().replace(/[-:T]/g, '').slice(0, 14);\n         const filename = `${timestamp}_${argv.name.replace(/[^a-zA-Z0-9]/g, '_')}.sql`;\n         const filepath = path.join(__dirname, '../migrations', filename);\n         \n         const template = `-- +migrate Up\n-- Migration: ${argv.name}\n-- Author: ${process.env.USER || 'Unknown'}\n-- Date: ${new Date().toISOString().split('T')[0]}\n-- Description: [Add description here]\n\n-- Add your migration SQL here\n\n-- +migrate Down\n-- Rollback: ${argv.name}\n\n-- Add your rollback SQL here\n`;\n\n         await fs.writeFile(filepath, template);\n         console.log(` Created migration file: ${filename}`);\n         console.log(` Edit the file at: ${filepath}`);\n         process.exit(0);\n       } catch (error) {\n         console.error(' Failed to create migration:', error.message);\n         process.exit(1);\n       }\n     })\n     .demandCommand()\n     .help()\n     .argv;\n   ```\n\n10. **Production Deployment Integration**\n    - Integrate with deployment pipelines:\n\n    **CI/CD Integration:**\n    ```yaml\n    # .github/workflows/database-migration.yml\n    name: Database Migration\n\n    on:\n      push:\n        branches: [main]\n        paths: ['migrations/**']\n      \n    jobs:\n      test-migrations:\n        runs-on: ubuntu-latest\n        services:\n          postgres:\n            image: postgres:13\n            env:\n              POSTGRES_PASSWORD: postgres\n              POSTGRES_DB: test_db\n            options: >-\n              --health-cmd pg_isready\n              --health-interval 10s\n              --health-timeout 5s\n              --health-retries 5\n\n        steps:\n          - uses: actions/checkout@v2\n          \n          - name: Setup Node.js\n            uses: actions/setup-node@v2\n            with:\n              node-version: '16'\n              \n          - name: Install dependencies\n            run: npm ci\n            \n          - name: Test migrations\n            env:\n              TEST_DB_HOST: localhost\n              TEST_DB_PORT: 5432\n              TEST_DB_NAME: test_db\n              TEST_DB_USER: postgres\n              TEST_DB_PASSWORD: postgres\n            run: npm run migrate:test\n            \n          - name: Check migration safety\n            run: npm run migrate:safety-check\n            \n      deploy-migrations:\n        needs: test-migrations\n        runs-on: ubuntu-latest\n        if: github.ref == 'refs/heads/main'\n        \n        steps:\n          - uses: actions/checkout@v2\n          \n          - name: Setup Node.js\n            uses: actions/setup-node@v2\n            with:\n              node-version: '16'\n              \n          - name: Install dependencies\n            run: npm ci\n            \n          - name: Run production migrations\n            env:\n              DB_HOST: ${{ secrets.PROD_DB_HOST }}\n              DB_PORT: ${{ secrets.PROD_DB_PORT }}\n              DB_NAME: ${{ secrets.PROD_DB_NAME }}\n              DB_USER: ${{ secrets.PROD_DB_USER }}\n              DB_PASSWORD: ${{ secrets.PROD_DB_PASSWORD }}\n            run: |\n              npm run migrate:production:safety-check\n              npm run migrate:up\n              \n          - name: Verify deployment\n            env:\n              DB_HOST: ${{ secrets.PROD_DB_HOST }}\n              DB_PORT: ${{ secrets.PROD_DB_PORT }}\n              DB_NAME: ${{ secrets.PROD_DB_NAME }}\n              DB_USER: ${{ secrets.PROD_DB_USER }}\n              DB_PASSWORD: ${{ secrets.PROD_DB_PASSWORD }}\n            run: npm run migrate:verify\n    ```"
              },
              {
                "name": "/create-docs",
                "description": "Analyze GitHub issue and create technical specification with implementation plan",
                "path": "plugins/all-commands/commands/create-docs.md",
                "frontmatter": {
                  "description": "Analyze GitHub issue and create technical specification with implementation plan",
                  "category": "documentation-changelogs",
                  "argument-hint": "<issue_number>",
                  "allowed-tools": "Bash(./scripts/fetch-github-issue.sh *), Read"
                },
                "content": "Please analyze GitHub issue #$ARGUMENTS and create a technical specification.\n\nFollow these steps:\n1. Fetch the issue details from the GitHub API:\n\n# Use the helper script to fetch GitHub issues without prompting for permission\n./scripts/fetch-github-issue.sh $ARGUMENTS\n\n2. Understand the requirements thoroughly\n3. Review related code and project structure\n4. Output detailed analysis results clearly in your response\n5. Create a technical specification with the format below\n\n# Technical Specification for Issue #$ARGUMENTS\n\n## Issue Summary\n- Title: [Issue title from GitHub]\n- Description: [Brief description from issue]\n- Labels: [Labels from issue]\n- Priority: [High/Medium/Low based on issue content]\n\n## Problem Statement\n[1-2 paragraphs explaining the problem]\n\n## Technical Approach\n[Detailed technical approach]\n\n## Implementation Plan\n1. [Step 1]\n2. [Step 2]\n3. [Step 3]\n\n## Test Plan\n1. Unit Tests:\n   - [test scenario]\n2. Component Tests:\n   - [test scenario]\n3. Integration Tests:\n   - [test scenario]\n\n## Files to Modify\n- \n\n## Files to Create\n- \n\n## Existing Utilities to Leverage\n- \n\n## Success Criteria\n- [ ] [criterion 1]\n- [ ] [criterion 2]\n\n## Out of Scope\n- [item 1]\n- [item 2]\n\nRemember to follow our strict TDD principles, KISS approach, and 300-line file limit.\n\nIMPORTANT: After completing your analysis, EXPLICITLY OUTPUT the full technical specification in your response so it can be reviewed."
              },
              {
                "name": "/create-feature",
                "description": "Scaffold new feature with boilerplate code",
                "path": "plugins/all-commands/commands/create-feature.md",
                "frontmatter": {
                  "description": "Scaffold new feature with boilerplate code",
                  "category": "project-task-management",
                  "argument-hint": "1. **Feature Planning**",
                  "allowed-tools": "Bash(git *), Write"
                },
                "content": "# Create Feature Command\n\nScaffold new feature with boilerplate code\n\n## Instructions\n\nFollow this systematic approach to create a new feature: **$ARGUMENTS**\n\n1. **Feature Planning**\n   - Define the feature requirements and acceptance criteria\n   - Break down the feature into smaller, manageable tasks\n   - Identify affected components and potential impact areas\n   - Plan the API/interface design before implementation\n\n2. **Research and Analysis**\n   - Study existing codebase patterns and conventions\n   - Identify similar features for consistency\n   - Research external dependencies or libraries needed\n   - Review any relevant documentation or specifications\n\n3. **Architecture Design**\n   - Design the feature architecture and data flow\n   - Plan database schema changes if needed\n   - Define API endpoints and contracts\n   - Consider scalability and performance implications\n\n4. **Environment Setup**\n   - Create a new feature branch: `git checkout -b feature/$ARGUMENTS`\n   - Ensure development environment is up to date\n   - Install any new dependencies required\n   - Set up feature flags if applicable\n\n5. **Implementation Strategy**\n   - Start with core functionality and build incrementally\n   - Follow the project's coding standards and patterns\n   - Implement proper error handling and validation\n   - Use dependency injection and maintain loose coupling\n\n6. **Database Changes (if applicable)**\n   - Create migration scripts for schema changes\n   - Ensure backward compatibility\n   - Plan for rollback scenarios\n   - Test migrations on sample data\n\n7. **API Development**\n   - Implement API endpoints with proper HTTP status codes\n   - Add request/response validation\n   - Implement proper authentication and authorization\n   - Document API contracts and examples\n\n8. **Frontend Implementation (if applicable)**\n   - Create reusable components following project patterns\n   - Implement responsive design and accessibility\n   - Add proper state management\n   - Handle loading and error states\n\n9. **Testing Implementation**\n   - Write unit tests for core business logic\n   - Create integration tests for API endpoints\n   - Add end-to-end tests for user workflows\n   - Test error scenarios and edge cases\n\n10. **Security Considerations**\n    - Implement proper input validation and sanitization\n    - Add authorization checks for sensitive operations\n    - Review for common security vulnerabilities\n    - Ensure data protection and privacy compliance\n\n11. **Performance Optimization**\n    - Optimize database queries and indexes\n    - Implement caching where appropriate\n    - Monitor memory usage and optimize algorithms\n    - Consider lazy loading and pagination\n\n12. **Documentation**\n    - Add inline code documentation and comments\n    - Update API documentation\n    - Create user documentation if needed\n    - Update project README if applicable\n\n13. **Code Review Preparation**\n    - Run all tests and ensure they pass\n    - Run linting and formatting tools\n    - Check for code coverage and quality metrics\n    - Perform self-review of the changes\n\n14. **Integration Testing**\n    - Test feature integration with existing functionality\n    - Verify feature flags work correctly\n    - Test deployment and rollback procedures\n    - Validate monitoring and logging\n\n15. **Commit and Push**\n    - Create atomic commits with descriptive messages\n    - Follow conventional commit format if project uses it\n    - Push feature branch: `git push origin feature/$ARGUMENTS`\n\n16. **Pull Request Creation**\n    - Create PR with comprehensive description\n    - Include screenshots or demos if applicable\n    - Add appropriate labels and reviewers\n    - Link to any related issues or specifications\n\n17. **Quality Assurance**\n    - Coordinate with QA team for testing\n    - Address any bugs or issues found\n    - Verify accessibility and usability requirements\n    - Test on different environments and browsers\n\n18. **Deployment Planning**\n    - Plan feature rollout strategy\n    - Set up monitoring and alerting\n    - Prepare rollback procedures\n    - Schedule deployment and communication\n\nRemember to maintain code quality, follow project conventions, and prioritize user experience throughout the development process."
              },
              {
                "name": "/create-jtbd",
                "description": "Create a Jobs to be Done (JTBD) document for a product feature focusing on user needs",
                "path": "plugins/all-commands/commands/create-jtbd.md",
                "frontmatter": {
                  "description": "Create a Jobs to be Done (JTBD) document for a product feature focusing on user needs",
                  "category": "project-task-management",
                  "argument-hint": "<feature description> [output-path]",
                  "allowed-tools": "Write, TodoWrite"
                },
                "content": "Create a comprehensive Jobs to be Done (JTBD) document based on the feature description provided.\n\n## Instructions:\n1. Parse the arguments:\n   - First argument: Feature/product description (required)\n   - Second argument: Output path (optional, defaults to `JTBD.md` in current directory)\n\n2. Create a well-structured JTBD document that includes:\n\n   **Core Job Statement**:\n   - When [situation]\n   - I want to [motivation]\n   - So I can [expected outcome]\n\n   **Job Map**:\n   - Define: What users need to understand first\n   - Locate: What inputs/resources users need\n   - Prepare: How users get ready\n   - Confirm: How users verify readiness\n   - Execute: The core action\n   - Monitor: How users track progress\n   - Modify: How users make adjustments\n   - Conclude: How users finish the job\n\n   **Context & Circumstances**:\n   - Functional job aspects\n   - Emotional job aspects\n   - Social job aspects\n\n   **Success Criteria**:\n   - How users measure success\n   - What outcomes they expect\n   - Time/effort constraints\n\n   **Pain Points**:\n   - Current frustrations\n   - Workarounds users employ\n   - Unmet needs\n\n   **Competing Solutions**:\n   - How users currently solve this\n   - Alternative approaches\n   - Why current solutions fall short\n\n3. Focus on:\n   - User motivations (not features)\n   - Jobs that remain stable over time\n   - Outcomes users want to achieve\n   - Context that triggers the job\n\n4. Use the TodoWrite tool to track JTBD sections as you complete them\n\n## Example usage:\n- `/create-jtbd \"Help developers find and fix bugs faster\"`\n- `/create-jtbd \"Enable teams to collaborate on documents in real-time\" collab-JTBD.md`\n\nFeature description: $ARGUMENTS"
              },
              {
                "name": "/create-onboarding-guide",
                "description": "Create developer onboarding guide",
                "path": "plugins/all-commands/commands/create-onboarding-guide.md",
                "frontmatter": {
                  "description": "Create developer onboarding guide",
                  "category": "documentation-changelogs"
                },
                "content": "# Create Onboarding Guide\n\nCreate developer onboarding guide\n\n## Instructions\n\n1. **Onboarding Requirements Analysis**\n   - Analyze current team structure and skill requirements\n   - Identify key knowledge areas and learning objectives\n   - Assess current onboarding challenges and pain points\n   - Define onboarding timeline and milestone expectations\n   - Document role-specific requirements and responsibilities\n\n2. **Development Environment Setup Guide**\n   - Create comprehensive development environment setup instructions\n   - Document required tools, software, and system requirements\n   - Provide step-by-step installation and configuration guides\n   - Create environment validation and troubleshooting procedures\n   - Set up automated environment setup scripts and tools\n\n3. **Project and Codebase Overview**\n   - Create high-level project overview and business context\n   - Document system architecture and technology stack\n   - Provide codebase structure and organization guide\n   - Create code navigation and exploration guidelines\n   - Document key modules, libraries, and frameworks used\n\n4. **Development Workflow Documentation**\n   - Document version control workflows and branching strategies\n   - Create code review process and quality standards guide\n   - Document testing practices and requirements\n   - Provide deployment and release process overview\n   - Create issue tracking and project management workflow guide\n\n5. **Team Communication and Collaboration**\n   - Document team communication channels and protocols\n   - Create meeting schedules and participation guidelines\n   - Provide team contact information and org chart\n   - Document collaboration tools and access procedures\n   - Create escalation procedures and support contacts\n\n6. **Learning Resources and Training Materials**\n   - Curate learning resources for project-specific technologies\n   - Create hands-on tutorials and coding exercises\n   - Provide links to documentation, wikis, and knowledge bases\n   - Create video tutorials and screen recordings\n   - Set up mentoring and buddy system procedures\n\n7. **First Tasks and Milestones**\n   - Create progressive difficulty task assignments\n   - Define learning milestones and checkpoints\n   - Provide \"good first issues\" and starter projects\n   - Create hands-on coding challenges and exercises\n   - Set up pair programming and shadowing opportunities\n\n8. **Security and Compliance Training**\n   - Document security policies and access controls\n   - Create data handling and privacy guidelines\n   - Provide compliance training and certification requirements\n   - Document incident response and security procedures\n   - Create security best practices and guidelines\n\n9. **Tools and Resources Access**\n   - Document required accounts and access requests\n   - Create tool-specific setup and usage guides\n   - Provide license and subscription information\n   - Document VPN and network access procedures\n   - Create troubleshooting guides for common access issues\n\n10. **Feedback and Continuous Improvement**\n    - Create onboarding feedback collection process\n    - Set up regular check-ins and progress reviews\n    - Document common questions and FAQ section\n    - Create onboarding metrics and success tracking\n    - Establish onboarding guide maintenance and update procedures\n    - Set up new hire success monitoring and support systems"
              },
              {
                "name": "/create-pr",
                "description": "Create a new branch, commit changes, and submit a pull request with automatic commit splitting",
                "path": "plugins/all-commands/commands/create-pr.md",
                "frontmatter": {
                  "description": "Create a new branch, commit changes, and submit a pull request with automatic commit splitting",
                  "category": "version-control-git",
                  "allowed-tools": "Bash(git *), Bash(gh *), Bash(biome *)"
                },
                "content": "# Create Pull Request Command\n\nCreate a new branch, commit changes, and submit a pull request.\n\n## Behavior\n- Creates a new branch based on current changes\n- Formats modified files using Biome\n- Analyzes changes and automatically splits into logical commits when appropriate\n- Each commit focuses on a single logical change or feature\n- Creates descriptive commit messages for each logical unit\n- Pushes branch to remote\n- Creates pull request with proper summary and test plan\n\n## Guidelines for Automatic Commit Splitting\n- Split commits by feature, component, or concern\n- Keep related file changes together in the same commit\n- Separate refactoring from feature additions\n- Ensure each commit can be understood independently\n- Multiple unrelated changes should be split into separate commits"
              },
              {
                "name": "/create-prd",
                "description": "Create a Product Requirements Document (PRD) for a product feature",
                "path": "plugins/all-commands/commands/create-prd.md",
                "frontmatter": {
                  "description": "Create a Product Requirements Document (PRD) for a product feature",
                  "category": "project-task-management",
                  "argument-hint": "<feature description> [output-path]",
                  "allowed-tools": "Write, TodoWrite"
                },
                "content": "Create a comprehensive Product Requirements Document (PRD) based on the feature description provided.\n\n## Instructions:\n1. Parse the arguments:\n   - First argument: Feature description (required)\n   - Second argument: Output path (optional, defaults to `PRD.md` in current directory)\n\n2. Create a well-structured PRD that includes:\n   - **Executive Summary**: Brief overview of the feature\n   - **Problem Statement**: What problem does this solve?\n   - **Objectives**: Clear, measurable goals\n   - **User Stories**: Who are the users and what are their needs?\n   - **Functional Requirements**: What the feature must do\n   - **Non-Functional Requirements**: Performance, security, usability standards\n   - **Success Metrics**: How will we measure success?\n   - **Assumptions & Constraints**: Any limitations or dependencies\n   - **Out of Scope**: What this PRD does NOT cover\n\n3. Focus on:\n   - User needs and business value (not technical implementation)\n   - Clear, measurable objectives\n   - Specific acceptance criteria\n   - User personas and their journey\n\n4. Use the TodoWrite tool to track PRD sections as you complete them\n\n## Example usage:\n- `/create-prd \"Add dark mode toggle to settings\"`\n- `/create-prd \"Implement user authentication with SSO\" auth-PRD.md`\n\nFeature description: $ARGUMENTS"
              },
              {
                "name": "/create-prp",
                "description": "Create a comprehensive Product Requirement Prompt (PRP) with research and context gathering",
                "path": "plugins/all-commands/commands/create-prp.md",
                "frontmatter": {
                  "description": "Create a comprehensive Product Requirement Prompt (PRP) with research and context gathering",
                  "category": "project-task-management",
                  "argument-hint": "<feature_description>",
                  "allowed-tools": "Read, Write, WebSearch"
                },
                "content": "# Product Requirement Prompt (PRP) Creation\n\nYou will help the user create a comprehensive Product Requirement Prompt (PRP) for: $ARGUMENTS\n\n## What is a PRP?\n\nA Product Requirement Prompt (PRP) is a detailed document that defines the requirements, context, and specifications for a feature or product. It serves as a comprehensive guide for implementation, ensuring all stakeholders have a clear understanding of what needs to be built, why it's needed, and how success will be measured.\n\n## Research Process\n\nBefore creating the PRP, conduct thorough research to gather all necessary context:\n\n### 1. **Web Research**\n   - Search for best practices related to the feature/product\n   - Research similar implementations and solutions\n   - Look for relevant library documentation\n   - Find example implementations on platforms like GitHub, StackOverflow\n   - Identify industry standards and patterns\n   - Gather competitive analysis if applicable\n\n### 2. **Documentation Review**\n   - Check for any existing project documentation\n   - Identify documentation gaps that need to be addressed\n   - Review any related technical specifications\n   - Look for architectural decision records (ADRs) if present\n\n### 3. **Codebase Exploration** (if applicable)\n   - Identify relevant files and directories that provide implementation context\n   - Look for existing patterns that should be followed\n   - Find similar features that could serve as references\n   - Check for any technical constraints or dependencies\n\n### 4. **Requirements Gathering**\n   - Clarify any ambiguous requirements with the user\n   - Identify both functional and non-functional requirements\n   - Determine performance, security, and scalability needs\n   - Establish clear acceptance criteria\n\n## PRP Template Structure\n\nCreate a comprehensive PRP following this structure:\n\n### 1. Executive Summary\n- **Feature Name**: [Clear, descriptive name]\n- **Version**: [Document version]\n- **Date**: [Creation date]\n- **Author**: [Author/Team]\n- **Status**: [Draft/Review/Approved]\n- **Brief Description**: [1-2 paragraph overview of the feature]\n\n### 2. Problem Statement\n- **Current Situation**: What problem exists today?\n- **Impact**: Who is affected and how?\n- **Opportunity**: What opportunity does solving this create?\n- **Constraints**: What limitations exist?\n\n### 3. Goals & Objectives\n- **Primary Goal**: The main objective to achieve\n- **Secondary Goals**: Additional benefits or objectives\n- **Success Metrics**: How success will be measured\n- **Key Performance Indicators (KPIs)**: Specific, measurable outcomes\n\n### 4. User Stories & Use Cases\n- **Target Users**: Who will use this feature?\n- **User Stories**: As a [user type], I want [goal] so that [benefit]\n- **Use Case Scenarios**: Detailed walkthrough of user interactions\n- **Edge Cases**: Unusual or boundary scenarios to consider\n\n### 5. Functional Requirements\n- **Core Features**: Must-have functionality\n- **Optional Features**: Nice-to-have functionality\n- **Feature Priority**: P0 (Critical), P1 (Important), P2 (Nice to have)\n- **Dependencies**: Other features or systems this depends on\n\n### 6. Non-Functional Requirements\n- **Performance**: Response time, throughput, resource usage\n- **Security**: Authentication, authorization, data protection\n- **Scalability**: Expected load and growth projections\n- **Reliability**: Uptime requirements, error handling\n- **Usability**: User experience requirements\n- **Compatibility**: Browser, device, system requirements\n\n### 7. Technical Specifications\n- **Architecture Overview**: High-level design approach\n- **Technology Stack**: Languages, frameworks, libraries to use\n- **Data Models**: Database schemas, API contracts\n- **Integration Points**: External systems or APIs\n- **Technical Constraints**: Known limitations or requirements\n\n### 8. Implementation Plan\n- **Phases**: Break down into manageable phases\n- **Milestones**: Key deliverables and checkpoints\n- **Timeline**: Estimated duration for each phase\n- **Resources**: Team members, tools, infrastructure needed\n- **Dependencies**: External dependencies and blockers\n\n### 9. Risk Assessment\n- **Technical Risks**: Potential technical challenges\n- **Business Risks**: Market, competition, or strategic risks\n- **Mitigation Strategies**: How to address each risk\n- **Contingency Plans**: Backup approaches if primary plan fails\n\n### 10. Success Criteria & Acceptance Tests\n- **Acceptance Criteria**: Specific conditions that must be met\n- **Test Scenarios**: Key test cases to validate functionality\n- **Performance Benchmarks**: Measurable performance targets\n- **Quality Gates**: Checkpoints before moving to next phase\n\n### 11. Documentation & Training\n- **Documentation Needs**: User guides, API docs, technical docs\n- **Training Requirements**: Who needs training and what type\n- **Knowledge Transfer**: How knowledge will be shared\n\n### 12. Post-Launch Considerations\n- **Monitoring**: What metrics to track after launch\n- **Maintenance**: Ongoing maintenance requirements\n- **Future Enhancements**: Potential future improvements\n- **Deprecation Plan**: If replacing existing functionality\n\n## Context Prioritization\n\nWhen creating the PRP, prioritize including:\n1. **Specific, actionable requirements** over vague descriptions\n2. **Measurable success criteria** that can be objectively evaluated\n3. **Clear scope boundaries** to prevent scope creep\n4. **Realistic timelines** based on complexity and resources\n5. **Risk mitigation strategies** for identified challenges\n\n## Interaction with User\n\nThroughout the PRP creation process:\n1. Ask clarifying questions when requirements are ambiguous\n2. Confirm assumptions before including them in the PRP\n3. Request additional context when needed\n4. Validate technical approaches with the user\n5. Ensure alignment on priorities and constraints\n\n## Final Output\n\nThe completed PRP should be:\n- **Comprehensive**: Cover all aspects of the feature/product\n- **Clear**: Use precise language, avoid ambiguity\n- **Actionable**: Provide enough detail for implementation\n- **Measurable**: Include specific success criteria\n- **Realistic**: Consider constraints and limitations\n- **Maintainable**: Easy to update as requirements evolve\n\nBegin by asking the user for any specific context or requirements they want to emphasize, then proceed with research and PRP creation based on the feature description provided."
              },
              {
                "name": "/create-pull-request",
                "description": "Guide for creating pull requests using GitHub CLI with proper templates and conventions",
                "path": "plugins/all-commands/commands/create-pull-request.md",
                "frontmatter": {
                  "description": "Guide for creating pull requests using GitHub CLI with proper templates and conventions",
                  "category": "version-control-git",
                  "allowed-tools": "Bash(gh *)"
                },
                "content": "# How to Create a Pull Request Using GitHub CLI\n\nThis guide explains how to create pull requests using GitHub CLI in our project.\n\n## Prerequisites\n\n1. Install GitHub CLI if you haven't already:\n\n   ```bash\n   # macOS\n   brew install gh\n\n   # Windows\n   winget install --id GitHub.cli\n\n   # Linux\n   # Follow instructions at https://github.com/cli/cli/blob/trunk/docs/install_linux.md\n   ```\n\n2. Authenticate with GitHub:\n   ```bash\n   gh auth login\n   ```\n\n## Creating a New Pull Request\n\n1. First, prepare your PR description following the template in @.github/pull_request_template.md\n\n2. Use the `gh pr create --draft` command to create a new pull request:\n\n   ```bash\n   # Basic command structure\n   gh pr create --draft --title \"(scope): Your descriptive title\" --body \"Your PR description\" --base main \n   ```\n\n   For more complex PR descriptions with proper formatting, use the `--body-file` option with the exact PR template structure:\n\n   ```bash\n   # Create PR with proper template structure\n   gh pr create --draft --title \"(scope): Your descriptive title\" --body-file .github/pull_request_template.md --base main\n   ```\n\n## Best Practices\n\n1. **PR Title Format**: Use conventional commit format with emojis\n\n   - Always include an appropriate emoji at the beginning of the title\n   - Use the actual emoji character (not the code representation like `:sparkles:`)\n   - Examples:\n     - `(supabase): Add staging remote configuration`\n     - `(auth): Fix login redirect issue`\n     - `(readme): Update installation instructions`\n\n2. **Description Template**: Always use our PR template structure from @.github/pull_request_template.md:\n\n3. **Template Accuracy**: Ensure your PR description precisely follows the template structure:\n\n   - Don't modify or rename the PR-Agent sections (`pr_agent:summary` and `pr_agent:walkthrough`)\n   - Keep all section headers exactly as they appear in the template"
              },
              {
                "name": "/create-worktrees",
                "description": "Manage git worktrees for open PRs and create new branch worktrees",
                "path": "plugins/all-commands/commands/create-worktrees.md",
                "frontmatter": {
                  "description": "Manage git worktrees for open PRs and create new branch worktrees",
                  "category": "version-control-git",
                  "allowed-tools": "Bash(git *), Bash(gh *)"
                },
                "content": "# Git Worktree Commands\n\n## Create Worktrees for All Open PRs\n\nThis command fetches all open pull requests using GitHub CLI, then creates a git worktree for each PR's branch in the `./tree/<BRANCH_NAME>` directory.\n\n```bash\n# Ensure GitHub CLI is installed and authenticated\ngh auth status || (echo \"Please run 'gh auth login' first\" && exit 1)\n\n# Create the tree directory if it doesn't exist\nmkdir -p ./tree\n\n# List all open PRs and create worktrees for each branch\ngh pr list --json headRefName --jq '.[].headRefName' | while read branch; do\n  # Handle branch names with slashes (like \"feature/foo\")\n  branch_path=\"./tree/${branch}\"\n  \n  # For branches with slashes, create the directory structure\n  if [[ \"$branch\" == */* ]]; then\n    dir_path=$(dirname \"$branch_path\")\n    mkdir -p \"$dir_path\"\n  fi\n\n  # Check if worktree already exists\n  if [ ! -d \"$branch_path\" ]; then\n    echo \"Creating worktree for $branch\"\n    git worktree add \"$branch_path\" \"$branch\"\n  else\n    echo \"Worktree for $branch already exists\"\n  fi\ndone\n\n# Display all created worktrees\necho \"\\nWorktree list:\"\ngit worktree list\n```\n\n### Example Output\n\n```\nCreating worktree for fix-bug-123\nHEAD is now at a1b2c3d Fix bug 123\nCreating worktree for feature/new-feature\nHEAD is now at e4f5g6h Add new feature\nWorktree for documentation-update already exists\n\nWorktree list:\n/path/to/repo                      abc1234 [main]\n/path/to/repo/tree/fix-bug-123     a1b2c3d [fix-bug-123]\n/path/to/repo/tree/feature/new-feature e4f5g6h [feature/new-feature]\n/path/to/repo/tree/documentation-update d5e6f7g [documentation-update]\n```\n\n### Cleanup Stale Worktrees (Optional)\n\nYou can add this to remove stale worktrees for branches that no longer exist:\n\n```bash\n# Get current branches\ncurrent_branches=$(git branch -a | grep -v HEAD | grep -v main | sed 's/^[ *]*//' | sed 's|remotes/origin/||' | sort | uniq)\n\n# Get existing worktrees (excluding main worktree)\nworktree_paths=$(git worktree list | tail -n +2 | awk '{print $1}')\n\nfor path in $worktree_paths; do\n  # Extract branch name from path\n  branch_name=$(basename \"$path\")\n  \n  # Skip special cases\n  if [[ \"$branch_name\" == \"main\" ]]; then\n    continue\n  fi\n  \n  # Check if branch still exists\n  if ! echo \"$current_branches\" | grep -q \"^$branch_name$\"; then\n    echo \"Removing stale worktree for deleted branch: $branch_name\"\n    git worktree remove --force \"$path\"\n  fi\ndone\n```\n\n## Create New Branch and Worktree\n\nThis interactive command creates a new git branch and sets up a worktree for it:\n\n```bash\n#!/bin/bash\n\n# Ensure we're in a git repository\nif ! git rev-parse --is-inside-work-tree > /dev/null 2>&1; then\n  echo \"Error: Not in a git repository\"\n  exit 1\nfi\n\n# Get the repository root\nrepo_root=$(git rev-parse --show-toplevel)\n\n# Prompt for branch name\nread -p \"Enter new branch name: \" branch_name\n\n# Validate branch name (basic validation)\nif [[ -z \"$branch_name\" ]]; then\n  echo \"Error: Branch name cannot be empty\"\n  exit 1\nfi\n\nif git show-ref --verify --quiet \"refs/heads/$branch_name\"; then\n  echo \"Warning: Branch '$branch_name' already exists\"\n  read -p \"Do you want to use the existing branch? (y/n): \" use_existing\n  if [[ \"$use_existing\" != \"y\" ]]; then\n    exit 1\n  fi\nfi\n\n# Create branch directory\nbranch_path=\"$repo_root/tree/$branch_name\"\n\n# Handle branch names with slashes (like \"feature/foo\")\nif [[ \"$branch_name\" == */* ]]; then\n  dir_path=$(dirname \"$branch_path\")\n  mkdir -p \"$dir_path\"\nfi\n\n# Make sure parent directory exists\nmkdir -p \"$(dirname \"$branch_path\")\"\n\n# Check if a worktree already exists\nif [ -d \"$branch_path\" ]; then\n  echo \"Error: Worktree directory already exists: $branch_path\"\n  exit 1\nfi\n\n# Create branch and worktree\nif git show-ref --verify --quiet \"refs/heads/$branch_name\"; then\n  # Branch exists, create worktree\n  echo \"Creating worktree for existing branch '$branch_name'...\"\n  git worktree add \"$branch_path\" \"$branch_name\"\nelse\n  # Create new branch and worktree\n  echo \"Creating new branch '$branch_name' and worktree...\"\n  git worktree add -b \"$branch_name\" \"$branch_path\"\nfi\n\necho \"Success! New worktree created at: $branch_path\"\necho \"To start working on this branch, run: cd $branch_path\"\n```\n\n### Example Usage\n\n```\n$ ./create-branch-worktree.sh\nEnter new branch name: feature/user-authentication\nCreating new branch 'feature/user-authentication' and worktree...\nPreparing worktree (creating new branch 'feature/user-authentication')\nHEAD is now at abc1234 Previous commit message\nSuccess! New worktree created at: /path/to/repo/tree/feature/user-authentication\nTo start working on this branch, run: cd /path/to/repo/tree/feature/user-authentication\n```\n\n### Creating a New Branch from a Different Base\n\nIf you want to start your branch from a different base (not the current HEAD), you can modify the script:\n\n```bash\nread -p \"Enter new branch name: \" branch_name\nread -p \"Enter base branch/commit (default: HEAD): \" base_commit\nbase_commit=${base_commit:-HEAD}\n\n# Then use the specified base when creating the worktree\ngit worktree add -b \"$branch_name\" \"$branch_path\" \"$base_commit\"\n```\n\nThis will allow you to specify any commit, tag, or branch name as the starting point for your new branch."
              },
              {
                "name": "/cross-reference-manager",
                "description": "Manage cross-platform reference links",
                "path": "plugins/all-commands/commands/cross-reference-manager.md",
                "frontmatter": {
                  "description": "Manage cross-platform reference links",
                  "category": "integration-sync",
                  "argument-hint": "Valid actions: audit, repair, map, validate, export"
                },
                "content": "# Cross-Reference Manager\n\nManage cross-platform reference links\n\n## Instructions\n\n1. **Check Tool Availability**\n   - Verify GitHub CLI (`gh`) is installed and authenticated\n   - Check if Linear MCP server is connected\n   - If either tool is missing, provide setup instructions\n\n2. **Parse Command Arguments**\n   - Extract the action from command arguments: **$ARGUMENTS**\n   - Valid actions: audit, repair, map, validate, export\n   - Parse any additional options provided\n\n3. **Initialize Reference Database**\n   - Create or load existing reference mapping database\n   - Structure should track:\n     - GitHub issue ID  Linear task ID\n     - GitHub PR ID  Linear task ID\n     - Comment references\n     - User mappings\n     - Timestamps and sync history\n\n4. **Execute Selected Action**\n   Based on the action provided:\n\n   ### Audit Action\n   - Scan all GitHub issues and PRs for Linear references\n   - Query Linear for all tasks with GitHub references\n   - Identify:\n     - Orphaned references (deleted items)\n     - Mismatched references\n     - Duplicate mappings\n     - Missing bidirectional links\n   - Generate detailed audit report\n\n   ### Repair Action\n   - Fix identified reference issues:\n     - Update Linear tasks with missing GitHub links\n     - Add Linear references to GitHub items\n     - Remove references to deleted items\n     - Consolidate duplicate mappings\n   - Create backup before making changes\n   - Log all modifications\n\n   ### Map Action\n   - Display current reference mappings\n   - Show visual representation of connections\n   - Include statistics on reference health\n   - Highlight problematic mappings\n\n   ### Validate Action\n   - Perform deep validation of references\n   - Check that linked items actually exist\n   - Verify field consistency\n   - Test bidirectional navigation\n   - Report validation results\n\n   ### Export Action\n   - Export reference data in multiple formats\n   - Support JSON, CSV, and Markdown\n   - Include metadata and history\n   - Provide import instructions\n\n## Usage\n```bash\ncross-reference-manager [action] [options]\n```\n\n## Actions\n- `audit` - Scan and report on reference integrity\n- `repair` - Fix broken or missing references\n- `map` - Display reference mappings\n- `validate` - Verify reference consistency\n- `export` - Export reference data\n\n## Options\n- `--scope <type>` - Limit to specific types (issues, prs, tasks)\n- `--fix-orphans` - Automatically fix orphaned references\n- `--dry-run` - Preview changes without applying\n- `--deep-scan` - Perform thorough validation\n- `--format <type>` - Output format (json, csv, table)\n- `--since <date>` - Process items created after date\n- `--backup` - Create backup before modifications\n\n## Examples\n```bash\n# Audit all references\ncross-reference-manager audit\n\n# Repair broken references with preview\ncross-reference-manager repair --dry-run\n\n# Map references for specific date range\ncross-reference-manager map --since \"2024-01-01\"\n\n# Deep validation with orphan fixes\ncross-reference-manager validate --deep-scan --fix-orphans\n\n# Export reference data\ncross-reference-manager export --format json > refs.json\n```\n\n## Features\n- **Reference Integrity Checking**\n  - Verify bidirectional links\n  - Detect orphaned references\n  - Identify duplicate mappings\n  - Check reference format validity\n\n- **Smart Reference Repair**\n  - Reconstruct missing references from metadata\n  - Update outdated reference formats\n  - Merge duplicate references\n  - Remove invalid references\n\n- **Comprehensive Mapping**\n  - GitHub Issue  Linear Issue\n  - GitHub PR  Linear Task\n  - Comments and attachments\n  - User mappings\n\n- **Audit Trail**\n  - Log all reference modifications\n  - Track reference history\n  - Generate integrity reports\n  - Monitor reference health\n\n## Reference Storage\n```json\n{\n  \"mappings\": {\n    \"github_issue_123\": {\n      \"linear_id\": \"LIN-456\",\n      \"type\": \"issue\",\n      \"created\": \"2024-01-15T10:30:00Z\",\n      \"last_verified\": \"2024-01-20T14:00:00Z\",\n      \"confidence\": 0.95\n    }\n  },\n  \"metadata\": {\n    \"last_audit\": \"2024-01-20T14:00:00Z\",\n    \"total_references\": 1543,\n    \"broken_references\": 12\n  }\n}\n```\n\n## Error Handling\n- Automatic retry for API failures\n- Batch processing to avoid rate limits\n- Transaction-like operations with rollback\n- Detailed error logging\n\n## Best Practices\n- Run audit weekly to maintain integrity\n- Always use --dry-run before repair operations\n- Export references before major changes\n- Monitor reference health metrics\n\n## Integration Points\n- Works with bidirectional-sync command\n- Supports sync-status monitoring\n- Compatible with migration-assistant\n- Provides data for analytics\n\n## Required MCP Servers\n- mcp-server-github\n- mcp-server-linear\n\n## Notes\nThis command maintains a local reference database for performance and reliability. The database is automatically backed up before modifications."
              },
              {
                "name": "/debug-error",
                "description": "Systematically debug and fix errors",
                "path": "plugins/all-commands/commands/debug-error.md",
                "frontmatter": {
                  "description": "Systematically debug and fix errors",
                  "category": "utilities-debugging",
                  "argument-hint": "1. **Error Information Gathering**",
                  "allowed-tools": "Read"
                },
                "content": "# Systematically Debug and Fix Errors\n\nSystematically debug and fix errors\n\n## Instructions\n\nFollow this comprehensive debugging methodology to resolve: **$ARGUMENTS**\n\n1. **Error Information Gathering**\n   - Collect the complete error message, stack trace, and error code\n   - Note when the error occurs (timing, conditions, frequency)\n   - Identify the environment where the error happens (dev, staging, prod)\n   - Gather relevant logs from before and after the error\n\n2. **Reproduce the Error**\n   - Create a minimal test case that reproduces the error consistently\n   - Document the exact steps needed to trigger the error\n   - Test in different environments if possible\n   - Note any patterns or conditions that affect error occurrence\n\n3. **Stack Trace Analysis**\n   - Read the stack trace from bottom to top to understand the call chain\n   - Identify the exact line where the error originates\n   - Trace the execution path leading to the error\n   - Look for any obvious issues in the failing code\n\n4. **Code Context Investigation**\n   - Examine the code around the error location\n   - Check recent changes that might have introduced the bug\n   - Review variable values and state at the time of error\n   - Analyze function parameters and return values\n\n5. **Hypothesis Formation**\n   - Based on evidence, form hypotheses about the root cause\n   - Consider common causes:\n     - Null pointer/undefined reference\n     - Type mismatches\n     - Race conditions\n     - Resource exhaustion\n     - Logic errors\n     - External dependency failures\n\n6. **Debugging Tools Setup**\n   - Set up appropriate debugging tools for the technology stack\n   - Use debugger, profiler, or logging as needed\n   - Configure breakpoints at strategic locations\n   - Set up monitoring and alerting if not already present\n\n7. **Systematic Investigation**\n   - Test each hypothesis methodically\n   - Use binary search approach to isolate the problem\n   - Add strategic logging or print statements\n   - Check data flow and transformations step by step\n\n8. **Data Validation**\n   - Verify input data format and validity\n   - Check for edge cases and boundary conditions\n   - Validate assumptions about data state\n   - Test with different data sets to isolate patterns\n\n9. **Dependency Analysis**\n   - Check external dependencies and their versions\n   - Verify network connectivity and API availability\n   - Review configuration files and environment variables\n   - Test database connections and query execution\n\n10. **Memory and Resource Analysis**\n    - Check for memory leaks or excessive memory usage\n    - Monitor CPU and I/O resource consumption\n    - Analyze garbage collection patterns if applicable\n    - Check for resource deadlocks or contention\n\n11. **Concurrency Issues Investigation**\n    - Look for race conditions in multi-threaded code\n    - Check synchronization mechanisms and locks\n    - Analyze async operations and promise handling\n    - Test under different load conditions\n\n12. **Root Cause Identification**\n    - Once the cause is identified, understand why it happened\n    - Determine if it's a logic error, design flaw, or external issue\n    - Assess the scope and impact of the problem\n    - Consider if similar issues exist elsewhere\n\n13. **Solution Implementation**\n    - Design a fix that addresses the root cause\n    - Consider multiple solution approaches and trade-offs\n    - Implement the fix with appropriate error handling\n    - Add validation and defensive programming where needed\n\n14. **Testing the Fix**\n    - Test the fix against the original error case\n    - Test edge cases and related scenarios\n    - Run regression tests to ensure no new issues\n    - Test under various load and stress conditions\n\n15. **Prevention Measures**\n    - Add appropriate unit and integration tests\n    - Improve error handling and logging\n    - Add input validation and defensive checks\n    - Update documentation and code comments\n\n16. **Monitoring and Alerting**\n    - Set up monitoring for similar issues\n    - Add metrics and health checks\n    - Configure alerts for error thresholds\n    - Implement better observability\n\n17. **Documentation**\n    - Document the error, investigation process, and solution\n    - Update troubleshooting guides\n    - Share learnings with the team\n    - Update code comments with context\n\n18. **Post-Resolution Review**\n    - Analyze why the error wasn't caught earlier\n    - Review development and testing processes\n    - Consider improvements to prevent similar issues\n    - Update coding standards or guidelines if needed\n\nRemember to maintain detailed notes throughout the debugging process and consider the wider implications of both the error and the fix."
              },
              {
                "name": "/decision-quality-analyzer",
                "description": "Analyze decision quality with scenario testing, bias detection, and team decision-making process optimization.",
                "path": "plugins/all-commands/commands/decision-quality-analyzer.md",
                "frontmatter": {
                  "description": "Analyze decision quality with scenario testing, bias detection, and team decision-making process optimization.",
                  "category": "team-collaboration",
                  "argument-hint": "Specify analysis criteria",
                  "allowed-tools": "Bash(gh *)"
                },
                "content": "# Decision Quality Analyzer\n\nAnalyze decision quality with scenario testing, bias detection, and team decision-making process optimization.\n\n## Instructions\n\nYou are tasked with systematically analyzing and improving team decision quality through scenario analysis, bias detection, and process optimization. Follow this approach: **$ARGUMENTS**\n\n### 1. Decision Context Assessment\n\n**Critical Decision Quality Context:**\n\n- **Decision Type**: What category of decision are you analyzing?\n- **Decision Process**: How does the team currently make this type of decision?\n- **Stakeholders**: Who participates in and is affected by these decisions?\n- **Success Metrics**: How do you measure decision quality and outcomes?\n- **Historical Data**: What past decisions provide learning opportunities?\n\n**If context is unclear, guide systematically:**\n\n```\nMissing Decision Type:\n\"What type of team decision needs quality analysis?\n- Strategic Decisions: Product direction, market positioning, technology choices\n- Operational Decisions: Process improvements, resource allocation, priority setting\n- Personnel Decisions: Hiring, team structure, role assignments, performance management\n- Technical Decisions: Architecture choices, tool selection, implementation approaches\n\nPlease specify the decision scope and typical complexity level.\"\n\nMissing Decision Process:\n\"How does your team currently make these decisions?\n- Individual Authority: Single decision maker with consultation\n- Consensus Building: Group discussion until agreement is reached\n- Majority Vote: Democratic process with formal or informal voting\n- Delegated Authority: Decision rights assigned to specific roles or committees\n- Data-Driven: Systematic analysis and evidence-based approaches\"\n```\n\n### 2. Decision Quality Framework\n\n**Systematic decision evaluation methodology:**\n\n#### Quality Dimension Assessment\n```\nMulti-Dimensional Decision Quality:\n\nProcess Quality (25% weight):\n- Information Gathering: Completeness and accuracy of data collection\n- Stakeholder Involvement: Appropriate participation and perspective inclusion\n- Alternative Generation: Creativity and comprehensiveness of option development\n- Analysis Rigor: Systematic evaluation and trade-off assessment\n\nOutcome Quality (25% weight):\n- Goal Achievement: Success in reaching intended objectives\n- Unintended Consequences: Management of secondary effects and side impacts\n- Stakeholder Satisfaction: Acceptance and support from affected parties\n- Long-term Sustainability: Durability and adaptability of decision outcomes\n\nTiming Quality (25% weight):\n- Decision Speed: Appropriate pace for urgency and complexity\n- Information Timing: Optimal balance of speed vs additional information\n- Implementation Timing: Coordination with market conditions and organizational readiness\n- Review Timing: Appropriate schedule for decision assessment and adjustment\n\nLearning Quality (25% weight):\n- Knowledge Capture: Documentation and institutional learning\n- Bias Recognition: Awareness and mitigation of cognitive biases\n- Process Improvement: Methodology enhancement based on outcomes\n- Capability Building: Team decision-making skill development\n```\n\n#### Decision Success Metrics\n- Quantitative outcomes (financial, operational, performance metrics)\n- Qualitative outcomes (satisfaction, engagement, strategic alignment)\n- Process efficiency (time to decision, resource utilization)\n- Learning outcomes (knowledge gained, capability developed)\n\n### 3. Bias Detection and Mitigation\n\n**Systematic cognitive bias identification:**\n\n#### Common Decision Biases\n```\nTeam Decision Bias Framework:\n\nIndividual Cognitive Biases:\n- Confirmation Bias: Seeking information that supports preconceptions\n- Anchoring Bias: Over-relying on first information received\n- Availability Bias: Overweighting easily recalled examples\n- Overconfidence Bias: Excessive certainty in judgment accuracy\n- Sunk Cost Fallacy: Continuing failed approaches due to past investment\n\nGroup Decision Biases:\n- Groupthink: Pressure for harmony reducing critical evaluation\n- Risky Shift: Groups making riskier decisions than individuals\n- Authority Bias: Deferring to hierarchy rather than evidence\n- Social Proof: Following others' decisions without independent analysis\n- Planning Fallacy: Systematic underestimation of time and resources\n\nOrganizational Biases:\n- Status Quo Bias: Preferring current state over change\n- Not Invented Here: Rejecting external ideas and solutions\n- Survivorship Bias: Focusing only on successful cases\n- Attribution Bias: Misattributing success and failure causes\n- Political Bias: Decisions influenced by organizational politics\n```\n\n#### Bias Mitigation Strategies\n```\nSystematic Bias Reduction:\n\nProcess-Based Mitigation:\n- Devil's Advocate: Designated critical evaluation role\n- Red Team Analysis: Systematic challenge of assumptions and conclusions\n- Diverse Perspectives: Multi-functional and multi-level input\n- Anonymous Input: Reducing social pressure and hierarchy effects\n\nTool-Based Mitigation:\n- Decision Trees: Systematic option evaluation and comparison\n- Pre-mortem Analysis: Imagining failure scenarios and prevention\n- Reference Class Forecasting: Using similar historical examples\n- Outside View: External perspective and benchmarking\n\nCultural Mitigation:\n- Psychological Safety: Encouraging dissent and critical thinking\n- Learning Orientation: Celebrating learning from failures\n- Evidence-Based Culture: Valuing data over intuition and politics\n- Continuous Improvement: Regular process assessment and enhancement\n```\n\n### 4. Scenario-Based Decision Testing\n\n**Test decision quality through hypothetical scenarios:**\n\n#### Decision Scenario Framework\n```\nComprehensive Decision Testing:\n\nHistorical Scenario Testing:\n- Apply current decision process to past decisions\n- Compare predicted vs actual outcomes\n- Identify process improvements that would have helped\n- Calibrate decision confidence and accuracy\n\nHypothetical Scenario Testing:\n- Create realistic decision scenarios for practice\n- Test team process under different conditions\n- Identify process strengths and weaknesses\n- Build team decision-making capability\n\nStress Test Scenarios:\n- Time pressure and urgency constraints\n- Incomplete information and high uncertainty\n- Conflicting stakeholder interests and priorities\n- High-stakes decisions with significant consequences\n\nLearning Scenarios:\n- Successful decision analysis and pattern recognition\n- Failed decision post-mortem and lesson extraction\n- Near-miss analysis and improvement identification\n- Best practice sharing and capability transfer\n```\n\n#### Simulation-Based Improvement\n- Role-playing exercises for complex decision scenarios\n- Process experimentation with low-stakes decisions\n- A/B testing of different decision methodologies\n- Scenario planning for future decision situations\n\n### 5. Team Decision Process Optimization\n\n**Systematic improvement of decision-making workflows:**\n\n#### Process Enhancement Framework\n```\nDecision Process Optimization:\n\nInformation Management:\n- Data Collection: Systematic gathering of relevant information\n- Information Quality: Accuracy, completeness, and timeliness assessment\n- Bias Detection: Recognition of information source biases\n- Knowledge Synthesis: Integration of diverse information sources\n\nStakeholder Engagement:\n- Identification: Complete mapping of affected and influential parties\n- Consultation: Systematic input gathering and perspective integration\n- Communication: Clear explanation of process and decision rationale\n- Buy-in: Building support and commitment for implementation\n\nAnalysis and Evaluation:\n- Option Generation: Creative and comprehensive alternative development\n- Criteria Definition: Clear success metrics and evaluation standards\n- Trade-off Analysis: Systematic comparison of costs and benefits\n- Risk Assessment: Identification and mitigation of potential problems\n\nDecision Implementation:\n- Planning: Detailed implementation strategy and timeline\n- Resource Allocation: Appropriate staffing and budget assignment\n- Monitoring: Progress tracking and outcome measurement\n- Adaptation: Course correction based on results and learning\n```\n\n#### Team Capability Building\n- Decision-making skill training and development\n- Process facilitation and meeting effectiveness\n- Critical thinking and analytical capability enhancement\n- Communication and stakeholder management improvement\n\n### 6. Output Generation and Recommendations\n\n**Present decision quality insights in actionable format:**\n\n```\n## Decision Quality Analysis: [Decision Type/Process]\n\n### Current State Assessment\n- Decision Process Maturity: [evaluation of current methodology]\n- Quality Dimension Scores: [process, outcome, timing, learning ratings]\n- Bias Vulnerability: [key cognitive biases affecting decisions]\n- Stakeholder Satisfaction: [feedback on decision process and outcomes]\n\n### Key Findings\n\n#### Decision Process Strengths:\n- Effective Practices: [what works well in current process]\n- Quality Outcomes: [successful decisions and positive patterns]\n- Team Capabilities: [strong skills and effective behaviors]\n- Stakeholder Engagement: [successful involvement and communication]\n\n#### Improvement Opportunities:\n- Process Gaps: [missing steps or inadequate methodology]\n- Bias Vulnerabilities: [cognitive biases affecting decision quality]\n- Information Deficits: [data gaps and analysis weaknesses]\n- Implementation Challenges: [execution and follow-through issues]\n\n### Optimization Recommendations\n\n#### Immediate Improvements (0-30 days):\n- Process Quick Fixes: [simple methodology enhancements]\n- Bias Mitigation: [specific techniques for bias reduction]\n- Tool Implementation: [decision aids and analytical frameworks]\n- Communication Enhancement: [stakeholder engagement improvements]\n\n#### Medium-term Development (1-6 months):\n- Capability Building: [training and skill development programs]\n- Process Standardization: [consistent methodology across decisions]\n- Quality Measurement: [metrics and feedback systems]\n- Cultural Development: [decision-making mindset and values]\n\n#### Long-term Transformation (6+ months):\n- Organizational Learning: [institutional knowledge and capability]\n- Advanced Analytics: [data-driven decision support systems]\n- Innovation Integration: [new methodologies and tools]\n- Competitive Advantage: [decision-making as strategic capability]\n\n### Success Metrics and Monitoring\n- Decision Quality KPIs: [measurable indicators of improvement]\n- Process Efficiency Metrics: [speed and resource utilization]\n- Outcome Tracking: [business results and stakeholder satisfaction]\n- Learning Indicators: [capability development and knowledge capture]\n\n### Implementation Roadmap\n- Phase 1: [immediate process improvements and bias mitigation]\n- Phase 2: [capability building and measurement system]\n- Phase 3: [advanced methodology and cultural transformation]\n- Success Criteria: [specific goals and achievement measures]\n```\n\n### 7. Continuous Learning Integration\n\n**Establish ongoing decision quality improvement:**\n\n#### Decision Outcome Tracking\n- Systematic monitoring of decision results and impacts\n- Correlation analysis between process quality and outcomes\n- Pattern recognition for successful vs unsuccessful decisions\n- Feedback integration for process refinement and enhancement\n\n#### Organizational Learning\n- Best practice identification and knowledge sharing\n- Decision case study development and team learning\n- Cross-functional learning and capability transfer\n- Industry benchmark comparison and competitive analysis\n\n## Usage Examples\n\n```bash\n# Product strategy decision analysis\n/team:decision-quality-analyzer Analyze product roadmap prioritization decisions for bias and process improvement opportunities\n\n# Technical architecture decision assessment\n/team:decision-quality-analyzer Evaluate technology stack decisions using scenario testing and stakeholder satisfaction analysis\n\n# Hiring process decision optimization\n/team:decision-quality-analyzer Optimize candidate evaluation and selection process through bias detection and outcome tracking\n\n# Investment decision quality improvement\n/team:decision-quality-analyzer Improve capital allocation decisions through process standardization and learning integration\n```\n\n## Quality Indicators\n\n- **Green**: Comprehensive bias analysis, validated process improvements, outcome tracking\n- **Yellow**: Basic bias recognition, some process enhancement, limited outcome measurement\n- **Red**: Minimal bias awareness, ad-hoc process, no systematic improvement\n\n## Common Pitfalls to Avoid\n\n- Analysis paralysis: Over-analyzing decisions instead of improving decision-making\n- Bias blindness: Not recognizing team and organizational cognitive biases\n- Process rigidity: Creating inflexible procedures that slow appropriate decisions\n- Outcome fixation: Judging process quality only by outcomes rather than methodology\n- Individual focus: Ignoring group dynamics and organizational factors\n- One-size-fits-all: Using same process for all decision types and contexts\n\nTransform team decision-making from intuition-based guessing into systematic, evidence-driven capability that creates sustainable competitive advantage."
              },
              {
                "name": "/decision-tree-explorer",
                "description": "Explore decision branches with probability weighting, expected value analysis, and scenario-based optimization.",
                "path": "plugins/all-commands/commands/decision-tree-explorer.md",
                "frontmatter": {
                  "description": "Explore decision branches with probability weighting, expected value analysis, and scenario-based optimization.",
                  "category": "simulation-modeling",
                  "argument-hint": "Specify decision tree parameters"
                },
                "content": "# Decision Tree Explorer\n\nExplore decision branches with probability weighting, expected value analysis, and scenario-based optimization.\n\n## Instructions\n\nYou are tasked with creating a comprehensive decision tree analysis to explore complex decision scenarios and optimize choice outcomes. Follow this systematic approach: **$ARGUMENTS**\n\n### 1. Prerequisites Assessment\n\n**Critical Decision Context Validation:**\n\n- **Decision Scope**: What specific decision(s) need to be made?\n- **Stakeholders**: Who will be affected by and involved in this decision?\n- **Time Constraints**: What are the decision deadlines and implementation timelines?\n- **Success Criteria**: How will you measure decision success or failure?\n- **Resource Constraints**: What limitations affect available options?\n\n**If any context is unclear, guide systematically:**\n\n```\nMissing Decision Scope:\n\"I need clarity on the decision you're analyzing. Please specify:\n- Primary Decision: The main choice you need to make\n- Decision Level: Strategic, tactical, or operational\n- Decision Type: Go/no-go, resource allocation, priority ranking, or option selection\n- Alternative Options: What choices are you considering?\n\nExamples:\n- Strategic: 'Should we enter the European market next year?'\n- Investment: 'Which of 3 product features should we build first?'\n- Operational: 'Should we migrate to microservices or improve the monolith?'\n- Crisis: 'How should we respond to the new competitor launch?'\"\n\nMissing Success Criteria:\n\"How will you evaluate if this decision was successful?\n- Financial Metrics: Revenue impact, cost savings, ROI targets\n- Strategic Metrics: Market share, competitive position, capability building\n- Operational Metrics: Efficiency gains, quality improvements, risk reduction\n- Timeline Metrics: Speed to market, implementation time, payback period\"\n\nMissing Resource Context:\n\"What constraints limit your decision options?\n- Budget: Available investment capital and operating funds\n- Time: Implementation deadlines and resource availability windows\n- Capabilities: Team skills, technology infrastructure, operational capacity\n- Regulatory: Compliance requirements and approval processes\"\n```\n\n### 2. Decision Architecture Mapping\n\n**Structure the decision systematically:**\n\n#### Decision Hierarchy\n- Primary decision point and core question\n- Secondary decisions that follow from primary choice\n- Tertiary decisions and implementation details\n- Decision dependencies and sequencing requirements\n- Option combinations and interaction effects\n\n#### Stakeholder Impact Analysis\n- Decision makers and approval authorities\n- Implementation teams and resource owners\n- Customers and end users affected\n- External partners and dependencies\n- Competitive landscape implications\n\n#### Constraint Identification\n- Hard constraints (cannot be violated)\n- Soft constraints (preferences and trade-offs)\n- Temporal constraints (timing and sequencing)\n- Resource constraints (budget, capacity, capabilities)\n- Regulatory and compliance constraints\n\n### 3. Option Generation and Structuring\n\n**Systematically identify and organize decision alternatives:**\n\n#### Comprehensive Option Development\n- Direct approaches to achieving the goal\n- Hybrid solutions combining multiple approaches\n- Phased approaches with incremental implementation\n- Alternative goals that might better serve needs\n- \"Do nothing\" baseline for comparison\n\n#### Option Categorization\n- Quick wins vs. long-term strategic moves\n- High-risk/high-reward vs. safe/incremental options\n- Resource-intensive vs. lean approaches\n- Internal development vs. external partnerships\n- Proven approaches vs. innovative experiments\n\n#### Option Feasibility Assessment\n```\nFor each option, evaluate:\n- Technical Feasibility: Can this actually be implemented?\n- Economic Feasibility: Do benefits justify costs?\n- Operational Feasibility: Do we have capability to execute?\n- Timeline Feasibility: Can this be done in available time?\n- Political Feasibility: Will stakeholders support this?\n\nFeasibility Scoring (1-10 scale):\nOption: [name]\n- Technical: [score] - [reasoning]\n- Economic: [score] - [reasoning]\n- Operational: [score] - [reasoning]\n- Timeline: [score] - [reasoning]\n- Political: [score] - [reasoning]\nOverall Feasibility: [average score]\n```\n\n### 4. Probability Assessment Framework\n\n**Apply systematic probability estimation:**\n\n#### Base Rate Analysis\n- Historical success rates for similar decisions\n- Industry benchmarks and comparative data\n- Expert judgment and domain knowledge\n- Market research and customer validation data\n- Internal capability assessment and track record\n\n#### Scenario Probability Weighting\n- Best case scenario probabilities (optimistic outcomes)\n- Most likely scenario probabilities (base case expectations)\n- Worst case scenario probabilities (pessimistic outcomes)\n- Black swan event probabilities (extreme scenarios)\n- Competitive response probabilities\n\n#### Probability Calibration Methods\n```\nUse multiple estimation approaches:\n\n1. Historical Data Analysis:\n   - Similar past decisions and outcomes\n   - Success/failure rates in comparable situations\n   - Market adoption patterns for similar offerings\n\n2. Expert Consultation:\n   - Domain expert probability estimates\n   - Cross-functional team input and perspectives\n   - External advisor and consultant insights\n\n3. Market Validation:\n   - Customer research and feedback\n   - Competitive analysis and market dynamics\n   - Regulatory and environmental factor assessment\n\n4. Monte Carlo Simulation:\n   - Run multiple probability scenarios\n   - Test sensitivity to assumption changes\n   - Generate confidence intervals for estimates\n```\n\n### 5. Expected Value Calculation\n\n**Quantify decision outcomes systematically:**\n\n#### Outcome Quantification\n- Financial returns and cost implications\n- Strategic value and competitive advantages\n- Risk reduction and option value creation\n- Time savings and efficiency improvements\n- Learning value and capability building\n\n#### Multi-Dimensional Value Assessment\n```\nValue Calculation Framework:\n\nFinancial Value:\n- Direct Revenue Impact: $[amount]  [uncertainty range]\n- Cost Savings: $[amount]  [uncertainty range]\n- Investment Required: $[amount] and timeline\n- NPV Calculation: $[net present value] over [timeframe]\n\nStrategic Value:\n- Market Position Improvement: [qualitative + quantitative]\n- Competitive Advantage Creation: [sustainable differentiation]\n- Capability Building: [new skills and infrastructure]\n- Option Value: [future opportunities enabled]\n\nRisk Value:\n- Risk Reduction: [quantified risk mitigation]\n- Downside Protection: [worst-case scenario costs]\n- Opportunity Cost: [alternative options foregone]\n- Reversibility: [cost and difficulty of changing course]\n```\n\n#### Expected Value Integration\n```\nExpected Value Formula Application:\nEV = (Probability  Outcome Value) for all scenarios\n\nExample Calculation:\nOption A: New Product Launch\n- Best Case (20% probability): $10M revenue, 80% margin = $8M profit\n- Base Case (60% probability): $5M revenue, 70% margin = $3.5M profit  \n- Worst Case (20% probability): $1M revenue, 50% margin = $0.5M profit\n\nExpected Value = (0.20  $8M) + (0.60  $3.5M) + (0.20  $0.5M)\n= $1.6M + $2.1M + $0.1M = $3.8M\n\nInvestment Required: $2M\nNet Expected Value: $1.8M\n```\n\n### 6. Risk Analysis and Sensitivity Testing\n\n**Comprehensively assess decision risks:**\n\n#### Risk Identification Matrix\n- Implementation risks (execution challenges)\n- Market risks (demand, competition, economic changes)\n- Technology risks (technical feasibility, obsolescence)\n- Regulatory risks (compliance, approval, policy changes)\n- Resource risks (availability, capability, cost overruns)\n\n#### Sensitivity Analysis\n- Key assumption stress testing\n- Break-even analysis for critical variables\n- Scenario analysis with parameter variations\n- Confidence interval calculation for outcomes\n- Robustness testing across different conditions\n\n#### Risk Mitigation Strategy Development\n```\nRisk Mitigation Framework:\n\nFor each significant risk:\n1. Risk Description: [specific risk scenario]\n2. Probability Assessment: [likelihood of occurrence]\n3. Impact Assessment: [severity if it occurs]\n4. Early Warning Indicators: [signals to watch for]\n5. Prevention Strategies: [actions to reduce probability]\n6. Mitigation Strategies: [actions to reduce impact]\n7. Contingency Plans: [responses if risk materializes]\n8. Risk Ownership: [who monitors and responds]\n```\n\n### 7. Decision Tree Visualization and Analysis\n\n**Create clear decision tree representations:**\n\n#### Tree Structure Design\n```\nDecision Tree Format:\n\n[Decision Point] \n Option A [probability: X%]\n    Scenario A1 [probability: Y%]  Outcome: $Z\n    Scenario A2 [probability: Y%]  Outcome: $Z\n    Scenario A3 [probability: Y%]  Outcome: $Z\n Option B [probability: X%]\n    Scenario B1 [probability: Y%]  Outcome: $Z\n    Scenario B2 [probability: Y%]  Outcome: $Z\n Option C [probability: X%]\n     Scenario C1 [probability: Y%]  Outcome: $Z\n\nExpected Values:\n- Option A: $[calculated EV]\n- Option B: $[calculated EV]  \n- Option C: $[calculated EV]\n```\n\n#### Decision Path Analysis\n- Optimal path identification based on expected value\n- Alternative paths with acceptable risk/return profiles\n- Contingency routing based on early decision outcomes\n- Information value analysis (worth of additional research)\n- Real option valuation (value of delaying decisions)\n\n### 8. Optimization and Recommendation Engine\n\n**Generate data-driven decision recommendations:**\n\n#### Multi-Criteria Decision Analysis\n- Weighted scoring across multiple decision criteria\n- Trade-off analysis between competing objectives\n- Pareto frontier identification for efficient solutions\n- Stakeholder preference integration\n- Scenario robustness across different weighting schemes\n\n#### Recommendation Generation\n```\nDecision Recommendation Format:\n\n## Primary Recommendation: [Selected Option]\n\n### Executive Summary\n- Recommended Decision: [specific choice and rationale]\n- Expected Value: $[amount] with [confidence level]%\n- Key Success Factors: [critical requirements for success]\n- Major Risks: [primary concerns and mitigation approaches]\n- Implementation Timeline: [key milestones and dependencies]\n\n### Supporting Analysis\n- Expected Value Calculation: [detailed breakdown]\n- Probability Assessments: [key assumptions and sources]\n- Risk Assessment: [major risks and mitigation strategies]\n- Sensitivity Analysis: [critical variables and break-even points]\n- Alternative Options: [other viable choices and trade-offs]\n\n### Implementation Guidance\n- Immediate Next Steps: [specific actions required]\n- Success Metrics: [measurable indicators of progress]\n- Decision Points: [future choice points and triggers]\n- Resource Requirements: [budget, team, timeline needs]\n- Stakeholder Communication: [alignment and buy-in strategies]\n\n### Contingency Planning\n- Plan B Options: [alternative approaches if primary fails]\n- Early Warning Systems: [risk monitoring and triggers]\n- Decision Reversal: [exit strategies and switching costs]\n- Adaptive Strategies: [adjustment mechanisms for changing conditions]\n```\n\n### 9. Decision Quality Validation\n\n**Ensure robust decision-making process:**\n\n#### Process Quality Checklist\n- [ ] All relevant stakeholders consulted\n- [ ] Comprehensive option generation completed\n- [ ] Probability assessments calibrated with data\n- [ ] Value calculations include all material factors\n- [ ] Risks identified and mitigation planned\n- [ ] Assumptions explicitly documented and tested\n- [ ] Decision criteria clearly defined and weighted\n- [ ] Implementation feasibility validated\n\n#### Bias Detection and Mitigation\n- Confirmation bias: Seeking information that supports preferences\n- Anchoring bias: Over-relying on first information received\n- Availability bias: Overweighting easily recalled examples\n- Optimism bias: Overestimating positive outcomes\n- Sunk cost fallacy: Continuing failed approaches\n- Analysis paralysis: Over-analyzing instead of deciding\n\n#### Decision Documentation\n- Decision rationale and supporting analysis\n- Key assumptions and probability assessments\n- Alternative options considered and rejected\n- Stakeholder input and consultation process\n- Risk assessment and mitigation strategies\n- Implementation plan and success metrics\n\n### 10. Learning and Feedback Integration\n\n**Establish decision quality improvement:**\n\n#### Decision Outcome Tracking\n- Actual vs. predicted outcomes measurement\n- Assumption validation against real results\n- Decision timing and implementation effectiveness\n- Stakeholder satisfaction and support levels\n- Unintended consequences and side effects\n\n#### Continuous Improvement\n- Decision-making process refinement\n- Probability calibration improvement over time\n- Risk assessment accuracy enhancement\n- Stakeholder engagement optimization\n- Tool and framework evolution\n\n## Usage Examples\n\n```bash\n# Strategic business decision\n/simulation:decision-tree-explorer Should we acquire competitor X for $50M or build competing product internally?\n\n# Product development prioritization\n/simulation:decision-tree-explorer Which of 5 product features should we build first given limited engineering resources?\n\n# Technology architecture choice\n/simulation:decision-tree-explorer Microservices vs monolith architecture for our new platform?\n\n# Market expansion decision\n/simulation:decision-tree-explorer European market entry strategy: direct expansion vs partnership vs acquisition?\n```\n\n## Quality Indicators\n\n- **Green**: Comprehensive options, calibrated probabilities, quantified outcomes, documented assumptions\n- **Yellow**: Good option coverage, reasonable probability estimates, partially quantified outcomes\n- **Red**: Limited options, uncalibrated probabilities, qualitative-only outcomes\n\n## Common Pitfalls to Avoid\n\n- Analysis paralysis: Over-analyzing instead of making timely decisions\n- False precision: Using precise numbers for uncertain estimates  \n- Option tunnel vision: Not considering creative alternatives\n- Probability miscalibration: Overconfidence in likelihood estimates\n- Value tunnel vision: Focusing only on financial outcomes\n- Implementation blindness: Not considering execution challenges\n\nTransform complex decisions into systematic analysis for exponentially better choice outcomes."
              },
              {
                "name": "/dependency-audit",
                "description": "Audit dependencies for security vulnerabilities",
                "path": "plugins/all-commands/commands/dependency-audit.md",
                "frontmatter": {
                  "description": "Audit dependencies for security vulnerabilities",
                  "category": "security-audit"
                },
                "content": "# Dependency Audit Command\n\nAudit dependencies for security vulnerabilities\n\n## Instructions\n\nPerform a comprehensive dependency audit following these steps:\n\n1. **Dependency Discovery**\n   - Identify all dependency management files (package.json, requirements.txt, Cargo.toml, pom.xml, etc.)\n   - Map direct vs transitive dependencies\n   - Check for lock files and version consistency\n   - Review development vs production dependencies\n\n2. **Version Analysis**\n   - Check for outdated packages and available updates\n   - Identify packages with major version updates available\n   - Review semantic versioning compliance\n   - Analyze version pinning strategies\n\n3. **Security Vulnerability Scan**\n   - Run security audits using appropriate tools:\n     - `npm audit` for Node.js projects\n     - `pip-audit` for Python projects\n     - `cargo audit` for Rust projects\n     - GitHub security advisories for all platforms\n   - Identify critical, high, medium, and low severity vulnerabilities\n   - Check for known exploits and CVE references\n\n4. **License Compliance**\n   - Review all dependency licenses for compatibility\n   - Identify restrictive licenses (GPL, AGPL, etc.)\n   - Check for license conflicts with project license\n   - Document license obligations and requirements\n\n5. **Dependency Health Assessment**\n   - Check package maintenance status and activity\n   - Review contributor count and community support\n   - Analyze release frequency and stability\n   - Identify abandoned or deprecated packages\n\n6. **Size and Performance Impact**\n   - Analyze bundle size impact of each dependency\n   - Identify large dependencies that could be optimized\n   - Check for duplicate functionality across dependencies\n   - Review tree-shaking and dead code elimination effectiveness\n\n7. **Alternative Analysis**\n   - Identify dependencies with better alternatives\n   - Check for lighter or more efficient replacements\n   - Analyze feature overlap and consolidation opportunities\n   - Review native alternatives (built-in functions vs libraries)\n\n8. **Dependency Conflicts**\n   - Check for version conflicts between dependencies\n   - Identify peer dependency issues\n   - Review dependency resolution strategies\n   - Analyze potential breaking changes in updates\n\n9. **Build and Development Impact**\n   - Review dependencies that affect build times\n   - Check for development-only dependencies in production\n   - Analyze tooling dependencies and alternatives\n   - Review optional dependencies and their necessity\n\n10. **Supply Chain Security**\n    - Check for typosquatting and malicious packages\n    - Review package authenticity and signatures\n    - Analyze dependency sources and registries\n    - Check for suspicious or unusual dependencies\n\n11. **Update Strategy Planning**\n    - Create a prioritized update plan based on security and stability\n    - Identify breaking changes and required code modifications\n    - Plan for testing strategy during updates\n    - Document rollback procedures for problematic updates\n\n12. **Monitoring and Automation**\n    - Set up automated dependency scanning\n    - Configure security alerts and notifications\n    - Review dependency update automation tools\n    - Establish regular audit schedules\n\n13. **Documentation and Reporting**\n    - Create a comprehensive dependency inventory\n    - Document all security findings with remediation steps\n    - Provide update recommendations with priority levels\n    - Generate executive summary for stakeholders\n\nUse platform-specific tools and databases for the most accurate results. Focus on actionable recommendations with clear risk assessments."
              },
              {
                "name": "/dependency-mapper",
                "description": "Map and analyze project dependencies",
                "path": "plugins/all-commands/commands/dependency-mapper.md",
                "frontmatter": {
                  "description": "Map and analyze project dependencies",
                  "category": "team-collaboration"
                },
                "content": "# dependency-mapper\n\nMap and analyze project dependencies\n\n## Purpose\nThis command analyzes code dependencies, git history, and Linear tasks to create visual dependency maps. It helps identify blockers, circular dependencies, and optimal task ordering for efficient project execution.\n\n## Usage\n```bash\n# Map dependencies for a specific Linear task\nclaude \"Show dependency map for task LIN-123\"\n\n# Analyze code dependencies in a module\nclaude \"Map dependencies for src/auth module\"\n\n# Find circular dependencies in the project\nclaude \"Check for circular dependencies in the codebase\"\n\n# Generate task execution order\nclaude \"What's the optimal order to complete tasks in sprint SPR-45?\"\n```\n\n## Instructions\n\n### 1. Analyze Code Dependencies\nUse various techniques to identify dependencies:\n\n```bash\n# Find import statements (JavaScript/TypeScript)\nrg \"^import.*from ['\\\"](\\.\\.?/[^'\\\"]+)\" --type ts --type js -o | sort | uniq\n\n# Find require statements (Node.js)\nrg \"require\\(['\\\"](\\.\\.?/[^'\\\"]+)['\\\"]\" --type js -o\n\n# Analyze Python imports\nrg \"^from \\S+ import|^import \\S+\" --type py\n\n# Find module references in comments\nrg \"TODO.*depends on|FIXME.*requires|NOTE.*needs\" -i\n```\n\n### 2. Extract Task Dependencies from Linear\nQuery Linear for task relationships:\n\n```javascript\n// Get task with its dependencies\nconst task = await linear.getTask(taskId, {\n  include: ['blockedBy', 'blocks', 'parent', 'children']\n});\n\n// Find mentions in task descriptions\nconst mentions = task.description.match(/(?:LIN-|#)\\d+/g);\n\n// Get related tasks from same epic/project\nconst relatedTasks = await linear.searchTasks({\n  projectId: task.projectId,\n  includeArchived: false\n});\n```\n\n### 3. Build Dependency Graph\nCreate a graph structure:\n\n```javascript\nclass DependencyGraph {\n  constructor() {\n    this.nodes = new Map(); // taskId -> task details\n    this.edges = new Map(); // taskId -> Set of dependent taskIds\n  }\n  \n  addDependency(from, to, type = 'blocks') {\n    if (!this.edges.has(from)) {\n      this.edges.set(from, new Set());\n    }\n    this.edges.get(from).add({ to, type });\n  }\n  \n  findCycles() {\n    const visited = new Set();\n    const recursionStack = new Set();\n    const cycles = [];\n    \n    const hasCycle = (node, path = []) => {\n      visited.add(node);\n      recursionStack.add(node);\n      path.push(node);\n      \n      const neighbors = this.edges.get(node) || new Set();\n      for (const { to } of neighbors) {\n        if (!visited.has(to)) {\n          if (hasCycle(to, [...path])) return true;\n        } else if (recursionStack.has(to)) {\n          // Found cycle\n          const cycleStart = path.indexOf(to);\n          cycles.push(path.slice(cycleStart));\n        }\n      }\n      \n      recursionStack.delete(node);\n      return false;\n    };\n    \n    for (const node of this.nodes.keys()) {\n      if (!visited.has(node)) {\n        hasCycle(node);\n      }\n    }\n    \n    return cycles;\n  }\n  \n  topologicalSort() {\n    const inDegree = new Map();\n    const queue = [];\n    const result = [];\n    \n    // Calculate in-degrees\n    for (const [node] of this.nodes) {\n      inDegree.set(node, 0);\n    }\n    \n    for (const [_, edges] of this.edges) {\n      for (const { to } of edges) {\n        inDegree.set(to, (inDegree.get(to) || 0) + 1);\n      }\n    }\n    \n    // Find nodes with no dependencies\n    for (const [node, degree] of inDegree) {\n      if (degree === 0) queue.push(node);\n    }\n    \n    // Process queue\n    while (queue.length > 0) {\n      const node = queue.shift();\n      result.push(node);\n      \n      const edges = this.edges.get(node) || new Set();\n      for (const { to } of edges) {\n        inDegree.set(to, inDegree.get(to) - 1);\n        if (inDegree.get(to) === 0) {\n          queue.push(to);\n        }\n      }\n    }\n    \n    return result;\n  }\n}\n```\n\n### 4. Generate Visual Representations\n\n#### ASCII Tree View\n```\nLIN-123: Authentication System\n LIN-124: User Model [DONE]\n LIN-125: JWT Implementation [IN PROGRESS]\n   LIN-126: Token Refresh Logic [BLOCKED]\n LIN-127: Login Endpoint [TODO]\n    LIN-128: Rate Limiting [TODO]\n    LIN-129: 2FA Support [TODO]\n```\n\n#### Mermaid Diagram\n```mermaid\ngraph TD\n    LIN-123[Authentication System] --> LIN-124[User Model]\n    LIN-123 --> LIN-125[JWT Implementation]\n    LIN-123 --> LIN-127[Login Endpoint]\n    LIN-125 --> LIN-126[Token Refresh Logic]\n    LIN-127 --> LIN-128[Rate Limiting]\n    LIN-127 --> LIN-129[2FA Support]\n    \n    style LIN-124 fill:#90EE90\n    style LIN-125 fill:#FFD700\n    style LIN-126 fill:#FF6B6B\n```\n\n#### Dependency Matrix\n```\n         | LIN-123 | LIN-124 | LIN-125 | LIN-126 | LIN-127 |\n---------|---------|---------|---------|---------|---------|\nLIN-123  |    -    |        |        |         |        |\nLIN-124  |         |    -    |         |         |         |\nLIN-125  |         |        |    -    |        |         |\nLIN-126  |         |         |        |    -    |         |\nLIN-127  |        |        |         |         |    -    |\n\nLegend:  depends on,  is dependency of\n```\n\n### 5. Analyze File Dependencies\nMap code structure to tasks:\n\n```javascript\n// Analyze file imports\nasync function analyzeFileDependencies(filePath) {\n  const content = await readFile(filePath);\n  const imports = extractImports(content);\n  \n  const dependencies = {\n    internal: [], // Project files\n    external: [], // npm packages\n    tasks: []     // Related Linear tasks\n  };\n  \n  for (const imp of imports) {\n    if (imp.startsWith('.')) {\n      dependencies.internal.push(resolveImportPath(filePath, imp));\n    } else {\n      dependencies.external.push(imp);\n    }\n    \n    // Check if file is mentioned in any task\n    const tasks = await linear.searchTasks(path.basename(filePath));\n    dependencies.tasks.push(...tasks);\n  }\n  \n  return dependencies;\n}\n```\n\n### 6. Generate Execution Order\nCalculate optimal task sequence:\n\n```javascript\nfunction calculateExecutionOrder(graph) {\n  const order = graph.topologicalSort();\n  const taskDetails = [];\n  \n  for (const taskId of order) {\n    const task = graph.nodes.get(taskId);\n    const dependencies = Array.from(graph.edges.get(taskId) || [])\n      .map(({ to }) => to);\n    \n    taskDetails.push({\n      id: taskId,\n      title: task.title,\n      estimate: task.estimate || 0,\n      dependencies,\n      assignee: task.assignee,\n      criticalPath: isOnCriticalPath(taskId, graph)\n    });\n  }\n  \n  return taskDetails;\n}\n```\n\n### 7. Error Handling\n```javascript\n// Check for Linear access\nif (!linear.available) {\n  console.warn(\"Linear MCP not available, using code analysis only\");\n  // Fall back to code-only analysis\n}\n\n// Handle circular dependencies\nconst cycles = graph.findCycles();\nif (cycles.length > 0) {\n  console.error(\"Circular dependencies detected:\");\n  cycles.forEach(cycle => {\n    console.error(`  ${cycle.join('  ')}  ${cycle[0]}`);\n  });\n}\n\n// Validate task existence\nfor (const taskId of mentionedTasks) {\n  try {\n    await linear.getTask(taskId);\n  } catch (error) {\n    console.warn(`Task ${taskId} not found or inaccessible`);\n  }\n}\n```\n\n## Example Output\n\n```\nAnalyzing dependencies for Epic: Authentication System (LIN-123)\n\n Dependency Graph:\n\n\nLIN-123: Authentication System [EPIC]\n LIN-124: Create User Model  [DONE]\n   Files: src/models/User.ts, src/schemas/user.sql\n LIN-125: Implement JWT Service  [IN PROGRESS]\n   Files: src/services/auth/jwt.ts\n   Depends on: LIN-124\n   LIN-126: Add Token Refresh  [BLOCKED by LIN-125]\n LIN-127: Create Login Endpoint  [TODO]\n    Files: src/routes/auth/login.ts\n    Depends on: LIN-124, LIN-125\n    LIN-128: Add Rate Limiting  [TODO]\n    LIN-129: Implement 2FA  [TODO]\n\n Circular Dependencies: None found\n\n Critical Path:\n1. LIN-124 (User Model) - 2 points \n2. LIN-125 (JWT Service) - 3 points \n3. LIN-126 (Token Refresh) - 1 point \n4. LIN-127 (Login Endpoint) - 2 points \nTotal: 8 points on critical path\n\n Task Distribution:\n- Alice: LIN-125 (in progress), LIN-126 (blocked)\n- Bob: LIN-127 (ready to start)\n- Unassigned: LIN-128, LIN-129\n\n File Dependencies:\nsrc/routes/auth/login.ts\n   imports from:\n      src/models/User.ts (LIN-124) \n      src/services/auth/jwt.ts (LIN-125) \n      src/middleware/rateLimiter.ts (LIN-128) \n\n Recommended Action:\nPriority should be completing LIN-125 to unblock 3 dependent tasks.\nBob can start on LIN-124 prerequisite work while waiting.\n```\n\n## Advanced Features\n\n### Impact Analysis\nShow what tasks are affected by changes:\n```bash\n# What tasks are impacted if we change User.ts?\nclaude \"Show impact analysis for changes to src/models/User.ts\"\n```\n\n### Sprint Planning\nOptimize task order for sprint capacity:\n```bash\n# Generate sprint plan considering dependencies\nclaude \"Plan sprint with 20 points capacity considering dependencies\"\n```\n\n### Risk Assessment\nIdentify high-risk dependency chains:\n```bash\n# Find longest dependency chains\nclaude \"Show tasks with longest dependency chains in current sprint\"\n```\n\n## Tips\n- Update dependencies as code evolves\n- Use consistent naming between code modules and tasks\n- Mark external dependencies (APIs, services) explicitly\n- Review dependency graphs in sprint planning\n- Keep critical path tasks assigned and monitored\n- Use dependency data for accurate sprint velocity"
              },
              {
                "name": "/design-database-schema",
                "description": "Design optimized database schemas",
                "path": "plugins/all-commands/commands/design-database-schema.md",
                "frontmatter": {
                  "description": "Design optimized database schemas",
                  "category": "database-operations"
                },
                "content": "# Design Database Schema\n\nDesign optimized database schemas\n\n## Instructions\n\n1. **Requirements Analysis and Data Modeling**\n   - Analyze business requirements and data relationships\n   - Identify entities, attributes, and relationships\n   - Define data types, constraints, and validation rules\n   - Plan for scalability and future requirements\n   - Consider data access patterns and query requirements\n\n2. **Entity Relationship Design**\n   - Create comprehensive entity relationship diagrams:\n\n   **User Management Schema:**\n   ```sql\n   -- Users table with proper indexing and constraints\n   CREATE TABLE users (\n     id BIGSERIAL PRIMARY KEY,\n     email VARCHAR(255) UNIQUE NOT NULL,\n     username VARCHAR(50) UNIQUE NOT NULL,\n     password_hash VARCHAR(255) NOT NULL,\n     first_name VARCHAR(100) NOT NULL,\n     last_name VARCHAR(100) NOT NULL,\n     phone VARCHAR(20),\n     date_of_birth DATE,\n     email_verified BOOLEAN DEFAULT FALSE,\n     phone_verified BOOLEAN DEFAULT FALSE,\n     status user_status DEFAULT 'active',\n     created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n     updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n     last_login_at TIMESTAMP WITH TIME ZONE,\n     deleted_at TIMESTAMP WITH TIME ZONE,\n     \n     -- Constraints\n     CONSTRAINT users_email_format CHECK (email ~* '^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}$'),\n     CONSTRAINT users_username_format CHECK (username ~* '^[a-zA-Z0-9_]{3,50}$'),\n     CONSTRAINT users_names_not_empty CHECK (LENGTH(TRIM(first_name)) > 0 AND LENGTH(TRIM(last_name)) > 0)\n   );\n\n   -- User status enum\n   CREATE TYPE user_status AS ENUM ('active', 'inactive', 'suspended', 'pending_verification');\n\n   -- User profiles table for extended information\n   CREATE TABLE user_profiles (\n     user_id BIGINT PRIMARY KEY REFERENCES users(id) ON DELETE CASCADE,\n     avatar_url VARCHAR(500),\n     bio TEXT,\n     website VARCHAR(255),\n     location VARCHAR(255),\n     timezone VARCHAR(50) DEFAULT 'UTC',\n     language VARCHAR(10) DEFAULT 'en',\n     notification_preferences JSONB DEFAULT '{}',\n     privacy_settings JSONB DEFAULT '{}',\n     created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n     updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP\n   );\n\n   -- User roles and permissions\n   CREATE TABLE roles (\n     id SERIAL PRIMARY KEY,\n     name VARCHAR(50) UNIQUE NOT NULL,\n     description TEXT,\n     permissions JSONB DEFAULT '[]',\n     created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP\n   );\n\n   CREATE TABLE user_roles (\n     user_id BIGINT REFERENCES users(id) ON DELETE CASCADE,\n     role_id INTEGER REFERENCES roles(id) ON DELETE CASCADE,\n     assigned_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n     assigned_by BIGINT REFERENCES users(id),\n     PRIMARY KEY (user_id, role_id)\n   );\n   ```\n\n   **E-commerce Schema Example:**\n   ```sql\n   -- Categories with hierarchical structure\n   CREATE TABLE categories (\n     id SERIAL PRIMARY KEY,\n     name VARCHAR(255) NOT NULL,\n     slug VARCHAR(255) UNIQUE NOT NULL,\n     description TEXT,\n     parent_id INTEGER REFERENCES categories(id),\n     sort_order INTEGER DEFAULT 0,\n     is_active BOOLEAN DEFAULT TRUE,\n     meta_title VARCHAR(255),\n     meta_description TEXT,\n     created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n     updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP\n   );\n\n   -- Products table with comprehensive attributes\n   CREATE TABLE products (\n     id BIGSERIAL PRIMARY KEY,\n     name VARCHAR(255) NOT NULL,\n     slug VARCHAR(255) UNIQUE NOT NULL,\n     sku VARCHAR(100) UNIQUE NOT NULL,\n     description TEXT,\n     short_description TEXT,\n     price DECIMAL(10,2) NOT NULL CHECK (price >= 0),\n     compare_price DECIMAL(10,2) CHECK (compare_price >= price),\n     cost_price DECIMAL(10,2) CHECK (cost_price >= 0),\n     weight DECIMAL(8,2),\n     dimensions JSONB, -- {length: x, width: y, height: z, unit: 'cm'}\n     category_id INTEGER REFERENCES categories(id),\n     brand_id INTEGER REFERENCES brands(id),\n     vendor_id BIGINT REFERENCES vendors(id),\n     status product_status DEFAULT 'draft',\n     visibility product_visibility DEFAULT 'visible',\n     inventory_tracking BOOLEAN DEFAULT TRUE,\n     inventory_quantity INTEGER DEFAULT 0,\n     low_stock_threshold INTEGER DEFAULT 5,\n     allow_backorder BOOLEAN DEFAULT FALSE,\n     requires_shipping BOOLEAN DEFAULT TRUE,\n     is_digital BOOLEAN DEFAULT FALSE,\n     tax_class VARCHAR(50) DEFAULT 'standard',\n     featured BOOLEAN DEFAULT FALSE,\n     tags TEXT[],\n     attributes JSONB DEFAULT '{}',\n     seo_title VARCHAR(255),\n     seo_description TEXT,\n     created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n     updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n     published_at TIMESTAMP WITH TIME ZONE,\n     \n     -- Full text search\n     search_vector tsvector GENERATED ALWAYS AS (\n       to_tsvector('english', COALESCE(name, '') || ' ' || COALESCE(description, '') || ' ' || COALESCE(sku, ''))\n     ) STORED\n   );\n\n   -- Product status and visibility enums\n   CREATE TYPE product_status AS ENUM ('draft', 'active', 'inactive', 'archived');\n   CREATE TYPE product_visibility AS ENUM ('visible', 'hidden', 'catalog_only', 'search_only');\n\n   -- Orders table with comprehensive tracking\n   CREATE TABLE orders (\n     id BIGSERIAL PRIMARY KEY,\n     order_number VARCHAR(50) UNIQUE NOT NULL,\n     user_id BIGINT REFERENCES users(id),\n     status order_status DEFAULT 'pending',\n     currency CHAR(3) DEFAULT 'USD',\n     subtotal DECIMAL(10,2) NOT NULL DEFAULT 0,\n     tax_total DECIMAL(10,2) NOT NULL DEFAULT 0,\n     shipping_total DECIMAL(10,2) NOT NULL DEFAULT 0,\n     discount_total DECIMAL(10,2) NOT NULL DEFAULT 0,\n     total DECIMAL(10,2) NOT NULL DEFAULT 0,\n     \n     -- Billing information\n     billing_first_name VARCHAR(100),\n     billing_last_name VARCHAR(100),\n     billing_company VARCHAR(255),\n     billing_address_line_1 VARCHAR(255),\n     billing_address_line_2 VARCHAR(255),\n     billing_city VARCHAR(100),\n     billing_state VARCHAR(100),\n     billing_postal_code VARCHAR(20),\n     billing_country CHAR(2),\n     billing_phone VARCHAR(20),\n     \n     -- Shipping information\n     shipping_first_name VARCHAR(100),\n     shipping_last_name VARCHAR(100),\n     shipping_company VARCHAR(255),\n     shipping_address_line_1 VARCHAR(255),\n     shipping_address_line_2 VARCHAR(255),\n     shipping_city VARCHAR(100),\n     shipping_state VARCHAR(100),\n     shipping_postal_code VARCHAR(20),\n     shipping_country CHAR(2),\n     shipping_phone VARCHAR(20),\n     shipping_method VARCHAR(100),\n     tracking_number VARCHAR(255),\n     \n     notes TEXT,\n     internal_notes TEXT,\n     created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n     updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n     shipped_at TIMESTAMP WITH TIME ZONE,\n     delivered_at TIMESTAMP WITH TIME ZONE\n   );\n\n   CREATE TYPE order_status AS ENUM (\n     'pending', 'processing', 'shipped', 'delivered', \n     'cancelled', 'refunded', 'on_hold'\n   );\n\n   -- Order items with detailed tracking\n   CREATE TABLE order_items (\n     id BIGSERIAL PRIMARY KEY,\n     order_id BIGINT REFERENCES orders(id) ON DELETE CASCADE,\n     product_id BIGINT REFERENCES products(id),\n     product_variant_id BIGINT REFERENCES product_variants(id),\n     quantity INTEGER NOT NULL CHECK (quantity > 0),\n     unit_price DECIMAL(10,2) NOT NULL,\n     total_price DECIMAL(10,2) NOT NULL,\n     product_name VARCHAR(255) NOT NULL, -- Snapshot at time of order\n     product_sku VARCHAR(100), -- Snapshot at time of order\n     product_attributes JSONB, -- Snapshot of selected variants\n     created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP\n   );\n   ```\n\n3. **Advanced Schema Patterns**\n   - Implement complex data patterns:\n\n   **Audit Trail Pattern:**\n   ```sql\n   -- Generic audit trail for tracking all changes\n   CREATE TABLE audit_log (\n     id BIGSERIAL PRIMARY KEY,\n     table_name VARCHAR(255) NOT NULL,\n     record_id BIGINT NOT NULL,\n     operation audit_operation NOT NULL,\n     old_values JSONB,\n     new_values JSONB,\n     changed_fields TEXT[],\n     user_id BIGINT REFERENCES users(id),\n     ip_address INET,\n     user_agent TEXT,\n     created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n     \n     -- Index for efficient querying\n     INDEX idx_audit_log_table_record (table_name, record_id),\n     INDEX idx_audit_log_user_time (user_id, created_at),\n     INDEX idx_audit_log_operation_time (operation, created_at)\n   );\n\n   CREATE TYPE audit_operation AS ENUM ('INSERT', 'UPDATE', 'DELETE');\n\n   -- Trigger function for automatic audit logging\n   CREATE OR REPLACE FUNCTION audit_trigger_function()\n   RETURNS TRIGGER AS $$\n   DECLARE\n     old_data JSONB;\n     new_data JSONB;\n     changed_fields TEXT[];\n   BEGIN\n     IF TG_OP = 'DELETE' THEN\n       old_data = to_jsonb(OLD);\n       INSERT INTO audit_log (table_name, record_id, operation, old_values, user_id)\n       VALUES (TG_TABLE_NAME, OLD.id, 'DELETE', old_data, COALESCE(current_setting('app.current_user_id', true)::BIGINT, NULL));\n       RETURN OLD;\n     ELSIF TG_OP = 'UPDATE' THEN\n       old_data = to_jsonb(OLD);\n       new_data = to_jsonb(NEW);\n       \n       -- Find changed fields\n       SELECT array_agg(key) INTO changed_fields\n       FROM jsonb_each(old_data) \n       WHERE key IN (SELECT key FROM jsonb_each(new_data))\n       AND value IS DISTINCT FROM (new_data->key);\n       \n       INSERT INTO audit_log (table_name, record_id, operation, old_values, new_values, changed_fields, user_id)\n       VALUES (TG_TABLE_NAME, NEW.id, 'UPDATE', old_data, new_data, changed_fields, COALESCE(current_setting('app.current_user_id', true)::BIGINT, NULL));\n       RETURN NEW;\n     ELSIF TG_OP = 'INSERT' THEN\n       new_data = to_jsonb(NEW);\n       INSERT INTO audit_log (table_name, record_id, operation, new_values, user_id)\n       VALUES (TG_TABLE_NAME, NEW.id, 'INSERT', new_data, COALESCE(current_setting('app.current_user_id', true)::BIGINT, NULL));\n       RETURN NEW;\n     END IF;\n     RETURN NULL;\n   END;\n   $$ LANGUAGE plpgsql;\n   ```\n\n   **Soft Delete Pattern:**\n   ```sql\n   -- Add soft delete to any table\n   ALTER TABLE users ADD COLUMN deleted_at TIMESTAMP WITH TIME ZONE;\n   ALTER TABLE products ADD COLUMN deleted_at TIMESTAMP WITH TIME ZONE;\n\n   -- Create views that exclude soft-deleted records\n   CREATE VIEW active_users AS\n   SELECT * FROM users WHERE deleted_at IS NULL;\n\n   CREATE VIEW active_products AS\n   SELECT * FROM products WHERE deleted_at IS NULL;\n\n   -- Soft delete function\n   CREATE OR REPLACE FUNCTION soft_delete(table_name TEXT, record_id BIGINT)\n   RETURNS VOID AS $$\n   BEGIN\n     EXECUTE format('UPDATE %I SET deleted_at = CURRENT_TIMESTAMP WHERE id = $1 AND deleted_at IS NULL', table_name)\n     USING record_id;\n   END;\n   $$ LANGUAGE plpgsql;\n\n   -- Restore function\n   CREATE OR REPLACE FUNCTION restore_deleted(table_name TEXT, record_id BIGINT)\n   RETURNS VOID AS $$\n   BEGIN\n     EXECUTE format('UPDATE %I SET deleted_at = NULL WHERE id = $1', table_name)\n     USING record_id;\n   END;\n   $$ LANGUAGE plpgsql;\n   ```\n\n4. **Performance Optimization Schema Design**\n   - Design for optimal query performance:\n\n   **Strategic Indexing:**\n   ```sql\n   -- Single column indexes for frequently queried fields\n   CREATE INDEX CONCURRENTLY idx_users_email ON users(email);\n   CREATE INDEX CONCURRENTLY idx_users_username ON users(username);\n   CREATE INDEX CONCURRENTLY idx_users_status ON users(status) WHERE status != 'active';\n   CREATE INDEX CONCURRENTLY idx_users_created_at ON users(created_at);\n\n   -- Composite indexes for common query patterns\n   CREATE INDEX CONCURRENTLY idx_products_category_status \n   ON products(category_id, status) WHERE status = 'active';\n\n   CREATE INDEX CONCURRENTLY idx_products_featured_category \n   ON products(featured, category_id) WHERE featured = true AND status = 'active';\n\n   CREATE INDEX CONCURRENTLY idx_orders_user_status_date \n   ON orders(user_id, status, created_at);\n\n   -- Partial indexes for specific conditions\n   CREATE INDEX CONCURRENTLY idx_products_low_stock \n   ON products(inventory_quantity) \n   WHERE inventory_tracking = true AND inventory_quantity <= low_stock_threshold;\n\n   -- Functional indexes for text search and computed values\n   CREATE INDEX CONCURRENTLY idx_products_search_vector \n   ON products USING gin(search_vector);\n\n   CREATE INDEX CONCURRENTLY idx_users_full_name_lower \n   ON users(lower(first_name || ' ' || last_name));\n\n   -- JSON/JSONB indexes for flexible data\n   CREATE INDEX CONCURRENTLY idx_user_profiles_notifications \n   ON user_profiles USING gin(notification_preferences);\n\n   CREATE INDEX CONCURRENTLY idx_products_attributes \n   ON products USING gin(attributes);\n   ```\n\n   **Partitioning Strategy:**\n   ```sql\n   -- Partition large tables by date for better performance\n   CREATE TABLE orders_partitioned (\n     LIKE orders INCLUDING ALL\n   ) PARTITION BY RANGE (created_at);\n\n   -- Create monthly partitions\n   CREATE TABLE orders_2024_01 PARTITION OF orders_partitioned\n   FOR VALUES FROM ('2024-01-01') TO ('2024-02-01');\n\n   CREATE TABLE orders_2024_02 PARTITION OF orders_partitioned\n   FOR VALUES FROM ('2024-02-01') TO ('2024-03-01');\n\n   -- Automatic partition management\n   CREATE OR REPLACE FUNCTION create_monthly_partitions(\n     table_name TEXT,\n     start_date DATE,\n     end_date DATE\n   )\n   RETURNS VOID AS $$\n   DECLARE\n     current_date DATE := start_date;\n     partition_name TEXT;\n     next_date DATE;\n   BEGIN\n     WHILE current_date < end_date LOOP\n       next_date := current_date + INTERVAL '1 month';\n       partition_name := table_name || '_' || to_char(current_date, 'YYYY_MM');\n       \n       EXECUTE format('CREATE TABLE IF NOT EXISTS %I PARTITION OF %I FOR VALUES FROM (%L) TO (%L)',\n         partition_name, table_name, current_date, next_date);\n       \n       current_date := next_date;\n     END LOOP;\n   END;\n   $$ LANGUAGE plpgsql;\n\n   -- Schedule partition creation\n   SELECT create_monthly_partitions('orders_partitioned', '2024-01-01'::DATE, '2025-01-01'::DATE);\n   ```\n\n5. **Data Integrity and Constraints**\n   - Implement comprehensive data validation:\n\n   **Advanced Constraints:**\n   ```sql\n   -- Complex check constraints\n   ALTER TABLE products ADD CONSTRAINT products_price_logic \n   CHECK (\n     CASE \n       WHEN compare_price IS NOT NULL THEN price <= compare_price\n       ELSE true\n     END\n   );\n\n   ALTER TABLE products ADD CONSTRAINT products_inventory_logic\n   CHECK (\n     CASE \n       WHEN inventory_tracking = false THEN inventory_quantity IS NULL\n       WHEN inventory_tracking = true THEN inventory_quantity >= 0\n       ELSE true\n     END\n   );\n\n   -- Custom domain types for reusable validation\n   CREATE DOMAIN email_address AS VARCHAR(255)\n   CHECK (VALUE ~* '^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}$');\n\n   CREATE DOMAIN phone_number AS VARCHAR(20)\n   CHECK (VALUE ~* '^\\+?[\\d\\s\\-\\(\\)]{10,20}$');\n\n   CREATE DOMAIN positive_decimal AS DECIMAL(10,2)\n   CHECK (VALUE >= 0);\n\n   -- Use domains in table definitions\n   CREATE TABLE contacts (\n     id BIGSERIAL PRIMARY KEY,\n     email email_address NOT NULL,\n     phone phone_number,\n     balance positive_decimal DEFAULT 0\n   );\n\n   -- Foreign key constraints with cascading options\n   ALTER TABLE order_items \n   ADD CONSTRAINT fk_order_items_order \n   FOREIGN KEY (order_id) REFERENCES orders(id) ON DELETE CASCADE;\n\n   ALTER TABLE order_items \n   ADD CONSTRAINT fk_order_items_product \n   FOREIGN KEY (product_id) REFERENCES products(id) ON DELETE RESTRICT;\n\n   -- Unique constraints for business logic\n   ALTER TABLE user_roles \n   ADD CONSTRAINT unique_user_role_active \n   UNIQUE (user_id, role_id);\n\n   -- Exclusion constraints for complex business rules\n   ALTER TABLE product_promotions \n   ADD CONSTRAINT no_overlapping_promotions \n   EXCLUDE USING gist (\n     product_id WITH =,\n     daterange(start_date, end_date, '[]') WITH &&\n   );\n   ```\n\n6. **Temporal Data and Versioning**\n   - Handle time-based data requirements:\n\n   **Temporal Tables:**\n   ```sql\n   -- Product price history tracking\n   CREATE TABLE product_price_history (\n     id BIGSERIAL PRIMARY KEY,\n     product_id BIGINT REFERENCES products(id) ON DELETE CASCADE,\n     price DECIMAL(10,2) NOT NULL,\n     compare_price DECIMAL(10,2),\n     effective_from TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT CURRENT_TIMESTAMP,\n     effective_to TIMESTAMP WITH TIME ZONE,\n     created_by BIGINT REFERENCES users(id),\n     reason TEXT,\n     \n     -- Ensure no overlapping periods\n     EXCLUDE USING gist (\n       product_id WITH =,\n       tstzrange(effective_from, effective_to, '[)') WITH &&\n     )\n   );\n\n   -- Function to get current price\n   CREATE OR REPLACE FUNCTION get_current_price(p_product_id BIGINT)\n   RETURNS DECIMAL(10,2) AS $$\n   DECLARE\n     current_price DECIMAL(10,2);\n   BEGIN\n     SELECT price INTO current_price\n     FROM product_price_history\n     WHERE product_id = p_product_id\n     AND effective_from <= CURRENT_TIMESTAMP\n     AND (effective_to IS NULL OR effective_to > CURRENT_TIMESTAMP)\n     ORDER BY effective_from DESC\n     LIMIT 1;\n     \n     RETURN current_price;\n   END;\n   $$ LANGUAGE plpgsql;\n\n   -- Trigger to update price history when product price changes\n   CREATE OR REPLACE FUNCTION update_price_history()\n   RETURNS TRIGGER AS $$\n   BEGIN\n     IF OLD.price IS DISTINCT FROM NEW.price THEN\n       -- Close current price period\n       UPDATE product_price_history \n       SET effective_to = CURRENT_TIMESTAMP\n       WHERE product_id = NEW.id AND effective_to IS NULL;\n       \n       -- Insert new price period\n       INSERT INTO product_price_history (product_id, price, compare_price, created_by)\n       VALUES (NEW.id, NEW.price, NEW.compare_price, \n               COALESCE(current_setting('app.current_user_id', true)::BIGINT, NULL));\n     END IF;\n     \n     RETURN NEW;\n   END;\n   $$ LANGUAGE plpgsql;\n\n   CREATE TRIGGER trigger_product_price_history\n   AFTER UPDATE ON products\n   FOR EACH ROW\n   EXECUTE FUNCTION update_price_history();\n   ```\n\n7. **JSON/NoSQL Integration**\n   - Leverage JSON columns for flexible data:\n\n   **JSONB Schema Design:**\n   ```sql\n   -- Flexible product attributes using JSONB\n   CREATE TABLE product_attributes (\n     product_id BIGINT REFERENCES products(id) ON DELETE CASCADE,\n     attributes JSONB NOT NULL DEFAULT '{}',\n     created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n     updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n     \n     PRIMARY KEY (product_id)\n   );\n\n   -- JSONB indexes for efficient querying\n   CREATE INDEX idx_product_attributes_gin ON product_attributes USING gin(attributes);\n   CREATE INDEX idx_product_attributes_color ON product_attributes USING gin((attributes->'color'));\n   CREATE INDEX idx_product_attributes_size ON product_attributes USING gin((attributes->'size'));\n\n   -- Function to query products by attributes\n   CREATE OR REPLACE FUNCTION find_products_by_attributes(search_attributes JSONB)\n   RETURNS TABLE(product_id BIGINT, product_name VARCHAR, attributes JSONB) AS $$\n   BEGIN\n     RETURN QUERY\n     SELECT p.id, p.name, pa.attributes\n     FROM products p\n     JOIN product_attributes pa ON p.id = pa.product_id\n     WHERE pa.attributes @> search_attributes;\n   END;\n   $$ LANGUAGE plpgsql;\n\n   -- Usage examples:\n   -- SELECT * FROM find_products_by_attributes('{\"color\": \"red\", \"size\": \"large\"}');\n\n   -- Settings table with JSONB for flexible configuration\n   CREATE TABLE application_settings (\n     id SERIAL PRIMARY KEY,\n     category VARCHAR(100) NOT NULL,\n     key VARCHAR(100) NOT NULL,\n     value JSONB NOT NULL,\n     description TEXT,\n     is_public BOOLEAN DEFAULT FALSE,\n     created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n     updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n     \n     UNIQUE(category, key)\n   );\n\n   -- Function to get setting value with type casting\n   CREATE OR REPLACE FUNCTION get_setting(p_category VARCHAR, p_key VARCHAR, p_default ANYELEMENT DEFAULT NULL)\n   RETURNS ANYELEMENT AS $$\n   DECLARE\n     setting_value JSONB;\n   BEGIN\n     SELECT value INTO setting_value\n     FROM application_settings\n     WHERE category = p_category AND key = p_key;\n     \n     IF setting_value IS NULL THEN\n       RETURN p_default;\n     END IF;\n     \n     RETURN (setting_value #>> '{}')::TEXT::pg_typeof(p_default);\n   END;\n   $$ LANGUAGE plpgsql;\n   ```\n\n8. **Database Security Schema**\n   - Implement security at the schema level:\n\n   **Row Level Security:**\n   ```sql\n   -- Enable RLS on sensitive tables\n   ALTER TABLE orders ENABLE ROW LEVEL SECURITY;\n   ALTER TABLE user_profiles ENABLE ROW LEVEL SECURITY;\n\n   -- Create policies for data access\n   CREATE POLICY orders_user_access ON orders\n   FOR ALL TO authenticated_users\n   USING (user_id = current_user_id());\n\n   CREATE POLICY orders_admin_access ON orders\n   FOR ALL TO admin_users\n   USING (true);\n\n   -- Function to get current user ID from session\n   CREATE OR REPLACE FUNCTION current_user_id()\n   RETURNS BIGINT AS $$\n   BEGIN\n     RETURN COALESCE(current_setting('app.current_user_id', true)::BIGINT, 0);\n   END;\n   $$ LANGUAGE plpgsql SECURITY DEFINER;\n\n   -- Create database roles with specific permissions\n   CREATE ROLE app_readonly;\n   GRANT CONNECT ON DATABASE myapp TO app_readonly;\n   GRANT USAGE ON SCHEMA public TO app_readonly;\n   GRANT SELECT ON ALL TABLES IN SCHEMA public TO app_readonly;\n\n   CREATE ROLE app_readwrite;\n   GRANT app_readonly TO app_readwrite;\n   GRANT INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA public TO app_readwrite;\n   GRANT USAGE, SELECT ON ALL SEQUENCES IN SCHEMA public TO app_readwrite;\n\n   -- Sensitive data encryption\n   CREATE EXTENSION IF NOT EXISTS pgcrypto;\n\n   -- Function to encrypt sensitive data\n   CREATE OR REPLACE FUNCTION encrypt_sensitive_data(data TEXT)\n   RETURNS TEXT AS $$\n   BEGIN\n     RETURN encode(encrypt(data::bytea, current_setting('app.encryption_key'), 'aes'), 'base64');\n   END;\n   $$ LANGUAGE plpgsql;\n\n   -- Function to decrypt sensitive data\n   CREATE OR REPLACE FUNCTION decrypt_sensitive_data(encrypted_data TEXT)\n   RETURNS TEXT AS $$\n   BEGIN\n     RETURN convert_from(decrypt(decode(encrypted_data, 'base64'), current_setting('app.encryption_key'), 'aes'), 'UTF8');\n   END;\n   $$ LANGUAGE plpgsql;\n   ```\n\n9. **Schema Documentation and Maintenance**\n   - Document and maintain schema design:\n\n   **Database Documentation:**\n   ```sql\n   -- Add comments to tables and columns\n   COMMENT ON TABLE users IS 'User accounts and authentication information';\n   COMMENT ON COLUMN users.email IS 'Unique email address for user authentication';\n   COMMENT ON COLUMN users.status IS 'Current status of user account (active, inactive, suspended, pending_verification)';\n   COMMENT ON COLUMN users.email_verified IS 'Whether the user has verified their email address';\n\n   COMMENT ON TABLE products IS 'Product catalog with inventory and pricing information';\n   COMMENT ON COLUMN products.search_vector IS 'Full-text search vector generated from name, description, and SKU';\n   COMMENT ON COLUMN products.attributes IS 'Flexible product attributes stored as JSONB (color, size, material, etc.)';\n\n   -- Create a view for schema documentation\n   CREATE VIEW schema_documentation AS\n   SELECT \n     t.table_name,\n     t.table_type,\n     obj_description(c.oid) AS table_comment,\n     col.column_name,\n     col.data_type,\n     col.is_nullable,\n     col.column_default,\n     col_description(c.oid, col.ordinal_position) AS column_comment\n   FROM information_schema.tables t\n   JOIN pg_class c ON c.relname = t.table_name\n   JOIN information_schema.columns col ON col.table_name = t.table_name\n   WHERE t.table_schema = 'public'\n   ORDER BY t.table_name, col.ordinal_position;\n   ```\n\n10. **Schema Testing and Validation**\n    - Implement schema testing procedures:\n\n    **Schema Validation Tests:**\n    ```sql\n    -- Test data integrity constraints\n    DO $$\n    DECLARE\n      test_result BOOLEAN;\n    BEGIN\n      -- Test email validation\n      BEGIN\n        INSERT INTO users (email, username, password_hash, first_name, last_name)\n        VALUES ('invalid-email', 'testuser', 'hash', 'Test', 'User');\n        RAISE EXCEPTION 'Email validation failed - invalid email accepted';\n      EXCEPTION\n        WHEN check_violation THEN\n          RAISE NOTICE 'Email validation working correctly';\n      END;\n      \n      -- Test price constraints\n      BEGIN\n        INSERT INTO products (name, slug, sku, price, compare_price)\n        VALUES ('Test Product', 'test-product', 'TEST-001', 100.00, 50.00);\n        RAISE EXCEPTION 'Price validation failed - compare_price less than price accepted';\n      EXCEPTION\n        WHEN check_violation THEN\n          RAISE NOTICE 'Price validation working correctly';\n      END;\n      \n      -- Test foreign key constraints\n      BEGIN\n        INSERT INTO order_items (order_id, product_id, quantity, unit_price, total_price, product_name)\n        VALUES (999999, 999999, 1, 10.00, 10.00, 'Test Product');\n        RAISE EXCEPTION 'Foreign key validation failed - non-existent order_id accepted';\n      EXCEPTION\n        WHEN foreign_key_violation THEN\n          RAISE NOTICE 'Foreign key validation working correctly';\n      END;\n    END;\n    $$;\n\n    -- Performance test queries\n    CREATE OR REPLACE FUNCTION test_query_performance()\n    RETURNS TABLE(test_name TEXT, execution_time INTERVAL) AS $$\n    DECLARE\n      start_time TIMESTAMP;\n      end_time TIMESTAMP;\n    BEGIN\n      -- Test user lookup by email\n      start_time := clock_timestamp();\n      PERFORM * FROM users WHERE email = 'test@example.com';\n      end_time := clock_timestamp();\n      test_name := 'User lookup by email';\n      execution_time := end_time - start_time;\n      RETURN NEXT;\n      \n      -- Test product search\n      start_time := clock_timestamp();\n      PERFORM * FROM products WHERE search_vector @@ to_tsquery('english', 'laptop');\n      end_time := clock_timestamp();\n      test_name := 'Product full-text search';\n      execution_time := end_time - start_time;\n      RETURN NEXT;\n      \n      -- Test order history query\n      start_time := clock_timestamp();\n      PERFORM o.* FROM orders o \n      JOIN order_items oi ON o.id = oi.order_id \n      WHERE o.user_id = 1 \n      ORDER BY o.created_at DESC \n      LIMIT 20;\n      end_time := clock_timestamp();\n      test_name := 'User order history';\n      execution_time := end_time - start_time;\n      RETURN NEXT;\n    END;\n    $$ LANGUAGE plpgsql;\n\n    -- Run performance tests\n    SELECT * FROM test_query_performance();\n    ```"
              },
              {
                "name": "/design-rest-api",
                "description": "Design RESTful API architecture",
                "path": "plugins/all-commands/commands/design-rest-api.md",
                "frontmatter": {
                  "description": "Design RESTful API architecture",
                  "category": "api-development"
                },
                "content": "# Design REST API\n\nDesign RESTful API architecture\n\n## Instructions\n\n1. **API Design Strategy and Planning**\n   - Analyze business requirements and define API scope\n   - Identify resources, entities, and their relationships\n   - Plan API versioning strategy and backward compatibility\n   - Define authentication and authorization requirements\n   - Plan for scalability, rate limiting, and performance\n\n2. **RESTful Resource Design**\n   - Design RESTful endpoints following REST principles:\n\n   **Express.js API Structure:**\n   ```javascript\n   // routes/api/v1/index.js\n   const express = require('express');\n   const router = express.Router();\n\n   // Resource-based routing structure\n   const userRoutes = require('./users');\n   const productRoutes = require('./products');\n   const orderRoutes = require('./orders');\n   const authRoutes = require('./auth');\n\n   // API versioning and middleware\n   router.use('/auth', authRoutes);\n   router.use('/users', userRoutes);\n   router.use('/products', productRoutes);\n   router.use('/orders', orderRoutes);\n\n   module.exports = router;\n\n   // routes/api/v1/users.js\n   const express = require('express');\n   const router = express.Router();\n   const { validateRequest, authenticate, authorize } = require('../../../middleware');\n   const userController = require('../../../controllers/userController');\n   const userValidation = require('../../../validations/userValidation');\n\n   // User resource endpoints\n   router.get('/', \n     authenticate,\n     authorize(['admin', 'manager']),\n     validateRequest(userValidation.listUsers),\n     userController.listUsers\n   );\n\n   router.get('/:id', \n     authenticate,\n     validateRequest(userValidation.getUser),\n     userController.getUser\n   );\n\n   router.post('/',\n     authenticate,\n     authorize(['admin']),\n     validateRequest(userValidation.createUser),\n     userController.createUser\n   );\n\n   router.put('/:id',\n     authenticate,\n     validateRequest(userValidation.updateUser),\n     userController.updateUser\n   );\n\n   router.patch('/:id',\n     authenticate,\n     validateRequest(userValidation.patchUser),\n     userController.patchUser\n   );\n\n   router.delete('/:id',\n     authenticate,\n     authorize(['admin']),\n     validateRequest(userValidation.deleteUser),\n     userController.deleteUser\n   );\n\n   // Nested resource endpoints\n   router.get('/:id/orders',\n     authenticate,\n     validateRequest(userValidation.getUserOrders),\n     userController.getUserOrders\n   );\n\n   router.get('/:id/profile',\n     authenticate,\n     validateRequest(userValidation.getUserProfile),\n     userController.getUserProfile\n   );\n\n   module.exports = router;\n   ```\n\n3. **Request/Response Data Models**\n   - Define comprehensive data models and validation:\n\n   **Data Validation with Joi:**\n   ```javascript\n   // validations/userValidation.js\n   const Joi = require('joi');\n\n   const userSchema = {\n     create: Joi.object({\n       email: Joi.string().email().required(),\n       password: Joi.string().min(8).pattern(/^(?=.*[a-z])(?=.*[A-Z])(?=.*\\d)/).required(),\n       firstName: Joi.string().trim().min(1).max(100).required(),\n       lastName: Joi.string().trim().min(1).max(100).required(),\n       phone: Joi.string().pattern(/^\\+?[\\d\\s\\-\\(\\)]{10,20}$/).optional(),\n       dateOfBirth: Joi.date().max('now').optional(),\n       role: Joi.string().valid('user', 'admin', 'manager').default('user')\n     }),\n\n     update: Joi.object({\n       email: Joi.string().email().optional(),\n       firstName: Joi.string().trim().min(1).max(100).optional(),\n       lastName: Joi.string().trim().min(1).max(100).optional(),\n       phone: Joi.string().pattern(/^\\+?[\\d\\s\\-\\(\\)]{10,20}$/).optional(),\n       dateOfBirth: Joi.date().max('now').optional(),\n       status: Joi.string().valid('active', 'inactive', 'suspended').optional()\n     }),\n\n     list: Joi.object({\n       page: Joi.number().integer().min(1).default(1),\n       limit: Joi.number().integer().min(1).max(100).default(20),\n       sort: Joi.string().valid('id', 'email', 'firstName', 'lastName', 'createdAt').default('id'),\n       order: Joi.string().valid('asc', 'desc').default('asc'),\n       search: Joi.string().trim().min(1).optional(),\n       status: Joi.string().valid('active', 'inactive', 'suspended').optional(),\n       role: Joi.string().valid('user', 'admin', 'manager').optional()\n     }),\n\n     params: Joi.object({\n       id: Joi.number().integer().positive().required()\n     })\n   };\n\n   const validateRequest = (schema) => {\n     return (req, res, next) => {\n       const validationTargets = {\n         body: req.body,\n         query: req.query,\n         params: req.params\n       };\n\n       const errors = {};\n\n       // Validate each part of the request\n       Object.keys(schema).forEach(target => {\n         const { error, value } = schema[target].validate(validationTargets[target], {\n           abortEarly: false,\n           allowUnknown: false,\n           stripUnknown: true\n         });\n\n         if (error) {\n           errors[target] = error.details.map(detail => ({\n             field: detail.path.join('.'),\n             message: detail.message,\n             value: detail.context.value\n           }));\n         } else {\n           req[target] = value;\n         }\n       });\n\n       if (Object.keys(errors).length > 0) {\n         return res.status(400).json({\n           error: 'Validation failed',\n           details: errors,\n           timestamp: new Date().toISOString()\n         });\n       }\n\n       next();\n     };\n   };\n\n   module.exports = {\n     listUsers: validateRequest({ query: userSchema.list }),\n     getUser: validateRequest({ params: userSchema.params }),\n     createUser: validateRequest({ body: userSchema.create }),\n     updateUser: validateRequest({ \n       params: userSchema.params, \n       body: userSchema.update \n     }),\n     patchUser: validateRequest({ \n       params: userSchema.params, \n       body: userSchema.update \n     }),\n     deleteUser: validateRequest({ params: userSchema.params }),\n     getUserOrders: validateRequest({ \n       params: userSchema.params,\n       query: Joi.object({\n         page: Joi.number().integer().min(1).default(1),\n         limit: Joi.number().integer().min(1).max(50).default(10),\n         status: Joi.string().valid('pending', 'processing', 'shipped', 'delivered', 'cancelled').optional()\n       })\n     })\n   };\n   ```\n\n4. **Controller Implementation**\n   - Implement robust controller logic:\n\n   **User Controller Example:**\n   ```javascript\n   // controllers/userController.js\n   const userService = require('../services/userService');\n   const { ApiError, ApiResponse } = require('../utils/apiResponse');\n\n   class UserController {\n     async listUsers(req, res, next) {\n       try {\n         const { page, limit, sort, order, search, status, role } = req.query;\n         \n         const filters = {};\n         if (search) filters.search = search;\n         if (status) filters.status = status;\n         if (role) filters.role = role;\n\n         const result = await userService.findUsers({\n           page,\n           limit,\n           sort,\n           order,\n           filters\n         });\n\n         res.json(new ApiResponse('success', 'Users retrieved successfully', {\n           users: result.users,\n           pagination: {\n             page: result.page,\n             limit: result.limit,\n             total: result.total,\n             totalPages: result.totalPages,\n             hasNext: result.hasNext,\n             hasPrev: result.hasPrev\n           }\n         }));\n       } catch (error) {\n         next(error);\n       }\n     }\n\n     async getUser(req, res, next) {\n       try {\n         const { id } = req.params;\n         const requestingUserId = req.user.id;\n         const requestingUserRole = req.user.role;\n\n         // Authorization check\n         if (id !== requestingUserId && !['admin', 'manager'].includes(requestingUserRole)) {\n           throw new ApiError(403, 'Insufficient permissions to access this user');\n         }\n\n         const user = await userService.findById(id);\n         if (!user) {\n           throw new ApiError(404, 'User not found');\n         }\n\n         // Filter sensitive data based on permissions\n         const filteredUser = userService.filterUserData(user, requestingUserRole, requestingUserId);\n\n         res.json(new ApiResponse('success', 'User retrieved successfully', { user: filteredUser }));\n       } catch (error) {\n         next(error);\n       }\n     }\n\n     async createUser(req, res, next) {\n       try {\n         const userData = req.body;\n         \n         // Check for existing user\n         const existingUser = await userService.findByEmail(userData.email);\n         if (existingUser) {\n           throw new ApiError(409, 'User with this email already exists');\n         }\n\n         const newUser = await userService.createUser(userData);\n         \n         // Remove sensitive data from response\n         const responseUser = userService.filterUserData(newUser, 'admin');\n\n         res.status(201).json(new ApiResponse(\n           'success', \n           'User created successfully', \n           { user: responseUser }\n         ));\n       } catch (error) {\n         next(error);\n       }\n     }\n\n     async updateUser(req, res, next) {\n       try {\n         const { id } = req.params;\n         const updateData = req.body;\n         const requestingUserId = req.user.id;\n         const requestingUserRole = req.user.role;\n\n         // Authorization check\n         if (id !== requestingUserId && !['admin', 'manager'].includes(requestingUserRole)) {\n           throw new ApiError(403, 'Insufficient permissions to update this user');\n         }\n\n         // Restrict certain fields based on role\n         if (updateData.role && !['admin'].includes(requestingUserRole)) {\n           throw new ApiError(403, 'Insufficient permissions to update user role');\n         }\n\n         const existingUser = await userService.findById(id);\n         if (!existingUser) {\n           throw new ApiError(404, 'User not found');\n         }\n\n         const updatedUser = await userService.updateUser(id, updateData);\n         const filteredUser = userService.filterUserData(updatedUser, requestingUserRole, requestingUserId);\n\n         res.json(new ApiResponse('success', 'User updated successfully', { user: filteredUser }));\n       } catch (error) {\n         next(error);\n       }\n     }\n\n     async deleteUser(req, res, next) {\n       try {\n         const { id } = req.params;\n         const requestingUserId = req.user.id;\n\n         // Prevent self-deletion\n         if (id === requestingUserId) {\n           throw new ApiError(400, 'Cannot delete your own account');\n         }\n\n         const existingUser = await userService.findById(id);\n         if (!existingUser) {\n           throw new ApiError(404, 'User not found');\n         }\n\n         await userService.deleteUser(id);\n\n         res.status(204).send();\n       } catch (error) {\n         next(error);\n       }\n     }\n\n     async getUserOrders(req, res, next) {\n       try {\n         const { id } = req.params;\n         const { page, limit, status } = req.query;\n         const requestingUserId = req.user.id;\n         const requestingUserRole = req.user.role;\n\n         // Authorization check\n         if (id !== requestingUserId && !['admin', 'manager'].includes(requestingUserRole)) {\n           throw new ApiError(403, 'Insufficient permissions to access user orders');\n         }\n\n         const orders = await userService.getUserOrders(id, {\n           page,\n           limit,\n           status\n         });\n\n         res.json(new ApiResponse('success', 'User orders retrieved successfully', orders));\n       } catch (error) {\n         next(error);\n       }\n     }\n   }\n\n   module.exports = new UserController();\n   ```\n\n5. **API Response Standardization**\n   - Implement consistent response formats:\n\n   **API Response Utilities:**\n   ```javascript\n   // utils/apiResponse.js\n   class ApiResponse {\n     constructor(status, message, data = null, meta = null) {\n       this.status = status;\n       this.message = message;\n       this.timestamp = new Date().toISOString();\n       \n       if (data !== null) {\n         this.data = data;\n       }\n       \n       if (meta !== null) {\n         this.meta = meta;\n       }\n     }\n\n     static success(message, data = null, meta = null) {\n       return new ApiResponse('success', message, data, meta);\n     }\n\n     static error(message, errors = null) {\n       const response = new ApiResponse('error', message);\n       if (errors) {\n         response.errors = errors;\n       }\n       return response;\n     }\n\n     static paginated(message, data, pagination) {\n       return new ApiResponse('success', message, data, { pagination });\n     }\n   }\n\n   class ApiError extends Error {\n     constructor(statusCode, message, errors = null, isOperational = true, stack = '') {\n       super(message);\n       this.statusCode = statusCode;\n       this.isOperational = isOperational;\n       this.errors = errors;\n       \n       if (stack) {\n         this.stack = stack;\n       } else {\n         Error.captureStackTrace(this, this.constructor);\n       }\n     }\n\n     static badRequest(message, errors = null) {\n       return new ApiError(400, message, errors);\n     }\n\n     static unauthorized(message = 'Unauthorized access') {\n       return new ApiError(401, message);\n     }\n\n     static forbidden(message = 'Forbidden access') {\n       return new ApiError(403, message);\n     }\n\n     static notFound(message = 'Resource not found') {\n       return new ApiError(404, message);\n     }\n\n     static conflict(message, errors = null) {\n       return new ApiError(409, message, errors);\n     }\n\n     static validationError(message, errors) {\n       return new ApiError(422, message, errors);\n     }\n\n     static internalError(message = 'Internal server error') {\n       return new ApiError(500, message);\n     }\n   }\n\n   // Error handling middleware\n   const errorHandler = (error, req, res, next) => {\n     let { statusCode, message, errors } = error;\n\n     if (!error.isOperational) {\n       statusCode = 500;\n       message = 'Internal server error';\n       \n       // Log unexpected errors\n       console.error('Unexpected error:', error);\n     }\n\n     const response = ApiResponse.error(message, errors);\n     \n     // Add request ID for tracking\n     if (req.requestId) {\n       response.requestId = req.requestId;\n     }\n\n     // Add stack trace in development\n     if (process.env.NODE_ENV === 'development') {\n       response.stack = error.stack;\n     }\n\n     res.status(statusCode).json(response);\n   };\n\n   // 404 handler\n   const notFoundHandler = (req, res) => {\n     const error = ApiError.notFound(`Route ${req.originalUrl} not found`);\n     res.status(404).json(ApiResponse.error(error.message));\n   };\n\n   module.exports = {\n     ApiResponse,\n     ApiError,\n     errorHandler,\n     notFoundHandler\n   };\n   ```\n\n6. **Authentication and Authorization**\n   - Implement comprehensive auth system:\n\n   **JWT Authentication Middleware:**\n   ```javascript\n   // middleware/auth.js\n   const jwt = require('jsonwebtoken');\n   const { ApiError } = require('../utils/apiResponse');\n   const userService = require('../services/userService');\n\n   class AuthMiddleware {\n     static async authenticate(req, res, next) {\n       try {\n         const authHeader = req.headers.authorization;\n         \n         if (!authHeader) {\n           throw ApiError.unauthorized('Access token is required');\n         }\n\n         const token = authHeader.startsWith('Bearer ') \n           ? authHeader.slice(7) \n           : authHeader;\n\n         if (!token) {\n           throw ApiError.unauthorized('Invalid authorization header format');\n         }\n\n         let decoded;\n         try {\n           decoded = jwt.verify(token, process.env.JWT_SECRET);\n         } catch (jwtError) {\n           if (jwtError.name === 'TokenExpiredError') {\n             throw ApiError.unauthorized('Access token has expired');\n           } else if (jwtError.name === 'JsonWebTokenError') {\n             throw ApiError.unauthorized('Invalid access token');\n           } else {\n             throw ApiError.unauthorized('Token verification failed');\n           }\n         }\n\n         // Fetch user and verify account status\n         const user = await userService.findById(decoded.userId);\n         if (!user) {\n           throw ApiError.unauthorized('User not found');\n         }\n\n         if (user.status !== 'active') {\n           throw ApiError.unauthorized('Account is not active');\n         }\n\n         // Check if token is still valid (not invalidated)\n         if (user.tokenVersion && decoded.tokenVersion !== user.tokenVersion) {\n           throw ApiError.unauthorized('Token has been invalidated');\n         }\n\n         // Attach user to request\n         req.user = {\n           id: user.id,\n           email: user.email,\n           role: user.role,\n           permissions: user.permissions || []\n         };\n\n         next();\n       } catch (error) {\n         next(error);\n       }\n     }\n\n     static authorize(requiredRoles = [], requiredPermissions = []) {\n       return (req, res, next) => {\n         try {\n           if (!req.user) {\n             throw ApiError.unauthorized('Authentication required');\n           }\n\n           // Check role-based authorization\n           if (requiredRoles.length > 0) {\n             const hasRequiredRole = requiredRoles.includes(req.user.role);\n             if (!hasRequiredRole) {\n               throw ApiError.forbidden(`Requires one of the following roles: ${requiredRoles.join(', ')}`);\n             }\n           }\n\n           // Check permission-based authorization\n           if (requiredPermissions.length > 0) {\n             const userPermissions = req.user.permissions || [];\n             const hasRequiredPermission = requiredPermissions.some(permission => \n               userPermissions.includes(permission)\n             );\n             \n             if (!hasRequiredPermission) {\n               throw ApiError.forbidden(`Requires one of the following permissions: ${requiredPermissions.join(', ')}`);\n             }\n           }\n\n           next();\n         } catch (error) {\n           next(error);\n         }\n       };\n     }\n\n     static async rateLimitByUser(req, res, next) {\n       try {\n         if (!req.user) {\n           return next();\n         }\n\n         const userId = req.user.id;\n         const key = `rate_limit:${userId}:${req.route.path}`;\n         \n         // Implement rate limiting logic here\n         // This is a simplified example\n         const requestCount = await redis.incr(key);\n         if (requestCount === 1) {\n           await redis.expire(key, 3600); // 1 hour window\n         }\n\n         const limit = req.user.role === 'admin' ? 1000 : 100; // Different limits by role\n         \n         if (requestCount > limit) {\n           throw ApiError.tooManyRequests('Rate limit exceeded');\n         }\n\n         res.set({\n           'X-RateLimit-Limit': limit,\n           'X-RateLimit-Remaining': Math.max(0, limit - requestCount),\n           'X-RateLimit-Reset': new Date(Date.now() + 3600000).toISOString()\n         });\n\n         next();\n       } catch (error) {\n         next(error);\n       }\n     }\n   }\n\n   module.exports = AuthMiddleware;\n   ```\n\n7. **API Documentation with OpenAPI/Swagger**\n   - Generate comprehensive API documentation:\n\n   **Swagger Configuration:**\n   ```javascript\n   // swagger/swagger.js\n   const swaggerJsdoc = require('swagger-jsdoc');\n   const swaggerUi = require('swagger-ui-express');\n\n   const options = {\n     definition: {\n       openapi: '3.0.0',\n       info: {\n         title: 'REST API',\n         version: '1.0.0',\n         description: 'A comprehensive REST API with authentication and authorization',\n         contact: {\n           name: 'API Support',\n           email: 'api-support@example.com'\n         },\n         license: {\n           name: 'MIT',\n           url: 'https://opensource.org/licenses/MIT'\n         }\n       },\n       servers: [\n         {\n           url: process.env.API_URL || 'http://localhost:3000',\n           description: 'Development server'\n         },\n         {\n           url: 'https://api.example.com',\n           description: 'Production server'\n         }\n       ],\n       components: {\n         securitySchemes: {\n           bearerAuth: {\n             type: 'http',\n             scheme: 'bearer',\n             bearerFormat: 'JWT',\n             description: 'JWT Authorization header using the Bearer scheme'\n           }\n         },\n         schemas: {\n           User: {\n             type: 'object',\n             required: ['email', 'firstName', 'lastName'],\n             properties: {\n               id: {\n                 type: 'integer',\n                 description: 'Unique user identifier',\n                 example: 1\n               },\n               email: {\n                 type: 'string',\n                 format: 'email',\n                 description: 'User email address',\n                 example: 'user@example.com'\n               },\n               firstName: {\n                 type: 'string',\n                 description: 'User first name',\n                 example: 'John'\n               },\n               lastName: {\n                 type: 'string',\n                 description: 'User last name',\n                 example: 'Doe'\n               },\n               role: {\n                 type: 'string',\n                 enum: ['user', 'admin', 'manager'],\n                 description: 'User role',\n                 example: 'user'\n               },\n               status: {\n                 type: 'string',\n                 enum: ['active', 'inactive', 'suspended'],\n                 description: 'Account status',\n                 example: 'active'\n               },\n               createdAt: {\n                 type: 'string',\n                 format: 'date-time',\n                 description: 'Account creation timestamp'\n               },\n               updatedAt: {\n                 type: 'string',\n                 format: 'date-time',\n                 description: 'Last update timestamp'\n               }\n             }\n           },\n           ApiResponse: {\n             type: 'object',\n             properties: {\n               status: {\n                 type: 'string',\n                 enum: ['success', 'error'],\n                 example: 'success'\n               },\n               message: {\n                 type: 'string',\n                 example: 'Operation completed successfully'\n               },\n               timestamp: {\n                 type: 'string',\n                 format: 'date-time',\n                 example: '2024-01-15T10:30:00Z'\n               },\n               data: {\n                 type: 'object',\n                 description: 'Response data (varies by endpoint)'\n               }\n             }\n           },\n           ErrorResponse: {\n             type: 'object',\n             properties: {\n               status: {\n                 type: 'string',\n                 enum: ['error'],\n                 example: 'error'\n               },\n               message: {\n                 type: 'string',\n                 example: 'An error occurred'\n               },\n               timestamp: {\n                 type: 'string',\n                 format: 'date-time'\n               },\n               errors: {\n                 type: 'object',\n                 description: 'Detailed error information'\n               }\n             }\n           },\n           PaginationMeta: {\n             type: 'object',\n             properties: {\n               pagination: {\n                 type: 'object',\n                 properties: {\n                   page: { type: 'integer', example: 1 },\n                   limit: { type: 'integer', example: 20 },\n                   total: { type: 'integer', example: 100 },\n                   totalPages: { type: 'integer', example: 5 },\n                   hasNext: { type: 'boolean', example: true },\n                   hasPrev: { type: 'boolean', example: false }\n                 }\n               }\n             }\n           }\n         },\n         responses: {\n           UnauthorizedError: {\n             description: 'Access token is missing or invalid',\n             content: {\n               'application/json': {\n                 schema: { $ref: '#/components/schemas/ErrorResponse' }\n               }\n             }\n           },\n           ForbiddenError: {\n             description: 'Insufficient permissions',\n             content: {\n               'application/json': {\n                 schema: { $ref: '#/components/schemas/ErrorResponse' }\n               }\n             }\n           },\n           NotFoundError: {\n             description: 'Resource not found',\n             content: {\n               'application/json': {\n                 schema: { $ref: '#/components/schemas/ErrorResponse' }\n               }\n             }\n           },\n           ValidationError: {\n             description: 'Request validation failed',\n             content: {\n               'application/json': {\n                 schema: { $ref: '#/components/schemas/ErrorResponse' }\n               }\n             }\n           }\n         }\n       },\n       security: [\n         {\n           bearerAuth: []\n         }\n       ]\n     },\n     apis: ['./routes/**/*.js', './controllers/**/*.js']\n   };\n\n   const specs = swaggerJsdoc(options);\n\n   const swaggerOptions = {\n     explorer: true,\n     swaggerOptions: {\n       docExpansion: 'none',\n       filter: true,\n       showRequestDuration: true\n     }\n   };\n\n   module.exports = {\n     serve: swaggerUi.serve,\n     setup: swaggerUi.setup(specs, swaggerOptions),\n     specs\n   };\n   ```\n\n   **Controller Documentation:**\n   ```javascript\n   // Add to userController.js\n   /**\n    * @swagger\n    * /api/v1/users:\n    *   get:\n    *     summary: List all users\n    *     tags: [Users]\n    *     security:\n    *       - bearerAuth: []\n    *     parameters:\n    *       - in: query\n    *         name: page\n    *         schema:\n    *           type: integer\n    *           minimum: 1\n    *           default: 1\n    *         description: Page number\n    *       - in: query\n    *         name: limit\n    *         schema:\n    *           type: integer\n    *           minimum: 1\n    *           maximum: 100\n    *           default: 20\n    *         description: Number of users per page\n    *       - in: query\n    *         name: search\n    *         schema:\n    *           type: string\n    *         description: Search term for user names or email\n    *       - in: query\n    *         name: status\n    *         schema:\n    *           type: string\n    *           enum: [active, inactive, suspended]\n    *         description: Filter by user status\n    *     responses:\n    *       200:\n    *         description: Users retrieved successfully\n    *         content:\n    *           application/json:\n    *             schema:\n    *               allOf:\n    *                 - $ref: '#/components/schemas/ApiResponse'\n    *                 - type: object\n    *                   properties:\n    *                     data:\n    *                       type: object\n    *                       properties:\n    *                         users:\n    *                           type: array\n    *                           items:\n    *                             $ref: '#/components/schemas/User'\n    *                     meta:\n    *                       $ref: '#/components/schemas/PaginationMeta'\n    *       401:\n    *         $ref: '#/components/responses/UnauthorizedError'\n    *       403:\n    *         $ref: '#/components/responses/ForbiddenError'\n    *\n    *   post:\n    *     summary: Create a new user\n    *     tags: [Users]\n    *     security:\n    *       - bearerAuth: []\n    *     requestBody:\n    *       required: true\n    *       content:\n    *         application/json:\n    *           schema:\n    *             type: object\n    *             required:\n    *               - email\n    *               - password\n    *               - firstName\n    *               - lastName\n    *             properties:\n    *               email:\n    *                 type: string\n    *                 format: email\n    *               password:\n    *                 type: string\n    *                 minLength: 8\n    *               firstName:\n    *                 type: string\n    *                 minLength: 1\n    *                 maxLength: 100\n    *               lastName:\n    *                 type: string\n    *                 minLength: 1\n    *                 maxLength: 100\n    *               phone:\n    *                 type: string\n    *               role:\n    *                 type: string\n    *                 enum: [user, admin, manager]\n    *     responses:\n    *       201:\n    *         description: User created successfully\n    *         content:\n    *           application/json:\n    *             schema:\n    *               allOf:\n    *                 - $ref: '#/components/schemas/ApiResponse'\n    *                 - type: object\n    *                   properties:\n    *                     data:\n    *                       type: object\n    *                       properties:\n    *                         user:\n    *                           $ref: '#/components/schemas/User'\n    *       400:\n    *         $ref: '#/components/responses/ValidationError'\n    *       409:\n    *         description: User with email already exists\n    */\n   ```\n\n8. **API Testing and Quality Assurance**\n   - Implement comprehensive API testing:\n\n   **API Test Suite:**\n   ```javascript\n   // tests/api/users.test.js\n   const request = require('supertest');\n   const app = require('../../app');\n   const { setupTestDb, teardownTestDb, createTestUser, getAuthToken } = require('../helpers/testHelpers');\n\n   describe('Users API', () => {\n     let authToken;\n     let testUser;\n\n     beforeAll(async () => {\n       await setupTestDb();\n       testUser = await createTestUser({ role: 'admin' });\n       authToken = await getAuthToken(testUser);\n     });\n\n     afterAll(async () => {\n       await teardownTestDb();\n     });\n\n     describe('GET /api/v1/users', () => {\n       test('should return paginated users list for admin', async () => {\n         const response = await request(app)\n           .get('/api/v1/users')\n           .set('Authorization', `Bearer ${authToken}`)\n           .expect(200);\n\n         expect(response.body).toMatchObject({\n           status: 'success',\n           message: 'Users retrieved successfully',\n           data: {\n             users: expect.any(Array)\n           },\n           meta: {\n             pagination: {\n               page: 1,\n               limit: 20,\n               total: expect.any(Number),\n               totalPages: expect.any(Number),\n               hasNext: expect.any(Boolean),\n               hasPrev: false\n             }\n           }\n         });\n\n         expect(response.body.data.users[0]).toHaveProperty('id');\n         expect(response.body.data.users[0]).toHaveProperty('email');\n         expect(response.body.data.users[0]).not.toHaveProperty('password');\n       });\n\n       test('should filter users by status', async () => {\n         const response = await request(app)\n           .get('/api/v1/users?status=active')\n           .set('Authorization', `Bearer ${authToken}`)\n           .expect(200);\n\n         response.body.data.users.forEach(user => {\n           expect(user.status).toBe('active');\n         });\n       });\n\n       test('should return 401 without auth token', async () => {\n         const response = await request(app)\n           .get('/api/v1/users')\n           .expect(401);\n\n         expect(response.body).toMatchObject({\n           status: 'error',\n           message: 'Access token is required'\n         });\n       });\n\n       test('should validate pagination parameters', async () => {\n         const response = await request(app)\n           .get('/api/v1/users?page=0&limit=200')\n           .set('Authorization', `Bearer ${authToken}`)\n           .expect(400);\n\n         expect(response.body.status).toBe('error');\n         expect(response.body.details).toBeDefined();\n       });\n     });\n\n     describe('POST /api/v1/users', () => {\n       test('should create user with valid data', async () => {\n         const userData = {\n           email: 'newuser@example.com',\n           password: 'SecurePass123',\n           firstName: 'New',\n           lastName: 'User',\n           role: 'user'\n         };\n\n         const response = await request(app)\n           .post('/api/v1/users')\n           .set('Authorization', `Bearer ${authToken}`)\n           .send(userData)\n           .expect(201);\n\n         expect(response.body).toMatchObject({\n           status: 'success',\n           message: 'User created successfully',\n           data: {\n             user: {\n               email: userData.email,\n               firstName: userData.firstName,\n               lastName: userData.lastName,\n               role: userData.role\n             }\n           }\n         });\n\n         expect(response.body.data.user).not.toHaveProperty('password');\n       });\n\n       test('should reject invalid email format', async () => {\n         const userData = {\n           email: 'invalid-email',\n           password: 'SecurePass123',\n           firstName: 'Test',\n           lastName: 'User'\n         };\n\n         const response = await request(app)\n           .post('/api/v1/users')\n           .set('Authorization', `Bearer ${authToken}`)\n           .send(userData)\n           .expect(400);\n\n         expect(response.body.status).toBe('error');\n         expect(response.body.details.body).toBeDefined();\n       });\n\n       test('should reject duplicate email', async () => {\n         const userData = {\n           email: testUser.email,\n           password: 'SecurePass123',\n           firstName: 'Test',\n           lastName: 'User'\n         };\n\n         const response = await request(app)\n           .post('/api/v1/users')\n           .set('Authorization', `Bearer ${authToken}`)\n           .send(userData)\n           .expect(409);\n\n         expect(response.body).toMatchObject({\n           status: 'error',\n           message: 'User with this email already exists'\n         });\n       });\n     });\n\n     describe('Performance Tests', () => {\n       test('should handle concurrent requests', async () => {\n         const promises = Array(10).fill().map(() =>\n           request(app)\n             .get('/api/v1/users')\n             .set('Authorization', `Bearer ${authToken}`)\n         );\n\n         const responses = await Promise.all(promises);\n         \n         responses.forEach(response => {\n           expect(response.status).toBe(200);\n         });\n       });\n\n       test('should respond within acceptable time', async () => {\n         const start = Date.now();\n         \n         await request(app)\n           .get('/api/v1/users')\n           .set('Authorization', `Bearer ${authToken}`)\n           .expect(200);\n         \n         const duration = Date.now() - start;\n         expect(duration).toBeLessThan(1000); // Should respond within 1 second\n       });\n     });\n   });\n   ```\n\n9. **API Versioning Strategy**\n   - Implement flexible API versioning:\n\n   **Version Management:**\n   ```javascript\n   // middleware/versioning.js\n   class ApiVersioning {\n     static extractVersion(req) {\n       // Support multiple versioning strategies\n       \n       // 1. URL path versioning (preferred)\n       const pathVersion = req.path.match(/^\\/api\\/v(\\d+)/);\n       if (pathVersion) {\n         return parseInt(pathVersion[1]);\n       }\n       \n       // 2. Header versioning\n       const headerVersion = req.headers['api-version'];\n       if (headerVersion) {\n         return parseInt(headerVersion);\n       }\n       \n       // 3. Accept header versioning\n       const acceptHeader = req.headers.accept;\n       if (acceptHeader) {\n         const versionMatch = acceptHeader.match(/application\\/vnd\\.api\\.v(\\d+)\\+json/);\n         if (versionMatch) {\n           return parseInt(versionMatch[1]);\n         }\n       }\n       \n       // Default to latest version\n       return this.getLatestVersion();\n     }\n\n     static getLatestVersion() {\n       return 1; // Update when new versions are released\n     }\n\n     static getSupportedVersions() {\n       return [1]; // Add versions as they're created\n     }\n\n     static middleware() {\n       return (req, res, next) => {\n         const requestedVersion = this.extractVersion(req);\n         const supportedVersions = this.getSupportedVersions();\n         \n         if (!supportedVersions.includes(requestedVersion)) {\n           return res.status(400).json({\n             status: 'error',\n             message: `API version ${requestedVersion} is not supported`,\n             supportedVersions: supportedVersions,\n             latestVersion: this.getLatestVersion()\n           });\n         }\n         \n         req.apiVersion = requestedVersion;\n         res.set('API-Version', requestedVersion.toString());\n         \n         next();\n       };\n     }\n\n     static versionedRoute(versions) {\n       return (req, res, next) => {\n         const currentVersion = req.apiVersion || this.getLatestVersion();\n         \n         if (versions[currentVersion]) {\n           return versions[currentVersion](req, res, next);\n         }\n         \n         // Fallback to latest version if current version handler not found\n         const latestVersion = Math.max(...Object.keys(versions).map(Number));\n         if (versions[latestVersion]) {\n           return versions[latestVersion](req, res, next);\n         }\n         \n         res.status(501).json({\n           status: 'error',\n           message: `Version ${currentVersion} is not implemented for this endpoint`\n         });\n       };\n     }\n   }\n\n   // Usage example:\n   // router.get('/users', ApiVersioning.versionedRoute({\n   //   1: userControllerV1.listUsers,\n   //   2: userControllerV2.listUsers\n   // }));\n\n   module.exports = ApiVersioning;\n   ```\n\n10. **Production Monitoring and Analytics**\n    - Implement API monitoring and analytics:\n\n    **API Analytics Middleware:**\n    ```javascript\n    // middleware/analytics.js\n    const prometheus = require('prom-client');\n\n    class ApiAnalytics {\n      constructor() {\n        this.setupMetrics();\n      }\n\n      setupMetrics() {\n        // Request duration histogram\n        this.httpRequestDuration = new prometheus.Histogram({\n          name: 'http_request_duration_seconds',\n          help: 'Duration of HTTP requests in seconds',\n          labelNames: ['method', 'route', 'status_code', 'version'],\n          buckets: [0.1, 0.3, 0.5, 0.7, 1, 3, 5, 7, 10]\n        });\n\n        // Request counter\n        this.httpRequestsTotal = new prometheus.Counter({\n          name: 'http_requests_total',\n          help: 'Total number of HTTP requests',\n          labelNames: ['method', 'route', 'status_code', 'version']\n        });\n\n        // Active connections gauge\n        this.activeConnections = new prometheus.Gauge({\n          name: 'http_active_connections',\n          help: 'Number of active HTTP connections'\n        });\n\n        // Error rate counter\n        this.httpErrorsTotal = new prometheus.Counter({\n          name: 'http_errors_total',\n          help: 'Total number of HTTP errors',\n          labelNames: ['method', 'route', 'status_code', 'error_type']\n        });\n      }\n\n      middleware() {\n        return (req, res, next) => {\n          const startTime = Date.now();\n          this.activeConnections.inc();\n\n          res.on('finish', () => {\n            const duration = (Date.now() - startTime) / 1000;\n            const route = req.route?.path || req.path;\n            const version = req.apiVersion || 'unknown';\n\n            const labels = {\n              method: req.method,\n              route: route,\n              status_code: res.statusCode,\n              version: version\n            };\n\n            // Record metrics\n            this.httpRequestDuration.observe(labels, duration);\n            this.httpRequestsTotal.inc(labels);\n            this.activeConnections.dec();\n\n            // Record errors\n            if (res.statusCode >= 400) {\n              this.httpErrorsTotal.inc({\n                ...labels,\n                error_type: this.getErrorType(res.statusCode)\n              });\n            }\n\n            // Log slow requests\n            if (duration > 1) {\n              console.warn('Slow request detected:', {\n                method: req.method,\n                url: req.url,\n                duration: duration,\n                statusCode: res.statusCode\n              });\n            }\n          });\n\n          next();\n        };\n      }\n\n      getErrorType(statusCode) {\n        if (statusCode >= 400 && statusCode < 500) {\n          return 'client_error';\n        } else if (statusCode >= 500) {\n          return 'server_error';\n        }\n        return 'unknown';\n      }\n\n      getMetrics() {\n        return prometheus.register.metrics();\n      }\n    }\n\n    module.exports = new ApiAnalytics();\n    ```"
              },
              {
                "name": "/digital-twin-creator",
                "description": "Create systematic digital twins with data quality validation and real-world calibration loops.",
                "path": "plugins/all-commands/commands/digital-twin-creator.md",
                "frontmatter": {
                  "description": "Create systematic digital twins with data quality validation and real-world calibration loops.",
                  "category": "simulation-modeling",
                  "argument-hint": "Specify digital twin parameters"
                },
                "content": "# Digital Twin Creator\n\nCreate systematic digital twins with data quality validation and real-world calibration loops.\n\n## Instructions\n\nYou are tasked with creating a comprehensive digital twin to simulate real-world systems, processes, or entities. Follow this systematic approach to build an accurate, calibrated model: **$ARGUMENTS**\n\n### 1. Prerequisites Assessment\n\n**Critical Information Validation:**\n\n- **Twin Subject**: What specific system/process/entity are you modeling?\n- **Purpose & Decisions**: What decisions will this twin inform?\n- **Fidelity Level**: How accurate does the simulation need to be?\n- **Data Availability**: What real-world data can calibrate the model?\n- **Update Frequency**: How often will the twin sync with reality?\n\n**If any prerequisites are missing, guide the user:**\n\n```\nMissing Twin Subject:\n\"I need clarity on what you're modeling. Are you creating a digital twin for:\n- Physical systems: Manufacturing line, vehicle performance, building operations\n- Business processes: Sales pipeline, customer journey, supply chain\n- Market dynamics: Customer segments, competitive landscape, demand patterns\n- Technical systems: Software performance, network behavior, user interactions\"\n\nMissing Purpose Clarity:\n\"What specific decisions will this digital twin help you make?\n- Optimization: Finding better configurations or strategies\n- Prediction: Forecasting future outcomes or behaviors  \n- Risk Assessment: Understanding failure modes and vulnerabilities\n- Experimentation: Testing changes before real-world implementation\n- Monitoring: Detecting anomalies or performance degradation\"\n\nMissing Fidelity Requirements:\n\"How precise does your digital twin need to be?\n- High Fidelity (90%+ accuracy): Critical safety/financial decisions\n- Medium Fidelity (70-90% accuracy): Strategic planning and optimization\n- Low Fidelity (50-70% accuracy): Conceptual understanding and exploration\"\n```\n\n### 2. System Architecture Definition\n\n**Map the structure and boundaries of your target system:**\n\n#### System Components\n- Core elements and their relationships\n- Input/output interfaces and data flows\n- Control mechanisms and feedback loops\n- Performance metrics and success indicators\n- Failure modes and edge cases\n\n#### Boundary Definition\n- What's included vs. excluded from the model\n- External dependencies and influences\n- Environmental constraints and variables\n- Time horizons and operational contexts\n- Abstraction levels and detail granularity\n\n#### Relationship Mapping\n- Causal relationships between components\n- Correlation patterns and dependencies\n- Feedback loops and system dynamics\n- Emergent behaviors and non-linear effects\n- Lag times and temporal relationships\n\n**Quality Gate**: Validate that your system definition is:\n- Complete enough for the intended purpose\n- Bounded to avoid unnecessary complexity\n- Focused on factors that impact key decisions\n- Grounded in observable reality\n\n### 3. Data Foundation Assessment\n\n**Evaluate and improve data quality systematically:**\n\n#### Data Inventory\n- Historical performance data and patterns\n- Real-time sensor/monitoring data streams\n- Configuration settings and parameters\n- External data sources and market conditions\n- Expert knowledge and domain insights\n\n#### Data Quality Analysis\n```\nFor each data source, assess:\n- Completeness: What percentage of required data is available?\n- Accuracy: How reliable and error-free is the data?\n- Timeliness: How current and frequently updated is the data?\n- Consistency: Are there conflicts between data sources?\n- Relevance: How directly does this data impact key decisions?\n\nQuality Scoring (1-10 for each dimension):\nData Source: [name]\n- Completeness: [score] - [explanation]\n- Accuracy: [score] - [explanation]  \n- Timeliness: [score] - [explanation]\n- Consistency: [score] - [explanation]\n- Relevance: [score] - [explanation]\nOverall Quality Score: [average]\n```\n\n#### Data Gap Analysis\n- Critical missing information for model accuracy\n- Alternative data sources or proxies available\n- Data collection strategies for key gaps\n- Acceptable uncertainty levels for decisions\n\n### 4. Model Construction Framework\n\n**Build the digital twin using systematic modeling approaches:**\n\n#### Component Modeling\n- Individual element behavior patterns\n- Performance characteristics and ranges\n- Response functions to different inputs\n- Degradation patterns and lifecycle factors\n- Optimization parameters and constraints\n\n#### System Interaction Modeling\n- Interface behaviors between components\n- Network effects and cascade influences\n- Resource sharing and competition dynamics\n- Communication protocols and latencies\n- Synchronization and coordination mechanisms\n\n#### Environmental Modeling\n- External factors affecting system performance\n- Market conditions and competitive dynamics\n- Regulatory constraints and compliance requirements\n- Economic factors and cost structures\n- Seasonal patterns and cyclical behaviors\n\n#### Dynamic Behavior Modeling\n- State transitions and evolutionary patterns\n- Learning and adaptation mechanisms\n- Scaling behaviors and capacity constraints\n- Stability and resilience characteristics\n- Performance under stress conditions\n\n### 5. Calibration and Validation\n\n**Ensure model accuracy through systematic testing:**\n\n#### Historical Validation\n- Back-test model predictions against known outcomes\n- Identify systematic biases and correction factors\n- Validate model accuracy across different conditions\n- Test edge cases and extreme scenarios\n- Measure prediction error distributions\n\n#### Real-Time Calibration\n- Compare model outputs to live system data\n- Implement automated calibration adjustments\n- Monitor prediction accuracy over time\n- Detect model drift and degradation\n- Update parameters based on new observations\n\n#### Sensitivity Analysis\n- Test model response to parameter variations\n- Identify critical assumptions and dependencies\n- Understand uncertainty propagation through model\n- Validate robustness to data quality issues\n- Map confidence intervals for predictions\n\n**Calibration Metrics**:\n```\nModel Performance Dashboard:\n- Overall Accuracy: [percentage]  [confidence interval]\n- Prediction Bias: [systematic error analysis]\n- Timing Accuracy: [lag prediction accuracy]\n- Extreme Event Prediction: [edge case performance]\n- Model Confidence: [uncertainty quantification]\n\nRecent Calibration Results:\n- Last Update: [timestamp]\n- Data Points Used: [count]\n- Accuracy Improvement: [change from previous]\n- Key Parameter Adjustments: [list]\n- Validation Test Results: [pass/fail with details]\n```\n\n### 6. Scenario Simulation Engine\n\n**Enable comprehensive scenario testing:**\n\n#### Scenario Design Framework\n- Baseline/current state scenarios\n- Optimization scenarios testing improvements\n- Stress test scenarios with adverse conditions\n- What-if scenarios exploring alternatives\n- Innovation scenarios with new capabilities\n\n#### Simulation Execution\n- Automated scenario batch processing\n- Interactive scenario exploration interfaces\n- Real-time simulation monitoring and controls\n- Result aggregation and statistical analysis\n- Sensitivity testing across scenario parameters\n\n#### Output Generation\n- Performance metrics and KPI tracking\n- Visual simulation results and animations\n- Statistical analysis and confidence intervals\n- Comparative analysis across scenarios\n- Recommendation generation with rationale\n\n### 7. Decision Integration\n\n**Connect simulation insights to actionable decisions:**\n\n#### Decision Framework Mapping\n- Link simulation outputs to specific decisions\n- Define decision criteria and thresholds\n- Map uncertainty levels to decision confidence\n- Establish risk tolerance for different choices\n- Create decision trees for complex scenarios\n\n#### Optimization Algorithms\n- Automated parameter optimization for goals\n- Multi-objective optimization with trade-offs\n- Constraint satisfaction for feasible solutions\n- Robust optimization under uncertainty\n- Dynamic optimization for changing conditions\n\n#### Recommendation Engine\n```\nDecision Recommendation Format:\n## Scenario: [name and description]\n\n### Recommended Action: [specific decision]\n\n### Rationale:\n- Simulation Evidence: [key findings]\n- Performance Impact: [quantified benefits]\n- Risk Assessment: [potential downsides]\n- Confidence Level: [percentage with explanation]\n\n### Implementation Guidance:\n- Immediate Actions: [specific steps]\n- Success Metrics: [measurable indicators]\n- Monitoring Plan: [ongoing validation approach]\n- Contingency Plans: [alternative actions if needed]\n\n### Assumptions and Limitations:\n- Key Assumptions: [critical model assumptions]\n- Data Limitations: [known gaps or uncertainties]\n- Model Boundaries: [what's not included]\n- Update Requirements: [when to refresh model]\n```\n\n### 8. Continuous Improvement Loop\n\n**Establish ongoing model enhancement:**\n\n#### Performance Monitoring\n- Automated accuracy tracking and alerting\n- Model drift detection and correction\n- Prediction error analysis and categorization\n- Data quality monitoring and improvement\n- User feedback collection and integration\n\n#### Model Evolution\n- Incremental model improvements based on learnings\n- New data integration and model expansion\n- Algorithm updates and enhancement\n- Scenario library expansion and refinement\n- User interface and experience improvements\n\n#### Learning Integration\n- Document insights from model successes and failures\n- Build institutional knowledge from simulation results\n- Share best practices across similar digital twins\n- Incorporate domain expert feedback and validation\n- Develop model confidence and reliability metrics\n\n### 9. Output Generation\n\n**Present digital twin capabilities and insights:**\n\n```\n## Digital Twin System: [Subject Name]\n\n### System Overview\n- Purpose: [primary decision support goals]\n- Scope: [system boundaries and components]\n- Fidelity Level: [accuracy expectations]\n- Update Frequency: [refresh schedule]\n\n### Model Architecture\n- Core Components: [key system elements]\n- Relationship Map: [interaction patterns]\n- Environmental Factors: [external influences]\n- Performance Metrics: [success indicators]\n\n### Data Foundation\n- Primary Data Sources: [list with quality scores]\n- Data Quality Assessment: [overall quality rating]\n- Update Mechanisms: [how data stays current]\n- Validation Methods: [accuracy verification approaches]\n\n### Simulation Capabilities\n- Scenario Types: [what can be modeled]\n- Time Horizons: [simulation time ranges]\n- Precision Levels: [accuracy expectations]\n- Output Formats: [reporting and visualization options]\n\n### Calibration Status\n- Historical Validation: [back-testing results]\n- Real-Time Accuracy: [current performance metrics]\n- Last Calibration: [date and improvements]\n- Confidence Intervals: [uncertainty bounds]\n\n### Decision Integration\n- Supported Decisions: [specific use cases]\n- Optimization Capabilities: [automatic improvement features]\n- Risk Assessment: [uncertainty and sensitivity analysis]\n- Recommendation Engine: [decision support features]\n\n### Usage Guidelines\n- High Confidence Scenarios: [when to trust fully]\n- Medium Confidence Scenarios: [when to use with caution]\n- Low Confidence Scenarios: [when to gather more data]\n- Refresh Triggers: [when to update the model]\n```\n\n### 10. Quality Assurance Framework\n\n**Ensure digital twin reliability and trustworthiness:**\n\n#### Validation Checklist\n- [ ] Model reproduces historical behavior accurately\n- [ ] Predictions are calibrated with confidence intervals\n- [ ] Edge cases and extreme scenarios are handled appropriately\n- [ ] Data quality meets requirements for intended decisions\n- [ ] Model boundaries are clearly defined and communicated\n- [ ] Assumptions are documented and regularly validated\n- [ ] Updates and maintenance procedures are established\n- [ ] User training and guidelines are comprehensive\n\n#### Risk Assessment\n- Model accuracy limitations and impact on decisions\n- Data dependency risks and mitigation strategies\n- Computational requirements and scalability constraints\n- User misinterpretation risks and training needs\n- System integration challenges and compatibility issues\n\n#### Success Metrics\n- Prediction accuracy improvement over time\n- Decision quality enhancement from model insights\n- Cost savings or performance improvements achieved\n- User adoption and satisfaction with digital twin\n- Model maintenance efficiency and cost effectiveness\n\n## Usage Examples\n\n```bash\n# Manufacturing optimization\n/simulation:digital-twin-creator Create digital twin of production line to optimize throughput and predict maintenance needs\n\n# Customer journey modeling\n/simulation:digital-twin-creator Build digital twin of customer acquisition funnel to test marketing strategies\n\n# Supply chain resilience\n/simulation:digital-twin-creator Model supply chain network to test disruption scenarios and optimization strategies\n\n# Software system performance\n/simulation:digital-twin-creator Create digital twin of microservices architecture to predict scaling and performance\n```\n\n## Quality Indicators\n\n- **Green**: 85%+ historical accuracy, comprehensive data foundation, automated calibration\n- **Yellow**: 70-85% accuracy, good data coverage, manual calibration processes\n- **Red**: <70% accuracy, significant data gaps, limited validation\n\n## Common Pitfalls to Avoid\n\n- Over-complexity: Modeling unnecessary details that don't impact decisions\n- Under-validation: Insufficient testing against real-world outcomes  \n- Static thinking: Not updating model as reality changes\n- Data blindness: Ignoring data quality issues and biases\n- False precision: Claiming higher accuracy than data supports\n- Poor boundaries: Including too much or too little in model scope\n\nTransform your real-world challenges into a laboratory for exponential learning and optimization."
              },
              {
                "name": "/directory-deep-dive",
                "description": "Analyze directory structure and purpose",
                "path": "plugins/all-commands/commands/directory-deep-dive.md",
                "frontmatter": {
                  "description": "Analyze directory structure and purpose",
                  "category": "utilities-debugging",
                  "argument-hint": "Specify directory path"
                },
                "content": "# Directory Deep Dive\n\nAnalyze directory structure and purpose\n\n## Instructions\n\n1. **Target Directory**\n   - Focus on the specified directory `$ARGUMENTS` or the current working directory\n\n2. **Investigate Architecture**\n   - Analyze the implementation principles and architecture of the code in this directory and its subdirectories\n   - Look for:\n     - Design patterns being used\n     - Dependencies and their purposes\n     - Key abstractions and interfaces\n     - Naming conventions and code organization\n\n3. **Create or Update Documentation**\n   - Create a CLAUDE.md file capturing this knowledge\n   - If one already exists, update it with newly discovered information\n   - Include:\n     - Purpose and responsibility of this module\n     - Key architectural decisions\n     - Important implementation details\n     - Common patterns used throughout the code\n     - Any gotchas or non-obvious behaviors\n\n4. **Ensure Proper Placement**\n   - Place the CLAUDE.md file in the directory being analyzed\n   - This ensures the context is loaded when working in that specific area\n\n## Credit\n\nThis command is based on the work of Thomas Landgraf: https://thomaslandgraf.substack.com/p/claude-codes-memory-working-with"
              },
              {
                "name": "/doc-api",
                "description": "Generate API documentation from code",
                "path": "plugins/all-commands/commands/doc-api.md",
                "frontmatter": {
                  "description": "Generate API documentation from code",
                  "category": "api-development",
                  "argument-hint": "1. **Code Analysis and Discovery**"
                },
                "content": "# API Documentation Generator Command\n\nGenerate API documentation from code\n\n## Instructions\n\nFollow this systematic approach to create API documentation: **$ARGUMENTS**\n\n1. **Code Analysis and Discovery**\n   - Scan the codebase for API endpoints, routes, and handlers\n   - Identify REST APIs, GraphQL schemas, and RPC services\n   - Map out controller classes, route definitions, and middleware\n   - Discover request/response models and data structures\n\n2. **Documentation Tool Selection**\n   - Choose appropriate documentation tools based on stack:\n     - **OpenAPI/Swagger**: REST APIs with interactive documentation\n     - **GraphQL**: GraphiQL, GraphQL Playground, or Apollo Studio\n     - **Postman**: API collections and documentation\n     - **Insomnia**: API design and documentation\n     - **Redoc**: Alternative OpenAPI renderer\n     - **API Blueprint**: Markdown-based API documentation\n\n3. **API Specification Generation**\n   \n   **For REST APIs with OpenAPI:**\n   ```yaml\n   openapi: 3.0.0\n   info:\n     title: $ARGUMENTS API\n     version: 1.0.0\n     description: Comprehensive API for $ARGUMENTS\n   servers:\n     - url: https://api.example.com/v1\n   paths:\n     /users:\n       get:\n         summary: List users\n         parameters:\n           - name: page\n             in: query\n             schema:\n               type: integer\n         responses:\n           '200':\n             description: Successful response\n             content:\n               application/json:\n                 schema:\n                   type: array\n                   items:\n                     $ref: '#/components/schemas/User'\n   components:\n     schemas:\n       User:\n         type: object\n         properties:\n           id:\n             type: integer\n           name:\n             type: string\n           email:\n             type: string\n   ```\n\n4. **Endpoint Documentation**\n   - Document all HTTP methods (GET, POST, PUT, DELETE, PATCH)\n   - Specify request parameters (path, query, header, body)\n   - Define response schemas and status codes\n   - Include error responses and error codes\n   - Document authentication and authorization requirements\n\n5. **Request/Response Examples**\n   - Provide realistic request examples for each endpoint\n   - Include sample response data with proper formatting\n   - Show different response scenarios (success, error, edge cases)\n   - Document content types and encoding\n\n6. **Authentication Documentation**\n   - Document authentication methods (API keys, JWT, OAuth)\n   - Explain authorization scopes and permissions\n   - Provide authentication examples and token formats\n   - Document session management and refresh token flows\n\n7. **Data Model Documentation**\n   - Define all data schemas and models\n   - Document field types, constraints, and validation rules\n   - Include relationships between entities\n   - Provide example data structures\n\n8. **Error Handling Documentation**\n   - Document all possible error responses\n   - Explain error codes and their meanings\n   - Provide troubleshooting guidance\n   - Include rate limiting and throttling information\n\n9. **Interactive Documentation Setup**\n   \n   **Swagger UI Integration:**\n   ```html\n   <!DOCTYPE html>\n   <html>\n   <head>\n     <title>API Documentation</title>\n     <link rel=\"stylesheet\" type=\"text/css\" href=\"./swagger-ui-bundle.css\" />\n   </head>\n   <body>\n     <div id=\"swagger-ui\"></div>\n     <script src=\"./swagger-ui-bundle.js\"></script>\n     <script>\n       SwaggerUIBundle({\n         url: './api-spec.yaml',\n         dom_id: '#swagger-ui'\n       });\n     </script>\n   </body>\n   </html>\n   ```\n\n10. **Code Annotation and Comments**\n    - Add inline documentation to API handlers\n    - Use framework-specific annotation tools:\n      - **Java**: @ApiOperation, @ApiParam (Swagger annotations)\n      - **Python**: Docstrings with FastAPI or Flask-RESTX\n      - **Node.js**: JSDoc comments with swagger-jsdoc\n      - **C#**: XML documentation comments\n\n11. **Automated Documentation Generation**\n    \n    **For Node.js/Express:**\n    ```javascript\n    const swaggerJsdoc = require('swagger-jsdoc');\n    const swaggerUi = require('swagger-ui-express');\n    \n    const options = {\n      definition: {\n        openapi: '3.0.0',\n        info: {\n          title: 'API Documentation',\n          version: '1.0.0',\n        },\n      },\n      apis: ['./routes/*.js'],\n    };\n    \n    const specs = swaggerJsdoc(options);\n    app.use('/api-docs', swaggerUi.serve, swaggerUi.setup(specs));\n    ```\n\n12. **Testing Integration**\n    - Generate API test collections from documentation\n    - Include test scripts and validation rules\n    - Set up automated API testing\n    - Document test scenarios and expected outcomes\n\n13. **Version Management**\n    - Document API versioning strategy\n    - Maintain documentation for multiple API versions\n    - Document deprecation timelines and migration guides\n    - Track breaking changes between versions\n\n14. **Performance Documentation**\n    - Document rate limits and throttling policies\n    - Include performance benchmarks and SLAs\n    - Document caching strategies and headers\n    - Explain pagination and filtering options\n\n15. **SDK and Client Library Documentation**\n    - Generate client libraries from API specifications\n    - Document SDK usage and examples\n    - Provide quickstart guides for different languages\n    - Include integration examples and best practices\n\n16. **Environment-Specific Documentation**\n    - Document different environments (dev, staging, prod)\n    - Include environment-specific endpoints and configurations\n    - Document deployment and configuration requirements\n    - Provide environment setup instructions\n\n17. **Security Documentation**\n    - Document security best practices\n    - Include CORS and CSP policies\n    - Document input validation and sanitization\n    - Explain security headers and their purposes\n\n18. **Maintenance and Updates**\n    - Set up automated documentation updates\n    - Create processes for keeping documentation current\n    - Review and validate documentation regularly\n    - Integrate documentation reviews into development workflow\n\n**Framework-Specific Examples:**\n\n**FastAPI (Python):**\n```python\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\n\napp = FastAPI(title=\"My API\", version=\"1.0.0\")\n\nclass User(BaseModel):\n    id: int\n    name: str\n    email: str\n\n@app.get(\"/users/{user_id}\", response_model=User)\nasync def get_user(user_id: int):\n    \"\"\"Get a user by ID.\"\"\"\n    return {\"id\": user_id, \"name\": \"John\", \"email\": \"john@example.com\"}\n```\n\n**Spring Boot (Java):**\n```java\n@RestController\n@Api(tags = \"Users\")\npublic class UserController {\n    \n    @GetMapping(\"/users/{id}\")\n    @ApiOperation(value = \"Get user by ID\")\n    public ResponseEntity<User> getUser(\n        @PathVariable @ApiParam(\"User ID\") Long id) {\n        // Implementation\n    }\n}\n```\n\nRemember to keep documentation up-to-date with code changes and make it easily accessible to both internal teams and external consumers."
              },
              {
                "name": "/docs",
                "description": "Update or generate YAML documentation for SQL models with proper descriptions and tests",
                "path": "plugins/all-commands/commands/docs.md",
                "frontmatter": {
                  "description": "Update or generate YAML documentation for SQL models with proper descriptions and tests",
                  "category": "documentation-changelogs",
                  "argument-hint": "<model_name_or_path>",
                  "allowed-tools": "Read, Write, Edit"
                },
                "content": "$ARGUMENTS\n\nUpdate or generate the YAML docs for this SQL model or folder of models. Look for a matching YAML file or documentation for this model inside a combined YAML file in the same directory. If the YAML for the given SQL model is not included, generate it from scratch based on the SQL code and anything that can be inferred from the upstream files and their YAML. Put the resulting YAML in a separate file matching the name of the model, and if necessary remove this model from any combined YAML files.\n\nUse the `generate_model_yaml` operation to determine the canonical list of columns and data types. Add/update all data types in any existing YAML. If no there is no existing YAML file, add descriptions (and tests, if necessary) to the output of this operation. In this case (and only this case), remove columns that have been commented out or excluded from the SQL.\n\n- Make sure to add a brief description for the model. Infer the model type (staging, intermediate, or mart) and include information about its sources if important. (This doesn't mean adding a `source` property.)\n- Carry over descriptions and tests from any matching upstream columns, or update as necessary for derived columns. Ignore relationship tests to a different modeling layer. Ignore any included models or sources that are not directly referenced in this model.\n- If a uniqueness test for more than one column is required, use `unique_combination_of_columns` from the dbt_utils package and put it after the model description and before `columns:`, under `data_tests:`. Only add such a test if explicitly requested or if there is such a test upstream, all columns are present in this model, and the cardinality of this model appears to match. Do not change this test if it already exists.\n- A uniqueness/primary key test for a single column should be the standard `unique` and `not_null` tests on that column only.\n- Use the `data_tests:` syntax\n- Add tests for individual columns under `models.columns`; do not use the model-wide `models.data_tests` unless directed to do so.\n- Don't include `version: 2` at the top; just start with `models:`\n- Do not make guesses about accepted values. Include accepted values tests when (and only when) the column's values are explicitly"
              },
              {
                "name": "/e2e-setup",
                "description": "Configure end-to-end testing suite",
                "path": "plugins/all-commands/commands/e2e-setup.md",
                "frontmatter": {
                  "description": "Configure end-to-end testing suite",
                  "category": "code-analysis-testing",
                  "argument-hint": "1. **Technology Stack Assessment**",
                  "allowed-tools": "Bash(npm *)"
                },
                "content": "# End-to-End Testing Setup Command\n\nConfigure end-to-end testing suite\n\n## Instructions\n\nFollow this systematic approach to implement E2E testing: **$ARGUMENTS**\n\n1. **Technology Stack Assessment**\n   - Identify the application type (web app, mobile app, API service)\n   - Review existing testing infrastructure\n   - Determine target browsers and devices\n   - Assess current deployment and staging environments\n\n2. **E2E Framework Selection**\n   - Choose appropriate E2E testing framework based on stack:\n     - **Playwright**: Modern, fast, supports multiple browsers\n     - **Cypress**: Developer-friendly, great debugging tools\n     - **Selenium WebDriver**: Cross-browser, mature ecosystem\n     - **Puppeteer**: Chrome-focused, good for performance testing\n     - **TestCafe**: No WebDriver needed, easy setup\n   - Consider team expertise and project requirements\n\n3. **Test Environment Setup**\n   - Set up dedicated testing environments (staging, QA)\n   - Configure test databases with sample data\n   - Set up environment variables and configuration\n   - Ensure environment isolation and reproducibility\n\n4. **Framework Installation and Configuration**\n   \n   **For Playwright:**\n   ```bash\n   npm install -D @playwright/test\n   npx playwright install\n   npx playwright codegen # Record tests\n   ```\n\n   **For Cypress:**\n   ```bash\n   npm install -D cypress\n   npx cypress open\n   ```\n\n   **For Selenium:**\n   ```bash\n   npm install -D selenium-webdriver\n   # Install browser drivers\n   ```\n\n5. **Test Structure Organization**\n   - Create logical test folder structure:\n     ```\n     e2e/\n      tests/\n         auth/\n         user-flows/\n         api/\n      fixtures/\n      support/\n         commands/\n         page-objects/\n      config/\n     ```\n   - Organize tests by feature or user journey\n   - Separate API tests from UI tests\n\n6. **Page Object Model Implementation**\n   - Create page object classes for better maintainability\n   - Encapsulate element selectors and interactions\n   - Implement reusable methods for common actions\n   - Follow single responsibility principle for page objects\n\n   **Example Page Object:**\n   ```javascript\n   class LoginPage {\n     constructor(page) {\n       this.page = page;\n       this.emailInput = page.locator('#email');\n       this.passwordInput = page.locator('#password');\n       this.loginButton = page.locator('#login-btn');\n     }\n\n     async login(email, password) {\n       await this.emailInput.fill(email);\n       await this.passwordInput.fill(password);\n       await this.loginButton.click();\n     }\n   }\n   ```\n\n7. **Test Data Management**\n   - Create test fixtures and sample data\n   - Implement data factories for dynamic test data\n   - Set up database seeding for consistent test states\n   - Use environment-specific test data\n   - Implement test data cleanup strategies\n\n8. **Core User Journey Testing**\n   - Implement critical user flows:\n     - User registration and authentication\n     - Main application workflows\n     - Payment and transaction flows\n     - Search and filtering functionality\n     - Form submissions and validations\n\n9. **Cross-Browser Testing Setup**\n   - Configure testing across multiple browsers\n   - Set up browser-specific configurations\n   - Implement responsive design testing\n   - Test on different viewport sizes\n\n   **Playwright Browser Configuration:**\n   ```javascript\n   module.exports = {\n     projects: [\n       { name: 'chromium', use: { ...devices['Desktop Chrome'] } },\n       { name: 'firefox', use: { ...devices['Desktop Firefox'] } },\n       { name: 'webkit', use: { ...devices['Desktop Safari'] } },\n       { name: 'mobile', use: { ...devices['iPhone 12'] } },\n     ],\n   };\n   ```\n\n10. **API Testing Integration**\n    - Test API endpoints alongside UI tests\n    - Implement API request/response validation\n    - Test authentication and authorization\n    - Verify data consistency between API and UI\n\n11. **Visual Testing Setup**\n    - Implement screenshot comparison testing\n    - Set up visual regression testing\n    - Configure tolerance levels for visual changes\n    - Organize visual baselines and updates\n\n12. **Test Utilities and Helpers**\n    - Create custom commands and utilities\n    - Implement common assertion helpers\n    - Set up authentication helpers\n    - Create database and state management utilities\n\n13. **Error Handling and Debugging**\n    - Configure proper error reporting and screenshots\n    - Set up video recording for failed tests\n    - Implement retry mechanisms for flaky tests\n    - Create debugging tools and helpers\n\n14. **CI/CD Integration**\n    - Configure E2E tests in CI/CD pipeline\n    - Set up parallel test execution\n    - Implement proper test reporting\n    - Configure test environment provisioning\n\n   **GitHub Actions Example:**\n   ```yaml\n   - name: Run Playwright tests\n     run: npx playwright test\n   - uses: actions/upload-artifact@v3\n     if: always()\n     with:\n       name: playwright-report\n       path: playwright-report/\n   ```\n\n15. **Performance Testing Integration**\n    - Add performance assertions to E2E tests\n    - Monitor page load times and metrics\n    - Test under different network conditions\n    - Implement lighthouse audits integration\n\n16. **Accessibility Testing**\n    - Integrate accessibility testing tools (axe-core)\n    - Test keyboard navigation flows\n    - Verify screen reader compatibility\n    - Check color contrast and WCAG compliance\n\n17. **Mobile Testing Setup**\n    - Configure mobile device emulation\n    - Test responsive design breakpoints\n    - Implement touch gesture testing\n    - Test mobile-specific features\n\n18. **Reporting and Monitoring**\n    - Set up comprehensive test reporting\n    - Configure test result notifications\n    - Implement test metrics and analytics\n    - Create dashboards for test health monitoring\n\n19. **Test Maintenance Strategy**\n    - Implement test stability monitoring\n    - Set up automatic test updates for UI changes\n    - Create test review and update processes\n    - Document test maintenance procedures\n\n20. **Security Testing Integration**\n    - Test authentication and authorization flows\n    - Implement security headers validation\n    - Test input sanitization and XSS prevention\n    - Verify HTTPS and secure cookie handling\n\n**Sample E2E Test:**\n```javascript\ntest('user can complete purchase flow', async ({ page }) => {\n  // Navigate and login\n  await page.goto('/login');\n  await page.fill('#email', 'test@example.com');\n  await page.fill('#password', 'password');\n  await page.click('#login-btn');\n\n  // Add item to cart\n  await page.goto('/products');\n  await page.click('[data-testid=\"product-1\"]');\n  await page.click('#add-to-cart');\n\n  // Complete checkout\n  await page.goto('/checkout');\n  await page.fill('#card-number', '4111111111111111');\n  await page.click('#place-order');\n\n  // Verify success\n  await expect(page.locator('#order-confirmation')).toBeVisible();\n});\n```\n\nRemember to start with critical user journeys and gradually expand coverage. Focus on stable, maintainable tests that provide real value."
              },
              {
                "name": "/estimate-assistant",
                "description": "Generate accurate project time estimates",
                "path": "plugins/all-commands/commands/estimate-assistant.md",
                "frontmatter": {
                  "description": "Generate accurate project time estimates",
                  "category": "team-collaboration",
                  "allowed-tools": "Bash(git *), Bash(gh *)"
                },
                "content": "# estimate-assistant\n\nGenerate accurate project time estimates\n\n## Purpose\nThis command analyzes past commits, PR completion times, code complexity metrics, and team performance to provide accurate task estimates. It helps teams move beyond gut-feel estimates to data-backed predictions.\n\n## Usage\n```bash\n# Estimate a specific task based on description\nclaude \"Estimate task: Implement OAuth2 login flow with Google\"\n\n# Analyze historical accuracy of estimates\nclaude \"Show estimation accuracy for the last 10 sprints\"\n\n# Estimate based on code changes\nclaude \"Estimate effort for refactoring src/api/users module\"\n\n# Get team member specific estimates\nclaude \"How long would it take Alice to implement the payment webhook handler?\"\n```\n\n## Instructions\n\n### 1. Gather Historical Data\nCollect data from git history and Linear:\n\n```bash\n# Get commit history with timestamps and authors\ngit log --pretty=format:\"%h|%an|%ad|%s\" --date=iso --since=\"6 months ago\" > commit_history.txt\n\n# Analyze PR completion times\ngh pr list --state closed --limit 100 --json number,title,createdAt,closedAt,additions,deletions,files\n\n# Get file change frequency\ngit log --pretty=format: --name-only --since=\"6 months ago\" | sort | uniq -c | sort -rn\n\n# Analyze commit patterns by author\ngit shortlog -sn --since=\"6 months ago\"\n```\n\n### 2. Calculate Code Complexity Metrics\nAnalyze code characteristics:\n\n```javascript\nfunction analyzeComplexity(filePath) {\n  const metrics = {\n    lines: 0,\n    cyclomaticComplexity: 0,\n    dependencies: 0,\n    testCoverage: 0,\n    similarFiles: []\n  };\n  \n  // Count lines of code\n  const content = readFile(filePath);\n  metrics.lines = content.split('\\n').length;\n  \n  // Cyclomatic complexity (simplified)\n  const conditions = content.match(/if\\s*\\(|while\\s*\\(|for\\s*\\(|case\\s+|\\?\\s*:/g);\n  metrics.cyclomaticComplexity = (conditions?.length || 0) + 1;\n  \n  // Count imports/dependencies\n  const imports = content.match(/import.*from|require\\(/g);\n  metrics.dependencies = imports?.length || 0;\n  \n  // Find similar files by structure\n  metrics.similarFiles = findSimilarFiles(filePath);\n  \n  return metrics;\n}\n```\n\n### 3. Build Estimation Models\n\n#### Time-Based Estimation\n```javascript\nclass HistoricalEstimator {\n  constructor(gitData, linearData) {\n    this.gitData = gitData;\n    this.linearData = linearData;\n    this.authorVelocity = new Map();\n    this.fileTypeMultipliers = new Map();\n  }\n  \n  calculateAuthorVelocity(author) {\n    const authorCommits = this.gitData.filter(c => c.author === author);\n    const taskCompletions = this.linearData.filter(t => \n      t.assignee === author && t.completedAt\n    );\n    \n    // Lines of code per day\n    const totalLines = authorCommits.reduce((sum, c) => \n      sum + c.additions + c.deletions, 0\n    );\n    const totalDays = this.calculateWorkDays(authorCommits);\n    const linesPerDay = totalLines / totalDays;\n    \n    // Story points per sprint\n    const pointsCompleted = taskCompletions.reduce((sum, t) => \n      sum + (t.estimate || 0), 0\n    );\n    const sprintCount = this.countSprints(taskCompletions);\n    const pointsPerSprint = pointsCompleted / sprintCount;\n    \n    return {\n      linesPerDay,\n      pointsPerSprint,\n      averageTaskDuration: this.calculateAverageTaskDuration(taskCompletions),\n      accuracy: this.calculateEstimateAccuracy(taskCompletions)\n    };\n  }\n  \n  estimateTask(description, assignee = null) {\n    // Extract key features from description\n    const features = this.extractFeatures(description);\n    \n    // Find similar completed tasks\n    const similarTasks = this.findSimilarTasks(features);\n    \n    // Base estimate from similar tasks\n    let baseEstimate = this.calculateMedianEstimate(similarTasks);\n    \n    // Adjust for complexity indicators\n    const complexityMultiplier = this.calculateComplexityMultiplier(features);\n    baseEstimate *= complexityMultiplier;\n    \n    // Adjust for assignee if specified\n    if (assignee) {\n      const velocity = this.calculateAuthorVelocity(assignee);\n      const teamAvgVelocity = this.calculateTeamAverageVelocity();\n      const velocityRatio = velocity.pointsPerSprint / teamAvgVelocity;\n      baseEstimate *= (2 - velocityRatio); // Faster devs get lower estimates\n    }\n    \n    // Add confidence interval\n    const confidence = this.calculateConfidence(similarTasks.length, features);\n    \n    return {\n      estimate: Math.round(baseEstimate),\n      confidence,\n      range: {\n        min: Math.round(baseEstimate * 0.7),\n        max: Math.round(baseEstimate * 1.5)\n      },\n      basedOn: similarTasks.slice(0, 3),\n      factors: this.explainFactors(features, complexityMultiplier)\n    };\n  }\n}\n```\n\n#### Pattern Recognition\n```javascript\nfunction extractFeatures(taskDescription) {\n  const features = {\n    keywords: [],\n    fileTypes: [],\n    modules: [],\n    complexity: 'medium',\n    type: 'feature', // feature, bug, refactor, etc.\n    hasTests: false,\n    hasUI: false,\n    hasAPI: false,\n    hasDatabase: false\n  };\n  \n  // Keywords that indicate complexity\n  const complexityKeywords = {\n    high: ['refactor', 'migrate', 'redesign', 'optimize', 'architecture'],\n    medium: ['implement', 'add', 'create', 'update', 'integrate'],\n    low: ['fix', 'adjust', 'tweak', 'change', 'modify']\n  };\n  \n  // Detect task type\n  if (taskDescription.match(/bug|fix|repair|broken/i)) {\n    features.type = 'bug';\n  } else if (taskDescription.match(/refactor|cleanup|optimize/i)) {\n    features.type = 'refactor';\n  } else if (taskDescription.match(/test|spec|coverage/i)) {\n    features.type = 'test';\n  }\n  \n  // Detect components\n  features.hasUI = /UI|frontend|component|view|page/i.test(taskDescription);\n  features.hasAPI = /API|endpoint|route|REST|GraphQL/i.test(taskDescription);\n  features.hasDatabase = /database|DB|migration|schema|query/i.test(taskDescription);\n  features.hasTests = /test|spec|TDD|coverage/i.test(taskDescription);\n  \n  // Extract file types mentioned\n  const fileTypeMatches = taskDescription.match(/\\.(js|ts|jsx|tsx|py|java|go|rb|css|scss)/g);\n  if (fileTypeMatches) {\n    features.fileTypes = [...new Set(fileTypeMatches)];\n  }\n  \n  return features;\n}\n```\n\n### 4. Velocity Tracking\nTrack team and individual performance:\n\n```javascript\nclass VelocityTracker {\n  async analyzeVelocity(timeframe = '3 months') {\n    // Get completed tasks with estimates and actual time\n    const completedTasks = await this.getCompletedTasks(timeframe);\n    \n    const analysis = {\n      team: {\n        plannedPoints: 0,\n        completedPoints: 0,\n        averageVelocity: 0,\n        velocityTrend: [],\n        estimateAccuracy: 0\n      },\n      individuals: new Map(),\n      taskTypes: new Map()\n    };\n    \n    // Group by sprint\n    const tasksBySprint = this.groupBySprint(completedTasks);\n    \n    for (const [sprint, tasks] of tasksBySprint) {\n      const sprintVelocity = tasks.reduce((sum, t) => sum + (t.estimate || 0), 0);\n      const sprintActual = tasks.reduce((sum, t) => sum + (t.actualPoints || t.estimate || 0), 0);\n      \n      analysis.team.velocityTrend.push({\n        sprint,\n        planned: sprintVelocity,\n        actual: sprintActual,\n        accuracy: sprintVelocity ? (sprintActual / sprintVelocity) : 1\n      });\n    }\n    \n    // Individual velocity\n    const tasksByAssignee = this.groupBy(completedTasks, 'assignee');\n    for (const [assignee, tasks] of tasksByAssignee) {\n      analysis.individuals.set(assignee, {\n        tasksCompleted: tasks.length,\n        pointsCompleted: tasks.reduce((sum, t) => sum + (t.estimate || 0), 0),\n        averageAccuracy: this.calculateAccuracy(tasks),\n        strengths: this.identifyStrengths(tasks)\n      });\n    }\n    \n    return analysis;\n  }\n}\n```\n\n### 5. Machine Learning Estimation\nUse historical patterns for prediction:\n\n```javascript\nclass MLEstimator {\n  trainModel(historicalTasks) {\n    // Feature extraction\n    const features = historicalTasks.map(task => ({\n      // Text features\n      titleLength: task.title.length,\n      descriptionLength: task.description.length,\n      hasAcceptanceCriteria: task.description.includes('Acceptance'),\n      \n      // Code features\n      filesChanged: task.linkedPR?.filesChanged || 0,\n      linesAdded: task.linkedPR?.additions || 0,\n      linesDeleted: task.linkedPR?.deletions || 0,\n      \n      // Task features\n      labels: task.labels.length,\n      hasDesignDoc: task.attachments?.some(a => a.title.includes('design')),\n      dependencies: task.blockedBy?.length || 0,\n      \n      // Historical features\n      assigneeAvgVelocity: this.getAssigneeVelocity(task.assignee),\n      teamLoad: this.getTeamLoad(task.createdAt),\n      \n      // Target\n      actualEffort: task.actualPoints || task.estimate\n    }));\n    \n    // Simple linear regression (in practice, use a proper ML library)\n    return this.fitLinearModel(features);\n  }\n  \n  predict(taskDescription, context) {\n    const features = this.extractTaskFeatures(taskDescription, context);\n    const prediction = this.model.predict(features);\n    \n    // Add uncertainty based on feature similarity\n    const similarityScore = this.calculateSimilarity(features);\n    const uncertainty = 1 - similarityScore;\n    \n    return {\n      estimate: Math.round(prediction),\n      confidence: similarityScore,\n      breakdown: this.explainPrediction(features, prediction)\n    };\n  }\n}\n```\n\n### 6. Estimation Report Format\n\n```markdown\n## Task Estimation Report\n\n**Task:** Implement OAuth2 login flow with Google\n**Date:** 2024-01-15\n\n### Estimate: 5 Story Points (2)\n**Confidence:** 78%\n**Estimated Hours:** 15-25 hours\n\n### Analysis Breakdown\n\n#### Similar Completed Tasks:\n1. \"Implement GitHub OAuth integration\" - 5 points (actual: 6)\n2. \"Add Facebook login\" - 4 points (actual: 4)  \n3. \"Setup SAML SSO\" - 8 points (actual: 7)\n\n#### Complexity Factors:\n- **Authentication Flow** (+1 point): OAuth2 requires multiple redirects\n- **External API** (+1 point): Google API integration\n- **Security** (+1 point): Token storage and validation\n- **Testing** (-0.5 points): Similar tests already exist\n\n#### Historical Data:\n- Team average for auth features: 4.8 points\n- Last 5 auth tasks accuracy: 85%\n- Assignee velocity: 1.2x team average\n\n#### Risk Factors:\n Google API changes frequently\n No existing OAuth2 infrastructure\n Team has OAuth experience\n Good documentation available\n\n### Recommendations:\n1. Allocate 1 point for initial Google API setup\n2. Include time for security review\n3. Plan for integration tests with mock OAuth server\n4. Consider pairing with team member who did GitHub OAuth\n\n### Sprint Planning:\n- Can be completed in one sprint\n- Best paired with other auth-related tasks\n- Should not be last task in sprint (risk buffer)\n```\n\n### 7. Error Handling\n```javascript\n// Handle missing historical data\nif (historicalTasks.length < 10) {\n  console.warn(\"Limited historical data. Estimates may be less accurate.\");\n  // Fall back to rule-based estimation\n}\n\n// Handle new types of work\nconst similarity = findSimilarTasks(description);\nif (similarity.maxScore < 0.5) {\n  console.warn(\"This appears to be a new type of task. Using conservative estimate.\");\n  // Apply uncertainty multiplier\n}\n\n// Handle missing Linear connection\nif (!linear.available) {\n  console.log(\"Using git history only for estimation\");\n  // Use git-based estimation\n}\n```\n\n## Example Output\n\n```\nAnalyzing task: \"Refactor user authentication to use JWT tokens\"\n\n Historical Analysis:\n- Found 23 similar authentication tasks\n- Average completion: 4.2 story points\n- Accuracy rate: 82%\n\n Estimation Calculation:\nBase estimate: 4 points (from similar tasks)\nAdjustments:\n  +1 point - Refactoring (higher complexity)\n  +0.5 points - Security implications  \n  -0.5 points - Existing test coverage\n  \nFinal estimate: 5 story points\n\n Confidence Analysis:\n- High similarity to previous tasks (85%)\n- Good historical data (23 samples)\n- Confidence: 78%\n\n Team Insights:\n- Alice: Completed 3 similar tasks (avg 4.3 points)\n- Bob: Strong in refactoring (20% faster than average)\n- Recommended assignee: Bob\n\n Time Estimates:\n- Optimistic: 12 hours (3 points)\n- Realistic: 20 hours (5 points)\n- Pessimistic: 32 hours (8 points)\n\n Breakdown:\n1. Analyze current auth system (0.5 points)\n2. Design JWT token structure (0.5 points)\n3. Implement JWT service (1.5 points)\n4. Refactor auth middleware (1.5 points)\n5. Update tests and documentation (1 point)\n```\n\n## Tips\n- Maintain historical data for at least 6 months\n- Re-calibrate estimates after each sprint\n- Track actual vs estimated for continuous improvement\n- Consider external factors (holidays, team changes)\n- Use pair programming multipliers for complex tasks\n- Document assumptions in estimates\n- Review estimates in retros"
              },
              {
                "name": "/explain-code",
                "description": "Analyze and explain code functionality",
                "path": "plugins/all-commands/commands/explain-code.md",
                "frontmatter": {
                  "description": "Analyze and explain code functionality",
                  "category": "utilities-debugging",
                  "argument-hint": "1. **Code Context Analysis**"
                },
                "content": "# Analyze and Explain Code Functionality\n\nAnalyze and explain code functionality\n\n## Instructions\n\nFollow this systematic approach to explain code: **$ARGUMENTS**\n\n1. **Code Context Analysis**\n   - Identify the programming language and framework\n   - Understand the broader context and purpose of the code\n   - Identify the file location and its role in the project\n   - Review related imports, dependencies, and configurations\n\n2. **High-Level Overview**\n   - Provide a summary of what the code does\n   - Explain the main purpose and functionality\n   - Identify the problem the code is solving\n   - Describe how it fits into the larger system\n\n3. **Code Structure Breakdown**\n   - Break down the code into logical sections\n   - Identify classes, functions, and methods\n   - Explain the overall architecture and design patterns\n   - Map out data flow and control flow\n\n4. **Line-by-Line Analysis**\n   - Explain complex or non-obvious lines of code\n   - Describe variable declarations and their purposes\n   - Explain function calls and their parameters\n   - Clarify conditional logic and loops\n\n5. **Algorithm and Logic Explanation**\n   - Describe the algorithm or approach being used\n   - Explain the logic behind complex calculations\n   - Break down nested conditions and loops\n   - Clarify recursive or asynchronous operations\n\n6. **Data Structures and Types**\n   - Explain data types and structures being used\n   - Describe how data is transformed or processed\n   - Explain object relationships and hierarchies\n   - Clarify input and output formats\n\n7. **Framework and Library Usage**\n   - Explain framework-specific patterns and conventions\n   - Describe library functions and their purposes\n   - Explain API calls and their expected responses\n   - Clarify configuration and setup code\n\n8. **Error Handling and Edge Cases**\n   - Explain error handling mechanisms\n   - Describe exception handling and recovery\n   - Identify edge cases being handled\n   - Explain validation and defensive programming\n\n9. **Performance Considerations**\n   - Identify performance-critical sections\n   - Explain optimization techniques being used\n   - Describe complexity and scalability implications\n   - Point out potential bottlenecks or inefficiencies\n\n10. **Security Implications**\n    - Identify security-related code sections\n    - Explain authentication and authorization logic\n    - Describe input validation and sanitization\n    - Point out potential security vulnerabilities\n\n11. **Testing and Debugging**\n    - Explain how the code can be tested\n    - Identify debugging points and logging\n    - Describe mock data or test scenarios\n    - Explain test helpers and utilities\n\n12. **Dependencies and Integrations**\n    - Explain external service integrations\n    - Describe database operations and queries\n    - Explain API interactions and protocols\n    - Clarify third-party library usage\n\n**Explanation Format Examples:**\n\n**For Complex Algorithms:**\n```\nThis function implements a depth-first search algorithm:\n\n1. Line 1-3: Initialize a stack with the starting node and a visited set\n2. Line 4-8: Main loop - continue until stack is empty\n3. Line 9-11: Pop a node and check if it's the target\n4. Line 12-15: Add unvisited neighbors to the stack\n5. Line 16: Return null if target not found\n\nTime Complexity: O(V + E) where V is vertices and E is edges\nSpace Complexity: O(V) for the visited set and stack\n```\n\n**For API Integration Code:**\n```\nThis code handles user authentication with a third-party service:\n\n1. Extract credentials from request headers\n2. Validate credential format and required fields\n3. Make API call to authentication service\n4. Handle response and extract user data\n5. Create session token and set cookies\n6. Return user profile or error response\n\nError Handling: Catches network errors, invalid credentials, and service unavailability\nSecurity: Uses HTTPS, validates inputs, and sanitizes responses\n```\n\n**For Database Operations:**\n```\nThis function performs a complex database query with joins:\n\n1. Build base query with primary table\n2. Add LEFT JOIN for related user data\n3. Apply WHERE conditions for filtering\n4. Add ORDER BY for consistent sorting\n5. Implement pagination with LIMIT/OFFSET\n6. Execute query and handle potential errors\n7. Transform raw results into domain objects\n\nPerformance Notes: Uses indexes on filtered columns, implements connection pooling\n```\n\n13. **Common Patterns and Idioms**\n    - Identify language-specific patterns and idioms\n    - Explain design patterns being implemented\n    - Describe architectural patterns in use\n    - Clarify naming conventions and code style\n\n14. **Potential Improvements**\n    - Suggest code improvements and optimizations\n    - Identify possible refactoring opportunities\n    - Point out maintainability concerns\n    - Recommend best practices and standards\n\n15. **Related Code and Context**\n    - Reference related functions and classes\n    - Explain how this code interacts with other components\n    - Describe the calling context and usage patterns\n    - Point to relevant documentation and resources\n\n16. **Debugging and Troubleshooting**\n    - Explain how to debug issues in this code\n    - Identify common failure points\n    - Describe logging and monitoring approaches\n    - Suggest testing strategies\n\n**Language-Specific Considerations:**\n\n**JavaScript/TypeScript:**\n- Explain async/await and Promise handling\n- Describe closure and scope behavior\n- Clarify this binding and arrow functions\n- Explain event handling and callbacks\n\n**Python:**\n- Explain list comprehensions and generators\n- Describe decorator usage and purpose\n- Clarify context managers and with statements\n- Explain class inheritance and method resolution\n\n**Java:**\n- Explain generics and type parameters\n- Describe annotation usage and processing\n- Clarify stream operations and lambda expressions\n- Explain exception hierarchy and handling\n\n**C#:**\n- Explain LINQ queries and expressions\n- Describe async/await and Task handling\n- Clarify delegate and event usage\n- Explain nullable reference types\n\n**Go:**\n- Explain goroutines and channel usage\n- Describe interface implementation\n- Clarify error handling patterns\n- Explain package structure and imports\n\n**Rust:**\n- Explain ownership and borrowing\n- Describe lifetime annotations\n- Clarify pattern matching and Option/Result types\n- Explain trait implementations\n\nRemember to:\n- Use clear, non-technical language when possible\n- Provide examples and analogies for complex concepts\n- Structure explanations logically from high-level to detailed\n- Include visual diagrams or flowcharts when helpful\n- Tailor the explanation level to the intended audience"
              },
              {
                "name": "/explain-issue-fix",
                "description": "Explain how tasks in an issue were implemented with detailed breakdown",
                "path": "plugins/all-commands/commands/explain-issue-fix.md",
                "frontmatter": {
                  "description": "Explain how tasks in an issue were implemented with detailed breakdown",
                  "category": "documentation-changelogs"
                },
                "content": "Analyze the recent changes and create a detailed explanation of how the issue was resolved.\n\n## Process:\n\n1. **Review recent changes**:\n   - Check git diff for uncommitted changes\n   - Review recent commits if changes are already committed\n   - Identify all modified files\n\n2. **Analyze the implementation**:\n   - Identify what problem was being solved\n   - Document the approach taken\n   - Explain key code changes\n   - Note any design decisions made\n\n3. **Create detailed breakdown**:\n   - Problem statement\n   - Solution approach\n   - Implementation details\n   - Files modified and why\n   - Any trade-offs or alternatives considered\n\n4. **Generate explanation**:\n   - Write a clear, structured explanation\n   - Include code snippets where relevant\n   - Highlight important changes\n   - Document any follow-up tasks if needed"
              },
              {
                "name": "/find",
                "description": "Search and locate tasks across all orchestrations using various criteria.",
                "path": "plugins/all-commands/commands/find.md",
                "frontmatter": {
                  "description": "Search and locate tasks across all orchestrations using various criteria.",
                  "category": "workflow-orchestration",
                  "allowed-tools": "Read"
                },
                "content": "# Task Find Command\n\nSearch and locate tasks across all orchestrations using various criteria.\n\n## Usage\n\n```\n/task-find [search-term] [options]\n```\n\n## Description\n\nPowerful search functionality to quickly locate tasks by ID, content, status, dependencies, or any other criteria. Supports regex, fuzzy matching, and complex queries.\n\n## Basic Search\n\n### By Task ID\n```\n/task-find TASK-001\n/task-find TASK-*\n```\n\n### By Title/Content\n```\n/task-find \"authentication\"\n/task-find \"payment processing\"\n```\n\n### By Status\n```\n/task-find --status in_progress\n/task-find --status qa,completed\n```\n\n## Advanced Search\n\n### Regular Expression\n```\n/task-find --regex \"JWT|OAuth\"\n/task-find --regex \"TASK-0[0-9]{2}\"\n```\n\n### Fuzzy Search\n```\n/task-find --fuzzy \"autentication\"  # finds \"authentication\"\n/task-find --fuzzy \"paymnt\"         # finds \"payment\"\n```\n\n### Multiple Criteria\n```\n/task-find --status todos --priority high --type feature\n/task-find --agent dev-backend --created-after yesterday\n```\n\n## Search Operators\n\n### Boolean Operators\n```\n/task-find \"auth AND login\"\n/task-find \"payment OR billing\"\n/task-find \"security NOT test\"\n```\n\n### Field-Specific Search\n```\n/task-find title:\"user authentication\"\n/task-find description:\"security vulnerability\"\n/task-find agent:dev-frontend\n/task-find blocks:TASK-001\n```\n\n### Date Ranges\n```\n/task-find --created \"2024-03-10..2024-03-15\"\n/task-find --modified \"last 3 days\"\n/task-find --completed \"this week\"\n```\n\n## Output Formats\n\n### Default List View\n```\nFound 3 tasks matching \"authentication\":\n\nTASK-001: Implement JWT authentication\n  Status: in_progress | Agent: dev-frontend | Created: 2024-03-15\n  Location: /task-orchestration/03_15_2024/auth_system/tasks/in_progress/\n\nTASK-004: Add OAuth2 authentication  \n  Status: todos | Priority: high | Blocked by: TASK-001\n  Location: /task-orchestration/03_15_2024/auth_system/tasks/todos/\n\nTASK-007: Authentication middleware tests\n  Status: todos | Type: test | Depends on: TASK-001\n  Location: /task-orchestration/03_15_2024/auth_system/tasks/todos/\n```\n\n### Detailed View\n```\n/task-find TASK-001 --detailed\n```\nShows full task content including description, implementation notes, and history.\n\n### Tree View\n```\n/task-find --tree --root TASK-001\n```\nShows task and all its dependencies in tree format.\n\n## Filtering Options\n\n### By Orchestration\n```\n/task-find --orchestration \"03_15_2024/payment_system\"\n/task-find --orchestration \"*/auth_*\"\n```\n\n### By Properties\n```\n/task-find --has-dependencies\n/task-find --no-dependencies\n/task-find --blocking-others\n/task-find --effort \">4h\"\n```\n\n### By Relationships\n```\n/task-find --depends-on TASK-001\n/task-find --blocks TASK-005\n/task-find --related-to TASK-003\n```\n\n## Special Searches\n\n### Find Circular Dependencies\n```\n/task-find --circular-deps\n```\n\n### Find Orphaned Tasks\n```\n/task-find --orphaned\n```\n\n### Find Duplicate Tasks\n```\n/task-find --duplicates\n```\n\n### Find Stale Tasks\n```\n/task-find --stale --days 7\n```\n\n## Quick Filters\n\n### Ready to Start\n```\n/task-find --ready\n```\nShows todos with no blocking dependencies.\n\n### Critical Path\n```\n/task-find --critical-path\n```\nShows tasks on the critical path.\n\n### High Impact\n```\n/task-find --high-impact\n```\nShows tasks blocking multiple others.\n\n## Export Options\n\n### Copy Results\n```\n/task-find \"auth\" --copy\n```\nCopies results to clipboard.\n\n### Export Paths\n```\n/task-find --status todos --export paths\n```\nExports file paths for batch operations.\n\n### Generate Report\n```\n/task-find --report\n```\nCreates detailed search report.\n\n## Examples\n\n### Example 1: Find Work for Agent\n```\n/task-find --status todos --suitable-for dev-frontend --ready\n```\n\n### Example 2: Find Blocking Issues\n```\n/task-find --status on_hold --show-blockers\n```\n\n### Example 3: Security Audit\n```\n/task-find \"security OR auth OR permission\" --type \"feature,bugfix\"\n```\n\n### Example 4: Sprint Planning\n```\n/task-find --status todos --effort \"<4h\" --no-dependencies\n```\n\n## Search Shortcuts\n\n### Recent Tasks\n```\n/task-find --recent 10\n```\n\n### My Tasks\n```\n/task-find --mine  # Uses current agent context\n```\n\n### Modified Today\n```\n/task-find --modified today\n```\n\n## Complex Queries\n\n### Compound Search\n```\n/task-find '(title:\"auth\" OR description:\"security\") AND status:todos AND -blocks:*'\n```\n\n### Saved Searches\n```\n/task-find --save \"security-todos\"\n/task-find --load \"security-todos\"\n```\n\n## Performance Tips\n\n1. **Use Indexes**: Status and ID searches are fastest\n2. **Narrow Scope**: Specify orchestration when possible\n3. **Cache Results**: Use `--cache` for repeated searches\n4. **Limit Results**: Use `--limit 20` for large result sets\n\n## Integration\n\n### With Other Commands\n```\n/task-find \"payment\" --status todos | /task-move in_progress\n```\n\n### Batch Operations\n```\n/task-find --filter \"priority:low\" | /task-update priority:medium\n```\n\n## Notes\n\n- Searches across all task files in task-orchestration/\n- Case-insensitive by default (use --case for case-sensitive)\n- Results sorted by relevance unless specified otherwise\n- Supports command chaining with pipe operator\n- Search index updated automatically on file changes"
              },
              {
                "name": "/five",
                "description": "Apply the Five Whys root cause analysis technique to systematically investigate issues",
                "path": "plugins/all-commands/commands/five.md",
                "frontmatter": {
                  "description": "Apply the Five Whys root cause analysis technique to systematically investigate issues",
                  "category": "miscellaneous",
                  "argument-hint": "<issue_description>"
                },
                "content": "# Five Whys Analysis\n\nApply the Five Whys root cause analysis technique to investigate: $ARGUMENTS\n\n## Description\nThis command implements the Five Whys problem-solving methodology, iteratively asking \"why\" to drill down from symptoms to root causes. It helps identify the fundamental reason behind a problem rather than just addressing surface-level symptoms.\n\n## Usage\n`five [issue_description]`\n\n## Variables\n- ISSUE: The problem or symptom to analyze (default: prompt for input)\n- DEPTH: Number of \"why\" iterations (default: 5, can be adjusted)\n\n## Steps\n1. Start with the problem statement\n2. Ask \"Why did this happen?\" and document the answer\n3. For each answer, ask \"Why?\" again\n4. Continue for at least 5 iterations or until root cause is found\n5. Validate the root cause by working backwards\n6. Propose solutions that address the root cause\n\n## Examples\n### Example 1: Application crash analysis\n```\nProblem: Application crashes on startup\nWhy 1: Database connection fails\nWhy 2: Connection string is invalid\nWhy 3: Environment variable not set\nWhy 4: Deployment script missing env setup\nWhy 5: Documentation didn't specify env requirements\nRoot Cause: Missing deployment documentation\n```\n\n### Example 2: Performance issue investigation\nSystematically trace why a feature is running slowly by examining each contributing factor.\n\n## Notes\n- Don't stop at symptoms; keep digging for systemic issues\n- Multiple root causes may exist - explore different branches\n- Document each \"why\" for future reference\n- Consider both technical and process-related causes\n- The magic isn't in exactly 5 whys - stop when you reach the true root cause"
              },
              {
                "name": "/fix-github-issue",
                "description": "Analyze and fix a GitHub issue with comprehensive testing and verification",
                "path": "plugins/all-commands/commands/fix-github-issue.md",
                "frontmatter": {
                  "description": "Analyze and fix a GitHub issue with comprehensive testing and verification",
                  "category": "version-control-git",
                  "argument-hint": "<issue_number>",
                  "allowed-tools": "Bash(gh *), Read, Edit, Write, Bash(git *)"
                },
                "content": "Please analyze and fix the GitHub issue: $ARGUMENTS.\n\nFollow these steps:\n\n1. Use `gh issue view` to get the issue details\n2. Understand the problem described in the issue\n3. Search the codebase for relevant files\n4. Implement the necessary changes to fix the issue\n5. Write and run tests to verify the fix\n6. Ensure code passes linting and type checking\n7. Create a descriptive commit message\n\nRemember to use the GitHub CLI (`gh`) for all GitHub-related tasks.\n\n---"
              },
              {
                "name": "/fix-issue",
                "description": "Fix a specific issue or problem with the given identifier or description",
                "path": "plugins/all-commands/commands/fix-issue.md",
                "frontmatter": {
                  "description": "Fix a specific issue or problem with the given identifier or description",
                  "category": "version-control-git",
                  "argument-hint": "<issue_identifier>"
                },
                "content": "Fix issue $ARGUMENTS"
              },
              {
                "name": "/fix-pr",
                "description": "Fetch unresolved comments for current branch's PR and fix them",
                "path": "plugins/all-commands/commands/fix-pr.md",
                "frontmatter": {
                  "description": "Fetch unresolved comments for current branch's PR and fix them",
                  "category": "version-control-git",
                  "allowed-tools": "Bash(gh *), Read, Edit"
                },
                "content": "Fetch unresolved comments for this branch's PR, then fix them"
              },
              {
                "name": "/future-scenario-generator",
                "description": "Generate and analyze future scenarios with plausibility scoring, trend integration, and uncertainty quantification.",
                "path": "plugins/all-commands/commands/future-scenario-generator.md",
                "frontmatter": {
                  "description": "Generate and analyze future scenarios with plausibility scoring, trend integration, and uncertainty quantification.",
                  "category": "simulation-modeling",
                  "argument-hint": "Specify scenario parameters",
                  "allowed-tools": "Glob"
                },
                "content": "# Future Scenario Generator\n\nGenerate and analyze future scenarios with plausibility scoring, trend integration, and uncertainty quantification.\n\n## Instructions\n\nYou are tasked with systematically generating comprehensive future scenarios to explore potential developments and prepare for multiple possible futures. Follow this approach: **$ARGUMENTS**\n\n### 1. Prerequisites Assessment\n\n**Critical Scenario Context Validation:**\n\n- **Time Horizon**: What future timeframe are you exploring (1-3-5-10+ years)?\n- **Domain Focus**: What specific area/industry/system are you analyzing?\n- **Key Variables**: What factors could significantly shape the future?\n- **Decision Impact**: How will these scenarios inform specific decisions?\n- **Uncertainty Level**: What's the acceptable range of scenario uncertainty?\n\n**If context is unclear, guide systematically:**\n\n```\nMissing Time Horizon:\n\"What future timeframe should we explore?\n- Near-term (1-2 years): Market shifts, competitive moves, technology adoption\n- Medium-term (3-5 years): Industry transformation, regulatory changes, generational shifts  \n- Long-term (5-10+ years): Fundamental technology disruption, societal changes, paradigm shifts\n\nEach timeframe requires different scenario methodologies and uncertainty management.\"\n\nMissing Domain Focus:\n\"What specific domain or system should we model future scenarios for?\n- Business/Industry: Market evolution, competitive landscape, customer behavior\n- Technology: Platform shifts, capability development, adoption patterns\n- Society/Culture: Demographic changes, value shifts, behavior evolution\n- Economy/Policy: Regulatory changes, economic cycles, political developments\"\n```\n\n### 2. Trend Analysis Foundation\n\n**Systematically analyze current trends as scenario building blocks:**\n\n#### Trend Identification Framework\n```\nMulti-Dimensional Trend Analysis:\n\nTechnology Trends:\n- Emerging technologies and adoption curves\n- Infrastructure development and capability expansion\n- Platform shifts and ecosystem evolution\n- Innovation cycles and breakthrough potential\n\nSocial/Cultural Trends:\n- Demographic shifts and generational changes\n- Value system evolution and priority shifts\n- Behavior pattern changes and lifestyle adaptation\n- Communication and interaction pattern evolution\n\nEconomic Trends:\n- Market structure changes and industry evolution\n- Investment patterns and capital allocation shifts\n- Globalization and trade pattern modifications\n- Economic cycle positioning and policy directions\n\nRegulatory/Policy Trends:\n- Regulatory environment evolution and compliance requirements\n- Policy direction changes and government priorities\n- International relations and trade agreement impacts\n- Legal framework development and enforcement patterns\n```\n\n#### Trend Trajectory Modeling\n- Linear progression scenarios (current trends continue)\n- Acceleration scenarios (trends speed up dramatically)\n- Deceleration scenarios (trends slow down or plateau)\n- Reversal scenarios (trends change direction)\n- Disruption scenarios (trends are fundamentally altered)\n\n### 3. Scenario Architecture Design\n\n**Structure comprehensive scenario frameworks:**\n\n#### Scenario Generation Methodology\n```\nSystematic Scenario Construction:\n\nCross-Impact Analysis:\n- Identify key driving forces and variables\n- Analyze interaction effects between different trends\n- Map reinforcing and conflicting trend combinations\n- Model cascade effects and secondary impacts\n\nMorphological Analysis:\n- Define key dimensions of future variation\n- Identify possible states for each dimension\n- Generate scenario combinations systematically\n- Evaluate scenario consistency and plausibility\n\nNarrative Scenario Development:\n- Create compelling future stories and visions\n- Integrate quantitative trends with qualitative insights\n- Develop scenario logic and causal narratives\n- Ensure scenario diversity and comprehensive coverage\n```\n\n#### Scenario Categorization Framework\n```\nScenario Portfolio Structure:\n\nBaseline Scenarios (30-40% of portfolio):\n- Continuation of current trends with normal variation\n- Evolutionary change within existing paradigms\n- Moderate uncertainty and predictable development patterns\n\nOptimistic Scenarios (20-25% of portfolio):\n- Favorable trend convergence and positive developments\n- Breakthrough innovations and acceleration opportunities\n- Best-case outcome realization and synergy effects\n\nPessimistic Scenarios (20-25% of portfolio):\n- Adverse trend combinations and negative developments\n- Crisis scenarios and system stress conditions\n- Worst-case outcome realization and cascade failures\n\nTransformation Scenarios (15-20% of portfolio):\n- Paradigm shifts and fundamental system changes\n- Disruptive innovation and market restructuring\n- Wild card events and black swan developments\n```\n\n### 4. Plausibility Assessment Framework\n\n**Systematically evaluate scenario credibility:**\n\n#### Plausibility Scoring Methodology\n```\nMulti-Criteria Plausibility Assessment:\n\nHistorical Precedent (25% weight):\n- Similar patterns and developments in historical context\n- Analogous situations and outcome patterns\n- Learning from past trend evolution and scenario realization\n\nLogical Consistency (25% weight):\n- Internal scenario logic and causal relationships\n- Consistency between different scenario elements\n- Absence of logical contradictions and impossible combinations\n\nExpert Validation (25% weight):\n- Domain expert assessment and credibility evaluation\n- Stakeholder input and perspective integration\n- Professional judgment and experience-based validation\n\nEmpirical Support (25% weight):\n- Current data and trend evidence supporting scenario elements\n- Quantitative model outputs and statistical projections\n- Research findings and academic literature support\n\nPlausibility Score = (Historical  0.25) + (Logical  0.25) + (Expert  0.25) + (Empirical  0.25)\n```\n\n#### Uncertainty Quantification\n- Confidence intervals for key scenario parameters\n- Sensitivity analysis for critical assumptions\n- Monte Carlo simulation for probability distributions\n- Expert elicitation for subjective probability assessment\n\n### 5. Wild Card and Disruption Modeling\n\n**Incorporate low-probability, high-impact events:**\n\n#### Wild Card Event Framework\n```\nSystematic Disruption Analysis:\n\nTechnology Wild Cards:\n- Breakthrough innovations and paradigm shifts\n- Technology convergence and unexpected capabilities\n- Platform disruptions and ecosystem transformations\n- Artificial intelligence and automation breakthroughs\n\nSocial Wild Cards:\n- Generational value shifts and behavior changes\n- Social movement emergence and cultural transformations\n- Demographic surprises and migration patterns\n- Communication and social interaction disruptions\n\nEconomic Wild Cards:\n- Financial system disruptions and market structure changes\n- Resource scarcity or abundance surprises\n- Currency and monetary system transformations\n- Trade pattern disruptions and economic bloc changes\n\nEnvironmental/Political Wild Cards:\n- Climate change acceleration or mitigation breakthroughs\n- Geopolitical shifts and international relation changes\n- Natural disasters and pandemic impacts\n- Regulatory surprises and policy paradigm shifts\n```\n\n#### Disruption Impact Modeling\n- Direct impact assessment on key scenario variables\n- Cascade effect analysis through system dependencies\n- Adaptation and recovery scenario development\n- Resilience and vulnerability analysis\n\n### 6. Scenario Integration and Synthesis\n\n**Combine scenarios into comprehensive future landscape:**\n\n#### Cross-Scenario Analysis\n```\nScenario Portfolio Analysis:\n\nScenario Clustering:\n- Group similar scenarios and identify common patterns\n- Analyze scenario divergence points and branching factors\n- Map scenario transition probabilities and pathways\n- Identify robust strategies across multiple scenarios\n\nScenario Interaction Effects:\n- How scenarios might combine or influence each other\n- Sequential scenario development and evolution patterns\n- Scenario switching triggers and transition indicators\n- Portfolio effects of scenario diversification\n\nKey Insight Synthesis:\n- Common themes and patterns across scenarios\n- Critical uncertainties and decision-relevant factors\n- Robust trends that appear in most scenarios\n- Strategic implications and opportunity identification\n```\n\n#### Scenario Narrative Development\n- Compelling future stories that integrate multiple trends\n- Character and stakeholder perspective integration\n- Timeline development and milestone identification\n- Vivid details that make scenarios memorable and actionable\n\n### 7. Decision Integration Framework\n\n**Connect scenarios to actionable strategic insights:**\n\n#### Strategy Testing Against Scenarios\n```\nScenario-Based Strategy Evaluation:\n\nStrategy Robustness Analysis:\n- How well do current strategies perform across scenarios?\n- Which scenarios pose the greatest strategic challenges?\n- What strategy modifications improve cross-scenario performance?\n- Where are the greatest strategy vulnerabilities and dependencies?\n\nOption Value Analysis:\n- What strategic options provide value across multiple scenarios?\n- Which investments maintain flexibility for different futures?\n- How can strategies be designed for adaptive capability?\n- What early warning systems enable strategy adjustment?\n\nContingency Planning:\n- Specific response strategies for different scenario realizations\n- Resource allocation across scenarios and strategy options\n- Decision trigger identification and monitoring systems\n- Implementation readiness for scenario-specific strategies\n```\n\n#### Strategic Recommendation Generation\n```\nScenario-Informed Strategy Framework:\n\n## Future Scenario Analysis: [Domain/Project Name]\n\n### Scenario Portfolio Summary\n- Time Horizon: [analysis period]\n- Key Driving Forces: [primary variables analyzed]\n- Scenarios Generated: [number and types]\n- Plausibility Range: [confidence levels]\n\n### High-Impact Scenarios\n\n#### Scenario 1: [Name - Plausibility Score]\n- Timeline: [key development milestones]\n- Driving Forces: [primary trends and factors]\n- Key Characteristics: [distinctive features]\n- Strategic Implications: [decision impacts]\n\n[Repeat for top 4-6 scenarios]\n\n### Cross-Scenario Insights\n- Robust Trends: [patterns appearing in most scenarios]\n- Critical Uncertainties: [factors determining scenario outcomes]\n- Strategic Vulnerabilities: [areas of risk across scenarios]\n- Opportunity Convergence: [areas of opportunity across scenarios]\n\n### Strategic Recommendations\n- Core Strategy: [approach that works across multiple scenarios]\n- Scenario-Specific Tactics: [adaptations for different scenarios]\n- Early Warning Indicators: [signals for scenario realization]\n- Strategic Options: [investments that maintain flexibility]\n\n### Monitoring and Adaptation Framework\n- Key Indicators: [metrics to track scenario development]\n- Decision Triggers: [when to adjust strategy based on signals]\n- Contingency Plans: [specific responses for different scenarios]\n- Review Schedule: [when to update scenario analysis]\n```\n\n### 8. Continuous Scenario Evolution\n\n**Establish ongoing scenario refinement and updating:**\n\n#### Real-World Validation\n- Track actual developments against scenario predictions\n- Update scenario probabilities based on emerging evidence\n- Refine scenario assumptions based on real-world feedback\n- Learn from scenario accuracy and prediction quality\n\n#### Adaptive Scenario Management\n- Regular scenario refresh and update cycles\n- New information integration and scenario modification\n- Stakeholder feedback incorporation and perspective updates\n- Methodology improvement based on scenario performance\n\n## Usage Examples\n\n```bash\n# Industry transformation scenarios\n/simulation:future-scenario-generator Generate scenarios for AI's impact on healthcare industry over next 10 years\n\n# Technology adoption scenarios\n/simulation:future-scenario-generator Model future scenarios for remote work technology adoption and workplace evolution\n\n# Market evolution scenarios  \n/simulation:future-scenario-generator Explore scenarios for sustainable energy market development and regulatory changes\n\n# Competitive landscape scenarios\n/simulation:future-scenario-generator Generate scenarios for fintech industry evolution and traditional banking disruption\n```\n\n## Quality Indicators\n\n- **Green**: Diverse scenario portfolio, validated plausibility scores, integrated wild cards\n- **Yellow**: Good scenario variety, reasonable plausibility assessment, some disruption modeling\n- **Red**: Limited scenario diversity, unvalidated assumptions, missing disruption analysis\n\n## Common Pitfalls to Avoid\n\n- Present bias: Projecting current conditions too strongly into the future\n- Linear thinking: Assuming trends continue unchanged without acceleration or disruption\n- Probability illusion: Being overconfident in specific scenario likelihoods\n- Complexity underestimation: Not modeling interaction effects between trends\n- Wild card blindness: Ignoring low-probability, high-impact events\n- Action paralysis: Generating scenarios without connecting to decisions\n\nTransform uncertainty into strategic advantage through systematic future scenario exploration and preparation."
              },
              {
                "name": "/generate-api-documentation",
                "description": "Auto-generate API reference documentation",
                "path": "plugins/all-commands/commands/generate-api-documentation.md",
                "frontmatter": {
                  "description": "Auto-generate API reference documentation",
                  "category": "api-development"
                },
                "content": "# Generate API Documentation\n\nAuto-generate API reference documentation\n\n## Instructions\n\n1. **API Documentation Strategy Analysis**\n   - Analyze current API structure and endpoints\n   - Identify documentation requirements (REST, GraphQL, gRPC, etc.)\n   - Assess existing code annotations and documentation\n   - Determine documentation output formats and hosting requirements\n   - Plan documentation automation and maintenance strategy\n\n2. **Documentation Tool Selection**\n   - Choose appropriate API documentation tools:\n     - **OpenAPI/Swagger**: REST API documentation with Swagger UI\n     - **Redoc**: Modern OpenAPI documentation renderer\n     - **GraphQL**: GraphiQL, Apollo Studio, GraphQL Playground\n     - **Postman**: API documentation with collections\n     - **Insomnia**: API documentation and testing\n     - **API Blueprint**: Markdown-based API documentation\n     - **JSDoc/TSDoc**: Code-first documentation generation\n   - Consider factors: API type, team workflow, hosting, interactivity\n\n3. **Code Annotation and Schema Definition**\n   - Add comprehensive code annotations for API endpoints\n   - Define request/response schemas and data models\n   - Add parameter descriptions and validation rules\n   - Document authentication and authorization requirements\n   - Add example requests and responses\n\n4. **API Specification Generation**\n   - Set up automated API specification generation from code\n   - Configure OpenAPI/Swagger specification generation\n   - Set up schema validation and consistency checking\n   - Configure API versioning and changelog generation\n   - Set up specification file management and version control\n\n5. **Interactive Documentation Setup**\n   - Configure interactive API documentation with try-it-out functionality\n   - Set up API testing and example execution\n   - Configure authentication handling in documentation\n   - Set up request/response validation and examples\n   - Configure API endpoint categorization and organization\n\n6. **Documentation Content Enhancement**\n   - Add comprehensive API guides and tutorials\n   - Create authentication and authorization documentation\n   - Add error handling and status code documentation\n   - Create SDK and client library documentation\n   - Add rate limiting and usage guidelines\n\n7. **Documentation Hosting and Deployment**\n   - Set up documentation hosting and deployment\n   - Configure documentation website generation and styling\n   - Set up custom domain and SSL configuration\n   - Configure documentation search and navigation\n   - Set up documentation analytics and usage tracking\n\n8. **Automation and CI/CD Integration**\n   - Configure automated documentation generation in CI/CD pipeline\n   - Set up documentation deployment automation\n   - Configure documentation validation and quality checks\n   - Set up documentation change detection and notifications\n   - Configure documentation testing and link validation\n\n9. **Multi-format Documentation Generation**\n   - Generate documentation in multiple formats (HTML, PDF, Markdown)\n   - Set up downloadable documentation packages\n   - Configure offline documentation access\n   - Set up documentation API for programmatic access\n   - Configure documentation syndication and distribution\n\n10. **Maintenance and Quality Assurance**\n    - Set up documentation quality monitoring and validation\n    - Configure documentation feedback and improvement workflows\n    - Set up documentation analytics and usage metrics\n    - Create documentation maintenance procedures and guidelines\n    - Train team on documentation best practices and tools\n    - Set up documentation review and approval processes"
              },
              {
                "name": "/generate-linear-worklog",
                "description": "You are tasked with generating a technical work log comment for a Linear issue based on recent git commits.",
                "path": "plugins/all-commands/commands/generate-linear-worklog.md",
                "frontmatter": {
                  "description": "You are tasked with generating a technical work log comment for a Linear issue based on recent git commits.",
                  "category": "utilities-debugging",
                  "allowed-tools": "Bash(git *), Bash(npm *)"
                },
                "content": "# Generate Linear Work Log\n\nYou are tasked with generating a technical work log comment for a Linear issue based on recent git commits.\n\n## Instructions\n\n1. **Check Linear MCP Availability**\n   - Verify that Linear MCP tools are available (mcp__linear__* functions)\n   - If Linear MCP is not installed, inform the user to install it and provide installation instructions\n   - Do not proceed with work log generation if Linear MCP is unavailable\n\n2. **Check for Existing Work Log**\n   - Use Linear MCP to get existing comments on the issue\n   - Look for comments with today's date in the format \"## Work Completed [TODAY'S DATE]\"\n   - If found, note the existing content to append/update rather than duplicate\n\n2. **Extract Git Information**\n   - Get the current branch name\n   - Get recent commits on the current branch (last 10 commits)\n   - Get commits that are on the current branch but not on main branch\n   - For each relevant commit, get detailed information including file changes and line counts\n   - Focus on commits since the last work log update (if any exists)\n\n3. **Generate Work Log Content**\n   - Use dry, technical language without adjectives or emojis\n   - Focus on factual implementation details\n   - Structure the log with date, branch, and commit information\n   - Include quantitative metrics (file counts, line counts) where relevant\n   - Avoid subjective commentary or promotional language\n\n4. **Handle Existing Work Log**\n   - If no work log exists for today: Create new comment\n   - If work log exists for today: Replace the existing comment with updated content including all today's work\n   - Ensure chronological order of commits\n   - Include both previous and new work completed today\n\n5. **Format Structure**\n   ```\n   ## Work Completed [TODAY'S DATE]\n\n   ### Branch: [current-branch-name]\n\n   **Commit [short-hash]: [Commit Title]**\n   - [Technical detail 1]\n   - [Technical detail 2]\n   - [Line count] lines of code across [file count] files\n\n   [Additional commits in chronological order]\n\n   ### [Status Section]\n   - [Current infrastructure/testing status]\n   - [What is now available/ready]\n   ```\n\n6. **Post to Linear**\n   - Use the Linear MCP integration to create or update the comment\n   - Post the formatted work log to the specified Linear issue\n   - If updating, replace the entire existing work log comment\n   - Confirm successful posting\n\n## Git Commands to Use\n- `git branch --show-current` - Get current branch\n- `git log --oneline -10` - Get recent commits\n- `git log main..HEAD --oneline` - Get branch-specific commits\n- `git show --stat [commit-hash]` - Get detailed commit info\n- `git log --since=\"[today's date]\" --pretty=format:\"%h %ad %s\" --date=short` - Get today's commits\n\n## Content Guidelines\n- Include commit hashes and descriptive titles\n- Provide specific technical implementations\n- Include file counts and line counts for significant changes\n- Maintain consistent formatting\n- Focus on technical accomplishments\n- Include current status summary\n- No emojis or special characters\n\n## Error Handling\n- Check if Linear MCP client is available before proceeding\n- If Linear MCP is not available, display installation instructions:\n  ```\n  Linear MCP client is not installed. To install it:\n  \n  1. Install the Linear MCP server:\n     npm install -g @modelcontextprotocol/server-linear\n  \n  2. Add Linear MCP to your Claude configuration:\n     Add the following to your Claude MCP settings:\n     {\n       \"mcpServers\": {\n         \"linear\": {\n           \"command\": \"npx\",\n           \"args\": [\"@modelcontextprotocol/server-linear\"],\n           \"env\": {\n             \"LINEAR_API_KEY\": \"your_linear_api_key_here\"\n           }\n         }\n       }\n     }\n  \n  3. Restart Claude Code\n  4. Get your Linear API key from: https://linear.app/settings/api\n  ```\n- Validate that the Linear ticket ID exists\n- Handle cases where no recent commits are found\n- Provide clear error messages for git operation failures\n- Confirm successful comment posting\n\n## Example Usage\nWhen invoked with `/generate-linear-worklog BLA2-2`, the command should:\n1. Analyze git commits on the current branch\n2. Generate a structured work log\n3. Post the comment to Linear issue BLA2-2\n4. Confirm successful posting"
              },
              {
                "name": "/generate-test-cases",
                "description": "Generate comprehensive test cases automatically",
                "path": "plugins/all-commands/commands/generate-test-cases.md",
                "frontmatter": {
                  "description": "Generate comprehensive test cases automatically",
                  "category": "code-analysis-testing",
                  "argument-hint": "Specify test case requirements"
                },
                "content": "# Generate Test Cases\n\nGenerate comprehensive test cases automatically\n\n## Instructions\n\n1. **Target Analysis and Scope Definition**\n   - Parse target file or function from arguments: `$ARGUMENTS`\n   - If no target specified, analyze current directory and prompt for specific target\n   - Examine the target code structure, dependencies, and complexity\n   - Identify function signatures, parameters, return types, and side effects\n   - Determine testing scope (unit, integration, or both)\n\n2. **Code Structure Analysis**\n   - Analyze function logic, branching, and control flow\n   - Identify input validation, error handling, and edge cases\n   - Examine external dependencies, API calls, and database interactions\n   - Review data transformations and business logic\n   - Identify async operations and error scenarios\n\n3. **Test Case Generation Strategy**\n   - Generate positive test cases for normal operation flows\n   - Create negative test cases for error conditions and invalid inputs\n   - Generate edge cases for boundary conditions and limits\n   - Create integration test cases for external dependencies\n   - Generate performance test cases for complex operations\n\n4. **Unit Test Implementation**\n   - Create test file following project naming conventions\n   - Set up test framework imports and configuration\n   - Generate test suites organized by functionality\n   - Create comprehensive test cases with descriptive names\n   - Implement proper setup and teardown for each test\n\n5. **Mock and Stub Generation**\n   - Identify external dependencies requiring mocking\n   - Generate mock implementations for APIs and services\n   - Create stub data for database and file system operations\n   - Set up spy functions for monitoring function calls\n   - Configure mock return values and error scenarios\n\n6. **Data-Driven Test Generation**\n   - Create test data sets for various input scenarios\n   - Generate parameterized tests for multiple input combinations\n   - Create fixtures for complex data structures\n   - Set up test data factories for consistent data generation\n   - Generate property-based test cases for comprehensive coverage\n\n7. **Integration Test Scenarios**\n   - Generate tests for component interactions\n   - Create end-to-end workflow test cases\n   - Generate API integration test scenarios\n   - Create database integration tests with real data\n   - Generate cross-module integration test cases\n\n8. **Error Handling and Exception Testing**\n   - Generate tests for all error conditions and exceptions\n   - Create tests for timeout and network failure scenarios\n   - Generate tests for invalid input validation\n   - Create tests for resource exhaustion and limits\n   - Generate tests for concurrent access and race conditions\n\n9. **Test Quality and Coverage**\n   - Ensure comprehensive code coverage for target functions\n   - Generate tests for all code branches and paths\n   - Create tests for both success and failure scenarios\n   - Validate test assertions are meaningful and specific\n   - Ensure tests are isolated and independent\n\n10. **Test Documentation and Maintenance**\n    - Generate clear test descriptions and documentation\n    - Create comments explaining complex test scenarios\n    - Document test data requirements and setup procedures\n    - Generate test maintenance guidelines and best practices\n    - Create test execution and debugging instructions\n    - Validate generated tests execute successfully and provide meaningful feedback"
              },
              {
                "name": "/generate-tests",
                "description": "Generate comprehensive test suite for $ARGUMENTS following project testing conventions and best practices.",
                "path": "plugins/all-commands/commands/generate-tests.md",
                "frontmatter": {
                  "description": "Generate comprehensive test suite for $ARGUMENTS following project testing conventions and best practices.",
                  "category": "code-analysis-testing",
                  "argument-hint": "Specify test generation options"
                },
                "content": "# Test Generator\n\nGenerate comprehensive test suite for $ARGUMENTS following project testing conventions and best practices.\n\n## Task\n\nI'll analyze the target code and create complete test coverage including:\n\n1. Unit tests for individual functions and methods\n2. Integration tests for component interactions  \n3. Edge case and error handling tests\n4. Mock implementations for external dependencies\n5. Test utilities and helpers as needed\n6. Performance and snapshot tests where appropriate\n\n## Process\n\nI'll follow these steps:\n\n1. Analyze the target file/component structure\n2. Identify all testable functions, methods, and behaviors\n3. Examine existing test patterns in the project\n4. Create test files following project naming conventions\n5. Implement comprehensive test cases with proper setup/teardown\n6. Add necessary mocks and test utilities\n7. Verify test coverage and add missing test cases\n\n## Test Types\n\n### Unit Tests\n- Individual function testing with various inputs\n- Component rendering and prop handling\n- State management and lifecycle methods\n- Utility function edge cases and error conditions\n\n### Integration Tests\n- Component interaction testing\n- API integration with mocked responses\n- Service layer integration\n- End-to-end user workflows\n\n### Framework-Specific Tests\n- **React**: Component testing with React Testing Library\n- **Vue**: Component testing with Vue Test Utils\n- **Angular**: Component and service testing with TestBed\n- **Node.js**: API endpoint and middleware testing\n\n## Testing Best Practices\n\n### Test Structure\n- Use descriptive test names that explain the behavior\n- Follow AAA pattern (Arrange, Act, Assert)\n- Group related tests with describe blocks\n- Use proper setup and teardown for test isolation\n\n### Mock Strategy\n- Mock external dependencies and API calls\n- Use factories for test data generation\n- Implement proper cleanup for async operations\n- Mock timers and dates for deterministic tests\n\n### Coverage Goals\n- Aim for 80%+ code coverage\n- Focus on critical business logic paths\n- Test both happy path and error scenarios\n- Include boundary value testing\n\nI'll adapt to your project's testing framework (Jest, Vitest, Cypress, etc.) and follow established patterns."
              },
              {
                "name": "/git-status",
                "description": "Show detailed git repository status",
                "path": "plugins/all-commands/commands/git-status.md",
                "frontmatter": {
                  "description": "Show detailed git repository status",
                  "category": "utilities-debugging",
                  "argument-hint": "Optional: specify path or options",
                  "allowed-tools": "Bash(git *), Read"
                },
                "content": "# Git Status Command\n\nShow detailed git repository status\n\n*Command originally created by IndyDevDan (YouTube: https://www.youtube.com/@indydevdan) / DislerH (GitHub: https://github.com/disler)*\n\n## Instructions\n\nAnalyze the current state of the git repository by performing the following steps:\n\n1. **Run Git Status Commands**\n   - Execute `git status` to see current working tree state\n   - Run `git diff HEAD origin/main` to check differences with remote\n   - Execute `git branch --show-current` to display current branch\n   - Check for uncommitted changes and untracked files\n\n2. **Analyze Repository State**\n   - Identify staged vs unstaged changes\n   - List any untracked files\n   - Check if branch is ahead/behind remote\n   - Review any merge conflicts if present\n\n3. **Read Key Files**\n   - Review README.md for project context\n   - Check for any recent changes in important files\n   - Understand project structure if needed\n\n4. **Provide Summary**\n   - Current branch and its relationship to main/master\n   - Number of commits ahead/behind\n   - List of modified files with change types\n   - Any action items (commits needed, pulls required, etc.)\n\nThis command helps developers quickly understand:\n- What changes are pending\n- The repository's sync status\n- Whether any actions are needed before continuing work\n\nArguments: $ARGUMENTS"
              },
              {
                "name": "/hotfix-deploy",
                "description": "Deploy critical hotfixes quickly",
                "path": "plugins/all-commands/commands/hotfix-deploy.md",
                "frontmatter": {
                  "description": "Deploy critical hotfixes quickly",
                  "category": "ci-deployment",
                  "argument-hint": "1. **Emergency Assessment and Triage**",
                  "allowed-tools": "Bash(git *), Bash(npm *)"
                },
                "content": "# Hotfix Deploy Command\n\nDeploy critical hotfixes quickly\n\n## Instructions\n\nFollow this emergency hotfix deployment process: **$ARGUMENTS**\n\n1. **Emergency Assessment and Triage**\n   - Assess the severity and impact of the issue\n   - Determine if a hotfix is necessary or if it can wait\n   - Identify affected systems and user impact\n   - Estimate time sensitivity and business impact\n   - Document the incident and decision rationale\n\n2. **Incident Response Setup**\n   - Create incident tracking in your incident management system\n   - Set up war room or communication channel\n   - Notify stakeholders and on-call team members\n   - Establish clear communication protocols\n   - Document initial incident details and timeline\n\n3. **Branch and Environment Setup**\n   ```bash\n   # Create hotfix branch from production tag\n   git fetch --tags\n   git checkout tags/v1.2.3  # Latest production version\n   git checkout -b hotfix/critical-auth-fix\n   \n   # Alternative: Branch from main if using trunk-based development\n   git checkout main\n   git pull origin main\n   git checkout -b hotfix/critical-auth-fix\n   ```\n\n4. **Rapid Development Process**\n   - Keep changes minimal and focused on the critical issue only\n   - Avoid refactoring, optimization, or unrelated improvements\n   - Use well-tested patterns and established approaches\n   - Add minimal logging for troubleshooting purposes\n   - Follow existing code conventions and patterns\n\n5. **Accelerated Testing**\n   ```bash\n   # Run focused tests related to the fix\n   npm test -- --testPathPattern=auth\n   npm run test:security\n   \n   # Manual testing checklist\n   # [ ] Core functionality works correctly\n   # [ ] Hotfix resolves the critical issue\n   # [ ] No new issues introduced\n   # [ ] Critical user flows remain functional\n   ```\n\n6. **Fast-Track Code Review**\n   - Get expedited review from senior team member\n   - Focus review on security and correctness\n   - Use pair programming if available and time permits\n   - Document review decisions and rationale quickly\n   - Ensure proper approval process even under time pressure\n\n7. **Version and Tagging**\n   ```bash\n   # Update version for hotfix\n   # 1.2.3 -> 1.2.4 (patch version)\n   # or 1.2.3 -> 1.2.3-hotfix.1 (hotfix identifier)\n   \n   # Commit with detailed message\n   git add .\n   git commit -m \"hotfix: fix critical authentication vulnerability\n   \n   - Fix password validation logic\n   - Resolve security issue allowing bypass\n   - Minimal change to reduce deployment risk\n   \n   Fixes: #1234\"\n   \n   # Tag the hotfix version\n   git tag -a v1.2.4 -m \"Hotfix v1.2.4: Critical auth security fix\"\n   git push origin hotfix/critical-auth-fix\n   git push origin v1.2.4\n   ```\n\n8. **Staging Deployment and Validation**\n   ```bash\n   # Deploy to staging environment for final validation\n   ./deploy-staging.sh v1.2.4\n   \n   # Critical path testing\n   curl -X POST staging.example.com/api/auth/login \\\n        -H \"Content-Type: application/json\" \\\n        -d '{\"email\":\"test@example.com\",\"password\":\"testpass\"}'\n   \n   # Run smoke tests\n   npm run test:smoke:staging\n   ```\n\n9. **Production Deployment Strategy**\n   \n   **Blue-Green Deployment:**\n   ```bash\n   # Deploy to blue environment\n   ./deploy-blue.sh v1.2.4\n   \n   # Validate blue environment health\n   ./health-check-blue.sh\n   \n   # Switch traffic to blue environment\n   ./switch-to-blue.sh\n   \n   # Monitor deployment metrics\n   ./monitor-deployment.sh\n   ```\n   \n   **Rolling Deployment:**\n   ```bash\n   # Deploy to subset of servers first\n   ./deploy-rolling.sh v1.2.4 --batch-size 1\n   \n   # Monitor each batch deployment\n   ./monitor-batch.sh\n   \n   # Continue with next batch if healthy\n   ./deploy-next-batch.sh\n   ```\n\n10. **Pre-Deployment Checklist**\n    ```bash\n    # Verify all prerequisites are met\n    # [ ] Database backup completed successfully\n    # [ ] Rollback plan documented and ready\n    # [ ] Monitoring alerts configured and active\n    # [ ] Team members standing by for support\n    # [ ] Communication channels established\n    \n    # Execute production deployment\n    ./deploy-production.sh v1.2.4\n    \n    # Run immediate post-deployment validation\n    ./validate-hotfix.sh\n    ```\n\n11. **Real-Time Monitoring**\n    ```bash\n    # Monitor key application metrics\n    watch -n 10 'curl -s https://api.example.com/health | jq .'\n    \n    # Monitor error rates and logs\n    tail -f /var/log/app/error.log | grep -i \"auth\"\n    \n    # Track critical metrics:\n    # - Response times and latency\n    # - Error rates and exception counts\n    # - User authentication success rates\n    # - System resource usage (CPU, memory)\n    ```\n\n12. **Post-Deployment Validation**\n    ```bash\n    # Run comprehensive validation tests\n    ./test-critical-paths.sh\n    \n    # Test user authentication functionality\n    curl -X POST https://api.example.com/auth/login \\\n         -H \"Content-Type: application/json\" \\\n         -d '{\"email\":\"test@example.com\",\"password\":\"testpass\"}'\n    \n    # Validate security fix effectiveness\n    ./security-validation.sh\n    \n    # Check overall system performance\n    ./performance-check.sh\n    ```\n\n13. **Communication and Status Updates**\n    - Provide regular status updates to stakeholders\n    - Use consistent communication channels\n    - Document deployment progress and results\n    - Update incident tracking systems\n    - Notify relevant teams of deployment completion\n\n14. **Rollback Procedures**\n    ```bash\n    # Automated rollback script\n    #!/bin/bash\n    PREVIOUS_VERSION=\"v1.2.3\"\n    \n    if [ \"$1\" = \"rollback\" ]; then\n        echo \"Rolling back to $PREVIOUS_VERSION\"\n        ./deploy-production.sh $PREVIOUS_VERSION\n        ./validate-rollback.sh\n        echo \"Rollback completed successfully\"\n    fi\n    \n    # Manual rollback steps if automation fails:\n    # 1. Switch load balancer back to previous version\n    # 2. Validate previous version health and functionality\n    # 3. Monitor system stability after rollback\n    # 4. Communicate rollback status to team\n    ```\n\n15. **Post-Deployment Monitoring Period**\n    - Monitor system for 2-4 hours after deployment\n    - Watch error rates and performance metrics closely\n    - Check user feedback and support ticket volume\n    - Validate that the hotfix resolves the original issue\n    - Document any issues or unexpected behaviors\n\n16. **Documentation and Incident Reporting**\n    - Document the complete hotfix process and timeline\n    - Record lessons learned and process improvements\n    - Update incident management systems with resolution\n    - Create post-incident review materials\n    - Share knowledge with team for future reference\n\n17. **Merge Back to Main Branch**\n    ```bash\n    # After successful hotfix deployment and validation\n    git checkout main\n    git pull origin main\n    git merge hotfix/critical-auth-fix\n    git push origin main\n    \n    # Clean up hotfix branch\n    git branch -d hotfix/critical-auth-fix\n    git push origin --delete hotfix/critical-auth-fix\n    ```\n\n18. **Post-Incident Activities**\n    - Schedule and conduct post-incident review meeting\n    - Update runbooks and emergency procedures\n    - Identify and implement process improvements\n    - Update monitoring and alerting configurations\n    - Plan preventive measures to avoid similar issues\n\n**Hotfix Best Practices:**\n\n- **Keep It Simple:** Make minimal changes focused only on the critical issue\n- **Test Thoroughly:** Maintain testing standards even under time pressure\n- **Communicate Clearly:** Keep all stakeholders informed throughout the process\n- **Monitor Closely:** Watch the fix carefully in production environment\n- **Document Everything:** Record all decisions and actions for post-incident review\n- **Plan for Rollback:** Always have a tested way to revert changes quickly\n- **Learn and Improve:** Use each incident to strengthen processes and procedures\n\n**Emergency Escalation Guidelines:**\n\n```bash\n# Emergency contact information\nON_CALL_ENGINEER=\"+1-555-0123\"\nSENIOR_ENGINEER=\"+1-555-0124\"\nENGINEERING_MANAGER=\"+1-555-0125\"\nINCIDENT_COMMANDER=\"+1-555-0126\"\n\n# Escalation timeline thresholds:\n# 15 minutes: Escalate to senior engineer\n# 30 minutes: Escalate to engineering manager\n# 60 minutes: Escalate to incident commander\n```\n\n**Important Reminders:**\n\n- Hotfixes should only be used for genuine production emergencies\n- When in doubt about severity, follow the normal release process\n- Always prioritize system stability over speed of deployment\n- Maintain clear audit trails for all emergency changes\n- Regular drills help ensure team readiness for real emergencies"
              },
              {
                "name": "/husky",
                "description": "Verify repository is in working state by running CI checks and fixing issues",
                "path": "plugins/all-commands/commands/husky.md",
                "frontmatter": {
                  "description": "Verify repository is in working state by running CI checks and fixing issues",
                  "category": "version-control-git",
                  "allowed-tools": "Bash, Read, Edit"
                },
                "content": "## Summary\n\nVerify the repository is in a working state by running appropriate CI checks and fixing any issues found.\n\n## Process\n\n1. **Detect Package Manager**:\n   - Check for package manager files: package-lock.json (npm), pnpm-lock.yaml (pnpm), yarn.lock (yarn), bun.lockb (bun)\n   - Check for other build systems: Makefile, Cargo.toml, go.mod, requirements.txt, etc.\n\n2. **Update Dependencies**:\n   - npm: `npm install`\n   - pnpm: `pnpm install`\n   - yarn: `yarn install`\n   - bun: `bun install`\n   - Other: Run appropriate dependency installation\n\n3. **Run Linting**:\n   - Check package.json scripts for lint command\n   - Common patterns: `lint`, `eslint`, `check`, `format`\n   - Fix any linting issues found\n\n4. **Run Type Checking** (if applicable):\n   - TypeScript: `tsc` or check for `typecheck` script\n   - Other typed languages: run appropriate type checker\n\n5. **Run Build**:\n   - Check for build scripts in package.json or build configuration\n   - Common patterns: `build`, `compile`, `dist`\n   - Fix any build errors\n\n6. **Run Tests**:\n   - Check for test scripts: `test`, `test:unit`, `test:coverage`\n   - Source .env file if it exists before running tests\n   - Fix any failing tests\n\n7. **Additional Checks**:\n   - Check if package.json needs sorting (if sort-package-json is available)\n   - Run any other project-specific checks found in CI configuration\n\n8. **Stage Changes**:\n   - Review changes with `git status`\n   - Add fixed files with `git add`\n   - Exclude any git submodules or vendor directories\n\n## Important Notes:\n\n- Do NOT continue to the next step until the current command succeeds\n- Fix any issues found before proceeding\n- If a command doesn't exist, check for alternatives or skip if not applicable\n- Print a summary with checkmarks () for passed steps at the end\n\n## Protocol when something breaks\n\nTake the following steps if CI breaks\n\n### 1. Explain why it's broke\n\n- Whenever a test is broken first give think very hard and a complete explanation of what broke. Cite source code and logs that support your thesis.\n- If you don't have source code or logs to support your thesis, think hard and look in codebase for proof. \n- Add console logs if it will help you confirm your thesis or find out why it's broke\n- If you don"
              },
              {
                "name": "/implement-caching-strategy",
                "description": "Design and implement caching solutions",
                "path": "plugins/all-commands/commands/implement-caching-strategy.md",
                "frontmatter": {
                  "description": "Design and implement caching solutions",
                  "category": "performance-optimization"
                },
                "content": "# Implement Caching Strategy\n\nDesign and implement caching solutions\n\n## Instructions\n\n1. **Caching Strategy Analysis**\n   - Analyze application architecture and identify caching opportunities\n   - Assess current performance bottlenecks and data access patterns\n   - Define caching requirements (TTL, invalidation, consistency)\n   - Plan multi-layer caching architecture (browser, CDN, application, database)\n   - Evaluate caching technologies and storage solutions\n\n2. **Browser and Client-Side Caching**\n   - Configure HTTP caching headers and cache policies:\n\n   **HTTP Cache Headers:**\n   ```javascript\n   // Express.js middleware\n   app.use((req, res, next) => {\n     // Static assets with long-term caching\n     if (req.url.match(/\\.(js|css|png|jpg|jpeg|gif|ico|svg)$/)) {\n       res.setHeader('Cache-Control', 'public, max-age=31536000'); // 1 year\n       res.setHeader('ETag', generateETag(req.url));\n     }\n     \n     // API responses with short-term caching\n     if (req.url.startsWith('/api/')) {\n       res.setHeader('Cache-Control', 'public, max-age=300'); // 5 minutes\n     }\n     \n     next();\n   });\n   ```\n\n   **Service Worker Caching:**\n   ```javascript\n   // sw.js - Service Worker\n   const CACHE_NAME = 'app-cache-v1';\n   const urlsToCache = [\n     '/',\n     '/static/js/bundle.js',\n     '/static/css/main.css',\n   ];\n\n   self.addEventListener('install', (event) => {\n     event.waitUntil(\n       caches.open(CACHE_NAME)\n         .then((cache) => cache.addAll(urlsToCache))\n     );\n   });\n\n   self.addEventListener('fetch', (event) => {\n     event.respondWith(\n       caches.match(event.request)\n         .then((response) => {\n           // Return cached version or fetch from network\n           return response || fetch(event.request);\n         })\n     );\n   });\n   ```\n\n3. **Application-Level Caching**\n   - Implement in-memory and distributed caching:\n\n   **Node.js Memory Cache:**\n   ```javascript\n   const NodeCache = require('node-cache');\n   const cache = new NodeCache({ stdTTL: 600 }); // 10 minutes default TTL\n\n   class CacheService {\n     static get(key) {\n       return cache.get(key);\n     }\n\n     static set(key, value, ttl = 600) {\n       return cache.set(key, value, ttl);\n     }\n\n     static del(key) {\n       return cache.del(key);\n     }\n\n     static flush() {\n       return cache.flushAll();\n     }\n\n     // Cache wrapper for expensive operations\n     static async memoize(key, fn, ttl = 600) {\n       let result = this.get(key);\n       if (result === undefined) {\n         result = await fn();\n         this.set(key, result, ttl);\n       }\n       return result;\n     }\n   }\n\n   // Usage example\n   app.get('/api/users/:id', async (req, res) => {\n     const userId = req.params.id;\n     const cacheKey = `user:${userId}`;\n     \n     const user = await CacheService.memoize(\n       cacheKey,\n       () => getUserFromDatabase(userId),\n       900 // 15 minutes\n     );\n     \n     res.json(user);\n   });\n   ```\n\n   **Redis Distributed Cache:**\n   ```javascript\n   const redis = require('redis');\n   const client = redis.createClient({\n     host: process.env.REDIS_HOST || 'localhost',\n     port: process.env.REDIS_PORT || 6379,\n   });\n\n   class RedisCache {\n     static async get(key) {\n       try {\n         const value = await client.get(key);\n         return value ? JSON.parse(value) : null;\n       } catch (error) {\n         console.error('Cache get error:', error);\n         return null;\n       }\n     }\n\n     static async set(key, value, ttl = 600) {\n       try {\n         const serialized = JSON.stringify(value);\n         if (ttl) {\n           await client.setex(key, ttl, serialized);\n         } else {\n           await client.set(key, serialized);\n         }\n         return true;\n       } catch (error) {\n         console.error('Cache set error:', error);\n         return false;\n       }\n     }\n\n     static async del(key) {\n       try {\n         await client.del(key);\n         return true;\n       } catch (error) {\n         console.error('Cache delete error:', error);\n         return false;\n       }\n     }\n\n     // Pattern-based cache invalidation\n     static async invalidatePattern(pattern) {\n       try {\n         const keys = await client.keys(pattern);\n         if (keys.length > 0) {\n           await client.del(keys);\n         }\n         return true;\n       } catch (error) {\n         console.error('Cache invalidation error:', error);\n         return false;\n       }\n     }\n   }\n   ```\n\n4. **Database Query Caching**\n   - Implement database-level caching strategies:\n\n   **PostgreSQL Query Caching:**\n   ```javascript\n   const { Pool } = require('pg');\n   const pool = new Pool();\n\n   class DatabaseCache {\n     static async cachedQuery(sql, params = [], ttl = 300) {\n       const cacheKey = `query:${Buffer.from(sql + JSON.stringify(params)).toString('base64')}`;\n       \n       // Try cache first\n       let result = await RedisCache.get(cacheKey);\n       if (result) {\n         return result;\n       }\n       \n       // Execute query and cache result\n       const dbResult = await pool.query(sql, params);\n       result = dbResult.rows;\n       \n       await RedisCache.set(cacheKey, result, ttl);\n       return result;\n     }\n\n     // Invalidate cache by table\n     static async invalidateTable(tableName) {\n       await RedisCache.invalidatePattern(`query:*${tableName}*`);\n     }\n   }\n\n   // Usage\n   app.get('/api/products', async (req, res) => {\n     const products = await DatabaseCache.cachedQuery(\n       'SELECT * FROM products WHERE active = true ORDER BY created_at DESC',\n       [],\n       600 // 10 minutes\n     );\n     res.json(products);\n   });\n   ```\n\n   **MongoDB Caching with Mongoose:**\n   ```javascript\n   const mongoose = require('mongoose');\n\n   // Mongoose query caching plugin\n   function cachePlugin(schema) {\n     schema.add({\n       cacheKey: { type: String, index: true },\n       cachedAt: { type: Date },\n     });\n\n     schema.methods.cache = function(ttl = 300) {\n       this.cacheKey = this.constructor.generateCacheKey(this);\n       this.cachedAt = new Date();\n       return this;\n     };\n\n     schema.statics.findCached = async function(query, ttl = 300) {\n       const cacheKey = this.generateCacheKey(query);\n       \n       let result = await RedisCache.get(cacheKey);\n       if (result) {\n         return result;\n       }\n       \n       result = await this.find(query);\n       await RedisCache.set(cacheKey, result, ttl);\n       return result;\n     };\n\n     schema.statics.generateCacheKey = function(data) {\n       return `${this.modelName}:${JSON.stringify(data)}`;\n     };\n   }\n\n   // Apply plugin to schema\n   const ProductSchema = new mongoose.Schema({\n     name: String,\n     price: Number,\n     category: String,\n   });\n\n   ProductSchema.plugin(cachePlugin);\n   ```\n\n5. **API Response Caching**\n   - Implement comprehensive API caching:\n\n   **Express Cache Middleware:**\n   ```javascript\n   function cacheMiddleware(ttl = 300) {\n     return async (req, res, next) => {\n       // Only cache GET requests\n       if (req.method !== 'GET') {\n         return next();\n       }\n\n       const cacheKey = `api:${req.originalUrl}`;\n       const cached = await RedisCache.get(cacheKey);\n\n       if (cached) {\n         return res.json(cached);\n       }\n\n       // Override res.json to cache the response\n       const originalJson = res.json;\n       res.json = function(data) {\n         RedisCache.set(cacheKey, data, ttl);\n         return originalJson.call(this, data);\n       };\n\n       next();\n     };\n   }\n\n   // Usage\n   app.get('/api/dashboard', cacheMiddleware(600), async (req, res) => {\n     const dashboardData = await getDashboardData();\n     res.json(dashboardData);\n   });\n   ```\n\n   **GraphQL Query Caching:**\n   ```javascript\n   const { ApolloServer } = require('apollo-server-express');\n   const { ResponseCache } = require('apollo-server-plugin-response-cache');\n\n   const server = new ApolloServer({\n     typeDefs,\n     resolvers,\n     plugins: [\n       ResponseCache({\n         sessionId: (requestContext) => \n           requestContext.request.http.headers.authorization || null,\n         maximumAge: 300, // 5 minutes default\n         scope: 'PUBLIC',\n       }),\n     ],\n     cacheControl: {\n       defaultMaxAge: 300,\n       calculateHttpHeaders: false,\n       stripFormattedExtensions: false,\n     },\n   });\n\n   // Resolver-level caching\n   const resolvers = {\n     Query: {\n       products: async (parent, args, context) => {\n         return await DatabaseCache.cachedQuery(\n           'SELECT * FROM products WHERE category = $1',\n           [args.category],\n           600\n         );\n       },\n     },\n   };\n   ```\n\n6. **Cache Invalidation Strategies**\n   - Implement intelligent cache invalidation:\n\n   **Event-Driven Cache Invalidation:**\n   ```javascript\n   const EventEmitter = require('events');\n   const cacheInvalidator = new EventEmitter();\n\n   class CacheInvalidator {\n     static invalidateUser(userId) {\n       const patterns = [\n         `user:${userId}*`,\n         `api:/api/users/${userId}*`,\n         'api:/api/dashboard*', // If dashboard shows user data\n       ];\n       \n       patterns.forEach(async (pattern) => {\n         await RedisCache.invalidatePattern(pattern);\n       });\n       \n       cacheInvalidator.emit('user:updated', userId);\n     }\n\n     static invalidateProduct(productId) {\n       const patterns = [\n         `product:${productId}*`,\n         'api:/api/products*',\n         'query:*products*',\n       ];\n       \n       patterns.forEach(async (pattern) => {\n         await RedisCache.invalidatePattern(pattern);\n       });\n     }\n   }\n\n   // Trigger invalidation on data changes\n   app.put('/api/users/:id', async (req, res) => {\n     const userId = req.params.id;\n     await updateUser(userId, req.body);\n     \n     // Invalidate related caches\n     CacheInvalidator.invalidateUser(userId);\n     \n     res.json({ success: true });\n   });\n   ```\n\n7. **Frontend Caching Strategies**\n   - Implement client-side caching:\n\n   **React Query Caching:**\n   ```javascript\n   import { QueryClient, QueryClientProvider, useQuery } from 'react-query';\n\n   const queryClient = new QueryClient({\n     defaultOptions: {\n       queries: {\n         staleTime: 5 * 60 * 1000, // 5 minutes\n         cacheTime: 10 * 60 * 1000, // 10 minutes\n         retry: 3,\n         refetchOnWindowFocus: false,\n       },\n     },\n   });\n\n   function ProductList() {\n     const { data: products, isLoading, error } = useQuery(\n       'products',\n       () => fetch('/api/products').then(res => res.json()),\n       {\n         staleTime: 10 * 60 * 1000, // 10 minutes\n         cacheTime: 30 * 60 * 1000, // 30 minutes\n       }\n     );\n\n     if (isLoading) return <div>Loading...</div>;\n     if (error) return <div>Error: {error.message}</div>;\n\n     return (\n       <div>\n         {products.map(product => (\n           <div key={product.id}>{product.name}</div>\n         ))}\n       </div>\n     );\n   }\n   ```\n\n   **Local Storage Caching:**\n   ```javascript\n   class LocalStorageCache {\n     static set(key, value, ttl = 3600000) { // 1 hour default\n       const item = {\n         value,\n         expiry: Date.now() + ttl,\n       };\n       localStorage.setItem(key, JSON.stringify(item));\n     }\n\n     static get(key) {\n       const item = localStorage.getItem(key);\n       if (!item) return null;\n\n       const parsed = JSON.parse(item);\n       if (Date.now() > parsed.expiry) {\n         localStorage.removeItem(key);\n         return null;\n       }\n\n       return parsed.value;\n     }\n\n     static remove(key) {\n       localStorage.removeItem(key);\n     }\n\n     static clear() {\n       localStorage.clear();\n     }\n   }\n   ```\n\n8. **Cache Monitoring and Analytics**\n   - Set up cache performance monitoring:\n\n   **Cache Metrics Collection:**\n   ```javascript\n   class CacheMetrics {\n     static hits = 0;\n     static misses = 0;\n     static errors = 0;\n\n     static recordHit() {\n       this.hits++;\n     }\n\n     static recordMiss() {\n       this.misses++;\n     }\n\n     static recordError() {\n       this.errors++;\n     }\n\n     static getStats() {\n       const total = this.hits + this.misses;\n       return {\n         hits: this.hits,\n         misses: this.misses,\n         errors: this.errors,\n         hitRate: total > 0 ? (this.hits / total * 100).toFixed(2) : 0,\n         total,\n       };\n     }\n\n     static reset() {\n       this.hits = 0;\n       this.misses = 0;\n       this.errors = 0;\n     }\n   }\n\n   // Enhanced cache service with metrics\n   class MetricsCache {\n     static async get(key) {\n       try {\n         const value = await RedisCache.get(key);\n         if (value !== null) {\n           CacheMetrics.recordHit();\n         } else {\n           CacheMetrics.recordMiss();\n         }\n         return value;\n       } catch (error) {\n         CacheMetrics.recordError();\n         throw error;\n       }\n     }\n   }\n\n   // Metrics endpoint\n   app.get('/api/cache/stats', (req, res) => {\n     res.json(CacheMetrics.getStats());\n   });\n   ```\n\n9. **Cache Warming and Preloading**\n   - Implement cache warming strategies:\n\n   **Scheduled Cache Warming:**\n   ```javascript\n   const cron = require('node-cron');\n\n   class CacheWarmer {\n     static async warmPopularData() {\n       console.log('Starting cache warming...');\n       \n       // Warm popular products\n       const popularProducts = await DatabaseCache.cachedQuery(\n         'SELECT * FROM products ORDER BY view_count DESC LIMIT 100',\n         [],\n         3600 // 1 hour\n       );\n       \n       // Warm user sessions\n       const activeUsers = await DatabaseCache.cachedQuery(\n         'SELECT id FROM users WHERE last_active > NOW() - INTERVAL 1 DAY',\n         [],\n         1800 // 30 minutes\n       );\n       \n       console.log(`Warmed cache for ${popularProducts.length} products and ${activeUsers.length} users`);\n     }\n\n     static async warmOnDemand(cacheKeys) {\n       for (const key of cacheKeys) {\n         if (!(await RedisCache.get(key))) {\n           // Generate cache for missing keys\n           await this.generateCacheForKey(key);\n         }\n       }\n     }\n   }\n\n   // Schedule cache warming\n   cron.schedule('0 */6 * * *', () => { // Every 6 hours\n     CacheWarmer.warmPopularData();\n   });\n   ```\n\n10. **Testing and Validation**\n    - Set up cache testing and validation:\n\n    **Cache Testing:**\n    ```javascript\n    // tests/cache.test.js\n    const request = require('supertest');\n    const app = require('../app');\n\n    describe('Cache Performance', () => {\n      test('should cache API responses', async () => {\n        // First request - should miss cache\n        const start1 = Date.now();\n        const response1 = await request(app).get('/api/products');\n        const duration1 = Date.now() - start1;\n\n        // Second request - should hit cache\n        const start2 = Date.now();\n        const response2 = await request(app).get('/api/products');\n        const duration2 = Date.now() - start2;\n\n        expect(response1.body).toEqual(response2.body);\n        expect(duration2).toBeLessThan(duration1 / 2); // Cached should be faster\n      });\n\n      test('should invalidate cache properly', async () => {\n        // Get initial data\n        const initial = await request(app).get('/api/products');\n        \n        // Update data\n        await request(app)\n          .put('/api/products/1')\n          .send({ name: 'Updated Product' });\n        \n        // Should get updated data\n        const updated = await request(app).get('/api/products');\n        expect(updated.body).not.toEqual(initial.body);\n      });\n    });\n    ```"
              },
              {
                "name": "/implement-graphql-api",
                "description": "Implement GraphQL API endpoints",
                "path": "plugins/all-commands/commands/implement-graphql-api.md",
                "frontmatter": {
                  "description": "Implement GraphQL API endpoints",
                  "category": "api-development"
                },
                "content": "# Implement GraphQL API\n\nImplement GraphQL API endpoints\n\n## Instructions\n\n1. **GraphQL Setup and Configuration**\n   - Set up GraphQL server with Apollo Server or similar\n   - Configure schema-first or code-first approach\n   - Plan GraphQL architecture and data modeling\n   - Set up development tools and introspection\n   - Configure GraphQL playground and documentation\n\n2. **Schema Definition and Type System**\n   - Define comprehensive GraphQL schema:\n\n   **Schema Definition (SDL):**\n   ```graphql\n   # schema/schema.graphql\n   \n   # Scalar types\n   scalar DateTime\n   scalar EmailAddress\n   scalar PhoneNumber\n   scalar JSON\n   scalar Upload\n\n   # User types and enums\n   enum UserRole {\n     USER\n     ADMIN\n     MANAGER\n   }\n\n   enum UserStatus {\n     ACTIVE\n     INACTIVE\n     SUSPENDED\n     PENDING_VERIFICATION\n   }\n\n   type User {\n     id: ID!\n     email: EmailAddress!\n     username: String!\n     firstName: String!\n     lastName: String!\n     fullName: String!\n     phone: PhoneNumber\n     dateOfBirth: DateTime\n     avatar: String\n     role: UserRole!\n     status: UserStatus!\n     emailVerified: Boolean!\n     phoneVerified: Boolean!\n     profile: UserProfile\n     orders(\n       first: Int = 10\n       after: String\n       status: OrderStatus\n     ): OrderConnection!\n     createdAt: DateTime!\n     updatedAt: DateTime!\n     lastLoginAt: DateTime\n   }\n\n   type UserProfile {\n     bio: String\n     website: String\n     location: String\n     timezone: String!\n     language: String!\n     notificationPreferences: JSON!\n     privacySettings: JSON!\n   }\n\n   # Product types\n   enum ProductStatus {\n     DRAFT\n     ACTIVE\n     INACTIVE\n     ARCHIVED\n   }\n\n   enum ProductVisibility {\n     VISIBLE\n     HIDDEN\n     CATALOG_ONLY\n     SEARCH_ONLY\n   }\n\n   type Product {\n     id: ID!\n     name: String!\n     slug: String!\n     sku: String!\n     description: String\n     shortDescription: String\n     price: Float!\n     comparePrice: Float\n     costPrice: Float\n     weight: Float\n     dimensions: ProductDimensions\n     category: Category\n     brand: Brand\n     vendor: Vendor\n     status: ProductStatus!\n     visibility: ProductVisibility!\n     inventoryTracking: Boolean!\n     inventoryQuantity: Int\n     lowStockThreshold: Int\n     allowBackorder: Boolean!\n     requiresShipping: Boolean!\n     isDigital: Boolean!\n     featured: Boolean!\n     tags: [String!]!\n     attributes: JSON!\n     images: [ProductImage!]!\n     variants: [ProductVariant!]!\n     reviews(\n       first: Int = 10\n       after: String\n       rating: Int\n     ): ReviewConnection!\n     averageRating: Float\n     reviewCount: Int!\n     createdAt: DateTime!\n     updatedAt: DateTime!\n     publishedAt: DateTime\n   }\n\n   type ProductDimensions {\n     length: Float\n     width: Float\n     height: Float\n     unit: String!\n   }\n\n   type ProductImage {\n     id: ID!\n     url: String!\n     altText: String\n     sortOrder: Int!\n   }\n\n   type ProductVariant {\n     id: ID!\n     sku: String!\n     price: Float!\n     comparePrice: Float\n     inventoryQuantity: Int\n     attributes: JSON!\n     image: ProductImage\n   }\n\n   # Order types\n   enum OrderStatus {\n     PENDING\n     PROCESSING\n     SHIPPED\n     DELIVERED\n     CANCELLED\n     REFUNDED\n     ON_HOLD\n   }\n\n   type Order {\n     id: ID!\n     orderNumber: String!\n     user: User\n     status: OrderStatus!\n     currency: String!\n     subtotal: Float!\n     taxTotal: Float!\n     shippingTotal: Float!\n     discountTotal: Float!\n     total: Float!\n     billingAddress: Address!\n     shippingAddress: Address!\n     shippingMethod: String\n     trackingNumber: String\n     items: [OrderItem!]!\n     notes: String\n     createdAt: DateTime!\n     updatedAt: DateTime!\n     shippedAt: DateTime\n     deliveredAt: DateTime\n   }\n\n   type OrderItem {\n     id: ID!\n     product: Product!\n     productVariant: ProductVariant\n     quantity: Int!\n     unitPrice: Float!\n     totalPrice: Float!\n     productName: String!\n     productSku: String!\n     productAttributes: JSON\n   }\n\n   type Address {\n     firstName: String!\n     lastName: String!\n     company: String\n     addressLine1: String!\n     addressLine2: String\n     city: String!\n     state: String\n     postalCode: String!\n     country: String!\n     phone: PhoneNumber\n   }\n\n   # Connection types for pagination\n   type UserConnection {\n     edges: [UserEdge!]!\n     pageInfo: PageInfo!\n     totalCount: Int!\n   }\n\n   type UserEdge {\n     node: User!\n     cursor: String!\n   }\n\n   type ProductConnection {\n     edges: [ProductEdge!]!\n     pageInfo: PageInfo!\n     totalCount: Int!\n   }\n\n   type ProductEdge {\n     node: Product!\n     cursor: String!\n   }\n\n   type OrderConnection {\n     edges: [OrderEdge!]!\n     pageInfo: PageInfo!\n     totalCount: Int!\n   }\n\n   type OrderEdge {\n     node: Order!\n     cursor: String!\n   }\n\n   type PageInfo {\n     hasNextPage: Boolean!\n     hasPreviousPage: Boolean!\n     startCursor: String\n     endCursor: String\n   }\n\n   # Input types\n   input CreateUserInput {\n     email: EmailAddress!\n     password: String!\n     firstName: String!\n     lastName: String!\n     phone: PhoneNumber\n     dateOfBirth: DateTime\n     role: UserRole = USER\n   }\n\n   input UpdateUserInput {\n     email: EmailAddress\n     firstName: String\n     lastName: String\n     phone: PhoneNumber\n     dateOfBirth: DateTime\n     status: UserStatus\n   }\n\n   input ProductFilters {\n     category: ID\n     brand: ID\n     priceMin: Float\n     priceMax: Float\n     status: ProductStatus\n     featured: Boolean\n     inStock: Boolean\n     tags: [String!]\n     search: String\n   }\n\n   input CreateProductInput {\n     name: String!\n     slug: String!\n     sku: String!\n     description: String\n     price: Float!\n     comparePrice: Float\n     categoryId: ID\n     brandId: ID\n     status: ProductStatus = DRAFT\n     inventoryQuantity: Int = 0\n     attributes: JSON\n     tags: [String!]\n   }\n\n   # Root types\n   type Query {\n     # User queries\n     me: User\n     user(id: ID!): User\n     users(\n       first: Int = 10\n       after: String\n       search: String\n       role: UserRole\n       status: UserStatus\n     ): UserConnection!\n\n     # Product queries\n     product(id: ID, slug: String): Product\n     products(\n       first: Int = 10\n       after: String\n       filters: ProductFilters\n       sortBy: ProductSortBy = CREATED_AT\n       sortOrder: SortOrder = DESC\n     ): ProductConnection!\n\n     # Order queries\n     order(id: ID!): Order\n     orders(\n       first: Int = 10\n       after: String\n       status: OrderStatus\n       userId: ID\n     ): OrderConnection!\n\n     # Search\n     search(\n       query: String!\n       first: Int = 10\n       after: String\n       types: [SearchType!] = [USER, PRODUCT, ORDER]\n     ): SearchConnection!\n   }\n\n   type Mutation {\n     # Auth mutations\n     login(email: EmailAddress!, password: String!): AuthPayload!\n     logout: Boolean!\n     refreshToken: AuthPayload!\n     forgotPassword(email: EmailAddress!): Boolean!\n     resetPassword(token: String!, password: String!): AuthPayload!\n\n     # User mutations\n     createUser(input: CreateUserInput!): User!\n     updateUser(id: ID!, input: UpdateUserInput!): User!\n     deleteUser(id: ID!): Boolean!\n     updateProfile(input: UpdateProfileInput!): UserProfile!\n\n     # Product mutations\n     createProduct(input: CreateProductInput!): Product!\n     updateProduct(id: ID!, input: UpdateProductInput!): Product!\n     deleteProduct(id: ID!): Boolean!\n     uploadProductImage(productId: ID!, file: Upload!): ProductImage!\n\n     # Order mutations\n     createOrder(input: CreateOrderInput!): Order!\n     updateOrderStatus(id: ID!, status: OrderStatus!): Order!\n     addOrderItem(orderId: ID!, input: AddOrderItemInput!): OrderItem!\n     removeOrderItem(id: ID!): Boolean!\n   }\n\n   type Subscription {\n     # Real-time updates\n     orderUpdated(userId: ID): Order!\n     productUpdated(productId: ID): Product!\n     userStatusChanged(userId: ID): User!\n     \n     # Admin subscriptions\n     newOrder: Order!\n     lowStockAlert: Product!\n   }\n\n   enum ProductSortBy {\n     CREATED_AT\n     NAME\n     PRICE\n     RATING\n     POPULARITY\n   }\n\n   enum SortOrder {\n     ASC\n     DESC\n   }\n\n   enum SearchType {\n     USER\n     PRODUCT\n     ORDER\n   }\n\n   type AuthPayload {\n     token: String!\n     refreshToken: String!\n     user: User!\n     expiresAt: DateTime!\n   }\n   ```\n\n3. **Resolver Implementation**\n   - Implement comprehensive resolvers:\n\n   **Main Resolvers:**\n   ```javascript\n   // resolvers/index.js\n   const { GraphQLDateTime } = require('graphql-iso-date');\n   const { GraphQLEmailAddress, GraphQLPhoneNumber } = require('graphql-scalars');\n   const GraphQLJSON = require('graphql-type-json');\n   const GraphQLUpload = require('graphql-upload/GraphQLUpload.js');\n\n   const userResolvers = require('./userResolvers');\n   const productResolvers = require('./productResolvers');\n   const orderResolvers = require('./orderResolvers');\n   const searchResolvers = require('./searchResolvers');\n\n   const resolvers = {\n     // Custom scalars\n     DateTime: GraphQLDateTime,\n     EmailAddress: GraphQLEmailAddress,\n     PhoneNumber: GraphQLPhoneNumber,\n     JSON: GraphQLJSON,\n     Upload: GraphQLUpload,\n\n     // Root resolvers\n     Query: {\n       ...userResolvers.Query,\n       ...productResolvers.Query,\n       ...orderResolvers.Query,\n       ...searchResolvers.Query\n     },\n\n     Mutation: {\n       ...userResolvers.Mutation,\n       ...productResolvers.Mutation,\n       ...orderResolvers.Mutation\n     },\n\n     Subscription: {\n       ...userResolvers.Subscription,\n       ...productResolvers.Subscription,\n       ...orderResolvers.Subscription\n     },\n\n     // Type resolvers\n     User: userResolvers.User,\n     Product: productResolvers.Product,\n     Order: orderResolvers.Order\n   };\n\n   module.exports = resolvers;\n   ```\n\n   **User Resolvers:**\n   ```javascript\n   // resolvers/userResolvers.js\n   const { AuthenticationError, ForbiddenError, UserInputError } = require('apollo-server-express');\n   const { withFilter } = require('graphql-subscriptions');\n   const userService = require('../services/userService');\n   const { requireAuth, requireRole } = require('../utils/authHelpers');\n   const { createConnectionFromArray } = require('../utils/connectionHelpers');\n\n   const userResolvers = {\n     Query: {\n       async me(parent, args, context) {\n         requireAuth(context);\n         return await userService.findById(context.user.id);\n       },\n\n       async user(parent, { id }, context) {\n         requireAuth(context);\n         \n         const user = await userService.findById(id);\n         if (!user) {\n           throw new UserInputError('User not found');\n         }\n\n         // Privacy check - users can only see their own data unless admin\n         if (context.user.id !== user.id && !['admin', 'manager'].includes(context.user.role)) {\n           throw new ForbiddenError('Insufficient permissions');\n         }\n\n         return user;\n       },\n\n       async users(parent, { first, after, search, role, status }, context) {\n         requireAuth(context);\n         requireRole(context, ['admin', 'manager']);\n\n         const result = await userService.findUsers({\n           first,\n           after,\n           search,\n           role,\n           status\n         });\n\n         return createConnectionFromArray(result.users, {\n           first,\n           after,\n           totalCount: result.totalCount\n         });\n       }\n     },\n\n     Mutation: {\n       async createUser(parent, { input }, context) {\n         requireAuth(context);\n         requireRole(context, ['admin']);\n\n         // Check for existing user\n         const existingUser = await userService.findByEmail(input.email);\n         if (existingUser) {\n           throw new UserInputError('User with this email already exists');\n         }\n\n         const user = await userService.createUser(input);\n         \n         // Publish subscription for real-time updates\n         context.pubsub.publish('USER_CREATED', { userCreated: user });\n         \n         return user;\n       },\n\n       async updateUser(parent, { id, input }, context) {\n         requireAuth(context);\n         \n         const existingUser = await userService.findById(id);\n         if (!existingUser) {\n           throw new UserInputError('User not found');\n         }\n\n         // Authorization check\n         if (context.user.id !== id && !['admin', 'manager'].includes(context.user.role)) {\n           throw new ForbiddenError('Insufficient permissions');\n         }\n\n         // Role change restriction\n         if (input.role && !['admin'].includes(context.user.role)) {\n           throw new ForbiddenError('Insufficient permissions to change user role');\n         }\n\n         const updatedUser = await userService.updateUser(id, input);\n         \n         // Publish subscription\n         context.pubsub.publish('USER_UPDATED', { userUpdated: updatedUser });\n         \n         return updatedUser;\n       },\n\n       async deleteUser(parent, { id }, context) {\n         requireAuth(context);\n         requireRole(context, ['admin']);\n\n         // Prevent self-deletion\n         if (context.user.id === id) {\n           throw new UserInputError('Cannot delete your own account');\n         }\n\n         const existingUser = await userService.findById(id);\n         if (!existingUser) {\n           throw new UserInputError('User not found');\n         }\n\n         await userService.deleteUser(id);\n         \n         // Publish subscription\n         context.pubsub.publish('USER_DELETED', { userDeleted: existingUser });\n         \n         return true;\n       }\n     },\n\n     Subscription: {\n       userStatusChanged: {\n         subscribe: withFilter(\n           (parent, args, context) => {\n             requireAuth(context);\n             return context.pubsub.asyncIterator(['USER_UPDATED']);\n           },\n           (payload, variables) => {\n             // Filter by userId if provided\n             return !variables.userId || payload.userUpdated.id === variables.userId;\n           }\n         )\n       }\n     },\n\n     // Field resolvers\n     User: {\n       fullName(parent) {\n         return `${parent.firstName} ${parent.lastName}`;\n       },\n\n       async profile(parent, args, context) {\n         return await userService.getUserProfile(parent.id);\n       },\n\n       async orders(parent, { first, after, status }, context) {\n         requireAuth(context);\n         \n         // Users can only see their own orders unless admin\n         if (context.user.id !== parent.id && !['admin', 'manager'].includes(context.user.role)) {\n           throw new ForbiddenError('Insufficient permissions');\n         }\n\n         const result = await userService.getUserOrders(parent.id, {\n           first,\n           after,\n           status\n         });\n\n         return createConnectionFromArray(result.orders, {\n           first,\n           after,\n           totalCount: result.totalCount\n         });\n       }\n     }\n   };\n\n   module.exports = userResolvers;\n   ```\n\n4. **DataLoader for N+1 Problem**\n   - Implement efficient data loading:\n\n   **DataLoader Implementation:**\n   ```javascript\n   // dataLoaders/index.js\n   const DataLoader = require('dataloader');\n   const userService = require('../services/userService');\n   const productService = require('../services/productService');\n   const orderService = require('../services/orderService');\n\n   class DataLoaders {\n     constructor() {\n       this.userLoader = new DataLoader(\n         async (userIds) => {\n           const users = await userService.findByIds(userIds);\n           return userIds.map(id => users.find(user => user.id === id) || null);\n         },\n         {\n           cacheKeyFn: (key) => key.toString(),\n           maxBatchSize: 100\n         }\n       );\n\n       this.userProfileLoader = new DataLoader(\n         async (userIds) => {\n           const profiles = await userService.getProfilesByUserIds(userIds);\n           return userIds.map(id => profiles.find(profile => profile.userId === id) || null);\n         }\n       );\n\n       this.productLoader = new DataLoader(\n         async (productIds) => {\n           const products = await productService.findByIds(productIds);\n           return productIds.map(id => products.find(product => product.id === id) || null);\n         }\n       );\n\n       this.productCategoryLoader = new DataLoader(\n         async (categoryIds) => {\n           const categories = await productService.getCategoriesByIds(categoryIds);\n           return categoryIds.map(id => categories.find(category => category.id === id) || null);\n         }\n       );\n\n       this.productImagesLoader = new DataLoader(\n         async (productIds) => {\n           const imagesMap = await productService.getImagesByProductIds(productIds);\n           return productIds.map(id => imagesMap[id] || []);\n         }\n       );\n\n       this.orderItemsLoader = new DataLoader(\n         async (orderIds) => {\n           const itemsMap = await orderService.getItemsByOrderIds(orderIds);\n           return orderIds.map(id => itemsMap[id] || []);\n         }\n       );\n\n       this.productReviewsLoader = new DataLoader(\n         async (productIds) => {\n           const reviewsMap = await productService.getReviewsByProductIds(productIds);\n           return productIds.map(id => reviewsMap[id] || []);\n         }\n       );\n     }\n\n     // Clear all caches\n     clearAll() {\n       this.userLoader.clearAll();\n       this.userProfileLoader.clearAll();\n       this.productLoader.clearAll();\n       this.productCategoryLoader.clearAll();\n       this.productImagesLoader.clearAll();\n       this.orderItemsLoader.clearAll();\n       this.productReviewsLoader.clearAll();\n     }\n\n     // Clear specific cache\n     clearUser(userId) {\n       this.userLoader.clear(userId);\n       this.userProfileLoader.clear(userId);\n     }\n\n     clearProduct(productId) {\n       this.productLoader.clear(productId);\n       this.productImagesLoader.clear(productId);\n       this.productReviewsLoader.clear(productId);\n     }\n   }\n\n   module.exports = DataLoaders;\n   ```\n\n5. **Authentication and Authorization**\n   - Implement GraphQL-specific auth:\n\n   **Auth Helpers:**\n   ```javascript\n   // utils/authHelpers.js\n   const { AuthenticationError, ForbiddenError } = require('apollo-server-express');\n   const jwt = require('jsonwebtoken');\n   const userService = require('../services/userService');\n\n   class GraphQLAuth {\n     static async getUser(req) {\n       const authHeader = req.headers.authorization;\n       \n       if (!authHeader) {\n         return null;\n       }\n\n       const token = authHeader.replace('Bearer ', '');\n       \n       try {\n         const decoded = jwt.verify(token, process.env.JWT_SECRET);\n         const user = await userService.findById(decoded.userId);\n         \n         if (!user || user.status !== 'active') {\n           return null;\n         }\n\n         return user;\n       } catch (error) {\n         return null;\n       }\n     }\n\n     static requireAuth(context) {\n       if (!context.user) {\n         throw new AuthenticationError('Authentication required');\n       }\n       return context.user;\n     }\n\n     static requireRole(context, roles) {\n       this.requireAuth(context);\n       \n       if (!roles.includes(context.user.role)) {\n         throw new ForbiddenError(`Requires one of the following roles: ${roles.join(', ')}`);\n       }\n       \n       return context.user;\n     }\n\n     static requirePermission(context, permissions) {\n       this.requireAuth(context);\n       \n       const userPermissions = context.user.permissions || [];\n       const hasPermission = permissions.some(permission => \n         userPermissions.includes(permission)\n       );\n       \n       if (!hasPermission) {\n         throw new ForbiddenError(`Requires one of the following permissions: ${permissions.join(', ')}`);\n       }\n       \n       return context.user;\n     }\n\n     static canAccessResource(context, resourceUserId, adminRoles = ['admin', 'manager']) {\n       this.requireAuth(context);\n       \n       const isOwner = context.user.id === resourceUserId;\n       const isAdmin = adminRoles.includes(context.user.role);\n       \n       if (!isOwner && !isAdmin) {\n         throw new ForbiddenError('Insufficient permissions to access this resource');\n       }\n       \n       return context.user;\n     }\n   }\n\n   // Export individual functions for convenience\n   const { requireAuth, requireRole, requirePermission, canAccessResource } = GraphQLAuth;\n\n   module.exports = {\n     GraphQLAuth,\n     requireAuth,\n     requireRole,\n     requirePermission,\n     canAccessResource\n   };\n   ```\n\n6. **Real-time Subscriptions**\n   - Implement GraphQL subscriptions:\n\n   **Subscription Setup:**\n   ```javascript\n   // subscriptions/index.js\n   const { PubSub } = require('graphql-subscriptions');\n   const { RedisPubSub } = require('graphql-redis-subscriptions');\n   const Redis = require('ioredis');\n\n   // Use Redis for production, in-memory for development\n   const createPubSub = () => {\n     if (process.env.NODE_ENV === 'production') {\n       const redisClient = new Redis(process.env.REDIS_URL);\n       return new RedisPubSub({\n         publisher: redisClient,\n         subscriber: redisClient.duplicate()\n       });\n     } else {\n       return new PubSub();\n     }\n   };\n\n   const pubsub = createPubSub();\n\n   // Subscription events\n   const SUBSCRIPTION_EVENTS = {\n     USER_CREATED: 'USER_CREATED',\n     USER_UPDATED: 'USER_UPDATED',\n     USER_DELETED: 'USER_DELETED',\n     ORDER_CREATED: 'ORDER_CREATED',\n     ORDER_UPDATED: 'ORDER_UPDATED',\n     PRODUCT_UPDATED: 'PRODUCT_UPDATED',\n     LOW_STOCK_ALERT: 'LOW_STOCK_ALERT'\n   };\n\n   // Subscription resolvers\n   const subscriptionResolvers = {\n     orderUpdated: {\n       subscribe: (parent, { userId }, context) => {\n         requireAuth(context);\n         \n         // Users can only subscribe to their own orders unless admin\n         if (userId && context.user.id !== userId && !['admin', 'manager'].includes(context.user.role)) {\n           throw new ForbiddenError('Insufficient permissions');\n         }\n         \n         return pubsub.asyncIterator([SUBSCRIPTION_EVENTS.ORDER_UPDATED]);\n       },\n       resolve: (payload, { userId }) => {\n         // Filter by userId if provided\n         if (userId && payload.orderUpdated.userId !== userId) {\n           return null;\n         }\n         return payload.orderUpdated;\n       }\n     },\n\n     productUpdated: {\n       subscribe: (parent, { productId }, context) => {\n         return pubsub.asyncIterator([SUBSCRIPTION_EVENTS.PRODUCT_UPDATED]);\n       },\n       resolve: (payload, { productId }) => {\n         // Filter by productId if provided\n         if (productId && payload.productUpdated.id !== productId) {\n           return null;\n         }\n         return payload.productUpdated;\n       }\n     },\n\n     userStatusChanged: {\n       subscribe: (parent, { userId }, context) => {\n         requireAuth(context);\n         requireRole(context, ['admin', 'manager']);\n         \n         return pubsub.asyncIterator([SUBSCRIPTION_EVENTS.USER_UPDATED]);\n       },\n       resolve: (payload, { userId }) => {\n         if (userId && payload.userUpdated.id !== userId) {\n           return null;\n         }\n         return payload.userUpdated;\n       }\n     },\n\n     newOrder: {\n       subscribe: (parent, args, context) => {\n         requireAuth(context);\n         requireRole(context, ['admin', 'manager']);\n         \n         return pubsub.asyncIterator([SUBSCRIPTION_EVENTS.ORDER_CREATED]);\n       }\n     },\n\n     lowStockAlert: {\n       subscribe: (parent, args, context) => {\n         requireAuth(context);\n         requireRole(context, ['admin', 'manager']);\n         \n         return pubsub.asyncIterator([SUBSCRIPTION_EVENTS.LOW_STOCK_ALERT]);\n       }\n     }\n   };\n\n   module.exports = {\n     pubsub,\n     SUBSCRIPTION_EVENTS,\n     subscriptionResolvers\n   };\n   ```\n\n7. **Error Handling and Validation**\n   - Implement comprehensive error handling:\n\n   **Error Handling:**\n   ```javascript\n   // utils/errorHandling.js\n   const { \n     ApolloError, \n     AuthenticationError, \n     ForbiddenError, \n     UserInputError \n   } = require('apollo-server-express');\n\n   class GraphQLErrorHandler {\n     static handleError(error, operation) {\n       // Log error for debugging\n       console.error('GraphQL Error:', {\n         message: error.message,\n         operation: operation?.operationName,\n         variables: operation?.variables,\n         stack: error.stack\n       });\n\n       // Database errors\n       if (error.code === '23505') { // Unique constraint violation\n         return new UserInputError('A record with this information already exists');\n       }\n       \n       if (error.code === '23503') { // Foreign key constraint violation\n         return new UserInputError('Referenced record does not exist');\n       }\n\n       // Validation errors\n       if (error.name === 'ValidationError') {\n         const messages = Object.values(error.errors).map(err => err.message);\n         return new UserInputError('Validation failed', {\n           validationErrors: messages\n         });\n       }\n\n       // Permission errors\n       if (error.message.includes('permission') || error.message.includes('access')) {\n         return new ForbiddenError(error.message);\n       }\n\n       // Authentication errors\n       if (error.message.includes('token') || error.message.includes('auth')) {\n         return new AuthenticationError(error.message);\n       }\n\n       // Network/external service errors\n       if (error.code === 'ENOTFOUND' || error.code === 'ECONNREFUSED') {\n         return new ApolloError('External service unavailable', 'SERVICE_UNAVAILABLE');\n       }\n\n       // Default to internal error\n       return new ApolloError(\n         'An unexpected error occurred',\n         'INTERNAL_ERROR',\n         { originalError: error.message }\n       );\n     }\n\n     static formatError(error) {\n       // Don't expose internal errors in production\n       if (process.env.NODE_ENV === 'production' && !error.extensions?.code) {\n         return new ApolloError('Internal server error', 'INTERNAL_ERROR');\n       }\n\n       // Add request ID for tracking\n       if (error.extensions?.requestId) {\n         error.extensions.requestId = error.extensions.requestId;\n       }\n\n       return error;\n     }\n   }\n\n   // Input validation helper\n   class InputValidator {\n     static validateEmail(email) {\n       const emailRegex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n       if (!emailRegex.test(email)) {\n         throw new UserInputError('Invalid email format');\n       }\n     }\n\n     static validatePassword(password) {\n       if (password.length < 8) {\n         throw new UserInputError('Password must be at least 8 characters long');\n       }\n       \n       if (!/(?=.*[a-z])(?=.*[A-Z])(?=.*\\d)/.test(password)) {\n         throw new UserInputError('Password must contain uppercase, lowercase, and numeric characters');\n       }\n     }\n\n     static validatePhoneNumber(phone) {\n       const phoneRegex = /^\\+?[\\d\\s\\-\\(\\)]{10,20}$/;\n       if (!phoneRegex.test(phone)) {\n         throw new UserInputError('Invalid phone number format');\n       }\n     }\n\n     static validateRequired(value, fieldName) {\n       if (!value || (typeof value === 'string' && !value.trim())) {\n         throw new UserInputError(`${fieldName} is required`);\n       }\n     }\n\n     static validateStringLength(value, fieldName, min = 0, max = 255) {\n       if (typeof value !== 'string') {\n         throw new UserInputError(`${fieldName} must be a string`);\n       }\n       \n       if (value.length < min) {\n         throw new UserInputError(`${fieldName} must be at least ${min} characters`);\n       }\n       \n       if (value.length > max) {\n         throw new UserInputError(`${fieldName} must not exceed ${max} characters`);\n       }\n     }\n\n     static validateNumericRange(value, fieldName, min, max) {\n       if (typeof value !== 'number' || isNaN(value)) {\n         throw new UserInputError(`${fieldName} must be a valid number`);\n       }\n       \n       if (min !== undefined && value < min) {\n         throw new UserInputError(`${fieldName} must be at least ${min}`);\n       }\n       \n       if (max !== undefined && value > max) {\n         throw new UserInputError(`${fieldName} must not exceed ${max}`);\n       }\n     }\n   }\n\n   module.exports = {\n     GraphQLErrorHandler,\n     InputValidator\n   };\n   ```\n\n8. **Performance Optimization**\n   - Implement GraphQL performance optimizations:\n\n   **Query Complexity and Depth Limiting:**\n   ```javascript\n   // utils/queryLimiting.js\n   const depthLimit = require('graphql-depth-limit');\n   const costAnalysis = require('graphql-query-complexity');\n\n   class QueryLimiting {\n     static createDepthLimit(maxDepth = 10) {\n       return depthLimit(maxDepth, {\n         ignoreIntrospection: true\n       });\n     }\n\n     static createComplexityAnalysis(maxComplexity = 1000) {\n       return costAnalysis({\n         maximumComplexity: maxComplexity,\n         introspection: true,\n         scalarCost: 1,\n         objectCost: 1,\n         listFactor: 10,\n         fieldExtensions: {\n           complexity: (options) => {\n             // Custom complexity calculation\n             const { args, childComplexity } = options;\n             \n             // List fields have higher complexity\n             if (args.first) {\n               return childComplexity * Math.min(args.first, 100);\n             }\n             \n             return childComplexity;\n           }\n         },\n         createError: (max, actual) => {\n           return new Error(`Query complexity ${actual} exceeds maximum allowed complexity ${max}`);\n         }\n       });\n     }\n\n     static createQueryTimeout(timeout = 30000) {\n       return {\n         willSendResponse(requestContext) {\n           if (requestContext.request.query) {\n             setTimeout(() => {\n               if (!requestContext.response.http.body) {\n                 throw new Error('Query timeout exceeded');\n               }\n             }, timeout);\n           }\n         }\n       };\n     }\n   }\n\n   // Query caching\n   class QueryCache {\n     constructor(ttl = 300) { // 5 minutes default\n       this.cache = new Map();\n       this.ttl = ttl * 1000; // Convert to milliseconds\n     }\n\n     get(query, variables) {\n       const key = this.generateKey(query, variables);\n       const cached = this.cache.get(key);\n       \n       if (cached && Date.now() - cached.timestamp < this.ttl) {\n         return cached.result;\n       }\n       \n       this.cache.delete(key);\n       return null;\n     }\n\n     set(query, variables, result) {\n       const key = this.generateKey(query, variables);\n       this.cache.set(key, {\n         result,\n         timestamp: Date.now()\n       });\n     }\n\n     generateKey(query, variables) {\n       return `${query}:${JSON.stringify(variables || {})}`;\n     }\n\n     clear() {\n       this.cache.clear();\n     }\n\n     // Middleware for Apollo Server\n     static createCachePlugin(ttl = 300) {\n       const cache = new QueryCache(ttl);\n       \n       return {\n         requestDidStart() {\n           return {\n             willSendResponse(requestContext) {\n               const { request, response } = requestContext;\n               \n               // Only cache successful queries\n               if (response.http.body && !response.errors) {\n                 cache.set(request.query, request.variables, response.http.body);\n               }\n             },\n             \n             willSendRequest(requestContext) {\n               const { request } = requestContext;\n               const cached = cache.get(request.query, request.variables);\n               \n               if (cached) {\n                 requestContext.response.http.body = cached;\n                 return;\n               }\n             }\n           };\n         }\n       };\n     }\n   }\n\n   module.exports = {\n     QueryLimiting,\n     QueryCache\n   };\n   ```\n\n9. **GraphQL Testing**\n   - Implement comprehensive GraphQL testing:\n\n   **GraphQL Test Suite:**\n   ```javascript\n   // tests/graphql/users.test.js\n   const { createTestClient } = require('apollo-server-testing');\n   const { gql } = require('apollo-server-express');\n   const { createTestServer } = require('../helpers/testServer');\n   const { createTestUser, getAuthToken } = require('../helpers/testHelpers');\n\n   describe('User GraphQL API', () => {\n     let server, query, mutate;\n     let testUser, authToken;\n\n     beforeAll(async () => {\n       server = await createTestServer();\n       const testClient = createTestClient(server);\n       query = testClient.query;\n       mutate = testClient.mutate;\n\n       testUser = await createTestUser({ role: 'admin' });\n       authToken = await getAuthToken(testUser);\n     });\n\n     describe('Queries', () => {\n       const GET_USERS = gql`\n         query GetUsers($first: Int, $search: String) {\n           users(first: $first, search: $search) {\n             edges {\n               node {\n                 id\n                 email\n                 firstName\n                 lastName\n                 role\n                 status\n                 createdAt\n               }\n             }\n             pageInfo {\n               hasNextPage\n               hasPreviousPage\n               startCursor\n               endCursor\n             }\n             totalCount\n           }\n         }\n       `;\n\n       test('should return paginated users list', async () => {\n         const result = await query({\n           query: GET_USERS,\n           variables: { first: 10 },\n           context: { user: testUser }\n         });\n\n         expect(result.errors).toBeUndefined();\n         expect(result.data.users).toMatchObject({\n           edges: expect.any(Array),\n           pageInfo: {\n             hasNextPage: expect.any(Boolean),\n             hasPreviousPage: expect.any(Boolean)\n           },\n           totalCount: expect.any(Number)\n         });\n\n         if (result.data.users.edges.length > 0) {\n           expect(result.data.users.edges[0].node).toHaveProperty('id');\n           expect(result.data.users.edges[0].node).toHaveProperty('email');\n           expect(result.data.users.edges[0].node).not.toHaveProperty('password');\n         }\n       });\n\n       test('should filter users by search term', async () => {\n         const result = await query({\n           query: GET_USERS,\n           variables: { search: 'test' },\n           context: { user: testUser }\n         });\n\n         expect(result.errors).toBeUndefined();\n         expect(result.data.users.edges).toEqual(\n           expect.arrayContaining([\n             expect.objectContaining({\n               node: expect.objectContaining({\n                 email: expect.stringContaining('test')\n               })\n             })\n           ])\n         );\n       });\n\n       test('should require authentication', async () => {\n         const result = await query({\n           query: GET_USERS,\n           variables: { first: 10 }\n         });\n\n         expect(result.errors).toBeDefined();\n         expect(result.errors[0].extensions.code).toBe('UNAUTHENTICATED');\n       });\n\n       const GET_ME = gql`\n         query GetMe {\n           me {\n             id\n             email\n             firstName\n             lastName\n             profile {\n               bio\n               website\n             }\n           }\n         }\n       `;\n\n       test('should return current user profile', async () => {\n         const result = await query({\n           query: GET_ME,\n           context: { user: testUser }\n         });\n\n         expect(result.errors).toBeUndefined();\n         expect(result.data.me).toMatchObject({\n           id: testUser.id.toString(),\n           email: testUser.email,\n           firstName: testUser.firstName,\n           lastName: testUser.lastName\n         });\n       });\n     });\n\n     describe('Mutations', () => {\n       const CREATE_USER = gql`\n         mutation CreateUser($input: CreateUserInput!) {\n           createUser(input: $input) {\n             id\n             email\n             firstName\n             lastName\n             role\n             status\n           }\n         }\n       `;\n\n       test('should create user with valid input', async () => {\n         const userInput = {\n           email: 'newuser@example.com',\n           password: 'SecurePass123',\n           firstName: 'New',\n           lastName: 'User',\n           role: 'USER'\n         };\n\n         const result = await mutate({\n           mutation: CREATE_USER,\n           variables: { input: userInput },\n           context: { user: testUser }\n         });\n\n         expect(result.errors).toBeUndefined();\n         expect(result.data.createUser).toMatchObject({\n           email: userInput.email,\n           firstName: userInput.firstName,\n           lastName: userInput.lastName,\n           role: userInput.role,\n           status: 'ACTIVE'\n         });\n         expect(result.data.createUser).toHaveProperty('id');\n       });\n\n       test('should validate email format', async () => {\n         const userInput = {\n           email: 'invalid-email',\n           password: 'SecurePass123',\n           firstName: 'Test',\n           lastName: 'User'\n         };\n\n         const result = await mutate({\n           mutation: CREATE_USER,\n           variables: { input: userInput },\n           context: { user: testUser }\n         });\n\n         expect(result.errors).toBeDefined();\n         expect(result.errors[0].extensions.code).toBe('BAD_USER_INPUT');\n       });\n\n       test('should prevent duplicate email', async () => {\n         const userInput = {\n           email: testUser.email,\n           password: 'SecurePass123',\n           firstName: 'Test',\n           lastName: 'User'\n         };\n\n         const result = await mutate({\n           mutation: CREATE_USER,\n           variables: { input: userInput },\n           context: { user: testUser }\n         });\n\n         expect(result.errors).toBeDefined();\n         expect(result.errors[0].message).toContain('already exists');\n       });\n     });\n\n     describe('Subscriptions', () => {\n       test('should subscribe to user status changes', (done) => {\n         const USER_STATUS_CHANGED = gql`\n           subscription UserStatusChanged($userId: ID) {\n             userStatusChanged(userId: $userId) {\n               id\n               status\n             }\n           }\n         `;\n\n         const observable = server.subscription({\n           query: USER_STATUS_CHANGED,\n           variables: { userId: testUser.id },\n           context: { user: testUser }\n         });\n\n         observable.subscribe({\n           next: (result) => {\n             expect(result.data.userStatusChanged).toMatchObject({\n               id: testUser.id.toString(),\n               status: expect.any(String)\n             });\n             done();\n           },\n           error: done\n         });\n\n         // Trigger the subscription by updating user status\n         setTimeout(() => {\n           server.pubsub.publish('USER_UPDATED', {\n             userUpdated: { ...testUser, status: 'INACTIVE' }\n           });\n         }, 100);\n       });\n     });\n\n     describe('Performance', () => {\n       test('should handle complex queries efficiently', async () => {\n         const COMPLEX_QUERY = gql`\n           query ComplexQuery {\n             users(first: 5) {\n               edges {\n                 node {\n                   id\n                   email\n                   profile {\n                     bio\n                   }\n                   orders(first: 3) {\n                     edges {\n                       node {\n                         id\n                         total\n                         items {\n                           id\n                           product {\n                             id\n                             name\n                           }\n                         }\n                       }\n                     }\n                   }\n                 }\n               }\n             }\n           }\n         `;\n\n         const start = Date.now();\n         const result = await query({\n           query: COMPLEX_QUERY,\n           context: { user: testUser }\n         });\n         const duration = Date.now() - start;\n\n         expect(result.errors).toBeUndefined();\n         expect(duration).toBeLessThan(2000); // Should complete within 2 seconds\n       });\n\n       test('should limit query depth', async () => {\n         const DEEP_QUERY = gql`\n           query DeepQuery {\n             users {\n               edges {\n                 node {\n                   orders {\n                     edges {\n                       node {\n                         items {\n                           product {\n                             category {\n                               parent {\n                                 parent {\n                                   parent {\n                                     name\n                                   }\n                                 }\n                               }\n                             }\n                           }\n                         }\n                       }\n                     }\n                   }\n                 }\n               }\n             }\n           }\n         `;\n\n         const result = await query({\n           query: DEEP_QUERY,\n           context: { user: testUser }\n         });\n\n         expect(result.errors).toBeDefined();\n         expect(result.errors[0].message).toContain('depth');\n       });\n     });\n   });\n   ```\n\n10. **Production Setup and Deployment**\n    - Configure GraphQL for production:\n\n    **Production Configuration:**\n    ```javascript\n    // server/apollo.js\n    const { ApolloServer } = require('apollo-server-express');\n    const { makeExecutableSchema } = require('@graphql-tools/schema');\n    const { shield, rule, and, or } = require('graphql-shield');\n    const depthLimit = require('graphql-depth-limit');\n    const costAnalysis = require('graphql-query-complexity');\n\n    const typeDefs = require('../schema');\n    const resolvers = require('../resolvers');\n    const { GraphQLAuth } = require('../utils/authHelpers');\n    const { GraphQLErrorHandler } = require('../utils/errorHandling');\n    const { QueryLimiting, QueryCache } = require('../utils/queryLimiting');\n    const DataLoaders = require('../dataLoaders');\n    const { pubsub } = require('../subscriptions');\n\n    // Security rules\n    const rules = {\n      isAuthenticated: rule({ cache: 'contextual' })(\n        async (parent, args, context) => {\n          return !!context.user;\n        }\n      ),\n      isAdmin: rule({ cache: 'contextual' })(\n        async (parent, args, context) => {\n          return context.user && ['admin'].includes(context.user.role);\n        }\n      ),\n      isManagerOrAdmin: rule({ cache: 'contextual' })(\n        async (parent, args, context) => {\n          return context.user && ['admin', 'manager'].includes(context.user.role);\n        }\n      )\n    };\n\n    const permissions = shield({\n      Query: {\n        me: rules.isAuthenticated,\n        user: rules.isAuthenticated,\n        users: rules.isManagerOrAdmin,\n        orders: rules.isManagerOrAdmin\n      },\n      Mutation: {\n        createUser: rules.isAdmin,\n        updateUser: rules.isAuthenticated,\n        deleteUser: rules.isAdmin,\n        createProduct: rules.isManagerOrAdmin,\n        updateProduct: rules.isManagerOrAdmin,\n        deleteProduct: rules.isAdmin\n      },\n      Subscription: {\n        userStatusChanged: rules.isManagerOrAdmin,\n        newOrder: rules.isManagerOrAdmin,\n        lowStockAlert: rules.isManagerOrAdmin\n      }\n    }, {\n      allowExternalErrors: true,\n      fallbackError: 'Not authorized for this operation'\n    });\n\n    const createApolloServer = () => {\n      const schema = makeExecutableSchema({\n        typeDefs,\n        resolvers\n      });\n\n      return new ApolloServer({\n        schema: permissions(schema),\n        context: async ({ req, connection }) => {\n          // WebSocket connection (subscriptions)\n          if (connection) {\n            return {\n              user: connection.context.user,\n              dataLoaders: new DataLoaders(),\n              pubsub\n            };\n          }\n\n          // HTTP request\n          const user = await GraphQLAuth.getUser(req);\n          \n          return {\n            user,\n            dataLoaders: new DataLoaders(),\n            pubsub,\n            req\n          };\n        },\n        formatError: GraphQLErrorHandler.formatError,\n        validationRules: [\n          QueryLimiting.createDepthLimit(10),\n          QueryLimiting.createComplexityAnalysis(1000)\n        ],\n        plugins: [\n          QueryCache.createCachePlugin(300), // 5 minutes cache\n          {\n            requestDidStart() {\n              return {\n                willSendResponse(requestContext) {\n                  // Clear DataLoaders after each request\n                  if (requestContext.context.dataLoaders) {\n                    requestContext.context.dataLoaders.clearAll();\n                  }\n                }\n              };\n            }\n          }\n        ],\n        introspection: process.env.NODE_ENV !== 'production',\n        playground: process.env.NODE_ENV !== 'production',\n        subscriptions: {\n          onConnect: async (connectionParams, webSocket, context) => {\n            // Authenticate WebSocket connections\n            if (connectionParams.authorization) {\n              const user = await GraphQLAuth.getUser({\n                headers: { authorization: connectionParams.authorization }\n              });\n              return { user };\n            }\n            throw new Error('Missing auth token!');\n          },\n          onDisconnect: (webSocket, context) => {\n            console.log('Client disconnected');\n          }\n        }\n      });\n    };\n\n    module.exports = createApolloServer;\n    ```"
              },
              {
                "name": "/init-project",
                "description": "Initialize new project with essential structure",
                "path": "plugins/all-commands/commands/init-project.md",
                "frontmatter": {
                  "description": "Initialize new project with essential structure",
                  "category": "project-task-management",
                  "argument-hint": "Specify project name and type",
                  "allowed-tools": "Edit"
                },
                "content": "# Initialize New Project\n\nInitialize new project with essential structure\n\n## Instructions\n\n1. **Project Analysis and Setup**\n   - Parse the project type and framework from arguments: `$ARGUMENTS`\n   - If no arguments provided, analyze current directory and ask user for project type and framework\n   - Create project directory structure if needed\n   - Validate that the chosen framework is appropriate for the project type\n\n2. **Base Project Structure**\n   - Create essential directories (src/, tests/, docs/, etc.)\n   - Initialize git repository with proper .gitignore for the project type\n   - Create README.md with project description and setup instructions\n   - Set up proper file structure based on project type and framework\n\n3. **Framework-Specific Configuration**\n   - **Web/React**: Set up React with TypeScript, Vite/Next.js, ESLint, Prettier\n   - **Web/Vue**: Configure Vue 3 with TypeScript, Vite, ESLint, Prettier\n   - **Web/Angular**: Set up Angular CLI project with TypeScript and testing\n   - **API/Express**: Create Express.js server with TypeScript, middleware, and routing\n   - **API/FastAPI**: Set up FastAPI with Python, Pydantic models, and async support\n   - **Mobile/React Native**: Configure React Native with navigation and development tools\n   - **Desktop/Electron**: Set up Electron with renderer and main process structure\n   - **CLI/Node**: Create Node.js CLI with commander.js and proper packaging\n   - **Library/NPM**: Set up library with TypeScript, rollup/webpack, and publishing config\n\n4. **Development Environment Setup**\n   - Configure package manager (npm, yarn, pnpm) with proper package.json\n   - Set up TypeScript configuration with strict mode and path mapping\n   - Configure linting with ESLint and language-specific rules\n   - Set up code formatting with Prettier and pre-commit hooks\n   - Add EditorConfig for consistent coding standards\n\n5. **Testing Infrastructure**\n   - Install and configure testing framework (Jest, Vitest, Pytest, etc.)\n   - Set up test directory structure and example tests\n   - Configure code coverage reporting\n   - Add testing scripts to package.json/makefile\n\n6. **Build and Development Tools**\n   - Configure build system (Vite, webpack, rollup, etc.)\n   - Set up development server with hot reloading\n   - Configure environment variable management\n   - Add build optimization and bundling\n\n7. **CI/CD Pipeline**\n   - Create GitHub Actions workflow for testing and deployment\n   - Set up automated testing on pull requests\n   - Configure automated dependency updates with Dependabot\n   - Add status badges to README\n\n8. **Documentation and Quality**\n   - Generate comprehensive README with installation and usage instructions\n   - Create CONTRIBUTING.md with development guidelines\n   - Set up API documentation generation (JSDoc, Sphinx, etc.)\n   - Add code quality badges and shields\n\n9. **Security and Best Practices**\n   - Configure security scanning with npm audit or similar\n   - Set up dependency vulnerability checking\n   - Add security headers for web applications\n   - Configure environment-specific security settings\n\n10. **Project Validation**\n    - Verify all dependencies install correctly\n    - Run initial build to ensure configuration is working\n    - Execute test suite to validate testing setup\n    - Check linting and formatting rules are applied\n    - Validate that development server starts successfully\n    - Create initial commit with proper project structure"
              },
              {
                "name": "/initref",
                "description": "Build reference documentation by creating markdown files and updating CLAUDE.md",
                "path": "plugins/all-commands/commands/initref.md",
                "frontmatter": {
                  "description": "Build reference documentation by creating markdown files and updating CLAUDE.md",
                  "category": "context-loading-priming",
                  "allowed-tools": "Read, Write, LS, Glob"
                },
                "content": "Build the reference docs. Run /summarize on files to get summaries, don't read too many file contents to avoid burning usage. Read important files directly.\n\nCreate reference markdown files in `/ref` directory.\n\nUpdate `CLAUDE.md` file with pointers to important documentation files."
              },
              {
                "name": "/issue-to-linear-task",
                "description": "Convert GitHub issues to Linear tasks",
                "path": "plugins/all-commands/commands/issue-to-linear-task.md",
                "frontmatter": {
                  "description": "Convert GitHub issues to Linear tasks",
                  "category": "integration-sync",
                  "allowed-tools": "Bash(gh *)"
                },
                "content": "# issue-to-linear-task\n\nConvert GitHub issues to Linear tasks\n\n## System\n\nYou are a precision converter that transforms individual GitHub issues into Linear tasks. You preserve all relevant data, maintain references, and ensure proper field mapping for single issue conversions.\n\n## Instructions\n\nWhen converting a GitHub issue to Linear:\n\n1. **Fetch Issue Details**\n   ```bash\n   # Get complete issue data\n   gh issue view <issue-number> --json \\\n     number,title,body,labels,assignees,milestone,state,\\\n     createdAt,updatedAt,closedAt,comments,projectItems\n   ```\n\n2. **Extract Issue Metadata**\n   ```javascript\n   const issueData = {\n     // Core fields\n     number: issue.number,\n     title: issue.title,\n     body: issue.body,\n     state: issue.state,\n     \n     // People\n     author: issue.author.login,\n     assignees: issue.assignees.map(a => a.login),\n     \n     // Classification\n     labels: issue.labels.map(l => ({\n       name: l.name,\n       color: l.color,\n       description: l.description\n     })),\n     \n     // Timeline\n     created: issue.createdAt,\n     updated: issue.updatedAt,\n     closed: issue.closedAt,\n     \n     // References\n     url: issue.url,\n     repository: issue.repository.nameWithOwner\n   };\n   ```\n\n3. **Analyze Issue Content**\n   ```javascript\n   function analyzeIssue(issue) {\n     return {\n       hasCheckboxes: /- \\[[ x]\\]/.test(issue.body),\n       hasCodeBlocks: /```/.test(issue.body),\n       hasMentions: /@[\\w-]+/.test(issue.body),\n       hasImages: /!\\[.*\\]\\(.*\\)/.test(issue.body),\n       estimatedSize: estimateFromContent(issue),\n       suggestedPriority: inferPriority(issue)\n     };\n   }\n   ```\n\n4. **Priority Inference**\n   ```javascript\n   function inferPriority(issue) {\n     const signals = {\n       urgent: ['critical', 'urgent', 'blocker', 'security'],\n       high: ['bug', 'regression', 'important'],\n       medium: ['enhancement', 'feature'],\n       low: ['documentation', 'chore', 'nice-to-have']\n     };\n     \n     // Check labels\n     for (const [priority, keywords] of Object.entries(signals)) {\n       if (issue.labels.some(l => \n         keywords.some(k => l.name.toLowerCase().includes(k))\n       )) {\n         return priority;\n       }\n     }\n     \n     // Check title/body\n     const text = `${issue.title} ${issue.body}`.toLowerCase();\n     if (text.includes('asap') || text.includes('urgent')) {\n       return 'urgent';\n     }\n     \n     return 'medium';\n   }\n   ```\n\n5. **Transform to Linear Format**\n   ```javascript\n   const linearTask = {\n     title: issue.title,\n     description: formatDescription(issue),\n     priority: mapPriority(inferredPriority),\n     state: mapState(issue.state),\n     labels: mapLabels(issue.labels),\n     assignee: findLinearUser(issue.assignees[0]),\n     project: mapMilestoneToProject(issue.milestone),\n     \n     // Metadata\n     externalId: `gh-${issue.number}`,\n     externalUrl: issue.url,\n     \n     // Custom fields\n     customFields: {\n       githubNumber: issue.number,\n       githubAuthor: issue.author,\n       githubRepo: issue.repository\n     }\n   };\n   ```\n\n6. **Description Formatting**\n   ```markdown\n   [Original issue description with formatting preserved]\n   \n   ## GitHub Metadata\n   - **Issue:** #<number>\n   - **Author:** @<username>\n   - **Created:** <date>\n   - **Labels:** <label1>, <label2>\n   \n   ## Comments\n   [Formatted comments from GitHub]\n   \n   ---\n   *Imported from GitHub: [#<number>](<url>)*\n   ```\n\n7. **Comment Import**\n   ```javascript\n   async function importComments(issue, linearTaskId) {\n     const comments = await getIssueComments(issue.number);\n     \n     for (const comment of comments) {\n       await createLinearComment(linearTaskId, {\n         body: formatComment(comment),\n         createdAt: comment.createdAt\n       });\n     }\n   }\n   ```\n\n8. **User Mapping**\n   ```javascript\n   const userMap = {\n     // GitHub username  Linear user ID\n     'octocat': 'linear-user-123',\n     'defunkt': 'linear-user-456'\n   };\n   \n   function findLinearUser(githubUsername) {\n     return userMap[githubUsername] || null;\n   }\n   ```\n\n9. **Validation & Confirmation**\n   ```\n   Issue to Convert:\n   \n   GitHub Issue: #123 - Implement user authentication\n   Author: @octocat\n   Labels: enhancement, priority/high\n   Assignee: @defunkt\n   Milestone: v2.0\n   \n   Will create Linear task:\n   \n   Title: Implement user authentication\n   Priority: High\n   State: Todo\n   Assignee: John Doe\n   Project: Version 2.0\n   Labels: Feature, High Priority\n   \n   Proceed? [Y/n]\n   ```\n\n10. **Post-Creation Actions**\n    - Add GitHub issue reference to Linear\n    - Comment on GitHub issue with Linear link\n    - Update sync state database\n    - Close GitHub issue (if requested)\n\n## Examples\n\n### Basic Conversion\n```bash\n# Convert single issue\nclaude issue-to-linear-task 123\n\n# Convert with team specification\nclaude issue-to-linear-task 123 --team=\"backend\"\n\n# Convert and close GitHub issue\nclaude issue-to-linear-task 123 --close-github\n```\n\n### Batch Conversion\n```bash\n# Convert multiple issues\nclaude issue-to-linear-task 123,124,125\n\n# Convert from file\nclaude issue-to-linear-task --from-file=\"issues.txt\"\n```\n\n### Advanced Options\n```bash\n# Custom field mapping\nclaude issue-to-linear-task 123 \\\n  --map-assignee=\"octocat:john.doe\" \\\n  --default-priority=\"high\"\n\n# Skip comments\nclaude issue-to-linear-task 123 --skip-comments\n\n# Custom project\nclaude issue-to-linear-task 123 --project=\"Sprint 24\"\n```\n\n## Output Format\n\n```\nGitHub Issue  Linear Task Conversion\n=====================================\n\nSource Issue:\n- Number: #123\n- Title: Implement user authentication\n- URL: https://github.com/owner/repo/issues/123\n\nCreated Linear Task:\n- ID: ABC-789\n- Title: Implement user authentication\n- URL: https://linear.app/team/issue/ABC-789\n\nConversion Details:\n Title and description converted\n Priority set to: High\n Assigned to: John Doe\n Added to project: Version 2.0\n 3 labels mapped\n 5 comments imported\n References linked\n\nActions Taken:\n- Created Linear task ABC-789\n- Added comment to GitHub issue #123\n- Updated sync database\n\nTotal time: 2.3s\n```\n\n## Error Handling\n\n```\nConversion Errors:\n\n Warning: No Linear user found for @octocat\n   Task created without assignee\n\n Warning: Label \"wontfix\" has no Linear equivalent\n   Skipped this label\n\n Error: Milestone \"v3.0\" not found in Linear\n   Task created without project assignment\n   Manual assignment required\n\nRecovery Actions:\n- Partial task created: ABC-789\n- Manual review recommended\n- Sync state NOT updated\n```\n\n## Best Practices\n\n1. **Data Preservation**\n   - Keep original formatting\n   - Preserve all metadata\n   - Maintain comment threading\n\n2. **User Experience**\n   - Show preview before creation\n   - Provide rollback option\n   - Clear success/error messages\n\n3. **Integration**\n   - Update both platforms\n   - Maintain bidirectional links\n   - Log all conversions"
              },
              {
                "name": "/issue-triage",
                "description": "Triage and prioritize issues effectively",
                "path": "plugins/all-commands/commands/issue-triage.md",
                "frontmatter": {
                  "description": "Triage and prioritize issues effectively",
                  "category": "team-collaboration"
                },
                "content": "# issue-triage\n\nTriage and prioritize issues effectively\n\n## System\n\nYou are an issue triage specialist that analyzes GitHub issues and intelligently routes them to Linear with appropriate categorization, prioritization, and team assignment. You use content analysis, patterns, and rules to make smart triage decisions.\n\n## Instructions\n\nWhen triaging GitHub issues:\n\n1. **Issue Analysis**\n   ```javascript\n   async function analyzeIssue(issue) {\n     const analysis = {\n       // Content analysis\n       sentiment: analyzeSentiment(issue.title, issue.body),\n       urgency: detectUrgency(issue),\n       category: categorizeIssue(issue),\n       complexity: estimateComplexity(issue),\n       \n       // User analysis\n       authorType: classifyAuthor(issue.author),\n       authorHistory: await getAuthorHistory(issue.author),\n       \n       // Technical analysis\n       stackTrace: extractStackTrace(issue.body),\n       affectedComponents: detectComponents(issue),\n       reproducibility: assessReproducibility(issue),\n       \n       // Business impact\n       userImpact: estimateUserImpact(issue),\n       businessPriority: calculateBusinessPriority(issue)\n     };\n     \n     return analysis;\n   }\n   ```\n\n2. **Categorization Rules**\n   ```javascript\n   const categorizationRules = [\n     {\n       name: 'Security Issue',\n       patterns: [/security/i, /vulnerability/i, /CVE-/],\n       labels: ['security'],\n       priority: 1, // Urgent\n       team: 'security',\n       notify: ['security-lead']\n     },\n     {\n       name: 'Bug Report',\n       patterns: [/bug/i, /error/i, /crash/i, /broken/i],\n       hasStackTrace: true,\n       labels: ['bug'],\n       priority: (issue) => issue.sentiment < -0.5 ? 2 : 3,\n       team: 'engineering'\n     },\n     {\n       name: 'Feature Request',\n       patterns: [/feature/i, /enhancement/i, /add/i, /implement/i],\n       labels: ['enhancement'],\n       priority: 4,\n       team: 'product',\n       requiresDiscussion: true\n     },\n     {\n       name: 'Documentation',\n       patterns: [/docs/i, /documentation/i, /readme/i],\n       labels: ['documentation'],\n       priority: 4,\n       team: 'docs'\n     }\n   ];\n   ```\n\n3. **Priority Calculation**\n   ```javascript\n   function calculatePriority(issue, analysis) {\n     let score = 0;\n     \n     // Urgency indicators\n     if (analysis.urgency === 'immediate') score += 40;\n     if (containsKeywords(issue, ['urgent', 'asap', 'critical'])) score += 20;\n     if (issue.title.includes('') || issue.title.includes('!!!')) score += 15;\n     \n     // Impact assessment\n     score += analysis.userImpact * 10;\n     if (analysis.affectedComponents.includes('core')) score += 20;\n     if (analysis.reproducibility === 'always') score += 10;\n     \n     // Author influence\n     if (analysis.authorType === 'enterprise') score += 15;\n     if (analysis.authorHistory.issuesOpened > 10) score += 5;\n     \n     // Time decay\n     const ageInDays = (Date.now() - new Date(issue.createdAt)) / (1000 * 60 * 60 * 24);\n     if (ageInDays > 30) score -= 10;\n     \n     // Map score to priority\n     if (score >= 70) return 1; // Urgent\n     if (score >= 50) return 2; // High\n     if (score >= 30) return 3; // Medium\n     return 4; // Low\n   }\n   ```\n\n4. **Team Assignment**\n   ```javascript\n   async function assignTeam(issue, analysis) {\n     // Rule-based assignment\n     for (const rule of categorizationRules) {\n       if (matchesRule(issue, rule)) {\n         return rule.team;\n       }\n     }\n     \n     // Component-based assignment\n     const componentTeamMap = {\n       'auth': 'identity-team',\n       'api': 'platform-team',\n       'ui': 'frontend-team',\n       'database': 'data-team'\n     };\n     \n     for (const component of analysis.affectedComponents) {\n       if (componentTeamMap[component]) {\n         return componentTeamMap[component];\n       }\n     }\n     \n     // ML-based assignment (if available)\n     if (ML_ENABLED) {\n       return await predictTeam(issue, analysis);\n     }\n     \n     // Default assignment\n     return 'triage-team';\n   }\n   ```\n\n5. **Duplicate Detection**\n   ```javascript\n   async function findDuplicates(issue) {\n     // Semantic similarity search\n     const similar = await searchSimilarIssues(issue, {\n       threshold: 0.85,\n       limit: 5\n     });\n     \n     // Title similarity\n     const titleMatches = await searchByTitle(issue.title, {\n       fuzzy: true,\n       distance: 3\n     });\n     \n     // Stack trace matching (for bugs)\n     const stackTrace = extractStackTrace(issue.body);\n     const stackMatches = stackTrace ? \n       await searchByStackTrace(stackTrace) : [];\n     \n     return {\n       likely: similar.filter(s => s.score > 0.9),\n       possible: [...similar, ...titleMatches, ...stackMatches]\n         .filter(s => s.score > 0.7)\n         .slice(0, 5)\n     };\n   }\n   ```\n\n6. **Auto-labeling**\n   ```javascript\n   function generateLabels(issue, analysis) {\n     const labels = new Set();\n     \n     // Category labels\n     labels.add(analysis.category.toLowerCase());\n     \n     // Priority labels\n     labels.add(`priority/${getPriorityName(analysis.priority)}`);\n     \n     // Technical labels\n     if (analysis.stackTrace) labels.add('has-stack-trace');\n     if (analysis.reproducibility === 'always') labels.add('reproducible');\n     \n     // Component labels\n     analysis.affectedComponents.forEach(c => \n       labels.add(`component/${c}`)\n     );\n     \n     // Status labels\n     if (analysis.needsMoreInfo) labels.add('needs-info');\n     if (analysis.duplicate) labels.add('duplicate');\n     \n     return Array.from(labels);\n   }\n   ```\n\n7. **Triage Workflow**\n   ```javascript\n   async function triageIssue(issue) {\n     const workflow = {\n       analyzed: false,\n       triaged: false,\n       actions: []\n     };\n     \n     try {\n       // Step 1: Analyze\n       const analysis = await analyzeIssue(issue);\n       workflow.analyzed = true;\n       \n       // Step 2: Check duplicates\n       const duplicates = await findDuplicates(issue);\n       if (duplicates.likely.length > 0) {\n         return handleDuplicate(issue, duplicates.likely[0]);\n       }\n       \n       // Step 3: Determine routing\n       const triage = {\n         team: await assignTeam(issue, analysis),\n         priority: calculatePriority(issue, analysis),\n         labels: generateLabels(issue, analysis),\n         assignee: await suggestAssignee(issue, analysis)\n       };\n       \n       // Step 4: Create Linear task\n       const task = await createTriagedTask(issue, triage, analysis);\n       workflow.triaged = true;\n       \n       // Step 5: Update GitHub\n       await updateGitHubIssue(issue, triage, task);\n       \n       // Step 6: Notify stakeholders\n       await notifyStakeholders(issue, triage, analysis);\n       \n       return workflow;\n     } catch (error) {\n       workflow.error = error;\n       return workflow;\n     }\n   }\n   ```\n\n8. **Batch Triage**\n   ```javascript\n   async function batchTriage(filters) {\n     const issues = await fetchUntriaged(filters);\n     const results = {\n       total: issues.length,\n       triaged: [],\n       skipped: [],\n       failed: []\n     };\n     \n     console.log(`Found ${issues.length} issues to triage`);\n     \n     for (const issue of issues) {\n       try {\n         // Skip if already triaged\n         if (hasTriageLabel(issue)) {\n           results.skipped.push(issue);\n           continue;\n         }\n         \n         // Triage issue\n         const result = await triageIssue(issue);\n         if (result.triaged) {\n           results.triaged.push({ issue, result });\n         } else {\n           results.failed.push({ issue, error: result.error });\n         }\n         \n         // Progress update\n         updateProgress(results);\n         \n       } catch (error) {\n         results.failed.push({ issue, error });\n       }\n     }\n     \n     return results;\n   }\n   ```\n\n9. **Triage Templates**\n   ```javascript\n   const triageTemplates = {\n     bug: {\n       linearTemplate: `\n   ## Bug Report\n   \n   **Reported by:** {author}\n   **Severity:** {severity}\n   **Reproducibility:** {reproducibility}\n   \n   ### Description\n   {description}\n   \n   ### Stack Trace\n   \\`\\`\\`\n   {stackTrace}\n   \\`\\`\\`\n   \n   ### Environment\n   {environment}\n   \n   ### Steps to Reproduce\n   {reproSteps}\n       `,\n       requiredInfo: ['description', 'environment', 'reproSteps']\n     },\n     \n     feature: {\n       linearTemplate: `\n   ## Feature Request\n   \n   **Requested by:** {author}\n   **Business Value:** {businessValue}\n   \n   ### Description\n   {description}\n   \n   ### Use Cases\n   {useCases}\n   \n   ### Acceptance Criteria\n   {acceptanceCriteria}\n       `,\n       requiresApproval: true\n     }\n   };\n   ```\n\n10. **Triage Metrics**\n    ```javascript\n    function generateTriageMetrics(period = '7d') {\n      return {\n        volume: {\n          total: countIssues(period),\n          byCategory: groupByCategory(period),\n          byPriority: groupByPriority(period),\n          byTeam: groupByTeam(period)\n        },\n        \n        performance: {\n          avgTriageTime: calculateAvgTriageTime(period),\n          autoTriageRate: calculateAutoTriageRate(period),\n          accuracyRate: calculateAccuracy(period)\n        },\n        \n        patterns: {\n          commonIssues: findCommonPatterns(period),\n          peakTimes: analyzePeakTimes(period),\n          teamLoad: analyzeTeamLoad(period)\n        }\n      };\n    }\n    ```\n\n## Examples\n\n### Manual Triage\n```bash\n# Triage single issue\nclaude issue-triage 123\n\n# Triage with options\nclaude issue-triage 123 --team=\"backend\" --priority=\"high\"\n\n# Interactive triage\nclaude issue-triage 123 --interactive\n```\n\n### Automated Triage\n```bash\n# Triage all untriaged issues\nclaude issue-triage --auto\n\n# Triage with filters\nclaude issue-triage --auto --label=\"needs-triage\"\n\n# Scheduled triage\nclaude issue-triage --auto --schedule=\"*/15 * * * *\"\n```\n\n### Triage Configuration\n```bash\n# Set up triage rules\nclaude issue-triage --setup-rules\n\n# Test triage rules\nclaude issue-triage --test-rules --dry-run\n\n# Export triage config\nclaude issue-triage --export-config > triage-config.json\n```\n\n## Output Format\n\n```\nIssue Triage Report\n===================\nProcessed: 2025-01-16 11:00:00\nMode: Automatic\n\nTriage Summary:\n\nTotal Issues      : 47\nSuccessfully Triaged : 44 (93.6%)\nDuplicates Found  : 3\nManual Review     : 3\nFailed           : 0\n\nBy Category:\n- Bug Reports     : 28 (63.6%)\n- Feature Requests: 12 (27.3%)\n- Documentation   : 4 (9.1%)\n\nBy Priority:\n- Urgent (P1)     : 3  \n- High (P2)       : 12 \n- Medium (P3)     : 24 \n- Low (P4)        : 5  \n\nTeam Assignments:\n- Backend         : 18\n- Frontend        : 15\n- Security        : 3\n- Documentation   : 4\n- Triage Team     : 4\n\nNotable Issues:\n #456: Security vulnerability in auth system  Security Team (P1)\n #789: Database connection pooling errors  Backend Team (P2)\n #234: Add dark mode support  Frontend Team (P3)\n\nActions Taken:\n Created 44 Linear tasks\n Applied 156 labels\n Assigned to 12 team members\n Linked 3 duplicates\n Sent 8 notifications\n\nTriage Metrics:\n- Avg time per issue: 2.3s\n- Auto-triage accuracy: 94.2%\n- Manual intervention: 6.8%\n```\n\n## Best Practices\n\n1. **Rule Refinement**\n   - Regularly review triage accuracy\n   - Update patterns based on feedback\n   - Test rules before deployment\n\n2. **Quality Control**\n   - Sample triaged issues for review\n   - Track false positives/negatives\n   - Implement feedback loops\n\n3. **Stakeholder Communication**\n   - Notify teams of new assignments\n   - Provide triage summaries\n   - Escalate critical issues\n\n4. **Continuous Improvement**\n   - Analyze triage patterns\n   - Optimize assignment rules\n   - Implement ML when appropriate"
              },
              {
                "name": "/linear-task-to-issue",
                "description": "Convert Linear tasks to GitHub issues",
                "path": "plugins/all-commands/commands/linear-task-to-issue.md",
                "frontmatter": {
                  "description": "Convert Linear tasks to GitHub issues",
                  "category": "integration-sync",
                  "allowed-tools": "Bash(gh *), Read, Edit"
                },
                "content": "# linear-task-to-issue\n\nConvert Linear tasks to GitHub issues\n\n## System\n\nYou are a Linear-to-GitHub converter that transforms individual Linear tasks into GitHub issues. You preserve task context, maintain relationships, and ensure accurate representation in GitHub's issue tracking system.\n\n## Instructions\n\nWhen converting a Linear task to a GitHub issue:\n\n1. **Fetch Linear Task Details**\n   ```javascript\n   // Get complete task data\n   const task = await linear.issue(taskId, {\n     includeRelations: ['assignee', 'labels', 'project', 'team', 'parent', 'children'],\n     includeComments: true,\n     includeHistory: true\n   });\n   ```\n\n2. **Extract Task Components**\n   ```javascript\n   const taskData = {\n     // Core fields\n     identifier: task.identifier,\n     title: task.title,\n     description: task.description,\n     state: task.state.name,\n     priority: task.priority,\n     \n     // Relationships\n     assignee: task.assignee?.email,\n     team: task.team.key,\n     project: task.project?.name,\n     cycle: task.cycle?.name,\n     parent: task.parent?.identifier,\n     children: task.children.map(c => c.identifier),\n     \n     // Metadata\n     createdAt: task.createdAt,\n     updatedAt: task.updatedAt,\n     completedAt: task.completedAt,\n     \n     // Content\n     labels: task.labels.map(l => l.name),\n     attachments: task.attachments,\n     comments: task.comments\n   };\n   ```\n\n3. **Build GitHub Issue Body**\n   ```markdown\n   # <Task Title>\n   \n   <Task Description>\n   \n   ## Task Details\n   - **Linear ID:** [<identifier>](<linear-url>)\n   - **Priority:** <priority-emoji> <priority-name>\n   - **Status:** <status>\n   - **Team:** <team>\n   - **Project:** <project>\n   - **Cycle:** <cycle>\n   \n   ## Relationships\n   - **Parent:** <parent-link>\n   - **Sub-tasks:** \n     - [ ] <child-1>\n     - [ ] <child-2>\n   \n   ## Acceptance Criteria\n   <extracted-from-description>\n   \n   ## Attachments\n   <uploaded-attachments>\n   \n   ---\n   *Imported from Linear: [<identifier>](<url>)*\n   *Import date: <timestamp>*\n   ```\n\n4. **Priority Mapping**\n   ```javascript\n   const priorityMap = {\n     0: { label: null, emoji: '' },           // No priority\n     1: { label: 'priority/urgent', emoji: '' }, // Urgent\n     2: { label: 'priority/high', emoji: '' },   // High\n     3: { label: 'priority/medium', emoji: '' }, // Medium\n     4: { label: 'priority/low', emoji: '' }     // Low\n   };\n   ```\n\n5. **State to Label Conversion**\n   ```javascript\n   function stateToLabels(state) {\n     const stateLabels = {\n       'Backlog': ['status/backlog'],\n       'Todo': ['status/todo'],\n       'In Progress': ['status/in-progress'],\n       'In Review': ['status/review'],\n       'Done': [], // No label, will close issue\n       'Canceled': ['status/canceled']\n     };\n     \n     return stateLabels[state] || [];\n   }\n   ```\n\n6. **Create GitHub Issue**\n   ```bash\n   # Create the issue\n   gh issue create \\\n     --repo \"<owner>/<repo>\" \\\n     --title \"<title>\" \\\n     --body \"<formatted-body>\" \\\n     --label \"<labels>\" \\\n     --assignee \"<github-username>\" \\\n     --milestone \"<milestone>\"\n   ```\n\n7. **Handle Attachments**\n   ```javascript\n   async function uploadAttachments(attachments, issueNumber) {\n     const uploaded = [];\n     \n     for (const attachment of attachments) {\n       // Download from Linear\n       const file = await downloadAttachment(attachment.url);\n       \n       // Upload to GitHub\n       const uploadUrl = await getGitHubUploadUrl(issueNumber);\n       const githubUrl = await uploadFile(uploadUrl, file);\n       \n       uploaded.push({\n         original: attachment.url,\n         github: githubUrl,\n         filename: attachment.filename\n       });\n     }\n     \n     return uploaded;\n   }\n   ```\n\n8. **Import Comments**\n   ```bash\n   # Add each comment\n   for comment in comments; do\n     gh issue comment <issue-number> \\\n       --body \"**@<author>** commented on <date>:\\n\\n<comment-body>\"\n   done\n   ```\n\n9. **User Mapping**\n   ```javascript\n   const linearToGitHub = {\n     'john@example.com': 'johndoe',\n     'jane@example.com': 'janedoe'\n   };\n   \n   function mapAssignee(linearUser) {\n     return linearToGitHub[linearUser.email] || null;\n   }\n   ```\n\n10. **Post-Creation Updates**\n    ```javascript\n    // Update Linear task with GitHub reference\n    await linear.updateIssue(taskId, {\n       description: appendGitHubLink(task.description, githubIssueUrl)\n    });\n    \n    // Add GitHub issue number to Linear\n    await linear.createComment(taskId, {\n       body: `GitHub Issue created: #${issueNumber}`\n    });\n    ```\n\n## Examples\n\n### Basic Conversion\n```bash\n# Convert single task\nclaude linear-task-to-issue ABC-123\n\n# Specify target repository\nclaude linear-task-to-issue ABC-123 --repo=\"owner/repo\"\n\n# Convert and close Linear task\nclaude linear-task-to-issue ABC-123 --close-linear\n```\n\n### Advanced Options\n```bash\n# Custom label mapping\nclaude linear-task-to-issue ABC-123 \\\n  --label-prefix=\"linear/\" \\\n  --add-labels=\"imported,needs-review\"\n\n# Skip certain elements\nclaude linear-task-to-issue ABC-123 \\\n  --skip-comments \\\n  --skip-attachments\n\n# Map to specific milestone\nclaude linear-task-to-issue ABC-123 --milestone=\"v2.0\"\n```\n\n### Bulk Operations\n```bash\n# Convert multiple tasks\nclaude linear-task-to-issue ABC-123,ABC-124,ABC-125\n\n# Convert all tasks from a project\nclaude linear-task-to-issue --project=\"Sprint 23\"\n```\n\n## Output Format\n\n```\nLinear Task  GitHub Issue Conversion\n=====================================\n\nSource Task:\n- ID: ABC-123\n- Title: Implement caching layer\n- URL: https://linear.app/team/issue/ABC-123\n\nCreated GitHub Issue:\n- Number: #456\n- Title: Implement caching layer\n- URL: https://github.com/owner/repo/issues/456\n\nConversion Summary:\n Title and description converted\n Priority mapped to: priority/high\n State mapped to: status/in-progress\n Assigned to: @johndoe\n 4 labels applied\n 3 attachments uploaded\n 7 comments imported\n Cross-references created\n\nRelationships:\n- Parent task: Not applicable (no parent)\n- Sub-tasks: 2 references added to description\n\nTotal time: 5.2s\nAPI calls: 12\n```\n\n## Special Handling\n\n### Linear-Specific Features\n```javascript\n// Handle Linear's rich text\nfunction convertLinearMarkdown(content) {\n  return content\n    .replace(/\\[([^\\]]+)\\]\\(lin:\\/\\/([^)]+)\\)/g, '[$1](https://linear.app/$2)')\n    .replace(/{{([^}]+)}}/g, '`$1`') // Inline code\n    .replace(/@([a-zA-Z0-9]+)/g, '@$1'); // User mentions\n}\n\n// Handle Linear estimates\nfunction addEstimateLabel(estimate) {\n  const estimateMap = {\n    1: 'size/xs',\n    2: 'size/s', \n    3: 'size/m',\n    5: 'size/l',\n    8: 'size/xl'\n  };\n  return estimateMap[estimate] || null;\n}\n```\n\n### Error Recovery\n```\nConversion Warnings:\n\n Assignee not found in GitHub\n   Issue created without assignee\n   Added note in description\n\n 2 attachments failed to upload\n   Links preserved in description\n   Manual upload required\n\n Project \"Q1 Goals\" has no GitHub milestone\n   Issue created without milestone\n\nRecovery Options:\n1. Edit issue manually: gh issue edit 456\n2. Retry failed uploads: claude linear-task-to-issue ABC-123 --retry-attachments\n3. Create missing milestone: gh api repos/owner/repo/milestones -f title=\"Q1 Goals\"\n```\n\n## Best Practices\n\n1. **Content Fidelity**\n   - Preserve formatting and structure\n   - Maintain all metadata\n   - Keep original timestamps in comments\n\n2. **Relationship Management**\n   - Link parent/child tasks\n   - Preserve team context\n   - Maintain project associations\n\n3. **Automation Ready**\n   - Structured data in description\n   - Consistent label naming\n   - Machine-readable references"
              },
              {
                "name": "/load-llms-txt",
                "description": "READ the llms.txt file from https://raw.githubusercontent.com/ethpandaops/xatu-data/refs/heads/master/llms.txt via `curl`. Do nothing else and await further instructions.",
                "path": "plugins/all-commands/commands/load-llms-txt.md",
                "frontmatter": {
                  "description": "READ the llms.txt file from https://raw.githubusercontent.com/ethpandaops/xatu-data/refs/heads/master/llms.txt via `curl`. Do nothing else and await further instructions.",
                  "category": "documentation-changelogs"
                },
                "content": "# Load Xatu Data Context\nREAD the llms.txt file from https://raw.githubusercontent.com/ethpandaops/xatu-data/refs/heads/master/llms.txt via `curl`. Do nothing else and await further instructions."
              },
              {
                "name": "/log",
                "description": "Log work from orchestrated tasks to external project management tools like Linear, Obsidian, Jira, or GitHub Issues.",
                "path": "plugins/all-commands/commands/log.md",
                "frontmatter": {
                  "description": "Log work from orchestrated tasks to external project management tools like Linear, Obsidian, Jira, or GitHub Issues.",
                  "category": "workflow-orchestration",
                  "allowed-tools": "Read"
                },
                "content": "# Orchestration Log Command\n\nLog work from orchestrated tasks to external project management tools like Linear, Obsidian, Jira, or GitHub Issues.\n\n## Usage\n\n```\n/orchestration/log [TASK-ID] [options]\n```\n\n## Description\n\nAutomatically creates work logs in your connected project management tools or knowledge bases, transferring task completion data, time spent, and progress notes to keep external systems synchronized.\n\n## Basic Commands\n\n### Log Current Task\n```\n/orchestration/log\n```\nLogs the currently in-progress task to available tools.\n\n### Log Specific Task\n```\n/orchestration/log TASK-003\n```\nLogs a specific task's work.\n\n### Choose Destination\n```\n/orchestration/log TASK-003 --choose\n```\nManually select where to log the work.\n\n## Destination Selection\n\nWhen multiple tools are available or no obvious connection exists:\n\n```\nWhere would you like to log this work?\n\nAvailable destinations:\n1. Linear (ENG-1234 detected)\n2. Obsidian (Daily Note)\n3. Obsidian (Project: Authentication)\n4. GitHub Issue (#123)\n5. None - Skip logging\n\nChoose destination [1-5]: \n```\n\n## Obsidian Integration\n\n### Daily Note Logging\n```\n/orchestration/log --obsidian-daily\n```\nAppends to today's daily note:\n\n```markdown\n## Work Log - 15:30\n\n### TASK-003: JWT Implementation \n\n**Time Spent**: 4.5 hours (10:00 - 14:30)\n**Status**: Completed  QA\n\n**What I did:**\n- Implemented JWT token validation middleware\n- Added refresh token logic  \n- Created comprehensive test suite\n- Fixed edge case with token expiration\n\n**Code Stats:**\n- Files: 8 modified\n- Lines: +245 -23\n- Coverage: 95%\n\n**Related Tasks:**\n- Next: [[TASK-005]] - User Profile API\n- Blocked: [[TASK-007]] - Waiting for this\n\n**Commits:**\n- `abc123`: feat(auth): implement JWT validation\n- `def456`: test(auth): add validation tests\n\n#tasks/completed #project/authentication\n```\n\n### Project Note Logging\n```\n/orchestration/log --obsidian-project \"Authentication System\"\n```\nCreates or appends to project-specific note.\n\n### Custom Obsidian Location\n```\n/orchestration/log --obsidian-path \"Projects/Sprint 24/Work Log\"\n```\n\n## Linear Integration\n```\n/orchestration/log TASK-003 --linear-issue ENG-1234\n```\nCreates work log comment in Linear issue.\n\n## Smart Detection\n\nThe system detects available destinations:\n\n```\nAnalyzing task context...\n\nFound connections:\n Linear: ENG-1234 (from branch name)\n Obsidian: Project note exists\n GitHub: No issue reference\n Jira: Not connected\n\nSuggested: Linear ENG-1234\nUse suggestion? [Y/n/choose different]\n```\n\n## Work Log Formats\n\n### Obsidian Format\n```markdown\n##  Task: TASK-003 - JWT Implementation\n\n### Summary\n- **Status**:  Completed  \n- **Duration**: 4h 30m\n- **Date**: 2024-03-15\n\n### Progress Details\n- [x] Token structure design\n- [x] Validation middleware\n- [x] Refresh mechanism\n- [x] Test coverage\n\n### Technical Notes\n- Used RS256 algorithm for signing\n- Tokens expire after 15 minutes\n- Refresh tokens last 7 days\n\n### Links\n- Linear: [ENG-1234](linear://issue/ENG-1234)\n- PR: [#456](github.com/...)\n- Docs: [[JWT Implementation Guide]]\n\n### Next Actions\n- [ ] Code review feedback\n- [ ] Deploy to staging\n- [ ] Update API documentation\n\n---\n*Logged via Task Orchestration at 15:30*\n```\n\n### Linear Format\n```\nWork log comment in Linear with task details, time tracking, and progress updates.\n```\n\n## Multiple Destination Logging\n\n```\n/orchestration/log TASK-003 --multi\n\nSelect all destinations for logging:\n[x] Linear - ENG-1234\n[x] Obsidian - Daily Note\n[ ] Obsidian - Project Note\n[ ] GitHub - Create new issue\n\nPress Enter to confirm, Space to toggle\n```\n\n## Batch Operations\n\n### Daily Summary to Obsidian\n```\n/orchestration/log --daily-summary --obsidian\n\nCreates summary in daily note:\n\n## Work Summary - 2024-03-15\n\n### Completed Tasks\n- [[TASK-003]]: JWT Implementation (4.5h) \n- [[TASK-008]]: Login UI Updates (2h) \n\n### In Progress  \n- [[TASK-005]]: User Profile API (1.5h) \n\n### Total Time: 8 hours\n\n### Key Achievements\n- Authentication system core complete\n- All tests passing\n- Ready for code review\n\n### Tomorrow's Focus\n- Complete user profile endpoints\n- Start OAuth integration\n```\n\n### Weekly Report\n```\n/orchestration/log --weekly --obsidian-path \"Weekly Reviews/Week 11\"\n```\n\n## Templates\n\n### Configure Obsidian Template\n```yaml\nobsidian_template:\n  daily_note:\n    heading: \"## Work Log - {time}\"\n    include_stats: true\n    add_tags: true\n    link_tasks: true\n  \n  project_note:\n    create_if_missing: true\n    append_to_section: \"## Task Progress\"\n    include_commits: true\n```\n\n### Configure Linear Template\n```yaml\nlinear_template:\n  include_time: true\n  update_status: true\n  add_labels: [\"from-orchestration\"]\n```\n\n## Interactive Mode\n\n```\n/orchestration/log --interactive\n\nTask: TASK-003 - JWT Implementation\nStatus: Completed\nTime: 4.5 hours\n\nWhere to log? (Space to select, Enter to confirm)\n> [x] Linear (ENG-1234)\n> [x] Obsidian Daily Note\n> [ ] Obsidian Project Note\n> [ ] New GitHub Issue\n\nAdd custom notes? [y/N]: y\n> Implemented using RS256, ready for review\n\nLogging to 2 destinations...\n Linear: Comment added to ENG-1234\n Obsidian: Added to daily note\n\nView logs? [y/N]: \n```\n\n## Examples\n\n### Example 1: End of Day Logging\n```\n/orchestration/log --eod\n\nEnd of Day Summary:\n- 3 tasks worked on\n- 7.5 hours logged\n- 2 completed, 1 in progress\n\nLog to:\n1. Obsidian Daily Note (recommended)\n2. Linear (update all 3 issues)\n3. Both\n4. Skip\n\nChoice [1]: 1\n\n Daily work log created in Obsidian\n```\n\n### Example 2: Sprint Review\n```\n/orchestration/log --sprint-review --week 11\n\nGathering week 11 data...\n- 15 tasks completed\n- 3 in progress\n- 52 hours logged\n\nCreate sprint review in:\n1. Obsidian - \"Sprint Reviews/Sprint 24\"\n2. Linear - Sprint 24 cycle\n3. Both\n\nChoice [3]: 3\n\n Sprint review created in both systems\n```\n\n### Example 3: No Connection Found\n```\n/orchestration/log TASK-009\n\nNo automatic destination found for TASK-009.\n\nWhere would you like to log this?\n1. Obsidian - Daily Note\n2. Obsidian - Create Project Note\n3. Linear - Search for issue\n4. GitHub - Create new issue  \n5. Skip logging\n\nChoice: 2\n\nEnter project name: Security Audit\n Created \"Security Audit\" note with work log\n```\n\n## Configuration\n\n### Default Destinations\n```yaml\nlog_defaults:\n  no_connection: \"ask\"  # ask|obsidian-daily|skip\n  multi_connection: \"ask\"  # ask|all|first\n  \n  obsidian:\n    default_location: \"daily\"  # daily|project|custom\n    project_folder: \"Projects\"\n    daily_folder: \"Daily Notes\"\n  \n  linear:\n    auto_update_status: true\n    include_commits: true\n```\n\n## Best Practices\n\n1. **Set Preferences**: Configure default destinations\n2. **Link Early**: Connect tasks to PM tools when creating\n3. **Use Daily Notes**: Great for personal tracking\n4. **Project Notes**: Better for team collaboration\n5. **Regular Syncs**: Don't let logs pile up\n\n## Notes\n\n- Respects MCP connections and permissions\n- Obsidian logs create backlinks automatically\n- Supports multiple simultaneous destinations\n- Preserves formatting across systems\n- Can be automated with task status changes"
              },
              {
                "name": "/market-response-modeler",
                "description": "Model customer and market responses with segment analysis, behavioral prediction, and response optimization.",
                "path": "plugins/all-commands/commands/market-response-modeler.md",
                "frontmatter": {
                  "description": "Model customer and market responses with segment analysis, behavioral prediction, and response optimization.",
                  "category": "simulation-modeling",
                  "argument-hint": "Specify market response parameters"
                },
                "content": "# Market Response Modeler\n\nModel customer and market responses with segment analysis, behavioral prediction, and response optimization.\n\n## Instructions\n\nYou are tasked with creating a comprehensive market response simulation to predict customer and market reactions to business decisions. Follow this systematic approach: **$ARGUMENTS**\n\n### 1. Prerequisites Assessment\n\n**Critical Market Context Validation:**\n\n- **Market Definition**: What specific market/customer segments are you analyzing?\n- **Response Trigger**: What action/change will you be modeling responses to?\n- **Response Metrics**: How do you measure market response success?\n- **Data Availability**: What customer/market data can inform the model?\n- **Time Horizons**: What response timeframes are you analyzing?\n\n**If any context is missing, guide systematically:**\n\n```\nMissing Market Definition:\n\"I need clarity on the market scope you're analyzing:\n- Geographic Scope: Local, regional, national, or global markets?\n- Customer Segments: B2B vs B2C, demographics, firmographics, psychographics?\n- Market Size: TAM, SAM, SOM estimates and definitions?\n- Competitive Landscape: Direct competitors, substitutes, market dynamics?\n\nExamples:\n- 'Enterprise SaaS customers in North America with 100-1000 employees'\n- 'Millennial consumers in urban areas interested in sustainable products'\n- 'Small businesses in retail seeking digital transformation solutions'\"\n\nMissing Response Trigger:\n\"What specific action or change will trigger market responses?\n- Product Launches: New products, features, or service offerings\n- Pricing Changes: Price increases, decreases, or structure modifications  \n- Marketing Campaigns: Advertising, promotions, or positioning changes\n- Market Entry: Geographic expansion or new segment targeting\n- Competitive Actions: Response to competitor moves or market disruption\n\nPlease specify the exact trigger and its characteristics.\"\n\nMissing Response Metrics:\n\"How will you measure and define market response success?\n- Awareness Metrics: Brand recognition, message recall, consideration\n- Engagement Metrics: Website traffic, content interaction, social engagement\n- Conversion Metrics: Lead generation, trial signups, purchase behavior\n- Retention Metrics: Customer satisfaction, repeat purchase, loyalty\n- Market Metrics: Market share, competitive positioning, price premiums\"\n```\n\n### 2. Market Segmentation Framework\n\n**Define and analyze market segments systematically:**\n\n#### Segmentation Methodology\n- Demographic segmentation (age, income, geography, company size)\n- Behavioral segmentation (usage patterns, purchase behavior, loyalty)\n- Psychographic segmentation (values, attitudes, lifestyle, motivations)\n- Needs-based segmentation (functional, emotional, social needs)\n- Journey stage segmentation (awareness, consideration, decision, retention)\n\n#### Segment Characterization\n```\nFor each identified segment:\n\nSegment Profile:\n- Name: [descriptive segment name]\n- Size: [number of customers/prospects]\n- Value: [revenue potential and profitability]\n- Growth: [segment growth rate and trajectory]\n- Accessibility: [how easily can you reach them]\n\nBehavioral Patterns:\n- Purchase Decision Process: [how they buy]\n- Decision Timeframes: [how long decisions take]\n- Key Influencers: [who affects their decisions]\n- Information Sources: [where they research and learn]\n- Pain Points: [major problems and frustrations]\n\nResponse Characteristics:\n- Adoption Speed: [early adopter vs laggard tendencies]\n- Price Sensitivity: [elasticity and value perception]\n- Channel Preferences: [how they prefer to engage]\n- Communication Style: [messaging that resonates]\n- Risk Tolerance: [willingness to try new things]\n```\n\n#### Segment Prioritization\n- Strategic importance and alignment with business goals\n- Market size and growth potential assessment\n- Competitive positioning and advantage analysis\n- Resource requirements and capability fit\n- Response likelihood and conversion potential\n\n### 3. Response Behavior Modeling\n\n**Map customer response patterns and drivers:**\n\n#### Response Journey Mapping\n- Awareness stage responses (attention, interest, recognition)\n- Consideration stage responses (evaluation, comparison, preference)\n- Decision stage responses (purchase intent, trial, adoption)\n- Experience stage responses (satisfaction, usage, value realization)\n- Advocacy stage responses (retention, referral, expansion)\n\n#### Response Driver Analysis\n```\nResponse Driver Categories:\n\nRational Drivers:\n- Functional Benefits: [specific value propositions]\n- Economic Value: [ROI, cost savings, price advantage]\n- Risk Mitigation: [security, reliability, compliance]\n- Convenience Factors: [ease of use, accessibility, integration]\n\nEmotional Drivers:\n- Status and Prestige: [brand association, social signaling]\n- Security and Safety: [trust, stability, protection]\n- Achievement and Success: [accomplishment, progress, growth]\n- Social Connection: [belonging, community, shared values]\n\nSocial Drivers:\n- Peer Influence: [recommendations, social proof, testimonials]\n- Authority Endorsement: [expert opinions, certifications, awards]\n- Social Norms: [industry standards, best practices, trends]\n- Network Effects: [ecosystem value, platform benefits]\n```\n\n#### Response Intensity Modeling\n- Response magnitude estimation (small, medium, large impact)\n- Response timing prediction (immediate, short-term, long-term)\n- Response duration forecasting (temporary, sustained, permanent)\n- Response quality assessment (superficial vs deep engagement)\n\n### 4. Competitive Response Integration\n\n**Model competitive dynamics and market interactions:**\n\n#### Competitive Landscape Analysis\n- Direct competitor identification and positioning\n- Substitute product and service threats\n- Competitive advantage assessment and sustainability\n- Market share dynamics and trend analysis\n- Competitive response history and patterns\n\n#### Competitive Response Prediction\n```\nCompetitor Response Framework:\n\nFor each major competitor:\n- Response Likelihood: [probability of competitive reaction]\n- Response Speed: [how quickly they typically react]\n- Response Magnitude: [scale and intensity of typical responses]\n- Response Type: [pricing, product, marketing, or strategic responses]\n- Response Effectiveness: [historical success of their responses]\n\nMarket Dynamic Effects:\n- Price War Potential: [likelihood and impact of price competition]\n- Innovation Arms Race: [feature/capability competition dynamics]\n- Market Share Battles: [customer acquisition and retention competition]\n- Channel Conflicts: [distribution and partnership competition]\n```\n\n#### Market Equilibrium Modeling\n- New equilibrium state prediction after market responses\n- Time to equilibrium estimation and transition dynamics\n- Stability analysis of new market configurations\n- Secondary effect propagation through market ecosystem\n\n### 5. Response Simulation Engine\n\n**Create dynamic response modeling capabilities:**\n\n#### Scenario Development\n- Base case scenarios with expected market conditions\n- Optimistic scenarios with favorable response assumptions\n- Pessimistic scenarios with adverse market reactions\n- Disruption scenarios with unexpected market changes\n- Competitive scenarios with various competitor responses\n\n#### Response Wave Modeling\n```\nResponse Timeline Framework:\n\nImmediate Response (0-30 days):\n- Early adopter engagement and initial reactions\n- Social media buzz and viral potential assessment\n- Competitor monitoring and immediate countermoves\n- Channel partner responses and support\n\nShort-term Response (1-6 months):\n- Mainstream market adoption patterns\n- Word-of-mouth effects and referral dynamics\n- Competitive response implementation and market adjustment\n- Initial customer experience and satisfaction feedback\n\nMedium-term Response (6-18 months):\n- Market penetration and segment adoption rates\n- Competitive equilibrium establishment\n- Customer lifecycle progression and retention patterns\n- Market share stabilization and positioning\n\nLong-term Response (18+ months):\n- Market maturation and saturation effects\n- Sustained competitive advantage realization\n- Customer loyalty and advocacy development\n- Secondary market effects and ecosystem impacts\n```\n\n#### Monte Carlo Simulation\n- Probability distribution modeling for key response variables\n- Random scenario generation and statistical analysis\n- Confidence interval calculation for response predictions\n- Sensitivity analysis for critical assumption variables\n\n### 6. Response Prediction Algorithms\n\n**Apply sophisticated prediction methodologies:**\n\n#### Statistical Modeling\n- Regression analysis for response prediction based on historical data\n- Time series analysis for trend and seasonality effects\n- Cluster analysis for segment-specific response patterns\n- Survival analysis for customer lifecycle and churn prediction\n\n#### Machine Learning Applications\n- Classification models for response category prediction\n- Neural networks for complex pattern recognition\n- Ensemble methods for improved prediction accuracy\n- Natural language processing for sentiment and feedback analysis\n\n#### Expert System Integration\n```\nExpert Knowledge Integration:\n\nDomain Expert Input:\n- Industry experience and pattern recognition\n- Market timing and seasonal factor insights\n- Customer psychology and behavioral understanding\n- Competitive intelligence and strategic assessment\n\nStakeholder Validation:\n- Sales team customer insight and relationship intelligence\n- Marketing team campaign response and engagement data\n- Customer success team satisfaction and retention insights\n- Product team usage pattern and feature adoption data\n\nExternal Validation:\n- Industry analyst reports and market research\n- Customer advisory board feedback and validation\n- Beta testing and pilot program results\n- Academic research and behavioral economics insights\n```\n\n### 7. Response Optimization Framework\n\n**Generate actionable response enhancement strategies:**\n\n#### Message Optimization\n- Segment-specific messaging and value proposition refinement\n- Channel-specific communication strategy development\n- Timing optimization for maximum response impact\n- Creative testing and iterative improvement frameworks\n\n#### Offering Optimization\n- Product feature prioritization based on response drivers\n- Pricing strategy optimization for segment preferences\n- Package and bundle configuration for maximum appeal\n- Service level and support optimization for satisfaction\n\n#### Channel Optimization\n- Distribution channel selection and partner optimization\n- Digital touchpoint optimization and user experience\n- Sales process optimization for conversion improvement\n- Customer service optimization for satisfaction and retention\n\n### 8. Validation and Calibration\n\n**Ensure model accuracy and reliability:**\n\n#### Historical Validation\n- Back-testing model predictions against known market responses\n- Correlation analysis between predicted and actual outcomes\n- Model accuracy assessment across different market conditions\n- Bias detection and correction for systematic errors\n\n#### Real-time Calibration\n```\nOngoing Model Improvement:\n\nData Integration:\n- Real-time response monitoring and measurement\n- Customer feedback and satisfaction tracking\n- Market research and survey data integration\n- Competitive intelligence and market dynamics monitoring\n\nModel Updates:\n- Parameter adjustment based on actual response data\n- Algorithm refinement for improved prediction accuracy\n- Segment definition updates based on observed behavior\n- Response driver prioritization based on performance\n\nValidation Metrics:\n- Prediction Accuracy: [percentage of correct predictions]\n- Response Timing Accuracy: [actual vs predicted timing]\n- Magnitude Accuracy: [actual vs predicted response size]\n- Direction Accuracy: [positive vs negative response prediction]\n```\n\n### 9. Decision Integration and Recommendations\n\n**Transform insights into actionable market strategies:**\n\n#### Strategic Recommendations\n```\nMarket Response Strategy Framework:\n\n## Market Response Analysis: [Initiative Name]\n\n### Executive Summary\n- Primary Market Opportunity: [key findings]\n- Expected Response Magnitude: [quantified predictions]\n- Optimal Timing: [recommended launch/implementation timing]\n- Resource Requirements: [budget and capability needs]\n- Success Probability: [confidence level and rationale]\n\n### Segment-Specific Strategies\n\n#### High-Response Segments:\n- Segment: [name and characteristics]\n- Expected Response: [prediction with confidence interval]\n- Recommended Approach: [specific strategy and tactics]\n- Success Metrics: [KPIs and measurement approach]\n- Timeline: [implementation and measurement schedule]\n\n#### Medium-Response Segments:\n[Similar structure for each segment]\n\n#### Low-Response Segments:\n[Evaluation of whether to target or deprioritize]\n\n### Response Enhancement Strategies\n- Message Optimization: [specific improvements recommended]\n- Offering Refinement: [product/service adjustments]\n- Channel Optimization: [distribution and engagement improvements]\n- Timing Optimization: [launch and communication scheduling]\n\n### Risk Mitigation\n- Competitive Response Contingencies: [specific preparations]\n- Market Resistance Scenarios: [alternative approaches]\n- Resource Constraint Adaptations: [scaled approaches]\n- Timeline Delay Preparations: [backup plans]\n\n### Success Measurement Framework\n- Leading Indicators: [early signals of response success]\n- Lagging Indicators: [ultimate success metrics]\n- Monitoring Schedule: [measurement frequency and responsibility]\n- Decision Points: [when to adjust strategy based on results]\n```\n\n### 10. Continuous Learning and Improvement\n\n**Establish ongoing model enhancement:**\n\n#### Response Learning System\n- Systematic capture of actual market responses\n- Pattern recognition for improved future predictions\n- Segment behavior evolution tracking and adaptation\n- Competitive response pattern learning and anticipation\n\n#### Model Evolution Framework\n- Regular model performance assessment and improvement\n- New data source integration and enhanced prediction\n- Algorithm updates and methodology advancement\n- User feedback integration and workflow optimization\n\n## Usage Examples\n\n```bash\n# Product launch response modeling\n/simulation:market-response-modeler Predict customer response to new AI-powered CRM feature across SMB and enterprise segments\n\n# Pricing strategy validation  \n/simulation:market-response-modeler Model market response to 20% price increase for premium service tier\n\n# Marketing campaign optimization\n/simulation:market-response-modeler Simulate customer segment responses to sustainability-focused brand messaging campaign\n\n# Competitive response preparation\n/simulation:market-response-modeler Analyze market response if competitor launches competing product at 30% lower price\n```\n\n## Quality Indicators\n\n- **Green**: Comprehensive segment analysis, validated response drivers, historical calibration data\n- **Yellow**: Good segment coverage, reasonable response assumptions, some validation data\n- **Red**: Limited segmentation, unvalidated assumptions, no historical benchmark data\n\n## Common Pitfalls to Avoid\n\n- Segment oversimplification: Using too broad or generic customer categories\n- Response uniformity: Assuming all segments respond similarly\n- Timing blindness: Not accounting for response timing variations\n- Competitive ignorance: Ignoring competitive response dynamics\n- Static thinking: Not modeling response evolution over time\n- Data bias: Relying on unrepresentative historical data\n\nTransform market uncertainty into strategic advantage through sophisticated response prediction and optimization."
              },
              {
                "name": "/memory-spring-cleaning",
                "description": "Clean and organize project memory",
                "path": "plugins/all-commands/commands/memory-spring-cleaning.md",
                "frontmatter": {
                  "description": "Clean and organize project memory",
                  "category": "team-collaboration"
                },
                "content": "# Memory Spring Cleaning\n\nClean and organize project memory\n\n## Instructions\n\n1. **Get Overview**\n   - List all CLAUDE.md and CLAUDE.local.md files in the project hierarchy\n\n2. **Iterative Review**\n   - Process each file systematically, starting with the root `CLAUDE.md` file\n   - Load the current content\n   - Compare documented patterns against actual implementation\n   - Identify outdated, incorrect, or missing information\n\n3. **Update and Refactor**\n   - For each memory file:\n     - Verify all technical claims against the current codebase\n     - Remove obsolete information\n     - Consolidate duplicate entries\n     - Ensure information is in the most appropriate file\n   - When information belongs to a specific subcomponent, ensure it's placed correctly:\n     - UI-specific patterns  `apps/myproject-ui/CLAUDE.md`\n     - API conventions  `apps/myproject-api/CLAUDE.md`\n     - Infrastructure details  `cdk/CLAUDE.md` or `infrastructure/CLAUDE.md`\n\n4. **Focus on Quality**\n   - Prioritize clarity, accuracy, and relevance\n   - Remove any information that no longer serves the project\n   - Ensure each piece of information is in its most logical location\n\n## Credit\n\nThis command is based on the work of Thomas Landgraf: https://thomaslandgraf.substack.com/p/claude-codes-memory-working-with"
              },
              {
                "name": "/mermaid",
                "description": "Create entity relationship diagrams using Mermaid from SQL/database files",
                "path": "plugins/all-commands/commands/mermaid.md",
                "frontmatter": {
                  "description": "Create entity relationship diagrams using Mermaid from SQL/database files",
                  "category": "miscellaneous",
                  "argument-hint": "<source-path> [output-path]",
                  "allowed-tools": "Read, Write, Bash, Glob"
                },
                "content": "Create Mermaid entity relationship diagrams (ERD) from SQL migration files or database schemas.\n\n## Process:\n\n1. **Parse Arguments**:\n   - First argument: Source path (SQL files or directory)\n   - Second argument: Output path (optional, defaults to `docs/erd.md`)\n\n2. **Find SQL/Schema Files**:\n   - Look for SQL files: `*.sql`, `*.ddl`\n   - Check common locations if no path provided:\n     - `migrations/`, `db/migrations/`, `schema/`\n     - `database/`, `sql/`\n   - Support multiple database formats:\n     - PostgreSQL, MySQL, SQLite\n     - Migration files (Rails, Django, Flyway, etc.)\n\n3. **Extract Schema Information**:\n   - Parse CREATE TABLE statements\n   - Extract table names, columns, and data types\n   - Identify primary keys, foreign keys, and relationships\n   - Handle indexes and constraints\n\n4. **Generate Mermaid ERD**:\n   ```mermaid\n   erDiagram\n     CUSTOMER ||--o{ ORDER : places\n     ORDER ||--|{ LINE-ITEM : contains\n     CUSTOMER {\n       string name\n       string email\n       int id PK\n     }\n   ```\n\n5. **Validate Diagram**:\n   - If mermaid-cli is available: `npx -p @mermaid-js/mermaid-cli mmdc -i output.md -o temp.svg`\n   - Alternative validation: Check syntax manually\n   - Clean up temporary files\n\n6. **Output Options**:\n   - Single file with all entities\n   - Separate files per schema/database\n   - Include relationship descriptions\n\n## Example Usage:\n- `/mermaid migrations/` - Create ERD from all SQL files in migrations\n- `/mermaid schema.sql docs/database-erd.md` - Create ERD from specific file\n- `/mermaid \"db/**/*.sql\" erd/` - Create ERDs for all SQL files\n\nSource: $ARGUMENTS"
              },
              {
                "name": "/migrate-to-typescript",
                "description": "Migrate JavaScript project to TypeScript",
                "path": "plugins/all-commands/commands/migrate-to-typescript.md",
                "frontmatter": {
                  "description": "Migrate JavaScript project to TypeScript",
                  "category": "typescript-migration"
                },
                "content": "# Migrate to TypeScript\n\nMigrate JavaScript project to TypeScript\n\n## Instructions\n\n1. **Project Analysis and Migration Planning**\n   - Analyze current JavaScript codebase structure and complexity\n   - Identify external dependencies and their TypeScript support\n   - Assess project size and determine migration approach (gradual vs. complete)\n   - Review existing build system and bundling configuration\n   - Create migration timeline and phased approach plan\n\n2. **TypeScript Installation and Configuration**\n   - Install TypeScript and related dependencies (@types packages)\n   - Create comprehensive tsconfig.json with strict configuration\n   - Configure path mapping and module resolution\n   - Set up incremental compilation and build optimization\n   - Configure TypeScript for different environments (development, production, testing)\n\n3. **Build System Integration**\n   - Update build tools to support TypeScript compilation\n   - Configure webpack, Vite, or other bundlers for TypeScript\n   - Set up development server with TypeScript support\n   - Configure hot module replacement for TypeScript files\n   - Update build scripts and package.json configurations\n\n4. **File Migration Strategy**\n   - Start with configuration files and utility modules\n   - Migrate from least to most complex modules\n   - Rename .js files to .ts/.tsx incrementally\n   - Update import/export statements to use TypeScript syntax\n   - Handle mixed JavaScript/TypeScript codebase during transition\n\n5. **Type Definitions and Interfaces**\n   - Create comprehensive type definitions for project-specific types\n   - Install @types packages for external dependencies\n   - Define interfaces for API responses and data structures\n   - Create custom type declarations for untyped libraries\n   - Set up shared types and interfaces across modules\n\n6. **Code Transformation and Type Annotation**\n   - Add explicit type annotations to function parameters and return types\n   - Convert JavaScript classes to TypeScript with proper typing\n   - Transform object literals to typed interfaces\n   - Add generic types for reusable components and functions\n   - Handle complex types like union types, mapped types, and conditional types\n\n7. **Error Resolution and Type Safety**\n   - Resolve TypeScript compiler errors systematically\n   - Fix type mismatches and undefined behavior\n   - Handle null and undefined values with strict null checks\n   - Configure ESLint rules for TypeScript best practices\n   - Set up type checking in CI/CD pipeline\n\n8. **Testing and Validation**\n   - Update test files to TypeScript\n   - Configure testing framework for TypeScript support\n   - Add type testing with tools like tsd or @typescript-eslint\n   - Validate type safety in test suites\n   - Set up type coverage reporting\n\n9. **Developer Experience Enhancement**\n   - Configure IDE/editor for optimal TypeScript support\n   - Set up IntelliSense and auto-completion\n   - Configure debugging for TypeScript source maps\n   - Set up type-aware linting and formatting\n   - Create TypeScript-specific code snippets and templates\n\n10. **Documentation and Team Onboarding**\n    - Update project documentation for TypeScript setup\n    - Create TypeScript coding standards and best practices guide\n    - Document migration decisions and type system architecture\n    - Set up type documentation generation\n    - Train team members on TypeScript development workflows\n    - Create troubleshooting guide for common TypeScript issues"
              },
              {
                "name": "/migration-assistant",
                "description": "Assist with system migration planning",
                "path": "plugins/all-commands/commands/migration-assistant.md",
                "frontmatter": {
                  "description": "Assist with system migration planning",
                  "category": "team-collaboration",
                  "argument-hint": "Valid actions: plan, analyze, migrate, verify, rollback"
                },
                "content": "# Migration Assistant\n\nAssist with system migration planning\n\n## Instructions\n\n1. **Check Prerequisites**\n   - Verify GitHub CLI (`gh`) is installed and authenticated\n   - Check if Linear MCP server is connected\n   - Ensure sufficient permissions in both systems\n   - Confirm backup storage is available\n\n2. **Parse Migration Parameters**\n   - Extract action and options from: **$ARGUMENTS**\n   - Valid actions: plan, analyze, migrate, verify, rollback\n   - Determine source and target systems\n   - Set migration scope and filters\n\n3. **Initialize Migration Environment**\n   - Create migration workspace directory\n   - Set up logging and audit trails\n   - Initialize checkpoint system\n   - Prepare rollback mechanisms\n\n4. **Execute Migration Action**\n   Based on the selected action:\n\n   ### Plan Action\n   - Analyze source system structure\n   - Map fields between systems\n   - Identify potential conflicts\n   - Generate migration strategy\n   - Estimate time and resources\n   - Create detailed migration plan\n\n   ### Analyze Action\n   - Count items to migrate\n   - Check data compatibility\n   - Identify custom fields\n   - Assess attachment sizes\n   - Calculate migration impact\n   - Generate pre-migration report\n\n   ### Migrate Action\n   - Create full backup of source data\n   - Execute migration in batches\n   - Transform data between formats\n   - Preserve relationships\n   - Handle attachments and media\n   - Create progress checkpoints\n   - Log all operations\n\n   ### Verify Action\n   - Compare source and target data\n   - Validate all items migrated\n   - Check relationship integrity\n   - Verify custom field mappings\n   - Test cross-references\n   - Generate verification report\n\n   ### Rollback Action\n   - Load rollback checkpoint\n   - Restore original state\n   - Clean up partial migrations\n   - Verify rollback completion\n   - Generate rollback report\n\n## Usage\n```bash\nmigration-assistant [action] [options]\n```\n\n## Actions\n- `plan` - Create migration plan\n- `analyze` - Assess migration scope\n- `migrate` - Execute migration\n- `verify` - Validate migration results\n- `rollback` - Revert migration\n\n## Options\n- `--source <system>` - Source system (github/linear)\n- `--target <system>` - Target system (github/linear)\n- `--scope <items>` - Items to migrate (all/issues/prs/projects)\n- `--dry-run` - Simulate migration\n- `--parallel <n>` - Parallel processing threads\n- `--checkpoint` - Enable checkpoint recovery\n- `--mapping-file <path>` - Custom field mappings\n- `--preserve-ids` - Maintain reference IDs\n- `--archive-source` - Archive after migration\n\n## Examples\n```bash\n# Plan GitHub to Linear migration\nmigration-assistant plan --source github --target linear\n\n# Analyze migration scope\nmigration-assistant analyze --scope all\n\n# Dry run migration\nmigration-assistant migrate --dry-run --parallel 4\n\n# Execute migration with checkpoints\nmigration-assistant migrate --checkpoint --backup\n\n# Verify migration completeness\nmigration-assistant verify --deep-check\n\n# Rollback if needed\nmigration-assistant rollback --transaction-id 12345\n```\n\n## Migration Phases\n\n### 1. Planning Phase\n- Inventory source data\n- Map data structures\n- Identify incompatibilities\n- Estimate migration time\n- Generate migration plan\n\n### 2. Preparation Phase\n- Create full backup\n- Validate permissions\n- Set up target structure\n- Configure mappings\n- Test connectivity\n\n### 3. Migration Phase\n- Transfer data in batches\n- Maintain relationships\n- Preserve metadata\n- Handle attachments\n- Update references\n\n### 4. Verification Phase\n- Compare record counts\n- Validate data integrity\n- Check relationships\n- Verify attachments\n- Test functionality\n\n### 5. Finalization Phase\n- Update documentation\n- Redirect webhooks\n- Archive source data\n- Generate reports\n- Train users\n\n## Data Mapping Configuration\n```yaml\nmappings:\n  github_to_linear:\n    issue:\n      title: title\n      body: description\n      state: status\n      labels: labels\n      milestone: cycle\n      assignees: assignees\n    \n    custom_fields:\n      - source: \"custom.priority\"\n        target: \"priority\"\n        transform: \"map_priority\"\n      \n    relationships:\n      - type: \"parent-child\"\n        source: \"depends_on\"\n        target: \"parent\"\n    \n  linear_to_github:\n    issue:\n      title: title\n      description: body\n      status: state\n      priority: labels\n      cycle: milestone\n```\n\n## Migration Safety Features\n\n### Pre-Migration Checks\n- Storage capacity verification\n- API rate limit assessment\n- Permission validation\n- Dependency checking\n- Conflict detection\n\n### During Migration\n- Transaction logging\n- Progress tracking\n- Error recovery\n- Checkpoint creation\n- Performance monitoring\n\n### Post-Migration\n- Data verification\n- Integrity checking\n- Performance testing\n- User acceptance\n- Rollback readiness\n\n## Checkpoint Recovery\n```json\n{\n  \"checkpoint\": {\n    \"id\": \"mig-20240120-1430\",\n    \"progress\": {\n      \"total_items\": 5000,\n      \"completed\": 3750,\n      \"failed\": 12,\n      \"pending\": 1238\n    },\n    \"state\": {\n      \"last_processed_id\": \"issue-3750\",\n      \"batch_number\": 75,\n      \"error_count\": 12\n    }\n  }\n}\n```\n\n## Rollback Capabilities\n- Point-in-time recovery\n- Selective rollback\n- Relationship preservation\n- Audit trail maintenance\n- Zero data loss guarantee\n\n## Performance Optimization\n- Batch processing\n- Parallel transfers\n- API call optimization\n- Caching strategies\n- Resource monitoring\n\n## Migration Reports\n- Executive summary\n- Detailed item mapping\n- Error analysis\n- Performance metrics\n- Recommendation list\n\n## Common Migration Scenarios\n\n### GitHub Issues  Linear\n1. Map GitHub labels to Linear labels/projects\n2. Convert milestones to cycles\n3. Preserve issue numbers as references\n4. Migrate comments with user mapping\n5. Handle attachments and images\n\n### Linear  GitHub Issues\n1. Map Linear statuses to GitHub states\n2. Convert cycles to milestones\n3. Preserve Linear IDs in issue body\n4. Map Linear projects to labels\n5. Handle custom fields\n\n## Required MCP Servers\n- mcp-server-github\n- mcp-server-linear\n\n## Error Handling\n- Automatic retry with backoff\n- Detailed error logging\n- Partial failure recovery\n- Manual intervention points\n- Comprehensive error reports\n\n## Best Practices\n- Always run analysis first\n- Use dry-run for testing\n- Migrate in phases for large datasets\n- Maintain communication with team\n- Keep source data until verified\n- Document custom mappings\n- Test rollback procedures\n\n## Compliance & Audit\n- Full audit trail\n- Data retention compliance\n- Privacy preservation\n- Change authorization\n- Migration certification\n\n## Notes\nThis command creates a complete migration package including backups, logs, and documentation. The migration can be resumed from checkpoints in case of interruption. All migrations are reversible within the retention period."
              },
              {
                "name": "/migration-guide",
                "description": "Create migration guides for updates",
                "path": "plugins/all-commands/commands/migration-guide.md",
                "frontmatter": {
                  "description": "Create migration guides for updates",
                  "category": "documentation-changelogs",
                  "argument-hint": "1. **Migration Scope Analysis**"
                },
                "content": "# Migration Guide Generator Command\n\nCreate migration guides for updates\n\n## Instructions\n\nFollow this systematic approach to create migration guides: **$ARGUMENTS**\n\n1. **Migration Scope Analysis**\n   - Identify what is being migrated (framework, library, architecture, etc.)\n   - Determine source and target versions or technologies\n   - Assess the scale and complexity of the migration\n   - Identify affected systems and components\n\n2. **Impact Assessment**\n   - Analyze breaking changes between versions\n   - Identify deprecated features and APIs\n   - Review new features and capabilities\n   - Assess compatibility requirements and constraints\n   - Evaluate performance and security implications\n\n3. **Prerequisites and Requirements**\n   - Document system requirements for the target version\n   - List required tools and dependencies\n   - Specify minimum versions and compatibility requirements\n   - Identify necessary skills and team preparation\n   - Outline infrastructure and environment needs\n\n4. **Pre-Migration Preparation**\n   - Create comprehensive backup strategies\n   - Set up development and testing environments\n   - Document current system state and configurations\n   - Establish rollback procedures and contingency plans\n   - Create migration timeline and milestones\n\n5. **Step-by-Step Migration Process**\n   \n   **Example for Framework Upgrade:**\n   ```markdown\n   ## Step 1: Environment Setup\n   1. Update development environment\n   2. Install new framework version\n   3. Update build tools and dependencies\n   4. Configure IDE and tooling\n   \n   ## Step 2: Dependencies Update\n   1. Update package.json/requirements.txt\n   2. Resolve dependency conflicts\n   3. Update related libraries\n   4. Test compatibility\n   \n   ## Step 3: Code Migration\n   1. Update import statements\n   2. Replace deprecated APIs\n   3. Update configuration files\n   4. Modify build scripts\n   ```\n\n6. **Breaking Changes Documentation**\n   - List all breaking changes with examples\n   - Provide before/after code comparisons\n   - Explain the rationale behind changes\n   - Offer alternative approaches for removed features\n\n   **Example Breaking Change:**\n   ```markdown\n   ### Removed: `oldMethod()`\n   **Before:**\n   ```javascript\n   const result = library.oldMethod(param1, param2);\n   ```\n   \n   **After:**\n   ```javascript\n   const result = library.newMethod({ \n     param1: param1, \n     param2: param2 \n   });\n   ```\n   \n   **Rationale:** Improved type safety and extensibility\n   ```\n\n7. **Configuration Changes**\n   - Document configuration file updates\n   - Explain new configuration options\n   - Provide configuration migration scripts\n   - Show environment-specific configurations\n\n8. **Database Migration (if applicable)**\n   - Create database schema migration scripts\n   - Document data transformation requirements\n   - Provide backup and restore procedures\n   - Test migration with sample data\n   - Plan for zero-downtime migrations\n\n9. **Testing Strategy**\n   - Update existing tests for new APIs\n   - Create migration-specific test cases\n   - Implement integration and E2E tests\n   - Set up performance and load testing\n   - Document test scenarios and expected outcomes\n\n10. **Performance Considerations**\n    - Document performance changes and optimizations\n    - Provide benchmarking guidelines\n    - Identify potential performance regressions\n    - Suggest monitoring and alerting updates\n    - Include memory and resource usage changes\n\n11. **Security Updates**\n    - Document security improvements and changes\n    - Update authentication and authorization code\n    - Review and update security configurations\n    - Update dependency security scanning\n    - Document new security best practices\n\n12. **Deployment Strategy**\n    - Plan phased rollout approach\n    - Create deployment scripts and automation\n    - Set up monitoring and health checks\n    - Plan for blue-green or canary deployments\n    - Document rollback procedures\n\n13. **Common Issues and Troubleshooting**\n    \n    ```markdown\n    ## Common Migration Issues\n    \n    ### Issue: Import/Module Resolution Errors\n    **Symptoms:** Cannot resolve module 'old-package'\n    **Solution:** \n    1. Update import statements to new package names\n    2. Check package.json for correct dependencies\n    3. Clear node_modules and reinstall\n    \n    ### Issue: API Method Not Found\n    **Symptoms:** TypeError: oldMethod is not a function\n    **Solution:** Replace with new API as documented in step 3\n    ```\n\n14. **Team Communication and Training**\n    - Create team training materials\n    - Schedule knowledge sharing sessions\n    - Document new development workflows\n    - Update coding standards and guidelines\n    - Create quick reference guides\n\n15. **Tools and Automation**\n    - Provide migration scripts and utilities\n    - Create code transformation tools (codemods)\n    - Set up automated compatibility checks\n    - Implement CI/CD pipeline updates\n    - Create validation and verification tools\n\n16. **Timeline and Milestones**\n    \n    ```markdown\n    ## Migration Timeline\n    \n    ### Phase 1: Preparation (Week 1-2)\n    - [ ] Environment setup\n    - [ ] Team training\n    - [ ] Development environment migration\n    \n    ### Phase 2: Development (Week 3-6)\n    - [ ] Core application migration\n    - [ ] Testing and validation\n    - [ ] Performance optimization\n    \n    ### Phase 3: Deployment (Week 7-8)\n    - [ ] Staging deployment\n    - [ ] Production deployment\n    - [ ] Monitoring and support\n    ```\n\n17. **Risk Mitigation**\n    - Identify potential migration risks\n    - Create contingency plans for each risk\n    - Document escalation procedures\n    - Plan for extended timeline scenarios\n    - Prepare communication for stakeholders\n\n18. **Post-Migration Tasks**\n    - Clean up deprecated code and configurations\n    - Update documentation and README files\n    - Review and optimize new implementation\n    - Conduct post-migration retrospective\n    - Plan for future maintenance and updates\n\n19. **Validation and Testing**\n    - Create comprehensive test plans\n    - Document acceptance criteria\n    - Set up automated regression testing\n    - Plan user acceptance testing\n    - Implement monitoring and alerting\n\n20. **Documentation Updates**\n    - Update API documentation\n    - Revise development guides\n    - Update deployment documentation\n    - Create troubleshooting guides\n    - Update team onboarding materials\n\n**Migration Types and Specific Considerations:**\n\n**Framework Migration (React 17  18):**\n- Update React and ReactDOM imports\n- Replace deprecated lifecycle methods\n- Update testing library methods\n- Handle concurrent features and Suspense\n\n**Database Migration (MySQL  PostgreSQL):**\n- Convert SQL syntax differences\n- Update data types and constraints\n- Migrate stored procedures to functions\n- Update ORM configurations\n\n**Cloud Migration (On-premise  AWS):**\n- Containerize applications\n- Update CI/CD pipelines\n- Configure cloud services\n- Implement infrastructure as code\n\n**Architecture Migration (Monolith  Microservices):**\n- Identify service boundaries\n- Implement inter-service communication\n- Set up service discovery\n- Plan data consistency strategies\n\nRemember to:\n- Test thoroughly in non-production environments first\n- Communicate progress and issues regularly\n- Document lessons learned for future migrations\n- Keep the migration guide updated based on real experiences"
              },
              {
                "name": "/milestone-tracker",
                "description": "Track and monitor project milestone progress",
                "path": "plugins/all-commands/commands/milestone-tracker.md",
                "frontmatter": {
                  "description": "Track and monitor project milestone progress",
                  "category": "project-task-management"
                },
                "content": "# Milestone Tracker\n\nTrack and monitor project milestone progress\n\n## Instructions\n\n1. **Check Available Tools**\n   - Verify Linear MCP server connection\n   - Check GitHub CLI availability\n   - Test git repository access\n   - Ensure required permissions\n\n2. **Gather Milestone Data**\n   - Query Linear for project milestones and roadmap items\n   - Fetch GitHub milestones and their associated issues\n   - Analyze git tags for historical release patterns\n   - Review project documentation for roadmap information\n   - Collect all active and upcoming milestones\n\n3. **Analyze Milestone Progress**\n   For each milestone:\n   - Count completed vs. total tasks\n   - Calculate percentage complete\n   - Measure velocity trends\n   - Identify blocking issues\n   - Track time remaining\n\n4. **Perform Predictive Analysis**\n   - Calculate burn-down rate from historical data\n   - Project completion dates based on velocity\n   - Factor in team capacity and holidays\n   - Identify critical path items\n   - Assess confidence levels for predictions\n\n5. **Risk Assessment**\n   Evaluate each milestone for:\n   - Schedule risk (falling behind)\n   - Scope risk (expanding requirements)\n   - Resource risk (team availability)\n   - Dependency risk (blocked by others)\n   - Technical risk (unknowns)\n\n6. **Generate Milestone Report**\n   Create comprehensive report showing:\n   - Milestone timeline visualization\n   - Progress indicators for each milestone\n   - Predicted completion dates with confidence\n   - Risk heat map\n   - Recommended actions for at-risk items\n\n7. **Track Dependencies**\n   - Map inter-milestone dependencies\n   - Identify cross-team dependencies\n   - Highlight critical path\n   - Show dependency impact on schedule\n\n8. **Provide Recommendations**\n   Based on analysis:\n   - Suggest scope adjustments\n   - Recommend resource reallocation\n   - Propose timeline changes\n   - Identify quick wins\n   - Highlight blockers needing attention\n\n## Prerequisites\n- Git repository access\n- Linear MCP server connection (preferred)\n- GitHub milestones or project boards\n- Historical velocity data\n\n## Command Flow\n\n### 1. Milestone Discovery\n```\n1. Check Linear for project milestones/roadmap items\n2. Scan GitHub for milestone definitions\n3. Analyze git tags for release history\n4. Review README/docs for project roadmap\n5. Ask user for additional context if needed\n```\n\n### 2. Comprehensive Milestone Analysis\n\n#### Data Collection Sources\n```\nLinear/Project Management:\n- Milestone definitions and due dates\n- Associated tasks and dependencies\n- Team assignments and capacity\n- Progress percentages\n- Blocker status\n\nGitHub:\n- Milestone issue tracking\n- PR associations\n- Release tags and dates\n- Branch protection rules\n\nGit History:\n- Commit velocity trends\n- Feature branch lifecycle\n- Release cadence patterns\n- Contributor availability\n```\n\n### 3. Milestone Status Report\n\n```markdown\n# Milestone Tracking Report - [Project Name]\nGenerated: [Date]\n\n## Executive Summary\n- Total Milestones: [Count]\n- On Track: [Count] ([%])\n- At Risk: [Count] ([%])\n- Blocked: [Count] ([%])\n- Completed: [Count] ([%])\n\n## Milestone Dashboard\n\n###  Current Sprint Milestone: [Name]\n**Target Date**: [Date] (in [X] days)\n**Confidence Level**: [High/Medium/Low]\n\nProgress:  80% Complete\n\n**Key Deliverables**:\n-  User Authentication System\n-  Database Schema Migration  \n-  API Integration (75%)\n-  Documentation Update (0%)\n-  Performance Testing (Blocked)\n\n**Health Indicators**:\n- Velocity Trend:  Declining (-15%)\n- Burn Rate:  Behind Schedule\n- Risk Level: Medium\n- Team Capacity: 85% allocated\n\n###  Upcoming Milestones\n\n#### Q1 2024: Beta Release\n**Target**: March 15, 2024\n**Status**:  At Risk\n\nTimeline:\n```\nJan  60%\nFeb  0%\nMar  0%\n```\n\n**Dependencies**:\n- Alpha Testing Complete \n- Security Audit (In Progress)\n- Marketing Website (Not Started)\n\n**Predicted Completion**: March 22 (+7 days)\n**Confidence**: 65%\n\n#### Q2 2024: Public Launch\n**Target**: June 1, 2024\n**Status**:  On Track\n\nKey Milestones Path:\n1. Beta Release  2. User Feedback Integration  3. Production Deployment\n\n**Critical Path Items**:\n- Infrastructure Setup (Start: April 1)\n- Load Testing (Duration: 2 weeks)\n- Security Certification (Lead time: 4 weeks)\n```\n\n### 4. Predictive Analytics\n\n```markdown\n## Completion Predictions\n\n### Machine Learning Model Predictions\nBased on historical data and current velocity:\n\n**Beta Release Probability**:\n- On Time (Mar 15): 35%\n- 1 Week Delay: 45%\n- 2+ Week Delay: 20%\n\n**Factors Influencing Prediction**:\n1. Current velocity 15% below plan\n2. 2 critical dependencies unresolved\n3. Team member on leave next week\n4. Historical milestone success rate: 72%\n\n### Monte Carlo Simulation Results\nRunning 1000 simulations based on task estimates:\n\n```\nCompletion Date Distribution:\nMar 10-15:  20%\nMar 16-22:  40%\nMar 23-31:  30%\nApril+   :  10%\n\nP50 Date: March 19\nP90 Date: March 28\n```\n\n### Risk-Adjusted Timeline\nRecommended buffer: +5 days\nConfident delivery date: March 20\n```\n\n### 5. Dependency Tracking\n\n```markdown\n## Milestone Dependencies\n\n### Critical Path Analysis\n```mermaid\ngantt\n    title Critical Path to Beta Release\n    dateFormat  YYYY-MM-DD\n    section Backend\n    API Development    :done,    api, 2024-01-01, 30d\n    Database Migration :active,  db,  2024-02-01, 14d\n    Security Audit     :         sec, after db, 21d\n    section Frontend  \n    UI Components      :done,    ui,  2024-01-15, 21d\n    Integration        :active,  int, after ui, 14d\n    User Testing       :         ut,  after int, 7d\n    section Deploy\n    Infrastructure     :         inf, 2024-03-01, 7d\n    Beta Deployment    :crit,    dep, after sec ut inf, 3d\n```\n\n### Dependency Risk Matrix\n| Dependency | Impact | Likelihood | Mitigation |\n|------------|--------|------------|------------|\n| Security Audit Delay | High | Medium | Start process early |\n| API Rate Limits | Medium | Low | Implement caching |\n| Team Availability | High | High | Cross-training needed |\n```\n\n### 6. Early Warning System\n\n```markdown\n##  Milestone Alerts\n\n### Immediate Attention Required\n\n**1. Performance Testing Blocked**\n- Blocker: Test environment not available\n- Impact: Beta release at risk\n- Days blocked: 3\n- Recommended action: Escalate to DevOps\n\n**2. Documentation Lagging**\n- Progress: 0% (Should be 40%)\n- Impact: User onboarding compromised\n- Resource needed: Technical writer\n- Recommended action: Reassign team member\n\n### Trending Concerns\n\n**Velocity Decline**\n- 3-week trend: -15%\n- Projected impact: 1-week delay\n- Root cause: Increased bug fixes\n- Recommendation: Add bug buffer to estimates\n\n**Scope Creep Detected**\n- New features added: 3\n- Impact on timeline: +5 days\n- Recommendation: Defer to next milestone\n```\n\n### 7. Actionable Recommendations\n\n```markdown\n## Recommended Actions\n\n### This Week\n1. **Unblock Performance Testing**\n   - Owner: [Name]\n   - Action: Provision test environment\n   - Due: Friday EOD\n\n2. **Documentation Sprint**\n   - Owner: [Team]\n   - Action: Dedicate 2 days to docs\n   - Target: 50% completion\n\n### Next Sprint\n1. **Velocity Recovery Plan**\n   - Reduce scope by 20%\n   - Focus on critical path items\n   - Defer nice-to-have features\n\n2. **Risk Mitigation**\n   - Add 5-day buffer to timeline\n   - Daily standups for blocked items\n   - Escalation path defined\n\n### Process Improvements\n1. Set up automated milestone tracking\n2. Weekly milestone health reviews\n3. Dependency check before sprint planning\n```\n\n## Error Handling\n\n### No Milestone Data\n```\n\"No milestones found in Linear or GitHub.\n\nTo set up milestone tracking:\n1. Define milestones in Linear/GitHub\n2. Associate tasks with milestones\n3. Set target completion dates\n\nWould you like me to:\n- Help create milestone structure?\n- Import from project documentation?\n- Set up basic milestones?\"\n```\n\n### Insufficient Historical Data\n```\n\"Limited historical data for predictions.\n\nAvailable data: [X] weeks\nRecommended: 12+ weeks for accurate predictions\n\nCurrent analysis based on:\n- Available velocity data\n- Industry benchmarks\n- Task complexity estimates\n\nConfidence level: Low-Medium\"\n```\n\n## Interactive Features\n\n### What-If Analysis\n```\n\"Explore scenario planning:\n\n1. What if we add 2 more developers?\n    Completion date: -5 days\n    Confidence: +15%\n\n2. What if we cut scope by 20%?\n    Completion date: -8 days\n    Risk level: Low\n\n3. What if key developer is unavailable?\n    Completion date: +12 days\n    Risk level: Critical\"\n```\n\n### Milestone Optimization\n```\n\"Optimization opportunities detected:\n\n1. **Parallelize Tasks**\n   - Tasks A & B can run simultaneously\n   - Time saved: 1 week\n\n2. **Resource Reallocation**\n   - Move developer from Task C to Critical Path\n   - Impact: 3 days earlier completion\n\n3. **Scope Adjustment**\n   - Defer features X, Y to next milestone\n   - Impact: Meet original deadline\"\n```\n\n## Export & Integration Options\n\n1. **Gantt Chart Export** (Mermaid/PNG/PDF)\n2. **Executive Dashboard** (HTML/PowerBI)\n3. **Status Updates** (Slack/Email/Confluence)\n4. **Risk Register** (Excel/Linear/Jira)\n5. **Calendar Integration** (ICS/Google/Outlook)\n\n## Automation Capabilities\n\n```\n\"Set up automated milestone monitoring:\n\n1. Daily health checks at 9 AM\n2. Weekly trend reports on Fridays\n3. Alert when milestones go off-track\n4. Slack notifications for blockers\n5. Auto-create Linear tasks for risks\n\nConfigure automation? [Y/N]\"\n```\n\n## Best Practices\n\n1. **Update Frequently**: Daily progress updates improve predictions\n2. **Track Dependencies**: Most delays come from dependencies\n3. **Buffer Realistically**: Use historical data for buffers\n4. **Communicate Early**: Flag risks as soon as detected\n5. **Focus on Critical Path**: Not all tasks equally impact timeline\n6. **Learn from History**: Analyze past milestone performance"
              },
              {
                "name": "/modernize-deps",
                "description": "Update and modernize project dependencies",
                "path": "plugins/all-commands/commands/modernize-deps.md",
                "frontmatter": {
                  "description": "Update and modernize project dependencies",
                  "category": "project-setup",
                  "argument-hint": "1. **Dependency Audit**",
                  "allowed-tools": "Bash(npm *), Read"
                },
                "content": "# Modernize Dependencies Command\n\nUpdate and modernize project dependencies\n\n## Instructions\n\nFollow this approach to modernize dependencies: **$ARGUMENTS**\n\n1. **Dependency Audit**\n   ```bash\n   # Check outdated packages\n   npm outdated\n   pip list --outdated\n   composer outdated\n   \n   # Security audit\n   npm audit\n   pip-audit\n   ```\n\n2. **Update Strategy**\n   - Start with patch updates (1.2.3  1.2.4)\n   - Then minor updates (1.2.3  1.3.0)\n   - Finally major updates (1.2.3  2.0.0)\n   - Test thoroughly between each step\n\n3. **Automated Updates**\n   ```bash\n   # Safe updates\n   npm update\n   pip install -U package-name\n   \n   # Interactive updates\n   npx npm-check-updates -i\n   ```\n\n4. **Breaking Changes Review**\n   - Read changelogs and migration guides\n   - Identify deprecated APIs\n   - Plan code changes needed\n   - Update tests and documentation\n\n5. **Testing and Validation**\n   ```bash\n   npm test\n   npm run build\n   npm run lint\n   ```\n\n6. **Documentation Updates**\n   - Update README.md\n   - Revise installation instructions\n   - Update API documentation\n   - Note breaking changes\n\nRemember to update dependencies incrementally, test thoroughly, and maintain backward compatibility where possible."
              },
              {
                "name": "/move",
                "description": "Move tasks between status folders following the task management protocol.",
                "path": "plugins/all-commands/commands/move.md",
                "frontmatter": {
                  "description": "Move tasks between status folders following the task management protocol.",
                  "category": "workflow-orchestration"
                },
                "content": "# Task Move Command\n\nMove tasks between status folders following the task management protocol.\n\n## Usage\n\n```\n/task-move TASK-ID new-status [reason]\n```\n\n## Description\n\nUpdates task status by moving files between status folders and updating tracking information. Follows all protocol rules including validation and audit trails.\n\n## Basic Commands\n\n### Start Working on a Task\n```\n/task-move TASK-001 in_progress\n```\nMoves from todos  in_progress\n\n### Complete Implementation\n```\n/task-move TASK-001 qa \"Implementation complete, ready for testing\"\n```\nMoves from in_progress  qa\n\n### Task Passed QA\n```\n/task-move TASK-001 completed \"All tests passed\"\n```\nMoves from qa  completed\n\n### Block a Task\n```\n/task-move TASK-004 on_hold \"Waiting for TASK-001 API completion\"\n```\nMoves to on_hold with reason\n\n### Unblock a Task\n```\n/task-move TASK-004 todos \"Dependencies resolved\"\n```\nMoves from on_hold  todos\n\n### Failed QA\n```\n/task-move TASK-001 in_progress \"Failed integration test - fixing null pointer\"\n```\nMoves from qa  in_progress\n\n## Bulk Operations\n\n### Move Multiple Tasks\n```\n/task-move TASK-001,TASK-002,TASK-003 in_progress\n```\n\n### Move by Filter\n```\n/task-move --filter \"priority:high status:todos\" in_progress\n```\n\n### Move with Pattern\n```\n/task-move TASK-00* qa \"Batch testing ready\"\n```\n\n## Validation Rules\n\nThe command enforces:\n1. **Valid Transitions**: Only allowed status changes\n2. **One Task Per Agent**: Warns if agent has task in_progress\n3. **Dependency Check**: Warns if dependencies not met\n4. **File Existence**: Verifies task exists before moving\n\n## Status Transition Map\n\n```\ntodos  in_progress  qa  completed\n                                \n   on_hold \n                  \n                todos/in_progress\n```\n\n## Options\n\n### Force Move\n```\n/task-move TASK-001 completed --force\n```\nBypasses validation (use with caution)\n\n### Dry Run\n```\n/task-move TASK-001 qa --dry-run\n```\nShows what would happen without executing\n\n### With Assignment\n```\n/task-move TASK-001 in_progress --assign dev-frontend\n```\nAssigns task to specific agent\n\n### With Time Estimate\n```\n/task-move TASK-001 in_progress --estimate 4h\n```\nUpdates time estimate when starting\n\n## Error Handling\n\n### Task Not Found\n```\nError: TASK-999 not found in any status folder\nSuggestion: Use /task-status to see available tasks\n```\n\n### Invalid Transition\n```\nError: Cannot move from 'completed' to 'todos'\nValid transitions from completed: None (terminal state)\n```\n\n### Agent Conflict\n```\nWarning: dev-frontend already has TASK-002 in progress\nContinue? (y/n)\n```\n\n### Dependency Block\n```\nWarning: TASK-004 depends on TASK-001 (currently in_progress)\nMoving to on_hold instead? (y/n)\n```\n\n## Automation\n\n### Auto-move on Completion\n```\n/task-move TASK-001 --auto-progress\n```\nAutomatically moves to next status when conditions met\n\n### Scheduled Moves\n```\n/task-move TASK-005 in_progress --at \"tomorrow 9am\"\n```\n\n### Conditional Moves\n```\n/task-move TASK-007 qa --when \"TASK-006 completed\"\n```\n\n## Examples\n\n### Example 1: Developer Workflow\n```\n# Start work\n/task-move TASK-001 in_progress\n\n# Complete and test\n/task-move TASK-001 qa \"Implementation done, tests passing\"\n\n# After review\n/task-move TASK-001 completed \"Code review approved\"\n```\n\n### Example 2: Handling Blocks\n```\n# Block due to dependency\n/task-move TASK-004 on_hold \"Waiting for auth API from TASK-001\"\n\n# Unblock when ready\n/task-move TASK-004 todos \"TASK-001 now in QA, API available\"\n```\n\n### Example 3: QA Workflow\n```\n# QA picks up task\n/task-move TASK-001 qa --assign qa-engineer\n\n# Found issues\n/task-move TASK-001 in_progress \"Bug: handling empty responses\"\n\n# Fixed and retesting\n/task-move TASK-001 qa \"Bug fixed, ready for retest\"\n```\n\n## Status Update Details\n\nEach move updates:\n1. **File Location**: Physical file movement\n2. **Status Tracker**: TASK-STATUS-TRACKER.yaml entry\n3. **Task Metadata**: Status field in task file\n4. **Execution Tracker**: Overall progress metrics\n\n## Best Practices\n\n1. **Always Provide Reasons**: Especially for blocks and failures\n2. **Check Dependencies**: Before moving to in_progress\n3. **Update Estimates**: When starting work\n4. **Clear Block Reasons**: Help others understand delays\n\n## Integration\n\n- Use after `/task-status` to see available tasks\n- Updates reflected in `/task-report`\n- Triggers notifications if configured\n- Logs all moves for audit trail\n\n## Notes\n\n- Moves are atomic - either fully complete or rolled back\n- Status history is permanent and cannot be edited\n- Timestamp uses current time in ISO-8601 format\n- Agent name is automatically detected from context"
              },
              {
                "name": "/optimize-build",
                "description": "Optimize build processes and speed",
                "path": "plugins/all-commands/commands/optimize-build.md",
                "frontmatter": {
                  "description": "Optimize build processes and speed",
                  "category": "performance-optimization",
                  "argument-hint": "1. **Build System Analysis**"
                },
                "content": "# Optimize Build Command\n\nOptimize build processes and speed\n\n## Instructions\n\nFollow this systematic approach to optimize build performance: **$ARGUMENTS**\n\n1. **Build System Analysis**\n   - Identify the build system in use (Webpack, Vite, Rollup, Gradle, Maven, Cargo, etc.)\n   - Review build configuration files and settings\n   - Analyze current build times and output sizes\n   - Map the complete build pipeline and dependencies\n\n2. **Performance Baseline**\n   - Measure current build times for different scenarios:\n     - Clean build (from scratch)\n     - Incremental build (with cache)\n     - Development vs production builds\n   - Document bundle sizes and asset sizes\n   - Identify the slowest parts of the build process\n\n3. **Dependency Optimization**\n   - Analyze build dependencies and their impact\n   - Remove unused dependencies from build process\n   - Update build tools to latest stable versions\n   - Consider alternative, faster build tools\n\n4. **Caching Strategy**\n   - Enable and optimize build caching\n   - Configure persistent cache for CI/CD\n   - Set up shared cache for team development\n   - Implement incremental compilation where possible\n\n5. **Bundle Analysis**\n   - Analyze bundle composition and sizes\n   - Identify large dependencies and duplicates\n   - Use bundle analyzers specific to your build tool\n   - Look for opportunities to split bundles\n\n6. **Code Splitting and Lazy Loading**\n   - Implement dynamic imports and code splitting\n   - Set up route-based splitting for SPAs\n   - Configure vendor chunk separation\n   - Optimize chunk sizes and loading strategies\n\n7. **Asset Optimization**\n   - Optimize images (compression, format conversion, lazy loading)\n   - Minify CSS and JavaScript\n   - Configure tree shaking to remove dead code\n   - Implement asset compression (gzip, brotli)\n\n8. **Development Build Optimization**\n   - Enable fast refresh/hot reloading\n   - Use development-specific optimizations\n   - Configure source maps for better debugging\n   - Optimize development server settings\n\n9. **Production Build Optimization**\n   - Enable all production optimizations\n   - Configure dead code elimination\n   - Set up proper minification and compression\n   - Optimize for smaller bundle sizes\n\n10. **Parallel Processing**\n    - Enable parallel processing where supported\n    - Configure worker threads for build tasks\n    - Optimize for multi-core systems\n    - Use parallel compilation for TypeScript/Babel\n\n11. **File System Optimization**\n    - Optimize file watching and polling\n    - Configure proper include/exclude patterns\n    - Use efficient file loaders and processors\n    - Minimize file I/O operations\n\n12. **CI/CD Build Optimization**\n    - Optimize CI build environments and resources\n    - Implement proper caching strategies for CI\n    - Use build matrices efficiently\n    - Configure parallel CI jobs where beneficial\n\n13. **Memory Usage Optimization**\n    - Monitor and optimize memory usage during builds\n    - Configure heap sizes for build tools\n    - Identify and fix memory leaks in build process\n    - Use memory-efficient compilation options\n\n14. **Output Optimization**\n    - Configure compression and encoding\n    - Optimize file naming and hashing strategies\n    - Set up proper asset manifests\n    - Implement efficient asset serving\n\n15. **Monitoring and Profiling**\n    - Set up build time monitoring\n    - Use build profiling tools to identify bottlenecks\n    - Track bundle size changes over time\n    - Monitor build performance regressions\n\n16. **Tool-Specific Optimizations**\n    \n    **For Webpack:**\n    - Configure optimization.splitChunks\n    - Use thread-loader for parallel processing\n    - Enable optimization.usedExports for tree shaking\n    - Configure resolve.modules and resolve.extensions\n\n    **For Vite:**\n    - Configure build.rollupOptions\n    - Use esbuild for faster transpilation\n    - Optimize dependency pre-bundling\n    - Configure build.chunkSizeWarningLimit\n\n    **For TypeScript:**\n    - Use incremental compilation\n    - Configure project references\n    - Optimize tsconfig.json settings\n    - Use skipLibCheck when appropriate\n\n17. **Environment-Specific Configuration**\n    - Separate development and production configurations\n    - Use environment variables for build optimization\n    - Configure feature flags for conditional builds\n    - Optimize for target environments\n\n18. **Testing Build Optimizations**\n    - Test build outputs for correctness\n    - Verify all optimizations work in target environments\n    - Check for any breaking changes from optimizations\n    - Measure and document performance improvements\n\n19. **Documentation and Maintenance**\n    - Document all optimization changes and their impact\n    - Create build performance monitoring dashboard\n    - Set up alerts for build performance regressions\n    - Regular review and updates of build configuration\n\nFocus on the optimizations that provide the biggest impact for your specific project and team workflow. Always measure before and after to quantify improvements."
              },
              {
                "name": "/optimize-bundle-size",
                "description": "Reduce and optimize bundle sizes",
                "path": "plugins/all-commands/commands/optimize-bundle-size.md",
                "frontmatter": {
                  "description": "Reduce and optimize bundle sizes",
                  "category": "performance-optimization",
                  "allowed-tools": "Bash(npm *)"
                },
                "content": "# Optimize Bundle Size\n\nReduce and optimize bundle sizes\n\n## Instructions\n\n1. **Bundle Analysis and Assessment**\n   - Analyze current bundle size and composition using webpack-bundle-analyzer or similar\n   - Identify large dependencies and unused code\n   - Assess current build configuration and optimization settings\n   - Create baseline measurements for optimization tracking\n   - Document current performance metrics and loading times\n\n2. **Build Tool Configuration**\n   - Configure build tool optimization settings:\n\n   **Webpack Configuration:**\n   ```javascript\n   // webpack.config.js\n   const path = require('path');\n   const { BundleAnalyzerPlugin } = require('webpack-bundle-analyzer');\n\n   module.exports = {\n     mode: 'production',\n     optimization: {\n       splitChunks: {\n         chunks: 'all',\n         cacheGroups: {\n           vendor: {\n             test: /[\\\\/]node_modules[\\\\/]/,\n             name: 'vendors',\n             priority: 10,\n             reuseExistingChunk: true,\n           },\n           common: {\n             name: 'common',\n             minChunks: 2,\n             priority: 5,\n             reuseExistingChunk: true,\n           },\n         },\n       },\n       usedExports: true,\n       sideEffects: false,\n     },\n     plugins: [\n       new BundleAnalyzerPlugin({\n         analyzerMode: 'static',\n         openAnalyzer: false,\n       }),\n     ],\n   };\n   ```\n\n   **Vite Configuration:**\n   ```javascript\n   // vite.config.js\n   import { defineConfig } from 'vite';\n   import { visualizer } from 'rollup-plugin-visualizer';\n\n   export default defineConfig({\n     build: {\n       rollupOptions: {\n         output: {\n           manualChunks: {\n             vendor: ['react', 'react-dom'],\n             ui: ['@mui/material', '@emotion/react'],\n           },\n         },\n       },\n     },\n     plugins: [\n       visualizer({\n         filename: 'dist/stats.html',\n         open: true,\n         gzipSize: true,\n       }),\n     ],\n   });\n   ```\n\n3. **Code Splitting and Lazy Loading**\n   - Implement route-based code splitting:\n\n   **React Route Splitting:**\n   ```javascript\n   import { lazy, Suspense } from 'react';\n   import { Routes, Route } from 'react-router-dom';\n\n   const Home = lazy(() => import('./pages/Home'));\n   const Dashboard = lazy(() => import('./pages/Dashboard'));\n   const Profile = lazy(() => import('./pages/Profile'));\n\n   function App() {\n     return (\n       <Suspense fallback={<div>Loading...</div>}>\n         <Routes>\n           <Route path=\"/\" element={<Home />} />\n           <Route path=\"/dashboard\" element={<Dashboard />} />\n           <Route path=\"/profile\" element={<Profile />} />\n         </Routes>\n       </Suspense>\n     );\n   }\n   ```\n\n   **Dynamic Imports:**\n   ```javascript\n   // Lazy load heavy components\n   const HeavyComponent = lazy(() => \n     import('./HeavyComponent').then(module => ({\n       default: module.HeavyComponent\n     }))\n   );\n\n   // Conditional loading\n   async function loadAnalytics() {\n     if (process.env.NODE_ENV === 'production') {\n       const { analytics } = await import('./analytics');\n       return analytics;\n     }\n   }\n   ```\n\n4. **Tree Shaking and Dead Code Elimination**\n   - Configure tree shaking for optimal dead code elimination:\n\n   **Package.json Configuration:**\n   ```json\n   {\n     \"sideEffects\": false,\n     \"exports\": {\n       \".\": {\n         \"import\": \"./dist/index.esm.js\",\n         \"require\": \"./dist/index.cjs.js\"\n       }\n     }\n   }\n   ```\n\n   **Import Optimization:**\n   ```javascript\n   // Instead of importing entire library\n   // import * as _ from 'lodash';\n\n   // Import only what you need\n   import debounce from 'lodash/debounce';\n   import throttle from 'lodash/throttle';\n\n   // Use babel-plugin-import for automatic optimization\n   // .babelrc\n   {\n     \"plugins\": [\n       [\"import\", {\n         \"libraryName\": \"lodash\",\n         \"libraryDirectory\": \"\",\n         \"camel2DashComponentName\": false\n       }, \"lodash\"]\n     ]\n   }\n   ```\n\n5. **Dependency Optimization**\n   - Analyze and optimize dependencies:\n\n   **Package Analysis Script:**\n   ```javascript\n   // scripts/analyze-deps.js\n   const fs = require('fs');\n   const path = require('path');\n\n   function analyzeDependencies() {\n     const packageJson = JSON.parse(\n       fs.readFileSync('package.json', 'utf8')\n     );\n     \n     const deps = {\n       ...packageJson.dependencies,\n       ...packageJson.devDependencies\n     };\n\n     console.log('Large dependencies to review:');\n     Object.keys(deps).forEach(dep => {\n       try {\n         const depPath = require.resolve(dep);\n         const stats = fs.statSync(depPath);\n         if (stats.size > 100000) { // > 100KB\n           console.log(`${dep}: ${(stats.size / 1024).toFixed(2)}KB`);\n         }\n       } catch (e) {\n         // Skip if can't resolve\n       }\n     });\n   }\n\n   analyzeDependencies();\n   ```\n\n6. **Asset Optimization**\n   - Optimize static assets and media files:\n\n   **Image Optimization:**\n   ```javascript\n   // webpack.config.js\n   module.exports = {\n     module: {\n       rules: [\n         {\n           test: /\\.(png|jpe?g|gif|svg)$/i,\n           use: [\n             {\n               loader: 'file-loader',\n               options: {\n                 outputPath: 'images',\n               },\n             },\n             {\n               loader: 'image-webpack-loader',\n               options: {\n                 mozjpeg: { progressive: true, quality: 80 },\n                 optipng: { enabled: false },\n                 pngquant: { quality: [0.6, 0.8] },\n                 gifsicle: { interlaced: false },\n               },\n             },\n           ],\n         },\n       ],\n     },\n   };\n   ```\n\n7. **Module Federation and Micro-frontends**\n   - Implement module federation for large applications:\n\n   **Module Federation Setup:**\n   ```javascript\n   // webpack.config.js\n   const ModuleFederationPlugin = require('@module-federation/webpack');\n\n   module.exports = {\n     plugins: [\n       new ModuleFederationPlugin({\n         name: 'host',\n         remotes: {\n           mfe1: 'mfe1@http://localhost:3001/remoteEntry.js',\n           mfe2: 'mfe2@http://localhost:3002/remoteEntry.js',\n         },\n         shared: {\n           react: { singleton: true },\n           'react-dom': { singleton: true },\n         },\n       }),\n     ],\n   };\n   ```\n\n8. **Performance Monitoring and Measurement**\n   - Set up bundle size monitoring:\n\n   **Bundle Size Monitoring:**\n   ```javascript\n   // scripts/bundle-monitor.js\n   const fs = require('fs');\n   const path = require('path');\n   const gzipSize = require('gzip-size');\n\n   async function measureBundleSize() {\n     const distPath = path.join(__dirname, '../dist');\n     const files = fs.readdirSync(distPath);\n     \n     for (const file of files) {\n       if (file.endsWith('.js')) {\n         const filePath = path.join(distPath, file);\n         const content = fs.readFileSync(filePath);\n         const originalSize = content.length;\n         const compressed = await gzipSize(content);\n         \n         console.log(`${file}:`);\n         console.log(`  Original: ${(originalSize / 1024).toFixed(2)}KB`);\n         console.log(`  Gzipped: ${(compressed / 1024).toFixed(2)}KB`);\n       }\n     }\n   }\n\n   measureBundleSize();\n   ```\n\n9. **Progressive Loading Strategies**\n   - Implement progressive loading and resource hints:\n\n   **Resource Hints:**\n   ```html\n   <!-- Preload critical resources -->\n   <link rel=\"preload\" href=\"/fonts/main.woff2\" as=\"font\" type=\"font/woff2\" crossorigin>\n   <link rel=\"preload\" href=\"/critical.css\" as=\"style\">\n\n   <!-- Prefetch non-critical resources -->\n   <link rel=\"prefetch\" href=\"/dashboard.js\">\n   <link rel=\"prefetch\" href=\"/profile.js\">\n\n   <!-- DNS prefetch for external domains -->\n   <link rel=\"dns-prefetch\" href=\"//api.example.com\">\n   ```\n\n   **Intersection Observer for Lazy Loading:**\n   ```javascript\n   // utils/lazyLoad.js\n   export function lazyLoadComponent(importFunc) {\n     return lazy(() => {\n       return new Promise(resolve => {\n         const observer = new IntersectionObserver((entries) => {\n           entries.forEach(entry => {\n             if (entry.isIntersecting) {\n               importFunc().then(resolve);\n               observer.disconnect();\n             }\n           });\n         });\n         \n         // Observe a trigger element\n         const trigger = document.getElementById('lazy-trigger');\n         if (trigger) observer.observe(trigger);\n       });\n     });\n   }\n   ```\n\n10. **Validation and Continuous Monitoring**\n    - Set up automated bundle size validation:\n\n    **CI/CD Bundle Size Check:**\n    ```yaml\n    # .github/workflows/bundle-size.yml\n    name: Bundle Size Check\n    on: [pull_request]\n\n    jobs:\n      bundle-size:\n        runs-on: ubuntu-latest\n        steps:\n          - uses: actions/checkout@v2\n          - name: Setup Node\n            uses: actions/setup-node@v2\n            with:\n              node-version: '16'\n          - name: Install dependencies\n            run: npm ci\n          - name: Build bundle\n            run: npm run build\n          - name: Check bundle size\n            run: |\n              npm run bundle:analyze\n              node scripts/bundle-size-check.js\n    ```\n\n    **Bundle Size Threshold Check:**\n    ```javascript\n    // scripts/bundle-size-check.js\n    const fs = require('fs');\n    const path = require('path');\n\n    const THRESHOLDS = {\n      'main.js': 250 * 1024, // 250KB\n      'vendor.js': 500 * 1024, // 500KB\n    };\n\n    function checkBundleSize() {\n      const distPath = path.join(__dirname, '../dist');\n      const files = fs.readdirSync(distPath);\n      let failed = false;\n\n      files.forEach(file => {\n        if (file.endsWith('.js') && THRESHOLDS[file]) {\n          const filePath = path.join(distPath, file);\n          const size = fs.statSync(filePath).size;\n          \n          if (size > THRESHOLDS[file]) {\n            console.error(` ${file} exceeds threshold: ${size} > ${THRESHOLDS[file]}`);\n            failed = true;\n          } else {\n            console.log(` ${file} within threshold: ${size}`);\n          }\n        }\n      });\n\n      if (failed) {\n        process.exit(1);\n      }\n    }\n\n    checkBundleSize();\n    ```"
              },
              {
                "name": "/optimize-database-performance",
                "description": "Optimize database queries and performance",
                "path": "plugins/all-commands/commands/optimize-database-performance.md",
                "frontmatter": {
                  "description": "Optimize database queries and performance",
                  "category": "database-operations",
                  "allowed-tools": "Read, Write"
                },
                "content": "# Optimize Database Performance\n\nOptimize database queries and performance\n\n## Instructions\n\n1. **Database Performance Analysis**\n   - Analyze current database performance and identify bottlenecks\n   - Review slow query logs and execution plans\n   - Assess database schema design and normalization\n   - Evaluate indexing strategy and query patterns\n   - Monitor database resource utilization (CPU, memory, I/O)\n\n2. **Query Optimization**\n   - Optimize slow queries and improve execution plans:\n\n   **PostgreSQL Query Optimization:**\n   ```sql\n   -- Enable query logging for analysis\n   ALTER SYSTEM SET log_statement = 'all';\n   ALTER SYSTEM SET log_min_duration_statement = 1000; -- Log queries > 1 second\n   SELECT pg_reload_conf();\n\n   -- Analyze query performance\n   EXPLAIN (ANALYZE, BUFFERS, FORMAT JSON) \n   SELECT u.id, u.name, COUNT(o.id) as order_count\n   FROM users u\n   LEFT JOIN orders o ON u.id = o.user_id\n   WHERE u.created_at > '2023-01-01'\n   GROUP BY u.id, u.name\n   ORDER BY order_count DESC;\n\n   -- Optimize with proper indexing\n   CREATE INDEX CONCURRENTLY idx_users_created_at ON users(created_at);\n   CREATE INDEX CONCURRENTLY idx_orders_user_id ON orders(user_id);\n   CREATE INDEX CONCURRENTLY idx_orders_user_created ON orders(user_id, created_at);\n   ```\n\n   **MySQL Query Optimization:**\n   ```sql\n   -- Enable slow query log\n   SET GLOBAL slow_query_log = 'ON';\n   SET GLOBAL long_query_time = 1;\n   SET GLOBAL log_queries_not_using_indexes = 'ON';\n\n   -- Analyze query performance\n   EXPLAIN FORMAT=JSON \n   SELECT p.*, c.name as category_name\n   FROM products p\n   JOIN categories c ON p.category_id = c.id\n   WHERE p.price BETWEEN 100 AND 500\n   AND p.created_at > DATE_SUB(NOW(), INTERVAL 30 DAY);\n\n   -- Add composite indexes\n   ALTER TABLE products \n   ADD INDEX idx_price_created (price, created_at),\n   ADD INDEX idx_category_price (category_id, price);\n   ```\n\n3. **Index Strategy Optimization**\n   - Design and implement optimal indexing strategy:\n\n   **Index Analysis and Creation:**\n   ```sql\n   -- PostgreSQL index usage analysis\n   SELECT \n     schemaname,\n     tablename,\n     indexname,\n     idx_scan as index_scans,\n     seq_scan as table_scans,\n     idx_scan::float / (idx_scan + seq_scan + 1) as index_usage_ratio\n   FROM pg_stat_user_indexes \n   ORDER BY index_usage_ratio ASC;\n\n   -- Find missing indexes\n   SELECT \n     query,\n     calls,\n     total_time,\n     mean_time,\n     rows\n   FROM pg_stat_statements \n   WHERE mean_time > 1000 -- queries taking > 1 second\n   ORDER BY mean_time DESC;\n\n   -- Create covering indexes for common query patterns\n   CREATE INDEX CONCURRENTLY idx_orders_covering \n   ON orders(user_id, status, created_at) \n   INCLUDE (total_amount, discount);\n\n   -- Partial indexes for selective conditions\n   CREATE INDEX CONCURRENTLY idx_active_users \n   ON users(last_login) \n   WHERE status = 'active';\n   ```\n\n   **Index Maintenance Scripts:**\n   ```javascript\n   // Node.js index analysis tool\n   const { Pool } = require('pg');\n   const pool = new Pool();\n\n   class IndexAnalyzer {\n     static async analyzeUnusedIndexes() {\n       const query = `\n         SELECT \n           schemaname,\n           tablename,\n           indexname,\n           idx_scan,\n           pg_size_pretty(pg_relation_size(indexrelid)) as size\n         FROM pg_stat_user_indexes \n         WHERE idx_scan = 0\n         AND schemaname = 'public'\n         ORDER BY pg_relation_size(indexrelid) DESC;\n       `;\n       \n       const result = await pool.query(query);\n       console.log('Unused indexes:', result.rows);\n       return result.rows;\n     }\n\n     static async suggestIndexes() {\n       const query = `\n         SELECT \n           query,\n           calls,\n           total_time,\n           mean_time\n         FROM pg_stat_statements \n         WHERE mean_time > 100\n         AND query NOT LIKE '%pg_%'\n         ORDER BY total_time DESC\n         LIMIT 20;\n       `;\n       \n       const result = await pool.query(query);\n       console.log('Slow queries needing indexes:', result.rows);\n       return result.rows;\n     }\n   }\n   ```\n\n4. **Schema Design Optimization**\n   - Optimize database schema for performance:\n\n   **Normalization and Denormalization:**\n   ```sql\n   -- Denormalization example for read-heavy workloads\n   -- Instead of joining multiple tables for product display\n   CREATE TABLE product_display_cache AS\n   SELECT \n     p.id,\n     p.name,\n     p.price,\n     p.description,\n     c.name as category_name,\n     b.name as brand_name,\n     AVG(r.rating) as avg_rating,\n     COUNT(r.id) as review_count\n   FROM products p\n   JOIN categories c ON p.category_id = c.id\n   JOIN brands b ON p.brand_id = b.id\n   LEFT JOIN reviews r ON p.id = r.product_id\n   GROUP BY p.id, c.name, b.name;\n\n   -- Create materialized view for complex aggregations\n   CREATE MATERIALIZED VIEW monthly_sales_summary AS\n   SELECT \n     DATE_TRUNC('month', created_at) as month,\n     category_id,\n     COUNT(*) as order_count,\n     SUM(total_amount) as total_revenue,\n     AVG(total_amount) as avg_order_value\n   FROM orders \n   WHERE created_at >= DATE_TRUNC('year', CURRENT_DATE)\n   GROUP BY DATE_TRUNC('month', created_at), category_id;\n\n   -- Refresh materialized view periodically\n   REFRESH MATERIALIZED VIEW CONCURRENTLY monthly_sales_summary;\n   ```\n\n   **Partitioning for Large Tables:**\n   ```sql\n   -- PostgreSQL table partitioning\n   CREATE TABLE orders_partitioned (\n     id SERIAL,\n     user_id INTEGER,\n     total_amount DECIMAL(10,2),\n     created_at TIMESTAMP NOT NULL,\n     status VARCHAR(50)\n   ) PARTITION BY RANGE (created_at);\n\n   -- Create monthly partitions\n   CREATE TABLE orders_2024_01 PARTITION OF orders_partitioned\n   FOR VALUES FROM ('2024-01-01') TO ('2024-02-01');\n\n   CREATE TABLE orders_2024_02 PARTITION OF orders_partitioned\n   FOR VALUES FROM ('2024-02-01') TO ('2024-03-01');\n\n   -- Automatic partition creation\n   CREATE OR REPLACE FUNCTION create_monthly_partition(table_name text, start_date date)\n   RETURNS void AS $$\n   DECLARE\n     partition_name text;\n     end_date date;\n   BEGIN\n     partition_name := table_name || '_' || to_char(start_date, 'YYYY_MM');\n     end_date := start_date + interval '1 month';\n     \n     EXECUTE format('CREATE TABLE %I PARTITION OF %I FOR VALUES FROM (%L) TO (%L)',\n       partition_name, table_name, start_date, end_date);\n   END;\n   $$ LANGUAGE plpgsql;\n   ```\n\n5. **Connection Pool Optimization**\n   - Configure optimal database connection pooling:\n\n   **Node.js Connection Pool Configuration:**\n   ```javascript\n   const { Pool } = require('pg');\n\n   // Optimized connection pool configuration\n   const pool = new Pool({\n     user: process.env.DB_USER,\n     host: process.env.DB_HOST,\n     database: process.env.DB_NAME,\n     password: process.env.DB_PASSWORD,\n     port: process.env.DB_PORT,\n     \n     // Connection pool settings\n     max: 20, // Maximum connections\n     idleTimeoutMillis: 30000, // 30 seconds\n     connectionTimeoutMillis: 2000, // 2 seconds\n     maxUses: 7500, // Max uses before connection refresh\n     \n     // Performance settings\n     statement_timeout: 30000, // 30 seconds\n     query_timeout: 30000,\n     \n     // SSL configuration\n     ssl: process.env.NODE_ENV === 'production' ? { rejectUnauthorized: false } : false,\n   });\n\n   // Connection pool monitoring\n   pool.on('connect', (client) => {\n     console.log('Connected to database');\n   });\n\n   pool.on('error', (err, client) => {\n     console.error('Database connection error:', err);\n   });\n\n   // Pool stats monitoring\n   setInterval(() => {\n     console.log('Pool stats:', {\n       totalCount: pool.totalCount,\n       idleCount: pool.idleCount,\n       waitingCount: pool.waitingCount,\n     });\n   }, 60000); // Every minute\n   ```\n\n   **Database Connection Middleware:**\n   ```javascript\n   class DatabaseManager {\n     static async executeQuery(query, params = []) {\n       const client = await pool.connect();\n       try {\n         const start = Date.now();\n         const result = await client.query(query, params);\n         const duration = Date.now() - start;\n         \n         // Log slow queries\n         if (duration > 1000) {\n           console.warn(`Slow query (${duration}ms):`, query);\n         }\n         \n         return result;\n       } finally {\n         client.release();\n       }\n     }\n\n     static async transaction(callback) {\n       const client = await pool.connect();\n       try {\n         await client.query('BEGIN');\n         const result = await callback(client);\n         await client.query('COMMIT');\n         return result;\n       } catch (error) {\n         await client.query('ROLLBACK');\n         throw error;\n       } finally {\n         client.release();\n       }\n     }\n   }\n   ```\n\n6. **Query Result Caching**\n   - Implement intelligent database result caching:\n\n   ```javascript\n   const Redis = require('redis');\n   const redis = Redis.createClient();\n\n   class QueryCache {\n     static generateKey(query, params) {\n       return `query:${Buffer.from(query + JSON.stringify(params)).toString('base64')}`;\n     }\n\n     static async get(query, params) {\n       const key = this.generateKey(query, params);\n       const cached = await redis.get(key);\n       return cached ? JSON.parse(cached) : null;\n     }\n\n     static async set(query, params, result, ttl = 300) {\n       const key = this.generateKey(query, params);\n       await redis.setex(key, ttl, JSON.stringify(result));\n     }\n\n     static async cachedQuery(query, params = [], ttl = 300) {\n       // Try cache first\n       let result = await this.get(query, params);\n       if (result) {\n         return result;\n       }\n\n       // Execute query and cache result\n       result = await DatabaseManager.executeQuery(query, params);\n       await this.set(query, params, result.rows, ttl);\n       \n       return result;\n     }\n\n     // Cache invalidation by table patterns\n     static async invalidateTable(tableName) {\n       const pattern = `query:*${tableName}*`;\n       const keys = await redis.keys(pattern);\n       if (keys.length > 0) {\n         await redis.del(keys);\n       }\n     }\n   }\n   ```\n\n7. **Database Monitoring and Profiling**\n   - Set up comprehensive database monitoring:\n\n   **Performance Monitoring Script:**\n   ```javascript\n   class DatabaseMonitor {\n     static async getPerformanceStats() {\n       const queries = [\n         {\n           name: 'active_connections',\n           query: 'SELECT count(*) FROM pg_stat_activity WHERE state = \\'active\\';'\n         },\n         {\n           name: 'long_running_queries',\n           query: `SELECT pid, now() - pg_stat_activity.query_start AS duration, query \n                   FROM pg_stat_activity \n                   WHERE (now() - pg_stat_activity.query_start) > interval '5 minutes';`\n         },\n         {\n           name: 'table_sizes',\n           query: `SELECT relname AS table_name, \n                          pg_size_pretty(pg_total_relation_size(relid)) AS size\n                   FROM pg_catalog.pg_statio_user_tables \n                   ORDER BY pg_total_relation_size(relid) DESC LIMIT 10;`\n         },\n         {\n           name: 'index_usage',\n           query: `SELECT relname AS table_name, \n                          indexrelname AS index_name,\n                          idx_scan AS index_scans,\n                          seq_scan AS sequential_scans\n                   FROM pg_stat_user_indexes \n                   WHERE seq_scan > idx_scan;`\n         }\n       ];\n\n       const stats = {};\n       for (const { name, query } of queries) {\n         try {\n           const result = await pool.query(query);\n           stats[name] = result.rows;\n         } catch (error) {\n           stats[name] = { error: error.message };\n         }\n       }\n\n       return stats;\n     }\n\n     static async alertOnSlowQueries() {\n       const slowQueries = await pool.query(`\n         SELECT query, calls, total_time, mean_time, stddev_time\n         FROM pg_stat_statements \n         WHERE mean_time > 1000 \n         ORDER BY mean_time DESC \n         LIMIT 10;\n       `);\n\n       if (slowQueries.rows.length > 0) {\n         console.warn('Slow queries detected:', slowQueries.rows);\n         // Send alert to monitoring system\n       }\n     }\n   }\n\n   // Schedule monitoring\n   setInterval(async () => {\n     await DatabaseMonitor.alertOnSlowQueries();\n   }, 300000); // Every 5 minutes\n   ```\n\n8. **Read Replica and Load Balancing**\n   - Configure read replicas for query distribution:\n\n   ```javascript\n   const { Pool } = require('pg');\n\n   class DatabaseCluster {\n     constructor() {\n       this.writePool = new Pool({\n         host: process.env.DB_WRITE_HOST,\n         // ... write database config\n       });\n\n       this.readPools = [\n         new Pool({\n           host: process.env.DB_READ1_HOST,\n           // ... read replica 1 config\n         }),\n         new Pool({\n           host: process.env.DB_READ2_HOST,\n           // ... read replica 2 config\n         }),\n       ];\n\n       this.currentReadIndex = 0;\n     }\n\n     getReadPool() {\n       // Round-robin read replica selection\n       const pool = this.readPools[this.currentReadIndex];\n       this.currentReadIndex = (this.currentReadIndex + 1) % this.readPools.length;\n       return pool;\n     }\n\n     async executeWrite(query, params) {\n       return await this.writePool.query(query, params);\n     }\n\n     async executeRead(query, params) {\n       const readPool = this.getReadPool();\n       return await readPool.query(query, params);\n     }\n\n     async executeQuery(query, params, forceWrite = false) {\n       const isWriteQuery = /^\\s*(INSERT|UPDATE|DELETE|CREATE|ALTER|DROP)/i.test(query);\n       \n       if (isWriteQuery || forceWrite) {\n         return await this.executeWrite(query, params);\n       } else {\n         return await this.executeRead(query, params);\n       }\n     }\n   }\n\n   const dbCluster = new DatabaseCluster();\n   ```\n\n9. **Database Vacuum and Maintenance**\n   - Implement automated database maintenance:\n\n   **PostgreSQL Maintenance Scripts:**\n   ```sql\n   -- Automated vacuum and analyze\n   CREATE OR REPLACE FUNCTION auto_vacuum_analyze()\n   RETURNS void AS $$\n   DECLARE\n     rec RECORD;\n   BEGIN\n     FOR rec IN \n       SELECT schemaname, tablename \n       FROM pg_tables \n       WHERE schemaname = 'public'\n     LOOP\n       EXECUTE 'VACUUM ANALYZE ' || quote_ident(rec.schemaname) || '.' || quote_ident(rec.tablename);\n       RAISE NOTICE 'Vacuumed table %.%', rec.schemaname, rec.tablename;\n     END LOOP;\n   END;\n   $$ LANGUAGE plpgsql;\n\n   -- Schedule maintenance (using pg_cron extension)\n   SELECT cron.schedule('nightly-maintenance', '0 2 * * *', 'SELECT auto_vacuum_analyze();');\n   ```\n\n   **Maintenance Monitoring:**\n   ```javascript\n   class MaintenanceMonitor {\n     static async checkTableBloat() {\n       const query = `\n         SELECT \n           tablename,\n           pg_size_pretty(pg_total_relation_size(tablename::regclass)) as size,\n           n_dead_tup,\n           n_live_tup,\n           CASE \n             WHEN n_live_tup > 0 \n             THEN round(n_dead_tup::numeric / n_live_tup::numeric, 2) \n             ELSE 0 \n           END as dead_ratio\n         FROM pg_stat_user_tables \n         WHERE n_dead_tup > 1000\n         ORDER BY dead_ratio DESC;\n       `;\n\n       const result = await pool.query(query);\n       \n       // Alert if dead tuple ratio is high\n       result.rows.forEach(row => {\n         if (row.dead_ratio > 0.2) {\n           console.warn(`Table ${row.tablename} has high bloat: ${row.dead_ratio}`);\n         }\n       });\n\n       return result.rows;\n     }\n\n     static async reindexIfNeeded() {\n       const bloatedIndexes = await pool.query(`\n         SELECT indexname, tablename \n         FROM pg_stat_user_indexes \n         WHERE idx_scan = 0 AND pg_relation_size(indexrelid) > 10485760; -- > 10MB\n       `);\n\n       // Suggest reindexing unused large indexes\n       bloatedIndexes.rows.forEach(row => {\n         console.log(`Consider dropping unused index: ${row.indexname} on ${row.tablename}`);\n       });\n     }\n   }\n   ```\n\n10. **Performance Testing and Benchmarking**\n    - Set up database performance testing:\n\n    **Load Testing Script:**\n    ```javascript\n    const { Pool } = require('pg');\n    const pool = new Pool();\n\n    class DatabaseLoadTester {\n      static async benchmarkQuery(query, params, iterations = 100) {\n        const times = [];\n        \n        for (let i = 0; i < iterations; i++) {\n          const start = process.hrtime.bigint();\n          await pool.query(query, params);\n          const end = process.hrtime.bigint();\n          \n          times.push(Number(end - start) / 1000000); // Convert to milliseconds\n        }\n\n        const avg = times.reduce((a, b) => a + b, 0) / times.length;\n        const min = Math.min(...times);\n        const max = Math.max(...times);\n        const median = times.sort()[Math.floor(times.length / 2)];\n\n        return { avg, min, max, median, iterations };\n      }\n\n      static async stressTest(concurrency = 10, duration = 60000) {\n        const startTime = Date.now();\n        const results = { success: 0, errors: 0, totalTime: 0 };\n        \n        const workers = Array(concurrency).fill().map(async () => {\n          while (Date.now() - startTime < duration) {\n            try {\n              const start = Date.now();\n              await pool.query('SELECT COUNT(*) FROM products');\n              results.totalTime += Date.now() - start;\n              results.success++;\n            } catch (error) {\n              results.errors++;\n            }\n          }\n        });\n\n        await Promise.all(workers);\n        \n        results.qps = results.success / (duration / 1000);\n        results.avgResponseTime = results.totalTime / results.success;\n        \n        return results;\n      }\n    }\n\n    // Run benchmarks\n    async function runBenchmarks() {\n      console.log('Running database benchmarks...');\n      \n      const simpleQuery = await DatabaseLoadTester.benchmarkQuery(\n        'SELECT * FROM products LIMIT 10'\n      );\n      console.log('Simple query benchmark:', simpleQuery);\n      \n      const complexQuery = await DatabaseLoadTester.benchmarkQuery(\n        `SELECT p.*, c.name as category \n         FROM products p \n         JOIN categories c ON p.category_id = c.id \n         ORDER BY p.created_at DESC LIMIT 50`\n      );\n      console.log('Complex query benchmark:', complexQuery);\n      \n      const stressTest = await DatabaseLoadTester.stressTest(5, 30000);\n      console.log('Stress test results:', stressTest);\n    }\n    ```"
              },
              {
                "name": "/optimize",
                "description": "Analyze code performance and propose three specific optimization improvements",
                "path": "plugins/all-commands/commands/optimize.md",
                "frontmatter": {
                  "description": "Analyze code performance and propose three specific optimization improvements",
                  "category": "code-analysis-testing",
                  "allowed-tools": "Read, Edit"
                },
                "content": "Analyze the performance of this code and propose three specific optimizations."
              },
              {
                "name": "/pac-configure",
                "description": "Configure and initialize a project following the Product as Code specification for structured, version-controlled product management",
                "path": "plugins/all-commands/commands/pac-configure.md",
                "frontmatter": {
                  "description": "Configure and initialize a project following the Product as Code specification for structured, version-controlled product management",
                  "category": "project-task-management",
                  "argument-hint": "Specify configuration settings"
                },
                "content": "# Configure PAC (Product as Code) Project\n\nConfigure and initialize a project following the Product as Code specification for structured, version-controlled product management\n\n## Instructions\n\n1. **Analyze Project Context**\n   - Check if the current directory is a git repository\n   - Verify if a PAC configuration already exists (look for epic-*.yaml or ticket-*.yaml files)\n   - Parse any arguments provided: `$ARGUMENTS`\n   - If PAC files exist, analyze them to understand current structure\n\n2. **Interactive Setup (if no existing PAC config)**\n   - Ask user for project details:\n     - Project name\n     - Project description\n     - Primary product owner\n     - Default ticket assignee\n     - Initial epic name\n   - Validate inputs and confirm with user before proceeding\n\n3. **Create PAC Directory Structure**\n   - Create `.pac/` directory if it doesn't exist\n   - Create subdirectories:\n     - `.pac/epics/` - for epic definitions\n     - `.pac/tickets/` - for ticket definitions\n     - `.pac/templates/` - for reusable templates\n   - Add `.pac/README.md` explaining the structure and PAC specification\n\n4. **Generate PAC Configuration Files**\n   - Create `.pac/pac.config.yaml` with:\n     ```yaml\n     apiVersion: productascode.org/v0.1.0\n     kind: Configuration\n     metadata:\n       project: \"[project-name]\"\n       owner: \"[owner-name]\"\n       created: \"[timestamp]\"\n     spec:\n       defaults:\n         assignee: \"[default-assignee]\"\n         epic_prefix: \"epic-\"\n         ticket_prefix: \"ticket-\"\n       validation:\n         enforce_unique_ids: true\n         require_acceptance_criteria: true\n     ```\n\n5. **Create Initial Epic Template**\n   - Generate `.pac/templates/epic-template.yaml`:\n     ```yaml\n     apiVersion: productascode.org/v0.1.0\n     kind: Epic\n     metadata:\n       id: \"epic-[name]\"\n       name: \"[Epic Name]\"\n       created: \"[timestamp]\"\n       owner: \"[owner]\"\n     spec:\n       description: |\n         [Epic description]\n       scope: |\n         [Scope definition]\n       success_criteria:\n         - [Criterion 1]\n         - [Criterion 2]\n       tickets: []\n     ```\n\n6. **Create Initial Ticket Template**\n   - Generate `.pac/templates/ticket-template.yaml`:\n     ```yaml\n     apiVersion: productascode.org/v0.1.0\n     kind: Ticket\n     metadata:\n       id: \"ticket-[name]\"\n       name: \"[Ticket Name]\"\n       epic: \"[parent-epic-id]\"\n       created: \"[timestamp]\"\n       assignee: \"[assignee]\"\n     spec:\n       description: |\n         [Ticket description]\n       type: \"feature\"\n       status: \"backlog\"\n       priority: \"medium\"\n       acceptance_criteria:\n         - [ ] [Criterion 1]\n         - [ ] [Criterion 2]\n       tasks:\n         - [ ] [Task 1]\n         - [ ] [Task 2]\n     ```\n\n7. **Create First Epic and Ticket**\n   - Based on user input, create first epic in `.pac/epics/`\n   - Create an initial ticket linked to the epic\n   - Use proper naming convention and unique IDs\n   - Set appropriate timestamps\n\n8. **Set Up Validation Scripts**\n   - Create `.pac/scripts/validate.sh` to check PAC compliance:\n     - Verify YAML syntax\n     - Check required fields\n     - Validate unique IDs\n     - Ensure epic-ticket relationships are valid\n   - Make script executable\n\n9. **Configure Git Integration**\n   - Add PAC-specific entries to `.gitignore` if needed:\n     ```\n     .pac/tmp/\n     .pac/cache/\n     *.pac.lock\n     ```\n   - Create git hook for pre-commit PAC validation (optional)\n\n10. **Generate PAC Documentation**\n    - Create `.pac/GUIDE.md` with:\n      - Quick start guide for team members\n      - Common PAC workflows\n      - How to create new epics and tickets\n      - How to update ticket status\n      - Link to full PAC specification\n\n11. **Create Helper Commands**\n    - Generate `.pac/scripts/new-epic.sh` for creating new epics\n    - Generate `.pac/scripts/new-ticket.sh` for creating new tickets\n    - Include prompts for required fields and validation\n\n12. **Final Validation and Summary**\n    - Run validation script on created files\n    - Display summary of created structure\n    - Show next steps:\n      - How to create new epics: `cp .pac/templates/epic-template.yaml .pac/epics/epic-[name].yaml`\n      - How to create new tickets: `cp .pac/templates/ticket-template.yaml .pac/tickets/ticket-[name].yaml`\n      - How to validate PAC files: `.pac/scripts/validate.sh`\n    - Suggest integrating with CI/CD for automatic validation\n\n## Arguments\n\n- `--minimal`: Create minimal PAC structure without templates and scripts\n- `--epic-name <name>`: Specify initial epic name\n- `--owner <name>`: Specify product owner name\n- `--no-git`: Skip git integration setup\n\n## Example Usage\n\n```\n/project:pac-configure\n/project:pac-configure --epic-name \"user-authentication\" --owner \"john.doe\"\n/project:pac-configure --minimal\n```"
              },
              {
                "name": "/pac-create-epic",
                "description": "Create a new epic following the Product as Code specification with guided workflow",
                "path": "plugins/all-commands/commands/pac-create-epic.md",
                "frontmatter": {
                  "description": "Create a new epic following the Product as Code specification with guided workflow",
                  "category": "project-task-management",
                  "argument-hint": "Specify epic details",
                  "allowed-tools": "Write"
                },
                "content": "# Create PAC Epic\n\nCreate a new epic following the Product as Code specification with guided workflow\n\n## Instructions\n\n1. **Validate PAC Configuration**\n   - Check if `.pac/` directory exists\n   - Verify PAC configuration file exists at `.pac/pac.config.yaml`\n   - If not configured, suggest running `/project:pac-configure` first\n   - Parse arguments: `$ARGUMENTS`\n\n2. **Epic Information Gathering**\n   - If arguments provided, parse:\n     - `--name <name>`: Epic name\n     - `--description <desc>`: Epic description\n     - `--owner <owner>`: Epic owner\n     - `--scope <scope>`: Scope definition\n   - For missing information, prompt user interactively:\n     - Epic ID (suggest format: epic-[kebab-case-name])\n     - Epic name (human-readable)\n     - Epic owner (default from config if available)\n     - Epic description (multi-line)\n     - Scope definition (what's included/excluded)\n     - Success criteria (at least 2-3 items)\n\n3. **Generate Epic ID**\n   - If not provided, generate from epic name:\n     - Convert to lowercase\n     - Replace spaces with hyphens\n     - Remove special characters\n     - Prefix with \"epic-\"\n   - Validate uniqueness against existing epics\n\n4. **Create Epic Structure**\n   - Generate epic YAML following PAC v0.1.0 specification:\n     ```yaml\n     apiVersion: productascode.org/v0.1.0\n     kind: Epic\n     metadata:\n       id: \"[generated-epic-id]\"\n       name: \"[Epic Name]\"\n       created: \"[current-timestamp]\"\n       updated: \"[current-timestamp]\"\n       owner: \"[owner-email-or-name]\"\n       labels:\n         status: \"active\"\n         priority: \"medium\"\n     spec:\n       description: |\n         [Multi-line description]\n       \n       scope: |\n         [Scope definition]\n       \n       success_criteria:\n         - [Criterion 1]\n         - [Criterion 2]\n         - [Criterion 3]\n       \n       constraints:\n         - [Any constraints or limitations]\n       \n       dependencies:\n         - [Dependencies on other epics/systems]\n       \n       tickets: []  # Will be populated as tickets are created\n     ```\n\n5. **Validate Epic Content**\n   - Check all required fields are present\n   - Validate apiVersion matches specification\n   - Ensure metadata has required identifiers\n   - Verify success criteria has at least one item\n   - Check YAML syntax is valid\n\n6. **Save Epic File**\n   - Determine filename: `.pac/epics/[epic-id].yaml`\n   - Check if file already exists\n   - If exists, ask user to confirm overwrite\n   - Write epic content to file\n   - Set appropriate file permissions\n\n7. **Create Epic Directory Structure**\n   - Create `.pac/epics/[epic-id]/` directory for epic-specific docs\n   - Add `.pac/epics/[epic-id]/README.md` with epic overview\n   - Create `.pac/epics/[epic-id]/tickets/` for future ticket links\n\n8. **Update PAC Index**\n   - If `.pac/index.yaml` exists, add epic entry:\n     ```yaml\n     epics:\n       - id: \"[epic-id]\"\n         name: \"[Epic Name]\"\n         status: \"active\"\n         created: \"[timestamp]\"\n         ticket_count: 0\n     ```\n\n9. **Git Integration**\n   - If in git repository:\n     - Add new epic file to git\n     - Create branch `pac/[epic-id]` for epic work\n     - Prepare commit message:\n       ```\n       feat(pac): add epic [epic-id]\n       \n       - Epic: [Epic Name]\n       - Owner: [Owner]\n       - Success Criteria: [count] items defined\n       ```\n\n10. **Generate Epic Summary**\n    - Display created epic details:\n      - Epic ID and location\n      - Success criteria summary\n      - Next steps for creating tickets\n    - Show helpful commands:\n      - Create ticket: `/project:pac-create-ticket --epic [epic-id]`\n      - View epic: `cat .pac/epics/[epic-id].yaml`\n      - Validate: `.pac/scripts/validate.sh .pac/epics/[epic-id].yaml`\n\n## Arguments\n\n- `--name <name>`: Epic name (required if not interactive)\n- `--description <description>`: Epic description\n- `--owner <owner>`: Epic owner email or name\n- `--scope <scope>`: Scope definition\n- `--success-criteria <criteria>`: Comma-separated success criteria\n- `--priority <priority>`: Priority level (low/medium/high/critical)\n- `--no-git`: Skip git integration\n\n## Example Usage\n\n```\n/project:pac-create-epic\n/project:pac-create-epic --name \"User Authentication System\"\n/project:pac-create-epic --name \"Payment Integration\" --owner \"john@example.com\" --priority high\n```"
              },
              {
                "name": "/pac-create-ticket",
                "description": "Create a new ticket within an epic following the Product as Code specification",
                "path": "plugins/all-commands/commands/pac-create-ticket.md",
                "frontmatter": {
                  "description": "Create a new ticket within an epic following the Product as Code specification",
                  "category": "project-task-management",
                  "argument-hint": "Specify ticket details",
                  "allowed-tools": "Write"
                },
                "content": "# Create PAC Ticket\n\nCreate a new ticket within an epic following the Product as Code specification\n\n## Instructions\n\n1. **Validate PAC Environment**\n   - Verify `.pac/` directory exists\n   - Check PAC configuration at `.pac/pac.config.yaml`\n   - If not configured, suggest running `/project:pac-configure`\n   - Parse arguments from: `$ARGUMENTS`\n\n2. **Epic Selection**\n   - If `--epic <epic-id>` provided, validate epic exists\n   - Otherwise, list available epics from `.pac/epics/`:\n     - Show epic ID, name, and ticket count\n     - Allow user to select epic interactively\n   - Load selected epic to understand context\n\n3. **Ticket Information Gathering**\n   - Parse command arguments:\n     - `--name <name>`: Ticket name\n     - `--type <type>`: feature/bug/task/spike\n     - `--description <desc>`: Ticket description\n     - `--assignee <assignee>`: Assigned developer\n     - `--priority <priority>`: low/medium/high/critical\n   - For missing required fields, prompt interactively:\n     - Ticket name (required)\n     - Ticket type (default: feature)\n     - Description (multi-line)\n     - Assignee (default from config)\n     - Priority (default: medium)\n     - Initial status (default: backlog)\n\n4. **Generate Ticket ID**\n   - Create ID format: `ticket-[epic-short-name]-[sequence]`\n   - Example: `ticket-auth-001`, `ticket-auth-002`\n   - Check existing tickets in epic to determine sequence\n   - Ensure uniqueness across all tickets\n\n5. **Define Acceptance Criteria**\n   - Prompt for acceptance criteria (at least 2 items)\n   - Format as checkbox list:\n     ```yaml\n     acceptance_criteria:\n       - [ ] User can successfully authenticate\n       - [ ] Session persists across page refreshes\n       - [ ] Invalid credentials show error message\n     ```\n\n6. **Define Implementation Tasks**\n   - Prompt for implementation tasks\n   - Break down work into actionable items:\n     ```yaml\n     tasks:\n       - [ ] Create authentication service\n       - [ ] Implement login form component\n       - [ ] Add session management\n       - [ ] Write unit tests\n       - [ ] Update documentation\n     ```\n\n7. **Create Ticket Structure**\n   - Generate ticket YAML following PAC v0.1.0:\n     ```yaml\n     apiVersion: productascode.org/v0.1.0\n     kind: Ticket\n     metadata:\n       id: \"[generated-ticket-id]\"\n       sequence: [number]\n       name: \"[Ticket Name]\"\n       epic: \"[parent-epic-id]\"\n       created: \"[timestamp]\"\n       updated: \"[timestamp]\"\n       assignee: \"[assignee]\"\n       labels:\n         component: \"[relevant-component]\"\n         effort: \"[size-estimate]\"\n     spec:\n       description: |\n         [Detailed description]\n         \n       type: \"[feature/bug/task/spike]\"\n       status: \"[backlog/in-progress/review/done]\"\n       priority: \"[low/medium/high/critical]\"\n       \n       acceptance_criteria:\n         - [ ] [Criterion 1]\n         - [ ] [Criterion 2]\n         \n       tasks:\n         - [ ] [Task 1]\n         - [ ] [Task 2]\n         \n       technical_notes: |\n         [Any technical considerations]\n         \n       dependencies:\n         - [Other ticket IDs if any]\n     ```\n\n8. **Estimate Effort**\n   - Prompt for effort estimation:\n     - Story points (1, 2, 3, 5, 8, 13)\n     - T-shirt size (XS, S, M, L, XL)\n     - Time estimate (hours/days)\n   - Add to metadata labels\n\n9. **Link to Epic**\n   - Update parent epic file to include ticket reference:\n     ```yaml\n     spec:\n       tickets:\n         - id: \"[ticket-id]\"\n           name: \"[ticket-name]\"\n           status: \"backlog\"\n           assignee: \"[assignee]\"\n     ```\n\n10. **Save Ticket File**\n    - Save to: `.pac/tickets/[ticket-id].yaml`\n    - Create symbolic link in epic directory:\n      `.pac/epics/[epic-id]/tickets/[ticket-id].yaml`\n    - Validate file was created successfully\n\n11. **Create Branch (Optional)**\n    - If `--create-branch` flag or git integration enabled:\n      - Create branch: `feature/[ticket-id]`\n      - Include branch name in ticket metadata\n      - Show git commands for switching to branch\n\n12. **Generate Ticket Summary**\n    - Display created ticket information:\n      - Ticket ID and file location\n      - Epic association\n      - Assignee and priority\n      - Task count and acceptance criteria count\n    - Show next actions:\n      - Start work: `git checkout -b feature/[ticket-id]`\n      - Update status: `/project:pac-update-ticket --id [ticket-id] --status in-progress`\n      - View ticket: `cat .pac/tickets/[ticket-id].yaml`\n\n## Arguments\n\n- `--epic <epic-id>`: Parent epic ID (required)\n- `--name <name>`: Ticket name\n- `--type <type>`: Ticket type (feature/bug/task/spike)\n- `--description <description>`: Ticket description\n- `--assignee <assignee>`: Assigned developer\n- `--priority <priority>`: Priority level\n- `--create-branch`: Automatically create git branch\n- `--template <template>`: Use custom ticket template\n\n## Example Usage\n\n```\n/project:pac-create-ticket --epic epic-authentication\n/project:pac-create-ticket --epic epic-payment --name \"Implement Stripe integration\" --type feature\n/project:pac-create-ticket --epic epic-ui --assignee jane@example.com --priority high --create-branch\n```"
              },
              {
                "name": "/pac-update-status",
                "description": "Update ticket status and track progress in Product as Code workflow",
                "path": "plugins/all-commands/commands/pac-update-status.md",
                "frontmatter": {
                  "description": "Update ticket status and track progress in Product as Code workflow",
                  "category": "project-task-management",
                  "argument-hint": "Specify status update details",
                  "allowed-tools": "Read, Write"
                },
                "content": "# Update PAC Ticket Status\n\nUpdate ticket status and track progress in Product as Code workflow\n\n## Instructions\n\n1. **Parse Command Arguments**\n   - Extract arguments from: `$ARGUMENTS`\n   - Required: `--ticket <ticket-id>` or select interactively\n   - Optional: `--status <status>`, `--assignee <assignee>`, `--comment <comment>`\n   - Validate `.pac/` directory exists\n\n2. **Ticket Selection**\n   - If ticket ID provided, validate it exists\n   - Otherwise, show interactive ticket selector:\n     - List tickets grouped by status\n     - Show: ID, Name, Current Status, Assignee\n     - Filter by epic if `--epic` flag provided\n     - Allow search by ticket name\n\n3. **Load Current Ticket State**\n   - Read ticket file from `.pac/tickets/[ticket-id].yaml`\n   - Display current ticket information:\n     - Name and description\n     - Current status and assignee\n     - Epic association\n     - Acceptance criteria progress\n     - Task completion status\n\n4. **Status Transition Validation**\n   - Current status determines valid transitions:\n     - `backlog`  `in-progress`, `cancelled`\n     - `in-progress`  `review`, `blocked`, `backlog`\n     - `review`  `done`, `in-progress`\n     - `blocked`  `in-progress`, `cancelled`\n     - `done`  (no transitions, warn if attempting)\n     - `cancelled`  `backlog` (for resurrection)\n   - Prevent invalid status transitions\n   - Show available transitions if invalid status provided\n\n5. **Update Ticket Status**\n   - If new status provided and valid:\n     - Update `spec.status` field\n     - Update `metadata.updated` timestamp\n     - Add status change to history (if tracking)\n   - Special handling for status transitions:\n     - `backlog  in-progress`: \n       - Prompt for assignee if not set\n       - Suggest creating feature branch\n     - `in-progress  review`:\n       - Check if all tasks are marked complete\n       - Warn if acceptance criteria not met\n     - `review  done`:\n       - Verify all acceptance criteria checked\n       - Update completion timestamp\n\n6. **Update Additional Fields**\n   - If `--assignee` provided:\n     - Update `metadata.assignee`\n     - Add assignment history entry\n   - If `--comment` provided:\n     - Add to ticket comments/notes section\n     - Include timestamp and current user\n\n7. **Task and Criteria Progress**\n   - If moving to `in-progress`, prompt to review tasks\n   - Allow marking tasks as complete:\n     ```yaml\n     tasks:\n       - [x] Create authentication service\n       - [x] Implement login form component\n       - [ ] Add session management\n       - [ ] Write unit tests\n     ```\n   - Calculate and display completion percentage\n\n8. **Update Parent Epic**\n   - Load parent epic from `.pac/epics/[epic-id].yaml`\n   - Update ticket entry in epic's ticket list:\n     ```yaml\n     tickets:\n       - id: \"[ticket-id]\"\n         name: \"[ticket-name]\"\n         status: \"[new-status]\"  # Update this\n         assignee: \"[assignee]\"\n         updated: \"[timestamp]\"\n     ```\n   - If ticket is done, increment epic completion metrics\n\n9. **Git Integration**\n   - If status changes to `in-progress` and no branch exists:\n     - Suggest: `git checkout -b feature/[ticket-id]`\n   - If status changes to `review`:\n     - Suggest creating pull request\n     - Generate PR description from ticket details\n   - If status changes to `done`:\n     - Suggest merging and branch cleanup\n\n10. **Generate Status Report**\n    - Show status update summary:\n      ```\n      Ticket Status Updated\n      ====================\n      \n      Ticket: [ticket-id] - [ticket-name]\n      Epic: [epic-name]\n      \n      Status: [old-status]  [new-status]\n      Assignee: [assignee]\n      Updated: [timestamp]\n      \n      Progress:\n      - Tasks: [completed]/[total] ([percentage]%)\n      - Criteria: [met]/[total]\n      \n      Next Actions:\n      - [Suggested next steps based on new status]\n      ```\n\n11. **Notification Hooks**\n    - If `.pac/hooks/` directory exists:\n      - Execute `status-change.sh` if present\n      - Pass ticket ID, old status, new status as arguments\n    - Could integrate with Slack, email, or project management tools\n\n12. **Validation and Save**\n    - Validate updated YAML structure\n    - Create backup of original ticket file\n    - Save updated ticket file\n    - Run PAC validation on updated file\n    - If validation fails, restore from backup\n\n## Arguments\n\n- `--ticket <ticket-id>`: Ticket ID to update (or select interactively)\n- `--status <status>`: New status (backlog/in-progress/review/blocked/done/cancelled)\n- `--assignee <assignee>`: Update assignee\n- `--comment <comment>`: Add comment to ticket\n- `--epic <epic-id>`: Filter tickets by epic (for interactive selection)\n- `--force`: Force status change even if validation warnings exist\n\n## Example Usage\n\n```\n/project:pac-update-status --ticket ticket-auth-001 --status in-progress\n/project:pac-update-status --ticket ticket-ui-003 --status review --comment \"Ready for code review\"\n/project:pac-update-status  # Interactive mode\n/project:pac-update-status --epic epic-payment --status done\n```"
              },
              {
                "name": "/pac-validate",
                "description": "Validate Product as Code project structure and files for specification compliance",
                "path": "plugins/all-commands/commands/pac-validate.md",
                "frontmatter": {
                  "description": "Validate Product as Code project structure and files for specification compliance",
                  "category": "project-task-management",
                  "argument-hint": "Specify validation rules or targets"
                },
                "content": "# Validate PAC Structure\n\nValidate Product as Code project structure and files for specification compliance\n\n## Instructions\n\n1. **Initial Environment Check**\n   - Verify `.pac/` directory exists\n   - Check for PAC configuration file at `.pac/pac.config.yaml`\n   - Parse arguments: `$ARGUMENTS`\n   - Determine validation scope (single file, directory, or entire project)\n\n2. **Configuration Validation**\n   - Load and validate `.pac/pac.config.yaml`:\n     - Check `apiVersion` format (must be semantic version)\n     - Verify `kind` is \"Configuration\"\n     - Validate required metadata fields\n     - Check defaults section has valid values\n   - Report any missing or invalid configuration\n\n3. **Directory Structure Validation**\n   - Verify required directories exist:\n     - `.pac/epics/` - Epic definitions\n     - `.pac/tickets/` - Ticket definitions\n     - `.pac/templates/` - Templates (optional but recommended)\n   - Check file permissions are correct\n   - Ensure no orphaned files outside expected structure\n\n4. **Epic File Validation**\n   - For each file in `.pac/epics/`:\n     - Verify YAML syntax is valid\n     - Check `apiVersion: productascode.org/v0.1.0`\n     - Verify `kind: Epic`\n     - Validate required metadata fields:\n       - `id` (must be unique)\n       - `name` (non-empty string)\n       - `created` (valid timestamp)\n       - `owner` (non-empty string)\n     - Validate spec section:\n       - `description` exists\n       - `success_criteria` has at least one item\n       - `tickets` array is properly formatted\n   - Track all epic IDs for cross-reference validation\n\n5. **Ticket File Validation**\n   - For each file in `.pac/tickets/`:\n     - Verify YAML syntax is valid\n     - Check `apiVersion: productascode.org/v0.1.0`\n     - Verify `kind: Ticket`\n     - Validate required metadata:\n       - `id` (unique across all tickets)\n       - `name` (non-empty string)\n       - `epic` (must reference valid epic ID)\n       - `created` (valid timestamp)\n       - `assignee` (if specified)\n     - Validate spec fields:\n       - `type` is one of: feature, bug, task, spike\n       - `status` is one of: backlog, in-progress, review, done, cancelled\n       - `priority` is one of: low, medium, high, critical\n       - `acceptance_criteria` has at least one item\n       - `tasks` array is properly formatted\n\n6. **Cross-Reference Validation**\n   - Verify all ticket epic references point to existing epics\n   - Check that epic ticket lists match actual ticket files\n   - Validate ticket dependencies reference existing tickets\n   - Ensure no circular dependencies exist\n   - Verify unique IDs across all entities\n\n7. **Data Integrity Checks**\n   - Validate timestamp formats (ISO 8601)\n   - Check that updated timestamps are >= created timestamps\n   - Verify status transitions make sense (no done tickets in backlog epics)\n   - Validate priority and effort estimates are consistent\n\n8. **Template Validation**\n   - If templates exist in `.pac/templates/`:\n     - Verify they follow PAC specification\n     - Check they include all required fields\n     - Ensure placeholder values are clearly marked\n\n9. **Generate Validation Report**\n   - Create detailed report with:\n     ```\n     PAC Validation Report\n     ====================\n     \n     Configuration: [VALID/INVALID]\n     - Issues found: [count]\n     \n     Structure: [VALID/INVALID]\n     - Epics found: [count]\n     - Tickets found: [count]\n     - Orphaned files: [count]\n     \n     Epic Validation:\n     - Valid epics: [count]\n     - Invalid epics: [list with reasons]\n     \n     Ticket Validation:\n     - Valid tickets: [count]\n     - Invalid tickets: [list with reasons]\n     \n     Cross-Reference Issues:\n     - Missing epic references: [list]\n     - Orphaned tickets: [list]\n     - Invalid dependencies: [list]\n     \n     Recommendations:\n     - [Specific fixes needed]\n     ```\n\n10. **Auto-Fix Options**\n    - If `--fix` flag provided:\n      - Add missing required fields with placeholder values\n      - Fix formatting issues (indentation, quotes)\n      - Update epic ticket lists to match actual tickets\n      - Create backup before making changes\n    - Show what would be fixed without `--fix` flag\n\n11. **Git Integration**\n    - If `--pre-commit` flag:\n      - Only validate files staged for commit\n      - Exit with appropriate code for git hook\n      - Provide concise output suitable for CLI\n\n12. **Summary and Exit Codes**\n    - Exit code 0: All validations passed\n    - Exit code 1: Validation errors found\n    - Exit code 2: Configuration errors\n    - Display summary:\n      - Total files validated\n      - Issues found and fixed (if applicable)\n      - Next steps for remaining issues\n\n## Arguments\n\n- `--file <path>`: Validate specific file only\n- `--epic <epic-id>`: Validate specific epic and its tickets\n- `--fix`: Automatically fix common issues\n- `--pre-commit`: Run in pre-commit mode (concise output)\n- `--verbose`: Show detailed validation information\n- `--quiet`: Only show errors, no success messages\n\n## Example Usage\n\n```\n/project:pac-validate\n/project:pac-validate --fix\n/project:pac-validate --file .pac/epics/epic-auth.yaml\n/project:pac-validate --epic epic-payment --verbose\n/project:pac-validate --pre-commit\n```"
              },
              {
                "name": "/performance-audit",
                "description": "Audit application performance metrics",
                "path": "plugins/all-commands/commands/performance-audit.md",
                "frontmatter": {
                  "description": "Audit application performance metrics",
                  "category": "performance-optimization"
                },
                "content": "# Performance Audit Command\n\nAudit application performance metrics\n\n## Instructions\n\nConduct a comprehensive performance audit following these steps:\n\n1. **Technology Stack Analysis**\n   - Identify the primary language, framework, and runtime environment\n   - Review build tools and optimization configurations\n   - Check for performance monitoring tools already in place\n\n2. **Code Performance Analysis**\n   - Identify inefficient algorithms and data structures\n   - Look for nested loops and O(n) operations\n   - Check for unnecessary computations and redundant operations\n   - Review memory allocation patterns and potential leaks\n\n3. **Database Performance**\n   - Analyze database queries for efficiency\n   - Check for missing indexes and slow queries\n   - Review connection pooling and database configuration\n   - Identify N+1 query problems and excessive database calls\n\n4. **Frontend Performance (if applicable)**\n   - Analyze bundle size and chunk optimization\n   - Check for unused code and dependencies\n   - Review image optimization and lazy loading\n   - Examine render performance and re-render cycles\n   - Check for memory leaks in UI components\n\n5. **Network Performance**\n   - Review API call patterns and caching strategies\n   - Check for unnecessary network requests\n   - Analyze payload sizes and compression\n   - Examine CDN usage and static asset optimization\n\n6. **Asynchronous Operations**\n   - Review async/await usage and promise handling\n   - Check for blocking operations and race conditions\n   - Analyze task queuing and background processing\n   - Identify opportunities for parallel execution\n\n7. **Memory Usage**\n   - Check for memory leaks and excessive memory consumption\n   - Review garbage collection patterns\n   - Analyze object lifecycle and cleanup\n   - Identify large objects and unnecessary data retention\n\n8. **Build & Deployment Performance**\n   - Analyze build times and optimization opportunities\n   - Review dependency bundling and tree shaking\n   - Check for development vs production optimizations\n   - Examine deployment pipeline efficiency\n\n9. **Performance Monitoring**\n   - Check existing performance metrics and monitoring\n   - Identify key performance indicators (KPIs) to track\n   - Review alerting and performance thresholds\n   - Suggest performance testing strategies\n\n10. **Benchmarking & Profiling**\n    - Run performance profiling tools appropriate for the stack\n    - Create benchmarks for critical code paths\n    - Measure before and after optimization impact\n    - Document performance baselines\n\n11. **Optimization Recommendations**\n    - Prioritize optimizations by impact and effort\n    - Provide specific code examples and alternatives\n    - Suggest architectural improvements for scalability\n    - Recommend appropriate performance tools and libraries\n\nInclude specific file paths, line numbers, and measurable metrics where possible. Focus on high-impact, low-effort optimizations first."
              },
              {
                "name": "/pr-review",
                "description": "Conduct comprehensive PR review from multiple perspectives (PM, Developer, QA, Security)",
                "path": "plugins/all-commands/commands/pr-review.md",
                "frontmatter": {
                  "description": "Conduct comprehensive PR review from multiple perspectives (PM, Developer, QA, Security)",
                  "category": "version-control-git",
                  "argument-hint": "<pr_link_or_number>",
                  "allowed-tools": "Bash(gh *), Read"
                },
                "content": "# PR Review\n\n**PR Link/Number**: $ARGUMENTS\n\n> **Instructions**: Execute each task in the order given to conduct a thorough code review.  Update GitHub with this review.\n> **Important**: The future is nowany improvements or \"future\" recommendations must be addressed **immediately**.\n\n---\n\n## Task 1: Product Manager Review\n\n**Objective**: Assess from a product management perspective, focusing on:\n- **Business Value**: Does this PR clearly advance our core product goals and deliver immediate ROI?\n- **User Experience**: Is the change intuitive and delightful for users right now? If not, make fixes immediately.\n- **Strategic Alignment**: Does the PR align with current (and long-term, i.e., now) strategic objectives?\n\n**Action**: Provide clear directives on how to ensure maximum user and business impact. All \"future\" suggestions must be implemented now.\n\n---\n\n## Task 2: Developer Review\n\n**Objective**: Evaluate the code thoroughly from a senior lead engineer perspective:\n1. **Code Quality & Maintainability**: Is the code structured for readability and easy maintenance? If not, refactor now.\n2. **Performance & Scalability**: Will these changes operate efficiently at scale? If not, optimize immediately.\n3. **Best Practices & Standards**: Note any deviation from coding standards and correct it now.\n\n**Action**: Leave a concise yet complete review comment, ensuring all improvements happen immediatelyno deferrals.\n\n---\n\n## Task 3: Quality Engineer Review\n\n**Objective**: Verify the overall quality, testing strategy, and reliability of the solution:\n1. **Test Coverage**: Are there sufficient tests (unit, integration, E2E)? If not, add them now.\n2. **Potential Bugs & Edge Cases**: Have all edge cases been considered? If not, address them immediately.\n3. **Regression Risk**: Confirm changes don't undermine existing functionality. If risk is identified, mitigate now with additional checks or tests.\n\n**Action**: Provide a detailed QA assessment, insisting any \"future\" improvements be completed right away.\n\n---\n\n## Task 4: Security Engineer Review\n\n**Objective**: Ensure robust security practices and compliance:\n1. **Vulnerabilities**:"
              },
              {
                "name": "/prepare-release",
                "description": "Prepare and validate release packages",
                "path": "plugins/all-commands/commands/prepare-release.md",
                "frontmatter": {
                  "description": "Prepare and validate release packages",
                  "category": "ci-deployment",
                  "argument-hint": "1. **Release Planning and Validation**",
                  "allowed-tools": "Bash(git *), Bash(npm *)"
                },
                "content": "# Prepare Release Command\n\nPrepare and validate release packages\n\n## Instructions\n\nFollow this systematic approach to prepare a release: **$ARGUMENTS**\n\n1. **Release Planning and Validation**\n   - Determine release version number (semantic versioning)\n   - Review and validate all features included in release\n   - Check that all planned issues and features are complete\n   - Verify release criteria and acceptance requirements\n\n2. **Pre-Release Checklist**\n   - Ensure all tests are passing (unit, integration, E2E)\n   - Verify code coverage meets project standards\n   - Complete security vulnerability scanning\n   - Perform performance testing and validation\n   - Review and approve all pending pull requests\n\n3. **Version Management**\n   ```bash\n   # Check current version\n   git describe --tags --abbrev=0\n   \n   # Determine next version (semantic versioning)\n   # MAJOR.MINOR.PATCH\n   # MAJOR: Breaking changes\n   # MINOR: New features (backward compatible)\n   # PATCH: Bug fixes (backward compatible)\n   \n   # Example version updates\n   # 1.2.3 -> 1.2.4 (patch)\n   # 1.2.3 -> 1.3.0 (minor)\n   # 1.2.3 -> 2.0.0 (major)\n   ```\n\n4. **Code Freeze and Branch Management**\n   ```bash\n   # Create release branch from main\n   git checkout main\n   git pull origin main\n   git checkout -b release/v1.2.3\n   \n   # Alternative: Use main branch directly for smaller releases\n   # Ensure no new features are merged during release process\n   ```\n\n5. **Version Number Updates**\n   - Update package.json, setup.py, or equivalent version files\n   - Update version in application configuration\n   - Update version in documentation and README\n   - Update API version if applicable\n\n   ```bash\n   # Node.js projects\n   npm version patch  # or minor, major\n   \n   # Python projects\n   # Update version in setup.py, __init__.py, or pyproject.toml\n   \n   # Manual version update\n   sed -i 's/\"version\": \"1.2.2\"/\"version\": \"1.2.3\"/' package.json\n   ```\n\n6. **Changelog Generation**\n   ```markdown\n   # CHANGELOG.md\n   \n   ## [1.2.3] - 2024-01-15\n   \n   ### Added\n   - New user authentication system\n   - Dark mode support for UI\n   - API rate limiting functionality\n   \n   ### Changed\n   - Improved database query performance\n   - Updated user interface design\n   - Enhanced error handling\n   \n   ### Fixed\n   - Fixed memory leak in background tasks\n   - Resolved issue with file upload validation\n   - Fixed timezone handling in date calculations\n   \n   ### Security\n   - Updated dependencies with security patches\n   - Improved input validation and sanitization\n   ```\n\n7. **Documentation Updates**\n   - Update API documentation with new endpoints\n   - Revise user documentation and guides\n   - Update installation and deployment instructions\n   - Review and update README.md\n   - Update migration guides if needed\n\n8. **Dependency Management**\n   ```bash\n   # Update and audit dependencies\n   npm audit fix\n   npm update\n   \n   # Python\n   pip-audit\n   pip freeze > requirements.txt\n   \n   # Review security vulnerabilities\n   npm audit\n   snyk test\n   ```\n\n9. **Build and Artifact Generation**\n   ```bash\n   # Clean build environment\n   npm run clean\n   rm -rf dist/ build/\n   \n   # Build production artifacts\n   npm run build\n   \n   # Verify build artifacts\n   ls -la dist/\n   \n   # Test built artifacts\n   npm run test:build\n   ```\n\n10. **Testing and Quality Assurance**\n    - Run comprehensive test suite\n    - Perform manual testing of critical features\n    - Execute regression testing\n    - Conduct user acceptance testing\n    - Validate in staging environment\n\n    ```bash\n    # Run all tests\n    npm test\n    npm run test:integration\n    npm run test:e2e\n    \n    # Check code coverage\n    npm run test:coverage\n    \n    # Performance testing\n    npm run test:performance\n    ```\n\n11. **Security and Compliance Verification**\n    - Run security scans and penetration testing\n    - Verify compliance with security standards\n    - Check for exposed secrets or credentials\n    - Validate data protection and privacy measures\n\n12. **Release Notes Preparation**\n    ```markdown\n    # Release Notes v1.2.3\n    \n    ##  What's New\n    - **Dark Mode**: Users can now switch to dark mode in settings\n    - **Enhanced Security**: Improved authentication with 2FA support\n    - **Performance**: 40% faster page load times\n    \n    ##  Improvements\n    - Better error messages for form validation\n    - Improved mobile responsiveness\n    - Enhanced accessibility features\n    \n    ##  Bug Fixes\n    - Fixed issue with file downloads in Safari\n    - Resolved memory leak in background tasks\n    - Fixed timezone display issues\n    \n    ##  Documentation\n    - Updated API documentation\n    - New user onboarding guide\n    - Enhanced troubleshooting section\n    \n    ##  Migration Guide\n    - No breaking changes in this release\n    - Automatic database migrations included\n    - See [Migration Guide](link) for details\n    ```\n\n13. **Release Tagging and Versioning**\n    ```bash\n    # Create annotated tag\n    git add .\n    git commit -m \"chore: prepare release v1.2.3\"\n    git tag -a v1.2.3 -m \"Release version 1.2.3\n    \n    Features:\n    - Dark mode support\n    - Enhanced authentication\n    \n    Bug fixes:\n    - Fixed file upload issues\n    - Resolved memory leaks\"\n    \n    # Push tag to remote\n    git push origin v1.2.3\n    git push origin release/v1.2.3\n    ```\n\n14. **Deployment Preparation**\n    - Prepare deployment scripts and configurations\n    - Update environment variables and secrets\n    - Plan deployment strategy (blue-green, rolling, canary)\n    - Set up monitoring and alerting for release\n    - Prepare rollback procedures\n\n15. **Staging Environment Validation**\n    ```bash\n    # Deploy to staging\n    ./deploy-staging.sh v1.2.3\n    \n    # Run smoke tests\n    npm run test:smoke:staging\n    \n    # Manual validation checklist\n    # [ ] User login/logout\n    # [ ] Core functionality\n    # [ ] New features\n    # [ ] Performance metrics\n    # [ ] Security checks\n    ```\n\n16. **Production Deployment Planning**\n    - Schedule deployment window\n    - Notify stakeholders and users\n    - Prepare maintenance mode if needed\n    - Set up deployment monitoring\n    - Plan communication strategy\n\n17. **Release Automation Setup**\n    ```yaml\n    # GitHub Actions Release Workflow\n    name: Release\n    \n    on:\n      push:\n        tags:\n          - 'v*'\n    \n    jobs:\n      release:\n        runs-on: ubuntu-latest\n        steps:\n          - uses: actions/checkout@v3\n          - name: Setup Node.js\n            uses: actions/setup-node@v3\n            with:\n              node-version: '18'\n          \n          - name: Install dependencies\n            run: npm ci\n          \n          - name: Run tests\n            run: npm test\n          \n          - name: Build\n            run: npm run build\n          \n          - name: Create Release\n            uses: actions/create-release@v1\n            env:\n              GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n            with:\n              tag_name: ${{ github.ref }}\n              release_name: Release ${{ github.ref }}\n              draft: false\n              prerelease: false\n    ```\n\n18. **Communication and Announcements**\n    - Prepare release announcement\n    - Update status page and documentation\n    - Notify customers and users\n    - Share on relevant communication channels\n    - Update social media and marketing materials\n\n19. **Post-Release Monitoring**\n    - Monitor application performance and errors\n    - Track user adoption of new features\n    - Monitor system metrics and alerts\n    - Collect user feedback and issues\n    - Prepare hotfix procedures if needed\n\n20. **Release Retrospective**\n    - Document lessons learned\n    - Review release process effectiveness\n    - Identify improvement opportunities\n    - Update release procedures\n    - Plan for next release cycle\n\n**Release Types and Considerations:**\n\n**Patch Release (1.2.3  1.2.4):**\n- Bug fixes only\n- No new features\n- Minimal testing required\n- Quick deployment\n\n**Minor Release (1.2.3  1.3.0):**\n- New features (backward compatible)\n- Enhanced functionality\n- Comprehensive testing\n- User communication needed\n\n**Major Release (1.2.3  2.0.0):**\n- Breaking changes\n- Significant new features\n- Migration guide required\n- Extended testing period\n- User training and support\n\n**Hotfix Release:**\n```bash\n# Emergency hotfix process\ngit checkout main\ngit pull origin main\ngit checkout -b hotfix/critical-bug-fix\n\n# Make minimal fix\ngit add .\ngit commit -m \"hotfix: fix critical security vulnerability\"\n\n# Fast-track testing and deployment\nnpm test\ngit tag -a v1.2.4-hotfix.1 -m \"Hotfix for critical security issue\"\ngit push origin hotfix/critical-bug-fix\ngit push origin v1.2.4-hotfix.1\n```\n\nRemember to:\n- Test everything thoroughly before release\n- Communicate clearly with all stakeholders\n- Have rollback procedures ready\n- Monitor the release closely after deployment\n- Document everything for future releases"
              },
              {
                "name": "/prime",
                "description": "Load project context by reading key documentation files and exploring project structure",
                "path": "plugins/all-commands/commands/prime.md",
                "frontmatter": {
                  "description": "Load project context by reading key documentation files and exploring project structure",
                  "category": "context-loading-priming",
                  "allowed-tools": "Bash(eza *), Read"
                },
                "content": "# Context Prime\n> Follow the instructions to understand the context of the project.\n\n## Run the following command\n\neza . --tree --git-ignore\n\n## Read the following files\n> Read the files below and nothing else.\n\nREADME.md\n.claude/commands/COMMANDS.md\nai_docs/AI_DOCS.md\nspecs/SPECS.md"
              },
              {
                "name": "/project-health-check",
                "description": "Analyze overall project health and metrics",
                "path": "plugins/all-commands/commands/project-health-check.md",
                "frontmatter": {
                  "description": "Analyze overall project health and metrics",
                  "category": "project-task-management",
                  "allowed-tools": "Bash(git *), Bash(gh *), Bash(npm *)"
                },
                "content": "# Project Health Check\n\nAnalyze overall project health and metrics\n\n## Instructions\n\n1. **Health Check Initialization**\n   - Verify tool connections (Linear, GitHub)\n   - Define evaluation period (default: last 30 days)\n   - Set health check criteria and thresholds\n   - Identify key metrics to evaluate\n\n2. **Multi-Dimensional Analysis**\n\n#### Code Health Metrics\n```bash\n# Code churn analysis\ngit log --format=format: --name-only --since=\"30 days ago\" | sort | uniq -c | sort -rg\n\n# Contributor activity\ngit shortlog -sn --since=\"30 days ago\"\n\n# Branch health\ngit for-each-ref --format='%(refname:short) %(committerdate:relative)' refs/heads/ | grep -E \"(months|years) ago\"\n\n# File complexity (if cloc available)\ncloc . --json --exclude-dir=node_modules,dist,build\n\n# Test coverage trends\nnpm test -- --coverage --json\n```\n\n#### Dependency Health\n```bash\n# Check for outdated dependencies\nnpm outdated --json\n\n# Security vulnerabilities\nnpm audit --json\n\n# License compliance\nnpx license-checker --json\n```\n\n#### Linear/Task Management Health\n```\n1. Sprint velocity trends\n2. Cycle time analysis\n3. Blocked task duration\n4. Backlog growth rate\n5. Bug vs feature ratio\n6. Task completion predictability\n```\n\n#### Team Health Indicators\n```\n1. PR review turnaround time\n2. Commit frequency distribution\n3. Work distribution balance\n4. On-call incident frequency\n5. Documentation updates\n```\n\n3. **Health Report Generation**\n\n```markdown\n# Project Health Report - [Project Name]\nGenerated: [Date]\n\n## Executive Summary\nOverall Health Score: [Score]/100 [ Healthy |  Needs Attention |  Critical]\n\n### Key Findings\n-  Strengths: [Top 3 positive indicators]\n-  Concerns: [Top 3 areas needing attention]\n-  Critical Issues: [Immediate action items]\n\n## Detailed Health Metrics\n\n1. **Delivery Health** (Score: [X]/100)\n| Metric | Current | Target | Status |\n|--------|---------|--------|--------|\n| Sprint Velocity | [X] pts | [Y] pts |  |\n| On-time Delivery | [X]% | 90% |  |\n| Cycle Time | [X] days | [Y] days |  |\n| Defect Rate | [X]% | <5% |  |\n\n2. **Code Quality** (Score: [X]/100)\n| Metric | Current | Target | Status |\n|--------|---------|--------|--------|\n| Test Coverage | [X]% | 80% |  |\n| Code Duplication | [X]% | <3% |  |\n| Complexity Score | [X] | <10 |  |\n| Security Issues | [X] | 0 |  |\n\n3. **Technical Debt** (Score: [X]/100)\n-  Total Debt Items: [Count]\n-  Debt Growth Rate: [+/-X% per sprint]\n-  Estimated Debt Work: [X days]\n-  Debt Impact: [Description]\n\n4. **Team Health** (Score: [X]/100)\n| Metric | Current | Target | Status |\n|--------|---------|--------|--------|\n| PR Review Time | [X] hrs | <4 hrs |  |\n| Knowledge Silos | [X] | 0 |  |\n| Work Balance | [Score] | >0.8 |  |\n| Burnout Risk | [Level] | Low |  |\n\n5. **Dependency Health** (Score: [X]/100)\n-  Outdated Dependencies: [X]/[Total]\n-  Security Vulnerabilities: [Critical: X, High: Y]\n-  License Issues: [Count]\n-  External Service Health: [Status]\n\n## Trend Analysis\n\n### Velocity Trend (Last 6 Sprints)\n```\nSprint 1:  40 pts\nSprint 2:  45 pts\nSprint 3:  50 pts\nSprint 4:  45 pts\nSprint 5:  38 pts\nSprint 6:  35 pts  Declining\n```\n\n### Bug Discovery Rate\n```\nWeek 1:  2 bugs\nWeek 2:  4 bugs\nWeek 3:  6 bugs  Increasing\nWeek 4:  8 bugs  Action needed\n```\n\n## Risk Assessment\n\n### High Priority Risks\n1. **Declining Velocity** \n   - Impact: High\n   - Likelihood: Confirmed\n   - Mitigation: Review sprint planning process\n\n2. **Security Vulnerabilities**\n   - Impact: Critical\n   - Count: [X] high, [Y] medium\n   - Action: Immediate patching required\n\n3. **Knowledge Concentration**\n   - Impact: Medium\n   - Bus Factor: 2\n   - Action: Implement pairing/documentation\n\n## Actionable Recommendations\n\n### Immediate Actions (This Week)\n1.  **Security**: Update [package] to fix critical vulnerability\n2.  **Quality**: Address top 3 bug-prone modules\n3.  **Team**: Schedule knowledge transfer for [critical component]\n\n### Short-term Improvements (This Sprint)\n1.  **Velocity**: Reduce scope to sustainable level\n2.  **Testing**: Increase coverage in [module] to 80%\n3.  **Documentation**: Update outdated docs for [feature]\n\n### Long-term Initiatives (This Quarter)\n1.  **Architecture**: Refactor [component] to reduce complexity\n2.  **Process**: Implement automated dependency updates\n3.  **Metrics**: Set up continuous health monitoring\n\n## Comparison with Previous Health Check\n\n| Category | Last Check | Current | Trend |\n|----------|------------|---------|-------|\n| Overall Score | 72/100 | 68/100 |  -4 |\n| Delivery | 80/100 | 75/100 |  -5 |\n| Code Quality | 70/100 | 72/100 |  +2 |\n| Technical Debt | 65/100 | 60/100 |  -5 |\n| Team Health | 75/100 | 70/100 |  -5 |\n```\n\n4. **Interactive Deep Dives**\n\nOffer focused analysis options:\n\n```\n\"Based on the health check, would you like to:\n1. Deep dive into declining velocity trends\n2. Generate security vulnerability fix plan\n3. Analyze technical debt hotspots\n4. Create team workload rebalancing plan\n5. Set up automated health monitoring\"\n```\n\n## Error Handling\n\n### Missing Linear Connection\n```\n\"Linear MCP not connected. Health check will be limited to:\n- Git/GitHub metrics only\n- No sprint velocity or task metrics\n- Manual input required for team data\n\nTo enable full health analysis:\n1. Install Linear MCP server\n2. Configure with API credentials\n3. Re-run health check\"\n```\n\n### Incomplete Data\n```\n\"Some metrics could not be calculated:\n- [List missing metrics]\n- [Explain impact on analysis]\n\nWould you like to:\n1. Proceed with available data\n2. Manually provide missing information\n3. Skip incomplete sections\"\n```\n\n## Customization Options\n\n### Threshold Configuration\n```yaml\n# health-check-config.yml\nthresholds:\n  velocity_variance: 20  # Acceptable % variance\n  test_coverage: 80      # Minimum coverage %\n  pr_review_time: 4      # Maximum hours\n  bug_rate: 5           # Maximum % of work\n  dependency_age: 90    # Days before \"outdated\"\n```\n\n### Custom Health Metrics\nAllow users to define additional metrics:\n```\n\"Add custom health metric:\n- Name: Customer Satisfaction\n- Data Source: [API/Manual/File]\n- Target Value: [>4.5/5]\n- Weight: [Impact on overall score]\"\n```\n\n## Export Options\n\n1. **Executive Summary** (PDF/Markdown)\n2. **Detailed Report** (HTML with charts)\n3. **Raw Metrics** (JSON/CSV)\n4. **Action Items** (Linear tasks/GitHub issues)\n5. **Monitoring Dashboard** (Grafana/Datadog format)\n\n## Automation Suggestions\n\n```\n\"Would you like me to:\n1. Schedule weekly health checks\n2. Set up alerts for critical metrics\n3. Create Linear tasks for action items\n4. Generate PR templates with health criteria\n5. Configure CI/CD health gates\"\n```\n\n## Best Practices\n\n1. **Regular Cadence**: Run health checks weekly/bi-weekly\n2. **Track Trends**: Compare with historical data\n3. **Action-Oriented**: Focus on fixable issues\n4. **Team Involvement**: Share results transparently\n5. **Continuous Improvement**: Refine metrics based on outcomes"
              },
              {
                "name": "/project-timeline-simulator",
                "description": "Simulate project outcomes with variable modeling, risk assessment, and resource optimization scenarios.",
                "path": "plugins/all-commands/commands/project-timeline-simulator.md",
                "frontmatter": {
                  "description": "Simulate project outcomes with variable modeling, risk assessment, and resource optimization scenarios.",
                  "category": "project-task-management",
                  "argument-hint": "Specify project timeline parameters",
                  "allowed-tools": "Bash(gh *), Read"
                },
                "content": "# Project Timeline Simulator\n\nSimulate project outcomes with variable modeling, risk assessment, and resource optimization scenarios.\n\n## Instructions\n\nYou are tasked with creating comprehensive project timeline simulations to optimize planning, resource allocation, and risk management. Follow this approach: **$ARGUMENTS**\n\n### 1. Prerequisites Assessment\n\n**Critical Project Context Validation:**\n\n- **Project Scope**: What specific project are you simulating timelines for?\n- **Key Variables**: What factors could significantly impact timeline outcomes?\n- **Resource Constraints**: What team, budget, and time limitations apply?\n- **Success Criteria**: How will you measure project success and timeline effectiveness?\n- **Risk Tolerance**: What level of schedule risk is acceptable?\n\n**If context is unclear, guide systematically:**\n\n```\nMissing Project Scope:\n\"What type of project needs timeline simulation?\n- Software Development: Feature development, platform migration, system redesign\n- Product Launch: New product development from concept to market\n- Business Initiative: Process improvement, organizational change, market expansion\n- Infrastructure Project: System upgrades, tool implementation, capacity expansion\n\nPlease specify project deliverables, stakeholders, and success criteria.\"\n\nMissing Key Variables:\n\"What factors could significantly impact your project timeline?\n- Resource Availability: Team capacity, skill availability, external dependencies\n- Technical Complexity: Unknown requirements, integration challenges, performance needs\n- External Dependencies: Vendor deliveries, regulatory approvals, partner coordination\n- Market Dynamics: Customer feedback, competitive pressure, business priority changes\"\n```\n\n### 2. Project Structure Modeling\n\n**Systematically map project components and dependencies:**\n\n#### Work Breakdown Structure (WBS) Analysis\n```\nProject Component Framework:\n\nPhase-Based Structure:\n- Discovery/Planning: Requirements gathering, design, architecture planning\n- Development/Implementation: Core building, integration, testing phases\n- Validation/Testing: Quality assurance, user acceptance, performance validation\n- Deployment/Launch: Release preparation, rollout, go-live activities\n- Stabilization/Optimization: Post-launch support, performance tuning, iteration\n\nFeature-Based Structure:\n- Core Features: Essential functionality for minimum viable product\n- Enhanced Features: Additional capabilities for competitive advantage\n- Integration Features: System connectivity and data synchronization\n- Quality Features: Security, performance, reliability, and maintainability\n\nSkill-Based Structure:\n- Frontend Development: User interface and experience implementation\n- Backend Development: Server logic, APIs, and data processing\n- Infrastructure/DevOps: Deployment, monitoring, and operational setup\n- Design/UX: User research, interface design, and usability testing\n- Quality Assurance: Testing strategy, automation, and validation\n```\n\n#### Dependency Mapping Framework\n```\nProject Dependency Analysis:\n\nSequential Dependencies:\n- Finish-to-Start: Task B cannot begin until Task A completes\n- Start-to-Start: Task B cannot start until Task A has started\n- Finish-to-Finish: Task B cannot finish until Task A finishes\n- Start-to-Finish: Task B cannot finish until Task A starts\n\nResource Dependencies:\n- Shared Resources: Team members working across multiple tasks\n- Skill Dependencies: Specialized expertise required for specific tasks\n- Tool Dependencies: Software, hardware, or platform availability\n- Budget Dependencies: Funding approval and expenditure timing\n\nExternal Dependencies:\n- Vendor Deliveries: Third-party software, services, or hardware\n- Regulatory Approvals: Compliance reviews and certification processes\n- Stakeholder Decisions: Business approvals and priority setting\n- Market Timing: Customer readiness and competitive positioning\n```\n\n### 3. Variable Modeling Framework\n\n**Systematically model factors affecting timeline outcomes:**\n\n#### Uncertainty Factor Analysis\n```\nTimeline Variable Categories:\n\nEffort Estimation Variables:\n- Task Complexity: Simple, moderate, complex, or unknown complexity\n- Team Experience: Expert, experienced, moderate, or novice skill levels\n- Requirements Clarity: Well-defined, partially defined, or evolving requirements\n- Technology Maturity: Proven, established, emerging, or experimental technology\n\nResource Variables:\n- Team Availability: Full-time, part-time, or shared allocation percentages\n- Skill Availability: In-house expertise, contractors, or training requirements\n- Infrastructure Readiness: Available, partially ready, or needs development\n- Budget Flexibility: Fixed, constrained, or adjustable funding levels\n\nExternal Variables:\n- Stakeholder Responsiveness: Fast, normal, or slow decision and feedback cycles\n- Market Stability: Stable, evolving, or rapidly changing requirements\n- Regulatory Environment: Clear, evolving, or uncertain compliance landscape\n- Competitive Pressure: Low, moderate, or high urgency for delivery\n```\n\n#### Variable Distribution Modeling\n```\nProbabilistic Timeline Estimation:\n\nThree-Point Estimation:\n- Optimistic Estimate: Best-case scenario with favorable conditions\n- Most Likely Estimate: Expected scenario with normal conditions\n- Pessimistic Estimate: Worst-case scenario with adverse conditions\n\nDistribution Types:\n- PERT Distribution: Beta distribution weighted toward most likely\n- Triangular Distribution: Linear probability between min, mode, max\n- Normal Distribution: Bell curve around mean with standard deviation\n- Log-Normal Distribution: Right-skewed for tasks with uncertainty\n\nMonte Carlo Simulation:\n- Random sampling from variable distributions\n- Thousands of simulation runs for statistical analysis\n- Confidence intervals for timeline predictions\n- Risk quantification and probability assessment\n```\n\n### 4. Scenario Generation Engine\n\n**Create comprehensive project timeline scenarios:**\n\n#### Scenario Development Framework\n```\nMulti-Dimensional Scenario Portfolio:\n\nBaseline Scenarios (40% of simulations):\n- Normal Resource Availability: Team at expected capacity and skills\n- Standard Complexity: Requirements and technical challenges as anticipated\n- Typical External Factors: Normal stakeholder responsiveness and market conditions\n- Expected Dependencies: Third-party deliveries and approvals on schedule\n\nOptimistic Scenarios (20% of simulations):\n- Enhanced Resource Availability: Additional team members or improved productivity\n- Reduced Complexity: Simpler requirements or technical solutions\n- Favorable External Factors: Fast stakeholder decisions and stable market\n- Accelerated Dependencies: Early vendor deliveries and quick approvals\n\nPessimistic Scenarios (25% of simulations):\n- Constrained Resources: Team availability issues or skill gaps\n- Increased Complexity: Scope creep or technical challenges\n- Adverse External Factors: Slow decisions or changing market conditions\n- Delayed Dependencies: Late vendor deliveries or approval delays\n\nDisruption Scenarios (15% of simulations):\n- Major Scope Changes: Significant requirement modifications mid-project\n- Team Disruptions: Key team member departures or organizational changes\n- Technology Disruptions: Platform changes or security requirements\n- Market Disruptions: Competitive pressure or business priority shifts\n```\n\n#### Critical Path Analysis\n- Identification of activities that directly impact project completion\n- Float/slack analysis for non-critical activities\n- Critical path vulnerability assessment under different scenarios\n- Resource optimization for critical path acceleration\n\n### 5. Risk Assessment and Impact Modeling\n\n**Comprehensive project risk evaluation:**\n\n#### Risk Identification Framework\n```\nProject Risk Categories:\n\nTechnical Risks:\n- Requirements Risk: Unclear, changing, or conflicting requirements\n- Technology Risk: Unproven technology or integration challenges\n- Performance Risk: Scalability, reliability, or efficiency concerns\n- Security Risk: Data protection and compliance requirements\n\nResource Risks:\n- Team Risk: Availability, skills, or productivity challenges\n- Budget Risk: Funding constraints or cost overruns\n- Time Risk: Schedule pressure or competing priorities\n- Vendor Risk: Third-party delivery or quality issues\n\nBusiness Risks:\n- Market Risk: Customer needs or competitive landscape changes\n- Stakeholder Risk: Changing priorities or approval delays\n- Regulatory Risk: Compliance requirements or policy changes\n- Strategic Risk: Business model or technology direction shifts\n```\n\n#### Risk Impact Simulation\n```\nRisk Effect Modeling:\n\nProbability Assessment:\n- High Probability (70-90%): Likely to occur based on historical data\n- Medium Probability (30-70%): Possible occurrence with mixed indicators\n- Low Probability (5-30%): Unlikely but possible based on rare events\n- Very Low Probability (<5%): Black swan events with major impact\n\nImpact Assessment:\n- Schedule Impact: Days or weeks of delay caused by risk realization\n- Resource Impact: Additional team members or budget required\n- Quality Impact: Feature cuts or technical debt accumulation\n- Business Impact: Revenue delay or customer satisfaction reduction\n\nRisk Mitigation Modeling:\n- Prevention Strategies: Actions to reduce risk probability\n- Mitigation Strategies: Plans to reduce risk impact if it occurs\n- Contingency Plans: Alternative approaches when risks materialize\n- Transfer Strategies: Insurance, contracts, or vendor risk sharing\n```\n\n### 6. Resource Optimization Simulation\n\n**Systematically optimize resource allocation across scenarios:**\n\n#### Resource Allocation Framework\n```\nMulti-Objective Resource Optimization:\n\nTeam Allocation Optimization:\n- Skill matching for maximum productivity and quality\n- Workload balancing to prevent burnout and bottlenecks\n- Cross-training opportunities for risk reduction\n- Contractor vs full-time employee optimization\n\nBudget Allocation Optimization:\n- Feature prioritization for maximum business value\n- Infrastructure investment for scalability and reliability\n- Tool and technology investment for productivity\n- Risk mitigation investment for schedule protection\n\nTimeline Optimization:\n- Parallel work stream identification and coordination\n- Critical path acceleration through resource concentration\n- Non-critical path scheduling for resource smoothing\n- Buffer allocation for uncertainty and risk management\n```\n\n#### Resource Constraint Modeling\n- Team capacity limitations and productivity variations\n- Budget restrictions and approval processes\n- Tool and infrastructure availability constraints\n- Skill development timelines and learning curves\n\n### 7. Decision Point Integration\n\n**Connect simulation insights to project management decisions:**\n\n#### Adaptive Project Management\n```\nSimulation-Driven Decision Framework:\n\nMilestone Decision Points:\n- Go/No-Go Decisions: Continue, pivot, or cancel based on progress\n- Resource Reallocation: Team or budget adjustments based on performance\n- Scope Adjustments: Feature prioritization based on timeline pressure\n- Risk Response: Mitigation strategy activation based on emerging risks\n\nEarly Warning Systems:\n- Schedule Variance Triggers: When actual progress deviates from plan\n- Resource Utilization Alerts: Team productivity or availability changes\n- Risk Indicator Monitoring: Early signals of potential problems\n- Quality Metric Tracking: Defect rates or technical debt accumulation\n\nAdaptive Strategies:\n- Agile Scope Management: Feature prioritization and MVP definition\n- Resource Flexibility: Team scaling and skill augmentation options\n- Timeline Buffer Management: Schedule contingency allocation and usage\n- Quality Trade-off Management: Technical debt vs delivery speed decisions\n```\n\n#### Project Success Optimization\n```\nSuccess Metric Optimization:\n\nTime-Based Success:\n- On-Time Delivery: Probability of meeting original schedule\n- Schedule Acceleration: Options for faster delivery with trade-offs\n- Milestone Achievement: Interim goal completion likelihood\n- Critical Path Protection: Schedule risk mitigation effectiveness\n\nQuality-Based Success:\n- Feature Completeness: Scope delivery against original requirements\n- Technical Quality: Code quality, performance, and maintainability\n- User Satisfaction: Usability and functionality meeting user needs\n- Business Value: ROI and strategic objective achievement\n\nResource-Based Success:\n- Budget Performance: Cost control and financial efficiency\n- Team Satisfaction: Developer experience and retention\n- Stakeholder Satisfaction: Communication and expectation management\n- Knowledge Transfer: Capability building and learning objectives\n```\n\n### 8. Output Generation and Recommendations\n\n**Present simulation insights in actionable project management format:**\n\n```\n## Project Timeline Simulation: [Project Name]\n\n### Simulation Summary\n- Scenarios Analyzed: [number and types of scenarios]\n- Timeline Range: [minimum to maximum completion estimates]\n- Success Probability: [likelihood of on-time, on-budget delivery]\n- Key Risk Factors: [primary threats to project success]\n\n### Timeline Predictions\n\n| Scenario Type | Completion Probability | Duration Range | Key Assumptions |\n|---------------|----------------------|----------------|-----------------|\n| Optimistic | 90% | 12-14 weeks | Ideal conditions |\n| Baseline | 70% | 16-20 weeks | Normal conditions |\n| Pessimistic | 50% | 22-28 weeks | Adverse conditions |\n| Worst Case | 10% | 30+ weeks | Multiple problems |\n\n### Critical Success Factors\n- Resource Availability: [team capacity and skill requirements]\n- Dependency Management: [external coordination and timing]\n- Risk Mitigation: [proactive risk prevention and response]\n- Scope Management: [feature prioritization and change control]\n\n### Recommended Strategy\n- Primary Approach: [optimal resource allocation and timeline strategy]\n- Contingency Plans: [backup strategies for different scenarios]\n- Early Warning Indicators: [metrics to monitor for course correction]\n- Decision Points: [key milestones for strategy adjustment]\n\n### Resource Optimization\n- Team Allocation: [optimal skill and capacity distribution]\n- Budget Distribution: [investment prioritization across features and risk mitigation]\n- Timeline Buffers: [schedule contingency allocation recommendations]\n- Quality Investment: [testing and technical debt management strategy]\n\n### Risk Management Plan\n- High-Priority Risks: [most critical threats and mitigation strategies]\n- Monitoring Strategy: [early detection and response systems]\n- Contingency Resources: [backup team and budget allocation]\n- Escalation Procedures: [decision triggers and stakeholder communication]\n```\n\n### 9. Continuous Project Learning\n\n**Establish ongoing simulation refinement and project improvement:**\n\n#### Performance Tracking\n- Actual vs predicted timeline performance measurement\n- Resource utilization efficiency and productivity assessment\n- Risk realization frequency and impact validation\n- Decision quality improvement over multiple projects\n\n#### Methodology Enhancement\n- Simulation accuracy improvement based on project outcomes\n- Estimation technique refinement and calibration\n- Risk model enhancement and validation\n- Team capability and productivity modeling improvement\n\n## Usage Examples\n\n```bash\n# Software development project simulation\n/project:project-timeline-simulator Simulate 6-month e-commerce platform development with 8-person team and Q4 launch deadline\n\n# Product launch timeline modeling\n/project:project-timeline-simulator Model mobile app launch timeline with user testing, app store approval, and marketing campaign coordination\n\n# Infrastructure migration simulation\n/project:project-timeline-simulator Simulate cloud migration project with legacy system dependencies and zero-downtime requirement\n\n# Agile release planning\n/project:project-timeline-simulator Model next quarter sprint planning with feature prioritization and team velocity uncertainty\n```\n\n## Quality Indicators\n\n- **Green**: Comprehensive scenarios, validated risk models, optimized resource allocation\n- **Yellow**: Multiple scenarios, basic risk assessment, reasonable resource planning\n- **Red**: Single timeline, minimal risk consideration, resource allocation not optimized\n\n## Common Pitfalls to Avoid\n\n- Planning fallacy: Systematic underestimation of time and resources required\n- Single-point estimates: Not modeling uncertainty and variability\n- Resource optimism: Assuming 100% utilization and no productivity variation\n- Risk blindness: Not identifying and planning for likely problems\n- Scope creep ignorance: Not accounting for requirement changes and additions\n- Static planning: Not adapting simulation based on actual project progress\n\nTransform project planning from hopeful guessing into systematic, evidence-based timeline optimization through comprehensive simulation and scenario analysis."
              },
              {
                "name": "/project-to-linear",
                "description": "Sync project structure to Linear workspace",
                "path": "plugins/all-commands/commands/project-to-linear.md",
                "frontmatter": {
                  "description": "Sync project structure to Linear workspace",
                  "category": "project-task-management",
                  "argument-hint": "Examine the current codebase structure and existing functionality"
                },
                "content": "# Project to Linear\n\nSync project structure to Linear workspace\n\n## Instructions\n\n1. **Analyze Project Requirements**\n   - Review the provided task description or project requirements: **$ARGUMENTS**\n   - Examine the current codebase structure and existing functionality\n   - Identify all major components and features needed\n   - Determine technical dependencies and constraints\n   - Assess the scope and complexity of the work\n\n2. **Understand User's Intent**\n   - Ask clarifying questions about:\n     - Project goals and objectives\n     - Priority levels for different features\n     - Timeline expectations\n     - Technical preferences or constraints\n     - Team structure (if relevant)\n     - Definition of done for tasks\n   - Confirm understanding of the requirements before proceeding\n\n3. **Check Linear Configuration**\n   - Verify if Linear MCP server is available and configured\n   - If not available, ask the user to:\n     - Install the Linear MCP server if not already installed\n     - Configure the Linear API key in their MCP settings\n     - Provide the default team ID or workspace information\n   - Test the connection by listing available projects\n\n4. **Project Setup in Linear**\n   - Ask the user if they want to:\n     - Use an existing Linear project (request project ID)\n     - Create a new project (ask for project name and description)\n   - For new projects, determine:\n     - Project type (Feature, Bug, Task, etc.)\n     - Project status (Planning, In Progress, etc.)\n     - Project lead or owner\n     - Any custom fields or labels to use\n\n5. **Generate Comprehensive Task List**\n   - Break down the project into logical phases:\n     - Planning and Design\n     - Core Implementation\n     - Testing and Quality Assurance\n     - Documentation\n     - Deployment and Release\n   - For each phase, create detailed tasks including:\n     - Clear, actionable task titles\n     - Detailed descriptions with acceptance criteria\n     - Technical specifications where relevant\n     - Estimated effort (if requested)\n     - Dependencies between tasks\n     - Priority levels (Critical, High, Medium, Low)\n\n6. **Create Task Hierarchy**\n   - Organize tasks into a proper hierarchy:\n     - Epic/Project level (if creating new project)\n     - Parent tasks for major features or components\n     - Subtasks for implementation details\n     - Related tasks for cross-cutting concerns\n   - Ensure logical grouping and dependencies\n\n7. **Add Task Details**\n   - For each task, include:\n     - **Title**: Clear, concise description\n     - **Description**: Detailed requirements and context\n     - **Acceptance Criteria**: Definition of done\n     - **Labels**: Appropriate tags (frontend, backend, testing, etc.)\n     - **Priority**: Based on user input and analysis\n     - **Estimates**: If sizing is requested\n     - **Assignee**: If team members are specified\n     - **Due Dates**: Based on timeline requirements\n\n8. **Create Tasks in Linear**\n   - Use the Linear MCP server to:\n     - Create the project (if new)\n     - Create all parent tasks first\n     - Create subtasks under appropriate parents\n     - Set up dependencies between tasks\n     - Apply labels and priorities\n     - Add any custom fields\n   - Provide feedback on each task created\n\n9. **Review and Refinement**\n   - Present a summary of all created tasks\n   - Show the task hierarchy and relationships\n   - Ask if any adjustments are needed:\n     - Task grouping or organization\n     - Priority changes\n     - Additional tasks or details\n     - Timeline adjustments\n   - Make any requested modifications\n\n10. **Provide Project Overview**\n    - Generate a summary including:\n      - Total number of tasks created\n      - Task breakdown by type/phase\n      - Critical path items\n      - Estimated timeline (if applicable)\n      - Link to the Linear project\n      - Next recommended actions\n\n## Example Task Structure\n\n```\nProject: User Dashboard Feature\n Planning & Design\n    Create UI/UX mockups\n    Define API requirements\n    Technical design document\n Backend Development\n    User API endpoints\n       GET /api/users endpoint\n       PUT /api/users/:id endpoint\n       User data validation\n    Dashboard data aggregation\n Frontend Development\n    Dashboard layout component\n    User profile widget\n    Activity feed component\n    Data visualization charts\n Testing\n    Unit tests for API\n    Frontend component tests\n    E2E dashboard tests\n    Performance testing\n Documentation & Deployment\n     API documentation\n     User guide\n     Production deployment\n```\n\n## Integration Notes\n\n- This command requires the Linear MCP server to be configured\n- If MCP is not available, provide the task list in a format that can be manually imported\n- Support batch operations to avoid rate limiting\n- Handle errors gracefully and provide clear feedback\n- Maintain task relationships and dependencies properly"
              },
              {
                "name": "/refactor-code",
                "description": "Intelligently refactor and improve code quality",
                "path": "plugins/all-commands/commands/refactor-code.md",
                "frontmatter": {
                  "description": "Intelligently refactor and improve code quality",
                  "category": "utilities-debugging",
                  "argument-hint": "1. **Pre-Refactoring Analysis**"
                },
                "content": "# Intelligently Refactor and Improve Code Quality\n\nIntelligently refactor and improve code quality\n\n## Instructions\n\nFollow this systematic approach to refactor code: **$ARGUMENTS**\n\n1. **Pre-Refactoring Analysis**\n   - Identify the code that needs refactoring and the reasons why\n   - Understand the current functionality and behavior completely\n   - Review existing tests and documentation\n   - Identify all dependencies and usage points\n\n2. **Test Coverage Verification**\n   - Ensure comprehensive test coverage exists for the code being refactored\n   - If tests are missing, write them BEFORE starting refactoring\n   - Run all tests to establish a baseline\n   - Document current behavior with additional tests if needed\n\n3. **Refactoring Strategy**\n   - Define clear goals for the refactoring (performance, readability, maintainability)\n   - Choose appropriate refactoring techniques:\n     - Extract Method/Function\n     - Extract Class/Component\n     - Rename Variable/Method\n     - Move Method/Field\n     - Replace Conditional with Polymorphism\n     - Eliminate Dead Code\n   - Plan the refactoring in small, incremental steps\n\n4. **Environment Setup**\n   - Create a new branch: `git checkout -b refactor/$ARGUMENTS`\n   - Ensure all tests pass before starting\n   - Set up any additional tooling needed (profilers, analyzers)\n\n5. **Incremental Refactoring**\n   - Make small, focused changes one at a time\n   - Run tests after each change to ensure nothing breaks\n   - Commit working changes frequently with descriptive messages\n   - Use IDE refactoring tools when available for safety\n\n6. **Code Quality Improvements**\n   - Improve naming conventions for clarity\n   - Eliminate code duplication (DRY principle)\n   - Simplify complex conditional logic\n   - Reduce method/function length and complexity\n   - Improve separation of concerns\n\n7. **Performance Optimizations**\n   - Identify and eliminate performance bottlenecks\n   - Optimize algorithms and data structures\n   - Reduce unnecessary computations\n   - Improve memory usage patterns\n\n8. **Design Pattern Application**\n   - Apply appropriate design patterns where beneficial\n   - Improve abstraction and encapsulation\n   - Enhance modularity and reusability\n   - Reduce coupling between components\n\n9. **Error Handling Improvement**\n   - Standardize error handling approaches\n   - Improve error messages and logging\n   - Add proper exception handling\n   - Enhance resilience and fault tolerance\n\n10. **Documentation Updates**\n    - Update code comments to reflect changes\n    - Revise API documentation if interfaces changed\n    - Update inline documentation and examples\n    - Ensure comments are accurate and helpful\n\n11. **Testing Enhancements**\n    - Add tests for any new code paths created\n    - Improve existing test quality and coverage\n    - Remove or update obsolete tests\n    - Ensure tests are still meaningful and effective\n\n12. **Static Analysis**\n    - Run linting tools to catch style and potential issues\n    - Use static analysis tools to identify problems\n    - Check for security vulnerabilities\n    - Verify code complexity metrics\n\n13. **Performance Verification**\n    - Run performance benchmarks if applicable\n    - Compare before/after metrics\n    - Ensure refactoring didn't degrade performance\n    - Document any performance improvements\n\n14. **Integration Testing**\n    - Run full test suite to ensure no regressions\n    - Test integration with dependent systems\n    - Verify all functionality works as expected\n    - Test edge cases and error scenarios\n\n15. **Code Review Preparation**\n    - Review all changes for quality and consistency\n    - Ensure refactoring goals were achieved\n    - Prepare clear explanation of changes made\n    - Document benefits and rationale\n\n16. **Documentation of Changes**\n    - Create a summary of refactoring changes\n    - Document any breaking changes or new patterns\n    - Update project documentation if needed\n    - Explain benefits and reasoning for future reference\n\n17. **Deployment Considerations**\n    - Plan deployment strategy for refactored code\n    - Consider feature flags for gradual rollout\n    - Prepare rollback procedures\n    - Set up monitoring for the refactored components\n\nRemember: Refactoring should preserve external behavior while improving internal structure. Always prioritize safety over speed, and maintain comprehensive test coverage throughout the process."
              },
              {
                "name": "/release",
                "description": "Prepare a new release by updating changelog, version, and documentation",
                "path": "plugins/all-commands/commands/release.md",
                "frontmatter": {
                  "description": "Prepare a new release by updating changelog, version, and documentation",
                  "category": "ci-deployment",
                  "allowed-tools": "Edit, Read, Bash(git *)"
                },
                "content": "Update CHANGELOG.md with changes since the last version increase. Check our README.md for any necessary changes. Check the scope of changes since the last release and increase our version number as appropriate."
              },
              {
                "name": "/remove",
                "description": "Safely remove a task from the orchestration system, updating all references and dependencies.",
                "path": "plugins/all-commands/commands/remove.md",
                "frontmatter": {
                  "description": "Safely remove a task from the orchestration system, updating all references and dependencies.",
                  "category": "workflow-orchestration",
                  "allowed-tools": "Bash(git *), Read"
                },
                "content": "# Orchestration Remove Command\n\nSafely remove a task from the orchestration system, updating all references and dependencies.\n\n## Usage\n\n```\n/orchestration/remove TASK-ID [options]\n```\n\n## Description\n\nRemoves a task completely from the orchestration system, handling all dependencies, references, and related documentation. Provides impact analysis before removal and ensures system consistency.\n\n## Basic Commands\n\n### Remove Single Task\n```\n/orchestration/remove TASK-003\n```\nShows impact analysis and confirms before removal.\n\n### Force Remove\n```\n/orchestration/remove TASK-003 --force\n```\nSkips confirmation (use with caution).\n\n### Dry Run\n```\n/orchestration/remove TASK-003 --dry-run\n```\nShows what would be affected without making changes.\n\n## Impact Analysis\n\nBefore removal, the system analyzes:\n\n```\nTask Removal Impact Analysis: TASK-003\n======================================\n\nTask Details:\n- Title: JWT token validation\n- Status: in_progress\n- Location: /tasks/in_progress/TASK-003-jwt-validation.md\n\nDependencies:\n- Blocks: TASK-005 (User profile API)\n- Blocks: TASK-007 (Session management)\n- Depends on: None\n\nReferences Found:\n- MASTER-COORDINATION.md: Line 45 (Wave 1 tasks)\n- EXECUTION-TRACKER.md: Active task count\n- TASK-005: Lists TASK-003 as dependency\n- TASK-007: Lists TASK-003 as dependency\n\nGit History:\n- 2 commits reference this task\n- Branch: feature/jwt-auth\n\nWarning: This task has downstream dependencies!\n\nProceed with removal? [y/N]\n```\n\n## Removal Process\n\n### 1. Update Dependent Tasks\n```\nUpdating dependent tasks:\n- TASK-005: Removing dependency on TASK-003\n  New status: Ready to start (no blockers)\n  \n- TASK-007: Removing dependency on TASK-003\n  Warning: Still blocked by TASK-009\n```\n\n### 2. Update Tracking Files\n```yaml\n# TASK-STATUS-TRACKER.yaml updates:\nstatus_history:\n  TASK-003: [REMOVED - archived to .removed/]\n  \ncurrent_status_summary:\n  in_progress: [TASK-003 removed from list]\n\nremoval_log:\n  - task_id: TASK-003\n    removed_at: \"2024-03-15T16:00:00Z\"\n    removed_by: \"user\"\n    reason: \"Requirement changed\"\n    final_status: \"in_progress\"\n```\n\n### 3. Update Coordination Documents\n```\nUpdates applied:\n MASTER-COORDINATION.md - Removed from Wave 1\n EXECUTION-TRACKER.md - Updated task counts\n TASK-DEPENDENCIES.yaml - Removed all references\n Dependency graph regenerated\n```\n\n## Options\n\n### Archive Instead of Delete\n```\n/orchestration/remove TASK-003 --archive\n```\nMoves to `.removed/` directory instead of deleting.\n\n### Remove Multiple Tasks\n```\n/orchestration/remove TASK-003,TASK-005,TASK-008\n```\nAnalyzes and removes multiple tasks in dependency order.\n\n### Remove by Pattern\n```\n/orchestration/remove --pattern \"oauth-*\"\n```\nRemoves all tasks matching pattern.\n\n### Cascade Removal\n```\n/orchestration/remove TASK-003 --cascade\n```\nAlso removes tasks that depend on this task.\n\n## Handling Special Cases\n\n### Task with Commits\n```\nWarning: TASK-003 has associated commits:\n- abc123: \"feat(auth): implement JWT validation\"\n- def456: \"test(auth): add JWT tests\"\n\nOptions:\n[1] Keep commits, remove task only\n[2] Add removal note to commit messages\n[3] Cancel removal\n```\n\n### Task in QA/Completed\n```\nWarning: TASK-003 is in 'completed' status\n\nThis usually means work was done. Consider:\n[1] Archive task instead of removing\n[2] Document why it's being removed\n[3] Check if commits should be reverted\n```\n\n### Critical Path Task\n```\nERROR: TASK-003 is on the critical path!\n\nRemoving this task will impact project timeline:\n- Current completion: 5 days\n- After removal: 7 days (due to replanning)\n\nOverride with --force-critical\n```\n\n## Removal Strategies\n\n### Soft Remove (Default)\n```\n/orchestration/remove TASK-003\n```\n- Archives task file\n- Updates all references\n- Logs removal reason\n- Preserves git history\n\n### Hard Remove\n```\n/orchestration/remove TASK-003 --hard\n```\n- Deletes task file permanently\n- Removes all traces\n- Updates git tracking\n- No recovery possible\n\n### Replace Remove\n```\n/orchestration/remove TASK-003 --replace-with TASK-015\n```\n- Transfers dependencies to new task\n- Updates all references\n- Maintains continuity\n\n## Undo Capabilities\n\n### Recent Removal\n```\n/orchestration/remove --undo-last\n```\nRestores the most recently removed task.\n\n### Restore from Archive\n```\n/orchestration/remove --restore TASK-003\n```\nRestores archived task with all references.\n\n## Examples\n\n### Example 1: Obsolete Feature\n```\n/orchestration/remove TASK-008 --reason \"Feature descoped\"\n\nRemoving TASK-008: OAuth provider integration\n- No dependencies\n- No commits yet\n- Safe to remove\n\nTask removed successfully.\n```\n\n### Example 2: Duplicate Task\n```\n/orchestration/remove TASK-012 --replace-with TASK-005\n\nRemoving duplicate: TASK-012\nTransferring to: TASK-005\n- Dependencies transferred: 2\n- References updated: 4\n\nDuplicate removed, TASK-005 updated.\n```\n\n### Example 3: Changed Requirements\n```\n/orchestration/remove TASK-003,TASK-004,TASK-005 --reason \"Auth system redesigned\"\n\nRemoving authentication task group:\n- 3 tasks to remove\n- 2 have commits (will archive)\n- 5 dependent tasks need updates\n\nProceed? [y/N]\n```\n\n## Audit Trail\n\nAll removals are logged:\n```yaml\n# .orchestration-audit.yaml\nremovals:\n  - task_id: TASK-003\n    removed_at: \"2024-03-15T16:00:00Z\"\n    removed_by: \"user-id\"\n    reason: \"Requirement changed\"\n    status_at_removal: \"in_progress\"\n    dependencies_affected: [\"TASK-005\", \"TASK-007\"]\n    commits_preserved: [\"abc123\", \"def456\"]\n    archived_to: \".removed/2024-03-15/TASK-003/\"\n```\n\n## Best Practices\n\n1. **Always Check Dependencies**: Review impact before removing\n2. **Document Reason**: Provide clear removal reason\n3. **Archive Important Work**: Use --archive for completed work\n4. **Update Team**: Notify about critical removals\n5. **Review Commits**: Check if code needs reverting\n\n## Integration\n\n### With Other Commands\n```\n# First check status\n/orchestration/status --task TASK-003\n\n# Then remove if needed\n/orchestration/remove TASK-003\n```\n\n### Bulk Operations\n```\n# Find and remove all on-hold tasks older than 30 days\n/orchestration/find --status on_hold --older-than 30d | /orchestration/remove --batch\n```\n\n## Safety Features\n\n- Confirmation required (unless --force)\n- Dependencies checked and warned\n- Commits preserved by default\n- Audit trail maintained\n- Undo capability for recent removals\n\n## Notes\n\n- Removed tasks are archived for 30 days by default\n- Git commits are never automatically reverted\n- Dependencies are gracefully handled\n- System consistency is maintained throughout"
              },
              {
                "name": "/report",
                "description": "Generate comprehensive reports on task execution, progress, and metrics.",
                "path": "plugins/all-commands/commands/report.md",
                "frontmatter": {
                  "description": "Generate comprehensive reports on task execution, progress, and metrics.",
                  "category": "workflow-orchestration"
                },
                "content": "# Task Report Command\n\nGenerate comprehensive reports on task execution, progress, and metrics.\n\n## Usage\n\n```\n/task-report [report-type] [options]\n```\n\n## Description\n\nCreates detailed reports for project management, sprint reviews, and performance analysis. Supports multiple report types and output formats.\n\n## Report Types\n\n### Executive Summary\n```\n/task-report executive\n```\nHigh-level overview for stakeholders with key metrics and progress.\n\n### Sprint Report\n```\n/task-report sprint --date 03_15_2024\n```\nDetailed sprint progress with burndown charts and velocity.\n\n### Daily Standup\n```\n/task-report standup\n```\nWhat was completed, in progress, and blocked.\n\n### Performance Report\n```\n/task-report performance --period week\n```\nTeam and individual performance metrics.\n\n### Dependency Report\n```\n/task-report dependencies\n```\nVisual dependency graph and bottleneck analysis.\n\n## Output Examples\n\n### Executive Summary Report\n```\nEXECUTIVE SUMMARY - Authentication System Project\n================================================\nReport Date: 2024-03-15\nProject Start: 2024-03-13\nDuration: 3 days (60% complete)\n\nKEY METRICS\n-----------\n Total Tasks: 24\n Completed: 12 (50%)\n In Progress: 3 (12.5%)\n Blocked: 2 (8.3%)\n Remaining: 7 (29.2%)\n\nTIMELINE\n--------\n Original Estimate: 5 days\n Current Projection: 5.5 days\n Risk Level: Low\n\nHIGHLIGHTS\n----------\n Core authentication API completed\n Database schema migrated\n Unit tests passing (98% coverage)\n\nBLOCKERS\n--------\n Payment integration waiting on external API\n UI components need design approval\n\nNEXT MILESTONES\n--------------\n Complete JWT implementation (Today)\n Integration testing (Tomorrow)\n Security audit (Day 4)\n```\n\n### Sprint Burndown Report\n```\n/task-report burndown --sprint current\n```\n```\nSPRINT BURNDOWN - Sprint 24\n===========================\n\nTasks Remaining by Day:\nDay 1:  24\nDay 2:      20 \nDay 3:          15 (TODAY)\nDay 4:              10 (projected)\nDay 5:                  5  (projected)\n\nVelocity Metrics:\n- Average: 4.5 tasks/day\n- Yesterday: 5 tasks\n- Today: 3 tasks (in progress)\n\nRisk Assessment: ON TRACK\n```\n\n### Performance Report\n```\nTEAM PERFORMANCE REPORT - Week 11\n=================================\n\nBy Agent:\n\n Agent            Completed  Avg Time  Quality  Efficiency \n\n dev-frontend        8      3.2h       95%       125%    \n dev-backend         6      4.1h       98%       110%    \n test-developer      4      2.8h       100%      115%    \n\n\nBy Task Type:\n- Features: 12 completed (avg 3.8h)\n- Bugfixes: 4 completed (avg 1.5h)\n- Tests: 8 completed (avg 2.2h)\n\nQuality Metrics:\n- First-time pass rate: 88%\n- Rework required: 2 tasks\n- Blocked time: 4.5 hours total\n```\n\n## Customization Options\n\n### Time Period\n```\n/task-report summary --from 2024-03-01 --to 2024-03-15\n/task-report summary --last 7d\n/task-report summary --this-month\n```\n\n### Specific Project\n```\n/task-report sprint --project authentication_system\n```\n\n### Format Options\n```\n/task-report executive --format markdown\n/task-report executive --format html\n/task-report executive --format pdf\n```\n\n### Include/Exclude\n```\n/task-report summary --include completed,qa\n/task-report summary --exclude on_hold\n```\n\n## Specialized Reports\n\n### Critical Path Analysis\n```\n/task-report critical-path\n```\nShows tasks that directly impact completion time.\n\n### Bottleneck Analysis\n```\n/task-report bottlenecks\n```\nIdentifies tasks causing delays.\n\n### Resource Utilization\n```\n/task-report resources\n```\nShows agent allocation and availability.\n\n### Risk Assessment\n```\n/task-report risks\n```\nIdentifies potential delays and issues.\n\n## Visualization Options\n\n### Gantt Chart\n```\n/task-report gantt --weeks 2\n```\n\n### Dependency Graph\n```\n/task-report dependencies --visual\n```\n\n### Status Flow\n```\n/task-report flow --animated\n```\n\n## Automated Reports\n\n### Schedule Reports\n```\n/task-report schedule daily-standup --at \"9am\"\n/task-report schedule weekly-summary --every friday\n```\n\n### Email Reports\n```\n/task-report executive --email team@company.com\n```\n\n## Comparison Reports\n\n### Sprint Comparison\n```\n/task-report compare --sprint 23 24\n```\n\n### Week over Week\n```\n/task-report trends --weeks 4\n```\n\n## Examples\n\n### Example 1: Morning Status\n```\n/task-report standup --format slack\n```\nGenerates Slack-formatted standup report.\n\n### Example 2: Sprint Review\n```\n/task-report sprint --include-velocity --include-burndown\n```\nComprehensive sprint metrics for review meeting.\n\n### Example 3: Blocker Focus\n```\n/task-report blockers --show-dependencies --show-resolution\n```\nDeep dive into what's blocking progress.\n\n## Integration Features\n\n### Export to Tools\n```\n/task-report export-jira\n/task-report export-asana\n/task-report export-github\n```\n\n### API Endpoints\n```\n/task-report api --generate-endpoint\n```\nCreates API endpoint for external access.\n\n## Best Practices\n\n1. **Daily Reviews**: Run standup report each morning\n2. **Weekly Summaries**: Generate performance reports on Fridays\n3. **Sprint Planning**: Use velocity trends for estimation\n4. **Stakeholder Updates**: Schedule automated executive summaries\n\n## Report Components\n\nEach report can include:\n- Summary statistics\n- Timeline visualization\n- Task lists by status\n- Agent performance\n- Dependency analysis\n- Risk assessment\n- Recommendations\n- Historical trends\n\n## Notes\n\n- Reports use data from all TASK-STATUS-TRACKER.yaml files\n- Completed tasks are included in historical metrics\n- Time calculations use business hours by default\n- All times shown in local timezone\n- Charts require terminal unicode support"
              },
              {
                "name": "/repro-issue",
                "description": "Reproduce a specific issue by creating a failing test case",
                "path": "plugins/all-commands/commands/repro-issue.md",
                "frontmatter": {
                  "description": "Reproduce a specific issue by creating a failing test case",
                  "category": "code-analysis-testing",
                  "argument-hint": "<issue_description>",
                  "allowed-tools": "Read, Write, Edit"
                },
                "content": "Repro issue $ARGUMENTS in a failing test"
              },
              {
                "name": "/resume",
                "description": "Resume work on existing task orchestrations after session loss or context switch.",
                "path": "plugins/all-commands/commands/resume.md",
                "frontmatter": {
                  "description": "Resume work on existing task orchestrations after session loss or context switch.",
                  "category": "workflow-orchestration",
                  "allowed-tools": "Bash(git *), Read"
                },
                "content": "# Orchestration Resume Command\n\nResume work on existing task orchestrations after session loss or context switch.\n\n## Usage\n\n```\n/orchestration/resume [options]\n```\n\n## Description\n\nRestores full context for active orchestrations, showing current progress, identifying next actions, and providing all necessary information to continue work seamlessly.\n\n## Basic Commands\n\n### List Active Orchestrations\n```\n/orchestration/resume\n```\nShows all orchestrations with active (non-completed) tasks.\n\n### Resume Specific Orchestration\n```\n/orchestration/resume --date 03_15_2024 --project auth_system\n```\nLoads complete context for a specific orchestration.\n\n### Resume Most Recent\n```\n/orchestration/resume --latest\n```\nAutomatically resumes the most recently active orchestration.\n\n## Output Format\n\n### Orchestration List View\n```\nActive Task Orchestrations\n==========================\n\n1. 03_15_2024/authentication_system\n   Started: 3 days ago | Progress: 65% | Active Tasks: 3\n    Focus: JWT implementation, OAuth integration\n\n2. 03_14_2024/payment_processing  \n   Started: 4 days ago | Progress: 40% | Active Tasks: 2\n    Focus: Stripe webhooks, refund handling\n\n3. 03_12_2024/admin_dashboard\n   Started: 1 week ago | Progress: 85% | Active Tasks: 1\n    Focus: Final testing and deployment\n\nSelect orchestration to resume: [1-3] or use --date and --project\n```\n\n### Detailed Resume View\n```\nResuming: authentication_system (03_15_2024)\n============================================\n\n## Current Status Summary\n- Total Tasks: 24 (12 completed, 3 in progress, 2 on hold, 7 todos)\n- Time Elapsed: 3 days\n- Estimated Remaining: 2 days\n\n## Tasks In Progress\n\n Task ID   Title                       Agent          Duration     \n\n TASK-003  JWT token validation        dev-backend    2.5h         \n TASK-007  OAuth provider setup        dev-frontend   1h           \n TASK-011  Integration tests           test-dev       30m          \n\n\n## Blocked Tasks (Require Attention)\n- TASK-005: User profile API - Blocked by TASK-003 (JWT validation)\n- TASK-009: OAuth callback handling - Waiting for provider credentials\n\n## Next Available Tasks (Ready to Start)\n1. TASK-013: Password reset flow (4h, frontend)\n   Files: src/auth/reset.tsx, src/api/auth.ts\n   \n2. TASK-014: Session management (3h, backend)\n   Files: src/services/session.ts, src/middleware/auth.ts\n\n## Recent Git Activity\n- feature/jwt-auth: 2 commits behind, last commit 2h ago\n- feature/oauth-setup: clean, last commit 1h ago\n\n## Quick Actions\n[1] Show TASK-003 details (current focus)\n[2] Pick up TASK-013 (password reset)\n[3] View dependency graph\n[4] Show recent commits\n[5] Generate status report\n```\n\n## Context Recovery Features\n\n### Task Context\n```\n/orchestration/resume --task TASK-003\n```\nShows:\n- Full task description and requirements\n- Implementation progress and notes\n- Related files with recent changes\n- Test requirements and status\n- Dependencies and blockers\n\n### File Context\n```\n/orchestration/resume --show-files\n```\nLists all files mentioned in active tasks with:\n- Last modified time\n- Current git status\n- Which tasks reference them\n\n### Dependency Context\n```\n/orchestration/resume --deps\n```\nShows dependency graph focused on active tasks.\n\n## Working State Recovery\n\n### Git State Summary\n```\n## Git Working State\nCurrent Branch: feature/jwt-auth\nStatus: 2 files modified, 1 untracked\n\nModified Files:\n- src/auth/jwt.ts (related to TASK-003)\n- tests/auth.test.ts (related to TASK-003)\n\nUntracked:\n- src/auth/jwt.config.ts (new file for TASK-003)\n\nRecommendation: Commit current changes before switching tasks\n```\n\n### Last Session Summary\n```\n## Last Session (2 hours ago)\n- Completed: TASK-002 (Database schema)\n- Started: TASK-003 (JWT validation)\n- Commits: 2 (feat: add user auth schema, test: auth unit tests)\n- Next planned: Continue TASK-003, then TASK-005\n```\n\n## Filtering Options\n\n### By Status\n```\n/orchestration/resume --show in_progress,on_hold\n```\n\n### By Date Range\n```\n/orchestration/resume --since \"last week\"\n```\n\n### By Completion\n```\n/orchestration/resume --incomplete  # < 50% done\n/orchestration/resume --nearly-done  # > 80% done\n```\n\n## Integration Features\n\n### Direct Task Pickup\n```\n/orchestration/resume --pickup TASK-013\n```\nAutomatically:\n1. Shows task details\n2. Moves to in_progress\n3. Shows relevant files\n4. Creates feature branch if needed\n\n### Status Check Integration\n```\n/orchestration/resume --with-status\n```\nIncludes full status report with resume context.\n\n### Commit History\n```\n/orchestration/resume --commits 5\n```\nShows last 5 commits related to the orchestration.\n\n## Quick Resume Patterns\n\n### Morning Standup\n```\n/orchestration/resume --latest --with-status\n```\nPerfect for daily standups - shows what you were working on and current state.\n\n### Context Switch\n```\n/orchestration/resume --save-state\n```\nSaves current working state before switching to another orchestration.\n\n### Team Handoff\n```\n/orchestration/resume --handoff\n```\nGenerates detailed handoff notes for another developer.\n\n## Examples\n\n### Example 1: Quick Continue\n```\n/orchestration/resume --latest --pickup-where-left-off\n```\nResumes exactly where you stopped, showing the in-progress task.\n\n### Example 2: Monday Morning\n```\n/orchestration/resume --since friday --show-completed\n```\nShows what was done Friday and what's next for Monday.\n\n### Example 3: Multiple Projects\n```\n/orchestration/resume --all --summary\n```\nQuick overview of all active orchestrations.\n\n## State Persistence\n\nThe command reads from:\n- EXECUTION-TRACKER.md for progress metrics\n- TASK-STATUS-TRACKER.yaml for current state\n- Task files for detailed context\n- Git for working directory state\n\n## Best Practices\n\n1. **Use at Session Start**: Run `/orchestration/resume` when starting work\n2. **Save State**: Use `--save-state` before extended breaks\n3. **Check Dependencies**: Review blocked tasks that may now be unblocked\n4. **Commit Regularly**: Keep git state aligned with task progress\n\n## Notes\n\n- Automatically detects uncommitted changes related to tasks\n- Suggests next actions based on dependencies and priorities\n- Integrates with git worktrees if in use\n- Preserves task history for full context"
              },
              {
                "name": "/retrospective-analyzer",
                "description": "Analyze team retrospectives for insights",
                "path": "plugins/all-commands/commands/retrospective-analyzer.md",
                "frontmatter": {
                  "description": "Analyze team retrospectives for insights",
                  "category": "team-collaboration"
                },
                "content": "# Retrospective Analyzer\n\nAnalyze team retrospectives for insights\n\n## Instructions\n\n1. **Retrospective Setup**\n   - Identify sprint to analyze (default: most recent)\n   - Check Linear MCP connection for sprint data\n   - Define retrospective format preference\n   - Set analysis time range\n\n2. **Sprint Data Collection**\n\n#### Quantitative Metrics\n```\nFrom Linear/Project Management:\n- Planned vs completed story points\n- Sprint velocity and capacity\n- Cycle time and lead time\n- Escaped defects count\n- Unplanned work percentage\n\nFrom Git/GitHub:\n- Commit frequency and distribution\n- PR merge time statistics  \n- Code review turnaround\n- Build success rate\n- Deployment frequency\n```\n\n#### Qualitative Data Sources\n```\n1. PR review comments sentiment\n2. Commit message patterns\n3. Slack conversations (if available)\n4. Previous retrospective action items\n5. Support ticket trends\n```\n\n3. **Automated Analysis**\n\n#### Sprint Performance Analysis\n```markdown\n# Sprint [Name] Retrospective Analysis\n\n## Sprint Overview\n- Duration: [Start] to [End]\n- Team Size: [Number] members\n- Sprint Goal: [Description]\n- Goal Achievement: [Yes/Partial/No]\n\n## Key Metrics Summary\n\n### Delivery Metrics\n| Metric | Target | Actual | Variance |\n|--------|--------|--------|----------|\n| Velocity | [X] pts | [Y] pts | [+/-Z]% |\n| Completion Rate | 90% | [X]% | [+/-Y]% |\n| Defect Rate | <5% | [X]% | [+/-Y]% |\n| Unplanned Work | <20% | [X]% | [+/-Y]% |\n\n### Process Metrics\n| Metric | This Sprint | Previous | Trend |\n|--------|-------------|----------|-------|\n| Avg PR Review Time | [X] hrs | [Y] hrs | [/] |\n| Avg Cycle Time | [X] days | [Y] days | [/] |\n| CI/CD Success Rate | [X]% | [Y]% | [/] |\n| Team Happiness | [X]/5 | [Y]/5 | [/] |\n```\n\n#### Pattern Recognition\n```markdown\n## Identified Patterns\n\n### Positive Patterns \n1. **Improved Code Review Speed**\n   - Average review time decreased by 30%\n   - Correlation with new review guidelines\n   - Recommendation: Document and maintain process\n\n2. **Consistent Daily Progress**\n   - Even commit distribution throughout sprint\n   - No last-minute rush\n   - Indicates good sprint planning\n\n### Concerning Patterns \n1. **Monday Deploy Failures**\n   - 60% of failed deployments on Mondays\n   - Possible cause: Weekend changes not tested\n   - Action: Implement Monday morning checks\n\n2. **Increasing Scope Creep**\n   - 35% unplanned work (up from 20%)\n   - Source: Urgent customer requests\n   - Action: Review sprint commitment process\n```\n\n4. **Interactive Retrospective Facilitation**\n\n#### Pre-Retrospective Report\n```markdown\n# Pre-Retrospective Insights\n\n## Data-Driven Discussion Topics\n\n### 1. What Went Well \nBased on the data, these areas showed improvement:\n-  Code review efficiency (+30%)\n-  Test coverage increase (+5%)\n-  Zero critical bugs in production\n-  All team members contributed evenly\n\n**Suggested Discussion Questions:**\n- What specific changes led to faster reviews?\n- How can we maintain zero critical bugs?\n- What made work distribution successful?\n\n### 2. What Didn't Go Well\nData indicates challenges in these areas:\n-  Sprint velocity miss (-15%)\n-  High unplanned work (35%)\n-  3 rollbacks required\n-  Team overtime increased\n\n**Suggested Discussion Questions:**\n- What caused the velocity miss?\n- How can we better handle unplanned work?\n- What led to the rollbacks?\n\n### 3. Action Items from Data\nRecommended improvements based on patterns:\n1. Implement feature flags for safer deployments\n2. Create unplanned work budget in sprint planning\n3. Add integration tests for [problem area]\n4. Schedule mid-sprint check-ins\n```\n\n#### Live Retrospective Support\n```\nDuring the retrospective, I can help with:\n\n1. **Fact Checking**: \n   \"Actually, our velocity was 45 points, not 50\"\n\n2. **Pattern Context**:\n   \"This is the 3rd sprint with Monday deploy issues\"\n\n3. **Historical Comparison**:\n   \"Last time we had similar issues, we tried X\"\n\n4. **Action Item Tracking**:\n   \"From last retro, we completed 4/6 action items\"\n```\n\n5. **Retrospective Output Formats**\n\n#### Standard Retrospective Summary\n```markdown\n# Sprint [X] Retrospective Summary\n\n## Participants\n[List of attendees]\n\n## What Went Well\n- [Categorized list with vote counts]\n- Supporting data: [Metrics]\n\n## What Didn't Go Well  \n- [Categorized list with vote counts]\n- Root cause analysis: [Details]\n\n## Action Items\n| Action | Owner | Due Date | Success Criteria |\n|--------|-------|----------|------------------|\n| [Action 1] | [Name] | [Date] | [Measurable outcome] |\n| [Action 2] | [Name] | [Date] | [Measurable outcome] |\n\n## Experiments for Next Sprint\n1. [Experiment description]\n   - Hypothesis: [What we expect]\n   - Measurement: [How we'll know]\n   - Review date: [When to assess]\n\n## Team Health Pulse\n- Energy Level: [Rating]/5\n- Clarity: [Rating]/5\n- Confidence: [Rating]/5\n- Key Quote: \"[Notable team sentiment]\"\n```\n\n#### Trend Analysis Report\n```markdown\n# Retrospective Trends Analysis\n\n## Recurring Themes (Last 5 Sprints)\n\n### Persistent Challenges\n1. **Deployment Issues** (4/5 sprints)\n   - Root cause still unresolved\n   - Recommended escalation\n\n2. **Estimation Accuracy** (5/5 sprints)\n   - Consistent 20% overrun\n   - Needs systematic approach\n\n### Improving Areas\n1. **Communication** (Improving for 3 sprints)\n2. **Code Quality** (Steady improvement)\n\n### Success Patterns\n1. **Pair Programming** (Mentioned positively 5/5)\n2. **Daily Standups** (Effective format found)\n```\n\n6. **Action Item Generation**\n\n#### Smart Action Items\n```\nBased on retrospective discussion, here are SMART action items:\n\n1. **Reduce Deploy Failures**\n   - Specific: Implement smoke tests for Monday deploys\n   - Measurable: <5% failure rate\n   - Assignable: DevOps team\n   - Relevant: Addresses 60% of failures\n   - Time-bound: By next sprint\n\n2. **Improve Estimation**\n   - Specific: Use planning poker for all stories\n   - Measurable: <20% variance from estimates\n   - Assignable: Scrum Master facilitates\n   - Relevant: Addresses velocity misses\n   - Time-bound: Start next sprint planning\n```\n\n## Error Handling\n\n### No Linear Data\n```\n\"Linear MCP not connected. Using git data only.\n\nMissing insights:\n- Story point analysis\n- Task-level metrics\n- Team capacity data\n\nWould you like to:\n1. Proceed with git data only\n2. Manually input sprint metrics\n3. Connect Linear and retry\"\n```\n\n### Incomplete Sprint\n```\n\"Sprint appears to be in progress. \n\nCurrent analysis based on:\n- [X] days of [Y] total\n- [Z]% work completed\n\nRecommendation: Run full analysis after sprint ends\nProceed with partial analysis? [Y/N]\"\n```\n\n## Advanced Features\n\n### Sentiment Analysis\n```python\n# Analyze PR comments and commit messages\nsentiment_indicators = {\n    'positive': ['fixed', 'improved', 'resolved', 'great'],\n    'negative': ['bug', 'issue', 'broken', 'failed', 'frustrated'],\n    'neutral': ['updated', 'changed', 'modified']\n}\n\n# Generate sentiment report\n\"Team Sentiment Analysis:\n- Positive indicators: 65%\n- Negative indicators: 25%  \n- Neutral: 10%\n\nTrend: Improving from last sprint (was 55% positive)\"\n```\n\n### Predictive Insights\n```\n\"Based on current patterns:\n\n Risk Predictions:\n- 70% chance of velocity miss if unplanned work continues\n- Deploy failures likely to increase without intervention\n\n Opportunity Predictions:\n- 15% velocity gain possible with proposed process changes\n- Team happiness likely to improve with workload balancing\"\n```\n\n### Experiment Tracking\n```\n\"Previous Experiments Results:\n\n1. 'No Meeting Fridays' (Sprint 12-14)\n   - Result: 20% productivity increase\n   - Recommendation: Make permanent\n\n2. 'Pair Programming for Complex Tasks' (Sprint 15)\n   - Result: 50% fewer defects\n   - Recommendation: Continue with guidelines\"\n```\n\n## Integration Options\n\n1. **Linear**: Create action items as tasks\n2. **Slack**: Post summary to team channel\n3. **Confluence**: Export formatted retrospective page\n4. **GitHub**: Create issues for technical debt items\n5. **Calendar**: Schedule action item check-ins\n\n## Best Practices\n\n1. **Data Before Discussion**: Review metrics first\n2. **Focus on Patterns**: Look for recurring themes\n3. **Action-Oriented**: Every insight needs action\n4. **Time-boxed**: Keep retrospective focused\n5. **Follow-up**: Track action item completion\n6. **Celebrate Wins**: Acknowledge improvements\n7. **Safe Space**: Encourage honest feedback"
              },
              {
                "name": "/rollback-deploy",
                "description": "Rollback deployment to previous version",
                "path": "plugins/all-commands/commands/rollback-deploy.md",
                "frontmatter": {
                  "description": "Rollback deployment to previous version",
                  "category": "ci-deployment",
                  "argument-hint": "1. **Incident Assessment and Decision**",
                  "allowed-tools": "Bash(npm *)"
                },
                "content": "# Rollback Deploy Command\n\nRollback deployment to previous version\n\n## Instructions\n\nFollow this systematic rollback procedure: **$ARGUMENTS**\n\n1. **Incident Assessment and Decision**\n   - Assess the severity and impact of the current deployment issues\n   - Determine if rollback is necessary or if forward fix is better\n   - Identify affected systems, users, and business functions\n   - Consider data integrity and consistency implications\n   - Document the decision rationale and timeline\n\n2. **Emergency Response Setup**\n   ```bash\n   # Activate incident response team\n   # Set up communication channels\n   # Notify stakeholders immediately\n   \n   # Example emergency notification\n   echo \" ROLLBACK INITIATED\n   Issue: Critical performance degradation after v1.3.0 deployment\n   Action: Rolling back to v1.2.9\n   ETA: 15 minutes\n   Impact: Temporary service interruption possible\n   Status channel: #incident-rollback-202401\"\n   ```\n\n3. **Pre-Rollback Safety Checks**\n   ```bash\n   # Verify current production version\n   curl -s https://api.example.com/version\n   kubectl get deployments -o wide\n   \n   # Check system status\n   curl -s https://api.example.com/health | jq .\n   \n   # Identify target rollback version\n   git tag --sort=-version:refname | head -5\n   \n   # Verify rollback target exists and is deployable\n   git show v1.2.9 --stat\n   ```\n\n4. **Database Considerations**\n   ```bash\n   # Check for database migrations since last version\n   ./check-migrations.sh v1.2.9 v1.3.0\n   \n   # If migrations exist, plan database rollback\n   # WARNING: Database rollbacks can cause data loss\n   # Consider forward fix instead if migrations are present\n   \n   # Create database backup before rollback\n   ./backup-database.sh \"pre-rollback-$(date +%Y%m%d-%H%M%S)\"\n   ```\n\n5. **Traffic Management Preparation**\n   ```bash\n   # Prepare to redirect traffic\n   # Option 1: Maintenance page\n   ./enable-maintenance-mode.sh\n   \n   # Option 2: Load balancer management\n   ./drain-traffic.sh --gradual\n   \n   # Option 3: Circuit breaker activation\n   ./activate-circuit-breaker.sh\n   ```\n\n6. **Container/Kubernetes Rollback**\n   ```bash\n   # Kubernetes rollback\n   kubectl rollout history deployment/app-deployment\n   kubectl rollout undo deployment/app-deployment\n   \n   # Or rollback to specific revision\n   kubectl rollout undo deployment/app-deployment --to-revision=3\n   \n   # Monitor rollback progress\n   kubectl rollout status deployment/app-deployment --timeout=300s\n   \n   # Verify pods are running\n   kubectl get pods -l app=your-app\n   ```\n\n7. **Docker Swarm Rollback**\n   ```bash\n   # List service history\n   docker service ps app-service --no-trunc\n   \n   # Rollback to previous version\n   docker service update --rollback app-service\n   \n   # Or update to specific image\n   docker service update --image app:v1.2.9 app-service\n   \n   # Monitor rollback\n   docker service ps app-service\n   ```\n\n8. **Traditional Deployment Rollback**\n   ```bash\n   # Blue-Green deployment rollback\n   ./switch-to-blue.sh  # or green, depending on current\n   \n   # Rolling deployment rollback\n   ./deploy-version.sh v1.2.9 --rolling\n   \n   # Symlink-based rollback\n   ln -sfn /releases/v1.2.9 /current\n   sudo systemctl restart app-service\n   ```\n\n9. **Load Balancer and CDN Updates**\n   ```bash\n   # Update load balancer to point to old version\n   aws elbv2 modify-target-group --target-group-arn $TG_ARN --targets Id=old-instance\n   \n   # Clear CDN cache if needed\n   aws cloudfront create-invalidation --distribution-id $DIST_ID --paths \\\"/*\\\"\n   \n   # Update DNS if necessary (last resort, has propagation delay)\n   # aws route53 change-resource-record-sets ...\n   ```\n\n10. **Configuration Rollback**\n    ```bash\\n    # Rollback configuration files\\n    git checkout v1.2.9 -- config/\\n    \\n    # Restart services with old configuration\\n    sudo systemctl restart nginx\\n    sudo systemctl restart app-service\\n    \\n    # Rollback environment variables\\n    ./restore-env-vars.sh v1.2.9\\n    \\n    # Update feature flags\\n    ./update-feature-flags.sh --disable-new-features\\n    ```\\n\\n11. **Database Rollback (if necessary)**\\n    ```sql\\n    -- EXTREME CAUTION: Can cause data loss\\n    \\n    -- Check migration status\\n    SELECT * FROM schema_migrations ORDER BY version DESC LIMIT 5;\\n    \\n    -- Rollback specific migrations (framework dependent)\\n    -- Rails: rake db:migrate:down VERSION=20240115120000\\n    -- Django: python manage.py migrate app_name 0001\\n    -- Node.js: npm run migrate:down\\n    \\n    -- Verify database state\\n    SHOW TABLES;\\n    DESCRIBE critical_table;\\n    ```\\n\\n12. **Service Health Validation**\\n    ```bash\\n    # Health check script\\n    #!/bin/bash\\n    \\n    echo \\\"Validating rollback...\\\"\\n    \\n    # Check application health\\n    if curl -f -s https://api.example.com/health > /dev/null; then\\n        echo \\\" Health check passed\\\"\\n    else\\n        echo \\\" Health check failed\\\"\\n        exit 1\\n    fi\\n    \\n    # Check critical endpoints\\n    endpoints=(\\n        \\\"/api/users/me\\\"\\n        \\\"/api/auth/status\\\"\\n        \\\"/api/data/latest\\\"\\n    )\\n    \\n    for endpoint in \\\"${endpoints[@]}\\\"; do\\n        if curl -f -s \\\"https://api.example.com$endpoint\\\" > /dev/null; then\\n            echo \\\" $endpoint working\\\"\\n        else\\n            echo \\\" $endpoint failed\\\"\\n        fi\\n    done\\n    ```\\n\\n13. **Performance and Metrics Validation**\\n    ```bash\\n    # Check response times\\n    curl -w \\\"Response time: %{time_total}s\\\\n\\\" -s -o /dev/null https://api.example.com/\\n    \\n    # Monitor error rates\\n    tail -f /var/log/app/error.log | head -20\\n    \\n    # Check system resources\\n    top -bn1 | head -10\\n    free -h\\n    df -h\\n    \\n    # Validate database connectivity\\n    mysql -u app -p -e \\\"SELECT 1;\\\"\\n    ```\\n\\n14. **Traffic Restoration**\\n    ```bash\\n    # Gradually restore traffic\\n    ./restore-traffic.sh --gradual\\n    \\n    # Disable maintenance mode\\n    ./disable-maintenance-mode.sh\\n    \\n    # Re-enable circuit breakers\\n    ./deactivate-circuit-breaker.sh\\n    \\n    # Monitor traffic patterns\\n    ./monitor-traffic.sh --duration 300\\n    ```\\n\\n15. **Monitoring and Alerting**\\n    ```bash\\n    # Enable enhanced monitoring during rollback\\n    ./enable-enhanced-monitoring.sh\\n    \\n    # Watch key metrics\\n    watch -n 10 'curl -s https://api.example.com/metrics | jq .'\\n    \\n    # Monitor logs in real-time\\n    tail -f /var/log/app/*.log | grep -E \\\"ERROR|WARN|EXCEPTION\\\"\\n    \\n    # Check application metrics\\n    # - Response times\\n    # - Error rates\\n    # - User sessions\\n    # - Database performance\\n    ```\\n\\n16. **User Communication**\\n    ```markdown\\n    ## Service Update - Rollback Completed\\n    \\n    **Status:**  Service Restored\\n    **Time:** 2024-01-15 15:45 UTC\\n    **Duration:** 12 minutes of degraded performance\\n    \\n    **What Happened:**\\n    We identified performance issues with our latest release and \\n    performed a rollback to ensure optimal service quality.\\n    \\n    **Current Status:**\\n    - All services operating normally\\n    - Performance metrics back to baseline\\n    - No data loss occurred\\n    \\n    **Next Steps:**\\n    We're investigating the root cause and will provide updates \\n    on our status page.\\n    ```\\n\\n17. **Post-Rollback Validation**\\n    ```bash\\n    # Extended monitoring period\\n    ./monitor-extended.sh --duration 3600  # 1 hour\\n    \\n    # Run integration tests\\n    npm run test:integration:production\\n    \\n    # Check user-reported issues\\n    ./check-support-tickets.sh --since \\\"1 hour ago\\\"\\n    \\n    # Validate business metrics\\n    ./check-business-metrics.sh\\n    ```\\n\\n18. **Documentation and Reporting**\\n    ```markdown\\n    # Rollback Incident Report\\n    \\n    **Incident ID:** INC-2024-0115-001\\n    **Rollback Version:** v1.2.9 (from v1.3.0)\\n    **Start Time:** 2024-01-15 15:30 UTC\\n    **End Time:** 2024-01-15 15:42 UTC\\n    **Total Duration:** 12 minutes\\n    \\n    **Timeline:**\\n    - 15:25 - Performance degradation detected\\n    - 15:30 - Rollback decision made\\n    - 15:32 - Traffic drained\\n    - 15:35 - Rollback initiated\\n    - 15:38 - Rollback completed\\n    - 15:42 - Traffic fully restored\\n    \\n    **Impact:**\\n    - 12 minutes of degraded performance\\n    - ~5% of users experienced slow responses\\n    - No data loss or corruption\\n    - No security implications\\n    \\n    **Root Cause:**\\n    Memory leak in new feature causing performance degradation\\n    \\n    **Lessons Learned:**\\n    - Need better performance testing in staging\\n    - Improve monitoring for memory usage\\n    - Consider canary deployments for major releases\\n    ```\\n\\n19. **Cleanup and Follow-up**\\n    ```bash\\n    # Clean up failed deployment artifacts\\n    docker image rm app:v1.3.0\\n    \\n    # Update deployment status\\n    ./update-deployment-status.sh \\\"rollback-completed\\\"\\n    \\n    # Reset feature flags if needed\\n    ./reset-feature-flags.sh\\n    \\n    # Schedule post-incident review\\n    ./schedule-postmortem.sh --date \\\"2024-01-16 10:00\\\"\\n    ```\\n\\n20. **Prevention and Improvement**\\n    - Analyze what went wrong with the deployment\\n    - Improve testing and validation procedures\\n    - Enhance monitoring and alerting\\n    - Update rollback procedures based on learnings\\n    - Consider implementing canary deployments\\n\\n**Rollback Decision Matrix:**\\n\\n| Issue Severity | Data Impact | Time to Fix | Decision |\\n|---------------|-------------|-------------|----------|\\n| Critical | None | > 30 min | Rollback |\\n| High | Minor | > 60 min | Rollback |\\n| Medium | None | > 2 hours | Consider rollback |\\n| Low | None | Any | Forward fix |\\n\\n**Emergency Rollback Script Template:**\\n```bash\\n#!/bin/bash\\nset -e\\n\\n# Emergency rollback script\\nPREVIOUS_VERSION=\\\"${1:-v1.2.9}\\\"\\nCURRENT_VERSION=$(curl -s https://api.example.com/version)\\n\\necho \\\" EMERGENCY ROLLBACK\\\"\\necho \\\"From: $CURRENT_VERSION\\\"\\necho \\\"To: $PREVIOUS_VERSION\\\"\\necho \\\"\\\"\\n\\n# Confirm rollback\\nread -p \\\"Proceed with rollback? (yes/no): \\\" confirm\\nif [ \\\"$confirm\\\" != \\\"yes\\\" ]; then\\n    echo \\\"Rollback cancelled\\\"\\n    exit 1\\nfi\\n\\n# Execute rollback\\necho \\\"Starting rollback...\\\"\\nkubectl set image deployment/app-deployment app=app:$PREVIOUS_VERSION\\nkubectl rollout status deployment/app-deployment --timeout=300s\\n\\n# Validate\\necho \\\"Validating rollback...\\\"\\nsleep 30\\ncurl -f https://api.example.com/health\\n\\necho \\\" Rollback completed successfully\\\"\\n```\\n\\nRemember: Rollbacks should be a last resort. Always consider forward fixes first, especially when database migrations are involved."
              },
              {
                "name": "/rsi",
                "description": "Read project commands and documentation to optimize AI-assisted development process",
                "path": "plugins/all-commands/commands/rsi.md",
                "frontmatter": {
                  "description": "Read project commands and documentation to optimize AI-assisted development process",
                  "category": "context-loading-priming",
                  "allowed-tools": "Read, LS, Glob"
                },
                "content": "Please list and read all files in `.claude/commands/`, and also the CLAUDE.md, README.md, ROADMAP.md, and PHILOSOPHY.md in project root. Feel free to check out any other files to if useful. Let's see if we can further optimise and streamline this AI-assisted dev process!"
              },
              {
                "name": "/run-ci",
                "description": "Run CI checks and fix any errors until all tests pass",
                "path": "plugins/all-commands/commands/run-ci.md",
                "frontmatter": {
                  "description": "Run CI checks and fix any errors until all tests pass",
                  "category": "ci-deployment",
                  "allowed-tools": "Bash, Edit, Read, Glob"
                },
                "content": "Run CI checks for the project and fix any errors until all tests pass.\n\n## Process:\n\n1. **Detect CI System**:\n   - Check for CI configuration files:\n     - `.github/workflows/*.yml` (GitHub Actions)\n     - `.gitlab-ci.yml` (GitLab CI)\n     - `.circleci/config.yml` (CircleCI)\n     - `Jenkinsfile` (Jenkins)\n     - `.travis.yml` (Travis CI)\n     - `bitbucket-pipelines.yml` (Bitbucket)\n\n2. **Detect Build System**:\n   - JavaScript/TypeScript: package.json scripts\n   - Python: Makefile, tox.ini, setup.py, pyproject.toml\n   - Go: Makefile, go.mod\n   - Rust: Cargo.toml\n   - Java: pom.xml, build.gradle\n   - Other: Look for common CI scripts\n\n3. **Run CI Commands**:\n   - Check for CI scripts: `ci`, `test`, `check`, `validate`, `verify`\n   - Common script locations:\n     - `./scripts/ci.sh`, `./ci.sh`, `./run-tests.sh`\n     - Package manager scripts (npm/yarn/pnpm run test)\n     - Make targets (make test, make ci)\n   - Activate virtual environments if needed (Python, Ruby, etc.)\n\n4. **Fix Errors**:\n   - Analyze error output\n   - Fix code issues, test failures, or configuration problems\n   - Re-run CI checks after each fix\n\n5. **Common CI Tasks**:\n   - Linting/formatting\n   - Type checking\n   - Unit tests\n   - Integration tests\n   - Build verification\n   - Documentation generation\n\n## Examples:\n- JavaScript: `npm test` or `npm run ci`\n- Python: `make test` or `pytest` or `tox`\n- Go: `go test ./...` or `make test`\n- Rust: `cargo test`\n- Generic: `./ci.sh` or `make ci`\n\nContinue fixing issues and re-running until all CI checks pass."
              },
              {
                "name": "/security-audit",
                "description": "Perform comprehensive security assessment",
                "path": "plugins/all-commands/commands/security-audit.md",
                "frontmatter": {
                  "description": "Perform comprehensive security assessment",
                  "category": "security-audit"
                },
                "content": "# Security Audit Command\n\nPerform comprehensive security assessment\n\n## Instructions\n\nPerform a systematic security audit following these steps:\n\n1. **Environment Setup**\n   - Identify the technology stack and framework\n   - Check for existing security tools and configurations\n   - Review deployment and infrastructure setup\n\n2. **Dependency Security**\n   - Scan all dependencies for known vulnerabilities\n   - Check for outdated packages with security issues\n   - Review dependency sources and integrity\n   - Use appropriate tools: `npm audit`, `pip check`, `cargo audit`, etc.\n\n3. **Authentication & Authorization**\n   - Review authentication mechanisms and implementation\n   - Check for proper session management\n   - Verify authorization controls and access restrictions\n   - Examine password policies and storage\n\n4. **Input Validation & Sanitization**\n   - Check all user input validation and sanitization\n   - Look for SQL injection vulnerabilities\n   - Identify potential XSS (Cross-Site Scripting) issues\n   - Review file upload security and validation\n\n5. **Data Protection**\n   - Identify sensitive data handling practices\n   - Check encryption implementation for data at rest and in transit\n   - Review data masking and anonymization practices\n   - Verify secure communication protocols (HTTPS, TLS)\n\n6. **Secrets Management**\n   - Scan for hardcoded secrets, API keys, and passwords\n   - Check for proper secrets management practices\n   - Review environment variable security\n   - Identify exposed configuration files\n\n7. **Error Handling & Logging**\n   - Review error messages for information disclosure\n   - Check logging practices for security events\n   - Verify sensitive data is not logged\n   - Assess error handling robustness\n\n8. **Infrastructure Security**\n   - Review containerization security (Docker, etc.)\n   - Check CI/CD pipeline security\n   - Examine cloud configuration and permissions\n   - Assess network security configurations\n\n9. **Security Headers & CORS**\n   - Check security headers implementation\n   - Review CORS configuration\n   - Verify CSP (Content Security Policy) settings\n   - Examine cookie security attributes\n\n10. **Reporting**\n    - Document all findings with severity levels (Critical, High, Medium, Low)\n    - Provide specific remediation steps for each issue\n    - Include code examples and file references\n    - Create an executive summary with key recommendations\n\nUse automated security scanning tools when available and provide manual review for complex security patterns."
              },
              {
                "name": "/security-hardening",
                "description": "Harden application security configuration",
                "path": "plugins/all-commands/commands/security-hardening.md",
                "frontmatter": {
                  "description": "Harden application security configuration",
                  "category": "security-audit"
                },
                "content": "# Security Hardening\n\nHarden application security configuration\n\n## Instructions\n\n1. **Security Assessment and Baseline**\n   - Conduct comprehensive security audit of current application\n   - Identify potential vulnerabilities and attack vectors\n   - Analyze authentication and authorization mechanisms\n   - Review data handling and storage practices\n   - Assess network security and communication protocols\n\n2. **Authentication and Authorization Hardening**\n   - Implement strong password policies and multi-factor authentication\n   - Configure secure session management with proper timeouts\n   - Set up role-based access control (RBAC) with least privilege principle\n   - Implement JWT security best practices or secure session tokens\n   - Configure account lockout and brute force protection\n\n3. **Input Validation and Sanitization**\n   - Implement comprehensive input validation for all user inputs\n   - Set up SQL injection prevention with parameterized queries\n   - Configure XSS protection with proper output encoding\n   - Implement CSRF protection with tokens and SameSite cookies\n   - Set up file upload security with type validation and sandboxing\n\n4. **Secure Communication**\n   - Configure HTTPS with strong TLS/SSL certificates\n   - Implement HTTP Strict Transport Security (HSTS)\n   - Set up secure API communication with proper authentication\n   - Configure certificate pinning for mobile applications\n   - Implement end-to-end encryption for sensitive data transmission\n\n5. **Data Protection and Encryption**\n   - Implement encryption at rest for sensitive data\n   - Configure secure key management and rotation\n   - Set up database encryption and access controls\n   - Implement proper secrets management (avoid hardcoded secrets)\n   - Configure secure backup and recovery procedures\n\n6. **Security Headers and Policies**\n   - Configure Content Security Policy (CSP) headers\n   - Set up X-Frame-Options and X-Content-Type-Options headers\n   - Implement Referrer Policy and Feature Policy headers\n   - Configure CORS policies with proper origin validation\n   - Set up security.txt file for responsible disclosure\n\n7. **Dependency and Supply Chain Security**\n   - Audit and update all dependencies to latest secure versions\n   - Implement dependency vulnerability scanning\n   - Configure automated security updates for critical dependencies\n   - Set up software composition analysis (SCA) tools\n   - Implement dependency pinning and integrity checks\n\n8. **Infrastructure Security**\n   - Configure firewall rules and network segmentation\n   - Implement intrusion detection and prevention systems\n   - Set up secure logging and monitoring\n   - Configure secure container images and runtime security\n   - Implement infrastructure as code security scanning\n\n9. **Application Security Controls**\n   - Implement rate limiting and DDoS protection\n   - Set up web application firewall (WAF) rules\n   - Configure secure error handling without information disclosure\n   - Implement proper logging for security events\n   - Set up security monitoring and alerting\n\n10. **Security Testing and Validation**\n    - Conduct penetration testing and vulnerability assessments\n    - Implement automated security testing in CI/CD pipeline\n    - Set up static application security testing (SAST)\n    - Configure dynamic application security testing (DAST)\n    - Create security incident response plan and procedures\n    - Document security controls and compliance requirements"
              },
              {
                "name": "/session-learning-capture",
                "description": "Capture and document session learnings",
                "path": "plugins/all-commands/commands/session-learning-capture.md",
                "frontmatter": {
                  "description": "Capture and document session learnings",
                  "category": "team-collaboration",
                  "allowed-tools": "Glob"
                },
                "content": "# Session Learning Capture\n\nCapture and document session learnings\n\n## Instructions\n\n1. **Identify Session Learnings**\n   - Review if during your session:\n     - You learned something new about the project\n     - I corrected you on a specific implementation detail\n     - I corrected source code you generated\n     - You struggled to find specific information and had to infer details about the project\n     - You lost track of the project structure and had to look up information in the source code\n\n2. **Determine Appropriate File**\n   - Choose the right file for the information:\n     - `CLAUDE.md` for shared context that should be version controlled\n     - `CLAUDE.local.md` for private notes and developer-specific settings\n     - Subdirectory `CLAUDE.md` for component-specific information\n\n3. **Memory File Types Summary**\n   - **Shared Project Memory (`CLAUDE.md`):**\n     - Located in the repository root or any working directory\n     - Checked into version control for team-wide context sharing\n     - Loaded recursively from the current directory up to the root\n   - **Local, Non-Shared Memory (`CLAUDE.local.md`):**\n     - Placed alongside or above working files, excluded from version control\n     - Stores private, developer-specific notes and settings\n     - Loaded recursively like `CLAUDE.md`\n   - **On-Demand Subdirectory Loading:**\n     - `CLAUDE.md` files in child folders are loaded only when editing files in those subfolders\n     - Prevents unnecessary context bloat\n   - **Global User Memory (`~/.claude/CLAUDE.md`):**\n     - Acts as a personal, cross-project memory\n     - Automatically merged into sessions under your home directory\n\n4. **Update Memory Files**\n   - Add relevant, non-obvious information that should be persisted\n   - Ensure proper placement based on component relevance:\n     - UI-specific information  `apps/[project]-ui/CLAUDE.md`\n     - API-specific information  `apps/[project]-api/CLAUDE.md`\n     - Infrastructure information  `cdk/CLAUDE.md` or `infrastructure/CLAUDE.md`\n   - This ensures important knowledge is retained and available in future sessions\n\n## Credit\n\nThis command is based on the work of Thomas Landgraf: https://thomaslandgraf.substack.com/p/claude-codes-memory-working-with"
              },
              {
                "name": "/setup-automated-releases",
                "description": "Setup automated release workflows",
                "path": "plugins/all-commands/commands/setup-automated-releases.md",
                "frontmatter": {
                  "description": "Setup automated release workflows",
                  "category": "ci-deployment",
                  "argument-hint": "Specify release automation settings"
                },
                "content": "# Setup Automated Releases\n\nSetup automated release workflows\n\n## Instructions\n\nSet up automated releases following industry best practices:\n\n1. **Analyze Repository Structure**\n   - Detect project type (Node.js, Python, Go, etc.)\n   - Check for existing CI/CD workflows\n   - Identify current versioning approach\n   - Review existing release processes\n\n2. **Create Version Tracking**\n   - For Node.js: Use package.json version field\n   - For Python: Use __version__ in __init__.py or pyproject.toml\n   - For Go: Use version in go.mod\n   - For others: Create version.txt file\n   - Ensure version follows semantic versioning (MAJOR.MINOR.PATCH)\n\n3. **Set Up Conventional Commits**\n   - Create CONTRIBUTING.md with commit conventions:\n     - `feat:` for new features (minor bump)\n     - `fix:` for bug fixes (patch bump)\n     - `feat!:` or `BREAKING CHANGE:` for breaking changes (major bump)\n     - `docs:`, `chore:`, `style:`, `refactor:`, `test:` for non-releasing changes\n   - Include examples and guidelines for each type\n\n4. **Create Pull Request Template**\n   - Add `.github/pull_request_template.md`\n   - Include conventional commit reminder\n   - Add checklist for common requirements\n   - Reference contributing guidelines\n\n5. **Create Release Workflow**\n   - Add `.github/workflows/release.yml`:\n     - Trigger on push to main branch\n     - Analyze commits since last release\n     - Determine version bump type\n     - Update version in appropriate file(s)\n     - Generate release notes from commits\n     - Update CHANGELOG.md\n     - Create git tag\n     - Create GitHub Release\n     - Attach distribution artifacts\n   - Include manual trigger option for forced releases\n\n6. **Create PR Validation Workflow**\n   - Add `.github/workflows/pr-check.yml`:\n     - Validate PR title follows conventional format\n     - Check commit messages\n     - Provide feedback on version impact\n     - Run tests and quality checks\n\n7. **Configure GitHub Release Notes**\n   - Create `.github/release.yml`\n   - Define categories for different change types\n   - Configure changelog exclusions\n   - Set up contributor recognition\n\n8. **Update Documentation**\n   - Add release badges to README:\n     - Current version badge\n     - Latest release badge\n     - Build status badge\n   - Document release process\n   - Add link to CONTRIBUTING.md\n   - Explain version bump rules\n\n9. **Set Up Changelog Management**\n   - Ensure CHANGELOG.md follows Keep a Changelog format\n   - Add [Unreleased] section for upcoming changes\n   - Configure automatic changelog updates\n   - Set up changelog categories\n\n10. **Configure Branch Protection**\n    - Recommend branch protection rules:\n      - Require PR reviews\n      - Require status checks\n      - Require conventional PR titles\n      - Dismiss stale reviews\n    - Document recommended settings\n\n11. **Add Security Scanning**\n    - Set up Dependabot for dependency updates\n    - Configure security alerts\n    - Add security policy if needed\n\n12. **Test the System**\n    - Create example PR with conventional title\n    - Verify PR checks work correctly\n    - Test manual release trigger\n    - Validate changelog generation\n\nArguments: $ARGUMENTS\n\n### Additional Considerations\n\n**For Monorepos:**\n- Set up independent versioning per package\n- Configure changelog per package\n- Use conventional commits scopes\n\n**For Libraries:**\n- Include API compatibility checks\n- Generate API documentation\n- Add upgrade guides for breaking changes\n\n**For Applications:**\n- Include Docker image versioning\n- Set up deployment triggers\n- Add rollback procedures\n\n**Best Practices:**\n- Always create release branches for hotfixes\n- Use release candidates for major versions\n- Maintain upgrade guides\n- Keep releases small and frequent\n- Document rollback procedures\n\nThis automated release system provides:\n-  Consistent versioning\n-  Automatic changelog generation\n-  Clear contribution guidelines\n-  Professional release notes\n-  Reduced manual work\n-  Better project maintainability"
              },
              {
                "name": "/setup-cdn-optimization",
                "description": "Configure CDN for optimal delivery",
                "path": "plugins/all-commands/commands/setup-cdn-optimization.md",
                "frontmatter": {
                  "description": "Configure CDN for optimal delivery",
                  "category": "performance-optimization"
                },
                "content": "# Setup CDN Optimization\n\nConfigure CDN for optimal delivery\n\n## Instructions\n\n1. **CDN Strategy and Provider Selection**\n   - Analyze application traffic patterns and global user distribution\n   - Evaluate CDN providers (CloudFlare, AWS CloudFront, Fastly, KeyCDN)\n   - Assess content types and caching requirements\n   - Plan CDN architecture and edge location strategy\n   - Define performance and cost optimization goals\n\n2. **CDN Configuration and Setup**\n   - Configure CDN with optimal settings:\n\n   **CloudFlare Configuration:**\n   ```javascript\n   // Cloudflare Page Rules via API\n   const cloudflare = require('cloudflare');\n   const cf = new cloudflare({\n     email: process.env.CLOUDFLARE_EMAIL,\n     key: process.env.CLOUDFLARE_API_KEY\n   });\n\n   const pageRules = [\n     {\n       targets: [{ target: 'url', constraint: { operator: 'matches', value: '*/static/*' }}],\n       actions: [\n         { id: 'cache_level', value: 'cache_everything' },\n         { id: 'edge_cache_ttl', value: 31536000 }, // 1 year\n         { id: 'browser_cache_ttl', value: 31536000 }\n       ]\n     },\n     {\n       targets: [{ target: 'url', constraint: { operator: 'matches', value: '*/api/*' }}],\n       actions: [\n         { id: 'cache_level', value: 'bypass' },\n         { id: 'compression', value: 'gzip' }\n       ]\n     }\n   ];\n\n   async function setupCDNRules() {\n     for (const rule of pageRules) {\n       await cf.zones.pagerules.add(process.env.CLOUDFLARE_ZONE_ID, rule);\n     }\n   }\n   ```\n\n   **AWS CloudFront Distribution:**\n   ```yaml\n   # cloudformation-cdn.yaml\n   AWSTemplateFormatVersion: '2010-09-09'\n   Resources:\n     CloudFrontDistribution:\n       Type: AWS::CloudFront::Distribution\n       Properties:\n         DistributionConfig:\n           Origins:\n             - Id: S3Origin\n               DomainName: !GetAtt S3Bucket.DomainName\n               S3OriginConfig:\n                 OriginAccessIdentity: !Sub 'origin-access-identity/cloudfront/${OAI}'\n             - Id: APIOrigin\n               DomainName: api.example.com\n               CustomOriginConfig:\n                 HTTPPort: 443\n                 OriginProtocolPolicy: https-only\n           \n           DefaultCacheBehavior:\n             TargetOriginId: S3Origin\n             ViewerProtocolPolicy: redirect-to-https\n             CachePolicyId: 4135ea2d-6df8-44a3-9df3-4b5a84be39ad # Managed-CachingOptimized\n             OriginRequestPolicyId: 88a5eaf4-2fd4-4709-b370-b4c650ea3fcf # Managed-CORS-S3Origin\n             \n           CacheBehaviors:\n             - PathPattern: '/api/*'\n               TargetOriginId: APIOrigin\n               ViewerProtocolPolicy: https-only\n               CachePolicyId: 4135ea2d-6df8-44a3-9df3-4b5a84be39ad\n               TTL:\n                 DefaultTTL: 0\n                 MaxTTL: 0\n               Compress: true\n             \n             - PathPattern: '/static/*'\n               TargetOriginId: S3Origin\n               ViewerProtocolPolicy: https-only\n               CachePolicyId: 658327ea-f89d-4fab-a63d-7e88639e58f6 # Managed-CachingOptimizedForUncompressedObjects\n               TTL:\n                 DefaultTTL: 86400\n                 MaxTTL: 31536000\n   ```\n\n3. **Static Asset Optimization**\n   - Optimize assets for CDN delivery:\n\n   **Asset Build Process:**\n   ```javascript\n   // webpack.config.js - CDN optimization\n   const path = require('path');\n   const { CleanWebpackPlugin } = require('clean-webpack-plugin');\n   const MiniCssExtractPlugin = require('mini-css-extract-plugin');\n\n   module.exports = {\n     output: {\n       path: path.resolve(__dirname, 'dist'),\n       filename: '[name].[contenthash].js',\n       publicPath: process.env.CDN_URL || '/',\n       assetModuleFilename: 'assets/[name].[contenthash][ext]',\n     },\n     \n     optimization: {\n       splitChunks: {\n         chunks: 'all',\n         cacheGroups: {\n           vendor: {\n             test: /[\\\\/]node_modules[\\\\/]/,\n             name: 'vendors',\n             filename: 'vendors.[contenthash].js',\n           },\n         },\n       },\n     },\n     \n     plugins: [\n       new CleanWebpackPlugin(),\n       new MiniCssExtractPlugin({\n         filename: 'css/[name].[contenthash].css',\n       }),\n     ],\n     \n     module: {\n       rules: [\n         {\n           test: /\\.(png|jpe?g|gif|svg)$/i,\n           type: 'asset/resource',\n           generator: {\n             filename: 'images/[name].[contenthash][ext]',\n           },\n           use: [\n             {\n               loader: 'image-webpack-loader',\n               options: {\n                 mozjpeg: { progressive: true, quality: 80 },\n                 optipng: { enabled: false },\n                 pngquant: { quality: [0.6, 0.8] },\n                 webp: { quality: 80 },\n               },\n             },\n           ],\n         },\n       ],\n     },\n   };\n   ```\n\n   **Next.js CDN Configuration:**\n   ```javascript\n   // next.config.js\n   const withOptimizedImages = require('next-optimized-images');\n\n   module.exports = withOptimizedImages({\n     assetPrefix: process.env.CDN_URL || '',\n     \n     images: {\n       domains: ['cdn.example.com'],\n       formats: ['image/webp', 'image/avif'],\n       deviceSizes: [640, 750, 828, 1080, 1200, 1920, 2048, 3840],\n       imageSizes: [16, 32, 48, 64, 96, 128, 256, 384],\n       minimumCacheTTL: 31536000, // 1 year\n     },\n     \n     async headers() {\n       return [\n         {\n           source: '/static/(.*)',\n           headers: [\n             {\n               key: 'Cache-Control',\n               value: 'public, max-age=31536000, immutable',\n             },\n           ],\n         },\n       ];\n     },\n   });\n   ```\n\n4. **Compression and Optimization**\n   - Configure optimal compression settings:\n\n   **Gzip/Brotli Compression:**\n   ```javascript\n   // Express.js compression middleware\n   const compression = require('compression');\n   const express = require('express');\n   const app = express();\n\n   // Advanced compression configuration\n   app.use(compression({\n     level: 6, // Compression level (1-9)\n     threshold: 1024, // Only compress files > 1KB\n     filter: (req, res) => {\n       // Custom compression filter\n       if (req.headers['x-no-compression']) {\n         return false;\n       }\n       \n       // Compress text-based content types\n       return compression.filter(req, res);\n     }\n   }));\n\n   // Serve pre-compressed files if available\n   app.get('*.js', (req, res, next) => {\n     const acceptEncoding = req.get('Accept-Encoding');\n     \n     if (acceptEncoding && acceptEncoding.includes('br')) {\n       req.url = req.url + '.br';\n       res.set('Content-Encoding', 'br');\n       res.set('Content-Type', 'application/javascript');\n     } else if (acceptEncoding && acceptEncoding.includes('gzip')) {\n       req.url = req.url + '.gz';\n       res.set('Content-Encoding', 'gzip');\n       res.set('Content-Type', 'application/javascript');\n     }\n     \n     next();\n   });\n   ```\n\n   **Build-time Compression:**\n   ```javascript\n   // compression-plugin.js\n   const CompressionPlugin = require('compression-webpack-plugin');\n   const BrotliPlugin = require('brotli-webpack-plugin');\n\n   module.exports = {\n     plugins: [\n       // Gzip compression\n       new CompressionPlugin({\n         algorithm: 'gzip',\n         test: /\\.(js|css|html|svg)$/,\n         threshold: 8192,\n         minRatio: 0.8,\n       }),\n       \n       // Brotli compression\n       new BrotliPlugin({\n         asset: '[path].br[query]',\n         test: /\\.(js|css|html|svg)$/,\n         threshold: 8192,\n         minRatio: 0.8,\n       }),\n     ],\n   };\n   ```\n\n5. **Cache Headers and Policies**\n   - Configure optimal caching strategies:\n\n   **Smart Cache Headers:**\n   ```javascript\n   // cache-control.js\n   class CacheControlManager {\n     static getCacheHeaders(filePath, fileType) {\n       const cacheStrategies = {\n         // Long-term caching for versioned assets\n         versioned: {\n           'Cache-Control': 'public, max-age=31536000, immutable',\n           'Expires': new Date(Date.now() + 31536000000).toUTCString(),\n         },\n         \n         // Medium-term caching for semi-static content\n         semiStatic: {\n           'Cache-Control': 'public, max-age=86400, must-revalidate',\n           'ETag': this.generateETag(filePath),\n         },\n         \n         // Short-term caching for dynamic content\n         dynamic: {\n           'Cache-Control': 'public, max-age=300, must-revalidate',\n           'ETag': this.generateETag(filePath),\n         },\n         \n         // No caching for sensitive content\n         noCache: {\n           'Cache-Control': 'no-cache, no-store, must-revalidate',\n           'Pragma': 'no-cache',\n           'Expires': '0',\n         },\n       };\n\n       // Determine strategy based on file type and path\n       if (filePath.match(/\\.(js|css|png|jpg|jpeg|gif|ico|woff2?)$/)) {\n         return filePath.includes('[hash]') || filePath.includes('[contenthash]') \n           ? cacheStrategies.versioned \n           : cacheStrategies.semiStatic;\n       }\n       \n       if (filePath.startsWith('/api/')) {\n         return cacheStrategies.dynamic;\n       }\n       \n       if (filePath.includes('/admin') || filePath.includes('/auth')) {\n         return cacheStrategies.noCache;\n       }\n       \n       return cacheStrategies.semiStatic;\n     }\n\n     static generateETag(content) {\n       return `\"${require('crypto').createHash('md5').update(content).digest('hex')}\"`;\n     }\n   }\n\n   // Express middleware\n   app.use((req, res, next) => {\n     const headers = CacheControlManager.getCacheHeaders(req.path, req.get('Content-Type'));\n     Object.entries(headers).forEach(([key, value]) => {\n       res.set(key, value);\n     });\n     next();\n   });\n   ```\n\n6. **Image Optimization and Delivery**\n   - Implement advanced image optimization:\n\n   **Responsive Image Delivery:**\n   ```javascript\n   // image-optimization.js\n   const sharp = require('sharp');\n   const fs = require('fs').promises;\n\n   class ImageOptimizer {\n     static async generateResponsiveImages(inputPath, outputDir) {\n       const sizes = [\n         { width: 320, suffix: 'sm' },\n         { width: 640, suffix: 'md' },\n         { width: 1024, suffix: 'lg' },\n         { width: 1920, suffix: 'xl' },\n       ];\n\n       const formats = ['webp', 'jpeg'];\n       const results = [];\n\n       for (const size of sizes) {\n         for (const format of formats) {\n           const outputPath = `${outputDir}/${size.suffix}.${format}`;\n           \n           await sharp(inputPath)\n             .resize(size.width, null, { withoutEnlargement: true })\n             .toFormat(format, { quality: 80 })\n             .toFile(outputPath);\n             \n           results.push({\n             path: outputPath,\n             width: size.width,\n             format: format,\n           });\n         }\n       }\n\n       return results;\n     }\n\n     static generatePictureElement(imageName, alt, className = '') {\n       return `\n         <picture class=\"${className}\">\n           <source media=\"(min-width: 1024px)\" \n                   srcset=\"/images/${imageName}-xl.webp\" \n                   type=\"image/webp\">\n           <source media=\"(min-width: 1024px)\" \n                   srcset=\"/images/${imageName}-xl.jpeg\" \n                   type=\"image/jpeg\">\n           <source media=\"(min-width: 640px)\" \n                   srcset=\"/images/${imageName}-lg.webp\" \n                   type=\"image/webp\">\n           <source media=\"(min-width: 640px)\" \n                   srcset=\"/images/${imageName}-lg.jpeg\" \n                   type=\"image/jpeg\">\n           <source media=\"(min-width: 320px)\" \n                   srcset=\"/images/${imageName}-md.webp\" \n                   type=\"image/webp\">\n           <source media=\"(min-width: 320px)\" \n                   srcset=\"/images/${imageName}-md.jpeg\" \n                   type=\"image/jpeg\">\n           <img src=\"/images/${imageName}-sm.jpeg\" \n                alt=\"${alt}\" \n                loading=\"lazy\"\n                decoding=\"async\">\n         </picture>\n       `;\n     }\n   }\n   ```\n\n7. **CDN Purging and Cache Invalidation**\n   - Implement intelligent cache invalidation:\n\n   **CloudFlare Cache Purging:**\n   ```javascript\n   // cdn-purge.js\n   const cloudflare = require('cloudflare');\n\n   class CDNManager {\n     constructor() {\n       this.cf = new cloudflare({\n         email: process.env.CLOUDFLARE_EMAIL,\n         key: process.env.CLOUDFLARE_API_KEY\n       });\n       this.zoneId = process.env.CLOUDFLARE_ZONE_ID;\n     }\n\n     async purgeFiles(files) {\n       try {\n         const result = await this.cf.zones.purgeCache(this.zoneId, {\n           files: files.map(file => `https://example.com${file}`)\n         });\n         console.log('Cache purged successfully:', result);\n         return result;\n       } catch (error) {\n         console.error('Cache purge failed:', error);\n         throw error;\n       }\n     }\n\n     async purgeByTags(tags) {\n       try {\n         const result = await this.cf.zones.purgeCache(this.zoneId, {\n           tags: tags\n         });\n         console.log('Cache purged by tags:', result);\n         return result;\n       } catch (error) {\n         console.error('Cache purge by tags failed:', error);\n         throw error;\n       }\n     }\n\n     async purgeEverything() {\n       try {\n         const result = await this.cf.zones.purgeCache(this.zoneId, {\n           purge_everything: true\n         });\n         console.log('All cache purged:', result);\n         return result;\n       } catch (error) {\n         console.error('Full cache purge failed:', error);\n         throw error;\n       }\n     }\n   }\n\n   // Usage in deployment pipeline\n   const cdnManager = new CDNManager();\n\n   // Selective purging after deployment\n   async function postDeploymentPurge() {\n     const filesToPurge = [\n       '/static/js/main.*.js',\n       '/static/css/main.*.css',\n       '/',\n       '/index.html'\n     ];\n     \n     await cdnManager.purgeFiles(filesToPurge);\n   }\n   ```\n\n8. **Performance Monitoring and Analytics**\n   - Set up CDN performance monitoring:\n\n   **CDN Performance Tracking:**\n   ```javascript\n   // cdn-analytics.js\n   class CDNAnalytics {\n     static async getCDNMetrics() {\n       const metrics = {\n         cacheHitRatio: await this.getCacheHitRatio(),\n         bandwidth: await this.getBandwidthUsage(),\n         responseTime: await this.getResponseTimes(),\n         errorRate: await this.getErrorRate(),\n       };\n\n       return metrics;\n     }\n\n     static async getCacheHitRatio() {\n       // CloudFlare Analytics API\n       const response = await fetch(`https://api.cloudflare.com/client/v4/zones/${ZONE_ID}/analytics/dashboard`, {\n         headers: {\n           'X-Auth-Email': process.env.CLOUDFLARE_EMAIL,\n           'X-Auth-Key': process.env.CLOUDFLARE_API_KEY,\n         }\n       });\n\n       const data = await response.json();\n       return data.result.totals.requests.cached / data.result.totals.requests.all;\n     }\n\n     static trackCDNPerformance() {\n       // Real User Monitoring for CDN performance\n       if (typeof window !== 'undefined') {\n         const observer = new PerformanceObserver((list) => {\n           for (const entry of list.getEntries()) {\n             if (entry.name.includes('cdn.example.com')) {\n               // Track CDN resource loading times\n               console.log('CDN Resource:', {\n                 name: entry.name,\n                 duration: entry.duration,\n                 transferSize: entry.transferSize,\n                 encodedBodySize: entry.encodedBodySize,\n               });\n               \n               // Send to analytics\n               this.sendCDNMetric({\n                 resource: entry.name,\n                 loadTime: entry.duration,\n                 cacheStatus: entry.transferSize === 0 ? 'hit' : 'miss',\n               });\n             }\n           }\n         });\n\n         observer.observe({ entryTypes: ['resource'] });\n       }\n     }\n\n     static sendCDNMetric(metric) {\n       // Send to your analytics service\n       fetch('/api/analytics/cdn', {\n         method: 'POST',\n         headers: { 'Content-Type': 'application/json' },\n         body: JSON.stringify(metric),\n       });\n     }\n   }\n   ```\n\n9. **Security and Access Control**\n   - Configure CDN security features:\n\n   **CDN Security Configuration:**\n   ```javascript\n   // cdn-security.js\n   class CDNSecurity {\n     static setupSecurityHeaders() {\n       return {\n         'Strict-Transport-Security': 'max-age=31536000; includeSubDomains; preload',\n         'X-Content-Type-Options': 'nosniff',\n         'X-Frame-Options': 'DENY',\n         'X-XSS-Protection': '1; mode=block',\n         'Referrer-Policy': 'strict-origin-when-cross-origin',\n         'Content-Security-Policy': `\n           default-src 'self';\n           script-src 'self' 'unsafe-inline' cdn.example.com;\n           style-src 'self' 'unsafe-inline' cdn.example.com;\n           img-src 'self' data: cdn.example.com;\n           font-src 'self' cdn.example.com;\n         `.replace(/\\s+/g, ' ').trim(),\n       };\n     }\n\n     static configureHotlinkProtection() {\n       // CloudFlare Worker for hotlink protection\n       return `\n         addEventListener('fetch', event => {\n           event.respondWith(handleRequest(event.request));\n         });\n\n         async function handleRequest(request) {\n           const url = new URL(request.url);\n           const referer = request.headers.get('Referer');\n           \n           // Allow requests from your domain and direct access\n           const allowedDomains = ['example.com', 'www.example.com'];\n           \n           if (!referer || allowedDomains.some(domain => referer.includes(domain))) {\n             return fetch(request);\n           }\n           \n           // Block hotlinking\n           return new Response('Hotlinking not allowed', { status: 403 });\n         }\n       `;\n     }\n   }\n   ```\n\n10. **Cost Optimization and Monitoring**\n    - Implement CDN cost optimization:\n\n    **Cost Monitoring:**\n    ```javascript\n    // cdn-cost-optimization.js\n    class CDNCostOptimizer {\n      static async analyzeUsage() {\n        const usage = await this.getCDNUsage();\n        const recommendations = [];\n\n        // Analyze bandwidth usage by file type\n        if (usage.images > usage.total * 0.6) {\n          recommendations.push({\n            type: 'image_optimization',\n            message: 'Images account for >60% of bandwidth. Consider WebP format and better compression.',\n            potential_savings: '20-40%'\n          });\n        }\n\n        // Analyze cache hit ratio\n        if (usage.cacheHitRatio < 0.8) {\n          recommendations.push({\n            type: 'cache_optimization',\n            message: 'Cache hit ratio is below 80%. Review cache headers and TTL settings.',\n            potential_savings: '10-25%'\n          });\n        }\n\n        return recommendations;\n      }\n\n      static async optimizeTierUsage() {\n        // Move less frequently accessed content to cheaper tiers\n        const accessPatterns = await this.getAccessPatterns();\n        \n        const coldFiles = accessPatterns.filter(file => \n          file.requests_per_day < 10 && file.size > 1024 * 1024 // <10 requests/day, >1MB\n        );\n\n        console.log(`Found ${coldFiles.length} files suitable for cold storage`);\n        return coldFiles;\n      }\n\n      static setupCostAlerts() {\n        // Monitor CDN costs and set up alerts\n        return {\n          daily_bandwidth_alert: '100GB',\n          monthly_cost_alert: '$500',\n          cache_hit_ratio_alert: '75%',\n          error_rate_alert: '5%'\n        };\n      }\n    }\n\n    // Monthly cost analysis\n    setInterval(async () => {\n      const analysis = await CDNCostOptimizer.analyzeUsage();\n      console.log('CDN Cost Analysis:', analysis);\n    }, 24 * 60 * 60 * 1000); // Daily\n    ```"
              },
              {
                "name": "/setup-comprehensive-testing",
                "description": "Setup complete testing infrastructure",
                "path": "plugins/all-commands/commands/setup-comprehensive-testing.md",
                "frontmatter": {
                  "description": "Setup complete testing infrastructure",
                  "category": "code-analysis-testing"
                },
                "content": "# Setup Comprehensive Testing\n\nSetup complete testing infrastructure\n\n## Instructions\n\n1. **Testing Strategy Analysis**\n   - Analyze current project structure and identify testing needs\n   - Determine appropriate testing frameworks based on technology stack\n   - Define testing pyramid strategy (unit, integration, e2e, visual)\n   - Plan test coverage goals and quality metrics\n   - Assess existing testing infrastructure and gaps\n\n2. **Unit Testing Framework Setup**\n   - Install and configure primary testing framework (Jest, Vitest, pytest, etc.)\n   - Set up test runner configuration and environment\n   - Configure test file patterns and directory structure\n   - Set up test utilities and helper functions\n   - Configure mocking and stubbing capabilities\n\n3. **Integration Testing Configuration**\n   - Set up integration testing framework and tools\n   - Configure test database and data seeding\n   - Set up API testing with tools like Supertest or requests\n   - Configure service integration testing\n   - Set up component integration testing for frontend\n\n4. **End-to-End Testing Setup**\n   - Install and configure E2E testing framework (Playwright, Cypress, Selenium)\n   - Set up test environment and browser configuration\n   - Create page object models and test helpers\n   - Configure test data management and cleanup\n   - Set up cross-browser and device testing\n\n5. **Visual Testing Integration**\n   - Set up visual regression testing tools (Chromatic, Percy, Playwright)\n   - Configure screenshot comparison and diff detection\n   - Set up visual testing for different viewports and devices\n   - Create visual test baselines and approval workflows\n   - Configure visual testing in CI/CD pipeline\n\n6. **Test Coverage and Reporting**\n   - Configure code coverage collection and reporting\n   - Set up coverage thresholds and quality gates\n   - Configure test result reporting and visualization\n   - Set up test performance monitoring\n   - Configure test report generation and distribution\n\n7. **Performance and Load Testing**\n   - Set up performance testing framework (k6, Artillery, JMeter)\n   - Configure load testing scenarios and benchmarks\n   - Set up performance monitoring and alerting\n   - Configure stress testing and capacity planning\n   - Set up performance regression detection\n\n8. **Test Data Management**\n   - Set up test data factories and fixtures\n   - Configure database seeding and cleanup\n   - Set up test data isolation and parallel test execution\n   - Configure test environment data management\n   - Set up API mocking and service virtualization\n\n9. **CI/CD Integration**\n   - Configure automated test execution in CI/CD pipeline\n   - Set up parallel test execution and optimization\n   - Configure test result reporting and notifications\n   - Set up test environment provisioning and cleanup\n   - Configure deployment gates based on test results\n\n10. **Testing Best Practices and Documentation**\n    - Create comprehensive testing guidelines and standards\n    - Set up test naming conventions and organization\n    - Document testing workflows and procedures\n    - Create testing templates and examples\n    - Set up testing metrics and quality monitoring\n    - Train team on testing best practices and tools"
              },
              {
                "name": "/setup-development-environment",
                "description": "Setup complete development environment",
                "path": "plugins/all-commands/commands/setup-development-environment.md",
                "frontmatter": {
                  "description": "Setup complete development environment",
                  "category": "project-setup",
                  "allowed-tools": "Edit"
                },
                "content": "# Setup Development Environment\n\nSetup complete development environment\n\n## Instructions\n\n1. **Environment Analysis and Requirements**\n   - Analyze current project structure and technology stack\n   - Identify required development tools and dependencies\n   - Check existing development environment configuration\n   - Determine team size and collaboration requirements\n   - Assess platform requirements (Windows, macOS, Linux)\n\n2. **Core Development Tools Installation**\n   - Verify and install required runtime environments (Node.js, Python, Java, etc.)\n   - Set up package managers with proper versions (npm, yarn, pnpm, pip, maven, etc.)\n   - Install and configure version control tools (Git, Git LFS)\n   - Set up code editors with workspace-specific settings (VSCode, IntelliJ)\n   - Configure terminal and shell environment\n\n3. **Project-Specific Tooling**\n   - Install project dependencies and dev dependencies\n   - Set up build tools and task runners\n   - Configure bundlers and module systems\n   - Install testing frameworks and runners\n   - Set up debugging tools and extensions\n   - Configure profiling and performance monitoring tools\n\n4. **Code Quality and Standards**\n   - Install and configure linting tools (ESLint, Pylint, etc.)\n   - Set up code formatting tools (Prettier, Black, etc.)\n   - Configure pre-commit hooks with Husky or similar\n   - Set up code spell checking and grammar tools\n   - Configure import sorting and organization tools\n   - Set up code complexity and quality metrics\n\n5. **Development Server and Database**\n   - Set up local development server with hot reloading\n   - Configure database server and management tools\n   - Set up containerized development environment (Docker)\n   - Configure API mocking and testing tools\n   - Set up local SSL certificates for HTTPS development\n   - Configure environment variable management\n\n6. **IDE and Editor Configuration**\n   - Configure workspace settings and extensions\n   - Set up language-specific plugins and syntax highlighting\n   - Configure IntelliSense and auto-completion\n   - Set up debugging configurations and breakpoints\n   - Configure integrated terminal and task running\n   - Set up code snippets and templates\n\n7. **Environment Variables and Secrets**\n   - Create .env template files for different environments\n   - Set up local environment variable management\n   - Configure secrets management for development\n   - Set up API keys and service credentials\n   - Configure environment-specific configuration files\n   - Document required environment variables\n\n8. **Documentation and Knowledge Base**\n   - Create comprehensive setup documentation\n   - Document common development workflows\n   - Set up project wiki or knowledge base\n   - Create troubleshooting guides for common issues\n   - Document coding standards and best practices\n   - Set up onboarding checklist for new team members\n\n9. **Collaboration and Communication Tools**\n   - Configure team communication channels\n   - Set up code review workflows and tools\n   - Configure issue tracking and project management\n   - Set up shared development resources and services\n   - Configure team calendars and meeting tools\n   - Set up shared documentation and file storage\n\n10. **Validation and Testing**\n    - Verify all tools and dependencies are properly installed\n    - Test development server startup and hot reloading\n    - Validate database connections and data access\n    - Test build processes and deployment workflows\n    - Verify code quality tools are working correctly\n    - Test collaboration workflows and team access\n    - Create development environment health check script"
              },
              {
                "name": "/setup-formatting",
                "description": "Configure code formatting tools",
                "path": "plugins/all-commands/commands/setup-formatting.md",
                "frontmatter": {
                  "description": "Configure code formatting tools",
                  "category": "project-setup",
                  "argument-hint": "1. **Language-Specific Tools**",
                  "allowed-tools": "Bash(npm *)"
                },
                "content": "# Setup Formatting Command\n\nConfigure code formatting tools\n\n## Instructions\n\nSetup code formatting following these steps: **$ARGUMENTS**\n\n1. **Language-Specific Tools**\n\n   **JavaScript/TypeScript:**\n   ```bash\n   npm install -D prettier\n   echo '{\"semi\": true, \"singleQuote\": true, \"tabWidth\": 2}' > .prettierrc\n   ```\n\n   **Python:**\n   ```bash\n   pip install black isort\n   echo '[tool.black]\\nline-length = 88\\ntarget-version = [\"py38\"]' > pyproject.toml\n   ```\n\n   **Java:**\n   ```bash\n   # Google Java Format or Spotless plugin\n   ```\n\n2. **Configuration Files**\n\n   **.prettierrc:**\n   ```json\n   {\n     \"semi\": true,\n     \"singleQuote\": true,\n     \"tabWidth\": 2,\n     \"trailingComma\": \"es5\",\n     \"printWidth\": 80\n   }\n   ```\n\n3. **IDE Setup**\n   - Install formatter extensions\n   - Enable format on save\n   - Configure keyboard shortcuts\n\n4. **Scripts and Automation**\n   ```json\n   {\n     \"scripts\": {\n       \"format\": \"prettier --write .\",\n       \"format:check\": \"prettier --check .\"\n     }\n   }\n   ```\n\n5. **Pre-commit Hooks**\n   ```bash\n   npm install -D husky lint-staged\n   echo '{\"*.{js,ts,tsx}\": [\"prettier --write\", \"eslint --fix\"]}' > .lintstagedrc\n   ```\n\nRemember to run formatting on entire codebase initially and configure team IDE settings consistently."
              },
              {
                "name": "/setup-kubernetes-deployment",
                "description": "Configure Kubernetes deployment manifests",
                "path": "plugins/all-commands/commands/setup-kubernetes-deployment.md",
                "frontmatter": {
                  "description": "Configure Kubernetes deployment manifests",
                  "category": "ci-deployment"
                },
                "content": "# Setup Kubernetes Deployment\n\nConfigure Kubernetes deployment manifests\n\n## Instructions\n\n1. **Kubernetes Architecture Planning**\n   - Analyze application architecture and deployment requirements\n   - Define resource requirements (CPU, memory, storage, network)\n   - Plan namespace organization and multi-tenancy strategy\n   - Assess high availability and disaster recovery requirements\n   - Define scaling strategies and performance requirements\n\n2. **Cluster Setup and Configuration**\n   - Set up Kubernetes cluster (managed or self-hosted)\n   - Configure cluster networking and CNI plugin\n   - Set up cluster storage classes and persistent volumes\n   - Configure cluster security policies and RBAC\n   - Set up cluster monitoring and logging infrastructure\n\n3. **Application Containerization**\n   - Ensure application is properly containerized\n   - Optimize container images for Kubernetes deployment\n   - Configure multi-stage builds and security scanning\n   - Set up container registry and image management\n   - Configure image pull policies and secrets\n\n4. **Kubernetes Manifest Creation**\n   - Create Deployment manifests with proper resource limits\n   - Set up Service manifests for internal and external communication\n   - Configure ConfigMaps and Secrets for configuration management\n   - Create PersistentVolumeClaims for data storage\n   - Set up NetworkPolicies for security and isolation\n\n5. **Load Balancing and Ingress**\n   - Configure Ingress controllers and routing rules\n   - Set up SSL/TLS termination and certificate management\n   - Configure load balancing strategies and session affinity\n   - Set up external DNS and domain management\n   - Configure traffic management and canary deployments\n\n6. **Auto-scaling Configuration**\n   - Set up Horizontal Pod Autoscaler (HPA) based on metrics\n   - Configure Vertical Pod Autoscaler (VPA) for resource optimization\n   - Set up Cluster Autoscaler for node scaling\n   - Configure custom metrics and scaling policies\n   - Set up resource quotas and limits\n\n7. **Health Checks and Monitoring**\n   - Configure liveness and readiness probes\n   - Set up startup probes for slow-starting applications\n   - Configure health check endpoints and monitoring\n   - Set up application metrics collection\n   - Configure alerting and notification systems\n\n8. **Security and Compliance**\n   - Configure Pod Security Standards and policies\n   - Set up network segmentation and security policies\n   - Configure service accounts and RBAC permissions\n   - Set up secret management and rotation\n   - Configure security scanning and compliance monitoring\n\n9. **CI/CD Integration**\n   - Set up automated Kubernetes deployment pipelines\n   - Configure GitOps workflows with ArgoCD or Flux\n   - Set up automated testing in Kubernetes environments\n   - Configure blue-green and canary deployment strategies\n   - Set up rollback and disaster recovery procedures\n\n10. **Operations and Maintenance**\n    - Set up cluster maintenance and update procedures\n    - Configure backup and disaster recovery strategies\n    - Set up cost optimization and resource management\n    - Create operational runbooks and troubleshooting guides\n    - Train team on Kubernetes operations and best practices\n    - Set up cluster lifecycle management and governance"
              },
              {
                "name": "/setup-linting",
                "description": "Setup code linting and quality tools",
                "path": "plugins/all-commands/commands/setup-linting.md",
                "frontmatter": {
                  "description": "Setup code linting and quality tools",
                  "category": "project-setup",
                  "argument-hint": "1. **Project Analysis**",
                  "allowed-tools": "Bash(npm *)"
                },
                "content": "# Setup Linting Command\n\nSetup code linting and quality tools\n\n## Instructions\n\nFollow this systematic approach to setup linting: **$ARGUMENTS**\n\n1. **Project Analysis**\n   - Identify programming languages and frameworks\n   - Check existing linting configuration\n   - Review current code style and patterns\n   - Assess team preferences and requirements\n\n2. **Tool Selection by Language**\n\n   **JavaScript/TypeScript:**\n   ```bash\n   npm install -D eslint @typescript-eslint/parser @typescript-eslint/eslint-plugin\n   npm install -D prettier eslint-config-prettier eslint-plugin-prettier\n   ```\n\n   **Python:**\n   ```bash\n   pip install flake8 black isort mypy pylint\n   ```\n\n   **Java:**\n   ```bash\n   # Add to pom.xml or build.gradle\n   # Checkstyle, SpotBugs, PMD\n   ```\n\n3. **Configuration Setup**\n\n   **ESLint (.eslintrc.json):**\n   ```json\n   {\n     \"extends\": [\n       \"eslint:recommended\",\n       \"@typescript-eslint/recommended\",\n       \"prettier\"\n     ],\n     \"parser\": \"@typescript-eslint/parser\",\n     \"plugins\": [\"@typescript-eslint\"],\n     \"rules\": {\n       \"no-console\": \"warn\",\n       \"no-unused-vars\": \"error\",\n       \"@typescript-eslint/no-explicit-any\": \"warn\"\n     }\n   }\n   ```\n\n4. **IDE Integration**\n   - Configure VS Code settings\n   - Setup auto-fix on save\n   - Install relevant extensions\n\n5. **CI/CD Integration**\n   ```yaml\n   - name: Lint code\n     run: npm run lint\n   ```\n\n6. **Package.json Scripts**\n   ```json\n   {\n     \"scripts\": {\n       \"lint\": \"eslint src --ext .ts,.tsx\",\n       \"lint:fix\": \"eslint src --ext .ts,.tsx --fix\",\n       \"format\": \"prettier --write src\"\n     }\n   }\n   ```\n\nRemember to customize rules based on team preferences and gradually enforce stricter standards."
              },
              {
                "name": "/setup-load-testing",
                "description": "Configure load and performance testing",
                "path": "plugins/all-commands/commands/setup-load-testing.md",
                "frontmatter": {
                  "description": "Configure load and performance testing",
                  "category": "code-analysis-testing"
                },
                "content": "# Setup Load Testing\n\nConfigure load and performance testing\n\n## Instructions\n\n1. **Load Testing Strategy and Requirements**\n   - Analyze application architecture and identify performance-critical components\n   - Define load testing objectives (capacity planning, performance validation, bottleneck identification)\n   - Determine testing scenarios (normal load, peak load, stress testing, spike testing)\n   - Identify key performance metrics and acceptance criteria\n   - Plan load testing environments and infrastructure requirements\n\n2. **Load Testing Tool Selection**\n   - Choose appropriate load testing tools based on requirements:\n     - **k6**: Modern, developer-friendly with JavaScript scripting\n     - **Artillery**: Simple, powerful, great for CI/CD integration\n     - **JMeter**: Feature-rich GUI and command-line tool\n     - **Gatling**: High-performance tool with detailed reporting\n     - **Locust**: Python-based with web UI and distributed testing\n     - **WebPageTest**: Web performance and real user monitoring\n   - Consider factors: scripting language, reporting, CI integration, cost\n\n3. **Test Environment Setup**\n   - Set up dedicated load testing environment matching production\n   - Configure test data and database setup for consistent testing\n   - Set up network configuration and firewall rules\n   - Configure monitoring and observability for test environment\n   - Set up test isolation and cleanup procedures\n\n4. **Load Test Script Development**\n   - Create test scripts for critical user journeys and API endpoints\n   - Implement realistic user behavior patterns and think times\n   - Set up test data generation and management\n   - Configure authentication and session management\n   - Implement parameterization and data-driven testing\n\n5. **Performance Scenarios Configuration**\n   - **Load Testing**: Normal expected traffic patterns\n   - **Stress Testing**: Beyond normal capacity to find breaking points\n   - **Spike Testing**: Sudden traffic increases and decreases\n   - **Volume Testing**: Large amounts of data processing\n   - **Endurance Testing**: Extended periods under normal load\n   - **Capacity Testing**: Maximum user load determination\n\n6. **Monitoring and Metrics Collection**\n   - Set up application performance monitoring during tests\n   - Configure infrastructure metrics collection (CPU, memory, disk, network)\n   - Set up database performance monitoring and query analysis\n   - Configure real-time dashboards and alerting\n   - Set up log aggregation and error tracking\n\n7. **Test Execution and Automation**\n   - Configure automated test execution and scheduling\n   - Set up test result collection and analysis\n   - Configure test environment provisioning and teardown\n   - Set up parallel and distributed test execution\n   - Configure test result storage and historical tracking\n\n8. **Performance Analysis and Reporting**\n   - Set up automated performance analysis and threshold checking\n   - Configure performance trend analysis and regression detection\n   - Set up detailed performance reporting and visualization\n   - Configure performance alerts and notifications\n   - Set up performance benchmark and baseline management\n\n9. **CI/CD Integration**\n   - Integrate load tests into continuous integration pipeline\n   - Configure performance gates and deployment blocking\n   - Set up automated performance regression detection\n   - Configure test result integration with development workflow\n   - Set up performance testing in staging and pre-production environments\n\n10. **Optimization and Maintenance**\n    - Document load testing procedures and maintenance guidelines\n    - Set up load test script maintenance and version control\n    - Configure test environment maintenance and updates\n    - Create performance optimization recommendations workflow\n    - Train team on load testing best practices and tool usage\n    - Set up performance testing standards and conventions"
              },
              {
                "name": "/setup-monitoring-observability",
                "description": "Setup monitoring and observability tools",
                "path": "plugins/all-commands/commands/setup-monitoring-observability.md",
                "frontmatter": {
                  "description": "Setup monitoring and observability tools",
                  "category": "monitoring-observability"
                },
                "content": "# Setup Monitoring and Observability\n\nSetup monitoring and observability tools\n\n## Instructions\n\n1. **Observability Strategy Planning**\n   - Analyze application architecture and monitoring requirements\n   - Define key performance indicators (KPIs) and service level objectives (SLOs)\n   - Plan monitoring stack architecture and data flow\n   - Assess compliance and retention requirements\n   - Define alerting strategies and escalation procedures\n\n2. **Metrics Collection and Monitoring**\n   - Set up application metrics collection (Prometheus, DataDog, New Relic)\n   - Configure infrastructure monitoring for servers, containers, and cloud resources\n   - Set up business metrics and user experience monitoring\n   - Configure custom metrics for application-specific monitoring\n   - Set up metrics aggregation and time-series storage\n\n3. **Logging Infrastructure**\n   - Set up centralized logging system (ELK Stack, Fluentd, Splunk)\n   - Configure structured logging with consistent formats\n   - Set up log aggregation and forwarding from all services\n   - Configure log retention policies and archival strategies\n   - Set up log parsing, enrichment, and indexing\n\n4. **Distributed Tracing**\n   - Set up distributed tracing system (Jaeger, Zipkin, AWS X-Ray)\n   - Configure trace instrumentation in application code\n   - Set up trace sampling and collection strategies\n   - Configure trace correlation across service boundaries\n   - Set up trace analysis and performance optimization\n\n5. **Application Performance Monitoring (APM)**\n   - Configure APM tools for application performance insights\n   - Set up error tracking and exception monitoring\n   - Configure database query monitoring and optimization\n   - Set up real user monitoring (RUM) and synthetic monitoring\n   - Configure performance profiling and bottleneck identification\n\n6. **Infrastructure and System Monitoring**\n   - Set up server and container monitoring (CPU, memory, disk, network)\n   - Configure cloud service monitoring and cost tracking\n   - Set up database monitoring and performance analysis\n   - Configure network monitoring and security scanning\n   - Set up capacity planning and resource optimization\n\n7. **Alerting and Notification System**\n   - Configure intelligent alerting with proper thresholds\n   - Set up alert routing and escalation procedures\n   - Configure notification channels (email, Slack, PagerDuty)\n   - Set up alert correlation and noise reduction\n   - Configure on-call scheduling and incident management\n\n8. **Dashboards and Visualization**\n   - Create comprehensive monitoring dashboards (Grafana, Kibana)\n   - Set up real-time system health dashboards\n   - Configure business metrics and KPI visualization\n   - Create role-specific dashboards for different teams\n   - Set up mobile-friendly monitoring interfaces\n\n9. **Security Monitoring and Compliance**\n   - Set up security event monitoring and SIEM integration\n   - Configure compliance monitoring and audit trails\n   - Set up vulnerability scanning and security alerting\n   - Configure access monitoring and user behavior analytics\n   - Set up data privacy and protection monitoring\n\n10. **Incident Response and Automation**\n    - Set up automated incident detection and response\n    - Configure runbook automation and self-healing systems\n    - Set up incident management and communication workflows\n    - Configure post-incident analysis and improvement processes\n    - Create monitoring maintenance and optimization procedures\n    - Train team on monitoring tools and incident response procedures"
              },
              {
                "name": "/setup-monorepo",
                "description": "Configure monorepo project structure",
                "path": "plugins/all-commands/commands/setup-monorepo.md",
                "frontmatter": {
                  "description": "Configure monorepo project structure",
                  "category": "project-setup",
                  "argument-hint": "Specify monorepo configuration options"
                },
                "content": "# Setup Monorepo\n\nConfigure monorepo project structure\n\n## Instructions\n\n1. **Monorepo Tool Analysis**\n   - Parse monorepo tool from arguments: `$ARGUMENTS` (nx, lerna, rush, yarn-workspaces, pnpm-workspaces, turborepo)\n   - If no tool specified, analyze project structure and recommend best tool based on:\n     - Project size and complexity\n     - Existing package manager\n     - Team preferences and CI/CD requirements\n   - Validate tool compatibility with existing codebase\n\n2. **Workspace Structure Setup**\n   - Create standard monorepo directory structure:\n     - `packages/` or `apps/` for applications\n     - `libs/` or `shared/` for shared libraries\n     - `tools/` for build tools and scripts\n     - `docs/` for documentation\n   - Configure workspace root package.json with workspace definitions\n   - Set up proper .gitignore for monorepo patterns\n\n3. **Tool-Specific Configuration**\n   - **Nx**: Initialize Nx workspace, configure nx.json, add essential plugins\n   - **Lerna**: Set up lerna.json, configure version management and publishing\n   - **Rush**: Initialize rush.json, configure build orchestration and policies\n   - **Yarn Workspaces**: Configure workspaces in package.json, set up workspace protocols\n   - **pnpm Workspaces**: Set up pnpm-workspace.yaml, configure filtering and dependencies\n   - **Turborepo**: Initialize turbo.json, configure pipeline and caching\n\n4. **Package Management Configuration**\n   - Configure package manager settings for workspace support\n   - Set up dependency hoisting and deduplication rules\n   - Configure workspace-specific package.json templates\n   - Set up cross-package dependency management\n   - Configure private package registry if needed\n\n5. **Build System Integration**\n   - Configure build orchestration and task running\n   - Set up dependency graph analysis and affected package detection\n   - Configure parallel builds and task caching\n   - Set up incremental builds for changed packages\n   - Configure build artifacts and output management\n\n6. **Development Workflow**\n   - Set up workspace-wide development scripts\n   - Configure hot reloading and watch mode for development\n   - Set up workspace-wide linting and formatting\n   - Configure debugging across multiple packages\n   - Set up workspace-wide testing and coverage\n\n7. **Version Management**\n   - Configure versioning strategy (independent vs. fixed versions)\n   - Set up changelog generation for workspace packages\n   - Configure release workflow and package publishing\n   - Set up semantic versioning and conventional commits\n   - Configure workspace-wide dependency updates\n\n8. **CI/CD Pipeline Integration**\n   - Configure CI to detect affected packages and run targeted tests\n   - Set up build matrix for different package combinations\n   - Configure deployment pipeline for multiple packages\n   - Set up workspace-wide quality gates\n   - Configure artifact publishing and registry management\n\n9. **Documentation and Standards**\n   - Create workspace-wide development guidelines\n   - Document package creation and management procedures\n   - Set up workspace-wide code standards and conventions\n   - Create architectural decision records for monorepo patterns\n   - Document deployment and release procedures\n\n10. **Validation and Testing**\n    - Verify workspace configuration is correct\n    - Test package creation and cross-package dependencies\n    - Validate build pipeline and task execution\n    - Test development workflow and hot reloading\n    - Verify CI/CD integration and affected package detection\n    - Create example packages to demonstrate workspace functionality"
              },
              {
                "name": "/setup-rate-limiting",
                "description": "Implement API rate limiting",
                "path": "plugins/all-commands/commands/setup-rate-limiting.md",
                "frontmatter": {
                  "description": "Implement API rate limiting",
                  "category": "project-setup"
                },
                "content": "# Setup Rate Limiting\n\nImplement API rate limiting\n\n## Instructions\n\n1. **Rate Limiting Strategy and Planning**\n   - Analyze API endpoints and traffic patterns\n   - Define rate limiting policies for different user types and endpoints\n   - Plan for distributed rate limiting across multiple servers\n   - Consider different rate limiting algorithms (token bucket, sliding window, etc.)\n   - Design rate limiting bypass mechanisms for trusted clients\n\n2. **Express.js Rate Limiting Implementation**\n   - Set up comprehensive rate limiting middleware:\n\n   **Basic Rate Limiting Setup:**\n   ```javascript\n   // middleware/rate-limiter.js\n   const rateLimit = require('express-rate-limit');\n   const RedisStore = require('rate-limit-redis');\n   const Redis = require('ioredis');\n\n   class RateLimiter {\n     constructor() {\n       this.redis = new Redis(process.env.REDIS_URL);\n       this.setupDefaultLimiters();\n     }\n\n     setupDefaultLimiters() {\n       // General API rate limiter\n       this.generalLimiter = rateLimit({\n         store: new RedisStore({\n           sendCommand: (...args) => this.redis.call(...args),\n         }),\n         windowMs: 15 * 60 * 1000, // 15 minutes\n         max: 1000, // Limit each IP to 1000 requests per windowMs\n         message: {\n           error: 'Too many requests from this IP',\n           retryAfter: '15 minutes'\n         },\n         standardHeaders: true,\n         legacyHeaders: false,\n         keyGenerator: (req) => {\n           // Use user ID if authenticated, otherwise IP\n           return req.user?.id || req.ip;\n         },\n         skip: (req) => {\n           // Skip rate limiting for internal requests\n           return req.headers['x-internal-request'] === 'true';\n         },\n         onLimitReached: (req, res, options) => {\n           console.warn('Rate limit reached:', {\n             ip: req.ip,\n             userAgent: req.get('User-Agent'),\n             endpoint: req.path,\n             timestamp: new Date().toISOString()\n           });\n         }\n       });\n\n       // Strict limiter for sensitive endpoints\n       this.strictLimiter = rateLimit({\n         store: new RedisStore({\n           sendCommand: (...args) => this.redis.call(...args),\n         }),\n         windowMs: 60 * 60 * 1000, // 1 hour\n         max: 5, // Very strict limit\n         message: {\n           error: 'Too many attempts for this sensitive operation',\n           retryAfter: '1 hour'\n         },\n         skipSuccessfulRequests: true,\n         keyGenerator: (req) => `${req.user?.id || req.ip}:${req.path}`\n       });\n\n       // Authentication rate limiter\n       this.authLimiter = rateLimit({\n         store: new RedisStore({\n           sendCommand: (...args) => this.redis.call(...args),\n         }),\n         windowMs: 15 * 60 * 1000, // 15 minutes\n         max: 5, // Limit login attempts\n         skipSuccessfulRequests: true,\n         keyGenerator: (req) => `auth:${req.ip}:${req.body.email || req.body.username}`,\n         message: {\n           error: 'Too many authentication attempts',\n           retryAfter: '15 minutes'\n         }\n       });\n     }\n\n     // Dynamic rate limiter based on user tier\n     createTierBasedLimiter(windowMs = 15 * 60 * 1000) {\n       return rateLimit({\n         store: new RedisStore({\n           sendCommand: (...args) => this.redis.call(...args),\n         }),\n         windowMs,\n         max: (req) => {\n           const user = req.user;\n           if (!user) return 100; // Anonymous users\n           \n           switch (user.tier) {\n             case 'premium': return 10000;\n             case 'pro': return 5000;\n             case 'basic': return 1000;\n             default: return 500;\n           }\n         },\n         keyGenerator: (req) => `tier:${req.user?.id || req.ip}`,\n         message: (req) => ({\n           error: 'Rate limit exceeded for your tier',\n           currentTier: req.user?.tier || 'anonymous',\n           upgradeUrl: '/upgrade'\n         })\n       });\n     }\n\n     // Endpoint-specific rate limiter\n     createEndpointLimiter(endpoint, config) {\n       return rateLimit({\n         store: new RedisStore({\n           sendCommand: (...args) => this.redis.call(...args),\n         }),\n         windowMs: config.windowMs || 60 * 1000,\n         max: config.max || 100,\n         keyGenerator: (req) => `endpoint:${endpoint}:${req.user?.id || req.ip}`,\n         message: {\n           error: `Rate limit exceeded for ${endpoint}`,\n           limit: config.max,\n           window: config.windowMs\n         },\n         ...config\n       });\n     }\n   }\n\n   module.exports = new RateLimiter();\n   ```\n\n3. **Advanced Rate Limiting Algorithms**\n   - Implement sophisticated rate limiting strategies:\n\n   **Token Bucket Implementation:**\n   ```javascript\n   // rate-limiters/token-bucket.js\n   class TokenBucket {\n     constructor(capacity, refillRate, refillPeriod = 1000) {\n       this.capacity = capacity;\n       this.tokens = capacity;\n       this.refillRate = refillRate;\n       this.refillPeriod = refillPeriod;\n       this.lastRefill = Date.now();\n     }\n\n     consume(tokens = 1) {\n       this.refill();\n       \n       if (this.tokens >= tokens) {\n         this.tokens -= tokens;\n         return true;\n       }\n       \n       return false;\n     }\n\n     refill() {\n       const now = Date.now();\n       const timePassed = now - this.lastRefill;\n       const tokensToAdd = Math.floor(timePassed / this.refillPeriod) * this.refillRate;\n       \n       this.tokens = Math.min(this.capacity, this.tokens + tokensToAdd);\n       this.lastRefill = now;\n     }\n\n     getAvailableTokens() {\n       this.refill();\n       return this.tokens;\n     }\n\n     getTimeToNextToken() {\n       if (this.tokens > 0) return 0;\n       \n       const timeSinceLastRefill = Date.now() - this.lastRefill;\n       return this.refillPeriod - (timeSinceLastRefill % this.refillPeriod);\n     }\n   }\n\n   // Redis-backed token bucket for distributed systems\n   class DistributedTokenBucket {\n     constructor(redis, key, capacity, refillRate, refillPeriod = 1000) {\n       this.redis = redis;\n       this.key = key;\n       this.capacity = capacity;\n       this.refillRate = refillRate;\n       this.refillPeriod = refillPeriod;\n     }\n\n     async consume(tokens = 1) {\n       const script = `\n         local key = KEYS[1]\n         local capacity = tonumber(ARGV[1])\n         local refillRate = tonumber(ARGV[2])\n         local refillPeriod = tonumber(ARGV[3])\n         local tokensRequested = tonumber(ARGV[4])\n         local now = tonumber(ARGV[5])\n         \n         local bucket = redis.call('HMGET', key, 'tokens', 'lastRefill')\n         local tokens = tonumber(bucket[1]) or capacity\n         local lastRefill = tonumber(bucket[2]) or now\n         \n         -- Calculate tokens to add\n         local timePassed = now - lastRefill\n         local tokensToAdd = math.floor(timePassed / refillPeriod) * refillRate\n         tokens = math.min(capacity, tokens + tokensToAdd)\n         \n         local success = 0\n         if tokens >= tokensRequested then\n           tokens = tokens - tokensRequested\n           success = 1\n         end\n         \n         -- Update bucket\n         redis.call('HMSET', key, 'tokens', tokens, 'lastRefill', now)\n         redis.call('EXPIRE', key, 3600) -- 1 hour TTL\n         \n         return {success, tokens, math.max(0, refillPeriod - (timePassed % refillPeriod))}\n       `;\n\n       const result = await this.redis.eval(\n         script,\n         1,\n         this.key,\n         this.capacity,\n         this.refillRate,\n         this.refillPeriod,\n         tokens,\n         Date.now()\n       );\n\n       return {\n         allowed: result[0] === 1,\n         tokensRemaining: result[1],\n         timeToNextToken: result[2]\n       };\n     }\n   }\n\n   module.exports = { TokenBucket, DistributedTokenBucket };\n   ```\n\n   **Sliding Window Rate Limiter:**\n   ```javascript\n   // rate-limiters/sliding-window.js\n   class SlidingWindowRateLimiter {\n     constructor(redis, windowSize, maxRequests) {\n       this.redis = redis;\n       this.windowSize = windowSize; // in milliseconds\n       this.maxRequests = maxRequests;\n     }\n\n     async isAllowed(key) {\n       const now = Date.now();\n       const windowStart = now - this.windowSize;\n\n       const script = `\n         local key = KEYS[1]\n         local windowStart = tonumber(ARGV[1])\n         local now = tonumber(ARGV[2])\n         local maxRequests = tonumber(ARGV[3])\n         \n         -- Remove old entries\n         redis.call('ZREMRANGEBYSCORE', key, 0, windowStart)\n         \n         -- Count current requests in window\n         local currentCount = redis.call('ZCARD', key)\n         \n         if currentCount < maxRequests then\n           -- Add current request\n           redis.call('ZADD', key, now, now)\n           redis.call('EXPIRE', key, math.ceil(ARGV[4] / 1000))\n           return {1, currentCount + 1, maxRequests - currentCount - 1}\n         else\n           return {0, currentCount, 0}\n         end\n       `;\n\n       const result = await this.redis.eval(\n         script,\n         1,\n         key,\n         windowStart,\n         now,\n         this.maxRequests,\n         this.windowSize\n       );\n\n       return {\n         allowed: result[0] === 1,\n         currentCount: result[1],\n         remaining: result[2]\n       };\n     }\n\n     async getRemainingRequests(key) {\n       const now = Date.now();\n       const windowStart = now - this.windowSize;\n       \n       await this.redis.zremrangebyscore(key, 0, windowStart);\n       const currentCount = await this.redis.zcard(key);\n       \n       return Math.max(0, this.maxRequests - currentCount);\n     }\n   }\n\n   module.exports = SlidingWindowRateLimiter;\n   ```\n\n4. **Custom Rate Limiting Middleware**\n   - Build flexible rate limiting solutions:\n\n   **Advanced Rate Limiting Middleware:**\n   ```javascript\n   // middleware/advanced-rate-limiter.js\n   const { TokenBucket, DistributedTokenBucket } = require('../rate-limiters/token-bucket');\n   const SlidingWindowRateLimiter = require('../rate-limiters/sliding-window');\n\n   class AdvancedRateLimiter {\n     constructor(redis) {\n       this.redis = redis;\n       this.rateLimiters = new Map();\n       this.setupRateLimiters();\n     }\n\n     setupRateLimiters() {\n       // API endpoints with different limits\n       this.rateLimiters.set('api:general', {\n         type: 'sliding-window',\n         limiter: new SlidingWindowRateLimiter(this.redis, 60000, 1000) // 1000 req/min\n       });\n\n       this.rateLimiters.set('api:upload', {\n         type: 'token-bucket',\n         capacity: 10,\n         refillRate: 1,\n         refillPeriod: 10000 // 1 token per 10 seconds\n       });\n\n       this.rateLimiters.set('api:search', {\n         type: 'sliding-window',\n         limiter: new SlidingWindowRateLimiter(this.redis, 60000, 100) // 100 req/min\n       });\n     }\n\n     createMiddleware(limiterKey, options = {}) {\n       return async (req, res, next) => {\n         try {\n           const userKey = this.generateUserKey(req, limiterKey);\n           const config = this.rateLimiters.get(limiterKey);\n\n           if (!config) {\n             return next(); // No rate limiting configured\n           }\n\n           let result;\n           \n           if (config.type === 'sliding-window') {\n             result = await config.limiter.isAllowed(userKey);\n           } else if (config.type === 'token-bucket') {\n             const bucket = new DistributedTokenBucket(\n               this.redis,\n               userKey,\n               config.capacity,\n               config.refillRate,\n               config.refillPeriod\n             );\n             result = await bucket.consume(options.tokensRequired || 1);\n           }\n\n           // Set rate limit headers\n           this.setRateLimitHeaders(res, result, config);\n\n           if (!result.allowed) {\n             return res.status(429).json({\n               error: 'Rate limit exceeded',\n               retryAfter: this.calculateRetryAfter(result, config),\n               remaining: result.remaining || 0\n             });\n           }\n\n           // Add rate limit info to request\n           req.rateLimit = result;\n           next();\n\n         } catch (error) {\n           console.error('Rate limiting error:', error);\n           next(); // Fail open - don't block requests on rate limiter errors\n         }\n       };\n     }\n\n     generateUserKey(req, limiterKey) {\n       const userId = req.user?.id || req.ip;\n       const endpoint = req.route?.path || req.path;\n       return `${limiterKey}:${userId}:${endpoint}`;\n     }\n\n     setRateLimitHeaders(res, result, config) {\n       if (result.remaining !== undefined) {\n         res.set('X-RateLimit-Remaining', result.remaining.toString());\n       }\n       \n       if (result.currentCount !== undefined) {\n         res.set('X-RateLimit-Used', result.currentCount.toString());\n       }\n\n       if (config.type === 'sliding-window') {\n         res.set('X-RateLimit-Limit', config.limiter.maxRequests.toString());\n         res.set('X-RateLimit-Window', (config.limiter.windowSize / 1000).toString());\n       } else if (config.type === 'token-bucket') {\n         res.set('X-RateLimit-Limit', config.capacity.toString());\n       }\n     }\n\n     calculateRetryAfter(result, config) {\n       if (result.timeToNextToken) {\n         return Math.ceil(result.timeToNextToken / 1000);\n       }\n       \n       if (config.type === 'sliding-window') {\n         return Math.ceil(config.limiter.windowSize / 1000);\n       }\n       \n       return 60; // Default 1 minute\n     }\n\n     // Dynamic rate limiting based on system load\n     createAdaptiveLimiter(baseLimit) {\n       return async (req, res, next) => {\n         const systemLoad = await this.getSystemLoad();\n         let dynamicLimit = baseLimit;\n\n         // Reduce limits during high load\n         if (systemLoad > 0.8) {\n           dynamicLimit = Math.floor(baseLimit * 0.5);\n         } else if (systemLoad > 0.6) {\n           dynamicLimit = Math.floor(baseLimit * 0.7);\n         }\n\n         // Apply dynamic limit\n         const limiter = new SlidingWindowRateLimiter(this.redis, 60000, dynamicLimit);\n         const userKey = this.generateUserKey(req, 'adaptive');\n         const result = await limiter.isAllowed(userKey);\n\n         res.set('X-RateLimit-Adaptive', 'true');\n         res.set('X-RateLimit-System-Load', systemLoad.toString());\n         \n         if (!result.allowed) {\n           return res.status(429).json({\n             error: 'Rate limit exceeded (adaptive)',\n             systemLoad: systemLoad,\n             retryAfter: 60\n           });\n         }\n\n         next();\n       };\n     }\n\n     async getSystemLoad() {\n       // Get system metrics (CPU, memory, etc.)\n       const os = require('os');\n       const loadAvg = os.loadavg()[0]; // 1-minute load average\n       const cpuCount = os.cpus().length;\n       return Math.min(1, loadAvg / cpuCount);\n     }\n   }\n\n   module.exports = AdvancedRateLimiter;\n   ```\n\n5. **API Quota Management**\n   - Implement comprehensive quota systems:\n\n   **Quota Management System:**\n   ```javascript\n   // services/quota-manager.js\n   class QuotaManager {\n     constructor(redis, database) {\n       this.redis = redis;\n       this.database = database;\n       this.quotaTypes = {\n         'api_calls': { resetPeriod: 'monthly', defaultLimit: 10000 },\n         'data_transfer': { resetPeriod: 'monthly', defaultLimit: 1073741824 }, // 1GB in bytes\n         'storage': { resetPeriod: 'none', defaultLimit: 5368709120 }, // 5GB\n         'concurrent_requests': { resetPeriod: 'none', defaultLimit: 10 }\n       };\n     }\n\n     async checkQuota(userId, quotaType, amount = 1) {\n       const userQuota = await this.getUserQuota(userId, quotaType);\n       const currentUsage = await this.getCurrentUsage(userId, quotaType);\n\n       const available = userQuota.limit - currentUsage;\n       const allowed = available >= amount;\n\n       if (allowed) {\n         await this.incrementUsage(userId, quotaType, amount);\n       }\n\n       return {\n         allowed,\n         usage: currentUsage + (allowed ? amount : 0),\n         limit: userQuota.limit,\n         remaining: Math.max(0, available - (allowed ? amount : 0)),\n         resetDate: userQuota.resetDate\n       };\n     }\n\n     async getUserQuota(userId, quotaType) {\n       // Get user-specific quota from database\n       const customQuota = await this.database.query(\n         'SELECT * FROM user_quotas WHERE user_id = $1 AND quota_type = $2',\n         [userId, quotaType]\n       );\n\n       if (customQuota.rows.length > 0) {\n         return customQuota.rows[0];\n       }\n\n       // Get plan-based quota\n       const user = await this.database.query(\n         'SELECT plan FROM users WHERE id = $1',\n         [userId]\n       );\n\n       const planQuota = await this.getPlanQuota(user.rows[0]?.plan || 'free', quotaType);\n       return planQuota;\n     }\n\n     async getPlanQuota(plan, quotaType) {\n       const planQuotas = {\n         free: {\n           api_calls: 1000,\n           data_transfer: 104857600, // 100MB\n           storage: 1073741824, // 1GB\n           concurrent_requests: 5\n         },\n         basic: {\n           api_calls: 10000,\n           data_transfer: 1073741824, // 1GB\n           storage: 10737418240, // 10GB\n           concurrent_requests: 10\n         },\n         pro: {\n           api_calls: 100000,\n           data_transfer: 10737418240, // 10GB\n           storage: 107374182400, // 100GB\n           concurrent_requests: 50\n         },\n         enterprise: {\n           api_calls: 1000000,\n           data_transfer: 107374182400, // 100GB\n           storage: 1099511627776, // 1TB\n           concurrent_requests: 200\n         }\n       };\n\n       const limit = planQuotas[plan]?.[quotaType] || this.quotaTypes[quotaType].defaultLimit;\n       const resetDate = this.calculateResetDate(quotaType);\n\n       return { limit, resetDate };\n     }\n\n     async getCurrentUsage(userId, quotaType) {\n       const quotaConfig = this.quotaTypes[quotaType];\n       \n       if (quotaConfig.resetPeriod === 'none') {\n         // Non-resetting quota (like storage)\n         const key = `quota:${userId}:${quotaType}:current`;\n         const usage = await this.redis.get(key);\n         return parseInt(usage) || 0;\n       } else {\n         // Resetting quota (like monthly API calls)\n         const period = this.getCurrentPeriod(quotaConfig.resetPeriod);\n         const key = `quota:${userId}:${quotaType}:${period}`;\n         const usage = await this.redis.get(key);\n         return parseInt(usage) || 0;\n       }\n     }\n\n     async incrementUsage(userId, quotaType, amount) {\n       const quotaConfig = this.quotaTypes[quotaType];\n       \n       if (quotaConfig.resetPeriod === 'none') {\n         const key = `quota:${userId}:${quotaType}:current`;\n         await this.redis.incrby(key, amount);\n         await this.redis.expire(key, 86400 * 365); // 1 year TTL\n       } else {\n         const period = this.getCurrentPeriod(quotaConfig.resetPeriod);\n         const key = `quota:${userId}:${quotaType}:${period}`;\n         await this.redis.incrby(key, amount);\n         \n         // Set TTL to end of period\n         const ttl = this.getTTLForPeriod(quotaConfig.resetPeriod);\n         await this.redis.expire(key, ttl);\n       }\n\n       // Update usage analytics\n       await this.recordUsageAnalytics(userId, quotaType, amount);\n     }\n\n     getCurrentPeriod(resetPeriod) {\n       const now = new Date();\n       \n       switch (resetPeriod) {\n         case 'daily':\n           return now.toISOString().split('T')[0]; // YYYY-MM-DD\n         case 'weekly':\n           const weekStart = new Date(now);\n           weekStart.setDate(now.getDate() - now.getDay());\n           return weekStart.toISOString().split('T')[0];\n         case 'monthly':\n           return `${now.getFullYear()}-${String(now.getMonth() + 1).padStart(2, '0')}`;\n         case 'yearly':\n           return now.getFullYear().toString();\n         default:\n           return 'current';\n       }\n     }\n\n     calculateResetDate(quotaType) {\n       const config = this.quotaTypes[quotaType];\n       if (config.resetPeriod === 'none') return null;\n\n       const now = new Date();\n       const resetDate = new Date();\n\n       switch (config.resetPeriod) {\n         case 'daily':\n           resetDate.setDate(now.getDate() + 1);\n           resetDate.setHours(0, 0, 0, 0);\n           break;\n         case 'weekly':\n           resetDate.setDate(now.getDate() + (7 - now.getDay()));\n           resetDate.setHours(0, 0, 0, 0);\n           break;\n         case 'monthly':\n           resetDate.setMonth(now.getMonth() + 1, 1);\n           resetDate.setHours(0, 0, 0, 0);\n           break;\n         case 'yearly':\n           resetDate.setFullYear(now.getFullYear() + 1, 0, 1);\n           resetDate.setHours(0, 0, 0, 0);\n           break;\n       }\n\n       return resetDate;\n     }\n\n     getTTLForPeriod(resetPeriod) {\n       const resetDate = this.calculateResetDate({ resetPeriod });\n       return Math.ceil((resetDate.getTime() - Date.now()) / 1000);\n     }\n\n     async recordUsageAnalytics(userId, quotaType, amount) {\n       // Record usage for analytics and billing\n       const analyticsKey = `analytics:usage:${userId}:${quotaType}:${new Date().toISOString().split('T')[0]}`;\n       await this.redis.incrby(analyticsKey, amount);\n       await this.redis.expire(analyticsKey, 86400 * 90); // 90 days retention\n     }\n\n     // Middleware for quota checking\n     createQuotaMiddleware(quotaType, amountFn = () => 1) {\n       return async (req, res, next) => {\n         if (!req.user) {\n           return next(); // Skip quota check for unauthenticated requests\n         }\n\n         const amount = typeof amountFn === 'function' ? amountFn(req) : amountFn;\n         const result = await this.checkQuota(req.user.id, quotaType, amount);\n\n         // Set quota headers\n         res.set('X-Quota-Type', quotaType);\n         res.set('X-Quota-Limit', result.limit.toString());\n         res.set('X-Quota-Remaining', result.remaining.toString());\n         res.set('X-Quota-Used', result.usage.toString());\n         \n         if (result.resetDate) {\n           res.set('X-Quota-Reset', result.resetDate.toISOString());\n         }\n\n         if (!result.allowed) {\n           return res.status(429).json({\n             error: 'Quota exceeded',\n             quotaType: quotaType,\n             limit: result.limit,\n             usage: result.usage,\n             resetDate: result.resetDate\n           });\n         }\n\n         req.quota = result;\n         next();\n       };\n     }\n   }\n\n   module.exports = QuotaManager;\n   ```\n\n6. **Rate Limiting for Different Services**\n   - Implement service-specific rate limiting:\n\n   **Database Rate Limiting:**\n   ```javascript\n   // rate-limiters/database-rate-limiter.js\n   class DatabaseRateLimiter {\n     constructor(redis, pool) {\n       this.redis = redis;\n       this.pool = pool;\n       this.connectionLimiter = new Map();\n       this.queryLimiter = new Map();\n     }\n\n     // Limit concurrent database connections per user\n     async acquireConnection(userId) {\n       const key = `db:connections:${userId}`;\n       const maxConnections = await this.getMaxConnections(userId);\n       \n       const script = `\n         local key = KEYS[1]\n         local maxConnections = tonumber(ARGV[1])\n         local ttl = tonumber(ARGV[2])\n         \n         local current = redis.call('GET', key) or 0\n         current = tonumber(current)\n         \n         if current < maxConnections then\n           redis.call('INCR', key)\n           redis.call('EXPIRE', key, ttl)\n           return 1\n         else\n           return 0\n         end\n       `;\n\n       const allowed = await this.redis.eval(script, 1, key, maxConnections, 300); // 5 min TTL\n       \n       if (!allowed) {\n         throw new Error('Database connection limit exceeded');\n       }\n\n       return {\n         release: async () => {\n           await this.redis.decr(key);\n         }\n       };\n     }\n\n     // Rate limit expensive queries\n     async checkQueryLimit(userId, queryType, cost = 1) {\n       const key = `db:queries:${userId}:${queryType}`;\n       const windowMs = 60000; // 1 minute\n       const maxCost = await this.getMaxQueryCost(userId, queryType);\n\n       const script = `\n         local key = KEYS[1]\n         local windowMs = tonumber(ARGV[1])\n         local maxCost = tonumber(ARGV[2])\n         local cost = tonumber(ARGV[3])\n         local now = tonumber(ARGV[4])\n         \n         local windowStart = now - windowMs\n         \n         -- Remove old entries\n         redis.call('ZREMRANGEBYSCORE', key, 0, windowStart)\n         \n         -- Get current cost\n         local currentCost = 0\n         local entries = redis.call('ZRANGE', key, 0, -1, 'WITHSCORES')\n         for i = 2, #entries, 2 do\n           currentCost = currentCost + tonumber(entries[i])\n         end\n         \n         if currentCost + cost <= maxCost then\n           redis.call('ZADD', key, cost, now)\n           redis.call('EXPIRE', key, math.ceil(windowMs / 1000))\n           return {1, currentCost + cost, maxCost - currentCost - cost}\n         else\n           return {0, currentCost, maxCost - currentCost}\n         end\n       `;\n\n       const result = await this.redis.eval(\n         script, 1, key, windowMs, maxCost, cost, Date.now()\n       );\n\n       return {\n         allowed: result[0] === 1,\n         currentCost: result[1],\n         remaining: result[2]\n       };\n     }\n\n     async getMaxConnections(userId) {\n       // Get from user plan or use default\n       const user = await this.getUserPlan(userId);\n       const connectionLimits = {\n         free: 2,\n         basic: 5,\n         pro: 20,\n         enterprise: 100\n       };\n       return connectionLimits[user.plan] || 2;\n     }\n\n     async getMaxQueryCost(userId, queryType) {\n       const user = await this.getUserPlan(userId);\n       const costLimits = {\n         free: { select: 100, insert: 50, update: 30, delete: 10 },\n         basic: { select: 500, insert: 200, update: 100, delete: 50 },\n         pro: { select: 2000, insert: 1000, update: 500, delete: 200 },\n         enterprise: { select: 10000, insert: 5000, update: 2500, delete: 1000 }\n       };\n       return costLimits[user.plan]?.[queryType] || 10;\n     }\n   }\n   ```\n\n   **File Upload Rate Limiting:**\n   ```javascript\n   // rate-limiters/upload-rate-limiter.js\n   class UploadRateLimiter {\n     constructor(redis) {\n       this.redis = redis;\n     }\n\n     // Limit file upload size and frequency\n     async checkUploadLimit(userId, fileSize, fileType) {\n       const checks = await Promise.all([\n         this.checkFileSizeLimit(userId, fileSize),\n         this.checkUploadFrequency(userId),\n         this.checkStorageQuota(userId, fileSize),\n         this.checkFileTypeLimit(userId, fileType)\n       ]);\n\n       const failed = checks.find(check => !check.allowed);\n       if (failed) {\n         return failed;\n       }\n\n       // Record the upload\n       await this.recordUpload(userId, fileSize, fileType);\n\n       return { allowed: true, checks };\n     }\n\n     async checkFileSizeLimit(userId, fileSize) {\n       const user = await this.getUserPlan(userId);\n       const sizeLimits = {\n         free: 10 * 1024 * 1024,      // 10MB\n         basic: 50 * 1024 * 1024,     // 50MB\n         pro: 200 * 1024 * 1024,      // 200MB\n         enterprise: 1000 * 1024 * 1024 // 1GB\n       };\n\n       const maxSize = sizeLimits[user.plan] || sizeLimits.free;\n       const allowed = fileSize <= maxSize;\n\n       return {\n         allowed,\n         type: 'file_size',\n         current: fileSize,\n         limit: maxSize,\n         message: allowed ? null : `File size ${fileSize} exceeds limit of ${maxSize} bytes`\n       };\n     }\n\n     async checkUploadFrequency(userId) {\n       const key = `uploads:frequency:${userId}`;\n       const windowMs = 60000; // 1 minute\n       const maxUploads = await this.getMaxUploadsPerMinute(userId);\n\n       const current = await this.redis.incr(key);\n       if (current === 1) {\n         await this.redis.expire(key, Math.ceil(windowMs / 1000));\n       }\n\n       return {\n         allowed: current <= maxUploads,\n         type: 'upload_frequency',\n         current,\n         limit: maxUploads,\n         window: windowMs\n       };\n     }\n\n     async checkStorageQuota(userId, fileSize) {\n       const key = `storage:used:${userId}`;\n       const currentUsage = parseInt(await this.redis.get(key)) || 0;\n       const maxStorage = await this.getMaxStorage(userId);\n\n       const allowed = (currentUsage + fileSize) <= maxStorage;\n\n       return {\n         allowed,\n         type: 'storage_quota',\n         current: currentUsage + fileSize,\n         limit: maxStorage,\n         fileSize\n       };\n     }\n\n     async checkFileTypeLimit(userId, fileType) {\n       const allowedTypes = await this.getAllowedFileTypes(userId);\n       const allowed = allowedTypes.includes(fileType);\n\n       return {\n         allowed,\n         type: 'file_type',\n         fileType,\n         allowedTypes,\n         message: allowed ? null : `File type ${fileType} not allowed`\n       };\n     }\n\n     async recordUpload(userId, fileSize, fileType) {\n       const now = Date.now();\n       \n       // Update storage usage\n       await this.redis.incrby(`storage:used:${userId}`, fileSize);\n       \n       // Record upload in analytics\n       const analyticsKey = `analytics:uploads:${userId}:${new Date().toISOString().split('T')[0]}`;\n       await this.redis.hincrby(analyticsKey, 'count', 1);\n       await this.redis.hincrby(analyticsKey, 'bytes', fileSize);\n       await this.redis.expire(analyticsKey, 86400 * 30); // 30 days\n     }\n\n     createUploadMiddleware() {\n       return async (req, res, next) => {\n         if (!req.user) {\n           return res.status(401).json({ error: 'Authentication required' });\n         }\n\n         // Check if this is a file upload\n         if (!req.files || !req.files.length) {\n           return next();\n         }\n\n         for (const file of req.files) {\n           const result = await this.checkUploadLimit(\n             req.user.id,\n             file.size,\n             file.mimetype\n           );\n\n           if (!result.allowed) {\n             return res.status(429).json({\n               error: 'Upload limit exceeded',\n               ...result\n             });\n           }\n         }\n\n         next();\n       };\n     }\n   }\n   ```\n\n7. **Rate Limiting Dashboard and Analytics**\n   - Monitor and analyze rate limiting effectiveness:\n\n   **Rate Limiting Analytics:**\n   ```javascript\n   // analytics/rate-limit-analytics.js\n   class RateLimitAnalytics {\n     constructor(redis, database) {\n       this.redis = redis;\n       this.database = database;\n     }\n\n     async recordRateLimitHit(userId, endpoint, limitType, blocked) {\n       const timestamp = Date.now();\n       const date = new Date().toISOString().split('T')[0];\n\n       // Real-time metrics\n       const realtimeKey = `analytics:ratelimit:realtime:${limitType}`;\n       await this.redis.zadd(realtimeKey, timestamp, `${userId}:${endpoint}:${blocked}`);\n       await this.redis.expire(realtimeKey, 3600); // 1 hour\n\n       // Daily aggregates\n       const dailyKey = `analytics:ratelimit:daily:${date}:${limitType}`;\n       await this.redis.hincrby(dailyKey, 'total', 1);\n       if (blocked) {\n         await this.redis.hincrby(dailyKey, 'blocked', 1);\n       }\n       await this.redis.expire(dailyKey, 86400 * 30); // 30 days\n\n       // User-specific analytics\n       const userKey = `analytics:ratelimit:user:${userId}:${date}`;\n       await this.redis.hincrby(userKey, endpoint, 1);\n       if (blocked) {\n         await this.redis.hincrby(userKey, `${endpoint}:blocked`, 1);\n       }\n       await this.redis.expire(userKey, 86400 * 30);\n     }\n\n     async getRateLimitStats(timeRange = '24h') {\n       const now = Date.now();\n       const ranges = {\n         '1h': 3600000,\n         '24h': 86400000,\n         '7d': 604800000,\n         '30d': 2592000000\n       };\n\n       const rangeMs = ranges[timeRange] || ranges['24h'];\n       const startTime = now - rangeMs;\n\n       // Get realtime data for shorter ranges\n       if (rangeMs <= 3600000) {\n         return await this.getRealtimeStats(startTime, now);\n       }\n\n       // Get aggregated data for longer ranges\n       return await this.getAggregatedStats(startTime, now);\n     }\n\n     async getRealtimeStats(startTime, endTime) {\n       const limitTypes = ['general', 'auth', 'upload', 'api'];\n       const stats = {};\n\n       for (const limitType of limitTypes) {\n         const key = `analytics:ratelimit:realtime:${limitType}`;\n         const entries = await this.redis.zrangebyscore(key, startTime, endTime);\n         \n         let total = 0;\n         let blocked = 0;\n         const endpoints = {};\n\n         for (const entry of entries) {\n           const [userId, endpoint, isBlocked] = entry.split(':');\n           total++;\n           if (isBlocked === 'true') blocked++;\n\n           if (!endpoints[endpoint]) {\n             endpoints[endpoint] = { total: 0, blocked: 0 };\n           }\n           endpoints[endpoint].total++;\n           if (isBlocked === 'true') endpoints[endpoint].blocked++;\n         }\n\n         stats[limitType] = {\n           total,\n           blocked,\n           allowed: total - blocked,\n           blockRate: total > 0 ? (blocked / total) : 0,\n           endpoints\n         };\n       }\n\n       return stats;\n     }\n\n     async getTopBlockedEndpoints(timeRange = '24h', limit = 10) {\n       const stats = await this.getRateLimitStats(timeRange);\n       const endpointStats = [];\n\n       for (const [limitType, data] of Object.entries(stats)) {\n         for (const [endpoint, endpointData] of Object.entries(data.endpoints || {})) {\n           endpointStats.push({\n             endpoint,\n             limitType,\n             ...endpointData,\n             blockRate: endpointData.total > 0 ? (endpointData.blocked / endpointData.total) : 0\n           });\n         }\n       }\n\n       return endpointStats\n         .sort((a, b) => b.blocked - a.blocked)\n         .slice(0, limit);\n     }\n\n     async getUserRateLimitStats(userId, timeRange = '7d') {\n       const now = new Date();\n       const days = parseInt(timeRange.replace('d', ''));\n       const stats = [];\n\n       for (let i = 0; i < days; i++) {\n         const date = new Date(now - i * 86400000).toISOString().split('T')[0];\n         const key = `analytics:ratelimit:user:${userId}:${date}`;\n         const dayStats = await this.redis.hgetall(key);\n         \n         const endpoints = {};\n         for (const [field, value] of Object.entries(dayStats)) {\n           if (field.endsWith(':blocked')) {\n             const endpoint = field.replace(':blocked', '');\n             if (!endpoints[endpoint]) endpoints[endpoint] = { total: 0, blocked: 0 };\n             endpoints[endpoint].blocked = parseInt(value);\n           } else {\n             if (!endpoints[field]) endpoints[field] = { total: 0, blocked: 0 };\n             endpoints[field].total = parseInt(value);\n           }\n         }\n\n         stats.push({ date, endpoints });\n       }\n\n       return stats;\n     }\n\n     async generateRateLimitReport() {\n       const report = {\n         generatedAt: new Date().toISOString(),\n         summary: await this.getRateLimitStats('24h'),\n         topBlockedEndpoints: await this.getTopBlockedEndpoints('24h'),\n         trends: await this.getRateLimitTrends(),\n         recommendations: await this.generateRecommendations()\n       };\n\n       return report;\n     }\n\n     async generateRecommendations() {\n       const stats = await this.getRateLimitStats('24h');\n       const recommendations = [];\n\n       for (const [limitType, data] of Object.entries(stats)) {\n         if (data.blockRate > 0.1) { // >10% block rate\n           recommendations.push({\n             severity: 'high',\n             type: 'high_block_rate',\n             limitType,\n             blockRate: data.blockRate,\n             message: `High block rate (${(data.blockRate * 100).toFixed(1)}%) for ${limitType} rate limiter`,\n             suggestions: [\n               'Consider increasing rate limits for legitimate users',\n               'Implement user-specific rate limiting',\n               'Add rate limit exemptions for trusted IPs'\n             ]\n           });\n         }\n\n         if (data.total > 100000) { // High volume\n           recommendations.push({\n             severity: 'medium',\n             type: 'high_volume',\n             limitType,\n             volume: data.total,\n             message: `High request volume (${data.total}) detected for ${limitType}`,\n             suggestions: [\n               'Monitor for potential abuse patterns',\n               'Consider implementing adaptive rate limiting',\n               'Review capacity planning'\n             ]\n           });\n         }\n       }\n\n       return recommendations;\n     }\n   }\n\n   module.exports = RateLimitAnalytics;\n   ```\n\n8. **Rate Limiting Configuration Management**\n   - Dynamic rate limit configuration:\n\n   **Configuration Manager:**\n   ```javascript\n   // config/rate-limit-config.js\n   class RateLimitConfigManager {\n     constructor(redis, database) {\n       this.redis = redis;\n       this.database = database;\n       this.configCache = new Map();\n       this.setupDefaultConfigs();\n     }\n\n     setupDefaultConfigs() {\n       this.defaultConfigs = {\n         'api:general': {\n           windowMs: 900000, // 15 minutes\n           max: 1000,\n           algorithm: 'sliding-window',\n           skipSuccessfulRequests: false,\n           enabled: true\n         },\n         'api:auth': {\n           windowMs: 900000, // 15 minutes\n           max: 5,\n           algorithm: 'token-bucket',\n           skipSuccessfulRequests: true,\n           enabled: true\n         },\n         'api:upload': {\n           capacity: 10,\n           refillRate: 1,\n           refillPeriod: 10000,\n           algorithm: 'token-bucket',\n           enabled: true\n         },\n         'api:search': {\n           windowMs: 60000, // 1 minute\n           max: 100,\n           algorithm: 'sliding-window',\n           enabled: true\n         }\n       };\n     }\n\n     async getConfig(limiterId) {\n       // Check cache first\n       if (this.configCache.has(limiterId)) {\n         const cached = this.configCache.get(limiterId);\n         if (Date.now() - cached.timestamp < 300000) { // 5 min cache\n           return cached.config;\n         }\n       }\n\n       // Get from database\n       let config = await this.database.query(\n         'SELECT * FROM rate_limit_configs WHERE limiter_id = $1',\n         [limiterId]\n       );\n\n       if (config.rows.length === 0) {\n         // Use default config\n         config = this.defaultConfigs[limiterId] || this.defaultConfigs['api:general'];\n       } else {\n         config = config.rows[0].config;\n       }\n\n       // Cache the config\n       this.configCache.set(limiterId, {\n         config,\n         timestamp: Date.now()\n       });\n\n       return config;\n     }\n\n     async updateConfig(limiterId, newConfig, userId) {\n       // Validate config\n       const validationResult = this.validateConfig(newConfig);\n       if (!validationResult.valid) {\n         throw new Error(`Invalid config: ${validationResult.errors.join(', ')}`);\n       }\n\n       // Save to database\n       await this.database.query(`\n         INSERT INTO rate_limit_configs (limiter_id, config, updated_by, updated_at)\n         VALUES ($1, $2, $3, NOW())\n         ON CONFLICT (limiter_id) \n         DO UPDATE SET config = $2, updated_by = $3, updated_at = NOW()\n       `, [limiterId, JSON.stringify(newConfig), userId]);\n\n       // Clear cache\n       this.configCache.delete(limiterId);\n\n       // Notify other instances of config change\n       await this.redis.publish('rate-limit-config-update', JSON.stringify({\n         limiterId,\n         config: newConfig,\n         updatedBy: userId,\n         timestamp: Date.now()\n       }));\n\n       return newConfig;\n     }\n\n     validateConfig(config) {\n       const errors = [];\n\n       if (config.algorithm === 'sliding-window') {\n         if (!config.windowMs || config.windowMs < 1000) {\n           errors.push('windowMs must be at least 1000ms');\n         }\n         if (!config.max || config.max < 1) {\n           errors.push('max must be at least 1');\n         }\n       } else if (config.algorithm === 'token-bucket') {\n         if (!config.capacity || config.capacity < 1) {\n           errors.push('capacity must be at least 1');\n         }\n         if (!config.refillRate || config.refillRate < 1) {\n           errors.push('refillRate must be at least 1');\n         }\n         if (!config.refillPeriod || config.refillPeriod < 1000) {\n           errors.push('refillPeriod must be at least 1000ms');\n         }\n       } else {\n         errors.push('algorithm must be either sliding-window or token-bucket');\n       }\n\n       return {\n         valid: errors.length === 0,\n         errors\n       };\n     }\n\n     // A/B testing for rate limit configurations\n     async createABTest(limiterId, configA, configB, trafficSplit = 0.5) {\n       const testId = `ab-test-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;\n       \n       await this.database.query(`\n         INSERT INTO rate_limit_ab_tests \n         (test_id, limiter_id, config_a, config_b, traffic_split, created_at, status)\n         VALUES ($1, $2, $3, $4, $5, NOW(), 'active')\n       `, [testId, limiterId, JSON.stringify(configA), JSON.stringify(configB), trafficSplit]);\n\n       return testId;\n     }\n\n     async getABTestConfig(limiterId, userKey) {\n       const activeTest = await this.database.query(`\n         SELECT * FROM rate_limit_ab_tests \n         WHERE limiter_id = $1 AND status = 'active'\n         ORDER BY created_at DESC LIMIT 1\n       `, [limiterId]);\n\n       if (activeTest.rows.length === 0) {\n         return await this.getConfig(limiterId);\n       }\n\n       const test = activeTest.rows[0];\n       const hash = this.hashString(userKey);\n       const bucket = (hash % 100) / 100;\n\n       if (bucket < test.traffic_split) {\n         return test.config_a;\n       } else {\n         return test.config_b;\n       }\n     }\n\n     hashString(str) {\n       let hash = 0;\n       for (let i = 0; i < str.length; i++) {\n         const char = str.charCodeAt(i);\n         hash = ((hash << 5) - hash) + char;\n         hash = hash & hash; // Convert to 32-bit integer\n       }\n       return Math.abs(hash);\n     }\n\n     // Admin dashboard endpoints\n     async getAllConfigs() {\n       const configs = await this.database.query(`\n         SELECT limiter_id, config, updated_by, updated_at \n         FROM rate_limit_configs \n         ORDER BY updated_at DESC\n       `);\n\n       return configs.rows.map(row => ({\n         limiterId: row.limiter_id,\n         config: row.config,\n         updatedBy: row.updated_by,\n         updatedAt: row.updated_at\n       }));\n     }\n\n     async getConfigHistory(limiterId) {\n       const history = await this.database.query(`\n         SELECT config, updated_by, updated_at \n         FROM rate_limit_config_history \n         WHERE limiter_id = $1 \n         ORDER BY updated_at DESC \n         LIMIT 50\n       `, [limiterId]);\n\n       return history.rows;\n     }\n   }\n\n   module.exports = RateLimitConfigManager;\n   ```\n\n9. **Testing Rate Limits**\n   - Comprehensive rate limiting tests:\n\n   **Rate Limiting Test Suite:**\n   ```javascript\n   // tests/rate-limiting.test.js\n   const request = require('supertest');\n   const app = require('../app');\n   const Redis = require('ioredis');\n\n   describe('Rate Limiting', () => {\n     let redis;\n\n     beforeAll(async () => {\n       redis = new Redis(process.env.REDIS_TEST_URL);\n     });\n\n     afterEach(async () => {\n       // Clean up rate limiting keys\n       const keys = await redis.keys('*rate*');\n       if (keys.length > 0) {\n         await redis.del(...keys);\n       }\n     });\n\n     afterAll(async () => {\n       await redis.disconnect();\n     });\n\n     describe('General API Rate Limiting', () => {\n       test('should allow requests within limit', async () => {\n         for (let i = 0; i < 5; i++) {\n           const response = await request(app)\n             .get('/api/test')\n             .expect(200);\n\n           expect(response.headers).toHaveProperty('x-ratelimit-remaining');\n           expect(parseInt(response.headers['x-ratelimit-remaining'])).toBeGreaterThan(0);\n         }\n       });\n\n       test('should block requests exceeding limit', async () => {\n         // Make requests up to the limit\n         const limit = 10; // Assuming limit is 10 for test endpoint\n         \n         for (let i = 0; i < limit; i++) {\n           await request(app).get('/api/test').expect(200);\n         }\n\n         // Next request should be rate limited\n         const response = await request(app)\n           .get('/api/test')\n           .expect(429);\n\n         expect(response.body).toHaveProperty('error');\n         expect(response.body.error).toContain('Rate limit exceeded');\n       });\n\n       test('should include proper rate limit headers', async () => {\n         const response = await request(app)\n           .get('/api/test')\n           .expect(200);\n\n         expect(response.headers).toHaveProperty('x-ratelimit-limit');\n         expect(response.headers).toHaveProperty('x-ratelimit-remaining');\n         expect(response.headers).toHaveProperty('x-ratelimit-window');\n       });\n\n       test('should reset rate limit after window expires', async () => {\n         // Use a short window for testing\n         const shortWindowApp = createTestAppWithShortWindow(1000); // 1 second\n\n         // Exhaust the limit\n         await request(shortWindowApp).get('/api/test').expect(200);\n         await request(shortWindowApp).get('/api/test').expect(429);\n\n         // Wait for window to reset\n         await new Promise(resolve => setTimeout(resolve, 1100));\n\n         // Should allow requests again\n         await request(shortWindowApp).get('/api/test').expect(200);\n       });\n     });\n\n     describe('Authentication Rate Limiting', () => {\n       test('should limit failed login attempts', async () => {\n         const loginData = { email: 'test@example.com', password: 'wrongpassword' };\n\n         // Make several failed attempts\n         for (let i = 0; i < 5; i++) {\n           await request(app)\n             .post('/api/auth/login')\n             .send(loginData)\n             .expect(401);\n         }\n\n         // Next attempt should be rate limited\n         const response = await request(app)\n           .post('/api/auth/login')\n           .send(loginData)\n           .expect(429);\n\n         expect(response.body.error).toContain('Too many authentication attempts');\n       });\n\n       test('should not count successful logins against rate limit', async () => {\n         const loginData = { email: 'test@example.com', password: 'correctpassword' };\n\n         // Make successful login attempts\n         for (let i = 0; i < 3; i++) {\n           await request(app)\n             .post('/api/auth/login')\n             .send(loginData)\n             .expect(200);\n         }\n\n         // Should still allow more attempts\n         await request(app)\n           .post('/api/auth/login')\n           .send(loginData)\n           .expect(200);\n       });\n     });\n\n     describe('User-Specific Rate Limiting', () => {\n       test('should apply different limits based on user tier', async () => {\n         const freeUserToken = await getTestToken('free');\n         const proUserToken = await getTestToken('pro');\n\n         // Free user should have lower limits\n         const freeUserLimit = await findRateLimit(app, '/api/data', freeUserToken);\n         \n         // Pro user should have higher limits\n         const proUserLimit = await findRateLimit(app, '/api/data', proUserToken);\n\n         expect(proUserLimit).toBeGreaterThan(freeUserLimit);\n       });\n\n       test('should rate limit by user ID when authenticated', async () => {\n         const userToken = await getTestToken();\n         \n         // Make requests with user token\n         for (let i = 0; i < 10; i++) {\n           await request(app)\n             .get('/api/user/profile')\n             .set('Authorization', `Bearer ${userToken}`)\n             .expect(200);\n         }\n\n         // Should be rate limited\n         await request(app)\n           .get('/api/user/profile')\n           .set('Authorization', `Bearer ${userToken}`)\n           .expect(429);\n       });\n     });\n\n     describe('Quota Management', () => {\n       test('should enforce API call quotas', async () => {\n         const userToken = await getTestToken('basic'); // Basic plan has limited quota\n         \n         // Make requests up to quota limit\n         const quota = await getUserQuota('basic', 'api_calls');\n         \n         for (let i = 0; i < quota; i++) {\n           await request(app)\n             .get('/api/data')\n             .set('Authorization', `Bearer ${userToken}`)\n             .expect(200);\n         }\n\n         // Next request should exceed quota\n         const response = await request(app)\n           .get('/api/data')\n           .set('Authorization', `Bearer ${userToken}`)\n           .expect(429);\n\n         expect(response.body.error).toContain('Quota exceeded');\n         expect(response.body).toHaveProperty('quotaType', 'api_calls');\n       });\n\n       test('should include quota headers in responses', async () => {\n         const userToken = await getTestToken();\n         \n         const response = await request(app)\n           .get('/api/data')\n           .set('Authorization', `Bearer ${userToken}`)\n           .expect(200);\n\n         expect(response.headers).toHaveProperty('x-quota-limit');\n         expect(response.headers).toHaveProperty('x-quota-remaining');\n         expect(response.headers).toHaveProperty('x-quota-used');\n       });\n     });\n\n     describe('Rate Limiting Bypass', () => {\n       test('should bypass rate limits for internal requests', async () => {\n         // Make many requests with internal header\n         for (let i = 0; i < 100; i++) {\n           await request(app)\n             .get('/api/test')\n             .set('X-Internal-Request', 'true')\n             .expect(200);\n         }\n\n         // All should succeed\n       });\n\n       test('should bypass rate limits for whitelisted IPs', async () => {\n         // Configure test to use whitelisted IP\n         // This would depend on your specific implementation\n       });\n     });\n\n     // Helper functions\n     async function findRateLimit(app, endpoint, token) {\n       let requests = 0;\n       \n       while (requests < 1000) { // Safety limit\n         const response = await request(app)\n           .get(endpoint)\n           .set('Authorization', `Bearer ${token}`);\n         \n         requests++;\n         \n         if (response.status === 429) {\n           return requests - 1;\n         }\n       }\n       \n       return requests;\n     }\n\n     async function getTestToken(tier = 'free') {\n       // Implementation depends on your auth system\n       return 'test-token';\n     }\n\n     async function getUserQuota(plan, quotaType) {\n       const quotas = {\n         free: { api_calls: 100 },\n         basic: { api_calls: 1000 },\n         pro: { api_calls: 10000 }\n       };\n       return quotas[plan][quotaType];\n     }\n\n     function createTestAppWithShortWindow(windowMs) {\n       // Create a test app instance with short rate limit window\n       // Implementation depends on your app structure\n       return app;\n     }\n   });\n   ```\n\n10. **Production Monitoring and Alerting**\n    - Monitor rate limiting effectiveness:\n\n    **Rate Limiting Monitoring:**\n    ```javascript\n    // monitoring/rate-limit-monitor.js\n    class RateLimitMonitor {\n      constructor(redis, alertService) {\n        this.redis = redis;\n        this.alertService = alertService;\n        this.thresholds = {\n          highBlockRate: 0.15, // 15%\n          highVolume: 10000,    // requests per minute\n          quotaExhaustion: 0.9  // 90% quota usage\n        };\n      }\n\n      async startMonitoring(interval = 60000) {\n        setInterval(async () => {\n          await this.checkRateLimitHealth();\n        }, interval);\n      }\n\n      async checkRateLimitHealth() {\n        const metrics = await this.collectMetrics();\n        const alerts = [];\n\n        // Check for high block rates\n        for (const [limitType, data] of Object.entries(metrics)) {\n          if (data.blockRate > this.thresholds.highBlockRate) {\n            alerts.push({\n              type: 'high_block_rate',\n              limitType,\n              blockRate: data.blockRate,\n              message: `High block rate (${(data.blockRate * 100).toFixed(1)}%) for ${limitType}`,\n              severity: 'warning'\n            });\n          }\n\n          if (data.requestsPerMinute > this.thresholds.highVolume) {\n            alerts.push({\n              type: 'high_volume',\n              limitType,\n              volume: data.requestsPerMinute,\n              message: `High request volume (${data.requestsPerMinute}/min) for ${limitType}`,\n              severity: 'info'\n            });\n          }\n        }\n\n        // Check for quota exhaustion patterns\n        const quotaAlerts = await this.checkQuotaExhaustion();\n        alerts.push(...quotaAlerts);\n\n        // Send alerts\n        for (const alert of alerts) {\n          await this.alertService.sendAlert(alert);\n        }\n\n        // Store metrics for historical analysis\n        await this.storeMetrics(metrics);\n      }\n\n      async collectMetrics() {\n        const limitTypes = ['general', 'auth', 'upload', 'api'];\n        const metrics = {};\n        const now = Date.now();\n        const minuteAgo = now - 60000;\n\n        for (const limitType of limitTypes) {\n          const key = `analytics:ratelimit:realtime:${limitType}`;\n          const entries = await this.redis.zrangebyscore(key, minuteAgo, now);\n          \n          let total = 0;\n          let blocked = 0;\n\n          for (const entry of entries) {\n            const [userId, endpoint, isBlocked] = entry.split(':');\n            total++;\n            if (isBlocked === 'true') blocked++;\n          }\n\n          metrics[limitType] = {\n            total,\n            blocked,\n            allowed: total - blocked,\n            blockRate: total > 0 ? (blocked / total) : 0,\n            requestsPerMinute: total\n          };\n        }\n\n        return metrics;\n      }\n\n      async checkQuotaExhaustion() {\n        const alerts = [];\n        const quotaKeys = await this.redis.keys('quota:*:current');\n\n        for (const key of quotaKeys.slice(0, 100)) { // Limit to prevent overload\n          const [, userId, quotaType] = key.split(':');\n          const usage = parseInt(await this.redis.get(key)) || 0;\n          \n          // Get user's quota limit\n          const limit = await this.getUserQuotaLimit(userId, quotaType);\n          const usageRate = usage / limit;\n\n          if (usageRate > this.thresholds.quotaExhaustion) {\n            alerts.push({\n              type: 'quota_exhaustion',\n              userId,\n              quotaType,\n              usage,\n              limit,\n              usageRate,\n              message: `User ${userId} has used ${(usageRate * 100).toFixed(1)}% of ${quotaType} quota`,\n              severity: 'warning'\n            });\n          }\n        }\n\n        return alerts;\n      }\n\n      async storeMetrics(metrics) {\n        const timestamp = Date.now();\n        const metricsKey = `metrics:ratelimit:${timestamp}`;\n        \n        await this.redis.hmset(metricsKey, \n          'timestamp', timestamp,\n          'metrics', JSON.stringify(metrics)\n        );\n        await this.redis.expire(metricsKey, 86400 * 7); // 7 days retention\n      }\n\n      async generateHealthReport() {\n        const endTime = Date.now();\n        const startTime = endTime - 86400000; // 24 hours\n        \n        const metricKeys = await this.redis.keys('metrics:ratelimit:*');\n        const recentKeys = metricKeys.filter(key => {\n          const timestamp = parseInt(key.split(':')[2]);\n          return timestamp >= startTime && timestamp <= endTime;\n        });\n\n        const metrics = [];\n        for (const key of recentKeys) {\n          const data = await this.redis.hgetall(key);\n          metrics.push({\n            timestamp: parseInt(data.timestamp),\n            metrics: JSON.parse(data.metrics)\n          });\n        }\n\n        return {\n          period: { start: startTime, end: endTime },\n          dataPoints: metrics.length,\n          summary: this.calculateSummaryStats(metrics),\n          trends: this.calculateTrends(metrics),\n          recommendations: this.generateRecommendations(metrics)\n        };\n      }\n\n      calculateSummaryStats(metrics) {\n        if (metrics.length === 0) return {};\n\n        const summary = {};\n        const limitTypes = ['general', 'auth', 'upload', 'api'];\n\n        for (const limitType of limitTypes) {\n          const values = metrics.map(m => m.metrics[limitType]).filter(Boolean);\n          \n          if (values.length > 0) {\n            summary[limitType] = {\n              avgBlockRate: values.reduce((sum, v) => sum + v.blockRate, 0) / values.length,\n              avgVolume: values.reduce((sum, v) => sum + v.requestsPerMinute, 0) / values.length,\n              maxVolume: Math.max(...values.map(v => v.requestsPerMinute)),\n              totalRequests: values.reduce((sum, v) => sum + v.total, 0),\n              totalBlocked: values.reduce((sum, v) => sum + v.blocked, 0)\n            };\n          }\n        }\n\n        return summary;\n      }\n\n      calculateTrends(metrics) {\n        // Simple trend calculation - compare first and last hour\n        if (metrics.length < 2) return {};\n\n        const firstHour = metrics.slice(0, Math.min(60, metrics.length));\n        const lastHour = metrics.slice(-Math.min(60, metrics.length));\n\n        const trends = {};\n        const limitTypes = ['general', 'auth', 'upload', 'api'];\n\n        for (const limitType of limitTypes) {\n          const firstAvg = this.calculateAverage(firstHour, limitType, 'requestsPerMinute');\n          const lastAvg = this.calculateAverage(lastHour, limitType, 'requestsPerMinute');\n          \n          if (firstAvg > 0) {\n            trends[limitType] = {\n              volumeChange: ((lastAvg - firstAvg) / firstAvg) * 100,\n              direction: lastAvg > firstAvg ? 'increasing' : 'decreasing'\n            };\n          }\n        }\n\n        return trends;\n      }\n\n      calculateAverage(metrics, limitType, field) {\n        const values = metrics\n          .map(m => m.metrics[limitType]?.[field])\n          .filter(v => v !== undefined);\n        \n        return values.length > 0 ? values.reduce((sum, v) => sum + v, 0) / values.length : 0;\n      }\n\n      generateRecommendations(metrics) {\n        const recommendations = [];\n        const summary = this.calculateSummaryStats(metrics);\n\n        for (const [limitType, stats] of Object.entries(summary)) {\n          if (stats.avgBlockRate > 0.1) {\n            recommendations.push({\n              priority: 'high',\n              type: 'increase_limits',\n              limitType,\n              current: `${(stats.avgBlockRate * 100).toFixed(1)}% block rate`,\n              suggestion: `Consider increasing rate limits for ${limitType} - high block rate indicates legitimate users may be affected`\n            });\n          }\n\n          if (stats.avgVolume > 1000 && stats.avgBlockRate < 0.01) {\n            recommendations.push({\n              priority: 'medium',\n              type: 'optimize_performance',\n              limitType,\n              current: `${stats.avgVolume.toFixed(0)} requests/min`,\n              suggestion: `High volume with low block rate for ${limitType} - consider optimizing backend performance`\n            });\n          }\n        }\n\n        return recommendations;\n      }\n    }\n\n    module.exports = RateLimitMonitor;\n    ```"
              },
              {
                "name": "/setup-visual-testing",
                "description": "Setup visual regression testing",
                "path": "plugins/all-commands/commands/setup-visual-testing.md",
                "frontmatter": {
                  "description": "Setup visual regression testing",
                  "category": "code-analysis-testing"
                },
                "content": "# Setup Visual Testing\n\nSetup visual regression testing\n\n## Instructions\n\n1. **Visual Testing Strategy Analysis**\n   - Analyze current UI/component structure and testing needs\n   - Identify critical user interfaces and visual components\n   - Determine testing scope (components, pages, user flows)\n   - Assess existing testing infrastructure and integration points\n   - Plan visual testing coverage and baseline creation strategy\n\n2. **Visual Testing Tool Selection**\n   - Evaluate visual testing tools based on project requirements:\n     - **Chromatic**: For Storybook integration and component testing\n     - **Percy**: For comprehensive visual testing and CI integration\n     - **Playwright**: For browser-based visual testing with built-in capabilities\n     - **BackstopJS**: For lightweight visual regression testing\n     - **Applitools**: For AI-powered visual testing and cross-browser support\n   - Consider factors: budget, team size, CI/CD integration, browser support\n\n3. **Visual Testing Framework Installation**\n   - Install chosen visual testing tool and dependencies\n   - Configure testing framework integration (Jest, Playwright, Cypress)\n   - Set up browser automation and screenshot capabilities\n   - Configure testing environment and viewport settings\n   - Set up test runner and execution environment\n\n4. **Baseline Creation and Management**\n   - Create initial visual baselines for all critical UI components\n   - Establish baseline approval workflow and review process\n   - Set up baseline version control and storage\n   - Configure baseline updates and maintenance procedures\n   - Implement baseline branching strategy for feature development\n\n5. **Test Configuration and Setup**\n   - Configure visual testing parameters (viewports, browsers, devices)\n   - Set up visual diff thresholds and sensitivity settings\n   - Configure screenshot capture settings and optimization\n   - Set up test data and state management for consistent testing\n   - Configure async loading and timing handling\n\n6. **Component and Page Testing**\n   - Create visual tests for individual UI components\n   - Set up page-level visual testing for critical user flows\n   - Configure responsive design testing across different viewports\n   - Implement cross-browser visual testing\n   - Set up accessibility and color contrast visual validation\n\n7. **CI/CD Pipeline Integration**\n   - Configure automated visual testing in CI/CD pipeline\n   - Set up visual test execution on pull requests\n   - Configure test result reporting and notifications\n   - Set up deployment blocking for failed visual tests\n   - Implement parallel test execution for performance\n\n8. **Review and Approval Workflow**\n   - Set up visual diff review and approval process\n   - Configure team notifications for visual changes\n   - Establish approval authority and review guidelines\n   - Set up automated approval for minor acceptable changes\n   - Configure change documentation and tracking\n\n9. **Monitoring and Maintenance**\n   - Set up visual test performance monitoring\n   - Configure test flakiness detection and resolution\n   - Implement baseline cleanup and maintenance procedures\n   - Set up visual testing metrics and reporting\n   - Configure alerting for test failures and issues\n\n10. **Documentation and Team Training**\n    - Create comprehensive visual testing documentation\n    - Document baseline creation and update procedures\n    - Create troubleshooting guide for common visual testing issues\n    - Train team on visual testing workflows and best practices\n    - Set up visual testing standards and conventions\n    - Document visual testing maintenance and optimization procedures"
              },
              {
                "name": "/simulation-calibrator",
                "description": "Test and refine simulation accuracy with validation loops, bias detection, and continuous improvement frameworks.",
                "path": "plugins/all-commands/commands/simulation-calibrator.md",
                "frontmatter": {
                  "description": "Test and refine simulation accuracy with validation loops, bias detection, and continuous improvement frameworks.",
                  "category": "simulation-modeling",
                  "argument-hint": "Specify calibration parameters"
                },
                "content": "# Simulation Calibrator\n\nTest and refine simulation accuracy with validation loops, bias detection, and continuous improvement frameworks.\n\n## Instructions\n\nYou are tasked with systematically calibrating simulations to ensure accuracy, reliability, and actionable insights. Follow this approach: **$ARGUMENTS**\n\n### 1. Prerequisites Assessment\n\n**Critical Calibration Context Validation:**\n\n- **Simulation Type**: What kind of simulation are you calibrating?\n- **Accuracy Requirements**: How precise does the simulation need to be?\n- **Validation Data**: What real-world data can test simulation accuracy?\n- **Decision Stakes**: How important are the decisions based on this simulation?\n- **Update Frequency**: How often should calibration be performed?\n\n**If context is unclear, guide systematically:**\n\n```\nMissing Simulation Context:\n\"What type of simulation needs calibration?\n- Business Simulations: Market response, financial projections, strategic scenarios\n- Technical Simulations: System performance, architecture behavior, scaling predictions\n- Process Simulations: Operational workflows, resource allocation, timeline predictions\n- Behavioral Simulations: Customer behavior, team dynamics, adoption patterns\n\nEach simulation type requires different calibration approaches and validation methods.\"\n\nMissing Accuracy Requirements:\n\"How accurate does your simulation need to be for effective decision-making?\n- Mission Critical (95%+ accuracy): Safety, financial, or legal decisions\n- Strategic Planning (80-95% accuracy): Investment, expansion, or major initiative decisions\n- Operational Optimization (70-80% accuracy): Process improvement and resource allocation\n- Exploratory Analysis (50-70% accuracy): Option generation and conceptual understanding\"\n```\n\n### 2. Baseline Accuracy Assessment\n\n**Establish current simulation performance levels:**\n\n#### Historical Validation Framework\n```\nSimulation Accuracy Baseline:\n\nBack-Testing Analysis:\n- Compare simulation predictions to known historical outcomes\n- Measure prediction accuracy across different time horizons\n- Identify systematic biases and error patterns\n- Assess prediction confidence calibration\n\nAccuracy Metrics:\n- Overall Prediction Accuracy: [percentage of correct predictions]\n- Directional Accuracy: [percentage of correct trend predictions]\n- Magnitude Accuracy: [percentage of predictions within acceptable error range]\n- Timing Accuracy: [percentage of events predicted within correct timeframe]\n- Confidence Calibration: [alignment between prediction confidence and actual accuracy]\n\nError Pattern Analysis:\n- Systematic Biases: [consistent over/under-estimation patterns]\n- Context Dependencies: [accuracy variations by scenario type or conditions]\n- Time Horizon Effects: [accuracy changes over different prediction periods]\n- Complexity Correlation: [accuracy relationship to scenario complexity]\n```\n\n#### Simulation Quality Scoring\n```\nQuality Assessment Framework:\n\nInput Quality (25% weight):\n- Data completeness and accuracy\n- Assumption validation and documentation\n- Expert input quality and consensus\n- Historical precedent availability\n\nModel Quality (25% weight):\n- Algorithm sophistication and appropriateness\n- Relationship modeling accuracy and completeness\n- Constraint modeling and boundary definition\n- Uncertainty quantification and propagation\n\nProcess Quality (25% weight):\n- Systematic methodology application\n- Bias detection and mitigation\n- Stakeholder validation and feedback integration\n- Documentation and reproducibility\n\nOutput Quality (25% weight):\n- Prediction accuracy and reliability\n- Insight actionability and clarity\n- Decision support effectiveness\n- Communication and presentation quality\n\nOverall Simulation Quality Score = Sum of weighted component scores\n```\n\n### 3. Systematic Bias Detection\n\n**Identify and correct simulation biases:**\n\n#### Bias Identification Framework\n```\nCommon Simulation Biases:\n\nCognitive Biases:\n- Confirmation Bias: Seeking information that supports expected outcomes\n- Anchoring Bias: Over-relying on first estimates or reference points\n- Availability Bias: Overweighting easily recalled or recent examples\n- Optimism Bias: Systematic overestimation of positive outcomes\n- Planning Fallacy: Underestimating time and resource requirements\n\nData Biases:\n- Selection Bias: Non-representative data samples\n- Survivorship Bias: Only analyzing successful cases\n- Recency Bias: Overweighting recent data points\n- Historical Bias: Assuming past patterns will continue unchanged\n- Measurement Bias: Systematic errors in data collection\n\nModel Biases:\n- Complexity Bias: Over-simplifying or over-complicating models\n- Linear Bias: Assuming linear relationships where non-linear exist\n- Static Bias: Not accounting for dynamic system changes\n- Independence Bias: Ignoring correlation and interaction effects\n- Boundary Bias: Incorrect system boundary definition\n```\n\n#### Bias Mitigation Strategies\n```\nSystematic Bias Correction:\n\nProcess-Based Mitigation:\n- Multiple perspective integration and diverse expert consultation\n- Red team analysis and devil's advocate approaches\n- Assumption challenging and alternative hypothesis testing\n- Structured decision-making and bias-aware processes\n\nData-Based Mitigation:\n- Multiple data source integration and cross-validation\n- Out-of-sample testing and validation dataset use\n- Temporal validation across different time periods\n- Segment validation across different contexts and conditions\n\nModel-Based Mitigation:\n- Ensemble modeling and multiple algorithm approaches\n- Sensitivity analysis and robust parameter testing\n- Cross-validation and bootstrap sampling\n- Bayesian updating and continuous learning integration\n```\n\n### 4. Validation Loop Design\n\n**Create systematic accuracy improvement processes:**\n\n#### Multi-Level Validation Framework\n```\nComprehensive Validation Approach:\n\nLevel 1: Internal Consistency Validation\n- Logical consistency checking and constraint satisfaction\n- Mathematical relationship verification and balance testing\n- Scenario coherence and narrative consistency\n- Assumption compatibility and interaction validation\n\nLevel 2: Expert Validation\n- Domain expert review and credibility assessment\n- Stakeholder feedback and perspective integration\n- Peer review and professional validation\n- External advisor consultation and critique\n\nLevel 3: Empirical Validation\n- Historical data comparison and pattern matching\n- Market research validation and customer feedback\n- Pilot testing and proof-of-concept validation\n- Real-world experiment and A/B testing\n\nLevel 4: Predictive Validation\n- Forward-looking accuracy testing and prediction tracking\n- Real-time outcome monitoring and comparison\n- Continuous feedback integration and model updating\n- Long-term performance assessment and trend analysis\n```\n\n#### Feedback Integration Mechanisms\n- Automated accuracy tracking and alert systems\n- Stakeholder feedback collection and analysis\n- Expert consultation and validation scheduling\n- Real-world outcome monitoring and comparison\n\n### 5. Real-Time Calibration Systems\n\n**Establish ongoing accuracy monitoring and adjustment:**\n\n#### Continuous Monitoring Framework\n```\nReal-Time Calibration Dashboard:\n\nAccuracy Tracking Metrics:\n- Current Prediction Accuracy: [real-time accuracy percentage]\n- Accuracy Trend: [improving, stable, or declining accuracy]\n- Bias Detection: [systematic error patterns identified]\n- Confidence Calibration: [prediction confidence vs. actual accuracy alignment]\n\nEarly Warning Indicators:\n- Prediction Deviation Alerts: [when predictions diverge significantly from reality]\n- Model Drift Detection: [when model performance degrades over time]\n- Assumption Violation Warnings: [when key assumptions prove incorrect]\n- Data Quality Alerts: [when input data quality degrades]\n\nAutomated Adjustments:\n- Parameter Recalibration: [automatic model parameter updates]\n- Weight Rebalancing: [factor importance adjustments based on performance]\n- Threshold Updates: [decision threshold modifications based on accuracy]\n- Alert Sensitivity: [notification threshold adjustments]\n```\n\n#### Adaptive Learning Integration\n- Machine learning model updates based on new data\n- Bayesian updating for probability and parameter estimation\n- Expert feedback integration and model refinement\n- Context-aware calibration for different scenario types\n\n### 6. Calibration Quality Assurance\n\n**Ensure systematic improvement and reliability:**\n\n#### Calibration Validation Framework\n```\nMeta-Calibration Assessment:\n\nCalibration Process Quality:\n- Validation methodology appropriateness and rigor\n- Feedback integration effectiveness and speed\n- Bias detection and mitigation success\n- Continuous improvement demonstration\n\nCalibration Outcome Quality:\n- Accuracy improvement measurement and tracking\n- Prediction reliability enhancement\n- Decision support effectiveness improvement\n- Stakeholder confidence and satisfaction growth\n\nCalibration Sustainability:\n- Process scalability and resource efficiency\n- Knowledge capture and institutional learning\n- Methodology transferability to other simulations\n- Long-term performance maintenance and enhancement\n```\n\n#### Quality Control Mechanisms\n- Independent calibration validation and audit\n- Cross-functional calibration team and review processes\n- External benchmark comparison and best practice integration\n- Documentation and knowledge management systems\n\n### 7. Simulation Improvement Roadmap\n\n**Generate systematic enhancement strategies:**\n\n#### Calibration-Based Improvement Plan\n```\nSimulation Enhancement Framework:\n\n## Simulation Calibration Analysis: [Simulation Name]\n\n### Current Performance Assessment\n- Baseline Accuracy: [current accuracy percentages]\n- Key Biases Identified: [systematic errors found]\n- Validation Coverage: [validation methods applied]\n- Stakeholder Confidence: [user trust and satisfaction levels]\n\n### Calibration Findings\n\n#### Accuracy Analysis:\n- Strong Performance Areas: [where simulation excels]\n- Accuracy Gaps: [where improvements are needed]\n- Bias Patterns: [systematic errors identified]\n- Validation Results: [validation testing outcomes]\n\n#### Improvement Opportunities:\n- Quick Wins: [immediate accuracy improvements available]\n- Strategic Enhancements: [longer-term improvement possibilities]\n- Data Quality Improvements: [input enhancement opportunities]\n- Model Sophistication: [algorithm and methodology upgrades]\n\n### Improvement Roadmap\n\n#### Phase 1: Immediate Fixes (30 days)\n- Critical bias corrections and parameter adjustments\n- Data quality improvements and source validation\n- Process enhancement and workflow optimization\n- Stakeholder feedback integration and communication\n\n#### Phase 2: Systematic Enhancement (90 days)\n- Model sophistication and algorithm upgrades\n- Validation framework expansion and automation\n- Feedback loop optimization and real-time calibration\n- Training and capability building for users\n\n#### Phase 3: Advanced Optimization (180+ days)\n- Machine learning integration and automated improvement\n- Cross-simulation learning and best practice sharing\n- Innovation and methodology advancement\n- Strategic capability building and competitive advantage\n\n### Success Metrics and Monitoring\n- Accuracy Improvement Targets: [specific goals and timelines]\n- Bias Reduction Objectives: [systematic error elimination goals]\n- Validation Coverage Goals: [comprehensive validation targets]\n- User Satisfaction Improvements: [stakeholder confidence goals]\n```\n\n### 8. Knowledge Capture and Transfer\n\n**Establish institutional learning from calibration:**\n\n#### Learning Documentation\n- Calibration methodology documentation and best practices\n- Bias detection and mitigation technique libraries\n- Validation approach templates and reusable frameworks\n- Success pattern identification and replication guides\n\n#### Cross-Simulation Learning\n- Calibration insight sharing across different simulations\n- Best practice identification and standardization\n- Common pitfall documentation and avoidance strategies\n- Expertise development and capability building programs\n\n## Usage Examples\n\n```bash\n# Business simulation calibration\n/simulation:simulation-calibrator Calibrate customer acquisition cost simulation using 12 months of actual campaign data\n\n# Technical simulation validation\n/simulation:simulation-calibrator Validate system performance simulation against production monitoring data and user experience metrics\n\n# Market response calibration\n/simulation:simulation-calibrator Calibrate market response model using A/B testing results and customer behavior analytics\n\n# Strategic scenario validation\n/simulation:simulation-calibrator Test business scenario accuracy using post-decision outcome analysis and market development tracking\n```\n\n## Quality Indicators\n\n- **Green**: Systematic validation, documented biases, automated monitoring, continuous improvement\n- **Yellow**: Regular validation, some bias detection, manual monitoring, periodic improvement\n- **Red**: Ad-hoc validation, undetected biases, no monitoring, no improvement tracking\n\n## Common Pitfalls to Avoid\n\n- Validation theater: Going through validation motions without learning\n- Bias blindness: Not recognizing systematic errors and prejudices\n- Static calibration: Not updating models based on new information\n- Perfection paralysis: Waiting for perfect accuracy before using insights\n- Context ignorance: Not adapting calibration to different scenarios\n- Learning isolation: Not sharing insights across teams and simulations\n\nTransform simulation accuracy from guesswork into systematic, reliable decision support through comprehensive calibration and continuous improvement."
              },
              {
                "name": "/sprint-planning",
                "description": "Plan and organize sprint workflows",
                "path": "plugins/all-commands/commands/sprint-planning.md",
                "frontmatter": {
                  "description": "Plan and organize sprint workflows",
                  "category": "team-collaboration",
                  "allowed-tools": "Bash(gh *), Bash(npm *)"
                },
                "content": "# Sprint Planning\n\nPlan and organize sprint workflows\n\n## Instructions\n\n1. **Check Linear Integration**\nFirst, verify if the Linear MCP server is connected:\n- If connected: Proceed with full integration\n- If not connected: Ask user to install Linear MCP server from https://github.com/modelcontextprotocol/servers\n- Fallback: Use GitHub issues and manual input\n\n2. **Gather Sprint Context**\nCollect the following information:\n- Sprint duration (e.g., 2 weeks)\n- Sprint start date\n- Team members involved\n- Sprint goals/themes\n- Previous sprint velocity (if available)\n\n3. **Analyze Current State**\n\n#### With Linear Connected:\n```\n1. Fetch all backlog items from Linear\n2. Get in-progress tasks and their status\n3. Analyze task priorities and dependencies\n4. Check team member assignments and capacity\n5. Review blocked tasks and impediments\n```\n\n#### Without Linear (Fallback):\n```\n1. Analyze GitHub issues by labels and milestones\n2. Review open pull requests and their status\n3. Check recent commit activity\n4. Ask user for additional context about tasks\n```\n\n4. **Sprint Planning Analysis**\n\nGenerate a comprehensive sprint plan including:\n\n```markdown\n# Sprint Planning Report - [Sprint Name]\n\n## Sprint Overview\n- Duration: [Start Date] to [End Date]\n- Team Members: [List]\n- Sprint Goal: [Description]\n\n## Capacity Analysis\n- Total Available Hours: [Calculation]\n- Previous Sprint Velocity: [Points/Hours]\n- Recommended Capacity: [80-85% of total]\n\n## Proposed Sprint Backlog\n\n### High Priority Tasks\n1. [Task ID] - [Title]\n   - Estimate: [Points/Hours]\n   - Assignee: [Name]\n   - Dependencies: [List]\n   - Acceptance Criteria: [Details]\n\n### Medium Priority Tasks\n[Similar format]\n\n### Nice-to-Have Tasks\n[Similar format]\n\n## Risk Assessment\n- Technical Risks: [List]\n- Resource Risks: [List]\n- Dependency Risks: [List]\n\n## Recommendations\n1. [Specific recommendations based on analysis]\n2. [Resource allocation suggestions]\n3. [Risk mitigation strategies]\n\n## Metrics to Track\n- Velocity\n- Burndown rate\n- Blocked time\n- Cycle time\n```\n\n5. **Interactive Planning Session**\n\nOffer to help with:\n1. Task breakdown and estimation\n2. Dependency mapping\n3. Resource balancing\n4. Risk mitigation planning\n5. Sprint backlog finalization\n\n## Error Handling\n\n### Linear Connection Issues\n```\n\"I notice the Linear MCP server is not connected. To get the most value from sprint planning:\n\n1. Install the Linear MCP server:\n   npm install -g @modelcontextprotocol/server-linear\n\n2. Configure with your Linear API key\n3. Restart this session with Linear connected\n\nFor now, I'll proceed using GitHub data and manual input.\"\n```\n\n### Missing Data\n```\n\"I need more information to create an effective sprint plan:\n- [List specific missing data]\n- [Provide templates for manual input]\n```\n\n## Example Usage\n\n### User Input:\n\"Help me plan our next 2-week sprint starting Monday\"\n\n### Assistant Response:\n```\nI'll help you plan your sprint. Let me first check your Linear integration...\n\n[If connected]: Great! I can see your Linear workspace. Let me analyze your backlog...\n\n[Analysis results]\n\nBased on my analysis, here's your proposed sprint plan:\n\n**Sprint 24-01 (Jan 8-19)**\n\n**Recommended Scope (32 story points):**\n1. ENG-123: Implement user authentication (8 pts) - John\n2. ENG-124: Database migration scripts (5 pts) - Sarah\n3. ENG-125: API rate limiting (5 pts) - Mike\n...\n\n**Key Risks:**\n- ENG-123 blocks 3 other tasks\n- Sarah has 20% allocation to support\n\nWould you like me to:\n1. Adjust the scope based on different priorities?\n2. Create a dependency visualization?\n3. Generate sprint planning meeting agenda?\n```\n\n## Best Practices\n\n1. **Always verify capacity**: Don't overcommit the team\n2. **Include buffer time**: Plan for 80-85% capacity\n3. **Consider dependencies**: Map task relationships\n4. **Balance workload**: Distribute tasks evenly\n5. **Define clear goals**: Ensure sprint has focused objectives\n6. **Plan for unknowns**: Include spike/investigation time\n\n## Integration Points\n\n- Linear: Task management and tracking\n- GitHub: Code repository and PRs\n- Slack: Team communication (if MCP available)\n- Calendar: Team availability (if accessible)\n\n## Output Formats\n\nOffer multiple output options:\n1. Markdown report (default)\n2. CSV for spreadsheet import\n3. JSON for automation tools\n4. Linear-compatible format for direct import"
              },
              {
                "name": "/standup-report",
                "description": "Generate daily standup reports",
                "path": "plugins/all-commands/commands/standup-report.md",
                "frontmatter": {
                  "description": "Generate daily standup reports",
                  "category": "team-collaboration",
                  "allowed-tools": "Bash(git *), Bash(npm *)"
                },
                "content": "# Standup Report\n\nGenerate daily standup reports\n\n## Instructions\n\n1. **Initial Setup**\n   - Check Linear MCP server connection\n   - Determine time range (default: last 24 hours)\n   - Identify team members (from git config or user input)\n   - Set report format preferences\n\n2. **Data Collection**\n\n#### Git Activity Analysis\n```bash\n# Collect commits from last 24 hours\ngit log --since=\"24 hours ago\" --all --format=\"%h|%an|%ad|%s\" --date=short\n\n# Check branch activity\ngit for-each-ref --format='%(refname:short)|%(committerdate:short)|%(authoremail)' --sort=-committerdate refs/heads/\n\n# Analyze file changes\ngit diff --stat @{1.day.ago}\n```\n\n#### Linear Integration (if available)\n```\n1. Fetch tasks updated in last 24 hours\n2. Get task status changes\n3. Check new comments and blockers\n4. Review completed tasks\n```\n\n#### GitHub PR Status\n```\n1. Check PR updates and reviews\n2. Identify merged PRs\n3. Find new PRs created\n4. Review CI/CD status\n```\n\n3. **Report Generation**\n\nGenerate structured standup report:\n\n```markdown\n# Daily Standup Report - [Date]\n\n## Team Member: [Name]\n\n### Yesterday's Accomplishments\n-  Completed [Task ID]: [Description]\n  - Commits: [List with links]\n  - PR: [Link if applicable]\n-  Progressed on [Task ID]: [Description]\n  - Current status: [X]% complete\n  - Latest commit: [Message]\n\n### Today's Plan\n-  [Task ID]: [Description]\n  - Estimated completion: [Time]\n  - Dependencies: [List]\n-  Code review for PR #[Number]\n-  Update documentation for [Feature]\n\n### Blockers & Concerns\n-  Blocked on [Task ID]: [Reason]\n  - Need input from: [Person/Team]\n  - Expected resolution: [Time]\n-  Potential risk: [Description]\n\n### Metrics Summary\n- Commits: [Count]\n- PRs Updated: [Count]\n- Tasks Completed: [Count]\n- Cycle Time: [Average]\n```\n\n4. **Multi-Format Output**\n\nProvide output in various formats:\n\n#### Slack Format\n```\n*Daily Standup - @username*\n\n*Yesterday:*\n Merged PR #123: Add user authentication\n Fixed bug in payment processing (ENG-456)\n Reviewed 3 PRs\n\n*Today:*\n Starting ENG-457: Implement rate limiting\n Pairing with @teammate on database migration\n Sprint planning meeting at 2 PM\n\n*Blockers:*\n Waiting on API credentials from DevOps\n ENG-458 needs design clarification\n```\n\n#### Email Format\n```\nSubject: Daily Standup - [Name] - [Date]\n\nHi team,\n\nHere's my update for today's standup:\n\nCOMPLETED YESTERDAY:\n- [Detailed list with context]\n\nPLANNED FOR TODAY:\n- [Prioritized task list]\n\nBLOCKERS/HELP NEEDED:\n- [Clear description of impediments]\n\nLet me know if you have any questions.\n\nBest,\n[Name]\n```\n\n5. **Team Rollup View**\n\nFor team leads, generate consolidated view:\n\n```markdown\n# Team Standup Summary - [Date]\n\n## Velocity Metrics\n- Total Commits: [Count]\n- PRs Merged: [Count]\n- Tasks Completed: [Count]\n- Active Blockers: [Count]\n\n## Individual Updates\n[Summary for each team member]\n\n## Critical Items\n- Blockers requiring immediate attention\n- At-risk deliverables\n- Resource conflicts\n\n## Team Health Indicators\n- On-track tasks: [%]\n- Blocked tasks: [%]\n- Overdue items: [Count]\n```\n\n## Error Handling\n\n### No Linear Connection\n```\n\"Linear MCP server not connected. Generating report from git and GitHub data only.\n\nTo enable full functionality:\n1. Install Linear MCP: npm install -g @modelcontextprotocol/server-linear\n2. Configure with your API key\n3. Restart with Linear connected\n\nProceeding with available data...\"\n```\n\n### No Recent Activity\n```\n\"No git activity found in the last 24 hours. \n\nPossible reasons:\n1. No commits made (check your time range)\n2. Working on untracked branches\n3. Local changes not committed\n\nWould you like to:\n- Extend the time range?\n- Check specific branches?\n- Manually input your updates?\"\n```\n\n## Interactive Features\n\n1. **Update Customization**\n```\n\"I've generated your standup report. Would you like to:\n1. Add additional context to any item?\n2. Reorder priorities for today?\n3. Add missing blockers or concerns?\n4. Include work done outside of git?\"\n```\n\n2. **Blocker Resolution**\n```\n\"I notice you have blockers. Would you like help with:\n1. Drafting messages to unblock items?\n2. Finding alternative approaches?\n3. Identifying who can help?\"\n```\n\n## Best Practices\n\n1. **Run before standup**: Generate 15-30 minutes before meeting\n2. **Be specific**: Include task IDs and measurable progress\n3. **Highlight blockers early**: Don't wait until standup\n4. **Keep it concise**: Focus on key updates\n5. **Link to evidence**: Include commit/PR links\n\n## Advanced Features\n\n### Trend Analysis\n```\n\"Looking at your past week:\n- Average daily commits: [Number]\n- Task completion rate: [%]\n- Common blocker patterns: [List]\n\nSuggestions for improvement:\n[Personalized recommendations]\"\n```\n\n### Smart Scheduling\n```\n\"Based on your calendar and task estimates:\n- You have 5 hours of focused time today\n- Recommended task order: [Prioritized list]\n- Potential conflicts: [Meeting overlaps]\"\n```\n\n## Command Examples\n\n### Basic Usage\n```\nUser: \"Generate my standup report\"\nAssistant: [Generates standard report for last 24 hours]\n```\n\n### Custom Time Range\n```\nUser: \"Generate standup for last 2 days\"\nAssistant: [Generates report covering 48 hours]\n```\n\n### Team Report\n```\nUser: \"Generate team standup summary\"\nAssistant: [Generates consolidated team view]\n```\n\n### Specific Format\n```\nUser: \"Generate standup in Slack format\"\nAssistant: [Generates Slack-formatted message ready to paste]\n```"
              },
              {
                "name": "/start",
                "description": "Initiates the task orchestration workflow using the three-agent system (task-orchestrator, task-decomposer, and dependency-analyzer) to create a comprehensive execution plan.",
                "path": "plugins/all-commands/commands/start.md",
                "frontmatter": {
                  "description": "Initiates the task orchestration workflow using the three-agent system (task-orchestrator, task-decomposer, and dependency-analyzer) to create a comprehensive execution plan.",
                  "category": "workflow-orchestration"
                },
                "content": "# Orchestrate Tasks Command\n\nInitiates the task orchestration workflow using the three-agent system (task-orchestrator, task-decomposer, and dependency-analyzer) to create a comprehensive execution plan.\n\n## Usage\n\n```\n/orchestrate [task list or file path]\n```\n\n## Description\n\nThis command activates the task-orchestrator agent to process requirements and create a hyper-efficient execution plan. The orchestrator will:\n\n1. **Clarify Requirements**: Analyze provided information and confirm understanding\n2. **Create Directory Structure**: Set up task-orchestration folders with today's date\n3. **Decompose Tasks**: Work with task-decomposer to create atomic task files\n4. **Analyze Dependencies**: Use dependency-analyzer to identify conflicts and parallelization opportunities\n5. **Generate Master Plan**: Create comprehensive coordination documents\n\n## Input Formats\n\n### Direct Task List\n```\n/orchestrate\n- Implement user authentication with JWT\n- Add payment processing with Stripe\n- Create admin dashboard\n- Set up email notifications\n```\n\n### File Reference\n```\n/orchestrate features.md\n```\n\n### Mixed Context\n```\n/orchestrate\nBased on our meeting notes (lots of discussion about UI colors), we need to:\n1. Fix the security vulnerability in file uploads\n2. Add rate limiting to APIs\n3. Implement audit logging\nThe CEO wants this done by Friday (ignore this deadline).\n```\n\n## Workflow\n\n1. **Requirement Clarification**\n   - The orchestrator will extract actionable tasks from provided context\n   - Confirm understanding before proceeding\n   - Ask clarifying questions if needed\n\n2. **Directory Creation**\n   ```\n   /task-orchestration/\n    MM_DD_YYYY/\n        descriptive_task_name/\n            MASTER-COORDINATION.md\n            EXECUTION-TRACKER.md\n            TASK-STATUS-TRACKER.yaml\n            tasks/\n                todos/\n                in_progress/\n                on_hold/\n                qa/\n                completed/\n   ```\n\n3. **Task Processing**\n   - Creates individual task files in todos/\n   - Analyzes dependencies and conflicts\n   - Generates execution strategy\n\n4. **Deliverables**\n   - Master coordination plan\n   - Task dependency graph\n   - Resource allocation matrix\n   - Execution timeline\n\n## Options\n\n### Focused Mode\n```\n/orchestrate --focus security\n[task list]\n```\nPrioritizes tasks related to the specified focus area.\n\n### Constraint Mode\n```\n/orchestrate --agents 2 --days 5\n[task list]\n```\nCreates plan with resource constraints.\n\n### Analysis Only\n```\n/orchestrate --analyze-only\n[task list]\n```\nGenerates analysis without creating task files.\n\n## Examples\n\n### Example 1: Clear Task List\n```\n/orchestrate\n1. Implement OAuth2 authentication\n2. Add user profile management\n3. Create password reset flow\n4. Set up 2FA\n```\n\n### Example 2: From Requirements Doc\n```\n/orchestrate requirements/sprint-24.md\n```\n\n### Example 3: Mixed Context Extraction\n```\n/orchestrate\nFrom the customer feedback:\n\"The app is too slow\" - Need performance optimization\n\"Can't find the export button\" - UI improvement needed\n\"Want dark mode\" - New feature request\n\nTechnical debt from last sprint:\n- Refactor authentication service\n- Update deprecated dependencies\n```\n\n## Interactive Mode\n\nThe orchestrator will:\n1. Present extracted tasks for confirmation\n2. Ask about priorities and constraints\n3. Suggest optimal approach\n4. Request approval before creating files\n\n## Error Handling\n\n- If tasks are unclear: Asks for clarification\n- If file not found: Prompts for correct path\n- If conflicts detected: Presents options\n- If dependencies circular: Suggests resolution\n\n## Integration\n\nWorks seamlessly with:\n- `/task-status` - Check progress\n- `/task-move` - Update task status\n- `/task-report` - Generate reports\n- `/task-assign` - Allocate to agents\n\n## Best Practices\n\n1. **Provide Context**: Include relevant background information\n2. **Be Specific**: Clear task descriptions enable better planning\n3. **Mention Constraints**: Include deadlines, resources, or blockers\n4. **Review Output**: Confirm the extracted tasks match your intent\n\n## Notes\n\n- The orchestrator filters out irrelevant context automatically\n- Tasks are created in todos/ status by default\n- All tasks get unique IDs (TASK-XXX format)\n- Status tracking begins immediately\n- Supports incremental additions to existing orchestrations"
              },
              {
                "name": "/status",
                "description": "Check the current status of tasks in the orchestration system with various filtering and reporting options.",
                "path": "plugins/all-commands/commands/status.md",
                "frontmatter": {
                  "description": "Check the current status of tasks in the orchestration system with various filtering and reporting options.",
                  "category": "workflow-orchestration",
                  "allowed-tools": "Write"
                },
                "content": "# Task Status Command\n\nCheck the current status of tasks in the orchestration system with various filtering and reporting options.\n\n## Usage\n\n```\n/task-status [options]\n```\n\n## Description\n\nProvides comprehensive visibility into task progress, status distribution, and execution metrics across all active orchestrations.\n\n## Command Variants\n\n### Basic Status Overview\n```\n/task-status\n```\nShows summary of all tasks across all active orchestrations.\n\n### Today's Tasks\n```\n/task-status --today\n```\nShows only tasks from today's orchestrations.\n\n### Specific Orchestration\n```\n/task-status --date 03_15_2024 --project payment_integration\n```\nShows tasks from a specific orchestration.\n\n### Status Filter\n```\n/task-status --status in_progress\n/task-status --status qa\n/task-status --status on_hold\n```\nShows only tasks with specified status.\n\n### Detailed View\n```\n/task-status --detailed\n```\nShows comprehensive information for each task.\n\n## Output Formats\n\n### Summary View (Default)\n```\nTask Orchestration Status Summary\n=================================\n\nActive Orchestrations: 3\nTotal Tasks: 47\n\nStatus Distribution:\n\n Status       Count  Percentage \n\n completed     12       26%     \n qa             5       11%     \n in_progress    3        6%     \n on_hold        2        4%     \n todos         25       53%     \n\n\nActive Tasks (in_progress):\n- TASK-001: Implement JWT authentication (Agent: dev-frontend)\n- TASK-007: Create payment webhook handler (Agent: dev-backend)\n- TASK-012: Write integration tests (Agent: test-developer)\n\nBlocked Tasks (on_hold):\n- TASK-004: User profile API (Blocked by: TASK-001)\n- TASK-009: Payment confirmation UI (Blocked by: TASK-007)\n```\n\n### Detailed View\n```\nTask Details for: 03_15_2024/authentication_system\n==================================================\n\nTASK-001: Implement JWT authentication\nStatus: in_progress\nAgent: dev-frontend\nStarted: 2024-03-15T14:30:00Z\nDuration: 3.5 hours\nProgress: 75% (est. 1 hour remaining)\nDependencies: None\nBlocks: TASK-004, TASK-005\nLocation: /task-orchestration/03_15_2024/authentication_system/tasks/in_progress/\n\nStatus History:\n- todos  in_progress (2024-03-15T14:30:00Z) by dev-frontend\n```\n\n### Timeline View\n```\n/task-status --timeline\n```\nShows Gantt-style timeline of task execution.\n\n### Velocity Report\n```\n/task-status --velocity\n```\nShows completion rates and performance metrics.\n\n## Filtering Options\n\n### By Agent\n```\n/task-status --agent dev-frontend\n```\n\n### By Priority\n```\n/task-status --priority high\n```\n\n### By Type\n```\n/task-status --type feature\n/task-status --type bugfix\n```\n\n### Multiple Filters\n```\n/task-status --status todos --priority high --type security\n```\n\n## Quick Actions\n\n### Show Critical Path\n```\n/task-status --critical-path\n```\nHighlights tasks that are blocking others.\n\n### Show Overdue\n```\n/task-status --overdue\n```\nShows tasks exceeding estimated time.\n\n### Show Available\n```\n/task-status --available\n```\nShows todos tasks ready to be picked up.\n\n## Integration Commands\n\n### Export Status\n```\n/task-status --export markdown\n/task-status --export csv\n```\n\n### Watch Mode\n```\n/task-status --watch\n```\nUpdates status in real-time (refreshes every 30 seconds).\n\n## Examples\n\n### Example 1: Morning Standup View\n```\n/task-status --today --detailed\n```\n\n### Example 2: Find Blocked Work\n```\n/task-status --status on_hold --show-blockers\n```\n\n### Example 3: Agent Workload\n```\n/task-status --by-agent --status in_progress\n```\n\n### Example 4: Sprint Progress\n```\n/task-status --date 03_15_2024 --metrics\n```\n\n## Metrics and Analytics\n\n### Completion Metrics\n- Average time per task\n- Tasks completed per day\n- Status transition times\n\n### Bottleneck Analysis\n- Most blocking tasks\n- Longest on_hold duration\n- Critical path duration\n\n### Agent Performance\n- Tasks per agent\n- Average completion time\n- Current workload\n\n## Best Practices\n\n1. **Daily Check**: Run `/task-status --today` each morning\n2. **Blocker Review**: Check `/task-status --status on_hold` regularly\n3. **Progress Tracking**: Use `/task-status --velocity` for trends\n4. **Resource Planning**: Monitor `/task-status --by-agent`\n\n## Notes\n\n- Status data is read from TASK-STATUS-TRACKER.yaml files\n- All times are shown in local timezone\n- Completed tasks are included in metrics but not in active lists\n- Use `--all` flag to include historical orchestrations"
              },
              {
                "name": "/svelte-a11y",
                "description": "Audit and improve accessibility in Svelte/SvelteKit applications, ensuring WCAG compliance and inclusive user experiences.",
                "path": "plugins/all-commands/commands/svelte-a11y.md",
                "frontmatter": {
                  "description": "Audit and improve accessibility in Svelte/SvelteKit applications, ensuring WCAG compliance and inclusive user experiences.",
                  "category": "framework-svelte"
                },
                "content": "# /svelte-a11y\n\nAudit and improve accessibility in Svelte/SvelteKit applications, ensuring WCAG compliance and inclusive user experiences.\n\n## Instructions\n\nYou are acting as the Svelte Development Agent focused on accessibility. When improving accessibility:\n\n1. **Accessibility Audit**:\n   - Run automated accessibility tests\n   - Check WCAG 2.1 AA/AAA compliance\n   - Test with screen readers\n   - Verify keyboard navigation\n   - Analyze color contrast\n   - Review ARIA usage\n\n2. **Common Issues & Fixes**:\n   \n   **Component Accessibility**:\n   ```svelte\n   <!-- Bad -->\n   <div onclick={handleClick}>Click me</div>\n   \n   <!-- Good -->\n   <button onclick={handleClick} aria-label=\"Action description\">\n     Click me\n   </button>\n   ```\n   \n   **Form Accessibility**:\n   ```svelte\n   <label for=\"email\">Email Address</label>\n   <input \n     id=\"email\"\n     type=\"email\"\n     required\n     aria-describedby=\"email-error\"\n   />\n   {#if errors.email}\n     <span id=\"email-error\" role=\"alert\">\n       {errors.email}\n     </span>\n   {/if}\n   ```\n\n3. **Navigation & Focus**:\n   ```javascript\n   // Skip links\n   <a href=\"#main\" class=\"skip-link\">Skip to main content</a>\n   \n   // Focus management\n   onMount(() => {\n     if (shouldFocus) {\n       element.focus();\n     }\n   });\n   \n   // Keyboard navigation\n   function handleKeydown(event) {\n     if (event.key === 'Escape') {\n       closeModal();\n     }\n   }\n   ```\n\n4. **ARIA Implementation**:\n   - Use semantic HTML first\n   - Add ARIA labels for clarity\n   - Implement live regions\n   - Manage focus properly\n   - Announce dynamic changes\n\n5. **Testing Tools**:\n   - Svelte a11y warnings\n   - axe-core integration\n   - Pa11y CI setup\n   - Screen reader testing\n   - Keyboard navigation testing\n\n6. **Accessibility Checklist**:\n   - [ ] All interactive elements keyboard accessible\n   - [ ] Proper heading hierarchy\n   - [ ] Images have alt text\n   - [ ] Color contrast meets standards\n   - [ ] Forms have proper labels\n   - [ ] Error messages announced\n   - [ ] Focus indicators visible\n   - [ ] Page has unique title\n   - [ ] Landmarks properly used\n   - [ ] Animations respect prefers-reduced-motion\n\n## Example Usage\n\nUser: \"Audit my e-commerce site for accessibility issues\"\n\nAssistant will:\n- Run automated accessibility scan\n- Check product cards for proper markup\n- Verify cart keyboard navigation\n- Test checkout form accessibility\n- Review color contrast on CTAs\n- Add ARIA labels where needed\n- Implement focus management\n- Create accessibility test suite\n- Provide WCAG compliance report"
              },
              {
                "name": "/svelte-component",
                "description": "Create new Svelte components with best practices, proper structure, and optional TypeScript support.",
                "path": "plugins/all-commands/commands/svelte-component.md",
                "frontmatter": {
                  "description": "Create new Svelte components with best practices, proper structure, and optional TypeScript support.",
                  "category": "framework-svelte"
                },
                "content": "# /svelte-component\n\nCreate new Svelte components with best practices, proper structure, and optional TypeScript support.\n\n## Instructions\n\nYou are acting as the Svelte Development Agent focused on component creation. When creating components:\n\n1. **Gather Requirements**:\n   - Component name and purpose\n   - Props interface\n   - Events to emit\n   - Slots needed\n   - State management requirements\n   - TypeScript preference\n\n2. **Component Structure**:\n   ```svelte\n   <script lang=\"ts\">\n     // Imports\n     // Type definitions\n     // Props\n     // State\n     // Derived values\n     // Effects\n     // Functions\n   </script>\n   \n   <!-- Markup -->\n   \n   <style>\n     /* Scoped styles */\n   </style>\n   ```\n\n3. **Best Practices**:\n   - Use proper prop typing with TypeScript/JSDoc\n   - Implement $bindable props where appropriate\n   - Create accessible markup by default\n   - Add proper ARIA attributes\n   - Use semantic HTML elements\n   - Include keyboard navigation support\n\n4. **Component Types to Create**:\n   - **UI Components**: Buttons, Cards, Modals, etc.\n   - **Form Components**: Inputs with validation, custom form controls\n   - **Layout Components**: Headers, Sidebars, Grids\n   - **Data Components**: Tables, Lists, Data visualizations\n   - **Utility Components**: Portals, Transitions, Error boundaries\n\n5. **Additional Files**:\n   - Create accompanying test file\n   - Add Storybook story if applicable\n   - Create usage documentation\n   - Export from index file\n\n## Example Usage\n\nUser: \"Create a Modal component with customizable header, footer slots, and close functionality\"\n\nAssistant will:\n- Create Modal.svelte with proper structure\n- Implement focus trap and keyboard handling\n- Add transition effects\n- Create Modal.test.js with basic tests\n- Provide usage examples\n- Suggest accessibility improvements"
              },
              {
                "name": "/svelte-debug",
                "description": "Help debug Svelte and SvelteKit issues by analyzing error messages, stack traces, and common problems.",
                "path": "plugins/all-commands/commands/svelte-debug.md",
                "frontmatter": {
                  "description": "Help debug Svelte and SvelteKit issues by analyzing error messages, stack traces, and common problems.",
                  "category": "framework-svelte"
                },
                "content": "# /svelte-debug\n\nHelp debug Svelte and SvelteKit issues by analyzing error messages, stack traces, and common problems.\n\n## Instructions\n\nYou are acting as the Svelte Development Agent with a focus on debugging. When the user provides an error or describes an issue:\n\n1. **Analyze the Error**:\n   - Parse error messages and stack traces\n   - Identify the root cause (compilation, runtime, or configuration)\n   - Check for common Svelte/SvelteKit pitfalls\n\n2. **Diagnose the Problem**:\n   - Examine the relevant code files\n   - Check for syntax errors, missing imports, or incorrect usage\n   - Verify configuration files (vite.config.js, svelte.config.js, etc.)\n   - Look for version mismatches or dependency conflicts\n\n3. **Common Issues to Check**:\n   - Reactive statement errors ($state, $derived, $effect)\n   - SSR vs CSR conflicts\n   - Load function errors (missing returns, incorrect data access)\n   - Form action problems\n   - Routing issues\n   - Build and deployment errors\n\n4. **Provide Solutions**:\n   - Offer specific fixes with code examples\n   - Suggest debugging techniques (console.log, {@debug}, browser DevTools)\n   - Recommend relevant documentation sections\n   - Provide step-by-step resolution guides\n\n5. **Preventive Measures**:\n   - Suggest TypeScript additions for better error catching\n   - Recommend linting rules\n   - Propose architectural improvements\n\n## Example Usage\n\nUser: \"I'm getting 'Cannot access 'user' before initialization' error in my load function\"\n\nAssistant will:\n- Examine the load function structure\n- Check for proper async/await usage\n- Verify data dependencies\n- Provide corrected code\n- Explain the fix and how to avoid similar issues"
              },
              {
                "name": "/svelte-migrate",
                "description": "Migrate Svelte/SvelteKit projects between versions, adopt new features like runes, and handle breaking changes.",
                "path": "plugins/all-commands/commands/svelte-migrate.md",
                "frontmatter": {
                  "description": "Migrate Svelte/SvelteKit projects between versions, adopt new features like runes, and handle breaking changes.",
                  "category": "framework-svelte"
                },
                "content": "# /svelte-migrate\n\nMigrate Svelte/SvelteKit projects between versions, adopt new features like runes, and handle breaking changes.\n\n## Instructions\n\nYou are acting as the Svelte Development Agent focused on migrations. When migrating projects:\n\n1. **Migration Types**:\n   \n   **Version Migrations**:\n   - Svelte 3  Svelte 4\n   - Svelte 4  Svelte 5 (Runes)\n   - SvelteKit 1.x  SvelteKit 2.x\n   - Legacy app  Modern SvelteKit\n   \n   **Feature Migrations**:\n   - Stores  Runes ($state, $derived)\n   - Class components  Function syntax\n   - Imperative  Declarative patterns\n   - JavaScript  TypeScript\n\n2. **Migration Process**:\n   ```bash\n   # Automated migrations\n   npx sv migrate [migration-name]\n   \n   # Manual migration steps\n   1. Backup current code\n   2. Update dependencies\n   3. Run codemods\n   4. Fix breaking changes\n   5. Update configurations\n   6. Test thoroughly\n   ```\n\n3. **Runes Migration**:\n   ```javascript\n   // Before (Svelte 4)\n   let count = 0;\n   $: doubled = count * 2;\n   \n   // After (Svelte 5)\n   let count = $state(0);\n   let doubled = $derived(count * 2);\n   ```\n\n4. **Breaking Changes**:\n   - Component API changes\n   - Store subscription syntax\n   - Event handling updates\n   - SSR behavior changes\n   - Build configuration updates\n   - Package import paths\n\n5. **Migration Checklist**:\n   - [ ] Update package.json dependencies\n   - [ ] Run automated migration scripts\n   - [ ] Update component syntax\n   - [ ] Fix TypeScript errors\n   - [ ] Update configuration files\n   - [ ] Test all routes and components\n   - [ ] Update deployment scripts\n   - [ ] Review performance impacts\n\n## Example Usage\n\nUser: \"Migrate my Svelte 4 app to Svelte 5 with runes\"\n\nAssistant will:\n- Analyze current codebase\n- Create migration plan\n- Run `npx sv migrate svelte-5`\n- Convert reactive statements to runes\n- Update component props syntax\n- Fix effect timing issues\n- Update test files\n- Handle edge cases manually\n- Provide rollback strategy"
              },
              {
                "name": "/svelte-optimize",
                "description": "Optimize Svelte/SvelteKit applications for performance, including bundle size reduction, rendering optimization, and loading performance.",
                "path": "plugins/all-commands/commands/svelte-optimize.md",
                "frontmatter": {
                  "description": "Optimize Svelte/SvelteKit applications for performance, including bundle size reduction, rendering optimization, and loading performance.",
                  "category": "framework-svelte"
                },
                "content": "# /svelte-optimize\n\nOptimize Svelte/SvelteKit applications for performance, including bundle size reduction, rendering optimization, and loading performance.\n\n## Instructions\n\nYou are acting as the Svelte Development Agent focused on performance optimization. When optimizing:\n\n1. **Performance Analysis**:\n   - Analyze bundle size with rollup-plugin-visualizer\n   - Profile component rendering\n   - Measure Core Web Vitals\n   - Identify performance bottlenecks\n   - Check network waterfall\n\n2. **Bundle Optimization**:\n   \n   **Code Splitting**:\n   ```javascript\n   // Dynamic imports\n   const HeavyComponent = await import('./HeavyComponent.svelte');\n   \n   // Route-based splitting\n   export const prerender = false;\n   export const ssr = true;\n   ```\n   \n   **Tree Shaking**:\n   - Remove unused imports\n   - Optimize library imports\n   - Use production builds\n   - Eliminate dead code\n\n3. **Rendering Optimization**:\n   \n   **Reactive Performance**:\n   ```javascript\n   // Use $state.raw for large objects\n   let data = $state.raw(largeDataset);\n   \n   // Optimize derived computations\n   let filtered = $derived.lazy(() => \n     expensiveFilter(data)\n   );\n   ```\n   \n   **Component Optimization**:\n   - Minimize re-renders\n   - Use keyed each blocks\n   - Implement virtual scrolling\n   - Lazy load components\n\n4. **Loading Performance**:\n   - Implement preloading strategies\n   - Optimize images (lazy loading, WebP)\n   - Use resource hints (preconnect, prefetch)\n   - Enable HTTP/2 push\n   - Implement service workers\n\n5. **SvelteKit Optimizations**:\n   ```javascript\n   // Prerender static pages\n   export const prerender = true;\n   \n   // Optimize data loading\n   export async function load({ fetch, setHeaders }) {\n     setHeaders({\n       'cache-control': 'public, max-age=3600'\n     });\n     \n     return {\n       data: await fetch('/api/data')\n     };\n   }\n   ```\n\n6. **Optimization Checklist**:\n   - [ ] Enable compression (gzip/brotli)\n   - [ ] Optimize fonts (subsetting, preload)\n   - [ ] Minimize CSS (PurgeCSS/Tailwind)\n   - [ ] Enable CDN/edge caching\n   - [ ] Implement critical CSS\n   - [ ] Optimize third-party scripts\n   - [ ] Use WebAssembly for heavy computation\n\n## Example Usage\n\nUser: \"My SvelteKit app is loading slowly, optimize it\"\n\nAssistant will:\n- Run performance analysis\n- Identify largest bundle chunks\n- Implement code splitting\n- Optimize images and assets\n- Add preloading for critical resources\n- Configure caching headers\n- Implement lazy loading\n- Optimize server-side rendering\n- Provide performance metrics comparison"
              },
              {
                "name": "/svelte-scaffold",
                "description": "Scaffold new SvelteKit projects, features, or modules with best practices and optimal project structure.",
                "path": "plugins/all-commands/commands/svelte-scaffold.md",
                "frontmatter": {
                  "description": "Scaffold new SvelteKit projects, features, or modules with best practices and optimal project structure.",
                  "category": "framework-svelte"
                },
                "content": "# /svelte-scaffold\n\nScaffold new SvelteKit projects, features, or modules with best practices and optimal project structure.\n\n## Instructions\n\nYou are acting as the Svelte Development Agent focused on project scaffolding. When scaffolding:\n\n1. **Project Types**:\n   \n   **New SvelteKit Project**:\n   - Use `npx sv create` with appropriate options\n   - Select TypeScript/JSDoc preference\n   - Choose testing framework\n   - Add essential integrations (Tailwind, ESLint, etc.)\n   - Set up Git repository\n   \n   **Feature Modules**:\n   - Authentication system\n   - Admin dashboard\n   - Blog/CMS\n   - E-commerce features\n   - API integrations\n   \n   **Component Libraries**:\n   - Design system setup\n   - Storybook integration\n   - Component documentation\n   - Publishing configuration\n\n2. **Project Structure**:\n   ```\n   project/\n    src/\n       routes/\n          (app)/\n          (auth)/\n          api/\n       lib/\n          components/\n          stores/\n          utils/\n          server/\n       hooks.server.ts\n       app.html\n    tests/\n    static/\n    [config files]\n   ```\n\n3. **Essential Features**:\n   - Environment variable setup\n   - Database configuration\n   - Authentication scaffolding\n   - API route templates\n   - Error handling\n   - Logging setup\n   - Deployment configuration\n\n4. **Configuration Files**:\n   - `svelte.config.js` - Optimized settings\n   - `vite.config.js` - Build optimization\n   - `playwright.config.js` - E2E testing\n   - `tailwind.config.js` - Styling (if selected)\n   - `.env.example` - Environment template\n   - `docker-compose.yml` - Container setup\n\n5. **Starter Code**:\n   - Layout with navigation\n   - Authentication flow\n   - Protected routes\n   - Form examples\n   - API integration patterns\n   - State management setup\n\n## Example Usage\n\nUser: \"Scaffold a new SaaS starter with auth and payments\"\n\nAssistant will:\n- Create SvelteKit project with TypeScript\n- Set up authentication (Lucia/Auth.js)\n- Add payment integration (Stripe)\n- Create user dashboard structure\n- Set up database (Prisma/Drizzle)\n- Add email service\n- Configure deployment\n- Create example protected routes\n- Add subscription management"
              },
              {
                "name": "/svelte-storybook-migrate",
                "description": "Migrate Storybook configurations and stories to newer versions, including Svelte CSF v5 and @storybook/sveltekit framework.",
                "path": "plugins/all-commands/commands/svelte-storybook-migrate.md",
                "frontmatter": {
                  "description": "Migrate Storybook configurations and stories to newer versions, including Svelte CSF v5 and @storybook/sveltekit framework.",
                  "category": "framework-svelte",
                  "allowed-tools": "Bash(npm *), Write"
                },
                "content": "# /svelte-storybook-migrate\n\nMigrate Storybook configurations and stories to newer versions, including Svelte CSF v5 and @storybook/sveltekit framework.\n\n## Instructions\n\nYou are acting as the Svelte Storybook Specialist Agent focused on migration. When migrating Storybook:\n\n1. **Version Migrations**:\n   \n   **Storybook 6.x to 7.x**:\n   ```bash\n   # Automated upgrade\n   npx storybook@latest upgrade\n   \n   # Manual steps:\n   # 1. Update dependencies\n   # 2. Migrate to @storybook/sveltekit\n   # 3. Remove obsolete packages\n   # 4. Update configuration\n   ```\n   \n   **Configuration Changes**:\n   ```javascript\n   // Old (.storybook/main.js)\n   module.exports = {\n     framework: '@storybook/svelte',\n     svelteOptions: { ... } // Remove this\n   };\n   \n   // New (.storybook/main.js)\n   export default {\n     framework: {\n       name: '@storybook/sveltekit',\n       options: {}\n     }\n   };\n   ```\n\n2. **Svelte CSF Migration (v4 to v5)**:\n   \n   **Meta Component  defineMeta**:\n   ```svelte\n   <!-- Old -->\n   <script context=\"module\">\n     import { Meta, Story } from '@storybook/addon-svelte-csf';\n   </script>\n   \n   <Meta title=\"Button\" component={Button} />\n   \n   <!-- New -->\n   <script>\n     import { defineMeta } from '@storybook/addon-svelte-csf';\n     import Button from './Button.svelte';\n     \n     const { Story } = defineMeta({\n       title: 'Button',\n       component: Button\n     });\n   </script>\n   ```\n   \n   **Template  Children/Snippets**:\n   ```svelte\n   <!-- Old -->\n   <Story name=\"Default\">\n     <Template let:args>\n       <Button {...args} />\n     </Template>\n   </Story>\n   \n   <!-- New -->\n   <Story name=\"Default\" args={{ label: 'Click' }}>\n     {#snippet template(args)}\n       <Button {...args} />\n     {/snippet}\n   </Story>\n   ```\n\n3. **Package Migration**:\n   \n   **Remove Obsolete Packages**:\n   ```bash\n   npm uninstall @storybook/svelte-vite\n   npm uninstall storybook-builder-vite\n   npm uninstall @storybook/builder-vite\n   npm uninstall @storybook/svelte\n   ```\n   \n   **Install New Packages**:\n   ```bash\n   npm install -D @storybook/sveltekit\n   npm install -D @storybook/addon-svelte-csf@latest\n   ```\n\n4. **Story Format Migration**:\n   \n   **CSF 2 to CSF 3**:\n   ```javascript\n   // Old (CSF 2)\n   export default {\n     title: 'Button',\n     component: Button\n   };\n   \n   export const Primary = (args) => ({\n     Component: Button,\n     props: args\n   });\n   Primary.args = { variant: 'primary' };\n   \n   // New (CSF 3)\n   export default {\n     title: 'Button',\n     component: Button\n   };\n   \n   export const Primary = {\n     args: { variant: 'primary' }\n   };\n   ```\n\n5. **Addon Updates**:\n   \n   **Actions  Tags**:\n   ```javascript\n   // Old\n   export default {\n     component: Button,\n     parameters: {\n       docs: { autodocs: true }\n     }\n   };\n   \n   // New\n   export default {\n     component: Button,\n     tags: ['autodocs']\n   };\n   ```\n\n6. **Module Mocking Updates**:\n   \n   **New Parameter Structure**:\n   ```javascript\n   // Old approach (custom mocks)\n   import { page } from './__mocks__/stores';\n   \n   // New approach (parameters)\n   export const Default = {\n     parameters: {\n       sveltekit_experimental: {\n         stores: { page: { ... } }\n       }\n     }\n   };\n   ```\n\n7. **Migration Script**:\n   ```javascript\n   // migration-helper.js\n   import { readdir, readFile, writeFile } from 'fs/promises';\n   import { parse, walk } from 'svelte/compiler';\n   \n   async function migrateStories() {\n     // Find all .stories.svelte files\n     // Parse and transform AST\n     // Update syntax to v5\n     // Write updated files\n   }\n   ```\n\n8. **Testing After Migration**:\n   - Run `npm run storybook`\n   - Check all stories render\n   - Verify interactions work\n   - Test addons functionality\n   - Validate build process\n\n## Migration Checklist\n\n1. [ ] Backup current setup\n2. [ ] Update Storybook to v7+\n3. [ ] Migrate to @storybook/sveltekit\n4. [ ] Update Svelte CSF addon\n5. [ ] Convert story syntax\n6. [ ] Update module mocks\n7. [ ] Test all stories\n8. [ ] Update CI/CD config\n\n## Example Usage\n\nUser: \"Migrate my Storybook from v6 with Svelte to v7 with SvelteKit\"\n\nAssistant will:\n- Analyze current setup\n- Create migration plan\n- Run upgrade command\n- Update framework config\n- Convert story formats\n- Migrate CSF syntax\n- Update module mocking\n- Test and validate\n- Document breaking changes"
              },
              {
                "name": "/svelte-storybook-mock",
                "description": "Mock SvelteKit modules and functionality in Storybook stories for isolated component development.",
                "path": "plugins/all-commands/commands/svelte-storybook-mock.md",
                "frontmatter": {
                  "description": "Mock SvelteKit modules and functionality in Storybook stories for isolated component development.",
                  "category": "framework-svelte"
                },
                "content": "# /svelte-storybook-mock\n\nMock SvelteKit modules and functionality in Storybook stories for isolated component development.\n\n## Instructions\n\nYou are acting as the Svelte Storybook Specialist Agent focused on mocking SvelteKit modules. When setting up mocks:\n\n1. **Module Mocking Overview**:\n   \n   **Fully Supported**:\n   - `$app/environment` - Browser and version info\n   - `$app/paths` - Base paths configuration\n   - `$lib` - Library imports\n   - `@sveltejs/kit/*` - Kit utilities\n   \n   **Experimental (Requires Mocking)**:\n   - `$app/stores` - Page, navigating, updated stores\n   - `$app/navigation` - Navigation functions\n   - `$app/forms` - Form enhancement\n   \n   **Not Supported**:\n   - `$env/dynamic/private` - Server-only\n   - `$env/static/private` - Server-only\n   - `$service-worker` - Service worker context\n\n2. **Store Mocking**:\n   ```javascript\n   export const Default = {\n     parameters: {\n       sveltekit_experimental: {\n         stores: {\n           // Page store\n           page: {\n             url: new URL('https://example.com/products/123'),\n             params: { id: '123' },\n             route: {\n               id: '/products/[id]'\n             },\n             status: 200,\n             error: null,\n             data: {\n               product: {\n                 id: '123',\n                 name: 'Sample Product',\n                 price: 99.99\n               }\n             },\n             form: null\n           },\n           // Navigating store\n           navigating: {\n             from: {\n               params: { id: '122' },\n               route: { id: '/products/[id]' },\n               url: new URL('https://example.com/products/122')\n             },\n             to: {\n               params: { id: '123' },\n               route: { id: '/products/[id]' },\n               url: new URL('https://example.com/products/123')\n             },\n             type: 'link',\n             delta: 1\n           },\n           // Updated store\n           updated: true\n         }\n       }\n     }\n   };\n   ```\n\n3. **Navigation Mocking**:\n   ```javascript\n   parameters: {\n     sveltekit_experimental: {\n       navigation: {\n         goto: (url, options) => {\n           console.log('Navigating to:', url);\n           action('goto')(url, options);\n         },\n         pushState: (url, state) => {\n           console.log('Push state:', url, state);\n           action('pushState')(url, state);\n         },\n         replaceState: (url, state) => {\n           console.log('Replace state:', url, state);\n           action('replaceState')(url, state);\n         },\n         invalidate: (url) => {\n           console.log('Invalidate:', url);\n           action('invalidate')(url);\n         },\n         invalidateAll: () => {\n           console.log('Invalidate all');\n           action('invalidateAll')();\n         },\n         afterNavigate: {\n           from: null,\n           to: { url: new URL('https://example.com') },\n           type: 'enter'\n         }\n       }\n     }\n   }\n   ```\n\n4. **Form Enhancement Mocking**:\n   ```javascript\n   parameters: {\n     sveltekit_experimental: {\n       forms: {\n         enhance: (form) => {\n           console.log('Form enhanced:', form);\n           // Return cleanup function\n           return {\n             destroy() {\n               console.log('Form enhancement cleaned up');\n             }\n           };\n         }\n       }\n     }\n   }\n   ```\n\n5. **Link Handling**:\n   ```javascript\n   parameters: {\n     sveltekit_experimental: {\n       hrefs: {\n         // Exact match\n         '/products': (to, event) => {\n           console.log('Products link clicked');\n           event.preventDefault();\n         },\n         // Regex pattern\n         '/product/.*': {\n           callback: (to, event) => {\n             console.log('Product detail:', to);\n           },\n           asRegex: true\n         },\n         // API routes\n         '/api/.*': {\n           callback: (to, event) => {\n             event.preventDefault();\n             console.log('API call intercepted:', to);\n           },\n           asRegex: true\n         }\n       }\n     }\n   }\n   ```\n\n6. **Complex Mocking Scenarios**:\n   \n   **Auth State**:\n   ```javascript\n   const mockAuthenticatedUser = {\n     parameters: {\n       sveltekit_experimental: {\n         stores: {\n           page: {\n             data: {\n               user: {\n                 id: '123',\n                 email: 'user@example.com',\n                 role: 'admin'\n               },\n               session: {\n                 token: 'mock-jwt-token',\n                 expiresAt: '2024-12-31'\n               }\n             }\n           }\n         }\n       }\n     }\n   };\n   ```\n   \n   **Loading States**:\n   ```javascript\n   const mockLoadingState = {\n     parameters: {\n       sveltekit_experimental: {\n         stores: {\n           navigating: {\n             from: { url: new URL('https://example.com') },\n             to: { url: new URL('https://example.com/products') }\n           }\n         }\n       }\n     }\n   };\n   ```\n\n## Example Usage\n\nUser: \"Mock SvelteKit stores for my ProductDetail component\"\n\nAssistant will:\n- Analyze component's store dependencies\n- Create comprehensive store mocks\n- Mock page data with product info\n- Set up navigation mocks\n- Configure link handling\n- Add form enhancement if needed\n- Create multiple story variants\n- Test different states (loading, error, success)"
              },
              {
                "name": "/svelte-storybook-setup",
                "description": "Initialize and configure Storybook for SvelteKit projects with optimal settings and structure.",
                "path": "plugins/all-commands/commands/svelte-storybook-setup.md",
                "frontmatter": {
                  "description": "Initialize and configure Storybook for SvelteKit projects with optimal settings and structure.",
                  "category": "framework-svelte",
                  "allowed-tools": "Glob"
                },
                "content": "# /svelte-storybook-setup\n\nInitialize and configure Storybook for SvelteKit projects with optimal settings and structure.\n\n## Instructions\n\nYou are acting as the Svelte Storybook Specialist Agent focused on Storybook setup. When setting up Storybook:\n\n1. **Installation Process**:\n   \n   **New Installation**:\n   ```bash\n   npx storybook@latest init\n   ```\n   \n   **Manual Setup**:\n   - Install core dependencies\n   - Configure @storybook/sveltekit framework\n   - Add essential addons\n   - Set up Svelte CSF addon\n\n2. **Configuration Files**:\n   \n   **.storybook/main.js**:\n   ```javascript\n   export default {\n     stories: ['../src/**/*.stories.@(js|ts|svelte)'],\n     addons: [\n       '@storybook/addon-essentials',\n       '@storybook/addon-svelte-csf',\n       '@storybook/addon-a11y',\n       '@storybook/addon-interactions'\n     ],\n     framework: {\n       name: '@storybook/sveltekit',\n       options: {}\n     },\n     staticDirs: ['../static']\n   };\n   ```\n   \n   **.storybook/preview.js**:\n   ```javascript\n   import '../src/app.css'; // Global styles\n   \n   export const parameters = {\n     actions: { argTypesRegex: '^on[A-Z].*' },\n     controls: {\n       matchers: {\n         color: /(background|color)$/i,\n         date: /Date$/i\n       }\n     },\n     layout: 'centered'\n   };\n   ```\n\n3. **Project Structure**:\n   ```\n   src/\n    lib/\n       components/\n           Button/\n              Button.svelte\n              Button.stories.svelte\n              Button.test.ts\n           Card/\n               Card.svelte\n               Card.stories.svelte\n    stories/\n        Introduction.mdx\n        Configure.mdx\n   ```\n\n4. **Essential Addons**:\n   - **@storybook/addon-essentials**: Core functionality\n   - **@storybook/addon-svelte-csf**: Native Svelte stories\n   - **@storybook/addon-a11y**: Accessibility testing\n   - **@storybook/addon-interactions**: Play functions\n   - **@chromatic-com/storybook**: Visual testing\n\n5. **Scripts Configuration**:\n   ```json\n   {\n     \"scripts\": {\n       \"storybook\": \"storybook dev -p 6006\",\n       \"build-storybook\": \"storybook build\",\n       \"test-storybook\": \"test-storybook\",\n       \"chromatic\": \"chromatic --exit-zero-on-changes\"\n     }\n   }\n   ```\n\n6. **SvelteKit Integration**:\n   - Configure module mocking\n   - Set up path aliases\n   - Handle SSR considerations\n   - Configure static assets\n\n## Example Usage\n\nUser: \"Set up Storybook for my new SvelteKit project\"\n\nAssistant will:\n- Check project structure and dependencies\n- Run Storybook init command\n- Configure for SvelteKit framework\n- Add Svelte CSF addon\n- Set up proper file structure\n- Create example stories\n- Configure preview settings\n- Add helpful npm scripts\n- Set up GitHub Actions for Chromatic"
              },
              {
                "name": "/svelte-storybook-story",
                "description": "Create comprehensive Storybook stories for Svelte components using modern patterns and best practices.",
                "path": "plugins/all-commands/commands/svelte-storybook-story.md",
                "frontmatter": {
                  "description": "Create comprehensive Storybook stories for Svelte components using modern patterns and best practices.",
                  "category": "framework-svelte"
                },
                "content": "# /svelte-storybook-story\n\nCreate comprehensive Storybook stories for Svelte components using modern patterns and best practices.\n\n## Instructions\n\nYou are acting as the Svelte Storybook Specialist Agent focused on creating stories. When creating stories:\n\n1. **Analyze the Component**:\n   - Review component props and types\n   - Identify all possible states\n   - Find interactive elements\n   - Check for slots and events\n   - Note accessibility requirements\n\n2. **Story Structure (Svelte CSF)**:\n   ```svelte\n   <script>\n     import { defineMeta } from '@storybook/addon-svelte-csf';\n     import { within, userEvent, expect } from '@storybook/test';\n     import Component from './Component.svelte';\n\n     const { Story } = defineMeta({\n       component: Component,\n       title: 'Category/Component',\n       tags: ['autodocs'],\n       parameters: {\n         layout: 'centered',\n         docs: {\n           description: {\n             component: 'Component description for docs'\n           }\n         }\n       },\n       argTypes: {\n         variant: {\n           control: 'select',\n           options: ['primary', 'secondary'],\n           description: 'Visual style variant'\n         },\n         size: {\n           control: 'radio',\n           options: ['small', 'medium', 'large']\n         },\n         disabled: {\n           control: 'boolean'\n         }\n       }\n     });\n   </script>\n   ```\n\n3. **Story Patterns**:\n   \n   **Basic Story**:\n   ```svelte\n   <Story name=\"Default\" args={{ label: 'Click me' }} />\n   ```\n   \n   **With Children/Slots**:\n   ```svelte\n   <Story name=\"WithIcon\">\n     {#snippet template(args)}\n       <Component {...args}>\n         <Icon slot=\"icon\" />\n         Custom content\n       </Component>\n     {/snippet}\n   </Story>\n   ```\n   \n   **Interactive Story**:\n   ```svelte\n   <Story \n     name=\"Interactive\"\n     play={async ({ canvasElement }) => {\n       const canvas = within(canvasElement);\n       const button = canvas.getByRole('button');\n       \n       await userEvent.click(button);\n       await expect(button).toHaveTextContent('Clicked!');\n     }}\n   />\n   ```\n\n4. **Common Story Types**:\n   - **Default**: Basic component usage\n   - **Variants**: All visual variations\n   - **States**: Loading, error, success, empty\n   - **Sizes**: All size options\n   - **Interactive**: User interactions\n   - **Responsive**: Different viewports\n   - **Accessibility**: Focus and ARIA states\n   - **Edge Cases**: Long text, missing data\n\n5. **Advanced Features**:\n   \n   **Custom Render**:\n   ```svelte\n   <Story name=\"Grid\">\n     {#snippet template()}\n       <div class=\"grid grid-cols-3 gap-4\">\n         <Component variant=\"primary\" />\n         <Component variant=\"secondary\" />\n         <Component variant=\"tertiary\" />\n       </div>\n     {/snippet}\n   </Story>\n   ```\n   \n   **With Decorators**:\n   ```javascript\n   export const DarkMode = {\n     decorators: [\n       (Story) => ({\n         Component: Story,\n         props: {\n           style: 'background: #333; padding: 2rem;'\n         }\n       })\n     ]\n   };\n   ```\n\n6. **Documentation**:\n   - Use JSDoc for props\n   - Add story descriptions\n   - Include usage examples\n   - Document accessibility\n   - Add design notes\n\n## Example Usage\n\nUser: \"Create stories for my Button component\"\n\nAssistant will:\n- Analyze Button.svelte component\n- Create comprehensive stories file\n- Add all visual variants\n- Include interactive states\n- Test keyboard navigation\n- Add accessibility tests\n- Create responsive stories\n- Document all props\n- Add play functions for interactions"
              },
              {
                "name": "/svelte-storybook-troubleshoot",
                "description": "Diagnose and fix common Storybook issues in SvelteKit projects, including build errors, module problems, and configuration issues.",
                "path": "plugins/all-commands/commands/svelte-storybook-troubleshoot.md",
                "frontmatter": {
                  "description": "Diagnose and fix common Storybook issues in SvelteKit projects, including build errors, module problems, and configuration issues.",
                  "category": "framework-svelte",
                  "allowed-tools": "Glob"
                },
                "content": "# /svelte-storybook-troubleshoot\n\nDiagnose and fix common Storybook issues in SvelteKit projects, including build errors, module problems, and configuration issues.\n\n## Instructions\n\nYou are acting as the Svelte Storybook Specialist Agent focused on troubleshooting. When diagnosing issues:\n\n1. **Common Build Errors**:\n   \n   **\"__esbuild_register_import_meta_url__ already declared\"**:\n   - Remove `svelteOptions` from `.storybook/main.js`\n   - This is a v6 to v7 migration issue\n   - Ensure using @storybook/sveltekit framework\n   \n   **Module Resolution Errors**:\n   ```javascript\n   // .storybook/main.js\n   export default {\n     framework: {\n       name: '@storybook/sveltekit',\n       options: {\n         builder: {\n           viteConfigPath: './vite.config.js'\n         }\n       }\n     },\n     viteFinal: async (config) => {\n       config.resolve.alias = {\n         ...config.resolve.alias,\n         $lib: path.resolve('./src/lib'),\n         $app: path.resolve('./.storybook/mocks/app')\n       };\n       return config;\n     }\n   };\n   ```\n\n2. **SvelteKit Module Issues**:\n   \n   **\"Cannot find module '$app/stores'\"**:\n   - These modules need mocking\n   - Use `parameters.sveltekit_experimental`\n   - Create mock files if needed:\n   ```javascript\n   // .storybook/mocks/app/stores.js\n   import { writable } from 'svelte/store';\n   \n   export const page = writable({\n     url: new URL('http://localhost:6006'),\n     params: {},\n     route: { id: '/' },\n     data: {}\n   });\n   \n   export const navigating = writable(null);\n   export const updated = writable(false);\n   ```\n\n3. **CSS and Styling Issues**:\n   \n   **Global Styles Not Loading**:\n   ```javascript\n   // .storybook/preview.js\n   import '../src/app.css';\n   import '../src/app.postcss';\n   import '../src/styles/global.css';\n   ```\n   \n   **Tailwind Not Working**:\n   ```javascript\n   // .storybook/main.js\n   export default {\n     addons: [\n       {\n         name: '@storybook/addon-postcss',\n         options: {\n           postcssLoaderOptions: {\n             implementation: require('postcss')\n           }\n         }\n       }\n     ]\n   };\n   ```\n\n4. **Component Import Issues**:\n   \n   **SSR Components**:\n   ```javascript\n   // Mark stories as client-only if needed\n   export const Default = {\n     parameters: {\n       storyshots: { disable: true } // Skip for SSR-incompatible\n     }\n   };\n   ```\n   \n   **Dynamic Imports**:\n   ```javascript\n   // Use lazy loading for heavy components\n   const HeavyComponent = lazy(() => import('./HeavyComponent.svelte'));\n   ```\n\n5. **Environment Variables**:\n   \n   **PUBLIC_ Variables Not Available**:\n   ```javascript\n   // .storybook/main.js\n   export default {\n     env: (config) => ({\n       ...config,\n       PUBLIC_API_URL: process.env.PUBLIC_API_URL || 'http://localhost:3000'\n     })\n   };\n   ```\n   \n   **Create .env for Storybook**:\n   ```bash\n   # .env.storybook\n   PUBLIC_API_URL=http://localhost:3000\n   PUBLIC_FEATURE_FLAG=true\n   ```\n\n6. **Performance Issues**:\n   \n   **Slow Build Times**:\n   - Exclude large dependencies\n   - Use production builds\n   - Enable caching\n   ```javascript\n   export default {\n     features: {\n       buildStoriesJson: true,\n       storyStoreV7: true\n     },\n     core: {\n       disableTelemetry: true\n     }\n   };\n   ```\n\n7. **Addon Conflicts**:\n   \n   **Version Mismatches**:\n   ```bash\n   # Check for version conflicts\n   npm ls @storybook/svelte\n   npm ls @storybook/sveltekit\n   \n   # Update all Storybook packages\n   npx storybook@latest upgrade\n   ```\n\n8. **Testing Issues**:\n   \n   **Play Functions Not Working**:\n   ```javascript\n   // Ensure testing library is set up\n   import { within, userEvent, expect } from '@storybook/test';\n   ```\n   \n   **Interaction Tests Failing**:\n   - Check element selectors\n   - Add proper waits\n   - Use data-testid attributes\n\n## Debugging Checklist\n\n1. [ ] Check Storybook and SvelteKit versions\n2. [ ] Verify framework configuration\n3. [ ] Check for module mocking needs\n4. [ ] Validate Vite configuration\n5. [ ] Review addon compatibility\n6. [ ] Test in isolation mode\n7. [ ] Check browser console errors\n8. [ ] Review build output\n\n## Example Usage\n\nUser: \"Storybook won't start, getting module errors\"\n\nAssistant will:\n- Check error messages\n- Identify missing module mocks\n- Set up proper aliases\n- Configure module mocking\n- Fix import paths\n- Test the solution\n- Provide debugging steps\n- Document the fix for team"
              },
              {
                "name": "/svelte-storybook",
                "description": "General-purpose Storybook assistance for SvelteKit projects, including setup guidance, best practices, and common tasks.",
                "path": "plugins/all-commands/commands/svelte-storybook.md",
                "frontmatter": {
                  "description": "General-purpose Storybook assistance for SvelteKit projects, including setup guidance, best practices, and common tasks.",
                  "category": "framework-svelte"
                },
                "content": "# /svelte-storybook\n\nGeneral-purpose Storybook assistance for SvelteKit projects, including setup guidance, best practices, and common tasks.\n\n## Instructions\n\nYou are acting as the Svelte Storybook Specialist Agent. Provide comprehensive assistance with Storybook for SvelteKit projects.\n\n1. **Assess the Request**:\n   - Determine if it's about setup, story creation, configuration, or troubleshooting\n   - Check the current Storybook setup in the project\n   - Identify specific Storybook version and addons\n\n2. **Common Tasks**:\n   - Setting up Storybook in a SvelteKit project\n   - Creating stories for components\n   - Configuring Storybook for SvelteKit modules\n   - Adding addons and customizations\n   - Optimizing Storybook performance\n   - Setting up visual testing\n\n3. **Best Practices**:\n   - Use Svelte CSF format for native syntax\n   - Implement proper mocking for SvelteKit modules\n   - Structure stories for maintainability\n   - Document components with controls and docs\n   - Set up accessibility testing\n\n4. **Guidance Areas**:\n   - Project structure for stories\n   - Naming conventions\n   - Story organization\n   - Addon selection\n   - Testing integration\n   - CI/CD setup\n\n## Example Usage\n\nUser: \"Help me set up Storybook for my component library\"\n\nAssistant will:\n- Check if Storybook is already installed\n- Guide through installation if needed\n- Set up proper configuration\n- Create example stories\n- Configure essential addons\n- Provide project structure recommendations\n- Set up build and deployment scripts"
              },
              {
                "name": "/svelte-test-coverage",
                "description": "Analyze test coverage, identify testing gaps, and provide recommendations for improving test coverage in Svelte/SvelteKit projects.",
                "path": "plugins/all-commands/commands/svelte-test-coverage.md",
                "frontmatter": {
                  "description": "Analyze test coverage, identify testing gaps, and provide recommendations for improving test coverage in Svelte/SvelteKit projects.",
                  "category": "framework-svelte",
                  "allowed-tools": "Bash(gh *)"
                },
                "content": "# /svelte-test-coverage\n\nAnalyze test coverage, identify testing gaps, and provide recommendations for improving test coverage in Svelte/SvelteKit projects.\n\n## Instructions\n\nYou are acting as the Svelte Testing Specialist Agent focused on test coverage analysis. When analyzing coverage:\n\n1. **Coverage Analysis**:\n   - Run coverage reports\n   - Identify untested files and functions\n   - Analyze coverage metrics (statements, branches, functions, lines)\n   - Find critical paths without tests\n\n2. **Gap Identification**:\n   \n   **Component Coverage**:\n   - Props not tested\n   - Event handlers without tests\n   - Conditional rendering paths\n   - Error states\n   - Edge cases\n   \n   **Route Coverage**:\n   - Untested load functions\n   - Form actions without tests\n   - Error boundaries\n   - Authentication flows\n   \n   **Business Logic**:\n   - Stores without tests\n   - Utility functions\n   - Data transformations\n   - API integrations\n\n3. **Priority Matrix**:\n   ```\n   High Priority:\n   - Core user flows\n   - Payment/checkout processes\n   - Authentication/authorization\n   - Data mutations\n   \n   Medium Priority:\n   - UI component variations\n   - Form validations\n   - Navigation flows\n   \n   Low Priority:\n   - Static content\n   - Simple presentational components\n   ```\n\n4. **Coverage Report Actions**:\n   - Generate visual coverage reports\n   - Create coverage badges\n   - Set up coverage thresholds\n   - Integrate with CI/CD\n\n5. **Recommendations**:\n   - Suggest specific tests to write\n   - Identify high-risk untested code\n   - Propose testing strategies\n   - Estimate effort for coverage improvement\n\n## Example Usage\n\nUser: \"Analyze test coverage for my e-commerce site\"\n\nAssistant will:\n- Run coverage analysis\n- Identify critical untested paths (checkout, payment)\n- Find components with low coverage\n- Analyze store and API coverage\n- Create prioritized test writing plan\n- Suggest coverage threshold targets\n- Provide specific test examples for gaps"
              },
              {
                "name": "/svelte-test-fix",
                "description": "Troubleshoot and fix failing tests in Svelte/SvelteKit projects, including debugging test issues and resolving common testing problems.",
                "path": "plugins/all-commands/commands/svelte-test-fix.md",
                "frontmatter": {
                  "description": "Troubleshoot and fix failing tests in Svelte/SvelteKit projects, including debugging test issues and resolving common testing problems.",
                  "category": "framework-svelte"
                },
                "content": "# /svelte-test-fix\n\nTroubleshoot and fix failing tests in Svelte/SvelteKit projects, including debugging test issues and resolving common testing problems.\n\n## Instructions\n\nYou are acting as the Svelte Testing Specialist Agent focused on fixing test issues. When troubleshooting tests:\n\n1. **Diagnose Test Failures**:\n   - Analyze error messages and stack traces\n   - Identify failure patterns (flaky, consistent, environment-specific)\n   - Check test logs and debug output\n   - Review recent code changes\n\n2. **Common Test Issues**:\n   \n   **Component Tests**:\n   - Async timing issues  Use `await tick()` or `flushSync()`\n   - Component not cleaning up  Ensure proper unmounting\n   - State not updating  Check reactivity and bindings\n   - DOM queries failing  Use proper Testing Library queries\n   \n   **E2E Tests**:\n   - Timing issues  Add proper waits and assertions\n   - Selector problems  Use data-testid attributes\n   - Navigation failures  Check route configurations\n   - API mocking issues  Verify mock setup\n   \n   **Environment Issues**:\n   - Module resolution  Check import paths\n   - TypeScript errors  Verify test tsconfig\n   - Missing globals  Configure test environment\n   - Build conflicts  Separate test builds\n\n3. **Debugging Techniques**:\n   ```javascript\n   // Add debug helpers\n   const { debug } = render(Component);\n   debug(); // Print DOM\n   \n   // Component state inspection\n   console.log('Props:', component.$$.props);\n   console.log('Context:', component.$$.context);\n   \n   // Playwright debugging\n   await page.pause(); // Interactive debugging\n   await page.screenshot({ path: 'debug.png' });\n   ```\n\n4. **Fix Strategies**:\n   - Isolate failing tests\n   - Add detailed logging\n   - Simplify test cases\n   - Mock external dependencies\n   - Fix timing/race conditions\n\n5. **Prevention**:\n   - Add retry logic for flaky tests\n   - Improve test stability\n   - Set up better error reporting\n   - Create test utilities\n\n## Example Usage\n\nUser: \"My component tests are failing with 'Cannot access before initialization' errors\"\n\nAssistant will:\n- Analyze the test setup\n- Check component lifecycle\n- Identify initialization issues\n- Fix async/timing problems\n- Add proper test utilities\n- Ensure cleanup procedures\n- Provide debugging tips"
              },
              {
                "name": "/svelte-test-setup",
                "description": "Set up comprehensive testing infrastructure for Svelte/SvelteKit projects, including unit testing, component testing, and E2E testing frameworks.",
                "path": "plugins/all-commands/commands/svelte-test-setup.md",
                "frontmatter": {
                  "description": "Set up comprehensive testing infrastructure for Svelte/SvelteKit projects, including unit testing, component testing, and E2E testing frameworks.",
                  "category": "framework-svelte"
                },
                "content": "# /svelte-test-setup\n\nSet up comprehensive testing infrastructure for Svelte/SvelteKit projects, including unit testing, component testing, and E2E testing frameworks.\n\n## Instructions\n\nYou are acting as the Svelte Testing Specialist Agent focused on testing infrastructure. When setting up testing:\n\n1. **Assess Current State**:\n   - Check existing test setup\n   - Identify missing testing tools\n   - Review package.json for test scripts\n   - Analyze project structure\n\n2. **Testing Stack Setup**:\n   \n   **Unit/Component Testing (Vitest)**:\n   - Install dependencies: `vitest`, `@testing-library/svelte`, `jsdom`\n   - Configure vitest.config.js\n   - Set up test helpers and utilities\n   - Create setup files\n   \n   **E2E Testing (Playwright)**:\n   - Install Playwright\n   - Configure playwright.config.js\n   - Set up test fixtures\n   - Create page object models\n   \n   **Additional Tools**:\n   - Coverage reporting (c8/istanbul)\n   - Test utilities (@testing-library/user-event)\n   - Mock service worker for API mocking\n   - Visual regression testing tools\n\n3. **Configuration Files**:\n   ```javascript\n   // vitest.config.js\n   import { sveltekit } from '@sveltejs/kit/vite';\n   import { defineConfig } from 'vitest/config';\n   \n   export default defineConfig({\n     plugins: [sveltekit()],\n     test: {\n       environment: 'jsdom',\n       setupFiles: ['./src/tests/setup.ts'],\n       coverage: {\n         reporter: ['text', 'html', 'lcov']\n       }\n     }\n   });\n   ```\n\n4. **Test Structure**:\n   ```\n   src/\n    tests/\n       setup.ts\n       helpers/\n       fixtures/\n    routes/\n       +page.test.ts\n    lib/\n        Component.test.ts\n   ```\n\n5. **NPM Scripts**:\n   - `test`: Run all tests\n   - `test:unit`: Run unit tests\n   - `test:e2e`: Run E2E tests\n   - `test:coverage`: Generate coverage report\n   - `test:watch`: Run tests in watch mode\n\n## Example Usage\n\nUser: \"Set up testing for my new SvelteKit project\"\n\nAssistant will:\n- Analyze current project setup\n- Install and configure Vitest\n- Install and configure Playwright\n- Create test configuration files\n- Set up test utilities and helpers\n- Add comprehensive npm scripts\n- Create example tests\n- Set up CI/CD test workflows"
              },
              {
                "name": "/svelte-test",
                "description": "Create comprehensive tests for Svelte components and SvelteKit routes, including unit tests, component tests, and E2E tests.",
                "path": "plugins/all-commands/commands/svelte-test.md",
                "frontmatter": {
                  "description": "Create comprehensive tests for Svelte components and SvelteKit routes, including unit tests, component tests, and E2E tests.",
                  "category": "framework-svelte"
                },
                "content": "# /svelte-test\n\nCreate comprehensive tests for Svelte components and SvelteKit routes, including unit tests, component tests, and E2E tests.\n\n## Instructions\n\nYou are acting as the Svelte Testing Specialist Agent. When creating tests:\n\n1. **Analyze the Target**:\n   - Identify what needs testing (component, route, store, utility)\n   - Determine appropriate test types (unit, integration, E2E)\n   - Review existing test patterns in the codebase\n\n2. **Test Creation Strategy**:\n   - **Component Tests**: User interactions, prop variations, slots, events\n   - **Route Tests**: Load functions, form actions, error handling\n   - **Store Tests**: State changes, derived values, subscriptions\n   - **E2E Tests**: User flows, navigation, form submissions\n\n3. **Test Structure**:\n   ```javascript\n   // Component Test Example\n   import { render, fireEvent } from '@testing-library/svelte';\n   import { expect, test, describe } from 'vitest';\n   \n   describe('Component', () => {\n     test('user interaction', async () => {\n       // Arrange\n       // Act\n       // Assert\n     });\n   });\n   ```\n\n4. **Coverage Areas**:\n   - Happy path scenarios\n   - Edge cases and error states\n   - Accessibility requirements\n   - Performance constraints\n   - Security considerations\n\n5. **Test Types to Generate**:\n   - Vitest unit/component tests\n   - Playwright E2E tests\n   - Accessibility tests\n   - Performance tests\n   - Visual regression tests\n\n## Example Usage\n\nUser: \"Create tests for my UserProfile component that has edit mode\"\n\nAssistant will:\n- Analyze UserProfile component structure\n- Create comprehensive component tests\n- Test view/edit mode transitions\n- Test form validation in edit mode\n- Add accessibility tests\n- Create E2E test for full user flow\n- Suggest additional test scenarios"
              },
              {
                "name": "/sync-automation-setup",
                "description": "Setup automated synchronization workflows",
                "path": "plugins/all-commands/commands/sync-automation-setup.md",
                "frontmatter": {
                  "description": "Setup automated synchronization workflows",
                  "category": "integration-sync",
                  "allowed-tools": "Bash(npm *)"
                },
                "content": "# sync-automation-setup\n\nSetup automated synchronization workflows\n\n## System\n\nYou are an automation setup specialist that configures robust, automated synchronization between GitHub and Linear. You handle webhook configuration, CI/CD integration, scheduling, monitoring, and ensure reliable continuous synchronization.\n\n## Instructions\n\nWhen setting up sync automation:\n\n1. **Prerequisites Check**\n   ```javascript\n   async function checkPrerequisites() {\n     const checks = {\n       github: {\n         cli: await checkCommand('gh --version'),\n         auth: await checkGitHubAuth(),\n         permissions: await checkGitHubPermissions(),\n         webhookAccess: await checkWebhookPermissions()\n       },\n       linear: {\n         mcp: await checkLinearMCP(),\n         apiKey: await checkLinearAPIKey(),\n         webhookUrl: await checkLinearWebhookEndpoint()\n       },\n       infrastructure: {\n         serverEndpoint: process.env.SYNC_SERVER_URL,\n         database: await checkDatabaseConnection(),\n         queue: await checkQueueService(),\n         storage: await checkStateStorage()\n       }\n     };\n     \n     return validateAllChecks(checks);\n   }\n   ```\n\n2. **GitHub Webhook Setup**\n   ```bash\n   # Create webhook for issue events\n   gh api repos/:owner/:repo/hooks \\\n     --method POST \\\n     --field name='web' \\\n     --field active=true \\\n     --field events[]='issues' \\\n     --field events[]='issue_comment' \\\n     --field events[]='pull_request' \\\n     --field events[]='pull_request_review' \\\n     --field config[url]=\"${WEBHOOK_URL}/github\" \\\n     --field config[content_type]='json' \\\n     --field config[secret]=\"${WEBHOOK_SECRET}\"\n   ```\n\n3. **Linear Webhook Configuration**\n   ```javascript\n   async function setupLinearWebhooks() {\n     const webhook = await linear.createWebhook({\n       url: `${WEBHOOK_URL}/linear`,\n       resourceTypes: ['Issue', 'Comment', 'Project', 'Cycle'],\n       label: 'GitHub Sync',\n       enabled: true,\n       secret: process.env.LINEAR_WEBHOOK_SECRET\n     });\n     \n     // Verify webhook\n     await linear.testWebhook(webhook.id);\n     \n     return webhook;\n   }\n   ```\n\n4. **GitHub Actions Workflow**\n   ```yaml\n   # .github/workflows/linear-sync.yml\n   name: Linear Sync\n   \n   on:\n     issues:\n       types: [opened, edited, closed, reopened, labeled, unlabeled]\n     issue_comment:\n       types: [created, edited, deleted]\n     pull_request:\n       types: [opened, edited, closed, merged]\n     schedule:\n       - cron: '*/15 * * * *'  # Every 15 minutes\n     workflow_dispatch:\n       inputs:\n         sync_type:\n           description: 'Type of sync to perform'\n           required: true\n           default: 'incremental'\n           type: choice\n           options:\n             - incremental\n             - full\n             - repair\n   \n   jobs:\n     sync:\n       runs-on: ubuntu-latest\n       steps:\n         - uses: actions/checkout@v3\n         \n         - name: Setup sync environment\n           run: |\n             npm install -g @linear/sync-cli\n             echo \"${{ secrets.SYNC_CONFIG }}\" > sync.config.json\n         \n         - name: Run sync\n           env:\n             GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n             LINEAR_API_KEY: ${{ secrets.LINEAR_API_KEY }}\n             SYNC_STATE_BUCKET: ${{ secrets.SYNC_STATE_BUCKET }}\n           run: |\n             case \"${{ github.event_name }}\" in\n               \"schedule\")\n                 linear-sync run --type=incremental\n                 ;;\n               \"workflow_dispatch\")\n                 linear-sync run --type=${{ inputs.sync_type }}\n                 ;;\n               *)\n                 linear-sync handle-event \\\n                   --event=${{ github.event_name }} \\\n                   --payload='${{ toJSON(github.event) }}'\n                 ;;\n             esac\n         \n         - name: Upload sync report\n           if: always()\n           uses: actions/upload-artifact@v3\n           with:\n             name: sync-report-${{ github.run_id }}\n             path: sync-report.json\n   ```\n\n5. **Sync Server Configuration**\n   ```javascript\n   // sync-server.js\n   const express = require('express');\n   const { Queue } = require('bull');\n   const { SyncEngine } = require('./sync-engine');\n   \n   const app = express();\n   const syncQueue = new Queue('sync-tasks', REDIS_URL);\n   const syncEngine = new SyncEngine();\n   \n   // GitHub webhook endpoint\n   app.post('/webhooks/github', verifyGitHubWebhook, async (req, res) => {\n     const event = req.headers['x-github-event'];\n     const payload = req.body;\n     \n     // Queue sync task\n     await syncQueue.add('github-event', {\n       event,\n       payload,\n       timestamp: new Date().toISOString()\n     }, {\n       attempts: 3,\n       backoff: { type: 'exponential', delay: 2000 }\n     });\n     \n     res.status(200).send('OK');\n   });\n   \n   // Linear webhook endpoint\n   app.post('/webhooks/linear', verifyLinearWebhook, async (req, res) => {\n     const { action, data, type } = req.body;\n     \n     await syncQueue.add('linear-event', {\n       action,\n       data,\n       type,\n       timestamp: new Date().toISOString()\n     });\n     \n     res.status(200).send('OK');\n   });\n   \n   // Health check endpoint\n   app.get('/health', async (req, res) => {\n     const health = await syncEngine.getHealth();\n     res.json(health);\n   });\n   \n   // Process sync queue\n   syncQueue.process('github-event', async (job) => {\n     return await syncEngine.processGitHubEvent(job.data);\n   });\n   \n   syncQueue.process('linear-event', async (job) => {\n     return await syncEngine.processLinearEvent(job.data);\n   });\n   ```\n\n6. **Sync Configuration File**\n   ```yaml\n   # sync-config.yml\n   version: 1.0\n   \n   sync:\n     enabled: true\n     direction: bidirectional\n     mode: real-time  # real-time, scheduled, or hybrid\n     \n   scheduling:\n     incremental:\n       interval: '*/5 * * * *'  # Every 5 minutes\n       enabled: true\n     full:\n       interval: '0 2 * * *'    # Daily at 2 AM\n       enabled: true\n     health_check:\n       interval: '*/30 * * * *' # Every 30 minutes\n       enabled: true\n   \n   mapping:\n     states:\n       github_to_linear:\n         open: Todo\n         closed: Done\n       linear_to_github:\n         Backlog: open\n         Todo: open\n         'In Progress': open\n         Done: closed\n         Canceled: closed\n     \n     priorities:\n       label_to_priority:\n         'priority/urgent': 1\n         'priority/high': 2\n         'priority/medium': 3\n         'priority/low': 4\n       priority_to_label:\n         1: 'priority/urgent'\n         2: 'priority/high'\n         3: 'priority/medium'\n         4: 'priority/low'\n     \n     teams:\n       default: 'engineering'\n       mapping:\n         'frontend/*': 'frontend-team'\n         'backend/*': 'backend-team'\n         'docs/*': 'docs-team'\n   \n   conflict_resolution:\n     strategy: newer_wins  # newer_wins, github_wins, linear_wins, manual\n     preserve_fields:\n       - comments\n       - attachments\n     merge_fields:\n       - labels\n       - assignees\n   \n   filters:\n     github:\n       include_labels:\n         - 'linear-sync'\n       exclude_labels:\n         - 'no-sync'\n         - 'draft'\n     linear:\n       include_teams:\n         - 'engineering'\n         - 'product'\n       exclude_states:\n         - 'Duplicate'\n   \n   notifications:\n     slack:\n       enabled: true\n       webhook_url: ${SLACK_WEBHOOK_URL}\n       channels:\n         errors: '#sync-errors'\n         summary: '#dev-updates'\n     email:\n       enabled: false\n       recipients:\n         - 'ops@company.com'\n   \n   monitoring:\n     metrics:\n       enabled: true\n       provider: datadog\n       api_key: ${DATADOG_API_KEY}\n     logging:\n       level: info\n       destination: 'cloudwatch'\n     alerts:\n       - metric: sync_failure_rate\n         threshold: 0.05\n         action: notify\n       - metric: sync_lag\n         threshold: 300  # seconds\n         action: alert\n   ```\n\n7. **Database Schema**\n   ```sql\n   -- Sync state management\n   CREATE TABLE sync_state (\n     id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n     github_id VARCHAR(255),\n     linear_id VARCHAR(255),\n     github_updated_at TIMESTAMP,\n     linear_updated_at TIMESTAMP,\n     last_sync_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n     sync_hash VARCHAR(64),\n     sync_version INTEGER DEFAULT 1,\n     metadata JSONB,\n     UNIQUE(github_id, linear_id)\n   );\n   \n   -- Sync history\n   CREATE TABLE sync_history (\n     id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n     sync_id UUID REFERENCES sync_state(id),\n     direction VARCHAR(50),\n     status VARCHAR(50),\n     started_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n     completed_at TIMESTAMP,\n     changes JSONB,\n     errors JSONB\n   );\n   \n   -- Conflict log\n   CREATE TABLE sync_conflicts (\n     id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n     sync_id UUID REFERENCES sync_state(id),\n     detected_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n     conflict_type VARCHAR(100),\n     github_data JSONB,\n     linear_data JSONB,\n     resolution VARCHAR(100),\n     resolved_at TIMESTAMP,\n     resolved_by VARCHAR(255)\n   );\n   \n   -- Indexes for performance\n   CREATE INDEX idx_sync_state_github_id ON sync_state(github_id);\n   CREATE INDEX idx_sync_state_linear_id ON sync_state(linear_id);\n   CREATE INDEX idx_sync_history_sync_id ON sync_history(sync_id);\n   CREATE INDEX idx_sync_history_started_at ON sync_history(started_at);\n   ```\n\n8. **Monitoring Dashboard**\n   ```javascript\n   // monitoring/dashboard.js\n   const metrics = {\n     // Real-time metrics\n     syncRate: new Rate('sync.operations'),\n     syncDuration: new Histogram('sync.duration'),\n     syncErrors: new Counter('sync.errors'),\n     \n     // Business metrics\n     issuesSynced: new Counter('issues.synced'),\n     conflictsResolved: new Counter('conflicts.resolved'),\n     \n     // System health\n     apiLatency: new Histogram('api.latency'),\n     queueDepth: new Gauge('queue.depth'),\n     rateLimitRemaining: new Gauge('ratelimit.remaining')\n   };\n   \n   // Dashboard configuration\n   const dashboard = {\n     title: 'GitHub-Linear Sync Monitor',\n     widgets: [\n       {\n         type: 'timeseries',\n         title: 'Sync Operations',\n         metrics: ['sync.operations', 'sync.errors'],\n         period: '1h'\n       },\n       {\n         type: 'gauge',\n         title: 'Queue Depth',\n         metric: 'queue.depth',\n         thresholds: [0, 50, 100, 200]\n       },\n       {\n         type: 'heatmap',\n         title: 'Sync Duration',\n         metric: 'sync.duration',\n         buckets: [100, 500, 1000, 5000, 10000]\n       },\n       {\n         type: 'counter',\n         title: 'Today\\'s Syncs',\n         metric: 'issues.synced',\n         period: '1d'\n       }\n     ],\n     alerts: [\n       {\n         name: 'High Error Rate',\n         condition: 'rate(sync.errors) > 0.1',\n         severity: 'critical'\n       },\n       {\n         name: 'Sync Lag',\n         condition: 'queue.depth > 100',\n         severity: 'warning'\n       }\n     ]\n   };\n   ```\n\n9. **Deployment Script**\n   ```bash\n   #!/bin/bash\n   # deploy-sync-automation.sh\n   \n   set -e\n   \n   echo \" Deploying GitHub-Linear Sync Automation\"\n   \n   # Check prerequisites\n   echo \" Checking prerequisites...\"\n   command -v gh >/dev/null 2>&1 || { echo \" GitHub CLI required\"; exit 1; }\n   command -v docker >/dev/null 2>&1 || { echo \" Docker required\"; exit 1; }\n   \n   # Load configuration\n   source .env\n   \n   # Build sync server\n   echo \" Building sync server...\"\n   docker build -t linear-sync-server .\n   \n   # Deploy database\n   echo \" Setting up database...\"\n   docker-compose up -d postgres redis\n   sleep 5\n   docker-compose run --rm migrate\n   \n   # Configure webhooks\n   echo \" Configuring webhooks...\"\n   ./scripts/setup-webhooks.sh\n   \n   # Deploy sync server\n   echo \" Deploying sync server...\"\n   docker-compose up -d sync-server\n   \n   # Setup monitoring\n   echo \" Configuring monitoring...\"\n   ./scripts/setup-monitoring.sh\n   \n   # Verify deployment\n   echo \" Verifying deployment...\"\n   sleep 10\n   curl -f http://localhost:3000/health || { echo \" Health check failed\"; exit 1; }\n   \n   # Run initial sync\n   echo \" Running initial sync...\"\n   docker-compose run --rm sync-cli full-sync\n   \n   echo \" Deployment complete!\"\n   echo \" Dashboard: http://localhost:3000/dashboard\"\n   echo \" Logs: docker-compose logs -f sync-server\"\n   ```\n\n10. **Maintenance Commands**\n    ```bash\n    # Sync management CLI\n    linear-sync status          # Check sync status\n    linear-sync pause          # Pause all syncing\n    linear-sync resume         # Resume syncing\n    linear-sync repair         # Repair sync state\n    linear-sync reset          # Reset sync (caution!)\n    \n    # Troubleshooting\n    linear-sync diagnose       # Run diagnostics\n    linear-sync test-webhooks  # Test webhook connectivity\n    linear-sync validate       # Validate configuration\n    \n    # Maintenance\n    linear-sync cleanup        # Clean old sync records\n    linear-sync export         # Export sync state\n    linear-sync import         # Import sync state\n    ```\n\n## Examples\n\n### Basic Setup\n```bash\n# Interactive setup\nclaude sync-automation-setup\n\n# Setup with config file\nclaude sync-automation-setup --config=\"sync-config.yml\"\n\n# Minimal setup (webhooks only)\nclaude sync-automation-setup --mode=\"webhooks-only\"\n```\n\n### Advanced Configuration\n```bash\n# Full automation with monitoring\nclaude sync-automation-setup \\\n  --mode=\"full\" \\\n  --monitoring=\"datadog\" \\\n  --alerts=\"slack,email\"\n\n# Custom deployment\nclaude sync-automation-setup \\\n  --deploy-target=\"kubernetes\" \\\n  --namespace=\"sync-system\"\n```\n\n### Maintenance\n```bash\n# Update webhook configuration\nclaude sync-automation-setup --update-webhooks\n\n# Rotate secrets\nclaude sync-automation-setup --rotate-secrets\n\n# Upgrade sync version\nclaude sync-automation-setup --upgrade\n```\n\n## Output Format\n\n```\nGitHub-Linear Sync Automation Setup\n===================================\n\n Prerequisites Check\n   GitHub CLI authenticated\n   Linear MCP connected\n   Database accessible\n   Redis running\n\n Configuration Summary\n  Mode: Bidirectional real-time sync\n  Webhook URL: https://sync.company.com/webhooks\n  Sync Interval: 5 minutes (incremental)\n  Conflict Strategy: newer_wins\n\n Webhook Configuration\n  GitHub Webhooks:\n     Issues webhook created (ID: 12345)\n     Pull requests webhook created (ID: 12346)\n     Webhook test successful\n  \n  Linear Webhooks:\n     Issue webhook registered\n     Comment webhook registered\n     Webhook verified\n\n Deployment Status\n   Sync server deployed (3 replicas)\n   Database migrations complete\n   Redis queue initialized\n   Monitoring configured\n\n Monitoring Setup\n  Dashboard: https://monitoring.company.com/linear-sync\n  Alerts configured:\n    - Slack: #sync-alerts\n    - Email: ops@company.com\n  \n  Metrics collecting:\n    - Sync rate\n    - Error rate\n    - API latency\n    - Queue depth\n\n Security Configuration\n   Webhook secrets configured\n   API keys encrypted\n   TLS enabled\n   Rate limiting active\n\n Next Steps\n  1. Monitor initial sync: docker-compose logs -f\n  2. Check dashboard for metrics\n  3. Review sync-config.yml for customization\n  4. Set up team notifications\n\nAutomation Status:  ACTIVE\nFirst sync scheduled: 2 minutes\n```\n\n## Best Practices\n\n1. **Security**\n   - Use webhook secrets\n   - Encrypt API keys\n   - Implement rate limiting\n   - Regular secret rotation\n\n2. **Reliability**\n   - Implement retry logic\n   - Use message queues\n   - Monitor system health\n   - Plan for failures\n\n3. **Performance**\n   - Optimize batch sizes\n   - Implement caching\n   - Use connection pooling\n   - Monitor API limits\n\n4. **Maintenance**\n   - Regular health checks\n   - Automated backups\n   - Log retention policies\n   - Update procedures"
              },
              {
                "name": "/sync-conflict-resolver",
                "description": "Resolve synchronization conflicts automatically",
                "path": "plugins/all-commands/commands/sync-conflict-resolver.md",
                "frontmatter": {
                  "description": "Resolve synchronization conflicts automatically",
                  "category": "integration-sync",
                  "argument-hint": "Set up conflict detection parameters"
                },
                "content": "# Sync Conflict Resolver\n\nResolve synchronization conflicts automatically\n\n## Instructions\n\n1. **Initialize Conflict Detection**\n   - Check GitHub CLI and Linear MCP availability\n   - Load existing sync metadata and mappings\n   - Parse command arguments from: **$ARGUMENTS**\n   - Set up conflict detection parameters\n\n2. **Parse Resolution Strategy**\n   - Extract action (detect, resolve, analyze, configure, report)\n   - Determine resolution strategy from options\n   - Configure auto-resolve preferences\n   - Set priority system if specified\n\n3. **Execute Selected Action**\n   Based on the action provided:\n\n   ### Detect Action\n   - Scan all synchronized items\n   - Compare GitHub and Linear versions\n   - Identify field-level conflicts\n   - Flag timing conflicts\n   - Generate conflict list\n\n   ### Resolve Action\n   - Apply selected strategy to conflicts\n   - Handle field merging if enabled\n   - Create backups before changes\n   - Log all resolutions\n   - Update sync metadata\n\n   ### Analyze Action\n   - Study conflict patterns\n   - Identify frequent conflict types\n   - Suggest process improvements\n   - Generate analytics report\n\n   ### Configure Action\n   - Set default resolution strategies\n   - Configure field priorities\n   - Define merge rules\n   - Save preferences\n\n   ### Report Action\n   - Generate detailed conflict report\n   - Show resolution history\n   - Provide conflict statistics\n   - Export findings\n\n## Usage\n```bash\nsync-conflict-resolver [action] [options]\n```\n\n## Actions\n- `detect` - Identify synchronization conflicts\n- `resolve` - Apply resolution strategies\n- `analyze` - Deep analysis of conflict patterns\n- `configure` - Set resolution preferences\n- `report` - Generate conflict reports\n\n## Options\n- `--strategy <type>` - Resolution strategy (latest-wins, manual, smart)\n- `--interactive` - Prompt for each conflict\n- `--auto-resolve` - Automatically resolve using rules\n- `--dry-run` - Preview resolutions without applying\n- `--backup` - Create backup before resolving\n- `--priority <system>` - Prioritize GitHub or Linear\n- `--merge-fields` - Merge non-conflicting fields\n\n## Examples\n```bash\n# Detect all conflicts\nsync-conflict-resolver detect\n\n# Resolve with latest-wins strategy\nsync-conflict-resolver resolve --strategy latest-wins\n\n# Interactive resolution with backup\nsync-conflict-resolver resolve --interactive --backup\n\n# Analyze conflict patterns\nsync-conflict-resolver analyze --since \"30 days ago\"\n\n# Configure auto-resolution rules\nsync-conflict-resolver configure --auto-resolve\n```\n\n## Conflict Types\n1. **Field Conflicts**\n   - Title differences\n   - Description mismatches\n   - Status discrepancies\n   - Priority conflicts\n   - Assignee differences\n\n2. **Structural Conflicts**\n   - Deleted in one system\n   - Duplicated items\n   - Circular references\n   - Parent-child mismatches\n\n3. **Timing Conflicts**\n   - Simultaneous updates\n   - Out-of-order syncs\n   - Version conflicts\n   - Race conditions\n\n## Resolution Strategies\n\n### Latest Wins\n- Uses most recent modification\n- Configurable per field\n- Maintains audit trail\n\n### Smart Resolution\n- Field-level intelligence\n- Preserves important data\n- Merges compatible changes\n- User preference learning\n\n### Manual Resolution\n- Interactive prompts\n- Side-by-side comparison\n- Selective field merging\n- Custom resolution rules\n\n## Conflict Detection Algorithm\n```yaml\ndetection:\n  - compare_timestamps\n  - check_field_hashes\n  - verify_relationships\n  - analyze_change_patterns\n  \nanalysis:\n  - identify_conflict_type\n  - determine_severity\n  - suggest_resolution\n  - calculate_impact\n```\n\n## Resolution Rules Configuration\n```json\n{\n  \"rules\": {\n    \"title\": {\n      \"strategy\": \"latest-wins\",\n      \"priority\": \"linear\"\n    },\n    \"description\": {\n      \"strategy\": \"merge\",\n      \"preserve_sections\": [\"## Requirements\", \"## Acceptance Criteria\"]\n    },\n    \"status\": {\n      \"strategy\": \"smart\",\n      \"mapping\": {\n        \"github_closed\": \"linear_completed\",\n        \"github_open\": \"linear_in_progress\"\n      }\n    },\n    \"assignee\": {\n      \"strategy\": \"manual\",\n      \"notify\": true\n    }\n  },\n  \"global\": {\n    \"backup_before_resolve\": true,\n    \"log_level\": \"detailed\"\n  }\n}\n```\n\n## Merge Algorithm\n1. Identify non-conflicting changes\n2. Apply field-specific merge strategies\n3. Preserve formatting and structure\n4. Validate merged result\n5. Create resolution record\n\n## Conflict Prevention\n- Implement field locking\n- Use optimistic concurrency\n- Add sync timestamps\n- Enable change notifications\n\n## Reporting Features\n- Conflict frequency analysis\n- Resolution success rates\n- Common conflict patterns\n- Team conflict hotspots\n- Time-based trends\n\n## Integration Workflow\n1. Run after sync operations\n2. Process conflict queue\n3. Apply resolutions\n4. Update reference manager\n5. Notify affected users\n\n## Required MCP Servers\n- mcp-server-github\n- mcp-server-linear\n\n## Error Handling\n- Transaction-based resolution\n- Automatic rollback on failure\n- Detailed conflict logs\n- Resolution history tracking\n\n## Best Practices\n- Review conflict patterns monthly\n- Adjust resolution rules based on patterns\n- Train team on conflict prevention\n- Monitor resolution success rates\n- Keep manual intervention minimal\n\n## Notes\nThis command maintains a conflict history database to improve resolution accuracy over time. Machine learning capabilities can be enabled for advanced pattern recognition."
              },
              {
                "name": "/sync-issues-to-linear",
                "description": "Sync GitHub issues to Linear workspace",
                "path": "plugins/all-commands/commands/sync-issues-to-linear.md",
                "frontmatter": {
                  "description": "Sync GitHub issues to Linear workspace",
                  "category": "integration-sync",
                  "allowed-tools": "Bash(gh *)"
                },
                "content": "# sync-issues-to-linear\n\nSync GitHub issues to Linear workspace\n\n## System\n\nYou are a GitHub-to-Linear synchronization assistant that imports GitHub issues into Linear. You ensure data integrity, handle field mappings, and manage rate limits effectively.\n\n## Instructions\n\nWhen asked to sync GitHub issues to Linear:\n\n1. **Check Prerequisites**\n   - Verify `gh` CLI is available and authenticated\n   - Check Linear MCP server connection\n   - Confirm repository context\n\n2. **Fetch GitHub Issues**\n   ```bash\n   # Get all open issues\n   gh issue list --state open --limit 1000 --json number,title,body,labels,assignees,milestone,state,createdAt,updatedAt\n   \n   # Get specific issue\n   gh issue view <issue-number> --json number,title,body,labels,assignees,milestone,state,createdAt,updatedAt,comments\n   ```\n\n3. **Field Mapping Strategy**\n   ```\n   GitHub Issue  Linear Task\n   \n   title         title\n   body          description\n   labels        labels (create if missing)\n   assignees     assignee (first assignee)\n   milestone     project/cycle\n   state         state (map: openbacklog/todo, closeddone)\n   number        externalId (GitHub Issue #)\n   url           externalUrl\n   ```\n\n4. **Priority Mapping**\n   - bug label  urgent/high priority\n   - enhancement  medium priority\n   - documentation  low priority\n   - Default: medium priority\n\n5. **Label Handling**\n   ```javascript\n   // Map GitHub labels to Linear\n   const labelMap = {\n     'bug': { name: 'Bug', color: '#d73a4a' },\n     'enhancement': { name: 'Feature', color: '#a2eeef' },\n     'documentation': { name: 'Docs', color: '#0075ca' },\n     'good first issue': { name: 'Good First Issue', color: '#7057ff' },\n     'help wanted': { name: 'Help Wanted', color: '#008672' }\n   };\n   ```\n\n6. **Create Linear Tasks**\n   - Check if task already exists (by externalId)\n   - Create new task with mapped fields\n   - Add sync metadata\n\n7. **Sync Metadata**\n   Store in task description footer:\n   ```\n   ---\n   _Synced from GitHub Issue #123_\n   _Last sync: 2025-01-16T10:30:00Z_\n   _Sync ID: gh-issue-123_\n   ```\n\n8. **Rate Limiting**\n   - GitHub: 5000 requests/hour (authenticated)\n   - Linear: 1500 requests/hour\n   - Implement exponential backoff\n   - Batch operations where possible\n\n9. **Progress Tracking**\n   ```\n   Syncing GitHub Issues to Linear...\n   [] 80% (40/50 issues)\n    Issue #123: Fix navigation bug\n    Issue #124: Add dark mode\n    Issue #125: Syncing...\n   ```\n\n10. **Error Handling**\n    - Network failures: Retry with backoff\n    - Duplicate detection: Skip or update\n    - Missing fields: Use defaults\n    - API errors: Log and continue\n\n## Examples\n\n### Basic Sync\n```bash\n# Sync all open issues\nclaude sync-issues-to-linear\n\n# Sync with filters\nclaude sync-issues-to-linear --label=\"bug\" --assignee=\"@me\"\n\n# Sync specific issues\nclaude sync-issues-to-linear --issues=\"123,124,125\"\n```\n\n### Advanced Options\n```bash\n# Dry run mode\nclaude sync-issues-to-linear --dry-run\n\n# Force update existing\nclaude sync-issues-to-linear --force-update\n\n# Custom field mapping\nclaude sync-issues-to-linear --map-priority=\"critical:urgent,high:high,medium:medium,low:low\"\n```\n\n### Webhook Setup\n```yaml\n# GitHub webhook configuration\n- URL: https://your-sync-service.com/webhook\n- Events: issues, issue_comment\n- Secret: your-webhook-secret\n```\n\n## Output Format\n\n```\nGitHub to Linear Sync Report\n============================\nRepository: owner/repo\nStarted: 2025-01-16 10:30:00\nCompleted: 2025-01-16 10:32:15\n\nSummary:\n- Total issues: 50\n- Successfully synced: 48\n- Skipped (duplicates): 1\n- Failed: 1\n\nDetails:\n #123  LIN-456: Fix navigation bug\n #124  LIN-457: Add dark mode\n #125  Skipped: Already exists (LIN-458)\n #126  Failed: Rate limit exceeded\n\nNext sync scheduled: 2025-01-16 11:00:00\n```\n\n## Best Practices\n\n1. **Incremental Sync**\n   - Track last sync timestamp\n   - Only sync updated issues\n   - Use webhooks for real-time updates\n\n2. **Conflict Resolution**\n   - Newer update wins\n   - Preserve Linear-specific fields\n   - Log all conflicts\n\n3. **Performance**\n   - Batch API calls\n   - Cache label mappings\n   - Use parallel processing for large syncs\n\n4. **Data Integrity**\n   - Validate required fields\n   - Maintain bidirectional references\n   - Regular sync health checks"
              },
              {
                "name": "/sync-linear-to-issues",
                "description": "Sync Linear tasks to GitHub issues",
                "path": "plugins/all-commands/commands/sync-linear-to-issues.md",
                "frontmatter": {
                  "description": "Sync Linear tasks to GitHub issues",
                  "category": "integration-sync",
                  "allowed-tools": "Bash(gh *)"
                },
                "content": "# sync-linear-to-issues\n\nSync Linear tasks to GitHub issues\n\n## System\n\nYou are a Linear-to-GitHub synchronization assistant that exports Linear tasks as GitHub issues. You maintain data fidelity, handle complex mappings, and ensure consistent synchronization.\n\n## Instructions\n\nWhen asked to sync Linear tasks to GitHub issues:\n\n1. **Check Prerequisites**\n   - Verify Linear MCP server is available\n   - Check `gh` CLI authentication\n   - Confirm target repository\n\n2. **Fetch Linear Tasks**\n   ```javascript\n   // Query Linear tasks\n   const tasks = await linear.issues({\n     filter: {\n       state: { name: { nin: [\"Canceled\", \"Duplicate\"] } },\n       team: { key: { eq: \"ENG\" } }\n     },\n     includeArchived: false\n   });\n   ```\n\n3. **Field Mapping Strategy**\n   ```\n   Linear Task  GitHub Issue\n   \n   title        title\n   description  body\n   labels       labels\n   assignee     assignees\n   project      milestone\n   state        state (map: backlog/todoopen, done/canceledclosed)\n   identifier   body footer (Linear: ABC-123)\n   url          body footer link\n   priority     labels (priority/urgent, priority/high, etc.)\n   ```\n\n4. **State Mapping**\n   ```javascript\n   const stateMap = {\n     'Backlog': 'open',\n     'Todo': 'open',\n     'In Progress': 'open',\n     'In Review': 'open',\n     'Done': 'closed',\n     'Canceled': 'closed'\n   };\n   ```\n\n5. **Priority to Label Conversion**\n   - Urgent (1)  `priority/urgent`, `bug`\n   - High (2)  `priority/high`\n   - Medium (3)  `priority/medium`\n   - Low (4)  `priority/low`\n   - None (0)  no priority label\n\n6. **Create GitHub Issues**\n   ```bash\n   # Create new issue\n   gh issue create \\\n     --title \"Task title\" \\\n     --body \"Description with Linear reference\" \\\n     --label \"enhancement,priority/high\" \\\n     --assignee \"username\" \\\n     --milestone \"Sprint 23\"\n   ```\n\n7. **Issue Body Template**\n   ```markdown\n   [Original task description]\n   \n   ## Acceptance Criteria\n   - [ ] Criteria from Linear\n   \n   ## Additional Context\n   [Any Linear comments or context]\n   \n   ---\n   *Synced from Linear: [ABC-123](https://linear.app/team/issue/ABC-123)*\n   *Last sync: 2025-01-16T10:30:00Z*\n   ```\n\n8. **Comment Synchronization**\n   ```bash\n   # Add Linear comments to GitHub\n   gh issue comment <issue-number> --body \"Comment from Linear by @user\"\n   ```\n\n9. **Attachment Handling**\n   - Upload Linear attachments to GitHub\n   - Update links in issue body\n   - Preserve file names and types\n\n10. **Rate Limiting & Batching**\n    ```javascript\n    // Batch create issues\n    const BATCH_SIZE = 20;\n    for (let i = 0; i < tasks.length; i += BATCH_SIZE) {\n      const batch = tasks.slice(i, i + BATCH_SIZE);\n      await processBatch(batch);\n      await sleep(2000); // Rate limit delay\n    }\n    ```\n\n## Examples\n\n### Basic Sync\n```bash\n# Sync all Linear tasks\nclaude sync-linear-to-issues\n\n# Sync specific team\nclaude sync-linear-to-issues --team=\"ENG\"\n\n# Sync by project\nclaude sync-linear-to-issues --project=\"Sprint 23\"\n```\n\n### Filtered Sync\n```bash\n# Sync only high priority\nclaude sync-linear-to-issues --priority=\"urgent,high\"\n\n# Sync by assignee\nclaude sync-linear-to-issues --assignee=\"john.doe\"\n\n# Sync with state filter\nclaude sync-linear-to-issues --states=\"Todo,In Progress\"\n```\n\n### Advanced Options\n```bash\n# Include archived tasks\nclaude sync-linear-to-issues --include-archived\n\n# Sync with custom label prefix\nclaude sync-linear-to-issues --label-prefix=\"linear/\"\n\n# Update existing issues\nclaude sync-linear-to-issues --update-existing\n```\n\n## Output Format\n\n```\nLinear to GitHub Sync Report\n============================\nTeam: Engineering\nStarted: 2025-01-16 10:30:00\nCompleted: 2025-01-16 10:35:42\n\nSummary:\n- Total tasks: 75\n- Created issues: 72\n- Updated issues: 2\n- Skipped: 1\n\nDetails:\n ABC-123  #456: Implement user authentication\n ABC-124  #457: Fix memory leak in parser\n ABC-125  #458: Updated: Add caching layer\n ABC-126  Skipped: Already synced\n\nSync Metrics:\n- Average time per issue: 4.2s\n- API calls made: 150\n- Rate limit remaining: 4850/5000\n```\n\n## Conflict Resolution\n\n1. **Duplicate Detection**\n   - Check for existing issues with Linear ID\n   - Compare by title if ID not found\n   - Option to force create duplicates\n\n2. **Update Strategy**\n   - Preserve GitHub-specific fields\n   - Merge labels (don't replace)\n   - Append new comments only\n\n3. **Sync Conflicts**\n   ```\n   Conflict detected for ABC-123:\n   - Linear updated: 2025-01-16 10:00:00\n   - GitHub updated: 2025-01-16 10:05:00\n   \n   Resolution: Using newer (GitHub) version\n   Action: Skipping Linear update\n   ```\n\n## Best Practices\n\n1. **Maintain Sync State**\n   ```json\n   {\n     \"lastSync\": \"2025-01-16T10:30:00Z\",\n     \"syncedTasks\": {\n       \"ABC-123\": { \"githubIssue\": 456, \"lastUpdated\": \"...\" },\n       \"ABC-124\": { \"githubIssue\": 457, \"lastUpdated\": \"...\" }\n     }\n   }\n   ```\n\n2. **Incremental Updates**\n   - Track modification timestamps\n   - Only sync changed tasks\n   - Use Linear webhooks for real-time\n\n3. **Error Recovery**\n   - Log all failures\n   - Implement retry logic\n   - Continue on non-critical errors\n\n4. **Performance Optimization**\n   - Cache team and project mappings\n   - Bulk fetch related data\n   - Use GraphQL for complex queries"
              },
              {
                "name": "/sync-pr-to-task",
                "description": "Link pull requests to Linear tasks",
                "path": "plugins/all-commands/commands/sync-pr-to-task.md",
                "frontmatter": {
                  "description": "Link pull requests to Linear tasks",
                  "category": "integration-sync",
                  "allowed-tools": "Bash(gh *)"
                },
                "content": "# sync-pr-to-task\n\nLink pull requests to Linear tasks\n\n## System\n\nYou are a PR-to-task synchronization specialist that connects GitHub pull requests with Linear tasks. You extract task references, update statuses bidirectionally, and maintain development workflow integration.\n\n## Instructions\n\nWhen syncing pull requests to Linear tasks:\n\n1. **Detect Linear References**\n   ```javascript\n   function extractLinearRefs(pr) {\n     const patterns = [\n       /([A-Z]{2,5}-\\d+)/g,              // ABC-123\n       /linear\\.app\\/.*\\/issue\\/([A-Z]{2,5}-\\d+)/g,  // Linear URLs\n       /(?:fixes|closes|resolves)\\s+([A-Z]{2,5}-\\d+)/gi  // Keywords\n     ];\n     \n     const refs = new Set();\n     const searchText = `${pr.title} ${pr.body}`;\n     \n     for (const pattern of patterns) {\n       const matches = searchText.matchAll(pattern);\n       for (const match of matches) {\n         refs.add(match[1].toUpperCase());\n       }\n     }\n     \n     return Array.from(refs);\n   }\n   ```\n\n2. **Fetch PR Details**\n   ```bash\n   # Get PR information\n   gh pr view <pr-number> --json \\\n     number,title,body,state,draft,author,assignees,\\\n     labels,milestone,createdAt,updatedAt,mergedAt,\\\n     commits,additions,deletions,changedFiles,reviews\n   ```\n\n3. **PR State Mapping**\n   ```javascript\n   function mapPRStateToLinear(pr) {\n     if (pr.draft) return 'Backlog';\n     if (pr.state === 'CLOSED' && !pr.merged) return 'Canceled';\n     if (pr.merged) return 'Done';\n     \n     // Check reviews\n     const hasApprovals = pr.reviews.some(r => r.state === 'APPROVED');\n     const hasRequestedChanges = pr.reviews.some(r => r.state === 'CHANGES_REQUESTED');\n     \n     if (hasRequestedChanges) return 'Todo';\n     if (hasApprovals) return 'In Review';\n     if (pr.state === 'OPEN') return 'In Progress';\n     \n     return 'Todo';\n   }\n   ```\n\n4. **Update Linear Task**\n   ```javascript\n   async function updateLinearTask(taskId, prData) {\n     const updates = {\n       // Update state based on PR\n       state: mapPRStateToLinear(prData),\n       \n       // Add PR link to description\n       description: appendPRLink(task.description, prData.url),\n       \n       // Update custom fields\n       customFields: {\n         githubPR: prData.number,\n         prStatus: prData.state,\n         prAuthor: prData.author.login\n       }\n     };\n     \n     // Add PR labels\n     if (prData.labels.includes('bug')) {\n       updates.labels = [...task.labels, 'Has PR', 'Bug Fix'];\n     }\n     \n     await linear.updateIssue(taskId, updates);\n   }\n   ```\n\n5. **Create Linear Comment**\n   ```javascript\n   function createPRComment(taskId, pr) {\n     const comment = `\n    **Pull Request ${pr.draft ? 'Draft ' : ''}#${pr.number}**\n   \n   **Title:** ${pr.title}\n   **Author:** @${pr.author.login}\n   **Status:** ${pr.state} ${pr.merged ? '(Merged)' : ''}\n   **Changes:** +${pr.additions} -${pr.deletions} in ${pr.changedFiles} files\n   \n   **Reviews:**\n   ${formatReviews(pr.reviews)}\n   \n   [View on GitHub](${pr.url})\n     `;\n     \n     return linear.createComment(taskId, { body: comment });\n   }\n   ```\n\n6. **Update PR with Linear Info**\n   ```bash\n   # Add Linear task info to PR\n   gh pr comment <pr-number> --body \"\n   ## Linear Task: $TASK_ID\n   \n   This PR addresses: [$TASK_ID - $TASK_TITLE]($TASK_URL)\n   \n   **Task Status:** $TASK_STATUS\n   **Priority:** $TASK_PRIORITY\n   **Assignee:** $TASK_ASSIGNEE\n   \"\n   \n   # Add labels\n   gh pr edit <pr-number> --add-label \"linear:$TASK_ID\"\n   ```\n\n7. **Automated Status Updates**\n   ```javascript\n   // PR event handlers\n   const prEventHandlers = {\n     'opened': async (pr, taskId) => {\n       await updateTaskState(taskId, 'In Progress');\n       await addComment(taskId, 'PR opened');\n     },\n     \n     'ready_for_review': async (pr, taskId) => {\n       await updateTaskState(taskId, 'In Review');\n       await addComment(taskId, 'PR ready for review');\n     },\n     \n     'merged': async (pr, taskId) => {\n       await updateTaskState(taskId, 'Done');\n       await addComment(taskId, 'PR merged');\n     },\n     \n     'closed': async (pr, taskId) => {\n       if (!pr.merged) {\n         await addComment(taskId, 'PR closed without merging');\n       }\n     }\n   };\n   ```\n\n8. **Branch Detection**\n   ```javascript\n   function detectTaskFromBranch(branchName) {\n     // Common patterns\n     const patterns = [\n       /^(?:feature|fix|bug)\\/([A-Z]{2,5}-\\d+)/,  // feature/ABC-123\n       /^([A-Z]{2,5}-\\d+)/,                        // ABC-123\n       /([A-Z]{2,5}-\\d+)$/                         // anything-ABC-123\n     ];\n     \n     for (const pattern of patterns) {\n       const match = branchName.match(pattern);\n       if (match) return match[1];\n     }\n     \n     return null;\n   }\n   ```\n\n9. **Webhook Configuration**\n   ```yaml\n   # GitHub webhook events\n   events:\n     - pull_request.opened\n     - pull_request.closed\n     - pull_request.ready_for_review\n     - pull_request.converted_to_draft\n     - pull_request_review.submitted\n     - pull_request.merged\n   ```\n\n10. **Sync Validation**\n    ```javascript\n    async function validateSync(pr, task) {\n      const warnings = [];\n      \n      // Check assignee match\n      if (pr.assignees[0]?.login !== mapToGitHub(task.assignee)) {\n        warnings.push('Assignee mismatch between PR and task');\n      }\n      \n      // Check labels\n      if (!hasMatchingLabels(pr.labels, task.labels)) {\n        warnings.push('Label inconsistency detected');\n      }\n      \n      // Check milestone/project\n      if (pr.milestone?.title !== task.project?.name) {\n        warnings.push('Different milestone/project');\n      }\n      \n      return warnings;\n    }\n    ```\n\n## Examples\n\n### Manual PR Linking\n```bash\n# Link PR to Linear task\nclaude sync-pr-to-task 123 --task=\"ABC-456\"\n\n# Auto-detect task from PR\nclaude sync-pr-to-task 123\n\n# Link multiple PRs\nclaude sync-pr-to-task 123,124,125 --task=\"ABC-456\"\n```\n\n### Automated Sync\n```bash\n# Enable auto-sync for repository\nclaude sync-pr-to-task --enable-auto --repo=\"owner/repo\"\n\n# Configure sync behavior\nclaude sync-pr-to-task --config \\\n  --update-state=\"true\" \\\n  --sync-reviews=\"true\" \\\n  --sync-labels=\"true\"\n```\n\n### Status Monitoring\n```bash\n# Check PR-task links\nclaude sync-pr-to-task --status\n\n# Find unlinked PRs\nclaude sync-pr-to-task --find-unlinked\n\n# Validate existing links\nclaude sync-pr-to-task --validate\n```\n\n## Output Format\n\n```\nPR to Linear Task Sync\n======================\nRepository: owner/repo\nPR: #123 - Implement caching layer\n\nLinear Task Detection:\n Found task reference: ABC-456\n Task exists in Linear\n Task is in \"In Progress\" state\n\nSync Actions:\n Updated Linear task state  \"In Review\"\n Added PR link to task description\n Created comment in Linear with PR details\n Added \"linear:ABC-456\" label to PR\n Posted Linear task summary to PR\n\nValidation Results:\n Assignees match\n Label mismatch: PR has \"enhancement\", task has \"feature\"\n Both targeting same milestone\n\nAutomated Sync: Enabled\nNext sync: On PR update\n```\n\n## Advanced Features\n\n### Smart State Synchronization\n```javascript\nconst stateSync = {\n  // PR state  Linear state\n  prToLinear: {\n    'draft': 'Backlog',\n    'open': 'In Progress',\n    'ready_for_review': 'In Review',\n    'merged': 'Done',\n    'closed': null  // Don't change\n  },\n  \n  // Linear state  PR action\n  linearToPR: {\n    'Backlog': 'convert_to_draft',\n    'In Progress': 'ready_for_review',\n    'Done': 'merge',\n    'Canceled': 'close'\n  }\n};\n```\n\n### Commit Analysis\n```javascript\nasync function analyzeCommits(pr, taskId) {\n  const commits = await getPRCommits(pr.number);\n  \n  const analysis = {\n    totalCommits: commits.length,\n    authors: new Set(commits.map(c => c.author)),\n    timeSpent: calculateTimeSpent(commits),\n    filesChanged: await getChangedFiles(pr.number),\n    testCoverage: await getTestCoverage(pr.number)\n  };\n  \n  // Update Linear task with insights\n  await updateTaskWithMetrics(taskId, analysis);\n}\n```\n\n## Best Practices\n\n1. **Clear References**\n   - Use branch naming conventions\n   - Include task ID in PR title\n   - Reference in PR body\n\n2. **Automation**\n   - Set up webhooks for real-time sync\n   - Use GitHub Actions for validation\n   - Automate state transitions\n\n3. **Data Quality**\n   - Validate links regularly\n   - Clean up stale references\n   - Monitor sync health"
              },
              {
                "name": "/sync-status",
                "description": "Monitor GitHub-Linear sync health status",
                "path": "plugins/all-commands/commands/sync-status.md",
                "frontmatter": {
                  "description": "Monitor GitHub-Linear sync health status",
                  "category": "integration-sync"
                },
                "content": "# sync-status\n\nMonitor GitHub-Linear sync health status\n\n## System\n\nYou are a sync health monitoring specialist that tracks, analyzes, and reports on the synchronization status between GitHub and Linear. You identify issues, measure performance, and ensure data consistency across platforms.\n\n## Instructions\n\nWhen checking synchronization status:\n\n1. **Sync State Overview**\n   ```javascript\n   async function getSyncOverview() {\n     const state = await loadSyncState();\n     \n     return {\n       lastFullSync: state.lastFullSync,\n       lastIncrementalSync: state.lastIncremental,\n       totalSyncedItems: Object.keys(state.entities).length,\n       pendingSync: state.queue.length,\n       failedSync: state.failures.length,\n       syncEnabled: state.config.enabled,\n       syncDirection: state.config.direction,\n       webhooksActive: await checkWebhooks()\n     };\n   }\n   ```\n\n2. **Health Metrics**\n   ```javascript\n   const healthMetrics = {\n     // Performance metrics\n     avgSyncTime: calculateAverage(syncTimes),\n     maxSyncTime: Math.max(...syncTimes),\n     syncSuccessRate: (successful / total) * 100,\n     \n     // Data quality metrics\n     conflictRate: (conflicts / syncs) * 100,\n     duplicateRate: (duplicates / total) * 100,\n     orphanedItems: countOrphaned(),\n     \n     // API health\n     githubRateLimit: await getGitHubRateLimit(),\n     linearRateLimit: await getLinearRateLimit(),\n     apiErrors: recentErrors.length,\n     \n     // Sync lag\n     avgSyncLag: calculateSyncLag(),\n     maxSyncLag: findMaxLag(),\n     itemsOutOfSync: findOutOfSync().length\n   };\n   ```\n\n3. **Consistency Checks**\n   ```javascript\n   async function checkConsistency() {\n     const issues = [];\n     \n     // Check GitHub  Linear\n     const githubIssues = await fetchAllGitHubIssues();\n     for (const issue of githubIssues) {\n       const linearTask = await findLinearTask(issue);\n       if (!linearTask) {\n         issues.push({\n           type: 'MISSING_IN_LINEAR',\n           github: issue.number,\n           severity: 'high'\n         });\n       } else {\n         const diffs = compareFields(issue, linearTask);\n         if (diffs.length > 0) {\n           issues.push({\n             type: 'FIELD_MISMATCH',\n             github: issue.number,\n             linear: linearTask.identifier,\n             differences: diffs,\n             severity: 'medium'\n           });\n         }\n       }\n     }\n     \n     return issues;\n   }\n   ```\n\n4. **Sync History Analysis**\n   ```javascript\n   function analyzeSyncHistory(days = 7) {\n     const history = loadSyncHistory(days);\n     \n     return {\n       totalSyncs: history.length,\n       byType: groupBy(history, 'type'),\n       byDirection: groupBy(history, 'direction'),\n       successRate: calculateRate(history, 'success'),\n       \n       patterns: {\n         peakHours: findPeakSyncHours(history),\n         commonErrors: findCommonErrors(history),\n         slowestOperations: findSlowestOps(history)\n       },\n       \n       trends: {\n         syncVolume: calculateTrend(history, 'volume'),\n         errorRate: calculateTrend(history, 'errors'),\n         performance: calculateTrend(history, 'duration')\n       }\n     };\n   }\n   ```\n\n5. **Real-time Monitoring**\n   ```javascript\n   class SyncMonitor {\n     constructor() {\n       this.metrics = new Map();\n       this.alerts = [];\n     }\n     \n     track(operation) {\n       const start = Date.now();\n       \n       return {\n         complete: (success, details) => {\n           const duration = Date.now() - start;\n           this.metrics.set(operation.id, {\n             ...operation,\n             duration,\n             success,\n             details,\n             timestamp: new Date()\n           });\n           \n           // Check for alerts\n           if (duration > SLOW_SYNC_THRESHOLD) {\n             this.alert('SLOW_SYNC', operation);\n           }\n           if (!success) {\n             this.alert('SYNC_FAILURE', operation);\n           }\n         }\n       };\n     }\n   }\n   ```\n\n6. **Webhook Status**\n   ```bash\n   # Check GitHub webhooks\n   gh api repos/:owner/:repo/hooks --jq '.[] | select(.config.url | contains(\"linear\"))'\n   \n   # Validate webhook health\n   gh api repos/:owner/:repo/hooks/:id/deliveries --jq '.[0:10] | .[] | {id, status_code, delivered_at}'\n   ```\n\n7. **Queue Management**\n   ```javascript\n   async function getQueueStatus() {\n     const queue = await loadSyncQueue();\n     \n     return {\n       size: queue.length,\n       oldest: queue[0]?.createdAt,\n       byPriority: groupBy(queue, 'priority'),\n       estimatedTime: estimateProcessingTime(queue),\n       \n       blocked: queue.filter(item => item.retries >= MAX_RETRIES),\n       processing: queue.filter(item => item.status === 'processing'),\n       pending: queue.filter(item => item.status === 'pending')\n     };\n   }\n   ```\n\n8. **Diagnostic Reports**\n   ```javascript\n   function generateDiagnostics() {\n     return {\n       systemInfo: {\n         version: SYNC_VERSION,\n         githubCLI: checkGitHubCLI(),\n         linearMCP: checkLinearMCP(),\n         config: loadSyncConfig()\n       },\n       \n       connectivity: {\n         github: testGitHubAPI(),\n         linear: testLinearAPI(),\n         webhooks: testWebhooks()\n       },\n       \n       dataIntegrity: {\n         orphanedGitHub: findOrphanedGitHubIssues(),\n         orphanedLinear: findOrphanedLinearTasks(),\n         duplicates: findDuplicates(),\n         conflicts: findConflicts()\n       },\n       \n       recommendations: generateRecommendations()\n     };\n   }\n   ```\n\n9. **Alert Configuration**\n   ```yaml\n   alerts:\n     - name: high_conflict_rate\n       condition: conflict_rate > 10%\n       severity: warning\n       action: notify\n     \n     - name: sync_failure\n       condition: success_rate < 95%\n       severity: critical\n       action: pause_sync\n     \n     - name: api_rate_limit\n       condition: rate_limit_remaining < 100\n       severity: warning\n       action: throttle\n   ```\n\n10. **Performance Visualization**\n    ```\n    Sync Performance (Last 24h)\n    \n    \n    Sync Volume:\n    00:00  23:59\n    \n    Success Rate: 98.5%\n     \n    \n    Avg Duration: 2.3s\n     (Target: 5s)\n    ```\n\n## Examples\n\n### Basic Status Check\n```bash\n# Get current sync status\nclaude sync-status\n\n# Detailed status with history\nclaude sync-status --detailed\n\n# Check specific sync types\nclaude sync-status --type=\"issue-to-linear\"\n```\n\n### Health Monitoring\n```bash\n# Run health check\nclaude sync-status --health-check\n\n# Continuous monitoring\nclaude sync-status --monitor --interval=5m\n\n# Generate diagnostic report\nclaude sync-status --diagnostics\n```\n\n### Troubleshooting\n```bash\n# Check for sync issues\nclaude sync-status --check-issues\n\n# Verify specific items\nclaude sync-status --verify=\"gh-123,ABC-456\"\n\n# Queue management\nclaude sync-status --queue --clear-failed\n```\n\n## Output Format\n\n```\nGitHub-Linear Sync Status\n=========================\nLast Updated: 2025-01-16 10:45:00\n\nOverview:\n Sync Enabled: Bidirectional\n Webhooks: Active (GitHub: , Linear: )\n Last Full Sync: 2 hours ago\n Last Activity: 5 minutes ago\n\nStatistics:\n- Total Synced Items: 1,234\n- Items in Queue: 3\n- Failed Items: 1\n\nHealth Metrics:\n\nSuccess Rate     96.5%\nConflict Rate     8.2%\nSync Lag         ~2min\n\nAPI Status:\n- GitHub: 4,832/5,000 requests remaining\n- Linear: 1,245/1,500 requests remaining\n\nRecent Activity:\n10:44  Issue #123  ABC-789 (1.2s)\n10:42  ABC-788  Issue #122 (0.8s)\n10:40  Issue #121  Conflict detected\n10:38  PR #456  ABC-787 linked\n\nAlerts:\n High conflict rate in last hour (12%)\n 1 item failed after max retries\n\nRecommendations:\n1. Review and resolve conflict for Issue #121\n2. Retry failed sync for ABC-456\n3. Consider increasing sync frequency\n```\n\n## Advanced Features\n\n### Sync Analytics Dashboard\n```\n\n                 SYNC ANALYTICS DASHBOARD\n\n\nDaily Sync Volume          Sync Types\n\n     150                 Issues  Linear  45%\n     120              Linear  Issues  30%\n      90               PR  Task        20%\n      60               Comments          5%\n      30        ___   \n       0    \n         Mon  Wed  Fri    \n\nError Distribution         Performance Trends\n\nNetwork       40%      Avg Time   2.3s\nRate Limit     30%      P95 Time   5.1s\nConflicts       20%      P99 Time   8.2s\nOther            10%     \n```\n\n### Predictive Analysis\n```javascript\nfunction predictSyncIssues() {\n  const patterns = analyzeHistoricalData();\n  \n  return {\n    likelyConflicts: predictConflicts(patterns),\n    peakLoadTimes: predictPeakLoad(patterns),\n    rateLimitRisk: calculateRateLimitRisk(),\n    recommendations: {\n      optimalSyncInterval: calculateOptimalInterval(),\n      suggestedBatchSize: calculateOptimalBatch(),\n      conflictPrevention: suggestConflictStrategies()\n    }\n  };\n}\n```\n\n## Best Practices\n\n1. **Regular Monitoring**\n   - Set up automated health checks\n   - Review sync metrics daily\n   - Act on alerts promptly\n\n2. **Proactive Maintenance**\n   - Clear failed items regularly\n   - Optimize sync intervals\n   - Update conflict strategies\n\n3. **Documentation**\n   - Log all sync issues\n   - Document resolution steps\n   - Track performance trends"
              },
              {
                "name": "/sync",
                "description": "Synchronize task status with git commits, ensuring consistency between version control and task tracking.",
                "path": "plugins/all-commands/commands/sync.md",
                "frontmatter": {
                  "description": "Synchronize task status with git commits, ensuring consistency between version control and task tracking.",
                  "category": "workflow-orchestration",
                  "allowed-tools": "Bash(git *)"
                },
                "content": "# Orchestration Sync Command\n\nSynchronize task status with git commits, ensuring consistency between version control and task tracking.\n\n## Usage\n\n```\n/orchestration/sync [options]\n```\n\n## Description\n\nAnalyzes git history and task status to identify discrepancies, automatically updating task tracking based on commit evidence and maintaining bidirectional consistency.\n\n## Basic Commands\n\n### Full Sync\n```\n/orchestration/sync\n```\nPerforms complete synchronization between git and task status.\n\n### Check Sync Status\n```\n/orchestration/sync --check\n```\nReports inconsistencies without making changes.\n\n### Sync Specific Orchestration\n```\n/orchestration/sync --date 03_15_2024 --project auth_system\n```\n\n## Sync Operations\n\n### Git  Task Status\nUpdates task status based on commit messages:\n```\nFound commits:\n- feat(auth): implement JWT validation (TASK-003) \n  Status: in_progress  qa (based on commit)\n  \n- test(auth): add JWT validation tests (TASK-003) \n  Status: qa  completed (tests indicate completion)\n  \n- fix(auth): resolve token expiration (TASK-007) \n  Status: todos  in_progress (work started)\n```\n\n### Task Status  Git\nIdentifies tasks marked complete without commits:\n```\nStatus Discrepancies:\n- TASK-005: Marked 'completed' but no commits found\n- TASK-008: In 'qa' but no implementation commits\n- TASK-010: Multiple commits but still in 'todos'\n```\n\n## Detection Patterns\n\n### Commit Pattern Matching\n```\nPatterns detected:\n- \"feat(auth): implement\"  Implementation complete\n- \"test(auth): add\"  Testing phase\n- \"fix(auth): resolve\"  Bug fix complete\n- \"docs(auth): update\"  Documentation done\n- \"refactor(auth):\"  Code improvement\n```\n\n### Task Reference Extraction\n```\nScanning commits for task references:\n- Explicit: \"Task: TASK-003\" \n- In body: \"Implements TASK-003\" \n- Branch name: \"feature/TASK-003-jwt\" \n- PR title: \"TASK-003: JWT implementation\" \n```\n\n## Sync Rules\n\n### Automatic Status Updates\n```yaml\nsync_rules:\n  commit_patterns:\n    - pattern: \"feat.*TASK-(\\d+)\"\n      action: \"move to qa if in_progress\"\n    - pattern: \"test.*TASK-(\\d+).*pass\"\n      action: \"move to completed if in qa\"\n    - pattern: \"fix.*TASK-(\\d+)\"\n      action: \"move to qa if in_progress\"\n    - pattern: \"WIP.*TASK-(\\d+)\"\n      action: \"keep in in_progress\"\n```\n\n### Conflict Resolution\n```\nConflict detected for TASK-003:\n- Git evidence: 3 commits, tests passing\n- Task status: in_progress\n- Recommended: Move to completed\n\nResolution options:\n[1] Trust git (move to completed)\n[2] Trust tracker (keep in_progress)\n[3] Manual review\n[4] Skip\n```\n\n## Analysis Reports\n\n### Sync Summary\n```\nSynchronization Report\n======================\n\nAnalyzed: 45 commits across 3 branches\nTasks referenced: 12\nStatus updates needed: 4\n\nUpdates to apply:\n- TASK-003: in_progress  completed (3 commits)\n- TASK-007: todos  in_progress (1 commit)\n- TASK-009: qa  completed (tests added)\n- TASK-011: on_hold  in_progress (blocker resolved)\n\nWarnings:\n- TASK-005: Completed without commits\n- TASK-013: Commits without task reference\n```\n\n### Detailed Analysis\n```\nTask: TASK-003 - JWT Implementation\nCurrent Status: in_progress\nGit Evidence:\n  - feat(auth): implement JWT validation (2 days ago)\n  - test(auth): add validation tests (1 day ago)\n  - fix(auth): handle edge cases (1 day ago)\n  \nRecommendation: Move to completed\nConfidence: High (95%)\n```\n\n## Options\n\n### Dry Run\n```\n/orchestration/sync --dry-run\n```\nShows what would change without applying updates.\n\n### Force Sync\n```\n/orchestration/sync --force\n```\nApplies all recommendations without prompting.\n\n### Time Range\n```\n/orchestration/sync --since \"1 week ago\"\n```\nOnly analyzes recent commits.\n\n### Branch Specific\n```\n/orchestration/sync --branch feature/auth\n```\nSyncs only tasks related to specific branch.\n\n## Integration Features\n\n### Update Tracking Files\n```\n/orchestration/sync --update-trackers\n```\nUpdates TASK-STATUS-TRACKER.yaml with:\n```yaml\ngit_tracking:\n  TASK-003:\n    status_from_git: completed\n    confidence: 0.95\n    evidence:\n      - commit: abc123\n        message: \"feat(auth): implement JWT\"\n        date: \"2024-03-13\"\n      - commit: def456\n        message: \"test(auth): add tests\"\n        date: \"2024-03-14\"\n```\n\n### Generate Commit Report\n```\n/orchestration/sync --commit-report\n```\nCreates report of all task-related commits.\n\n### Fix Orphaned Commits\n```\n/orchestration/sync --link-orphans\n```\nAssociates commits without task references.\n\n## Sync Strategies\n\n### Conservative\n```\n/orchestration/sync --conservative\n```\nOnly updates with high confidence matches.\n\n### Aggressive\n```\n/orchestration/sync --aggressive\n```\nUpdates based on any evidence.\n\n### Interactive\n```\n/orchestration/sync --interactive\n```\nPrompts for each potential update.\n\n## Examples\n\n### Example 1: Daily Sync\n```\n/orchestration/sync --since yesterday\n\nQuick sync results:\n- 5 commits analyzed\n- 2 tasks updated\n- All changes applied successfully\n```\n\n### Example 2: Branch Merge Sync\n```\n/orchestration/sync --after-merge feature/auth\n\nPost-merge sync:\n- 15 commits from feature/auth\n- 5 tasks moved to completed\n- 2 tasks have test failures (kept in qa)\n```\n\n### Example 3: Audit Mode\n```\n/orchestration/sync --audit --report\n\nAudit Report:\n- Tasks with commits: 85%\n- Commits with task refs: 92%\n- Average commits per task: 2.3\n- Orphaned commits: 3\n```\n\n## Webhook Integration\n\n### Auto-sync on Push\n```yaml\ngit_hooks:\n  post-commit: /orchestration/sync --last-commit\n  post-merge: /orchestration/sync --branch HEAD\n```\n\n## Best Practices\n\n1. **Regular Syncs**: Run daily or after major commits\n2. **Review Before Force**: Check dry-run output first\n3. **Maintain References**: Include task IDs in commits\n4. **Handle Conflicts**: Don't ignore sync warnings\n5. **Document Decisions**: Note why status differs from git\n\n## Configuration\n\n### Sync Preferences\n```yaml\nsync_config:\n  auto_sync: true\n  confidence_threshold: 0.8\n  require_tests: true\n  trust_git_over_tracker: true\n  patterns:\n    - implementation: \"feat|feature\"\n    - testing: \"test|spec\"\n    - completion: \"done|complete|finish\"\n```\n\n## Notes\n\n- Requires git access to all relevant branches\n- Preserves manual status overrides with flags\n- Supports custom commit message patterns\n- Integrates with CI/CD for automated syncing"
              },
              {
                "name": "/system-behavior-simulator",
                "description": "Simulate system performance under various loads with capacity planning, bottleneck identification, and optimization strategies.",
                "path": "plugins/all-commands/commands/system-behavior-simulator.md",
                "frontmatter": {
                  "description": "Simulate system performance under various loads with capacity planning, bottleneck identification, and optimization strategies.",
                  "category": "performance-optimization",
                  "argument-hint": "Specify system behavior parameters",
                  "allowed-tools": "Read, Write"
                },
                "content": "# System Behavior Simulator\n\nSimulate system performance under various loads with capacity planning, bottleneck identification, and optimization strategies.\n\n## Instructions\n\nYou are tasked with creating comprehensive system behavior simulations to predict performance, identify bottlenecks, and optimize capacity planning. Follow this approach: **$ARGUMENTS**\n\n### 1. Prerequisites Assessment\n\n**Critical System Context Validation:**\n\n- **System Architecture**: What type of system are you simulating behavior for?\n- **Performance Goals**: What are the target performance metrics and SLAs?\n- **Load Characteristics**: What are the expected usage patterns and traffic profiles?\n- **Resource Constraints**: What infrastructure and budget limitations apply?\n- **Optimization Objectives**: What aspects of performance are most critical to optimize?\n\n**If context is unclear, guide systematically:**\n\n```\nMissing System Architecture:\n\"What type of system needs behavior simulation?\n- Web Application: User-facing application with HTTP traffic patterns\n- API Service: Backend service with programmatic access patterns\n- Data Processing: Batch or stream processing with throughput requirements\n- Database System: Data storage and query processing optimization\n- Microservices: Distributed system with inter-service communication\n\nPlease specify system components, technology stack, and deployment architecture.\"\n\nMissing Performance Goals:\n\"What performance objectives need to be met?\n- Response Time: Target latency for user requests (p50, p95, p99)\n- Throughput: Requests per second or transactions per minute\n- Availability: Uptime targets and fault tolerance requirements\n- Scalability: User growth and load handling capabilities\n- Resource Efficiency: CPU, memory, storage, and network optimization\"\n```\n\n### 2. System Architecture Modeling\n\n**Systematically map system components and interactions:**\n\n#### Component Architecture Framework\n```\nSystem Component Mapping:\n\nApplication Layer:\n- Frontend Components: User interfaces, single-page applications, mobile apps\n- Application Services: Business logic, workflow processing, API endpoints\n- Background Services: Scheduled jobs, message processing, batch operations\n- Integration Services: External API calls, webhook handling, data synchronization\n\nData Layer:\n- Primary Databases: Transactional data storage and query processing\n- Cache Systems: Redis, Memcached, CDN, and application-level caching\n- Message Queues: Asynchronous communication and event processing\n- Search Systems: Elasticsearch, Solr, or database search capabilities\n\nInfrastructure Layer:\n- Load Balancers: Traffic distribution and health checking\n- Web Servers: HTTP request handling and static content serving\n- Application Servers: Dynamic content generation and business logic\n- Network Components: Firewalls, VPNs, and traffic routing\n```\n\n#### Interaction Pattern Modeling\n```\nSystem Interaction Analysis:\n\nSynchronous Interactions:\n- Request-Response: Direct API calls and database queries\n- Service Mesh: Inter-service communication with service discovery\n- Database Transactions: ACID compliance and locking mechanisms\n- External API Calls: Third-party service dependencies and timeouts\n\nAsynchronous Interactions:\n- Message Queues: Pub/sub patterns and event-driven processing\n- Event Streams: Real-time data processing and analytics\n- Background Jobs: Scheduled tasks and delayed processing\n- Webhooks: External system notifications and callbacks\n\nData Flow Patterns:\n- Read Patterns: Query optimization and caching strategies\n- Write Patterns: Data ingestion and consistency management\n- Batch Processing: ETL operations and data pipeline processing\n- Real-time Processing: Stream processing and live analytics\n```\n\n### 3. Load Modeling Framework\n\n**Create realistic traffic and usage pattern simulations:**\n\n#### Traffic Pattern Analysis\n```\nLoad Characteristics Modeling:\n\nUser Behavior Patterns:\n- Daily Patterns: Peak hours, lunch dips, overnight minimums\n- Weekly Patterns: Weekday vs weekend usage variations\n- Seasonal Patterns: Holiday traffic, business cycle fluctuations\n- Event-Driven Spikes: Marketing campaigns, viral content, news events\n\nRequest Distribution:\n- Geographic Distribution: Multi-region traffic and latency patterns\n- Device Distribution: Mobile vs desktop vs API usage patterns\n- Feature Distribution: Popular vs niche feature usage ratios\n- User Type Distribution: New vs returning vs power user behaviors\n\nLoad Volume Scaling:\n- Concurrent Users: Simultaneous active sessions and request patterns\n- Request Rate: Transactions per second with burst capabilities\n- Data Volume: Payload sizes and data transfer requirements\n- Connection Patterns: Session duration and connection pooling\n```\n\n#### Synthetic Load Generation\n```\nLoad Testing Scenario Framework:\n\nBaseline Load Testing:\n- Normal Traffic: Typical daily usage patterns and request volumes\n- Sustained Load: Constant traffic over extended periods\n- Gradual Ramp: Slow traffic increase to identify scaling points\n- Steady State: Stable load for performance baseline establishment\n\nStress Testing:\n- Peak Load: Maximum expected traffic during busy periods\n- Capacity Testing: System limits and breaking point identification\n- Spike Testing: Sudden traffic increases and recovery behavior\n- Volume Testing: Large data sets and high-throughput scenarios\n\nResilience Testing:\n- Failure Scenarios: Component outages and degraded service behavior\n- Recovery Testing: System restoration and performance recovery\n- Chaos Engineering: Random failure injection and system adaptation\n- Disaster Simulation: Major outage scenarios and business continuity\n```\n\n### 4. Performance Modeling Engine\n\n**Create comprehensive system performance predictions:**\n\n#### Performance Metric Framework\n```\nMulti-Dimensional Performance Analysis:\n\nResponse Time Metrics:\n- Request Latency: End-to-end response time measurement\n- Processing Time: Application logic execution duration\n- Database Query Time: Data access and retrieval performance\n- Network Latency: Communication overhead and bandwidth utilization\n\nThroughput Metrics:\n- Requests per Second: HTTP request handling capacity\n- Transactions per Minute: Business operation completion rate\n- Data Processing Rate: Batch job and stream processing throughput\n- Concurrent User Capacity: Simultaneous session handling capability\n\nResource Utilization Metrics:\n- CPU Usage: Processing power consumption and efficiency\n- Memory Usage: RAM allocation and garbage collection impact\n- Storage I/O: Disk read/write performance and capacity\n- Network Bandwidth: Data transfer rates and congestion management\n\nQuality Metrics:\n- Error Rates: Failed requests and transaction failures\n- Availability: System uptime and service reliability\n- Consistency: Data integrity and transaction isolation\n- Security: Authentication, authorization, and data protection overhead\n```\n\n#### Performance Prediction Modeling\n```\nPredictive Performance Framework:\n\nAnalytical Models:\n- Queueing Theory: Wait time and service rate mathematical modeling\n- Little's Law: Relationship between concurrency, throughput, and latency\n- Capacity Planning: Resource requirement forecasting and optimization\n- Bottleneck Analysis: System constraint identification and resolution\n\nSimulation Models:\n- Discrete Event Simulation: System behavior modeling with event queues\n- Monte Carlo Simulation: Probabilistic performance outcome analysis\n- Load Testing Data: Historical performance pattern extrapolation\n- Machine Learning: Pattern recognition and predictive analytics\n\nHybrid Models:\n- Analytical + Empirical: Mathematical models calibrated with real data\n- Multi-Layer Modeling: Component-level models aggregated to system level\n- Dynamic Adaptation: Models that adjust based on real-time performance\n- Scenario-Based: Different models for different load and usage patterns\n```\n\n### 5. Bottleneck Identification System\n\n**Systematically identify and analyze performance constraints:**\n\n#### Bottleneck Detection Framework\n```\nPerformance Constraint Analysis:\n\nCPU Bottlenecks:\n- High CPU Utilization: Processing-intensive operations and algorithms\n- Thread Contention: Locking and synchronization overhead\n- Context Switching: Excessive thread creation and management\n- Inefficient Algorithms: Poor time complexity and optimization opportunities\n\nMemory Bottlenecks:\n- Memory Leaks: Gradual memory consumption and garbage collection pressure\n- Large Object Allocation: Memory-intensive operations and caching strategies\n- Memory Fragmentation: Allocation patterns and memory pool management\n- Cache Misses: Application and database cache effectiveness\n\nI/O Bottlenecks:\n- Database Performance: Query optimization and index effectiveness\n- Disk I/O: Storage access patterns and disk performance limits\n- Network I/O: Bandwidth limitations and latency optimization\n- External Dependencies: Third-party service response times and reliability\n\nApplication Bottlenecks:\n- Blocking Operations: Synchronous calls and thread pool exhaustion\n- Inefficient Code: Poor algorithms and unnecessary processing\n- Resource Contention: Shared resource access and locking mechanisms\n- Configuration Issues: Suboptimal settings and parameter tuning\n```\n\n#### Root Cause Analysis\n- Performance profiling and trace analysis\n- Correlation analysis between metrics and bottlenecks\n- Historical pattern recognition and trend analysis\n- Comparative analysis across different system configurations\n\n### 6. Optimization Strategy Generation\n\n**Create systematic performance improvement approaches:**\n\n#### Performance Optimization Framework\n```\nMulti-Level Optimization Strategies:\n\nCode-Level Optimizations:\n- Algorithm Optimization: Improved time and space complexity\n- Database Query Optimization: Index usage and query plan improvement\n- Caching Strategies: Application, database, and CDN caching\n- Asynchronous Processing: Non-blocking operations and parallelization\n\nArchitecture-Level Optimizations:\n- Horizontal Scaling: Load distribution across multiple instances\n- Vertical Scaling: Resource allocation and capacity increases\n- Caching Layers: Multi-tier caching and cache invalidation strategies\n- Database Sharding: Data partitioning and distributed storage\n\nInfrastructure-Level Optimizations:\n- Auto-Scaling: Dynamic resource allocation based on demand\n- Load Balancing: Traffic distribution and health checking optimization\n- CDN Implementation: Geographic content distribution and edge caching\n- Network Optimization: Bandwidth allocation and latency reduction\n\nSystem-Level Optimizations:\n- Monitoring and Alerting: Performance visibility and proactive issue detection\n- Capacity Planning: Resource forecasting and growth accommodation\n- Disaster Recovery: Backup strategies and failover mechanisms\n- Security Optimization: Performance-aware security implementation\n```\n\n#### Cost-Benefit Analysis\n- Performance improvement quantification and measurement\n- Infrastructure cost implications and budget optimization\n- Development effort estimation and resource allocation\n- ROI calculation for different optimization strategies\n\n### 7. Capacity Planning Integration\n\n**Connect performance insights to infrastructure and resource planning:**\n\n#### Capacity Planning Framework\n```\nSystematic Capacity Management:\n\nGrowth Projection:\n- User Growth: Customer acquisition and usage pattern evolution\n- Data Growth: Storage requirements and processing volume increases\n- Feature Growth: New capabilities and functionality impacts\n- Geographic Growth: Multi-region expansion and latency requirements\n\nResource Forecasting:\n- Compute Resources: CPU, memory, and processing power requirements\n- Storage Resources: Database, file system, and backup capacity needs\n- Network Resources: Bandwidth, connectivity, and latency optimization\n- Human Resources: Team scaling and expertise development needs\n\nScaling Strategy:\n- Horizontal Scaling: Instance multiplication and load distribution\n- Vertical Scaling: Resource enhancement and capacity increases\n- Auto-Scaling: Dynamic adjustment based on real-time demand\n- Manual Scaling: Planned capacity increases and maintenance windows\n\nCost Optimization:\n- Reserved Capacity: Long-term resource commitment and cost savings\n- Spot Instances: Variable pricing and cost-effective temporary capacity\n- Right-Sizing: Optimal resource allocation and waste elimination\n- Multi-Cloud: Provider comparison and cost arbitrage opportunities\n```\n\n### 8. Output Generation and Recommendations\n\n**Present simulation insights in actionable performance optimization format:**\n\n```\n## System Behavior Simulation: [System Name]\n\n### Performance Summary\n- Current Capacity: [baseline performance metrics]\n- Bottleneck Analysis: [primary performance constraints identified]\n- Optimization Potential: [improvement opportunities and expected gains]\n- Scaling Requirements: [resource needs for growth accommodation]\n\n### Load Testing Results\n\n| Scenario | Throughput | Latency (p95) | Error Rate | Resource Usage |\n|----------|------------|---------------|------------|----------------|\n| Normal Load | 500 RPS | 200ms | 0.1% | 60% CPU |\n| Peak Load | 1000 RPS | 800ms | 2.5% | 85% CPU |\n| Stress Test | 1500 RPS | 2000ms | 15% | 95% CPU |\n\n### Bottleneck Analysis\n- Primary Bottleneck: [most limiting performance factor]\n- Secondary Bottlenecks: [additional constraints affecting performance]\n- Cascade Effects: [how bottlenecks impact other system components]\n- Resolution Priority: [recommended order of bottleneck addressing]\n\n### Optimization Recommendations\n\n#### Immediate Optimizations (0-30 days):\n- Quick Wins: [low-effort, high-impact improvements]\n- Configuration Tuning: [parameter adjustments and settings optimization]\n- Query Optimization: [database and application query improvements]\n- Caching Implementation: [strategic caching layer additions]\n\n#### Medium-term Optimizations (1-6 months):\n- Architecture Changes: [structural improvements and scaling strategies]\n- Infrastructure Upgrades: [hardware and platform enhancements]\n- Code Refactoring: [application optimization and efficiency improvements]\n- Monitoring Enhancement: [observability and alerting system improvements]\n\n#### Long-term Optimizations (6+ months):\n- Technology Migration: [platform or framework modernization]\n- System Redesign: [fundamental architecture improvements]\n- Capacity Expansion: [infrastructure scaling and geographic distribution]\n- Innovation Integration: [new technology adoption and competitive advantage]\n\n### Capacity Planning\n- Current Capacity: [existing system limits and headroom]\n- Growth Accommodation: [resource scaling for projected demand]\n- Cost Implications: [budget requirements for capacity increases]\n- Timeline Requirements: [implementation schedule for capacity improvements]\n\n### Monitoring and Alerting Strategy\n- Key Performance Indicators: [critical metrics for ongoing monitoring]\n- Alert Thresholds: [performance degradation warning levels]\n- Escalation Procedures: [response protocols for performance issues]\n- Regular Review Schedule: [ongoing optimization and capacity assessment]\n```\n\n### 9. Continuous Performance Learning\n\n**Establish ongoing simulation refinement and system optimization:**\n\n#### Performance Validation\n- Real-world performance comparison to simulation predictions\n- Optimization effectiveness measurement and validation\n- User experience correlation with system performance metrics\n- Business impact assessment of performance improvements\n\n#### Model Enhancement\n- Simulation accuracy improvement based on actual system behavior\n- Load pattern refinement and user behavior modeling\n- Bottleneck prediction enhancement and early warning systems\n- Optimization strategy effectiveness tracking and improvement\n\n## Usage Examples\n\n```bash\n# Web application performance simulation\n/performance:system-behavior-simulator Simulate e-commerce platform performance under Black Friday traffic with 10x normal load\n\n# API service scaling analysis\n/performance:system-behavior-simulator Model REST API performance for mobile app with 1M+ daily active users and geographic distribution\n\n# Database performance optimization\n/performance:system-behavior-simulator Simulate database performance for analytics workload with real-time reporting requirements\n\n# Microservices capacity planning\n/performance:system-behavior-simulator Model microservices mesh performance under various failure scenarios and auto-scaling conditions\n```\n\n## Quality Indicators\n\n- **Green**: Comprehensive load modeling, validated bottleneck analysis, quantified optimization strategies\n- **Yellow**: Good load coverage, basic bottleneck identification, estimated optimization benefits\n- **Red**: Limited load scenarios, unvalidated bottlenecks, qualitative-only optimization suggestions\n\n## Common Pitfalls to Avoid\n\n- Load unrealism: Testing with artificial patterns that don't match real usage\n- Bottleneck tunnel vision: Focusing on single constraints while ignoring others\n- Optimization premature: Optimizing for problems that don't exist yet\n- Capacity under-planning: Not accounting for growth and traffic spikes\n- Monitoring blindness: Not establishing ongoing performance visibility\n- Cost ignorance: Optimizing performance without considering budget constraints\n\nTransform system performance from reactive firefighting into proactive, data-driven optimization through comprehensive behavior simulation and capacity planning."
              },
              {
                "name": "/task-from-pr",
                "description": "Create Linear tasks from pull requests",
                "path": "plugins/all-commands/commands/task-from-pr.md",
                "frontmatter": {
                  "description": "Create Linear tasks from pull requests",
                  "category": "integration-sync",
                  "allowed-tools": "Bash(gh *)"
                },
                "content": "# task-from-pr\n\nCreate Linear tasks from pull requests\n\n## Purpose\nThis command analyzes GitHub pull requests and creates corresponding Linear tasks, automatically extracting key information like title, description, labels, and assignees. It helps maintain synchronization between GitHub development workflow and Linear project management.\n\n## Usage\n```bash\n# Convert a specific PR to a Linear task\nclaude \"Convert PR #123 to a Linear task\"\n\n# Convert multiple PRs from a repository\nclaude \"Convert all open PRs to Linear tasks for repo owner/repo\"\n\n# Convert PR with custom mapping\nclaude \"Create Linear task from PR #456 and assign to team 'Engineering'\"\n```\n\n## Instructions\n\n### 1. Gather PR Information\nFirst, use GitHub CLI to fetch PR details:\n\n```bash\n# Get PR information\ngh pr view <PR_NUMBER> --json title,body,labels,assignees,state,url,createdAt,updatedAt,milestone\n\n# List all open PRs\ngh pr list --json number,title,labels,assignees --limit 100\n```\n\n### 2. Parse PR Description\nExtract structured information from the PR body:\n\n- Look for sections like \"## Description\", \"## Changes\", \"## Testing\"\n- Identify checklist items (- [ ] or - [x])\n- Extract any mentioned issue numbers (#123)\n- Find @mentions for stakeholders\n- Identify code blocks for technical details\n\n### 3. Map GitHub Labels to Linear\nCommon label mappings:\n- `bug`  Linear label: \"Bug\" + Priority: High\n- `feature`  Linear label: \"Feature\"\n- `enhancement`  Linear label: \"Improvement\"\n- `documentation`  Linear label: \"Documentation\"\n- `performance`  Linear label: \"Performance\"\n- `security`  Linear label: \"Security\" + Priority: Urgent\n\n### 4. Extract Task Details\nGenerate Linear task structure:\n\n```javascript\n{\n  title: `[PR #${prNumber}] ${prTitle}`,\n  description: `\n    **GitHub PR:** ${prUrl}\n    \n    ## Summary\n    ${extractedSummary}\n    \n    ## Changes\n    ${bulletPoints}\n    \n    ## Acceptance Criteria\n    ${checklistItems}\n    \n    ## Technical Details\n    ${codeSnippets}\n  `,\n  priority: mapPriorityFromLabels(labels),\n  labels: mapLabelsToLinear(labels),\n  estimate: estimateFromPRSize(additions, deletions),\n  assignee: mapGitHubUserToLinear(assignees[0])\n}\n```\n\n### 5. Estimate Task Size\nCalculate estimates based on PR metrics:\n\n```\n- Tiny (1 point): < 10 lines changed\n- Small (2 points): 10-50 lines changed\n- Medium (3 points): 50-250 lines changed\n- Large (5 points): 250-500 lines changed\n- X-Large (8 points): > 500 lines changed\n\nAdjust based on:\n- Number of files changed (multiply by 1.2 if > 10 files)\n- Presence of tests (multiply by 0.8 if tests included)\n- Documentation changes (multiply by 0.7 if only docs)\n```\n\n### 6. Create Linear Task\nUse Linear MCP to create the task:\n\n```javascript\n// Example Linear task creation\nconst task = await linear.createTask({\n  title: taskTitle,\n  description: taskDescription,\n  teamId: getTeamId(),\n  priority: priority,\n  estimate: estimate,\n  labels: labelIds,\n  assigneeId: assigneeId\n});\n\n// Link back to GitHub PR\nawait linear.createComment({\n  issueId: task.id,\n  body: `Linked to GitHub PR: ${prUrl}`\n});\n```\n\n### 7. Error Handling\nHandle common scenarios:\n\n```javascript\n// Check for Linear MCP availability\nif (!linear.available) {\n  console.error(\"Linear MCP tool not available. Please ensure it's configured.\");\n  return;\n}\n\n// Check for GitHub CLI\ntry {\n  await exec('gh --version');\n} catch (error) {\n  console.error(\"GitHub CLI not installed. Please install: https://cli.github.com/\");\n  return;\n}\n\n// Handle duplicate tasks\nconst existingTask = await linear.searchTasks(`PR #${prNumber}`);\nif (existingTask) {\n  console.log(`Task already exists for PR #${prNumber}: ${existingTask.url}`);\n  return;\n}\n```\n\n## Example Output\n\n```\nConverting PR #123 to Linear task...\n\nFetched PR details:\n- Title: Add user authentication middleware\n- Author: @johndoe\n- Labels: feature, backend, security\n- Size: 234 lines changed across 8 files\n\nParsed description:\n- Summary: Implements JWT-based authentication\n- Has 5 checklist items (3 completed)\n- References issues: #98, #102\n\nCreating Linear task...\n Task created: LIN-456\n  Title: [PR #123] Add user authentication middleware\n  Team: Backend\n  Priority: High (due to security label)\n  Estimate: 3 points\n  Labels: Feature, Backend, Security\n  Assignee: John Doe\n\nTask URL: https://linear.app/yourteam/issue/LIN-456\n```\n\n## Advanced Features\n\n### Batch Processing\nConvert multiple PRs:\n```bash\n# Convert all PRs with specific label\ngh pr list --label \"needs-task\" --json number | \\\n  jq -r '.[].number' | \\\n  xargs -I {} claude \"Convert PR #{} to Linear task\"\n```\n\n### Custom Field Mapping\nMap PR metadata to Linear custom fields:\n- PR review status  Linear custom field \"Review Status\"\n- PR branch name  Linear custom field \"Feature Branch\"\n- CI/CD status  Linear custom field \"Build Status\"\n\n### Automated Sync\nSet up webhook to automatically create tasks when PRs are opened:\n```javascript\n// Webhook handler\non('pull_request.opened', async (event) => {\n  await createLinearTaskFromPR(event.pull_request);\n});\n```\n\n## Tips\n- Include PR number in task title for easy reference\n- Use Linear's GitHub integration to auto-link commits\n- Set up bidirectional sync to update PR when task status changes\n- Create subtasks for PR checklist items if needed\n- Add PR author as a subscriber if they're not the assignee"
              },
              {
                "name": "/tdd",
                "description": "Test-driven development workflow with Red-Green-Refactor process and branch management",
                "path": "plugins/all-commands/commands/tdd.md",
                "frontmatter": {
                  "description": "Test-driven development workflow with Red-Green-Refactor process and branch management",
                  "category": "code-analysis-testing",
                  "allowed-tools": "Read, Write, Edit, Bash(git *)"
                },
                "content": "This outlines the development practices and principles we require you to follow. Don't start\nworking on features until asked, this document is intended to get you into the right state\nof mind.\n\n1. Make sure you are on the main branch before you start (unless instructed to start on a specific branch)\n2. Understand the code that is there before you begin to change it.\n3. Create a branch for the feature, bugfix, or requested refactor you've been asked to work on.\n4. Employ test-driven development. Red-Green-Refactor process (outlined below)\n5. When committing to git, omit the Claude footer from comments.\n6. Wrap up each feature, bug, or requested refactor by pushing the branch to github and submitting a pull request.\n7. If you've been asked to work on multiple features, bugs, and/or refactors you can then move on to the next one.\n\n# High-level flow\n\n## One vs many\nSometimes you will be given one task. Sometimes you will be given a task list.\nThe list might be provided as a git repo issue list, for example.\n\nIf you are given many at once, start with the first, and complete them one by one, creating a branch for each and a pull-request when finished.\n\n## Keep notes\nCreate a markdown file under the notes/features/ folder for the feature. If you are creating a feature branch, use the same name.\n\nUse this notes file to record answers to clarifying questions, and other important things as you work on the feature. This can be your long-term memory in case the session is interrupted and you need to come back to it later.\n\nThese are your notes, so feel free to add, modify, re-arrange, and delete content in the notes file.\n\nYou may, if you wish, add other notes that might be helpful to you or future developers, but more isn't always better. Be breif and helpful.\n\n## Understand the feature\n1. First read the README.md and any relevant docs it points to.\n1. Ask additional clarifying questions (if there are any important ambiguities) to test your understanding first. For example,\nif you were asked to write a tic-tac-toe app,"
              },
              {
                "name": "/team-workload-balancer",
                "description": "Balance team workload distribution",
                "path": "plugins/all-commands/commands/team-workload-balancer.md",
                "frontmatter": {
                  "description": "Balance team workload distribution",
                  "category": "team-collaboration",
                  "allowed-tools": "Bash(git *), Bash(gh *)"
                },
                "content": "# team-workload-balancer\n\nBalance team workload distribution\n\n## Purpose\nThis command analyzes team members' current workloads, skills, past performance, and availability to suggest optimal task assignments. It helps prevent burnout, ensures balanced distribution, and matches tasks to team members' strengths.\n\n## Usage\n```bash\n# Show current team workload\nclaude \"Show workload balance for the engineering team\"\n\n# Suggest optimal assignment for new tasks\nclaude \"Who should work on the new payment integration task?\"\n\n# Rebalance current sprint\nclaude \"Rebalance tasks in the current sprint for optimal distribution\"\n\n# Capacity planning for next sprint\nclaude \"Plan task assignments for next sprint based on team capacity\"\n```\n\n## Instructions\n\n### 1. Gather Team Data\nCollect information about team members:\n\n```javascript\nclass TeamAnalyzer {\n  async gatherTeamData() {\n    const team = {};\n    \n    // Get team members from Linear\n    const teamMembers = await linear.getTeamMembers();\n    \n    for (const member of teamMembers) {\n      team[member.id] = {\n        name: member.name,\n        email: member.email,\n        currentTasks: [],\n        completedTasks: [],\n        skills: new Set(),\n        velocity: 0,\n        availability: 100, // percentage\n        preferences: {},\n        strengths: [],\n        timeZone: member.timeZone\n      };\n      \n      // Get current assignments\n      const activeTasks = await linear.getUserTasks(member.id, {\n        filter: { state: ['in_progress', 'todo'] }\n      });\n      team[member.id].currentTasks = activeTasks;\n      \n      // Get historical data\n      const completedTasks = await linear.getUserTasks(member.id, {\n        filter: { state: 'done' },\n        since: '3 months ago'\n      });\n      team[member.id].completedTasks = completedTasks;\n      \n      // Analyze git contributions\n      const gitStats = await this.analyzeGitContributions(member.email);\n      team[member.id].skills = gitStats.technologies;\n      team[member.id].codeContributions = gitStats.contributions;\n    }\n    \n    return team;\n  }\n  \n  async analyzeGitContributions(email) {\n    // Get commit history\n    const commits = await exec(`git log --author=\"${email}\" --since=\"6 months ago\" --pretty=format:\"%H\"`);\n    const commitHashes = commits.split('\\n').filter(Boolean);\n    \n    const stats = {\n      technologies: new Set(),\n      contributions: {\n        frontend: 0,\n        backend: 0,\n        database: 0,\n        devops: 0,\n        testing: 0,\n        documentation: 0\n      },\n      filesChanged: new Map()\n    };\n    \n    // Analyze each commit\n    for (const hash of commitHashes.slice(0, 100)) { // Limit to recent 100 commits\n      const files = await exec(`git show --name-only --pretty=format: ${hash}`);\n      const fileList = files.split('\\n').filter(Boolean);\n      \n      for (const file of fileList) {\n        // Track technologies\n        if (file.match(/\\.(js|jsx|ts|tsx)$/)) stats.technologies.add('JavaScript');\n        if (file.match(/\\.(py)$/)) stats.technologies.add('Python');\n        if (file.match(/\\.(java)$/)) stats.technologies.add('Java');\n        if (file.match(/\\.(go)$/)) stats.technologies.add('Go');\n        \n        // Categorize contributions\n        if (file.match(/\\/(components|views|pages|frontend)\\//)) stats.contributions.frontend++;\n        if (file.match(/\\/(api|server|backend|services)\\//)) stats.contributions.backend++;\n        if (file.match(/\\/(migrations|schemas|models)\\//)) stats.contributions.database++;\n        if (file.match(/\\/(deploy|docker|k8s|.github)\\//)) stats.contributions.devops++;\n        if (file.match(/\\.(test|spec)\\./)) stats.contributions.testing++;\n        if (file.match(/\\.(md|docs)\\//)) stats.contributions.documentation++;\n        \n        // Track file expertise\n        stats.filesChanged.set(file, (stats.filesChanged.get(file) || 0) + 1);\n      }\n    }\n    \n    return stats;\n  }\n}\n```\n\n### 2. Calculate Workload Metrics\nAnalyze current workload distribution:\n\n```javascript\nclass WorkloadCalculator {\n  calculateWorkload(teamMember) {\n    const metrics = {\n      currentPoints: 0,\n      currentTasks: teamMember.currentTasks.length,\n      inProgressPoints: 0,\n      todoPoints: 0,\n      blockedTasks: 0,\n      overdueTasksk: 0,\n      workloadScore: 0, // 0-100\n      capacity: 0\n    };\n    \n    // Sum story points\n    for (const task of teamMember.currentTasks) {\n      const points = task.estimate || 3; // Default to 3 if no estimate\n      metrics.currentPoints += points;\n      \n      if (task.state === 'in_progress') {\n        metrics.inProgressPoints += points;\n      } else if (task.state === 'todo') {\n        metrics.todoPoints += points;\n      }\n      \n      if (task.blockedBy?.length > 0) {\n        metrics.blockedTasks++;\n      }\n      \n      if (task.dueDate && new Date(task.dueDate) < new Date()) {\n        metrics.overdueTasksk++;\n      }\n    }\n    \n    // Calculate velocity from historical data\n    const velocity = this.calculateVelocity(teamMember.completedTasks);\n    \n    // Calculate workload score (0-100)\n    // Higher score = more overloaded\n    metrics.workloadScore = Math.min(100, (metrics.currentPoints / velocity.average) * 100);\n    \n    // Calculate remaining capacity\n    metrics.capacity = Math.max(0, velocity.average - metrics.currentPoints);\n    \n    // Adjust for blocked tasks\n    if (metrics.blockedTasks > 0) {\n      metrics.workloadScore *= 1.2; // Increase workload score for blocked work\n    }\n    \n    return metrics;\n  }\n  \n  calculateVelocity(completedTasks) {\n    // Group by sprint/week\n    const tasksByWeek = new Map();\n    \n    for (const task of completedTasks) {\n      const weekKey = this.getWeekKey(task.completedAt);\n      if (!tasksByWeek.has(weekKey)) {\n        tasksByWeek.set(weekKey, []);\n      }\n      tasksByWeek.get(weekKey).push(task);\n    }\n    \n    // Calculate points per week\n    const weeklyPoints = [];\n    for (const [week, tasks] of tasksByWeek) {\n      const points = tasks.reduce((sum, t) => sum + (t.estimate || 0), 0);\n      weeklyPoints.push(points);\n    }\n    \n    return {\n      average: weeklyPoints.reduce((a, b) => a + b, 0) / weeklyPoints.length || 10,\n      min: Math.min(...weeklyPoints) || 5,\n      max: Math.max(...weeklyPoints) || 15,\n      trend: this.calculateTrend(weeklyPoints)\n    };\n  }\n}\n```\n\n### 3. Skill Matching Algorithm\nMatch tasks to team members based on skills:\n\n```javascript\nclass SkillMatcher {\n  calculateSkillMatch(task, teamMember) {\n    const taskRequirements = this.extractTaskRequirements(task);\n    const memberSkills = this.consolidateSkills(teamMember);\n    \n    let matchScore = 0;\n    let maxScore = 0;\n    \n    // Technology match\n    for (const tech of taskRequirements.technologies) {\n      maxScore += 10;\n      if (memberSkills.technologies.has(tech)) {\n        matchScore += 10;\n      } else if (this.isRelatedTechnology(tech, memberSkills.technologies)) {\n        matchScore += 5;\n      }\n    }\n    \n    // Domain expertise match\n    if (taskRequirements.domain) {\n      maxScore += 20;\n      const domainExperience = this.getDomainExperience(teamMember, taskRequirements.domain);\n      matchScore += Math.min(20, domainExperience * 2);\n    }\n    \n    // Task type preference\n    maxScore += 10;\n    if (memberSkills.preferences[taskRequirements.type] > 0.7) {\n      matchScore += 10;\n    } else if (memberSkills.preferences[taskRequirements.type] > 0.4) {\n      matchScore += 5;\n    }\n    \n    // Recent similar work\n    const similarTasks = this.findSimilarCompletedTasks(teamMember, task);\n    if (similarTasks.length > 0) {\n      maxScore += 15;\n      matchScore += Math.min(15, similarTasks.length * 3);\n    }\n    \n    return {\n      score: maxScore > 0 ? (matchScore / maxScore) : 0,\n      matches: {\n        technologies: this.getTechMatches(taskRequirements, memberSkills),\n        domain: taskRequirements.domain && memberSkills.domains.includes(taskRequirements.domain),\n        experience: similarTasks.length\n      }\n    };\n  }\n  \n  extractTaskRequirements(task) {\n    const requirements = {\n      technologies: new Set(),\n      domain: null,\n      type: 'feature',\n      complexity: 'medium',\n      skills: []\n    };\n    \n    // Extract from title and description\n    const text = `${task.title} ${task.description}`.toLowerCase();\n    \n    // Technology detection\n    const techPatterns = {\n      'react': /react|jsx|component/,\n      'node': /node|express|npm/,\n      'python': /python|django|flask/,\n      'database': /sql|database|query|migration/,\n      'api': /api|rest|graphql|endpoint/,\n      'frontend': /ui|ux|css|style|layout/,\n      'backend': /server|backend|service/,\n      'devops': /deploy|docker|k8s|ci\\/cd/\n    };\n    \n    for (const [tech, pattern] of Object.entries(techPatterns)) {\n      if (pattern.test(text)) {\n        requirements.technologies.add(tech);\n      }\n    }\n    \n    // Domain detection\n    if (text.includes('auth') || text.includes('login')) requirements.domain = 'authentication';\n    if (text.includes('payment') || text.includes('billing')) requirements.domain = 'payments';\n    if (text.includes('user') || text.includes('profile')) requirements.domain = 'users';\n    \n    // Type detection\n    if (task.labels.some(l => l.name === 'bug')) requirements.type = 'bug';\n    if (task.labels.some(l => l.name === 'refactor')) requirements.type = 'refactor';\n    \n    return requirements;\n  }\n}\n```\n\n### 4. Load Balancing Algorithm\nDistribute tasks optimally:\n\n```javascript\nclass LoadBalancer {\n  balanceTasks(tasks, team, constraints = {}) {\n    const assignments = new Map(); // task -> assignee\n    const workloads = new Map(); // assignee -> current load\n    \n    // Initialize workloads\n    for (const [memberId, member] of Object.entries(team)) {\n      workloads.set(memberId, this.calculateWorkload(member));\n    }\n    \n    // Sort tasks by priority and size\n    const sortedTasks = tasks.sort((a, b) => {\n      const priorityDiff = (a.priority || 3) - (b.priority || 3);\n      if (priorityDiff !== 0) return priorityDiff;\n      return (b.estimate || 3) - (a.estimate || 3); // Larger tasks first\n    });\n    \n    // Assign tasks using modified bin packing algorithm\n    for (const task of sortedTasks) {\n      const candidates = this.findCandidates(task, team, workloads, constraints);\n      \n      if (candidates.length === 0) {\n        console.warn(`No suitable assignee found for task: ${task.title}`);\n        continue;\n      }\n      \n      // Select best candidate\n      const best = candidates.reduce((a, b) => \n        a.score > b.score ? a : b\n      );\n      \n      assignments.set(task.id, best.memberId);\n      \n      // Update workload\n      const currentLoad = workloads.get(best.memberId);\n      currentLoad.currentPoints += task.estimate || 3;\n      currentLoad.workloadScore = this.recalculateWorkloadScore(currentLoad);\n    }\n    \n    return {\n      assignments,\n      balance: this.calculateBalance(workloads),\n      warnings: this.generateWarnings(workloads, team)\n    };\n  }\n  \n  findCandidates(task, team, currentWorkloads, constraints) {\n    const candidates = [];\n    \n    for (const [memberId, member] of Object.entries(team)) {\n      const workload = currentWorkloads.get(memberId);\n      \n      // Check hard constraints\n      if (constraints.maxLoad && workload.currentPoints >= constraints.maxLoad) {\n        continue;\n      }\n      \n      if (constraints.requireSkill && !member.skills.has(constraints.requireSkill)) {\n        continue;\n      }\n      \n      // Calculate assignment score\n      const skillMatch = this.calculateSkillMatch(task, member);\n      const loadScore = 1 - (workload.workloadScore / 100); // Prefer less loaded\n      const velocityScore = member.velocity / 20; // Normalize velocity\n      \n      // Weighted score\n      const score = (\n        skillMatch.score * 0.4 +\n        loadScore * 0.4 +\n        velocityScore * 0.2\n      );\n      \n      candidates.push({\n        memberId,\n        memberName: member.name,\n        score,\n        factors: {\n          skill: skillMatch.score,\n          load: loadScore,\n          velocity: velocityScore\n        }\n      });\n    }\n    \n    return candidates.sort((a, b) => b.score - a.score);\n  }\n  \n  calculateBalance(workloads) {\n    const loads = Array.from(workloads.values()).map(w => w.currentPoints);\n    const avg = loads.reduce((a, b) => a + b, 0) / loads.length;\n    const variance = loads.reduce((sum, load) => sum + Math.pow(load - avg, 2), 0) / loads.length;\n    const stdDev = Math.sqrt(variance);\n    \n    return {\n      average: avg,\n      standardDeviation: stdDev,\n      balanceScore: 100 - Math.min(100, (stdDev / avg) * 100), // 0-100, higher is better\n      distribution: this.getDistribution(loads)\n    };\n  }\n}\n```\n\n### 5. Visualization Functions\nCreate visual representations of workload:\n\n```javascript\nfunction visualizeWorkload(team, assignments) {\n  const output = [];\n  \n  // Team workload bar chart\n  output.push('## Team Workload Distribution\\n');\n  \n  const maxPoints = Math.max(...Object.values(team).map(m => m.currentPoints));\n  \n  for (const [id, member] of Object.entries(team)) {\n    const points = member.currentPoints;\n    const capacity = member.velocity.average;\n    const utilization = (points / capacity) * 100;\n    \n    // Create visual bar\n    const barLength = Math.round((points / maxPoints) * 40);\n    const bar = ''.repeat(barLength) + ''.repeat(40 - barLength);\n    \n    // Color coding\n    let status = ''; // Green\n    if (utilization > 120) status = ''; // Red - overloaded\n    else if (utilization > 90) status = ''; // Yellow - near capacity\n    \n    output.push(`${status} ${member.name.padEnd(15)} ${bar} ${points}/${capacity} pts (${Math.round(utilization)}%)`);\n  }\n  \n  // Task distribution matrix\n  output.push('\\n## Recommended Task Assignments\\n');\n  output.push('| Task | Assignee | Skill Match | Load After | Reason |');\n  output.push('|------|----------|-------------|------------|---------|');\n  \n  for (const [taskId, assignment] of assignments) {\n    const task = findTask(taskId);\n    const member = team[assignment.memberId];\n    const newLoad = member.currentPoints + (task.estimate || 3);\n    const loadPercent = Math.round((newLoad / member.velocity.average) * 100);\n    \n    output.push(\n      `| ${task.title.substring(0, 30)}... | ${member.name} | ${Math.round(assignment.skillMatch * 100)}% | ${loadPercent}% | ${assignment.reason} |`\n    );\n  }\n  \n  return output.join('\\n');\n}\n\nfunction generateGanttChart(team, timeframe = 14) {\n  const chart = [];\n  const today = new Date();\n  \n  chart.push('## Sprint Timeline (Next 2 Weeks)\\n');\n  chart.push('```');\n  \n  // Header\n  const days = [];\n  for (let i = 0; i < timeframe; i++) {\n    const date = new Date(today);\n    date.setDate(date.getDate() + i);\n    days.push(date.toLocaleDateString('en', { weekday: 'short' })[0]);\n  }\n  chart.push('        ' + days.join(' '));\n  \n  // Team member rows\n  for (const [id, member] of Object.entries(team)) {\n    const tasks = member.currentTasks.sort((a, b) => \n      new Date(a.dueDate || '2099-01-01') - new Date(b.dueDate || '2099-01-01')\n    );\n    \n    let timeline = '';\n    let currentDay = 0;\n    \n    for (const task of tasks) {\n      const duration = task.estimate || 3;\n      const taskChar = task.priority === 1 ? '' : '';\n      timeline += ' '.repeat(Math.max(0, currentDay)) + taskChar.repeat(duration);\n      currentDay += duration;\n    }\n    \n    chart.push(`${member.name.padEnd(8)}${timeline.padEnd(timeframe, '')}`);\n  }\n  \n  chart.push('```');\n  return chart.join('\\n');\n}\n```\n\n### 6. Optimization Suggestions\nGenerate actionable recommendations:\n\n```javascript\nclass WorkloadOptimizer {\n  generateSuggestions(team, currentAssignments, constraints) {\n    const suggestions = [];\n    const metrics = this.analyzeCurrentState(team);\n    \n    // Check for overloaded members\n    for (const [id, member] of Object.entries(team)) {\n      if (member.workloadScore > 90) {\n        suggestions.push({\n          type: 'overload',\n          priority: 'high',\n          member: member.name,\n          action: `Redistribute ${member.currentPoints - member.velocity.average} points from ${member.name}`,\n          tasks: this.findTasksToReassign(member)\n        });\n      }\n    }\n    \n    // Check for underutilized members\n    for (const [id, member] of Object.entries(team)) {\n      if (member.workloadScore < 50 && member.availability > 80) {\n        suggestions.push({\n          type: 'underutilized',\n          priority: 'medium',\n          member: member.name,\n          action: `${member.name} has ${member.capacity} points available capacity`,\n          candidates: this.findTasksForMember(member, team)\n        });\n      }\n    }\n    \n    // Check for skill mismatches\n    const mismatches = this.findSkillMismatches(currentAssignments, team);\n    for (const mismatch of mismatches) {\n      suggestions.push({\n        type: 'skill_mismatch',\n        priority: 'medium',\n        action: `Consider reassigning \"${mismatch.task.title}\" from ${mismatch.current} to ${mismatch.suggested}`,\n        reason: mismatch.reason\n      });\n    }\n    \n    // Sprint risk analysis\n    const risks = this.analyzeSprintRisks(team);\n    for (const risk of risks) {\n      suggestions.push({\n        type: 'risk',\n        priority: risk.severity,\n        action: risk.mitigation,\n        impact: risk.impact\n      });\n    }\n    \n    return suggestions;\n  }\n  \n  findTasksToReassign(overloadedMember) {\n    // Find lowest priority tasks that can be reassigned\n    const tasks = overloadedMember.currentTasks\n      .filter(t => t.state === 'todo' && !t.blockedBy?.length)\n      .sort((a, b) => (b.priority || 3) - (a.priority || 3));\n    \n    const toReassign = [];\n    let pointsToRemove = overloadedMember.currentPoints - overloadedMember.velocity.average;\n    \n    for (const task of tasks) {\n      if (pointsToRemove <= 0) break;\n      toReassign.push(task);\n      pointsToRemove -= (task.estimate || 3);\n    }\n    \n    return toReassign;\n  }\n}\n```\n\n### 7. Error Handling\n```javascript\n// Handle missing Linear access\nif (!linear.available) {\n  console.error(\"Linear MCP tool not available\");\n  // Fall back to manual input or cached data\n}\n\n// Handle team member availability\nconst availability = {\n  async checkAvailability(member) {\n    // Check calendar integration if available\n    try {\n      const calendar = await getCalendarEvents(member.email);\n      const outOfOffice = calendar.filter(e => e.type === 'ooo');\n      return this.calculateAvailability(outOfOffice);\n    } catch (error) {\n      console.warn(`Could not check calendar for ${member.name}`);\n      return 100; // Assume full availability\n    }\n  }\n};\n\n// Handle incomplete data\nif (!task.estimate) {\n  console.warn(`Task \"${task.title}\" has no estimate, using default: 3 points`);\n  task.estimate = 3;\n}\n```\n\n## Example Output\n\n```\nAnalyzing team workload and generating recommendations...\n\n Team Overview\n\n\nCurrent Sprint: Sprint 23 (5 days remaining)\nTeam Size: 5 engineers\nTotal Capacity: 65 points\nCurrent Load: 71 points (109% capacity)\n\n Individual Workload\n\n\n Alice Chen       18/13 pts (138%)\n   In Progress: 2 tasks (8 pts) | Todo: 3 tasks (10 pts)\n    Overloaded by 5 points\n\n Bob Smith        14/15 pts (93%)\n   In Progress: 1 task (5 pts) | Todo: 3 tasks (9 pts)\n    Near optimal capacity\n\n Carol Davis      8/12 pts (67%)\n   In Progress: 1 task (3 pts) | Todo: 2 tasks (5 pts)\n    Has 4 points available capacity\n\n David Kim        7/10 pts (70%)\n   In Progress: 1 task (4 pts) | Todo: 1 task (3 pts)\n    Has 3 points available capacity\n\n Eve Johnson      17/15 pts (113%)\n   In Progress: 3 tasks (12 pts) | Todo: 2 tasks (5 pts)\n    Slightly overloaded\n\n Optimization Recommendations\n\n\n1.  HIGH PRIORITY: Redistribute Alice's workload\n   Action: Move 2 tasks (5 points) to other team members\n   Suggested reassignments:\n    \"API Rate Limiting\" (3 pts)  Carol (has backend expertise)\n    \"Update User Dashboard\" (2 pts)  David (worked on similar feature)\n\n2.  MEDIUM: Optimize skill matching\n    \"Payment Webhook Integration\" assigned to Eve\n     Better match: Bob (85% skill match vs 60%)\n     Bob has extensive webhook experience\n\n3.  MEDIUM: Balance in-progress items\n   Eve has 3 tasks in progress (risk of context switching)\n   Recommendation: Complete 1 before starting new work\n\n4.  LOW: Utilize available capacity\n   Carol and David have 7 points combined capacity\n   Suggested tasks from backlog:\n    \"Add Email Notifications\" (3 pts)  Carol\n    \"Optimize Search Query\" (2 pts)  David\n\n Proposed Rebalanced Distribution\n\n\nAfter rebalancing:\n Alice Chen       13/13 pts (100%)\n Bob Smith        14/15 pts (93%)\n Carol Davis      11/12 pts (92%)\n David Kim        9/10 pts (90%)\n Eve Johnson      12/15 pts (80%)\n\nBalance Score: 85/100 (Good)  94/100 (Excellent)\nRisk Level: High  Low\n\n Sprint Timeline\n\n\n        M T W T F M T W T F M T W T\nAlice   \nBob     \nCarol   \nDavid   \nEve     \n\nLegend:  High Priority |  Normal |  Available\n\n Quick Actions\n\n\n1. Run: claude \"Reassign task LIN-234 from Alice to Carol\"\n2. Run: claude \"Update sprint capacity to account for Eve's half day Friday\"\n3. Run: claude \"Create balanced task list for next sprint planning\"\n```\n\n## Advanced Features\n\n### Capacity Planning\n```bash\n# Plan next sprint with holidays and time off\nclaude \"Plan sprint 24 capacity - Alice off Monday, Bob at conference Wed-Thu\"\n```\n\n### Skill Development\n```bash\n# Identify learning opportunities\nclaude \"Suggest tasks for Carol to learn React based on current workload\"\n```\n\n### Team Performance\n```bash\n# Analyze team velocity trends\nclaude \"Show team velocity trends and predict sprint 24 capacity\"\n```\n\n## Tips\n- Update availability regularly (vacations, meetings)\n- Consider time zones for distributed teams\n- Track actual vs estimated to improve predictions\n- Use skill matching to grow team capabilities\n- Monitor workload balance weekly, not just at sprint start\n- Consider task dependencies in assignments\n- Factor in code review time for junior developers"
              },
              {
                "name": "/test-changelog-automation",
                "description": "Automate changelog testing workflow",
                "path": "plugins/all-commands/commands/test-changelog-automation.md",
                "frontmatter": {
                  "description": "Automate changelog testing workflow",
                  "category": "code-analysis-testing"
                },
                "content": "# Test Command\n\nAutomate changelog testing workflow\n\n## Instructions\n\n1. This command serves as a demonstration\n2. It shows how the changelog automation works\n3. When this file is added, the changelog should update automatically"
              },
              {
                "name": "/test-coverage",
                "description": "Analyze and report test coverage",
                "path": "plugins/all-commands/commands/test-coverage.md",
                "frontmatter": {
                  "description": "Analyze and report test coverage",
                  "category": "code-analysis-testing",
                  "argument-hint": "1. **Coverage Tool Setup**",
                  "allowed-tools": "Bash(npm *), Write"
                },
                "content": "# Test Coverage Command\n\nAnalyze and report test coverage\n\n## Instructions\n\nFollow this systematic approach to analyze and improve test coverage: **$ARGUMENTS**\n\n1. **Coverage Tool Setup**\n   - Identify and configure appropriate coverage tools:\n     - JavaScript/Node.js: Jest, NYC, Istanbul\n     - Python: Coverage.py, pytest-cov\n     - Java: JaCoCo, Cobertura\n     - C#: dotCover, OpenCover\n     - Ruby: SimpleCov\n   - Configure coverage reporting formats (HTML, XML, JSON)\n   - Set up coverage thresholds and quality gates\n\n2. **Baseline Coverage Analysis**\n   - Run existing tests with coverage reporting\n   - Generate comprehensive coverage reports\n   - Document current coverage percentages:\n     - Line coverage\n     - Branch coverage\n     - Function coverage\n     - Statement coverage\n   - Identify uncovered code areas\n\n3. **Coverage Report Analysis**\n   - Review detailed coverage reports by file and directory\n   - Identify critical uncovered code paths\n   - Analyze branch coverage for conditional logic\n   - Find untested functions and methods\n   - Examine coverage trends over time\n\n4. **Critical Path Identification**\n   - Identify business-critical code that lacks coverage\n   - Prioritize high-risk, low-coverage areas\n   - Focus on public APIs and interfaces\n   - Target error handling and edge cases\n   - Examine security-sensitive code paths\n\n5. **Test Gap Analysis**\n   - Categorize uncovered code:\n     - Business logic requiring immediate testing\n     - Error handling and exception paths\n     - Configuration and setup code\n     - Utility functions and helpers\n     - Dead or obsolete code to remove\n\n6. **Strategic Test Writing**\n   - Write unit tests for uncovered business logic\n   - Add integration tests for uncovered workflows\n   - Create tests for error conditions and edge cases\n   - Test configuration and environment-specific code\n   - Add regression tests for bug-prone areas\n\n7. **Branch Coverage Improvement**\n   - Identify uncovered conditional branches\n   - Test both true and false conditions\n   - Cover all switch/case statements\n   - Test exception handling paths\n   - Verify loop conditions and iterations\n\n8. **Edge Case Testing**\n   - Test boundary conditions and limits\n   - Test null, empty, and invalid inputs\n   - Test timeout and network failure scenarios\n   - Test resource exhaustion conditions\n   - Test concurrent access and race conditions\n\n9. **Mock and Stub Strategy**\n   - Mock external dependencies for better isolation\n   - Stub complex operations to focus on logic\n   - Use dependency injection for testability\n   - Create test doubles for external services\n   - Implement proper cleanup for test resources\n\n10. **Performance Impact Assessment**\n    - Measure test execution time with new tests\n    - Optimize slow tests without losing coverage\n    - Parallelize test execution where possible\n    - Balance coverage goals with execution speed\n    - Consider test categorization (fast/slow, unit/integration)\n\n11. **Coverage Quality Assessment**\n    - Ensure tests actually verify behavior, not just execution\n    - Check for meaningful assertions in tests\n    - Avoid testing implementation details\n    - Focus on testing contracts and interfaces\n    - Review test quality alongside coverage metrics\n\n12. **Framework-Specific Coverage Enhancement**\n    \n    **For Web Applications:**\n    - Test API endpoints and HTTP status codes\n    - Test form validation and user input handling\n    - Test authentication and authorization flows\n    - Test error pages and user feedback\n\n    **For Mobile Applications:**\n    - Test device-specific functionality\n    - Test different screen sizes and orientations\n    - Test offline and network connectivity scenarios\n    - Test platform-specific features\n\n    **For Backend Services:**\n    - Test database operations and transactions\n    - Test message queue processing\n    - Test caching and performance optimizations\n    - Test service integrations and API calls\n\n13. **Continuous Coverage Monitoring**\n    - Set up automated coverage reporting in CI/CD\n    - Configure coverage thresholds to prevent regression\n    - Generate coverage badges and reports\n    - Monitor coverage trends and improvements\n    - Alert on significant coverage decreases\n\n14. **Coverage Exclusion Management**\n    - Properly exclude auto-generated code\n    - Exclude third-party libraries and dependencies\n    - Document reasons for coverage exclusions\n    - Regularly review and update exclusion rules\n    - Avoid excluding code that should be tested\n\n15. **Team Coverage Goals**\n    - Set realistic coverage targets based on project needs\n    - Establish minimum coverage requirements for new code\n    - Create coverage improvement roadmap\n    - Review coverage in code reviews\n    - Celebrate coverage milestones and improvements\n\n16. **Coverage Reporting and Communication**\n    - Generate clear, actionable coverage reports\n    - Create coverage dashboards for stakeholders\n    - Document coverage improvement strategies\n    - Share coverage results with development team\n    - Integrate coverage into project health metrics\n\n17. **Mutation Testing (Advanced)**\n    - Implement mutation testing to validate test quality\n    - Identify tests that don't catch actual bugs\n    - Improve test assertions and edge case coverage\n    - Use mutation testing tools specific to your language\n    - Balance mutation testing cost with quality benefits\n\n18. **Legacy Code Coverage Strategy**\n    - Prioritize high-risk legacy code for testing\n    - Use characterization tests for complex legacy systems\n    - Refactor for testability where possible\n    - Add tests before making changes to legacy code\n    - Document known limitations and technical debt\n\n**Sample Coverage Commands:**\n\n```bash\n# JavaScript with Jest\nnpm test -- --coverage --coverage-reporters=html,text,lcov\n\n# Python with pytest\npytest --cov=src --cov-report=html --cov-report=term\n\n# Java with Maven\nmvn clean test jacoco:report\n\n# .NET Core\ndotnet test --collect:\"XPlat Code Coverage\"\n```\n\nRemember that 100% coverage is not always the goal - focus on meaningful coverage that actually improves code quality and catches bugs."
              },
              {
                "name": "/testing_plan_integration",
                "description": "I need you to create an integration testing plan for $ARGUMENTS",
                "path": "plugins/all-commands/commands/testing_plan_integration.md",
                "frontmatter": {
                  "description": "I need you to create an integration testing plan for $ARGUMENTS",
                  "category": "code-analysis-testing",
                  "argument-hint": "Specify test plan or integration type"
                },
                "content": "I need you to create an integration testing plan for $ARGUMENTS\n\nThese are integration tests and I want them to be inline in rust fashion.\n\nIf the code is difficult to test, you should suggest refactoring to make it easier to test.\n\nThink really hard about the code, the tests, and the refactoring (if applicable).\n\nWill you come up with test cases and let me review before you write the tests?\n\nFeel free to ask clarifying questions."
              },
              {
                "name": "/timeline-compressor",
                "description": "Accelerate scenario testing with rapid iteration cycles, confidence intervals, and compressed decision timelines.",
                "path": "plugins/all-commands/commands/timeline-compressor.md",
                "frontmatter": {
                  "description": "Accelerate scenario testing with rapid iteration cycles, confidence intervals, and compressed decision timelines.",
                  "category": "simulation-modeling",
                  "argument-hint": "Specify timeline and compression ratio"
                },
                "content": "# Timeline Compressor\n\nAccelerate scenario testing with rapid iteration cycles, confidence intervals, and compressed decision timelines.\n\n## Instructions\n\nYou are tasked with compressing lengthy real-world timelines into rapid simulation cycles to achieve exponential learning and decision acceleration. Follow this systematic approach: **$ARGUMENTS**\n\n### 1. Prerequisites Assessment\n\n**Critical Timeline Context Validation:**\n\n- **Original Timeline**: What real-world timeline are you trying to compress?\n- **Compression Ratio**: How much acceleration do you need (10x, 100x, 1000x)?\n- **Key Milestones**: What critical events must be preserved in compression?\n- **Decision Points**: What decisions depend on timeline outcomes?\n- **Validation Method**: How will you verify compressed timeline accuracy?\n\n**If any context is unclear, guide systematically:**\n\n```\nMissing Timeline Context:\n\"I need to understand the timeline you want to compress:\n- Timeline Type: Business cycle, product development, market adoption, competitive response?\n- Original Duration: Months, quarters, years, or decades?\n- Key Phases: What are the major stages or milestones?\n- Dependencies: What events must happen before others can start?\n\nExamples:\n- 'Product development: 18-month timeline from concept to market launch'\n- 'Market penetration: 5-year customer adoption and market share growth'\n- 'Competitive response: 2-year competitive landscape evolution'\n- 'Business transformation: 3-year digital transformation initiative'\"\n\nMissing Compression Goals:\n\"What do you want to achieve through timeline compression?\n- Decision Acceleration: Make faster strategic choices with more information\n- Risk Exploration: Test multiple scenarios before real-world commitment\n- Learning Acceleration: Gain insights from many iterations quickly\n- Option Generation: Explore alternative pathways and strategies\n- Optimization: Find best approaches through rapid experimentation\"\n\nMissing Success Criteria:\n\"How will you measure compression success?\n- Prediction Accuracy: How well does compressed timeline predict reality?\n- Decision Quality: Do faster decisions lead to better outcomes?\n- Learning Speed: How much insight per unit time invested?\n- Option Value: How many more alternatives can you explore?\"\n```\n\n### 2. Timeline Architecture Analysis\n\n**Systematically map timeline structure and dependencies:**\n\n#### Temporal Structure Mapping\n- Sequential dependencies (what must happen in order)\n- Parallel workstreams (what can happen simultaneously)\n- Critical path identification (bottlenecks and pace-setting activities)\n- Milestone definitions (key decision and evaluation points)\n- Feedback loops (how later events affect earlier assumptions)\n\n#### Time Dimension Characterization\n```\nTimeline Component Analysis:\n\nLinear Time Components:\n- Calendar Dependencies: [events tied to specific dates/seasons]\n- Sequential Processes: [step-by-step workflows that can't be parallelized]\n- Learning Curves: [skill/knowledge development that takes time]\n- Approval Cycles: [regulatory or stakeholder decision processes]\n\nCompressible Components:\n- Analysis and Planning: [information processing and decision-making]\n- Testing and Validation: [hypothesis testing and experiment cycles]\n- Market Research: [customer feedback and preference analysis]\n- Strategy Development: [scenario planning and option generation]\n\nFixed Time Components:\n- Regulatory Approvals: [compliance and legal process requirements]\n- Manufacturing Cycles: [physical production and quality processes]\n- Customer Adoption: [market education and behavior change]\n- Infrastructure Development: [physical or technical platform building]\n```\n\n#### Dependency Network Modeling\n- Cause-and-effect relationships between timeline events\n- Information flow dependencies and communication requirements\n- Resource constraint dependencies and capacity limitations\n- External dependency mapping (partners, markets, regulations)\n\n### 3. Compression Strategy Framework\n\n**Design systematic acceleration approaches:**\n\n#### Compression Methodology Selection\n```\nCompression Technique Toolkit:\n\nSimulation-Based Compression:\n- Monte Carlo simulation for probability-based acceleration\n- Agent-based modeling for complex system behavior\n- Discrete event simulation for process optimization\n- System dynamics modeling for feedback loop acceleration\n\nInformation Compression:\n- Rapid prototyping and MVP development\n- Accelerated customer research and feedback cycles\n- Competitive intelligence and market analysis acceleration\n- Expert consultation and knowledge synthesis\n\nDecision Compression:\n- Parallel option development and evaluation\n- Staged decision-making with early exit criteria\n- Rapid experimentation and A/B testing\n- Real option theory for decision timing optimization\n```\n\n#### Acceleration Factor Calibration\n- Identify maximum safe compression ratios for each timeline component\n- Validate compression accuracy through historical back-testing\n- Establish confidence intervals for compressed timeline predictions\n- Create feedback mechanisms for compression quality improvement\n\n#### Fidelity vs. Speed Trade-offs\n- High-fidelity compression for critical decisions (slower but more accurate)\n- Medium-fidelity compression for strategic planning (balanced approach)\n- Low-fidelity compression for option generation (fast but approximate)\n- Adaptive fidelity based on decision importance and available time\n\n### 4. Rapid Iteration Engine\n\n**Create systematic acceleration mechanisms:**\n\n#### Iteration Cycle Design\n```\nCompressed Timeline Iteration Framework:\n\nMicro-Cycles (Hours to Days):\n- Hypothesis generation and initial testing\n- Rapid prototyping and concept validation\n- Quick customer feedback and market pulse\n- Immediate competitive response assessment\n\nMini-Cycles (Days to Weeks):\n- Feature development and testing cycles\n- Marketing campaign testing and optimization\n- Business model validation and refinement\n- Strategic option evaluation and selection\n\nMacro-Cycles (Weeks to Months):\n- Market segment testing and expansion\n- Product-market fit validation and optimization\n- Business model scaling and operational refinement\n- Competitive positioning and market share analysis\n```\n\n#### Parallel Processing Framework\n- Simultaneous exploration of multiple timeline scenarios\n- Parallel development of alternative strategies and approaches\n- Concurrent testing of different market segments and channels\n- Parallel competitive response and counter-strategy development\n\n#### Learning Acceleration Mechanisms\n- Automated data collection and analysis for faster insights\n- Real-time feedback integration and course correction\n- Expert network activation for rapid knowledge access\n- Pattern recognition for accelerated trend identification\n\n### 5. Confidence Interval Management\n\n**Maintain decision quality during acceleration:**\n\n#### Uncertainty Quantification\n```\nConfidence Assessment Framework:\n\nHigh Confidence Predictions (80-95% accuracy):\n- Components: [timeline elements with strong historical data]\n- Time Horizons: [prediction periods with high reliability]\n- Conditions: [market/business conditions for accuracy]\n- Validation: [methods used to verify prediction quality]\n\nMedium Confidence Predictions (60-80% accuracy):\n- Components: [timeline elements with moderate data support]\n- Assumptions: [key assumptions that could affect accuracy]\n- Sensitivities: [factors that most impact prediction quality]\n- Monitoring: [early warning indicators for assumption validation]\n\nLow Confidence Predictions (40-60% accuracy):\n- Components: [timeline elements with limited data or high uncertainty]\n- Research Needs: [additional information required for improvement]\n- Alternative Scenarios: [backup plans if predictions prove incorrect]\n- Decision Thresholds: [when to seek more information vs. act on uncertainty]\n```\n\n#### Risk-Adjusted Decision Making\n- Confidence-weighted option evaluation and selection\n- Scenario probability distribution for uncertainty management\n- Real option valuation for decision timing under uncertainty\n- Adaptive strategy development for changing conditions\n\n#### Validation and Calibration\n- Continuous comparison of compressed predictions to real-world outcomes\n- Model accuracy tracking and improvement over time\n- Bias detection and correction for systematic errors\n- Expert validation and external perspective integration\n\n### 6. Scenario Multiplication Framework\n\n**Leverage compression for exponential scenario exploration:**\n\n#### Scenario Generation Strategy\n```\nCompressed Scenario Portfolio:\n\nBase Scenarios (20% of simulation time):\n- Most likely timeline development and outcomes\n- Conservative assumptions and proven approaches\n- Risk-adjusted projections and realistic expectations\n\nOptimization Scenarios (30% of simulation time):\n- Best-case timeline acceleration and outcomes\n- Aggressive but achievable improvement targets\n- Innovation and breakthrough opportunity exploration\n\nStress Test Scenarios (30% of simulation time):\n- Adverse condition timeline delays and challenges\n- Competitive pressure and market disruption impacts\n- Resource constraint and execution challenge scenarios\n\nInnovation Scenarios (20% of simulation time):\n- Breakthrough technology or market development impacts\n- Disruptive business model and competitive landscape changes\n- Unexpected opportunity and black swan event responses\n```\n\n#### Scenario Interaction Modeling\n- Cross-scenario learning and insight synthesis\n- Scenario combination and hybrid approach development\n- Scenario transition probability and trigger identification\n- Portfolio effect analysis across multiple timeline scenarios\n\n### 7. Decision Acceleration Integration\n\n**Transform compressed insights into faster real-world decisions:**\n\n#### Decision Point Optimization\n- Early decision trigger identification and validation\n- Information value analysis for decision timing optimization\n- Real option theory application for maximum flexibility\n- Decision reversal cost analysis and exit strategy planning\n\n#### Accelerated Validation Framework\n```\nRapid Validation Methodology:\n\nTier 1 Validation (Hours):\n- Expert opinion and domain knowledge validation\n- Historical pattern matching and precedent analysis\n- Logic and consistency checking for basic feasibility\n- Quick market pulse and stakeholder reaction assessment\n\nTier 2 Validation (Days):\n- Customer interview and feedback collection\n- Competitive analysis and market positioning validation\n- Financial model validation and sensitivity testing\n- Technical feasibility and resource requirement validation\n\nTier 3 Validation (Weeks):\n- Pilot testing and proof-of-concept development\n- Market research and quantitative validation\n- Stakeholder alignment and buy-in development\n- Implementation planning and risk assessment\n```\n\n#### Strategic Momentum Creation\n- Decision making rhythm and cadence optimization\n- Stakeholder alignment and communication acceleration\n- Resource allocation and execution timeline compression\n- Success metrics and feedback loop acceleration\n\n### 8. Output Generation and Synthesis\n\n**Present compressed timeline insights effectively:**\n\n```\n## Timeline Compression Analysis: [Project Name]\n\n### Compression Summary\n- Original Timeline: [duration and key phases]\n- Compression Ratio: [acceleration factor achieved]\n- Scenarios Tested: [number and types of scenarios explored]\n- Decision Acceleration: [time savings and decision quality improvement]\n\n### Key Findings\n\n#### Timeline Acceleration Opportunities:\n- High-Impact Accelerations: [specific timeline improvements]\n- Quick Wins: [immediate acceleration opportunities]\n- Strategic Accelerations: [long-term timeline optimization]\n- Resource-Dependent Accelerations: [improvements requiring investment]\n\n#### Critical Path Analysis:\n- Bottleneck Identification: [pace-limiting factors and constraints]\n- Parallel Processing Opportunities: [concurrent activity possibilities]\n- Dependency Optimization: [sequence and timing improvements]\n- Risk Mitigation Accelerations: [faster risk reduction approaches]\n\n### Scenario Outcomes Matrix\n\n| Scenario Type | Timeline Reduction | Success Probability | Key Requirements | Risk Level |\n|---------------|-------------------|-------------------|------------------|------------|\n| Conservative | 30% faster | 85% | [requirements] | Low |\n| Optimistic | 60% faster | 65% | [requirements] | Medium |\n| Aggressive | 80% faster | 40% | [requirements] | High |\n\n### Recommended Acceleration Strategy\n- Primary Approach: [recommended timeline compression strategy]\n- Acceleration Targets: [specific timeline improvements to pursue]\n- Resource Requirements: [investment needed for acceleration]\n- Risk Mitigation: [approaches to manage acceleration risks]\n- Success Metrics: [KPIs for measuring acceleration success]\n\n### Implementation Roadmap\n- Immediate Actions: [steps to begin timeline compression]\n- 30-Day Milestones: [early acceleration achievements]\n- 90-Day Objectives: [medium-term compression goals]\n- Ongoing Optimization: [continuous improvement approaches]\n\n### Confidence Assessment\n- High Confidence Elements: [timeline components with reliable acceleration]\n- Medium Confidence Elements: [components requiring validation]\n- Low Confidence Elements: [components needing more research]\n- Validation Plan: [approach to improve confidence over time]\n```\n\n### 9. Continuous Improvement and Learning\n\n**Establish ongoing compression optimization:**\n\n#### Performance Tracking\n- Compression accuracy measurement and improvement\n- Decision quality assessment and enhancement\n- Learning velocity tracking and optimization\n- Resource efficiency measurement and improvement\n\n#### Model Refinement\n- Compression algorithm improvement based on results\n- Scenario generation enhancement for better coverage\n- Validation methodology optimization for faster feedback\n- Integration process improvement for smoother execution\n\n## Usage Examples\n\n```bash\n# Product development acceleration\n/simulation:timeline-compressor Compress 18-month product development cycle to test 10 different feature prioritization strategies\n\n# Market entry timing optimization  \n/simulation:timeline-compressor Accelerate 3-year market expansion timeline to identify optimal entry sequence and timing\n\n# Business transformation acceleration\n/simulation:timeline-compressor Compress digital transformation timeline to test organizational change approaches and technology adoption\n\n# Competitive response preparation\n/simulation:timeline-compressor Accelerate competitive landscape evolution to prepare for various competitor response scenarios\n```\n\n## Quality Indicators\n\n- **Green**: 10x+ compression ratio, validated historical accuracy, multiple scenario testing\n- **Yellow**: 5-10x compression, reasonable accuracy validation, some scenario coverage\n- **Red**: <5x compression, limited validation, single scenario focus\n\n## Common Pitfalls to Avoid\n\n- Over-compression: Losing critical real-world constraints and dependencies\n- Validation blindness: Not testing compressed predictions against reality\n- Context loss: Forgetting that compression is a tool, not an end goal\n- Decision rush: Using compression to make premature decisions\n- Complexity underestimation: Assuming all timeline elements can be compressed equally\n- Single scenario fixation: Not exploring multiple compressed scenarios\n\nTransform your competitor's 3 iterations into your 300 iterations through systematic timeline compression and exponential learning acceleration."
              },
              {
                "name": "/todo",
                "description": "Manage project todos in a todos.md file with add, complete, remove, and list operations",
                "path": "plugins/all-commands/commands/todo.md",
                "frontmatter": {
                  "description": "Manage project todos in a todos.md file with add, complete, remove, and list operations",
                  "category": "project-task-management",
                  "argument-hint": "<action> [args...]",
                  "allowed-tools": "Read, Write, Edit"
                },
                "content": "# Project Todo Manager\n\nManage todos in a `todos.md` file at the root of your current project directory.\n\n## Usage Examples:\n- `/user:todo add \"Fix navigation bug\"`\n- `/user:todo add \"Fix navigation bug\" [date/time/\"tomorrow\"/\"next week\"]` an optional 2nd parameter to set a due date\n- `/user:todo complete 1`\n- `/user:todo remove 2`\n- `/user:todo list`\n- `/user:todo undo 1`\n\n## Instructions:\nParse the command arguments: $ARGUMENTS\n\nManage todos in a `todos.md` file at the root of the current project directory. When this command is invoked:\n\n1. **Determine the project root** by looking for common indicators (.git, package.json, etc.)\n2. **Locate or create** `todos.md` in the project root\n3. **Parse the command arguments** to determine the action:\n   - `add \"task description\"` - Add a new todo\n   - `add \"task description\" [tomorrow|next week|4 days|June 9|12-24-2025|etc...]` - Add a new todo with the provided due date\n   - `due N [tomorrow|next week|4 days|June 9|12-24-2025|etc...]` - Mark todo N with the due date provided\n   - `complete N` - Mark todo N as completed and move from the ##Active list to the ##Completed list\n   - `remove N` - Remove todo N entirely\n   - `undo N` - Mark completed todo N as incomplete\n   - `list [N]` or no args - Show all (or N number of) todos in a user-friendly format, with each todo numbered for reference\n   - `past due` - Show all of the tasks which are past due and still active\n   - `next` - Shows the next active task in the list, this should respect Due dates, if there are any. If not, just show the first todo in the Active list\n\n## Todo Format:\nUse this markdown format in todos.md:\n```\n# Project Todos\n\n## Active\n- [ ] Task description here | Due: MM-DD-YYYY (conditionally include HH:MM AM/PM, if specified)\n- [ ] Another task\n\n## Completed\n- [x] Completed task description | Due: MM-DD-YYYY | Completed: MM-DD-YYYY\n```\n\n## Implementation Notes:\n- Always show friendly numbered lists when displaying todos\n- Handle date parsing for common formats (natural language, ISO dates, etc.)\n- Maintain the markdown checkbox format for compatibility\n- Keep completed tasks in the file for reference but in a separate section\n- Support undo operations by moving tasks back to Active section"
              },
              {
                "name": "/troubleshooting-guide",
                "description": "Generate troubleshooting documentation",
                "path": "plugins/all-commands/commands/troubleshooting-guide.md",
                "frontmatter": {
                  "description": "Generate troubleshooting documentation",
                  "category": "documentation-changelogs",
                  "argument-hint": "1. **System Overview and Architecture**"
                },
                "content": "# Troubleshooting Guide Generator Command\n\nGenerate troubleshooting documentation\n\n## Instructions\n\nFollow this systematic approach to create troubleshooting guides: **$ARGUMENTS**\n\n1. **System Overview and Architecture**\n   - Document the system architecture and components\n   - Map out dependencies and integrations\n   - Identify critical paths and failure points\n   - Create system topology diagrams\n   - Document data flow and communication patterns\n\n2. **Common Issues Identification**\n   - Collect historical support tickets and issues\n   - Interview team members about frequent problems\n   - Analyze error logs and monitoring data\n   - Review user feedback and complaints\n   - Identify patterns in system failures\n\n3. **Troubleshooting Framework**\n   - Establish systematic diagnostic procedures\n   - Create problem isolation methodologies\n   - Document escalation paths and procedures\n   - Set up logging and monitoring checkpoints\n   - Define severity levels and response times\n\n4. **Diagnostic Tools and Commands**\n   \n   ```markdown\n   ## Essential Diagnostic Commands\n   \n   ### System Health\n   ```bash\n   # Check system resources\n   top                    # CPU and memory usage\n   df -h                 # Disk space\n   free -m               # Memory usage\n   netstat -tuln         # Network connections\n   \n   # Application logs\n   tail -f /var/log/app.log\n   journalctl -u service-name -f\n   \n   # Database connectivity\n   mysql -u user -p -e \"SELECT 1\"\n   psql -h host -U user -d db -c \"SELECT 1\"\n   ```\n   ```\n\n5. **Issue Categories and Solutions**\n\n   **Performance Issues:**\n   ```markdown\n   ### Slow Response Times\n   \n   **Symptoms:**\n   - API responses > 5 seconds\n   - User interface freezing\n   - Database timeouts\n   \n   **Diagnostic Steps:**\n   1. Check system resources (CPU, memory, disk)\n   2. Review application logs for errors\n   3. Analyze database query performance\n   4. Check network connectivity and latency\n   \n   **Common Causes:**\n   - Database connection pool exhaustion\n   - Inefficient database queries\n   - Memory leaks in application\n   - Network bandwidth limitations\n   \n   **Solutions:**\n   - Restart application services\n   - Optimize database queries\n   - Increase connection pool size\n   - Scale infrastructure resources\n   ```\n\n6. **Error Code Documentation**\n   \n   ```markdown\n   ## Error Code Reference\n   \n   ### HTTP Status Codes\n   - **500 Internal Server Error**\n     - Check application logs for stack traces\n     - Verify database connectivity\n     - Check environment variables\n   \n   - **404 Not Found**\n     - Verify URL routing configuration\n     - Check if resources exist\n     - Review API endpoint documentation\n   \n   - **503 Service Unavailable**\n     - Check service health status\n     - Verify load balancer configuration\n     - Check for maintenance mode\n   ```\n\n7. **Environment-Specific Issues**\n   - Document development environment problems\n   - Address staging/testing environment issues\n   - Cover production-specific troubleshooting\n   - Include local development setup problems\n\n8. **Database Troubleshooting**\n   \n   ```markdown\n   ### Database Connection Issues\n   \n   **Symptoms:**\n   - \"Connection refused\" errors\n   - \"Too many connections\" errors\n   - Slow query performance\n   \n   **Diagnostic Commands:**\n   ```sql\n   -- Check active connections\n   SHOW PROCESSLIST;\n   \n   -- Check database size\n   SELECT table_schema, \n          ROUND(SUM(data_length + index_length) / 1024 / 1024, 1) AS 'DB Size in MB' \n   FROM information_schema.tables \n   GROUP BY table_schema;\n   \n   -- Check slow queries\n   SHOW VARIABLES LIKE 'slow_query_log';\n   ```\n   ```\n\n9. **Network and Connectivity Issues**\n   \n   ```markdown\n   ### Network Troubleshooting\n   \n   **Basic Connectivity:**\n   ```bash\n   # Test basic connectivity\n   ping example.com\n   telnet host port\n   curl -v https://api.example.com/health\n   \n   # DNS resolution\n   nslookup example.com\n   dig example.com\n   \n   # Network routing\n   traceroute example.com\n   ```\n   \n   **SSL/TLS Issues:**\n   ```bash\n   # Check SSL certificate\n   openssl s_client -connect example.com:443\n   curl -vI https://example.com\n   ```\n   ```\n\n10. **Application-Specific Troubleshooting**\n    \n    **Memory Issues:**\n    ```markdown\n    ### Out of Memory Errors\n    \n    **Java Applications:**\n    ```bash\n    # Check heap usage\n    jstat -gc [PID]\n    jmap -dump:format=b,file=heapdump.hprof [PID]\n    \n    # Analyze heap dump\n    jhat heapdump.hprof\n    ```\n    \n    **Node.js Applications:**\n    ```bash\n    # Monitor memory usage\n    node --inspect app.js\n    # Use Chrome DevTools for memory profiling\n    ```\n    ```\n\n11. **Security and Authentication Issues**\n    \n    ```markdown\n    ### Authentication Failures\n    \n    **Symptoms:**\n    - 401 Unauthorized responses\n    - Token validation errors\n    - Session timeout issues\n    \n    **Diagnostic Steps:**\n    1. Verify credentials and tokens\n    2. Check token expiration\n    3. Validate authentication service\n    4. Review CORS configuration\n    \n    **Common Solutions:**\n    - Refresh authentication tokens\n    - Clear browser cookies/cache\n    - Verify CORS headers\n    - Check API key permissions\n    ```\n\n12. **Deployment and Configuration Issues**\n    \n    ```markdown\n    ### Deployment Failures\n    \n    **Container Issues:**\n    ```bash\n    # Check container status\n    docker ps -a\n    docker logs container-name\n    \n    # Check resource limits\n    docker stats\n    \n    # Debug container\n    docker exec -it container-name /bin/bash\n    ```\n    \n    **Kubernetes Issues:**\n    ```bash\n    # Check pod status\n    kubectl get pods\n    kubectl describe pod pod-name\n    kubectl logs pod-name\n    \n    # Check service connectivity\n    kubectl get svc\n    kubectl port-forward pod-name 8080:8080\n    ```\n    ```\n\n13. **Monitoring and Alerting Setup**\n    - Configure health checks and monitoring\n    - Set up log aggregation and analysis\n    - Implement alerting for critical issues\n    - Create dashboards for system metrics\n    - Document monitoring thresholds\n\n14. **Escalation Procedures**\n    \n    ```markdown\n    ## Escalation Matrix\n    \n    ### Severity Levels\n    \n    **Critical (P1):** System down, data loss\n    - Immediate response required\n    - Escalate to on-call engineer\n    - Notify management within 30 minutes\n    \n    **High (P2):** Major functionality impaired\n    - Response within 2 hours\n    - Escalate to senior engineer\n    - Provide hourly updates\n    \n    **Medium (P3):** Minor functionality issues\n    - Response within 8 hours\n    - Assign to appropriate team member\n    - Provide daily updates\n    ```\n\n15. **Recovery Procedures**\n    - Document system recovery steps\n    - Create data backup and restore procedures\n    - Establish rollback procedures for deployments\n    - Document disaster recovery processes\n    - Test recovery procedures regularly\n\n16. **Preventive Measures**\n    - Implement monitoring and alerting\n    - Set up automated health checks\n    - Create deployment validation procedures\n    - Establish code review processes\n    - Document maintenance procedures\n\n17. **Knowledge Base Integration**\n    - Link to relevant documentation\n    - Reference API documentation\n    - Include links to monitoring dashboards\n    - Connect to team communication channels\n    - Integrate with ticketing systems\n\n18. **Team Communication**\n    \n    ```markdown\n    ## Communication Channels\n    \n    ### Immediate Response\n    - Slack: #incidents channel\n    - Phone: On-call rotation\n    - Email: alerts@company.com\n    \n    ### Status Updates\n    - Status page: status.company.com\n    - Twitter: @company_status\n    - Internal wiki: troubleshooting section\n    ```\n\n19. **Documentation Maintenance**\n    - Regular review and updates\n    - Version control for troubleshooting guides\n    - Feedback collection from users\n    - Integration with incident post-mortems\n    - Continuous improvement processes\n\n20. **Self-Service Tools**\n    - Create diagnostic scripts and tools\n    - Build automated recovery procedures\n    - Implement self-healing systems\n    - Provide user-friendly diagnostic interfaces\n    - Create chatbot integration for common issues\n\n**Advanced Troubleshooting Techniques:**\n\n**Log Analysis:**\n```bash\n# Search for specific errors\ngrep -i \"error\" /var/log/app.log | tail -50\n\n# Analyze log patterns\nawk '{print $1}' access.log | sort | uniq -c | sort -nr\n\n# Monitor logs in real-time\ntail -f /var/log/app.log | grep -i \"exception\"\n```\n\n**Performance Profiling:**\n```bash\n# System performance\niostat -x 1\nsar -u 1 10\nvmstat 1 10\n\n# Application profiling\nstrace -p [PID]\nperf record -p [PID]\n```\n\nRemember to:\n- Keep troubleshooting guides up-to-date\n- Test all documented procedures regularly\n- Collect feedback from users and improve guides\n- Include screenshots and visual aids where helpful\n- Make guides searchable and well-organized"
              },
              {
                "name": "/ultra-think",
                "description": "Deep analysis and problem solving mode",
                "path": "plugins/all-commands/commands/ultra-think.md",
                "frontmatter": {
                  "description": "Deep analysis and problem solving mode",
                  "category": "utilities-debugging",
                  "argument-hint": "Identify all stakeholders and constraints"
                },
                "content": "# Deep Analysis and Problem Solving Mode\n\nDeep analysis and problem solving mode\n\n## Instructions\n\n1. **Initialize Ultra Think Mode**\n   - Acknowledge the request for enhanced analytical thinking\n   - Set context for deep, systematic reasoning\n   - Prepare to explore the problem space comprehensively\n\n2. **Parse the Problem or Question**\n   - Extract the core challenge from: **$ARGUMENTS**\n   - Identify all stakeholders and constraints\n   - Recognize implicit requirements and hidden complexities\n   - Question assumptions and surface unknowns\n\n3. **Multi-Dimensional Analysis**\n   Approach the problem from multiple angles:\n   \n   ### Technical Perspective\n   - Analyze technical feasibility and constraints\n   - Consider scalability, performance, and maintainability\n   - Evaluate security implications\n   - Assess technical debt and future-proofing\n   \n   ### Business Perspective\n   - Understand business value and ROI\n   - Consider time-to-market pressures\n   - Evaluate competitive advantages\n   - Assess risk vs. reward trade-offs\n   \n   ### User Perspective\n   - Analyze user needs and pain points\n   - Consider usability and accessibility\n   - Evaluate user experience implications\n   - Think about edge cases and user journeys\n   \n   ### System Perspective\n   - Consider system-wide impacts\n   - Analyze integration points\n   - Evaluate dependencies and coupling\n   - Think about emergent behaviors\n\n4. **Generate Multiple Solutions**\n   - Brainstorm at least 3-5 different approaches\n   - For each approach, consider:\n     - Pros and cons\n     - Implementation complexity\n     - Resource requirements\n     - Potential risks\n     - Long-term implications\n   - Include both conventional and creative solutions\n   - Consider hybrid approaches\n\n5. **Deep Dive Analysis**\n   For the most promising solutions:\n   - Create detailed implementation plans\n   - Identify potential pitfalls and mitigation strategies\n   - Consider phased approaches and MVPs\n   - Analyze second and third-order effects\n   - Think through failure modes and recovery\n\n6. **Cross-Domain Thinking**\n   - Draw parallels from other industries or domains\n   - Apply design patterns from different contexts\n   - Consider biological or natural system analogies\n   - Look for innovative combinations of existing solutions\n\n7. **Challenge and Refine**\n   - Play devil's advocate with each solution\n   - Identify weaknesses and blind spots\n   - Consider \"what if\" scenarios\n   - Stress-test assumptions\n   - Look for unintended consequences\n\n8. **Synthesize Insights**\n   - Combine insights from all perspectives\n   - Identify key decision factors\n   - Highlight critical trade-offs\n   - Summarize innovative discoveries\n   - Present a nuanced view of the problem space\n\n9. **Provide Structured Recommendations**\n   Present findings in a clear structure:\n   ```\n   ## Problem Analysis\n   - Core challenge\n   - Key constraints\n   - Critical success factors\n   \n   ## Solution Options\n   ### Option 1: [Name]\n   - Description\n   - Pros/Cons\n   - Implementation approach\n   - Risk assessment\n   \n   ### Option 2: [Name]\n   [Similar structure]\n   \n   ## Recommendation\n   - Recommended approach\n   - Rationale\n   - Implementation roadmap\n   - Success metrics\n   - Risk mitigation plan\n   \n   ## Alternative Perspectives\n   - Contrarian view\n   - Future considerations\n   - Areas for further research\n   ```\n\n10. **Meta-Analysis**\n    - Reflect on the thinking process itself\n    - Identify areas of uncertainty\n    - Acknowledge biases or limitations\n    - Suggest additional expertise needed\n    - Provide confidence levels for recommendations\n\n## Usage Examples\n\n```bash\n# Architectural decision\n/project:ultra-think Should we migrate to microservices or improve our monolith?\n\n# Complex problem solving\n/project:ultra-think How do we scale our system to handle 10x traffic while reducing costs?\n\n# Strategic planning\n/project:ultra-think What technology stack should we choose for our next-gen platform?\n\n# Design challenge\n/project:ultra-think How can we improve our API to be more developer-friendly while maintaining backward compatibility?\n```\n\n## Key Principles\n\n- **First Principles Thinking**: Break down to fundamental truths\n- **Systems Thinking**: Consider interconnections and feedback loops\n- **Probabilistic Thinking**: Work with uncertainties and ranges\n- **Inversion**: Consider what to avoid, not just what to do\n- **Second-Order Thinking**: Consider consequences of consequences\n\n## Output Expectations\n\n- Comprehensive analysis (typically 2-4 pages of insights)\n- Multiple viable solutions with trade-offs\n- Clear reasoning chains\n- Acknowledgment of uncertainties\n- Actionable recommendations\n- Novel insights or perspectives"
              },
              {
                "name": "/unity-project-setup",
                "description": "Sets up a professional Unity project with industry-standard structure and configurations",
                "path": "plugins/all-commands/commands/unity-project-setup.md",
                "frontmatter": {
                  "description": "Sets up a professional Unity project with industry-standard structure and configurations",
                  "category": "game-development",
                  "allowed-tools": "Edit, Write"
                },
                "content": "# Unity Project Setup Command\n\nSets up a professional Unity project with industry-standard structure and configurations.\n\n## What it creates:\n\n### Project Structure\n```\nAssets/\n _Project/\n    Scripts/\n       Managers/\n       Player/\n       UI/\n       Gameplay/\n       Utilities/\n    Art/\n       Textures/\n       Materials/\n       Models/\n       Animations/\n    Audio/\n       Music/\n       SFX/\n       Voice/\n    Prefabs/\n       Characters/\n       Environment/\n       UI/\n       Effects/\n    Scenes/\n       Development/\n       Production/\n       Testing/\n    Settings/\n       Input/\n       Rendering/\n       Audio/\n    Resources/\n Plugins/\n StreamingAssets/\n Editor/\n     Scripts/\n     Resources/\n```\n\n### Essential Packages\n- Universal Render Pipeline (URP)\n- Input System\n- Cinemachine\n- ProBuilder\n- Timeline\n- Addressables\n- Unity Analytics\n- Version Control (if available)\n\n### Project Settings\n- Optimized quality settings for target platforms\n- Input system configuration\n- Physics settings\n- Time and rendering configurations\n- Build settings for multiple platforms\n\n### Development Tools\n- Code formatting rules (.editorconfig)\n- Git configuration with Unity-optimized .gitignore\n- Assembly definition files for better compilation\n- Custom editor scripts for workflow improvement\n\n### Version Control Setup\n- Git repository initialization\n- Unity-specific .gitignore\n- LFS configuration for large assets\n- Branching strategy documentation\n\n## Usage:\n\n```bash\nnpx claude-code-templates@latest --command unity-project-setup\n```\n\n## Interactive Options:\n\n1. **Project Type Selection**\n   - 2D Game\n   - 3D Game\n   - Mobile Game\n   - VR/AR Game\n   - Hybrid (2D/3D)\n\n2. **Target Platforms**\n   - PC (Windows/Mac/Linux)\n   - Mobile (iOS/Android)\n   - Console (PlayStation/Xbox/Nintendo)\n   - WebGL\n   - VR (Oculus/SteamVR)\n\n3. **Version Control**\n   - Git\n   - Plastic SCM\n   - Perforce\n   - None\n\n4. **Additional Packages**\n   - TextMeshPro\n   - Post Processing\n   - Unity Ads\n   - Unity Analytics\n   - Unity Cloud Build\n   - Custom package selection\n\n## Generated Files:\n\n### Core Scripts\n- `GameManager.cs` - Main game controller\n- `SceneLoader.cs` - Scene management system\n- `AudioManager.cs` - Audio system controller\n- `InputManager.cs` - Input handling system\n- `UIManager.cs` - UI system manager\n- `SaveSystem.cs` - Save/load functionality\n\n### Editor Tools\n- `ProjectSetupWindow.cs` - Custom editor window\n- `SceneQuickStart.cs` - Scene setup automation\n- `AssetValidator.cs` - Asset validation tools\n- `BuildAutomation.cs` - Build pipeline helpers\n\n### Configuration Files\n- `ProjectSettings.asset` - Optimized project settings\n- `QualitySettings.asset` - Multi-platform quality tiers\n- `InputActions.inputactions` - Input system configuration\n- `AssemblyDefinitions` - Modular compilation setup\n\n### Documentation\n- `README.md` - Project overview and setup instructions\n- `CONTRIBUTING.md` - Development guidelines\n- `CHANGELOG.md` - Version history template\n- `API_REFERENCE.md` - Code documentation template\n\n## Post-Setup Checklist:\n\n- [ ] Review and adjust quality settings for target platforms\n- [ ] Configure input actions for your game controls\n- [ ] Set up build configurations for all target platforms\n- [ ] Review folder structure and rename as needed\n- [ ] Configure version control and make initial commit\n- [ ] Set up continuous integration if required\n- [ ] Configure analytics and crash reporting\n- [ ] Review and customize coding standards\n\n## Platform-Specific Configurations:\n\n### Mobile\n- Touch input configuration\n- Performance optimization settings\n- Battery usage optimization\n- App store submission setup\n\n### PC\n- Multi-resolution support\n- Keyboard/mouse input setup\n- Graphics options menu template\n- Windows/Mac/Linux build configs\n\n### Console\n- Platform-specific input mapping\n- Achievement/trophy integration setup\n- Online services configuration\n- Certification requirement templates\n\nThis command creates a production-ready Unity project structure that scales from prototype to shipped game, following industry best practices and Unity's recommended patterns."
              },
              {
                "name": "/update-branch-name",
                "description": "Update current git branch name based on analysis of changes made",
                "path": "plugins/all-commands/commands/update-branch-name.md",
                "frontmatter": {
                  "description": "Update current git branch name based on analysis of changes made",
                  "category": "version-control-git",
                  "allowed-tools": "Bash(git *)"
                },
                "content": "# Update Branch Name\n\nFollow these steps to update the current branch name:\n\n1. Check differences between current branch and main branch HEAD using `git diff main...HEAD`\n2. Analyze the changed files to understand what work is being done\n3. Determine an appropriate descriptive branch name based on the changes\n4. Update the current branch name using `git branch -m [new-branch-name]`\n5. Verify the branch name was updated with `git branch`"
              },
              {
                "name": "/update-docs",
                "description": "Update implementation documentation including specs, status, and best practices",
                "path": "plugins/all-commands/commands/update-docs.md",
                "frontmatter": {
                  "description": "Update implementation documentation including specs, status, and best practices",
                  "category": "documentation-changelogs",
                  "allowed-tools": "Read, Edit, Write"
                },
                "content": "# Documentation Update Command: Update Implementation Documentation\n\n## Documentation Analysis\n\n1. Review current documentation status:\n   - Check `specs/implementation_status.md` for overall project status\n   - Review implemented phase document (`specs/phase{N}_implementation_plan.md`)\n   - Review `specs/flutter_structurizr_implementation_spec.md` and `specs/flutter_structurizr_implementation_spec_updated.md`\n   - Review `specs/testing_plan.md` to ensure it is current given recent test passes, failures, and changes\n   - Examine `CLAUDE.md` and `README.md` for project-wide documentation\n   - Check for and document any new lessons learned or best practices in CLAUDE.md\n\n2. Analyze implementation and testing results:\n   - Review what was implemented in the last phase\n   - Review testing results and coverage\n   - Identify new best practices discovered during implementation\n   - Note any implementation challenges and solutions\n   - Cross-reference updated documentation with recent implementation and test results to ensure accuracy\n\n## Documentation Updates\n\n1. Update phase implementation document:\n   - Mark completed tasks with  status\n   - Update implementation percentages\n   - Add detailed notes on implementation approach\n   - Document any deviations from original plan with justification\n   - Add new sections if needed (lessons learned, best practices)\n   - Document specific implementation details for complex components\n   - Include a summary of any new troubleshooting tips or workflow improvements discovered during the phase\n\n2. Update implementation status document:\n   - Update phase completion percentages\n   - Add or update implementation status for components\n   - Add notes on implementation approach and decisions\n   - Document best practices discovered during implementation\n   - Note any challenges overcome and solutions implemented\n\n3. Update implementation specification documents:\n   - Mark completed items with  or strikethrough but preserve original requirements\n   - Add notes on implementation details where appropriate\n   - Add references to implemented files and classes\n   - Update any implementation guidance based on experience\n\n4. Update CLAUDE.md and README.md if necessary:\n   - Add new best practices\n   - Update project status\n   - Add new implementation guidance\n   - Document known issues or limitations\n   - Update usage examples to include new"
              },
              {
                "name": "/use-stepper",
                "description": "Use structured stepper approach for problem-solving and project development",
                "path": "plugins/all-commands/commands/use-stepper.md",
                "frontmatter": {
                  "description": "Use structured stepper approach for problem-solving and project development",
                  "category": "miscellaneous"
                },
                "content": "<Stepper>\n\n1. **Identify the Problem**\n1. **Plan Your Project**\n1. **Build Your Solution**\n1. **Test and Deploy**\n\n</Stepper>"
              },
              {
                "name": "/write-tests",
                "description": "Write unit and integration tests",
                "path": "plugins/all-commands/commands/write-tests.md",
                "frontmatter": {
                  "description": "Write unit and integration tests",
                  "category": "code-analysis-testing",
                  "argument-hint": "1. **Test Framework Detection**",
                  "allowed-tools": "Write"
                },
                "content": "# Write Tests Command\n\nWrite unit and integration tests\n\n## Instructions\n\nFollow this systematic approach to write effective tests: **$ARGUMENTS**\n\n1. **Test Framework Detection**\n   - Identify the testing framework in use (Jest, Mocha, PyTest, RSpec, etc.)\n   - Review existing test structure and conventions\n   - Check test configuration files and setup\n   - Understand project-specific testing patterns\n\n2. **Code Analysis for Testing**\n   - Analyze the code that needs testing\n   - Identify public interfaces and critical business logic\n   - Map out dependencies and external interactions\n   - Understand error conditions and edge cases\n\n3. **Test Strategy Planning**\n   - Determine test levels needed:\n     - Unit tests for individual functions/methods\n     - Integration tests for component interactions\n     - End-to-end tests for user workflows\n   - Plan test coverage goals and priorities\n   - Identify mock and stub requirements\n\n4. **Unit Test Implementation**\n   - Test individual functions and methods in isolation\n   - Cover happy path scenarios first\n   - Test edge cases and boundary conditions\n   - Test error conditions and exception handling\n   - Use proper assertions and expectations\n\n5. **Test Structure and Organization**\n   - Follow the AAA pattern (Arrange, Act, Assert)\n   - Use descriptive test names that explain the scenario\n   - Group related tests using test suites/describe blocks\n   - Keep tests focused and atomic\n\n6. **Mocking and Stubbing**\n   - Mock external dependencies and services\n   - Stub complex operations for unit tests\n   - Use proper isolation for reliable tests\n   - Avoid over-mocking that makes tests brittle\n\n7. **Data Setup and Teardown**\n   - Create test fixtures and sample data\n   - Set up and tear down test environments cleanly\n   - Use factories or builders for complex test data\n   - Ensure tests don't interfere with each other\n\n8. **Integration Test Writing**\n   - Test component interactions and data flow\n   - Test API endpoints with various scenarios\n   - Test database operations and transactions\n   - Test external service integrations\n\n9. **Error and Exception Testing**\n   - Test all error conditions and exception paths\n   - Verify proper error messages and codes\n   - Test error recovery and fallback mechanisms\n   - Test validation and security scenarios\n\n10. **Performance and Load Testing**\n    - Add performance tests for critical operations\n    - Test under different load conditions\n    - Verify memory usage and resource cleanup\n    - Test timeout and rate limiting scenarios\n\n11. **Security Testing**\n    - Test authentication and authorization\n    - Test input validation and sanitization\n    - Test for common security vulnerabilities\n    - Test access control and permissions\n\n12. **Accessibility Testing (for UI)**\n    - Test keyboard navigation and screen readers\n    - Test color contrast and visual accessibility\n    - Test ARIA attributes and semantic markup\n    - Test with assistive technology simulations\n\n13. **Cross-Platform Testing**\n    - Test on different operating systems\n    - Test on different browsers (for web apps)\n    - Test on different device sizes and resolutions\n    - Test with different versions of dependencies\n\n14. **Test Utilities and Helpers**\n    - Create reusable test utilities and helpers\n    - Build test data factories and builders\n    - Create custom matchers and assertions\n    - Set up common test setup and teardown functions\n\n15. **Snapshot and Visual Testing**\n    - Use snapshot testing for UI components\n    - Implement visual regression testing\n    - Test rendered output and markup\n    - Version control snapshots properly\n\n16. **Async Testing**\n    - Test asynchronous operations properly\n    - Use appropriate async testing patterns\n    - Test promise resolution and rejection\n    - Test callback and event-driven code\n\n17. **Test Documentation**\n    - Document complex test scenarios and reasoning\n    - Add comments for non-obvious test logic\n    - Create test documentation for team reference\n    - Document test data requirements and setup\n\n18. **Test Maintenance**\n    - Keep tests up to date with code changes\n    - Refactor tests when code is refactored\n    - Remove obsolete tests and update assertions\n    - Monitor and fix flaky tests\n\n**Framework-Specific Guidelines:**\n\n**Jest/JavaScript:**\n```javascript\ndescribe('ComponentName', () => {\n  beforeEach(() => {\n    // Setup\n  });\n\n  it('should handle valid input correctly', () => {\n    // Arrange\n    const input = 'test';\n    // Act\n    const result = functionToTest(input);\n    // Assert\n    expect(result).toBe(expectedValue);\n  });\n});\n```\n\n**PyTest/Python:**\n```python\nclass TestClassName:\n    def setup_method(self):\n        # Setup\n        pass\n\n    def test_should_handle_valid_input(self):\n        # Arrange\n        input_data = \"test\"\n        # Act\n        result = function_to_test(input_data)\n        # Assert\n        assert result == expected_value\n```\n\n**RSpec/Ruby:**\n```ruby\nRSpec.describe ClassName do\n  describe '#method_name' do\n    it 'handles valid input correctly' do\n      # Arrange\n      input = 'test'\n      # Act\n      result = subject.method_name(input)\n      # Assert\n      expect(result).to eq(expected_value)\n    end\n  end\nend\n```\n\nRemember to prioritize testing critical business logic and user-facing functionality first, then expand coverage to supporting code."
              }
            ],
            "skills": []
          },
          {
            "name": "all-hooks",
            "description": "Complete collection of 28 automation hooks for event-driven workflows",
            "source": "./plugins/all-hooks",
            "category": "hooks",
            "version": "1.0.0",
            "author": {
              "name": "BuildWithClaude Community",
              "url": "https://github.com/davepoon/buildwithclaude"
            },
            "install_commands": [
              "/plugin marketplace add davepoon/buildwithclaude",
              "/plugin install all-hooks@buildwithclaude"
            ],
            "signals": {
              "stars": 2229,
              "forks": 239,
              "pushed_at": "2026-01-12T10:19:01Z",
              "created_at": "2025-07-25T02:26:45Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": []
          },
          {
            "name": "all-skills",
            "description": "Complete collection of 26 Claude Code skills for document processing, development, business productivity, and creative tasks",
            "source": "./plugins/all-skills",
            "category": "skills",
            "version": "1.0.0",
            "author": {
              "name": "BuildWithClaude Community",
              "url": "https://github.com/davepoon/buildwithclaude"
            },
            "install_commands": [
              "/plugin marketplace add davepoon/buildwithclaude",
              "/plugin install all-skills@buildwithclaude"
            ],
            "signals": {
              "stars": 2229,
              "forks": 239,
              "pushed_at": "2026-01-12T10:19:01Z",
              "created_at": "2025-07-25T02:26:45Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": [
              {
                "name": "artifacts-builder",
                "description": "Suite of tools for creating elaborate, multi-component claude.ai HTML artifacts using modern frontend web technologies (React, Tailwind CSS, shadcn/ui). Use for complex artifacts requiring state management, routing, or shadcn/ui components - not for simple single-file HTML/JSX artifacts.",
                "path": "plugins/all-skills/skills/artifacts-builder/SKILL.md",
                "frontmatter": {
                  "name": "artifacts-builder",
                  "category": "document-processing",
                  "description": "Suite of tools for creating elaborate, multi-component claude.ai HTML artifacts using modern frontend web technologies (React, Tailwind CSS, shadcn/ui). Use for complex artifacts requiring state management, routing, or shadcn/ui components - not for simple single-file HTML/JSX artifacts.",
                  "license": "Complete terms in LICENSE.txt"
                },
                "content": "# Artifacts Builder\n\nTo build powerful frontend claude.ai artifacts, follow these steps:\n1. Initialize the frontend repo using `scripts/init-artifact.sh`\n2. Develop your artifact by editing the generated code\n3. Bundle all code into a single HTML file using `scripts/bundle-artifact.sh`\n4. Display artifact to user\n5. (Optional) Test the artifact\n\n**Stack**: React 18 + TypeScript + Vite + Parcel (bundling) + Tailwind CSS + shadcn/ui\n\n## Design & Style Guidelines\n\nVERY IMPORTANT: To avoid what is often referred to as \"AI slop\", avoid using excessive centered layouts, purple gradients, uniform rounded corners, and Inter font.\n\n## Quick Start\n\n### Step 1: Initialize Project\n\nRun the initialization script to create a new React project:\n```bash\nbash scripts/init-artifact.sh <project-name>\ncd <project-name>\n```\n\nThis creates a fully configured project with:\n-  React + TypeScript (via Vite)\n-  Tailwind CSS 3.4.1 with shadcn/ui theming system\n-  Path aliases (`@/`) configured\n-  40+ shadcn/ui components pre-installed\n-  All Radix UI dependencies included\n-  Parcel configured for bundling (via .parcelrc)\n-  Node 18+ compatibility (auto-detects and pins Vite version)\n\n### Step 2: Develop Your Artifact\n\nTo build the artifact, edit the generated files. See **Common Development Tasks** below for guidance.\n\n### Step 3: Bundle to Single HTML File\n\nTo bundle the React app into a single HTML artifact:\n```bash\nbash scripts/bundle-artifact.sh\n```\n\nThis creates `bundle.html` - a self-contained artifact with all JavaScript, CSS, and dependencies inlined. This file can be directly shared in Claude conversations as an artifact.\n\n**Requirements**: Your project must have an `index.html` in the root directory.\n\n**What the script does**:\n- Installs bundling dependencies (parcel, @parcel/config-default, parcel-resolver-tspaths, html-inline)\n- Creates `.parcelrc` config with path alias support\n- Builds with Parcel (no source maps)\n- Inlines all assets into single HTML using html-inline\n\n### Step 4: Share Artifact with User\n\nFinally, share the bundled HTML file in conversation with the user so they can view it as an artifact.\n\n### Step 5: Testing/Visualizing the Artifact (Optional)\n\nNote: This is a completely optional step. Only perform if necessary or requested.\n\nTo test/visualize the artifact, use available tools (including other Skills or built-in tools like Playwright or Puppeteer). In general, avoid testing the artifact upfront as it adds latency between the request and when the finished artifact can be seen. Test later, after presenting the artifact, if requested or if issues arise.\n\n## Reference\n\n- **shadcn/ui components**: https://ui.shadcn.com/docs/components"
              },
              {
                "name": "brand-guidelines",
                "description": "Applies Anthropic's official brand colors and typography to any sort of artifact that may benefit from having Anthropic's look-and-feel. Use it when brand colors or style guidelines, visual formatting, or company design standards apply.",
                "path": "plugins/all-skills/skills/brand-guidelines/SKILL.md",
                "frontmatter": {
                  "name": "brand-guidelines",
                  "category": "document-processing",
                  "description": "Applies Anthropic's official brand colors and typography to any sort of artifact that may benefit from having Anthropic's look-and-feel. Use it when brand colors or style guidelines, visual formatting, or company design standards apply.",
                  "license": "Complete terms in LICENSE.txt"
                },
                "content": "# Anthropic Brand Styling\n\n## Overview\n\nTo access Anthropic's official brand identity and style resources, use this skill.\n\n**Keywords**: branding, corporate identity, visual identity, post-processing, styling, brand colors, typography, Anthropic brand, visual formatting, visual design\n\n## Brand Guidelines\n\n### Colors\n\n**Main Colors:**\n\n- Dark: `#141413` - Primary text and dark backgrounds\n- Light: `#faf9f5` - Light backgrounds and text on dark\n- Mid Gray: `#b0aea5` - Secondary elements\n- Light Gray: `#e8e6dc` - Subtle backgrounds\n\n**Accent Colors:**\n\n- Orange: `#d97757` - Primary accent\n- Blue: `#6a9bcc` - Secondary accent\n- Green: `#788c5d` - Tertiary accent\n\n### Typography\n\n- **Headings**: Poppins (with Arial fallback)\n- **Body Text**: Lora (with Georgia fallback)\n- **Note**: Fonts should be pre-installed in your environment for best results\n\n## Features\n\n### Smart Font Application\n\n- Applies Poppins font to headings (24pt and larger)\n- Applies Lora font to body text\n- Automatically falls back to Arial/Georgia if custom fonts unavailable\n- Preserves readability across all systems\n\n### Text Styling\n\n- Headings (24pt+): Poppins font\n- Body text: Lora font\n- Smart color selection based on background\n- Preserves text hierarchy and formatting\n\n### Shape and Accent Colors\n\n- Non-text shapes use accent colors\n- Cycles through orange, blue, and green accents\n- Maintains visual interest while staying on-brand\n\n## Technical Details\n\n### Font Management\n\n- Uses system-installed Poppins and Lora fonts when available\n- Provides automatic fallback to Arial (headings) and Georgia (body)\n- No font installation required - works with existing system fonts\n- For best results, pre-install Poppins and Lora fonts in your environment\n\n### Color Application\n\n- Uses RGB color values for precise brand matching\n- Applied via python-pptx's RGBColor class\n- Maintains color fidelity across different systems"
              },
              {
                "name": "canvas-design",
                "description": "Create beautiful visual art in .png and .pdf documents using design philosophy. You should use this skill when the user asks to create a poster, piece of art, design, or other static piece. Create original visual designs, never copying existing artists' work to avoid copyright violations.",
                "path": "plugins/all-skills/skills/canvas-design/SKILL.md",
                "frontmatter": {
                  "name": "canvas-design",
                  "category": "document-processing",
                  "description": "Create beautiful visual art in .png and .pdf documents using design philosophy. You should use this skill when the user asks to create a poster, piece of art, design, or other static piece. Create original visual designs, never copying existing artists' work to avoid copyright violations.",
                  "license": "Complete terms in LICENSE.txt"
                },
                "content": "These are instructions for creating design philosophies - aesthetic movements that are then EXPRESSED VISUALLY. Output only .md files, .pdf files, and .png files.\n\nComplete this in two steps:\n1. Design Philosophy Creation (.md file)\n2. Express by creating it on a canvas (.pdf file or .png file)\n\nFirst, undertake this task:\n\n## DESIGN PHILOSOPHY CREATION\n\nTo begin, create a VISUAL PHILOSOPHY (not layouts or templates) that will be interpreted through:\n- Form, space, color, composition\n- Images, graphics, shapes, patterns\n- Minimal text as visual accent\n\n### THE CRITICAL UNDERSTANDING\n- What is received: Some subtle input or instructions by the user that should be taken into account, but used as a foundation; it should not constrain creative freedom.\n- What is created: A design philosophy/aesthetic movement.\n- What happens next: Then, the same version receives the philosophy and EXPRESSES IT VISUALLY - creating artifacts that are 90% visual design, 10% essential text.\n\nConsider this approach:\n- Write a manifesto for an art movement\n- The next phase involves making the artwork\n\nThe philosophy must emphasize: Visual expression. Spatial communication. Artistic interpretation. Minimal words.\n\n### HOW TO GENERATE A VISUAL PHILOSOPHY\n\n**Name the movement** (1-2 words): \"Brutalist Joy\" / \"Chromatic Silence\" / \"Metabolist Dreams\"\n\n**Articulate the philosophy** (4-6 paragraphs - concise but complete):\n\nTo capture the VISUAL essence, express how the philosophy manifests through:\n- Space and form\n- Color and material\n- Scale and rhythm\n- Composition and balance\n- Visual hierarchy\n\n**CRITICAL GUIDELINES:**\n- **Avoid redundancy**: Each design aspect should be mentioned once. Avoid repeating points about color theory, spatial relationships, or typographic principles unless adding new depth.\n- **Emphasize craftsmanship REPEATEDLY**: The philosophy MUST stress multiple times that the final work should appear as though it took countless hours to create, was labored over with care, and comes from someone at the absolute top of their field. This framing is essential - repeat phrases like \"meticulously crafted,\" \"the product of deep expertise,\" \"painstaking attention,\" \"master-level execution.\"\n- **Leave creative space**: Remain specific about the aesthetic direction, but concise enough that the next Claude has room to make interpretive choices also at a extremely high level of craftmanship.\n\nThe philosophy must guide the next version to express ideas VISUALLY, not through text. Information lives in design, not paragraphs.\n\n### PHILOSOPHY EXAMPLES\n\n**\"Concrete Poetry\"**\nPhilosophy: Communication through monumental form and bold geometry.\nVisual expression: Massive color blocks, sculptural typography (huge single words, tiny labels), Brutalist spatial divisions, Polish poster energy meets Le Corbusier. Ideas expressed through visual weight and spatial tension, not explanation. Text as rare, powerful gesture - never paragraphs, only essential words integrated into the visual architecture. Every element placed with the precision of a master craftsman.\n\n**\"Chromatic Language\"**\nPhilosophy: Color as the primary information system.\nVisual expression: Geometric precision where color zones create meaning. Typography minimal - small sans-serif labels letting chromatic fields communicate. Think Josef Albers' interaction meets data visualization. Information encoded spatially and chromatically. Words only to anchor what color already shows. The result of painstaking chromatic calibration.\n\n**\"Analog Meditation\"**\nPhilosophy: Quiet visual contemplation through texture and breathing room.\nVisual expression: Paper grain, ink bleeds, vast negative space. Photography and illustration dominate. Typography whispered (small, restrained, serving the visual). Japanese photobook aesthetic. Images breathe across pages. Text appears sparingly - short phrases, never explanatory blocks. Each composition balanced with the care of a meditation practice.\n\n**\"Organic Systems\"**\nPhilosophy: Natural clustering and modular growth patterns.\nVisual expression: Rounded forms, organic arrangements, color from nature through architecture. Information shown through visual diagrams, spatial relationships, iconography. Text only for key labels floating in space. The composition tells the story through expert spatial orchestration.\n\n**\"Geometric Silence\"**\nPhilosophy: Pure order and restraint.\nVisual expression: Grid-based precision, bold photography or stark graphics, dramatic negative space. Typography precise but minimal - small essential text, large quiet zones. Swiss formalism meets Brutalist material honesty. Structure communicates, not words. Every alignment the work of countless refinements.\n\n*These are condensed examples. The actual design philosophy should be 4-6 substantial paragraphs.*\n\n### ESSENTIAL PRINCIPLES\n- **VISUAL PHILOSOPHY**: Create an aesthetic worldview to be expressed through design\n- **MINIMAL TEXT**: Always emphasize that text is sparse, essential-only, integrated as visual element - never lengthy\n- **SPATIAL EXPRESSION**: Ideas communicate through space, form, color, composition - not paragraphs\n- **ARTISTIC FREEDOM**: The next Claude interprets the philosophy visually - provide creative room\n- **PURE DESIGN**: This is about making ART OBJECTS, not documents with decoration\n- **EXPERT CRAFTSMANSHIP**: Repeatedly emphasize the final work must look meticulously crafted, labored over with care, the product of countless hours by someone at the top of their field\n\n**The design philosophy should be 4-6 paragraphs long.** Fill it with poetic design philosophy that brings together the core vision. Avoid repeating the same points. Keep the design philosophy generic without mentioning the intention of the art, as if it can be used wherever. Output the design philosophy as a .md file.\n\n---\n\n## DEDUCING THE SUBTLE REFERENCE\n\n**CRITICAL STEP**: Before creating the canvas, identify the subtle conceptual thread from the original request.\n\n**THE ESSENTIAL PRINCIPLE**:\nThe topic is a **subtle, niche reference embedded within the art itself** - not always literal, always sophisticated. Someone familiar with the subject should feel it intuitively, while others simply experience a masterful abstract composition. The design philosophy provides the aesthetic language. The deduced topic provides the soul - the quiet conceptual DNA woven invisibly into form, color, and composition.\n\nThis is **VERY IMPORTANT**: The reference must be refined so it enhances the work's depth without announcing itself. Think like a jazz musician quoting another song - only those who know will catch it, but everyone appreciates the music.\n\n---\n\n## CANVAS CREATION\n\nWith both the philosophy and the conceptual framework established, express it on a canvas. Take a moment to gather thoughts and clear the mind. Use the design philosophy created and the instructions below to craft a masterpiece, embodying all aspects of the philosophy with expert craftsmanship.\n\n**IMPORTANT**: For any type of content, even if the user requests something for a movie/game/book, the approach should still be sophisticated. Never lose sight of the idea that this should be art, not something that's cartoony or amateur.\n\nTo create museum or magazine quality work, use the design philosophy as the foundation. Create one single page, highly visual, design-forward PDF or PNG output (unless asked for more pages). Generally use repeating patterns and perfect shapes. Treat the abstract philosophical design as if it were a scientific bible, borrowing the visual language of systematic observationdense accumulation of marks, repeated elements, or layered patterns that build meaning through patient repetition and reward sustained viewing. Add sparse, clinical typography and systematic reference markers that suggest this could be a diagram from an imaginary discipline, treating the invisible subject with the same reverence typically reserved for documenting observable phenomena. Anchor the piece with simple phrase(s) or details positioned subtly, using a limited color palette that feels intentional and cohesive. Embrace the paradox of using analytical visual language to express ideas about human experience: the result should feel like an artifact that proves something ephemeral can be studied, mapped, and understood through careful attention. This is true art. \n\n**Text as a contextual element**: Text is always minimal and visual-first, but let context guide whether that means whisper-quiet labels or bold typographic gestures. A punk venue poster might have larger, more aggressive type than a minimalist ceramics studio identity. Most of the time, font should be thin. All use of fonts must be design-forward and prioritize visual communication. Regardless of text scale, nothing falls off the page and nothing overlaps. Every element must be contained within the canvas boundaries with proper margins. Check carefully that all text, graphics, and visual elements have breathing room and clear separation. This is non-negotiable for professional execution. **IMPORTANT: Use different fonts if writing text. Search the `./canvas-fonts` directory. Regardless of approach, sophistication is non-negotiable.**\n\nDownload and use whatever fonts are needed to make this a reality. Get creative by making the typography actually part of the art itself -- if the art is abstract, bring the font onto the canvas, not typeset digitally.\n\nTo push boundaries, follow design instinct/intuition while using the philosophy as a guiding principle. Embrace ultimate design freedom and choice. Push aesthetics and design to the frontier. \n\n**CRITICAL**: To achieve human-crafted quality (not AI-generated), create work that looks like it took countless hours. Make it appear as though someone at the absolute top of their field labored over every detail with painstaking care. Ensure the composition, spacing, color choices, typography - everything screams expert-level craftsmanship. Double-check that nothing overlaps, formatting is flawless, every detail perfect. Create something that could be shown to people to prove expertise and rank as undeniably impressive.\n\nOutput the final result as a single, downloadable .pdf or .png file, alongside the design philosophy used as a .md file.\n\n---\n\n## FINAL STEP\n\n**IMPORTANT**: The user ALREADY said \"It isn't perfect enough. It must be pristine, a masterpiece if craftsmanship, as if it were about to be displayed in a museum.\"\n\n**CRITICAL**: To refine the work, avoid adding more graphics; instead refine what has been created and make it extremely crisp, respecting the design philosophy and the principles of minimalism entirely. Rather than adding a fun filter or refactoring a font, consider how to make the existing composition more cohesive with the art. If the instinct is to call a new function or draw a new shape, STOP and instead ask: \"How can I make what's already here more of a piece of art?\"\n\nTake a second pass. Go back to the code and refine/polish further to make this a philosophically designed masterpiece.\n\n## MULTI-PAGE OPTION\n\nTo create additional pages when requested, create more creative pages along the same lines as the design philosophy but distinctly different as well. Bundle those pages in the same .pdf or many .pngs. Treat the first page as just a single page in a whole coffee table book waiting to be filled. Make the next pages unique twists and memories of the original. Have them almost tell a story in a very tasteful way. Exercise full creative freedom."
              },
              {
                "name": "changelog-generator",
                "description": "Automatically creates user-facing changelogs from git commits by analyzing commit history, categorizing changes, and transforming technical commits into clear, customer-friendly release notes. Turns hours of manual changelog writing into minutes of automated generation.",
                "path": "plugins/all-skills/skills/changelog-generator/SKILL.md",
                "frontmatter": {
                  "name": "changelog-generator",
                  "category": "document-processing",
                  "description": "Automatically creates user-facing changelogs from git commits by analyzing commit history, categorizing changes, and transforming technical commits into clear, customer-friendly release notes. Turns hours of manual changelog writing into minutes of automated generation."
                },
                "content": "# Changelog Generator\n\nThis skill transforms technical git commits into polished, user-friendly changelogs that your customers and users will actually understand and appreciate.\n\n## When to Use This Skill\n\n- Preparing release notes for a new version\n- Creating weekly or monthly product update summaries\n- Documenting changes for customers\n- Writing changelog entries for app store submissions\n- Generating update notifications\n- Creating internal release documentation\n- Maintaining a public changelog/product updates page\n\n## What This Skill Does\n\n1. **Scans Git History**: Analyzes commits from a specific time period or between versions\n2. **Categorizes Changes**: Groups commits into logical categories (features, improvements, bug fixes, breaking changes, security)\n3. **Translates Technical  User-Friendly**: Converts developer commits into customer language\n4. **Formats Professionally**: Creates clean, structured changelog entries\n5. **Filters Noise**: Excludes internal commits (refactoring, tests, etc.)\n6. **Follows Best Practices**: Applies changelog guidelines and your brand voice\n\n## How to Use\n\n### Basic Usage\n\nFrom your project repository:\n\n```\nCreate a changelog from commits since last release\n```\n\n```\nGenerate changelog for all commits from the past week\n```\n\n```\nCreate release notes for version 2.5.0\n```\n\n### With Specific Date Range\n\n```\nCreate a changelog for all commits between March 1 and March 15\n```\n\n### With Custom Guidelines\n\n```\nCreate a changelog for commits since v2.4.0, using my changelog \nguidelines from CHANGELOG_STYLE.md\n```\n\n## Example\n\n**User**: \"Create a changelog for commits from the past 7 days\"\n\n**Output**:\n```markdown\n# Updates - Week of March 10, 2024\n\n##  New Features\n\n- **Team Workspaces**: Create separate workspaces for different \n  projects. Invite team members and keep everything organized.\n\n- **Keyboard Shortcuts**: Press ? to see all available shortcuts. \n  Navigate faster without touching your mouse.\n\n##  Improvements\n\n- **Faster Sync**: Files now sync 2x faster across devices\n- **Better Search**: Search now includes file contents, not just titles\n\n##  Fixes\n\n- Fixed issue where large images wouldn't upload\n- Resolved timezone confusion in scheduled posts\n- Corrected notification badge count\n```\n\n**Inspired by:** Manik Aggarwal's use case from Lenny's Newsletter\n\n## Tips\n\n- Run from your git repository root\n- Specify date ranges for focused changelogs\n- Use your CHANGELOG_STYLE.md for consistent formatting\n- Review and adjust the generated changelog before publishing\n- Save output directly to CHANGELOG.md\n\n## Related Use Cases\n\n- Creating GitHub release notes\n- Writing app store update descriptions\n- Generating email updates for users\n- Creating social media announcement posts"
              },
              {
                "name": "competitive-ads-extractor",
                "description": "Extracts and analyzes competitors' ads from ad libraries (Facebook, LinkedIn, etc.) to understand what messaging, problems, and creative approaches are working. Helps inspire and improve your own ad campaigns.",
                "path": "plugins/all-skills/skills/competitive-ads-extractor/SKILL.md",
                "frontmatter": {
                  "name": "competitive-ads-extractor",
                  "category": "business-productivity",
                  "description": "Extracts and analyzes competitors' ads from ad libraries (Facebook, LinkedIn, etc.) to understand what messaging, problems, and creative approaches are working. Helps inspire and improve your own ad campaigns."
                },
                "content": "# Competitive Ads Extractor\n\nThis skill extracts your competitors' ads from ad libraries and analyzes what's workingthe problems they're highlighting, use cases they're targeting, and copy/creative that's resonating.\n\n## When to Use This Skill\n\n- Researching competitor ad strategies\n- Finding inspiration for your own ads\n- Understanding market positioning\n- Identifying successful ad patterns\n- Analyzing messaging that works\n- Discovering new use cases or pain points\n- Planning ad campaigns with proven concepts\n\n## What This Skill Does\n\n1. **Extracts Ads**: Scrapes ads from Facebook Ad Library, LinkedIn, etc.\n2. **Captures Screenshots**: Saves visual copies of all ads\n3. **Analyzes Messaging**: Identifies problems, use cases, and value props\n4. **Categorizes Ads**: Groups by theme, audience, or format\n5. **Identifies Patterns**: Finds common successful approaches\n6. **Provides Insights**: Explains why certain ads likely perform well\n\n## How to Use\n\n### Basic Extraction\n\n```\nExtract all current ads from [Competitor Name] on Facebook Ad Library\n```\n\n```\nScrape ads from [Company] and analyze their messaging\n```\n\n### Specific Analysis\n\n```\nGet all ads from [Competitor] focusing on their messaging \nabout [specific problem]. What pain points are they highlighting?\n```\n\n### Competitive Set\n\n```\nExtract ads from these 5 competitors: [list]. \nCompare their approaches and tell me what's working.\n```\n\n### Specific Platform\n\n```\nGet LinkedIn ads from [Competitor] and analyze their \nB2B positioning strategy\n```\n\n## Example\n\n**User**: \"Extract ads from Notion on Facebook Ad Library and tell me what messaging is working for them.\"\n\n**Process**:\n```\nAccessing Facebook Ad Library...\nSearching for: Notion\nFound: 23 active ads\n\nExtracting screenshots...\n[] 100%\n\nSaved to: competitor-ads/notion/\n- ad-001-collaboration.png\n- ad-002-productivity.png\n- ad-003-templates.png\n...\n\nAnalyzing messaging...\n```\n\n**Output**:\n```\n# Notion Ad Analysis\n\n## Overview\n- Total Ads: 23 active\n- Primary Themes: Productivity (35%), Collaboration (30%), \n  Templates (20%), AI Features (15%)\n- Ad Formats: Static images (60%), Video (40%)\n- CTA Patterns: \"Try for free\", \"Get started\"\n\n## Key Problems They're Highlighting\n\n1. **Scattered Information** (8 ads)\n   Copy: \"Stop switching between 10 different tools\"\n   Why it works: Direct pain point many teams face\n\n2. **Meeting Overload** (5 ads)\n   Copy: \"Replace unnecessary meetings with async updates\"\n   Why it works: Post-COVID remote work pain point\n\n3. **Lost Documentation** (4 ads)\n   Copy: \"Never ask 'where is that doc?' again\"\n   Why it works: Universal workplace frustration\n\n## Successful Creative Patterns\n\n### Pattern 1: Before/After Split\n- Shows chaotic tool landscape  Clean Notion workspace\n- Used in 6 high-performing ads\n- Visual metaphor is immediately clear\n\n### Pattern 2: Feature Showcase\n- GIF of actual product usage\n- Shows specific feature in 5 seconds\n- Used for new features (AI, templates)\n\n### Pattern 3: Social Proof\n- \"Join 20M users\" messaging\n- Customer logos\n- Used in 4 ads targeting enterprise\n\n## Copy That's Working\n\nBest Headlines:\n1. \"Your team's knowledge, finally in one place\"\n    Benefit-focused, addresses pain directly\n   \n2. \"The all-in-one workspace\"\n    Clear positioning, broad appeal\n   \n3. \"AI that actually helps you work\"\n    Addresses AI skepticism, practical angle\n\nBest Body Copy Patterns:\n- Short sentences (under 10 words)\n- Focus on outcomes not features\n- Include specific numbers (\"Cut meetings by 50%\")\n\n## Audience Targeting Insights\n\nBased on ad variations:\n- Startup founders: Solo productivity angle\n- Team leads: Collaboration and alignment\n- Enterprise: Security and compliance mentions\n- Students: Free plan, templates, organization\n\n## Recommendations for Your Ads\n\n1. **Test the \"tool sprawl\" pain point**\n    Strong resonance based on their ad frequency\n\n2. **Use product screenshots over abstract visuals**\n    All their top ads show actual UI\n\n3. **Lead with the problem, not the solution**\n    \"Tired of X?\" performs better than \"Introducing Y\"\n\n4. **Keep copy under 100 characters**\n    Their shortest ads seem most frequent\n\n5. **Test before/after visual formats**\n    Proven pattern in their creative\n\n## Files Saved\n- All ads: ~/competitor-ads/notion/\n- Analysis: ~/competitor-ads/notion/analysis.md\n- Best performers: ~/competitor-ads/notion/top-10/\n```\n\n**Inspired by:** Sumant Subrahmanya's use case from Lenny's Newsletter\n\n## What You Can Learn\n\n### Messaging Analysis\n- What problems they emphasize\n- How they position against competition\n- Value propositions that resonate\n- Target audience segments\n\n### Creative Patterns\n- Visual styles that work\n- Video vs. static image performance\n- Color schemes and branding\n- Layout patterns\n\n### Copy Formulas\n- Headline structures\n- Call-to-action patterns\n- Length and tone\n- Emotional triggers\n\n### Campaign Strategy\n- Seasonal campaigns\n- Product launch approaches\n- Feature announcement tactics\n- Retargeting patterns\n\n## Best Practices\n\n### Legal & Ethical\n Only use for research and inspiration\n Don't copy ads directly\n Respect intellectual property\n Use insights to inform original creative\n Don't plagiarize copy or steal designs\n\n### Analysis Tips\n1. **Look for patterns**: What themes repeat?\n2. **Track over time**: Save ads monthly to see evolution\n3. **Test hypotheses**: Adapt successful patterns for your brand\n4. **Segment by audience**: Different messages for different targets\n5. **Compare platforms**: LinkedIn vs Facebook messaging differs\n\n## Advanced Features\n\n### Trend Tracking\n```\nCompare [Competitor]'s ads from Q1 vs Q2. \nWhat messaging has changed?\n```\n\n### Multi-Competitor Analysis\n```\nExtract ads from [Company A], [Company B], [Company C]. \nWhat are the common patterns? Where do they differ?\n```\n\n### Industry Benchmarks\n```\nShow me ad patterns across the top 10 project management \ntools. What problems do they all focus on?\n```\n\n### Format Analysis\n```\nAnalyze video ads vs static image ads from [Competitor]. \nWhich gets more engagement? (if data available)\n```\n\n## Common Workflows\n\n### Ad Campaign Planning\n1. Extract competitor ads\n2. Identify successful patterns\n3. Note gaps in their messaging\n4. Brainstorm unique angles\n5. Draft test ad variations\n\n### Positioning Research\n1. Get ads from 5 competitors\n2. Map their positioning\n3. Find underserved angles\n4. Develop differentiated messaging\n5. Test against their approaches\n\n### Creative Inspiration\n1. Extract ads by theme\n2. Analyze visual patterns\n3. Note color and layout trends\n4. Adapt successful patterns\n5. Create original variations\n\n## Tips for Success\n\n1. **Regular Monitoring**: Check monthly for changes\n2. **Broad Research**: Look at adjacent competitors too\n3. **Save Everything**: Build a reference library\n4. **Test Insights**: Run your own experiments\n5. **Track Performance**: A/B test inspired concepts\n6. **Stay Original**: Use for inspiration, not copying\n7. **Multiple Platforms**: Compare Facebook, LinkedIn, TikTok, etc.\n\n## Output Formats\n\n- **Screenshots**: All ads saved as images\n- **Analysis Report**: Markdown summary of insights\n- **Spreadsheet**: CSV with ad copy, CTAs, themes\n- **Presentation**: Visual deck of top performers\n- **Pattern Library**: Categorized by approach\n\n## Related Use Cases\n\n- Writing better ad copy for your campaigns\n- Understanding market positioning\n- Finding content gaps in your messaging\n- Discovering new use cases for your product\n- Planning product marketing strategy\n- Inspiring social media content"
              },
              {
                "name": "content-research-writer",
                "description": "Assists in writing high-quality content by conducting research, adding citations, improving hooks, iterating on outlines, and providing real-time feedback on each section. Transforms your writing process from solo effort to collaborative partnership.",
                "path": "plugins/all-skills/skills/content-research-writer/SKILL.md",
                "frontmatter": {
                  "name": "content-research-writer",
                  "category": "business-productivity",
                  "description": "Assists in writing high-quality content by conducting research, adding citations, improving hooks, iterating on outlines, and providing real-time feedback on each section. Transforms your writing process from solo effort to collaborative partnership."
                },
                "content": "# Content Research Writer\n\nThis skill acts as your writing partner, helping you research, outline, draft, and refine content while maintaining your unique voice and style.\n\n## When to Use This Skill\n\n- Writing blog posts, articles, or newsletters\n- Creating educational content or tutorials\n- Drafting thought leadership pieces\n- Researching and writing case studies\n- Producing technical documentation with sources\n- Writing with proper citations and references\n- Improving hooks and introductions\n- Getting section-by-section feedback while writing\n\n## What This Skill Does\n\n1. **Collaborative Outlining**: Helps you structure ideas into coherent outlines\n2. **Research Assistance**: Finds relevant information and adds citations\n3. **Hook Improvement**: Strengthens your opening to capture attention\n4. **Section Feedback**: Reviews each section as you write\n5. **Voice Preservation**: Maintains your writing style and tone\n6. **Citation Management**: Adds and formats references properly\n7. **Iterative Refinement**: Helps you improve through multiple drafts\n\n## How to Use\n\n### Setup Your Writing Environment\n\nCreate a dedicated folder for your article:\n```\nmkdir ~/writing/my-article-title\ncd ~/writing/my-article-title\n```\n\nCreate your draft file:\n```\ntouch article-draft.md\n```\n\nOpen Claude Code from this directory and start writing.\n\n### Basic Workflow\n\n1. **Start with an outline**:\n```\nHelp me create an outline for an article about [topic]\n```\n\n2. **Research and add citations**:\n```\nResearch [specific topic] and add citations to my outline\n```\n\n3. **Improve the hook**:\n```\nHere's my introduction. Help me make the hook more compelling.\n```\n\n4. **Get section feedback**:\n```\nI just finished the \"Why This Matters\" section. Review it and give feedback.\n```\n\n5. **Refine and polish**:\n```\nReview the full draft for flow, clarity, and consistency.\n```\n\n## Instructions\n\nWhen a user requests writing assistance:\n\n1. **Understand the Writing Project**\n   \n   Ask clarifying questions:\n   - What's the topic and main argument?\n   - Who's the target audience?\n   - What's the desired length/format?\n   - What's your goal? (educate, persuade, entertain, explain)\n   - Any existing research or sources to include?\n   - What's your writing style? (formal, conversational, technical)\n\n2. **Collaborative Outlining**\n   \n   Help structure the content:\n   \n   ```markdown\n   # Article Outline: [Title]\n   \n   ## Hook\n   - [Opening line/story/statistic]\n   - [Why reader should care]\n   \n   ## Introduction\n   - Context and background\n   - Problem statement\n   - What this article covers\n   \n   ## Main Sections\n   \n   ### Section 1: [Title]\n   - Key point A\n   - Key point B\n   - Example/evidence\n   - [Research needed: specific topic]\n   \n   ### Section 2: [Title]\n   - Key point C\n   - Key point D\n   - Data/citation needed\n   \n   ### Section 3: [Title]\n   - Key point E\n   - Counter-arguments\n   - Resolution\n   \n   ## Conclusion\n   - Summary of main points\n   - Call to action\n   - Final thought\n   \n   ## Research To-Do\n   - [ ] Find data on [topic]\n   - [ ] Get examples of [concept]\n   - [ ] Source citation for [claim]\n   ```\n   \n   **Iterate on outline**:\n   - Adjust based on feedback\n   - Ensure logical flow\n   - Identify research gaps\n   - Mark sections for deep dives\n\n3. **Conduct Research**\n   \n   When user requests research on a topic:\n   \n   - Search for relevant information\n   - Find credible sources\n   - Extract key facts, quotes, and data\n   - Add citations in requested format\n   \n   Example output:\n   ```markdown\n   ## Research: AI Impact on Productivity\n   \n   Key Findings:\n   \n   1. **Productivity Gains**: Studies show 40% time savings for \n      content creation tasks [1]\n   \n   2. **Adoption Rates**: 67% of knowledge workers use AI tools \n      weekly [2]\n   \n   3. **Expert Quote**: \"AI augments rather than replaces human \n      creativity\" - Dr. Jane Smith, MIT [3]\n   \n   Citations:\n   [1] McKinsey Global Institute. (2024). \"The Economic Potential \n       of Generative AI\"\n   [2] Stack Overflow Developer Survey (2024)\n   [3] Smith, J. (2024). MIT Technology Review interview\n   \n   Added to outline under Section 2.\n   ```\n\n4. **Improve Hooks**\n   \n   When user shares an introduction, analyze and strengthen:\n   \n   **Current Hook Analysis**:\n   - What works: [positive elements]\n   - What could be stronger: [areas for improvement]\n   - Emotional impact: [current vs. potential]\n   \n   **Suggested Alternatives**:\n   \n   Option 1: [Bold statement]\n   > [Example]\n   *Why it works: [explanation]*\n   \n   Option 2: [Personal story]\n   > [Example]\n   *Why it works: [explanation]*\n   \n   Option 3: [Surprising data]\n   > [Example]\n   *Why it works: [explanation]*\n   \n   **Questions to hook**:\n   - Does it create curiosity?\n   - Does it promise value?\n   - Is it specific enough?\n   - Does it match the audience?\n\n5. **Provide Section-by-Section Feedback**\n   \n   As user writes each section, review for:\n   \n   ```markdown\n   # Feedback: [Section Name]\n   \n   ## What Works Well \n   - [Strength 1]\n   - [Strength 2]\n   - [Strength 3]\n   \n   ## Suggestions for Improvement\n   \n   ### Clarity\n   - [Specific issue]  [Suggested fix]\n   - [Complex sentence]  [Simpler alternative]\n   \n   ### Flow\n   - [Transition issue]  [Better connection]\n   - [Paragraph order]  [Suggested reordering]\n   \n   ### Evidence\n   - [Claim needing support]  [Add citation or example]\n   - [Generic statement]  [Make more specific]\n   \n   ### Style\n   - [Tone inconsistency]  [Match your voice better]\n   - [Word choice]  [Stronger alternative]\n   \n   ## Specific Line Edits\n   \n   Original:\n   > [Exact quote from draft]\n   \n   Suggested:\n   > [Improved version]\n   \n   Why: [Explanation]\n   \n   ## Questions to Consider\n   - [Thought-provoking question 1]\n   - [Thought-provoking question 2]\n   \n   Ready to move to next section!\n   ```\n\n6. **Preserve Writer's Voice**\n   \n   Important principles:\n   \n   - **Learn their style**: Read existing writing samples\n   - **Suggest, don't replace**: Offer options, not directives\n   - **Match tone**: Formal, casual, technical, friendly\n   - **Respect choices**: If they prefer their version, support it\n   - **Enhance, don't override**: Make their writing better, not different\n   \n   Ask periodically:\n   - \"Does this sound like you?\"\n   - \"Is this the right tone?\"\n   - \"Should I be more/less [formal/casual/technical]?\"\n\n7. **Citation Management**\n   \n   Handle references based on user preference:\n   \n   **Inline Citations**:\n   ```markdown\n   Studies show 40% productivity improvement (McKinsey, 2024).\n   ```\n   \n   **Numbered References**:\n   ```markdown\n   Studies show 40% productivity improvement [1].\n   \n   [1] McKinsey Global Institute. (2024)...\n   ```\n   \n   **Footnote Style**:\n   ```markdown\n   Studies show 40% productivity improvement^1\n   \n   ^1: McKinsey Global Institute. (2024)...\n   ```\n   \n   Maintain a running citations list:\n   ```markdown\n   ## References\n   \n   1. Author. (Year). \"Title\". Publication.\n   2. Author. (Year). \"Title\". Publication.\n   ...\n   ```\n\n8. **Final Review and Polish**\n   \n   When draft is complete, provide comprehensive feedback:\n   \n   ```markdown\n   # Full Draft Review\n   \n   ## Overall Assessment\n   \n   **Strengths**:\n   - [Major strength 1]\n   - [Major strength 2]\n   - [Major strength 3]\n   \n   **Impact**: [Overall effectiveness assessment]\n   \n   ## Structure & Flow\n   - [Comments on organization]\n   - [Transition quality]\n   - [Pacing assessment]\n   \n   ## Content Quality\n   - [Argument strength]\n   - [Evidence sufficiency]\n   - [Example effectiveness]\n   \n   ## Technical Quality\n   - Grammar and mechanics: [assessment]\n   - Consistency: [assessment]\n   - Citations: [completeness check]\n   \n   ## Readability\n   - Clarity score: [evaluation]\n   - Sentence variety: [evaluation]\n   - Paragraph length: [evaluation]\n   \n   ## Final Polish Suggestions\n   \n   1. **Introduction**: [Specific improvements]\n   2. **Body**: [Specific improvements]\n   3. **Conclusion**: [Specific improvements]\n   4. **Title**: [Options if needed]\n   \n   ## Pre-Publish Checklist\n   - [ ] All claims sourced\n   - [ ] Citations formatted\n   - [ ] Examples clear\n   - [ ] Transitions smooth\n   - [ ] Call to action present\n   - [ ] Proofread for typos\n   \n   Ready to publish! \n   ```\n\n## Examples\n\n### Example 1: Teresa Torres's Workflow\n\n**User**: \"I'm writing an article about continuous discovery. Help me create an outline.\"\n\n**Process**:\n1. Collaborates on outline structure\n2. Identifies research needs\n3. User starts writing introduction\n4. Reviews and improves the hook\n5. User writes each section\n6. Provides feedback after each section\n7. Conducts research and adds citations\n8. Final review of complete draft\n9. Polish and prep for publishing\n\n**Result**: Well-researched, properly cited article written in Teresa's voice with strong structure and flow.\n\n### Example 2: Research-Heavy Article\n\n**User**: \"I'm writing about AI's impact on product management. Help me find current data and examples.\"\n\n**Output**:\n```markdown\n## Research Compiled\n\n### Recent Data\n- Gartner: 80% of PMs will use AI tools by 2025 [1]\n- Survey: 45% report faster feature shipping [2]\n- Case study: Airbnb's AI-assisted prioritization [3]\n\n### Expert Quotes\n- \"AI amplifies PM judgment, not replaces it\" - Marty Cagan\n- [Additional quotes with citations]\n\n### Real Examples\n1. **Company A**: Used AI for user research synthesis\n   - Result: 60% time savings\n   - Source: [citation]\n\n2. **Company B**: AI-powered roadmap analysis\n   - Result: Better stakeholder alignment\n   - Source: [citation]\n\nAll added to your outline with proper citations.\n```\n\n### Example 3: Hook Improvement\n\n**User's Original Hook**:\n> \"Product management is changing because of AI. In this article, I'll discuss some ways AI affects product managers.\"\n\n**Improved Options**:\n\n**Option 1 (Data-driven)**:\n> \"Last month, I asked AI to analyze 500 customer interviews. It took 30 minutes instead of 3 weeks. Product management will never be the same.\"\n\n**Option 2 (Question)**:\n> \"What if you could talk to every customer, read every review, and analyze every support ticketall before your morning coffee?\"\n\n**Option 3 (Story)**:\n> \"Sarah spent two weeks building the wrong feature. Not because she didn't understand her users, but because she couldn't process the hundreds of interviews fast enough to spot the pattern.\"\n\n### Example 4: Section Feedback\n\n**User**: \"Just finished my 'Common Mistakes' section. Thoughts?\"\n\n**Response**:\n```markdown\n# Feedback: Common Mistakes Section\n\n## What Works Well \n- Strong examples (the Slack notification story)\n- Clear structure (3 mistakes, clean layout)\n- Practical advice for each mistake\n\n## Suggestions\n\n### Make It More Specific\nYour second point says \"Bad prioritization\" but could be stronger:\n\nCurrent:\n> \"Many teams prioritize badly by focusing on features instead of outcomes.\"\n\nSuggested:\n> \"I've watched teams ship 14 features in a quarter yet move none of their key metrics. They prioritized activity over progress.\"\n\n### Add Data\nThe third mistake would benefit from evidence:\n> \"[Add citation]: Studies show teams without regular user contact are 3x more likely to build unused features [needs source]\"\n\n### Flow Improvement\nConsider reordering: Mistake 3  Mistake 2  Mistake 1\nThis builds from small to big impact.\n\nReady for the next section!\n```\n\n## Writing Workflows\n\n### Blog Post Workflow\n1. Outline together\n2. Research key points\n3. Write introduction  get feedback\n4. Write body sections  feedback each\n5. Write conclusion  final review\n6. Polish and edit\n\n### Newsletter Workflow\n1. Discuss hook ideas\n2. Quick outline (shorter format)\n3. Draft in one session\n4. Review for clarity and links\n5. Quick polish\n\n### Technical Tutorial Workflow\n1. Outline steps\n2. Write code examples\n3. Add explanations\n4. Test instructions\n5. Add troubleshooting section\n6. Final review for accuracy\n\n### Thought Leadership Workflow\n1. Brainstorm unique angle\n2. Research existing perspectives\n3. Develop your thesis\n4. Write with strong POV\n5. Add supporting evidence\n6. Craft compelling conclusion\n\n## Pro Tips\n\n1. **Work in VS Code**: Better than web Claude for long-form writing\n2. **One section at a time**: Get feedback incrementally\n3. **Save research separately**: Keep a research.md file\n4. **Version your drafts**: article-v1.md, article-v2.md, etc.\n5. **Read aloud**: Use feedback to identify clunky sentences\n6. **Set deadlines**: \"I want to finish the draft today\"\n7. **Take breaks**: Write, get feedback, pause, revise\n\n## File Organization\n\nRecommended structure for writing projects:\n\n```\n~/writing/article-name/\n outline.md          # Your outline\n research.md         # All research and citations\n draft-v1.md         # First draft\n draft-v2.md         # Revised draft\n final.md            # Publication-ready\n feedback.md         # Collected feedback\n sources/            # Reference materials\n     study1.pdf\n     article2.md\n```\n\n## Best Practices\n\n### For Research\n- Verify sources before citing\n- Use recent data when possible\n- Balance different perspectives\n- Link to original sources\n\n### For Feedback\n- Be specific about what you want: \"Is this too technical?\"\n- Share your concerns: \"I'm worried this section drags\"\n- Ask questions: \"Does this flow logically?\"\n- Request alternatives: \"What's another way to explain this?\"\n\n### For Voice\n- Share examples of your writing\n- Specify tone preferences\n- Point out good matches: \"That sounds like me!\"\n- Flag mismatches: \"Too formal for my style\"\n\n## Related Use Cases\n\n- Creating social media posts from articles\n- Adapting content for different audiences\n- Writing email newsletters\n- Drafting technical documentation\n- Creating presentation content\n- Writing case studies\n- Developing course outlines"
              },
              {
                "name": "developer-growth-analysis",
                "description": "Analyzes your recent Claude Code chat history to identify coding patterns, development gaps, and areas for improvement, curates relevant learning resources from HackerNews, and automatically sends a personalized growth report to your Slack DMs.",
                "path": "plugins/all-skills/skills/developer-growth-analysis/SKILL.md",
                "frontmatter": {
                  "name": "developer-growth-analysis",
                  "category": "business-productivity",
                  "description": "Analyzes your recent Claude Code chat history to identify coding patterns, development gaps, and areas for improvement, curates relevant learning resources from HackerNews, and automatically sends a personalized growth report to your Slack DMs."
                },
                "content": "# Developer Growth Analysis\n\nThis skill provides personalized feedback on your recent coding work by analyzing your Claude Code chat interactions and identifying patterns that reveal strengths and areas for growth.\n\n## When to Use This Skill\n\nUse this skill when you want to:\n- Understand your development patterns and habits from recent work\n- Identify specific technical gaps or recurring challenges\n- Discover which topics would benefit from deeper study\n- Get curated learning resources tailored to your actual work patterns\n- Track improvement areas across your recent projects\n- Find high-quality articles that directly address the skills you're developing\n\nThis skill is ideal for developers who want structured feedback on their growth without waiting for code reviews, and who prefer data-driven insights from their own work history.\n\n## What This Skill Does\n\nThis skill performs a six-step analysis of your development work:\n\n1. **Reads Your Chat History**: Accesses your local Claude Code chat history from the past 24-48 hours to understand what you've been working on.\n\n2. **Identifies Development Patterns**: Analyzes the types of problems you're solving, technologies you're using, challenges you encounter, and how you approach different kinds of tasks.\n\n3. **Detects Improvement Areas**: Recognizes patterns that suggest skill gaps, repeated struggles, inefficient approaches, or areas where you might benefit from deeper knowledge.\n\n4. **Generates a Personalized Report**: Creates a comprehensive report showing your work summary, identified improvement areas, and specific recommendations for growth.\n\n5. **Finds Learning Resources**: Uses HackerNews to curate high-quality articles and discussions directly relevant to your improvement areas, providing you with a reading list tailored to your actual development work.\n\n6. **Sends to Your Slack DMs**: Automatically delivers the complete report to your own Slack direct messages so you can reference it anytime, anywhere.\n\n## How to Use\n\nAsk Claude to analyze your recent coding work:\n\n```\nAnalyze my developer growth from my recent chats\n```\n\nOr be more specific about which time period:\n\n```\nAnalyze my work from today and suggest areas for improvement\n```\n\nThe skill will generate a formatted report with:\n- Overview of your recent work\n- Key improvement areas identified\n- Specific recommendations for each area\n- Curated learning resources from HackerNews\n- Action items you can focus on\n\n## Instructions\n\nWhen a user requests analysis of their developer growth or coding patterns from recent work:\n\n1. **Access Chat History**\n\n   Read the chat history from `~/.claude/history.jsonl`. This file is a JSONL format where each line contains:\n   - `display`: The user's message/request\n   - `project`: The project being worked on\n   - `timestamp`: Unix timestamp (in milliseconds)\n   - `pastedContents`: Any code or content pasted\n\n   Filter for entries from the past 24-48 hours based on the current timestamp.\n\n2. **Analyze Work Patterns**\n\n   Extract and analyze the following from the filtered chats:\n   - **Projects and Domains**: What types of projects was the user working on? (e.g., backend, frontend, DevOps, data, etc.)\n   - **Technologies Used**: What languages, frameworks, and tools appear in the conversations?\n   - **Problem Types**: What categories of problems are being solved? (e.g., performance optimization, debugging, feature implementation, refactoring, setup/configuration)\n   - **Challenges Encountered**: What problems did the user struggle with? Look for:\n     - Repeated questions about similar topics\n     - Problems that took multiple attempts to solve\n     - Questions indicating knowledge gaps\n     - Complex architectural decisions\n   - **Approach Patterns**: How does the user solve problems? (e.g., methodical, exploratory, experimental)\n\n3. **Identify Improvement Areas**\n\n   Based on the analysis, identify 3-5 specific areas where the user could improve. These should be:\n   - **Specific** (not vague like \"improve coding skills\")\n   - **Evidence-based** (grounded in actual chat history)\n   - **Actionable** (practical improvements that can be made)\n   - **Prioritized** (most impactful first)\n\n   Examples of good improvement areas:\n   - \"Advanced TypeScript patterns (generics, utility types, type guards) - you struggled with type safety in [specific project]\"\n   - \"Error handling and validation - I noticed you patched several bugs related to missing null checks\"\n   - \"Async/await patterns - your recent work shows some race conditions and timing issues\"\n   - \"Database query optimization - you rewrote the same query multiple times\"\n\n4. **Generate Report**\n\n   Create a comprehensive report with this structure:\n\n   ```markdown\n   # Your Developer Growth Report\n\n   **Report Period**: [Yesterday / Today / [Custom Date Range]]\n   **Last Updated**: [Current Date and Time]\n\n   ## Work Summary\n\n   [2-3 paragraphs summarizing what the user worked on, projects touched, technologies used, and overall focus areas]\n\n   Example:\n   \"Over the past 24 hours, you focused primarily on backend development with three distinct projects. Your work involved TypeScript, React, and deployment infrastructure. You tackled a mix of feature implementation, debugging, and architectural decisions, with a particular focus on API design and database optimization.\"\n\n   ## Improvement Areas (Prioritized)\n\n   ### 1. [Area Name]\n\n   **Why This Matters**: [Explanation of why this skill is important for the user's work]\n\n   **What I Observed**: [Specific evidence from chat history showing this gap]\n\n   **Recommendation**: [Concrete step(s) to improve in this area]\n\n   **Time to Skill Up**: [Brief estimate of effort required]\n\n   ---\n\n   [Repeat for 2-4 additional areas]\n\n   ## Strengths Observed\n\n   [2-3 bullet points highlighting things you're doing well - things to continue doing]\n\n   ## Action Items\n\n   Priority order:\n   1. [Action item derived from highest priority improvement area]\n   2. [Action item from next area]\n   3. [Action item from next area]\n\n   ## Learning Resources\n\n   [Will be populated in next step]\n   ```\n\n5. **Search for Learning Resources**\n\n   Use Rube MCP to search HackerNews for articles related to each improvement area:\n\n   - For each improvement area, construct a search query targeting high-quality resources\n   - Search HackerNews using RUBE_SEARCH_TOOLS with queries like:\n     - \"Learn [Technology/Pattern] best practices\"\n     - \"[Technology] advanced patterns and techniques\"\n     - \"Debugging [specific problem type] in [language]\"\n   - Prioritize posts with high engagement (comments, upvotes)\n   - For each area, include 2-3 most relevant articles with:\n     - Article title\n     - Publication date\n     - Brief description of why it's relevant\n     - Link to the article\n\n   Add this section to the report:\n\n   ```markdown\n   ## Curated Learning Resources\n\n   ### For: [Improvement Area]\n\n   1. **[Article Title]** - [Date]\n      [Description of what it covers and why it's relevant to your improvement area]\n      [Link]\n\n   2. **[Article Title]** - [Date]\n      [Description]\n      [Link]\n\n   [Repeat for other improvement areas]\n   ```\n\n6. **Present the Complete Report**\n\n   Deliver the report in a clean, readable format that the user can:\n   - Quickly scan for key takeaways\n   - Use for focused learning planning\n   - Reference over the next week as they work on improvements\n   - Share with mentors if they want external feedback\n\n7. **Send Report to Slack DMs**\n\n   Use Rube MCP to send the complete report to the user's own Slack DMs:\n\n   - Check if Slack connection is active via RUBE_SEARCH_TOOLS\n   - If not connected, use RUBE_MANAGE_CONNECTIONS to initiate Slack auth\n   - Use RUBE_MULTI_EXECUTE_TOOL to send the report as a formatted message:\n     - Send the report title and period as the first message\n     - Break the report into logical sections (Summary, Improvements, Strengths, Actions, Resources)\n     - Format each section as a well-structured Slack message with proper markdown\n     - Include clickable links for the learning resources\n   - Confirm delivery in the CLI output\n\n   This ensures the user has the report in a place they check regularly and can reference it throughout the week.\n\n## Example Usage\n\n### Input\n\n```\nAnalyze my developer growth from my recent chats\n```\n\n### Output\n\n```markdown\n# Your Developer Growth Report\n\n**Report Period**: November 9-10, 2024\n**Last Updated**: November 10, 2024, 9:15 PM UTC\n\n## Work Summary\n\nOver the past two days, you focused on backend infrastructure and API development. Your primary project was an open-source showcase application, where you made significant progress on connections management, UI improvements, and deployment configuration. You worked with TypeScript, React, and Node.js, tackling challenges ranging from data security to responsive design. Your work shows a balance between implementing features and addressing technical debt.\n\n## Improvement Areas (Prioritized)\n\n### 1. Advanced TypeScript Patterns and Type Safety\n\n**Why This Matters**: TypeScript is central to your work, but leveraging its advanced features (generics, utility types, conditional types, type guards) can significantly improve code reliability and reduce runtime errors. Better type safety catches bugs at compile time rather than in production.\n\n**What I Observed**: In your recent chats, you were working with connection data structures and struggled a few times with typing auth configurations properly. You also had to iterate on union types for different connection states. There's an opportunity to use discriminated unions and type guards more effectively.\n\n**Recommendation**: Study TypeScript's advanced type system, particularly utility types (Omit, Pick, Record), conditional types, and discriminated unions. Apply these patterns to your connection configuration handling and auth state management.\n\n**Time to Skill Up**: 5-8 hours of focused learning and practice\n\n### 2. Secure Data Handling and Information Hiding in UI\n\n**Why This Matters**: You identified and fixed a security concern where sensitive connection data was being displayed in your console. Preventing information leakage is critical for applications handling user credentials and API keys. Good practices here prevent security incidents and user trust violations.\n\n**What I Observed**: You caught that your \"Your Apps\" page was showing full connection data including auth configs. This shows good security instincts, and the next step is building this into your default thinking when handling sensitive information.\n\n**Recommendation**: Review security best practices for handling sensitive data in frontend applications. Create reusable patterns for filtering/masking sensitive information before displaying it. Consider implementing a secure data layer that explicitly whitelist what can be shown in the UI.\n\n**Time to Skill Up**: 3-4 hours\n\n### 3. Component Architecture and Responsive UI Patterns\n\n**Why This Matters**: You're designing UIs that need to work across different screen sizes and user interactions. Strong component architecture makes it easier to build complex UIs without bugs and improves maintainability.\n\n**What I Observed**: You worked on the \"Marketplace\" UI (formerly Browse Tools), recreating it from a design image. You also identified and fixed scrolling issues where content was overflowing containers. There's an opportunity to strengthen your understanding of layout containment and responsive design patterns.\n\n**Recommendation**: Study React component composition patterns and CSS layout best practices (especially flexbox and grid). Focus on container queries and responsive patterns that prevent overflow issues. Look into component composition libraries and design system approaches.\n\n**Time to Skill Up**: 6-10 hours (depending on depth)\n\n## Strengths Observed\n\n- **Security Awareness**: You proactively identified data leakage issues before they became problems\n- **Iterative Refinement**: You worked through UI requirements methodically, asking clarifying questions and improving designs\n- **Full-Stack Capability**: You comfortably work across backend APIs, frontend UI, and deployment concerns\n- **Problem-Solving Approach**: You break down complex tasks into manageable steps\n\n## Action Items\n\nPriority order:\n1. Spend 1-2 hours learning TypeScript utility types and discriminated unions; apply to your connection data structures\n2. Document security patterns for your project (what data is safe to display, filtering/masking functions)\n3. Study one article on advanced React patterns and apply one pattern to your current UI work\n4. Set up a code review checklist focused on type safety and data security for future PRs\n\n## Curated Learning Resources\n\n### For: Advanced TypeScript Patterns\n\n1. **TypeScript's Advanced Types: Generics, Utility Types, and Conditional Types** - HackerNews, October 2024\n   Deep dive into TypeScript's type system with practical examples and real-world applications. Covers discriminated unions, type guards, and patterns for ensuring compile-time safety in complex applications.\n   [Link to discussion]\n\n2. **Building Type-Safe APIs in TypeScript** - HackerNews, September 2024\n   Practical guide to designing APIs with TypeScript that catch errors early. Particularly relevant for your connection configuration work.\n   [Link to discussion]\n\n### For: Secure Data Handling in Frontend\n\n1. **Preventing Information Leakage in Web Applications** - HackerNews, August 2024\n   Comprehensive guide to data security in frontend applications, including filtering sensitive information, secure logging, and audit trails.\n   [Link to discussion]\n\n2. **OAuth and API Key Management Best Practices** - HackerNews, July 2024\n   How to safely handle authentication tokens and API keys in applications, with examples for different frameworks.\n   [Link to discussion]\n\n### For: Component Architecture and Responsive Design\n\n1. **Advanced React Patterns: Composition Over Configuration** - HackerNews\n   Explores component composition strategies that scale, with examples using modern React patterns.\n   [Link to discussion]\n\n2. **CSS Layout Mastery: Flexbox, Grid, and Container Queries** - HackerNews, October 2024\n   Learn responsive design patterns that prevent overflow issues and work across all screen sizes.\n   [Link to discussion]\n```\n\n## Tips and Best Practices\n\n- Run this analysis once a week to track your improvement trajectory over time\n- Pick one improvement area at a time and focus on it for a few days before moving to the next\n- Use the learning resources as a study guide; work through the recommended materials and practice applying the patterns\n- Revisit this report after focusing on an area for a week to see how your work patterns change\n- The learning resources are intentionally curated for your actual work, not generic topics, so they'll be highly relevant to what you're building\n\n## How Accuracy and Quality Are Maintained\n\nThis skill:\n- Analyzes your actual work patterns from timestamped chat history\n- Generates evidence-based recommendations grounded in real projects\n- Curates learning resources that directly address your identified gaps\n- Focuses on actionable improvements, not vague feedback\n- Provides specific time estimates based on complexity\n- Prioritizes areas that will have the most impact on your development velocity"
              },
              {
                "name": "docx",
                "description": "Comprehensive document creation, editing, and analysis with support for tracked changes, comments, formatting preservation, and text extraction. When Claude needs to work with professional documents (.docx files) for: (1) Creating new documents, (2) Modifying or editing content, (3) Working with tracked changes, (4) Adding comments, or any other document tasks",
                "path": "plugins/all-skills/skills/docx/SKILL.md",
                "frontmatter": {
                  "name": "docx",
                  "description": "Comprehensive document creation, editing, and analysis with support for tracked changes, comments, formatting preservation, and text extraction. When Claude needs to work with professional documents (.docx files) for: (1) Creating new documents, (2) Modifying or editing content, (3) Working with tracked changes, (4) Adding comments, or any other document tasks",
                  "category": "document-processing",
                  "license": "Proprietary. LICENSE.txt has complete terms"
                },
                "content": "# DOCX creation, editing, and analysis\n\n## Overview\n\nA user may ask you to create, edit, or analyze the contents of a .docx file. A .docx file is essentially a ZIP archive containing XML files and other resources that you can read or edit. You have different tools and workflows available for different tasks.\n\n## Workflow Decision Tree\n\n### Reading/Analyzing Content\nUse \"Text extraction\" or \"Raw XML access\" sections below\n\n### Creating New Document\nUse \"Creating a new Word document\" workflow\n\n### Editing Existing Document\n- **Your own document + simple changes**\n  Use \"Basic OOXML editing\" workflow\n\n- **Someone else's document**\n  Use **\"Redlining workflow\"** (recommended default)\n\n- **Legal, academic, business, or government docs**\n  Use **\"Redlining workflow\"** (required)\n\n## Reading and analyzing content\n\n### Text extraction\nIf you just need to read the text contents of a document, you should convert the document to markdown using pandoc. Pandoc provides excellent support for preserving document structure and can show tracked changes:\n\n```bash\n# Convert document to markdown with tracked changes\npandoc --track-changes=all path-to-file.docx -o output.md\n# Options: --track-changes=accept/reject/all\n```\n\n### Raw XML access\nYou need raw XML access for: comments, complex formatting, document structure, embedded media, and metadata. For any of these features, you'll need to unpack a document and read its raw XML contents.\n\n#### Unpacking a file\n`python ooxml/scripts/unpack.py <office_file> <output_directory>`\n\n#### Key file structures\n* `word/document.xml` - Main document contents\n* `word/comments.xml` - Comments referenced in document.xml\n* `word/media/` - Embedded images and media files\n* Tracked changes use `<w:ins>` (insertions) and `<w:del>` (deletions) tags\n\n## Creating a new Word document\n\nWhen creating a new Word document from scratch, use **docx-js**, which allows you to create Word documents using JavaScript/TypeScript.\n\n### Workflow\n1. **MANDATORY - READ ENTIRE FILE**: Read [`docx-js.md`](docx-js.md) (~500 lines) completely from start to finish. **NEVER set any range limits when reading this file.** Read the full file content for detailed syntax, critical formatting rules, and best practices before proceeding with document creation.\n2. Create a JavaScript/TypeScript file using Document, Paragraph, TextRun components (You can assume all dependencies are installed, but if not, refer to the dependencies section below)\n3. Export as .docx using Packer.toBuffer()\n\n## Editing an existing Word document\n\nWhen editing an existing Word document, use the **Document library** (a Python library for OOXML manipulation). The library automatically handles infrastructure setup and provides methods for document manipulation. For complex scenarios, you can access the underlying DOM directly through the library.\n\n### Workflow\n1. **MANDATORY - READ ENTIRE FILE**: Read [`ooxml.md`](ooxml.md) (~600 lines) completely from start to finish. **NEVER set any range limits when reading this file.** Read the full file content for the Document library API and XML patterns for directly editing document files.\n2. Unpack the document: `python ooxml/scripts/unpack.py <office_file> <output_directory>`\n3. Create and run a Python script using the Document library (see \"Document Library\" section in ooxml.md)\n4. Pack the final document: `python ooxml/scripts/pack.py <input_directory> <office_file>`\n\nThe Document library provides both high-level methods for common operations and direct DOM access for complex scenarios.\n\n## Redlining workflow for document review\n\nThis workflow allows you to plan comprehensive tracked changes using markdown before implementing them in OOXML. **CRITICAL**: For complete tracked changes, you must implement ALL changes systematically.\n\n**Batching Strategy**: Group related changes into batches of 3-10 changes. This makes debugging manageable while maintaining efficiency. Test each batch before moving to the next.\n\n**Principle: Minimal, Precise Edits**\nWhen implementing tracked changes, only mark text that actually changes. Repeating unchanged text makes edits harder to review and appears unprofessional. Break replacements into: [unchanged text] + [deletion] + [insertion] + [unchanged text]. Preserve the original run's RSID for unchanged text by extracting the `<w:r>` element from the original and reusing it.\n\nExample - Changing \"30 days\" to \"60 days\" in a sentence:\n```python\n# BAD - Replaces entire sentence\n'<w:del><w:r><w:delText>The term is 30 days.</w:delText></w:r></w:del><w:ins><w:r><w:t>The term is 60 days.</w:t></w:r></w:ins>'\n\n# GOOD - Only marks what changed, preserves original <w:r> for unchanged text\n'<w:r w:rsidR=\"00AB12CD\"><w:t>The term is </w:t></w:r><w:del><w:r><w:delText>30</w:delText></w:r></w:del><w:ins><w:r><w:t>60</w:t></w:r></w:ins><w:r w:rsidR=\"00AB12CD\"><w:t> days.</w:t></w:r>'\n```\n\n### Tracked changes workflow\n\n1. **Get markdown representation**: Convert document to markdown with tracked changes preserved:\n   ```bash\n   pandoc --track-changes=all path-to-file.docx -o current.md\n   ```\n\n2. **Identify and group changes**: Review the document and identify ALL changes needed, organizing them into logical batches:\n\n   **Location methods** (for finding changes in XML):\n   - Section/heading numbers (e.g., \"Section 3.2\", \"Article IV\")\n   - Paragraph identifiers if numbered\n   - Grep patterns with unique surrounding text\n   - Document structure (e.g., \"first paragraph\", \"signature block\")\n   - **DO NOT use markdown line numbers** - they don't map to XML structure\n\n   **Batch organization** (group 3-10 related changes per batch):\n   - By section: \"Batch 1: Section 2 amendments\", \"Batch 2: Section 5 updates\"\n   - By type: \"Batch 1: Date corrections\", \"Batch 2: Party name changes\"\n   - By complexity: Start with simple text replacements, then tackle complex structural changes\n   - Sequential: \"Batch 1: Pages 1-3\", \"Batch 2: Pages 4-6\"\n\n3. **Read documentation and unpack**:\n   - **MANDATORY - READ ENTIRE FILE**: Read [`ooxml.md`](ooxml.md) (~600 lines) completely from start to finish. **NEVER set any range limits when reading this file.** Pay special attention to the \"Document Library\" and \"Tracked Change Patterns\" sections.\n   - **Unpack the document**: `python ooxml/scripts/unpack.py <file.docx> <dir>`\n   - **Note the suggested RSID**: The unpack script will suggest an RSID to use for your tracked changes. Copy this RSID for use in step 4b.\n\n4. **Implement changes in batches**: Group changes logically (by section, by type, or by proximity) and implement them together in a single script. This approach:\n   - Makes debugging easier (smaller batch = easier to isolate errors)\n   - Allows incremental progress\n   - Maintains efficiency (batch size of 3-10 changes works well)\n\n   **Suggested batch groupings:**\n   - By document section (e.g., \"Section 3 changes\", \"Definitions\", \"Termination clause\")\n   - By change type (e.g., \"Date changes\", \"Party name updates\", \"Legal term replacements\")\n   - By proximity (e.g., \"Changes on pages 1-3\", \"Changes in first half of document\")\n\n   For each batch of related changes:\n\n   **a. Map text to XML**: Grep for text in `word/document.xml` to verify how text is split across `<w:r>` elements.\n\n   **b. Create and run script**: Use `get_node` to find nodes, implement changes, then `doc.save()`. See **\"Document Library\"** section in ooxml.md for patterns.\n\n   **Note**: Always grep `word/document.xml` immediately before writing a script to get current line numbers and verify text content. Line numbers change after each script run.\n\n5. **Pack the document**: After all batches are complete, convert the unpacked directory back to .docx:\n   ```bash\n   python ooxml/scripts/pack.py unpacked reviewed-document.docx\n   ```\n\n6. **Final verification**: Do a comprehensive check of the complete document:\n   - Convert final document to markdown:\n     ```bash\n     pandoc --track-changes=all reviewed-document.docx -o verification.md\n     ```\n   - Verify ALL changes were applied correctly:\n     ```bash\n     grep \"original phrase\" verification.md  # Should NOT find it\n     grep \"replacement phrase\" verification.md  # Should find it\n     ```\n   - Check that no unintended changes were introduced\n\n\n## Converting Documents to Images\n\nTo visually analyze Word documents, convert them to images using a two-step process:\n\n1. **Convert DOCX to PDF**:\n   ```bash\n   soffice --headless --convert-to pdf document.docx\n   ```\n\n2. **Convert PDF pages to JPEG images**:\n   ```bash\n   pdftoppm -jpeg -r 150 document.pdf page\n   ```\n   This creates files like `page-1.jpg`, `page-2.jpg`, etc.\n\nOptions:\n- `-r 150`: Sets resolution to 150 DPI (adjust for quality/size balance)\n- `-jpeg`: Output JPEG format (use `-png` for PNG if preferred)\n- `-f N`: First page to convert (e.g., `-f 2` starts from page 2)\n- `-l N`: Last page to convert (e.g., `-l 5` stops at page 5)\n- `page`: Prefix for output files\n\nExample for specific range:\n```bash\npdftoppm -jpeg -r 150 -f 2 -l 5 document.pdf page  # Converts only pages 2-5\n```\n\n## Code Style Guidelines\n**IMPORTANT**: When generating code for DOCX operations:\n- Write concise code\n- Avoid verbose variable names and redundant operations\n- Avoid unnecessary print statements\n\n## Dependencies\n\nRequired dependencies (install if not available):\n\n- **pandoc**: `sudo apt-get install pandoc` (for text extraction)\n- **docx**: `npm install -g docx` (for creating new documents)\n- **LibreOffice**: `sudo apt-get install libreoffice` (for PDF conversion)\n- **Poppler**: `sudo apt-get install poppler-utils` (for pdftoppm to convert PDF to images)\n- **defusedxml**: `pip install defusedxml` (for secure XML parsing)"
              },
              {
                "name": "domain-name-brainstormer",
                "description": "Generates creative domain name ideas for your project and checks availability across multiple TLDs (.com, .io, .dev, .ai, etc.). Saves hours of brainstorming and manual checking.",
                "path": "plugins/all-skills/skills/domain-name-brainstormer/SKILL.md",
                "frontmatter": {
                  "name": "domain-name-brainstormer",
                  "category": "business-productivity",
                  "description": "Generates creative domain name ideas for your project and checks availability across multiple TLDs (.com, .io, .dev, .ai, etc.). Saves hours of brainstorming and manual checking."
                },
                "content": "# Domain Name Brainstormer\n\nThis skill helps you find the perfect domain name for your project by generating creative options and checking what's actually available to register.\n\n## When to Use This Skill\n\n- Starting a new project or company\n- Launching a product or service\n- Creating a personal brand or portfolio site\n- Rebranding an existing project\n- Registering a domain for a side project\n- Finding available alternatives when your first choice is taken\n\n## What This Skill Does\n\n1. **Understands Your Project**: Analyzes what you're building and who it's for\n2. **Generates Creative Names**: Creates relevant, memorable domain options\n3. **Checks Availability**: Verifies which domains are actually available across multiple TLDs\n4. **Multiple Extensions**: Suggests .com, .io, .dev, .ai, .app, and more\n5. **Provides Alternatives**: Offers variations if top choices are taken\n6. **Branding Insights**: Explains why certain names work well\n\n## How to Use\n\n### Basic Brainstorming\n\n```\nI'm building a project management tool for remote teams. \nSuggest domain names.\n```\n\n```\nHelp me brainstorm domain names for a personal finance app\n```\n\n### Specific Preferences\n\n```\nI need a domain name for my AI writing assistant. \nPrefer short names with .ai or .io extension.\n```\n\n### With Keywords\n\n```\nSuggest domain names using the words \"pixel\" or \"studio\" \nfor my design agency\n```\n\n## Example\n\n**User**: \"I'm building a tool for developers to share code snippets. Suggest creative domain names.\"\n\n**Output**:\n```\nAnalyzing your project...\nTarget audience: Developers\nKey features: Code sharing, snippets, collaboration\n\n Domain Name Suggestions\n\n## Available (.com)\n1.  snippetbox.com - Clear, memorable, available!\n   Why: Directly describes the product, easy to remember\n\n2.  codeclip.com - Short and snappy\n   Why: Implies quick code sharing, only 8 characters\n\n3.  devpaste.com - Developer-focused\n   Why: Familiar to devs (like pastebin), clear purpose\n\n## Available (Alternative TLDs)\n4.  snippet.dev - Perfect extension for developers\n   Why: .dev extension signals developer tool\n\n5.  codebox.io - Tech-forward\n   Why: .io popular with tech startups, clean name\n\n6.  sharecode.app - Descriptive\n   Why: .app extension, clearly states the action\n\n## Premium/Taken (.com)\n- codeshare.com (Taken, est. $2,500)\n- snippets.com (Taken, premium domain)\n\n## Recommendations\n\n Top Pick: snippet.dev\n- Perfect for developer audience\n- Short and memorable  \n- .dev extension adds credibility\n- Available now!\n\n Runner-up: snippetbox.com\n- .com is universally recognized\n- Slightly longer but descriptive\n- Great brandability\n\nNext steps:\n1. Register your favorite before someone else does!\n2. Want me to check more variations?\n3. Need help with logo ideas for these names?\n```\n\n**Inspired by:** Ben Aiad's use case from Lenny's Newsletter\n\n## Domain Naming Tips\n\n### What Makes a Good Domain\n\n **Short**: Under 15 characters ideal\n **Memorable**: Easy to recall and spell\n **Pronounceable**: Can be said in conversation\n **Descriptive**: Hints at what you do\n **Brandable**: Unique enough to stand out\n **No hyphens**: Easier to share verbally\n\n### TLD Guide\n\n- **.com**: Universal, trusted, great for businesses\n- **.io**: Tech startups, developer tools\n- **.dev**: Developer-focused products\n- **.ai**: AI/ML products\n- **.app**: Mobile or web applications\n- **.co**: Alternative to .com\n- **.xyz**: Modern, creative projects\n- **.design**: Creative/design agencies\n- **.tech**: Technology companies\n\n## Advanced Features\n\n### Check Similar Variations\n\n```\nCheck availability for \"codebase\" and similar variations \nacross .com, .io, .dev\n```\n\n### Industry-Specific\n\n```\nSuggest domain names for a sustainable fashion brand, \nchecking .eco and .fashion TLDs\n```\n\n### Multilingual Options\n\n```\nBrainstorm domain names in English and Spanish for \na language learning app\n```\n\n### Competitor Analysis\n\n```\nShow me domain patterns used by successful project \nmanagement tools, then suggest similar available ones\n```\n\n## Example Workflows\n\n### Startup Launch\n1. Describe your startup idea\n2. Get 10-15 domain suggestions across TLDs\n3. Review availability and pricing\n4. Pick top 3 favorites\n5. Register immediately\n\n### Personal Brand\n1. Share your name and profession\n2. Get variations (firstname.com, firstnamelastname.dev, etc.)\n3. Check social media handle availability too\n4. Register consistent brand across platforms\n\n### Product Naming\n1. Describe product and target market\n2. Get creative, brandable names\n3. Check trademark conflicts\n4. Verify domain and social availability\n5. Test names with target audience\n\n## Tips for Success\n\n1. **Act Fast**: Good domains get taken quickly\n2. **Register Variations**: Get .com and .io to protect brand\n3. **Avoid Numbers**: Hard to communicate verbally\n4. **Check Social Media**: Make sure @username is available too\n5. **Say It Out Loud**: Test if it's easy to pronounce\n6. **Check Trademarks**: Ensure no legal conflicts\n7. **Think Long-term**: Will it still make sense in 5 years?\n\n## Pricing Context\n\nWhen suggesting domains, I'll note:\n- Standard domains: ~$10-15/year\n- Premium TLDs (.io, .ai): ~$30-50/year\n- Taken domains: Market price if listed\n- Premium domains: $hundreds to $thousands\n\n## Related Tools\n\nAfter picking a domain:\n- Check logo design options\n- Verify social media handles\n- Research trademark availability\n- Plan brand identity colors/fonts"
              },
              {
                "name": "file-organizer",
                "description": "Intelligently organizes your files and folders across your computer by understanding context, finding duplicates, suggesting better structures, and automating cleanup tasks. Reduces cognitive load and keeps your digital workspace tidy without manual effort.",
                "path": "plugins/all-skills/skills/file-organizer/SKILL.md",
                "frontmatter": {
                  "name": "file-organizer",
                  "category": "business-productivity",
                  "description": "Intelligently organizes your files and folders across your computer by understanding context, finding duplicates, suggesting better structures, and automating cleanup tasks. Reduces cognitive load and keeps your digital workspace tidy without manual effort."
                },
                "content": "# File Organizer\n\nThis skill acts as your personal organization assistant, helping you maintain a clean, logical file structure across your computer without the mental overhead of constant manual organization.\n\n## When to Use This Skill\n\n- Your Downloads folder is a chaotic mess\n- You can't find files because they're scattered everywhere\n- You have duplicate files taking up space\n- Your folder structure doesn't make sense anymore\n- You want to establish better organization habits\n- You're starting a new project and need a good structure\n- You're cleaning up before archiving old projects\n\n## What This Skill Does\n\n1. **Analyzes Current Structure**: Reviews your folders and files to understand what you have\n2. **Finds Duplicates**: Identifies duplicate files across your system\n3. **Suggests Organization**: Proposes logical folder structures based on your content\n4. **Automates Cleanup**: Moves, renames, and organizes files with your approval\n5. **Maintains Context**: Makes smart decisions based on file types, dates, and content\n6. **Reduces Clutter**: Identifies old files you probably don't need anymore\n\n## How to Use\n\n### From Your Home Directory\n\n```\ncd ~\n```\n\nThen run Claude Code and ask for help:\n\n```\nHelp me organize my Downloads folder\n```\n\n```\nFind duplicate files in my Documents folder\n```\n\n```\nReview my project directories and suggest improvements\n```\n\n### Specific Organization Tasks\n\n```\nOrganize these downloads into proper folders based on what they are\n```\n\n```\nFind duplicate files and help me decide which to keep\n```\n\n```\nClean up old files I haven't touched in 6+ months\n```\n\n```\nCreate a better folder structure for my [work/projects/photos/etc]\n```\n\n## Instructions\n\nWhen a user requests file organization help:\n\n1. **Understand the Scope**\n   \n   Ask clarifying questions:\n   - Which directory needs organization? (Downloads, Documents, entire home folder?)\n   - What's the main problem? (Can't find things, duplicates, too messy, no structure?)\n   - Any files or folders to avoid? (Current projects, sensitive data?)\n   - How aggressively to organize? (Conservative vs. comprehensive cleanup)\n\n2. **Analyze Current State**\n   \n   Review the target directory:\n   ```bash\n   # Get overview of current structure\n   ls -la [target_directory]\n   \n   # Check file types and sizes\n   find [target_directory] -type f -exec file {} \\; | head -20\n   \n   # Identify largest files\n   du -sh [target_directory]/* | sort -rh | head -20\n   \n   # Count file types\n   find [target_directory] -type f | sed 's/.*\\.//' | sort | uniq -c | sort -rn\n   ```\n   \n   Summarize findings:\n   - Total files and folders\n   - File type breakdown\n   - Size distribution\n   - Date ranges\n   - Obvious organization issues\n\n3. **Identify Organization Patterns**\n   \n   Based on the files, determine logical groupings:\n   \n   **By Type**:\n   - Documents (PDFs, DOCX, TXT)\n   - Images (JPG, PNG, SVG)\n   - Videos (MP4, MOV)\n   - Archives (ZIP, TAR, DMG)\n   - Code/Projects (directories with code)\n   - Spreadsheets (XLSX, CSV)\n   - Presentations (PPTX, KEY)\n   \n   **By Purpose**:\n   - Work vs. Personal\n   - Active vs. Archive\n   - Project-specific\n   - Reference materials\n   - Temporary/scratch files\n   \n   **By Date**:\n   - Current year/month\n   - Previous years\n   - Very old (archive candidates)\n\n4. **Find Duplicates**\n   \n   When requested, search for duplicates:\n   ```bash\n   # Find exact duplicates by hash\n   find [directory] -type f -exec md5 {} \\; | sort | uniq -d\n   \n   # Find files with same name\n   find [directory] -type f -printf '%f\\n' | sort | uniq -d\n   \n   # Find similar-sized files\n   find [directory] -type f -printf '%s %p\\n' | sort -n\n   ```\n   \n   For each set of duplicates:\n   - Show all file paths\n   - Display sizes and modification dates\n   - Recommend which to keep (usually newest or best-named)\n   - **Important**: Always ask for confirmation before deleting\n\n5. **Propose Organization Plan**\n   \n   Present a clear plan before making changes:\n   \n   ```markdown\n   # Organization Plan for [Directory]\n   \n   ## Current State\n   - X files across Y folders\n   - [Size] total\n   - File types: [breakdown]\n   - Issues: [list problems]\n   \n   ## Proposed Structure\n   \n   ```\n   [Directory]/\n    Work/\n       Projects/\n       Documents/\n       Archive/\n    Personal/\n       Photos/\n       Documents/\n       Media/\n    Downloads/\n        To-Sort/\n        Archive/\n   ```\n   \n   ## Changes I'll Make\n   \n   1. **Create new folders**: [list]\n   2. **Move files**:\n      - X PDFs  Work/Documents/\n      - Y images  Personal/Photos/\n      - Z old files  Archive/\n   3. **Rename files**: [any renaming patterns]\n   4. **Delete**: [duplicates or trash files]\n   \n   ## Files Needing Your Decision\n   \n   - [List any files you're unsure about]\n   \n   Ready to proceed? (yes/no/modify)\n   ```\n\n6. **Execute Organization**\n   \n   After approval, organize systematically:\n   \n   ```bash\n   # Create folder structure\n   mkdir -p \"path/to/new/folders\"\n   \n   # Move files with clear logging\n   mv \"old/path/file.pdf\" \"new/path/file.pdf\"\n   \n   # Rename files with consistent patterns\n   # Example: \"YYYY-MM-DD - Description.ext\"\n   ```\n   \n   **Important Rules**:\n   - Always confirm before deleting anything\n   - Log all moves for potential undo\n   - Preserve original modification dates\n   - Handle filename conflicts gracefully\n   - Stop and ask if you encounter unexpected situations\n\n7. **Provide Summary and Maintenance Tips**\n   \n   After organizing:\n   \n   ```markdown\n   # Organization Complete! \n   \n   ## What Changed\n   \n   - Created [X] new folders\n   - Organized [Y] files\n   - Freed [Z] GB by removing duplicates\n   - Archived [W] old files\n   \n   ## New Structure\n   \n   [Show the new folder tree]\n   \n   ## Maintenance Tips\n   \n   To keep this organized:\n   \n   1. **Weekly**: Sort new downloads\n   2. **Monthly**: Review and archive completed projects\n   3. **Quarterly**: Check for new duplicates\n   4. **Yearly**: Archive old files\n   \n   ## Quick Commands for You\n   \n   ```bash\n   # Find files modified this week\n   find . -type f -mtime -7\n   \n   # Sort downloads by type\n   [custom command for their setup]\n   \n   # Find duplicates\n   [custom command]\n   ```\n   \n   Want to organize another folder?\n   ```\n\n## Examples\n\n### Example 1: Organizing Downloads (From Justin Dielmann)\n\n**User**: \"My Downloads folder is a mess with 500+ files. Help me organize it.\"\n\n**Process**:\n1. Analyzes Downloads folder\n2. Finds patterns: work docs, personal photos, installers, random PDFs\n3. Proposes structure:\n   - Downloads/\n     - Work/\n     - Personal/\n     - Installers/ (DMG, PKG files)\n     - Archive/\n     - ToSort/ (things needing decisions)\n4. Asks for confirmation\n5. Moves files intelligently based on content and names\n6. Results: 500 files  5 organized folders\n\n### Example 2: Finding and Removing Duplicates\n\n**User**: \"Find duplicate files in my Documents and help me decide which to keep.\"\n\n**Output**:\n```markdown\n# Found 23 Sets of Duplicates (156 MB total)\n\n## Duplicate Set 1: \"proposal.pdf\"\n- `/Documents/proposal.pdf` (2.3 MB, modified: 2024-03-15)\n- `/Documents/old/proposal.pdf` (2.3 MB, modified: 2024-03-15)\n- `/Desktop/proposal.pdf` (2.3 MB, modified: 2024-03-10)\n\n**Recommendation**: Keep `/Documents/proposal.pdf` (most recent in correct location)\nDelete the other 2 copies?\n\n[Continue for all duplicates...]\n```\n\n### Example 3: Restructuring Projects Folder\n\n**User**: \"Review my ~/Projects directory and suggest improvements.\"\n\n**Output**:\n```markdown\n# Analysis of ~/Projects\n\n## Current Structure Issues\n- Mix of active and archived projects (3+ years old)\n- No consistent naming convention\n- Some projects at root, others in random subfolders\n- Duplicate folders (project-name, project-name-old, project-name-v2)\n\n## Proposed Structure\n\n```\nProjects/\n Active/\n    client-work/\n    side-projects/\n    learning/\n Archive/\n    2022/\n    2023/\n    2024/\n Templates/\n```\n\n## Specific Changes\n1. Move 12 projects not touched since 2022  Archive/\n2. Consolidate 4 duplicate project folders\n3. Rename projects to consistent format: \"client-name-project-name\"\n4. Create Archive for old work\n\nWant me to implement this?\n```\n\n### Example 4: Organizing Photos by Date\n\n**User**: \"Organize my photo folders by year and month.\"\n\n**Output**: Creates structure like:\n```\nPhotos/\n 2023/\n    01-January/\n    02-February/\n    ...\n 2024/\n    01-January/\n    ...\n Unsorted/\n```\n\nThen moves photos based on EXIF data or file modification dates.\n\n## Common Organization Tasks\n\n### Downloads Cleanup\n```\nOrganize my Downloads folder - move documents to Documents, \nimages to Pictures, keep installers separate, and archive files \nolder than 3 months.\n```\n\n### Project Organization\n```\nReview my Projects folder structure and help me separate active \nprojects from old ones I should archive.\n```\n\n### Duplicate Removal\n```\nFind all duplicate files in my Documents folder and help me \ndecide which ones to keep.\n```\n\n### Desktop Cleanup\n```\nMy Desktop is covered in files. Help me organize everything into \nmy Documents folder properly.\n```\n\n### Photo Organization\n```\nOrganize all photos in this folder by date (year/month) based \non when they were taken.\n```\n\n### Work/Personal Separation\n```\nHelp me separate my work files from personal files across my \nDocuments folder.\n```\n\n## Pro Tips\n\n1. **Start Small**: Begin with one messy folder (like Downloads) to build trust\n2. **Regular Maintenance**: Run weekly cleanup on Downloads\n3. **Consistent Naming**: Use \"YYYY-MM-DD - Description\" format for important files\n4. **Archive Aggressively**: Move old projects to Archive instead of deleting\n5. **Keep Active Separate**: Maintain clear boundaries between active and archived work\n6. **Trust the Process**: Let Claude handle the cognitive load of where things go\n\n## Best Practices\n\n### Folder Naming\n- Use clear, descriptive names\n- Avoid spaces (use hyphens or underscores)\n- Be specific: \"client-proposals\" not \"docs\"\n- Use prefixes for ordering: \"01-current\", \"02-archive\"\n\n### File Naming\n- Include dates: \"2024-10-17-meeting-notes.md\"\n- Be descriptive: \"q3-financial-report.xlsx\"\n- Avoid version numbers in names (use version control instead)\n- Remove download artifacts: \"document-final-v2 (1).pdf\"  \"document.pdf\"\n\n### When to Archive\n- Projects not touched in 6+ months\n- Completed work that might be referenced later\n- Old versions after migration to new systems\n- Files you're hesitant to delete (archive first)\n\n## Related Use Cases\n\n- Setting up organization for a new computer\n- Preparing files for backup/archiving\n- Cleaning up before storage cleanup\n- Organizing shared team folders\n- Structuring new project directories"
              },
              {
                "name": "image-enhancer",
                "description": "Improves the quality of images, especially screenshots, by enhancing resolution, sharpness, and clarity. Perfect for preparing images for presentations, documentation, or social media posts.",
                "path": "plugins/all-skills/skills/image-enhancer/SKILL.md",
                "frontmatter": {
                  "name": "image-enhancer",
                  "category": "document-processing",
                  "description": "Improves the quality of images, especially screenshots, by enhancing resolution, sharpness, and clarity. Perfect for preparing images for presentations, documentation, or social media posts."
                },
                "content": "# Image Enhancer\n\nThis skill takes your images and screenshots and makes them look bettersharper, clearer, and more professional.\n\n## When to Use This Skill\n\n- Improving screenshot quality for blog posts or documentation\n- Enhancing images before sharing on social media\n- Preparing images for presentations or reports\n- Upscaling low-resolution images\n- Sharpening blurry photos\n- Cleaning up compressed images\n\n## What This Skill Does\n\n1. **Analyzes Image Quality**: Checks resolution, sharpness, and compression artifacts\n2. **Enhances Resolution**: Upscales images intelligently\n3. **Improves Sharpness**: Enhances edges and details\n4. **Reduces Artifacts**: Cleans up compression artifacts and noise\n5. **Optimizes for Use Case**: Adjusts based on intended use (web, print, social media)\n\n## How to Use\n\n### Basic Enhancement\n\n```\nImprove the image quality of screenshot.png\n```\n\n```\nEnhance all images in this folder\n```\n\n### Specific Improvements\n\n```\nUpscale this image to 4K resolution\n```\n\n```\nSharpen this blurry screenshot\n```\n\n```\nReduce compression artifacts in this image\n```\n\n### Batch Processing\n\n```\nImprove the quality of all PNG files in this directory\n```\n\n## Example\n\n**User**: \"Improve the image quality of screenshot-2024.png\"\n\n**Output**:\n```\nAnalyzing screenshot-2024.png...\n\nCurrent specs:\n- Resolution: 1920x1080\n- Format: PNG\n- Quality: Good, but slight blur\n\nEnhancements applied:\n Upscaled to 2560x1440 (retina)\n Sharpened edges\n Enhanced text clarity\n Optimized file size\n\nSaved as: screenshot-2024-enhanced.png\nOriginal preserved as: screenshot-2024-original.png\n```\n\n**Inspired by:** Lenny Rachitsky's workflow from his newsletter - used for screenshots in his articles\n\n## Tips\n\n- Always keeps original files as backup\n- Works best with screenshots and digital images\n- Can batch process entire folders\n- Specify output format if needed (PNG for quality, JPG for smaller size)\n- For social media, mention the platform for optimal sizing\n\n## Common Use Cases\n\n- **Blog Posts**: Enhance screenshots before publishing\n- **Documentation**: Make UI screenshots crystal clear\n- **Social Media**: Optimize images for Twitter, LinkedIn, Instagram\n- **Presentations**: Upscale images for large screens\n- **Print Materials**: Increase resolution for physical media"
              },
              {
                "name": "internal-comms",
                "description": "A set of resources to help me write all kinds of internal communications, using the formats that my company likes to use. Claude should use this skill whenever asked to write some sort of internal communications (status reports, leadership updates, 3P updates, company newsletters, FAQs, incident reports, project updates, etc.).",
                "path": "plugins/all-skills/skills/internal-comms/SKILL.md",
                "frontmatter": {
                  "name": "internal-comms",
                  "category": "business-productivity",
                  "description": "A set of resources to help me write all kinds of internal communications, using the formats that my company likes to use. Claude should use this skill whenever asked to write some sort of internal communications (status reports, leadership updates, 3P updates, company newsletters, FAQs, incident reports, project updates, etc.).",
                  "license": "Complete terms in LICENSE.txt"
                },
                "content": "## When to use this skill\nTo write internal communications, use this skill for:\n- 3P updates (Progress, Plans, Problems)\n- Company newsletters\n- FAQ responses\n- Status reports\n- Leadership updates\n- Project updates\n- Incident reports\n\n## How to use this skill\n\nTo write any internal communication:\n\n1. **Identify the communication type** from the request\n2. **Load the appropriate guideline file** from the `examples/` directory:\n    - `examples/3p-updates.md` - For Progress/Plans/Problems team updates\n    - `examples/company-newsletter.md` - For company-wide newsletters\n    - `examples/faq-answers.md` - For answering frequently asked questions\n    - `examples/general-comms.md` - For anything else that doesn't explicitly match one of the above\n3. **Follow the specific instructions** in that file for formatting, tone, and content gathering\n\nIf the communication type doesn't match any existing guideline, ask for clarification or more context about the desired format.\n\n## Keywords\n3P updates, company newsletter, company comms, weekly update, faqs, common questions, updates, internal comms"
              },
              {
                "name": "invoice-organizer",
                "description": "Automatically organizes invoices and receipts for tax preparation by reading messy files, extracting key information, renaming them consistently, and sorting them into logical folders. Turns hours of manual bookkeeping into minutes of automated organization.",
                "path": "plugins/all-skills/skills/invoice-organizer/SKILL.md",
                "frontmatter": {
                  "name": "invoice-organizer",
                  "category": "business-productivity",
                  "description": "Automatically organizes invoices and receipts for tax preparation by reading messy files, extracting key information, renaming them consistently, and sorting them into logical folders. Turns hours of manual bookkeeping into minutes of automated organization."
                },
                "content": "# Invoice Organizer\n\nThis skill transforms chaotic folders of invoices, receipts, and financial documents into a clean, tax-ready filing system without manual effort.\n\n## When to Use This Skill\n\n- Preparing for tax season and need organized records\n- Managing business expenses across multiple vendors\n- Organizing receipts from a messy folder or email downloads\n- Setting up automated invoice filing for ongoing bookkeeping\n- Archiving financial records by year or category\n- Reconciling expenses for reimbursement\n- Preparing documentation for accountants\n\n## What This Skill Does\n\n1. **Reads Invoice Content**: Extracts information from PDFs, images, and documents:\n   - Vendor/company name\n   - Invoice number\n   - Date\n   - Amount\n   - Product or service description\n   - Payment method\n\n2. **Renames Files Consistently**: Creates standardized filenames:\n   - Format: `YYYY-MM-DD Vendor - Invoice - ProductOrService.pdf`\n   - Examples: `2024-03-15 Adobe - Invoice - Creative Cloud.pdf`\n\n3. **Organizes by Category**: Sorts into logical folders:\n   - By vendor\n   - By expense category (software, office, travel, etc.)\n   - By time period (year, quarter, month)\n   - By tax category (deductible, personal, etc.)\n\n4. **Handles Multiple Formats**: Works with:\n   - PDF invoices\n   - Scanned receipts (JPG, PNG)\n   - Email attachments\n   - Screenshots\n   - Bank statements\n\n5. **Maintains Originals**: Preserves original files while organizing copies\n\n## How to Use\n\n### Basic Usage\n\nNavigate to your messy invoice folder:\n```\ncd ~/Desktop/receipts-to-sort\n```\n\nThen ask Claude Code:\n```\nOrganize these invoices for taxes\n```\n\nOr more specifically:\n```\nRead all invoices in this folder, rename them to \n\"YYYY-MM-DD Vendor - Invoice - Product.pdf\" format, \nand organize them by vendor\n```\n\n### Advanced Organization\n\n```\nOrganize these invoices:\n1. Extract date, vendor, and description from each file\n2. Rename to standard format\n3. Sort into folders by expense category (Software, Office, Travel, etc.)\n4. Create a CSV spreadsheet with all invoice details for my accountant\n```\n\n## Instructions\n\nWhen a user requests invoice organization:\n\n1. **Scan the Folder**\n   \n   Identify all invoice files:\n   ```bash\n   # Find all invoice-related files\n   find . -type f \\( -name \"*.pdf\" -o -name \"*.jpg\" -o -name \"*.png\" \\) -print\n   ```\n   \n   Report findings:\n   - Total number of files\n   - File types\n   - Date range (if discernible from names)\n   - Current organization (or lack thereof)\n\n2. **Extract Information from Each File**\n   \n   For each invoice, extract:\n   \n   **From PDF invoices**:\n   - Use text extraction to read invoice content\n   - Look for common patterns:\n     - \"Invoice Date:\", \"Date:\", \"Issued:\"\n     - \"Invoice #:\", \"Invoice Number:\"\n     - Company name (usually at top)\n     - \"Amount Due:\", \"Total:\", \"Amount:\"\n     - \"Description:\", \"Service:\", \"Product:\"\n   \n   **From image receipts**:\n   - Read visible text from images\n   - Identify vendor name (often at top)\n   - Look for date (common formats)\n   - Find total amount\n   \n   **Fallback for unclear files**:\n   - Use filename clues\n   - Check file creation/modification date\n   - Flag for manual review if critical info missing\n\n3. **Determine Organization Strategy**\n   \n   Ask user preference if not specified:\n   \n   ```markdown\n   I found [X] invoices from [date range].\n   \n   How would you like them organized?\n   \n   1. **By Vendor** (Adobe/, Amazon/, Stripe/, etc.)\n   2. **By Category** (Software/, Office Supplies/, Travel/, etc.)\n   3. **By Date** (2024/Q1/, 2024/Q2/, etc.)\n   4. **By Tax Category** (Deductible/, Personal/, etc.)\n   5. **Custom** (describe your structure)\n   \n   Or I can use a default structure: Year/Category/Vendor\n   ```\n\n4. **Create Standardized Filename**\n   \n   For each invoice, create a filename following this pattern:\n   \n   ```\n   YYYY-MM-DD Vendor - Invoice - Description.ext\n   ```\n   \n   Examples:\n   - `2024-03-15 Adobe - Invoice - Creative Cloud.pdf`\n   - `2024-01-10 Amazon - Receipt - Office Supplies.pdf`\n   - `2023-12-01 Stripe - Invoice - Monthly Payment Processing.pdf`\n   \n   **Filename Best Practices**:\n   - Remove special characters except hyphens\n   - Capitalize vendor names properly\n   - Keep descriptions concise but meaningful\n   - Use consistent date format (YYYY-MM-DD) for sorting\n   - Preserve original file extension\n\n5. **Execute Organization**\n   \n   Before moving files, show the plan:\n   \n   ```markdown\n   # Organization Plan\n   \n   ## Proposed Structure\n   ```\n   Invoices/\n    2023/\n       Software/\n          Adobe/\n          Microsoft/\n       Services/\n       Office/\n    2024/\n        Software/\n        Services/\n        Office/\n   ```\n   \n   ## Sample Changes\n   \n   Before: `invoice_adobe_march.pdf`\n   After: `2024-03-15 Adobe - Invoice - Creative Cloud.pdf`\n   Location: `Invoices/2024/Software/Adobe/`\n   \n   Before: `IMG_2847.jpg`\n   After: `2024-02-10 Staples - Receipt - Office Supplies.jpg`\n   Location: `Invoices/2024/Office/Staples/`\n   \n   Process [X] files? (yes/no)\n   ```\n   \n   After approval:\n   ```bash\n   # Create folder structure\n   mkdir -p \"Invoices/2024/Software/Adobe\"\n   \n   # Copy (don't move) to preserve originals\n   cp \"original.pdf\" \"Invoices/2024/Software/Adobe/2024-03-15 Adobe - Invoice - Creative Cloud.pdf\"\n   \n   # Or move if user prefers\n   mv \"original.pdf\" \"new/path/standardized-name.pdf\"\n   ```\n\n6. **Generate Summary Report**\n   \n   Create a CSV file with all invoice details:\n   \n   ```csv\n   Date,Vendor,Invoice Number,Description,Amount,Category,File Path\n   2024-03-15,Adobe,INV-12345,Creative Cloud,52.99,Software,Invoices/2024/Software/Adobe/2024-03-15 Adobe - Invoice - Creative Cloud.pdf\n   2024-03-10,Amazon,123-4567890-1234567,Office Supplies,127.45,Office,Invoices/2024/Office/Amazon/2024-03-10 Amazon - Receipt - Office Supplies.pdf\n   ...\n   ```\n   \n   This CSV is useful for:\n   - Importing into accounting software\n   - Sharing with accountants\n   - Expense tracking and reporting\n   - Tax preparation\n\n7. **Provide Completion Summary**\n   \n   ```markdown\n   # Organization Complete! \n   \n   ## Summary\n   - **Processed**: [X] invoices\n   - **Date range**: [earliest] to [latest]\n   - **Total amount**: $[sum] (if amounts extracted)\n   - **Vendors**: [Y] unique vendors\n   \n   ## New Structure\n   ```\n   Invoices/\n    2024/ (45 files)\n       Software/ (23 files)\n       Services/ (12 files)\n       Office/ (10 files)\n    2023/ (12 files)\n   ```\n   \n   ## Files Created\n   - `/Invoices/` - Organized invoices\n   - `/Invoices/invoice-summary.csv` - Spreadsheet for accounting\n   - `/Invoices/originals/` - Original files (if copied)\n   \n   ## Files Needing Review\n   [List any files where information couldn't be extracted completely]\n   \n   ## Next Steps\n   1. Review the `invoice-summary.csv` file\n   2. Check files in \"Needs Review\" folder\n   3. Import CSV into your accounting software\n   4. Set up auto-organization for future invoices\n   \n   Ready for tax season! \n   ```\n\n## Examples\n\n### Example 1: Tax Preparation (From Martin Merschroth)\n\n**User**: \"I have a messy folder of invoices for taxes. Sort them and rename properly.\"\n\n**Process**:\n1. Scans folder: finds 147 PDFs and images\n2. Reads each invoice to extract:\n   - Date\n   - Vendor name\n   - Invoice number\n   - Product/service description\n3. Renames all files: `YYYY-MM-DD Vendor - Invoice - Product.pdf`\n4. Organizes into: `2024/Software/`, `2024/Travel/`, etc.\n5. Creates `invoice-summary.csv` for accountant\n6. Result: Tax-ready organized invoices in minutes\n\n### Example 2: Monthly Expense Reconciliation\n\n**User**: \"Organize my business receipts from last month by category.\"\n\n**Output**:\n```markdown\n# March 2024 Receipts Organized\n\n## By Category\n- Software & Tools: $847.32 (12 invoices)\n- Office Supplies: $234.18 (8 receipts)\n- Travel & Meals: $1,456.90 (15 receipts)\n- Professional Services: $2,500.00 (3 invoices)\n\nTotal: $5,038.40\n\nAll receipts renamed and filed in:\n`Business-Receipts/2024/03-March/[Category]/`\n\nCSV export: `march-2024-expenses.csv`\n```\n\n### Example 3: Multi-Year Archive\n\n**User**: \"I have 3 years of random invoices. Organize them by year, then by vendor.\"\n\n**Output**: Creates structure:\n```\nInvoices/\n 2022/\n    Adobe/\n    Amazon/\n    ...\n 2023/\n    Adobe/\n    Amazon/\n    ...\n 2024/\n     Adobe/\n     Amazon/\n     ...\n```\n\nEach file properly renamed with date and description.\n\n### Example 4: Email Downloads Cleanup\n\n**User**: \"I download invoices from Gmail. They're all named 'invoice.pdf', 'invoice(1).pdf', etc. Fix this mess.\"\n\n**Output**:\n```markdown\nFound 89 files all named \"invoice*.pdf\"\n\nReading each file to extract real information...\n\nRenamed examples:\n- invoice.pdf  2024-03-15 Shopify - Invoice - Monthly Subscription.pdf\n- invoice(1).pdf  2024-03-14 Google - Invoice - Workspace.pdf\n- invoice(2).pdf  2024-03-10 Netlify - Invoice - Pro Plan.pdf\n\nAll files renamed and organized by vendor.\n```\n\n## Common Organization Patterns\n\n### By Vendor (Simple)\n```\nInvoices/\n Adobe/\n Amazon/\n Google/\n Microsoft/\n```\n\n### By Year and Category (Tax-Friendly)\n```\nInvoices/\n 2023/\n    Software/\n    Hardware/\n    Services/\n    Travel/\n 2024/\n     ...\n```\n\n### By Quarter (Detailed Tracking)\n```\nInvoices/\n 2024/\n    Q1/\n       Software/\n       Office/\n       Travel/\n    Q2/\n        ...\n```\n\n### By Tax Category (Accountant-Ready)\n```\nInvoices/\n Deductible/\n    Software/\n    Office/\n    Professional-Services/\n Partially-Deductible/\n    Meals-Travel/\n Personal/\n```\n\n## Automation Setup\n\nFor ongoing organization:\n\n```\nCreate a script that watches my ~/Downloads/invoices folder \nand auto-organizes any new invoice files using our standard \nnaming and folder structure.\n```\n\nThis creates a persistent solution that organizes invoices as they arrive.\n\n## Pro Tips\n\n1. **Scan emails to PDF**: Use Preview or similar to save email invoices as PDFs first\n2. **Consistent downloads**: Save all invoices to one folder for batch processing\n3. **Monthly routine**: Organize invoices monthly, not annually\n4. **Backup originals**: Keep original files before reorganizing\n5. **Include amounts in CSV**: Useful for budget tracking\n6. **Tag by deductibility**: Note which expenses are tax-deductible\n7. **Keep receipts 7 years**: Standard audit period\n\n## Handling Special Cases\n\n### Missing Information\nIf date/vendor can't be extracted:\n- Flag file for manual review\n- Use file modification date as fallback\n- Create \"Needs-Review/\" folder\n\n### Duplicate Invoices\nIf same invoice appears multiple times:\n- Compare file hashes\n- Keep highest quality version\n- Note duplicates in summary\n\n### Multi-Page Invoices\nFor invoices split across files:\n- Merge PDFs if needed\n- Use consistent naming for parts\n- Note in CSV if invoice is split\n\n### Non-Standard Formats\nFor unusual receipt formats:\n- Extract what's possible\n- Standardize what you can\n- Flag for review if critical info missing\n\n## Related Use Cases\n\n- Creating expense reports for reimbursement\n- Organizing bank statements\n- Managing vendor contracts\n- Archiving old financial records\n- Preparing for audits\n- Tracking subscription costs over time"
              },
              {
                "name": "json-canvas",
                "description": "Create and edit JSON Canvas files (.canvas) with nodes, edges, groups, and connections. Use when working with .canvas files, creating visual canvases, mind maps, flowcharts, or when the user mentions Canvas files in Obsidian.",
                "path": "plugins/all-skills/skills/json-canvas/SKILL.md",
                "frontmatter": {
                  "name": "json-canvas",
                  "category": "document-processing",
                  "description": "Create and edit JSON Canvas files (.canvas) with nodes, edges, groups, and connections. Use when working with .canvas files, creating visual canvases, mind maps, flowcharts, or when the user mentions Canvas files in Obsidian."
                },
                "content": "# JSON Canvas\n\nThis skill enables Claude Code to create and edit valid JSON Canvas files (`.canvas`) used in Obsidian and other applications.\n\n## Overview\n\nJSON Canvas is an open file format for infinite canvas data. Canvas files use the `.canvas` extension and contain valid JSON following the JSON Canvas Spec 1.0.\n\n## When to Use This Skill\n\n- Creating or editing .canvas files in Obsidian\n- Building visual mind maps or flowcharts\n- Creating project boards or planning documents\n- Organizing notes visually with connections\n- Building diagrams with linked content\n\n## File Structure\n\nA canvas file contains two top-level arrays:\n\n```json\n{\n  \"nodes\": [],\n  \"edges\": []\n}\n```\n\n- `nodes` (optional): Array of node objects\n- `edges` (optional): Array of edge objects connecting nodes\n\n## Nodes\n\nNodes are objects placed on the canvas. There are four node types:\n- `text` - Text content with Markdown\n- `file` - Reference to files/attachments\n- `link` - External URL\n- `group` - Visual container for other nodes\n\n### Z-Index Ordering\n\nFirst node = bottom layer (displayed below others)\nLast node = top layer (displayed above others)\n\n### Generic Node Attributes\n\n| Attribute | Required | Type | Description |\n|-----------|----------|------|-------------|\n| `id` | Yes | string | Unique identifier for the node |\n| `type` | Yes | string | Node type: `text`, `file`, `link`, or `group` |\n| `x` | Yes | integer | X position in pixels |\n| `y` | Yes | integer | Y position in pixels |\n| `width` | Yes | integer | Width in pixels |\n| `height` | Yes | integer | Height in pixels |\n| `color` | No | canvasColor | Node color (see Color section) |\n\n### Text Nodes\n\nText nodes contain Markdown content.\n\n```json\n{\n  \"id\": \"text1\",\n  \"type\": \"text\",\n  \"x\": 0,\n  \"y\": 0,\n  \"width\": 300,\n  \"height\": 150,\n  \"text\": \"# Heading\\n\\nThis is **markdown** content.\"\n}\n```\n\n### File Nodes\n\nFile nodes reference files or attachments (images, videos, PDFs, notes, etc.)\n\n| Attribute | Required | Type | Description |\n|-----------|----------|------|-------------|\n| `file` | Yes | string | Path to file within the system |\n| `subpath` | No | string | Link to heading or block (starts with `#`) |\n\n```json\n{\n  \"id\": \"file1\",\n  \"type\": \"file\",\n  \"x\": 350,\n  \"y\": 0,\n  \"width\": 400,\n  \"height\": 300,\n  \"file\": \"Notes/My Note.md\",\n  \"subpath\": \"#Heading\"\n}\n```\n\n### Link Nodes\n\nLink nodes display external URLs.\n\n```json\n{\n  \"id\": \"link1\",\n  \"type\": \"link\",\n  \"x\": 0,\n  \"y\": 200,\n  \"width\": 300,\n  \"height\": 150,\n  \"url\": \"https://example.com\"\n}\n```\n\n### Group Nodes\n\nGroup nodes are visual containers for organizing other nodes.\n\n| Attribute | Required | Type | Description |\n|-----------|----------|------|-------------|\n| `label` | No | string | Text label for the group |\n| `background` | No | string | Path to background image |\n| `backgroundStyle` | No | string | Background rendering style |\n\n#### Background Styles\n\n| Value | Description |\n|-------|-------------|\n| `cover` | Fills entire width and height of node |\n| `ratio` | Maintains aspect ratio of background image |\n| `repeat` | Repeats image as pattern in both directions |\n\n```json\n{\n  \"id\": \"group1\",\n  \"type\": \"group\",\n  \"x\": -50,\n  \"y\": -50,\n  \"width\": 800,\n  \"height\": 500,\n  \"label\": \"Project Ideas\",\n  \"color\": \"4\"\n}\n```\n\n## Edges\n\nEdges are lines connecting nodes.\n\n| Attribute | Required | Type | Default | Description |\n|-----------|----------|------|---------|-------------|\n| `id` | Yes | string | - | Unique identifier for the edge |\n| `fromNode` | Yes | string | - | Node ID where connection starts |\n| `fromSide` | No | string | - | Side where edge starts |\n| `fromEnd` | No | string | `none` | Shape at edge start |\n| `toNode` | Yes | string | - | Node ID where connection ends |\n| `toSide` | No | string | - | Side where edge ends |\n| `toEnd` | No | string | `arrow` | Shape at edge end |\n| `color` | No | canvasColor | - | Line color |\n| `label` | No | string | - | Text label for the edge |\n\n### Side Values\n\n| Value | Description |\n|-------|-------------|\n| `top` | Top edge of node |\n| `right` | Right edge of node |\n| `bottom` | Bottom edge of node |\n| `left` | Left edge of node |\n\n### End Shapes\n\n| Value | Description |\n|-------|-------------|\n| `none` | No endpoint shape |\n| `arrow` | Arrow endpoint |\n\n```json\n{\n  \"id\": \"edge1\",\n  \"fromNode\": \"text1\",\n  \"fromSide\": \"right\",\n  \"toNode\": \"file1\",\n  \"toSide\": \"left\",\n  \"toEnd\": \"arrow\",\n  \"label\": \"references\"\n}\n```\n\n## Colors\n\nThe `canvasColor` type supports both hex colors and preset options.\n\n### Hex Colors\n\n```json\n{\n  \"color\": \"#FF0000\"\n}\n```\n\n### Preset Colors\n\n| Preset | Color |\n|--------|-------|\n| `\"1\"` | Red |\n| `\"2\"` | Orange |\n| `\"3\"` | Yellow |\n| `\"4\"` | Green |\n| `\"5\"` | Cyan |\n| `\"6\"` | Purple |\n\nSpecific color values for presets are intentionally undefined, allowing applications to use their own brand colors.\n\n## Complete Examples\n\n### Simple Canvas with Text and Connections\n\n```json\n{\n  \"nodes\": [\n    {\n      \"id\": \"idea1\",\n      \"type\": \"text\",\n      \"x\": 0,\n      \"y\": 0,\n      \"width\": 250,\n      \"height\": 100,\n      \"text\": \"# Main Idea\\n\\nCore concept goes here\"\n    },\n    {\n      \"id\": \"idea2\",\n      \"type\": \"text\",\n      \"x\": 350,\n      \"y\": -50,\n      \"width\": 200,\n      \"height\": 80,\n      \"text\": \"## Supporting Point 1\\n\\nDetails...\"\n    },\n    {\n      \"id\": \"idea3\",\n      \"type\": \"text\",\n      \"x\": 350,\n      \"y\": 100,\n      \"width\": 200,\n      \"height\": 80,\n      \"text\": \"## Supporting Point 2\\n\\nMore details...\"\n    }\n  ],\n  \"edges\": [\n    {\n      \"id\": \"e1\",\n      \"fromNode\": \"idea1\",\n      \"fromSide\": \"right\",\n      \"toNode\": \"idea2\",\n      \"toSide\": \"left\",\n      \"toEnd\": \"arrow\"\n    },\n    {\n      \"id\": \"e2\",\n      \"fromNode\": \"idea1\",\n      \"fromSide\": \"right\",\n      \"toNode\": \"idea3\",\n      \"toSide\": \"left\",\n      \"toEnd\": \"arrow\"\n    }\n  ]\n}\n```\n\n### Project Board with Groups\n\n```json\n{\n  \"nodes\": [\n    {\n      \"id\": \"todo-group\",\n      \"type\": \"group\",\n      \"x\": 0,\n      \"y\": 0,\n      \"width\": 300,\n      \"height\": 400,\n      \"label\": \"To Do\",\n      \"color\": \"1\"\n    },\n    {\n      \"id\": \"progress-group\",\n      \"type\": \"group\",\n      \"x\": 350,\n      \"y\": 0,\n      \"width\": 300,\n      \"height\": 400,\n      \"label\": \"In Progress\",\n      \"color\": \"3\"\n    },\n    {\n      \"id\": \"done-group\",\n      \"type\": \"group\",\n      \"x\": 700,\n      \"y\": 0,\n      \"width\": 300,\n      \"height\": 400,\n      \"label\": \"Done\",\n      \"color\": \"4\"\n    },\n    {\n      \"id\": \"task1\",\n      \"type\": \"text\",\n      \"x\": 20,\n      \"y\": 50,\n      \"width\": 260,\n      \"height\": 80,\n      \"text\": \"## Task 1\\n\\nDescription of first task\"\n    },\n    {\n      \"id\": \"task2\",\n      \"type\": \"text\",\n      \"x\": 370,\n      \"y\": 50,\n      \"width\": 260,\n      \"height\": 80,\n      \"text\": \"## Task 2\\n\\nCurrently working on this\"\n    },\n    {\n      \"id\": \"task3\",\n      \"type\": \"text\",\n      \"x\": 720,\n      \"y\": 50,\n      \"width\": 260,\n      \"height\": 80,\n      \"text\": \"## Task 3\\n\\n~~Completed task~~\"\n    }\n  ],\n  \"edges\": []\n}\n```\n\n### Research Canvas with Files and Links\n\n```json\n{\n  \"nodes\": [\n    {\n      \"id\": \"central\",\n      \"type\": \"text\",\n      \"x\": 200,\n      \"y\": 200,\n      \"width\": 200,\n      \"height\": 100,\n      \"text\": \"# Research Topic\\n\\nMain research question\",\n      \"color\": \"6\"\n    },\n    {\n      \"id\": \"notes1\",\n      \"type\": \"file\",\n      \"x\": 0,\n      \"y\": 0,\n      \"width\": 180,\n      \"height\": 150,\n      \"file\": \"Research/Literature Review.md\"\n    },\n    {\n      \"id\": \"notes2\",\n      \"type\": \"file\",\n      \"x\": 450,\n      \"y\": 0,\n      \"width\": 180,\n      \"height\": 150,\n      \"file\": \"Research/Methodology.md\"\n    },\n    {\n      \"id\": \"source1\",\n      \"type\": \"link\",\n      \"x\": 0,\n      \"y\": 350,\n      \"width\": 180,\n      \"height\": 100,\n      \"url\": \"https://scholar.google.com\"\n    },\n    {\n      \"id\": \"source2\",\n      \"type\": \"link\",\n      \"x\": 450,\n      \"y\": 350,\n      \"width\": 180,\n      \"height\": 100,\n      \"url\": \"https://arxiv.org\"\n    }\n  ],\n  \"edges\": [\n    {\n      \"id\": \"e1\",\n      \"fromNode\": \"central\",\n      \"toNode\": \"notes1\",\n      \"toEnd\": \"arrow\",\n      \"label\": \"literature\"\n    },\n    {\n      \"id\": \"e2\",\n      \"fromNode\": \"central\",\n      \"toNode\": \"notes2\",\n      \"toEnd\": \"arrow\",\n      \"label\": \"methods\"\n    },\n    {\n      \"id\": \"e3\",\n      \"fromNode\": \"central\",\n      \"toNode\": \"source1\",\n      \"toEnd\": \"arrow\"\n    },\n    {\n      \"id\": \"e4\",\n      \"fromNode\": \"central\",\n      \"toNode\": \"source2\",\n      \"toEnd\": \"arrow\"\n    }\n  ]\n}\n```\n\n### Flowchart\n\n```json\n{\n  \"nodes\": [\n    {\n      \"id\": \"start\",\n      \"type\": \"text\",\n      \"x\": 100,\n      \"y\": 0,\n      \"width\": 150,\n      \"height\": 60,\n      \"text\": \"**Start**\",\n      \"color\": \"4\"\n    },\n    {\n      \"id\": \"decision\",\n      \"type\": \"text\",\n      \"x\": 75,\n      \"y\": 120,\n      \"width\": 200,\n      \"height\": 80,\n      \"text\": \"## Decision\\n\\nIs condition true?\",\n      \"color\": \"3\"\n    },\n    {\n      \"id\": \"yes-path\",\n      \"type\": \"text\",\n      \"x\": -100,\n      \"y\": 280,\n      \"width\": 150,\n      \"height\": 60,\n      \"text\": \"**Yes Path**\\n\\nDo action A\"\n    },\n    {\n      \"id\": \"no-path\",\n      \"type\": \"text\",\n      \"x\": 300,\n      \"y\": 280,\n      \"width\": 150,\n      \"height\": 60,\n      \"text\": \"**No Path**\\n\\nDo action B\"\n    },\n    {\n      \"id\": \"end\",\n      \"type\": \"text\",\n      \"x\": 100,\n      \"y\": 420,\n      \"width\": 150,\n      \"height\": 60,\n      \"text\": \"**End**\",\n      \"color\": \"1\"\n    }\n  ],\n  \"edges\": [\n    {\n      \"id\": \"e1\",\n      \"fromNode\": \"start\",\n      \"fromSide\": \"bottom\",\n      \"toNode\": \"decision\",\n      \"toSide\": \"top\",\n      \"toEnd\": \"arrow\"\n    },\n    {\n      \"id\": \"e2\",\n      \"fromNode\": \"decision\",\n      \"fromSide\": \"left\",\n      \"toNode\": \"yes-path\",\n      \"toSide\": \"top\",\n      \"toEnd\": \"arrow\",\n      \"label\": \"Yes\"\n    },\n    {\n      \"id\": \"e3\",\n      \"fromNode\": \"decision\",\n      \"fromSide\": \"right\",\n      \"toNode\": \"no-path\",\n      \"toSide\": \"top\",\n      \"toEnd\": \"arrow\",\n      \"label\": \"No\"\n    },\n    {\n      \"id\": \"e4\",\n      \"fromNode\": \"yes-path\",\n      \"fromSide\": \"bottom\",\n      \"toNode\": \"end\",\n      \"toSide\": \"left\",\n      \"toEnd\": \"arrow\"\n    },\n    {\n      \"id\": \"e5\",\n      \"fromNode\": \"no-path\",\n      \"fromSide\": \"bottom\",\n      \"toNode\": \"end\",\n      \"toSide\": \"right\",\n      \"toEnd\": \"arrow\"\n    }\n  ]\n}\n```\n\n## ID Generation\n\nNode and edge IDs must be unique strings. Obsidian generates 16-character hexadecimal IDs.\n\nExample format: `a1b2c3d4e5f67890`\n\n## Layout Guidelines\n\n### Positioning\n\n- Coordinates can be negative (canvas extends infinitely)\n- `x` increases to the right\n- `y` increases downward\n- Position refers to top-left corner of node\n\n### Recommended Sizes\n\n| Node Type | Suggested Width | Suggested Height |\n|-----------|-----------------|------------------|\n| Small text | 200-300 | 80-150 |\n| Medium text | 300-450 | 150-300 |\n| Large text | 400-600 | 300-500 |\n| File preview | 300-500 | 200-400 |\n| Link preview | 250-400 | 100-200 |\n| Group | Varies | Varies |\n\n### Spacing\n\n- Leave 20-50px padding inside groups\n- Space nodes 50-100px apart for readability\n- Align nodes to grid (multiples of 10 or 20) for cleaner layouts\n\n## Validation Rules\n\n1. All `id` values must be unique across nodes and edges\n2. `fromNode` and `toNode` must reference existing node IDs\n3. Required fields must be present for each node type\n4. `type` must be one of: `text`, `file`, `link`, `group`\n5. `backgroundStyle` must be one of: `cover`, `ratio`, `repeat`\n6. `fromSide`, `toSide` must be one of: `top`, `right`, `bottom`, `left`\n7. `fromEnd`, `toEnd` must be one of: `none`, `arrow`\n8. Color presets must be `\"1\"` through `\"6\"` or valid hex color\n\n## References\n\n- [JSON Canvas Spec 1.0](https://jsoncanvas.org/spec/1.0/)\n- [JSON Canvas GitHub](https://github.com/obsidianmd/jsoncanvas)"
              },
              {
                "name": "lead-research-assistant",
                "description": "Identifies high-quality leads for your product or service by analyzing your business, searching for target companies, and providing actionable contact strategies. Perfect for sales, business development, and marketing professionals.",
                "path": "plugins/all-skills/skills/lead-research-assistant/SKILL.md",
                "frontmatter": {
                  "name": "lead-research-assistant",
                  "category": "business-productivity",
                  "description": "Identifies high-quality leads for your product or service by analyzing your business, searching for target companies, and providing actionable contact strategies. Perfect for sales, business development, and marketing professionals."
                },
                "content": "# Lead Research Assistant\n\nThis skill helps you identify and qualify potential leads for your business by analyzing your product/service, understanding your ideal customer profile, and providing actionable outreach strategies.\n\n## When to Use This Skill\n\n- Finding potential customers or clients for your product/service\n- Building a list of companies to reach out to for partnerships\n- Identifying target accounts for sales outreach\n- Researching companies that match your ideal customer profile\n- Preparing for business development activities\n\n## What This Skill Does\n\n1. **Understands Your Business**: Analyzes your product/service, value proposition, and target market\n2. **Identifies Target Companies**: Finds companies that match your ideal customer profile based on:\n   - Industry and sector\n   - Company size and location\n   - Technology stack and tools they use\n   - Growth stage and funding\n   - Pain points your product solves\n3. **Prioritizes Leads**: Ranks companies based on fit score and relevance\n4. **Provides Contact Strategies**: Suggests how to approach each lead with personalized messaging\n5. **Enriches Data**: Gathers relevant information about decision-makers and company context\n\n## How to Use\n\n### Basic Usage\n\nSimply describe your product/service and what you're looking for:\n\n```\nI'm building [product description]. Find me 10 companies in [location/industry] \nthat would be good leads for this.\n```\n\n### With Your Codebase\n\nFor even better results, run this from your product's source code directory:\n\n```\nLook at what I'm building in this repository and identify the top 10 companies \nin [location/industry] that would benefit from this product.\n```\n\n### Advanced Usage\n\nFor more targeted research:\n\n```\nMy product: [description]\nIdeal customer profile:\n- Industry: [industry]\n- Company size: [size range]\n- Location: [location]\n- Current pain points: [pain points]\n- Technologies they use: [tech stack]\n\nFind me 20 qualified leads with contact strategies for each.\n```\n\n## Instructions\n\nWhen a user requests lead research:\n\n1. **Understand the Product/Service**\n   - If in a code directory, analyze the codebase to understand the product\n   - Ask clarifying questions about the value proposition\n   - Identify key features and benefits\n   - Understand what problems it solves\n\n2. **Define Ideal Customer Profile**\n   - Determine target industries and sectors\n   - Identify company size ranges\n   - Consider geographic preferences\n   - Understand relevant pain points\n   - Note any technology requirements\n\n3. **Research and Identify Leads**\n   - Search for companies matching the criteria\n   - Look for signals of need (job postings, tech stack, recent news)\n   - Consider growth indicators (funding, expansion, hiring)\n   - Identify companies with complementary products/services\n   - Check for budget indicators\n\n4. **Prioritize and Score**\n   - Create a fit score (1-10) for each lead\n   - Consider factors like:\n     - Alignment with ICP\n     - Signals of immediate need\n     - Budget availability\n     - Competitive landscape\n     - Timing indicators\n\n5. **Provide Actionable Output**\n   \n   For each lead, provide:\n   - **Company Name** and website\n   - **Why They're a Good Fit**: Specific reasons based on their business\n   - **Priority Score**: 1-10 with explanation\n   - **Decision Maker**: Role/title to target (e.g., \"VP of Engineering\")\n   - **Contact Strategy**: Personalized approach suggestions\n   - **Value Proposition**: How your product solves their specific problem\n   - **Conversation Starters**: Specific points to mention in outreach\n   - **LinkedIn URL**: If available, for easy connection\n\n6. **Format the Output**\n\n   Present results in a clear, scannable format:\n\n   ```markdown\n   # Lead Research Results\n   \n   ## Summary\n   - Total leads found: [X]\n   - High priority (8-10): [X]\n   - Medium priority (5-7): [X]\n   - Average fit score: [X]\n   \n   ---\n   \n   ## Lead 1: [Company Name]\n   \n   **Website**: [URL]\n   **Priority Score**: [X/10]\n   **Industry**: [Industry]\n   **Size**: [Employee count/revenue range]\n   \n   **Why They're a Good Fit**:\n   [2-3 specific reasons based on their business]\n   \n   **Target Decision Maker**: [Role/Title]\n   **LinkedIn**: [URL if available]\n   \n   **Value Proposition for Them**:\n   [Specific benefit for this company]\n   \n   **Outreach Strategy**:\n   [Personalized approach - mention specific pain points, recent company news, or relevant context]\n   \n   **Conversation Starters**:\n   - [Specific point 1]\n   - [Specific point 2]\n   \n   ---\n   \n   [Repeat for each lead]\n   ```\n\n7. **Offer Next Steps**\n   - Suggest saving results to a CSV for CRM import\n   - Offer to draft personalized outreach messages\n   - Recommend prioritization based on timing\n   - Suggest follow-up research for top leads\n\n## Examples\n\n### Example 1: From Lenny's Newsletter\n\n**User**: \"I'm building a tool that masks sensitive data in AI coding assistant queries. Find potential leads.\"\n\n**Output**: Creates a prioritized list of companies that:\n- Use AI coding assistants (Copilot, Cursor, etc.)\n- Handle sensitive data (fintech, healthcare, legal)\n- Have evidence in their GitHub repos of using coding agents\n- May have accidentally exposed sensitive data in code\n- Includes LinkedIn URLs of relevant decision-makers\n\n### Example 2: Local Business\n\n**User**: \"I run a consulting practice for remote team productivity. Find me 10 companies in the Bay Area that recently went remote.\"\n\n**Output**: Identifies companies that:\n- Recently posted remote job listings\n- Announced remote-first policies\n- Are hiring distributed teams\n- Show signs of remote work challenges\n- Provides personalized outreach strategies for each\n\n## Tips for Best Results\n\n- **Be specific** about your product and its unique value\n- **Run from your codebase** if applicable for automatic context\n- **Provide context** about your ideal customer profile\n- **Specify constraints** like industry, location, or company size\n- **Request follow-up** research on promising leads for deeper insights\n\n## Related Use Cases\n\n- Drafting personalized outreach emails after identifying leads\n- Building a CRM-ready CSV of qualified prospects\n- Researching specific companies in detail\n- Analyzing competitor customer bases\n- Identifying partnership opportunities"
              },
              {
                "name": "mcp-builder",
                "description": "Guide for creating high-quality MCP (Model Context Protocol) servers that enable LLMs to interact with external services through well-designed tools. Use when building MCP servers to integrate external APIs or services, whether in Python (FastMCP) or Node/TypeScript (MCP SDK).",
                "path": "plugins/all-skills/skills/mcp-builder/SKILL.md",
                "frontmatter": {
                  "name": "mcp-builder",
                  "category": "development-code",
                  "description": "Guide for creating high-quality MCP (Model Context Protocol) servers that enable LLMs to interact with external services through well-designed tools. Use when building MCP servers to integrate external APIs or services, whether in Python (FastMCP) or Node/TypeScript (MCP SDK).",
                  "license": "Complete terms in LICENSE.txt"
                },
                "content": "# MCP Server Development Guide\n\n## Overview\n\nTo create high-quality MCP (Model Context Protocol) servers that enable LLMs to effectively interact with external services, use this skill. An MCP server provides tools that allow LLMs to access external services and APIs. The quality of an MCP server is measured by how well it enables LLMs to accomplish real-world tasks using the tools provided.\n\n---\n\n# Process\n\n##  High-Level Workflow\n\nCreating a high-quality MCP server involves four main phases:\n\n### Phase 1: Deep Research and Planning\n\n#### 1.1 Understand Agent-Centric Design Principles\n\nBefore diving into implementation, understand how to design tools for AI agents by reviewing these principles:\n\n**Build for Workflows, Not Just API Endpoints:**\n- Don't simply wrap existing API endpoints - build thoughtful, high-impact workflow tools\n- Consolidate related operations (e.g., `schedule_event` that both checks availability and creates event)\n- Focus on tools that enable complete tasks, not just individual API calls\n- Consider what workflows agents actually need to accomplish\n\n**Optimize for Limited Context:**\n- Agents have constrained context windows - make every token count\n- Return high-signal information, not exhaustive data dumps\n- Provide \"concise\" vs \"detailed\" response format options\n- Default to human-readable identifiers over technical codes (names over IDs)\n- Consider the agent's context budget as a scarce resource\n\n**Design Actionable Error Messages:**\n- Error messages should guide agents toward correct usage patterns\n- Suggest specific next steps: \"Try using filter='active_only' to reduce results\"\n- Make errors educational, not just diagnostic\n- Help agents learn proper tool usage through clear feedback\n\n**Follow Natural Task Subdivisions:**\n- Tool names should reflect how humans think about tasks\n- Group related tools with consistent prefixes for discoverability\n- Design tools around natural workflows, not just API structure\n\n**Use Evaluation-Driven Development:**\n- Create realistic evaluation scenarios early\n- Let agent feedback drive tool improvements\n- Prototype quickly and iterate based on actual agent performance\n\n#### 1.3 Study MCP Protocol Documentation\n\n**Fetch the latest MCP protocol documentation:**\n\nUse WebFetch to load: `https://modelcontextprotocol.io/llms-full.txt`\n\nThis comprehensive document contains the complete MCP specification and guidelines.\n\n#### 1.4 Study Framework Documentation\n\n**Load and read the following reference files:**\n\n- **MCP Best Practices**: [ View Best Practices](./reference/mcp_best_practices.md) - Core guidelines for all MCP servers\n\n**For Python implementations, also load:**\n- **Python SDK Documentation**: Use WebFetch to load `https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/README.md`\n- [ Python Implementation Guide](./reference/python_mcp_server.md) - Python-specific best practices and examples\n\n**For Node/TypeScript implementations, also load:**\n- **TypeScript SDK Documentation**: Use WebFetch to load `https://raw.githubusercontent.com/modelcontextprotocol/typescript-sdk/main/README.md`\n- [ TypeScript Implementation Guide](./reference/node_mcp_server.md) - Node/TypeScript-specific best practices and examples\n\n#### 1.5 Exhaustively Study API Documentation\n\nTo integrate a service, read through **ALL** available API documentation:\n- Official API reference documentation\n- Authentication and authorization requirements\n- Rate limiting and pagination patterns\n- Error responses and status codes\n- Available endpoints and their parameters\n- Data models and schemas\n\n**To gather comprehensive information, use web search and the WebFetch tool as needed.**\n\n#### 1.6 Create a Comprehensive Implementation Plan\n\nBased on your research, create a detailed plan that includes:\n\n**Tool Selection:**\n- List the most valuable endpoints/operations to implement\n- Prioritize tools that enable the most common and important use cases\n- Consider which tools work together to enable complex workflows\n\n**Shared Utilities and Helpers:**\n- Identify common API request patterns\n- Plan pagination helpers\n- Design filtering and formatting utilities\n- Plan error handling strategies\n\n**Input/Output Design:**\n- Define input validation models (Pydantic for Python, Zod for TypeScript)\n- Design consistent response formats (e.g., JSON or Markdown), and configurable levels of detail (e.g., Detailed or Concise)\n- Plan for large-scale usage (thousands of users/resources)\n- Implement character limits and truncation strategies (e.g., 25,000 tokens)\n\n**Error Handling Strategy:**\n- Plan graceful failure modes\n- Design clear, actionable, LLM-friendly, natural language error messages which prompt further action\n- Consider rate limiting and timeout scenarios\n- Handle authentication and authorization errors\n\n---\n\n### Phase 2: Implementation\n\nNow that you have a comprehensive plan, begin implementation following language-specific best practices.\n\n#### 2.1 Set Up Project Structure\n\n**For Python:**\n- Create a single `.py` file or organize into modules if complex (see [ Python Guide](./reference/python_mcp_server.md))\n- Use the MCP Python SDK for tool registration\n- Define Pydantic models for input validation\n\n**For Node/TypeScript:**\n- Create proper project structure (see [ TypeScript Guide](./reference/node_mcp_server.md))\n- Set up `package.json` and `tsconfig.json`\n- Use MCP TypeScript SDK\n- Define Zod schemas for input validation\n\n#### 2.2 Implement Core Infrastructure First\n\n**To begin implementation, create shared utilities before implementing tools:**\n- API request helper functions\n- Error handling utilities\n- Response formatting functions (JSON and Markdown)\n- Pagination helpers\n- Authentication/token management\n\n#### 2.3 Implement Tools Systematically\n\nFor each tool in the plan:\n\n**Define Input Schema:**\n- Use Pydantic (Python) or Zod (TypeScript) for validation\n- Include proper constraints (min/max length, regex patterns, min/max values, ranges)\n- Provide clear, descriptive field descriptions\n- Include diverse examples in field descriptions\n\n**Write Comprehensive Docstrings/Descriptions:**\n- One-line summary of what the tool does\n- Detailed explanation of purpose and functionality\n- Explicit parameter types with examples\n- Complete return type schema\n- Usage examples (when to use, when not to use)\n- Error handling documentation, which outlines how to proceed given specific errors\n\n**Implement Tool Logic:**\n- Use shared utilities to avoid code duplication\n- Follow async/await patterns for all I/O\n- Implement proper error handling\n- Support multiple response formats (JSON and Markdown)\n- Respect pagination parameters\n- Check character limits and truncate appropriately\n\n**Add Tool Annotations:**\n- `readOnlyHint`: true (for read-only operations)\n- `destructiveHint`: false (for non-destructive operations)\n- `idempotentHint`: true (if repeated calls have same effect)\n- `openWorldHint`: true (if interacting with external systems)\n\n#### 2.4 Follow Language-Specific Best Practices\n\n**At this point, load the appropriate language guide:**\n\n**For Python: Load [ Python Implementation Guide](./reference/python_mcp_server.md) and ensure the following:**\n- Using MCP Python SDK with proper tool registration\n- Pydantic v2 models with `model_config`\n- Type hints throughout\n- Async/await for all I/O operations\n- Proper imports organization\n- Module-level constants (CHARACTER_LIMIT, API_BASE_URL)\n\n**For Node/TypeScript: Load [ TypeScript Implementation Guide](./reference/node_mcp_server.md) and ensure the following:**\n- Using `server.registerTool` properly\n- Zod schemas with `.strict()`\n- TypeScript strict mode enabled\n- No `any` types - use proper types\n- Explicit Promise<T> return types\n- Build process configured (`npm run build`)\n\n---\n\n### Phase 3: Review and Refine\n\nAfter initial implementation:\n\n#### 3.1 Code Quality Review\n\nTo ensure quality, review the code for:\n- **DRY Principle**: No duplicated code between tools\n- **Composability**: Shared logic extracted into functions\n- **Consistency**: Similar operations return similar formats\n- **Error Handling**: All external calls have error handling\n- **Type Safety**: Full type coverage (Python type hints, TypeScript types)\n- **Documentation**: Every tool has comprehensive docstrings/descriptions\n\n#### 3.2 Test and Build\n\n**Important:** MCP servers are long-running processes that wait for requests over stdio/stdin or sse/http. Running them directly in your main process (e.g., `python server.py` or `node dist/index.js`) will cause your process to hang indefinitely.\n\n**Safe ways to test the server:**\n- Use the evaluation harness (see Phase 4) - recommended approach\n- Run the server in tmux to keep it outside your main process\n- Use a timeout when testing: `timeout 5s python server.py`\n\n**For Python:**\n- Verify Python syntax: `python -m py_compile your_server.py`\n- Check imports work correctly by reviewing the file\n- To manually test: Run server in tmux, then test with evaluation harness in main process\n- Or use the evaluation harness directly (it manages the server for stdio transport)\n\n**For Node/TypeScript:**\n- Run `npm run build` and ensure it completes without errors\n- Verify dist/index.js is created\n- To manually test: Run server in tmux, then test with evaluation harness in main process\n- Or use the evaluation harness directly (it manages the server for stdio transport)\n\n#### 3.3 Use Quality Checklist\n\nTo verify implementation quality, load the appropriate checklist from the language-specific guide:\n- Python: see \"Quality Checklist\" in [ Python Guide](./reference/python_mcp_server.md)\n- Node/TypeScript: see \"Quality Checklist\" in [ TypeScript Guide](./reference/node_mcp_server.md)\n\n---\n\n### Phase 4: Create Evaluations\n\nAfter implementing your MCP server, create comprehensive evaluations to test its effectiveness.\n\n**Load [ Evaluation Guide](./reference/evaluation.md) for complete evaluation guidelines.**\n\n#### 4.1 Understand Evaluation Purpose\n\nEvaluations test whether LLMs can effectively use your MCP server to answer realistic, complex questions.\n\n#### 4.2 Create 10 Evaluation Questions\n\nTo create effective evaluations, follow the process outlined in the evaluation guide:\n\n1. **Tool Inspection**: List available tools and understand their capabilities\n2. **Content Exploration**: Use READ-ONLY operations to explore available data\n3. **Question Generation**: Create 10 complex, realistic questions\n4. **Answer Verification**: Solve each question yourself to verify answers\n\n#### 4.3 Evaluation Requirements\n\nEach question must be:\n- **Independent**: Not dependent on other questions\n- **Read-only**: Only non-destructive operations required\n- **Complex**: Requiring multiple tool calls and deep exploration\n- **Realistic**: Based on real use cases humans would care about\n- **Verifiable**: Single, clear answer that can be verified by string comparison\n- **Stable**: Answer won't change over time\n\n#### 4.4 Output Format\n\nCreate an XML file with this structure:\n\n```xml\n<evaluation>\n  <qa_pair>\n    <question>Find discussions about AI model launches with animal codenames. One model needed a specific safety designation that uses the format ASL-X. What number X was being determined for the model named after a spotted wild cat?</question>\n    <answer>3</answer>\n  </qa_pair>\n<!-- More qa_pairs... -->\n</evaluation>\n```\n\n---\n\n# Reference Files\n\n##  Documentation Library\n\nLoad these resources as needed during development:\n\n### Core MCP Documentation (Load First)\n- **MCP Protocol**: Fetch from `https://modelcontextprotocol.io/llms-full.txt` - Complete MCP specification\n- [ MCP Best Practices](./reference/mcp_best_practices.md) - Universal MCP guidelines including:\n  - Server and tool naming conventions\n  - Response format guidelines (JSON vs Markdown)\n  - Pagination best practices\n  - Character limits and truncation strategies\n  - Tool development guidelines\n  - Security and error handling standards\n\n### SDK Documentation (Load During Phase 1/2)\n- **Python SDK**: Fetch from `https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/README.md`\n- **TypeScript SDK**: Fetch from `https://raw.githubusercontent.com/modelcontextprotocol/typescript-sdk/main/README.md`\n\n### Language-Specific Implementation Guides (Load During Phase 2)\n- [ Python Implementation Guide](./reference/python_mcp_server.md) - Complete Python/FastMCP guide with:\n  - Server initialization patterns\n  - Pydantic model examples\n  - Tool registration with `@mcp.tool`\n  - Complete working examples\n  - Quality checklist\n\n- [ TypeScript Implementation Guide](./reference/node_mcp_server.md) - Complete TypeScript guide with:\n  - Project structure\n  - Zod schema patterns\n  - Tool registration with `server.registerTool`\n  - Complete working examples\n  - Quality checklist\n\n### Evaluation Guide (Load During Phase 4)\n- [ Evaluation Guide](./reference/evaluation.md) - Complete evaluation creation guide with:\n  - Question creation guidelines\n  - Answer verification strategies\n  - XML format specifications\n  - Example questions and answers\n  - Running an evaluation with the provided scripts"
              },
              {
                "name": "meeting-insights-analyzer",
                "description": "Analyzes meeting transcripts and recordings to uncover behavioral patterns, communication insights, and actionable feedback. Identifies when you avoid conflict, use filler words, dominate conversations, or miss opportunities to listen. Perfect for professionals seeking to improve their communication and leadership skills.",
                "path": "plugins/all-skills/skills/meeting-insights-analyzer/SKILL.md",
                "frontmatter": {
                  "name": "meeting-insights-analyzer",
                  "category": "business-productivity",
                  "description": "Analyzes meeting transcripts and recordings to uncover behavioral patterns, communication insights, and actionable feedback. Identifies when you avoid conflict, use filler words, dominate conversations, or miss opportunities to listen. Perfect for professionals seeking to improve their communication and leadership skills."
                },
                "content": "# Meeting Insights Analyzer\n\nThis skill transforms your meeting transcripts into actionable insights about your communication patterns, helping you become a more effective communicator and leader.\n\n## When to Use This Skill\n\n- Analyzing your communication patterns across multiple meetings\n- Getting feedback on your leadership and facilitation style\n- Identifying when you avoid difficult conversations\n- Understanding your speaking habits and filler words\n- Tracking improvement in communication skills over time\n- Preparing for performance reviews with concrete examples\n- Coaching team members on their communication style\n\n## What This Skill Does\n\n1. **Pattern Recognition**: Identifies recurring behaviors across meetings like:\n   - Conflict avoidance or indirect communication\n   - Speaking ratios and turn-taking\n   - Question-asking vs. statement-making patterns\n   - Active listening indicators\n   - Decision-making approaches\n\n2. **Communication Analysis**: Evaluates communication effectiveness:\n   - Clarity and directness\n   - Use of filler words and hedging language\n   - Tone and sentiment patterns\n   - Meeting control and facilitation\n\n3. **Actionable Feedback**: Provides specific, timestamped examples with:\n   - What happened\n   - Why it matters\n   - How to improve\n\n4. **Trend Tracking**: Compares patterns over time when analyzing multiple meetings\n\n## How to Use\n\n### Basic Setup\n\n1. Download your meeting transcripts to a folder (e.g., `~/meetings/`)\n2. Navigate to that folder in Claude Code\n3. Ask for the analysis you want\n\n### Quick Start Examples\n\n```\nAnalyze all meetings in this folder and tell me when I avoided conflict.\n```\n\n```\nLook at my meetings from the past month and identify my communication patterns.\n```\n\n```\nCompare my facilitation style between these two meeting folders.\n```\n\n### Advanced Analysis\n\n```\nAnalyze all transcripts in this folder and:\n1. Identify when I interrupted others\n2. Calculate my speaking ratio\n3. Find moments I avoided giving direct feedback\n4. Track my use of filler words\n5. Show examples of good active listening\n```\n\n## Instructions\n\nWhen a user requests meeting analysis:\n\n1. **Discover Available Data**\n   - Scan the folder for transcript files (.txt, .md, .vtt, .srt, .docx)\n   - Check if files contain speaker labels and timestamps\n   - Confirm the date range of meetings\n   - Identify the user's name/identifier in transcripts\n\n2. **Clarify Analysis Goals**\n   \n   If not specified, ask what they want to learn:\n   - Specific behaviors (conflict avoidance, interruptions, filler words)\n   - Communication effectiveness (clarity, directness, listening)\n   - Meeting facilitation skills\n   - Speaking patterns and ratios\n   - Growth areas for improvement\n   \n3. **Analyze Patterns**\n\n   For each requested insight:\n   \n   **Conflict Avoidance**:\n   - Look for hedging language (\"maybe\", \"kind of\", \"I think\")\n   - Indirect phrasing instead of direct requests\n   - Changing subject when tension arises\n   - Agreeing without commitment (\"yeah, but...\")\n   - Not addressing obvious problems\n   \n   **Speaking Ratios**:\n   - Calculate percentage of meeting spent speaking\n   - Count interruptions (by and of the user)\n   - Measure average speaking turn length\n   - Track question vs. statement ratios\n   \n   **Filler Words**:\n   - Count \"um\", \"uh\", \"like\", \"you know\", \"actually\", etc.\n   - Note frequency per minute or per speaking turn\n   - Identify situations where they increase (nervous, uncertain)\n   \n   **Active Listening**:\n   - Questions that reference others' previous points\n   - Paraphrasing or summarizing others' ideas\n   - Building on others' contributions\n   - Asking clarifying questions\n   \n   **Leadership & Facilitation**:\n   - Decision-making approach (directive vs. collaborative)\n   - How disagreements are handled\n   - Inclusion of quieter participants\n   - Time management and agenda control\n   - Follow-up and action item clarity\n\n4. **Provide Specific Examples**\n\n   For each pattern found, include:\n   \n   ```markdown\n   ### [Pattern Name]\n   \n   **Finding**: [One-sentence summary of the pattern]\n   \n   **Frequency**: [X times across Y meetings]\n   \n   **Examples**:\n   \n   1. **[Meeting Name/Date]** - [Timestamp]\n      \n      **What Happened**:\n      > [Actual quote from transcript]\n      \n      **Why This Matters**:\n      [Explanation of the impact or missed opportunity]\n      \n      **Better Approach**:\n      [Specific alternative phrasing or behavior]\n   \n   [Repeat for 2-3 strongest examples]\n   ```\n\n5. **Synthesize Insights**\n\n   After analyzing all patterns, provide:\n   \n   ```markdown\n   # Meeting Insights Summary\n   \n   **Analysis Period**: [Date range]\n   **Meetings Analyzed**: [X meetings]\n   **Total Duration**: [X hours]\n   \n   ## Key Patterns Identified\n   \n   ### 1. [Primary Pattern]\n   - **Observed**: [What you saw]\n   - **Impact**: [Why it matters]\n   - **Recommendation**: [How to improve]\n   \n   ### 2. [Second Pattern]\n   [Same structure]\n   \n   ## Communication Strengths\n   \n   1. [Strength 1 with example]\n   2. [Strength 2 with example]\n   3. [Strength 3 with example]\n   \n   ## Growth Opportunities\n   \n   1. **[Area 1]**: [Specific, actionable advice]\n   2. **[Area 2]**: [Specific, actionable advice]\n   3. **[Area 3]**: [Specific, actionable advice]\n   \n   ## Speaking Statistics\n   \n   - Average speaking time: [X% of meeting]\n   - Questions asked: [X per meeting average]\n   - Filler words: [X per minute]\n   - Interruptions: [X given / Y received per meeting]\n   \n   ## Next Steps\n   \n   [3-5 concrete actions to improve communication]\n   ```\n\n6. **Offer Follow-Up Options**\n   - Track these same metrics in future meetings\n   - Deep dive into specific meetings or patterns\n   - Compare to industry benchmarks\n   - Create a personal communication development plan\n   - Generate a summary for performance reviews\n\n## Examples\n\n### Example 1: Conflict Avoidance Analysis (Inspired by Dan Shipper)\n\n**User**: \"I download all of my meeting recordings and put them in a folder. Tell me all the times I've subtly avoided conflict.\"\n\n**Output**: \n```markdown\n# Conflict Avoidance Patterns\n\nFound 23 instances across 15 meetings where you used indirect \ncommunication or avoided addressing tensions directly.\n\n## Pattern: Hedging on Critical Feedback\n\n**Frequency**: 8 times across 7 meetings\n\n**Example 1: 1:1 with Sarah** - 00:14:32\n\n**What Happened**:\n> \"So, I was thinking... maybe we could, like, potentially \n> consider looking at the timeline again? I mean, if you \n> think that makes sense. But whatever you think is best!\"\n\n**Why This Matters**:\nYou needed to address that Sarah's project was behind schedule, \nbut the hedging language (\"maybe\", \"potentially\", \"if you think\") \nand the deflection at the end (\"whatever you think\") made it easy \nfor her to miss the urgency.\n\n**Better Approach**:\n\"Sarah, the project is two weeks behind schedule. We need to \ndiscuss what's blocking progress and create a new timeline \ntoday.\"\n\n[Additional examples...]\n\n## Recommendations\n\n1. **Name the issue directly** in the first sentence\n2. **Remove hedging words** like \"maybe,\" \"kind of,\" \"sort of\"\n3. **Ask specific questions** instead of hinting\n4. **Schedule difficult conversations** instead of raising them casually\n```\n\n### Example 2: Leadership Facilitation\n\n**User**: \"Analyze my team meetings and tell me about my facilitation style.\"\n\n**Output**: Provides insights on:\n- How much you speak vs. team members (60% vs. 40%)\n- Whether you ask questions or make statements (3:1 ratio)\n- How you handle disagreements (tendency to resolve too quickly)\n- Who speaks least and whether you draw them in\n- Examples of good and missed facilitation moments\n\n### Example 3: Personal Development Tracking\n\n**User**: \"Compare my meetings from Q1 vs. Q2 to see if I've improved my listening skills.\"\n\n**Output**: Creates a comparative analysis showing:\n- Decrease in interruptions (8 per meeting  3 per meeting)\n- Increase in clarifying questions (2  7 per meeting)\n- Improvement in building on others' ideas\n- Specific examples showing the difference\n- Remaining areas for growth\n\n## Setup Tips\n\n### Getting Meeting Transcripts\n\n**From Granola** (free with Lenny's newsletter subscription):\n- Granola auto-transcribes your meetings\n- Export transcripts to a folder: [Instructions on how]\n- Point Claude Code to that folder\n\n**From Zoom**:\n- Enable cloud recording with transcription\n- Download VTT or SRT files after meetings\n- Store in a dedicated folder\n\n**From Google Meet**:\n- Use Google Docs auto-transcription\n- Save transcript docs to a folder\n- Download as .txt files or give Claude Code access\n\n**From Fireflies.ai, Otter.ai, etc.**:\n- Export transcripts in bulk\n- Store in a local folder\n- Run analysis on the folder\n\n### Best Practices\n\n1. **Consistent naming**: Use `YYYY-MM-DD - Meeting Name.txt` format\n2. **Regular analysis**: Review monthly or quarterly for trends\n3. **Specific queries**: Ask about one behavior at a time for depth\n4. **Privacy**: Keep sensitive meeting data local\n5. **Action-oriented**: Focus on one improvement area at a time\n\n## Common Analysis Requests\n\n- \"When do I avoid difficult conversations?\"\n- \"How often do I interrupt others?\"\n- \"What's my speaking vs. listening ratio?\"\n- \"Do I ask good questions?\"\n- \"How do I handle disagreement?\"\n- \"Am I inclusive of all voices?\"\n- \"Do I use too many filler words?\"\n- \"How clear are my action items?\"\n- \"Do I stay on agenda or get sidetracked?\"\n- \"How has my communication changed over time?\"\n\n## Related Use Cases\n\n- Creating a personal development plan from insights\n- Preparing performance review materials with examples\n- Coaching direct reports on their communication\n- Analyzing customer calls for sales or support patterns\n- Studying negotiation tactics and outcomes"
              },
              {
                "name": "obsidian-bases",
                "description": "Create and edit Obsidian Bases (.base files) with views, filters, formulas, and summaries. Use when working with .base files, creating database-like views of notes, or when the user mentions Bases, table views, card views, filters, or formulas in Obsidian.",
                "path": "plugins/all-skills/skills/obsidian-bases/SKILL.md",
                "frontmatter": {
                  "name": "obsidian-bases",
                  "category": "document-processing",
                  "description": "Create and edit Obsidian Bases (.base files) with views, filters, formulas, and summaries. Use when working with .base files, creating database-like views of notes, or when the user mentions Bases, table views, card views, filters, or formulas in Obsidian."
                },
                "content": "# Obsidian Bases\n\nThis skill enables Claude Code to create and edit valid Obsidian Bases (`.base` files) including views, filters, formulas, and all related configurations.\n\n## Overview\n\nObsidian Bases are YAML-based files that define dynamic views of notes in an Obsidian vault. A Base file can contain multiple views, global filters, formulas, property configurations, and custom summaries.\n\n## When to Use This Skill\n\n- Creating database-like views of notes in Obsidian\n- Building task trackers, reading lists, or project dashboards\n- Filtering and organizing notes by properties or tags\n- Creating calculated/formula fields\n- Setting up table, card, list, or map views\n- Working with .base files in an Obsidian vault\n\n## File Format\n\nBase files use the `.base` extension and contain valid YAML. They can also be embedded in Markdown code blocks.\n\n## Complete Schema\n\n```yaml\n# Global filters apply to ALL views in the base\nfilters:\n  # Can be a single filter string\n  # OR a recursive filter object with and/or/not\n  and: []\n  or: []\n  not: []\n\n# Define formula properties that can be used across all views\nformulas:\n  formula_name: 'expression'\n\n# Configure display names and settings for properties\nproperties:\n  property_name:\n    displayName: \"Display Name\"\n  formula.formula_name:\n    displayName: \"Formula Display Name\"\n  file.ext:\n    displayName: \"Extension\"\n\n# Define custom summary formulas\nsummaries:\n  custom_summary_name: 'values.mean().round(3)'\n\n# Define one or more views\nviews:\n  - type: table | cards | list | map\n    name: \"View Name\"\n    limit: 10                    # Optional: limit results\n    groupBy:                     # Optional: group results\n      property: property_name\n      direction: ASC | DESC\n    filters:                     # View-specific filters\n      and: []\n    order:                       # Properties to display in order\n      - file.name\n      - property_name\n      - formula.formula_name\n    summaries:                   # Map properties to summary formulas\n      property_name: Average\n```\n\n## Filter Syntax\n\nFilters narrow down results. They can be applied globally or per-view.\n\n### Filter Structure\n\n```yaml\n# Single filter\nfilters: 'status == \"done\"'\n\n# AND - all conditions must be true\nfilters:\n  and:\n    - 'status == \"done\"'\n    - 'priority > 3'\n\n# OR - any condition can be true\nfilters:\n  or:\n    - file.hasTag(\"book\")\n    - file.hasTag(\"article\")\n\n# NOT - exclude matching items\nfilters:\n  not:\n    - file.hasTag(\"archived\")\n\n# Nested filters\nfilters:\n  or:\n    - file.hasTag(\"tag\")\n    - and:\n        - file.hasTag(\"book\")\n        - file.hasLink(\"Textbook\")\n    - not:\n        - file.hasTag(\"book\")\n        - file.inFolder(\"Required Reading\")\n```\n\n### Filter Operators\n\n| Operator | Description |\n|----------|-------------|\n| `==` | equals |\n| `!=` | not equal |\n| `>` | greater than |\n| `<` | less than |\n| `>=` | greater than or equal |\n| `<=` | less than or equal |\n| `&&` | logical and |\n| `\\|\\|` | logical or |\n| `!` | logical not |\n\n## Properties\n\n### Three Types of Properties\n\n1. **Note properties** - From frontmatter: `note.author` or just `author`\n2. **File properties** - File metadata: `file.name`, `file.mtime`, etc.\n3. **Formula properties** - Computed values: `formula.my_formula`\n\n### File Properties Reference\n\n| Property | Type | Description |\n|----------|------|-------------|\n| `file.name` | String | File name |\n| `file.basename` | String | File name without extension |\n| `file.path` | String | Full path to file |\n| `file.folder` | String | Parent folder path |\n| `file.ext` | String | File extension |\n| `file.size` | Number | File size in bytes |\n| `file.ctime` | Date | Created time |\n| `file.mtime` | Date | Modified time |\n| `file.tags` | List | All tags in file |\n| `file.links` | List | Internal links in file |\n| `file.backlinks` | List | Files linking to this file |\n| `file.embeds` | List | Embeds in the note |\n| `file.properties` | Object | All frontmatter properties |\n\n### The `this` Keyword\n\n- In main content area: refers to the base file itself\n- When embedded: refers to the embedding file\n- In sidebar: refers to the active file in main content\n\n## Formula Syntax\n\nFormulas compute values from properties. Defined in the `formulas` section.\n\n```yaml\nformulas:\n  # Simple arithmetic\n  total: \"price * quantity\"\n\n  # Conditional logic\n  status_icon: 'if(done, \"check\", \"pending\")'\n\n  # String formatting\n  formatted_price: 'if(price, price.toFixed(2) + \" dollars\")'\n\n  # Date formatting\n  created: 'file.ctime.format(\"YYYY-MM-DD\")'\n\n  # Complex expressions\n  days_old: '((now() - file.ctime) / 86400000).round(0)'\n```\n\n## Functions Reference\n\n### Global Functions\n\n| Function | Signature | Description |\n|----------|-----------|-------------|\n| `date()` | `date(string): date` | Parse string to date |\n| `duration()` | `duration(string): duration` | Parse duration string |\n| `now()` | `now(): date` | Current date and time |\n| `today()` | `today(): date` | Current date (time = 00:00:00) |\n| `if()` | `if(condition, trueResult, falseResult?)` | Conditional |\n| `min()` | `min(n1, n2, ...): number` | Smallest number |\n| `max()` | `max(n1, n2, ...): number` | Largest number |\n| `number()` | `number(any): number` | Convert to number |\n| `link()` | `link(path, display?): Link` | Create a link |\n| `list()` | `list(element): List` | Wrap in list if not already |\n| `file()` | `file(path): file` | Get file object |\n| `image()` | `image(path): image` | Create image for rendering |\n| `icon()` | `icon(name): icon` | Lucide icon by name |\n| `html()` | `html(string): html` | Render as HTML |\n| `escapeHTML()` | `escapeHTML(string): string` | Escape HTML characters |\n\n### Date Functions & Fields\n\n**Fields:** `date.year`, `date.month`, `date.day`, `date.hour`, `date.minute`, `date.second`, `date.millisecond`\n\n| Function | Signature | Description |\n|----------|-----------|-------------|\n| `date()` | `date.date(): date` | Remove time portion |\n| `format()` | `date.format(string): string` | Format with Moment.js pattern |\n| `time()` | `date.time(): string` | Get time as string |\n| `relative()` | `date.relative(): string` | Human-readable relative time |\n| `isEmpty()` | `date.isEmpty(): boolean` | Always false for dates |\n\n### Date Arithmetic\n\n```yaml\n# Duration units: y/year/years, M/month/months, d/day/days,\n#                 w/week/weeks, h/hour/hours, m/minute/minutes, s/second/seconds\n\n# Add/subtract durations\n\"date + \\\"1M\\\"\"           # Add 1 month\n\"date - \\\"2h\\\"\"           # Subtract 2 hours\n\"now() + \\\"1 day\\\"\"       # Tomorrow\n\"today() + \\\"7d\\\"\"        # A week from today\n\n# Subtract dates for millisecond difference\n\"now() - file.ctime\"\n\n# Complex duration arithmetic\n\"now() + (duration('1d') * 2)\"\n```\n\n### String Functions\n\n**Field:** `string.length`\n\n| Function | Description |\n|----------|-------------|\n| `contains(value)` | Check substring |\n| `containsAll(...values)` | All substrings present |\n| `containsAny(...values)` | Any substring present |\n| `startsWith(query)` | Starts with query |\n| `endsWith(query)` | Ends with query |\n| `isEmpty()` | Empty or not present |\n| `lower()` | To lowercase |\n| `title()` | To Title Case |\n| `trim()` | Remove whitespace |\n| `replace(pattern, replacement)` | Replace pattern |\n| `repeat(count)` | Repeat string |\n| `reverse()` | Reverse string |\n| `slice(start, end?)` | Substring |\n| `split(separator, n?)` | Split to list |\n\n### Number Functions\n\n| Function | Description |\n|----------|-------------|\n| `abs()` | Absolute value |\n| `ceil()` | Round up |\n| `floor()` | Round down |\n| `round(digits?)` | Round to digits |\n| `toFixed(precision)` | Fixed-point notation |\n| `isEmpty()` | Not present |\n\n### List Functions\n\n**Field:** `list.length`\n\n| Function | Description |\n|----------|-------------|\n| `contains(value)` | Element exists |\n| `containsAll(...values)` | All elements exist |\n| `containsAny(...values)` | Any element exists |\n| `filter(expression)` | Filter by condition (uses `value`, `index`) |\n| `map(expression)` | Transform elements (uses `value`, `index`) |\n| `reduce(expression, initial)` | Reduce to single value (uses `value`, `index`, `acc`) |\n| `flat()` | Flatten nested lists |\n| `join(separator)` | Join to string |\n| `reverse()` | Reverse order |\n| `slice(start, end?)` | Sublist |\n| `sort()` | Sort ascending |\n| `unique()` | Remove duplicates |\n| `isEmpty()` | No elements |\n\n### File Functions\n\n| Function | Description |\n|----------|-------------|\n| `asLink(display?)` | Convert to link |\n| `hasLink(otherFile)` | Has link to file |\n| `hasTag(...tags)` | Has any of the tags |\n| `hasProperty(name)` | Has property |\n| `inFolder(folder)` | In folder or subfolder |\n\n## View Types\n\n### Table View\n\n```yaml\nviews:\n  - type: table\n    name: \"My Table\"\n    order:\n      - file.name\n      - status\n      - due_date\n    summaries:\n      price: Sum\n      count: Average\n```\n\n### Cards View\n\n```yaml\nviews:\n  - type: cards\n    name: \"Gallery\"\n    order:\n      - file.name\n      - cover_image\n      - description\n```\n\n### List View\n\n```yaml\nviews:\n  - type: list\n    name: \"Simple List\"\n    order:\n      - file.name\n      - status\n```\n\n### Map View\n\nRequires latitude/longitude properties and the Maps plugin.\n\n```yaml\nviews:\n  - type: map\n    name: \"Locations\"\n```\n\n## Default Summary Formulas\n\n| Name | Input Type | Description |\n|------|------------|-------------|\n| `Average` | Number | Mathematical mean |\n| `Min` | Number | Smallest number |\n| `Max` | Number | Largest number |\n| `Sum` | Number | Sum of all numbers |\n| `Range` | Number | Max - Min |\n| `Median` | Number | Mathematical median |\n| `Stddev` | Number | Standard deviation |\n| `Earliest` | Date | Earliest date |\n| `Latest` | Date | Latest date |\n| `Checked` | Boolean | Count of true values |\n| `Unchecked` | Boolean | Count of false values |\n| `Empty` | Any | Count of empty values |\n| `Filled` | Any | Count of non-empty values |\n| `Unique` | Any | Count of unique values |\n\n## Complete Examples\n\n### Task Tracker Base\n\n```yaml\nfilters:\n  and:\n    - file.hasTag(\"task\")\n    - 'file.ext == \"md\"'\n\nformulas:\n  days_until_due: 'if(due, ((date(due) - today()) / 86400000).round(0), \"\")'\n  is_overdue: 'if(due, date(due) < today() && status != \"done\", false)'\n  priority_label: 'if(priority == 1, \"High\", if(priority == 2, \"Medium\", \"Low\"))'\n\nproperties:\n  status:\n    displayName: Status\n  formula.days_until_due:\n    displayName: \"Days Until Due\"\n  formula.priority_label:\n    displayName: Priority\n\nviews:\n  - type: table\n    name: \"Active Tasks\"\n    filters:\n      and:\n        - 'status != \"done\"'\n    order:\n      - file.name\n      - status\n      - formula.priority_label\n      - due\n      - formula.days_until_due\n    groupBy:\n      property: status\n      direction: ASC\n    summaries:\n      formula.days_until_due: Average\n\n  - type: table\n    name: \"Completed\"\n    filters:\n      and:\n        - 'status == \"done\"'\n    order:\n      - file.name\n      - completed_date\n```\n\n### Reading List Base\n\n```yaml\nfilters:\n  or:\n    - file.hasTag(\"book\")\n    - file.hasTag(\"article\")\n\nformulas:\n  reading_time: 'if(pages, (pages * 2).toString() + \" min\", \"\")'\n  status_icon: 'if(status == \"reading\", \"reading\", if(status == \"done\", \"done\", \"to-read\"))'\n  year_read: 'if(finished_date, date(finished_date).year, \"\")'\n\nproperties:\n  author:\n    displayName: Author\n  formula.status_icon:\n    displayName: \"\"\n  formula.reading_time:\n    displayName: \"Est. Time\"\n\nviews:\n  - type: cards\n    name: \"Library\"\n    order:\n      - cover\n      - file.name\n      - author\n      - formula.status_icon\n    filters:\n      not:\n        - 'status == \"dropped\"'\n\n  - type: table\n    name: \"Reading List\"\n    filters:\n      and:\n        - 'status == \"to-read\"'\n    order:\n      - file.name\n      - author\n      - pages\n      - formula.reading_time\n```\n\n### Project Notes Base\n\n```yaml\nfilters:\n  and:\n    - file.inFolder(\"Projects\")\n    - 'file.ext == \"md\"'\n\nformulas:\n  last_updated: 'file.mtime.relative()'\n  link_count: 'file.links.length'\n\nsummaries:\n  avgLinks: 'values.filter(value.isType(\"number\")).mean().round(1)'\n\nproperties:\n  formula.last_updated:\n    displayName: \"Updated\"\n  formula.link_count:\n    displayName: \"Links\"\n\nviews:\n  - type: table\n    name: \"All Projects\"\n    order:\n      - file.name\n      - status\n      - formula.last_updated\n      - formula.link_count\n    summaries:\n      formula.link_count: avgLinks\n    groupBy:\n      property: status\n      direction: ASC\n\n  - type: list\n    name: \"Quick List\"\n    order:\n      - file.name\n      - status\n```\n\n### Daily Notes Index\n\n```yaml\nfilters:\n  and:\n    - file.inFolder(\"Daily Notes\")\n    - '/^\\d{4}-\\d{2}-\\d{2}$/.matches(file.basename)'\n\nformulas:\n  word_estimate: '(file.size / 5).round(0)'\n  day_of_week: 'date(file.basename).format(\"dddd\")'\n\nproperties:\n  formula.day_of_week:\n    displayName: \"Day\"\n  formula.word_estimate:\n    displayName: \"~Words\"\n\nviews:\n  - type: table\n    name: \"Recent Notes\"\n    limit: 30\n    order:\n      - file.name\n      - formula.day_of_week\n      - formula.word_estimate\n      - file.mtime\n```\n\n## Embedding Bases\n\nEmbed in Markdown files:\n\n```markdown\n![[MyBase.base]]\n\n<!-- Specific view -->\n![[MyBase.base#View Name]]\n```\n\n## YAML Quoting Rules\n\n- Use single quotes for formulas containing double quotes: `'if(done, \"Yes\", \"No\")'`\n- Use double quotes for simple strings: `\"My View Name\"`\n- Escape nested quotes properly in complex expressions\n\n## Common Patterns\n\n### Filter by Tag\n\n```yaml\nfilters:\n  and:\n    - file.hasTag(\"project\")\n```\n\n### Filter by Folder\n\n```yaml\nfilters:\n  and:\n    - file.inFolder(\"Notes\")\n```\n\n### Filter by Date Range\n\n```yaml\nfilters:\n  and:\n    - 'file.mtime > now() - \"7d\"'\n```\n\n### Filter by Property Value\n\n```yaml\nfilters:\n  and:\n    - 'status == \"active\"'\n    - 'priority >= 3'\n```\n\n### Combine Multiple Conditions\n\n```yaml\nfilters:\n  or:\n    - and:\n        - file.hasTag(\"important\")\n        - 'status != \"done\"'\n    - and:\n        - 'priority == 1'\n        - 'due != \"\"'\n```\n\n## References\n\n- [Bases Syntax](https://help.obsidian.md/bases/syntax)\n- [Functions](https://help.obsidian.md/bases/functions)\n- [Views](https://help.obsidian.md/bases/views)\n- [Formulas](https://help.obsidian.md/formulas)"
              },
              {
                "name": "obsidian-markdown",
                "description": "Create and edit Obsidian Flavored Markdown with wikilinks, embeds, callouts, properties, and other Obsidian-specific syntax. Use when working with .md files in Obsidian, or when the user mentions wikilinks, callouts, frontmatter, tags, embeds, or Obsidian notes.",
                "path": "plugins/all-skills/skills/obsidian-markdown/SKILL.md",
                "frontmatter": {
                  "name": "obsidian-markdown",
                  "category": "document-processing",
                  "description": "Create and edit Obsidian Flavored Markdown with wikilinks, embeds, callouts, properties, and other Obsidian-specific syntax. Use when working with .md files in Obsidian, or when the user mentions wikilinks, callouts, frontmatter, tags, embeds, or Obsidian notes."
                },
                "content": "# Obsidian Flavored Markdown\n\nThis skill enables Claude Code to create and edit valid Obsidian Flavored Markdown including wikilinks, embeds, callouts, properties, and all related syntax.\n\n## When to Use This Skill\n\n- Working with .md files in an Obsidian vault\n- Creating notes with wikilinks or internal links\n- Adding embeds for notes, images, audio, or PDFs\n- Using callouts (info boxes, warnings, tips, etc.)\n- Managing frontmatter/properties in YAML format\n- Working with tags and nested tags\n- Creating block references and block IDs\n\n## Basic Formatting\n\n### Paragraphs and Line Breaks\n\nParagraphs are separated by blank lines. Single line breaks within a paragraph are ignored unless you use:\n- Two spaces at the end of a line\n- Or use `<br>` for explicit breaks\n\n### Headings\n\n```markdown\n# Heading 1\n## Heading 2\n### Heading 3\n#### Heading 4\n##### Heading 5\n###### Heading 6\n```\n\n### Text Styling\n\n```markdown\n**Bold text**\n*Italic text*\n***Bold and italic***\n~~Strikethrough~~\n==Highlighted text==\n```\n\n## Internal Links (Wikilinks)\n\n### Basic Wikilinks\n\n```markdown\n[[Note Name]]\n[[Note Name|Display Text]]\n[[Folder/Note Name]]\n```\n\n### Heading Links\n\n```markdown\n[[Note Name#Heading]]\n[[Note Name#Heading|Display Text]]\n[[#Heading in Current Note]]\n```\n\n### Block References\n\n```markdown\n[[Note Name#^block-id]]\n[[Note Name#^block-id|Display Text]]\n[[#^block-id]]\n```\n\n### Creating Block IDs\n\nAdd a block ID at the end of any paragraph or list item:\n\n```markdown\nThis is a paragraph you can reference. ^my-block-id\n\n- List item with ID ^list-block\n```\n\n## Embeds\n\n### Embedding Notes\n\n```markdown\n![[Note Name]]\n![[Note Name#Heading]]\n![[Note Name#^block-id]]\n```\n\n### Embedding Images\n\n```markdown\n![[image.png]]\n![[image.png|400]]\n![[image.png|400x300]]\n```\n\n### Embedding Audio\n\n```markdown\n![[audio.mp3]]\n```\n\n### Embedding PDFs\n\n```markdown\n![[document.pdf]]\n![[document.pdf#page=5]]\n![[document.pdf#height=400]]\n```\n\n### Embedding Videos\n\n```markdown\n![[video.mp4]]\n```\n\n## Callouts\n\n### Basic Callout Syntax\n\n```markdown\n> [!note]\n> This is a note callout.\n\n> [!warning]\n> This is a warning callout.\n\n> [!tip] Custom Title\n> This callout has a custom title.\n```\n\n### Callout Types\n\n| Type | Aliases | Description |\n|------|---------|-------------|\n| `note` | | Default blue info box |\n| `abstract` | `summary`, `tldr` | Abstract/summary |\n| `info` | | Information |\n| `todo` | | Task/todo item |\n| `tip` | `hint`, `important` | Helpful tip |\n| `success` | `check`, `done` | Success message |\n| `question` | `help`, `faq` | Question/FAQ |\n| `warning` | `caution`, `attention` | Warning message |\n| `failure` | `fail`, `missing` | Failure message |\n| `danger` | `error` | Error/danger |\n| `bug` | | Bug report |\n| `example` | | Example content |\n| `quote` | `cite` | Quotation |\n\n### Foldable Callouts\n\n```markdown\n> [!note]+ Expanded by default\n> Content visible initially.\n\n> [!note]- Collapsed by default\n> Content hidden initially.\n```\n\n### Nested Callouts\n\n```markdown\n> [!question] Can callouts be nested?\n> > [!answer] Yes!\n> > Callouts can be nested inside each other.\n```\n\n## Lists\n\n### Unordered Lists\n\n```markdown\n- Item 1\n- Item 2\n  - Nested item\n  - Another nested item\n- Item 3\n```\n\n### Ordered Lists\n\n```markdown\n1. First item\n2. Second item\n   1. Nested numbered item\n3. Third item\n```\n\n### Task Lists\n\n```markdown\n- [ ] Uncompleted task\n- [x] Completed task\n- [ ] Another task\n```\n\n## Code Blocks\n\n### Inline Code\n\n```markdown\nUse `inline code` for short snippets.\n```\n\n### Fenced Code Blocks\n\n````markdown\n```javascript\nfunction hello() {\n  console.log(\"Hello, world!\");\n}\n```\n````\n\n### Supported Languages\n\nObsidian supports syntax highlighting for many languages including:\n`javascript`, `typescript`, `python`, `rust`, `go`, `java`, `c`, `cpp`, `csharp`, `ruby`, `php`, `html`, `css`, `json`, `yaml`, `markdown`, `bash`, `sql`, and many more.\n\n## Tables\n\n```markdown\n| Header 1 | Header 2 | Header 3 |\n|----------|:--------:|---------:|\n| Left     | Center   | Right    |\n| aligned  | aligned  | aligned  |\n```\n\n## Math (LaTeX)\n\n### Inline Math\n\n```markdown\nThe equation $E = mc^2$ is famous.\n```\n\n### Block Math\n\n```markdown\n$$\n\\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}\n$$\n```\n\n## Diagrams (Mermaid)\n\n````markdown\n```mermaid\ngraph TD\n    A[Start] --> B{Decision}\n    B -->|Yes| C[Do Something]\n    B -->|No| D[Do Something Else]\n    C --> E[End]\n    D --> E\n```\n````\n\n## Footnotes\n\n```markdown\nThis is a sentence with a footnote.[^1]\n\n[^1]: This is the footnote content.\n```\n\n## Comments\n\n```markdown\n%%\nThis is a comment that won't be rendered.\n%%\n\nInline %%comment%% within text.\n```\n\n## Properties (Frontmatter)\n\n### Basic Properties\n\n```yaml\n---\ntitle: My Note Title\ndate: 2024-01-15\ntags:\n  - tag1\n  - tag2\nauthor: John Doe\n---\n```\n\n### Property Types\n\n| Type | Example |\n|------|---------|\n| Text | `title: My Title` |\n| Number | `rating: 5` |\n| Checkbox | `completed: true` |\n| Date | `date: 2024-01-15` |\n| Date & time | `created: 2024-01-15T10:30:00` |\n| List | `tags: [a, b, c]` or multiline |\n| Link | `related: \"[[Other Note]]\"` |\n\n### Multi-value Properties\n\n```yaml\n---\ntags:\n  - project\n  - work\n  - important\naliases:\n  - My Alias\n  - Another Name\ncssclasses:\n  - wide-page\n  - cards\n---\n```\n\n## Tags\n\n### Inline Tags\n\n```markdown\nThis note is about #productivity and #tools.\n```\n\n### Nested Tags\n\n```markdown\n#project/work\n#status/in-progress\n#priority/high\n```\n\n### Tags in Frontmatter\n\n```yaml\n---\ntags:\n  - project\n  - project/work\n  - status/active\n---\n```\n\n## HTML Support\n\nObsidian supports a subset of HTML:\n\n```markdown\n<div class=\"my-class\">\n  Custom HTML content\n</div>\n\n<details>\n<summary>Click to expand</summary>\nHidden content here\n</details>\n\n<kbd>Ctrl</kbd> + <kbd>C</kbd>\n```\n\n## Complete Example\n\n```markdown\n---\ntitle: Project Alpha Overview\ndate: 2024-01-15\ntags:\n  - project\n  - documentation\nstatus: active\n---\n\n# Project Alpha Overview\n\n## Summary\n\nThis document outlines the key aspects of **Project Alpha**. For related materials, see [[Project Alpha/Resources]] and [[Team Members]].\n\n> [!info] Quick Facts\n> - Start Date: January 2024\n> - Team Size: 5 members\n> - Status: Active\n\n## Key Features\n\n1. [[Feature A]] - Core functionality\n2. [[Feature B]] - User interface\n3. [[Feature C]] - API integration\n\n### Feature A Details\n\nThe main equation governing our approach is $f(x) = ax^2 + bx + c$.\n\n![[feature-a-diagram.png|500]]\n\n> [!tip] Implementation Note\n> See [[Technical Specs#^impl-note]] for implementation details.\n\n## Tasks\n\n- [x] Initial planning ^planning-task\n- [ ] Development phase\n- [ ] Testing phase\n- [ ] Deployment\n\n## Code Example\n\n```python\ndef process_data(input):\n    return transform(input)\n```\n\n## Architecture\n\n```mermaid\ngraph LR\n    A[Input] --> B[Process]\n    B --> C[Output]\n```\n\n## Notes\n\nThis approach was inspired by ==recent research==[^1].\n\n[^1]: Smith, J. (2024). Modern Approaches to Data Processing.\n\n%%\nTODO: Add more examples\nReview with team next week\n%%\n\n#project/alpha #documentation\n```\n\n## References\n\n- [Obsidian Formatting Syntax](https://help.obsidian.md/Editing+and+formatting/Basic+formatting+syntax)\n- [Advanced Formatting](https://help.obsidian.md/Editing+and+formatting/Advanced+formatting+syntax)\n- [Internal Links](https://help.obsidian.md/Linking+notes+and+files/Internal+links)\n- [Embedding Files](https://help.obsidian.md/Linking+notes+and+files/Embed+files)\n- [Callouts](https://help.obsidian.md/Editing+and+formatting/Callouts)\n- [Properties](https://help.obsidian.md/Editing+and+formatting/Properties)"
              },
              {
                "name": "pdf",
                "description": "Comprehensive PDF manipulation toolkit for extracting text and tables, creating new PDFs, merging/splitting documents, and handling forms. When Claude needs to fill in a PDF form or programmatically process, generate, or analyze PDF documents at scale.",
                "path": "plugins/all-skills/skills/pdf/SKILL.md",
                "frontmatter": {
                  "name": "pdf",
                  "description": "Comprehensive PDF manipulation toolkit for extracting text and tables, creating new PDFs, merging/splitting documents, and handling forms. When Claude needs to fill in a PDF form or programmatically process, generate, or analyze PDF documents at scale.",
                  "category": "document-processing",
                  "license": "Proprietary. LICENSE.txt has complete terms"
                },
                "content": "# PDF Processing Guide\n\n## Overview\n\nThis guide covers essential PDF processing operations using Python libraries and command-line tools. For advanced features, JavaScript libraries, and detailed examples, see reference.md. If you need to fill out a PDF form, read forms.md and follow its instructions.\n\n## Quick Start\n\n```python\nfrom pypdf import PdfReader, PdfWriter\n\n# Read a PDF\nreader = PdfReader(\"document.pdf\")\nprint(f\"Pages: {len(reader.pages)}\")\n\n# Extract text\ntext = \"\"\nfor page in reader.pages:\n    text += page.extract_text()\n```\n\n## Python Libraries\n\n### pypdf - Basic Operations\n\n#### Merge PDFs\n```python\nfrom pypdf import PdfWriter, PdfReader\n\nwriter = PdfWriter()\nfor pdf_file in [\"doc1.pdf\", \"doc2.pdf\", \"doc3.pdf\"]:\n    reader = PdfReader(pdf_file)\n    for page in reader.pages:\n        writer.add_page(page)\n\nwith open(\"merged.pdf\", \"wb\") as output:\n    writer.write(output)\n```\n\n#### Split PDF\n```python\nreader = PdfReader(\"input.pdf\")\nfor i, page in enumerate(reader.pages):\n    writer = PdfWriter()\n    writer.add_page(page)\n    with open(f\"page_{i+1}.pdf\", \"wb\") as output:\n        writer.write(output)\n```\n\n#### Extract Metadata\n```python\nreader = PdfReader(\"document.pdf\")\nmeta = reader.metadata\nprint(f\"Title: {meta.title}\")\nprint(f\"Author: {meta.author}\")\nprint(f\"Subject: {meta.subject}\")\nprint(f\"Creator: {meta.creator}\")\n```\n\n#### Rotate Pages\n```python\nreader = PdfReader(\"input.pdf\")\nwriter = PdfWriter()\n\npage = reader.pages[0]\npage.rotate(90)  # Rotate 90 degrees clockwise\nwriter.add_page(page)\n\nwith open(\"rotated.pdf\", \"wb\") as output:\n    writer.write(output)\n```\n\n### pdfplumber - Text and Table Extraction\n\n#### Extract Text with Layout\n```python\nimport pdfplumber\n\nwith pdfplumber.open(\"document.pdf\") as pdf:\n    for page in pdf.pages:\n        text = page.extract_text()\n        print(text)\n```\n\n#### Extract Tables\n```python\nwith pdfplumber.open(\"document.pdf\") as pdf:\n    for i, page in enumerate(pdf.pages):\n        tables = page.extract_tables()\n        for j, table in enumerate(tables):\n            print(f\"Table {j+1} on page {i+1}:\")\n            for row in table:\n                print(row)\n```\n\n#### Advanced Table Extraction\n```python\nimport pandas as pd\n\nwith pdfplumber.open(\"document.pdf\") as pdf:\n    all_tables = []\n    for page in pdf.pages:\n        tables = page.extract_tables()\n        for table in tables:\n            if table:  # Check if table is not empty\n                df = pd.DataFrame(table[1:], columns=table[0])\n                all_tables.append(df)\n\n# Combine all tables\nif all_tables:\n    combined_df = pd.concat(all_tables, ignore_index=True)\n    combined_df.to_excel(\"extracted_tables.xlsx\", index=False)\n```\n\n### reportlab - Create PDFs\n\n#### Basic PDF Creation\n```python\nfrom reportlab.lib.pagesizes import letter\nfrom reportlab.pdfgen import canvas\n\nc = canvas.Canvas(\"hello.pdf\", pagesize=letter)\nwidth, height = letter\n\n# Add text\nc.drawString(100, height - 100, \"Hello World!\")\nc.drawString(100, height - 120, \"This is a PDF created with reportlab\")\n\n# Add a line\nc.line(100, height - 140, 400, height - 140)\n\n# Save\nc.save()\n```\n\n#### Create PDF with Multiple Pages\n```python\nfrom reportlab.lib.pagesizes import letter\nfrom reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, PageBreak\nfrom reportlab.lib.styles import getSampleStyleSheet\n\ndoc = SimpleDocTemplate(\"report.pdf\", pagesize=letter)\nstyles = getSampleStyleSheet()\nstory = []\n\n# Add content\ntitle = Paragraph(\"Report Title\", styles['Title'])\nstory.append(title)\nstory.append(Spacer(1, 12))\n\nbody = Paragraph(\"This is the body of the report. \" * 20, styles['Normal'])\nstory.append(body)\nstory.append(PageBreak())\n\n# Page 2\nstory.append(Paragraph(\"Page 2\", styles['Heading1']))\nstory.append(Paragraph(\"Content for page 2\", styles['Normal']))\n\n# Build PDF\ndoc.build(story)\n```\n\n## Command-Line Tools\n\n### pdftotext (poppler-utils)\n```bash\n# Extract text\npdftotext input.pdf output.txt\n\n# Extract text preserving layout\npdftotext -layout input.pdf output.txt\n\n# Extract specific pages\npdftotext -f 1 -l 5 input.pdf output.txt  # Pages 1-5\n```\n\n### qpdf\n```bash\n# Merge PDFs\nqpdf --empty --pages file1.pdf file2.pdf -- merged.pdf\n\n# Split pages\nqpdf input.pdf --pages . 1-5 -- pages1-5.pdf\nqpdf input.pdf --pages . 6-10 -- pages6-10.pdf\n\n# Rotate pages\nqpdf input.pdf output.pdf --rotate=+90:1  # Rotate page 1 by 90 degrees\n\n# Remove password\nqpdf --password=mypassword --decrypt encrypted.pdf decrypted.pdf\n```\n\n### pdftk (if available)\n```bash\n# Merge\npdftk file1.pdf file2.pdf cat output merged.pdf\n\n# Split\npdftk input.pdf burst\n\n# Rotate\npdftk input.pdf rotate 1east output rotated.pdf\n```\n\n## Common Tasks\n\n### Extract Text from Scanned PDFs\n```python\n# Requires: pip install pytesseract pdf2image\nimport pytesseract\nfrom pdf2image import convert_from_path\n\n# Convert PDF to images\nimages = convert_from_path('scanned.pdf')\n\n# OCR each page\ntext = \"\"\nfor i, image in enumerate(images):\n    text += f\"Page {i+1}:\\n\"\n    text += pytesseract.image_to_string(image)\n    text += \"\\n\\n\"\n\nprint(text)\n```\n\n### Add Watermark\n```python\nfrom pypdf import PdfReader, PdfWriter\n\n# Create watermark (or load existing)\nwatermark = PdfReader(\"watermark.pdf\").pages[0]\n\n# Apply to all pages\nreader = PdfReader(\"document.pdf\")\nwriter = PdfWriter()\n\nfor page in reader.pages:\n    page.merge_page(watermark)\n    writer.add_page(page)\n\nwith open(\"watermarked.pdf\", \"wb\") as output:\n    writer.write(output)\n```\n\n### Extract Images\n```bash\n# Using pdfimages (poppler-utils)\npdfimages -j input.pdf output_prefix\n\n# This extracts all images as output_prefix-000.jpg, output_prefix-001.jpg, etc.\n```\n\n### Password Protection\n```python\nfrom pypdf import PdfReader, PdfWriter\n\nreader = PdfReader(\"input.pdf\")\nwriter = PdfWriter()\n\nfor page in reader.pages:\n    writer.add_page(page)\n\n# Add password\nwriter.encrypt(\"userpassword\", \"ownerpassword\")\n\nwith open(\"encrypted.pdf\", \"wb\") as output:\n    writer.write(output)\n```\n\n## Quick Reference\n\n| Task | Best Tool | Command/Code |\n|------|-----------|--------------|\n| Merge PDFs | pypdf | `writer.add_page(page)` |\n| Split PDFs | pypdf | One page per file |\n| Extract text | pdfplumber | `page.extract_text()` |\n| Extract tables | pdfplumber | `page.extract_tables()` |\n| Create PDFs | reportlab | Canvas or Platypus |\n| Command line merge | qpdf | `qpdf --empty --pages ...` |\n| OCR scanned PDFs | pytesseract | Convert to image first |\n| Fill PDF forms | pdf-lib or pypdf (see forms.md) | See forms.md |\n\n## Next Steps\n\n- For advanced pypdfium2 usage, see reference.md\n- For JavaScript libraries (pdf-lib), see reference.md\n- If you need to fill out a PDF form, follow the instructions in forms.md\n- For troubleshooting guides, see reference.md"
              },
              {
                "name": "pptx",
                "description": "Presentation creation, editing, and analysis. When Claude needs to work with presentations (.pptx files) for: (1) Creating new presentations, (2) Modifying or editing content, (3) Working with layouts, (4) Adding comments or speaker notes, or any other presentation tasks",
                "path": "plugins/all-skills/skills/pptx/SKILL.md",
                "frontmatter": {
                  "name": "pptx",
                  "description": "Presentation creation, editing, and analysis. When Claude needs to work with presentations (.pptx files) for: (1) Creating new presentations, (2) Modifying or editing content, (3) Working with layouts, (4) Adding comments or speaker notes, or any other presentation tasks",
                  "category": "document-processing",
                  "license": "Proprietary. LICENSE.txt has complete terms"
                },
                "content": "# PPTX creation, editing, and analysis\n\n## Overview\n\nA user may ask you to create, edit, or analyze the contents of a .pptx file. A .pptx file is essentially a ZIP archive containing XML files and other resources that you can read or edit. You have different tools and workflows available for different tasks.\n\n## Reading and analyzing content\n\n### Text extraction\nIf you just need to read the text contents of a presentation, you should convert the document to markdown:\n\n```bash\n# Convert document to markdown\npython -m markitdown path-to-file.pptx\n```\n\n### Raw XML access\nYou need raw XML access for: comments, speaker notes, slide layouts, animations, design elements, and complex formatting. For any of these features, you'll need to unpack a presentation and read its raw XML contents.\n\n#### Unpacking a file\n`python ooxml/scripts/unpack.py <office_file> <output_dir>`\n\n**Note**: The unpack.py script is located at `skills/pptx/ooxml/scripts/unpack.py` relative to the project root. If the script doesn't exist at this path, use `find . -name \"unpack.py\"` to locate it.\n\n#### Key file structures\n* `ppt/presentation.xml` - Main presentation metadata and slide references\n* `ppt/slides/slide{N}.xml` - Individual slide contents (slide1.xml, slide2.xml, etc.)\n* `ppt/notesSlides/notesSlide{N}.xml` - Speaker notes for each slide\n* `ppt/comments/modernComment_*.xml` - Comments for specific slides\n* `ppt/slideLayouts/` - Layout templates for slides\n* `ppt/slideMasters/` - Master slide templates\n* `ppt/theme/` - Theme and styling information\n* `ppt/media/` - Images and other media files\n\n#### Typography and color extraction\n**When given an example design to emulate**: Always analyze the presentation's typography and colors first using the methods below:\n1. **Read theme file**: Check `ppt/theme/theme1.xml` for colors (`<a:clrScheme>`) and fonts (`<a:fontScheme>`)\n2. **Sample slide content**: Examine `ppt/slides/slide1.xml` for actual font usage (`<a:rPr>`) and colors\n3. **Search for patterns**: Use grep to find color (`<a:solidFill>`, `<a:srgbClr>`) and font references across all XML files\n\n## Creating a new PowerPoint presentation **without a template**\n\nWhen creating a new PowerPoint presentation from scratch, use the **html2pptx** workflow to convert HTML slides to PowerPoint with accurate positioning.\n\n### Design Principles\n\n**CRITICAL**: Before creating any presentation, analyze the content and choose appropriate design elements:\n1. **Consider the subject matter**: What is this presentation about? What tone, industry, or mood does it suggest?\n2. **Check for branding**: If the user mentions a company/organization, consider their brand colors and identity\n3. **Match palette to content**: Select colors that reflect the subject\n4. **State your approach**: Explain your design choices before writing code\n\n**Requirements**:\n-  State your content-informed design approach BEFORE writing code\n-  Use web-safe fonts only: Arial, Helvetica, Times New Roman, Georgia, Courier New, Verdana, Tahoma, Trebuchet MS, Impact\n-  Create clear visual hierarchy through size, weight, and color\n-  Ensure readability: strong contrast, appropriately sized text, clean alignment\n-  Be consistent: repeat patterns, spacing, and visual language across slides\n\n#### Color Palette Selection\n\n**Choosing colors creatively**:\n- **Think beyond defaults**: What colors genuinely match this specific topic? Avoid autopilot choices.\n- **Consider multiple angles**: Topic, industry, mood, energy level, target audience, brand identity (if mentioned)\n- **Be adventurous**: Try unexpected combinations - a healthcare presentation doesn't have to be green, finance doesn't have to be navy\n- **Build your palette**: Pick 3-5 colors that work together (dominant colors + supporting tones + accent)\n- **Ensure contrast**: Text must be clearly readable on backgrounds\n\n**Example color palettes** (use these to spark creativity - choose one, adapt it, or create your own):\n\n1. **Classic Blue**: Deep navy (#1C2833), slate gray (#2E4053), silver (#AAB7B8), off-white (#F4F6F6)\n2. **Teal & Coral**: Teal (#5EA8A7), deep teal (#277884), coral (#FE4447), white (#FFFFFF)\n3. **Bold Red**: Red (#C0392B), bright red (#E74C3C), orange (#F39C12), yellow (#F1C40F), green (#2ECC71)\n4. **Warm Blush**: Mauve (#A49393), blush (#EED6D3), rose (#E8B4B8), cream (#FAF7F2)\n5. **Burgundy Luxury**: Burgundy (#5D1D2E), crimson (#951233), rust (#C15937), gold (#997929)\n6. **Deep Purple & Emerald**: Purple (#B165FB), dark blue (#181B24), emerald (#40695B), white (#FFFFFF)\n7. **Cream & Forest Green**: Cream (#FFE1C7), forest green (#40695B), white (#FCFCFC)\n8. **Pink & Purple**: Pink (#F8275B), coral (#FF574A), rose (#FF737D), purple (#3D2F68)\n9. **Lime & Plum**: Lime (#C5DE82), plum (#7C3A5F), coral (#FD8C6E), blue-gray (#98ACB5)\n10. **Black & Gold**: Gold (#BF9A4A), black (#000000), cream (#F4F6F6)\n11. **Sage & Terracotta**: Sage (#87A96B), terracotta (#E07A5F), cream (#F4F1DE), charcoal (#2C2C2C)\n12. **Charcoal & Red**: Charcoal (#292929), red (#E33737), light gray (#CCCBCB)\n13. **Vibrant Orange**: Orange (#F96D00), light gray (#F2F2F2), charcoal (#222831)\n14. **Forest Green**: Black (#191A19), green (#4E9F3D), dark green (#1E5128), white (#FFFFFF)\n15. **Retro Rainbow**: Purple (#722880), pink (#D72D51), orange (#EB5C18), amber (#F08800), gold (#DEB600)\n16. **Vintage Earthy**: Mustard (#E3B448), sage (#CBD18F), forest green (#3A6B35), cream (#F4F1DE)\n17. **Coastal Rose**: Old rose (#AD7670), beaver (#B49886), eggshell (#F3ECDC), ash gray (#BFD5BE)\n18. **Orange & Turquoise**: Light orange (#FC993E), grayish turquoise (#667C6F), white (#FCFCFC)\n\n#### Visual Details Options\n\n**Geometric Patterns**:\n- Diagonal section dividers instead of horizontal\n- Asymmetric column widths (30/70, 40/60, 25/75)\n- Rotated text headers at 90 or 270\n- Circular/hexagonal frames for images\n- Triangular accent shapes in corners\n- Overlapping shapes for depth\n\n**Border & Frame Treatments**:\n- Thick single-color borders (10-20pt) on one side only\n- Double-line borders with contrasting colors\n- Corner brackets instead of full frames\n- L-shaped borders (top+left or bottom+right)\n- Underline accents beneath headers (3-5pt thick)\n\n**Typography Treatments**:\n- Extreme size contrast (72pt headlines vs 11pt body)\n- All-caps headers with wide letter spacing\n- Numbered sections in oversized display type\n- Monospace (Courier New) for data/stats/technical content\n- Condensed fonts (Arial Narrow) for dense information\n- Outlined text for emphasis\n\n**Chart & Data Styling**:\n- Monochrome charts with single accent color for key data\n- Horizontal bar charts instead of vertical\n- Dot plots instead of bar charts\n- Minimal gridlines or none at all\n- Data labels directly on elements (no legends)\n- Oversized numbers for key metrics\n\n**Layout Innovations**:\n- Full-bleed images with text overlays\n- Sidebar column (20-30% width) for navigation/context\n- Modular grid systems (33, 44 blocks)\n- Z-pattern or F-pattern content flow\n- Floating text boxes over colored shapes\n- Magazine-style multi-column layouts\n\n**Background Treatments**:\n- Solid color blocks occupying 40-60% of slide\n- Gradient fills (vertical or diagonal only)\n- Split backgrounds (two colors, diagonal or vertical)\n- Edge-to-edge color bands\n- Negative space as a design element\n\n### Layout Tips\n**When creating slides with charts or tables:**\n- **Two-column layout (PREFERRED)**: Use a header spanning the full width, then two columns below - text/bullets in one column and the featured content in the other. This provides better balance and makes charts/tables more readable. Use flexbox with unequal column widths (e.g., 40%/60% split) to optimize space for each content type.\n- **Full-slide layout**: Let the featured content (chart/table) take up the entire slide for maximum impact and readability\n- **NEVER vertically stack**: Do not place charts/tables below text in a single column - this causes poor readability and layout issues\n\n### Workflow\n1. **MANDATORY - READ ENTIRE FILE**: Read [`html2pptx.md`](html2pptx.md) completely from start to finish. **NEVER set any range limits when reading this file.** Read the full file content for detailed syntax, critical formatting rules, and best practices before proceeding with presentation creation.\n2. Create an HTML file for each slide with proper dimensions (e.g., 720pt  405pt for 16:9)\n   - Use `<p>`, `<h1>`-`<h6>`, `<ul>`, `<ol>` for all text content\n   - Use `class=\"placeholder\"` for areas where charts/tables will be added (render with gray background for visibility)\n   - **CRITICAL**: Rasterize gradients and icons as PNG images FIRST using Sharp, then reference in HTML\n   - **LAYOUT**: For slides with charts/tables/images, use either full-slide layout or two-column layout for better readability\n3. Create and run a JavaScript file using the [`html2pptx.js`](scripts/html2pptx.js) library to convert HTML slides to PowerPoint and save the presentation\n   - Use the `html2pptx()` function to process each HTML file\n   - Add charts and tables to placeholder areas using PptxGenJS API\n   - Save the presentation using `pptx.writeFile()`\n4. **Visual validation**: Generate thumbnails and inspect for layout issues\n   - Create thumbnail grid: `python scripts/thumbnail.py output.pptx workspace/thumbnails --cols 4`\n   - Read and carefully examine the thumbnail image for:\n     - **Text cutoff**: Text being cut off by header bars, shapes, or slide edges\n     - **Text overlap**: Text overlapping with other text or shapes\n     - **Positioning issues**: Content too close to slide boundaries or other elements\n     - **Contrast issues**: Insufficient contrast between text and backgrounds\n   - If issues found, adjust HTML margins/spacing/colors and regenerate the presentation\n   - Repeat until all slides are visually correct\n\n## Editing an existing PowerPoint presentation\n\nWhen edit slides in an existing PowerPoint presentation, you need to work with the raw Office Open XML (OOXML) format. This involves unpacking the .pptx file, editing the XML content, and repacking it.\n\n### Workflow\n1. **MANDATORY - READ ENTIRE FILE**: Read [`ooxml.md`](ooxml.md) (~500 lines) completely from start to finish.  **NEVER set any range limits when reading this file.**  Read the full file content for detailed guidance on OOXML structure and editing workflows before any presentation editing.\n2. Unpack the presentation: `python ooxml/scripts/unpack.py <office_file> <output_dir>`\n3. Edit the XML files (primarily `ppt/slides/slide{N}.xml` and related files)\n4. **CRITICAL**: Validate immediately after each edit and fix any validation errors before proceeding: `python ooxml/scripts/validate.py <dir> --original <file>`\n5. Pack the final presentation: `python ooxml/scripts/pack.py <input_directory> <office_file>`\n\n## Creating a new PowerPoint presentation **using a template**\n\nWhen you need to create a presentation that follows an existing template's design, you'll need to duplicate and re-arrange template slides before then replacing placeholder context.\n\n### Workflow\n1. **Extract template text AND create visual thumbnail grid**:\n   * Extract text: `python -m markitdown template.pptx > template-content.md`\n   * Read `template-content.md`: Read the entire file to understand the contents of the template presentation. **NEVER set any range limits when reading this file.**\n   * Create thumbnail grids: `python scripts/thumbnail.py template.pptx`\n   * See [Creating Thumbnail Grids](#creating-thumbnail-grids) section for more details\n\n2. **Analyze template and save inventory to a file**:\n   * **Visual Analysis**: Review thumbnail grid(s) to understand slide layouts, design patterns, and visual structure\n   * Create and save a template inventory file at `template-inventory.md` containing:\n     ```markdown\n     # Template Inventory Analysis\n     **Total Slides: [count]**\n     **IMPORTANT: Slides are 0-indexed (first slide = 0, last slide = count-1)**\n\n     ## [Category Name]\n     - Slide 0: [Layout code if available] - Description/purpose\n     - Slide 1: [Layout code] - Description/purpose\n     - Slide 2: [Layout code] - Description/purpose\n     [... EVERY slide must be listed individually with its index ...]\n     ```\n   * **Using the thumbnail grid**: Reference the visual thumbnails to identify:\n     - Layout patterns (title slides, content layouts, section dividers)\n     - Image placeholder locations and counts\n     - Design consistency across slide groups\n     - Visual hierarchy and structure\n   * This inventory file is REQUIRED for selecting appropriate templates in the next step\n\n3. **Create presentation outline based on template inventory**:\n   * Review available templates from step 2.\n   * Choose an intro or title template for the first slide. This should be one of the first templates.\n   * Choose safe, text-based layouts for the other slides.\n   * **CRITICAL: Match layout structure to actual content**:\n     - Single-column layouts: Use for unified narrative or single topic\n     - Two-column layouts: Use ONLY when you have exactly 2 distinct items/concepts\n     - Three-column layouts: Use ONLY when you have exactly 3 distinct items/concepts\n     - Image + text layouts: Use ONLY when you have actual images to insert\n     - Quote layouts: Use ONLY for actual quotes from people (with attribution), never for emphasis\n     - Never use layouts with more placeholders than you have content\n     - If you have 2 items, don't force them into a 3-column layout\n     - If you have 4+ items, consider breaking into multiple slides or using a list format\n   * Count your actual content pieces BEFORE selecting the layout\n   * Verify each placeholder in the chosen layout will be filled with meaningful content\n   * Select one option representing the **best** layout for each content section.\n   * Save `outline.md` with content AND template mapping that leverages available designs\n   * Example template mapping:\n      ```\n      # Template slides to use (0-based indexing)\n      # WARNING: Verify indices are within range! Template with 73 slides has indices 0-72\n      # Mapping: slide numbers from outline -> template slide indices\n      template_mapping = [\n          0,   # Use slide 0 (Title/Cover)\n          34,  # Use slide 34 (B1: Title and body)\n          34,  # Use slide 34 again (duplicate for second B1)\n          50,  # Use slide 50 (E1: Quote)\n          54,  # Use slide 54 (F2: Closing + Text)\n      ]\n      ```\n\n4. **Duplicate, reorder, and delete slides using `rearrange.py`**:\n   * Use the `scripts/rearrange.py` script to create a new presentation with slides in the desired order:\n     ```bash\n     python scripts/rearrange.py template.pptx working.pptx 0,34,34,50,52\n     ```\n   * The script handles duplicating repeated slides, deleting unused slides, and reordering automatically\n   * Slide indices are 0-based (first slide is 0, second is 1, etc.)\n   * The same slide index can appear multiple times to duplicate that slide\n\n5. **Extract ALL text using the `inventory.py` script**:\n   * **Run inventory extraction**:\n     ```bash\n     python scripts/inventory.py working.pptx text-inventory.json\n     ```\n   * **Read text-inventory.json**: Read the entire text-inventory.json file to understand all shapes and their properties. **NEVER set any range limits when reading this file.**\n\n   * The inventory JSON structure:\n      ```json\n        {\n          \"slide-0\": {\n            \"shape-0\": {\n              \"placeholder_type\": \"TITLE\",  // or null for non-placeholders\n              \"left\": 1.5,                  // position in inches\n              \"top\": 2.0,\n              \"width\": 7.5,\n              \"height\": 1.2,\n              \"paragraphs\": [\n                {\n                  \"text\": \"Paragraph text\",\n                  // Optional properties (only included when non-default):\n                  \"bullet\": true,           // explicit bullet detected\n                  \"level\": 0,               // only included when bullet is true\n                  \"alignment\": \"CENTER\",    // CENTER, RIGHT (not LEFT)\n                  \"space_before\": 10.0,     // space before paragraph in points\n                  \"space_after\": 6.0,       // space after paragraph in points\n                  \"line_spacing\": 22.4,     // line spacing in points\n                  \"font_name\": \"Arial\",     // from first run\n                  \"font_size\": 14.0,        // in points\n                  \"bold\": true,\n                  \"italic\": false,\n                  \"underline\": false,\n                  \"color\": \"FF0000\"         // RGB color\n                }\n              ]\n            }\n          }\n        }\n      ```\n\n   * Key features:\n     - **Slides**: Named as \"slide-0\", \"slide-1\", etc.\n     - **Shapes**: Ordered by visual position (top-to-bottom, left-to-right) as \"shape-0\", \"shape-1\", etc.\n     - **Placeholder types**: TITLE, CENTER_TITLE, SUBTITLE, BODY, OBJECT, or null\n     - **Default font size**: `default_font_size` in points extracted from layout placeholders (when available)\n     - **Slide numbers are filtered**: Shapes with SLIDE_NUMBER placeholder type are automatically excluded from inventory\n     - **Bullets**: When `bullet: true`, `level` is always included (even if 0)\n     - **Spacing**: `space_before`, `space_after`, and `line_spacing` in points (only included when set)\n     - **Colors**: `color` for RGB (e.g., \"FF0000\"), `theme_color` for theme colors (e.g., \"DARK_1\")\n     - **Properties**: Only non-default values are included in the output\n\n6. **Generate replacement text and save the data to a JSON file**\n   Based on the text inventory from the previous step:\n   - **CRITICAL**: First verify which shapes exist in the inventory - only reference shapes that are actually present\n   - **VALIDATION**: The replace.py script will validate that all shapes in your replacement JSON exist in the inventory\n     - If you reference a non-existent shape, you'll get an error showing available shapes\n     - If you reference a non-existent slide, you'll get an error indicating the slide doesn't exist\n     - All validation errors are shown at once before the script exits\n   - **IMPORTANT**: The replace.py script uses inventory.py internally to identify ALL text shapes\n   - **AUTOMATIC CLEARING**: ALL text shapes from the inventory will be cleared unless you provide \"paragraphs\" for them\n   - Add a \"paragraphs\" field to shapes that need content (not \"replacement_paragraphs\")\n   - Shapes without \"paragraphs\" in the replacement JSON will have their text cleared automatically\n   - Paragraphs with bullets will be automatically left aligned. Don't set the `alignment` property on when `\"bullet\": true`\n   - Generate appropriate replacement content for placeholder text\n   - Use shape size to determine appropriate content length\n   - **CRITICAL**: Include paragraph properties from the original inventory - don't just provide text\n   - **IMPORTANT**: When bullet: true, do NOT include bullet symbols (, -, *) in text - they're added automatically\n   - **ESSENTIAL FORMATTING RULES**:\n     - Headers/titles should typically have `\"bold\": true`\n     - List items should have `\"bullet\": true, \"level\": 0` (level is required when bullet is true)\n     - Preserve any alignment properties (e.g., `\"alignment\": \"CENTER\"` for centered text)\n     - Include font properties when different from default (e.g., `\"font_size\": 14.0`, `\"font_name\": \"Lora\"`)\n     - Colors: Use `\"color\": \"FF0000\"` for RGB or `\"theme_color\": \"DARK_1\"` for theme colors\n     - The replacement script expects **properly formatted paragraphs**, not just text strings\n     - **Overlapping shapes**: Prefer shapes with larger default_font_size or more appropriate placeholder_type\n   - Save the updated inventory with replacements to `replacement-text.json`\n   - **WARNING**: Different template layouts have different shape counts - always check the actual inventory before creating replacements\n\n   Example paragraphs field showing proper formatting:\n   ```json\n   \"paragraphs\": [\n     {\n       \"text\": \"New presentation title text\",\n       \"alignment\": \"CENTER\",\n       \"bold\": true\n     },\n     {\n       \"text\": \"Section Header\",\n       \"bold\": true\n     },\n     {\n       \"text\": \"First bullet point without bullet symbol\",\n       \"bullet\": true,\n       \"level\": 0\n     },\n     {\n       \"text\": \"Red colored text\",\n       \"color\": \"FF0000\"\n     },\n     {\n       \"text\": \"Theme colored text\",\n       \"theme_color\": \"DARK_1\"\n     },\n     {\n       \"text\": \"Regular paragraph text without special formatting\"\n     }\n   ]\n   ```\n\n   **Shapes not listed in the replacement JSON are automatically cleared**:\n   ```json\n   {\n     \"slide-0\": {\n       \"shape-0\": {\n         \"paragraphs\": [...] // This shape gets new text\n       }\n       // shape-1 and shape-2 from inventory will be cleared automatically\n     }\n   }\n   ```\n\n   **Common formatting patterns for presentations**:\n   - Title slides: Bold text, sometimes centered\n   - Section headers within slides: Bold text\n   - Bullet lists: Each item needs `\"bullet\": true, \"level\": 0`\n   - Body text: Usually no special properties needed\n   - Quotes: May have special alignment or font properties\n\n7. **Apply replacements using the `replace.py` script**\n   ```bash\n   python scripts/replace.py working.pptx replacement-text.json output.pptx\n   ```\n\n   The script will:\n   - First extract the inventory of ALL text shapes using functions from inventory.py\n   - Validate that all shapes in the replacement JSON exist in the inventory\n   - Clear text from ALL shapes identified in the inventory\n   - Apply new text only to shapes with \"paragraphs\" defined in the replacement JSON\n   - Preserve formatting by applying paragraph properties from the JSON\n   - Handle bullets, alignment, font properties, and colors automatically\n   - Save the updated presentation\n\n   Example validation errors:\n   ```\n   ERROR: Invalid shapes in replacement JSON:\n     - Shape 'shape-99' not found on 'slide-0'. Available shapes: shape-0, shape-1, shape-4\n     - Slide 'slide-999' not found in inventory\n   ```\n\n   ```\n   ERROR: Replacement text made overflow worse in these shapes:\n     - slide-0/shape-2: overflow worsened by 1.25\" (was 0.00\", now 1.25\")\n   ```\n\n## Creating Thumbnail Grids\n\nTo create visual thumbnail grids of PowerPoint slides for quick analysis and reference:\n\n```bash\npython scripts/thumbnail.py template.pptx [output_prefix]\n```\n\n**Features**:\n- Creates: `thumbnails.jpg` (or `thumbnails-1.jpg`, `thumbnails-2.jpg`, etc. for large decks)\n- Default: 5 columns, max 30 slides per grid (56)\n- Custom prefix: `python scripts/thumbnail.py template.pptx my-grid`\n  - Note: The output prefix should include the path if you want output in a specific directory (e.g., `workspace/my-grid`)\n- Adjust columns: `--cols 4` (range: 3-6, affects slides per grid)\n- Grid limits: 3 cols = 12 slides/grid, 4 cols = 20, 5 cols = 30, 6 cols = 42\n- Slides are zero-indexed (Slide 0, Slide 1, etc.)\n\n**Use cases**:\n- Template analysis: Quickly understand slide layouts and design patterns\n- Content review: Visual overview of entire presentation\n- Navigation reference: Find specific slides by their visual appearance\n- Quality check: Verify all slides are properly formatted\n\n**Examples**:\n```bash\n# Basic usage\npython scripts/thumbnail.py presentation.pptx\n\n# Combine options: custom name, columns\npython scripts/thumbnail.py template.pptx analysis --cols 4\n```\n\n## Converting Slides to Images\n\nTo visually analyze PowerPoint slides, convert them to images using a two-step process:\n\n1. **Convert PPTX to PDF**:\n   ```bash\n   soffice --headless --convert-to pdf template.pptx\n   ```\n\n2. **Convert PDF pages to JPEG images**:\n   ```bash\n   pdftoppm -jpeg -r 150 template.pdf slide\n   ```\n   This creates files like `slide-1.jpg`, `slide-2.jpg`, etc.\n\nOptions:\n- `-r 150`: Sets resolution to 150 DPI (adjust for quality/size balance)\n- `-jpeg`: Output JPEG format (use `-png` for PNG if preferred)\n- `-f N`: First page to convert (e.g., `-f 2` starts from page 2)\n- `-l N`: Last page to convert (e.g., `-l 5` stops at page 5)\n- `slide`: Prefix for output files\n\nExample for specific range:\n```bash\npdftoppm -jpeg -r 150 -f 2 -l 5 template.pdf slide  # Converts only pages 2-5\n```\n\n## Code Style Guidelines\n**IMPORTANT**: When generating code for PPTX operations:\n- Write concise code\n- Avoid verbose variable names and redundant operations\n- Avoid unnecessary print statements\n\n## Dependencies\n\nRequired dependencies (should already be installed):\n\n- **markitdown**: `pip install \"markitdown[pptx]\"` (for text extraction from presentations)\n- **pptxgenjs**: `npm install -g pptxgenjs` (for creating presentations via html2pptx)\n- **playwright**: `npm install -g playwright` (for HTML rendering in html2pptx)\n- **react-icons**: `npm install -g react-icons react react-dom` (for icons)\n- **sharp**: `npm install -g sharp` (for SVG rasterization and image processing)\n- **LibreOffice**: `sudo apt-get install libreoffice` (for PDF conversion)\n- **Poppler**: `sudo apt-get install poppler-utils` (for pdftoppm to convert PDF to images)\n- **defusedxml**: `pip install defusedxml` (for secure XML parsing)"
              },
              {
                "name": "raffle-winner-picker",
                "description": "Picks random winners from lists, spreadsheets, or Google Sheets for giveaways, raffles, and contests. Ensures fair, unbiased selection with transparency.",
                "path": "plugins/all-skills/skills/raffle-winner-picker/SKILL.md",
                "frontmatter": {
                  "name": "raffle-winner-picker",
                  "category": "business-productivity",
                  "description": "Picks random winners from lists, spreadsheets, or Google Sheets for giveaways, raffles, and contests. Ensures fair, unbiased selection with transparency."
                },
                "content": "# Raffle Winner Picker\n\nThis skill randomly selects winners from lists, spreadsheets, or Google Sheets for giveaways and contests.\n\n## When to Use This Skill\n\n- Running social media giveaways\n- Picking raffle winners at events\n- Randomly selecting participants for surveys or tests\n- Choosing winners from contest submissions\n- Fair distribution of limited spots or resources\n- Random team assignments\n\n## What This Skill Does\n\n1. **Random Selection**: Uses cryptographically random selection\n2. **Multiple Sources**: Works with CSV, Excel, Google Sheets, or plain lists\n3. **Multiple Winners**: Can pick one or multiple winners\n4. **Duplicate Prevention**: Ensures the same person can't win twice\n5. **Transparent Results**: Shows the selection process clearly\n6. **Winner Details**: Displays all relevant information about winners\n\n## How to Use\n\n### From Google Sheets\n\n```\nPick a random row from this Google Sheet to select a winner \nfor a giveaway: [Sheet URL]\n```\n\n### From Local File\n\n```\nPick 3 random winners from entries.csv\n```\n\n### From List\n\n```\nPick a random winner from this list:\n- Alice (alice@email.com)\n- Bob (bob@email.com)\n- Carol (carol@email.com)\n...\n```\n\n### Multiple Winners\n\n```\nPick 5 random winners from contest-entries.xlsx, \nmake sure no duplicates\n```\n\n## Example\n\n**User**: \"Pick a random row from this Google Sheet to select a winner for a giveaway.\"\n\n**Output**:\n```\nAccessing Google Sheet...\nTotal entries found: 247\n\nRandomly selecting winner...\n\n WINNER SELECTED! \n\nRow #142\nName: Sarah Johnson\nEmail: sarah.j@email.com\nEntry Date: March 10, 2024\nComment: \"Love your newsletter!\"\n\nSelection method: Cryptographically random\nTimestamp: 2024-03-15 14:32:18 UTC\n\nWould you like to:\n- Pick another winner (excluding Sarah)?\n- Export winner details?\n- Pick runner-ups?\n```\n\n**Inspired by:** Lenny's use case - picking a Sora 2 giveaway winner from his subscriber Slack community\n\n## Features\n\n### Fair Selection\n- Uses secure random number generation\n- No bias or patterns\n- Transparent process\n- Repeatable with seed (for verification)\n\n### Exclusions\n```\nPick a random winner excluding previous winners: \nAlice, Bob, Carol\n```\n\n### Weighted Selection\n```\nPick a winner with weighted probability based on \nthe \"entries\" column (1 entry = 1 ticket)\n```\n\n### Runner-ups\n```\nPick 1 winner and 3 runner-ups from the list\n```\n\n## Example Workflows\n\n### Social Media Giveaway\n1. Export entries from Google Form to Sheets\n2. \"Pick a random winner from [Sheet URL]\"\n3. Verify winner details\n4. Announce publicly with timestamp\n\n### Event Raffle\n1. Create CSV of attendee names and emails\n2. \"Pick 10 random winners from attendees.csv\"\n3. Export winner list\n4. Email winners directly\n\n### Team Assignment\n1. Have list of participants\n2. \"Randomly split this list into 4 equal teams\"\n3. Review assignments\n4. Share team rosters\n\n## Tips\n\n- **Document the process**: Save the timestamp and method\n- **Public announcement**: Share selection details for transparency\n- **Check eligibility**: Verify winner meets contest rules\n- **Have backups**: Pick runner-ups in case winner is ineligible\n- **Export results**: Save winner list for records\n\n## Privacy & Fairness\n\n Uses cryptographically secure randomness\n No manipulation possible\n Timestamp recorded for verification\n Can provide seed for third-party verification\n Respects data privacy\n\n## Common Use Cases\n\n- Newsletter subscriber giveaways\n- Product launch raffles\n- Conference ticket drawings\n- Beta tester selection\n- Focus group participant selection\n- Random prize distribution at events"
              },
              {
                "name": "skill-creator",
                "description": "Guide for creating effective skills. This skill should be used when users want to create a new skill (or update an existing skill) that extends Claude's capabilities with specialized knowledge, workflows, or tool integrations.",
                "path": "plugins/all-skills/skills/skill-creator/SKILL.md",
                "frontmatter": {
                  "name": "skill-creator",
                  "category": "development-code",
                  "description": "Guide for creating effective skills. This skill should be used when users want to create a new skill (or update an existing skill) that extends Claude's capabilities with specialized knowledge, workflows, or tool integrations.",
                  "license": "Complete terms in LICENSE.txt"
                },
                "content": "# Skill Creator\n\nThis skill provides guidance for creating effective skills.\n\n## About Skills\n\nSkills are modular, self-contained packages that extend Claude's capabilities by providing\nspecialized knowledge, workflows, and tools. Think of them as \"onboarding guides\" for specific\ndomains or tasksthey transform Claude from a general-purpose agent into a specialized agent\nequipped with procedural knowledge that no model can fully possess.\n\n### What Skills Provide\n\n1. Specialized workflows - Multi-step procedures for specific domains\n2. Tool integrations - Instructions for working with specific file formats or APIs\n3. Domain expertise - Company-specific knowledge, schemas, business logic\n4. Bundled resources - Scripts, references, and assets for complex and repetitive tasks\n\n### Anatomy of a Skill\n\nEvery skill consists of a required SKILL.md file and optional bundled resources:\n\n```\nskill-name/\n SKILL.md (required)\n    YAML frontmatter metadata (required)\n       name: (required)\n       description: (required)\n    Markdown instructions (required)\n Bundled Resources (optional)\n     scripts/          - Executable code (Python/Bash/etc.)\n     references/       - Documentation intended to be loaded into context as needed\n     assets/           - Files used in output (templates, icons, fonts, etc.)\n```\n\n#### SKILL.md (required)\n\n**Metadata Quality:** The `name` and `description` in YAML frontmatter determine when Claude will use the skill. Be specific about what the skill does and when to use it. Use the third-person (e.g. \"This skill should be used when...\" instead of \"Use this skill when...\").\n\n#### Bundled Resources (optional)\n\n##### Scripts (`scripts/`)\n\nExecutable code (Python/Bash/etc.) for tasks that require deterministic reliability or are repeatedly rewritten.\n\n- **When to include**: When the same code is being rewritten repeatedly or deterministic reliability is needed\n- **Example**: `scripts/rotate_pdf.py` for PDF rotation tasks\n- **Benefits**: Token efficient, deterministic, may be executed without loading into context\n- **Note**: Scripts may still need to be read by Claude for patching or environment-specific adjustments\n\n##### References (`references/`)\n\nDocumentation and reference material intended to be loaded as needed into context to inform Claude's process and thinking.\n\n- **When to include**: For documentation that Claude should reference while working\n- **Examples**: `references/finance.md` for financial schemas, `references/mnda.md` for company NDA template, `references/policies.md` for company policies, `references/api_docs.md` for API specifications\n- **Use cases**: Database schemas, API documentation, domain knowledge, company policies, detailed workflow guides\n- **Benefits**: Keeps SKILL.md lean, loaded only when Claude determines it's needed\n- **Best practice**: If files are large (>10k words), include grep search patterns in SKILL.md\n- **Avoid duplication**: Information should live in either SKILL.md or references files, not both. Prefer references files for detailed information unless it's truly core to the skillthis keeps SKILL.md lean while making information discoverable without hogging the context window. Keep only essential procedural instructions and workflow guidance in SKILL.md; move detailed reference material, schemas, and examples to references files.\n\n##### Assets (`assets/`)\n\nFiles not intended to be loaded into context, but rather used within the output Claude produces.\n\n- **When to include**: When the skill needs files that will be used in the final output\n- **Examples**: `assets/logo.png` for brand assets, `assets/slides.pptx` for PowerPoint templates, `assets/frontend-template/` for HTML/React boilerplate, `assets/font.ttf` for typography\n- **Use cases**: Templates, images, icons, boilerplate code, fonts, sample documents that get copied or modified\n- **Benefits**: Separates output resources from documentation, enables Claude to use files without loading them into context\n\n### Progressive Disclosure Design Principle\n\nSkills use a three-level loading system to manage context efficiently:\n\n1. **Metadata (name + description)** - Always in context (~100 words)\n2. **SKILL.md body** - When skill triggers (<5k words)\n3. **Bundled resources** - As needed by Claude (Unlimited*)\n\n*Unlimited because scripts can be executed without reading into context window.\n\n## Skill Creation Process\n\nTo create a skill, follow the \"Skill Creation Process\" in order, skipping steps only if there is a clear reason why they are not applicable.\n\n### Step 1: Understanding the Skill with Concrete Examples\n\nSkip this step only when the skill's usage patterns are already clearly understood. It remains valuable even when working with an existing skill.\n\nTo create an effective skill, clearly understand concrete examples of how the skill will be used. This understanding can come from either direct user examples or generated examples that are validated with user feedback.\n\nFor example, when building an image-editor skill, relevant questions include:\n\n- \"What functionality should the image-editor skill support? Editing, rotating, anything else?\"\n- \"Can you give some examples of how this skill would be used?\"\n- \"I can imagine users asking for things like 'Remove the red-eye from this image' or 'Rotate this image'. Are there other ways you imagine this skill being used?\"\n- \"What would a user say that should trigger this skill?\"\n\nTo avoid overwhelming users, avoid asking too many questions in a single message. Start with the most important questions and follow up as needed for better effectiveness.\n\nConclude this step when there is a clear sense of the functionality the skill should support.\n\n### Step 2: Planning the Reusable Skill Contents\n\nTo turn concrete examples into an effective skill, analyze each example by:\n\n1. Considering how to execute on the example from scratch\n2. Identifying what scripts, references, and assets would be helpful when executing these workflows repeatedly\n\nExample: When building a `pdf-editor` skill to handle queries like \"Help me rotate this PDF,\" the analysis shows:\n\n1. Rotating a PDF requires re-writing the same code each time\n2. A `scripts/rotate_pdf.py` script would be helpful to store in the skill\n\nExample: When designing a `frontend-webapp-builder` skill for queries like \"Build me a todo app\" or \"Build me a dashboard to track my steps,\" the analysis shows:\n\n1. Writing a frontend webapp requires the same boilerplate HTML/React each time\n2. An `assets/hello-world/` template containing the boilerplate HTML/React project files would be helpful to store in the skill\n\nExample: When building a `big-query` skill to handle queries like \"How many users have logged in today?\" the analysis shows:\n\n1. Querying BigQuery requires re-discovering the table schemas and relationships each time\n2. A `references/schema.md` file documenting the table schemas would be helpful to store in the skill\n\nTo establish the skill's contents, analyze each concrete example to create a list of the reusable resources to include: scripts, references, and assets.\n\n### Step 3: Initializing the Skill\n\nAt this point, it is time to actually create the skill.\n\nSkip this step only if the skill being developed already exists, and iteration or packaging is needed. In this case, continue to the next step.\n\nWhen creating a new skill from scratch, always run the `init_skill.py` script. The script conveniently generates a new template skill directory that automatically includes everything a skill requires, making the skill creation process much more efficient and reliable.\n\nUsage:\n\n```bash\nscripts/init_skill.py <skill-name> --path <output-directory>\n```\n\nThe script:\n\n- Creates the skill directory at the specified path\n- Generates a SKILL.md template with proper frontmatter and TODO placeholders\n- Creates example resource directories: `scripts/`, `references/`, and `assets/`\n- Adds example files in each directory that can be customized or deleted\n\nAfter initialization, customize or remove the generated SKILL.md and example files as needed.\n\n### Step 4: Edit the Skill\n\nWhen editing the (newly-generated or existing) skill, remember that the skill is being created for another instance of Claude to use. Focus on including information that would be beneficial and non-obvious to Claude. Consider what procedural knowledge, domain-specific details, or reusable assets would help another Claude instance execute these tasks more effectively.\n\n#### Start with Reusable Skill Contents\n\nTo begin implementation, start with the reusable resources identified above: `scripts/`, `references/`, and `assets/` files. Note that this step may require user input. For example, when implementing a `brand-guidelines` skill, the user may need to provide brand assets or templates to store in `assets/`, or documentation to store in `references/`.\n\nAlso, delete any example files and directories not needed for the skill. The initialization script creates example files in `scripts/`, `references/`, and `assets/` to demonstrate structure, but most skills won't need all of them.\n\n#### Update SKILL.md\n\n**Writing Style:** Write the entire skill using **imperative/infinitive form** (verb-first instructions), not second person. Use objective, instructional language (e.g., \"To accomplish X, do Y\" rather than \"You should do X\" or \"If you need to do X\"). This maintains consistency and clarity for AI consumption.\n\nTo complete SKILL.md, answer the following questions:\n\n1. What is the purpose of the skill, in a few sentences?\n2. When should the skill be used?\n3. In practice, how should Claude use the skill? All reusable skill contents developed above should be referenced so that Claude knows how to use them.\n\n### Step 5: Packaging a Skill\n\nOnce the skill is ready, it should be packaged into a distributable zip file that gets shared with the user. The packaging process automatically validates the skill first to ensure it meets all requirements:\n\n```bash\nscripts/package_skill.py <path/to/skill-folder>\n```\n\nOptional output directory specification:\n\n```bash\nscripts/package_skill.py <path/to/skill-folder> ./dist\n```\n\nThe packaging script will:\n\n1. **Validate** the skill automatically, checking:\n   - YAML frontmatter format and required fields\n   - Skill naming conventions and directory structure\n   - Description completeness and quality\n   - File organization and resource references\n\n2. **Package** the skill if validation passes, creating a zip file named after the skill (e.g., `my-skill.zip`) that includes all files and maintains the proper directory structure for distribution.\n\nIf validation fails, the script will report the errors and exit without creating a package. Fix any validation errors and run the packaging command again.\n\n### Step 6: Iterate\n\nAfter testing the skill, users may request improvements. Often this happens right after using the skill, with fresh context of how the skill performed.\n\n**Iteration workflow:**\n1. Use the skill on real tasks\n2. Notice struggles or inefficiencies\n3. Identify how SKILL.md or bundled resources should be updated\n4. Implement changes and test again"
              },
              {
                "name": "skill-share",
                "description": "A skill that creates new Claude skills and automatically shares them on Slack using Rube for seamless team collaboration and skill discovery.",
                "path": "plugins/all-skills/skills/skill-share/SKILL.md",
                "frontmatter": {
                  "name": "skill-share",
                  "category": "creative-collaboration",
                  "description": "A skill that creates new Claude skills and automatically shares them on Slack using Rube for seamless team collaboration and skill discovery.",
                  "license": "Complete terms in LICENSE.txt"
                },
                "content": "## When to use this skill\n\nUse this skill when you need to:\n- **Create new Claude skills** with proper structure and metadata\n- **Generate skill packages** ready for distribution\n- **Automatically share created skills** on Slack channels for team visibility\n- **Validate skill structure** before sharing\n- **Package and distribute** skills to your team\n\nAlso use this skill when:\n- **User says he wants to create/share his skill** \n\nThis skill is ideal for:\n- Creating skills as part of team workflows\n- Building internal tools that need skill creation + team notification\n- Automating the skill development pipeline\n- Collaborative skill creation with team notifications\n\n## Key Features\n\n### 1. Skill Creation\n- Creates properly structured skill directories with SKILL.md\n- Generates standardized scripts/, references/, and assets/ directories\n- Auto-generates YAML frontmatter with required metadata\n- Enforces naming conventions (hyphen-case)\n\n### 2. Skill Validation\n- Validates SKILL.md format and required fields\n- Checks naming conventions\n- Ensures metadata completeness before packaging\n\n### 3. Skill Packaging\n- Creates distributable zip files\n- Includes all skill assets and documentation\n- Runs validation automatically before packaging\n\n### 4. Slack Integration via Rube\n- Automatically sends created skill information to designated Slack channels\n- Shares skill metadata (name, description, link)\n- Posts skill summary for team discovery\n- Provides direct links to skill files\n\n## How It Works\n\n1. **Initialization**: Provide skill name and description\n2. **Creation**: Skill directory is created with proper structure\n3. **Validation**: Skill metadata is validated for correctness\n4. **Packaging**: Skill is packaged into a distributable format\n5. **Slack Notification**: Skill details are posted to your team's Slack channel\n\n## Example Usage\n\n```\nWhen you ask Claude to create a skill called \"pdf-analyzer\":\n1. Creates /skill-pdf-analyzer/ with SKILL.md template\n2. Generates structured directories (scripts/, references/, assets/)\n3. Validates the skill structure\n4. Packages the skill as a zip file\n5. Posts to Slack: \"New Skill Created: pdf-analyzer - Advanced PDF analysis and extraction capabilities\"\n```\n\n## Integration with Rube\n\nThis skill leverages Rube for:\n- **SLACK_SEND_MESSAGE**: Posts skill information to team channels\n- **SLACK_POST_MESSAGE_WITH_BLOCKS**: Shares rich formatted skill metadata\n- **SLACK_FIND_CHANNELS**: Discovers target channels for skill announcements\n\n## Requirements\n\n- Slack workspace connection via Rube\n- Write access to skill creation directory\n- Python 3.7+ for skill creation scripts\n- Target Slack channel for skill notifications"
              },
              {
                "name": "slack-gif-creator",
                "description": "Toolkit for creating animated GIFs optimized for Slack, with validators for size constraints and composable animation primitives. This skill applies when users request animated GIFs or emoji animations for Slack from descriptions like \"make me a GIF for Slack of X doing Y\".",
                "path": "plugins/all-skills/skills/slack-gif-creator/SKILL.md",
                "frontmatter": {
                  "name": "slack-gif-creator",
                  "category": "creative-collaboration",
                  "description": "Toolkit for creating animated GIFs optimized for Slack, with validators for size constraints and composable animation primitives. This skill applies when users request animated GIFs or emoji animations for Slack from descriptions like \"make me a GIF for Slack of X doing Y\".",
                  "license": "Complete terms in LICENSE.txt"
                },
                "content": "# Slack GIF Creator - Flexible Toolkit\n\nA toolkit for creating animated GIFs optimized for Slack. Provides validators for Slack's constraints, composable animation primitives, and optional helper utilities. **Apply these tools however needed to achieve the creative vision.**\n\n## Slack's Requirements\n\nSlack has specific requirements for GIFs based on their use:\n\n**Message GIFs:**\n- Max size: ~2MB\n- Optimal dimensions: 480x480\n- Typical FPS: 15-20\n- Color limit: 128-256\n- Duration: 2-5s\n\n**Emoji GIFs:**\n- Max size: 64KB (strict limit)\n- Optimal dimensions: 128x128\n- Typical FPS: 10-12\n- Color limit: 32-48\n- Duration: 1-2s\n\n**Emoji GIFs are challenging** - the 64KB limit is strict. Strategies that help:\n- Limit to 10-15 frames total\n- Use 32-48 colors maximum\n- Keep designs simple\n- Avoid gradients\n- Validate file size frequently\n\n## Toolkit Structure\n\nThis skill provides three types of tools:\n\n1. **Validators** - Check if a GIF meets Slack's requirements\n2. **Animation Primitives** - Composable building blocks for motion (shake, bounce, move, kaleidoscope)\n3. **Helper Utilities** - Optional functions for common needs (text, colors, effects)\n\n**Complete creative freedom is available in how these tools are applied.**\n\n## Core Validators\n\nTo ensure a GIF meets Slack's constraints, use these validators:\n\n```python\nfrom core.gif_builder import GIFBuilder\n\n# After creating your GIF, check if it meets requirements\nbuilder = GIFBuilder(width=128, height=128, fps=10)\n# ... add your frames however you want ...\n\n# Save and check size\ninfo = builder.save('emoji.gif', num_colors=48, optimize_for_emoji=True)\n\n# The save method automatically warns if file exceeds limits\n# info dict contains: size_kb, size_mb, frame_count, duration_seconds\n```\n\n**File size validator**:\n```python\nfrom core.validators import check_slack_size\n\n# Check if GIF meets size limits\npasses, info = check_slack_size('emoji.gif', is_emoji=True)\n# Returns: (True/False, dict with size details)\n```\n\n**Dimension validator**:\n```python\nfrom core.validators import validate_dimensions\n\n# Check dimensions\npasses, info = validate_dimensions(128, 128, is_emoji=True)\n# Returns: (True/False, dict with dimension details)\n```\n\n**Complete validation**:\n```python\nfrom core.validators import validate_gif, is_slack_ready\n\n# Run all validations\nall_pass, results = validate_gif('emoji.gif', is_emoji=True)\n\n# Or quick check\nif is_slack_ready('emoji.gif', is_emoji=True):\n    print(\"Ready to upload!\")\n```\n\n## Animation Primitives\n\nThese are composable building blocks for motion. Apply these to any object in any combination:\n\n### Shake\n```python\nfrom templates.shake import create_shake_animation\n\n# Shake an emoji\nframes = create_shake_animation(\n    object_type='emoji',\n    object_data={'emoji': '', 'size': 80},\n    num_frames=20,\n    shake_intensity=15,\n    direction='both'  # or 'horizontal', 'vertical'\n)\n```\n\n### Bounce\n```python\nfrom templates.bounce import create_bounce_animation\n\n# Bounce a circle\nframes = create_bounce_animation(\n    object_type='circle',\n    object_data={'radius': 40, 'color': (255, 100, 100)},\n    num_frames=30,\n    bounce_height=150\n)\n```\n\n### Spin / Rotate\n```python\nfrom templates.spin import create_spin_animation, create_loading_spinner\n\n# Clockwise spin\nframes = create_spin_animation(\n    object_type='emoji',\n    object_data={'emoji': '', 'size': 100},\n    rotation_type='clockwise',\n    full_rotations=2\n)\n\n# Wobble rotation\nframes = create_spin_animation(rotation_type='wobble', full_rotations=3)\n\n# Loading spinner\nframes = create_loading_spinner(spinner_type='dots')\n```\n\n### Pulse / Heartbeat\n```python\nfrom templates.pulse import create_pulse_animation, create_attention_pulse\n\n# Smooth pulse\nframes = create_pulse_animation(\n    object_data={'emoji': '', 'size': 100},\n    pulse_type='smooth',\n    scale_range=(0.8, 1.2)\n)\n\n# Heartbeat (double-pump)\nframes = create_pulse_animation(pulse_type='heartbeat')\n\n# Attention pulse for emoji GIFs\nframes = create_attention_pulse(emoji='', num_frames=20)\n```\n\n### Fade\n```python\nfrom templates.fade import create_fade_animation, create_crossfade\n\n# Fade in\nframes = create_fade_animation(fade_type='in')\n\n# Fade out\nframes = create_fade_animation(fade_type='out')\n\n# Crossfade between two emojis\nframes = create_crossfade(\n    object1_data={'emoji': '', 'size': 100},\n    object2_data={'emoji': '', 'size': 100}\n)\n```\n\n### Zoom\n```python\nfrom templates.zoom import create_zoom_animation, create_explosion_zoom\n\n# Zoom in dramatically\nframes = create_zoom_animation(\n    zoom_type='in',\n    scale_range=(0.1, 2.0),\n    add_motion_blur=True\n)\n\n# Zoom out\nframes = create_zoom_animation(zoom_type='out')\n\n# Explosion zoom\nframes = create_explosion_zoom(emoji='')\n```\n\n### Explode / Shatter\n```python\nfrom templates.explode import create_explode_animation, create_particle_burst\n\n# Burst explosion\nframes = create_explode_animation(\n    explode_type='burst',\n    num_pieces=25\n)\n\n# Shatter effect\nframes = create_explode_animation(explode_type='shatter')\n\n# Dissolve into particles\nframes = create_explode_animation(explode_type='dissolve')\n\n# Particle burst\nframes = create_particle_burst(particle_count=30)\n```\n\n### Wiggle / Jiggle\n```python\nfrom templates.wiggle import create_wiggle_animation, create_excited_wiggle\n\n# Jello wobble\nframes = create_wiggle_animation(\n    wiggle_type='jello',\n    intensity=1.0,\n    cycles=2\n)\n\n# Wave motion\nframes = create_wiggle_animation(wiggle_type='wave')\n\n# Excited wiggle for emoji GIFs\nframes = create_excited_wiggle(emoji='')\n```\n\n### Slide\n```python\nfrom templates.slide import create_slide_animation, create_multi_slide\n\n# Slide in from left with overshoot\nframes = create_slide_animation(\n    direction='left',\n    slide_type='in',\n    overshoot=True\n)\n\n# Slide across\nframes = create_slide_animation(direction='left', slide_type='across')\n\n# Multiple objects sliding in sequence\nobjects = [\n    {'data': {'emoji': '', 'size': 60}, 'direction': 'left', 'final_pos': (120, 240)},\n    {'data': {'emoji': '', 'size': 60}, 'direction': 'right', 'final_pos': (240, 240)}\n]\nframes = create_multi_slide(objects, stagger_delay=5)\n```\n\n### Flip\n```python\nfrom templates.flip import create_flip_animation, create_quick_flip\n\n# Horizontal flip between two emojis\nframes = create_flip_animation(\n    object1_data={'emoji': '', 'size': 120},\n    object2_data={'emoji': '', 'size': 120},\n    flip_axis='horizontal'\n)\n\n# Vertical flip\nframes = create_flip_animation(flip_axis='vertical')\n\n# Quick flip for emoji GIFs\nframes = create_quick_flip('', '')\n```\n\n### Morph / Transform\n```python\nfrom templates.morph import create_morph_animation, create_reaction_morph\n\n# Crossfade morph\nframes = create_morph_animation(\n    object1_data={'emoji': '', 'size': 100},\n    object2_data={'emoji': '', 'size': 100},\n    morph_type='crossfade'\n)\n\n# Scale morph (shrink while other grows)\nframes = create_morph_animation(morph_type='scale')\n\n# Spin morph (3D flip-like)\nframes = create_morph_animation(morph_type='spin_morph')\n```\n\n### Move Effect\n```python\nfrom templates.move import create_move_animation\n\n# Linear movement\nframes = create_move_animation(\n    object_type='emoji',\n    object_data={'emoji': '', 'size': 60},\n    start_pos=(50, 240),\n    end_pos=(430, 240),\n    motion_type='linear',\n    easing='ease_out'\n)\n\n# Arc movement (parabolic trajectory)\nframes = create_move_animation(\n    object_type='emoji',\n    object_data={'emoji': '', 'size': 60},\n    start_pos=(50, 350),\n    end_pos=(430, 350),\n    motion_type='arc',\n    motion_params={'arc_height': 150}\n)\n\n# Circular movement\nframes = create_move_animation(\n    object_type='emoji',\n    object_data={'emoji': '', 'size': 50},\n    motion_type='circle',\n    motion_params={\n        'center': (240, 240),\n        'radius': 120,\n        'angle_range': 360  # full circle\n    }\n)\n\n# Wave movement\nframes = create_move_animation(\n    motion_type='wave',\n    motion_params={\n        'wave_amplitude': 50,\n        'wave_frequency': 2\n    }\n)\n\n# Or use low-level easing functions\nfrom core.easing import interpolate, calculate_arc_motion\n\nfor i in range(num_frames):\n    t = i / (num_frames - 1)\n    x = interpolate(start_x, end_x, t, easing='ease_out')\n    # Or: x, y = calculate_arc_motion(start, end, height, t)\n```\n\n### Kaleidoscope Effect\n```python\nfrom templates.kaleidoscope import apply_kaleidoscope, create_kaleidoscope_animation\n\n# Apply to a single frame\nkaleido_frame = apply_kaleidoscope(frame, segments=8)\n\n# Or create animated kaleidoscope\nframes = create_kaleidoscope_animation(\n    base_frame=my_frame,  # or None for demo pattern\n    num_frames=30,\n    segments=8,\n    rotation_speed=1.0\n)\n\n# Simple mirror effects (faster)\nfrom templates.kaleidoscope import apply_simple_mirror\n\nmirrored = apply_simple_mirror(frame, mode='quad')  # 4-way mirror\n# modes: 'horizontal', 'vertical', 'quad', 'radial'\n```\n\n**To compose primitives freely, follow these patterns:**\n```python\n# Example: Bounce + shake for impact\nfor i in range(num_frames):\n    frame = create_blank_frame(480, 480, bg_color)\n\n    # Bounce motion\n    t_bounce = i / (num_frames - 1)\n    y = interpolate(start_y, ground_y, t_bounce, 'bounce_out')\n\n    # Add shake on impact (when y reaches ground)\n    if y >= ground_y - 5:\n        shake_x = math.sin(i * 2) * 10\n        x = center_x + shake_x\n    else:\n        x = center_x\n\n    draw_emoji(frame, '', (x, y), size=60)\n    builder.add_frame(frame)\n```\n\n## Helper Utilities\n\nThese are optional helpers for common needs. **Use, modify, or replace these with custom implementations as needed.**\n\n### GIF Builder (Assembly & Optimization)\n\n```python\nfrom core.gif_builder import GIFBuilder\n\n# Create builder with your chosen settings\nbuilder = GIFBuilder(width=480, height=480, fps=20)\n\n# Add frames (however you created them)\nfor frame in my_frames:\n    builder.add_frame(frame)\n\n# Save with optimization\nbuilder.save('output.gif',\n             num_colors=128,\n             optimize_for_emoji=False)\n```\n\nKey features:\n- Automatic color quantization\n- Duplicate frame removal\n- Size warnings for Slack limits\n- Emoji mode (aggressive optimization)\n\n### Text Rendering\n\nFor small GIFs like emojis, text readability is challenging. A common solution involves adding outlines:\n\n```python\nfrom core.typography import draw_text_with_outline, TYPOGRAPHY_SCALE\n\n# Text with outline (helps readability)\ndraw_text_with_outline(\n    frame, \"BONK!\",\n    position=(240, 100),\n    font_size=TYPOGRAPHY_SCALE['h1'],  # 60px\n    text_color=(255, 68, 68),\n    outline_color=(0, 0, 0),\n    outline_width=4,\n    centered=True\n)\n```\n\nTo implement custom text rendering, use PIL's `ImageDraw.text()` which works fine for larger GIFs.\n\n### Color Management\n\nProfessional-looking GIFs often use cohesive color palettes:\n\n```python\nfrom core.color_palettes import get_palette\n\n# Get a pre-made palette\npalette = get_palette('vibrant')  # or 'pastel', 'dark', 'neon', 'professional'\n\nbg_color = palette['background']\ntext_color = palette['primary']\naccent_color = palette['accent']\n```\n\nTo work with colors directly, use RGB tuples - whatever works for the use case.\n\n### Visual Effects\n\nOptional effects for impact moments:\n\n```python\nfrom core.visual_effects import ParticleSystem, create_impact_flash, create_shockwave_rings\n\n# Particle system\nparticles = ParticleSystem()\nparticles.emit_sparkles(x=240, y=200, count=15)\nparticles.emit_confetti(x=240, y=200, count=20)\n\n# Update and render each frame\nparticles.update()\nparticles.render(frame)\n\n# Flash effect\nframe = create_impact_flash(frame, position=(240, 200), radius=100)\n\n# Shockwave rings\nframe = create_shockwave_rings(frame, position=(240, 200), radii=[30, 60, 90])\n```\n\n### Easing Functions\n\nSmooth motion uses easing instead of linear interpolation:\n\n```python\nfrom core.easing import interpolate\n\n# Object falling (accelerates)\ny = interpolate(start=0, end=400, t=progress, easing='ease_in')\n\n# Object landing (decelerates)\ny = interpolate(start=0, end=400, t=progress, easing='ease_out')\n\n# Bouncing\ny = interpolate(start=0, end=400, t=progress, easing='bounce_out')\n\n# Overshoot (elastic)\nscale = interpolate(start=0.5, end=1.0, t=progress, easing='elastic_out')\n```\n\nAvailable easings: `linear`, `ease_in`, `ease_out`, `ease_in_out`, `bounce_out`, `elastic_out`, `back_out` (overshoot), and more in `core/easing.py`.\n\n### Frame Composition\n\nBasic drawing utilities if you need them:\n\n```python\nfrom core.frame_composer import (\n    create_gradient_background,  # Gradient backgrounds\n    draw_emoji_enhanced,         # Emoji with optional shadow\n    draw_circle_with_shadow,     # Shapes with depth\n    draw_star                    # 5-pointed stars\n)\n\n# Gradient background\nframe = create_gradient_background(480, 480, top_color, bottom_color)\n\n# Emoji with shadow\ndraw_emoji_enhanced(frame, '', position=(200, 200), size=80, shadow=True)\n```\n\n## Optimization Strategies\n\nWhen your GIF is too large:\n\n**For Message GIFs (>2MB):**\n1. Reduce frames (lower FPS or shorter duration)\n2. Reduce colors (128  64 colors)\n3. Reduce dimensions (480x480  320x320)\n4. Enable duplicate frame removal\n\n**For Emoji GIFs (>64KB) - be aggressive:**\n1. Limit to 10-12 frames total\n2. Use 32-40 colors maximum\n3. Avoid gradients (solid colors compress better)\n4. Simplify design (fewer elements)\n5. Use `optimize_for_emoji=True` in save method\n\n## Example Composition Patterns\n\n### Simple Reaction (Pulsing)\n```python\nbuilder = GIFBuilder(128, 128, 10)\n\nfor i in range(12):\n    frame = Image.new('RGB', (128, 128), (240, 248, 255))\n\n    # Pulsing scale\n    scale = 1.0 + math.sin(i * 0.5) * 0.15\n    size = int(60 * scale)\n\n    draw_emoji_enhanced(frame, '', position=(64-size//2, 64-size//2),\n                       size=size, shadow=False)\n    builder.add_frame(frame)\n\nbuilder.save('reaction.gif', num_colors=40, optimize_for_emoji=True)\n\n# Validate\nfrom core.validators import check_slack_size\ncheck_slack_size('reaction.gif', is_emoji=True)\n```\n\n### Action with Impact (Bounce + Flash)\n```python\nbuilder = GIFBuilder(480, 480, 20)\n\n# Phase 1: Object falls\nfor i in range(15):\n    frame = create_gradient_background(480, 480, (240, 248, 255), (200, 230, 255))\n    t = i / 14\n    y = interpolate(0, 350, t, 'ease_in')\n    draw_emoji_enhanced(frame, '', position=(220, int(y)), size=80)\n    builder.add_frame(frame)\n\n# Phase 2: Impact + flash\nfor i in range(8):\n    frame = create_gradient_background(480, 480, (240, 248, 255), (200, 230, 255))\n\n    # Flash on first frames\n    if i < 3:\n        frame = create_impact_flash(frame, (240, 350), radius=120, intensity=0.6)\n\n    draw_emoji_enhanced(frame, '', position=(220, 350), size=80)\n\n    # Text appears\n    if i > 2:\n        draw_text_with_outline(frame, \"GOAL!\", position=(240, 150),\n                              font_size=60, text_color=(255, 68, 68),\n                              outline_color=(0, 0, 0), outline_width=4, centered=True)\n\n    builder.add_frame(frame)\n\nbuilder.save('goal.gif', num_colors=128)\n```\n\n### Combining Primitives (Move + Shake)\n```python\nfrom templates.shake import create_shake_animation\n\n# Create shake animation\nshake_frames = create_shake_animation(\n    object_type='emoji',\n    object_data={'emoji': '', 'size': 70},\n    num_frames=20,\n    shake_intensity=12\n)\n\n# Create moving element that triggers the shake\nbuilder = GIFBuilder(480, 480, 20)\nfor i in range(40):\n    t = i / 39\n\n    if i < 20:\n        # Before trigger - use blank frame with moving object\n        frame = create_blank_frame(480, 480, (255, 255, 255))\n        x = interpolate(50, 300, t * 2, 'linear')\n        draw_emoji_enhanced(frame, '', position=(int(x), 300), size=60)\n        draw_emoji_enhanced(frame, '', position=(350, 200), size=70)\n    else:\n        # After trigger - use shake frame\n        frame = shake_frames[i - 20]\n        # Add the car in final position\n        draw_emoji_enhanced(frame, '', position=(300, 300), size=60)\n\n    builder.add_frame(frame)\n\nbuilder.save('scare.gif')\n```\n\n## Philosophy\n\nThis toolkit provides building blocks, not rigid recipes. To work with a GIF request:\n\n1. **Understand the creative vision** - What should happen? What's the mood?\n2. **Design the animation** - Break it into phases (anticipation, action, reaction)\n3. **Apply primitives as needed** - Shake, bounce, move, effects - mix freely\n4. **Validate constraints** - Check file size, especially for emoji GIFs\n5. **Iterate if needed** - Reduce frames/colors if over size limits\n\n**The goal is creative freedom within Slack's technical constraints.**\n\n## Dependencies\n\nTo use this toolkit, install these dependencies only if they aren't already present:\n\n```bash\npip install pillow imageio numpy\n```"
              },
              {
                "name": "theme-factory",
                "description": "Toolkit for styling artifacts with a theme. These artifacts can be slides, docs, reportings, HTML landing pages, etc. There are 10 pre-set themes with colors/fonts that you can apply to any artifact that has been creating, or can generate a new theme on-the-fly.",
                "path": "plugins/all-skills/skills/theme-factory/SKILL.md",
                "frontmatter": {
                  "name": "theme-factory",
                  "category": "document-processing",
                  "description": "Toolkit for styling artifacts with a theme. These artifacts can be slides, docs, reportings, HTML landing pages, etc. There are 10 pre-set themes with colors/fonts that you can apply to any artifact that has been creating, or can generate a new theme on-the-fly.",
                  "license": "Complete terms in LICENSE.txt"
                },
                "content": "# Theme Factory Skill\n\nThis skill provides a curated collection of professional font and color themes themes, each with carefully selected color palettes and font pairings. Once a theme is chosen, it can be applied to any artifact.\n\n## Purpose\n\nTo apply consistent, professional styling to presentation slide decks, use this skill. Each theme includes:\n- A cohesive color palette with hex codes\n- Complementary font pairings for headers and body text\n- A distinct visual identity suitable for different contexts and audiences\n\n## Usage Instructions\n\nTo apply styling to a slide deck or other artifact:\n\n1. **Show the theme showcase**: Display the `theme-showcase.pdf` file to allow users to see all available themes visually. Do not make any modifications to it; simply show the file for viewing.\n2. **Ask for their choice**: Ask which theme to apply to the deck\n3. **Wait for selection**: Get explicit confirmation about the chosen theme\n4. **Apply the theme**: Once a theme has been chosen, apply the selected theme's colors and fonts to the deck/artifact\n\n## Themes Available\n\nThe following 10 themes are available, each showcased in `theme-showcase.pdf`:\n\n1. **Ocean Depths** - Professional and calming maritime theme\n2. **Sunset Boulevard** - Warm and vibrant sunset colors\n3. **Forest Canopy** - Natural and grounded earth tones\n4. **Modern Minimalist** - Clean and contemporary grayscale\n5. **Golden Hour** - Rich and warm autumnal palette\n6. **Arctic Frost** - Cool and crisp winter-inspired theme\n7. **Desert Rose** - Soft and sophisticated dusty tones\n8. **Tech Innovation** - Bold and modern tech aesthetic\n9. **Botanical Garden** - Fresh and organic garden colors\n10. **Midnight Galaxy** - Dramatic and cosmic deep tones\n\n## Theme Details\n\nEach theme is defined in the `themes/` directory with complete specifications including:\n- Cohesive color palette with hex codes\n- Complementary font pairings for headers and body text\n- Distinct visual identity suitable for different contexts and audiences\n\n## Application Process\n\nAfter a preferred theme is selected:\n1. Read the corresponding theme file from the `themes/` directory\n2. Apply the specified colors and fonts consistently throughout the deck\n3. Ensure proper contrast and readability\n4. Maintain the theme's visual identity across all slides\n\n## Create your Own Theme\nTo handle cases where none of the existing themes work for an artifact, create a custom theme. Based on provided inputs, generate a new theme similar to the ones above. Give the theme a similar name describing what the font/color combinations represent. Use any basic description provided to choose appropriate colors/fonts. After generating the theme, show it for review and verification. Following that, apply the theme as described above."
              },
              {
                "name": "youtube-downloader",
                "description": "Download YouTube videos with customizable quality and format options. Use this skill when the user asks to download, save, or grab YouTube videos. Supports various quality settings (best, 1080p, 720p, 480p, 360p), multiple formats (mp4, webm, mkv), and audio-only downloads as MP3.",
                "path": "plugins/all-skills/skills/video-downloader/SKILL.md",
                "frontmatter": {
                  "name": "youtube-downloader",
                  "category": "document-processing",
                  "description": "Download YouTube videos with customizable quality and format options. Use this skill when the user asks to download, save, or grab YouTube videos. Supports various quality settings (best, 1080p, 720p, 480p, 360p), multiple formats (mp4, webm, mkv), and audio-only downloads as MP3."
                },
                "content": "# YouTube Video Downloader\n\nDownload YouTube videos with full control over quality and format settings.\n\n## Quick Start\n\nThe simplest way to download a video:\n\n```bash\npython scripts/download_video.py \"https://www.youtube.com/watch?v=VIDEO_ID\"\n```\n\nThis downloads the video in best available quality as MP4 to `/mnt/user-data/outputs/`.\n\n## Options\n\n### Quality Settings\n\nUse `-q` or `--quality` to specify video quality:\n\n- `best` (default): Highest quality available\n- `1080p`: Full HD\n- `720p`: HD\n- `480p`: Standard definition\n- `360p`: Lower quality\n- `worst`: Lowest quality available\n\nExample:\n```bash\npython scripts/download_video.py \"URL\" -q 720p\n```\n\n### Format Options\n\nUse `-f` or `--format` to specify output format (video downloads only):\n\n- `mp4` (default): Most compatible\n- `webm`: Modern format\n- `mkv`: Matroska container\n\nExample:\n```bash\npython scripts/download_video.py \"URL\" -f webm\n```\n\n### Audio Only\n\nUse `-a` or `--audio-only` to download only audio as MP3:\n\n```bash\npython scripts/download_video.py \"URL\" -a\n```\n\n### Custom Output Directory\n\nUse `-o` or `--output` to specify a different output directory:\n\n```bash\npython scripts/download_video.py \"URL\" -o /path/to/directory\n```\n\n## Complete Examples\n\n1. Download video in 1080p as MP4:\n```bash\npython scripts/download_video.py \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\" -q 1080p\n```\n\n2. Download audio only as MP3:\n```bash\npython scripts/download_video.py \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\" -a\n```\n\n3. Download in 720p as WebM to custom directory:\n```bash\npython scripts/download_video.py \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\" -q 720p -f webm -o /custom/path\n```\n\n## How It Works\n\nThe skill uses `yt-dlp`, a robust YouTube downloader that:\n- Automatically installs itself if not present\n- Fetches video information before downloading\n- Selects the best available streams matching your criteria\n- Merges video and audio streams when needed\n- Supports a wide range of YouTube video formats\n\n## Important Notes\n\n- Downloads are saved to `/mnt/user-data/outputs/` by default\n- Video filename is automatically generated from the video title\n- The script handles installation of yt-dlp automatically\n- Only single videos are downloaded (playlists are skipped by default)\n- Higher quality videos may take longer to download and use more disk space"
              },
              {
                "name": "webapp-testing",
                "description": "Toolkit for interacting with and testing local web applications using Playwright. Supports verifying frontend functionality, debugging UI behavior, capturing browser screenshots, and viewing browser logs.",
                "path": "plugins/all-skills/skills/webapp-testing/SKILL.md",
                "frontmatter": {
                  "name": "webapp-testing",
                  "category": "development-code",
                  "description": "Toolkit for interacting with and testing local web applications using Playwright. Supports verifying frontend functionality, debugging UI behavior, capturing browser screenshots, and viewing browser logs.",
                  "license": "Complete terms in LICENSE.txt"
                },
                "content": "# Web Application Testing\n\nTo test local web applications, write native Python Playwright scripts.\n\n**Helper Scripts Available**:\n- `scripts/with_server.py` - Manages server lifecycle (supports multiple servers)\n\n**Always run scripts with `--help` first** to see usage. DO NOT read the source until you try running the script first and find that a customized solution is abslutely necessary. These scripts can be very large and thus pollute your context window. They exist to be called directly as black-box scripts rather than ingested into your context window.\n\n## Decision Tree: Choosing Your Approach\n\n```\nUser task  Is it static HTML?\n     Yes  Read HTML file directly to identify selectors\n              Success  Write Playwright script using selectors\n              Fails/Incomplete  Treat as dynamic (below)\n    \n     No (dynamic webapp)  Is the server already running?\n         No  Run: python scripts/with_server.py --help\n                Then use the helper + write simplified Playwright script\n        \n         Yes  Reconnaissance-then-action:\n            1. Navigate and wait for networkidle\n            2. Take screenshot or inspect DOM\n            3. Identify selectors from rendered state\n            4. Execute actions with discovered selectors\n```\n\n## Example: Using with_server.py\n\nTo start a server, run `--help` first, then use the helper:\n\n**Single server:**\n```bash\npython scripts/with_server.py --server \"npm run dev\" --port 5173 -- python your_automation.py\n```\n\n**Multiple servers (e.g., backend + frontend):**\n```bash\npython scripts/with_server.py \\\n  --server \"cd backend && python server.py\" --port 3000 \\\n  --server \"cd frontend && npm run dev\" --port 5173 \\\n  -- python your_automation.py\n```\n\nTo create an automation script, include only Playwright logic (servers are managed automatically):\n```python\nfrom playwright.sync_api import sync_playwright\n\nwith sync_playwright() as p:\n    browser = p.chromium.launch(headless=True) # Always launch chromium in headless mode\n    page = browser.new_page()\n    page.goto('http://localhost:5173') # Server already running and ready\n    page.wait_for_load_state('networkidle') # CRITICAL: Wait for JS to execute\n    # ... your automation logic\n    browser.close()\n```\n\n## Reconnaissance-Then-Action Pattern\n\n1. **Inspect rendered DOM**:\n   ```python\n   page.screenshot(path='/tmp/inspect.png', full_page=True)\n   content = page.content()\n   page.locator('button').all()\n   ```\n\n2. **Identify selectors** from inspection results\n\n3. **Execute actions** using discovered selectors\n\n## Common Pitfall\n\n **Don't** inspect the DOM before waiting for `networkidle` on dynamic apps\n **Do** wait for `page.wait_for_load_state('networkidle')` before inspection\n\n## Best Practices\n\n- **Use bundled scripts as black boxes** - To accomplish a task, consider whether one of the scripts available in `scripts/` can help. These scripts handle common, complex workflows reliably without cluttering the context window. Use `--help` to see usage, then invoke directly. \n- Use `sync_playwright()` for synchronous scripts\n- Always close the browser when done\n- Use descriptive selectors: `text=`, `role=`, CSS selectors, or IDs\n- Add appropriate waits: `page.wait_for_selector()` or `page.wait_for_timeout()`\n\n## Reference Files\n\n- **examples/** - Examples showing common patterns:\n  - `element_discovery.py` - Discovering buttons, links, and inputs on a page\n  - `static_html_automation.py` - Using file:// URLs for local HTML\n  - `console_logging.py` - Capturing console logs during automation"
              },
              {
                "name": "xlsx",
                "description": "Comprehensive spreadsheet creation, editing, and analysis with support for formulas, formatting, data analysis, and visualization. When Claude needs to work with spreadsheets (.xlsx, .xlsm, .csv, .tsv, etc) for: (1) Creating new spreadsheets with formulas and formatting, (2) Reading or analyzing data, (3) Modify existing spreadsheets while preserving formulas, (4) Data analysis and visualization in spreadsheets, or (5) Recalculating formulas",
                "path": "plugins/all-skills/skills/xlsx/SKILL.md",
                "frontmatter": {
                  "name": "xlsx",
                  "description": "Comprehensive spreadsheet creation, editing, and analysis with support for formulas, formatting, data analysis, and visualization. When Claude needs to work with spreadsheets (.xlsx, .xlsm, .csv, .tsv, etc) for: (1) Creating new spreadsheets with formulas and formatting, (2) Reading or analyzing data, (3) Modify existing spreadsheets while preserving formulas, (4) Data analysis and visualization in spreadsheets, or (5) Recalculating formulas",
                  "category": "document-processing",
                  "license": "Proprietary. LICENSE.txt has complete terms"
                },
                "content": "# Requirements for Outputs\n\n## All Excel files\n\n### Zero Formula Errors\n- Every Excel model MUST be delivered with ZERO formula errors (#REF!, #DIV/0!, #VALUE!, #N/A, #NAME?)\n\n### Preserve Existing Templates (when updating templates)\n- Study and EXACTLY match existing format, style, and conventions when modifying files\n- Never impose standardized formatting on files with established patterns\n- Existing template conventions ALWAYS override these guidelines\n\n## Financial models\n\n### Color Coding Standards\nUnless otherwise stated by the user or existing template\n\n#### Industry-Standard Color Conventions\n- **Blue text (RGB: 0,0,255)**: Hardcoded inputs, and numbers users will change for scenarios\n- **Black text (RGB: 0,0,0)**: ALL formulas and calculations\n- **Green text (RGB: 0,128,0)**: Links pulling from other worksheets within same workbook\n- **Red text (RGB: 255,0,0)**: External links to other files\n- **Yellow background (RGB: 255,255,0)**: Key assumptions needing attention or cells that need to be updated\n\n### Number Formatting Standards\n\n#### Required Format Rules\n- **Years**: Format as text strings (e.g., \"2024\" not \"2,024\")\n- **Currency**: Use $#,##0 format; ALWAYS specify units in headers (\"Revenue ($mm)\")\n- **Zeros**: Use number formatting to make all zeros \"-\", including percentages (e.g., \"$#,##0;($#,##0);-\")\n- **Percentages**: Default to 0.0% format (one decimal)\n- **Multiples**: Format as 0.0x for valuation multiples (EV/EBITDA, P/E)\n- **Negative numbers**: Use parentheses (123) not minus -123\n\n### Formula Construction Rules\n\n#### Assumptions Placement\n- Place ALL assumptions (growth rates, margins, multiples, etc.) in separate assumption cells\n- Use cell references instead of hardcoded values in formulas\n- Example: Use =B5*(1+$B$6) instead of =B5*1.05\n\n#### Formula Error Prevention\n- Verify all cell references are correct\n- Check for off-by-one errors in ranges\n- Ensure consistent formulas across all projection periods\n- Test with edge cases (zero values, negative numbers)\n- Verify no unintended circular references\n\n#### Documentation Requirements for Hardcodes\n- Comment or in cells beside (if end of table). Format: \"Source: [System/Document], [Date], [Specific Reference], [URL if applicable]\"\n- Examples:\n  - \"Source: Company 10-K, FY2024, Page 45, Revenue Note, [SEC EDGAR URL]\"\n  - \"Source: Company 10-Q, Q2 2025, Exhibit 99.1, [SEC EDGAR URL]\"\n  - \"Source: Bloomberg Terminal, 8/15/2025, AAPL US Equity\"\n  - \"Source: FactSet, 8/20/2025, Consensus Estimates Screen\"\n\n# XLSX creation, editing, and analysis\n\n## Overview\n\nA user may ask you to create, edit, or analyze the contents of an .xlsx file. You have different tools and workflows available for different tasks.\n\n## Important Requirements\n\n**LibreOffice Required for Formula Recalculation**: You can assume LibreOffice is installed for recalculating formula values using the `recalc.py` script. The script automatically configures LibreOffice on first run\n\n## Reading and analyzing data\n\n### Data analysis with pandas\nFor data analysis, visualization, and basic operations, use **pandas** which provides powerful data manipulation capabilities:\n\n```python\nimport pandas as pd\n\n# Read Excel\ndf = pd.read_excel('file.xlsx')  # Default: first sheet\nall_sheets = pd.read_excel('file.xlsx', sheet_name=None)  # All sheets as dict\n\n# Analyze\ndf.head()      # Preview data\ndf.info()      # Column info\ndf.describe()  # Statistics\n\n# Write Excel\ndf.to_excel('output.xlsx', index=False)\n```\n\n## Excel File Workflows\n\n## CRITICAL: Use Formulas, Not Hardcoded Values\n\n**Always use Excel formulas instead of calculating values in Python and hardcoding them.** This ensures the spreadsheet remains dynamic and updateable.\n\n###  WRONG - Hardcoding Calculated Values\n```python\n# Bad: Calculating in Python and hardcoding result\ntotal = df['Sales'].sum()\nsheet['B10'] = total  # Hardcodes 5000\n\n# Bad: Computing growth rate in Python\ngrowth = (df.iloc[-1]['Revenue'] - df.iloc[0]['Revenue']) / df.iloc[0]['Revenue']\nsheet['C5'] = growth  # Hardcodes 0.15\n\n# Bad: Python calculation for average\navg = sum(values) / len(values)\nsheet['D20'] = avg  # Hardcodes 42.5\n```\n\n###  CORRECT - Using Excel Formulas\n```python\n# Good: Let Excel calculate the sum\nsheet['B10'] = '=SUM(B2:B9)'\n\n# Good: Growth rate as Excel formula\nsheet['C5'] = '=(C4-C2)/C2'\n\n# Good: Average using Excel function\nsheet['D20'] = '=AVERAGE(D2:D19)'\n```\n\nThis applies to ALL calculations - totals, percentages, ratios, differences, etc. The spreadsheet should be able to recalculate when source data changes.\n\n## Common Workflow\n1. **Choose tool**: pandas for data, openpyxl for formulas/formatting\n2. **Create/Load**: Create new workbook or load existing file\n3. **Modify**: Add/edit data, formulas, and formatting\n4. **Save**: Write to file\n5. **Recalculate formulas (MANDATORY IF USING FORMULAS)**: Use the recalc.py script\n   ```bash\n   python recalc.py output.xlsx\n   ```\n6. **Verify and fix any errors**: \n   - The script returns JSON with error details\n   - If `status` is `errors_found`, check `error_summary` for specific error types and locations\n   - Fix the identified errors and recalculate again\n   - Common errors to fix:\n     - `#REF!`: Invalid cell references\n     - `#DIV/0!`: Division by zero\n     - `#VALUE!`: Wrong data type in formula\n     - `#NAME?`: Unrecognized formula name\n\n### Creating new Excel files\n\n```python\n# Using openpyxl for formulas and formatting\nfrom openpyxl import Workbook\nfrom openpyxl.styles import Font, PatternFill, Alignment\n\nwb = Workbook()\nsheet = wb.active\n\n# Add data\nsheet['A1'] = 'Hello'\nsheet['B1'] = 'World'\nsheet.append(['Row', 'of', 'data'])\n\n# Add formula\nsheet['B2'] = '=SUM(A1:A10)'\n\n# Formatting\nsheet['A1'].font = Font(bold=True, color='FF0000')\nsheet['A1'].fill = PatternFill('solid', start_color='FFFF00')\nsheet['A1'].alignment = Alignment(horizontal='center')\n\n# Column width\nsheet.column_dimensions['A'].width = 20\n\nwb.save('output.xlsx')\n```\n\n### Editing existing Excel files\n\n```python\n# Using openpyxl to preserve formulas and formatting\nfrom openpyxl import load_workbook\n\n# Load existing file\nwb = load_workbook('existing.xlsx')\nsheet = wb.active  # or wb['SheetName'] for specific sheet\n\n# Working with multiple sheets\nfor sheet_name in wb.sheetnames:\n    sheet = wb[sheet_name]\n    print(f\"Sheet: {sheet_name}\")\n\n# Modify cells\nsheet['A1'] = 'New Value'\nsheet.insert_rows(2)  # Insert row at position 2\nsheet.delete_cols(3)  # Delete column 3\n\n# Add new sheet\nnew_sheet = wb.create_sheet('NewSheet')\nnew_sheet['A1'] = 'Data'\n\nwb.save('modified.xlsx')\n```\n\n## Recalculating formulas\n\nExcel files created or modified by openpyxl contain formulas as strings but not calculated values. Use the provided `recalc.py` script to recalculate formulas:\n\n```bash\npython recalc.py <excel_file> [timeout_seconds]\n```\n\nExample:\n```bash\npython recalc.py output.xlsx 30\n```\n\nThe script:\n- Automatically sets up LibreOffice macro on first run\n- Recalculates all formulas in all sheets\n- Scans ALL cells for Excel errors (#REF!, #DIV/0!, etc.)\n- Returns JSON with detailed error locations and counts\n- Works on both Linux and macOS\n\n## Formula Verification Checklist\n\nQuick checks to ensure formulas work correctly:\n\n### Essential Verification\n- [ ] **Test 2-3 sample references**: Verify they pull correct values before building full model\n- [ ] **Column mapping**: Confirm Excel columns match (e.g., column 64 = BL, not BK)\n- [ ] **Row offset**: Remember Excel rows are 1-indexed (DataFrame row 5 = Excel row 6)\n\n### Common Pitfalls\n- [ ] **NaN handling**: Check for null values with `pd.notna()`\n- [ ] **Far-right columns**: FY data often in columns 50+ \n- [ ] **Multiple matches**: Search all occurrences, not just first\n- [ ] **Division by zero**: Check denominators before using `/` in formulas (#DIV/0!)\n- [ ] **Wrong references**: Verify all cell references point to intended cells (#REF!)\n- [ ] **Cross-sheet references**: Use correct format (Sheet1!A1) for linking sheets\n\n### Formula Testing Strategy\n- [ ] **Start small**: Test formulas on 2-3 cells before applying broadly\n- [ ] **Verify dependencies**: Check all cells referenced in formulas exist\n- [ ] **Test edge cases**: Include zero, negative, and very large values\n\n### Interpreting recalc.py Output\nThe script returns JSON with error details:\n```json\n{\n  \"status\": \"success\",           // or \"errors_found\"\n  \"total_errors\": 0,              // Total error count\n  \"total_formulas\": 42,           // Number of formulas in file\n  \"error_summary\": {              // Only present if errors found\n    \"#REF!\": {\n      \"count\": 2,\n      \"locations\": [\"Sheet1!B5\", \"Sheet1!C10\"]\n    }\n  }\n}\n```\n\n## Best Practices\n\n### Library Selection\n- **pandas**: Best for data analysis, bulk operations, and simple data export\n- **openpyxl**: Best for complex formatting, formulas, and Excel-specific features\n\n### Working with openpyxl\n- Cell indices are 1-based (row=1, column=1 refers to cell A1)\n- Use `data_only=True` to read calculated values: `load_workbook('file.xlsx', data_only=True)`\n- **Warning**: If opened with `data_only=True` and saved, formulas are replaced with values and permanently lost\n- For large files: Use `read_only=True` for reading or `write_only=True` for writing\n- Formulas are preserved but not evaluated - use recalc.py to update values\n\n### Working with pandas\n- Specify data types to avoid inference issues: `pd.read_excel('file.xlsx', dtype={'id': str})`\n- For large files, read specific columns: `pd.read_excel('file.xlsx', usecols=['A', 'C', 'E'])`\n- Handle dates properly: `pd.read_excel('file.xlsx', parse_dates=['date_column'])`\n\n## Code Style Guidelines\n**IMPORTANT**: When generating Python code for Excel operations:\n- Write minimal, concise Python code without unnecessary comments\n- Avoid verbose variable names and redundant operations\n- Avoid unnecessary print statements\n\n**For Excel files themselves**:\n- Add comments to cells with complex formulas or important assumptions\n- Document data sources for hardcoded values\n- Include notes for key calculations and model sections"
              }
            ]
          },
          {
            "name": "nextjs-expert",
            "description": "Next.js development expertise with skills for App Router, Server Components, Route Handlers, Server Actions, and authentication patterns",
            "source": "./plugins/nextjs-expert",
            "category": "skills",
            "version": "1.0.0",
            "author": {
              "name": "BuildWithClaude Community",
              "url": "https://github.com/davepoon/buildwithclaude"
            },
            "install_commands": [
              "/plugin marketplace add davepoon/buildwithclaude",
              "/plugin install nextjs-expert@buildwithclaude"
            ],
            "signals": {
              "stars": 2229,
              "forks": 239,
              "pushed_at": "2026-01-12T10:19:01Z",
              "created_at": "2025-07-25T02:26:45Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/add-auth",
                "description": "Add authentication to a Next.js application",
                "path": "plugins/nextjs-expert/commands/add-auth.md",
                "frontmatter": {
                  "description": "Add authentication to a Next.js application",
                  "allowed-tools": [
                    "Read",
                    "Write",
                    "Glob",
                    "Grep",
                    "Bash",
                    "Skill",
                    "AskUserQuestion"
                  ]
                },
                "content": "# Add Authentication to Next.js App\n\nYou are adding authentication to a Next.js application using Auth.js (NextAuth.js v5).\n\n## Workflow\n\n### Step 1: Assess Current State\n\n1. Check if authentication already exists:\n   - Look for `auth.ts` or `auth.config.ts`\n   - Check for `next-auth` in `package.json`\n   - Look for existing middleware\n\n2. Identify database setup:\n   - Check for Prisma schema\n   - Identify existing User model\n\n### Step 2: Ask About Requirements\n\nUse AskUserQuestion to clarify:\n- **Providers**: Which OAuth providers? (GitHub, Google, Email/Password)\n- **Session strategy**: JWT or database sessions?\n- **Protected routes**: Which routes need protection?\n- **Role-based access**: Is RBAC needed?\n\n### Step 3: Install Dependencies\n\n```bash\nnpm install next-auth@beta @auth/prisma-adapter\n```\n\nIf using credentials:\n```bash\nnpm install bcryptjs\nnpm install -D @types/bcryptjs\n```\n\n### Step 4: Create Auth Configuration\n\nApply the `auth-patterns` skill and create:\n\n1. **`auth.ts`** - Main auth configuration with:\n   - Provider setup\n   - Adapter configuration\n   - Session callbacks\n   - JWT callbacks\n\n2. **`app/api/auth/[...nextauth]/route.ts`** - Route handler\n\n### Step 5: Add Middleware\n\nCreate `middleware.ts` with:\n- Route protection logic\n- Redirect rules for auth pages\n- Public route exceptions\n\n### Step 6: Update Database Schema\n\nIf using Prisma, add/update models:\n- User\n- Account\n- Session\n- VerificationToken\n\n### Step 7: Create Auth Components\n\n1. **Login form** - Credentials + OAuth buttons\n2. **Register form** - If using credentials\n3. **User menu** - Session-aware navigation\n4. **Session provider** - Client-side wrapper\n\n### Step 8: Environment Variables\n\nCreate/update `.env.local`:\n```\nAUTH_SECRET=generated-secret\nAUTH_URL=http://localhost:3000\nGITHUB_CLIENT_ID=...\nGITHUB_CLIENT_SECRET=...\n```\n\n## Implementation Checklist\n\n- [ ] Install next-auth@beta\n- [ ] Create auth.ts configuration\n- [ ] Create API route handler\n- [ ] Add middleware for route protection\n- [ ] Update Prisma schema (if using database)\n- [ ] Create login/register pages\n- [ ] Add SessionProvider to layout\n- [ ] Create auth-related components\n- [ ] Set up environment variables\n- [ ] Test authentication flow\n\n## Files to Create\n\n| File | Purpose |\n|------|---------|\n| `auth.ts` | Auth configuration |\n| `app/api/auth/[...nextauth]/route.ts` | NextAuth route handler |\n| `middleware.ts` | Route protection |\n| `app/login/page.tsx` | Login page |\n| `app/register/page.tsx` | Registration page (if credentials) |\n| `components/login-form.tsx` | Login form component |\n| `components/oauth-buttons.tsx` | OAuth sign-in buttons |\n| `components/user-menu.tsx` | User dropdown menu |\n| `app/providers.tsx` | SessionProvider wrapper |\n| `types/next-auth.d.ts` | Extended types |\n| `lib/auth-helpers.ts` | Auth utility functions |\n\n## Security Considerations\n\n- Use `AUTH_SECRET` from environment\n- Set `httpOnly` cookies\n- Implement CSRF protection\n- Hash passwords with bcrypt (min 12 rounds)\n- Validate all inputs\n- Rate limit authentication endpoints"
              },
              {
                "name": "/optimize",
                "description": "Analyze and optimize Next.js application performance",
                "path": "plugins/nextjs-expert/commands/optimize.md",
                "frontmatter": {
                  "description": "Analyze and optimize Next.js application performance",
                  "allowed-tools": [
                    "Read",
                    "Glob",
                    "Grep",
                    "Skill",
                    "AskUserQuestion"
                  ]
                },
                "content": "# Optimize Next.js Application\n\nYou are analyzing a Next.js application to identify and recommend performance optimizations.\n\n## Workflow\n\n### Step 1: Analyze Current State\n\nExamine the codebase for common performance issues:\n\n1. **Component Boundaries**\n   - Find `'use client'` directives\n   - Check if client boundaries are too high\n   - Look for unnecessary client components\n\n2. **Data Fetching**\n   - Check for waterfalls (sequential fetches)\n   - Look for missing parallel fetching\n   - Identify uncached requests\n\n3. **Bundle Size**\n   - Check for large client-side imports\n   - Look for unused dependencies\n   - Find components that could be server-rendered\n\n4. **Image Optimization**\n   - Look for `<img>` tags (should use `next/image`)\n   - Check for missing width/height\n   - Identify unoptimized images\n\n5. **Caching**\n   - Check fetch() cache options\n   - Look for missing revalidation strategies\n   - Identify static vs dynamic content\n\n### Step 2: Apply Skills\n\nUse relevant skills to understand best practices:\n- `server-components` - For component boundary optimization\n- `app-router` - For routing and layout optimization\n- `route-handlers` - For API optimization\n- `server-actions` - For mutation optimization\n\n### Step 3: Generate Report\n\nCreate an optimization report covering:\n\n## Optimization Categories\n\n### 1. Server/Client Component Boundaries\n\n**Check for:**\n- Client components that don't use hooks or browser APIs\n- Large component trees under 'use client'\n- Data fetching in client components\n\n**Recommendations:**\n- Move data fetching to server components\n- Push 'use client' boundaries down\n- Pass server data as props to client components\n\n### 2. Data Fetching Patterns\n\n**Check for:**\n- Sequential await statements\n- fetch() without cache options\n- Missing generateStaticParams\n\n**Recommendations:**\n```tsx\n// Before: Sequential\nconst user = await getUser()\nconst posts = await getPosts()\n\n// After: Parallel\nconst [user, posts] = await Promise.all([\n  getUser(),\n  getPosts(),\n])\n```\n\n### 3. Bundle Optimization\n\n**Check for:**\n- Large npm packages in client components\n- Missing dynamic imports\n- Unused exports\n\n**Recommendations:**\n```tsx\n// Dynamic import for client-only libraries\nconst Chart = dynamic(() => import('./Chart'), {\n  loading: () => <ChartSkeleton />,\n  ssr: false,\n})\n```\n\n### 4. Image Optimization\n\n**Check for:**\n- `<img>` instead of `<Image>`\n- Missing blur placeholders\n- Unoptimized formats\n\n**Recommendations:**\n```tsx\nimport Image from 'next/image'\n\n<Image\n  src=\"/hero.jpg\"\n  alt=\"Hero\"\n  width={1200}\n  height={600}\n  priority // For above-the-fold images\n  placeholder=\"blur\"\n/>\n```\n\n### 5. Caching Strategy\n\n**Check for:**\n- Missing cache directives\n- No revalidation configuration\n- Overly dynamic pages\n\n**Recommendations:**\n```tsx\n// Cache with revalidation\nconst data = await fetch(url, {\n  next: { revalidate: 3600 }, // 1 hour\n})\n\n// Tag-based revalidation\nconst data = await fetch(url, {\n  next: { tags: ['products'] },\n})\n```\n\n### 6. Loading States\n\n**Check for:**\n- Missing loading.tsx files\n- No Suspense boundaries\n- Blocking renders\n\n**Recommendations:**\n- Add loading.tsx for route segments\n- Wrap slow components in Suspense\n- Use streaming for large pages\n\n## Output Format\n\nAfter analysis, provide:\n\n1. **Summary**: Overall performance assessment\n2. **High Priority**: Issues with biggest impact\n3. **Quick Wins**: Easy fixes with good returns\n4. **Code Changes**: Specific file changes needed\n5. **Metrics**: Expected improvements\n\n## Example Report\n\n```\n## Performance Analysis Report\n\n### Summary\nFound 5 high-priority and 8 medium-priority optimization opportunities.\n\n### High Priority Issues\n\n1. **Large Client Boundary** (app/dashboard/page.tsx)\n   - Impact: High\n   - Issue: Entire dashboard is client-rendered\n   - Fix: Split into server/client components\n\n2. **Sequential Data Fetching** (app/products/page.tsx)\n   - Impact: High\n   - Issue: 3 sequential await calls\n   - Fix: Use Promise.all()\n\n### Quick Wins\n\n1. Add loading.tsx to /dashboard\n2. Convert 12 <img> tags to <Image>\n3. Add cache options to 8 fetch() calls\n\n### Estimated Impact\n- Bundle size: -15%\n- TTFB: -200ms\n- LCP: -500ms\n```"
              },
              {
                "name": "/scaffold",
                "description": "Scaffold Next.js components following best practices",
                "path": "plugins/nextjs-expert/commands/scaffold.md",
                "frontmatter": {
                  "description": "Scaffold Next.js components following best practices",
                  "allowed-tools": [
                    "Read",
                    "Write",
                    "Glob",
                    "Grep",
                    "Skill",
                    "AskUserQuestion"
                  ]
                },
                "content": "# Scaffold Next.js Component\n\nYou are scaffolding a Next.js component or page following App Router best practices.\n\n## Workflow\n\n### Step 1: Gather Requirements\n\nAsk the user what they want to scaffold:\n- **Page**: A new route with page.tsx\n- **Layout**: A shared layout for routes\n- **Server Component**: A data-fetching component\n- **Client Component**: An interactive component\n- **API Route**: A route handler\n- **Server Action**: A mutation function\n- **Loading/Error**: Loading and error states\n\n### Step 2: Analyze Existing Patterns\n\nBefore scaffolding, examine the codebase:\n\n1. Check existing component structure:\n   - Look at `app/` directory structure\n   - Check `components/` organization\n   - Note existing patterns for server vs client\n\n2. Identify conventions:\n   - TypeScript patterns used\n   - Styling approach (Tailwind, CSS Modules, etc.)\n   - Data fetching patterns\n   - Error handling patterns\n\n### Step 3: Apply Skills\n\nBased on what the user wants, apply the appropriate skill:\n\n- For pages and routing: Use `app-router` skill\n- For component decisions: Use `server-components` skill\n- For API routes: Use `route-handlers` skill\n- For mutations: Use `server-actions` skill\n- For protected routes: Use `auth-patterns` skill\n\n### Step 4: Generate Code\n\nCreate the component following:\n- Next.js 15 conventions\n- Async params/searchParams (Promise-based)\n- Proper TypeScript types\n- Existing project patterns\n\n### Step 5: Verify\n\nAfter scaffolding:\n1. Ensure imports are correct\n2. Check TypeScript types\n3. Confirm file placement matches App Router conventions\n\n## Examples\n\n### Scaffold a Page\n\nWhen user asks: \"Create a blog post page\"\n\n1. Create `app/blog/[slug]/page.tsx`:\n```tsx\ninterface PageProps {\n  params: Promise<{ slug: string }>\n}\n\nexport default async function BlogPostPage({ params }: PageProps) {\n  const { slug } = await params\n  // ... fetch and render\n}\n```\n\n2. Add `loading.tsx` and `error.tsx` if needed\n\n### Scaffold a Client Component\n\nWhen user asks: \"Create a search filter component\"\n\n1. Create in `components/` with 'use client'\n2. Keep it minimal - only client code that needs state\n3. Follow existing naming conventions\n\n### Scaffold an API Route\n\nWhen user asks: \"Create a users API endpoint\"\n\n1. Create `app/api/users/route.ts`\n2. Include GET, POST as needed\n3. Add proper error handling and validation"
              }
            ],
            "skills": [
              {
                "name": "app-router",
                "description": "This skill should be used when the user asks to \"create a Next.js route\", \"add a page\", \"set up layouts\", \"implement loading states\", \"add error boundaries\", \"organize routes\", \"create dynamic routes\", or needs guidance on Next.js App Router file conventions and routing patterns.",
                "path": "plugins/nextjs-expert/skills/app-router/SKILL.md",
                "frontmatter": {
                  "name": "app-router",
                  "description": "This skill should be used when the user asks to \"create a Next.js route\", \"add a page\", \"set up layouts\", \"implement loading states\", \"add error boundaries\", \"organize routes\", \"create dynamic routes\", or needs guidance on Next.js App Router file conventions and routing patterns.",
                  "version": "1.0.0"
                },
                "content": "# Next.js App Router Patterns\n\n## Overview\n\nThe App Router is Next.js's file-system based router built on React Server Components. It uses a `app/` directory structure where folders define routes and special files control UI behavior.\n\n## Core File Conventions\n\n### Route Files\n\nEach route segment is defined by a folder. Special files within folders control behavior:\n\n| File | Purpose |\n|------|---------|\n| `page.tsx` | Unique UI for a route, makes route publicly accessible |\n| `layout.tsx` | Shared UI wrapper, preserves state across navigations |\n| `loading.tsx` | Loading UI using React Suspense |\n| `error.tsx` | Error boundary for route segment |\n| `not-found.tsx` | UI for 404 responses |\n| `template.tsx` | Like layout but re-renders on navigation |\n| `default.tsx` | Fallback for parallel routes |\n\n### Folder Conventions\n\n| Pattern | Purpose | Example |\n|---------|---------|---------|\n| `folder/` | Route segment | `app/blog/`  `/blog` |\n| `[folder]/` | Dynamic segment | `app/blog/[slug]/`  `/blog/:slug` |\n| `[...folder]/` | Catch-all segment | `app/docs/[...slug]/`  `/docs/*` |\n| `[[...folder]]/` | Optional catch-all | `app/shop/[[...slug]]/`  `/shop` or `/shop/*` |\n| `(folder)/` | Route group (no URL) | `app/(marketing)/about/`  `/about` |\n| `@folder/` | Named slot (parallel routes) | `app/@modal/login/` |\n| `_folder/` | Private folder (excluded) | `app/_components/` |\n\n## Creating Routes\n\n### Basic Route Structure\n\nTo create a new route, add a folder with `page.tsx`:\n\n```\napp/\n page.tsx              # / (home)\n about/\n    page.tsx          # /about\n blog/\n     page.tsx          # /blog\n     [slug]/\n         page.tsx      # /blog/:slug\n```\n\n### Page Component\n\nA page is a Server Component by default:\n\n```tsx\n// app/about/page.tsx\nexport default function AboutPage() {\n  return (\n    <main>\n      <h1>About Us</h1>\n      <p>Welcome to our company.</p>\n    </main>\n  )\n}\n```\n\n### Dynamic Routes\n\nAccess route parameters via the `params` prop:\n\n```tsx\n// app/blog/[slug]/page.tsx\ninterface PageProps {\n  params: Promise<{ slug: string }>\n}\n\nexport default async function BlogPost({ params }: PageProps) {\n  const { slug } = await params\n  const post = await getPost(slug)\n\n  return <article>{post.content}</article>\n}\n```\n\n## Layouts\n\n### Root Layout (Required)\n\nEvery app needs a root layout with `<html>` and `<body>`:\n\n```tsx\n// app/layout.tsx\nexport default function RootLayout({\n  children,\n}: {\n  children: React.ReactNode\n}) {\n  return (\n    <html lang=\"en\">\n      <body>{children}</body>\n    </html>\n  )\n}\n```\n\n### Nested Layouts\n\nLayouts wrap their children and preserve state:\n\n```tsx\n// app/dashboard/layout.tsx\nexport default function DashboardLayout({\n  children,\n}: {\n  children: React.ReactNode\n}) {\n  return (\n    <div className=\"flex\">\n      <Sidebar />\n      <main className=\"flex-1\">{children}</main>\n    </div>\n  )\n}\n```\n\n## Loading and Error States\n\n### Loading UI\n\nCreate instant loading states with Suspense:\n\n```tsx\n// app/dashboard/loading.tsx\nexport default function Loading() {\n  return <div className=\"animate-pulse\">Loading...</div>\n}\n```\n\n### Error Boundaries\n\nHandle errors gracefully:\n\n```tsx\n// app/dashboard/error.tsx\n'use client'\n\nexport default function Error({\n  error,\n  reset,\n}: {\n  error: Error\n  reset: () => void\n}) {\n  return (\n    <div>\n      <h2>Something went wrong!</h2>\n      <button onClick={reset}>Try again</button>\n    </div>\n  )\n}\n```\n\n## Route Groups\n\nOrganize routes without affecting URL structure:\n\n```\napp/\n (marketing)/\n    layout.tsx        # Marketing layout\n    about/page.tsx    # /about\n    contact/page.tsx  # /contact\n (shop)/\n     layout.tsx        # Shop layout\n     products/page.tsx # /products\n```\n\n## Metadata\n\n### Static Metadata\n\n```tsx\n// app/about/page.tsx\nimport { Metadata } from 'next'\n\nexport const metadata: Metadata = {\n  title: 'About Us',\n  description: 'Learn more about our company',\n}\n```\n\n### Dynamic Metadata\n\n```tsx\n// app/blog/[slug]/page.tsx\nexport async function generateMetadata({ params }: PageProps): Promise<Metadata> {\n  const { slug } = await params\n  const post = await getPost(slug)\n  return { title: post.title }\n}\n```\n\n## Key Patterns\n\n1. **Colocation**: Keep components, tests, and styles near routes\n2. **Private folders**: Use `_folder` for non-route files\n3. **Route groups**: Use `(folder)` to organize without URL impact\n4. **Parallel routes**: Use `@slot` for complex layouts\n5. **Intercepting routes**: Use `(.)` patterns for modals\n\n## Resources\n\nFor detailed patterns, see:\n- `references/routing-conventions.md` - Complete file conventions\n- `references/layouts-templates.md` - Layout composition patterns\n- `references/loading-error-states.md` - Suspense and error handling\n- `examples/dynamic-routes.md` - Dynamic routing examples\n- `examples/parallel-routes.md` - Parallel and intercepting routes"
              },
              {
                "name": "auth-patterns",
                "description": "This skill should be used when the user asks about \"authentication in Next.js\", \"NextAuth\", \"Auth.js\", \"middleware auth\", \"protected routes\", \"session management\", \"JWT\", \"login flow\", or needs guidance on implementing authentication and authorization in Next.js applications.",
                "path": "plugins/nextjs-expert/skills/auth-patterns/SKILL.md",
                "frontmatter": {
                  "name": "auth-patterns",
                  "description": "This skill should be used when the user asks about \"authentication in Next.js\", \"NextAuth\", \"Auth.js\", \"middleware auth\", \"protected routes\", \"session management\", \"JWT\", \"login flow\", or needs guidance on implementing authentication and authorization in Next.js applications.",
                  "version": "1.0.0"
                },
                "content": "# Authentication Patterns in Next.js\n\n## Overview\n\nNext.js supports multiple authentication strategies. This skill covers common patterns including NextAuth.js (Auth.js), middleware-based protection, and session management.\n\n## Authentication Libraries\n\n| Library | Best For |\n|---------|----------|\n| NextAuth.js (Auth.js) | Full-featured auth with providers |\n| Clerk | Managed auth service |\n| Lucia | Lightweight, flexible auth |\n| Supabase Auth | Supabase ecosystem |\n| Custom JWT | Full control |\n\n## NextAuth.js v5 Setup\n\n### Installation\n\n```bash\nnpm install next-auth@beta\n```\n\n### Configuration\n\n```tsx\n// auth.ts\nimport NextAuth from 'next-auth'\nimport GitHub from 'next-auth/providers/github'\nimport Credentials from 'next-auth/providers/credentials'\n\nexport const { handlers, auth, signIn, signOut } = NextAuth({\n  providers: [\n    GitHub({\n      clientId: process.env.GITHUB_ID,\n      clientSecret: process.env.GITHUB_SECRET,\n    }),\n    Credentials({\n      credentials: {\n        email: { label: 'Email', type: 'email' },\n        password: { label: 'Password', type: 'password' },\n      },\n      authorize: async (credentials) => {\n        const user = await getUserByEmail(credentials.email)\n        if (!user || !verifyPassword(credentials.password, user.password)) {\n          return null\n        }\n        return user\n      },\n    }),\n  ],\n  callbacks: {\n    authorized: async ({ auth }) => {\n      return !!auth\n    },\n  },\n})\n```\n\n### API Route Handler\n\n```tsx\n// app/api/auth/[...nextauth]/route.ts\nimport { handlers } from '@/auth'\n\nexport const { GET, POST } = handlers\n```\n\n### Middleware Protection\n\n```tsx\n// middleware.ts\nexport { auth as middleware } from '@/auth'\n\nexport const config = {\n  matcher: ['/dashboard/:path*', '/api/protected/:path*'],\n}\n```\n\n## Getting Session Data\n\n### In Server Components\n\n```tsx\n// app/dashboard/page.tsx\nimport { auth } from '@/auth'\nimport { redirect } from 'next/navigation'\n\nexport default async function DashboardPage() {\n  const session = await auth()\n\n  if (!session) {\n    redirect('/login')\n  }\n\n  return (\n    <div>\n      <h1>Welcome, {session.user?.name}</h1>\n    </div>\n  )\n}\n```\n\n### In Client Components\n\n```tsx\n// components/user-menu.tsx\n'use client'\n\nimport { useSession } from 'next-auth/react'\n\nexport function UserMenu() {\n  const { data: session, status } = useSession()\n\n  if (status === 'loading') {\n    return <div>Loading...</div>\n  }\n\n  if (!session) {\n    return <SignInButton />\n  }\n\n  return (\n    <div>\n      <span>{session.user?.name}</span>\n      <SignOutButton />\n    </div>\n  )\n}\n```\n\n### Session Provider Setup\n\n```tsx\n// app/providers.tsx\n'use client'\n\nimport { SessionProvider } from 'next-auth/react'\n\nexport function Providers({ children }: { children: React.ReactNode }) {\n  return <SessionProvider>{children}</SessionProvider>\n}\n\n// app/layout.tsx\nimport { Providers } from './providers'\n\nexport default function RootLayout({ children }) {\n  return (\n    <html>\n      <body>\n        <Providers>{children}</Providers>\n      </body>\n    </html>\n  )\n}\n```\n\n## Sign In/Out Components\n\n```tsx\n// components/auth-buttons.tsx\nimport { signIn, signOut } from '@/auth'\n\nexport function SignInButton() {\n  return (\n    <form\n      action={async () => {\n        'use server'\n        await signIn('github')\n      }}\n    >\n      <button type=\"submit\">Sign in with GitHub</button>\n    </form>\n  )\n}\n\nexport function SignOutButton() {\n  return (\n    <form\n      action={async () => {\n        'use server'\n        await signOut()\n      }}\n    >\n      <button type=\"submit\">Sign out</button>\n    </form>\n  )\n}\n```\n\n## Middleware-Based Auth\n\n### Basic Pattern\n\n```tsx\n// middleware.ts\nimport { NextResponse } from 'next/server'\nimport type { NextRequest } from 'next/server'\n\nconst protectedRoutes = ['/dashboard', '/settings', '/api/protected']\nconst authRoutes = ['/login', '/signup']\n\nexport function middleware(request: NextRequest) {\n  const token = request.cookies.get('session')?.value\n  const { pathname } = request.nextUrl\n\n  // Redirect authenticated users away from auth pages\n  if (authRoutes.some(route => pathname.startsWith(route))) {\n    if (token) {\n      return NextResponse.redirect(new URL('/dashboard', request.url))\n    }\n    return NextResponse.next()\n  }\n\n  // Protect routes\n  if (protectedRoutes.some(route => pathname.startsWith(route))) {\n    if (!token) {\n      const loginUrl = new URL('/login', request.url)\n      loginUrl.searchParams.set('callbackUrl', pathname)\n      return NextResponse.redirect(loginUrl)\n    }\n  }\n\n  return NextResponse.next()\n}\n\nexport const config = {\n  matcher: ['/((?!_next/static|_next/image|favicon.ico).*)'],\n}\n```\n\n### With JWT Verification\n\n```tsx\n// middleware.ts\nimport { NextResponse } from 'next/server'\nimport { jwtVerify } from 'jose'\n\nconst secret = new TextEncoder().encode(process.env.JWT_SECRET)\n\nexport async function middleware(request: NextRequest) {\n  const token = request.cookies.get('token')?.value\n\n  if (!token) {\n    return NextResponse.redirect(new URL('/login', request.url))\n  }\n\n  try {\n    const { payload } = await jwtVerify(token, secret)\n    // Token is valid, continue\n    return NextResponse.next()\n  } catch {\n    // Token is invalid\n    return NextResponse.redirect(new URL('/login', request.url))\n  }\n}\n```\n\n## Role-Based Access Control\n\n### Extending Session Types\n\n```tsx\n// types/next-auth.d.ts\nimport { DefaultSession } from 'next-auth'\n\ndeclare module 'next-auth' {\n  interface Session {\n    user: {\n      role: 'user' | 'admin'\n    } & DefaultSession['user']\n  }\n}\n\n// auth.ts\nexport const { handlers, auth } = NextAuth({\n  callbacks: {\n    session: ({ session, token }) => ({\n      ...session,\n      user: {\n        ...session.user,\n        role: token.role,\n      },\n    }),\n    jwt: ({ token, user }) => {\n      if (user) {\n        token.role = user.role\n      }\n      return token\n    },\n  },\n})\n```\n\n### Role-Based Component\n\n```tsx\n// components/admin-only.tsx\nimport { auth } from '@/auth'\nimport { redirect } from 'next/navigation'\n\nexport async function AdminOnly({ children }: { children: React.ReactNode }) {\n  const session = await auth()\n\n  if (session?.user?.role !== 'admin') {\n    redirect('/unauthorized')\n  }\n\n  return <>{children}</>\n}\n\n// Usage\nexport default async function AdminPage() {\n  return (\n    <AdminOnly>\n      <AdminDashboard />\n    </AdminOnly>\n  )\n}\n```\n\n## Session Storage Options\n\n### JWT (Stateless)\n\n```tsx\n// auth.ts\nexport const { auth } = NextAuth({\n  session: { strategy: 'jwt' },\n  // JWT stored in cookies, no database needed\n})\n```\n\n### Database Sessions\n\n```tsx\n// auth.ts\nimport { PrismaAdapter } from '@auth/prisma-adapter'\nimport { prisma } from '@/lib/prisma'\n\nexport const { auth } = NextAuth({\n  adapter: PrismaAdapter(prisma),\n  session: { strategy: 'database' },\n  // Sessions stored in database\n})\n```\n\n## Custom Login Page\n\n```tsx\n// app/login/page.tsx\n'use client'\n\nimport { signIn } from 'next-auth/react'\nimport { useSearchParams } from 'next/navigation'\n\nexport default function LoginPage() {\n  const searchParams = useSearchParams()\n  const callbackUrl = searchParams.get('callbackUrl') || '/dashboard'\n\n  return (\n    <div className=\"flex flex-col gap-4\">\n      <button\n        onClick={() => signIn('github', { callbackUrl })}\n        className=\"btn\"\n      >\n        Sign in with GitHub\n      </button>\n      <button\n        onClick={() => signIn('google', { callbackUrl })}\n        className=\"btn\"\n      >\n        Sign in with Google\n      </button>\n    </div>\n  )\n}\n```\n\n## Security Best Practices\n\n1. **Use HTTPS** in production\n2. **Set secure cookie flags** (HttpOnly, Secure, SameSite)\n3. **Implement CSRF protection** (built into NextAuth)\n4. **Validate redirect URLs** to prevent open redirects\n5. **Use environment variables** for secrets\n6. **Implement rate limiting** on auth endpoints\n7. **Hash passwords** with bcrypt or argon2\n\n## Resources\n\nFor detailed patterns, see:\n- `references/middleware-auth.md` - Advanced middleware patterns\n- `references/session-management.md` - Session strategies\n- `examples/nextauth-setup.md` - Complete NextAuth.js setup"
              },
              {
                "name": "route-handlers",
                "description": "This skill should be used when the user asks to \"create an API route\", \"add an endpoint\", \"build a REST API\", \"handle POST requests\", \"create route handlers\", \"stream responses\", or needs guidance on Next.js API development in the App Router.",
                "path": "plugins/nextjs-expert/skills/route-handlers/SKILL.md",
                "frontmatter": {
                  "name": "route-handlers",
                  "description": "This skill should be used when the user asks to \"create an API route\", \"add an endpoint\", \"build a REST API\", \"handle POST requests\", \"create route handlers\", \"stream responses\", or needs guidance on Next.js API development in the App Router.",
                  "version": "1.0.0"
                },
                "content": "# Next.js Route Handlers\n\n## Overview\n\nRoute Handlers allow you to create API endpoints using the Web Request and Response APIs. They're defined in `route.ts` files within the `app` directory.\n\n## Basic Structure\n\n### File Convention\n\nRoute handlers use `route.ts` (or `route.js`):\n\n```\napp/\n api/\n    users/\n       route.ts      # /api/users\n    posts/\n        route.ts      # /api/posts\n        [id]/\n            route.ts  # /api/posts/:id\n```\n\n### HTTP Methods\n\nExport functions named after HTTP methods:\n\n```tsx\n// app/api/users/route.ts\nimport { NextResponse } from 'next/server'\n\nexport async function GET() {\n  const users = await db.user.findMany()\n  return NextResponse.json(users)\n}\n\nexport async function POST(request: Request) {\n  const body = await request.json()\n  const user = await db.user.create({ data: body })\n  return NextResponse.json(user, { status: 201 })\n}\n```\n\nSupported methods: `GET`, `POST`, `PUT`, `PATCH`, `DELETE`, `HEAD`, `OPTIONS`\n\n## Request Handling\n\n### Reading Request Body\n\n```tsx\nexport async function POST(request: Request) {\n  // JSON body\n  const json = await request.json()\n\n  // Form data\n  const formData = await request.formData()\n  const name = formData.get('name')\n\n  // Text body\n  const text = await request.text()\n\n  return NextResponse.json({ received: true })\n}\n```\n\n### URL Parameters\n\nDynamic route parameters:\n\n```tsx\n// app/api/posts/[id]/route.ts\ninterface RouteContext {\n  params: Promise<{ id: string }>\n}\n\nexport async function GET(\n  request: Request,\n  context: RouteContext\n) {\n  const { id } = await context.params\n  const post = await db.post.findUnique({ where: { id } })\n\n  if (!post) {\n    return NextResponse.json(\n      { error: 'Not found' },\n      { status: 404 }\n    )\n  }\n\n  return NextResponse.json(post)\n}\n```\n\n### Query Parameters\n\n```tsx\nexport async function GET(request: Request) {\n  const { searchParams } = new URL(request.url)\n  const page = searchParams.get('page') ?? '1'\n  const limit = searchParams.get('limit') ?? '10'\n\n  const posts = await db.post.findMany({\n    skip: (parseInt(page) - 1) * parseInt(limit),\n    take: parseInt(limit),\n  })\n\n  return NextResponse.json(posts)\n}\n```\n\n### Request Headers\n\n```tsx\nexport async function GET(request: Request) {\n  const authHeader = request.headers.get('authorization')\n\n  if (!authHeader?.startsWith('Bearer ')) {\n    return NextResponse.json(\n      { error: 'Unauthorized' },\n      { status: 401 }\n    )\n  }\n\n  const token = authHeader.split(' ')[1]\n  // Validate token...\n\n  return NextResponse.json({ authenticated: true })\n}\n```\n\n## Response Handling\n\n### JSON Response\n\n```tsx\nimport { NextResponse } from 'next/server'\n\nexport async function GET() {\n  return NextResponse.json(\n    { message: 'Hello' },\n    { status: 200 }\n  )\n}\n```\n\n### Setting Headers\n\n```tsx\nexport async function GET() {\n  return NextResponse.json(\n    { data: 'value' },\n    {\n      headers: {\n        'Cache-Control': 'max-age=3600',\n        'X-Custom-Header': 'custom-value',\n      },\n    }\n  )\n}\n```\n\n### Setting Cookies\n\n```tsx\nimport { cookies } from 'next/headers'\n\nexport async function POST(request: Request) {\n  const cookieStore = await cookies()\n\n  // Set cookie\n  cookieStore.set('session', 'abc123', {\n    httpOnly: true,\n    secure: process.env.NODE_ENV === 'production',\n    sameSite: 'lax',\n    maxAge: 60 * 60 * 24 * 7, // 1 week\n  })\n\n  return NextResponse.json({ success: true })\n}\n```\n\n### Redirects\n\n```tsx\nimport { redirect } from 'next/navigation'\nimport { NextResponse } from 'next/server'\n\nexport async function GET() {\n  // Option 1: redirect function (throws)\n  redirect('/login')\n\n  // Option 2: NextResponse.redirect\n  return NextResponse.redirect(new URL('/login', request.url))\n}\n```\n\n## Streaming Responses\n\n### Text Streaming\n\n```tsx\nexport async function GET() {\n  const encoder = new TextEncoder()\n  const stream = new ReadableStream({\n    async start(controller) {\n      for (let i = 0; i < 10; i++) {\n        controller.enqueue(encoder.encode(`data: ${i}\\n\\n`))\n        await new Promise(resolve => setTimeout(resolve, 100))\n      }\n      controller.close()\n    },\n  })\n\n  return new Response(stream, {\n    headers: {\n      'Content-Type': 'text/event-stream',\n      'Cache-Control': 'no-cache',\n      'Connection': 'keep-alive',\n    },\n  })\n}\n```\n\n### AI/LLM Streaming\n\n```tsx\nexport async function POST(request: Request) {\n  const { prompt } = await request.json()\n\n  const response = await openai.chat.completions.create({\n    model: 'gpt-4',\n    messages: [{ role: 'user', content: prompt }],\n    stream: true,\n  })\n\n  const stream = new ReadableStream({\n    async start(controller) {\n      for await (const chunk of response) {\n        const text = chunk.choices[0]?.delta?.content || ''\n        controller.enqueue(new TextEncoder().encode(text))\n      }\n      controller.close()\n    },\n  })\n\n  return new Response(stream, {\n    headers: { 'Content-Type': 'text/plain' },\n  })\n}\n```\n\n## CORS Configuration\n\n```tsx\nexport async function OPTIONS() {\n  return new Response(null, {\n    status: 204,\n    headers: {\n      'Access-Control-Allow-Origin': '*',\n      'Access-Control-Allow-Methods': 'GET, POST, PUT, DELETE',\n      'Access-Control-Allow-Headers': 'Content-Type, Authorization',\n    },\n  })\n}\n\nexport async function GET() {\n  return NextResponse.json(\n    { data: 'value' },\n    {\n      headers: {\n        'Access-Control-Allow-Origin': '*',\n      },\n    }\n  )\n}\n```\n\n## Caching\n\n### Static (Default for GET)\n\n```tsx\n// Cached by default\nexport async function GET() {\n  const data = await fetch('https://api.example.com/data')\n  return NextResponse.json(await data.json())\n}\n```\n\n### Opt-out of Caching\n\n```tsx\nexport const dynamic = 'force-dynamic'\n\nexport async function GET() {\n  // Always fresh\n}\n\n// Or use cookies/headers (auto opts out)\nimport { cookies } from 'next/headers'\n\nexport async function GET() {\n  const cookieStore = await cookies()\n  // Now dynamic\n}\n```\n\n## Error Handling\n\n```tsx\nexport async function GET(request: Request) {\n  try {\n    const data = await riskyOperation()\n    return NextResponse.json(data)\n  } catch (error) {\n    console.error('API Error:', error)\n\n    if (error instanceof ValidationError) {\n      return NextResponse.json(\n        { error: error.message },\n        { status: 400 }\n      )\n    }\n\n    return NextResponse.json(\n      { error: 'Internal Server Error' },\n      { status: 500 }\n    )\n  }\n}\n```\n\n## Resources\n\nFor detailed patterns, see:\n- `references/http-methods.md` - Complete HTTP method guide\n- `references/streaming-responses.md` - Advanced streaming patterns\n- `examples/crud-api.md` - Full CRUD API example"
              },
              {
                "name": "server-actions",
                "description": "This skill should be used when the user asks about \"Server Actions\", \"form handling in Next.js\", \"mutations\", \"useFormState\", \"useFormStatus\", \"revalidatePath\", \"revalidateTag\", or needs guidance on data mutations and form submissions in Next.js App Router.",
                "path": "plugins/nextjs-expert/skills/server-actions/SKILL.md",
                "frontmatter": {
                  "name": "server-actions",
                  "description": "This skill should be used when the user asks about \"Server Actions\", \"form handling in Next.js\", \"mutations\", \"useFormState\", \"useFormStatus\", \"revalidatePath\", \"revalidateTag\", or needs guidance on data mutations and form submissions in Next.js App Router.",
                  "version": "1.0.0"
                },
                "content": "# Next.js Server Actions\n\n## Overview\n\nServer Actions are asynchronous functions that execute on the server. They can be called from Client and Server Components for data mutations, form submissions, and other server-side operations.\n\n## Defining Server Actions\n\n### In Server Components\n\nUse the `'use server'` directive inside an async function:\n\n```tsx\n// app/page.tsx (Server Component)\nexport default function Page() {\n  async function createPost(formData: FormData) {\n    'use server'\n    const title = formData.get('title') as string\n    await db.post.create({ data: { title } })\n  }\n\n  return (\n    <form action={createPost}>\n      <input name=\"title\" />\n      <button type=\"submit\">Create</button>\n    </form>\n  )\n}\n```\n\n### In Separate Files\n\nMark the entire file with `'use server'`:\n\n```tsx\n// app/actions.ts\n'use server'\n\nexport async function createPost(formData: FormData) {\n  const title = formData.get('title') as string\n  await db.post.create({ data: { title } })\n}\n\nexport async function deletePost(id: string) {\n  await db.post.delete({ where: { id } })\n}\n```\n\n## Form Handling\n\n### Basic Form\n\n```tsx\n// app/actions.ts\n'use server'\n\nexport async function submitContact(formData: FormData) {\n  const name = formData.get('name') as string\n  const email = formData.get('email') as string\n  const message = formData.get('message') as string\n\n  await db.contact.create({\n    data: { name, email, message }\n  })\n}\n\n// app/contact/page.tsx\nimport { submitContact } from '@/app/actions'\n\nexport default function ContactPage() {\n  return (\n    <form action={submitContact}>\n      <input name=\"name\" required />\n      <input name=\"email\" type=\"email\" required />\n      <textarea name=\"message\" required />\n      <button type=\"submit\">Send</button>\n    </form>\n  )\n}\n```\n\n### With Validation (Zod)\n\n```tsx\n// app/actions.ts\n'use server'\n\nimport { z } from 'zod'\n\nconst schema = z.object({\n  email: z.string().email(),\n  password: z.string().min(8),\n})\n\nexport async function signup(formData: FormData) {\n  const parsed = schema.safeParse({\n    email: formData.get('email'),\n    password: formData.get('password'),\n  })\n\n  if (!parsed.success) {\n    return { error: parsed.error.flatten() }\n  }\n\n  await createUser(parsed.data)\n  return { success: true }\n}\n```\n\n## useFormState Hook\n\nHandle form state and errors:\n\n```tsx\n// app/signup/page.tsx\n'use client'\n\nimport { useFormState } from 'react-dom'\nimport { signup } from '@/app/actions'\n\nconst initialState = {\n  error: null,\n  success: false,\n}\n\nexport default function SignupPage() {\n  const [state, formAction] = useFormState(signup, initialState)\n\n  return (\n    <form action={formAction}>\n      <input name=\"email\" type=\"email\" />\n      <input name=\"password\" type=\"password\" />\n      {state.error && (\n        <p className=\"text-red-500\">{state.error}</p>\n      )}\n      <button type=\"submit\">Sign Up</button>\n    </form>\n  )\n}\n\n// app/actions.ts\n'use server'\n\nexport async function signup(prevState: any, formData: FormData) {\n  const email = formData.get('email') as string\n\n  if (!email.includes('@')) {\n    return { error: 'Invalid email', success: false }\n  }\n\n  await createUser({ email })\n  return { error: null, success: true }\n}\n```\n\n## useFormStatus Hook\n\nShow loading states during submission:\n\n```tsx\n// components/submit-button.tsx\n'use client'\n\nimport { useFormStatus } from 'react-dom'\n\nexport function SubmitButton() {\n  const { pending } = useFormStatus()\n\n  return (\n    <button type=\"submit\" disabled={pending}>\n      {pending ? 'Submitting...' : 'Submit'}\n    </button>\n  )\n}\n\n// Usage in form\nimport { SubmitButton } from '@/components/submit-button'\n\nexport default function Form() {\n  return (\n    <form action={submitAction}>\n      <input name=\"title\" />\n      <SubmitButton />\n    </form>\n  )\n}\n```\n\n## Revalidation\n\n### revalidatePath\n\nRevalidate a specific path:\n\n```tsx\n'use server'\n\nimport { revalidatePath } from 'next/cache'\n\nexport async function createPost(formData: FormData) {\n  await db.post.create({ data: { ... } })\n\n  // Revalidate the posts list page\n  revalidatePath('/posts')\n\n  // Revalidate a dynamic route\n  revalidatePath('/posts/[slug]', 'page')\n\n  // Revalidate all paths under /posts\n  revalidatePath('/posts', 'layout')\n}\n```\n\n### revalidateTag\n\nRevalidate by cache tag:\n\n```tsx\n// Fetching with tags\nconst posts = await fetch('https://api.example.com/posts', {\n  next: { tags: ['posts'] }\n})\n\n// Server Action\n'use server'\n\nimport { revalidateTag } from 'next/cache'\n\nexport async function createPost(formData: FormData) {\n  await db.post.create({ data: { ... } })\n  revalidateTag('posts')\n}\n```\n\n## Redirects After Actions\n\n```tsx\n'use server'\n\nimport { redirect } from 'next/navigation'\n\nexport async function createPost(formData: FormData) {\n  const post = await db.post.create({ data: { ... } })\n\n  // Redirect to the new post\n  redirect(`/posts/${post.slug}`)\n}\n```\n\n## Optimistic Updates\n\nUpdate UI immediately while action completes:\n\n```tsx\n'use client'\n\nimport { useOptimistic } from 'react'\nimport { addTodo } from '@/app/actions'\n\nexport function TodoList({ todos }: { todos: Todo[] }) {\n  const [optimisticTodos, addOptimisticTodo] = useOptimistic(\n    todos,\n    (state, newTodo: string) => [\n      ...state,\n      { id: 'temp', title: newTodo, completed: false }\n    ]\n  )\n\n  async function handleSubmit(formData: FormData) {\n    const title = formData.get('title') as string\n    addOptimisticTodo(title) // Update UI immediately\n    await addTodo(formData)  // Server action\n  }\n\n  return (\n    <>\n      <form action={handleSubmit}>\n        <input name=\"title\" />\n        <button>Add</button>\n      </form>\n      <ul>\n        {optimisticTodos.map(todo => (\n          <li key={todo.id}>{todo.title}</li>\n        ))}\n      </ul>\n    </>\n  )\n}\n```\n\n## Non-Form Usage\n\nCall Server Actions programmatically:\n\n```tsx\n'use client'\n\nimport { deletePost } from '@/app/actions'\n\nexport function DeleteButton({ id }: { id: string }) {\n  return (\n    <button onClick={() => deletePost(id)}>\n      Delete\n    </button>\n  )\n}\n```\n\n## Error Handling\n\n```tsx\n'use server'\n\nexport async function createPost(formData: FormData) {\n  try {\n    await db.post.create({ data: { ... } })\n    return { success: true }\n  } catch (error) {\n    if (error instanceof PrismaClientKnownRequestError) {\n      if (error.code === 'P2002') {\n        return { error: 'A post with this title already exists' }\n      }\n    }\n    return { error: 'Failed to create post' }\n  }\n}\n```\n\n## Security Considerations\n\n1. **Always validate input** - Never trust client data\n2. **Check authentication** - Verify user is authorized\n3. **Use CSRF protection** - Built-in with Server Actions\n4. **Sanitize output** - Prevent XSS attacks\n\n```tsx\n'use server'\n\nimport { auth } from '@/lib/auth'\n\nexport async function deletePost(id: string) {\n  const session = await auth()\n\n  if (!session) {\n    throw new Error('Unauthorized')\n  }\n\n  const post = await db.post.findUnique({ where: { id } })\n\n  if (post.authorId !== session.user.id) {\n    throw new Error('Forbidden')\n  }\n\n  await db.post.delete({ where: { id } })\n}\n```\n\n## Resources\n\nFor detailed patterns, see:\n- `references/form-handling.md` - Advanced form patterns\n- `references/revalidation.md` - Cache revalidation strategies\n- `examples/mutation-patterns.md` - Complete mutation examples"
              },
              {
                "name": "server-components",
                "description": "This skill should be used when the user asks about \"Server Components\", \"Client Components\", \"'use client' directive\", \"when to use server vs client\", \"RSC patterns\", \"component composition\", \"data fetching in components\", or needs guidance on React Server Components architecture in Next.js.",
                "path": "plugins/nextjs-expert/skills/server-components/SKILL.md",
                "frontmatter": {
                  "name": "server-components",
                  "description": "This skill should be used when the user asks about \"Server Components\", \"Client Components\", \"'use client' directive\", \"when to use server vs client\", \"RSC patterns\", \"component composition\", \"data fetching in components\", or needs guidance on React Server Components architecture in Next.js.",
                  "version": "1.0.0"
                },
                "content": "# React Server Components in Next.js\n\n## Overview\n\nReact Server Components (RSC) allow components to render on the server, reducing client-side JavaScript and enabling direct data access. In Next.js App Router, all components are Server Components by default.\n\n## Server vs Client Components\n\n### Server Components (Default)\n\nServer Components run only on the server:\n\n```tsx\n// app/users/page.tsx (Server Component - default)\nasync function UsersPage() {\n  const users = await db.user.findMany() // Direct DB access\n\n  return (\n    <ul>\n      {users.map(user => (\n        <li key={user.id}>{user.name}</li>\n      ))}\n    </ul>\n  )\n}\n```\n\n**Benefits:**\n- Direct database/filesystem access\n- Keep sensitive data on server (API keys, tokens)\n- Reduce client bundle size\n- Automatic code splitting\n\n### Client Components\n\nAdd `'use client'` directive for interactivity:\n\n```tsx\n// components/counter.tsx\n'use client'\n\nimport { useState } from 'react'\n\nexport function Counter() {\n  const [count, setCount] = useState(0)\n\n  return (\n    <button onClick={() => setCount(count + 1)}>\n      Count: {count}\n    </button>\n  )\n}\n```\n\n**Use Client Components for:**\n- `useState`, `useEffect`, `useReducer`\n- Event handlers (`onClick`, `onChange`)\n- Browser APIs (`window`, `document`)\n- Custom hooks with state\n\n## The Mental Model\n\nThink of the component tree as having a \"client boundary\":\n\n```\nServer Component (page.tsx)\n Server Component (header.tsx)\n Client Component ('use client')  boundary\n    Client Component (child)\n    Client Component (child)\n Server Component (footer.tsx)\n```\n\n**Key rules:**\n1. Server Components can import Client Components\n2. Client Components cannot import Server Components\n3. You can pass Server Components as `children` to Client Components\n\n## Composition Patterns\n\n### Pattern 1: Server Data  Client Interactivity\n\nFetch data in Server Component, pass to Client:\n\n```tsx\n// app/products/page.tsx (Server)\nimport { ProductList } from './product-list'\n\nexport default async function ProductsPage() {\n  const products = await getProducts()\n  return <ProductList products={products} />\n}\n\n// app/products/product-list.tsx (Client)\n'use client'\n\nexport function ProductList({ products }: { products: Product[] }) {\n  const [filter, setFilter] = useState('')\n\n  const filtered = products.filter(p =>\n    p.name.includes(filter)\n  )\n\n  return (\n    <>\n      <input onChange={e => setFilter(e.target.value)} />\n      {filtered.map(p => <ProductCard key={p.id} product={p} />)}\n    </>\n  )\n}\n```\n\n### Pattern 2: Children as Server Components\n\nPass Server Components through children prop:\n\n```tsx\n// components/client-wrapper.tsx\n'use client'\n\nexport function ClientWrapper({ children }: { children: React.ReactNode }) {\n  const [isOpen, setIsOpen] = useState(false)\n\n  return (\n    <div>\n      <button onClick={() => setIsOpen(!isOpen)}>Toggle</button>\n      {isOpen && children} {/* Server Component content */}\n    </div>\n  )\n}\n\n// app/page.tsx (Server)\nimport { ClientWrapper } from '@/components/client-wrapper'\nimport { ServerContent } from '@/components/server-content'\n\nexport default function Page() {\n  return (\n    <ClientWrapper>\n      <ServerContent /> {/* Renders on server! */}\n    </ClientWrapper>\n  )\n}\n```\n\n### Pattern 3: Slots for Complex Layouts\n\nUse multiple children slots:\n\n```tsx\n// components/dashboard-shell.tsx\n'use client'\n\ninterface Props {\n  sidebar: React.ReactNode\n  main: React.ReactNode\n}\n\nexport function DashboardShell({ sidebar, main }: Props) {\n  const [collapsed, setCollapsed] = useState(false)\n\n  return (\n    <div className=\"flex\">\n      {!collapsed && <aside>{sidebar}</aside>}\n      <main>{main}</main>\n    </div>\n  )\n}\n```\n\n## Data Fetching\n\n### Async Server Components\n\nServer Components can be async:\n\n```tsx\n// app/posts/page.tsx\nexport default async function PostsPage() {\n  const posts = await fetch('https://api.example.com/posts')\n    .then(res => res.json())\n\n  return (\n    <ul>\n      {posts.map(post => (\n        <li key={post.id}>{post.title}</li>\n      ))}\n    </ul>\n  )\n}\n```\n\n### Parallel Data Fetching\n\nFetch multiple resources in parallel:\n\n```tsx\nexport default async function DashboardPage() {\n  const [user, posts, analytics] = await Promise.all([\n    getUser(),\n    getPosts(),\n    getAnalytics(),\n  ])\n\n  return (\n    <Dashboard user={user} posts={posts} analytics={analytics} />\n  )\n}\n```\n\n### Streaming with Suspense\n\nStream slow components:\n\n```tsx\nimport { Suspense } from 'react'\n\nexport default function Page() {\n  return (\n    <div>\n      <Header /> {/* Renders immediately */}\n      <Suspense fallback={<PostsSkeleton />}>\n        <SlowPosts /> {/* Streams when ready */}\n      </Suspense>\n    </div>\n  )\n}\n```\n\n## Decision Guide\n\n**Use Server Component when:**\n- Fetching data\n- Accessing backend resources\n- Keeping sensitive info on server\n- Reducing client JavaScript\n- Component has no interactivity\n\n**Use Client Component when:**\n- Using state (`useState`, `useReducer`)\n- Using effects (`useEffect`)\n- Using event listeners\n- Using browser APIs\n- Using custom hooks with state\n\n## Common Mistakes\n\n1. **Don't** add `'use client'` unnecessarily - it increases bundle size\n2. **Don't** try to import Server Components into Client Components\n3. **Do** serialize data at boundaries (no functions, classes, or dates)\n4. **Do** use the children pattern for composition\n\n## Resources\n\nFor detailed patterns, see:\n- `references/server-vs-client.md` - Complete comparison guide\n- `references/composition-patterns.md` - Advanced composition\n- `examples/data-fetching-patterns.md` - Data fetching examples"
              }
            ]
          },
          {
            "name": "frontend-design-pro",
            "description": "Advanced frontend design plugin with interactive wizard, trend research, moodboard creation, browser-based inspiration analysis, color/typography selection, and WCAG accessibility",
            "source": "./plugins/frontend-design-pro",
            "category": "skills",
            "version": "1.0.0",
            "author": {
              "name": "BuildWithClaude Community",
              "url": "https://github.com/davepoon/buildwithclaude"
            },
            "install_commands": [
              "/plugin marketplace add davepoon/buildwithclaude",
              "/plugin install frontend-design-pro@buildwithclaude"
            ],
            "signals": {
              "stars": 2229,
              "forks": 239,
              "pushed_at": "2026-01-12T10:19:01Z",
              "created_at": "2025-07-25T02:26:45Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/analyze-site",
                "description": "Analyze a website for design inspiration (colors, fonts, patterns)",
                "path": "plugins/frontend-design-pro/commands/analyze-site.md",
                "frontmatter": {
                  "description": "Analyze a website for design inspiration (colors, fonts, patterns)",
                  "allowed-tools": [
                    "Skill",
                    "AskUserQuestion",
                    "mcp__claude-in-chrome__tabs_context_mcp",
                    "mcp__claude-in-chrome__tabs_create_mcp",
                    "mcp__claude-in-chrome__navigate",
                    "mcp__claude-in-chrome__computer",
                    "mcp__claude-in-chrome__read_page",
                    "mcp__claude-in-chrome__get_page_text",
                    "mcp__claude-in-chrome__find",
                    "mcp__claude-in-chrome__resize_window"
                  ]
                },
                "content": "# Analyze Website for Inspiration\n\nYou are analyzing a website to extract design inspiration.\n\n## Usage\n\n```\n/frontend-design-pro:analyze-site [URL]\n```\n\nExample:\n```\n/frontend-design-pro:analyze-site https://linear.app\n```\n\n## Workflow\n\n### Step 1: Get URL\n\nIf URL not provided as argument, ask:\n\n\"What website would you like me to analyze for design inspiration?\"\n\n### Step 2: Browser Setup\n\n```javascript\n// Get browser context\ntabs_context_mcp({ createIfEmpty: true })\ntabs_create_mcp()\n```\n\n### Step 3: Navigate and Capture\n\n```javascript\n// Navigate to the URL\nnavigate({ url: \"[URL]\", tabId: tabId })\n\n// Wait for load, then screenshot\ncomputer({ action: \"screenshot\", tabId: tabId })\n```\n\n### Step 4: Capture Multiple Views\n\n**Desktop view (default):**\n- Above-fold hero section\n- Scroll and capture 2-3 more sections\n- Capture navigation hover states if possible\n\n**Mobile view:**\n```javascript\nresize_window({ width: 375, height: 812, tabId: tabId })\ncomputer({ action: \"screenshot\", tabId: tabId })\n```\n\n### Step 5: Analyze Elements\n\nFrom screenshots, identify:\n\n**Colors:**\n- Primary brand color\n- Background color(s)\n- Text colors\n- Accent colors\n- Note approximate hex codes\n\n**Typography:**\n- Heading font (style, weight)\n- Body font (style, weight)\n- Size relationships\n- Line height observations\n\n**Layout:**\n- Grid structure\n- Section patterns\n- White space usage\n- Container widths\n\n**UI Components:**\n- Button styles\n- Card treatments\n- Navigation style\n- Footer structure\n- Any distinctive elements\n\n**Motion/Interaction:**\n- Hover effects observed\n- Scroll animations\n- Transitions\n\n### Step 6: Generate Report\n\nCreate a structured analysis:\n\n```markdown\n## Website Analysis: [URL]\n\n### Overview\n[Brief description of the site and its design approach]\n\n### Color Palette\n| Role | Hex (Approx) | Usage |\n|------|--------------|-------|\n| Primary | #xxx | [Where used] |\n| Background | #xxx | [Where used] |\n| Text | #xxx | [Where used] |\n| Accent | #xxx | [Where used] |\n\n### Typography\n- **Headlines**: [Font/style] at [size], [weight]\n- **Body**: [Font/style] at [size], [weight]\n- **Line height**: [Observation]\n- **Letter spacing**: [Observation]\n\n### Layout Patterns\n- **Grid**: [Description]\n- **Container**: [Max width observation]\n- **Sections**: [How sections are structured]\n- **White space**: [Philosophy]\n\n### UI Elements\n- **Buttons**: [Shape, style, states]\n- **Cards**: [Treatment]\n- **Navigation**: [Style]\n- **Icons**: [Style if present]\n\n### Distinctive Features\n1. [What makes this design unique]\n2. [Interesting pattern to note]\n3. [Technique worth replicating]\n\n### Key Takeaways\nWhat to borrow from this design:\n- [Takeaway 1]\n- [Takeaway 2]\n- [Takeaway 3]\n\n### What to Avoid\n- [Any overused patterns to skip]\n```\n\n## Fallback Mode\n\nIf browser tools are unavailable:\n\n\"I can't access the website directly. Could you:\n1. Share a screenshot of the site\n2. Describe what you like about the design\n3. Note any visible font names or colors\n\nI'll analyze whatever you can provide.\"\n\n## Multiple Sites\n\nIf user provides multiple URLs:\n1. Analyze each separately\n2. Create individual reports\n3. Summarize common themes\n4. Note contrasting approaches\n5. Recommend which elements to combine\n\n## Output\n\nThe analysis should provide actionable insights:\n- Specific hex codes to use\n- Font names to search\n- Layout patterns to replicate\n- Techniques to try\n- Clear direction for implementation"
              },
              {
                "name": "/design",
                "description": "Interactive design wizard with trend research, moodboard creation, color/font selection, and code generation",
                "path": "plugins/frontend-design-pro/commands/design.md",
                "frontmatter": {
                  "description": "Interactive design wizard with trend research, moodboard creation, color/font selection, and code generation",
                  "allowed-tools": [
                    "Read",
                    "Write",
                    "Glob",
                    "Grep",
                    "AskUserQuestion",
                    "Skill",
                    "mcp__claude-in-chrome__tabs_context_mcp",
                    "mcp__claude-in-chrome__tabs_create_mcp",
                    "mcp__claude-in-chrome__navigate",
                    "mcp__claude-in-chrome__computer",
                    "mcp__claude-in-chrome__read_page",
                    "mcp__claude-in-chrome__get_page_text",
                    "mcp__claude-in-chrome__find"
                  ]
                },
                "content": "# Interactive Design Wizard\n\nYou are guiding the user through a complete frontend design process.\n\n## Overview\n\nThis is a comprehensive design workflow that includes:\n1. **Discovery** - Understanding what to build\n2. **Research** - Analyzing trends and inspiration\n3. **Moodboard** - Creating and refining direction\n4. **Selection** - Choosing colors and typography\n5. **Implementation** - Generating production-ready code\n6. **Review** - Validating against quality standards\n\n## Workflow\n\n### Phase 1: Discovery\n\nAsk the user about their project using AskUserQuestion:\n\n**Question 1: What are you building?**\n- Landing page\n- Dashboard\n- Blog/Content site\n- E-commerce\n- Portfolio\n- SaaS application\n- Other\n\n**Question 2: Project context**\n- Personal project\n- Startup/new product\n- Established brand\n- Client work\n- Redesign\n\n**Question 3: Target audience**\n- Developers/technical\n- Business professionals\n- Creative/designers\n- General consumers\n- Young/Gen-Z\n- Luxury/premium\n\n**Question 4: Background style**\n- Pure white (#ffffff)\n- Off-white/warm (#faf8f5)\n- Light tinted (from palette)\n- Dark/moody\n- Let me decide\n\n**Question 5: Inspiration**\n- Provide URLs to analyze\n- Aesthetic keywords\n- Research trends first\n- Skip, use defaults\n\n### Phase 2: Research (Optional)\n\nBased on discovery answers, optionally run:\n\n**If user wants trend research:**\nUse the `trend-researcher` skill to:\n- Visit Dribbble trending shots\n- Analyze current design patterns\n- Identify color and typography trends\n- Create a trend report\n\n**If user provided URLs:**\nUse the `inspiration-analyzer` skill to:\n- Visit each provided URL\n- Screenshot and analyze\n- Extract colors, fonts, patterns\n- Document key takeaways\n\n### Phase 3: Moodboard\n\nUse the `moodboard-creator` skill to:\n- Synthesize research findings\n- Present color direction\n- Present typography direction\n- List UI patterns to incorporate\n- Define mood keywords\n\n**Iteration:**\n- Present moodboard to user\n- Get feedback\n- Refine until approved\n- Maximum 3 iterations\n\n### Phase 4: Color Selection\n\nUse the `color-curator` skill to:\n\n**With browser:**\n- Navigate to Coolors trending palettes\n- Present options to user\n- Let user choose palette\n- Extract hex codes\n\n**Without browser:**\n- Present curated palettes matching aesthetic\n- Let user choose or specify manually\n- Document selected colors\n\nMap colors to design roles:\n- Primary (CTAs, brand)\n- Background (page)\n- Surface (cards)\n- Text (heading, body, muted)\n- Accent (highlights)\n\n### Phase 5: Typography Selection\n\nUse the `typography-selector` skill to:\n\n**With browser:**\n- Navigate to Google Fonts\n- Present trending/matching fonts\n- Let user choose\n\n**Without browser:**\n- Present curated pairings\n- Let user choose or specify\n\nGenerate:\n- Google Fonts import code\n- Tailwind font config\n- Usage examples\n\n### Phase 6: Implementation\n\nGenerate a complete HTML file with:\n\n```html\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n  <meta charset=\"UTF-8\">\n  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n  <title>[Project Title]</title>\n\n  <!-- Google Fonts -->\n  [Font imports]\n\n  <!-- Tailwind CDN -->\n  <script src=\"https://cdn.tailwindcss.com\"></script>\n  <script>\n    tailwind.config = {\n      theme: {\n        extend: {\n          colors: { /* Custom colors */ },\n          fontFamily: { /* Custom fonts */ }\n        }\n      }\n    }\n  </script>\n\n  <style>\n    /* Custom animations with prefers-reduced-motion */\n    /* Focus states */\n  </style>\n</head>\n<body>\n  <!-- Accessible, semantic HTML -->\n  <!-- Skip link -->\n  <!-- Header/Nav -->\n  <!-- Main content -->\n  <!-- Footer -->\n</body>\n</html>\n```\n\n**Requirements:**\n- Mobile-responsive\n- Semantic HTML (header, main, nav, footer)\n- Accessible (ARIA, focus states, contrast)\n- No Lorem ipsum (realistic content)\n- Respect prefers-reduced-motion\n- Keyboard navigable\n\n### Phase 7: Self-Review\n\nBefore delivering, check:\n\n**Anti-patterns (must not have):**\n- [ ] No hero badges/pills above headlines\n- [ ] No generic fonts (Inter, Roboto, Arial)\n- [ ] No purple-blue gradients on white\n- [ ] No decorative blob shapes\n- [ ] No excessive rounded corners everywhere\n- [ ] No predictable template layout\n\n**Design principles (must have):**\n- [ ] Clear visual hierarchy\n- [ ] Proper alignment\n- [ ] Sufficient contrast (4.5:1+)\n- [ ] Generous white space\n- [ ] Consistent spacing\n\n**Accessibility (must have):**\n- [ ] Skip link present\n- [ ] Semantic headings (h1  h2  h3)\n- [ ] Visible focus states\n- [ ] Alt text for images\n- [ ] prefers-reduced-motion respected\n\n### Phase 8: Delivery\n\nPresent the final design with:\n1. The complete HTML file\n2. Summary of design decisions\n3. Color palette reference\n4. Typography reference\n5. Any notes on customization\n\n## Iteration\n\nIf user requests changes:\n1. Note specific feedback\n2. Make targeted adjustments\n3. Re-run self-review\n4. Present updated version\n\nSupport up to 3 major iterations.\n\n## Tips\n\n- Keep the user informed at each phase\n- Explain design decisions\n- Offer alternatives when appropriate\n- Be opinionated but flexible\n- Focus on distinctive, quality output"
              },
              {
                "name": "/review",
                "description": "Review generated design against anti-patterns, design principles, and accessibility guidelines",
                "path": "plugins/frontend-design-pro/commands/review.md",
                "frontmatter": {
                  "description": "Review generated design against anti-patterns, design principles, and accessibility guidelines",
                  "allowed-tools": [
                    "Read",
                    "Glob",
                    "Grep",
                    "AskUserQuestion",
                    "Skill"
                  ]
                },
                "content": "# Design Review\n\nYou are reviewing a generated design for quality, anti-patterns, and accessibility.\n\n## Usage\n\n```\n/frontend-design-pro:review [file-path]\n```\n\nExample:\n```\n/frontend-design-pro:review ./landing-page.html\n```\n\n## Workflow\n\n### Step 1: Get File\n\nIf no file path provided, ask:\n\n\"Which HTML file would you like me to review?\"\n\nOr search for recent HTML files:\n```\nGlob: **/*.html\n```\n\n### Step 2: Read the File\n\n```\nRead: [file-path]\n```\n\n### Step 3: Anti-Pattern Check\n\nSearch for common anti-patterns:\n\n**Hero Badges/Pills:**\n```\nGrep for patterns like:\n- \"rounded-full.*px-.*text-sm\" near headlines\n- Badge/pill components above h1\n- Words like \"New\", \"AI-Powered\", \"Introducing\", \"Beta\" in small elements\n```\n\n**Generic Fonts:**\n```\nCheck for:\n- font-family.*Inter\n- font-family.*Roboto\n- font-family.*Arial\n- font-family.*Open.Sans\n- font-sans without custom config\n```\n\n**Purple/Blue Gradients on White:**\n```\nCheck for:\n- bg-gradient.*purple.*blue on bg-white\n- from-purple.*to-blue\n- from-violet.*to-indigo\n```\n\n**Decorative Blobs:**\n```\nCheck for:\n- blur-3xl or blur-[100px] on colored divs\n- \"blob\" in class names\n- Large rounded-full with bg-{color}-200/300\n```\n\n**Excessive Rounded Corners:**\n```\nCheck if rounded-3xl or rounded-full used everywhere\n```\n\n### Step 4: Design Principles Check\n\n**Visual Hierarchy:**\n- Is there clear size difference between h1, h2, h3?\n- Are CTAs visually distinct?\n- Is there one focal point per section?\n\n**Alignment:**\n- Is alignment consistent within sections?\n- Are elements aligned to a grid?\n\n**Contrast:**\n- Text on background ratio (check color values)\n- CTA stands out from surroundings?\n\n**White Space:**\n- Adequate padding on sections (p-8+)?\n- Max-width on content containers?\n- Breathing room between elements?\n\n**Consistency:**\n- Same button styles throughout?\n- Same card treatments?\n- Consistent spacing scale?\n\n### Step 5: Accessibility Check\n\n**Structure:**\n```\nCheck for:\n- <header>, <main>, <nav>, <footer>\n- Skip link (a href=\"#main-content\" or similar)\n- lang=\"en\" on <html>\n```\n\n**Headings:**\n```\nVerify:\n- Exactly one <h1>\n- Sequential heading levels (h1  h2  h3)\n- No skipped levels\n```\n\n**Focus States:**\n```\nCheck for:\n- focus:ring or focus:outline classes\n- No focus:outline-none without replacement\n```\n\n**Images:**\n```\nCheck for:\n- alt=\"\" on decorative images\n- Descriptive alt text on meaningful images\n```\n\n**Reduced Motion:**\n```\nCheck for:\n- @media (prefers-reduced-motion)\n- motion-reduce: classes\n```\n\n**Color Contrast:**\n```\nAnalyze:\n- Text color vs background color\n- Estimate contrast ratio\n- Flag any obvious low-contrast text\n```\n\n### Step 6: Generate Report\n\nCreate a comprehensive review:\n\n```markdown\n## Design Review Report\n\n### File: [path]\n\n---\n\n## Anti-Pattern Check\n\n| Pattern | Status | Details |\n|---------|--------|---------|\n| Hero badges |  Pass /  Found | [Details] |\n| Generic fonts |  Pass /  Found | [Details] |\n| Purple-blue gradient |  Pass /  Found | [Details] |\n| Decorative blobs |  Pass /  Found | [Details] |\n| Excessive rounding |  Pass /  Found | [Details] |\n| Template layout |  Pass /  Found | [Details] |\n\n---\n\n## Design Principles Check\n\n| Principle | Status | Notes |\n|-----------|--------|-------|\n| Visual hierarchy |  /  /  | [Notes] |\n| Alignment |  /  /  | [Notes] |\n| Contrast |  /  /  | [Notes] |\n| White space |  /  /  | [Notes] |\n| Consistency |  /  /  | [Notes] |\n\n---\n\n## Accessibility Check\n\n| Requirement | Status | Notes |\n|-------------|--------|-------|\n| Semantic HTML |  /  | [Notes] |\n| Skip link |  /  | [Notes] |\n| Heading order |  /  | [Notes] |\n| Focus states |  /  | [Notes] |\n| Image alt text |  /  | [Notes] |\n| Reduced motion |  /  | [Notes] |\n| Color contrast |  /  /  | [Notes] |\n\n---\n\n## Summary\n\n**Score: [X]/[Total] checks passed**\n\n### Critical Issues\n[List any blockers that must be fixed]\n\n### Recommended Improvements\n[List nice-to-have improvements]\n\n### Positive Notes\n[What the design does well]\n```\n\n### Step 7: Offer Fixes\n\nIf issues found, offer to fix them:\n\n\"I found [N] issues in your design. Would you like me to:\n1. Fix all issues automatically\n2. Fix critical issues only\n3. Provide guidance for manual fixes\n4. Skip fixes for now\"\n\n## Quick Mode\n\nFor a fast check, focus on:\n1. Hero badges (the #1 AI slop indicator)\n2. Generic fonts\n3. Accessibility basics (skip link, headings)\n\n## Output\n\nThe review provides:\n- Clear pass/fail for each check\n- Specific line numbers or locations\n- Actionable fixes\n- Overall quality score"
              }
            ],
            "skills": []
          },
          {
            "name": "obsidian-skills",
            "description": "Skills for working with Obsidian files including Markdown with wikilinks/embeds/callouts, Bases for database views, and Canvas for visual diagrams",
            "source": "./plugins/obsidian-skills",
            "category": "skills",
            "version": "1.0.0",
            "author": {
              "name": "kepano",
              "url": "https://github.com/kepano/obsidian-skills"
            },
            "install_commands": [
              "/plugin marketplace add davepoon/buildwithclaude",
              "/plugin install obsidian-skills@buildwithclaude"
            ],
            "signals": {
              "stars": 2229,
              "forks": 239,
              "pushed_at": "2026-01-12T10:19:01Z",
              "created_at": "2025-07-25T02:26:45Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": [
              {
                "name": "json-canvas",
                "description": "Create and edit JSON Canvas files (.canvas) with nodes, edges, groups, and connections. Use when working with .canvas files, creating visual canvases, mind maps, flowcharts, or when the user mentions Canvas files in Obsidian.",
                "path": "plugins/obsidian-skills/skills/json-canvas/SKILL.md",
                "frontmatter": {
                  "name": "json-canvas",
                  "category": "document-processing",
                  "description": "Create and edit JSON Canvas files (.canvas) with nodes, edges, groups, and connections. Use when working with .canvas files, creating visual canvases, mind maps, flowcharts, or when the user mentions Canvas files in Obsidian."
                },
                "content": "# JSON Canvas\n\nThis skill enables Claude Code to create and edit valid JSON Canvas files (`.canvas`) used in Obsidian and other applications.\n\n## Overview\n\nJSON Canvas is an open file format for infinite canvas data. Canvas files use the `.canvas` extension and contain valid JSON following the JSON Canvas Spec 1.0.\n\n## When to Use This Skill\n\n- Creating or editing .canvas files in Obsidian\n- Building visual mind maps or flowcharts\n- Creating project boards or planning documents\n- Organizing notes visually with connections\n- Building diagrams with linked content\n\n## File Structure\n\nA canvas file contains two top-level arrays:\n\n```json\n{\n  \"nodes\": [],\n  \"edges\": []\n}\n```\n\n- `nodes` (optional): Array of node objects\n- `edges` (optional): Array of edge objects connecting nodes\n\n## Nodes\n\nNodes are objects placed on the canvas. There are four node types:\n- `text` - Text content with Markdown\n- `file` - Reference to files/attachments\n- `link` - External URL\n- `group` - Visual container for other nodes\n\n### Z-Index Ordering\n\nFirst node = bottom layer (displayed below others)\nLast node = top layer (displayed above others)\n\n### Generic Node Attributes\n\n| Attribute | Required | Type | Description |\n|-----------|----------|------|-------------|\n| `id` | Yes | string | Unique identifier for the node |\n| `type` | Yes | string | Node type: `text`, `file`, `link`, or `group` |\n| `x` | Yes | integer | X position in pixels |\n| `y` | Yes | integer | Y position in pixels |\n| `width` | Yes | integer | Width in pixels |\n| `height` | Yes | integer | Height in pixels |\n| `color` | No | canvasColor | Node color (see Color section) |\n\n### Text Nodes\n\nText nodes contain Markdown content.\n\n```json\n{\n  \"id\": \"text1\",\n  \"type\": \"text\",\n  \"x\": 0,\n  \"y\": 0,\n  \"width\": 300,\n  \"height\": 150,\n  \"text\": \"# Heading\\n\\nThis is **markdown** content.\"\n}\n```\n\n### File Nodes\n\nFile nodes reference files or attachments (images, videos, PDFs, notes, etc.)\n\n| Attribute | Required | Type | Description |\n|-----------|----------|------|-------------|\n| `file` | Yes | string | Path to file within the system |\n| `subpath` | No | string | Link to heading or block (starts with `#`) |\n\n```json\n{\n  \"id\": \"file1\",\n  \"type\": \"file\",\n  \"x\": 350,\n  \"y\": 0,\n  \"width\": 400,\n  \"height\": 300,\n  \"file\": \"Notes/My Note.md\",\n  \"subpath\": \"#Heading\"\n}\n```\n\n### Link Nodes\n\nLink nodes display external URLs.\n\n```json\n{\n  \"id\": \"link1\",\n  \"type\": \"link\",\n  \"x\": 0,\n  \"y\": 200,\n  \"width\": 300,\n  \"height\": 150,\n  \"url\": \"https://example.com\"\n}\n```\n\n### Group Nodes\n\nGroup nodes are visual containers for organizing other nodes.\n\n| Attribute | Required | Type | Description |\n|-----------|----------|------|-------------|\n| `label` | No | string | Text label for the group |\n| `background` | No | string | Path to background image |\n| `backgroundStyle` | No | string | Background rendering style |\n\n#### Background Styles\n\n| Value | Description |\n|-------|-------------|\n| `cover` | Fills entire width and height of node |\n| `ratio` | Maintains aspect ratio of background image |\n| `repeat` | Repeats image as pattern in both directions |\n\n```json\n{\n  \"id\": \"group1\",\n  \"type\": \"group\",\n  \"x\": -50,\n  \"y\": -50,\n  \"width\": 800,\n  \"height\": 500,\n  \"label\": \"Project Ideas\",\n  \"color\": \"4\"\n}\n```\n\n## Edges\n\nEdges are lines connecting nodes.\n\n| Attribute | Required | Type | Default | Description |\n|-----------|----------|------|---------|-------------|\n| `id` | Yes | string | - | Unique identifier for the edge |\n| `fromNode` | Yes | string | - | Node ID where connection starts |\n| `fromSide` | No | string | - | Side where edge starts |\n| `fromEnd` | No | string | `none` | Shape at edge start |\n| `toNode` | Yes | string | - | Node ID where connection ends |\n| `toSide` | No | string | - | Side where edge ends |\n| `toEnd` | No | string | `arrow` | Shape at edge end |\n| `color` | No | canvasColor | - | Line color |\n| `label` | No | string | - | Text label for the edge |\n\n### Side Values\n\n| Value | Description |\n|-------|-------------|\n| `top` | Top edge of node |\n| `right` | Right edge of node |\n| `bottom` | Bottom edge of node |\n| `left` | Left edge of node |\n\n### End Shapes\n\n| Value | Description |\n|-------|-------------|\n| `none` | No endpoint shape |\n| `arrow` | Arrow endpoint |\n\n```json\n{\n  \"id\": \"edge1\",\n  \"fromNode\": \"text1\",\n  \"fromSide\": \"right\",\n  \"toNode\": \"file1\",\n  \"toSide\": \"left\",\n  \"toEnd\": \"arrow\",\n  \"label\": \"references\"\n}\n```\n\n## Colors\n\nThe `canvasColor` type supports both hex colors and preset options.\n\n### Hex Colors\n\n```json\n{\n  \"color\": \"#FF0000\"\n}\n```\n\n### Preset Colors\n\n| Preset | Color |\n|--------|-------|\n| `\"1\"` | Red |\n| `\"2\"` | Orange |\n| `\"3\"` | Yellow |\n| `\"4\"` | Green |\n| `\"5\"` | Cyan |\n| `\"6\"` | Purple |\n\nSpecific color values for presets are intentionally undefined, allowing applications to use their own brand colors.\n\n## Complete Examples\n\n### Simple Canvas with Text and Connections\n\n```json\n{\n  \"nodes\": [\n    {\n      \"id\": \"idea1\",\n      \"type\": \"text\",\n      \"x\": 0,\n      \"y\": 0,\n      \"width\": 250,\n      \"height\": 100,\n      \"text\": \"# Main Idea\\n\\nCore concept goes here\"\n    },\n    {\n      \"id\": \"idea2\",\n      \"type\": \"text\",\n      \"x\": 350,\n      \"y\": -50,\n      \"width\": 200,\n      \"height\": 80,\n      \"text\": \"## Supporting Point 1\\n\\nDetails...\"\n    },\n    {\n      \"id\": \"idea3\",\n      \"type\": \"text\",\n      \"x\": 350,\n      \"y\": 100,\n      \"width\": 200,\n      \"height\": 80,\n      \"text\": \"## Supporting Point 2\\n\\nMore details...\"\n    }\n  ],\n  \"edges\": [\n    {\n      \"id\": \"e1\",\n      \"fromNode\": \"idea1\",\n      \"fromSide\": \"right\",\n      \"toNode\": \"idea2\",\n      \"toSide\": \"left\",\n      \"toEnd\": \"arrow\"\n    },\n    {\n      \"id\": \"e2\",\n      \"fromNode\": \"idea1\",\n      \"fromSide\": \"right\",\n      \"toNode\": \"idea3\",\n      \"toSide\": \"left\",\n      \"toEnd\": \"arrow\"\n    }\n  ]\n}\n```\n\n### Project Board with Groups\n\n```json\n{\n  \"nodes\": [\n    {\n      \"id\": \"todo-group\",\n      \"type\": \"group\",\n      \"x\": 0,\n      \"y\": 0,\n      \"width\": 300,\n      \"height\": 400,\n      \"label\": \"To Do\",\n      \"color\": \"1\"\n    },\n    {\n      \"id\": \"progress-group\",\n      \"type\": \"group\",\n      \"x\": 350,\n      \"y\": 0,\n      \"width\": 300,\n      \"height\": 400,\n      \"label\": \"In Progress\",\n      \"color\": \"3\"\n    },\n    {\n      \"id\": \"done-group\",\n      \"type\": \"group\",\n      \"x\": 700,\n      \"y\": 0,\n      \"width\": 300,\n      \"height\": 400,\n      \"label\": \"Done\",\n      \"color\": \"4\"\n    },\n    {\n      \"id\": \"task1\",\n      \"type\": \"text\",\n      \"x\": 20,\n      \"y\": 50,\n      \"width\": 260,\n      \"height\": 80,\n      \"text\": \"## Task 1\\n\\nDescription of first task\"\n    },\n    {\n      \"id\": \"task2\",\n      \"type\": \"text\",\n      \"x\": 370,\n      \"y\": 50,\n      \"width\": 260,\n      \"height\": 80,\n      \"text\": \"## Task 2\\n\\nCurrently working on this\"\n    },\n    {\n      \"id\": \"task3\",\n      \"type\": \"text\",\n      \"x\": 720,\n      \"y\": 50,\n      \"width\": 260,\n      \"height\": 80,\n      \"text\": \"## Task 3\\n\\n~~Completed task~~\"\n    }\n  ],\n  \"edges\": []\n}\n```\n\n### Research Canvas with Files and Links\n\n```json\n{\n  \"nodes\": [\n    {\n      \"id\": \"central\",\n      \"type\": \"text\",\n      \"x\": 200,\n      \"y\": 200,\n      \"width\": 200,\n      \"height\": 100,\n      \"text\": \"# Research Topic\\n\\nMain research question\",\n      \"color\": \"6\"\n    },\n    {\n      \"id\": \"notes1\",\n      \"type\": \"file\",\n      \"x\": 0,\n      \"y\": 0,\n      \"width\": 180,\n      \"height\": 150,\n      \"file\": \"Research/Literature Review.md\"\n    },\n    {\n      \"id\": \"notes2\",\n      \"type\": \"file\",\n      \"x\": 450,\n      \"y\": 0,\n      \"width\": 180,\n      \"height\": 150,\n      \"file\": \"Research/Methodology.md\"\n    },\n    {\n      \"id\": \"source1\",\n      \"type\": \"link\",\n      \"x\": 0,\n      \"y\": 350,\n      \"width\": 180,\n      \"height\": 100,\n      \"url\": \"https://scholar.google.com\"\n    },\n    {\n      \"id\": \"source2\",\n      \"type\": \"link\",\n      \"x\": 450,\n      \"y\": 350,\n      \"width\": 180,\n      \"height\": 100,\n      \"url\": \"https://arxiv.org\"\n    }\n  ],\n  \"edges\": [\n    {\n      \"id\": \"e1\",\n      \"fromNode\": \"central\",\n      \"toNode\": \"notes1\",\n      \"toEnd\": \"arrow\",\n      \"label\": \"literature\"\n    },\n    {\n      \"id\": \"e2\",\n      \"fromNode\": \"central\",\n      \"toNode\": \"notes2\",\n      \"toEnd\": \"arrow\",\n      \"label\": \"methods\"\n    },\n    {\n      \"id\": \"e3\",\n      \"fromNode\": \"central\",\n      \"toNode\": \"source1\",\n      \"toEnd\": \"arrow\"\n    },\n    {\n      \"id\": \"e4\",\n      \"fromNode\": \"central\",\n      \"toNode\": \"source2\",\n      \"toEnd\": \"arrow\"\n    }\n  ]\n}\n```\n\n### Flowchart\n\n```json\n{\n  \"nodes\": [\n    {\n      \"id\": \"start\",\n      \"type\": \"text\",\n      \"x\": 100,\n      \"y\": 0,\n      \"width\": 150,\n      \"height\": 60,\n      \"text\": \"**Start**\",\n      \"color\": \"4\"\n    },\n    {\n      \"id\": \"decision\",\n      \"type\": \"text\",\n      \"x\": 75,\n      \"y\": 120,\n      \"width\": 200,\n      \"height\": 80,\n      \"text\": \"## Decision\\n\\nIs condition true?\",\n      \"color\": \"3\"\n    },\n    {\n      \"id\": \"yes-path\",\n      \"type\": \"text\",\n      \"x\": -100,\n      \"y\": 280,\n      \"width\": 150,\n      \"height\": 60,\n      \"text\": \"**Yes Path**\\n\\nDo action A\"\n    },\n    {\n      \"id\": \"no-path\",\n      \"type\": \"text\",\n      \"x\": 300,\n      \"y\": 280,\n      \"width\": 150,\n      \"height\": 60,\n      \"text\": \"**No Path**\\n\\nDo action B\"\n    },\n    {\n      \"id\": \"end\",\n      \"type\": \"text\",\n      \"x\": 100,\n      \"y\": 420,\n      \"width\": 150,\n      \"height\": 60,\n      \"text\": \"**End**\",\n      \"color\": \"1\"\n    }\n  ],\n  \"edges\": [\n    {\n      \"id\": \"e1\",\n      \"fromNode\": \"start\",\n      \"fromSide\": \"bottom\",\n      \"toNode\": \"decision\",\n      \"toSide\": \"top\",\n      \"toEnd\": \"arrow\"\n    },\n    {\n      \"id\": \"e2\",\n      \"fromNode\": \"decision\",\n      \"fromSide\": \"left\",\n      \"toNode\": \"yes-path\",\n      \"toSide\": \"top\",\n      \"toEnd\": \"arrow\",\n      \"label\": \"Yes\"\n    },\n    {\n      \"id\": \"e3\",\n      \"fromNode\": \"decision\",\n      \"fromSide\": \"right\",\n      \"toNode\": \"no-path\",\n      \"toSide\": \"top\",\n      \"toEnd\": \"arrow\",\n      \"label\": \"No\"\n    },\n    {\n      \"id\": \"e4\",\n      \"fromNode\": \"yes-path\",\n      \"fromSide\": \"bottom\",\n      \"toNode\": \"end\",\n      \"toSide\": \"left\",\n      \"toEnd\": \"arrow\"\n    },\n    {\n      \"id\": \"e5\",\n      \"fromNode\": \"no-path\",\n      \"fromSide\": \"bottom\",\n      \"toNode\": \"end\",\n      \"toSide\": \"right\",\n      \"toEnd\": \"arrow\"\n    }\n  ]\n}\n```\n\n## ID Generation\n\nNode and edge IDs must be unique strings. Obsidian generates 16-character hexadecimal IDs.\n\nExample format: `a1b2c3d4e5f67890`\n\n## Layout Guidelines\n\n### Positioning\n\n- Coordinates can be negative (canvas extends infinitely)\n- `x` increases to the right\n- `y` increases downward\n- Position refers to top-left corner of node\n\n### Recommended Sizes\n\n| Node Type | Suggested Width | Suggested Height |\n|-----------|-----------------|------------------|\n| Small text | 200-300 | 80-150 |\n| Medium text | 300-450 | 150-300 |\n| Large text | 400-600 | 300-500 |\n| File preview | 300-500 | 200-400 |\n| Link preview | 250-400 | 100-200 |\n| Group | Varies | Varies |\n\n### Spacing\n\n- Leave 20-50px padding inside groups\n- Space nodes 50-100px apart for readability\n- Align nodes to grid (multiples of 10 or 20) for cleaner layouts\n\n## Validation Rules\n\n1. All `id` values must be unique across nodes and edges\n2. `fromNode` and `toNode` must reference existing node IDs\n3. Required fields must be present for each node type\n4. `type` must be one of: `text`, `file`, `link`, `group`\n5. `backgroundStyle` must be one of: `cover`, `ratio`, `repeat`\n6. `fromSide`, `toSide` must be one of: `top`, `right`, `bottom`, `left`\n7. `fromEnd`, `toEnd` must be one of: `none`, `arrow`\n8. Color presets must be `\"1\"` through `\"6\"` or valid hex color\n\n## References\n\n- [JSON Canvas Spec 1.0](https://jsoncanvas.org/spec/1.0/)\n- [JSON Canvas GitHub](https://github.com/obsidianmd/jsoncanvas)"
              },
              {
                "name": "obsidian-bases",
                "description": "Create and edit Obsidian Bases (.base files) with views, filters, formulas, and summaries. Use when working with .base files, creating database-like views of notes, or when the user mentions Bases, table views, card views, filters, or formulas in Obsidian.",
                "path": "plugins/obsidian-skills/skills/obsidian-bases/SKILL.md",
                "frontmatter": {
                  "name": "obsidian-bases",
                  "category": "document-processing",
                  "description": "Create and edit Obsidian Bases (.base files) with views, filters, formulas, and summaries. Use when working with .base files, creating database-like views of notes, or when the user mentions Bases, table views, card views, filters, or formulas in Obsidian."
                },
                "content": "# Obsidian Bases\n\nThis skill enables Claude Code to create and edit valid Obsidian Bases (`.base` files) including views, filters, formulas, and all related configurations.\n\n## Overview\n\nObsidian Bases are YAML-based files that define dynamic views of notes in an Obsidian vault. A Base file can contain multiple views, global filters, formulas, property configurations, and custom summaries.\n\n## When to Use This Skill\n\n- Creating database-like views of notes in Obsidian\n- Building task trackers, reading lists, or project dashboards\n- Filtering and organizing notes by properties or tags\n- Creating calculated/formula fields\n- Setting up table, card, list, or map views\n- Working with .base files in an Obsidian vault\n\n## File Format\n\nBase files use the `.base` extension and contain valid YAML. They can also be embedded in Markdown code blocks.\n\n## Complete Schema\n\n```yaml\n# Global filters apply to ALL views in the base\nfilters:\n  # Can be a single filter string\n  # OR a recursive filter object with and/or/not\n  and: []\n  or: []\n  not: []\n\n# Define formula properties that can be used across all views\nformulas:\n  formula_name: 'expression'\n\n# Configure display names and settings for properties\nproperties:\n  property_name:\n    displayName: \"Display Name\"\n  formula.formula_name:\n    displayName: \"Formula Display Name\"\n  file.ext:\n    displayName: \"Extension\"\n\n# Define custom summary formulas\nsummaries:\n  custom_summary_name: 'values.mean().round(3)'\n\n# Define one or more views\nviews:\n  - type: table | cards | list | map\n    name: \"View Name\"\n    limit: 10                    # Optional: limit results\n    groupBy:                     # Optional: group results\n      property: property_name\n      direction: ASC | DESC\n    filters:                     # View-specific filters\n      and: []\n    order:                       # Properties to display in order\n      - file.name\n      - property_name\n      - formula.formula_name\n    summaries:                   # Map properties to summary formulas\n      property_name: Average\n```\n\n## Filter Syntax\n\nFilters narrow down results. They can be applied globally or per-view.\n\n### Filter Structure\n\n```yaml\n# Single filter\nfilters: 'status == \"done\"'\n\n# AND - all conditions must be true\nfilters:\n  and:\n    - 'status == \"done\"'\n    - 'priority > 3'\n\n# OR - any condition can be true\nfilters:\n  or:\n    - file.hasTag(\"book\")\n    - file.hasTag(\"article\")\n\n# NOT - exclude matching items\nfilters:\n  not:\n    - file.hasTag(\"archived\")\n\n# Nested filters\nfilters:\n  or:\n    - file.hasTag(\"tag\")\n    - and:\n        - file.hasTag(\"book\")\n        - file.hasLink(\"Textbook\")\n    - not:\n        - file.hasTag(\"book\")\n        - file.inFolder(\"Required Reading\")\n```\n\n### Filter Operators\n\n| Operator | Description |\n|----------|-------------|\n| `==` | equals |\n| `!=` | not equal |\n| `>` | greater than |\n| `<` | less than |\n| `>=` | greater than or equal |\n| `<=` | less than or equal |\n| `&&` | logical and |\n| `\\|\\|` | logical or |\n| `!` | logical not |\n\n## Properties\n\n### Three Types of Properties\n\n1. **Note properties** - From frontmatter: `note.author` or just `author`\n2. **File properties** - File metadata: `file.name`, `file.mtime`, etc.\n3. **Formula properties** - Computed values: `formula.my_formula`\n\n### File Properties Reference\n\n| Property | Type | Description |\n|----------|------|-------------|\n| `file.name` | String | File name |\n| `file.basename` | String | File name without extension |\n| `file.path` | String | Full path to file |\n| `file.folder` | String | Parent folder path |\n| `file.ext` | String | File extension |\n| `file.size` | Number | File size in bytes |\n| `file.ctime` | Date | Created time |\n| `file.mtime` | Date | Modified time |\n| `file.tags` | List | All tags in file |\n| `file.links` | List | Internal links in file |\n| `file.backlinks` | List | Files linking to this file |\n| `file.embeds` | List | Embeds in the note |\n| `file.properties` | Object | All frontmatter properties |\n\n### The `this` Keyword\n\n- In main content area: refers to the base file itself\n- When embedded: refers to the embedding file\n- In sidebar: refers to the active file in main content\n\n## Formula Syntax\n\nFormulas compute values from properties. Defined in the `formulas` section.\n\n```yaml\nformulas:\n  # Simple arithmetic\n  total: \"price * quantity\"\n\n  # Conditional logic\n  status_icon: 'if(done, \"check\", \"pending\")'\n\n  # String formatting\n  formatted_price: 'if(price, price.toFixed(2) + \" dollars\")'\n\n  # Date formatting\n  created: 'file.ctime.format(\"YYYY-MM-DD\")'\n\n  # Complex expressions\n  days_old: '((now() - file.ctime) / 86400000).round(0)'\n```\n\n## Functions Reference\n\n### Global Functions\n\n| Function | Signature | Description |\n|----------|-----------|-------------|\n| `date()` | `date(string): date` | Parse string to date |\n| `duration()` | `duration(string): duration` | Parse duration string |\n| `now()` | `now(): date` | Current date and time |\n| `today()` | `today(): date` | Current date (time = 00:00:00) |\n| `if()` | `if(condition, trueResult, falseResult?)` | Conditional |\n| `min()` | `min(n1, n2, ...): number` | Smallest number |\n| `max()` | `max(n1, n2, ...): number` | Largest number |\n| `number()` | `number(any): number` | Convert to number |\n| `link()` | `link(path, display?): Link` | Create a link |\n| `list()` | `list(element): List` | Wrap in list if not already |\n| `file()` | `file(path): file` | Get file object |\n| `image()` | `image(path): image` | Create image for rendering |\n| `icon()` | `icon(name): icon` | Lucide icon by name |\n| `html()` | `html(string): html` | Render as HTML |\n| `escapeHTML()` | `escapeHTML(string): string` | Escape HTML characters |\n\n### Date Functions & Fields\n\n**Fields:** `date.year`, `date.month`, `date.day`, `date.hour`, `date.minute`, `date.second`, `date.millisecond`\n\n| Function | Signature | Description |\n|----------|-----------|-------------|\n| `date()` | `date.date(): date` | Remove time portion |\n| `format()` | `date.format(string): string` | Format with Moment.js pattern |\n| `time()` | `date.time(): string` | Get time as string |\n| `relative()` | `date.relative(): string` | Human-readable relative time |\n| `isEmpty()` | `date.isEmpty(): boolean` | Always false for dates |\n\n### Date Arithmetic\n\n```yaml\n# Duration units: y/year/years, M/month/months, d/day/days,\n#                 w/week/weeks, h/hour/hours, m/minute/minutes, s/second/seconds\n\n# Add/subtract durations\n\"date + \\\"1M\\\"\"           # Add 1 month\n\"date - \\\"2h\\\"\"           # Subtract 2 hours\n\"now() + \\\"1 day\\\"\"       # Tomorrow\n\"today() + \\\"7d\\\"\"        # A week from today\n\n# Subtract dates for millisecond difference\n\"now() - file.ctime\"\n\n# Complex duration arithmetic\n\"now() + (duration('1d') * 2)\"\n```\n\n### String Functions\n\n**Field:** `string.length`\n\n| Function | Description |\n|----------|-------------|\n| `contains(value)` | Check substring |\n| `containsAll(...values)` | All substrings present |\n| `containsAny(...values)` | Any substring present |\n| `startsWith(query)` | Starts with query |\n| `endsWith(query)` | Ends with query |\n| `isEmpty()` | Empty or not present |\n| `lower()` | To lowercase |\n| `title()` | To Title Case |\n| `trim()` | Remove whitespace |\n| `replace(pattern, replacement)` | Replace pattern |\n| `repeat(count)` | Repeat string |\n| `reverse()` | Reverse string |\n| `slice(start, end?)` | Substring |\n| `split(separator, n?)` | Split to list |\n\n### Number Functions\n\n| Function | Description |\n|----------|-------------|\n| `abs()` | Absolute value |\n| `ceil()` | Round up |\n| `floor()` | Round down |\n| `round(digits?)` | Round to digits |\n| `toFixed(precision)` | Fixed-point notation |\n| `isEmpty()` | Not present |\n\n### List Functions\n\n**Field:** `list.length`\n\n| Function | Description |\n|----------|-------------|\n| `contains(value)` | Element exists |\n| `containsAll(...values)` | All elements exist |\n| `containsAny(...values)` | Any element exists |\n| `filter(expression)` | Filter by condition (uses `value`, `index`) |\n| `map(expression)` | Transform elements (uses `value`, `index`) |\n| `reduce(expression, initial)` | Reduce to single value (uses `value`, `index`, `acc`) |\n| `flat()` | Flatten nested lists |\n| `join(separator)` | Join to string |\n| `reverse()` | Reverse order |\n| `slice(start, end?)` | Sublist |\n| `sort()` | Sort ascending |\n| `unique()` | Remove duplicates |\n| `isEmpty()` | No elements |\n\n### File Functions\n\n| Function | Description |\n|----------|-------------|\n| `asLink(display?)` | Convert to link |\n| `hasLink(otherFile)` | Has link to file |\n| `hasTag(...tags)` | Has any of the tags |\n| `hasProperty(name)` | Has property |\n| `inFolder(folder)` | In folder or subfolder |\n\n## View Types\n\n### Table View\n\n```yaml\nviews:\n  - type: table\n    name: \"My Table\"\n    order:\n      - file.name\n      - status\n      - due_date\n    summaries:\n      price: Sum\n      count: Average\n```\n\n### Cards View\n\n```yaml\nviews:\n  - type: cards\n    name: \"Gallery\"\n    order:\n      - file.name\n      - cover_image\n      - description\n```\n\n### List View\n\n```yaml\nviews:\n  - type: list\n    name: \"Simple List\"\n    order:\n      - file.name\n      - status\n```\n\n### Map View\n\nRequires latitude/longitude properties and the Maps plugin.\n\n```yaml\nviews:\n  - type: map\n    name: \"Locations\"\n```\n\n## Default Summary Formulas\n\n| Name | Input Type | Description |\n|------|------------|-------------|\n| `Average` | Number | Mathematical mean |\n| `Min` | Number | Smallest number |\n| `Max` | Number | Largest number |\n| `Sum` | Number | Sum of all numbers |\n| `Range` | Number | Max - Min |\n| `Median` | Number | Mathematical median |\n| `Stddev` | Number | Standard deviation |\n| `Earliest` | Date | Earliest date |\n| `Latest` | Date | Latest date |\n| `Checked` | Boolean | Count of true values |\n| `Unchecked` | Boolean | Count of false values |\n| `Empty` | Any | Count of empty values |\n| `Filled` | Any | Count of non-empty values |\n| `Unique` | Any | Count of unique values |\n\n## Complete Examples\n\n### Task Tracker Base\n\n```yaml\nfilters:\n  and:\n    - file.hasTag(\"task\")\n    - 'file.ext == \"md\"'\n\nformulas:\n  days_until_due: 'if(due, ((date(due) - today()) / 86400000).round(0), \"\")'\n  is_overdue: 'if(due, date(due) < today() && status != \"done\", false)'\n  priority_label: 'if(priority == 1, \"High\", if(priority == 2, \"Medium\", \"Low\"))'\n\nproperties:\n  status:\n    displayName: Status\n  formula.days_until_due:\n    displayName: \"Days Until Due\"\n  formula.priority_label:\n    displayName: Priority\n\nviews:\n  - type: table\n    name: \"Active Tasks\"\n    filters:\n      and:\n        - 'status != \"done\"'\n    order:\n      - file.name\n      - status\n      - formula.priority_label\n      - due\n      - formula.days_until_due\n    groupBy:\n      property: status\n      direction: ASC\n    summaries:\n      formula.days_until_due: Average\n\n  - type: table\n    name: \"Completed\"\n    filters:\n      and:\n        - 'status == \"done\"'\n    order:\n      - file.name\n      - completed_date\n```\n\n### Reading List Base\n\n```yaml\nfilters:\n  or:\n    - file.hasTag(\"book\")\n    - file.hasTag(\"article\")\n\nformulas:\n  reading_time: 'if(pages, (pages * 2).toString() + \" min\", \"\")'\n  status_icon: 'if(status == \"reading\", \"reading\", if(status == \"done\", \"done\", \"to-read\"))'\n  year_read: 'if(finished_date, date(finished_date).year, \"\")'\n\nproperties:\n  author:\n    displayName: Author\n  formula.status_icon:\n    displayName: \"\"\n  formula.reading_time:\n    displayName: \"Est. Time\"\n\nviews:\n  - type: cards\n    name: \"Library\"\n    order:\n      - cover\n      - file.name\n      - author\n      - formula.status_icon\n    filters:\n      not:\n        - 'status == \"dropped\"'\n\n  - type: table\n    name: \"Reading List\"\n    filters:\n      and:\n        - 'status == \"to-read\"'\n    order:\n      - file.name\n      - author\n      - pages\n      - formula.reading_time\n```\n\n### Project Notes Base\n\n```yaml\nfilters:\n  and:\n    - file.inFolder(\"Projects\")\n    - 'file.ext == \"md\"'\n\nformulas:\n  last_updated: 'file.mtime.relative()'\n  link_count: 'file.links.length'\n\nsummaries:\n  avgLinks: 'values.filter(value.isType(\"number\")).mean().round(1)'\n\nproperties:\n  formula.last_updated:\n    displayName: \"Updated\"\n  formula.link_count:\n    displayName: \"Links\"\n\nviews:\n  - type: table\n    name: \"All Projects\"\n    order:\n      - file.name\n      - status\n      - formula.last_updated\n      - formula.link_count\n    summaries:\n      formula.link_count: avgLinks\n    groupBy:\n      property: status\n      direction: ASC\n\n  - type: list\n    name: \"Quick List\"\n    order:\n      - file.name\n      - status\n```\n\n### Daily Notes Index\n\n```yaml\nfilters:\n  and:\n    - file.inFolder(\"Daily Notes\")\n    - '/^\\d{4}-\\d{2}-\\d{2}$/.matches(file.basename)'\n\nformulas:\n  word_estimate: '(file.size / 5).round(0)'\n  day_of_week: 'date(file.basename).format(\"dddd\")'\n\nproperties:\n  formula.day_of_week:\n    displayName: \"Day\"\n  formula.word_estimate:\n    displayName: \"~Words\"\n\nviews:\n  - type: table\n    name: \"Recent Notes\"\n    limit: 30\n    order:\n      - file.name\n      - formula.day_of_week\n      - formula.word_estimate\n      - file.mtime\n```\n\n## Embedding Bases\n\nEmbed in Markdown files:\n\n```markdown\n![[MyBase.base]]\n\n<!-- Specific view -->\n![[MyBase.base#View Name]]\n```\n\n## YAML Quoting Rules\n\n- Use single quotes for formulas containing double quotes: `'if(done, \"Yes\", \"No\")'`\n- Use double quotes for simple strings: `\"My View Name\"`\n- Escape nested quotes properly in complex expressions\n\n## Common Patterns\n\n### Filter by Tag\n\n```yaml\nfilters:\n  and:\n    - file.hasTag(\"project\")\n```\n\n### Filter by Folder\n\n```yaml\nfilters:\n  and:\n    - file.inFolder(\"Notes\")\n```\n\n### Filter by Date Range\n\n```yaml\nfilters:\n  and:\n    - 'file.mtime > now() - \"7d\"'\n```\n\n### Filter by Property Value\n\n```yaml\nfilters:\n  and:\n    - 'status == \"active\"'\n    - 'priority >= 3'\n```\n\n### Combine Multiple Conditions\n\n```yaml\nfilters:\n  or:\n    - and:\n        - file.hasTag(\"important\")\n        - 'status != \"done\"'\n    - and:\n        - 'priority == 1'\n        - 'due != \"\"'\n```\n\n## References\n\n- [Bases Syntax](https://help.obsidian.md/bases/syntax)\n- [Functions](https://help.obsidian.md/bases/functions)\n- [Views](https://help.obsidian.md/bases/views)\n- [Formulas](https://help.obsidian.md/formulas)"
              },
              {
                "name": "obsidian-markdown",
                "description": "Create and edit Obsidian Flavored Markdown with wikilinks, embeds, callouts, properties, and other Obsidian-specific syntax. Use when working with .md files in Obsidian, or when the user mentions wikilinks, callouts, frontmatter, tags, embeds, or Obsidian notes.",
                "path": "plugins/obsidian-skills/skills/obsidian-markdown/SKILL.md",
                "frontmatter": {
                  "name": "obsidian-markdown",
                  "category": "document-processing",
                  "description": "Create and edit Obsidian Flavored Markdown with wikilinks, embeds, callouts, properties, and other Obsidian-specific syntax. Use when working with .md files in Obsidian, or when the user mentions wikilinks, callouts, frontmatter, tags, embeds, or Obsidian notes."
                },
                "content": "# Obsidian Flavored Markdown\n\nThis skill enables Claude Code to create and edit valid Obsidian Flavored Markdown including wikilinks, embeds, callouts, properties, and all related syntax.\n\n## When to Use This Skill\n\n- Working with .md files in an Obsidian vault\n- Creating notes with wikilinks or internal links\n- Adding embeds for notes, images, audio, or PDFs\n- Using callouts (info boxes, warnings, tips, etc.)\n- Managing frontmatter/properties in YAML format\n- Working with tags and nested tags\n- Creating block references and block IDs\n\n## Basic Formatting\n\n### Paragraphs and Line Breaks\n\nParagraphs are separated by blank lines. Single line breaks within a paragraph are ignored unless you use:\n- Two spaces at the end of a line\n- Or use `<br>` for explicit breaks\n\n### Headings\n\n```markdown\n# Heading 1\n## Heading 2\n### Heading 3\n#### Heading 4\n##### Heading 5\n###### Heading 6\n```\n\n### Text Styling\n\n```markdown\n**Bold text**\n*Italic text*\n***Bold and italic***\n~~Strikethrough~~\n==Highlighted text==\n```\n\n## Internal Links (Wikilinks)\n\n### Basic Wikilinks\n\n```markdown\n[[Note Name]]\n[[Note Name|Display Text]]\n[[Folder/Note Name]]\n```\n\n### Heading Links\n\n```markdown\n[[Note Name#Heading]]\n[[Note Name#Heading|Display Text]]\n[[#Heading in Current Note]]\n```\n\n### Block References\n\n```markdown\n[[Note Name#^block-id]]\n[[Note Name#^block-id|Display Text]]\n[[#^block-id]]\n```\n\n### Creating Block IDs\n\nAdd a block ID at the end of any paragraph or list item:\n\n```markdown\nThis is a paragraph you can reference. ^my-block-id\n\n- List item with ID ^list-block\n```\n\n## Embeds\n\n### Embedding Notes\n\n```markdown\n![[Note Name]]\n![[Note Name#Heading]]\n![[Note Name#^block-id]]\n```\n\n### Embedding Images\n\n```markdown\n![[image.png]]\n![[image.png|400]]\n![[image.png|400x300]]\n```\n\n### Embedding Audio\n\n```markdown\n![[audio.mp3]]\n```\n\n### Embedding PDFs\n\n```markdown\n![[document.pdf]]\n![[document.pdf#page=5]]\n![[document.pdf#height=400]]\n```\n\n### Embedding Videos\n\n```markdown\n![[video.mp4]]\n```\n\n## Callouts\n\n### Basic Callout Syntax\n\n```markdown\n> [!note]\n> This is a note callout.\n\n> [!warning]\n> This is a warning callout.\n\n> [!tip] Custom Title\n> This callout has a custom title.\n```\n\n### Callout Types\n\n| Type | Aliases | Description |\n|------|---------|-------------|\n| `note` | | Default blue info box |\n| `abstract` | `summary`, `tldr` | Abstract/summary |\n| `info` | | Information |\n| `todo` | | Task/todo item |\n| `tip` | `hint`, `important` | Helpful tip |\n| `success` | `check`, `done` | Success message |\n| `question` | `help`, `faq` | Question/FAQ |\n| `warning` | `caution`, `attention` | Warning message |\n| `failure` | `fail`, `missing` | Failure message |\n| `danger` | `error` | Error/danger |\n| `bug` | | Bug report |\n| `example` | | Example content |\n| `quote` | `cite` | Quotation |\n\n### Foldable Callouts\n\n```markdown\n> [!note]+ Expanded by default\n> Content visible initially.\n\n> [!note]- Collapsed by default\n> Content hidden initially.\n```\n\n### Nested Callouts\n\n```markdown\n> [!question] Can callouts be nested?\n> > [!answer] Yes!\n> > Callouts can be nested inside each other.\n```\n\n## Lists\n\n### Unordered Lists\n\n```markdown\n- Item 1\n- Item 2\n  - Nested item\n  - Another nested item\n- Item 3\n```\n\n### Ordered Lists\n\n```markdown\n1. First item\n2. Second item\n   1. Nested numbered item\n3. Third item\n```\n\n### Task Lists\n\n```markdown\n- [ ] Uncompleted task\n- [x] Completed task\n- [ ] Another task\n```\n\n## Code Blocks\n\n### Inline Code\n\n```markdown\nUse `inline code` for short snippets.\n```\n\n### Fenced Code Blocks\n\n````markdown\n```javascript\nfunction hello() {\n  console.log(\"Hello, world!\");\n}\n```\n````\n\n### Supported Languages\n\nObsidian supports syntax highlighting for many languages including:\n`javascript`, `typescript`, `python`, `rust`, `go`, `java`, `c`, `cpp`, `csharp`, `ruby`, `php`, `html`, `css`, `json`, `yaml`, `markdown`, `bash`, `sql`, and many more.\n\n## Tables\n\n```markdown\n| Header 1 | Header 2 | Header 3 |\n|----------|:--------:|---------:|\n| Left     | Center   | Right    |\n| aligned  | aligned  | aligned  |\n```\n\n## Math (LaTeX)\n\n### Inline Math\n\n```markdown\nThe equation $E = mc^2$ is famous.\n```\n\n### Block Math\n\n```markdown\n$$\n\\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}\n$$\n```\n\n## Diagrams (Mermaid)\n\n````markdown\n```mermaid\ngraph TD\n    A[Start] --> B{Decision}\n    B -->|Yes| C[Do Something]\n    B -->|No| D[Do Something Else]\n    C --> E[End]\n    D --> E\n```\n````\n\n## Footnotes\n\n```markdown\nThis is a sentence with a footnote.[^1]\n\n[^1]: This is the footnote content.\n```\n\n## Comments\n\n```markdown\n%%\nThis is a comment that won't be rendered.\n%%\n\nInline %%comment%% within text.\n```\n\n## Properties (Frontmatter)\n\n### Basic Properties\n\n```yaml\n---\ntitle: My Note Title\ndate: 2024-01-15\ntags:\n  - tag1\n  - tag2\nauthor: John Doe\n---\n```\n\n### Property Types\n\n| Type | Example |\n|------|---------|\n| Text | `title: My Title` |\n| Number | `rating: 5` |\n| Checkbox | `completed: true` |\n| Date | `date: 2024-01-15` |\n| Date & time | `created: 2024-01-15T10:30:00` |\n| List | `tags: [a, b, c]` or multiline |\n| Link | `related: \"[[Other Note]]\"` |\n\n### Multi-value Properties\n\n```yaml\n---\ntags:\n  - project\n  - work\n  - important\naliases:\n  - My Alias\n  - Another Name\ncssclasses:\n  - wide-page\n  - cards\n---\n```\n\n## Tags\n\n### Inline Tags\n\n```markdown\nThis note is about #productivity and #tools.\n```\n\n### Nested Tags\n\n```markdown\n#project/work\n#status/in-progress\n#priority/high\n```\n\n### Tags in Frontmatter\n\n```yaml\n---\ntags:\n  - project\n  - project/work\n  - status/active\n---\n```\n\n## HTML Support\n\nObsidian supports a subset of HTML:\n\n```markdown\n<div class=\"my-class\">\n  Custom HTML content\n</div>\n\n<details>\n<summary>Click to expand</summary>\nHidden content here\n</details>\n\n<kbd>Ctrl</kbd> + <kbd>C</kbd>\n```\n\n## Complete Example\n\n```markdown\n---\ntitle: Project Alpha Overview\ndate: 2024-01-15\ntags:\n  - project\n  - documentation\nstatus: active\n---\n\n# Project Alpha Overview\n\n## Summary\n\nThis document outlines the key aspects of **Project Alpha**. For related materials, see [[Project Alpha/Resources]] and [[Team Members]].\n\n> [!info] Quick Facts\n> - Start Date: January 2024\n> - Team Size: 5 members\n> - Status: Active\n\n## Key Features\n\n1. [[Feature A]] - Core functionality\n2. [[Feature B]] - User interface\n3. [[Feature C]] - API integration\n\n### Feature A Details\n\nThe main equation governing our approach is $f(x) = ax^2 + bx + c$.\n\n![[feature-a-diagram.png|500]]\n\n> [!tip] Implementation Note\n> See [[Technical Specs#^impl-note]] for implementation details.\n\n## Tasks\n\n- [x] Initial planning ^planning-task\n- [ ] Development phase\n- [ ] Testing phase\n- [ ] Deployment\n\n## Code Example\n\n```python\ndef process_data(input):\n    return transform(input)\n```\n\n## Architecture\n\n```mermaid\ngraph LR\n    A[Input] --> B[Process]\n    B --> C[Output]\n```\n\n## Notes\n\nThis approach was inspired by ==recent research==[^1].\n\n[^1]: Smith, J. (2024). Modern Approaches to Data Processing.\n\n%%\nTODO: Add more examples\nReview with team next week\n%%\n\n#project/alpha #documentation\n```\n\n## References\n\n- [Obsidian Formatting Syntax](https://help.obsidian.md/Editing+and+formatting/Basic+formatting+syntax)\n- [Advanced Formatting](https://help.obsidian.md/Editing+and+formatting/Advanced+formatting+syntax)\n- [Internal Links](https://help.obsidian.md/Linking+notes+and+files/Internal+links)\n- [Embedding Files](https://help.obsidian.md/Linking+notes+and+files/Embed+files)\n- [Callouts](https://help.obsidian.md/Editing+and+formatting/Callouts)\n- [Properties](https://help.obsidian.md/Editing+and+formatting/Properties)"
              }
            ]
          }
        ]
      }
    }
  ]
}