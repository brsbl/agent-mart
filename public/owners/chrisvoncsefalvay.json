{
  "owner": {
    "id": "chrisvoncsefalvay",
    "display_name": "Chris von Csefalvay",
    "type": "User",
    "avatar_url": "https://avatars.githubusercontent.com/u/2670100?u=f96086afdbb09c4b3e6445cd51114612ad8edb10&v=4",
    "url": "https://github.com/chrisvoncsefalvay",
    "bio": "Data scientist, computational epidemiologist, Principal at @HCLTech-DES-Practice, Oliver's dad.",
    "stats": {
      "total_repos": 1,
      "total_plugins": 1,
      "total_commands": 0,
      "total_skills": 6,
      "total_stars": 4,
      "total_forks": 0
    }
  },
  "repos": [
    {
      "full_name": "chrisvoncsefalvay/funsloth",
      "url": "https://github.com/chrisvoncsefalvay/funsloth",
      "description": "The fun way to manage fine-tuning with Unsloth.",
      "homepage": null,
      "signals": {
        "stars": 4,
        "forks": 0,
        "pushed_at": "2025-12-18T15:13:51Z",
        "created_at": "2025-12-17T00:49:18Z",
        "license": null
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 984
        },
        {
          "path": ".github",
          "type": "tree",
          "size": null
        },
        {
          "path": ".github/assets",
          "type": "tree",
          "size": null
        },
        {
          "path": ".github/assets/.gitkeep",
          "type": "blob",
          "size": 1
        },
        {
          "path": ".github/assets/logo.jpeg",
          "type": "blob",
          "size": 227282
        },
        {
          "path": ".gitignore",
          "type": "blob",
          "size": 361
        },
        {
          "path": "README.md",
          "type": "blob",
          "size": 3819
        },
        {
          "path": "package.json",
          "type": "blob",
          "size": 2491
        },
        {
          "path": "skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/funsloth-check",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/funsloth-check/DATA_FORMATS.md",
          "type": "blob",
          "size": 7083
        },
        {
          "path": "skills/funsloth-check/SKILL.md",
          "type": "blob",
          "size": 2219
        },
        {
          "path": "skills/funsloth-check/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/funsloth-check/scripts/validate_dataset.py",
          "type": "blob",
          "size": 12569
        },
        {
          "path": "skills/funsloth-hfjobs",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/funsloth-hfjobs/SKILL.md",
          "type": "blob",
          "size": 2889
        },
        {
          "path": "skills/funsloth-hfjobs/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/funsloth-hfjobs/references/HARDWARE_GUIDE.md",
          "type": "blob",
          "size": 5260
        },
        {
          "path": "skills/funsloth-hfjobs/references/PLATFORM_COMPARISON.md",
          "type": "blob",
          "size": 5214
        },
        {
          "path": "skills/funsloth-hfjobs/references/TROUBLESHOOTING.md",
          "type": "blob",
          "size": 6245
        },
        {
          "path": "skills/funsloth-hfjobs/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/funsloth-hfjobs/scripts/estimate_cost.py",
          "type": "blob",
          "size": 9914
        },
        {
          "path": "skills/funsloth-hfjobs/scripts/train_sft.py",
          "type": "blob",
          "size": 8752
        },
        {
          "path": "skills/funsloth-local",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/funsloth-local/SKILL.md",
          "type": "blob",
          "size": 4174
        },
        {
          "path": "skills/funsloth-local/notebooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/funsloth-local/notebooks/sft_template.ipynb",
          "type": "blob",
          "size": 13457
        },
        {
          "path": "skills/funsloth-local/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/funsloth-local/references/HARDWARE_GUIDE.md",
          "type": "blob",
          "size": 5260
        },
        {
          "path": "skills/funsloth-local/references/TROUBLESHOOTING.md",
          "type": "blob",
          "size": 6245
        },
        {
          "path": "skills/funsloth-local/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/funsloth-local/scripts/train_sft.py",
          "type": "blob",
          "size": 8752
        },
        {
          "path": "skills/funsloth-runpod",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/funsloth-runpod/SKILL.md",
          "type": "blob",
          "size": 3592
        },
        {
          "path": "skills/funsloth-runpod/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/funsloth-runpod/references/PLATFORM_COMPARISON.md",
          "type": "blob",
          "size": 5214
        },
        {
          "path": "skills/funsloth-runpod/references/TROUBLESHOOTING.md",
          "type": "blob",
          "size": 6245
        },
        {
          "path": "skills/funsloth-runpod/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/funsloth-runpod/scripts/estimate_cost.py",
          "type": "blob",
          "size": 9914
        },
        {
          "path": "skills/funsloth-runpod/scripts/train_sft.py",
          "type": "blob",
          "size": 8752
        },
        {
          "path": "skills/funsloth-train",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/funsloth-train/SKILL.md",
          "type": "blob",
          "size": 3869
        },
        {
          "path": "skills/funsloth-train/notebooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/funsloth-train/notebooks/sft_template.ipynb",
          "type": "blob",
          "size": 13457
        },
        {
          "path": "skills/funsloth-train/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/funsloth-train/references/HARDWARE_GUIDE.md",
          "type": "blob",
          "size": 5260
        },
        {
          "path": "skills/funsloth-train/references/MODEL_SELECTION.md",
          "type": "blob",
          "size": 4836
        },
        {
          "path": "skills/funsloth-train/references/TRAINING_METHODS.md",
          "type": "blob",
          "size": 6568
        },
        {
          "path": "skills/funsloth-train/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/funsloth-train/scripts/train_dpo.py",
          "type": "blob",
          "size": 4005
        },
        {
          "path": "skills/funsloth-train/scripts/train_grpo.py",
          "type": "blob",
          "size": 6929
        },
        {
          "path": "skills/funsloth-train/scripts/train_sft.py",
          "type": "blob",
          "size": 8752
        },
        {
          "path": "skills/funsloth-upload",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/funsloth-upload/SKILL.md",
          "type": "blob",
          "size": 3509
        },
        {
          "path": "skills/funsloth-upload/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/funsloth-upload/references/GGUF_GUIDE.md",
          "type": "blob",
          "size": 5681
        },
        {
          "path": "skills/funsloth-upload/references/TROUBLESHOOTING.md",
          "type": "blob",
          "size": 6245
        },
        {
          "path": "skills/funsloth-upload/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/funsloth-upload/scripts/convert_gguf.py",
          "type": "blob",
          "size": 6973
        }
      ],
      "marketplace": {
        "name": "funsloth",
        "version": null,
        "description": null,
        "owner_info": {
          "name": "Chris von Csefalvay",
          "email": "chris@chrisvoncsefalvay.com"
        },
        "keywords": [],
        "plugins": [
          {
            "name": "funsloth",
            "description": "Complete workflow for fine-tuning LLMs with Unsloth: dataset validation, notebook generation, training management, and model deployment",
            "source": "./",
            "category": "machine-learning",
            "version": "1.0.0",
            "author": {
              "name": "Chris von Csefalvay",
              "email": "chris@chrisvoncsefalvay.com"
            },
            "install_commands": [
              "/plugin marketplace add chrisvoncsefalvay/funsloth",
              "/plugin install funsloth@funsloth"
            ],
            "signals": {
              "stars": 4,
              "forks": 0,
              "pushed_at": "2025-12-18T15:13:51Z",
              "created_at": "2025-12-17T00:49:18Z",
              "license": null
            },
            "commands": [],
            "skills": [
              {
                "name": "funsloth-check",
                "description": "Validate datasets for Unsloth fine-tuning. Use when the user wants to check a dataset, analyze tokens, calculate Chinchilla optimality, or prepare data for training.",
                "path": "skills/funsloth-check/SKILL.md",
                "frontmatter": {
                  "name": "funsloth-check",
                  "description": "Validate datasets for Unsloth fine-tuning. Use when the user wants to check a dataset, analyze tokens, calculate Chinchilla optimality, or prepare data for training."
                },
                "content": "# Dataset Validation for Unsloth Fine-tuning\n\nValidate datasets before fine-tuning with Unsloth.\n\n## Quick Start\n\nFor automated validation, use the script:\n```bash\npython scripts/validate_dataset.py --dataset \"dataset-id\" --model llama-3.1-8b --lora-rank 16\n```\n\n## Workflow\n\n### 1. Get Dataset Source\nAsk for: HF dataset ID (e.g., `mlabonne/FineTome-100k`) or local path (e.g., `./data.jsonl`)\n\n### 2. Load and Detect Format\nAuto-detect format from structure. See [DATA_FORMATS.md](DATA_FORMATS.md) for details.\n\n| Format | Detection | Key Fields |\n|--------|-----------|------------|\n| Raw | `text` only | `text` |\n| Alpaca | `instruction` + `output` | `instruction`, `output` |\n| ShareGPT | `conversations` array | `from`, `value` |\n| ChatML | `messages` array | `role`, `content` |\n\n### 3. Validate Schema\nCheck required fields exist. Report issues with fix suggestions.\n\n### 4. Show Samples\nDisplay 2-3 examples for visual verification.\n\n### 5. Token Analysis\nReport statistics: total tokens, min/max/mean/median sequence length.\n\nFlag concerns:\n- Sequences > 4096 tokens\n- Sequences < 10 tokens\n\n### 6. Chinchilla Analysis\nAsk for target model and LoRA rank, then calculate:\n\n| Chinchilla Fraction | Interpretation |\n|--------------------|----------------|\n| < 0.5x | Dataset may be too small |\n| 0.5x - 2.0x | Good range |\n| > 2.0x | Large dataset, may take longer |\n\n### 7. Recommendations\nBased on analysis, suggest:\n- `standardize_sharegpt()` for ShareGPT data\n- Sequence length adjustments\n- Learning rate for small datasets\n\n### 8. Optional: HF Upload\nOffer to upload local datasets to Hub.\n\n### 9. Handoff\nPass context to `funsloth-train`:\n```yaml\ndataset_id: \"mlabonne/FineTome-100k\"\nformat_type: \"sharegpt\"\ntotal_tokens: 15000000\ntarget_model: \"llama-3.1-8b\"\nuse_lora: true\nlora_rank: 16\nchinchilla_fraction: 1.2\n```\n\n## Bundled Resources\n\n- [scripts/validate_dataset.py](scripts/validate_dataset.py) - Automated validation script\n- [DATA_FORMATS.md](DATA_FORMATS.md) - Dataset format reference"
              },
              {
                "name": "funsloth-hfjobs",
                "description": "Training manager for Hugging Face Jobs - launch fine-tuning on HF cloud GPUs with optional WandB monitoring",
                "path": "skills/funsloth-hfjobs/SKILL.md",
                "frontmatter": {
                  "name": "funsloth-hfjobs",
                  "description": "Training manager for Hugging Face Jobs - launch fine-tuning on HF cloud GPUs with optional WandB monitoring"
                },
                "content": "# Hugging Face Jobs Training Manager\n\nRun Unsloth training on Hugging Face Jobs (cloud GPU training).\n\n## Prerequisites\n\n1. **HF Authentication**: `huggingface-cli whoami` (login if needed)\n2. **HF Jobs Access**: Requires PRO subscription or org compute access\n3. **Training notebook/script**: From `funsloth-train`\n\n## Workflow\n\n### 1. Select Hardware\n\n| GPU | VRAM | Cost | Best For |\n|-----|------|------|----------|\n| A10G | 24GB | ~$1.50/hr | 7-14B LoRA |\n| A100 40GB | 40GB | ~$4/hr | 14-34B |\n| A100 80GB | 80GB | ~$6/hr | 70B |\n| H100 | 80GB | ~$8/hr | Fastest |\n\nSee [references/HARDWARE_GUIDE.md](references/HARDWARE_GUIDE.md) for model-to-GPU mapping.\n\n### 2. Convert Notebook to Script\n\nHF Jobs requires PEP 723 script format:\n\n```python\n# /// script\n# requires-python = \">=3.10\"\n# dependencies = [\n#     \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\",\n#     \"torch>=2.0\",\n#     \"transformers>=4.45\",\n#     \"trl>=0.12\",\n#     \"peft>=0.13\",\n#     \"datasets>=2.18\",\n# ]\n# ///\n```\n\nUse [scripts/train_sft.py](scripts/train_sft.py) as a template.\n\n### 3. Optional: WandB Integration\n\nAdd to script:\n```python\nimport wandb\nwandb.init(project=\"funsloth-training\")\n# Add report_to=\"wandb\" in TrainingArguments\n```\n\nSet: `export WANDB_API_KEY=\"your-key\"`\n\n### 4. Estimate Costs\n\nUse the cost estimator:\n```bash\npython scripts/estimate_cost.py --tokens {total_tokens} --platform hfjobs\n```\n\n### 5. Launch Job\n\n```bash\n# Create job config\ncat > job_config.yaml << 'EOF'\ncompute:\n  gpu: {gpu_type}\n  gpu_count: 1\nscript: train_hfjobs.py\noutputs:\n  - /outputs/*\nEOF\n\n# Submit\nhuggingface-cli jobs create --config job_config.yaml\n```\n\n### 6. Monitor Progress\n\n```bash\nhuggingface-cli jobs status {job_id}\nhuggingface-cli jobs logs {job_id} --follow\n```\n\nWandB: `https://wandb.ai/{username}/funsloth-training`\n\n### 7. Download Artifacts\n\n```python\nfrom huggingface_hub import snapshot_download\nsnapshot_download(repo_id=\"{username}/funsloth-job\", local_dir=\"./outputs\")\n```\n\n### 8. Handoff\n\nOffer `funsloth-upload` for Hub upload with model card.\n\n## Error Handling\n\n| Error | Resolution |\n|-------|------------|\n| No HF Jobs access | Get PRO subscription |\n| OOM | Reduce batch size or upgrade GPU |\n| Job timeout | Enable checkpointing |\n| Script error | Check PEP 723 dependencies |\n\n## Bundled Resources\n\n- [scripts/train_sft.py](scripts/train_sft.py) - PEP 723 script template\n- [scripts/estimate_cost.py](scripts/estimate_cost.py) - Cost estimation\n- [references/PLATFORM_COMPARISON.md](references/PLATFORM_COMPARISON.md) - HF Jobs vs alternatives\n- [references/HARDWARE_GUIDE.md](references/HARDWARE_GUIDE.md) - VRAM requirements\n- [references/TROUBLESHOOTING.md](references/TROUBLESHOOTING.md) - Common issues"
              },
              {
                "name": "funsloth-local",
                "description": "Training manager for local GPU training - validate CUDA, manage GPU selection, monitor progress, handle checkpoints",
                "path": "skills/funsloth-local/SKILL.md",
                "frontmatter": {
                  "name": "funsloth-local",
                  "description": "Training manager for local GPU training - validate CUDA, manage GPU selection, monitor progress, handle checkpoints"
                },
                "content": "# Local GPU Training Manager\n\nRun Unsloth training on your local GPU.\n\n## Prerequisites Check\n\n### 1. Verify CUDA\n\n```python\nimport torch\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"GPU: {torch.cuda.get_device_name(0)}\")\nprint(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n```\n\nIf CUDA not available:\n- Check NVIDIA drivers: `nvidia-smi`\n- Check CUDA: `nvcc --version`\n- Reinstall PyTorch: `pip install torch --index-url https://download.pytorch.org/whl/cu121`\n\n### 2. Check VRAM\n\nSee [references/HARDWARE_GUIDE.md](references/HARDWARE_GUIDE.md) for requirements:\n\n| VRAM | Recommended Setup |\n|------|-------------------|\n| 8GB | 7B, 4-bit, batch=1, LoRA r=8 |\n| 12GB | 7B, 4-bit, batch=2, LoRA r=16 |\n| 16GB | 7-13B, 4-bit, batch=2, LoRA r=16-32 |\n| 24GB | 7-14B, 4-bit, batch=4, LoRA r=32 |\n\n### 3. Check Dependencies\n\n```bash\npip install unsloth torch transformers trl peft datasets accelerate bitsandbytes\n```\n\n## Docker Option\n\nUse the [official Unsloth Docker image](https://docs.unsloth.ai/new/how-to-fine-tune-llms-with-unsloth-and-docker) for a pre-configured environment (supports all GPUs including Blackwell/50-series):\n\n```bash\ndocker run -d \\\n  -e JUPYTER_PASSWORD=\"unsloth\" \\\n  -p 8888:8888 \\\n  -v $(pwd)/work:/workspace/work \\\n  --gpus all \\\n  unsloth/unsloth\n```\n\nAccess Jupyter at `http://localhost:8888`. Example notebooks are in `/workspace/unsloth-notebooks/`.\n\nEnvironment variables:\n\n- `JUPYTER_PASSWORD` - Jupyter auth (default: `unsloth`)\n- `JUPYTER_PORT` - Port (default: `8888`)\n- `USER_PASSWORD` - User/sudo password (default: `unsloth`)\n\n## Run Training\n\n### Option 1: Notebook\n\n```bash\njupyter notebook notebooks/sft_template.ipynb\n```\n\n### Option 2: Script\n\n```bash\n# Edit configuration in script, then run\npython scripts/train_sft.py\n```\n\n### GPU Selection (Multi-GPU)\n\n```python\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Use first GPU\n```\n\n## Monitor Training\n\n### Terminal\n\n```bash\n# Watch GPU usage\nwatch -n 1 nvidia-smi\n\n# Or use nvitop (more detailed)\npip install nvitop && nvitop\n```\n\n### WandB (Optional)\n\n```bash\nexport WANDB_API_KEY=\"your-key\"\n# Add report_to=\"wandb\" in TrainingArguments\n```\n\n## Troubleshooting\n\n### OOM Error\n\nTry in order:\n1. Reduce batch_size (to 1)\n2. Increase gradient_accumulation\n3. Reduce max_seq_length\n4. Reduce LoRA rank\n5. `torch.cuda.empty_cache()`\n\n### Loss Not Decreasing\n\n1. Check learning rate (try higher or lower)\n2. Verify chat template matches model\n3. Inspect data format\n\n### Training Too Slow\n\n1. Enable bf16 if supported\n2. Use `packing=True` for short sequences\n3. Reduce logging_steps\n\nSee [references/TROUBLESHOOTING.md](references/TROUBLESHOOTING.md) for more solutions.\n\n## Resume from Checkpoint\n\n```python\nTrainingArguments(\n    resume_from_checkpoint=True,  # Auto-find latest\n    # Or: resume_from_checkpoint=\"outputs/checkpoint-500\"\n)\n```\n\n## Save Model\n\nTraining script automatically saves:\n- `outputs/lora_adapter/` - LoRA weights\n- `outputs/merged_16bit/` - Merged model (optional)\n\n## Test Inference\n\n```python\nfrom unsloth import FastLanguageModel\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\"outputs/lora_adapter\")\nFastLanguageModel.for_inference(model)\n\nmessages = [{\"role\": \"user\", \"content\": \"Hello!\"}]\ninputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(inputs, max_new_tokens=100)\nprint(tokenizer.decode(outputs[0]))\n```\n\n## Handoff\n\nOffer `funsloth-upload` for Hub upload with model card.\n\n## Tips\n\n1. **Close other GPU apps** before training\n2. **Monitor temps** - keep under 85C\n3. **Use UPS** for long runs\n4. **Save frequently** with `save_steps`\n\n## Bundled Resources\n\n- [notebooks/sft_template.ipynb](notebooks/sft_template.ipynb) - Notebook template\n- [scripts/train_sft.py](scripts/train_sft.py) - Script template\n- [references/HARDWARE_GUIDE.md](references/HARDWARE_GUIDE.md) - VRAM requirements\n- [references/TROUBLESHOOTING.md](references/TROUBLESHOOTING.md) - Common issues"
              },
              {
                "name": "funsloth-runpod",
                "description": "Training manager for RunPod GPU instances - configure pods, launch training, monitor progress, retrieve checkpoints",
                "path": "skills/funsloth-runpod/SKILL.md",
                "frontmatter": {
                  "name": "funsloth-runpod",
                  "description": "Training manager for RunPod GPU instances - configure pods, launch training, monitor progress, retrieve checkpoints"
                },
                "content": "# RunPod Training Manager\n\nRun Unsloth training on RunPod GPU instances.\n\n## Prerequisites\n\n1. **RunPod API Key**: `echo $RUNPOD_API_KEY` (get at runpod.io/console/user/settings)\n2. **RunPod SDK**: `pip install runpod`\n3. **Training notebook/script**: From `funsloth-train`\n\n## Workflow\n\n### 1. Select GPU\n\n| GPU | VRAM | Cost | Best For |\n|-----|------|------|----------|\n| RTX 3090 | 24GB | ~$0.35/hr | Budget 7-14B |\n| RTX 4090 | 24GB | ~$0.55/hr | Fast 7-14B |\n| A100 40GB | 40GB | ~$1.50/hr | 14-34B |\n| A100 80GB | 80GB | ~$2.00/hr | 70B |\n| H100 | 80GB | ~$3.50/hr | Fastest |\n\nRunPod typically has better prices than HF Jobs.\n\n### 2. Choose Deployment\n\n- **Pod** (Recommended): Persistent, SSH access, network storage\n- **Serverless**: Pay per second, complex setup (better for inference)\n\n### 3. Configure Network Volume (Recommended)\n\n```python\nimport runpod\nvolume = runpod.create_network_volume(name=\"funsloth-training\", size_gb=50, region=\"US\")\n```\n\nAllows: resume training, download checkpoints, share between pods.\n\n### 4. Launch Pod\n\nUse the [official Unsloth Docker image](https://docs.unsloth.ai/new/how-to-fine-tune-llms-with-unsloth-and-docker) for a pre-configured environment:\n\n```python\nimport runpod\n\npod = runpod.create_pod(\n    name=\"funsloth-training\",\n    image_name=\"unsloth/unsloth\",  # Official image, supports all GPUs incl. Blackwell\n    gpu_type_id=\"{gpu_type}\",\n    volume_in_gb=50,\n    network_volume_id=\"{volume_id}\",\n    env={\n        \"HF_TOKEN\": \"{token}\",\n        \"WANDB_API_KEY\": \"{key}\",\n        \"JUPYTER_PASSWORD\": \"unsloth\",\n    },\n    ports=\"8888/http,22/tcp\",\n)\n```\n\nThe Unsloth image includes Jupyter Lab (port 8888) and example notebooks in `/workspace/unsloth-notebooks/`.\n\n### 5. Upload and Run\n\n```bash\n# SSH into pod\nssh root@{pod_ip}\n\n# Upload script\nscp train.py root@{pod_ip}:/workspace/\n\n# Run training (use tmux for persistence)\ntmux new -s training\ncd /workspace && python train.py\n# Ctrl+B, D to detach\n```\n\n### 6. Monitor\n\n```bash\n# SSH monitoring\ntail -f /workspace/training.log\nnvidia-smi -l 1\n\n# Dashboard\nhttps://runpod.io/console/pods/{pod_id}\n```\n\n### 7. Retrieve Checkpoints\n\n```bash\n# Save to network volume\ncp -r /workspace/outputs /runpod-volume/\n\n# Download via SCP\nscp -r root@{pod_ip}:/workspace/outputs ./\n\n# Or push to HF Hub from pod\n```\n\n### 8. Stop Pod\n\n```python\nrunpod.stop_pod(pod_id)    # Can resume later\nrunpod.terminate_pod(pod_id)  # Deletes pod, keeps volume\n```\n\n### 9. Handoff\n\nOffer `funsloth-upload` for Hub upload with model card.\n\n## Best Practices\n\n1. **Always use network volumes** - pod storage is ephemeral\n2. **Use spot instances** for lower costs (risk of preemption)\n3. **Set up SSH keys** before creating pods\n4. **Stop pods when not training** - charges per minute\n5. **Save checkpoints frequently** with `save_steps`\n\n## Error Handling\n\n| Error | Resolution |\n|-------|------------|\n| Pod creation failed | Try different GPU type or region |\n| SSH refused | Wait 1-2 min, check IP |\n| Out of disk | Increase volume or clean up |\n| Volume not mounting | Check same region as pod |\n\n## Bundled Resources\n\n- [scripts/train_sft.py](scripts/train_sft.py) - Training script template\n- [scripts/estimate_cost.py](scripts/estimate_cost.py) - Cost estimation\n- [references/PLATFORM_COMPARISON.md](references/PLATFORM_COMPARISON.md) - RunPod vs alternatives\n- [references/TROUBLESHOOTING.md](references/TROUBLESHOOTING.md) - Common issues"
              },
              {
                "name": "funsloth-train",
                "description": "Generate Unsloth training notebooks and scripts. Use when the user wants to create a training notebook, configure fine-tuning parameters, or set up SFT/DPO/GRPO training.",
                "path": "skills/funsloth-train/SKILL.md",
                "frontmatter": {
                  "name": "funsloth-train",
                  "description": "Generate Unsloth training notebooks and scripts. Use when the user wants to create a training notebook, configure fine-tuning parameters, or set up SFT/DPO/GRPO training."
                },
                "content": "# Unsloth Training Notebook Generator\n\nGenerate training notebooks for fine-tuning with Unsloth.\n\n## Quick Start\n\nCopy and customize the template notebook:\n```\nnotebooks/sft_template.ipynb\n```\n\nOr use a training script directly:\n```bash\npython scripts/train_sft.py  # Supervised fine-tuning\npython scripts/train_dpo.py  # Direct preference optimization\npython scripts/train_grpo.py # Group relative policy optimization\n```\n\n## Configuration Modes\n\nAsk the user which mode they prefer:\n\n1. **Sensible defaults** - Production-ready notebook with recommended settings\n2. **Guide me** - Walk through each option with explanations\n3. **Leave it empty** - Notebook with ipywidgets for runtime configuration\n\n## Mode 1: Sensible Defaults\n\nUse these production-ready defaults:\n\n| Parameter | Default | Reasoning |\n|-----------|---------|-----------|\n| Model | `unsloth/llama-3.1-8b-unsloth-bnb-4bit` | Good balance |\n| Max seq length | 2048 | Covers most use cases |\n| Load in 4-bit | True | 70% VRAM reduction |\n| LoRA rank | 16 | Good trade-off |\n| Batch size | 2 | Works on 8GB+ VRAM |\n| Gradient accumulation | 4 | Effective batch of 8 |\n| Learning rate | 2e-4 | Unsloth recommended |\n| Epochs | 1 | Often sufficient |\n\n## Mode 2: Guide Me\n\nAsk questions in order. See [MODEL_SELECTION.md](references/MODEL_SELECTION.md) for model options and [TRAINING_METHODS.md](references/TRAINING_METHODS.md) for technique details.\n\n### Key Questions\n\n1. **Model family**: Llama, Qwen, Gemma, Phi, Mistral, DeepSeek?\n2. **Model size**: Based on VRAM (see [HARDWARE_GUIDE.md](references/HARDWARE_GUIDE.md))\n3. **Training technique**: SFT, DPO, GRPO, ORPO, KTO?\n4. **Quantization**: 4-bit (recommended), 8-bit, 16-bit?\n5. **LoRA rank**: 8, 16, 32, 64?\n6. **Sequence length**: 512, 1024, 2048, 4096?\n7. **Batch size**: 1, 2, 4, 8?\n8. **Learning rate**: 1e-5, 5e-5, 2e-4, 5e-4?\n9. **Training duration**: 1 epoch, 3 epochs, or specific steps?\n\n## Mode 3: ipywidgets\n\nGenerate a notebook with interactive configuration widgets. Users select options at runtime.\n\n## Notebook Structure\n\nGenerate notebooks with these sections:\n\n1. **Title and Overview** - What the notebook does\n2. **Installation** - Install Unsloth\n3. **Imports and GPU Check** - Verify environment\n4. **Configuration** - All tunable parameters\n5. **Load Model** - FastLanguageModel.from_pretrained()\n6. **Apply LoRA** - FastLanguageModel.get_peft_model()\n7. **Load Dataset** - Format-appropriate loading\n8. **Training** - SFTTrainer/DPOTrainer/GRPOTrainer\n9. **Save Model** - LoRA adapter + merged model\n10. **Test Inference** - Quick verification\n\n## After Generation\n\nAsk where to run training:\n1. **Hugging Face Jobs** - Cloud GPUs (`funsloth-hfjobs`)\n2. **RunPod** - Flexible GPU rentals (`funsloth-runpod`)\n3. **Local** - Your own GPU (`funsloth-local`)\n\n## Context to Pass\n\n```yaml\nnotebook_path: \"./training_notebook.ipynb\"\nmodel_name: \"unsloth/llama-3.1-8b-unsloth-bnb-4bit\"\ndataset_name: \"mlabonne/FineTome-100k\"\ntechnique: \"SFT\"\nlora_rank: 16\nmax_seq_length: 2048\nbatch_size: 2\nlearning_rate: 2e-4\nnum_epochs: 1\n```\n\n## Bundled Resources\n\n- [notebooks/sft_template.ipynb](notebooks/sft_template.ipynb) - Ready-to-use SFT template\n- [scripts/train_sft.py](scripts/train_sft.py) - SFT script template\n- [scripts/train_dpo.py](scripts/train_dpo.py) - DPO script template\n- [scripts/train_grpo.py](scripts/train_grpo.py) - GRPO script template\n- [references/MODEL_SELECTION.md](references/MODEL_SELECTION.md) - Model recommendations\n- [references/HARDWARE_GUIDE.md](references/HARDWARE_GUIDE.md) - VRAM requirements\n- [references/TRAINING_METHODS.md](references/TRAINING_METHODS.md) - SFT vs DPO vs GRPO"
              },
              {
                "name": "funsloth-upload",
                "description": "Generate comprehensive model cards and upload fine-tuned models to Hugging Face Hub with professional documentation",
                "path": "skills/funsloth-upload/SKILL.md",
                "frontmatter": {
                  "name": "funsloth-upload",
                  "description": "Generate comprehensive model cards and upload fine-tuned models to Hugging Face Hub with professional documentation"
                },
                "content": "# Model Upload & Card Generator\n\nCreate model cards and upload fine-tuned models to Hugging Face Hub.\n\n## Gather Context\n\nIf coming from training manager, you should have:\n- `model_path`, `base_model`, `dataset`, `technique`\n- `training_config` (LoRA rank, LR, epochs)\n- `final_loss`, `training_time`, `hardware`\n\nIf missing, ask for essential information.\n\n## Configuration\n\n### 1. Repository Settings\n\nAsk for:\n- **Repo name**: `username/model-name`\n- **Visibility**: Public or Private\n- **License**: MIT, Apache 2.0, CC-BY-4.0, Llama 3 Community, etc.\n\n### 2. Export Formats\n\nOptions:\n1. **LoRA adapter only** (~50-200MB) - Users merge themselves\n2. **Merged 16-bit** (15-140GB) - Ready to use\n3. **GGUF quantized** (4-8GB) - For llama.cpp/Ollama\n4. **All of the above** (Recommended)\n\n### 3. GGUF Quantization\n\nIf GGUF selected, ask which levels. See [references/GGUF_GUIDE.md](references/GGUF_GUIDE.md).\n\n| Method | Size | Quality |\n|--------|------|---------|\n| Q4_K_M | ~4GB | Good (Recommended) |\n| Q5_K_M | ~5GB | Better |\n| Q8_0 | ~8GB | Best |\n\n## Generate Model Card\n\nCreate README.md with:\n\n1. **YAML Metadata** - license, tags, base_model, datasets\n2. **Model Description** - Table with key attributes\n3. **Training Details** - Hyperparameters, LoRA config, results\n4. **Usage Examples** - Transformers, Unsloth, Ollama, llama.cpp\n5. **Intended Use** - Primary use cases, out-of-scope\n6. **Limitations** - Biases, known issues\n7. **Citation** - BibTeX entry\n\n## Execute Upload\n\n### 1. Create Repository\n\n```python\nfrom huggingface_hub import create_repo\ncreate_repo(\"username/model-name\", private=False, exist_ok=True)\n```\n\n### 2. Upload Files\n\n```python\nfrom huggingface_hub import HfApi\napi = HfApi()\n\n# LoRA adapter\napi.upload_folder(folder_path=\"./outputs/lora_adapter\", repo_id=\"username/model\")\n\n# Model card\napi.upload_file(path_or_fileobj=\"README.md\", path_in_repo=\"README.md\", repo_id=\"username/model\")\n```\n\n### 3. Generate GGUF (if selected)\n\n```python\nfrom unsloth import FastLanguageModel\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\"./outputs/lora_adapter\")\nmodel.save_pretrained_gguf(\"./gguf\", tokenizer, quantization_method=\"q4_k_m\")\n```\n\nUse [scripts/convert_gguf.py](scripts/convert_gguf.py) for multiple quantizations.\n\n### 4. Verify\n\n```python\nfrom huggingface_hub import list_repo_files\nprint(list_repo_files(\"username/model\"))\n```\n\n## Final Report\n\n> **Upload Complete!**\n>\n> Model: https://huggingface.co/{repo_name}\n>\n> **Uploaded:**\n> - LoRA adapter\n> - Model card\n> - GGUF files (if selected)\n>\n> **Next steps:**\n> - Verify model page\n> - Add example outputs\n> - Run benchmarks\n> - Share on social media\n\n## Model Card Best Practices\n\n1. **Be specific about limitations**\n2. **Include usage examples** - copy-pasteable\n3. **Document training details**\n4. **Credit sources** - base model, dataset, tools\n5. **Use tables** - easier to scan\n\n## Error Handling\n\n| Error | Resolution |\n|-------|------------|\n| Repo exists | Use `exist_ok=True` |\n| Permission denied | Check HF token has write access |\n| Upload timeout | Use chunked upload |\n\n## Bundled Resources\n\n- [scripts/convert_gguf.py](scripts/convert_gguf.py) - GGUF conversion\n- [references/GGUF_GUIDE.md](references/GGUF_GUIDE.md) - GGUF details and Ollama setup\n- [references/TROUBLESHOOTING.md](references/TROUBLESHOOTING.md) - Upload issues"
              }
            ]
          }
        ]
      }
    }
  ]
}