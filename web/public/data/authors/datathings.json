{
  "author": {
    "id": "datathings",
    "display_name": "DataThings",
    "type": "Organization",
    "avatar_url": "https://avatars.githubusercontent.com/u/17784757?v=4",
    "url": "https://github.com/datathings",
    "bio": "Start-up in big data, data analytics, machine learning, predictive and prescriptive analytics, data modelling, temporal graph database based in Luxembourg",
    "stats": {
      "total_marketplaces": 1,
      "total_plugins": 4,
      "total_commands": 12,
      "total_skills": 3,
      "total_stars": 4,
      "total_forks": 0
    }
  },
  "marketplaces": [
    {
      "name": "datathings",
      "version": null,
      "description": "llama.cpp C API reference skill - Complete documentation for 163 functions covering local LLM inference",
      "owner_info": {
        "name": "Datathings",
        "email": "contact@datathings.com"
      },
      "keywords": [],
      "repo_full_name": "datathings/marketplace",
      "repo_url": "https://github.com/datathings/marketplace",
      "repo_description": null,
      "homepage": null,
      "signals": {
        "stars": 4,
        "forks": 0,
        "pushed_at": "2026-01-29T13:15:24Z",
        "created_at": "2026-01-05T09:58:02Z",
        "license": "Apache-2.0"
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 1023
        },
        {
          "path": "plugins",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/greycat-c",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/greycat-c/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/greycat-c/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 591
        },
        {
          "path": "plugins/greycat-c/README.md",
          "type": "blob",
          "size": 5928
        },
        {
          "path": "plugins/greycat-c/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/greycat-c/skills/greycat-c",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/greycat-c/skills/greycat-c/SKILL.md",
          "type": "blob",
          "size": 3655
        },
        {
          "path": "plugins/greycat-c/skills/greycat-c/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/greycat-c/skills/greycat-c/references/api_reference.md",
          "type": "blob",
          "size": 48164
        },
        {
          "path": "plugins/greycat-c/skills/greycat-c/references/standard_library.md",
          "type": "blob",
          "size": 21959
        },
        {
          "path": "plugins/greycat-lsp",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/greycat-lsp/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/greycat-lsp/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 858
        },
        {
          "path": "plugins/greycat-lsp/README.md",
          "type": "blob",
          "size": 3594
        },
        {
          "path": "plugins/greycat",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/greycat/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/greycat/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 711
        },
        {
          "path": "plugins/greycat/README.md",
          "type": "blob",
          "size": 10151
        },
        {
          "path": "plugins/greycat/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/greycat/commands/apicheck.md",
          "type": "blob",
          "size": 23985
        },
        {
          "path": "plugins/greycat/commands/backend.md",
          "type": "blob",
          "size": 28920
        },
        {
          "path": "plugins/greycat/commands/coverage.md",
          "type": "blob",
          "size": 17400
        },
        {
          "path": "plugins/greycat/commands/docs.md",
          "type": "blob",
          "size": 21311
        },
        {
          "path": "plugins/greycat/commands/frontend.md",
          "type": "blob",
          "size": 24359
        },
        {
          "path": "plugins/greycat/commands/init.md",
          "type": "blob",
          "size": 16116
        },
        {
          "path": "plugins/greycat/commands/migrate.md",
          "type": "blob",
          "size": 19214
        },
        {
          "path": "plugins/greycat/commands/optimize.md",
          "type": "blob",
          "size": 17833
        },
        {
          "path": "plugins/greycat/commands/scaffold.md",
          "type": "blob",
          "size": 22064
        },
        {
          "path": "plugins/greycat/commands/tutorial.md",
          "type": "blob",
          "size": 15764
        },
        {
          "path": "plugins/greycat/commands/typecheck.md",
          "type": "blob",
          "size": 12931
        },
        {
          "path": "plugins/greycat/commands/upgrade.md",
          "type": "blob",
          "size": 22022
        },
        {
          "path": "plugins/greycat/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/greycat/skills/greycat",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/greycat/skills/greycat/SKILL.md",
          "type": "blob",
          "size": 12840
        },
        {
          "path": "plugins/greycat/skills/greycat/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/greycat/skills/greycat/references/LIBRARIES.md",
          "type": "blob",
          "size": 5378
        },
        {
          "path": "plugins/greycat/skills/greycat/references/ai",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/greycat/skills/greycat/references/ai/llm.md",
          "type": "blob",
          "size": 12711
        },
        {
          "path": "plugins/greycat/skills/greycat/references/algebra",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/greycat/skills/greycat/references/algebra/climate.md",
          "type": "blob",
          "size": 12165
        },
        {
          "path": "plugins/greycat/skills/greycat/references/algebra/compute.md",
          "type": "blob",
          "size": 25743
        },
        {
          "path": "plugins/greycat/skills/greycat/references/algebra/kmeans.md",
          "type": "blob",
          "size": 16091
        },
        {
          "path": "plugins/greycat/skills/greycat/references/algebra/ml.md",
          "type": "blob",
          "size": 20773
        },
        {
          "path": "plugins/greycat/skills/greycat/references/algebra/nn.md",
          "type": "blob",
          "size": 18478
        },
        {
          "path": "plugins/greycat/skills/greycat/references/algebra/patterns.md",
          "type": "blob",
          "size": 15852
        },
        {
          "path": "plugins/greycat/skills/greycat/references/algebra/transforms.md",
          "type": "blob",
          "size": 15486
        },
        {
          "path": "plugins/greycat/skills/greycat/references/cli.md",
          "type": "blob",
          "size": 7715
        },
        {
          "path": "plugins/greycat/skills/greycat/references/concurrency.md",
          "type": "blob",
          "size": 5219
        },
        {
          "path": "plugins/greycat/skills/greycat/references/data_structures.md",
          "type": "blob",
          "size": 3039
        },
        {
          "path": "plugins/greycat/skills/greycat/references/finance",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/greycat/skills/greycat/references/finance/finance.md",
          "type": "blob",
          "size": 13425
        },
        {
          "path": "plugins/greycat/skills/greycat/references/frontend.md",
          "type": "blob",
          "size": 15216
        },
        {
          "path": "plugins/greycat/skills/greycat/references/io.md",
          "type": "blob",
          "size": 4876
        },
        {
          "path": "plugins/greycat/skills/greycat/references/kafka",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/greycat/skills/greycat/references/kafka/kafka.md",
          "type": "blob",
          "size": 14912
        },
        {
          "path": "plugins/greycat/skills/greycat/references/lsp.md",
          "type": "blob",
          "size": 12829
        },
        {
          "path": "plugins/greycat/skills/greycat/references/nodes.md",
          "type": "blob",
          "size": 4180
        },
        {
          "path": "plugins/greycat/skills/greycat/references/opcua",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/greycat/skills/greycat/references/opcua/opcua.md",
          "type": "blob",
          "size": 20796
        },
        {
          "path": "plugins/greycat/skills/greycat/references/patterns.md",
          "type": "blob",
          "size": 9835
        },
        {
          "path": "plugins/greycat/skills/greycat/references/permissions.md",
          "type": "blob",
          "size": 1789
        },
        {
          "path": "plugins/greycat/skills/greycat/references/powerflow",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/greycat/skills/greycat/references/powerflow/powerflow.md",
          "type": "blob",
          "size": 18205
        },
        {
          "path": "plugins/greycat/skills/greycat/references/s3",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/greycat/skills/greycat/references/s3/s3.md",
          "type": "blob",
          "size": 16888
        },
        {
          "path": "plugins/greycat/skills/greycat/references/sql",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/greycat/skills/greycat/references/sql/postgres.md",
          "type": "blob",
          "size": 16565
        },
        {
          "path": "plugins/greycat/skills/greycat/references/std",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/greycat/skills/greycat/references/std/core.md",
          "type": "blob",
          "size": 21752
        },
        {
          "path": "plugins/greycat/skills/greycat/references/std/io.md",
          "type": "blob",
          "size": 7444
        },
        {
          "path": "plugins/greycat/skills/greycat/references/std/runtime.md",
          "type": "blob",
          "size": 16038
        },
        {
          "path": "plugins/greycat/skills/greycat/references/std/util.md",
          "type": "blob",
          "size": 9319
        },
        {
          "path": "plugins/greycat/skills/greycat/references/testing.md",
          "type": "blob",
          "size": 1447
        },
        {
          "path": "plugins/greycat/skills/greycat/references/time.md",
          "type": "blob",
          "size": 4273
        },
        {
          "path": "plugins/greycat/skills/greycat/references/useragent",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/greycat/skills/greycat/references/useragent/useragent.md",
          "type": "blob",
          "size": 17398
        },
        {
          "path": "plugins/llamacpp",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/llamacpp/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/llamacpp/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 760
        },
        {
          "path": "plugins/llamacpp/README.md",
          "type": "blob",
          "size": 8968
        },
        {
          "path": "plugins/llamacpp/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/llamacpp/skills/llamacpp",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/llamacpp/skills/llamacpp/SKILL.md",
          "type": "blob",
          "size": 8407
        },
        {
          "path": "plugins/llamacpp/skills/llamacpp/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/llamacpp/skills/llamacpp/references/api-advanced.md",
          "type": "blob",
          "size": 9674
        },
        {
          "path": "plugins/llamacpp/skills/llamacpp/references/api-context.md",
          "type": "blob",
          "size": 11947
        },
        {
          "path": "plugins/llamacpp/skills/llamacpp/references/api-core.md",
          "type": "blob",
          "size": 6529
        },
        {
          "path": "plugins/llamacpp/skills/llamacpp/references/api-inference.md",
          "type": "blob",
          "size": 12063
        },
        {
          "path": "plugins/llamacpp/skills/llamacpp/references/api-model-info.md",
          "type": "blob",
          "size": 5348
        },
        {
          "path": "plugins/llamacpp/skills/llamacpp/references/api-sampling.md",
          "type": "blob",
          "size": 13794
        },
        {
          "path": "plugins/llamacpp/skills/llamacpp/references/api.md",
          "type": "blob",
          "size": 52485
        },
        {
          "path": "plugins/llamacpp/skills/llamacpp/references/workflows.md",
          "type": "blob",
          "size": 45647
        }
      ],
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"name\": \"datathings\",\n  \"license\": \"Apache-2.0\",\n  \"owner\": {\n    \"name\": \"Datathings\",\n    \"email\": \"contact@datathings.com\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"llamacpp\",\n      \"source\": \"./plugins/llamacpp\",\n      \"description\": \"llama.cpp C API reference skill - Complete documentation for 163 functions covering local LLM inference\",\n      \"version\": \"1.7.6\"\n    },\n    {\n      \"name\": \"greycat-c\",\n      \"source\": \"./plugins/greycat-c\",\n      \"description\": \"GreyCat C API & Standard Library skill - Native C SDK documentation for GreyCat\",\n      \"version\": \"1.7.6\"\n    },\n    {\n      \"name\": \"greycat\",\n      \"source\": \"./plugins/greycat\",\n      \"description\": \"Full-stack GreyCat development skill - GCL language, backend development, and React frontend integration\",\n      \"version\": \"1.7.6\"\n    },\n    {\n      \"name\": \"greycat-lsp\",\n      \"source\": \"./plugins/greycat-lsp\",\n      \"description\": \"GreyCat Language Server Protocol plugin - LSP support for .gcl files\",\n      \"version\": \"1.7.6\"\n    }\n  ]\n}\n",
        "plugins/greycat-c/.claude-plugin/plugin.json": "{\n  \"name\": \"greycat-c\",\n  \"version\": \"1.7.6\",\n  \"description\": \"Comprehensive reference for GreyCat C API and GCL Standard Library. Covers native function implementation, tensor operations, scheduling, I/O, statistics, and all std modules.\",\n  \"author\": {\n    \"name\": \"Datathings\",\n    \"email\": \"contact@datathings.com\"\n  },\n  \"license\": \"MIT\",\n  \"repository\": \"https://github.com/datathings/marketplace\",\n  \"keywords\": [\"greycat\", \"c-api\", \"standard-library\", \"native-development\", \"gcl\", \"tensor\", \"scheduler\", \"data-processing\", \"io-operations\", \"statistics\"],\n  \"skills\": \"./skills/\"\n}\n",
        "plugins/greycat-c/README.md": "# GreyCat C - API & Standard Library Skill\n\nA comprehensive AI skill providing complete reference for GreyCat C API and GCL Standard Library.\n\n## Overview\n\nThis skill enables AI assistants to help with GreyCat development by providing detailed knowledge of:\n\n- **C API** - Native function implementation, tensor operations, object manipulation, memory management\n- **Standard Library (std)** - Runtime features, I/O operations, collections, statistics, and utilities\n\n## What's Included\n\n### 1. C API Reference\nComplete C API documentation covering:\n- Machine context (`gc_machine_t`) and function parameters\n- Universal value containers (`gc_slot_t`) and type system\n- Object field access and manipulation\n- Tensor operations (multi-dimensional arrays)\n- Buffer building and string operations\n- Memory management (per-worker and global allocators)\n- HTTP client, cryptography, and I/O operations\n\n### 2. Standard Library Documentation\nComprehensive coverage of the GCL Standard Library organized into four modules:\n\n- **std::core** - Fundamental types (Date, Tuple), geospatial types (GeoCircle, GeoPoly, GeoBox), enumerations\n- **std::runtime** - Scheduler (task automation), logging, security, system operations, OpenID Connect, MCP\n- **std::io** - File I/O, CSV/JSON/XML parsing, HTTP client, email, binary serialization\n- **std::util** - Collections (Queue, Stack, SlidingWindow), statistics (Gaussian, Histogram), quantizers, crypto utilities\n\n## When to Use This Skill\n\nThis skill activates when working on:\n\n1. **Native C Functions** - Implementing custom native functions, working with tensors, managing memory\n2. **Scheduled Tasks** - Setting up recurring tasks with the Scheduler\n3. **File Operations** - Reading/writing CSV, JSON, XML, or binary files\n4. **HTTP Integration** - Making REST API calls or handling HTTP requests\n5. **Data Processing** - Statistical analysis, sliding windows, data quantization\n6. **Security** - User management, access control, authentication\n7. **System Operations** - Running processes, logging, system queries\n\n## Skill Activation Triggers\n\nThis skill automatically activates based on context. The AI assistant will use this skill when it detects:\n\n### Code & API Keywords\n- **C API**: `gc_machine_t`, `gc_slot_t`, `gc_object_t`, `gc_type_t`, tensor operations, memory allocators\n- **Core Types**: `Date`, `Time`, `Duration`, `Tuple`, `Error`, `GeoCircle`, `GeoPoly`, `GeoBox`\n- **Runtime**: `Scheduler`, `Task`, `Job`, `Logger`, `User`, `SecurityPolicy`, `OpenIDConnect`\n- **I/O**: `TextReader`, `TextWriter`, `CsvReader`, `CsvWriter`, `HttpClient`, `JsonReader`, `XmlParser`\n- **Utilities**: `Queue`, `Stack`, `SlidingWindow`, `Gaussian`, `Histogram`, `Quantizer`, `Assert`\n\n### Task Descriptions\n- \"Implement a native function\" / \"create a native C function\"\n- \"Schedule a task\" / \"set up recurring jobs\" / \"daily/weekly automation\"\n- \"Parse CSV\" / \"read JSON\" / \"make HTTP request\" / \"send email\"\n- \"Calculate statistics\" / \"sliding window\" / \"histogram analysis\"\n- \"Work with tensors\" / \"multi-dimensional arrays\"\n- \"User authentication\" / \"access control\" / \"OpenID Connect\"\n- \"Geospatial operations\" / \"GeoCircle\" / \"geographic boundaries\"\n\n### File Extensions & Contexts\n- Working with `.gcl` files (GreyCat Language)\n- Discussions of GreyCat runtime, SDK, or native development\n- Questions about GCL standard library modules (`std::core`, `std::runtime`, `std::io`, `std::util`)\n\n## Repository Structure\n\n```\nplugins/greycat-c/\n├── README.md               # This file\n├── package.sh              # Script to package the skill\n└── skills/greycat-c/       # Skill files\n    ├── SKILL.md            # Main skill entry point (concise overview)\n    └── references/\n        ├── api_reference.md        # Comprehensive C API reference\n        └── standard_library.md     # Complete Standard Library documentation\n```\n\n## Installation\n\n### Option 1: Clone and Package\n```bash\ngit clone https://github.com/datathings/marketplace.git\ncd marketplace/plugins/greycat-c\n./package.sh\n# Install the greycat-c.skill file to your skills directory\n```\n\n### Option 2: Install from Marketplace\nInstall directly via Claude Code:\n```bash\n/plugin install greycat-c@datathings\n```\n\n## Usage Examples\n\nOnce installed, the AI assistant will automatically use this skill when you ask questions or request help with GreyCat development:\n\n```\nUser: \"How do I create a 2D tensor in GreyCat C API?\"\nAssistant: *Uses the skill to provide accurate C API example*\n\nUser: \"Help me set up a daily scheduled task in GCL\"\nAssistant: *Uses the skill to show Scheduler usage with DailyPeriodicity*\n\nUser: \"How do I read a CSV file in GCL?\"\nAssistant: *Uses the skill to demonstrate CsvReader usage*\n```\n\n## Development\n\n### Skill Structure\n\nThis skill follows AI skill best practices:\n\n- **SKILL.md** - Lean entry point (~230 lines) with quick reference and links to detailed docs\n- **Progressive Disclosure** - Detailed content in separate reference files, loaded only when needed\n- **No Redundancy** - Information lives in one place (either SKILL.md or references)\n- **Comprehensive Triggering** - Description includes all use cases to ensure proper triggering\n\n### Making Changes\n\n1. Edit files in `greycat-c/` directory\n2. Update version in `marketplace.json` if publishing\n3. Run `./package.sh` to create the distributable `.skill` file\n4. Test the packaged skill before publishing\n\n## License\n\nThis skill documentation is maintained as part of the GreyCat SDK project.\n\n## Contributing\n\nFor issues or contributions, please use the repository issue tracker.\n\n## Links\n\n- **GreyCat Website**: https://greycat.io/\n- **GreyCat Documentation**: https://doc.greycat.io/\n- **GreyCat Installation**: https://get.greycat.io/\n- **Datathings**: https://datathings.com/\n- **Repository**: https://github.com/datathings/marketplace\n- **Skill Marketplace**: https://skillsmp.com/\n",
        "plugins/greycat-c/skills/greycat-c/SKILL.md": "---\nname: greycat-c\ndescription: \"GreyCat C API and GCL Standard Library reference. Use for: (1) Native C development with gc_machine_t context, tensors, objects, memory management, HTTP, crypto, I/O; (2) GCL Standard Library modules - std::core (Date/Time/Tuple/geospatial types), std::runtime (Scheduler/Task/Logger/User/Security/System/OpenAPI/MCP), std::io (CSV/JSON/XML/HTTP/Email/FileWalker), std::util (Queue/Stack/SlidingWindow/Gaussian/Histogram/Quantizers/Random/Plot). Keywords: GreyCat, GCL, native functions, tensors, task automation, scheduler.\"\n---\n\n# GreyCat SDK - C API & Standard Library\n\nComprehensive reference for GreyCat native development (C API) and the GCL Standard Library.\n\n## Contents\n\n1. **C API** - Native function implementation, tensor operations, object manipulation\n2. **Standard Library (std)** - GCL runtime features, I/O, collections, and utilities\n\n---\n\n# GreyCat C API\n\n## Core Concepts\n\n**gc_machine_t** - Execution context passed to all native functions. Use to get parameters, set results, report errors, and create objects.\n\n**gc_slot_t** - Universal value container (union type) holding any GreyCat value: integers, floats, bools, objects, etc.\n\n**gc_type_t** - Type system enum defining all GreyCat types (null, bool, int, float, str, object, static_field, etc.).\n\n## Common Operations\n\n**Parameter handling:**\n```c\ngc_slot_t param = gc_machine__get_param(ctx, offset);\ngc_type_t type = gc_machine__get_param_type(ctx, offset);\n```\n\n**Object field access:**\n```c\ngc_slot_t value = gc_object__get_at(obj, field_offset, &type, ctx);\n```\n\n**Tensor operations:**\n```c\ngc_core_tensor_t *tensor = gc_core_tensor__create(ctx);\ngc_core_tensor__init_2d(tensor, rows, cols, gc_core_TensorType_f32, ctx);\nf32_t val = gc_core_tensor__get_2d_f32(tensor, row, col, ctx);\n```\n\n**Memory management:**\n```c\nchar *temp = (char *)gc_gnu_malloc(size);      // Per-worker allocator\ndouble *shared = (double *)gc_global_gnu_malloc(size);  // Global allocator\n```\n\n## Detailed Reference\n\n**File:** [references/api_reference.md](references/api_reference.md) (1,461 lines)\n\n**Load when implementing:**\n- Native C functions with gc_machine_t\n- Tensor operations (multi-dimensional arrays)\n- Object/field manipulation, type introspection\n- Buffer building, string operations\n- HTTP client, cryptography, I/O operations\n\n**Contains:** Complete function signatures, real production examples, and documentation for Machine API, Object API, Tensor API, Array/Table APIs, Buffer/String APIs, Memory Allocation, Program/Type System, HTTP Client, Cryptography, and I/O.\n\n---\n\n# GreyCat Standard Library (std)\n\n## Module Organization\n\n- **std::core** - Fundamental types (Date, Time, Duration, Tuple, Error, geospatial types, enumerations)\n- **std::runtime** - Scheduler, Task, Job, Logger, User/Security, System, ChildProcess, License, OpenAPI, MCP\n- **std::io** - Text/Binary I/O, CSV, JSON, XML, HTTP client, Email/SMTP, FileWalker\n- **std::util** - Collections (Queue, Stack, SlidingWindow, TimeWindow), Statistics (Gaussian, Histogram), Quantizers, Assert, ProgressTracker, Crypto, Random, Plot\n\n## Detailed Reference\n\n**File:** [references/standard_library.md](references/standard_library.md) (957 lines)\n\n**Load when working with:**\n- Task scheduling and automation (Scheduler with periodicities)\n- File I/O operations (CSV, JSON, XML, binary files)\n- HTTP integration and REST APIs\n- Statistical analysis and data processing\n- Security, authentication, and user management\n- System operations and logging\n\n**Contains:** Complete documentation for all four standard library modules with code examples, usage patterns, and best practices.\n",
        "plugins/greycat-c/skills/greycat-c/references/api_reference.md": "# GreyCat C API Complete Reference\n\nThis document provides comprehensive coverage of the GreyCat C API, organized by functional area with extensive examples from real usage.\n\n## Table of Contents\n\n1. [Core Types and Structures](#core-types-and-structures)\n2. [Machine API](#machine-api)\n3. [Object API](#object-api)\n4. [Tensor API](#tensor-api)\n5. [Tensor Operation Examples](#tensor-operation-examples)\n6. [Array API](#array-api)\n7. [Table API](#table-api)\n8. [Buffer API](#buffer-api)\n9. [String API](#string-api)\n10. [Memory Allocation](#memory-allocation)\n11. [Program and Type System](#program-and-type-system)\n12. [Host and Task Management](#host-and-task-management)\n13. [HTTP Client](#http-client)\n14. [Cryptography](#cryptography)\n15. [Utility Functions](#utility-functions)\n16. [I/O Operations](#io-operations)\n17. [Common Patterns](#common-patterns)\n\n---\n\n## Core Types and Structures\n\n### Basic Type Definitions\n\n```c\n// Integer types\ntypedef int8_t i8_t;\ntypedef uint8_t u8_t;\ntypedef int16_t i16_t;\ntypedef uint16_t u16_t;\ntypedef int32_t i32_t;\ntypedef uint32_t u32_t;\ntypedef int64_t i64_t;\ntypedef uint64_t u64_t;\n\n// Floating point types\ntypedef float f32_t;\ntypedef double f64_t;\n\n// Complex types (not available on WASM)\ntypedef double _Complex c128_t;\ntypedef float _Complex c64_t;\n\n// Geospatial type\ntypedef u64_t geo_t;\n```\n\n### gc_type_t Enum\n\nThe `gc_type_t` enum defines all possible GreyCat types. Must fit in 8 bits.\n\n```c\ntypedef enum {\n    gc_type_null = 0,\n    gc_type_bool = 1,\n    gc_type_char = 2,\n    gc_type_int = 3,\n    gc_type_float = 4,\n    gc_type_node = 5,\n    gc_type_node_time = 6,\n    gc_type_node_index = 7,\n    gc_type_node_list = 8,\n    gc_type_node_geo = 9,\n    gc_type_geo = 10,\n    gc_type_time = 11,\n    gc_type_duration = 12,\n    gc_type_cubic = 13,\n    gc_type_static_field = 14,\n    gc_type_object = 15,\n    gc_type_t2 = 16,\n    gc_type_t3 = 17,\n    gc_type_t4 = 18,\n    gc_type_str = 19,\n    gc_type_t2f = 20,\n    gc_type_t3f = 21,\n    gc_type_t4f = 22,\n    gc_type_block_ref = 23,\n    gc_type_block_inline = 24,\n    gc_type_function = 25,\n    gc_type_undefined = 26,\n    gc_type_type = 27,\n    gc_type_field = 28,\n    gc_type_stringlit = 29,  // serialization only\n    gc_type_error = 30       // C internal only\n} gc_type_t;\n```\n\n### gc_object_t Structure\n\nGeneric handle for GreyCat Objects. Must be packed to 128 bits (64+32+32).\n\n```c\ntypedef struct {\n    gc_block_t *block;\n    u32_t marks;\n    u32_t type_id;\n} gc_object_t;\n```\n\n### gc_slot_t Union\n\nGeneric GreyCat variable handle that can hold any type of value.\n\n```c\ntypedef struct {\n    union {\n        bool b;\n        u8_t byte[8];\n        u32_t u32;\n        i64_t i64;\n        u64_t u64;\n        f64_t f64;\n        gc_slot_tuple_u32_t tu32;\n        gc_object_t *object;\n    };\n} gc_slot_t;\n```\n\n**Example: Creating slots with different types**\n\n```c\n// Boolean slot\ngc_slot_t bool_slot = {.b = true};\n\n// Integer slot\ngc_slot_t int_slot = {.i64 = 42};\n\n// Float slot\ngc_slot_t float_slot = {.f64 = 3.14159};\n\n// Object slot\ngc_slot_t obj_slot = {.object = some_object};\n\n// Static field (enum) slot\ngc_slot_t enum_slot = {.tu32 = {.left = module_id, .right = value}};\n```\n\n---\n\n## Machine API\n\nThe machine context (`gc_machine_t`) is the primary interface for native function implementations.\n\n### Getting Parameters\n\n```c\n// Get parameter by offset (0-indexed)\ngc_slot_t gc_machine__get_param(const gc_machine_t *ctx, u32_t offset);\n\n// Get parameter type\ngc_type_t gc_machine__get_param_type(const gc_machine_t *ctx, u32_t offset);\n\n// Get number of parameters\nu32_t gc_machine__get_param_nb(const gc_machine_t *ctx);\n```\n\n**Example: Reading function parameters**\n\n```c\nvoid my_native_function(gc_machine_t *ctx) {\n    // Check parameter count\n    u32_t param_count = gc_machine__get_param_nb(ctx);\n    if (param_count < 2) {\n        gc_machine__set_runtime_error(ctx, \"Expected at least 2 parameters\");\n        return;\n    }\n\n    // Get first parameter\n    gc_slot_t param0 = gc_machine__get_param(ctx, 0);\n    gc_type_t type0 = gc_machine__get_param_type(ctx, 0);\n\n    // Check type and use value\n    if (type0 == gc_type_int) {\n        i64_t value = param0.i64;\n        // Use the value...\n    } else if (type0 == gc_type_object) {\n        gc_object_t *obj = param0.object;\n        // Use the object...\n    }\n}\n```\n\n### Setting Results\n\n```c\n// Set the function result\nvoid gc_machine__set_result(gc_machine_t *self, gc_slot_t slot, gc_type_t slot_type);\n\n// Get the expected return type\nu32_t gc_machine__return_type(gc_machine_t *self);\n\n// Create an object of the return type\ngc_object_t *gc_machine__create_return_type_object(gc_machine_t *ctx);\n```\n\n**Example: Returning values**\n\n```c\nvoid compute_sum(gc_machine_t *ctx) {\n    gc_slot_t a = gc_machine__get_param(ctx, 0);\n    gc_slot_t b = gc_machine__get_param(ctx, 1);\n\n    i64_t sum = a.i64 + b.i64;\n\n    // Return the result\n    gc_slot_t result = {.i64 = sum};\n    gc_machine__set_result(ctx, result, gc_type_int);\n}\n```\n\n### Error Handling\n\n```c\n// Set a runtime error with custom message\nvoid gc_machine__set_runtime_error(gc_machine_t *ctx, const char *msg);\n\n// Set a runtime error from system errno\nvoid gc_machine__set_runtime_error_syserr(gc_machine_t *ctx);\n\n// Check if an error occurred\nbool gc_machine__error(gc_machine_t *ctx);\n```\n\n**Example: Error handling (from matmul.c)**\n\n```c\nif (!gc_core_tensor_descriptor__check_type(&inputs[0]->desc, &inputs[1]->desc, ctx)) {\n    gc_machine__set_runtime_error(ctx, \"type incompatible\");\n    return false;\n}\n```\n\n### Object Creation\n\n```c\n// Create an object of a specific type\ngc_object_t *gc_machine__create_object(const gc_machine_t *ctx, u32_t object_type_code);\n```\n\n**Example: Reading layer configuration (from dense_layer.c)**\n\n```c\ngc_type_t type = gc_type_null;\ngc_slot_t value = gc_object__get_at(layer, gc_compute_ComputeLayerDense_use_bias, &type, ctx);\nif (type == gc_type_bool) {\n    use_bias = value.b;\n}\n\nvalue = gc_object__get_at(layer, gc_compute_ComputeLayerDense_type, &type, ctx);\nif (type == gc_type_static_field) {\n    tensor_type = value.tu32.right;\n}\n```\n\n---\n\n## Object API\n\n### Getting Field Values\n\n```c\n// Get field by key (symbol ID)\ngc_slot_t gc_object__get(gc_object_t *self, u32_t key, gc_type_t *type, gc_machine_t *ctx);\n\n// Get field by offset\ngc_slot_t gc_object__get_at(const gc_object_t *self, u32_t offset, gc_type_t *type_res, const gc_machine_t *ctx);\n\n// Get default value for a field\ngc_slot_t gc_object__get_default_at(u32_t type_id, u32_t offset, gc_type_t *type_res, gc_machine_t *ctx);\n```\n\n**Example: Reading object fields (from dense_layer.c)**\n\n```c\nbool use_bias = true;\ngc_type_t type = gc_type_null;\ngc_slot_t value = gc_object__get_at(layer, gc_compute_ComputeLayerDense_use_bias, &type, ctx);\nif (type == gc_type_bool) {\n    use_bias = value.b;\n}\n\nvalue = gc_object__get_at(layer, gc_compute_ComputeLayerDense_inputs, &type, ctx);\nif (type == gc_type_int) {\n    i64_t inputs = value.i64;\n}\n\nvalue = gc_object__get_at(layer, gc_compute_ComputeLayerDense_activation, &type, ctx);\nif (type == gc_type_object) {\n    gc_object_t *activation = value.object;\n    // Use activation object...\n}\n```\n\n### Setting Field Values\n\n```c\n// Set field by key\nbool gc_object__set(gc_object_t *self, u32_t key, gc_slot_t value, gc_type_t type, gc_machine_t *ctx);\n\n// Set field by offset\nbool gc_object__set_at(gc_object_t *self, u32_t offset, gc_slot_t value, gc_type_t value_type, gc_machine_t *ctx);\n```\n\n### Object Operations\n\n```c\n// Mark/unmark object (for GC)\nvoid gc_object__mark(gc_object_t *self);\nvoid gc_object__un_mark(gc_object_t *self, gc_machine_t *ctx);\n\n// Deep clone an object\ngc_object_t *gc_object__deep_clone_object(gc_object_t *self, gc_machine_t *ctx);\n\n// Declare object dirty (for persistence)\nvoid gc_object__declare_dirty(gc_object_t *self);\n\n// Type checking\nbool gc_object__is_instance_of(const gc_object_t *self, u32_t of_type, gc_machine_t *ctx);\n```\n\n### Slot Type Conversion\n\n```c\n// Convert slot to int64\ni64_t gc_slot__to_i64(gc_slot_t slot, gc_type_t slot_type);\n\n// Convert slot to float64\nf64_t gc_slot__to_f64(gc_slot_t slot, gc_type_t slot_type);\n```\n\n---\n\n## Tensor API\n\nTensors are multi-dimensional arrays with support for various numeric types, optimized for ML and numerical computing workloads.\n\n### Tensor Types\n\n```c\n#define gc_core_TensorType_i32  0  // 32-bit integer\n#define gc_core_TensorType_i64  1  // 64-bit integer\n#define gc_core_TensorType_f32  2  // 32-bit float (single precision)\n#define gc_core_TensorType_f64  3  // 64-bit float (double precision)\n#define gc_core_TensorType_c64  4  // Complex float (32-bit real + 32-bit imaginary)\n#define gc_core_TensorType_c128 5  // Complex double (64-bit real + 64-bit imaginary)\n```\n\n### Tensor Descriptor\n\nThe descriptor contains tensor metadata: shape, type, size, and memory layout.\n\n```c\n#define GC_CORE_TENSOR_DIM_MAX 8  // Maximum 8 dimensions\n#define GC_CORE_TENSOR_SIZE_MAX 9223372036854775807     // Max elements per tensor\n#define GC_CORE_TENSOR_CAPACITY_MAX 9223372036854775807 // Max capacity in bytes\n\ntypedef struct {\n    i64_t dim[GC_CORE_TENSOR_DIM_MAX];  // Shape: dimensions along each axis\n    i8_t nb_dim;                         // Number of dimensions (1-8)\n    i8_t batch_dim;                      // Batch dimension index (-1 if none)\n    u8_t type;                           // Tensor type (TensorType enum)\n    u8_t nature;                         // Tensor nature/category\n    i64_t size;                          // Total number of elements\n    i64_t capacity;                      // Allocated capacity in bytes\n} gc_core_tensor_descriptor_t;\n```\n\n**Key descriptor fields:**\n- `dim[]`: Shape array (e.g., `[3, 224, 224]` for 3-channel 224x224 image)\n- `nb_dim`: Number of dimensions (3 for above example)\n- `batch_dim`: Which dimension is the batch (-1 if not batched)\n- `type`: Data type (f32, f64, i32, etc.)\n- `size`: Total elements (`3 * 224 * 224 = 150528` for above)\n- `capacity`: Memory allocated in bytes (`size * sizeof(type)`)\n\n### Creating and Initializing Tensors\n\n```c\n// Create empty tensor (must initialize before use)\ngc_core_tensor_t *gc_core_tensor__create(const gc_machine_t *ctx);\n\n// Initialize with custom descriptor and data\ngc_core_tensor_t *gc_machine__init_tensor(gc_core_tensor_descriptor_t desc,\n                                          gc_object_t *proxy,\n                                          char *data,\n                                          const gc_machine_t *ctx);\n\n// Initialize with specific dimensionality (convenience functions)\nvoid gc_core_tensor__init_1d(gc_core_tensor_t *self, i64_t size, u8_t tensor_type, gc_machine_t *ctx);\nvoid gc_core_tensor__init_2d(gc_core_tensor_t *self, i64_t rows, i64_t columns, u8_t tensor_type, gc_machine_t *ctx);\nvoid gc_core_tensor__init_3d(gc_core_tensor_t *self, i64_t c, i64_t h, i64_t w, u8_t tensor_type, gc_machine_t *ctx);\nvoid gc_core_tensor__init_4d(gc_core_tensor_t *self, i64_t n, i64_t c, i64_t h, i64_t w, u8_t tensor_type, gc_machine_t *ctx);\nvoid gc_core_tensor__init_5d(gc_core_tensor_t *self, i64_t t, i64_t n, i64_t c, i64_t h, i64_t w, u8_t tensor_type, gc_machine_t *ctx);\n\n// Initialize with arbitrary dimensions\nvoid gc_core_tensor__init_xd(gc_core_tensor_t *self, i64_t tensor_dim, const i64_t shape[], u8_t tensor_type, gc_machine_t *ctx);\n```\n\n**Example: Creating various tensor shapes**\n\n```c\n// 1D vector (1000 elements)\ngc_core_tensor_t *vec = gc_core_tensor__create(ctx);\ngc_core_tensor__init_1d(vec, 1000, gc_core_TensorType_f32, ctx);\n\n// 2D matrix (100 rows x 50 columns)\ngc_core_tensor_t *matrix = gc_core_tensor__create(ctx);\ngc_core_tensor__init_2d(matrix, 100, 50, gc_core_TensorType_f64, ctx);\n\n// 3D tensor - RGB image (3 channels x 224 height x 224 width)\ngc_core_tensor_t *image = gc_core_tensor__create(ctx);\ngc_core_tensor__init_3d(image, 3, 224, 224, gc_core_TensorType_f32, ctx);\n\n// 4D tensor - batch of images (32 batch x 3 channels x 224 height x 224 width)\ngc_core_tensor_t *batch = gc_core_tensor__create(ctx);\ngc_core_tensor__init_4d(batch, 32, 3, 224, 224, gc_core_TensorType_f32, ctx);\n\n// Custom 6D tensor\ni64_t shape[] = {2, 3, 4, 5, 6, 7};\ngc_core_tensor_t *custom = gc_core_tensor__create(ctx);\ngc_core_tensor__init_xd(custom, 6, shape, gc_core_TensorType_f64, ctx);\n```\n\n### Getting Tensor Values\n\nEach type has specialized getter functions for 1D, 2D, 3D, and N-D access:\n\n```c\n// i32 getters\ni32_t gc_core_tensor__get_1d_i32(const gc_core_tensor_t *tensor, i64_t pos, gc_machine_t *ctx);\ni32_t gc_core_tensor__get_2d_i32(const gc_core_tensor_t *tensor, i64_t row, i64_t column, gc_machine_t *ctx);\ni32_t gc_core_tensor__get_3d_i32(const gc_core_tensor_t *tensor, i64_t c, i64_t h, i64_t w, gc_machine_t *ctx);\ni32_t gc_core_tensor__get_nd_i32(const gc_core_tensor_t *tensor, i64_t n, const i64_t pos[], gc_machine_t *ctx);\n\n// i64 getters\ni64_t gc_core_tensor__get_1d_i64(const gc_core_tensor_t *tensor, i64_t pos, gc_machine_t *ctx);\ni64_t gc_core_tensor__get_2d_i64(const gc_core_tensor_t *tensor, i64_t row, i64_t column, gc_machine_t *ctx);\ni64_t gc_core_tensor__get_3d_i64(const gc_core_tensor_t *tensor, i64_t c, i64_t h, i64_t w, gc_machine_t *ctx);\ni64_t gc_core_tensor__get_nd_i64(const gc_core_tensor_t *tensor, i64_t n, const i64_t pos[], gc_machine_t *ctx);\n\n// f32 getters\nf32_t gc_core_tensor__get_1d_f32(const gc_core_tensor_t *tensor, i64_t pos, gc_machine_t *ctx);\nf32_t gc_core_tensor__get_2d_f32(const gc_core_tensor_t *tensor, i64_t row, i64_t column, gc_machine_t *ctx);\nf32_t gc_core_tensor__get_3d_f32(const gc_core_tensor_t *tensor, i64_t c, i64_t h, i64_t w, gc_machine_t *ctx);\nf32_t gc_core_tensor__get_nd_f32(const gc_core_tensor_t *tensor, i64_t n, const i64_t pos[], gc_machine_t *ctx);\n\n// f64 getters\nf64_t gc_core_tensor__get_1d_f64(const gc_core_tensor_t *tensor, i64_t pos, gc_machine_t *ctx);\nf64_t gc_core_tensor__get_2d_f64(const gc_core_tensor_t *tensor, i64_t row, i64_t column, gc_machine_t *ctx);\nf64_t gc_core_tensor__get_3d_f64(const gc_core_tensor_t *tensor, i64_t c, i64_t h, i64_t w, gc_machine_t *ctx);\nf64_t gc_core_tensor__get_nd_f64(const gc_core_tensor_t *tensor, i64_t n, const i64_t pos[], gc_machine_t *ctx);\n\n// c64 getters (complex float)\nc64_t gc_core_tensor__get_1d_c64(const gc_core_tensor_t *tensor, i64_t pos, gc_machine_t *ctx);\nc64_t gc_core_tensor__get_2d_c64(const gc_core_tensor_t *tensor, i64_t row, i64_t column, gc_machine_t *ctx);\nc64_t gc_core_tensor__get_3d_c64(const gc_core_tensor_t *tensor, i64_t c, i64_t h, i64_t w, gc_machine_t *ctx);\nc64_t gc_core_tensor__get_nd_c64(const gc_core_tensor_t *tensor, i64_t n, const i64_t pos[], gc_machine_t *ctx);\n\n// c128 getters (complex double)\nc128_t gc_core_tensor__get_1d_c128(const gc_core_tensor_t *tensor, i64_t pos, gc_machine_t *ctx);\nc128_t gc_core_tensor__get_2d_c128(const gc_core_tensor_t *tensor, i64_t row, i64_t column, gc_machine_t *ctx);\nc128_t gc_core_tensor__get_3d_c128(const gc_core_tensor_t *tensor, i64_t c, i64_t h, i64_t w, gc_machine_t *ctx);\nc128_t gc_core_tensor__get_nd_c128(const gc_core_tensor_t *tensor, i64_t n, const i64_t pos[], gc_machine_t *ctx);\n```\n\n**Example: Reading tensor values**\n\n```c\n// Read from 1D vector\nf32_t value = gc_core_tensor__get_1d_f32(vec, 42, ctx);\n\n// Read from 2D matrix\nf64_t element = gc_core_tensor__get_2d_f64(matrix, 5, 10, ctx);\n\n// Read pixel from RGB image (channel 0, row 112, col 112)\nf32_t pixel = gc_core_tensor__get_3d_f32(image, 0, 112, 112, ctx);\n\n// Read from 6D tensor using N-D accessor\ni64_t indices[] = {1, 2, 3, 4, 5, 6};\nf64_t nd_value = gc_core_tensor__get_nd_f64(custom, 6, indices, ctx);\n```\n\n### Setting Tensor Values\n\nEach type has specialized setter functions matching the getters:\n\n```c\n// f32 setters\nvoid gc_core_tensor__set_1d_f32(const gc_core_tensor_t *tensor, i64_t pos, f32_t value, gc_machine_t *ctx);\nvoid gc_core_tensor__set_2d_f32(const gc_core_tensor_t *tensor, i64_t row, i64_t column, f32_t value, gc_machine_t *ctx);\nvoid gc_core_tensor__set_3d_f32(const gc_core_tensor_t *tensor, i64_t c, i64_t h, i64_t w, f32_t value, gc_machine_t *ctx);\nvoid gc_core_tensor__set_nd_f32(const gc_core_tensor_t *tensor, i64_t n, const i64_t pos[], f32_t value, gc_machine_t *ctx);\n\n// f64 setters\nvoid gc_core_tensor__set_1d_f64(const gc_core_tensor_t *tensor, i64_t pos, f64_t value, gc_machine_t *ctx);\nvoid gc_core_tensor__set_2d_f64(const gc_core_tensor_t *tensor, i64_t row, i64_t column, f64_t value, gc_machine_t *ctx);\nvoid gc_core_tensor__set_3d_f64(const gc_core_tensor_t *tensor, i64_t c, i64_t h, i64_t w, f64_t value, gc_machine_t *ctx);\nvoid gc_core_tensor__set_nd_f64(const gc_core_tensor_t *tensor, i64_t n, const i64_t pos[], f64_t value, gc_machine_t *ctx);\n\n// Similar patterns for i32, i64, c64, c128...\n```\n\n**Example: Writing tensor values**\n\n```c\n// Write to 1D vector\ngc_core_tensor__set_1d_f32(vec, 42, 3.14f, ctx);\n\n// Write to 2D matrix\ngc_core_tensor__set_2d_f64(matrix, 5, 10, 2.71828, ctx);\n\n// Initialize 3D tensor to zeros\ngc_core_tensor_descriptor_t *desc = gc_core_tensor__get_descriptor(image);\nfor (i64_t c = 0; c < desc->dim[0]; c++) {\n    for (i64_t h = 0; h < desc->dim[1]; h++) {\n        for (i64_t w = 0; w < desc->dim[2]; w++) {\n            gc_core_tensor__set_3d_f32(image, c, h, w, 0.0f, ctx);\n        }\n    }\n}\n```\n\n### Atomic Add Operations\n\nAdds a value to the current value and returns the new value (atomic for gradient accumulation):\n\n```c\n// f32 add operations\nf32_t gc_core_tensor__add_1d_f32(const gc_core_tensor_t *tensor, i64_t pos, f32_t value, gc_machine_t *ctx);\nf32_t gc_core_tensor__add_2d_f32(const gc_core_tensor_t *tensor, i64_t row, i64_t column, f32_t value, gc_machine_t *ctx);\nf32_t gc_core_tensor__add_3d_f32(const gc_core_tensor_t *tensor, i64_t c, i64_t h, i64_t w, f32_t value, gc_machine_t *ctx);\nf32_t gc_core_tensor__add_nd_f32(const gc_core_tensor_t *tensor, i64_t n, const i64_t pos[], f32_t value, gc_machine_t *ctx);\n\n// f64 add operations\nf64_t gc_core_tensor__add_1d_f64(const gc_core_tensor_t *tensor, i64_t pos, f64_t value, gc_machine_t *ctx);\nf64_t gc_core_tensor__add_2d_f64(const gc_core_tensor_t *tensor, i64_t row, i64_t column, f64_t value, gc_machine_t *ctx);\nf64_t gc_core_tensor__add_3d_f64(const gc_core_tensor_t *tensor, i64_t c, i64_t h, i64_t w, f64_t value, gc_machine_t *ctx);\nf64_t gc_core_tensor__add_nd_f64(const gc_core_tensor_t *tensor, i64_t n, const i64_t pos[], f64_t value, gc_machine_t *ctx);\n\n// Similar patterns for i32, i64, c64, c128...\n```\n\n**Example: Gradient accumulation (from mul.c backward pass)**\n\n```c\n// Element-wise multiply backward: dA[i] += B[i] * dC[i]\nconst i64_t N = inputs_desc[0]->size;\nf64_t *dA = inputs_grad[0];\nconst f64_t *B = inputs[1];\nconst f64_t *dC = outputs_grad[0];\n\nif (dA != NULL) {\n    for (i64_t i = 0; i < N; i++) {\n        dA[i] += B[i] * dC[i];  // Accumulate gradient\n    }\n}\n```\n\n### Tensor Descriptor Operations\n\n```c\n// Get/set descriptor\ngc_core_tensor_descriptor_t *gc_core_tensor__get_descriptor(gc_core_tensor_t *t);\nvoid gc_core_tensor__set_descriptor(gc_core_tensor_t *t, gc_core_tensor_descriptor_t desc);\n\n// Get raw data pointer (for direct memory access)\nchar *gc_core_tensor__get_data(gc_core_tensor_t *t);\n\n// Set proxy object\nvoid gc_core_tensor__set_proxy(gc_core_tensor_t *tensor, gc_object_t *proxy);\n\n// Descriptor type utilities\nu8_t gc_core_tensor_descriptor__type_size(u8_t t);        // Size in bytes for type\ngc_type_t gc_core_tensor_descriptor__type(u8_t t);        // Convert to gc_type_t\ni64_t gc_core_tensor_descriptor__nb_arrays(gc_core_tensor_descriptor_t *descriptor);\ni64_t gc_core_tensor_descriptor__matrix_count(gc_core_tensor_descriptor_t *a);    // For batched matrices\ni64_t gc_core_tensor_descriptor__leading_dim(gc_core_tensor_descriptor_t *descriptor);\n\n// Index utilities\nbool gc_core_tensor_descriptor__increment_index(i8_t nb_dim, const i64_t *dim, i64_t *index);\ni64_t gc_core_tensor_descriptor__index_to_offset(i8_t nb_dim, const i64_t *dim, const i64_t *index);\n\n// JSON serialization\nvoid gc_core_tensor_descriptor__to_json(const gc_core_tensor_descriptor_t *self, gc_buffer_t *buffer, const gc_program_t *prog);\n```\n\n**Example: Direct memory access pattern (from add.c)**\n\n```c\n// Get raw pointers for performance-critical loops\nconst i64_t N = inputs_desc[0]->size;\nconst f64_t *A = (f64_t *)gc_core_tensor__get_data(input_a);\nconst f64_t *B = (f64_t *)gc_core_tensor__get_data(input_b);\nf64_t *C = (f64_t *)gc_core_tensor__get_data(output);\n\n// Element-wise addition\nfor (i64_t i = 0; i < N; i++) {\n    C[i] = A[i] + B[i];\n}\n```\n\n### Tensor Validation and Shape Operations\n\n```c\n// Validate shape array\nbool gc_core_tensor__check_shape(gc_core_array_t *shape, u64_t *tot_size, bool skip_zero);\n\n// Update capacity based on size\nbool gc_core_tensor__update_capacity(gc_core_tensor_t *tensor, gc_machine_t *ctx);\n\n// Type and dimension checking\nbool gc_core_tensor_descriptor__check_type(gc_core_tensor_descriptor_t *a, gc_core_tensor_descriptor_t *b, gc_machine_t *ctx);\nbool gc_core_tensor_descriptor__check_dim(gc_core_tensor_descriptor_t *a, gc_core_tensor_descriptor_t *b, gc_machine_t *ctx);\nbool gc_core_tensor_descriptor__update_size(gc_core_tensor_descriptor_t *descriptor, gc_machine_t *ctx);\n\n// Supported type check\nbool gc_core_tensor_descriptor__supported_types(gc_core_tensor_descriptor_t *a, gc_machine_t *ctx);\nbool gc_core_tensor_descriptor__default_check(gc_core_tensor_descriptor_t *a, gc_core_tensor_descriptor_t *b, gc_machine_t *ctx);\n```\n\n**Example: Type validation (from matmul.c)**\n\n```c\n// Ensure tensors have compatible types\nif (!gc_core_tensor_descriptor__check_type(&inputs[0]->desc, &inputs[1]->desc, ctx)) {\n    gc_machine__set_runtime_error(ctx, \"type incompatible\");\n    return false;\n}\n```\n\n### Matrix Multiplication Validation\n\n```c\n// Check if matrix multiplication is valid\nbool gc_core_tensor_descriptor__tensor_mult_check(gc_core_tensor_descriptor_t *a, bool trans_a,\n                                                  gc_core_tensor_descriptor_t *b, bool trans_b,\n                                                  gc_machine_t *ctx);\n\n// Check and compute result shape\nbool gc_core_tensor_descriptor__tensor_mult_check_result(gc_core_tensor_descriptor_t *a, bool trans_a,\n                                                        gc_core_tensor_descriptor_t *b, bool trans_b,\n                                                        gc_core_tensor_descriptor_t *result,\n                                                        gc_machine_t *ctx);\n\n// Compute output shape for matmul\nbool gc_core_tensor_descriptor__tensor_mult_size(gc_core_tensor_descriptor_t *a, bool trans_a,\n                                                gc_core_tensor_descriptor_t *b, bool trans_b,\n                                                gc_core_tensor_descriptor_t *result,\n                                                gc_machine_t *ctx);\n```\n\n**Example: Matrix multiplication validation (from matmul.c)**\n\n```c\nbool transpose_a = constants[0].b;\nbool transpose_b = constants[1].b;\n\n// Validate multiplication is possible\nif (!gc_core_tensor_descriptor__tensor_mult_check(&inputs[0]->desc, transpose_a,\n                                                  &inputs[1]->desc, transpose_b, ctx)) {\n    return false;\n}\n\n// Compute output shape\nif (!gc_core_tensor_descriptor__tensor_mult_size(&inputs[0]->desc, transpose_a,\n                                                &inputs[1]->desc, transpose_b,\n                                                &outputs[0]->desc, ctx)) {\n    return false;\n}\n```\n\n### Add Bias Validation\n\n```c\n// Check if add bias operation is valid\nbool gc_core_tensor_descriptor__add_bias_check_result(gc_core_tensor_descriptor_t *a,\n                                                     gc_core_tensor_descriptor_t *b,\n                                                     gc_core_tensor_descriptor_t *result,\n                                                     gc_machine_t *ctx);\n\n// Compute output shape for add bias\nbool gc_core_tensor_descriptor__add_bias_size(gc_core_tensor_descriptor_t *a,\n                                             gc_core_tensor_descriptor_t *b,\n                                             gc_core_tensor_descriptor_t *result,\n                                             gc_machine_t *ctx);\n```\n\n### Sum Operations\n\n```c\n// Check sum reduction validity\nbool gc_core_tensor_descriptor__sum_check_result(gc_core_tensor_descriptor_t *a, i64_t dim,\n                                                gc_core_tensor_descriptor_t *result,\n                                                gc_machine_t *ctx);\n\n// Compute output shape for sum\nbool gc_core_tensor_descriptor__sum_size(gc_core_tensor_descriptor_t *a, i64_t dim,\n                                        gc_core_tensor_descriptor_t *result,\n                                        gc_machine_t *ctx);\n```\n\n### Tensor Utility Functions\n\n```c\n// Clone and reset operations\nvoid gc_core_tensor__reset_internal(gc_core_tensor_t *self);\nvoid gc_core_tensor__clone_internal(gc_core_tensor_t *dst, gc_core_tensor_t *src);\n\n// Compute difference between tensors\nf64_t gc_core_tensor__diff(gc_core_tensor_t *t1, gc_core_tensor_t *t2, gc_machine_t *ctx);\n\n// Print tensor for debugging\nvoid gc_core_tensor__print(gc_core_tensor_t *self, const char *name);\n```\n\n---\n\n## Tensor Operation Examples\n\n### Example 1: Element-wise Addition (from add.c)\n\n```c\nstatic void tensor_add_f64(f64_t **inputs,\n                          gc_core_tensor_descriptor_t **inputs_desc,\n                          f64_t **outputs) {\n    const i64_t N = inputs_desc[0]->size;\n    const f64_t *A = inputs[0];\n    const f64_t *B = inputs[1];\n    f64_t *C = outputs[0];\n\n    for (i64_t i = 0; i < N; i++) {\n        C[i] = A[i] + B[i];\n    }\n}\n```\n\n### Example 2: Element-wise Multiplication (from mul.c)\n\n```c\nstatic void tensor_mul_f64(f64_t **inputs,\n                          gc_core_tensor_descriptor_t **inputs_desc,\n                          f64_t **outputs) {\n    const i64_t N = inputs_desc[0]->size;\n    const f64_t *A = inputs[0];\n    const f64_t *B = inputs[1];\n    f64_t *C = outputs[0];\n\n    for (i64_t i = 0; i < N; i++) {\n        C[i] = A[i] * B[i];\n    }\n}\n\n// Backward pass with gradient accumulation\nstatic void tensor_mul_backward_f64(f64_t **inputs,\n                                   f64_t **inputs_grad,\n                                   gc_core_tensor_descriptor_t **inputs_desc,\n                                   f64_t **outputs_grad) {\n    const i64_t N = inputs_desc[0]->size;\n    f64_t *dA = inputs_grad[0];\n    f64_t *dB = inputs_grad[1];\n    const f64_t *A = inputs[0];\n    const f64_t *B = inputs[1];\n    const f64_t *dC = outputs_grad[0];\n\n    if (dA != NULL && dB != NULL) {\n        // Both gradients needed\n        for (i64_t i = 0; i < N; i++) {\n            dA[i] += B[i] * dC[i];  // ∂L/∂A = B ⊙ ∂L/∂C\n            dB[i] += A[i] * dC[i];  // ∂L/∂B = A ⊙ ∂L/∂C\n        }\n    }\n}\n```\n\n### Example 3: ReLU Activation (from relu.c)\n\n```c\nstatic void tensor_relu_forward_f64(f64_t **inputs,\n                                   gc_core_tensor_descriptor_t **inputs_desc,\n                                   f64_t **outputs) {\n    const i64_t N = inputs_desc[0]->size;\n    const f64_t *X = inputs[0];\n    f64_t *Y = outputs[0];\n\n    for (i64_t i = 0; i < N; i++) {\n        Y[i] = (X[i] >= 0) ? X[i] : 0.0;  // max(0, x)\n    }\n}\n\nstatic void tensor_relu_backward_f64(f64_t **inputs,\n                                    f64_t **inputs_grad,\n                                    gc_core_tensor_descriptor_t **inputs_desc,\n                                    f64_t **outputs_grad) {\n    const i64_t N = inputs_desc[0]->size;\n    f64_t *dX = inputs_grad[0];\n    const f64_t *X = inputs[0];\n    const f64_t *dY = outputs_grad[0];\n\n    if (dX != NULL) {\n        for (i64_t i = 0; i < N; i++) {\n            if (X[i] >= 0) {\n                dX[i] += dY[i];  // Gradient flows through if x > 0\n            }\n            // else: gradient is zero (derivative of 0)\n        }\n    }\n}\n```\n\n### Example 4: Sigmoid Activation (from sigmoid.c)\n\n```c\nstatic void tensor_sigmoid_forward_f64(f64_t **inputs,\n                                      gc_core_tensor_descriptor_t **inputs_desc,\n                                      f64_t **outputs) {\n    const i64_t N = inputs_desc[0]->size;\n    const f64_t *X = inputs[0];\n    f64_t *Y = outputs[0];\n\n    for (i64_t i = 0; i < N; i++) {\n        Y[i] = 1.0 / (1.0 + exp(-X[i]));  // σ(x) = 1/(1+e^(-x))\n    }\n}\n\nstatic void tensor_sigmoid_backward_f64(f64_t **inputs,\n                                       f64_t **inputs_grad,\n                                       gc_core_tensor_descriptor_t **inputs_desc,\n                                       f64_t **outputs,\n                                       f64_t **outputs_grad) {\n    const i64_t N = inputs_desc[0]->size;\n    f64_t *dX = inputs_grad[0];\n    const f64_t *Y = outputs[0];  // Use cached output\n    const f64_t *dY = outputs_grad[0];\n\n    if (dX != NULL) {\n        for (i64_t i = 0; i < N; i++) {\n            // σ'(x) = σ(x) * (1 - σ(x))\n            dX[i] += Y[i] * (1.0 - Y[i]) * dY[i];\n        }\n    }\n}\n```\n\n### Example 5: Softmax (from softmax.c)\n\n```c\nstatic void tensor_softmax_forward_f64(f64_t **inputs,\n                                      gc_core_tensor_descriptor_t **inputs_desc,\n                                      f64_t **outputs) {\n    // Compute softmax over last dimension\n    const i64_t N = inputs_desc[0]->size / inputs_desc[0]->dim[inputs_desc[0]->nb_dim - 1];\n    const i64_t M = inputs_desc[0]->dim[inputs_desc[0]->nb_dim - 1];\n    const f64_t *X = inputs[0];\n    f64_t *Y = outputs[0];\n\n    for (i64_t i = 0; i < N; i++) {\n        i64_t offset = i * M;\n\n        // Find max for numerical stability\n        f64_t max_val = X[offset];\n        for (i64_t j = 1; j < M; j++) {\n            if (X[offset + j] > max_val) {\n                max_val = X[offset + j];\n            }\n        }\n\n        // Compute exp(x - max) and sum\n        f64_t sum = 0.0;\n        for (i64_t j = 0; j < M; j++) {\n            f64_t exp_val = exp(X[offset + j] - max_val);\n            Y[offset + j] = exp_val;\n            sum += exp_val;\n        }\n\n        // Normalize by sum\n        for (i64_t j = 0; j < M; j++) {\n            Y[offset + j] /= sum;\n        }\n    }\n}\n```\n\n### Example 6: Matrix Multiplication (from matmul.c)\n\n```c\nstatic void tensor_matmul_f64(f64_t **inputs,\n                             gc_core_tensor_descriptor_t **inputs_desc,\n                             f64_t **outputs,\n                             gc_core_tensor_descriptor_t **outputs_desc,\n                             const gc_slot_t *constants) {\n    i8_t dim_a = inputs_desc[0]->nb_dim;\n    i8_t dim_b = inputs_desc[1]->nb_dim;\n    i8_t dim_res = outputs_desc[0]->nb_dim;\n\n    bool transpose_a = constants[0].b;\n    bool transpose_b = constants[1].b;\n    f64_t alpha = constants[2].f64;\n    f64_t beta = constants[3].f64;\n\n    if (dim_res == 2) {\n        // Simple 2D matrix multiplication: C = alpha * A @ B + beta * C\n        i64_t M = outputs_desc[0]->dim[0];\n        i64_t N = outputs_desc[0]->dim[1];\n        i64_t K = transpose_a ? inputs_desc[0]->dim[0] : inputs_desc[0]->dim[1];\n\n        gc_compute_ops__matmul_f64(\n            transpose_a, transpose_b, M, N, K, alpha,\n            (f64_t *)inputs[0], gc_core_tensor_descriptor__leading_dim(inputs_desc[0]),\n            (f64_t *)inputs[1], gc_core_tensor_descriptor__leading_dim(inputs_desc[1]),\n            beta,\n            (f64_t *)outputs[0], gc_core_tensor_descriptor__leading_dim(outputs_desc[0])\n        );\n    } else {\n        // Batched matrix multiplication\n        i64_t M = outputs_desc[0]->dim[dim_res - 2];\n        i64_t N = outputs_desc[0]->dim[dim_res - 1];\n        i64_t K = transpose_a ? inputs_desc[0]->dim[dim_a - 2] : inputs_desc[0]->dim[dim_a - 1];\n\n        i64_t stepA = (dim_a == 2) ? 0 : M * K;\n        i64_t stepB = (dim_b == 2) ? 0 : N * K;\n        i64_t stepC = M * N;\n\n        i64_t batch_count = (dim_a >= dim_b)\n            ? gc_core_tensor_descriptor__matrix_count(inputs_desc[0])\n            : gc_core_tensor_descriptor__matrix_count(inputs_desc[1]);\n\n        gc_compute_ops__matmul_tensor_f64(\n            transpose_a, transpose_b, M, N, K, alpha,\n            (f64_t *)inputs[0], gc_core_tensor_descriptor__leading_dim(inputs_desc[0]),\n            (f64_t *)inputs[1], gc_core_tensor_descriptor__leading_dim(inputs_desc[1]),\n            beta,\n            (f64_t *)outputs[0], gc_core_tensor_descriptor__leading_dim(outputs_desc[0]),\n            stepA, stepB, stepC, batch_count\n        );\n    }\n}\n```\n\n### Example 7: Dense Layer Compilation (from dense_layer.c)\n\n```c\n// Create weight matrix and matmul operation for dense layer\ni64_t weight_dims[2] = {inputs, outputs};\nu32_t weight_var_offset = gc_compute_engine__compile_add_varo(\n    self, layer_name_offset, weight_name_offset,\n    weight_dims, 2, inputs * outputs,\n    weight_l1, weight_l2, init_w_p1, init_w_p2, init_w_p3, init_w_p4, init_w_p5,\n    tensor_type, weight_initializer\n);\n\n// Create matmul operation: output = input @ weights\nu32_t op_offset = gc_compute_ComputeEngine__create_op(self);\ngc_compute_engine_op_t *op = (self->ops.data + op_offset);\nop->params_offset = self->constants.size;\nop->code = gc_compute_operation_code_matmul;\n\n// Add operation parameters\ngc_compute_engine__compile_add_const(self, (gc_slot_t){.u32 = input_var_offset});\ngc_compute_engine__compile_add_const(self, (gc_slot_t){.u32 = weight_var_offset});\ngc_compute_engine__compile_add_const(self, (gc_slot_t){.u32 = output_var_offset});\ngc_compute_engine__compile_add_const(self, (gc_slot_t){.b = false});  // transpose_a\ngc_compute_engine__compile_add_const(self, (gc_slot_t){.b = false});  // transpose_b\ngc_compute_engine__compile_add_const(self, (gc_slot_t){.f64 = 1.0});  // alpha\ngc_compute_engine__compile_add_const(self, (gc_slot_t){.f64 = 0.0});  // beta\n\nop->nb_inputs = 2;\nop->nb_outputs = 1;\nop->nb_params = 4;\n```\n\n---\n\n## Array API\n\nArrays are dynamic collections of elements.\n\n```c\n// Set element at offset\nbool gc_core_array__set_slot(gc_core_array_t *self, u32_t offset, gc_slot_t value, gc_type_t type, gc_machine_t *ctx);\n\n// Get element at offset\nbool gc_core_array__get_slot(const gc_core_array_t *self, u32_t offset, gc_slot_t *value, gc_type_t *type);\n\n// Add element to end\nbool gc_core_array__add_slot(gc_core_array_t *self, gc_slot_t value, gc_type_t value_type, gc_machine_t *ctx);\n\n// Remove all elements\nvoid gc_core_array__remove_all(gc_core_array_t *self, gc_machine_t *ctx);\n\n// Get array size\nu32_t gc_core_array__size(const gc_core_array_t *self);\n```\n\n---\n\n## Table API\n\nTables are 2D data structures with rows and columns.\n\n```c\n// Initialize with capacity\nvoid gc_core_table__init(gc_core_table_t *table, u32_t capacity);\n\n// Get dimensions\nu32_t gc_core_table__nb_cols(gc_core_table_t *self);\nu32_t gc_core_table__nb_rows(gc_core_table_t *self);\n\n// Get/set cell value\nbool gc_core_table__get_cell_value(const gc_core_table_t *self, i64_t row, i64_t col, gc_slot_t *value, gc_type_t *type);\nvoid gc_core_table__set_cell_value(gc_core_table_t *self, i64_t row, i64_t col, gc_slot_t value, gc_type_t value_type, gc_machine_t *ctx);\n```\n\n---\n\n## Buffer API\n\nBuffers are dynamic byte arrays with helper functions for building strings and serializing data.\n\n### Basic Operations\n\n```c\n// Create and destroy\ngc_buffer_t *gc_buffer__create();\nvoid gc_buffer__finalize(gc_buffer_t *self);\n\n// Clear buffer\nvoid gc_buffer__clear(gc_buffer_t *self);\nvoid gc_buffer__clear_secure(gc_buffer_t *self);  // Overwrites with zeros\n\n// Get buffer properties\nchar *gc_buffer__data(gc_buffer_t *self);\nu64_t gc_buffer__size(gc_buffer_t *self);\nu64_t gc_buffer__capacity(gc_buffer_t *self);\n```\n\n### Adding Data\n\n```c\n// Add strings\nvoid gc_buffer__add_str(gc_buffer_t *self, const char *c, u32_t len);\nvoid gc_buffer__add_cstr(gc_buffer_t *self, const char *c);  // Null-terminated\nvoid gc_buffer__prepend_str(gc_buffer_t *self, const char *c, u32_t len);\n\n// Add characters\nvoid gc_buffer__add_char(gc_buffer_t *self, char c);\nvoid gc_buffer__add_escaped_char(gc_buffer_t *self, char c);\nvoid gc_buffer__add_escaped_str(gc_buffer_t *self, const char *str, u32_t str_len);\n\n// Add numbers\nvoid gc_buffer__add_u64(gc_buffer_t *self, u64_t i);\nvoid gc_buffer__add_f64(gc_buffer_t *self, f64_t f);\n\n// Add slots and objects\nvoid gc_buffer__add_slot(gc_buffer_t *self, gc_slot_t slot, gc_type_t type, const gc_program_t *prog);\nvoid gc_buffer__add_slot_as_json(gc_buffer_t *self, gc_slot_t slot, gc_type_t type, const gc_program_t *prog);\n```\n\n### Path and URL Operations\n\n```c\n// Path utilities\nvoid gc_buffer__add_path_sep(gc_buffer_t *self);\nvoid gc_buffer__add_cwd(gc_buffer_t *self);\n\n// URL encoding\nvoid gc_buffer__add_uri_encoded(gc_buffer_t *self, const char *c, u32_t len);\nvoid gc_buffer__add_queryparams(gc_buffer_t *buf, const char *query_data, u32_t query_len);\n```\n\n---\n\n## String API\n\n```c\n// Create string from buffer\ngc_core_string_t *gc_core_string__create_from(const char *str, u64_t len);\n\n// Create from buffer or use existing symbol\ngc_core_string_t *gc_core_string__create_from_or_symbol(const gc_program_t *prog, const char *str, u64_t len);\n\n// Create from gc_buffer_t\ngc_core_string_t *gc_core_string__create_from_buffer(const gc_buffer_t *buf);\n\n// Get string data\nconst char *gc_core_string__buffer(const gc_core_string_t *str);\nu32_t gc_core_string__size(const gc_core_string_t *str);\n\n// Check if string is a literal symbol\nbool gc_core_string__is_lit(const gc_core_string_t *str);\n```\n\n---\n\n## Memory Allocation\n\n### GNU-style Allocator\n\n```c\n// GNU-style malloc/free (per-worker)\nvoid *gc_gnu_malloc(size_t size);\nvoid gc_gnu_free(void *ptr);\nvoid *gc_gnu_calloc(size_t count, size_t size);\nvoid *gc_gnu_realloc(void *ptr, size_t new_size);\nsize_t gc_gnu_alloc_size(void *ptr);\n\n// Global allocator (shared across workers)\nvoid *gc_global_gnu_malloc(size_t size);\nvoid *gc_global_gnu_calloc(size_t count, size_t size);\nvoid *gc_global_gnu_realloc(void *ptr, size_t new_size);\nvoid gc_global_gnu_free(void *ptr);\n```\n\n---\n\n## Program and Type System\n\n### Program Access\n\n```c\n// Resolve symbols\nu32_t gc_program__resolve_symbol(const gc_program_t *program, const char *str, u32_t len);\nu32_t gc_program__resolve_module(const gc_program_t *program, u32_t mod_name_offset);\nu32_t gc_program__resolve_type(const gc_program_t *prog, u32_t mod_offset, u32_t type_name_off);\n\n// Get symbol by offset\ngc_program_symbol_t *gc_program__get_symbol(const gc_program_t *program, u32_t symb_off);\nu32_t gc_program__get_symbol_off(const gc_program_symbol_t *symb);\n```\n\n### Type Information\n\n```c\n// Get type information\ngc_program_type_t *gc_program__get_program_type(const gc_program_t *prog, u32_t type_id);\n\n// Type fields\ngc_program_type_field_t *gc_program_type__get_field(const gc_program_type_t *program_type, u32_t field_offset);\ngc_program_type_field_t *gc_program_type__get_field_by_key(const gc_program_type_t *program_type, u32_t key);\nu32_t gc_program_type__nb_fields(const gc_program_type_t *program_type);\n\n// Generic type information\nu32_t gc_program_type__get_g1_type_id(const gc_program_type_t *program_type);\nu32_t gc_program_type__get_g2_type_id(const gc_program_type_t *program_type);\n```\n\n### Type Configuration (for library authors)\n\n```c\n// Configure native type with finalizer\nvoid gc_program_type__configure(gc_program_t *prog, u32_t type_id, u32_t header_bytes,\n                                gc_object_type_native_finalize_t *native_finalize);\n```\n\n**Example: Type configuration (from algebra.c)**\n\n```c\nbool gc_lib_algebra__link_native(gc_program_t *prog, gc_program_library_t *lib) {\n    gc_program_library__set_lib_hooks(lib, gc_lib_algebra__start, gc_lib_algebra__stop);\n\n    // Configure custom types with finalizers\n    gc_program_type__configure(prog, gc_compute_ComputeState,\n                              sizeof(gc_compute_state_t),\n                              gc_compute_ComputeState__finalize);\n    gc_program_type__configure(prog, gc_compute_ComputeEngine,\n                              sizeof(gc_compute_engine_t),\n                              gc_compute_ComputeEngine__finalize);\n    return true;\n}\n```\n\n### Function Information\n\n```c\n// Get function count\nu32_t gc_program__nb_functions(const gc_program_t *prog);\n\n// Get function details\ngc_program_function_t *gc_program__get_function(const gc_program_t *prog, u32_t fn_off);\nbool gc_program_function__is_native_without_body(const gc_program_t *prog, u32_t fn_off);\nvoid gc_program_function__set_body(const gc_program_t *prog, u32_t fn_off, gc_program_function_body_t *fn_body);\n\n// Function parameters\nu8_t gc_program_function__nb_params(const gc_program_function_t *fn);\nbool gc_program_function__get_param_by_off(const gc_program_function_t *fn, u32_t offset, gc_function_param_t *out);\n```\n\n### Linking Native Functions\n\n```c\n// Link module function\nbool gc_program__link_mod_fn(const gc_program_t *prg, u32_t module_id,\n                            gc_program_function_body_t *function, u32_t fn_name_symbol);\n\n// Link type method\nbool gc_program__link_type_fn(const gc_program_t *prg, u32_t type_id,\n                             gc_program_function_body_t *function, u32_t fn_name_symbol);\n```\n\n### Library Hooks\n\n```c\n// Set library lifecycle hooks\nvoid gc_program_library__set_lib_hooks(gc_program_library_t *lib,\n                                       gc_hook_function_t *lib_start_hook,\n                                       gc_hook_function_t *lib_stop_hook);\n\n// Set worker lifecycle hooks\nvoid gc_program_library__set_worker_hooks(gc_program_library_t *lib,\n                                         gc_hook_function_t *worker_start_hook,\n                                         gc_hook_function_t *worker_stop_hook);\n```\n\n---\n\n## Host and Task Management\n\n### Task Management\n\n```c\ntypedef enum {\n    gc_task_status_empty = 0,\n    gc_task_status_waiting = 1,\n    gc_task_status_running = 2,\n    gc_task_status_await = 3,\n    gc_task_status_cancelled = 4,\n    gc_task_status_error = 5,\n    gc_task_status_ended = 6,\n    gc_task_status_ended_with_errors = 7,\n} gc_task_status_t;\n\n// Get global host\ngc_host_t *gc_host__get_global();\n\n// Spawn task without arguments\nbool gc_host__spawn_task(gc_host_t *self, u32_t fn_off, u32_t user_id,\n                        u64_t roles_flags, i64_t *created_task_id);\n\n// Get task status\nbool gc_host__get_task_status(gc_host_t *self, u32_t task_id, gc_task_status_t *status);\n\n// Cancel task\nbool gc_host__cancel_task(gc_host_t *self, u32_t task_id);\n```\n\n---\n\n## HTTP Client\n\n### HTTP Request Structure\n\n```c\ntypedef enum gc_http_method {\n    gc_http_method_get = 0,\n    gc_http_method_post = 2,\n    gc_http_method_put = 3,\n    gc_http_method_delete = 4,\n} gc_http_method_t;\n\ntypedef struct gc_http_request {\n    gc_http_method_t method;\n    bool use_ssl;\n    i32_t port;\n    struct { const char *data; u32_t len; } host;\n    struct { const char *data; u32_t len; } path;\n    struct { const char *data; u32_t len; } query;\n    gc_buffer_t *headers;\n    u16_t status_code;\n} gc_http_request_t;\n```\n\n### HTTP Operations\n\n```c\n// Parse URL into request\nvoid gc_http_request__parse(gc_http_request_t *req, const char *url, u32_t url_len);\n\n// Execute HTTP request\nbool gc_http_request__call(gc_http_request_t *req, bool *json_detected,\n                          gc_buffer_t *buf, const gc_program_t *prog);\n\n// Add headers\nu32_t gc_http_request__add_header(gc_http_request_t *req,\n                                  const char *name, u32_t name_len,\n                                  const char *value, u32_t value_len);\n\n// Clear and finalize\nvoid gc_http_request__clear(gc_http_request_t *req);\nvoid gc_http_request__finalize(gc_http_request_t *req);\n```\n\n---\n\n## Cryptography\n\n### SHA-256\n\n```c\n#define gc_crypto_sha256_len 32\n\ntypedef struct gc_crypto_sha256 {\n    union {\n        u32_t u32[8];\n        unsigned char u8[gc_crypto_sha256_len];\n    } u;\n} gc_crypto_sha256_t;\n\n// Compute SHA-256 hash\nvoid gc_crypto_sha256(gc_crypto_sha256_t *sha, const void *p, size_t size);\n```\n\n### HMAC-SHA256\n\n```c\n// Compute HMAC-SHA256\nvoid gc_crypto_hmac_sha256(gc_crypto_hmac_sha256_ctx_t *ctx,\n                          gc_crypto_hmac_sha256_t *hmac,\n                          const void *k, size_t ksize,\n                          const void *d, size_t dsize);\n```\n\n### Hex Encoding/Decoding\n\n```c\n// Convert hex string to binary\nbool gc_common__hex2bin(char *dest, const char *src, u32_t len);\nu32_t gc_common__hex2bin_len(u32_t len);\n\n// Convert binary to hex string\nvoid gc_common__bin2hex(char *dest, const char *src, u32_t len);\nu32_t gc_common__bin2hex_len(u32_t len);\n```\n\n---\n\n## Utility Functions\n\n### Parsing\n\n```c\n// Parse unsigned integer\nu64_t gc_common__parse_number(const char *str, u32_t *str_len);\n\n// Parse signed integer\ni64_t gc_common__parse_sign_number(const char *str, u32_t *str_len);\n\n// Parse ISO 8601 date\nbool gc_common__parse_date_iso8601(char *data, u32_t data_len, i64_t *epoch_utc);\n```\n\n### Time\n\n```c\n// Get current time in microseconds\ni64_t gc_common__current_us();\n```\n\n### JSON Parsing\n\n```c\n// Parse JSON string into a value\nbool gc_json__parse(gc_machine_t *ctx, char *str, u32_t str_len,\n                   gc_slot_t *result, gc_type_t *result_type, u32_t type_d);\n```\n\n---\n\n## I/O Operations\n\n```c\n// Open file read/write\ni32_t gc_io_file__open_rdwr(gc_core_string_t *path, gc_machine_t *ctx);\n\n// Open file read-only\ni32_t gc_io_file__open_read(gc_core_string_t *path, gc_machine_t *ctx);\n\n// Sync file to disk\nvoid gc_io_file__sync(i32_t fp);\n```\n\n---\n\n## Common Patterns\n\n### Error Handling Pattern\n\n```c\nvoid safe_operation(gc_machine_t *ctx) {\n    // Validate parameters\n    if (gc_machine__get_param_nb(ctx) < 2) {\n        gc_machine__set_runtime_error(ctx, \"Expected 2 parameters\");\n        return;\n    }\n\n    // Get parameters with type checking\n    gc_slot_t param0 = gc_machine__get_param(ctx, 0);\n    gc_type_t type0 = gc_machine__get_param_type(ctx, 0);\n\n    if (type0 != gc_type_int) {\n        gc_machine__set_runtime_error(ctx, \"Parameter 0 must be int\");\n        return;\n    }\n\n    // Perform operation...\n    // Check for errors\n    if (gc_machine__error(ctx)) {\n        return;  // Error already set\n    }\n\n    // Set result\n    gc_machine__set_result(ctx, result, result_type);\n}\n```\n\n### Tensor Operation Pattern (from sigmoid.c)\n\n```c\nstatic void gc_compute_op_sigmoid__fwd_f64(gc_unused gc_compute_engine_t *engine,\n                                           f64_t **inputs,\n                                           gc_core_tensor_descriptor_t **inputs_desc,\n                                           f64_t **outputs,\n                                           gc_unused gc_core_tensor_descriptor_t **outputs_desc,\n                                           gc_unused const gc_slot_t *constants,\n                                           gc_unused char *workspace) {\n    const i64_t N = inputs_desc[0]->size;\n    const f64_t *X = inputs[0];\n    f64_t *Y = outputs[0];\n\n    for (i64_t i = 0; i < N; i++) {\n        Y[i] = 1.0 / (1.0 + exp(-X[i]));\n    }\n}\n```\n\n### Computing Engine Pattern (from dense_layer.c)\n\n```c\n// Create operation\nu32_t new_op_offset = gc_compute_ComputeEngine__create_op(self);\ngc_compute_engine_op_t *op = (self->ops.data + new_op_offset);\nop->params_offset = self->constants.size;\nop->code = gc_compute_operation_code_matmul;\n\n// Add parameters as constants\ngc_compute_engine__compile_add_const(self, (gc_slot_t){.u32 = input_var_offset});\ngc_compute_engine__compile_add_const(self, (gc_slot_t){.u32 = weight_var_offset});\ngc_compute_engine__compile_add_const(self, (gc_slot_t){.u32 = output_var_offset});\ngc_compute_engine__compile_add_const(self, (gc_slot_t){.b = false});  // transpose_a\ngc_compute_engine__compile_add_const(self, (gc_slot_t){.b = false});  // transpose_b\ngc_compute_engine__compile_add_const(self, (gc_slot_t){.f64 = 1.0});  // alpha\ngc_compute_engine__compile_add_const(self, (gc_slot_t){.f64 = 0.0});  // beta\n\nop->nb_inputs = 2;\nop->nb_outputs = 1;\nop->nb_params = 4;\n```\n",
        "plugins/greycat-c/skills/greycat-c/references/standard_library.md": "# GreyCat Standard Library (std)\n\n## Overview\n\nThe GreyCat Standard Library provides essential data structures, I/O operations, runtime features, and utilities for GCL applications. The library is organized into four modules:\n\n- **core** - Fundamental types and data structures\n- **runtime** - Scheduler, tasks, logging, security, system operations\n- **io** - File I/O, CSV/JSON/XML, HTTP client, email\n- **util** - Collections, statistics, quantizers, cryptography\n\n## Table of Contents\n\n### Modules\n- [std::core - Core Types](#core-module-stdcore)\n  - [Fundamental Types](#fundamental-types) - Tuple, Date, Error\n  - [Geospatial Types](#geospatial-types) - GeoCircle, GeoPoly, GeoBox\n  - [Enumerations](#enumerations) - FloatPrecision, CalendarUnit, TensorType, etc.\n\n- [std::runtime - Runtime Features](#runtime-module-stdruntime)\n  - [Scheduler & Task Automation](#scheduler---task-automation)\n  - [Task & Job Management](#task--job-management)\n  - [Logging](#logging)\n  - [System Information](#system-information)\n  - [Security](#security) - User, UserGroup, SecurityPolicy, OpenIDConnect\n  - [License Management](#license-management)\n  - [OpenAPI Integration](#openapi-integration)\n  - [Model Context Protocol (MCP)](#model-context-protocol-mcp)\n\n- [std::io - Input/Output](#io-module-stdio)\n  - [Binary I/O](#binary-io) - GcbWriter, GcbReader\n  - [Text I/O](#text-io) - TextWriter, TextReader\n  - [JSON I/O](#json-io) - JsonWriter, JsonReader\n  - [CSV I/O](#csv-io) - CsvWriter, CsvReader, CsvAnalyzer\n  - [File System](#file-system) - File, FileWalker\n  - [HTTP Client](#http-client) - Http, Url\n  - [Email](#email) - Email, Smtp\n  - [XML I/O](#xml-io) - XmlReader\n\n- [std::util - Utilities](#util-module-stdutil)\n  - [Collections](#collections) - Queue, Stack, SlidingWindow, TimeWindow\n  - [Statistics](#statistics) - Gaussian, Histogram, GaussianProfile\n  - [Quantizers](#quantizers) - Linear, Log, Custom, Multi\n  - [Utilities](#utilities) - Random, Assert, ProgressTracker, Crypto, Plot\n\n### Additional Sections\n- [Usage Guidelines](#usage-guidelines)\n- [Best Practices](#best-practices)\n- [Common Patterns](#common-patterns)\n\n---\n\n## Core Module (std::core)\n\n### Fundamental Types\n\n#### Tuple<T, U>\nTwo-element tuple for pairing values of potentially different types.\n```gcl\nvar pair = Tuple<String, int> { first: \"count\", second: 42 };\nprintln(\"${pair.first}: ${pair.second}\");\n```\n\n#### Date\nDate/time representation with calendar operations.\n```gcl\nvar now = Date::now();\nvar yesterday = now - 1day;\nvar next_week = now + 7day;\n```\n\n#### Error & ErrorFrame\nError handling with stack traces.\n```gcl\ntype Error {\n  code: ErrorCode;\n  message: String;\n  stack: Array<ErrorFrame>;\n}\n```\n\n### Geospatial Types\n\n#### GeoCircle\nCircular geographic region defined by center point and radius.\n```gcl\nvar zone = GeoCircle {\n  lat: 45.5,\n  lng: -73.6,\n  radius: 5000.0  // meters\n};\n```\n\n#### GeoPoly\nPolygon geographic region.\n```gcl\nvar boundary = GeoPoly {\n  points: [\n    GeoPoint { lat: 45.5, lng: -73.6 },\n    GeoPoint { lat: 45.6, lng: -73.5 },\n    GeoPoint { lat: 45.4, lng: -73.4 }\n  ]\n};\n```\n\n#### GeoBox\nRectangular bounding box for geographic queries.\n```gcl\nvar bbox = GeoBox {\n  south_west: GeoPoint { lat: 45.4, lng: -73.7 },\n  north_east: GeoPoint { lat: 45.6, lng: -73.5 }\n};\n```\n\n### Enumerations\n\n#### FloatPrecision\nPrecision levels for floating-point calculations: `p1`, `p10`, `p100`, `p1000`, etc.\n\n#### CalendarUnit\nTime units for date operations: `year`, `month`, `week`, `day`, `hour`, `minute`, `second`.\n\n#### DurationUnit\nDuration units: `us` (microseconds), `ms`, `s`, `min`, `hour`, `day`.\n\n#### TensorType\nTensor data types: `i8`, `i16`, `i32`, `i64`, `u8`, `u16`, `u32`, `u64`, `f32`, `f64`, `c64`, `c128`.\n\n#### TensorDistance\nDistance metrics for tensors: `euclidean`, `cosine`, `manhattan`, `hamming`.\n\n## Runtime Module (std::runtime)\n\n### Scheduler - Task Automation\n\nThe scheduler manages recurring tasks with various periodicity options.\n\n#### Basic Usage\n```gcl\nfn backup_database() {\n  // Backup logic...\n}\n\nfn schedule_backups() {\n  Scheduler::add(\n    backup_database,\n    DailyPeriodicity { hour: 2 },  // Run at 2 AM daily\n    null\n  );\n}\n```\n\n#### Periodicity Types\n\n**FixedPeriodicity** - Run every N time units\n```gcl\nFixedPeriodicity { every: 5min }\nFixedPeriodicity { every: 1hour }\n```\n\n**DailyPeriodicity** - Run at specific time each day\n```gcl\nDailyPeriodicity { hour: 14, minute: 30 }  // 2:30 PM daily\n```\n\n**WeeklyPeriodicity** - Run on specific day/time each week\n```gcl\nWeeklyPeriodicity {\n  day: DayOfWeek::monday,\n  hour: 9,\n  minute: 0\n}\n```\n\n**MonthlyPeriodicity** - Run on specific day/time each month\n```gcl\nMonthlyPeriodicity { day: 1, hour: 0, minute: 0 }  // First of month\n```\n\n**YearlyPeriodicity** - Run on specific date/time each year\n```gcl\nYearlyPeriodicity {\n  month: Month::january,\n  day: 1,\n  hour: 0\n}\n```\n\n#### Advanced Scheduling\n```gcl\nfn health_check() {\n  // Check system health...\n}\n\nScheduler::add(\n  health_check,\n  FixedPeriodicity { every: 5min },\n  PeriodicOptions {\n    start: time::now() + 1hour,    // Start in 1 hour\n    max_duration: 30s,              // Timeout after 30 seconds\n  }\n);\n```\n\n#### Managing Tasks\n```gcl\n// Find task\nvar ptask = Scheduler::find(health_check);\nif (ptask != null) {\n  println(\"Runs every ${ptask.periodicity.every}\");\n}\n\n// Control execution\nScheduler::deactivate(backup_database);  // Pause\nScheduler::activate(backup_database);    // Resume\nScheduler::remove(backup_database);      // Delete\n\n// List all tasks\nvar all_tasks = Scheduler::list();\nfor (_, task in all_tasks) {\n  println(\"${task.function}: active=${task.is_active}\");\n}\n```\n\n### Task & Job Management\n\n#### Task\nAsynchronous task execution with status tracking.\n```gcl\nenum TaskStatus { running, completed, failed, cancelled }\n\ntype Task {\n  id: String;\n  status: TaskStatus;\n  progress: float?;\n  error: Error?;\n}\n```\n\n#### Job<T>\nGeneric job container for asynchronous operations.\n```gcl\nvar job = Job<Result> {};\n// Submit for processing...\n```\n\n### Logging\n\n#### Log\nStructured logging with severity levels.\n```gcl\nenum LogLevel { trace, debug, info, warn, error, fatal }\n\nLog::info(\"Application started\");\nLog::error(\"Failed to connect: ${error_message}\");\nLog::debug(\"Processing item ${id}\");\n```\n\n#### LogDataUsage\nTrack data usage metrics.\n```gcl\nvar usage = LogDataUsage {\n  bytes_read: 1024000,\n  bytes_written: 512000,\n  timestamp: time::now()\n};\n```\n\n### System Information\n\n#### Runtime\nQuery runtime environment information.\n```gcl\nvar info = RuntimeInfo {};\nprintln(\"Platform: ${info.platform}\");\nprintln(\"Version: ${info.version}\");\n```\n\n#### System\nSystem-level operations and utilities.\n```gcl\nvar sys_info = System::info();\nprintln(\"CPU cores: ${sys_info.cpu_count}\");\n```\n\n#### ChildProcess\nExecute external processes.\n```gcl\ntype ChildProcess {\n  command: String;\n  args: Array<String>?;\n  env: Map<String, String>?;\n}\n\ntype ChildProcessResult {\n  exit_code: int;\n  stdout: String;\n  stderr: String;\n}\n```\n\n### Security\n\n#### User & UserGroup\nUser and group management.\n```gcl\ntype User extends SecurityEntity {\n  username: String;\n  email: String;\n  groups: Array<UserGroup>;\n}\n\ntype UserGroup extends SecurityEntity {\n  name: String;\n  policies: Array<UserGroupPolicy>;\n}\n```\n\n#### SecurityPolicy\nAccess control and permissions.\n```gcl\ntype SecurityPolicy {\n  resource: String;\n  actions: Array<String>;\n  allow: bool;\n}\n```\n\n#### OpenIDConnect\nOpenID Connect authentication.\n```gcl\ntype OpenIDConnect {\n  issuer: String;\n  client_id: String;\n  client_secret: String;\n  redirect_uri: String;\n}\n```\n\n### License Management\n\n```gcl\nenum LicenseType { trial, commercial, enterprise, opensource }\n\ntype License {\n  type: LicenseType;\n  valid_until: time;\n  features: Array<String>;\n}\n```\n\n### OpenAPI Integration\n\n```gcl\ntype OpenApi {\n  title: String;\n  version: String;\n  paths: Map<String, any>;\n}\n```\n\n### Model Context Protocol (MCP)\n\nTypes for MCP server integration:\n- `McpInitializeParams`, `McpInitializeResult`\n- `McpServerCapabilities`, `McpClientCapabilities`\n- `McpTool`, `McpToolsListParams`, `McpToolsCallParams`\n- `McpTextContent`, `McpImageContent`, `McpAudioContent`\n- `McpPriority`, `McpRole`, `McpContentType`\n\n## I/O Module (std::io)\n\n### Binary I/O\n\n#### GcbWriter<T> / GcbReader<T>\nBinary serialization using GreyCat's ABI encoding.\n```gcl\n// Write\nvar writer = GcbWriter<MyType> { path: \"/data/output.gcb\" };\nwriter.write(my_object);\nwriter.flush();\n\n// Read\nvar reader = GcbReader<MyType> { path: \"/data/output.gcb\" };\nwhile (reader.can_read()) {\n  var obj = reader.read();\n  process(obj);\n}\n```\n\n### Text I/O\n\n#### TextWriter<T> / TextReader\nUTF-8 text file operations.\n```gcl\n// Write\nvar writer = TextWriter<String> { path: \"/logs/output.txt\" };\nwriter.writeln(\"Line 1\");\nwriter.writeln(\"Line 2\");\nwriter.flush();\n\n// Read\nvar reader = TextReader { path: \"/logs/output.txt\" };\nwhile (reader.can_read()) {\n  var line = reader.read();\n  println(line);\n}\n```\n\n### JSON I/O\n\n#### JsonWriter<T> / JsonReader<T>\nJSON and NDJSON (newline-delimited) support.\n```gcl\n// Write NDJSON\nvar writer = JsonWriter<Person> { path: \"/data/people.json\" };\nwriter.writeln(person1);  // One JSON object per line\nwriter.writeln(person2);\nwriter.flush();\n\n// Read NDJSON\nvar reader = JsonReader<Person> { path: \"/data/people.json\" };\nwhile (reader.can_read()) {\n  var person = reader.read();\n  process(person);\n}\n\n// Parse JSON string\nvar json = Json<Person> {};\nvar obj = json.parse(\"{\\\"name\\\":\\\"Alice\\\",\\\"age\\\":30}\");\n```\n\n### CSV I/O\n\n#### CsvWriter<T> / CsvReader<T>\nCSV operations with automatic header generation.\n```gcl\n// Configure format\nvar format = CsvFormat {\n  header_lines: 1,\n  separator: ',',\n  string_delimiter: '\"'\n};\n\n// Write\nvar writer = CsvWriter<Employee> {\n  path: \"/data/employees.csv\",\n  format: format\n};\nwriter.write(emp1);  // Headers auto-generated from type\nwriter.write(emp2);\nwriter.flush();\n\n// Read\nvar reader = CsvReader<Employee> {\n  path: \"/data/employees.csv\",\n  format: format\n};\nwhile (reader.can_read()) {\n  var emp = reader.read();\n  process(emp);\n}\n```\n\n#### CSV Analysis\nAnalyze CSV structure and generate GCL types.\n```gcl\nvar config = CsvAnalysisConfig {\n  row_limit: 1000,\n  enumerable_limit: 50\n};\n\nvar files = Array<File> { File::open(\"/data/sales.csv\")!! };\nvar stats = Csv::analyze(files, config);\n\n// Generate type definitions\nvar type_code = Csv::generate(stats);\nprintln(type_code);  // Generated GCL type definitions\n\n// Sample data\nvar reader = CsvReader<any> { path: \"/data/sales.csv\" };\nvar sample = Csv::sample(reader, 100);  // First 100 rows\n```\n\n### File System\n\n#### File\nComprehensive file operations.\n```gcl\n// Discovery\nvar csv_files = File::ls(\"/data\", \".csv\", true);  // Recursive\n\n// Open and inspect\nvar file = File::open(\"/data/input.txt\")!!;\nprintln(\"Size: ${file.size}\");\nprintln(\"Extension: ${file.extension()!!}\");\nprintln(\"Is directory: ${file.isDir()}\");\nprintln(\"SHA-256: ${file.sha256()!!}\");\n\n// Operations\nFile::copy(\"/data/src.txt\", \"/data/dst.txt\");\nFile::rename(\"/data/old.txt\", \"/data/new.txt\");\nFile::delete(\"/data/temp.txt\");\nFile::mkdir(\"/data/archive\");\n\n// Paths\nvar base = File::baseDir();\nvar user = File::userDir();\nvar work = File::workingDir();\n```\n\n#### FileWalker\nIterate through directory hierarchies.\n```gcl\nvar walker = FileWalker { path: \"/data\" };\nvar file_count = 0;\n\nwhile (!walker.isEmpty()) {\n  var file = walker.next();\n  if (file != null && !file.isDir()) {\n    file_count++;\n    println(file.path);\n  }\n}\n```\n\n### HTTP Client\n\n#### Http<T>\nREST API client with type-safe requests.\n```gcl\nvar headers = [\n  HttpHeader { name: \"Authorization\", value: \"Bearer token\" },\n  HttpHeader { name: \"Accept\", value: \"application/json\" }\n];\n\n// GET request\nvar response = Http<User> {}.get(\n  \"https://api.example.com/users/123\",\n  headers\n);\n\n// Download file\nHttp<any> {}.getFile(\n  \"https://example.com/data.csv\",\n  \"/local/data.csv\",\n  null\n);\n\n// POST request\nvar new_user = User { name: \"Alice\", email: \"alice@example.com\" };\nvar result = Http<User> {}.post(\n  \"https://api.example.com/users\",\n  new_user,\n  headers\n);\n```\n\n#### Url\nURL parsing and manipulation.\n```gcl\nvar url = Url::parse(\"https://api.example.com:8080/users?active=true#top\");\n\nprintln(\"Protocol: ${url.protocol}\");  // \"https\"\nprintln(\"Host: ${url.host}\");          // \"api.example.com\"\nprintln(\"Port: ${url.port}\");          // 8080\nprintln(\"Path: ${url.path}\");          // \"/users\"\nprintln(\"Param: ${url.params?.get(\"active\")}\");  // \"true\"\nprintln(\"Hash: ${url.hash}\");          // \"top\"\n```\n\n### Email\n\n#### Email & Smtp\nEmail composition and SMTP delivery.\n```gcl\nvar smtp = Smtp {\n  host: \"smtp.gmail.com\",\n  port: 587,\n  mode: SmtpMode::starttls,\n  authenticate: SmtpAuth::plain,\n  user: \"sender@example.com\",\n  pass: \"app_password\"\n};\n\nvar email = Email {\n  from: \"sender@example.com\",\n  to: [\"recipient@example.com\"],\n  cc: [\"cc@example.com\"],\n  subject: \"Monthly Report\",\n  body: \"<h1>Report</h1><p>Content here...</p>\",\n  body_is_html: true,\n  attachments: [\"/reports/monthly.pdf\"]\n};\n\nsmtp.send(email);\n```\n\n### XML I/O\n\n#### XmlReader<T>\nXML parsing and deserialization.\n```gcl\nvar reader = XmlReader<Config> { path: \"/config/settings.xml\" };\nwhile (reader.can_read()) {\n  var config = reader.read();\n  apply_config(config);\n}\n```\n\n## Util Module (std::util)\n\n### Collections\n\n#### Queue<T>\nFIFO queue with optional capacity bounds.\n```gcl\nvar queue = Queue<String> { capacity: 100 };\nqueue.push(\"first\");\nqueue.push(\"second\");\n\nvar item = queue.pop();       // \"first\"\nvar next = queue.front();     // Peek without removing\nvar last = queue.back();      // View last element\n```\n\n#### Stack<T>\nLIFO stack.\n```gcl\nvar stack = Stack<int> {};\nstack.push(10);\nstack.push(20);\n\nvar top = stack.pop();        // 20\nvar peek = stack.last();      // Peek at top\nvar bottom = stack.first();   // View bottom\n```\n\n#### SlidingWindow<T>\nFixed-size window with streaming statistics.\n```gcl\nvar window = SlidingWindow<float> { span: 100 };\n\nwindow.add(10.5);\nwindow.add(20.3);\nwindow.add(15.7);\n\nvar avg = window.avg()!!;\nvar std = window.std()!!;\nvar median = window.median()!!;\nvar min = window.min();\nvar max = window.max();\n```\n\n#### TimeWindow<T>\nTime-based sliding window with automatic expiration.\n```gcl\nvar window = TimeWindow<float> { span: 5min };\n\nwindow.add(time::now(), temperature);\nwindow.add(time::now() + 30s, next_temp);\n\n// Statistics on recent data only\nvar avg = window.avg();\nvar min_tuple = window.min();  // Tuple<time, float>\nvar max_tuple = window.max();  // Tuple<time, float>\n```\n\n### Statistics\n\n#### Gaussian<T>\nRunning statistical profile with normalization.\n```gcl\nvar profile = Gaussian<float> {};\n\nprofile.add(10.0);\nprofile.add(20.0);\nprofile.add(30.0);\n\nvar avg = profile.avg()!!;        // 20.0\nvar std = profile.std()!!;\nvar min = profile.min;            // 10.0\nvar max = profile.max;            // 30.0\n\n// Normalize: (value - min) / (max - min)\nvar norm = profile.normalize(15.0)!!;  // 0.25\n\n// Standardize: (value - avg) / std\nvar z_score = profile.standardize(25.0);\n```\n\n#### Histogram<T>\nBinned distribution analysis.\n```gcl\nvar quantizer = LinearQuantizer<float> {\n  min: 0.0,\n  max: 100.0,\n  bins: 20\n};\nvar histogram = Histogram<float> { quantizer: quantizer };\n\nfor (_, score in test_scores) {\n  histogram.add(score);\n}\n\nvar median = histogram.percentile(0.5);      // 50th percentile\nvar p90 = histogram.percentile(0.9);         // 90th percentile\nvar below_60 = histogram.ratio_under(60.0);  // Fraction below 60\n\nvar stats = histogram.stats();  // Comprehensive statistics\n```\n\n#### GaussianProfile<T>\nMulti-dimensional Gaussian statistics by category.\n```gcl\nvar quantizer = LinearQuantizer<int> { min: 0, max: 100, bins: 10 };\nvar profile = GaussianProfile<int> {\n  quantizer: quantizer,\n  precision: FloatPrecision::p1000\n};\n\nprofile.add(age_group, salary);\nprofile.add(age_group, another_salary);\n\nvar avg_salary = profile.avg(age_group);\nvar std_salary = profile.std(age_group);\n```\n\n### Quantizers\n\n#### LinearQuantizer<T>\nUniform bin spacing.\n```gcl\nvar linear = LinearQuantizer<float> {\n  min: 0.0,\n  max: 100.0,\n  bins: 10\n};\n\nvar bin = linear.quantize(25.0);  // Returns bin index (2)\nvar bounds = linear.bounds(2);    // QuantizerSlotBound\n// bounds.min = 20.0, bounds.max = 30.0, bounds.center = 25.0\n```\n\n#### LogQuantizer<T>\nLogarithmic bin spacing for exponential distributions.\n```gcl\nvar log_quant = LogQuantizer<float> {\n  min: 1.0,\n  max: 1000.0,\n  bins: 10\n};\nvar bin = log_quant.quantize(50.0);\n```\n\n#### CustomQuantizer<T>\nUser-defined bin boundaries.\n```gcl\nvar age_bins = CustomQuantizer<int> {\n  min: 0,\n  max: 100,\n  step_starts: [0, 18, 25, 40, 65]  // Custom age groups\n};\nvar group = age_bins.quantize(32);  // Returns appropriate bin\n```\n\n#### MultiQuantizer<T>\nMulti-dimensional quantization.\n```gcl\nvar quantizers = Array<Quantizer<float>> {\n  LinearQuantizer<float> { min: 0.0, max: 100.0, bins: 5 },\n  LogQuantizer<float> { min: 1000.0, max: 200000.0, bins: 8 },\n  LinearQuantizer<float> { min: 0.0, max: 100.0, bins: 10 }\n};\n\nvar multi = MultiQuantizer<float> { quantizers: quantizers };\nvar slot = multi.quantize([35.0, 45000.0, 87.5]);\nvar vector = multi.slot_vector(slot);  // [age_bin, income_bin, score_bin]\n```\n\n### Utilities\n\n#### Random\nSeeded random number generator.\n```gcl\nvar rng = Random { seed: 12345 };\n\nvar dice = rng.uniform(1, 7);           // 1-6 inclusive\nvar prob = rng.uniformf(0.0, 1.0);      // Float [0.0, 1.0)\n\n// Fill array with normal distribution\nvar samples = Array<float> {};\nrng.fill(samples, 1000, 50.0, 60.0);    // 1000 samples, mean=50, std=10\n```\n\n#### Assert\nTesting utilities.\n```gcl\nAssert::equals(actual, expected);\nAssert::equalsd(pi, 3.14159, 0.001);    // Float with epsilon\nAssert::equalst(tensor_a, tensor_b, 0.01);  // Tensor with epsilon\nAssert::isTrue(condition);\nAssert::isFalse(condition);\nAssert::isNotNull(value);\nAssert::isNull(value);\n```\n\n#### ProgressTracker\nMonitor long-running operations.\n```gcl\nvar tracker = ProgressTracker {\n  start: time::now(),\n  total: 10000\n};\n\n// Update progress\ntracker.update(2500);\nprintln(\"Progress: ${tracker.progress * 100}%\");     // 25%\nprintln(\"Speed: ${tracker.speed} items/sec\");\nprintln(\"Remaining: ${tracker.remaining}\");\n\n// Complete\ntracker.update(10000);\nprintln(\"Done! Progress: ${tracker.progress}\");      // 1.0\n```\n\n#### Crypto\nCryptographic operations.\n```gcl\nvar input = \"sensitive data\";\n\n// Hashing\nvar sha1 = Crypto::sha1hex(input);\nvar sha256 = Crypto::sha256hex(input);\n\n// Encoding\nvar b64 = Crypto::base64_encode(input);\nvar decoded = Crypto::base64_decode(b64);\n\nvar url_enc = Crypto::url_encode(\"param with spaces\");\nvar url_dec = Crypto::url_decode(url_enc);\n\n// PKCS1 signing (requires private key)\nvar signature = Crypto::pkcs1_sign(data, private_key);\n```\n\n#### Plot\nBasic plotting from tabular data.\n```gcl\nvar data = Table {};\ndata.set_row(0, [\"Jan\", 1, 10.5]);\ndata.set_row(1, [\"Feb\", 2, 15.3]);\ndata.set_row(2, [\"Mar\", 3, 20.1]);\n\n// Plot column 1 (x-axis) vs columns 2+ (y-axis series)\nPlot::scatter_plot(data, 1, [2], \"output.png\");\n```\n\n## Usage Guidelines\n\n### When to Use Each Module\n\n**core** - Use for fundamental data types, error handling, and geospatial operations.\n\n**runtime** - Use for:\n- Scheduled/recurring tasks (Scheduler)\n- Background job processing (Task, Job)\n- Application logging (Log)\n- Security and authentication (User, SecurityPolicy, OpenIDConnect)\n- System information queries (Runtime, System)\n\n**io** - Use for:\n- File I/O operations (File, FileWalker)\n- Data serialization (GcbWriter, JsonWriter, CsvWriter)\n- HTTP API calls (Http, Url)\n- Email notifications (Email, Smtp)\n\n**util** - Use for:\n- Data structures (Queue, Stack, SlidingWindow)\n- Statistical analysis (Gaussian, Histogram)\n- Data binning (LinearQuantizer, LogQuantizer)\n- Testing (Assert)\n- Monitoring (ProgressTracker)\n\n### Best Practices\n\n1. **Resource Management**: Always call `flush()` on writers and close readers when done.\n2. **Error Handling**: Check return values (especially `?` nullable types) before use.\n3. **Scheduling**: Use appropriate periodicity type for your use case; `FixedPeriodicity` for simple intervals, `DailyPeriodicity` for scheduled times.\n4. **CSV Analysis**: Use `Csv::analyze()` to understand data structure before processing large files.\n5. **Statistics**: Use `Gaussian` for running statistics, `Histogram` for distribution analysis.\n6. **File Operations**: Use `File::ls()` with file extensions for efficient discovery.\n7. **HTTP**: Always include appropriate headers (Authorization, Content-Type) in requests.\n\n### Common Patterns\n\n#### Scheduled Data Processing\n```gcl\nfn process_daily_data() {\n  var files = File::ls(\"/data/incoming\", \".csv\", false);\n  for (_, file in files) {\n    var reader = CsvReader<Record> { path: file.path };\n    while (reader.can_read()) {\n      process_record(reader.read());\n    }\n    File::rename(file.path, \"/data/processed/${file.name}\");\n  }\n}\n\nScheduler::add(\n  process_daily_data,\n  DailyPeriodicity { hour: 1, minute: 0 },\n  null\n);\n```\n\n#### Streaming Statistics\n```gcl\nvar window = SlidingWindow<float> { span: 1000 };\n\nfn process_sensor_data(readings: Array<float>) {\n  for (_, value in readings) {\n    window.add(value);\n\n    if (window.size() >= 100) {\n      var avg = window.avg()!!;\n      var std = window.std()!!;\n\n      if (value > avg + 3.0 * std) {\n        Log::warn(\"Anomaly detected: ${value}\");\n      }\n    }\n  }\n}\n```\n\n#### HTTP API Integration\n```gcl\nfn fetch_and_store_data() {\n  var headers = [\n    HttpHeader { name: \"Authorization\", value: \"Bearer ${api_key}\" }\n  ];\n\n  var response = Http<Array<Record>> {}.get(\n    \"https://api.example.com/data\",\n    headers\n  );\n\n  if (response != null) {\n    var writer = JsonWriter<Record> { path: \"/data/cache.json\" };\n    for (_, record in response) {\n      writer.writeln(record);\n    }\n    writer.flush();\n  }\n}\n```\n",
        "plugins/greycat-lsp/.claude-plugin/plugin.json": "{\n  \"name\": \"greycat-lsp\",\n  \"version\": \"1.7.6\",\n  \"description\": \"GreyCat Language Server Protocol plugin - Provides LSP support for .gcl files including code completion, diagnostics, go-to-definition, and hover information.\",\n  \"author\": {\n    \"name\": \"Datathings\",\n    \"email\": \"contact@datathings.com\"\n  },\n  \"license\": \"MIT\",\n  \"repository\": \"https://github.com/datathings/marketplace\",\n  \"keywords\": [\n    \"greycat\",\n    \"lsp\",\n    \"language-server\",\n    \"gcl\",\n    \"code-completion\",\n    \"diagnostics\",\n    \"ide-support\"\n  ],\n  \"lspServers\": {\n    \"greycat\": {\n      \"command\": \"greycat-lang\",\n      \"args\": [\"server\", \"--stdio\"],\n      \"extensionToLanguage\": {\n        \".gcl\": \"greycat\"\n      },\n      \"transport\": \"stdio\",\n      \"startupTimeout\": 10000,\n      \"shutdownTimeout\": 5000,\n      \"restartOnCrash\": true,\n      \"maxRestarts\": 3\n    }\n  }\n}",
        "plugins/greycat-lsp/README.md": "# GreyCat LSP Plugin\n\nLanguage Server Protocol support for GreyCat `.gcl` files in Claude Code.\n\n## Overview\n\nThis plugin enables IDE-like features when working with `.gcl` files (GreyCat source code). It connects Claude Code to the GreyCat Language Server, providing intelligent code assistance for GreyCat development.\n\n**Use this plugin when:**\n- Writing or editing `.gcl` files\n- Developing GreyCat backend applications\n- Working with `project.gcl` and related source files\n\n## Prerequisites\n\nThe `greycat-lang` binary must be installed and available in your PATH.\n\n```bash\n# Verify installation\ngreycat-lang --version\n```\n\nSee https://get.greycat.io for installation instructions.\n\n## Standalone CLI Commands\n\nBeyond the LSP server, `greycat-lang` provides CLI tools for batch operations and CI/CD:\n\n```bash\n# Lint a project\ngreycat-lang lint project.gcl\n\n# Format GCL files (outputs to stdout by default)\ngreycat-lang fmt file.gcl\n```\n\nThese are useful for pre-commit hooks, CI pipelines, or quick command-line checks. The LSP server provides the same functionality (linting via diagnostics, formatting via document formatting) through the standard protocol.\n\n## Features\n\nThe GreyCat LSP provides comprehensive IDE features:\n\n### Code Intelligence\n- **Code Completion** - Intelligent suggestions with trigger characters\n- **Signature Help** - Function parameter hints on `(` and `,`\n- **Hover Information** - Type information and documentation on hover\n- **Inlay Hints** - Inline type annotations\n\n### Navigation\n- **Go to Definition** - Jump to symbol definitions\n- **Find References** - Find all usages of a symbol\n- **Document Symbols** - Outline view of file structure\n\n### Code Quality\n- **Diagnostics** - Real-time error and warning detection (on save)\n- **Code Actions** - Quick fixes and refactoring suggestions\n- **Semantic Tokens** - Enhanced syntax highlighting\n\n### Editing\n- **Document Formatting** - Auto-format `.gcl` files\n- **Rename Symbol** - Safely rename across the codebase\n- **Code Lens** - Inline actionable information\n\n### Workspace\n- **Multi-folder Support** - Works with workspace folders\n- **File Watching** - Tracks `.gcl` file creation/deletion\n- **Library Detection** - Monitors `lib/installed` for dependencies\n\n## Configuration\n\nThe LSP is configured in `.lsp.json`:\n\n```json\n{\n  \"greycat\": {\n    \"command\": \"greycat-lang\",\n    \"args\": [\"server\"],\n    \"extensionToLanguage\": {\n      \".gcl\": \"greycat\"\n    }\n  }\n}\n```\n\n### Available Options\n\n| Option | Default | Description |\n|--------|---------|-------------|\n| `command` | `greycat-lang` | Path to the LSP binary |\n| `args` | `[\"server\"]` | Command-line arguments |\n| `transport` | `stdio` | Communication transport |\n| `startupTimeout` | `2000` | Max startup time (ms) |\n| `restartOnCrash` | `true` | Auto-restart on crash |\n| `maxRestarts` | `3` | Maximum restart attempts |\n\n## Troubleshooting\n\n### LSP not starting\n\n1. Verify `greycat-lang` is in your PATH:\n   ```bash\n   which greycat-lang\n   ```\n\n2. Test the LSP server directly:\n   ```bash\n   greycat-lang server\n   ```\n\n### Completion not working\n\nEnsure you're in a valid GreyCat project with a `project.gcl` file.\n\n### Diagnostics not updating\n\nDiagnostics update on file save. Save the file to see updated errors/warnings.\n\n## Links\n\n- **GreyCat Website**: https://greycat.io/\n- **GreyCat Documentation**: https://doc.greycat.io/\n- **GreyCat Installation**: https://get.greycat.io/\n- **Datathings**: https://datathings.com/\n- **Repository**: https://github.com/datathings/marketplace\n\n## License\n\nMIT License - See LICENSE file for details.\n",
        "plugins/greycat/.claude-plugin/plugin.json": "{\n  \"name\": \"greycat\",\n  \"version\": \"1.7.6\",\n  \"description\": \"Comprehensive GreyCat backend development skill for graph-based language with built-in persistence. Covers data modeling, API development, parallel processing, React integration, and all standard libraries.\",\n  \"author\": {\n    \"name\": \"Datathings\",\n    \"email\": \"contact@datathings.com\"\n  },\n  \"license\": \"MIT\",\n  \"repository\": \"https://github.com/datathings/marketplace\",\n  \"keywords\": [\"greycat\", \"backend\", \"graph-database\", \"persistence\", \"time-series\", \"geo-spatial\", \"react\", \"typescript\", \"full-stack\", \"api-development\", \"parallel-processing\", \"data-modeling\", \"machine-learning\", \"event-driven\", \"integrations\"],\n  \"skills\": \"./skills/\"\n}\n",
        "plugins/greycat/README.md": "# GreyCat Skill\n\nA comprehensive skill for building GreyCat backend applications with the graph-based language and built-in persistence.\n\n## Overview\n\nThis skill provides expert guidance for developing with GreyCat - a unique language that combines graph-based programming with built-in persistence. It covers everything from basic syntax to advanced patterns for building production-ready applications.\n\n## What is GreyCat?\n\nGreyCat is a graph-based programming language with built-in persistence. Unlike traditional databases, GreyCat applications are **evolving coded structures** where your data model, business logic, and persistence are unified in a single codebase.\n\n**Key Features:**\n- **Unified persistence** - Data, schema, and logic in one evolving codebase (not a separate database)\n- **Graph-native references** - Lightweight 64-bit node references instead of heavy object graphs\n- **Built-in indexing** - nodeIndex (hash), nodeTime (time-series), nodeGeo (spatial), nodeList (ordered)\n- **Parallel execution** - Job API with fork-join pattern and automatic transaction merging\n- **Type-safe APIs** - @expose + @permission decorators with full TypeScript SDK generation\n- **Zero-config persistence** - No ORM, no migrations, no serialization - just code\n- **React integration** - Official @greycat/web SDK with automatic type generation\n\n## Skill Activation Triggers\n\nThis skill automatically activates when you're working with:\n\n### File Types & Extensions\n- `.gcl` files (GreyCat source code)\n- GreyCat project structures with `project.gcl`\n- Files containing GreyCat syntax\n\n### Language Features\n- **Decorators**: `@expose`, `@permission`, `@volatile`, `@role`, `@library`, `@include`, `@test`, `@format_indent`\n- **Indexed Collections**: `nodeList`, `nodeIndex`, `nodeTime`, `nodeGeo`\n- **Node Operations**: Persisted nodes, transactions, node references (`node<Type>`)\n- **Abstract Types**: Service patterns, polymorphism, inheritance\n- **Parallel Processing**: Jobs, `await()`, `PeriodicTask`, fork-join patterns\n\n### Framework Components\n- **Backend**: Data models, services, API endpoints, persistence patterns\n- **Frontend**: `@greycat/web` SDK, React integration, TypeScript type generation\n- **Standard library**: core, io, runtime, util\n- **Pro libraries**: ai, algebra, finance, kafka, opcua, powerflow, s3, sql, useragent\n\n### CLI Commands\n- `greycat serve` - Start development server\n- `greycat test` - Run tests\n- `greycat run import` - Execute import functions\n- `greycat install` - Download library dependencies\n- `greycat-lang lint` - Check for errors\n\n### Use Cases\n- Building graph-based data models\n- Creating time-series or geo-spatial applications\n- Implementing RBAC with permissions and roles\n- Developing full-stack applications with React frontends\n- Working with persistent node structures\n- Debugging GreyCat applications\n\n### When This Skill Does NOT Activate\n- General graph databases (Neo4j, ArangoDB, JanusGraph)\n- Generic React applications (without @greycat/web)\n- SQL databases (PostgreSQL, MySQL, SQLite)\n- Traditional ORM frameworks (Prisma, TypeORM, Sequelize)\n\n## Why This Skill?\n\nThis skill transforms your AI assistant into a GreyCat development expert by providing:\n\n- **Complete language reference** - All GCL syntax, types, and patterns in one place\n- **Best practices** - Avoid common pitfalls with proven patterns (Services, Views, transactions)\n- **Full-stack guidance** - Both backend (.gcl) and frontend (React/TypeScript) development\n- **Library coverage** - Complete API references for all 10+ GreyCat libraries\n- **Progressive disclosure** - Core patterns immediately available, detailed references loaded only when needed\n\n**Perfect for:**\n- GreyCat beginners learning the fundamentals\n- Experienced developers building production applications\n- Full-stack teams integrating React frontends\n- Data engineers working with time-series and geo-spatial data\n\n## When to Use This Skill\n\nUse this skill when you need help with:\n- Building GreyCat applications (`.gcl` files)\n- Working with persisted nodes and indexed collections (nodeList, nodeIndex, nodeTime, nodeGeo)\n- Creating data models and services\n- Writing API endpoints with `@permission` decorators\n- Implementing parallel processing with Jobs\n- Integrating React frontends with `@greycat/web` SDK\n- Running GreyCat commands (serve, test, run import)\n- Debugging GreyCat projects\n\n## GreyCat vs. Traditional Stacks\n\n| Traditional Approach | GreyCat Approach |\n|---------------------|------------------|\n| Separate database (Postgres, MongoDB) | Built-in persistence in GCL |\n| ORM layer (Prisma, TypeORM) | Direct node references |\n| Manual indexing setup | nodeIndex, nodeTime, nodeGeo built-in |\n| Separate job queue (Bull, BeeQueue) | Job API with await() |\n| API framework (Express, Fastify) | @expose decorators |\n| Manual TypeScript types | Auto-generated from GCL |\n\n**When to use GreyCat:**\n- Building graph-based applications with complex relationships\n- Time-series or geo-spatial data with built-in indexing\n- Need persistence without ORM complexity\n- Want unified backend + frontend type safety\n\n**When NOT to use GreyCat:**\n- Standard CRUD apps (traditional frameworks may be simpler)\n- Existing database migration requirements\n- Team has no capacity to learn a new language/paradigm\n\n## What's Included\n\n### Core Documentation (SKILL.md)\n\nConcise reference covering:\n- Project setup and architecture\n- Type system and nullability\n- Nodes and persistence\n- Indexed collections\n- Services and API patterns\n- Abstract types and inheritance\n- Parallelization basics\n- Testing and common pitfalls\n\n### Detailed References\n\n**Backend Development:**\n- **nodes.md** - Deep dive into persistence, transactions, indexed collections\n- **data_structures.md** - Tensor, Table, Buffer, Windows, Stack, Queue\n- **concurrency.md** - Jobs, await, parallel writes, Tasks, PeriodicTask\n- **io.md** - CSV/JSON reading/writing, File operations, HTTP client, SMTP\n- **time.md** - Time handling, Date, duration, format specifiers\n- **permissions.md** - RBAC, @permission, @role, SSO integration\n- **testing.md** - @test, Assert, setup/teardown conventions\n\n**Frontend Development:**\n- **frontend.md** - Complete React integration guide:\n  - @greycat/web SDK setup\n  - TypeScript type generation\n  - Authentication & authorization\n  - React Query integration\n  - Error handling and best practices\n\n**Library References:**\n- Complete GCL type definitions for all GreyCat libraries:\n  - **std/** - Core types, I/O, runtime, utilities\n  - **ai/** - LLM integration (llama.cpp)\n  - **algebra/** - ML, neural networks, numerical computing\n  - **kafka/** - Apache Kafka integration\n  - **sql/** - PostgreSQL integration\n  - **s3/** - Amazon S3 integration\n  - **opcua/** - Industrial protocol integration\n  - **finance/** - Financial modeling\n  - **powerflow/** - Electrical power flow analysis\n\n## Repository Structure\n\n```\nplugins/greycat/\n├── README.md              # This file\n├── package.sh             # Packaging script\n└── skills/greycat/        # Skill content\n    ├── SKILL.md           # Main skill documentation\n    └── references/        # Detailed reference files\n        ├── frontend.md\n        ├── nodes.md\n        ├── time.md\n        ├── concurrency.md\n        ├── data_structures.md\n        ├── io.md\n        ├── permissions.md\n        ├── testing.md\n        └── [library GCL definitions]\n```\n\n## Building the Skill\n\nTo package the skill for distribution:\n\n```bash\n./package.sh              # Creates greycat.skill in current directory\n./package.sh /path/to/dir # Creates greycat.skill in specified directory\n```\n\nThe script:\n- Validates SKILL.md frontmatter\n- Packages all files from `greycat/` directory\n- Creates a `.skill` file (zip archive with .skill extension)\n- Excludes development files (node_modules, cache, etc.)\n\n## Installation\n\n### From Marketplace\n\n```bash\n# Install via skills marketplace\nskills install greycat\n```\n\n### Manual Installation\n\n```bash\ngit clone https://github.com/datathings/marketplace.git\ncd marketplace/plugins/greycat\n./package.sh\n# Install the greycat.skill file to your AI assistant's skills directory\n```\n\n## Usage Examples\n\nOnce installed, this skill will automatically activate when you're working on GreyCat projects.\n\n**Example interactions:**\n- \"Create a GreyCat data model for a city with buildings and residents\"\n- \"Help me implement parallel processing for analyzing cities\"\n- \"Set up React frontend integration with authentication\"\n- \"How do I use nodeTime for time-series data?\"\n- \"Debug this GreyCat function that's failing\"\n\n## Quick Start Example\n\n```gcl\n// project.gcl\n@library(\"std\", \"7.5.125-dev\");\n@include(\"backend\");\n\n@permission(\"app.user\", \"app user permission\");\n@role(\"user\", \"app.user\", \"public\", \"api\", \"files\");\n\nfn main() { }\n```\n\n```gcl\n// backend/src/model/city.gcl\nvar cities_by_name: nodeIndex<String, node<City>>;\n\ntype City {\n    name: String;\n    country: node<Country>;\n    streets: nodeList<node<Street>>;\n}\n```\n\n```gcl\n// backend/src/api/city_api.gcl\n@volatile\ntype CityView {\n    name: String;\n    streetCount: int;\n}\n\n@expose\n@permission(\"app.user\")\nfn getCities(): Array<CityView> {\n    var results = Array<CityView>{};\n    for (name, city in cities_by_name) {\n        results.add(CityView {\n            name: city->name,\n            streetCount: city->streets.size()\n        });\n    }\n    return results;\n}\n```\n\n## Contributing\n\nContributions are welcome! This skill is maintained by the GreyCat team at DataThings.\n\n## Resources\n\n- [GreyCat Website](https://greycat.io/)\n- [Official GreyCat Documentation](https://doc.greycat.io/)\n- [GreyCat Installation](https://get.greycat.io/)\n- [Datathings](https://datathings.com/)\n- [Skills Marketplace](https://skillsmp.com/)\n\n## License\n\nMIT License - See LICENSE file for details\n\n## Support\n\nFor issues or questions:\n- Open an issue in this repository\n- Visit [GreyCat Documentation](https://doc.greycat.io/)\n- Contact DataThings support\n\n---\n\n**Built for AI-assisted development** - Enhancing GreyCat productivity\n",
        "plugins/greycat/commands/apicheck.md": "---\nname: apicheck\ndescription: Review all @expose endpoints for security, performance, and best practices\nallowed-tools: Bash, Read, Grep, Glob\n---\n\n# API Endpoint Review\n\n**Purpose**: Comprehensive review of all @expose API endpoints for security, performance, best practices, and consistency\n\n**Run After**: Each sprint, before releases, when adding new endpoints\n\n---\n\n## Overview\n\nThis command performs a thorough review of all @expose functions across 6 categories:\n\n1. **Security** - Permission checks, input validation, sensitive data\n2. **Performance** - Pagination, data volume, query efficiency\n3. **Error Handling** - Try-catch blocks, null checks, user-friendly errors\n4. **Type Safety** - @volatile decorators, return types, parameter types\n5. **Best Practices** - Naming, documentation, consistency\n6. **API Design** - REST principles, versioning, backwards compatibility\n\n---\n\n## Phase 1: Security Review\n\n### Objective\nEnsure all endpoints are secure and properly protected.\n\n### Step 1.1: Permission Decorator Check\n\n**Find @expose without @permission**:\n```bash\n# Find all @expose functions\ngrep -rn \"@expose\" backend/src/api/ --include=\"*.gcl\" -B 1 -A 5 | grep -A 5 -B 1 \"@expose\"\n```\n\nFor each @expose function, verify:\n- Has @permission decorator\n- Permission level is appropriate for operation\n- Permission is not too broad (avoid \"public\" for mutations)\n\n**Output Format**:\n```\n📍 backend/src/api/admin_api.gcl:45\n\n⚠️ SECURITY: Missing @permission decorator\n\nFunction:\n  @expose\n  fn deleteUser(userId: String): bool {  ← No @permission\n      UserService::deleteUser(userId);\n  }\n\nProblem:\n  - Sensitive operation (deletion) has no permission check\n  - Function is accessible to all users (default public)\n  - Should require admin permission\n\nRecommendation:\n  @expose\n  @permission(\"app.admin\")  ← Add this\n  fn deleteUser(userId: String): bool {\n      UserService::deleteUser(userId);\n  }\n\nSeverity: CRITICAL\nRisk: Unauthorized data deletion\n```\n\n### Step 1.2: Input Validation\n\nCheck for dangerous input patterns:\n\n```bash\n# Find endpoints that take String parameters (potential injection)\ngrep -rn \"@expose\" backend/src/api/ --include=\"*.gcl\" -A 10 | grep \"fn.*String\"\n```\n\nVerify each endpoint validates input:\n- String length limits\n- Format validation (email, URL, etc.)\n- SQL/command injection prevention\n- XSS prevention\n\n**Example Output**:\n```\n📍 backend/src/api/search_api.gcl:23\n\n⚠️ SECURITY: Insufficient input validation\n\nFunction:\n  @expose\n  @permission(\"app.user\")\n  fn search(query: String, ...): SearchResults {\n      // No validation on query length\n      return SearchService::execute(query);  ← Potentially dangerous\n  }\n\nProblem:\n  - No check on query string length\n  - Could accept extremely long queries (DoS risk)\n  - No sanitization before processing\n\nRecommendation:\n  @expose\n  @permission(\"app.user\")\n  fn search(query: String, ...): SearchResults {\n      // Validate input\n      if (query.size() > 1000) {\n          throw \"Query too long (max 1000 characters)\";\n      }\n      if (query.size() == 0) {\n          throw \"Query cannot be empty\";\n      }\n\n      return SearchService::execute(query);\n  }\n\nSeverity: HIGH\nRisk: Denial of service, resource exhaustion\n```\n\n### Step 1.3: Sensitive Data Exposure\n\nCheck for endpoints returning sensitive information:\n\n```bash\n# Find endpoints returning User, Password, Token types\ngrep -rn \"@expose\" backend/src/api/ --include=\"*.gcl\" -A 10 | grep -E \"User|Password|Token|Secret|Key\"\n```\n\nVerify:\n- Passwords never returned\n- Sensitive fields filtered from responses\n- Using view types instead of full objects\n\n**Example Output**:\n```\n📍 backend/src/api/user_api.gcl:67\n\n⚠️ SECURITY: Potential sensitive data exposure\n\nFunction:\n  @expose\n  @permission(\"app.user\")\n  fn getCurrentUser(): User {  ← Returns full User object\n      return SecurityService::requireLoggedUser();\n  }\n\nType Definition:\n  type User {\n      username: String;\n      email: String;\n      passwordHash: String;  ← Sensitive!\n      role: String;\n  }\n\nProblem:\n  - Returns passwordHash field to client\n  - Leaks sensitive authentication data\n\nRecommendation:\n  Create view type without sensitive fields:\n\n  @volatile\n  type UserView {\n      username: String;\n      email: String;\n      role: String;\n      // passwordHash excluded\n  }\n\n  @expose\n  @permission(\"app.user\")\n  fn getCurrentUser(): UserView {\n      var user = SecurityService::requireLoggedUser();\n      return UserView {\n          username: user.username,\n          email: user.email,\n          role: user.role\n      };\n  }\n\nSeverity: CRITICAL\nRisk: Password hash exposure, security breach\n```\n\n### Step 1.4: Authentication Bypass\n\nFind endpoints that should check authentication:\n\n```bash\n# Find mutation operations with public permission\ngrep -rn '@permission(\"public\")' backend/src/api/ --include=\"*.gcl\" -A 10 | grep -E \"fn (create|update|delete|set|modify|remove)\"\n```\n\n**Example Output**:\n```\n📍 backend/src/api/document_api.gcl:89\n\n⚠️ SECURITY: Mutation allowed without authentication\n\nFunction:\n  @expose\n  @permission(\"public\")  ← Should require auth!\n  fn updateDocument(id: String, title: String): bool {\n      var doc = documents_by_id.get(id);\n      if (doc != null) {\n          doc->title = title;\n          return true;\n      }\n      return false;\n  }\n\nProblem:\n  - Data mutation with public permission\n  - No authentication required for modifying data\n  - Allows anonymous users to edit documents\n\nRecommendation:\n  @expose\n  @permission(\"app.user\")  ← Require authentication\n  fn updateDocument(id: String, title: String): bool {\n      var user = SecurityService::requireLoggedUser();\n      // ... update logic\n  }\n\nSeverity: CRITICAL\nRisk: Unauthorized data modification\n```\n\n---\n\n## Phase 2: Performance Review\n\n### Objective\nEnsure endpoints perform efficiently and scale properly.\n\n### Step 2.1: Missing Pagination\n\nFind list endpoints without pagination:\n\n```bash\n# Find endpoints returning Array without pagination parameters\ngrep -rn \"@expose\" backend/src/api/ --include=\"*.gcl\" -A 10 | grep \"Array<\"\n```\n\nCheck if endpoint has offset/limit parameters:\n\n**Example Output**:\n```\n📍 backend/src/api/document_api.gcl:34\n\n⚠️ PERFORMANCE: Missing pagination on list endpoint\n\nFunction:\n  @expose\n  @permission(\"app.user\")\n  fn getAllDocuments(): Array<DocumentView> {  ← No pagination!\n      var results = Array<DocumentView> {};\n      for (i, doc_node in documents_by_id) {\n          results.add(buildView(doc_node.resolve()));\n      }\n      return results;  ← Could return thousands of documents!\n  }\n\nProblem:\n  - Returns all documents in single request\n  - No limit on result size (could be 10K+ items)\n  - Large payload impacts performance and memory\n\nRecommendation:\n  @expose\n  @permission(\"app.user\")\n  fn getAllDocuments(offset: int, limit: int): PaginatedDocuments {\n      var validated_offset = PaginationService::validateOffset(offset);\n      var validated_limit = PaginationService::validateLimit(limit);\n\n      var results = Array<DocumentView> {};\n      var count = 0;\n      var skipped = 0;\n\n      for (i, doc_node in documents_by_id) {\n          if (skipped < validated_offset) {\n              skipped++;\n              continue;\n          }\n          if (count >= validated_limit) {\n              break;\n          }\n          results.add(buildView(doc_node.resolve()));\n          count++;\n      }\n\n      return PaginatedDocuments {\n          items: results,\n          total: documents_by_id.size(),\n          offset: validated_offset,\n          limit: validated_limit,\n          hasMore: (validated_offset + validated_limit) < documents_by_id.size()\n      };\n  }\n\nSeverity: HIGH\nRisk: Performance degradation, memory issues, timeouts\n```\n\n### Step 2.2: Expensive Operations\n\nFind endpoints with potentially expensive operations:\n\n```bash\n# Find nested loops in @expose functions\ngrep -rn \"@expose\" backend/src/api/ --include=\"*.gcl\" -A 50 | grep -E \"for.*for\"\n```\n\n**Example Output**:\n```\n📍 backend/src/api/stats_api.gcl:45\n\n⚠️ PERFORMANCE: Expensive nested loops in endpoint\n\nFunction:\n  @expose\n  @permission(\"app.user\")\n  fn getDetailedStats(): StatsView {\n      var stats = StatsView {};\n\n      // Nested loop through ALL documents and chunks\n      for (i, doc_node in documents_by_id) {      ← O(N)\n          var doc = doc_node.resolve();\n          for (j, chunk_node in doc.chunks) {      ← O(M)\n              var chunk = chunk_node.resolve();    ← O(N*M)!\n              // Process chunk...\n          }\n      }\n\n      return stats;\n  }\n\nProblem:\n  - O(N*M) complexity (documents × chunks per document)\n  - Resolves potentially thousands of nodes\n  - No caching or optimization\n  - Will timeout with large datasets\n\nRecommendation:\n  Option 1: Cache results\n  - Compute stats during import/update\n  - Store in global variable\n  - Return cached value instantly\n\n  Option 2: Limit scope\n  - Add filters to reduce dataset\n  - Use indices for targeted queries\n  - Implement pagination\n\n  Option 3: Background computation\n  - Use PeriodicTask to compute stats\n  - Store results in nodes\n  - API returns pre-computed data\n\nSeverity: HIGH\nRisk: Timeouts, slow response times, resource exhaustion\n```\n\n### Step 2.3: N+1 Query Problem\n\nFind endpoints that might have N+1 query issues:\n\n```bash\n# Look for loops with individual node resolutions\ngrep -rn \"for.*in.*\" backend/src/api/ --include=\"*.gcl\" -A 5 | grep \"\\.resolve()\"\n```\n\n**Example Output**:\n```\n📍 backend/src/api/document_api.gcl:78\n\n⚠️ PERFORMANCE: N+1 query pattern\n\nFunction:\n  @expose\n  @permission(\"app.user\")\n  fn getDocumentsWithAuthors(): Array<DocumentWithAuthor> {\n      var results = Array<DocumentWithAuthor> {};\n\n      for (i, doc_node in documents_by_id) {\n          var doc = doc_node.resolve();              ← Query 1\n          var author = doc.author.resolve();         ← N more queries!\n          results.add(DocumentWithAuthor {\n              document: doc,\n              authorName: author.name                ← Each iteration resolves author\n          });\n      }\n\n      return results;\n  }\n\nProblem:\n  - 1 query for documents + N queries for authors\n  - With 1000 documents = 1001 database queries\n  - Extremely inefficient\n\nRecommendation:\n  Option 1: Batch resolve (if supported)\n  Option 2: Include author data in document view\n  Option 3: Use index to get authors in one pass\n\n  var authors_map = Map<String, Author> {};\n  // First, collect all unique author nodes\n  for (i, doc_node in documents_by_id) {\n      var doc = doc_node.resolve();\n      var author_id = doc.author->id;  // Use arrow without resolve\n      if (authors_map.get(author_id) == null) {\n          authors_map.set(author_id, doc.author.resolve());\n      }\n  }\n  // Then, build results using cached authors\n  for (i, doc_node in documents_by_id) {\n      var doc = doc_node.resolve();\n      var author = authors_map.get(doc.author->id);\n      // ... build result\n  }\n\nSeverity: HIGH\nRisk: Slow queries, database load\n```\n\n---\n\n## Phase 3: Error Handling Review\n\n### Objective\nEnsure all endpoints handle errors gracefully.\n\n### Step 3.1: Missing Try-Catch\n\nFind endpoints without error handling:\n\n```bash\n# Find @expose functions without try-catch\ngrep -rn \"@expose\" backend/src/api/ --include=\"*.gcl\" -A 30 | grep -L \"try\\|catch\"\n```\n\n**Example Output**:\n```\n📍 backend/src/api/data_api.gcl:56\n\n⚠️ ERROR HANDLING: No try-catch block\n\nFunction:\n  @expose\n  @permission(\"app.user\")\n  fn processData(data: String): Result {\n      var parsed = parseJSON(data);        ← Can throw\n      var validated = validate(parsed);    ← Can throw\n      var result = store(validated);       ← Can throw\n      return result;\n  }\n\nProblem:\n  - No error handling for parsing failures\n  - No handling for validation errors\n  - Exceptions bubble up as generic errors\n  - Poor user experience (unclear error messages)\n\nRecommendation:\n  @expose\n  @permission(\"app.user\")\n  fn processData(data: String): Result {\n      try {\n          var parsed = parseJSON(data);\n          var validated = validate(parsed);\n          var result = store(validated);\n          return result;\n      } catch (ex) {\n          error(\"Data processing failed: ${ex}\");\n          throw \"Invalid data format. Please check your input.\";\n      }\n  }\n\nSeverity: MEDIUM\nRisk: Poor error messages, difficult debugging\n```\n\n### Step 3.2: Null Pointer Risks\n\nFind endpoints with potential null dereference:\n\n```bash\n# Find .resolve() calls without null checks\ngrep -rn \"\\.resolve()\\.\" backend/src/api/ --include=\"*.gcl\"\n```\n\n**Example Output**:\n```\n📍 backend/src/api/document_api.gcl:90\n\n⚠️ ERROR HANDLING: Potential null pointer dereference\n\nFunction:\n  @expose\n  @permission(\"app.user\")\n  fn getDocumentAuthor(docId: String): String {\n      var doc_node = documents_by_id.get(docId);\n      return doc_node.resolve().author.resolve().name;  ← Multiple failure points!\n  }\n\nProblem:\n  - No null check on doc_node (docId might not exist)\n  - No null check on author (document might not have author)\n  - Will throw cryptic error if any step fails\n\nRecommendation:\n  @expose\n  @permission(\"app.user\")\n  fn getDocumentAuthor(docId: String): String {\n      var doc_node = documents_by_id.get(docId);\n      if (doc_node == null) {\n          throw \"Document not found: ${docId}\";\n      }\n\n      var doc = doc_node.resolve();\n      if (doc.author == null) {\n          throw \"Document has no author\";\n      }\n\n      var author = doc.author.resolve();\n      return author.name;\n  }\n\n  // Or use optional chaining:\n  fn getDocumentAuthor(docId: String): String? {\n      return documents_by_id.get(docId)?.resolve().author?.resolve().name;\n  }\n\nSeverity: MEDIUM\nRisk: Runtime errors, poor user experience\n```\n\n### Step 3.3: Insufficient Error Messages\n\nCheck for helpful error messages:\n\n**Example Output**:\n```\n📍 backend/src/api/user_api.gcl:45\n\n⚠️ ERROR HANDLING: Unhelpful error message\n\nFunction:\n  @expose\n  @permission(\"app.admin\")\n  fn createUser(...): User {\n      if (email.size() == 0) {\n          throw \"Error\";  ← Generic, unhelpful\n      }\n      // ...\n  }\n\nProblem:\n  - Error message is too generic\n  - Doesn't explain what went wrong\n  - Doesn't tell user how to fix it\n\nRecommendation:\n  if (email.size() == 0) {\n      throw \"Email address is required\";  ← Clear and specific\n  }\n\n  if (!email.contains(\"@\")) {\n      throw \"Invalid email format. Email must contain @\";\n  }\n\nSeverity: LOW\nRisk: Poor user experience, support burden\n```\n\n---\n\n## Phase 4: Type Safety Review\n\n### Objective\nEnsure proper use of types, especially @volatile for API responses.\n\n### Step 4.1: Missing @volatile on Response Types\n\nFind response types missing @volatile:\n\n```bash\n# Extract return types from @expose functions\ngrep -rn \"@expose\" backend/src/api/ --include=\"*.gcl\" -A 5 | grep \"fn.*:\" | sed 's/.*: //' | sed 's/ {.*//' | sort -u\n```\n\nFor each type, check if it has @volatile decorator:\n\n**Example Output**:\n```\n📍 backend/src/api/api_types.gcl:45\n\n⚠️ TYPE SAFETY: Missing @volatile on API response type\n\nType:\n  type SearchResults {  ← Used in API, should be @volatile\n      items: Array<ResultView>;\n      total: int;\n      offset: int;\n  }\n\nUsed in:\n  - backend/src/api/search_api.gcl:23 (return type)\n\nProblem:\n  - API response type not marked as @volatile\n  - Will be persisted unnecessarily\n  - Wastes database storage\n\nRecommendation:\n  @volatile\n  type SearchResults {\n      items: Array<ResultView>;\n      total: int;\n      offset: int;\n  }\n\nSeverity: MEDIUM\nRisk: Unnecessary persistence, database bloat\n```\n\n### Step 4.2: Loose Parameter Types\n\nFind endpoints with overly broad types:\n\n```bash\n# Find endpoints using Object, any, or dynamic types\ngrep -rn \"@expose\" backend/src/api/ --include=\"*.gcl\" -A 10 | grep -E \": Object|: any\"\n```\n\n**Example Output**:\n```\n📍 backend/src/api/generic_api.gcl:34\n\n⚠️ TYPE SAFETY: Overly broad parameter type\n\nFunction:\n  @expose\n  @permission(\"app.user\")\n  fn processData(data: Object): Result {  ← Too generic!\n      // ... process\n  }\n\nProblem:\n  - Parameter type is too broad (Object)\n  - No type checking on input\n  - Error-prone and hard to maintain\n\nRecommendation:\n  Define specific type for input:\n\n  @volatile\n  type DataInput {\n      name: String;\n      value: int;\n      metadata: Map<String, String>?;\n  }\n\n  @expose\n  @permission(\"app.user\")\n  fn processData(data: DataInput): Result {\n      // Now type-safe\n  }\n\nSeverity: MEDIUM\nRisk: Runtime errors, maintenance difficulty\n```\n\n---\n\n## Phase 5: Best Practices Review\n\n### Objective\nEnsure endpoints follow GreyCat and API design best practices.\n\n### Step 5.1: Naming Consistency\n\nCheck endpoint naming conventions:\n\n```bash\n# Extract all function names from @expose\ngrep -rn \"@expose\" backend/src/api/ --include=\"*.gcl\" -A 1 | grep \"fn \" | sed 's/.*fn //' | sed 's/(.*//'\n```\n\nVerify:\n- Verb-based names (get, create, update, delete, search, list)\n- Consistent naming pattern\n- Clear and descriptive\n\n**Example Output**:\n```\n⚠️ NAMING: Inconsistent endpoint naming\n\nFound endpoints:\n  ✓ getDocument          (good - clear verb)\n  ✓ createUser           (good - clear verb)\n  ✓ searchDocuments      (good - clear verb)\n  ⚠️ document            (bad - no verb, unclear)\n  ⚠️ data                (bad - too generic)\n  ⚠️ process             (bad - unclear what it processes)\n\nRecommendation:\n  - Rename \"document\" → \"getDocument\" or \"listDocuments\"\n  - Rename \"data\" → \"getData\" or be more specific\n  - Rename \"process\" → \"processSearchQuery\" (be specific)\n\nSeverity: LOW\nRisk: API confusion, poor developer experience\n```\n\n### Step 5.2: Missing Documentation\n\nFind endpoints without comments:\n\n```bash\n# Find @expose functions without preceding comment\ngrep -rn \"@expose\" backend/src/api/ --include=\"*.gcl\" -B 3 | grep -v \"//.*\"\n```\n\n**Example Output**:\n```\n📍 backend/src/api/search_api.gcl:67\n\n⚠️ DOCUMENTATION: Missing function documentation\n\nFunction:\n  @expose\n  @permission(\"app.user\")\n  fn advancedSearch(query: String, filters: SearchFilters): SearchResults {\n      // ... complex logic\n  }\n\nProblem:\n  - No description of what function does\n  - No explanation of parameters\n  - No documentation of return value\n  - Difficult for other developers to use\n\nRecommendation:\n  /**\n   * Performs advanced search with filters\n   *\n   * @param query - Search query string (max 1000 chars)\n   * @param filters - Optional filters (formex type, date range, etc.)\n   * @returns SearchResults with paginated items and metadata\n   */\n  @expose\n  @permission(\"app.user\")\n  fn advancedSearch(query: String, filters: SearchFilters): SearchResults {\n      // ...\n  }\n\nSeverity: LOW\nRisk: Poor maintainability, unclear API\n```\n\n### Step 5.3: Overly Complex Endpoints\n\nFind endpoints with too much logic:\n\n```bash\n# Find @expose functions with many lines\nfor file in $(find backend/src/api -name \"*.gcl\"); do\n    grep -n \"@expose\" \"$file\" -A 100 | awk '/^[0-9]+-fn /,/^[0-9]+-}/' | wc -l\ndone\n```\n\nFunctions > 50 lines should be reviewed.\n\n**Example Output**:\n```\n📍 backend/src/api/complex_api.gcl:23\n\n⚠️ COMPLEXITY: Endpoint too complex\n\nFunction: processComplexWorkflow\nLines: 147 lines\n\nProblem:\n  - Business logic in API layer (should be in service)\n  - Difficult to test\n  - Difficult to maintain\n  - Violates separation of concerns\n\nRecommendation:\n  Move logic to service:\n\n  // In backend/src/service/workflow_service.gcl\n  abstract type WorkflowService {\n      static fn processComplexWorkflow(input: Input): Result {\n          // ... 147 lines of logic here\n      }\n  }\n\n  // In API (simple delegation)\n  @expose\n  @permission(\"app.user\")\n  fn processComplexWorkflow(input: Input): Result {\n      return WorkflowService::processComplexWorkflow(input);\n  }\n\nSeverity: MEDIUM\nRisk: Hard to test, maintain, debug\n```\n\n---\n\n## Phase 6: API Design Review\n\n### Objective\nEnsure RESTful principles and good API design.\n\n### Step 6.1: HTTP Semantics\n\nWhile GreyCat uses POST for all calls, function names should reflect intent:\n\n**Example Output**:\n```\n⚠️ API DESIGN: Misleading function name\n\nFunction:\n  @expose\n  fn getData(...): bool {  ← Returns bool but name says \"get\"\n      // Actually modifies data\n      item.status = \"processed\";\n      return true;\n  }\n\nProblem:\n  - Name implies read operation (\"get\")\n  - Actually performs write operation\n  - Misleading to API consumers\n\nRecommendation:\n  Rename to reflect mutation:\n  - updateDataStatus\n  - processData\n  - markAsProcessed\n\nSeverity: MEDIUM\nRisk: API misuse, unexpected side effects\n```\n\n### Step 6.2: Return Value Consistency\n\nCheck for consistent return patterns:\n\n**Example Output**:\n```\n⚠️ API DESIGN: Inconsistent return values\n\nFound patterns:\n  - getDocument() returns DocumentView?\n  - getUser() returns User (throws if not found)\n  - getCase() returns null\n\nProblem:\n  - Inconsistent null handling\n  - Some throw, some return null\n  - Confusing for API consumers\n\nRecommendation:\n  Standardize on one pattern:\n\n  Option 1: Nullable returns\n  - fn getDocument(id): DocumentView?\n  - Returns null if not found\n  - Caller handles null\n\n  Option 2: Throw on not found\n  - fn getDocument(id): DocumentView\n  - Throws \"Document not found\" if missing\n  - Caller uses try-catch\n\n  Pick one and use consistently across all endpoints.\n\nSeverity: MEDIUM\nRisk: Inconsistent API behavior, confusion\n```\n\n---\n\n## Output Format\n\n### Executive Summary\n\n```\n===============================================================================\nAPI ENDPOINT REVIEW\n===============================================================================\n\nEndpoints Analyzed: 34\nFiles Scanned: 12\n\nISSUES FOUND:\n\nCRITICAL (Immediate Fix Required):\n  [ ] 3 endpoints missing @permission decorator\n  [ ] 2 sensitive data exposure risks\n  [ ] 1 authentication bypass\n\nHIGH (Fix This Sprint):\n  [ ] 5 endpoints missing pagination\n  [ ] 4 expensive operations (nested loops)\n  [ ] 3 missing error handling\n  [ ] 2 N+1 query patterns\n\nMEDIUM (Fix Next Sprint):\n  [ ] 8 missing @volatile decorators\n  [ ] 6 insufficient input validation\n  [ ] 4 overly complex endpoints\n  [ ] 3 null pointer risks\n\nLOW (Nice to Have):\n  [ ] 12 missing documentation\n  [ ] 7 naming inconsistencies\n  [ ] 5 minor optimizations\n\nTOTAL ISSUES: 58\n\nESTIMATED FIX TIME:\n  Critical: 2-3 hours\n  High:     1 day\n  Medium:   2 days\n  Low:      1 day\n\n===============================================================================\n```\n\n### Detailed Report\n\nInclude all findings from each phase with:\n- Location (file:line)\n- Severity\n- Problem description\n- Code example\n- Recommendation\n- Estimated fix time\n\n---\n\n## Success Criteria\n\n✓ **All @expose functions analyzed** (across all API files)\n✓ **Security issues identified** (permissions, validation, data exposure)\n✓ **Performance issues flagged** (pagination, expensive ops, N+1 queries)\n✓ **Error handling gaps found** (try-catch, null checks, messages)\n✓ **Type safety checked** (@volatile, parameter types)\n✓ **Best practices validated** (naming, documentation, complexity)\n✓ **Report generated** with prioritized action items\n\n---\n\n## Notes\n\n- **Run After Every Sprint**: APIs change frequently\n- **Before Releases**: Critical for production readiness\n- **Security First**: Fix CRITICAL issues immediately\n- **Incremental Fixes**: Don't try to fix everything at once\n- **Re-run After Fixes**: Verify improvements\n\n---\n\n## Example Workflow\n\n```bash\n# 1. Complete sprint with 3 new API endpoints\n# 2. Run API review\n/apicheck\n\n# 3. Review findings\n# - 1 CRITICAL: Missing permission on deleteDocument\n# - 2 HIGH: Missing pagination on listDocuments\n# - 3 MEDIUM: Missing @volatile on response types\n\n# 4. Fix CRITICAL immediately\n# Add @permission(\"app.admin\") to deleteDocument\n\n# 5. Create issues for HIGH/MEDIUM\n# Track in sprint backlog\n\n# 6. Re-run review after fixes\n/apicheck\n# Verify CRITICAL issues resolved\n\n# 7. Before release, ensure all CRITICAL/HIGH fixed\n```\n",
        "plugins/greycat/commands/backend.md": "---\nname: backend\ndescription: Comprehensive backend review and cleanup for GreyCat projects\nallowed-tools: Bash, Read, Grep, Glob, Edit, Write\n---\n\n# Backend Review & Cleanup\n\n**Purpose**: Comprehensive analysis of GreyCat backend code to identify dead code, duplications, anti-patterns, and optimization opportunities\n\n**Run After**: Each sprint, before releases, during refactoring sessions\n\n---\n\n## Overview\n\nThis command performs a multi-phase analysis:\n\n1. **Phase 1**: Dead Code Detection\n2. **Phase 2**: Code Duplication Analysis\n3. **Phase 3**: Anti-Pattern Detection\n4. **Phase 4**: Optimization Opportunities\n5. **Phase 5**: GreyCat Best Practices Check\n6. **Phase 6**: Cleanup (optional, with confirmation)\n\n---\n\n## Phase 1: Dead Code Detection\n\n### Objective\nIdentify unused code that can be safely removed.\n\n### Step 1.1: Find Unused Types\n\n**A. Find All Type Definitions**:\n```bash\n# Find all types (excluding @volatile API response types initially)\ngrep -rn \"^type [A-Z]\" backend/src/ --include=\"*.gcl\" | grep -v \"@volatile\"\n```\n\n**B. Check Usage**:\nFor each type found, search for references:\n```bash\n# Search for type name usage across codebase\ngrep -r \"TypeName\" backend/ --include=\"*.gcl\" | grep -v \"^type TypeName\"\n```\n\n**C. Categorize**:\n- **SAFE TO DELETE**: Type defined but never used (0 references outside definition)\n- **KEEP**: Type used in other files\n- **REVIEW**: Type only used in same file (might be helper)\n\n**Output Format**:\n```\n📍 backend/src/model/old_model.gcl:42\n\n❌ UNUSED TYPE: OldDataStructure\n   References: 0\n   Defined: Line 42\n   Safe to delete: YES\n\n   Impact: None (no usages found)\n```\n\n### Step 1.2: Find Unused Functions\n\n**A. Find All Functions**:\n```bash\n# Find static functions (services)\ngrep -rn \"static fn [a-z_]\" backend/src/service/ --include=\"*.gcl\"\n\n# Find type methods\ngrep -rn \"^\\s*fn [a-z_]\" backend/src/model/ --include=\"*.gcl\"\n\n# Find API endpoints\ngrep -rn \"@expose\" backend/src/api/ --include=\"*.gcl\" -A 1\n```\n\n**B. Check Usage**:\nFor each function:\n```bash\n# Search for function calls\ngrep -r \"FunctionName(\" backend/ --include=\"*.gcl\"\n```\n\n**C. Special Cases**:\n- **@expose functions**: Keep even if no internal calls (called externally)\n- **main()**: Keep (entry point)\n- **@test functions**: Keep (test suite)\n\n**Output Format**:\n```\n📍 backend/src/service/data/helper_service.gcl:156\n\n❌ UNUSED FUNCTION: processOldFormat\n   References: 0 call sites\n   Type: Static function\n   Safe to delete: YES\n\n   Code:\n   156: static fn processOldFormat(data: String): Result {\n   157:     // ... 20 lines of unused logic\n   177: }\n```\n\n### Step 1.3: Find Unused Global Variables\n\n**A. Find Global Variables**:\n```bash\n# Find module-level variables\ngrep -rn \"^var [a-z_]\" backend/src/model/ --include=\"*.gcl\"\n```\n\n**B. Check Usage**:\n```bash\n# Search for variable references\ngrep -r \"variableName\" backend/ --include=\"*.gcl\"\n```\n\n**C. Important**:\n- **Global indices** (nodeIndex, nodeList) might have low usage but serve as primary storage\n- Check if variable is populated during import/init\n\n**Output Format**:\n```\n📍 backend/src/model/cache.gcl:12\n\n⚠️ UNUSED VARIABLE: temp_cache\n   Type: nodeIndex<String, String>\n   References: 1 (only definition)\n   Safe to delete: REVIEW (might be populated dynamically)\n```\n\n### Step 1.4: Find Unused Imports\n\n**Note**: GreyCat uses `@include` for files, not per-symbol imports. Check if entire files are unused.\n\n```bash\n# Files with no references from other files\nfor file in backend/src/**/*.gcl; do\n    filename=$(basename \"$file\" .gcl)\n    refs=$(grep -r \"$filename\" backend/ --include=\"*.gcl\" | grep -v \"$file\" | wc -l)\n    if [ $refs -eq 0 ]; then\n        echo \"Unused file: $file\"\n    fi\ndone\n```\n\n### Step 1.5: Find Commented Code\n\n```bash\n# Find large commented blocks (potential dead code)\ngrep -rn \"^//.*fn \\|^//.*type \\|^//.*var \" backend/ --include=\"*.gcl\"\n```\n\n**Output Format**:\n```\n📍 backend/src/service/old_service.gcl:45-78\n\n⚠️ COMMENTED CODE: 33 lines\n   Content: Old implementation of processData function\n   Recommendation: Delete if no longer needed, or document why kept\n```\n\n---\n\n## Phase 2: Code Duplication Analysis\n\n### Objective\nIdentify repeated code patterns that could be refactored.\n\n### Step 2.1: Find Duplicated Logic Patterns\n\n**Common Patterns to Detect**:\n\n**A. Repeated Validation Logic**:\n```bash\n# Search for similar validation patterns\ngrep -rn \"if.*== null.*return null\" backend/ --include=\"*.gcl\"\ngrep -rn \"if.*\\.size\\(\\) == 0.*return\" backend/ --include=\"*.gcl\"\n```\n\n**Example Output**:\n```\n📍 DUPLICATION: Null checks (8 occurrences)\n\nLocations:\n  - backend/src/service/user_service.gcl:23\n  - backend/src/service/document_service.gcl:45\n  - backend/src/service/search_service.gcl:67\n  ... (5 more)\n\nPattern:\n  if (input == null) {\n      return null;\n  }\n\nSuggestion:\n  Extract to validation utility:\n\n  abstract type ValidationUtils {\n      static fn requireNonNull<T>(value: T?, errorMsg: String): T {\n          if (value == null) {\n              throw errorMsg;\n          }\n          return value;\n      }\n  }\n\n  Usage:\n  var validated = ValidationUtils::requireNonNull(input, \"Input required\");\n```\n\n**B. Repeated Query Patterns**:\n```bash\n# Find similar loops over global indices\ngrep -rn \"for.*in.*_by_id\" backend/ --include=\"*.gcl\" -A 3\n```\n\n**C. Repeated Type Conversions**:\n```bash\n# Find repeated mapping logic\ngrep -rn \"\\.map\\|for.*in.*{\" backend/ --include=\"*.gcl\" -B 2 -A 5\n```\n\n### Step 2.2: Find Similar Type Definitions\n\nCompare types with similar field structures:\n\n```bash\n# Extract all type definitions\ngrep -rn \"^type [A-Z]\" backend/src/model/ --include=\"*.gcl\" -A 10\n```\n\nAnalyze for:\n- Types with >70% field overlap\n- Potential inheritance opportunities\n- Consolidation candidates\n\n**Example Output**:\n```\n📍 SIMILAR TYPES: DocumentView and DocumentDetailView\n\nDocumentView (backend/src/api/api_types.gcl:45):\n  - id: String\n  - title: String\n  - date: time?\n  - type: String\n\nDocumentDetailView (backend/src/api/api_types.gcl:89):\n  - id: String\n  - title: String\n  - date: time?\n  - type: String\n  - content: String      ← Only difference\n  - sections: Array<SectionView>\n\nSimilarity: 80% (4/5 fields identical)\n\nSuggestion:\n  Consider making DocumentDetailView extend DocumentView,\n  or create shared base type for common fields.\n```\n\n### Step 2.3: Find Copy-Pasted Code Blocks\n\nUse heuristics to detect copy-pasted code:\n\n1. Identical line sequences (>5 lines)\n2. Similar variable names with different prefixes\n3. Repeated error handling patterns\n\n**Example Output**:\n```\n📍 DUPLICATED BLOCK: Error handling (identical in 3 locations)\n\nLocations:\n  - backend/src/api/user_api.gcl:67-74\n  - backend/src/api/document_api.gcl:123-130\n  - backend/src/api/search_api.gcl:45-52\n\nCode (8 lines):\n  try {\n      // ... operation\n  } catch (ex) {\n      error(\"Operation failed: ${ex}\");\n      return ErrorResponse { message: \"Internal error\" };\n  }\n\nSuggestion:\n  Extract to error handling utility:\n\n  abstract type ErrorHandler {\n      static fn handle<T>(operation: fn(): T, errorMsg: String): T {\n          try {\n              return operation();\n          } catch (ex) {\n              error(\"${errorMsg}: ${ex}\");\n              throw \"Internal error\";\n          }\n      }\n  }\n```\n\n---\n\n## Phase 3: Anti-Pattern Detection\n\n### Objective\nIdentify GreyCat-specific anti-patterns and bad practices.\n\n### Step 3.1: Incorrect Persistent Collection Usage\n\n**Pattern**: Using nodeList/nodeIndex for temporary local variables\n\n```bash\n# Find local variable declarations with persistent types\ngrep -rn \"var.*= nodeList\\|var.*= nodeIndex\" backend/ --include=\"*.gcl\" -B 2 -A 2\n```\n\n**Analysis**:\n- If variable is inside function body → **LIKELY WRONG** (use Array/Map)\n- If variable is type field or global → **CORRECT** (persistent)\n\n**Example Output**:\n```\n📍 backend/src/service/builder_service.gcl:34\n\n⚠️ ANTI-PATTERN: Local variable using nodeList\n\nCode:\n  32: fn buildResults(items: Array<Item>): Array<Result> {\n  33:     var results = nodeList<node<Result>> {};  ← WRONG\n  34:     for (i, item in items) {\n  35:         results.add(node<Result>{ ... });\n  36:     }\n  37:     return results;  ← Type mismatch (nodeList vs Array)\n  38: }\n\nProblem:\n  - Using persistent nodeList for temporary local variable\n  - Should use Array for non-persisted collections\n\nFix:\n  var results = Array<node<Result>> {};\n\nImpact: Performance overhead, unnecessary persistence\nPriority: HIGH\n```\n\n### Step 3.2: Missing @volatile on API Response Types\n\n**Pattern**: Types returned by @expose without @volatile\n\n```bash\n# Find @expose functions\ngrep -rn \"@expose\" backend/src/api/ --include=\"*.gcl\" -A 5\n\n# For each, check return type has @volatile\n```\n\n**Example Output**:\n```\n📍 backend/src/api/search_api.gcl:23\n\n⚠️ ANTI-PATTERN: Missing @volatile on API response type\n\nFunction:\n  @expose\n  fn search(...): SearchResults {  ← Return type SearchResults\n      ...\n  }\n\nType Definition (backend/src/api/api_types.gcl:45):\n  type SearchResults {  ← Missing @volatile decorator\n      items: Array<ResultView>;\n      total: int;\n  }\n\nProblem:\n  - API response type should be @volatile (not persisted)\n  - Missing @volatile causes unnecessary database storage\n\nFix:\n  @volatile\n  type SearchResults {\n      items: Array<ResultView>;\n      total: int;\n  }\n\nPriority: MEDIUM\n```\n\n### Step 3.3: Direct Object Storage in Collections\n\n**Pattern**: Storing objects directly instead of node references\n\n```bash\n# Find nodeList/nodeIndex storing objects not nodes\ngrep -rn \"nodeList<[^n].*>\" backend/ --include=\"*.gcl\"\ngrep -rn \"nodeIndex<.*,\\s*[^n].*>\" backend/ --include=\"*.gcl\"\n```\n\n**Example Output**:\n```\n📍 backend/src/model/data.gcl:12\n\n⚠️ ANTI-PATTERN: Storing objects directly in nodeList\n\nCode:\n  var items: nodeList<Item>;  ← WRONG (should be nodeList<node<Item>>)\n\nProblem:\n  - nodeList should store node references, not objects directly\n  - Objects should be wrapped in nodes for persistence\n\nFix:\n  var items: nodeList<node<Item>>;\n\n  // When adding:\n  items.add(node<Item>{ Item { ... } });\n\nPriority: HIGH (breaks persistence model)\n```\n\n### Step 3.4: Missing Collection Initialization\n\n**Pattern**: Non-nullable collections not initialized in constructor\n\n```bash\n# Find type fields with collections\ngrep -rn \"^\\s*[a-z_]*:\\s*\\(Array\\|Map\\|nodeList\\|nodeIndex\\)\" backend/src/model/ --include=\"*.gcl\"\n```\n\nCheck if initialization happens in object creation.\n\n**Example Output**:\n```\n📍 backend/src/model/document.gcl:8\n\n⚠️ ANTI-PATTERN: Non-nullable collection not initialized\n\nType:\n  type Document {\n      id: String;\n      chunks: nodeList<node<Chunk>>;  ← Non-nullable, must initialize\n  }\n\nUsage (backend/src/service/import_service.gcl:45):\n  var doc = node<Document>{ Document {\n      id: \"123\",\n      // chunks not initialized ← WILL FAIL at runtime\n  }};\n\nProblem:\n  - Non-nullable collections must be initialized on object creation\n  - Missing initialization causes runtime errors\n\nFix Option 1 (Make nullable):\n  chunks: nodeList<node<Chunk>>?;\n\nFix Option 2 (Initialize):\n  var doc = node<Document>{ Document {\n      id: \"123\",\n      chunks: nodeList<node<Chunk>> {}\n  }};\n\nPriority: HIGH (runtime error)\n```\n\n### Step 3.5: Incorrect Null Handling\n\n**Pattern**: Missing null checks, incorrect optional chaining\n\n```bash\n# Find potential null dereference\ngrep -rn \"\\.resolve()\\..*\\..*\\|->.*->.*\" backend/ --include=\"*.gcl\"\n```\n\n**Example Output**:\n```\n📍 backend/src/service/data_service.gcl:67\n\n⚠️ ANTI-PATTERN: Potential null pointer dereference\n\nCode:\n  var name = user.resolve().profile.name;  ← Can fail if profile is null\n\nProblem:\n  - No null check on profile field\n  - Will throw runtime error if profile is null\n\nFix:\n  var name = user.resolve().profile?.name ?? \"Unknown\";\n\nPriority: MEDIUM\n```\n\n---\n\n## Phase 4: Optimization Opportunities\n\n### Objective\nIdentify performance bottlenecks and inefficiencies.\n\n### Step 4.1: Expensive Operations in Loops\n\n```bash\n# Find loops with potential expensive operations\ngrep -rn \"for.*in.*{\" backend/ --include=\"*.gcl\" -A 10 | grep -E \"\\.resolve\\(\\)|\\.get\\(|->.*->\"\n```\n\n**Example Output**:\n```\n📍 backend/src/service/query_service.gcl:89\n\n⚠️ OPTIMIZATION: Repeated node resolution in loop\n\nCode:\n  for (i, doc_node in documents) {\n      var doc = doc_node.resolve();      ← Resolving in every iteration\n      var chunks = doc.chunks.resolve(); ← Nested resolution\n      for (j, chunk_node in chunks) {\n          var chunk = chunk_node.resolve();\n          // process chunk\n      }\n  }\n\nProblem:\n  - Resolving nodes inside nested loops\n  - Potential for hundreds/thousands of resolution calls\n\nOptimization:\n  // Batch resolve if possible\n  // Or restructure query to minimize resolution depth\n\nEstimated Impact: HIGH (if documents is large collection)\nPriority: MEDIUM\n```\n\n### Step 4.2: Missing Indices on Frequent Queries\n\nAnalyze query patterns to suggest missing indices:\n\n```bash\n# Find linear searches that could use indices\ngrep -rn \"for.*in.*if.*==.*return\" backend/ --include=\"*.gcl\"\n```\n\n**Example Output**:\n```\n📍 backend/src/service/user_service.gcl:34\n\n⚠️ OPTIMIZATION: Linear search could use index\n\nCode:\n  fn getUserByEmail(email: String): User? {\n      for (i, user_node in users) {\n          var user = user_node.resolve();\n          if (user.email == email) {\n              return user;\n          }\n      }\n      return null;\n  }\n\nProblem:\n  - O(n) linear search through all users\n  - Called frequently (23 call sites found)\n\nSuggestion:\n  Create email index in model:\n\n  var users_by_email: nodeIndex<String, node<User>>;\n\n  fn getUserByEmail(email: String): User? {\n      var user_node = users_by_email.get(email);\n      return user_node?.resolve();  // O(1) lookup\n  }\n\nEstimated Impact: HIGH (if users collection is large)\nPriority: HIGH\n```\n\n### Step 4.3: Inefficient Loops\n\n```bash\n# Find loops that could use for-in syntax or better iteration\ngrep -rn \"for (.*=.*;.*<.*\\.size().*;.*++)\" backend/ --include=\"*.gcl\"\n```\n\n**Example Output**:\n```\n📍 backend/src/service/processor.gcl:56\n\n⚠️ OPTIMIZATION: C-style loop could use for-in\n\nCode:\n  for (var i = 0; i < items.size(); i++) {\n      var item = items[i];\n      process(item);\n  }\n\nOptimization:\n  for (i, item in items) {\n      process(item);\n  }\n\nBenefits:\n  - More readable\n  - Potentially faster (no repeated .size() calls)\n  - Idiomatic GreyCat style\n\nPriority: LOW (readability improvement)\n```\n\n### Step 4.4: Unnecessary Data in API Responses\n\n```bash\n# Find @expose functions returning full objects\ngrep -rn \"@expose\" backend/src/api/ --include=\"*.gcl\" -A 10\n```\n\nCheck if response includes unnecessary fields.\n\n**Example Output**:\n```\n📍 backend/src/api/document_api.gcl:45\n\n⚠️ OPTIMIZATION: API returning too much data\n\nFunction:\n  @expose\n  fn searchDocuments(...): Array<Document> {\n      // Returns full Document objects\n  }\n\nProblem:\n  - Document has many fields (20+)\n  - Search results only need: id, title, excerpt\n  - Sending unnecessary data over network\n\nSuggestion:\n  Create lightweight view type:\n\n  @volatile\n  type DocumentSearchResult {\n      id: String;\n      title: String;\n      excerpt: String;\n      score: float;\n  }\n\n  fn searchDocuments(...): Array<DocumentSearchResult> {\n      // Map to view\n  }\n\nEstimated Impact: MEDIUM (network payload reduction)\nPriority: MEDIUM\n```\n\n### Step 4.5: Unnecessary Persistence\n\n**Detect node<T> usage where plain objects suffice**:\n\n```bash\n# Find local variables with node types\ngrep -rn \"var [a-z_][a-zA-Z0-9_]* = node\\(List\\|Index\\|Time\\|Geo\\)?<\" backend/src --include=\"*.gcl\" | \\\n    grep \"    \" | \\  # Has indentation (not module-level)\n    while IFS=: read -r file line content; do\n        echo \"  ⚠ $file:$line - Local variable using persistent type\"\n    done\n\n# Find function returns with nodeList/nodeIndex\ngrep -rn \"fn [a-z_][a-zA-Z0-9_]*(.*).*: node\\(List\\|Index\\)\" backend/src --include=\"*.gcl\" | \\\n    while IFS=: read -r file line content; do\n        echo \"  ⚠ $file:$line - Function returning persistent collection\"\n    done\n```\n\n**Example Output**:\n```\n📍 backend/src/service/processor.gcl:120\n\n🔴 CRITICAL: Unnecessary node allocation\n\nCode:\n  fn process_items(items: Array<Item>): nodeList<node<Item>> {\n      var results = nodeList<node<Item>> {};\n      for (item in items) {\n          results.add(node<Item>{item});\n      }\n      return results;\n  }\n\nProblem:\n  - Local variable using nodeList (should be Array)\n  - Creating persistent nodes for temporary results\n  - Returning nodeList from function (should be Array)\n\nOptimization:\n  fn process_items(items: Array<Item>): Array<Item> {\n      var results = Array<Item> {};\n      for (item in items) {\n          results.add(item);\n      }\n      return results;\n  }\n\nBenefits:\n  - No unnecessary persistence overhead\n  - Cleaner code\n  - Better performance\n\nEstimated Impact: HIGH\nPriority: CRITICAL\n```\n\n### Step 4.6: Reimplemented Native Functions\n\n**Detect custom implementations of stdlib functions**:\n\n```bash\n# Look for custom sort implementations\ngrep -rn \"fn [a-z_]*sort\" backend/src --include=\"*.gcl\" -A 20 | \\\n    grep -A 15 \"for.*for\" | \\  # Nested loops suggest manual sort\n    head -20\n\n# Look for custom min/max\ngrep -rn \"fn find_\\(max\\|min\\)\" backend/src --include=\"*.gcl\"\n\n# Look for custom string join\ngrep -rn \"fn [a-z_]*join\" backend/src --include=\"*.gcl\" -A 10\n```\n\n**Example Output**:\n```\n📍 backend/src/util/array_utils.gcl:23\n\n🟡 MEDIUM: Reimplemented native function\n\nCode:\n  fn sort_by_priority(items: Array<Item>): Array<Item> {\n      // Bubble sort implementation\n      for (i in 0..items.size()) {\n          for (j in i+1..items.size()) {\n              if (items[i]->priority > items[j]->priority) {\n                  // swap...\n              }\n          }\n      }\n      return items;\n  }\n\nProblem:\n  - Custom bubble sort (O(n²))\n  - Native sort_by is faster and cleaner\n\nOptimization:\n  items.sort_by(Item::priority, SortOrder::asc);\n\nBenefits:\n  - Simpler code (1 line vs 10+)\n  - Better performance (native implementation)\n  - More maintainable\n\nEstimated Impact: MEDIUM\nPriority: MEDIUM\n```\n\n### Step 4.7: Useless Function Wrappers\n\n**Detect one-line wrappers**:\n\n```bash\n# Find functions with single return statement calling another function\nfor file in $(find backend/src -name \"*.gcl\"); do\n    awk '/^fn [a-z_].*{$/ {\n        func_line = NR;\n        func = $0;\n        getline;\n        if ($0 ~ /^    return .*::/) {\n            print FILENAME \":\" func_line \": One-line wrapper - \" func\n        }\n    }' \"$file\"\ndone\n```\n\n**Example Output**:\n```\n📍 backend/src/api/user_api.gcl:67\n\n🟡 MEDIUM: Useless function wrapper\n\nCode:\n  fn get_user(id: int): node<User>? {\n      return UserService::find_by_id(id);\n  }\n\nProblem:\n  - Single-line wrapper adds no value\n  - Just forwards call to service\n  - Extra indirection for no benefit\n\nSuggestion:\n  Remove wrapper, call UserService::find_by_id() directly\n\nBenefits:\n  - Less code to maintain\n  - Clearer call chain\n  - Slightly faster (no extra function call)\n\nEstimated Impact: LOW\nPriority: LOW\n```\n\n### Step 4.8: Algorithmic Complexity\n\n**Detect O(n²) operations**:\n\n```bash\n# Find nested loops with conditionals (potential O(n²))\ngrep -rn \"for.*in.*{\" backend/src --include=\"*.gcl\" -A 5 | \\\n    grep -B 1 \"for.*in.*{\" | \\\n    grep -A 3 \"if.*==\" | \\\n    head -30\n```\n\n**Example Output**:\n```\n📍 backend/src/processor/matcher.gcl:89\n\n🔴 CRITICAL: O(n²) algorithmic complexity\n\nCode:\n  for (user in all_users) {\n      for (order in all_orders) {\n          if (order->user_id == user->id) {\n              process(order, user);\n          }\n      }\n  }\n\nProblem:\n  - Nested iteration: users × orders\n  - If 1000 users × 10000 orders = 10M iterations\n  - Linear search for matching orders\n\nOptimization:\n  // Create index if it doesn't exist:\n  var orders_by_user_id: nodeIndex<int, nodeList<node<Order>>>;\n\n  // Use O(1) lookup:\n  for (user in all_users) {\n      var user_orders = orders_by_user_id.get(user->id);\n      if (user_orders != null) {\n          for (i, order in user_orders) {\n              process(order, user);\n          }\n      }\n  }\n\nBenefits:\n  - O(n) instead of O(n²)\n  - 1000× faster for large datasets\n  - Scales better\n\nEstimated Impact: VERY HIGH\nPriority: CRITICAL\n```\n\n---\n\n## Phase 5: GreyCat Best Practices Check\n\n### Step 5.1: Service Layer Organization\n\nCheck for proper service abstraction:\n\n```bash\n# Find abstract types (services)\ngrep -rn \"^abstract type.*Service\" backend/src/service/ --include=\"*.gcl\"\n```\n\nVerify:\n- Services use `abstract type` pattern\n- Static functions for operations\n- No business logic in API layer\n\n**Example Output**:\n```\n✓ GOOD: Proper service organization\n\nServices found:\n  - UserService (backend/src/service/auth/user_service.gcl)\n  - DocumentService (backend/src/service/data/document_service.gcl)\n  - SearchService (backend/src/service/search/search_service.gcl)\n\nPattern compliance:\n  ✓ All use abstract type\n  ✓ All static functions\n  ✓ Clear separation from API layer\n```\n\n### Step 5.2: Global Index Definition Order\n\nGlobal indices must be defined before types that use them:\n\n```bash\n# Check definition order in model files\ngrep -rn \"^var\\|^type\" backend/src/model/ --include=\"*.gcl\"\n```\n\n**Example Output**:\n```\n⚠️ WARNING: Index defined after type that uses it\n\nFile: backend/src/model/data.gcl\n\nLine 10: type Document {\nLine 15:     // references documents_by_id\nLine 20: }\nLine 25: var documents_by_id: nodeIndex<String, node<Document>>;  ← Defined after type\n\nProblem:\n  - Global variables should be defined before types\n  - May cause initialization issues\n\nFix:\n  Move variable definition to top of file (before type definitions)\n\nPriority: MEDIUM\n```\n\n### Step 5.3: API Permission Checks\n\nVerify all @expose functions have @permission decorator:\n\n```bash\n# Find @expose without @permission\ngrep -rn \"@expose\" backend/src/api/ --include=\"*.gcl\" -A 1 | grep -v \"@permission\"\n```\n\n**Example Output**:\n```\n⚠️ WARNING: @expose function missing @permission\n\nFunction: backend/src/api/admin_api.gcl:34\n  @expose\n  fn deleteUser(userId: String): bool {  ← Missing @permission\n      ...\n  }\n\nProblem:\n  - No permission check on sensitive operation\n  - Function is publicly accessible\n\nFix:\n  @expose\n  @permission(\"app.admin\")  ← Add appropriate permission\n  fn deleteUser(userId: String): bool {\n      ...\n  }\n\nPriority: HIGH (security issue)\n```\n\n### Step 5.4: Error Handling at API Boundaries\n\nCheck for proper error handling in API functions:\n\n```bash\n# Find @expose functions without try-catch\ngrep -rn \"@expose\" backend/src/api/ --include=\"*.gcl\" -A 20 | grep -v \"try\\|catch\"\n```\n\n---\n\n## Phase 6: Cleanup (Interactive)\n\n### Objective\nOptionally clean up detected issues with user confirmation.\n\n### Step 6.1: Present Cleanup Options\n\nAfter analysis, present summary:\n\n```\n===============================================================================\nCLEANUP OPTIONS\n===============================================================================\n\nFound Issues:\n  [ ] 12 unused functions (safe to delete)\n  [ ] 5 unused types (safe to delete)\n  [ ] 3 unused global variables (needs review)\n  [ ] 8 duplicated code blocks (extract to utilities)\n  [ ] 15 anti-patterns (requires code changes)\n  [ ] 23 optimization opportunities (NEW):\n      - 5 unnecessary persistence (critical)\n      - 3 reimplemented native functions (medium)\n      - 5 useless wrappers (medium)\n      - 2 O(n²) complexity (critical)\n      - 8 other optimizations\n\nAuto-fix available for:\n  ✓ Unused code deletion\n  ✓ Unnecessary persistence (node<T> → plain objects)\n  ✓ Useless function wrappers (can be removed)\n  ! O(n²) operations (requires index creation - semi-automated)\n\nWhat would you like to do?\n\nA) Delete unused code (functions + types)\nB) Fix anti-patterns (persistence issues, missing @volatile)\nC) Fix performance issues (unnecessary persistence, wrappers)\nD) Show detailed report only (no changes)\nE) Custom selection (I'll ask for each category)\n```\n\n### Step 6.2: Delete Unused Code (If Confirmed)\n\nIf user selects option A:\n\n1. **Backup First**:\n```bash\ngit status\n# Ensure working directory is clean or create backup branch\n```\n\n2. **Delete Unused Functions**:\n```bash\n# For each unused function, use Edit tool to remove\n# Example:\n# Edit file: backend/src/service/helper.gcl\n# Remove lines 45-67 (unused function processOldFormat)\n```\n\n3. **Delete Unused Types**:\n```bash\n# For each unused type, use Edit tool to remove\n```\n\n4. **Verify After Deletion**:\n```bash\ngreycat-lang lint\n# Ensure no errors introduced\n```\n\n5. **Report**:\n```\n✓ Cleanup Complete\n\nDeleted:\n  - 12 unused functions (saved ~450 lines)\n  - 5 unused types (saved ~120 lines)\n\nTotal reduction: 570 lines\n\nNext steps:\n  1. Run: greycat-lang lint (should pass)\n  2. Run: greycat test (verify no broken tests)\n  3. Commit changes: git commit -m \"Clean up dead code\"\n```\n\n### Step 6.3: Fix Anti-Patterns (If Confirmed)\n\nIf user selects option B:\n\nFor each anti-pattern, apply automated fix:\n\n**Example: Fix persistent collection usage**:\n```bash\n# Find: var results = nodeList<T> {};\n# Replace: var results = Array<T> {};\n```\n\n**Example: Add @volatile to API response types**:\n```bash\n# Find type definition for API response\n# Add @volatile decorator before type\n```\n\nReport each fix applied and verify with lint.\n\n---\n\n## Output Format\n\n### Executive Summary\n\n```\n===============================================================================\nBACKEND REVIEW & CLEANUP REPORT\n===============================================================================\nProject: backend/src/\nFiles Analyzed: 45 .gcl files\nAnalysis Date: 2024-01-15\n\nSUMMARY:\n  Dead Code:           17 items (12 functions, 5 types)\n  Duplications:        8 code blocks\n  Anti-Patterns:       15 issues\n  Optimizations:       23 opportunities\n  Best Practice Gaps:  6 issues\n\nPRIORITY BREAKDOWN:\n  HIGH Priority:    18 issues (security, runtime errors, major performance)\n  MEDIUM Priority:  24 issues (performance, maintainability)\n  LOW Priority:     19 issues (readability, minor optimizations)\n\nTOTAL ISSUES:       61\n\nESTIMATED CLEANUP EFFORT:\n  Quick Wins:   2 hours  (delete dead code, fix anti-patterns)\n  Refactoring:  1 day    (extract duplications)\n  Optimization: 2 days   (add indices, restructure queries)\n\n===============================================================================\n```\n\n### Detailed Report (Sample)\n\n```\n===============================================================================\nHIGH PRIORITY ISSUES\n===============================================================================\n\n1. SECURITY: Missing @permission on admin function\n   📍 backend/src/api/admin_api.gcl:34\n   Priority: HIGH\n   Effort: 5 minutes\n   [Details above]\n\n2. ANTI-PATTERN: Local variable using nodeList\n   📍 backend/src/service/builder.gcl:67\n   Priority: HIGH\n   Effort: 2 minutes\n   [Details above]\n\n3. PERFORMANCE: Missing index on frequent query\n   📍 backend/src/service/user_service.gcl:34\n   Priority: HIGH\n   Effort: 15 minutes\n   [Details above]\n\n...\n\n===============================================================================\nMEDIUM PRIORITY ISSUES\n===============================================================================\n...\n\n===============================================================================\n```\n\n---\n\n## Success Criteria\n\n✓ **All backend files scanned** (.gcl files in backend/src/)\n✓ **Dead code identified** with safe-to-delete classification\n✓ **Duplications detected** with refactoring suggestions\n✓ **Anti-patterns found** with fix recommendations\n✓ **Optimizations suggested** with impact estimates\n✓ **Best practices checked** against GreyCat guidelines\n✓ **Report generated** with prioritized action items\n✓ **Lint passes** after cleanup (if cleanup performed)\n\n---\n\n## Notes\n\n- **Run in Clean State**: Ensure `greycat-lang lint` passes before running\n- **Backup Recommended**: Use git to create safety checkpoint\n- **Incremental Cleanup**: Fix HIGH priority issues first\n- **Test After Changes**: Run `greycat test` after cleanup\n- **Review Automated Fixes**: Some patterns need manual review\n\n---\n\n## Example Workflow\n\n```bash\n# 1. Run backend review\n/backend\n\n# 2. Review report (61 issues found)\n\n# 3. Choose cleanup option\n# → A) Delete unused code\n\n# 4. Cleanup executes\n# - 12 functions deleted\n# - 5 types removed\n# - 570 lines saved\n\n# 5. Verify\ngreycat-lang lint  # ✓ Passes\ngreycat test       # ✓ All tests pass\n\n# 6. Commit\ngit add backend/\ngit commit -m \"Backend cleanup: remove dead code, fix anti-patterns\"\n\n# 7. Address remaining issues\n# - Fix HIGH priority anti-patterns (15 min)\n# - Extract duplicated logic (1 hour)\n# - Add missing indices (30 min)\n\n# 8. Re-run review to verify improvements\n/backend\n# → Issues reduced from 61 to 23\n```\n",
        "plugins/greycat/commands/coverage.md": "---\nname: coverage\ndescription: Generate test coverage report and suggest new tests to implement\nallowed-tools: Bash, Read, Grep, Glob, Write\n---\n\n# Test Coverage Analysis & Gap Identification\n\n**Purpose**: Analyze current test coverage, identify critical untested code, and generate specific test suggestions prioritized by risk.\n\n**Run After**: Each development sprint, before releases, when adding new features\n\n---\n\n## Overview\n\nThis command performs a comprehensive test coverage analysis in 3 phases:\n\n1. **Phase 1**: Generate current coverage report\n2. **Phase 2**: Identify critical gaps (untested code)\n3. **Phase 3**: Suggest specific tests to implement (prioritized)\n\n---\n\n## Phase 1: Generate Coverage Report\n\n### Step 1.1: Run All Tests\n\n```bash\necho \"================================================================================\"\necho \"PHASE 1: GENERATING TEST COVERAGE REPORT\"\necho \"================================================================================\"\necho \"\"\n\necho \"Running all tests...\"\ngreycat test\nTEST_EXIT_CODE=$?\n\necho \"\"\nif [ $TEST_EXIT_CODE -eq 0 ]; then\n    echo \"✓ All tests passed\"\nelse\n    echo \"⚠ Some tests failed (exit code: $TEST_EXIT_CODE)\"\n    echo \"  Continuing with coverage analysis...\"\nfi\necho \"\"\n```\n\n### Step 1.2: Collect Test Files\n\nUse Glob to find all test files:\n\n```bash\n# Find all test files\nfind backend/test -name \"*_test.gcl\" -o -name \"*test.gcl\" 2>/dev/null\n```\n\nAnalyze each test file to extract:\n- Test function names (`@test fn test_name()`)\n- What services/types they test\n- Coverage scope (unit vs integration)\n\n### Step 1.3: Build Coverage Map\n\nFor each test file, identify:\n- **Service/Type Under Test**: Extract from imports or function calls\n- **Functions Tested**: Parse test assertions and function calls\n- **Coverage Type**: Unit, integration, or end-to-end\n\nCreate a coverage map:\n```\n{\n  \"PaginationService\": {\n    \"tested_functions\": [\"validateOffset\", \"validateLimit\"],\n    \"untested_functions\": [\"calculatePage\", \"calculateTotalPages\"],\n    \"test_files\": [\"pagination_service_test.gcl\"]\n  },\n  \"SearchEngine\": {\n    \"tested_functions\": [\"bm25Score\", \"normalizeScore\"],\n    \"untested_functions\": [],\n    \"test_files\": [\"search_engine_test.gcl\"]\n  }\n}\n```\n\n---\n\n## Phase 2: Identify Critical Gaps\n\n### Step 2.1: Scan Backend Code\n\nFind all services, API endpoints, and critical business logic:\n\n**A. Find All Services** (abstract types with static functions):\n```bash\n# Search for abstract types (services)\ngrep -r \"^abstract type.*Service\" backend/src/service/ --include=\"*.gcl\"\n```\n\n**B. Find All API Endpoints** (@expose functions):\n```bash\n# Search for @expose functions\ngrep -r \"@expose\" backend/src/api/ --include=\"*.gcl\" -A 2\n```\n\n**C. Find All Model Types**:\n```bash\n# Search for type definitions\ngrep -r \"^type [A-Z]\" backend/src/model/ --include=\"*.gcl\"\n```\n\n**D. Find Critical Business Logic**:\n- Functions with complex logic (> 20 lines)\n- Functions with multiple branches\n- Functions handling money/security/data integrity\n\n### Step 2.2: Cross-Reference with Tests\n\nFor each discovered item, check if tests exist:\n\n**Service Functions**:\n```gcl\n// Service: backend/src/service/auth/user_service.gcl\nabstract type UserService {\n    static fn createUser(...): User;      // ⚠️ No test found\n    static fn getUserByEmail(...): User?; // ✓ Tested in user_service_test.gcl\n    static fn deleteUser(...): bool;      // ⚠️ No test found\n}\n```\n\n**API Endpoints**:\n```gcl\n// API: backend/src/api/search_api.gcl\n@expose\n@permission(\"app.user\")\nfn search(query: String, ...): SearchResponse {  // ⚠️ No integration test\n    // ...\n}\n```\n\n**Model Validation**:\n```gcl\n// Model: backend/src/model/user.gcl\ntype User {\n    email: String;\n\n    fn validate(): bool {  // ⚠️ No test found\n        // complex validation logic\n    }\n}\n```\n\n### Step 2.3: Calculate Risk Scores\n\nPrioritize gaps by risk (0-100):\n\n**Risk Formula**:\n```\nRisk = (Complexity × 20) + (Usage × 30) + (Criticality × 50)\n\nWhere:\n- Complexity: Lines of code / branches (0-1 normalized)\n- Usage: Number of call sites (0-1 normalized)\n- Criticality: Domain importance (0-1: 0=low, 0.5=medium, 1=high)\n```\n\n**Criticality Heuristics**:\n- **HIGH (1.0)**: Auth, payments, data mutations, @expose endpoints\n- **MEDIUM (0.5)**: Search, filtering, data retrieval, validation\n- **LOW (0.0)**: Utilities, formatters, helpers, UI-only code\n\n---\n\n## Phase 3: Suggest Specific Tests\n\n### Step 3.1: Generate Test Suggestions\n\nFor each gap, create actionable test suggestions with:\n\n1. **Test file name**: `backend/test/xxx_test.gcl`\n2. **Test function names**: Specific to scenario\n3. **Test code template**: Ready-to-use skeleton\n4. **Priority**: HIGH/MEDIUM/LOW (based on risk score)\n5. **Rationale**: Why this test matters\n\n### Step 3.2: Template Generation\n\nGenerate test templates based on code patterns:\n\n**Template 1: Service Function Test**\n```gcl\n// backend/test/user_service_test.gcl\n\n@test\nfn test_create_user_valid_input() {\n    var user = UserService::createUser(\"test@example.com\", \"password123\", \"user\");\n\n    Assert::isTrue(user != null);\n    Assert::equals(user.email, \"test@example.com\");\n    Assert::equals(user.role, \"user\");\n}\n\n@test\nfn test_create_user_duplicate_email() {\n    // Create first user\n    UserService::createUser(\"test@example.com\", \"pass1\", \"user\");\n\n    // Attempt duplicate\n    try {\n        UserService::createUser(\"test@example.com\", \"pass2\", \"user\");\n        Assert::isFalse(true);  // Should have thrown\n    } catch (ex) {\n        Assert::isTrue(true);   // Expected error\n    }\n}\n\n@test\nfn test_create_user_invalid_email() {\n    try {\n        UserService::createUser(\"invalid-email\", \"password\", \"user\");\n        Assert::isFalse(true);\n    } catch (ex) {\n        var msg = \"${ex}\";\n        Assert::isTrue(msg.contains(\"email\") || msg.contains(\"invalid\"));\n    }\n}\n```\n\n**Template 2: API Endpoint Test**\n```gcl\n// backend/test/api/search_api_test.gcl\n\n@test\nfn test_search_valid_query() {\n    var response = search(\"privacy\", null, 0, 10);\n\n    Assert::isTrue(response != null);\n    Assert::isTrue(response.results != null);\n    Assert::isTrue(response.total >= 0);\n}\n\n@test\nfn test_search_empty_query() {\n    var response = search(\"\", null, 0, 10);\n\n    // Should return empty results, not error\n    Assert::equals(response.total, 0);\n    Assert::equals(response.results.size(), 0);\n}\n\n@test\nfn test_search_pagination() {\n    var page1 = search(\"test\", null, 0, 5);\n    var page2 = search(\"test\", null, 5, 5);\n\n    Assert::isTrue(page1.results.size() <= 5);\n    Assert::isTrue(page2.results.size() <= 5);\n\n    // Verify no overlap\n    if (page1.results.size() > 0 && page2.results.size() > 0) {\n        var id1 = page1.results[0].id;\n        var id2 = page2.results[0].id;\n        Assert::isTrue(id1 != id2);\n    }\n}\n\n@test\nfn test_search_requires_permission() {\n    // This test verifies @permission(\"app.user\") is enforced\n    // Requires authentication context setup\n    // Implementation depends on your auth system\n}\n```\n\n**Template 3: Validation Logic Test**\n```gcl\n// backend/test/model/user_test.gcl\n\n@test\nfn test_user_validate_valid_email() {\n    var user = User { email: \"valid@example.com\", ... };\n    Assert::isTrue(user.validate());\n}\n\n@test\nfn test_user_validate_invalid_email() {\n    var user = User { email: \"not-an-email\", ... };\n    Assert::isFalse(user.validate());\n}\n\n@test\nfn test_user_validate_missing_fields() {\n    var user = User { email: \"\", ... };\n    Assert::isFalse(user.validate());\n}\n```\n\n**Template 4: Edge Case Test**\n```gcl\n@test\nfn test_function_boundary_min() {\n    var result = Service::process(0);\n    // Verify behavior at minimum value\n}\n\n@test\nfn test_function_boundary_max() {\n    var result = Service::process(2147483647);\n    // Verify behavior at maximum int\n}\n\n@test\nfn test_function_null_handling() {\n    var result = Service::process(null);\n    Assert::isTrue(result == null || /* expected behavior */);\n}\n\n@test\nfn test_function_empty_collection() {\n    var result = Service::processList(Array<T>{});\n    Assert::equals(result.size(), 0);\n}\n```\n\n---\n\n## Output Format\n\n### Summary Report\n\n```\n===============================================================================\nTEST COVERAGE ANALYSIS REPORT\n===============================================================================\n\nCURRENT COVERAGE:\n  Total Services:        15\n  Tested Services:       8  (53%)\n  Untested Services:     7  (47%)\n\n  Total API Endpoints:   22\n  Tested Endpoints:      5  (23%)\n  Untested Endpoints:    17 (77%)\n\n  Total Test Files:      6\n  Total Test Functions:  45\n\nRISK SUMMARY:\n  HIGH Priority Gaps:    12 items\n  MEDIUM Priority Gaps:  18 items\n  LOW Priority Gaps:     7 items\n\n===============================================================================\n```\n\n### Detailed Gap Report\n\n```\n===============================================================================\nHIGH PRIORITY TEST GAPS (Risk Score > 70)\n===============================================================================\n\n📍 backend/src/service/auth/user_service.gcl\n\n⚠️ UNTESTED: UserService::createUser\n   Risk Score: 95 (HIGH)\n   Criticality: HIGH (auth function)\n   Complexity: MEDIUM (30 lines, 5 branches)\n   Usage: 8 call sites\n\n   Why Test This:\n   - Authentication is critical for security\n   - Creates persistent user data\n   - Has multiple failure modes (duplicate email, invalid input)\n\n   Suggested Tests:\n   1. test_create_user_valid_input\n   2. test_create_user_duplicate_email\n   3. test_create_user_invalid_email\n   4. test_create_user_invalid_role\n   5. test_create_user_weak_password\n\n   Test File: backend/test/user_service_test.gcl\n   Estimated Effort: 30 minutes\n\n---\n\n📍 backend/src/api/payment_api.gcl\n\n⚠️ UNTESTED: processPayment\n   Risk Score: 98 (HIGH)\n   Criticality: HIGH (payment processing)\n   Complexity: HIGH (80 lines, 12 branches)\n   Usage: 3 call sites\n\n   Why Test This:\n   - Financial transactions require thorough testing\n   - Complex error handling (network, validation, auth)\n   - Data integrity critical\n\n   Suggested Tests:\n   1. test_process_payment_valid\n   2. test_process_payment_insufficient_funds\n   3. test_process_payment_invalid_card\n   4. test_process_payment_network_error\n   5. test_process_payment_duplicate_transaction\n\n   Test File: backend/test/api/payment_api_test.gcl\n   Estimated Effort: 60 minutes\n\n===============================================================================\nMEDIUM PRIORITY TEST GAPS (Risk Score 40-70)\n===============================================================================\n\n📍 backend/src/service/search/search_service.gcl\n\n⚠️ UNTESTED: SearchService::buildQuery\n   Risk Score: 62 (MEDIUM)\n   Criticality: MEDIUM (search functionality)\n   Complexity: MEDIUM (40 lines, 8 branches)\n   Usage: 12 call sites\n\n   Why Test This:\n   - High usage across application\n   - Complex query construction logic\n   - Multiple filter combinations\n\n   Suggested Tests:\n   1. test_build_query_simple\n   2. test_build_query_with_filters\n   3. test_build_query_with_date_range\n   4. test_build_query_empty_input\n\n   Test File: backend/test/search_service_test.gcl\n   Estimated Effort: 20 minutes\n\n===============================================================================\n```\n\n### Test Implementation Plan\n\n```\n===============================================================================\nRECOMMENDED TEST IMPLEMENTATION PLAN\n===============================================================================\n\nSPRINT 1 (HIGH Priority - 2-3 days):\n  [ ] UserService::createUser (6 tests)\n  [ ] UserService::deleteUser (4 tests)\n  [ ] processPayment API endpoint (5 tests)\n  [ ] updateUserRole API endpoint (3 tests)\n\nSPRINT 2 (MEDIUM Priority - 2 days):\n  [ ] SearchService::buildQuery (4 tests)\n  [ ] DocumentService::getDocumentCandidates (5 tests)\n  [ ] PaginationService (remaining functions - 8 tests)\n\nSPRINT 3 (LOW Priority - 1 day):\n  [ ] Utility functions (string formatting, date helpers)\n  [ ] View builders (formatters, mappers)\n\nTOTAL ESTIMATED EFFORT: 5-6 days\nPRIORITY: Complete Sprint 1 before next release\n\n===============================================================================\n```\n\n### Generated Test Files\n\nFor each high-priority gap, create ready-to-use test file templates:\n\n**File: backend/test/user_service_test.gcl** (generated)\n```gcl\n// AUTO-GENERATED TEST TEMPLATE\n// TODO: Implement test assertions based on your business logic\n\n@test\nfn test_create_user_valid_input() {\n    // TODO: Implement test\n    // Expected: User created successfully with correct fields\n    var user = UserService::createUser(\"test@example.com\", \"password123\", \"user\");\n\n    Assert::isTrue(user != null);\n    Assert::equals(user.email, \"test@example.com\");\n    // Add more assertions...\n}\n\n@test\nfn test_create_user_duplicate_email() {\n    // TODO: Implement test\n    // Expected: Should throw error or return null\n    UserService::createUser(\"dup@example.com\", \"pass1\", \"user\");\n\n    try {\n        UserService::createUser(\"dup@example.com\", \"pass2\", \"user\");\n        Assert::isFalse(true);  // Should not reach here\n    } catch (ex) {\n        Assert::isTrue(true);   // Expected\n    }\n}\n\n// ... more tests\n```\n\n---\n\n## Execution Steps\n\n### Step 1: Run Coverage Analysis\n\nExecute the command and let it analyze your codebase:\n\n1. Scans all backend files for services, APIs, models\n2. Cross-references with existing test files\n3. Calculates risk scores\n4. Generates prioritized gap report\n\n### Step 2: Review Suggestions\n\nPresent the report to the user:\n- Show summary statistics\n- Highlight HIGH priority gaps\n- Explain risk rationale for each item\n\n### Step 3: Offer to Generate Tests\n\nAsk the user:\n```\nI found 12 HIGH priority test gaps. Would you like me to:\n\nA) Generate test file templates for HIGH priority items (recommended)\nB) Generate tests for specific items (I'll ask which ones)\nC) Just show the report (you'll write tests manually)\nD) Generate all suggested tests (HIGH + MEDIUM + LOW)\n```\n\n### Step 4: Generate Test Templates\n\nBased on user choice, create test files:\n- Use Write tool to create `backend/test/xxx_test.gcl` files\n- Include TODO comments for user to complete\n- Add descriptive test names and structure\n- Include edge cases and error scenarios\n\n### Step 5: Verify Generated Tests\n\n```bash\n# Run linter on generated tests\ngreycat-lang lint\n\n# Try running the new (incomplete) tests\ngreycat test\n\necho \"\"\necho \"✓ Test templates generated successfully\"\necho \"  Next steps:\"\necho \"  1. Complete TODO sections in generated test files\"\necho \"  2. Run 'greycat test' to verify\"\necho \"  3. Add more test cases as needed\"\n```\n\n---\n\n## Detection Heuristics\n\n### Finding Untested Code\n\n**Service Functions**:\n```bash\n# Find all static functions in services\ngrep -rn \"static fn\" backend/src/service/ --include=\"*.gcl\"\n\n# For each function, check if test exists\n# Search pattern: test_<snake_case_function_name>\n```\n\n**API Endpoints**:\n```bash\n# Find all @expose functions\ngrep -rn \"@expose\" backend/src/api/ --include=\"*.gcl\" -A 5\n\n# Extract function signature\n# Check if integration test exists\n```\n\n**Model Methods**:\n```bash\n# Find all type methods\ngrep -rn \"^\\s*fn [a-z]\" backend/src/model/ --include=\"*.gcl\"\n\n# Exclude simple getters/setters\n# Check for tests in backend/test/model/\n```\n\n### Complexity Calculation\n\n**Lines of Code**:\n```bash\n# Count lines in function (between fn declaration and closing })\nsed -n '/fn functionName/,/^}/p' file.gcl | wc -l\n```\n\n**Branch Count**:\n```bash\n# Count if/else/match/for/while keywords\ngrep -o '\\(if\\|else\\|match\\|for\\|while\\)' file.gcl | wc -l\n```\n\n**Call Sites** (usage):\n```bash\n# Search for function calls across codebase\ngrep -r \"FunctionName(\" backend/ --include=\"*.gcl\" | wc -l\n```\n\n---\n\n## Success Criteria\n\n✓ **Coverage report generated** with statistics\n✓ **All services scanned** for test coverage\n✓ **All API endpoints analyzed** for integration tests\n✓ **Risk scores calculated** for each gap\n✓ **Prioritized suggestions** (HIGH/MEDIUM/LOW)\n✓ **Test templates generated** (if requested)\n✓ **Templates lint successfully** (`greycat-lang lint` passes)\n\n---\n\n## Notes\n\n- **Data Dependency**: Some tests require database data (use setup/teardown or fixtures)\n- **Authentication Tests**: May require auth context setup\n- **Integration Tests**: May need running server (`greycat serve`)\n- **Generated Tests**: Templates include TODOs - user must complete business logic\n- **Iterative**: Run after each sprint to track coverage improvements\n\n---\n\n## Example Workflow\n\n```bash\n# 1. Run test gap analysis\n/coverage\n\n# 2. Review HIGH priority gaps (12 items)\n# 3. Choose to generate templates for HIGH priority\n\n# 4. Templates created in backend/test/\n#    - user_service_test.gcl (6 tests)\n#    - payment_api_test.gcl (5 tests)\n\n# 5. Complete TODO sections in generated files\n\n# 6. Run tests\ngreycat test\n\n# 7. Fix any failures, add more tests as needed\n\n# 8. Re-run /coverage to see improved coverage\n```\n\n---\n\n## Future Enhancements\n\n- **Coverage Percentage**: Calculate actual line/branch coverage (requires instrumentation)\n- **Mutation Testing**: Detect weak tests that don't catch bugs\n- **Performance Tests**: Suggest performance benchmarks for critical paths\n- **Regression Tests**: Auto-generate tests from production bugs\n- **Test Quality**: Analyze existing tests for anti-patterns\n",
        "plugins/greycat/commands/docs.md": "---\nname: docs\ndescription: Update README, API documentation, and MCP server documentation\nallowed-tools: Bash, Read, Grep, Glob, Write, Edit\n---\n\n# Documentation Generator\n\n**Purpose**: Generate and update comprehensive project documentation (README, API docs, MCP server docs)\n\n**Run After**: Each sprint, before releases, when adding features/APIs\n\n---\n\n## Overview\n\nThis command generates three types of documentation:\n\n1. **README.md** - Project overview, setup, architecture\n2. **API Documentation** - OpenAPI/Markdown docs for @expose endpoints\n3. **MCP Documentation** - Model Context Protocol server docs (if using @tag(\"mcp\"))\n\n---\n\n## Phase 1: README.md Generation\n\n### Objective\nCreate comprehensive README that reflects current project state.\n\n### Step 1.1: Analyze Project Structure\n\n```bash\n# Detect project type and structure\necho \"Analyzing project structure...\"\n\n# Check for frontend\nHAS_FRONTEND=false\nif [ -d \"frontend\" ]; then\n    HAS_FRONTEND=true\nfi\n\n# Check for tests\nHAS_TESTS=false\nif [ -d \"backend/test\" ]; then\n    HAS_TESTS=true\nfi\n\n# Count files and data\nBACKEND_FILES=$(find backend/src -name \"*.gcl\" | wc -l)\nTEST_FILES=$(find backend/test -name \"*_test.gcl\" 2>/dev/null | wc -l)\n```\n\n### Step 1.2: Extract Project Information\n\n**A. From project.gcl**:\n```bash\n# Extract library versions\ngrep \"@library\" project.gcl\n\n# Extract permissions and roles\ngrep \"@permission\\|@role\" project.gcl\n\n# Extract main function to understand entry point\n```\n\n**B. From Backend**:\n```bash\n# Find all types (data model)\ngrep -rn \"^type [A-Z]\" backend/src/model/ --include=\"*.gcl\"\n\n# Find all services\ngrep -rn \"^abstract type.*Service\" backend/src/service/ --include=\"*.gcl\"\n\n# Find all API endpoints\ngrep -rn \"@expose\" backend/src/api/ --include=\"*.gcl\"\n```\n\n**C. From Frontend** (if exists):\n```bash\n# Check package.json for dependencies\ncat frontend/package.json | grep -E \"react|typescript|vite\"\n\n# Find pages\nfind frontend/src/pages -name \"*.tsx\" 2>/dev/null\n```\n\n**D. From Data**:\n```bash\n# Check for data files\nls data/ 2>/dev/null\n\n# Check for embedding models\nfind data/ -name \"*.gguf\" 2>/dev/null\n```\n\n### Step 1.3: Generate README Content\n\n**Template Structure**:\n\n```markdown\n# [Project Name]\n\n> [Brief one-line description extracted from project purpose]\n\n## Overview\n\n[Auto-generated project description based on detected features]\n\n**Technology Stack**:\n- Backend: GreyCat [version] (GCL language)\n- [If has_frontend] Frontend: React + TypeScript + [build tool]\n- [If has AI libs] AI/ML: llama.cpp, embeddings\n- [List other detected libraries: kafka, sql, etc.]\n\n**Key Features**:\n[Auto-detected from code analysis]\n- Graph-based data storage with [X] node types\n- [X] REST API endpoints\n- [If has auth] User authentication and role-based access\n- [If has vector index] Semantic search with vector embeddings\n- [List other detected features]\n\n## Quick Start\n\n### Prerequisites\n\n- GreyCat CLI (version [X] or later)\n- [If has_frontend] Node.js 18+ and pnpm/npm\n- [List other requirements detected]\n\n### Installation\n\n\\`\\`\\`bash\n# Clone the repository\ngit clone [repository-url]\ncd [project-name]\n\n# Install GreyCat libraries\ngreycat install\n\n# [If has_frontend] Install frontend dependencies\ncd frontend\npnpm install\ncd ..\n\\`\\`\\`\n\n### Running the Application\n\n**Backend**:\n\\`\\`\\`bash\n# Start GreyCat server\ngreycat serve\n\n# Server will start on http://localhost:8080\n# [If has explorer] Graph explorer available at http://localhost:8080/explorer\n\\`\\`\\`\n\n**[If has_frontend] Frontend**:\n\\`\\`\\`bash\ncd frontend\npnpm dev\n\n# Frontend will start on http://localhost:3000\n\\`\\`\\`\n\n**[If has data import] Data Import**:\n\\`\\`\\`bash\n# Import initial data\ngreycat run import\n\n# [If has vector data] Import vector embeddings\ngreycat run importVector\n\\`\\`\\`\n\n## Architecture\n\n### Data Model\n\n**Core Node Types**:\n[Auto-extracted from backend/src/model/]\n\n\\`\\`\\`\n[Generate simple ASCII art or list of types with relationships]\n\\`\\`\\`\n\n### Service Layer\n\n[List all services with brief descriptions]\n\n- **[ServiceName]** - [Purpose inferred from functions]\n  - `functionName()` - [Description]\n\n### API Endpoints\n\nSee [API Documentation](#api-documentation) for full details.\n\n**Available Endpoints**:\n[Auto-generated from @expose functions]\n\n| Endpoint | Permission | Description |\n|----------|------------|-------------|\n| [function name] | [permission] | [purpose] |\n\n## Development\n\n### Project Structure\n\n\\`\\`\\`\n.\n├── project.gcl                 # Entry point, libraries, permissions\n├── backend/\n│   ├── src/\n│   │   ├── model/              # Data types and global indices\n│   │   ├── service/            # Business logic services\n│   │   ├── api/                # REST API endpoints (@expose)\n│   │   └── edi/                # Import/export logic\n│   └── test/                   # Test files (*_test.gcl)\n[If has_frontend]\n├── frontend/\n│   ├── src/\n│   │   ├── pages/              # React pages\n│   │   ├── components/         # React components\n│   │   └── api/                # API client\n│   └── package.json\n[If has data]\n├── data/                       # Data files, models, embeddings\n└── README.md\n\\`\\`\\`\n\n### Common Commands\n\n**Backend (GreyCat)**:\n\\`\\`\\`bash\ngreycat build                   # Build project\ngreycat-lang lint               # Lint code (run after each change!)\ngreycat serve                   # Start server\ngreycat test                    # Run tests\ngreycat run [function]          # Run specific function\ngreycat codegen ts              # Generate TypeScript types\n\\`\\`\\`\n\n**[If has_frontend] Frontend**:\n\\`\\`\\`bash\ncd frontend\npnpm dev                        # Dev server\npnpm build                      # Production build\npnpm lint                       # Lint TypeScript/React\npnpm test                       # Run tests\n\\`\\`\\`\n\n### Development Workflow\n\n**CRITICAL: Always lint after each change**\n\n\\`\\`\\`bash\n# 1. Make changes to .gcl files\n# 2. Run linter immediately\ngreycat-lang lint\n\n# 3. Fix any errors before proceeding\n# 4. Test your changes\ngreycat test\n\n# 5. [If has_frontend] Update frontend types if backend changed\ngreycat codegen ts\n\\`\\`\\`\n\n## Testing\n\n[If has_tests]\n**Backend Tests**:\n\\`\\`\\`bash\ngreycat test                    # Run all tests\ngreycat test backend/test/specific_test.gcl  # Run specific test\n\\`\\`\\`\n\nCurrent test coverage: [X test files, Y test functions]\n\n[If has_frontend with tests]\n**Frontend Tests**:\n\\`\\`\\`bash\ncd frontend\npnpm test                       # Run tests\npnpm test:coverage              # Coverage report\n\\`\\`\\`\n\n## Configuration\n\n**Environment Variables** (.env):\n[Auto-detect from .env or common patterns]\n\n\\`\\`\\`bash\nGREYCAT_PORT=8080               # Server port\nGREYCAT_CACHE=30000             # Cache size\n[List other detected env vars]\n\\`\\`\\`\n\n**Libraries** (project.gcl):\n[List current library versions from project.gcl]\n\n## Authentication & Permissions\n\n[If has auth detected]\n\n**Roles**:\n[Extract from @role declarations in project.gcl]\n\n**Permissions**:\n[Extract from @permission declarations]\n\n**Usage**:\n\\`\\`\\`gcl\n// Get logged in user\nvar user = SecurityService::getLoggedUser();\n\n// Require authentication\nvar user = SecurityService::requireLoggedUser();\n\n// Check admin role\nif (SecurityService::isAdmin()) { ... }\n\\`\\`\\`\n\n## API Documentation\n\nSee full API documentation at [API.md](API.md) (auto-generated).\n\n[Brief overview of main API categories]\n\n## [If MCP detected] MCP Server\n\nThis project exposes GreyCat functions as Model Context Protocol (MCP) tools.\n\nSee [MCP.md](MCP.md) for full documentation.\n\n## Database Management\n\n**⚠️ Development Mode**: No migrations, delete deprecated fields immediately.\n\n**Reset Database**:\n\\`\\`\\`bash\nrm -rf gcdata                   # ⚠️ DELETES ALL DATA\ngreycat run import              # Reimport from data files\n\\`\\`\\`\n\n**Backup**:\n\\`\\`\\`bash\ntar -czf gcdata-backup.tar.gz gcdata/\n\\`\\`\\`\n\n## Troubleshooting\n\n**Lint Errors**:\n\\`\\`\\`bash\n# Run linter and review errors\ngreycat-lang lint\n\n# Common issues:\n# - Missing imports → Add @include in project.gcl\n# - Type mismatch → Check type definitions\n# - Unknown function → Check spelling, imports\n\\`\\`\\`\n\n**Server Won't Start**:\n\\`\\`\\`bash\n# Check port availability\nlsof -i :8080\n\n# Check database integrity\nrm -rf gcdata && greycat run import\n\\`\\`\\`\n\n**[If has_frontend] Frontend API Errors**:\n\\`\\`\\`bash\n# Ensure backend is running\ngreycat serve\n\n# Regenerate TypeScript types\ngreycat codegen ts\n\n# Check network proxy in vite.config.ts\n\\`\\`\\`\n\n## Project Statistics\n\n[Auto-generate current stats]\n\n- Backend Files: [X] .gcl files\n- Data Model: [Y] types\n- API Endpoints: [Z] @expose functions\n- Services: [N] service classes\n- Tests: [M] test files with [P] test functions\n- [If has_frontend] Frontend: [Q] pages, [R] components\n\n---\n\n**Built with GreyCat** - [https://greycat.io](https://greycat.io)\n```\n\n### Step 1.4: Content Guidelines\n\n**✅ DO INCLUDE**:\n- Technical details and architecture\n- Setup and installation instructions\n- API documentation references\n- Development workflow\n- Testing instructions\n- Configuration details\n- Troubleshooting tips\n- Project statistics\n\n**❌ DO NOT INCLUDE**:\n- Contributor guidelines\n- License information\n- Author names or credits\n- AI assistant mentions (Claude, etc.)\n- Copyright notices\n- Acknowledgments sections\n\n### Step 1.5: Write README.md\n\n```bash\n# Use Write tool to create/update README.md\n# Include all sections generated above\n```\n\n---\n\n## Phase 2: API Documentation Generation\n\n### Objective\nGenerate comprehensive API documentation from @expose endpoints.\n\n### Step 2.1: Extract All @expose Functions\n\n```bash\n# Find all API endpoints\ngrep -rn \"@expose\" backend/src/api/ --include=\"*.gcl\" -A 20\n```\n\nFor each endpoint, extract:\n- **Function name**\n- **Parameters** (with types)\n- **Return type**\n- **Permission required** (@permission decorator)\n- **Description** (from comments if available)\n\n### Step 2.2: Generate API.md\n\n**Template**:\n\n```markdown\n# API Documentation\n\nAuto-generated API documentation for all exposed endpoints.\n\n**Base URL**: `http://localhost:8080/api`\n\n**Authentication**: Required for endpoints marked with permission\n\n---\n\n## Table of Contents\n\n[Auto-generate ToC from API categories]\n\n- [Authentication](#authentication)\n- [Users](#users)\n- [Data Management](#data-management)\n- [Search](#search)\n- [Statistics](#statistics)\n\n---\n\n## Authentication\n\n[If auth endpoints detected]\n\n### Login\n\n**Endpoint**: `POST /api/project::login`\n\n**Permission**: `public` (no authentication required)\n\n**Parameters**:\n| Name | Type | Required | Description |\n|------|------|----------|-------------|\n| username | String | Yes | User's username |\n| password | String | Yes | User's password |\n\n**Returns**: `User` - Authenticated user object\n\n**Example Request**:\n\\`\\`\\`typescript\nconst response = await axios.post('/api/project::login', [\n  'username',\n  'password'\n]);\n\\`\\`\\`\n\n**Example Response**:\n\\`\\`\\`json\n{\n  \"username\": \"admin\",\n  \"email\": \"admin@example.com\",\n  \"role\": \"admin\"\n}\n\\`\\`\\`\n\n**Error Cases**:\n- Invalid credentials → Throws error\n- Account locked → Throws error\n\n---\n\n[Repeat for each endpoint]\n\n## [Category Name]\n\n### [Function Name]\n\n**Endpoint**: `POST /api/project::[functionName]`\n\n**Permission**: `[permission]`\n\n**Description**: [Auto-generated or from comments]\n\n**Parameters**:\n[Table of parameters]\n\n**Returns**: [Return type description]\n\n**Example**:\n[Auto-generate example based on types]\n\n---\n\n## Type Definitions\n\n[Include all @volatile types used in API responses]\n\n### User\n\n\\`\\`\\`gcl\n@volatile\ntype User {\n    username: String;\n    email: String;\n    role: String;\n}\n\\`\\`\\`\n\n[Repeat for all API types]\n\n---\n\n## Error Handling\n\nAll endpoints may throw errors for:\n- **Authentication failures**: User not logged in or insufficient permissions\n- **Validation errors**: Invalid input parameters\n- **Not found**: Requested resource doesn't exist\n- **Internal errors**: Server-side issues\n\n**Error Response Format**:\n\\`\\`\\`json\n{\n  \"error\": \"Error message here\"\n}\n\\`\\`\\`\n\n---\n\n## Usage Examples\n\n### JavaScript/TypeScript\n\n\\`\\`\\`typescript\nimport axios from 'axios';\n\n// Call API endpoint\nconst result = await axios.post('/api/project::functionName', [\n  param1,\n  param2\n]);\n\\`\\`\\`\n\n### cURL\n\n\\`\\`\\`bash\ncurl -X POST http://localhost:8080/api/project::functionName \\\\\n  -H \"Content-Type: application/json\" \\\\\n  -d '[\"param1\", \"param2\"]'\n\\`\\`\\`\n\n---\n\n**Last Updated**: [Auto-generate timestamp]\n```\n\n### Step 2.3: Generate OpenAPI Spec (Optional)\n\nCreate `openapi.yaml` for API tools:\n\n```yaml\nopenapi: 3.0.0\ninfo:\n  title: [Project Name] API\n  version: 1.0.0\n  description: Auto-generated API documentation\n\nservers:\n  - url: http://localhost:8080/api\n    description: Local development server\n\npaths:\n  /project::functionName:\n    post:\n      summary: [Function description]\n      security:\n        - cookieAuth: []\n      requestBody:\n        content:\n          application/json:\n            schema:\n              type: array\n              items: [parameter types]\n      responses:\n        '200':\n          description: Successful response\n          content:\n            application/json:\n              schema:\n                [return type schema]\n\n[Auto-generate for all endpoints]\n```\n\n---\n\n## Phase 3: MCP Documentation Generation\n\n### Objective\nDocument Model Context Protocol (MCP) server if project uses @tag(\"mcp\").\n\n### Step 3.1: Detect MCP Tools\n\n```bash\n# Find functions tagged with @tag(\"mcp\")\ngrep -rn '@tag(\"mcp\")' backend/ --include=\"*.gcl\" -A 10\n```\n\n### Step 3.2: Generate MCP.md\n\n**Template**:\n\n```markdown\n# MCP Server Documentation\n\nThis GreyCat project exposes functions as Model Context Protocol (MCP) tools.\n\n## What is MCP?\n\nModel Context Protocol allows AI assistants (like Claude) to call GreyCat functions as tools during conversations.\n\n## Available Tools\n\n[Auto-generate from @tag(\"mcp\") functions - extract from /// comments]\n\n---\n\n### [functionName]\n\n[Auto-extract description from /// comments - 1-2 sentences]\n\n**Parameters**:\n- `param1` (Type) - [Brief description from @param]\n- `param2` (Type) - [Brief description from @param]\n\n**Returns**: [Brief description from @return]\n\n**Example Usage**:\n\\`\\`\\`\n[functionName](\"example query\", 10, null)\n→ Returns: PaginatedResult with matching items\n\\`\\`\\`\n\n---\n\n[Repeat for each MCP tool - keep it SHORT]\n\n## Enabling MCP Server\n\n### Option 1: Built-in MCP Server\n\nGreyCat can expose MCP tools natively:\n\n\\`\\`\\`bash\n# Start server with MCP enabled\ngreycat serve --enable-mcp\n\n# MCP server will be available at:\n# stdio: greycat mcp\n# http: http://localhost:8080/mcp\n\\`\\`\\`\n\n### Option 2: Claude Desktop Integration\n\nAdd to Claude Desktop config (`~/Library/Application Support/Claude/claude_desktop_config.json`):\n\n\\`\\`\\`json\n{\n  \"mcpServers\": {\n    \"[project-name]\": {\n      \"command\": \"greycat\",\n      \"args\": [\"mcp\"],\n      \"cwd\": \"/path/to/project\"\n    }\n  }\n}\n\\`\\`\\`\n\nRestart Claude Desktop to load the MCP server.\n\n## Testing MCP Tools\n\n### Using MCP Inspector\n\n\\`\\`\\`bash\n# Install MCP inspector\nnpm install -g @modelcontextprotocol/inspector\n\n# Run inspector\nmcp-inspector greycat mcp\n\\`\\`\\`\n\n### Using cURL\n\n\\`\\`\\`bash\n# Call MCP tool via HTTP\ncurl -X POST http://localhost:8080/mcp/tools/call \\\\\n  -H \"Content-Type: application/json\" \\\\\n  -d '{\n    \"name\": \"toolName\",\n    \"arguments\": {\n      \"param1\": \"value1\"\n    }\n  }'\n\\`\\`\\`\n\n## Security Considerations\n\n**Permissions**: MCP tools respect @permission decorators\n- Tools with `@permission(\"public\")` are accessible without auth\n- Tools with `@permission(\"app.user\")` require authentication\n- Tools with `@permission(\"app.admin\")` require admin role\n\n**Best Practices**:\n- Only expose safe read operations as MCP tools\n- Use @permission for all sensitive operations\n- Validate all inputs in tool functions\n- Log MCP tool usage for audit trail\n\n## Troubleshooting\n\n**MCP Server Not Starting**:\n\\`\\`\\`bash\n# Check if GreyCat server is running\ngreycat serve\n\n# Check MCP configuration\ngreycat mcp --help\n\\`\\`\\`\n\n**Tools Not Appearing in Claude**:\n- Verify @tag(\"mcp\") decorator on functions\n- Restart Claude Desktop after config changes\n- Check Claude Desktop logs for errors\n\n**Permission Errors**:\n- Ensure user is authenticated for protected tools\n- Check @permission matches user role\n- Verify SecurityService is configured correctly\n\n---\n\n**Last Updated**: [Auto-generate timestamp]\n```\n\n---\n\n## Output Format\n\n### Summary\n\n```\n===============================================================================\nDOCUMENTATION GENERATED\n===============================================================================\n\nCreated/Updated:\n  ✓ README.md (2,450 lines)\n    - Project overview\n    - Quick start guide\n    - Architecture documentation\n    - [X] API endpoints listed\n    - Development workflow\n    - Troubleshooting guide\n\n  ✓ API.md (1,850 lines)\n    - [X] API endpoints documented\n    - Request/response examples\n    - Type definitions\n    - Error handling guide\n    - Usage examples (TypeScript, cURL)\n\n  [If MCP detected]\n  ✓ MCP.md (680 lines)\n    - [Y] MCP tools documented\n    - Setup instructions\n    - Security guidelines\n    - Testing guide\n\n  [If OpenAPI generated]\n  ✓ openapi.yaml\n    - OpenAPI 3.0 specification\n    - Ready for Postman/Swagger\n\nNext Steps:\n  1. Review generated documentation for accuracy\n  2. Add project-specific details where marked with [TODO]\n  3. Commit documentation:\n     git add README.md API.md MCP.md\n     git commit -m \"Update documentation\"\n\n===============================================================================\n```\n\n---\n\n## Execution Steps\n\n### Step 1: Analyze Project\n\n```bash\necho \"Analyzing project structure...\"\n# Detect: frontend, tests, data, libraries, auth, MCP\n```\n\n### Step 2: Generate Documentation\n\n```bash\necho \"Generating README.md...\"\n# Extract info, generate content, write file\n\necho \"Generating API.md...\"\n# Extract @expose functions, generate docs\n\n[If MCP detected]\necho \"Generating MCP.md...\"\n# Extract @tag(\"mcp\") functions, generate docs\n```\n\n### Step 3: Verify\n\n```bash\n# Check files created\nls -lh README.md API.md MCP.md\n\n# Validate markdown\n# (Optional: use markdown linter if available)\n```\n\n### Step 4: Report\n\nPresent summary of generated documentation to user.\n\n---\n\n## Customization\n\n### Adding Project-Specific Sections\n\nAfter generation, you may want to add:\n\n**To README.md**:\n- Specific deployment instructions\n- Integration guides\n- Performance benchmarks\n- Known limitations\n\n**To API.md**:\n- Complex workflow examples\n- Integration patterns\n- Rate limiting details\n- Versioning strategy\n\n**To MCP.md**:\n- Custom tool workflows\n- Domain-specific examples\n- Advanced security configurations\n\n### Excluding Sections\n\nSome projects may not need all sections. The generator automatically detects and includes only relevant sections:\n\n- No frontend → Skip frontend setup\n- No tests → Skip testing section\n- No MCP → Skip MCP documentation\n- No auth → Skip authentication section\n\n---\n\n## Best Practices\n\n### When to Regenerate\n\n**✅ Regenerate After**:\n- Adding new API endpoints\n- Changing data model significantly\n- Adding/removing libraries\n- Updating authentication system\n- Before releases\n\n**❌ Don't Regenerate For**:\n- Minor bug fixes\n- Internal refactoring (no API changes)\n- Documentation-only changes\n- Test additions\n\n### Maintaining Documentation\n\n**Keep Custom Content**:\n- Add custom sections AFTER auto-generated sections\n- Use clear markers for custom content\n- Regeneration will preserve or warn about conflicts\n\n**Version Control**:\n```bash\n# Commit documentation with code changes\ngit add README.md API.md\ngit commit -m \"feat: add new search endpoint\n\n- Added semantic search API\n- Updated API documentation\"\n```\n\n---\n\n## Success Criteria\n\n✓ **README.md generated** with complete project overview\n✓ **API.md generated** documenting all @expose endpoints\n✓ **MCP.md generated** (if MCP tools detected)\n✓ **OpenAPI spec** created (optional)\n✓ **Markdown is valid** (no syntax errors)\n✓ **Content is accurate** (reflects current code state)\n✓ **No forbidden content** (no license, contributors, Claude mentions)\n\n---\n\n## Notes\n\n- **Auto-Detection**: Generator automatically detects project features\n- **Markdown Format**: All output in GitHub-flavored Markdown\n- **Regeneration Safe**: Can be run multiple times, updates content\n- **Customization Welcome**: Add project-specific sections after generation\n- **No External Dependencies**: Uses only GreyCat code analysis\n\n---\n\n## Example Workflow\n\n```bash\n# 1. Complete a sprint of development\n# - Added 3 new API endpoints\n# - Added MCP tool for search\n# - Updated data model\n\n# 2. Run documentation generator\n/docs\n\n# 3. Review generated docs\ncat README.md  # Check accuracy\ncat API.md     # Verify new endpoints documented\ncat MCP.md     # Check MCP tools\n\n# 4. Customize if needed\n# Add deployment section to README.md\n# Add complex example to API.md\n\n# 5. Commit\ngit add README.md API.md MCP.md\ngit commit -m \"docs: update documentation for v2.0 release\"\n\n# 6. Use in development\n# - New team members use README for onboarding\n# - Frontend devs reference API.md\n# - AI integration team uses MCP.md\n```\n",
        "plugins/greycat/commands/frontend.md": "---\nname: frontend\ndescription: Review frontend codebase for code quality, performance, and best practices\nallowed-tools: Bash, Read, Grep, Glob\n---\n\n# Frontend Review & Analysis\n\n**Purpose**: Comprehensive analysis of React/TypeScript frontend for code quality, performance, accessibility, and dead code\n\n**Run After**: Each sprint, before releases, when adding features\n\n---\n\n## Overview\n\nThis command performs multi-category frontend analysis:\n\n1. **Code Quality** - TypeScript practices, memoization, error handling\n2. **Performance** - Re-renders, bundle size, computations\n3. **Architecture** - Component structure, hooks, routing\n4. **Accessibility** - ARIA, semantic HTML, focus management\n5. **Testing** - Coverage gaps, test suggestions\n6. **UI/UX Consistency** - Styling, theming, responsiveness\n7. **Security** - XSS, API patterns, data exposure\n8. **Dead Code** - Unused exports, files, dependencies\n\n---\n\n## Phase 0: Coding Standards Reference\n\nBefore reviewing, understand the required patterns. These are the standards to check against:\n\n### Component Pattern (MANDATORY)\n\n```tsx\n/**\n * Displays paginated search results with highlighting\n */\ninterface SearchResultsProps {\n  results: SearchResult[] | null\n  isLoading: boolean\n  onResultClick?: (result: SearchResult) => void\n}\n\n// ✅ Named export + memo (pages use default export)\nexport const SearchResults = memo(function SearchResults({\n  results,\n  isLoading,\n  onResultClick\n}: SearchResultsProps) {\n  // Component implementation\n})\n```\n\n**Rules**:\n- Props interface ABOVE component with JSDoc\n- Named export + memo for all components\n- Exception: Default export for page components only\n- JSDoc on ALL components, hooks, and services\n\n### Service Pattern\n\n```tsx\n// ✅ Named export object with withRetry wrapper\nexport const DocumentService = {\n  getDocument: (id: string): Promise<gc.api_types.Document> =>\n    withRetry(() => gc.document(id)),\n\n  search: (query: string): Promise<gc.api_types.SearchResults> =>\n    withRetry(() => gc.search(query)),\n}\n```\n\n### Hook Pattern\n\n```tsx\n/**\n * Hook for managing search state\n * @returns Object with search state and handlers\n */\nexport function useSearch() {\n  // ✅ useCallback with functional setState to minimize deps\n  const handleSearch = useCallback((query: string) => {\n    setSubmittedQuery(query)  // Param-based, no deps needed\n  }, [])\n\n  // ✅ useMemo for derived data\n  const sortedResults = useMemo(() =>\n    results ? [...results.items].sort((a, b) => b.score - a.score) : []\n  , [results])\n\n  // ✅ Return object (not array) for named access\n  return { query, results: sortedResults, handleSearch }\n}\n```\n\n### React Query Pattern\n\n```tsx\nconst { data, isLoading } = useQuery({\n  queryKey: ['search', query, searchType, options],  // All deps in key\n  queryFn: () => SearchService.search(query),\n  enabled: !!query,                                   // Conditional\n  staleTime: QUERY_CONFIG.staleTime.search           // From config\n})\n```\n\n### State Management Strategy\n\n| State Type | Solution | Example |\n|------------|----------|---------|\n| URL params | useSearchParams | search query, filters, pagination |\n| Persistence | localStorage | history, preferences |\n| App-wide | Context | theme, user session |\n| Server | React Query | API data, cache |\n| Local | useState/useReducer | form state, UI toggles |\n\n**NO Redux/Zustand** - use hooks and context instead.\n\n---\n\n## Phase 1: Code Quality & Best Practices\n\n### Objective\nEnsure TypeScript and React best practices are followed.\n\n### Step 1.1: TypeScript Type Safety\n\n**Find `any` usage**:\n```bash\n# Search for 'any' type usage\ngrep -rn \": any\\|<any>\" frontend/src/ --include=\"*.ts\" --include=\"*.tsx\"\n```\n\n**Example Output**:\n```\n📍 frontend/src/components/DataTable.tsx:45\n\n⚠️ CODE QUALITY: Using 'any' type\n\nCode:\n  45: const handleData = (data: any) => {\n  46:     processData(data);\n  47: }\n\nProblem:\n  - Using 'any' bypasses TypeScript type checking\n  - Loses type safety benefits\n  - Makes refactoring dangerous\n\nFix:\n  interface DataType {\n      id: string;\n      value: number;\n      // ... other fields\n  }\n\n  const handleData = (data: DataType) => {\n      processData(data);\n  }\n\nPriority: MEDIUM\nImpact: Type safety, maintainability\n```\n\n### Step 1.2: Missing Memoization\n\n**Find expensive computations**:\n```bash\n# Find components without React.memo\ngrep -rn \"^export const.*: React.FC\\|^function.*Component\" frontend/src/components/ --include=\"*.tsx\" | grep -v \"React.memo\"\n```\n\n**Example Output**:\n```\n📍 frontend/src/components/SearchResults.tsx:12\n\n⚠️ PERFORMANCE: Component without memoization\n\nCode:\n  12: export const SearchResults: React.FC<Props> = ({ items, onSelect }) => {\n  13:     // Heavy rendering logic\n  14:     return <div>{items.map(...)}</div>;\n  15: };\n\nProblem:\n  - Component re-renders on every parent render\n  - No props comparison\n  - Expensive for large lists\n\nFix:\n  export const SearchResults = React.memo<Props>(\n      ({ items, onSelect }) => {\n          // ... component logic\n      }\n  );\n\n  // Or with custom comparison:\n  export const SearchResults = React.memo<Props>(\n      ({ items, onSelect }) => { ... },\n      (prevProps, nextProps) => prevProps.items === nextProps.items\n  );\n\nPriority: HIGH (if component renders frequently)\nImpact: Performance, user experience\n```\n\n### Step 1.3: Component Export Pattern\n\n```bash\n# Find components not using memo\ngrep -rn \"export const.*=.*function\\|export function\" frontend/src/components/ --include=\"*.tsx\" | grep -v \"memo(\"\n```\n\n**Example Output**:\n```\n📍 frontend/src/components/SearchResults.tsx:15\n\n⚠️ CODE QUALITY: Component not wrapped in memo\n\nCode:\n  15: export const SearchResults = function SearchResults({ results }: Props) {\n\nProblem:\n  - Component re-renders on every parent render\n  - Props interface not above component\n  - Missing JSDoc documentation\n\nFix:\n  /**\n   * Displays search results with highlighting\n   */\n  interface SearchResultsProps {\n    results: SearchResult[] | null\n    isLoading: boolean\n  }\n\n  export const SearchResults = memo(function SearchResults({\n    results,\n    isLoading\n  }: SearchResultsProps) {\n    // ...\n  })\n\nPriority: MEDIUM\nImpact: Performance, code consistency\n```\n\n### Step 1.4: Services Without Retry\n\n```bash\n# Find service calls not using withRetry\ngrep -rn \"gc\\.\\w\\+(\" frontend/src/services/ --include=\"*.ts\" | grep -v \"withRetry\"\n```\n\n**Example Output**:\n```\n📍 frontend/src/services/documentService.ts:12\n\n⚠️ CODE QUALITY: Service call without retry wrapper\n\nCode:\n  12: getDocument: (id: string) => gc.document(id),\n\nProblem:\n  - No retry on transient failures\n  - No consistent error handling\n\nFix:\n  getDocument: (id: string): Promise<gc.api_types.Document> =>\n    withRetry(() => gc.document(id)),\n\nPriority: MEDIUM\nImpact: Reliability, error handling\n```\n\n### Step 1.5: Prop Drilling\n\n**Find deep prop passing**:\n```bash\n# Look for components passing many props\ngrep -rn \"props\\.\" frontend/src/components/ --include=\"*.tsx\" | wc -l\n```\n\nIdentify chains where props are passed through 3+ levels.\n\n**Example Output**:\n```\n⚠️ ARCHITECTURE: Prop drilling detected\n\nChain:\n  App.tsx → Dashboard.tsx → UserPanel.tsx → UserCard.tsx → UserAvatar.tsx\n\nProps passed: user, theme, onUpdate (through 4 levels)\n\nProblem:\n  - Props passed through intermediate components\n  - Intermediate components don't use the props\n  - Makes refactoring difficult\n\nFix Option 1 (Context):\n  // Create context\n  const UserContext = createContext<UserContextType>(null!);\n\n  // In App.tsx\n  <UserContext.Provider value={{ user, onUpdate }}>\n      <Dashboard />\n  </UserContext.Provider>\n\n  // In UserAvatar.tsx\n  const { user, onUpdate } = useContext(UserContext);\n\nFix Option 2 (State Management):\n  // Use Zustand/Redux/Jotai\n  const useUserStore = create((set) => ({\n      user: null,\n      setUser: (user) => set({ user })\n  }));\n\nPriority: MEDIUM\nImpact: Maintainability, code clarity\n```\n\n---\n\n## Phase 2: Performance Analysis\n\n### Step 2.1: Missing React Keys\n\n```bash\n# Find .map without keys\ngrep -rn \"\\.map(\" frontend/src/ --include=\"*.tsx\" -A 2 | grep -v \"key=\"\n```\n\n**Example Output**:\n```\n📍 frontend/src/components/List.tsx:34\n\n⚠️ PERFORMANCE: Missing key prop in list\n\nCode:\n  34: items.map((item) => (\n  35:     <ListItem item={item} />  ← No key\n  36: ))\n\nProblem:\n  - React can't efficiently track list items\n  - May cause unnecessary re-renders\n  - Can lead to state bugs\n\nFix:\n  items.map((item) => (\n      <ListItem key={item.id} item={item} />\n  ))\n\nPriority: HIGH\nImpact: Performance, correctness\n```\n\n### Step 2.2: Bundle Size Analysis\n\n```bash\n# Run bundle analyzer (if available)\ncd frontend && npm run analyze 2>/dev/null || echo \"No bundle analyzer configured\"\n```\n\nLook for:\n- Large dependencies (>100KB)\n- Duplicate dependencies\n- Tree-shaking opportunities\n\n**Example Output**:\n```\n⚠️ PERFORMANCE: Large dependencies detected\n\nAnalysis:\n  - moment.js: 288KB (could use date-fns: 13KB)\n  - lodash: 71KB (could use lodash-es for tree-shaking)\n  - @mui/material: imported entirely (400KB), only using 5 components\n\nRecommendations:\n  1. Replace moment with date-fns:\n     import { format } from 'date-fns';\n\n  2. Use tree-shakeable lodash:\n     import debounce from 'lodash-es/debounce';\n\n  3. Import MUI components individually:\n     import Button from '@mui/material/Button';\n\nEstimated savings: ~600KB (minified)\n\nPriority: HIGH\nImpact: Load time, user experience\n```\n\n---\n\n## Phase 3: Architecture & Organization\n\n### Step 3.1: Code Duplication\n\n```bash\n# Find similar component patterns\nfind frontend/src/components -name \"*.tsx\" -exec wc -l {} \\; | sort -rn | head -20\n```\n\nManually review large files for duplication.\n\n**Example Output**:\n```\n⚠️ ARCHITECTURE: Duplicated form logic\n\nFiles with similar patterns:\n  - frontend/src/components/UserForm.tsx (150 lines)\n  - frontend/src/components/ProductForm.tsx (145 lines)\n  - frontend/src/components/OrderForm.tsx (142 lines)\n\nDuplication:\n  - Form validation logic (30 lines each)\n  - Submit handling (20 lines each)\n  - Error display (15 lines each)\n\nRecommendation:\n  Create custom hook:\n\n  // useForm.ts\n  export function useForm<T>(initialValues: T, onSubmit: (values: T) => void) {\n      const [values, setValues] = useState(initialValues);\n      const [errors, setErrors] = useState<Record<string, string>>({});\n\n      const handleSubmit = async (e: FormEvent) => {\n          e.preventDefault();\n          // Shared validation + submit logic\n      };\n\n      return { values, errors, handleSubmit, setValues };\n  }\n\n  // Usage in each form:\n  const { values, errors, handleSubmit } = useForm(initialValues, handleFormSubmit);\n\nPriority: MEDIUM\nImpact: Maintainability, DRY principle\n```\n\n### Step 3.2: Lazy Loading Opportunities\n\n```bash\n# Find large pages without lazy loading\ngrep -rn \"import.*from.*pages\" frontend/src/ --include=\"*.tsx\" | grep -v \"React.lazy\"\n```\n\n**Example Output**:\n```\n⚠️ PERFORMANCE: Missing lazy loading on routes\n\nCurrent imports:\n  import { HomePage } from './pages/HomePage';\n  import { SettingsPage } from './pages/SettingsPage';\n  import { AdminPage } from './pages/AdminPage';\n\nProblem:\n  - All pages loaded upfront\n  - Increases initial bundle size\n  - Slower initial load\n\nFix:\n  const HomePage = React.lazy(() => import('./pages/HomePage'));\n  const SettingsPage = React.lazy(() => import('./pages/SettingsPage'));\n  const AdminPage = React.lazy(() => import('./pages/AdminPage'));\n\n  // Wrap routes with Suspense:\n  <Suspense fallback={<LoadingSpinner />}>\n      <Routes>\n          <Route path=\"/\" element={<HomePage />} />\n          <Route path=\"/settings\" element={<SettingsPage />} />\n          <Route path=\"/admin\" element={<AdminPage />} />\n      </Routes>\n  </Suspense>\n\nPriority: HIGH\nImpact: Initial load time\n```\n\n---\n\n## Phase 4: Accessibility (a11y)\n\n### Step 4.1: Missing ARIA Labels\n\n```bash\n# Find buttons/links without aria-label or text\ngrep -rn \"<button\\|<a href\" frontend/src/ --include=\"*.tsx\" | grep -v \"aria-label\\|children\"\n```\n\n**Example Output**:\n```\n📍 frontend/src/components/Header.tsx:23\n\n⚠️ ACCESSIBILITY: Missing accessible label\n\nCode:\n  23: <button onClick={handleMenu}>\n  24:     <MenuIcon />\n  25: </button>\n\nProblem:\n  - Screen readers can't identify button purpose\n  - No text or aria-label\n\nFix:\n  <button onClick={handleMenu} aria-label=\"Open menu\">\n      <MenuIcon />\n  </button>\n\nPriority: HIGH\nImpact: Accessibility compliance\n```\n\n### Step 4.2: Semantic HTML\n\n```bash\n# Find div soup (many nested divs)\ngrep -rn \"<div><div><div>\" frontend/src/ --include=\"*.tsx\"\n```\n\n**Example Output**:\n```\n⚠️ ACCESSIBILITY: Non-semantic markup\n\nCode:\n  <div className=\"header\">\n      <div className=\"nav\">\n          <div className=\"link\">Home</div>\n      </div>\n  </div>\n\nFix:\n  <header>\n      <nav>\n          <a href=\"/\">Home</a>\n      </nav>\n  </header>\n\nPriority: MEDIUM\nImpact: SEO, accessibility\n```\n\n---\n\n## Phase 5: Testing\n\n### Step 5.1: Test Structure & Patterns\n\n**Required test structure** (nested describes):\n```tsx\ndescribe('ComponentName', () => {\n  describe('Feature Area', () => {\n    it('describes specific behavior with outcome', () => {\n      // Test implementation\n    })\n  })\n})\n```\n\n**Use test fixtures** (if test/fixtures.ts exists):\n```tsx\nimport { fixtures } from '@/test/fixtures'\n\nit('renders document details', () => {\n  const mockDoc = fixtures.document({ id: 'test-1', title: 'Test' })\n  render(<DocumentCard document={mockDoc} />)\n  expect(screen.getByText('Test')).toBeInTheDocument()\n})\n```\n\n**Use test helpers** (if available):\n```tsx\n// Service mocks\nimport { setupGcMockForService } from '@/test/serviceMockHelpers'\n\nbeforeEach(() => {\n  setupGcMockForService('document')\n})\n\n// Component helpers\nimport { testClickableCard, testCountDisplay } from '@/test/cardTestHelpers'\nimport { testCheckboxFilter, testRangeFilter } from '@/test/filterTestHelpers'\n```\n\n### Step 5.2: Untested Components\n\n```bash\n# Find components without test files\nfor file in frontend/src/components/*.tsx; do\n    base=$(basename \"$file\" .tsx)\n    if [ ! -f \"frontend/src/components/${base}.test.tsx\" ]; then\n        echo \"Missing test: $file\"\n    fi\ndone\n```\n\n**Example Output**:\n```\n⚠️ TESTING: Missing test coverage\n\nUntested components:\n  - frontend/src/components/SearchBar.tsx\n  - frontend/src/components/ResultsList.tsx\n  - frontend/src/components/FilterPanel.tsx\n\nRecommendation:\n  Create test files with nested describes:\n\n  // SearchBar.test.tsx\n  import { render, screen, fireEvent } from '@testing-library/react';\n  import { SearchBar } from './SearchBar';\n\n  describe('SearchBar', () => {\n      describe('rendering', () => {\n          it('renders input field', () => {\n              render(<SearchBar onSearch={vi.fn()} />);\n              expect(screen.getByRole('textbox')).toBeInTheDocument();\n          });\n      });\n\n      describe('interactions', () => {\n          it('calls onSearch when submitted', () => {\n              const onSearch = vi.fn();\n              render(<SearchBar onSearch={onSearch} />);\n\n              fireEvent.change(screen.getByRole('textbox'), {\n                  target: { value: 'test query' }\n              });\n              fireEvent.submit(screen.getByRole('form'));\n\n              expect(onSearch).toHaveBeenCalledWith('test query');\n          });\n      });\n  });\n\nPriority: MEDIUM\nImpact: Test coverage, confidence in changes\n```\n\n---\n\n## Phase 6: UI/UX Consistency & Styling\n\n### Step 6.1: Inline Styles (CRITICAL)\n\n**Rule: NO inline styles except truly dynamic values**\n\n```bash\n# Find inline styles\ngrep -rn \"style={{\" frontend/src/ --include=\"*.tsx\"\n```\n\n**Allowed inline styles** (dynamic values only):\n```tsx\n// ✅ Dynamic percentage\n<div style={{ width: `${progress}%` }} />\n\n// ✅ Dynamic rotation\n<div style={{ transform: `rotate(${angle}deg)` }} />\n\n// ✅ Computed colors (from data)\n<div style={{ backgroundColor: item.color }} />\n```\n\n**Forbidden inline styles**:\n```tsx\n// ❌ Hardcoded colors\n<div style={{ color: '#1976d2' }} />\n\n// ❌ Fixed sizes\n<div style={{ width: 200, padding: 16 }} />\n\n// ❌ Static positioning\n<div style={{ marginTop: 10 }} />\n```\n\n**Fix**: Use theme constants or Tailwind utilities:\n```tsx\n// ✅ Theme constants from config/theme\nimport { buttonStyles, textStyles, panelStyles } from '@/config/theme'\n<button className={buttonStyles.primary}>Click</button>\n\n// ✅ Tailwind utilities\n<div className=\"w-48 p-4 mt-2 text-blue-600\" />\n```\n\n### Step 6.2: Hardcoded Colors\n\n```bash\n# Find hardcoded colors\ngrep -rn \"color: '#\\|backgroundColor: '#\" frontend/src/ --include=\"*.tsx\" --include=\"*.css\"\n```\n\n**Example Output**:\n```\n⚠️ UI CONSISTENCY: Hardcoded colors\n\nLocations:\n  - frontend/src/components/Button.tsx:23: color: '#1976d2'\n  - frontend/src/components/Header.tsx:45: backgroundColor: '#ffffff'\n  - frontend/src/pages/Dashboard.tsx:67: color: '#333333'\n\nProblem:\n  - Can't switch themes easily\n  - Inconsistent color usage\n  - Hard to maintain\n\nFix:\n  // Use theme config\n  import { colors } from '@/config/theme'\n  <Button className={colors.primary} />\n\n  // Or Tailwind\n  <Button className=\"text-primary bg-white\" />\n\n  // Or CSS variables\n  :root {\n      --color-primary: #1976d2;\n  }\n  .button { color: var(--color-primary); }\n\nPriority: HIGH\nImpact: Theming, maintainability\n```\n\n---\n\n## Phase 7: Security\n\n### Step 7.1: XSS Vulnerabilities\n\n```bash\n# Find dangerouslySetInnerHTML usage\ngrep -rn \"dangerouslySetInnerHTML\" frontend/src/ --include=\"*.tsx\"\n```\n\n**Example Output**:\n```\n📍 frontend/src/components/Content.tsx:34\n\n⚠️ SECURITY: Potential XSS vulnerability\n\nCode:\n  34: <div dangerouslySetInnerHTML={{ __html: userContent }} />\n\nProblem:\n  - Renders user-provided HTML without sanitization\n  - XSS attack vector if userContent is untrusted\n\nFix:\n  import DOMPurify from 'dompurify';\n\n  <div dangerouslySetInnerHTML={{\n      __html: DOMPurify.sanitize(userContent)\n  }} />\n\nPriority: CRITICAL\nImpact: Security vulnerability\n```\n\n### Step 7.2: Sensitive Data in Client\n\n```bash\n# Find potential secrets in code\ngrep -rn \"api.*key\\|secret\\|password\" frontend/src/ --include=\"*.ts\" --include=\"*.tsx\" -i\n```\n\n**Example Output**:\n```\n⚠️ SECURITY: Potential secret in client code\n\nCode:\n  const API_KEY = \"sk_live_abc123...\";  ← Hardcoded secret\n\nProblem:\n  - API key visible in client code\n  - Exposed in production bundle\n  - Security risk\n\nFix:\n  // Use environment variables\n  const API_KEY = import.meta.env.VITE_API_KEY;\n\n  // In .env (NOT committed)\n  VITE_API_KEY=sk_live_abc123...\n\n  // For public API keys, this is OK\n  // For private keys, use backend proxy\n\nPriority: CRITICAL (if private key)\nImpact: Security breach\n```\n\n---\n\n## Phase 8: Dead Code Analysis\n\n### Step 8.1: Run Dead Code Detector\n\n**Detect package manager**:\n```bash\nif [ -f \"frontend/package-lock.json\" ]; then\n    PKG_MGR=\"npm\"\nelif [ -f \"frontend/pnpm-lock.yaml\" ]; then\n    PKG_MGR=\"pnpm\"\nelif [ -f \"frontend/yarn.lock\" ]; then\n    PKG_MGR=\"yarn\"\nelse\n    PKG_MGR=\"npm\"\nfi\n```\n\n**Run analyzer**:\n```bash\ncd frontend\n\n# Check if knip is configured\nif grep -q \"knip\" package.json; then\n    echo \"Running dead code analysis...\"\n    $PKG_MGR run dead-code 2>&1 || echo \"No dead-code script found\"\nelse\n    echo \"⚠️  No dead code analyzer (knip) configured\"\n    echo \"To add: npm install -D knip\"\nfi\n```\n\n**Example Output**:\n```\n===============================================================================\nDEAD CODE ANALYSIS\n===============================================================================\n\nUnused exports (12):\n  - src/utils/formatDate.ts: formatDateRange (exported but never imported)\n  - src/hooks/useDebounce.ts: default (exported but never imported)\n  - src/components/Button.tsx: ButtonProps (type exported but never used)\n\nUnused files (3):\n  - src/utils/oldHelper.ts (not imported anywhere)\n  - src/components/DeprecatedModal.tsx (not imported anywhere)\n\nUnused dependencies (5):\n  - moment (in package.json, not imported)\n  - classnames (in package.json, not imported)\n\nDuplicate exports (2):\n  - src/utils/index.ts and src/utils/helpers.ts both export 'formatCurrency'\n\nRecommendations:\n  1. Remove unused exports or files\n  2. Remove unused dependencies:\n     npm remove moment classnames\n  3. Consolidate duplicate exports\n\nEstimated cleanup: 450 lines, 3 files, ~500KB dependencies\n```\n\n**Auto-fix** (optional):\n```bash\n# Let knip automatically fix removable issues\n$PKG_MGR run dead-code:fix\n\necho \"⚠️  Review changes carefully before committing\"\necho \"Some 'unused' code may be:\"\necho \"  - Entry points (main.tsx)\"\necho \"  - Used by external tools\"\necho \"  - Dynamic imports\"\n```\n\n---\n\n## Output Format\n\n### Executive Summary\n\n```\n===============================================================================\nFRONTEND REVIEW REPORT\n===============================================================================\n\nFiles Analyzed: 127 (.ts/.tsx files)\nAnalysis Date: 2026-01-08\n\nISSUES FOUND:\n\nCRITICAL (Security/Breaking):\n  [ ] 2 XSS vulnerabilities (dangerouslySetInnerHTML)\n  [ ] 1 hardcoded API key\n\nHIGH (Performance/Accessibility):\n  [ ] 8 components without memoization\n  [ ] 12 missing React keys\n  [ ] 15 missing ARIA labels\n  [ ] 5 routes without lazy loading\n\nMEDIUM (Code Quality/Maintainability):\n  [ ] 23 uses of 'any' type\n  [ ] 7 instances of prop drilling\n  [ ] 18 duplicated code blocks\n  [ ] 12 untested components\n  [ ] 31 hardcoded theme values\n\nLOW (Nice to Have):\n  [ ] 12 unused exports\n  [ ] 5 unused dependencies\n  [ ] 8 non-semantic HTML sections\n\nTOTAL ISSUES: 144\n\nESTIMATED FIX TIME:\n  Critical: 2 hours   (fix immediately)\n  High:     1 day     (this sprint)\n  Medium:   3 days    (next sprint)\n  Low:      4 hours   (cleanup sprint)\n\nBUNDLE SIZE OPPORTUNITIES:\n  Potential savings: ~600KB (45% reduction)\n  - Replace moment.js with date-fns\n  - Tree-shake lodash imports\n  - Lazy load admin routes\n\n===============================================================================\n```\n\n---\n\n## Success Criteria\n\n✓ **All frontend files scanned** (.ts/.tsx in frontend/src/)\n✓ **TypeScript quality checked** (any usage, type safety)\n✓ **Performance analyzed** (memoization, keys, bundle)\n✓ **Architecture reviewed** (duplication, organization)\n✓ **Accessibility verified** (ARIA, semantic HTML)\n✓ **Testing gaps identified** (missing test files)\n✓ **Security checked** (XSS, secrets)\n✓ **Dead code detected** (if knip configured)\n✓ **Report generated** with prioritized issues\n\n---\n\n## Notes\n\n- **Works with any React/TypeScript project**: Generic patterns\n- **Package manager agnostic**: Detects npm/pnpm/yarn\n- **Dead code requires knip**: Install if not present\n- **Fix CRITICAL first**: Security issues are urgent\n- **Test after changes**: Run tests after applying fixes\n- **Phase 0 is reference**: Contains required patterns to check against\n- **GreyCat projects**: Services should use withRetry wrapper for gc.* calls\n\n---\n\n## Frontend Consistency Checklist\n\n**Before every commit**:\n\n```\n[ ] Components: named export + memo (pages: default export only)\n[ ] Props interface ABOVE component with JSDoc\n[ ] All components/hooks/services have JSDoc documentation\n[ ] NO inline styles (except dynamic %, deg, computed colors)\n[ ] Theme constants from config/theme OR Tailwind utilities\n[ ] Services use withRetry wrapper\n[ ] React Query: queryKey includes all dependencies\n[ ] Hooks: useCallback with minimal deps, useMemo for derived data\n[ ] Tests use fixtures from test/fixtures.ts (if exists)\n[ ] Tests use setupGcMockForService for service tests\n[ ] npm run lint passes\n[ ] npm run test passes\n```\n\n---\n\n## Example Workflow\n\n```bash\n# 1. Run frontend review\n/frontend\n\n# 2. Review report (144 issues found)\n# - 3 CRITICAL (security)\n# - 40 HIGH\n# - 101 MEDIUM/LOW\n\n# 3. Fix CRITICAL immediately\n# - Sanitize dangerouslySetInnerHTML\n# - Move API key to environment variable\n\n# 4. Fix HIGH this sprint\n# - Add React.memo to 8 components\n# - Add keys to 12 lists\n# - Add ARIA labels to 15 buttons\n\n# 5. Run dead code cleanup\ncd frontend\npnpm dead-code:fix\n\n# 6. Verify changes\nnpm run lint\nnpm run test\nnpm run build\n\n# 7. Re-run review\n/frontend\n# → CRITICAL: 0\n# → HIGH: 10 (reduced from 40)\n\n# 8. Commit\ngit add frontend/\ngit commit -m \"fix: resolve frontend security and performance issues\"\n```\n",
        "plugins/greycat/commands/init.md": "---\nname: init\ndescription: Initialize CLAUDE.md with generic GreyCat development guidelines\nallowed-tools: Write, Read, Bash\n---\n\n# Initialize GreyCat Project Documentation\n\n**Purpose**: Generate CLAUDE.md file with generic GreyCat development best practices and workflow guidelines\n\n**Run When**: Starting new GreyCat project, setting up Claude Code integration\n\n---\n\n## Overview\n\nThis command creates a `CLAUDE.md` file with:\n\n1. **Critical Development Rules** - Always lint, verify dependencies, common gotchas\n2. **GreyCat Language Guide** - Syntax patterns, common mistakes, best practices\n3. **Development Workflow** - Commands, testing, debugging\n4. **Project Structure** - Standard GreyCat project layout\n5. **Common Pitfalls** - What to avoid when developing with GreyCat\n\n---\n\n## CLAUDE.md Template\n\n```markdown\n# CLAUDE.md\n\n[One-line project description]\n\n## ⚠️ CRITICAL RULES\n\n### 1. ALWAYS LINT AFTER EACH CHANGE\n\\`\\`\\`bash\ngreycat-lang lint  # Run IMMEDIATELY after ANY code change - verify 0 errors\n\\`\\`\\`\n\n### 2. VERIFY BEFORE DELETING\nUse \\`Grep\\` to search for usages before removing types/functions/variables.\n\n### 3. GREYCAT GOTCHAS\n\\`\\`\\`gcl\n// ❌ String.substring() doesn't exist\n// ✅ String.slice(from, to)\n\n// ❌ Static methods can't have generics\nabstract type MyService { static fn process<T>(v: T): T { } }\n\n// ✅ Non-static methods with generics\ntype MyHelper<T> { fn process(v: T): T { return v; } }\n\n// ❌ Array<T>::new() | nodeList<T> for local vars\n// ✅ Array<T> {} | Array<T> for non-persistent data\n\\`\\`\\`\n\n### 4. USE GREYCAT SKILL\n**Mandatory** for GCL backend work: use \\`/greycat\\` skill.\n\n---\n\n## Commands\n\n\\`\\`\\`bash\n# Backend\ngreycat-lang lint          # Lint (after EVERY change!)\ngreycat-lang fmt -p project.gcl -w  # Format all .gcl files\ngreycat-lang fmt <file> -w          # Format specific file\ngreycat build/test/serve   # Build/test/start server (port 8080)\ngreycat run [function]     # Run function (default: main)\ngreycat codegen ts         # Generate project.d.ts\ngreycat install            # Install libraries\n\n# Frontend (if exists) - run from root, package.json in root\npnpm install               # First time setup\npnpm dev                   # Dev server (proxies to backend)\npnpm build                 # Build frontend/ → webroot/\npnpm lint                  # Lint TypeScript/React\npnpm test                  # Run tests\n\\`\\`\\`\n\n---\n\n## Stack\n\n**Backend**: GreyCat [version] (GCL)\n**Frontend** (if exists): React + TypeScript + Vite, Tailwind CSS, React Router, TanStack Query\n**Libraries**: \\`@library(\"std\", \"[version]\")\\`, \\`@library(\"explorer\", \"[version]\")\\`\n**Testing**: Vitest + React Testing Library (backend: @test annotation)\n\n**Frontend Setup**: Config files in root (package.json, vite.config.ts, tsconfig.json), source in frontend/, builds to webroot/\n**Frontend Dependencies**: Use exact versions (e.g., \\`\"5.6.9\"\\` instead of \\`\"^5.6.9\"\\`)\n\n---\n\n## Project Structure\n\n\\`\\`\\`\n.\n├── project.gcl                 # Entry point, libraries, permissions\n├── backend/\n│   ├── src/\n│   │   ├── model/              # Data types and global indices\n│   │   ├── service/            # Business logic services\n│   │   ├── api/                # REST API endpoints (@expose)\n│   │   └── edi/                # Import/export logic (optional)\n│   └── test/                   # Test files (*_test.gcl)\n├── frontend/                   # Frontend source (if exists)\n│   ├── src/\n│   │   ├── pages/              # Page components\n│   │   ├── components/         # Reusable components\n│   │   ├── services/           # API clients\n│   │   ├── hooks/              # Custom hooks\n│   │   └── utils/              # Utilities\n│   └── index.html              # HTML entry point\n├── package.json                # Frontend deps (root level)\n├── vite.config.ts              # Vite config (root, builds to webroot/)\n├── tsconfig.json               # TypeScript config (root level)\n├── .gitignore                  # Git ignore rules (GreyCat essentials)\n├── webroot/                    # Built frontend (gitignored, served by GreyCat)\n├── lib/                        # Installed GreyCat libraries (gitignored)\n├── gcdata/                     # Database storage (gitignored)\n└── CLAUDE.md                   # This file\n\\`\\`\\`\n\n**Frontend Build**: Vite builds frontend/ → webroot/, GreyCat serves webroot/ on \\`GREYCAT_WEBROOT=webroot\\`\n\n---\n\n## .gitignore\n\nEssential entries for GreyCat projects:\n\n\\`\\`\\`gitignore\n# GreyCat\n/gcdata                    # Database storage\n/lib                       # Installed GreyCat libraries\n/webroot                   # Built frontend\nproject.d.ts               # Generated TypeScript types\nproject.gcp                # Project cache\n/project_types             # Generated type files\n\n# Frontend dependencies\n/node_modules\n/.pnp\n.pnp.js\n\n# Build outputs\n/dist\n/build\n/coverage\n/outputs\n/generated\n\n# Environment\n.env\n.env.local\n.env.development.local\n.env.test.local\n.env.production.local\n\n# OS\n.DS_Store\nThumbs.db\n\n# IDE\n.vscode/\n.idea/\n*.swp\n*.swo\n\n# Python (if using scripts)\n__pycache__/\n*.pyc\n*.pyo\n\n# Logs\nnpm-debug.log*\nyarn-debug.log*\nyarn-error.log*\npnpm-debug.log*\n\n# Testing\n.playwright-mcp/\n\n# Misc\n.parcel-cache\n\\`\\`\\`\n\n---\n\n## Coding Style\n\n### Backend (GreyCat/GCL)\n\n**Documentation** (REQUIRED): \\`///\\` for ALL functions/types, @param/@return/@throws/@example, \\`// ===\\` section headers\n\\`\\`\\`gcl\n/// Retrieve document by ID.\n/// @param id Document identifier\n/// @return Document object\n/// @throws NotFoundError if not found\n/// @example document(\"62009CJ0204\")\n@expose @permission(\"public\")\nfn document(id: String): Document { }\n\\`\\`\\`\n\n**Services**: Abstract types with static methods, @volatile for transient types, null-safe with \\`?\\`\n\\`\\`\\`gcl\nabstract type SearchService {\n    static fn search(query: String): Array<Result> { }\n}\n@volatile  // REQUIRED for non-persisted types\ntype SearchResults { items: Array<ResultView>; total: int; }\n\\`\\`\\`\n\n**Error Handling** (MANDATORY): try/catch on ALL @expose functions, typed errors only, \\`error()\\` logging\n\\`\\`\\`gcl\n@expose @permission(\"public\")\nfn document(id: String): Document {\n  try {\n    var doc = documents_by_id.get(id);\n    if (doc == null) {\n      throw NotFoundError { message: \"Not found\", id: id };\n    }\n    return doc.resolve();\n  } catch (ex) {\n    error(\"document(${id}) failed: ${ex}\");\n    throw ex;\n  }\n}\n\\`\\`\\`\n\n**Collections**: \\`Array<T> {}\\`, \\`Map<K,V> {}\\`, \\`nodeIndex<K, node<V>>\\`, initialize collection attributes\n**Naming**: snake_case fields, camelCase functions\n\n### Frontend (React/TypeScript) - if exists\n\n**Components** (MANDATORY): Named export + memo, props interface ABOVE component, JSDoc\n\\`\\`\\`tsx\n/**\n * Search results display with pagination\n */\ninterface SearchResultsProps {\n  results: SearchResult[]\n  isLoading: boolean\n}\nexport const SearchResults = memo(function SearchResults({ results, isLoading }: SearchResultsProps) { })\n\\`\\`\\`\n**Exception**: Default export for pages only\n\n**Hooks**: use* prefix, useCallback w/ deps, useMemo for derived, return objects\n**React Query**: queryKey arrays w/ all deps, staleTime config, enabled for conditional\n**Services**: Named export objects, explicit return types from project.d.ts\n**State**: URL (useSearchParams), localStorage, Context (theme/user)\n**Styling**: Theme constants, Tailwind utilities, NO inline styles except dynamic values\n**Naming**: camelCase (variables, functions), PascalCase (components, types)\n\n### Testing\n\n**Backend**: @test annotation, test_function_scenario naming (snake_case), Assert class, \\`// ===\\` headers\n\\`\\`\\`gcl\n@test\nfn test_search_validQuery() {\n    var results = SearchService::search(\"test\");\n    Assert::notNull(results);\n}\n\\`\\`\\`\n\n**Frontend**: Nested describe blocks, fixtures for mock data, test helpers\n\\`\\`\\`tsx\ndescribe('Component', () => {\n  describe('Feature', () => {\n    it('behavior with outcome', () => {\n      render(<Component />)\n      expect(screen.getByText('...')).toBeInTheDocument()\n    })\n  })\n})\n\\`\\`\\`\n\n---\n\n## GreyCat Language Patterns\n\n### Nullability\n\\`\\`\\`gcl\nvar city: City?;                    // nullable\ncity?.name?.size();                 // optional chaining\ncity?.name ?? \"Unknown\";            // nullish coalescing\n(answer as String?) ?? \"default\"    // ⚠️ parens for cast + coalescing\n\nif (country == null) { return null; }\nreturn country->name;               // ✅ no !! after null check\n\\`\\`\\`\n\n**⚠️ NO TERNARY** — use if/else\n\n### Nodes (Persistence)\n\\`\\`\\`gcl\ntype Country { name: String; code: int; }\nvar n = node<Country>{ Country { name: \"LU\", code: 352 }};\nn->name;         // arrow: deref + field\nn.resolve();     // method\n\\`\\`\\`\n\n### Indexed Collections\n| Persisted | Key | In-Memory |\n|-----------|-----|-----------|\n| \\`node<T>\\` | — | \\`Array<T>\\`, \\`Map<K,V>\\` |\n| \\`nodeList<node<T>>\\` | int | \\`Stack<T>\\`, \\`Queue<T>\\` |\n| \\`nodeIndex<K, node<V>>\\` | hash | \\`Set<T>\\`, \\`Tuple<A,B>\\` |\n| \\`nodeTime<node<T>>\\` | time | \\`Buffer\\`, \\`Table\\`, \\`Tensor\\` |\n| \\`nodeGeo<node<T>>\\` | geo | — |\n\n**⚠️ CRITICAL**: Initialize collection attributes on creation\n\n---\n\n## Common Pitfalls\n\n| ❌ Don't | ✅ Do |\n|---------|-------|\n| \\`String.substring()\\` | \\`String.slice(from, to)\\` |\n| Delete without Grep | Grep first, verify no usages |\n| Skip linting | Lint after EACH change |\n| \\`static fn process<T>\\` | Remove static OR generics |\n| \\`Array<T>::new()\\` | \\`Array<T> {}\\` |\n| \\`nodeList<T>\\` for local vars | \\`Array<T>\\` for non-persistent |\n| Missing @volatile | Always add @volatile |\n| Uninitialized collections | Initialize in constructor |\n| \\`throw \"error\"\\` | \\`throw TypedError { ... }\\` |\n| @expose without try/catch | Always wrap + error() |\n| Functions without /// docs | Document ALL functions |\n\n---\n\n## Database (DEV MODE)\n\n**No migrations**: Delete deprecated fields, reset \\`gcdata/\\` freely, use nullable for new fields\n**Reset**: \\`rm -rf gcdata && greycat run import\\`\n\n---\n\n## Environment\n\n\\`\\`\\`bash\n# Backend (.env)\nGREYCAT_PORT=8080\nGREYCAT_WEBROOT=webroot      # Serve built frontend from webroot/\nGREYCAT_CACHE=30000\n\n# Frontend (.env) - if exists\nVITE_GREYCAT_URL=http://localhost:8080\n\\`\\`\\`\n\n**Vite Config** (vite.config.ts in root):\n\\`\\`\\`ts\nexport default defineConfig({\n  root: 'frontend',           // Source files in frontend/\n  build: {\n    outDir: '../webroot',     // Build to webroot/ for GreyCat\n    emptyOutDir: true\n  }\n})\n\\`\\`\\`\n\n**TypeScript Config** (tsconfig.json in root):\n\\`\\`\\`json\n{\n  \"compilerOptions\": {\n    \"baseUrl\": \".\",\n    \"paths\": {\n      \"@/*\": [\"frontend/src/*\"]\n    }\n  },\n  \"include\": [\"frontend/src\"]\n}\n\\`\\`\\`\n\n---\n\n## Auth & Permissions\n\n**project.gcl**:\n\\`\\`\\`gcl\n@permission(\"app.admin\", \"admin permission\");\n@permission(\"app.user\", \"user permission\");\n@role(\"admin\", \"app.admin\", \"app.user\", \"public\", \"admin\", \"api\");\n@role(\"user\", \"app.user\", \"public\", \"api\");\n\\`\\`\\`\n\n**Usage**: \\`@permission(\"app.user\")\\`, \\`SecurityService::getLoggedUser()\\`\n\n---\n\n## Development Workflow\n\n1. Use \\`/greycat\\` skill for backend work\n2. \\`greycat-lang lint\\` after EVERY change (0 errors required)\n3. \\`greycat-lang fmt -p project.gcl -w\\` to format all files\n4. \\`Grep\\` before deleting (verify no usages)\n5. \\`greycat codegen ts\\` after backend type changes\n6. Test: \\`greycat test\\` (backend), \\`pnpm test\\` (frontend)\n\n---\n\n## Consistency Checklist\n\n**Before commit**:\n- [ ] \\`greycat-lang lint\\` shows 0 errors\n- [ ] \\`greycat-lang fmt -p project.gcl -w\\` applied\n- [ ] All @expose functions have try/catch with error() logging\n- [ ] All functions/types have /// documentation\n- [ ] Transient types marked @volatile\n- [ ] Frontend: exact versions in package.json (no ^ or ~)\n- [ ] Tests pass\n\n---\n\n## LSP (Language Server)\n\n**Start**: \\`greycat-lang server --stdio\\`\n**Features**: Autocomplete, hover docs, go-to-def, diagnostics, format, rename\n**Use**: IDE integration for real-time feedback, BUT always \\`greycat-lang lint\\` before commit\n\n---\n\nMore: https://doc.greycat.io/\n```\n\n---\n\n## Execution Steps\n\n### Step 1: Check if CLAUDE.md Exists\n\n```bash\nif [ -f \"CLAUDE.md\" ]; then\n    echo \"⚠️  CLAUDE.md already exists\"\n    echo \"Options:\"\n    echo \"  A) Backup existing and create new\"\n    echo \"  B) Cancel\"\n    # Ask user for choice\nelse\n    echo \"✓ No CLAUDE.md found, creating new file\"\nfi\n```\n\n### Step 2: Detect Project Features\n\n```bash\n# Check for frontend (frontend/ directory or package.json with React)\nHAS_FRONTEND=false\nif [ -d \"frontend\" ] || ([ -f \"package.json\" ] && grep -q \"react\" package.json); then\n    HAS_FRONTEND=true\nfi\n\n# Detect GreyCat version from project.gcl\nGREYCAT_VERSION=$(grep '@library(\"std\"' project.gcl | sed -E 's/.*\"([^\"]+)\".*/\\1/' || echo \"[version]\")\n```\n\n### Step 3: Check/Create .gitignore\n\n```bash\nif [ ! -f \".gitignore\" ]; then\n    echo \"✓ No .gitignore found, will create with GreyCat essentials\"\n    CREATE_GITIGNORE=true\nelif ! grep -q \"gcdata\" .gitignore; then\n    echo \"⚠️  .gitignore exists but missing GreyCat entries\"\n    echo \"Will append GreyCat-specific entries\"\n    APPEND_GITIGNORE=true\nelse\n    echo \"✓ .gitignore exists with GreyCat entries\"\nfi\n```\n\n### Step 4: Generate CLAUDE.md\n\nUse Write tool to create CLAUDE.md with template above, customizing:\n\n- **Replace placeholders**: `[One-line project description]`, `[version]`\n- **Remove frontend sections** if no frontend detected:\n  - Frontend commands (pnpm commands)\n  - Frontend section in Stack\n  - Frontend structure in Project Structure\n  - Frontend section in Coding Style\n  - Frontend testing patterns\n  - Frontend environment variables\n- **Keep all backend sections** (always present in GreyCat projects)\n\nIf CREATE_GITIGNORE or APPEND_GITIGNORE, add/update .gitignore with essential GreyCat entries from template\n\n### Step 5: Report\n\n```\n===============================================================================\nGREYCAT PROJECT INITIALIZED\n===============================================================================\n\nCreated: CLAUDE.md (~300 lines)\n[Created/Updated: .gitignore with GreyCat essentials]\n\nCLAUDE.md includes:\n  ✓ Critical rules (lint, verify, gotchas)\n  ✓ Commands (backend [+ frontend])\n  ✓ Coding style (backend [+ frontend])\n  ✓ GreyCat patterns (nullability, nodes, collections)\n  ✓ .gitignore section (GreyCat essentials)\n  ✓ Common pitfalls table\n  ✓ Development workflow\n\n.gitignore [created/updated]:\n  ✓ /gcdata, /lib, /webroot (GreyCat)\n  ✓ project.d.ts, project.gcp (generated)\n  ✓ /node_modules, .env (frontend)\n\nNext steps:\n  1. Replace \"[One-line project description]\" in CLAUDE.md\n  2. Add project-specific sections (data model, architecture)\n  3. Commit:\n     git add CLAUDE.md .gitignore\n     git commit -m \"docs: initialize Claude Code development guide\"\n\n===============================================================================\n```\n\n---\n\n## Success Criteria\n\n✓ **CLAUDE.md created** (~300 lines, compressed format)\n✓ **.gitignore created/updated** with GreyCat essentials (gcdata, lib, webroot, project.d.ts, project.gcp)\n✓ **Generic rules included** (linting, gotchas, workflows)\n✓ **pnpm commands** (not npm)\n✓ **Root-level configs** (package.json, vite.config.ts, tsconfig.json), source in frontend/\n✓ **Vite builds to webroot/** (GreyCat-compatible)\n✓ **Exact version note** for frontend deps\n✓ **Customized for frontend presence**\n\n---\n\n## Notes\n\n- **Compressed format**: ~300 lines vs 600 in old template\n- **Generic template**: No project-specific details\n- **GreyCat essentials**: Creates/updates .gitignore with gcdata/, lib/, webroot/, project.d.ts, project.gcp\n- **Frontend structure**: All config in root (package.json, vite.config.ts, tsconfig.json), source in frontend/, builds to webroot/\n- **Frontend preferences**: pnpm, exact versions (no ^ or ~), webroot/ for GreyCat compatibility\n- **Can be regenerated**: Safe to run multiple times (with backup option)\n",
        "plugins/greycat/commands/migrate.md": "---\nname: migrate\ndescription: Schema evolution, data migrations, import/export, and storage health management\nallowed-tools: AskUserQuestion, Read, Write, Edit, Bash, Grep, Glob\n---\n\n# GreyCat Migration & Database Management\n\n**Purpose**: Comprehensive database lifecycle management - schema changes, data transformations, bulk operations, storage maintenance\n\n**Run When**: Schema changes, data migrations, bulk import/export, storage maintenance\n\n---\n\n## Overview\n\nThis command handles four main database operations:\n\n1. **Schema Evolution** - Add/remove fields safely, detect breaking changes\n2. **Data Migration** - Transform existing data after schema changes\n3. **Import/Export** - Bulk data operations (CSV, JSON, JSONL)\n4. **Storage Health** - Defrag, backup, diagnostics\n\n---\n\n## Step 1: Choose Operation\n\n**Ask user** via AskUserQuestion:\n\n```typescript\nAskUserQuestion({\n  questions: [{\n    question: \"What database operation do you need?\",\n    header: \"Operation\",\n    multiSelect: false,\n    options: [\n      {\n        label: \"Schema Evolution\",\n        description: \"Add/remove fields safely, handle schema changes\"\n      },\n      {\n        label: \"Data Migration\",\n        description: \"Transform existing data after schema changes\"\n      },\n      {\n        label: \"Import/Export\",\n        description: \"Bulk data import from CSV/JSON or export to files\"\n      },\n      {\n        label: \"Storage Health\",\n        description: \"Defrag, backup, diagnostics, storage optimization\"\n      }\n    ]\n  }]\n})\n```\n\n---\n\n## Operation A: Schema Evolution\n\n### Phase 1: Detect Changes\n\n**Scan current model files**:\n\n```bash\necho \"================================================================================\"\necho \"SCANNING MODEL DEFINITIONS\"\necho \"================================================================================\"\necho \"\"\n\n# Find all model files\nMODEL_FILES=$(find backend/src/model -name \"*.gcl\" -type f)\n\necho \"Model files found:\"\necho \"$MODEL_FILES\" | sed 's/^/  /'\necho \"\"\n```\n\n**Parse type definitions** using Grep:\n\n```bash\n# Extract all type definitions\nfor file in $MODEL_FILES; do\n    echo \"Analyzing: $file\"\n\n    # Find type names\n    TYPES=$(grep -o \"^type [A-Z][a-zA-Z0-9]*\" \"$file\" | awk '{print $2}')\n\n    for type in $TYPES; do\n        echo \"  Type: $type\"\n\n        # Extract fields\n        FIELDS=$(grep -A 50 \"^type $type\" \"$file\" | \\\n                 sed -n '/^type/,/^}/p' | \\\n                 grep -o \"^\\s*[a-z_][a-zA-Z0-9_]*:\" | \\\n                 sed 's/://;s/^\\s*//')\n\n        echo \"    Fields: $(echo $FIELDS | tr '\\n' ', ')\"\n    done\n    echo \"\"\ndone\n```\n\n### Phase 2: Safety Analysis\n\n**Ask user what they want to change**:\n\n```typescript\nAskUserQuestion({\n  questions: [{\n    question: \"What schema change do you want to make?\",\n    header: \"Change Type\",\n    multiSelect: false,\n    options: [\n      {\n        label: \"Add new field\",\n        description: \"Add a new field to an existing type\"\n      },\n      {\n        label: \"Remove field\",\n        description: \"Remove an existing field from a type\"\n      },\n      {\n        label: \"Change field type\",\n        description: \"Modify the type of an existing field\"\n      },\n      {\n        label: \"Rename field\",\n        description: \"Rename an existing field (requires migration)\"\n      }\n    ]\n  }]\n})\n```\n\n**For \"Add new field\"**:\n\n```typescript\n// Ask for details\nAskUserQuestion({\n  questions: [\n    {\n      question: \"Which type do you want to modify?\",\n      header: \"Type\",\n      multiSelect: false,\n      options: [/* detected types */]\n    },\n    {\n      question: \"Field name?\",\n      header: \"Field Name\",\n      multiSelect: false,\n      options: [{ label: \"I'll provide it\", description: \"\" }]\n    },\n    {\n      question: \"Field type?\",\n      header: \"Type\",\n      multiSelect: false,\n      options: [\n        { label: \"String\", description: \"Text value\" },\n        { label: \"int\", description: \"64-bit integer\" },\n        { label: \"float\", description: \"Floating point\" },\n        { label: \"bool\", description: \"Boolean\" },\n        { label: \"time\", description: \"Timestamp\" },\n        { label: \"geo\", description: \"Geographic coordinates\" },\n        { label: \"node<T>\", description: \"Reference to another type\" },\n        { label: \"Array<T>\", description: \"List of values\" },\n        { label: \"Custom\", description: \"I'll specify the type\" }\n      ]\n    },\n    {\n      question: \"Is the field nullable?\",\n      header: \"Nullable\",\n      multiSelect: false,\n      options: [\n        { label: \"Yes (nullable with ?)\", description: \"Field can be null (Recommended for new fields)\" },\n        { label: \"No (required)\", description: \"Field must have a value (requires default or migration)\" }\n      ]\n    }\n  ]\n})\n```\n\n**Safety check**:\n\n```\n===============================================================================\nSCHEMA CHANGE ANALYSIS\n===============================================================================\n\nType: Device\nChange: Add field 'priority: int'\n\nSafety: ⚠ BREAKING CHANGE\nReason: Adding non-nullable field to existing type with persisted data\n\nImpact:\n  - Existing Device nodes in gcdata/ don't have 'priority' field\n  - Reading existing nodes will fail without migration\n\nOptions:\n  A) Make field nullable (priority: int?) - SAFE\n  B) Provide default value and migrate data - REQUIRES MIGRATION\n  C) Cancel and add manually\n\n===============================================================================\n```\n\n**Handle based on user choice**:\n\n### Option A: Make Nullable (Safe)\n\n```bash\n# Add nullable field to type definition using Edit tool\n# Example: add \"priority: int?;\" to Device type\n\necho \"Adding nullable field to type definition...\"\n\n# Use Edit tool to add field\n# Original type:\n# type Device {\n#     id: int;\n#     name: String;\n# }\n#\n# Modified:\n# type Device {\n#     id: int;\n#     name: String;\n#     priority: int?;\n# }\n\ngreycat-lang lint --fix\n\necho \"✓ Field added safely (nullable)\"\n```\n\n### Option B: Provide Default Value (Requires Migration)\n\n**Ask for default value**:\n\n```typescript\nAskUserQuestion({\n  questions: [{\n    question: \"What default value should existing nodes get?\",\n    header: \"Default\",\n    multiSelect: false,\n    options: [\n      { label: \"0\", description: \"Zero/empty value\" },\n      { label: \"Custom\", description: \"I'll provide a value\" }\n    ]\n  }]\n})\n```\n\n**Generate migration function**:\n\n```gcl\n// Generated: backend/src/migration/migrate_20260109_add_device_priority.gcl\n\nfn migrate_add_device_priority() {\n    info(\"Starting migration: add Device.priority field\");\n\n    var count = 0;\n    var errors = 0;\n\n    // Iterate all devices\n    for (id: int, device in devices_by_id) {\n        try {\n            // Set default value\n            device->priority = 1;  // User-provided default\n            count = count + 1;\n\n            if (count % 100 == 0) {\n                info(\"Migrated ${count} devices...\");\n            }\n        } catch (ex) {\n            error(\"Failed to migrate device ${id}: ${ex}\");\n            errors = errors + 1;\n        }\n    }\n\n    info(\"Migration complete: ${count} devices updated, ${errors} errors\");\n}\n```\n\n**Execute migration**:\n\n```bash\necho \"================================================================================\"\necho \"EXECUTING MIGRATION\"\necho \"================================================================================\"\necho \"\"\n\n# Run migration function\ngreycat run migrate_add_device_priority\n\necho \"\"\necho \"✓ Migration complete\"\n```\n\n**Update type definition**:\n\n```bash\n# Add non-nullable field after migration\n# Use Edit tool to add \"priority: int;\" to Device type\n\ngreycat-lang lint --fix\n```\n\n### Phase 3: Remove Field (Simpler)\n\n**For \"Remove field\"**:\n\n1. **Grep for field usage** across codebase:\n\n```bash\necho \"Checking usage of field 'old_field' in Device type...\"\n\n# Search for usage\nUSAGE=$(grep -r \"device->old_field\" backend/ --include=\"*.gcl\")\n\nif [ -z \"$USAGE\" ]; then\n    echo \"✓ No usages found - safe to remove\"\nelse\n    echo \"⚠ Field is used in:\"\n    echo \"$USAGE\"\n    echo \"\"\n    echo \"Please remove these usages first\"\n    exit 1\nfi\n```\n\n2. **Remove from type definition**:\n\n```bash\n# Use Edit tool to remove field line\necho \"Removing field from type definition...\"\n\ngreycat-lang lint --fix\n\necho \"✓ Field removed\"\n```\n\n3. **Note about storage**:\n\n```\nNOTE: Existing nodes in gcdata/ still contain the old field data.\nThis is harmless but wastes space. Consider running:\n  - greycat defrag (to compact storage)\n  - OR: rm -rf gcdata && greycat run import (to rebuild from scratch)\n```\n\n---\n\n## Operation B: Data Migration\n\n### Purpose\n\nTransform existing data when schema changes require more than adding/removing fields.\n\n**Examples**:\n- Split `name` field into `first_name` and `last_name`\n- Compute derived fields from existing data\n- Normalize data structures\n- Fix data quality issues\n\n### Step 1: Describe Migration\n\n**Ask user**:\n\n```typescript\nAskUserQuestion({\n  questions: [{\n    question: \"Describe the data transformation needed:\",\n    header: \"Migration\",\n    multiSelect: false,\n    options: [\n      { label: \"I'll describe it\", description: \"Custom transformation logic\" }\n    ]\n  }]\n})\n```\n\n### Step 2: Generate Migration Template\n\n**Create migration file**:\n\n```gcl\n// backend/src/migration/migrate_YYYYMMDD_description.gcl\n\nfn migrate_transform_user_names() {\n    info(\"Starting migration: transform user names\");\n\n    var count = 0;\n    var errors = 0;\n    var skipped = 0;\n\n    // Batch processing (commit every 1000 records)\n    var batch_size = 1000;\n    var batch_count = 0;\n\n    for (id: int, user in users_by_id) {\n        try {\n            // Skip if already migrated\n            if (user->first_name != null) {\n                skipped = skipped + 1;\n                continue;\n            }\n\n            // Parse full name\n            var parts = user->name.split(\" \");\n            if (parts.size() >= 2) {\n                user->first_name = parts[0];\n                user->last_name = parts[parts.size() - 1];\n            } else {\n                user->first_name = user->name;\n                user->last_name = \"\";\n            }\n\n            count = count + 1;\n            batch_count = batch_count + 1;\n\n            // Progress reporting\n            if (count % 100 == 0) {\n                info(\"Migrated ${count} users...\");\n            }\n\n            // Batch commit (GreyCat handles transactions per function)\n            // For very large datasets, consider splitting into multiple function calls\n\n        } catch (ex) {\n            error(\"Failed to migrate user ${id}: ${ex}\");\n            errors = errors + 1;\n        }\n    }\n\n    info(\"Migration complete: ${count} updated, ${skipped} skipped, ${errors} errors\");\n}\n```\n\n### Step 3: Review & Execute\n\n**Show generated migration** to user for review.\n\n**Execute**:\n\n```bash\necho \"================================================================================\"\necho \"EXECUTING MIGRATION\"\necho \"================================================================================\"\necho \"\"\n\n# Backup first\necho \"Creating backup...\"\ntar -czf gcdata-backup-$(date +%Y%m%d-%H%M%S).tar.gz gcdata/\necho \"✓ Backup created\"\necho \"\"\n\n# Run migration\necho \"Running migration...\"\ngreycat run migrate_transform_user_names\n\nMIGRATION_EXIT=$?\n\nif [ $MIGRATION_EXIT -eq 0 ]; then\n    echo \"\"\n    echo \"✓ Migration completed successfully\"\nelse\n    echo \"\"\n    echo \"⚠ Migration failed - restore from backup if needed\"\n    echo \"To restore: tar -xzf gcdata-backup-*.tar.gz\"\nfi\n```\n\n---\n\n## Operation C: Import/Export\n\n### C1: Import CSV\n\n**Step 1: Analyze CSV**\n\n**Ask for CSV file path**:\n\n```bash\n# User provides path\nCSV_FILE=\"path/to/data.csv\"\n\n# Read header\nHEADER=$(head -1 \"$CSV_FILE\")\necho \"CSV Columns: $HEADER\"\n\n# Detect delimiter (comma, semicolon, tab)\nif grep -q \";\" <<< \"$HEADER\"; then\n    DELIMITER=\";\"\nelif grep -q $'\\t' <<< \"$HEADER\"; then\n    DELIMITER=$'\\t'\nelse\n    DELIMITER=\",\"\nfi\n\n# Count rows\nROW_COUNT=$(wc -l < \"$CSV_FILE\")\necho \"Rows: $ROW_COUNT\"\n```\n\n**Ask mapping**:\n\n```typescript\nAskUserQuestion({\n  questions: [{\n    question: \"Map CSV to which GreyCat type?\",\n    header: \"Target Type\",\n    multiSelect: false,\n    options: [/* detected types from model files */]\n  }]\n})\n```\n\n**Step 2: Generate Import Function**\n\n```gcl\n// backend/src/import/import_devices_csv.gcl\n\nfn import_devices_from_csv() {\n    info(\"Starting CSV import...\");\n\n    // Read CSV\n    var csv = CSV::parse(\"/path/to/devices.csv\", true);  // true = has header\n\n    var count = 0;\n    var errors = 0;\n\n    for (row in csv.rows) {\n        try {\n            // Map CSV columns to type fields\n            var name = row.get(\"name\") as String;\n            var lat = row.get(\"latitude\") as float;\n            var lng = row.get(\"longitude\") as float;\n            var status = row.get(\"status\") as String?;\n\n            // Create entity via service\n            var device = DeviceService::create(name, lat, lng, status);\n            count = count + 1;\n\n            if (count % 100 == 0) {\n                info(\"Imported ${count} devices...\");\n            }\n        } catch (ex) {\n            error(\"Failed to import row: ${ex}\");\n            errors = errors + 1;\n        }\n    }\n\n    info(\"Import complete: ${count} imported, ${errors} errors\");\n}\n```\n\n**Step 3: Execute Import**\n\n```bash\necho \"================================================================================\"\necho \"IMPORTING CSV\"\necho \"================================================================================\"\necho \"\"\n\ngreycat run import_devices_from_csv\n\necho \"\"\necho \"✓ Import complete\"\n```\n\n### C2: Export to CSV\n\n**Step 1: Select Data**\n\n**Ask what to export**:\n\n```typescript\nAskUserQuestion({\n  questions: [{\n    question: \"Which data to export?\",\n    header: \"Export\",\n    multiSelect: false,\n    options: [\n      { label: \"All Devices\", description: \"Export devices_by_id index\" },\n      { label: \"All Users\", description: \"Export users_by_id index\" },\n      { label: \"Custom query\", description: \"Filter data before export\" }\n    ]\n  }]\n})\n```\n\n**Step 2: Generate Export Function**\n\n```gcl\n// backend/src/export/export_devices_csv.gcl\n\nfn export_devices_to_csv() {\n    info(\"Starting CSV export...\");\n\n    var rows = Array<Array<String>> {};\n\n    // Header\n    rows.add(Array<String> { \"id\", \"name\", \"latitude\", \"longitude\", \"status\", \"created_at\" });\n\n    var count = 0;\n\n    // Data rows\n    for (id: int, device in devices_by_id) {\n        var row = Array<String> {\n            device->id.toString(),\n            device->name,\n            device->location.lat.toString(),\n            device->location.lng.toString(),\n            device->status ?? \"\",\n            device->created_at.toString()\n        };\n        rows.add(row);\n        count = count + 1;\n    }\n\n    // Write CSV\n    var csv = CSV::write(rows);\n    var file = File::create(\"exports/devices_export_$(Time::now()).csv\");\n    file.write(csv);\n\n    info(\"Export complete: ${count} devices exported\");\n}\n```\n\n**Step 3: Execute Export**\n\n```bash\necho \"================================================================================\"\necho \"EXPORTING TO CSV\"\necho \"================================================================================\"\necho \"\"\n\nmkdir -p exports\n\ngreycat run export_devices_to_csv\n\necho \"\"\necho \"✓ Export complete\"\nls -lh exports/*.csv | tail -1\n```\n\n---\n\n## Operation D: Storage Health\n\n### D1: Defrag\n\n**Run defragmentation**:\n\n```bash\necho \"================================================================================\"\necho \"STORAGE DEFRAGMENTATION\"\necho \"================================================================================\"\necho \"\"\n\n# Check current size\nBEFORE_SIZE=$(du -sh gcdata/ | awk '{print $1}')\necho \"Current size: $BEFORE_SIZE\"\necho \"\"\n\n# Run defrag\necho \"Running defrag...\"\ngreycat defrag\n\n# Check new size\nAFTER_SIZE=$(du -sh gcdata/ | awk '{print $1}')\necho \"\"\necho \"After defrag: $AFTER_SIZE\"\necho \"Saved: $(expr $(du -s gcdata/ | awk '{print $1}') - $(du -s gcdata/ | awk '{print $1}'))B\"\necho \"\"\necho \"✓ Defrag complete\"\n```\n\n### D2: Backup\n\n**Create backup**:\n\n```bash\necho \"================================================================================\"\necho \"CREATING BACKUP\"\necho \"================================================================================\"\necho \"\"\n\nBACKUP_NAME=\"gcdata-backup-$(date +%Y%m%d-%H%M%S).tar.gz\"\n\necho \"Creating backup: $BACKUP_NAME\"\ntar -czf \"$BACKUP_NAME\" gcdata/\n\nBACKUP_SIZE=$(du -sh \"$BACKUP_NAME\" | awk '{print $1}')\n\necho \"\"\necho \"✓ Backup created: $BACKUP_NAME ($BACKUP_SIZE)\"\necho \"\"\necho \"To restore:\"\necho \"  rm -rf gcdata\"\necho \"  tar -xzf $BACKUP_NAME\"\n```\n\n### D3: Diagnostics\n\n**Analyze storage**:\n\n```bash\necho \"================================================================================\"\necho \"STORAGE DIAGNOSTICS\"\necho \"================================================================================\"\necho \"\"\n\n# Size\necho \"Storage size:\"\ndu -sh gcdata/\necho \"\"\n\n# File count\necho \"Files in storage:\"\nfind gcdata/ -type f | wc -l\necho \"\"\n\n# Largest files\necho \"Largest files:\"\nfind gcdata/ -type f -exec du -h {} \\; | sort -rh | head -10\necho \"\"\n\n# Age\necho \"Last modified:\"\nstat -c \"%y\" gcdata/ 2>/dev/null || stat -f \"%Sm\" gcdata/\necho \"\"\n\n# Fragmentation estimate (simple heuristic)\nTOTAL_SIZE=$(du -sb gcdata/ | awk '{print $1}')\nFILE_COUNT=$(find gcdata/ -type f | wc -l)\nAVG_FILE_SIZE=$(expr $TOTAL_SIZE / $FILE_COUNT)\n\necho \"Average file size: $(numfmt --to=iec $AVG_FILE_SIZE 2>/dev/null || echo ${AVG_FILE_SIZE}B)\"\n\nif [ $AVG_FILE_SIZE -lt 1000 ]; then\n    echo \"⚠ High fragmentation suspected (many small files)\"\n    echo \"  Consider running: greycat defrag\"\nelse\n    echo \"✓ Fragmentation looks healthy\"\nfi\n```\n\n**Generate report**:\n\n```\n===============================================================================\nSTORAGE HEALTH REPORT\n===============================================================================\n\nSize: 2.4 GB\nFiles: 15,432\nLast modified: 2026-01-09 10:30:00\n\nLargest files:\n  1.2 GB - gcdata/nodeIndex_devices_by_id\n  450 MB - gcdata/nodeTime_sensor_readings\n  320 MB - gcdata/nodeList_cities_streets\n\nFragmentation: ✓ Healthy\nAverage file size: 158 KB\n\nRecommendations:\n  [✓] Storage size normal\n  [ ] Consider defrag if performance degrades\n  [ ] Setup automated backups\n  [✓] No immediate issues detected\n\n===============================================================================\n```\n\n---\n\n## Success Criteria\n\n✓ **Schema changes applied** safely without data loss\n✓ **Migrations executed** successfully with progress reporting\n✓ **Import/export complete** with error handling\n✓ **Storage optimized** via defrag when needed\n✓ **Backups created** before risky operations\n✓ **greycat-lang lint --fix passes** after schema changes\n\n---\n\n## Notes\n\n- **Backup before migrations**: Always backup gcdata/ before risky operations\n- **Test migrations**: Run on copy of data first\n- **Batch processing**: Large migrations should batch commits\n- **Progress reporting**: Log progress every N records\n- **Rollback plan**: Keep backups and migration reversal functions\n- **Dev vs Prod**: In dev, can delete gcdata/; in prod, must migrate\n",
        "plugins/greycat/commands/optimize.md": "---\nname: optimize\ndescription: Detect and auto-fix performance anti-patterns in GreyCat code\nallowed-tools: Bash, Read, Grep, Glob, Edit, Write, AskUserQuestion\n---\n\n# GreyCat Performance Optimizer\n\n**Purpose**: Detect and automatically fix performance anti-patterns - unnecessary node allocation, reimplemented native functions, code bloat, algorithmic complexity issues\n\n**Run When**: Quick performance checks, before releases, when performance degrades\n\n---\n\n## Overview\n\nThis command performs fast, focused analysis on performance issues:\n\n1. **Unnecessary Persistence** - Using node<T> when plain objects suffice\n2. **Native Function Reimplementation** - Custom code duplicating stdlib\n3. **Useless Function Wrappers** - One-line functions that just call another function\n4. **Algorithmic Complexity** - O(n²) operations where O(n) or O(1) exists\n5. **Code Duplication** - Copy-pasted logic\n\n**Output**: Analysis report + auto-fix options\n\n---\n\n## Step 1: Scan Backend Files\n\n**Find all .gcl files**:\n\n```bash\necho \"================================================================================\"\necho \"SCANNING GREYCAT BACKEND\"\necho \"================================================================================\"\necho \"\"\n\n# Find all GCL files\nGCL_FILES=$(find backend/src -name \"*.gcl\" -type f | sort)\nFILE_COUNT=$(echo \"$GCL_FILES\" | wc -l)\n\necho \"Found $FILE_COUNT files to analyze\"\necho \"\"\n```\n\n---\n\n## Step 2: Phase 1 - Unnecessary Persistence Detection\n\n### Concept\n\n**node<T>**, **nodeList**, **nodeIndex**, **nodeTime**, **nodeGeo** should only be used for:\n- Module-level variables (global indices)\n- Type fields that need persistence\n\n**Local variables, function parameters, and function returns** should use:\n- Plain objects: `T` instead of `node<T>`\n- Arrays: `Array<T>` instead of `nodeList<node<T>>`\n- Maps: `Map<K,V>` instead of `nodeIndex<K,V>`\n\n### Detection Logic\n\n**Scan for**:\n\n```bash\necho \"Phase 1: Detecting unnecessary persistence...\"\necho \"\"\n\n# Find local variables with node types\necho \"Checking local variables...\"\ngrep -rn \"var [a-z_][a-zA-Z0-9_]* = node\\(List\\|Index\\|Time\\|Geo\\)?<\" backend/src --include=\"*.gcl\" | \\\n    grep -v \"^[[:space:]]*var [a-z_][a-zA-Z0-9_]*:\" | \\  # Exclude module-level (no indentation)\n    while IFS=: read -r file line content; do\n        echo \"  ⚠ $file:$line\"\n        echo \"     $content\"\n    done\n\n# Find function parameters with node types\necho \"\"\necho \"Checking function parameters...\"\ngrep -rn \"fn [a-z_][a-zA-Z0-9_]*(.*node\\(List\\|Index\\|Time\\|Geo\\)<\" backend/src --include=\"*.gcl\" | \\\n    while IFS=: read -r file line content; do\n        # Skip if it's a service method expecting persisted nodes\n        if ! grep -q \"Service\" <<< \"$file\"; then\n            echo \"  ⚠ $file:$line\"\n            echo \"     $content\"\n        fi\n    done\n\n# Find function returns with nodeList/nodeIndex\necho \"\"\necho \"Checking function return types...\"\ngrep -rn \"fn [a-z_][a-zA-Z0-9_]*(.*).*: node\\(List\\|Index\\)\" backend/src --include=\"*.gcl\" | \\\n    while IFS=: read -r file line content; do\n        echo \"  ⚠ $file:$line\"\n        echo \"     $content\"\n    done\n```\n\n**Categorize findings**:\n\n```\n===============================================================================\nPHASE 1: UNNECESSARY PERSISTENCE\n===============================================================================\n\n🔴 CRITICAL (3 issues):\n\n  backend/src/api/device_api.gcl:45\n    fn get_devices(): nodeList<node<Device>>\n    → Should return: Array<DeviceView> (API best practice)\n\n  backend/src/service/processor.gcl:120\n    var results = nodeList<node<Item>> {};\n    → Should use: Array<Item> {} (local variable, not persisted)\n\n  backend/src/api/user_api.gcl:78\n    fn process_users(users: nodeList<node<User>>)\n    → Should accept: Array<User> (function parameter)\n\n===============================================================================\n```\n\n### Auto-fix Logic\n\n**For local variables**:\n\n```bash\n# Example fix:\n# BEFORE: var results = nodeList<node<Item>> {};\n# AFTER:  var results = Array<Item> {};\n\n# Use Edit tool to replace\n# Pattern: nodeList<node<T>> → Array<T>\n#          nodeIndex<K, node<V>> → Map<K, V>\n#          node<T>{obj} → obj (if local scope)\n```\n\n---\n\n## Step 3: Phase 2 - Native Function Reimplementation\n\n### Common Patterns\n\n**Sorting**:\n```gcl\n// ❌ Custom bubble sort\nfn sort_items(items: Array<Item>): Array<Item> {\n    for (i in 0..items.size()) {\n        for (j in i+1..items.size()) {\n            if (items[i]->priority > items[j]->priority) {\n                // swap\n            }\n        }\n    }\n    return items;\n}\n\n// ✅ Use native\nitems.sort_by(Item::priority, SortOrder::asc);\n```\n\n**Min/Max**:\n```gcl\n// ❌ Custom max finder\nfn find_max(values: Array<float>): float {\n    var max = values[0];\n    for (v in values) {\n        if (v > max) { max = v; }\n    }\n    return max;\n}\n\n// ✅ Use native or Tensor\n// Math::max(values) or tensor operations\n```\n\n**String operations**:\n```gcl\n// ❌ Custom string join\nfn join_strings(strings: Array<String>, sep: String): String {\n    var result = \"\";\n    for (i, s in strings) {\n        if (i > 0) { result = result + sep; }\n        result = result + s;\n    }\n    return result;\n}\n\n// ✅ Use native\nstrings.join(sep)\n```\n\n### Detection\n\n```bash\necho \"Phase 2: Detecting reimplemented native functions...\"\necho \"\"\n\n# Look for sorting implementations\necho \"Checking for custom sort implementations...\"\ngrep -rn \"fn [a-z_]*sort\" backend/src --include=\"*.gcl\" -A 20 | \\\n    grep -B 1 -A 15 \"for.*for\" | \\  # Nested loops suggest bubble/selection sort\n    while IFS=: read -r file line content; do\n        echo \"  ⚠ $file:$line - Possible custom sort (use .sort_by())\"\n    done\n\n# Look for min/max implementations\necho \"\"\necho \"Checking for custom min/max...\"\ngrep -rn \"fn find_\\(max\\|min\\|maximum\\|minimum\\)\" backend/src --include=\"*.gcl\" | \\\n    while IFS=: read -r file line content; do\n        echo \"  ⚠ $file:$line - Custom min/max (use Math:: or Tensor)\"\n    done\n\n# Look for string join implementations\necho \"\"\necho \"Checking for custom string operations...\"\ngrep -rn \"fn [a-z_]*join\" backend/src --include=\"*.gcl\" -A 10 | \\\n    grep -B 1 \"for.*in.*{\" | \\\n    while IFS=: read -r file line content; do\n        echo \"  ⚠ $file:$line - Custom join (use .join())\"\n    done\n```\n\n**Report**:\n\n```\n===============================================================================\nPHASE 2: REIMPLEMENTED NATIVE FUNCTIONS\n===============================================================================\n\n🟡 MEDIUM (3 issues):\n\n  backend/src/util/array_utils.gcl:23\n    fn sort_by_priority(items: Array<Item>)\n    → Use native: items.sort_by(Item::priority, SortOrder::asc)\n\n  backend/src/util/math_utils.gcl:45\n    fn find_maximum(values: Array<float>)\n    → Use Math:: module or Tensor operations\n\n  backend/src/util/string_utils.gcl:67\n    fn join_with_comma(strings: Array<String>)\n    → Use native: strings.join(\", \")\n\n===============================================================================\n```\n\n---\n\n## Step 4: Phase 3 - Useless Function Wrappers\n\n### Pattern\n\nFunctions that contain only a single statement calling another function:\n\n```gcl\n// ❌ Useless wrapper\nfn get_user(id: int): node<User>? {\n    return UserService::find_by_id(id);\n}\n\n// ✅ Just call UserService::find_by_id(id) directly\n```\n\n### Detection\n\n```bash\necho \"Phase 3: Detecting useless function wrappers...\"\necho \"\"\n\n# Find functions with single return statement\nfor file in $GCL_FILES; do\n    # Extract function definitions\n    awk '/^fn [a-z_]/ {\n        func_line = NR;\n        func = $0;\n        getline; # Read opening brace\n        if ($0 ~ /return.*::/) {\n            getline; # Read closing brace\n            if ($0 ~ /^}/) {\n                print FILENAME \":\" func_line \": \" func\n            }\n        }\n    }' \"$file\"\ndone | while IFS=: read -r file line content; do\n    echo \"  ⚠ $file:$line\"\n    echo \"     $content\"\n    echo \"     (One-line wrapper - consider removing)\"\ndone\n```\n\n**Report**:\n\n```\n===============================================================================\nPHASE 3: USELESS FUNCTION WRAPPERS\n===============================================================================\n\n🟡 MEDIUM (5 issues):\n\n  backend/src/api/user_api.gcl:67\n    fn get_user(id: int): node<User>?\n    → Single-line wrapper for UserService::find_by_id()\n    → Consider calling UserService directly\n\n  backend/src/service/device_helper.gcl:34\n    fn find_device(id: int): node<Device>?\n    → Wraps DeviceService::find_by_id()\n\n  ... (3 more)\n\n===============================================================================\n```\n\n---\n\n## Step 5: Phase 4 - Algorithmic Complexity\n\n### O(n²) Nested Loops\n\n**Problem**:\n```gcl\n// ❌ O(n²) - iterates all users × all orders\nfor (user in all_users) {\n    for (order in all_orders) {\n        if (order->user_id == user->id) {\n            // Process order for user\n        }\n    }\n}\n\n// ✅ O(n) - use nodeIndex for O(1) lookup\nfor (user in all_users) {\n    var user_orders = orders_by_user_id.get(user->id) ?? Array<node<Order>>{};\n    for (order in user_orders) {\n        // Process order for user\n    }\n}\n```\n\n### Detection\n\n```bash\necho \"Phase 4: Detecting algorithmic complexity issues...\"\necho \"\"\n\n# Find nested loops\necho \"Checking for nested loops...\"\ngrep -rn \"for.*in.*{\" backend/src --include=\"*.gcl\" -A 5 | \\\n    grep \"for.*in.*{\" | \\\n    while IFS=: read -r file line content; do\n        # Check if inner loop has conditional matching\n        CONTEXT=$(sed -n \"${line},$((line+10))p\" \"$file\")\n        if grep -q \"if.*==.*{\" <<< \"$CONTEXT\"; then\n            echo \"  ⚠ $file:$line - Possible O(n²) with conditional match\"\n            echo \"     Consider using nodeIndex for O(1) lookup\"\n        fi\n    done\n\n# Find linear searches where index exists\necho \"\"\necho \"Checking for linear searches...\"\ngrep -rn \"for.*in.*{\" backend/src --include=\"*.gcl\" -A 3 | \\\n    grep -B 1 \"if.*->id ==\" | \\\n    while IFS=: read -r file line content; do\n        echo \"  ⚠ $file:$line - Linear search by ID\"\n        echo \"     Consider using nodeIndex for direct lookup\"\n    done\n```\n\n**Report**:\n\n```\n===============================================================================\nPHASE 4: ALGORITHMIC COMPLEXITY\n===============================================================================\n\n🔴 CRITICAL (2 issues):\n\n  backend/src/processor/matcher.gcl:89\n    Nested loop: for (user in users) { for (order in orders) { if (order->user_id == user->id) ... } }\n    → O(n²) complexity\n    → Solution: Create orders_by_user_id: nodeIndex<int, node<Order>>\n\n  backend/src/service/lookup.gcl:134\n    Linear search: for (item in items) { if (item->id == target_id) ... }\n    → O(n) when O(1) possible\n    → Solution: Use items_by_id nodeIndex\n\n===============================================================================\n```\n\n### Auto-fix\n\n**Suggest index creation**:\n\n```gcl\n// Add to model file:\nvar orders_by_user_id: nodeIndex<int, nodeList<node<Order>>>;\n\n// Update service to maintain index:\nabstract type OrderService {\n    static fn create(user_id: int, amount: float): node<Order> {\n        var order = node<Order>{ Order { user_id: user_id, amount: amount }};\n\n        // Add to user's orders\n        var user_orders = orders_by_user_id.get(user_id);\n        if (user_orders == null) {\n            user_orders = nodeList<node<Order>>{};\n            orders_by_user_id.set(user_id, user_orders);\n        }\n        user_orders.add(order);\n\n        return order;\n    }\n}\n\n// Use in code:\nfor (user in all_users) {\n    var user_orders = orders_by_user_id.get(user->id);\n    if (user_orders != null) {\n        for (i, order in user_orders) {\n            // Process\n        }\n    }\n}\n```\n\n---\n\n## Step 6: Phase 5 - Code Duplication\n\n**Leverage Grep to find similar code blocks**:\n\n```bash\necho \"Phase 5: Detecting code duplication...\"\necho \"\"\n\n# Find duplicate function signatures (same name, different files)\ngrep -rn \"^fn [a-z_][a-zA-Z0-9_]*(\" backend/src --include=\"*.gcl\" | \\\n    awk -F: '{print $3}' | \\\n    sort | \\\n    uniq -d | \\\n    while read func; do\n        echo \"  ⚠ Duplicate function signature: $func\"\n        grep -rn \"$func\" backend/src --include=\"*.gcl\"\n    done\n\n# Find repeated patterns (e.g., same error message strings)\necho \"\"\necho \"Checking for repeated error messages...\"\ngrep -rn \"throw \\\"\" backend/src --include=\"*.gcl\" | \\\n    awk -F'\"' '{print $2}' | \\\n    sort | \\\n    uniq -c | \\\n    sort -rn | \\\n    head -10 | \\\n    while read count msg; do\n        if [ $count -gt 2 ]; then\n            echo \"  ⚠ Error message repeated $count times: \\\"$msg\\\"\"\n        fi\n    done\n```\n\n**Report**:\n\n```\n===============================================================================\nPHASE 5: CODE DUPLICATION\n===============================================================================\n\n🟢 LOW (4 issues):\n\n  Duplicate error messages (3+ occurrences):\n    - \"User not found\" (5 times)\n    - \"Invalid email format\" (4 times)\n    → Consider creating error constant or helper function\n\n  Similar validation logic in 3 files:\n    - backend/src/service/user_service.gcl:45\n    - backend/src/service/admin_service.gcl:78\n    - backend/src/api/auth_api.gcl:23\n    → Extract to shared validation function\n\n===============================================================================\n```\n\n---\n\n## Step 7: Consolidate Report\n\n**Generate summary**:\n\n```\n===============================================================================\nPERFORMANCE OPTIMIZATION REPORT\n===============================================================================\n\nAnalyzed: 47 files (backend/src)\n\nFound 17 issues:\n\n🔴 CRITICAL (5):\n  1. backend/src/api/device_api.gcl:45 - API returning nodeList instead of Array<View>\n  2. backend/src/service/processor.gcl:120 - Local var using nodeList instead of Array\n  3. backend/src/processor/matcher.gcl:89 - O(n²) nested loop, needs nodeIndex\n  4. backend/src/api/user_api.gcl:78 - Function parameter using nodeList\n  5. backend/src/service/lookup.gcl:134 - Linear search, needs nodeIndex\n\n🟡 MEDIUM (8):\n  6-10. Reimplemented native functions (sort, max, join, etc.)\n  11-13. Useless function wrappers\n\n🟢 LOW (4):\n  14-17. Code duplication (error messages, validation logic)\n\nAuto-fix available for 12/17 issues\n\nEstimated performance improvement: ~35%\n  - Removing unnecessary persistence: ~20%\n  - Fixing O(n²) operations: ~15%\n  - Native functions: ~5%\n\n===============================================================================\n```\n\n---\n\n## Step 8: Apply Fixes\n\n**Ask user**:\n\n```typescript\nAskUserQuestion({\n  questions: [{\n    question: \"Apply automatic fixes?\",\n    header: \"Auto-fix\",\n    multiSelect: false,\n    options: [\n      {\n        label: \"Fix critical issues only (Recommended)\",\n        description: \"Auto-fix the 5 critical performance issues\"\n      },\n      {\n        label: \"Fix all auto-fixable issues\",\n        description: \"Fix 12 issues automatically (critical + medium)\"\n      },\n      {\n        label: \"Show detailed report first\",\n        description: \"Review each issue before fixing\"\n      },\n      {\n        label: \"Cancel\",\n        description: \"No changes, just report\"\n      }\n    ]\n  }]\n})\n```\n\n**Apply fixes** using Edit tool:\n\n```bash\necho \"================================================================================\"\necho \"APPLYING FIXES\"\necho \"================================================================================\"\necho \"\"\n\n# Fix 1: API returning nodeList\necho \"Fixing: backend/src/api/device_api.gcl:45\"\n# Use Edit tool to change return type from nodeList<node<Device>> to Array<DeviceView>\n\n# Fix 2: Local var using nodeList\necho \"Fixing: backend/src/service/processor.gcl:120\"\n# Use Edit tool to change \"var results = nodeList<node<Item>> {};\" to \"var results = Array<Item> {};\"\n\n# ... apply other fixes\n\necho \"\"\necho \"✓ Applied 12 fixes\"\n```\n\n**Run lint**:\n\n```bash\necho \"================================================================================\"\necho \"VERIFYING FIXES\"\necho \"================================================================================\"\necho \"\"\n\ngreycat-lang lint --fix\n\nLINT_EXIT=$?\n\nif [ $LINT_EXIT -eq 0 ]; then\n    echo \"\"\n    echo \"✓ All fixes applied successfully, lint passes\"\nelse\n    echo \"\"\n    echo \"⚠ Some fixes may need manual adjustment\"\nfi\n```\n\n**Final report**:\n\n```\n===============================================================================\nOPTIMIZATION COMPLETE\n===============================================================================\n\nFixed 12 issues:\n  ✓ 5 critical (unnecessary persistence, O(n²) operations)\n  ✓ 7 medium (native functions, wrappers)\n\nRemaining 5 issues require manual review:\n  ! backend/src/service/complex.gcl:234 - Complex duplication, needs refactoring\n  ! backend/src/processor/advanced.gcl:567 - Algorithmic improvement needs design\n\nLint: ✓ Passes\n\nEstimated performance improvement: ~35%\n\nNext steps:\n  1. Review remaining issues manually\n  2. Run tests: greycat test\n  3. Benchmark before/after if needed\n\n===============================================================================\n```\n\n---\n\n## Success Criteria\n\n✓ **Performance issues detected** across all categories\n✓ **Auto-fixes applied** safely with lint verification\n✓ **Detailed report generated** with severity levels\n✓ **Estimated improvements** calculated\n✓ **greycat-lang lint --fix passes** after fixes\n\n---\n\n## Notes\n\n- **Focus**: Quick, automated performance wins\n- **Safe fixes**: Only applies changes that don't alter logic\n- **Comprehensive**: Covers persistence, complexity, duplication\n- **Complement to backend**: Use /greycat:backend for full code review\n- **Regular use**: Run before releases or when performance degrades\n",
        "plugins/greycat/commands/scaffold.md": "---\nname: scaffold\ndescription: Generate models, services, APIs, and tests with proper GreyCat structure\nallowed-tools: AskUserQuestion, Read, Write, Bash, Grep, Glob\n---\n\n# GreyCat Scaffold Generator\n\n**Purpose**: Generate models, services, API endpoints, and tests with proper GreyCat structure and conventions\n\n**Run When**: Starting new features, adding CRUD operations, creating new domain entities\n\n---\n\n## Overview\n\nThis command scaffolds complete GreyCat features with:\n\n1. **Model** - Type definition with proper fields and initialization\n2. **Global Indices** - nodeIndex/nodeList/nodeTime/nodeGeo for lookups\n3. **Service** - CRUD operations with validation\n4. **API Layer** - @expose endpoints with @volatile response types\n5. **Tests** - Comprehensive test suite\n\n**Templates**:\n- **CRUD Service** - Full create/read/update/delete\n- **Time-series Collector** - Model with nodeTime + data collection\n- **Graph Traversal** - Model with relationships + traversal queries\n- **Custom** - Interactive builder\n\n---\n\n## Step 1: Template Selection\n\n**Ask user** via AskUserQuestion:\n\n```typescript\nAskUserQuestion({\n  questions: [{\n    question: \"Which scaffold template would you like to use?\",\n    header: \"Template\",\n    multiSelect: false,\n    options: [\n      {\n        label: \"CRUD Service (Recommended)\",\n        description: \"Complete create/read/update/delete for an entity with global indices\"\n      },\n      {\n        label: \"Time-series Collector\",\n        description: \"Model with nodeTime index for temporal data collection and querying\"\n      },\n      {\n        label: \"Graph Traversal API\",\n        description: \"Model with relationships and traversal query endpoints\"\n      },\n      {\n        label: \"Custom (Guided)\",\n        description: \"Step-by-step builder for specific needs\"\n      }\n    ]\n  }]\n})\n```\n\n---\n\n## Step 2: Detect Project Structure\n\n**Check existing project structure**:\n\n```bash\n# Verify we're in a GreyCat project\nif [ ! -f \"project.gcl\" ]; then\n    echo \"ERROR: Not in a GreyCat project root (project.gcl not found)\"\n    exit 1\nfi\n\n# Check standard directories exist\nif [ ! -d \"backend/src/model\" ]; then\n    mkdir -p backend/src/model\nfi\n\nif [ ! -d \"backend/src/service\" ]; then\n    mkdir -p backend/src/service\nfi\n\nif [ ! -d \"backend/src/api\" ]; then\n    mkdir -p backend/src/api\nfi\n\nif [ ! -d \"backend/test\" ]; then\n    mkdir -p backend/test\nfi\n```\n\n---\n\n## Step 3: Gather Entity Details\n\n**Ask for entity information** via AskUserQuestion:\n\n```typescript\nAskUserQuestion({\n  questions: [\n    {\n      question: \"What is the entity name? (PascalCase, e.g., Device, User, Order)\",\n      header: \"Entity Name\",\n      multiSelect: false,\n      options: [\n        { label: \"I'll provide it\", description: \"Enter custom entity name\" }\n      ]\n    }\n  ]\n})\n```\n\n**Get field definitions** (ask user to provide):\n\nExample format:\n```\nname: String\nemail: String\nage: int?\ncreated_at: time\n```\n\n**Parse fields**:\n- Split by newline\n- Extract field name, type, nullable (?)\n- Validate types against GreyCat types\n\n**Ask for indices** via AskUserQuestion:\n\n```typescript\nAskUserQuestion({\n  questions: [{\n    question: \"Which indices do you need for lookups?\",\n    header: \"Indices\",\n    multiSelect: true,\n    options: [\n      {\n        label: \"By ID (nodeIndex<int, node<T>>)\",\n        description: \"Standard ID-based lookup\"\n      },\n      {\n        label: \"By unique field (nodeIndex<String, node<T>>)\",\n        description: \"Lookup by email, username, or other unique field\"\n      },\n      {\n        label: \"List (nodeList<node<T>>)\",\n        description: \"Ordered collection for iteration\"\n      },\n      {\n        label: \"Time-series (nodeTime<node<T>> or nodeTime<primitive>)\",\n        description: \"Temporal data with time-based queries\"\n      },\n      {\n        label: \"Geo-spatial (nodeGeo<node<T>>)\",\n        description: \"Geographic queries with bounding box/circle/polygon\"\n      }\n    ]\n  }]\n})\n```\n\n---\n\n## Step 4: Analyze Existing Code Style\n\n**Read existing files to detect naming conventions**:\n\n```bash\n# Check for existing services\nEXISTING_SERVICES=$(find backend/src/service -name \"*_service.gcl\" 2>/dev/null | head -3)\n\n# Check for existing models\nEXISTING_MODELS=$(find backend/src/model -name \"*.gcl\" 2>/dev/null | head -3)\n\n# Check for existing APIs\nEXISTING_APIS=$(find backend/src/api -name \"*_api.gcl\" 2>/dev/null | head -3)\n```\n\n**Detect patterns** using Read tool:\n- Indentation (tabs vs spaces, 2/4 spaces)\n- Line width (from project.gcl @format_line_width)\n- Naming conventions (snake_case files confirmed)\n- Error handling style (try/catch vs early return)\n\n---\n\n## Step 5: Generate Files\n\n### A. Generate Model File\n\n**File**: `backend/src/model/{entity_name_snake}.gcl`\n\n**Template for CRUD**:\n\n```gcl\n// {EntityName} model and global indices\ntype {EntityName} {\n    {field1}: {Type1};\n    {field2}: {Type2};\n    // ... all fields\n}\n\n// Global indices\nvar {entity_plural}_by_id: nodeIndex<int, node<{EntityName}>>;\n{optional_additional_indices}\n\n// ID counter for auto-increment\nvar {entity}_id_counter: node<int?>;\n```\n\n**Example output** for `Device`:\n\n```gcl\n// Device model and global indices\ntype Device {\n    id: int;\n    name: String;\n    location: geo;\n    status: String?;\n    created_at: time;\n}\n\n// Global indices\nvar devices_by_id: nodeIndex<int, node<Device>>;\nvar devices_by_name: nodeIndex<String, node<Device>>;\n\n// ID counter\nvar device_id_counter: node<int?>;\n```\n\n**Use Write tool** to create the file.\n\n### B. Generate Service File\n\n**File**: `backend/src/service/{entity_name_snake}_service.gcl`\n\n**Template**:\n\n```gcl\n// {EntityName} service - business logic and CRUD operations\nabstract type {EntityName}Service {\n\n    static fn create({params}): node<{EntityName}> {\n        // Validation\n        {validation_logic}\n\n        // Generate ID\n        var id = ({entity}_id_counter.resolve() ?? 0) + 1;\n        {entity}_id_counter.set(id);\n\n        // Create entity\n        var {entity} = node<{EntityName}>{ {EntityName} {\n            id: id,\n            {field_assignments}\n            created_at: Time::now()\n        }};\n\n        // Store in indices\n        {entity_plural}_by_id.set(id, {entity});\n        {additional_index_inserts}\n\n        return {entity};\n    }\n\n    static fn find_by_id(id: int): node<{EntityName}>? {\n        return {entity_plural}_by_id.get(id);\n    }\n\n    {additional_find_methods}\n\n    static fn list_all(): Array<node<{EntityName}>> {\n        var results = Array<node<{EntityName}>> {};\n        for (id, {entity} in {entity_plural}_by_id) {\n            results.add({entity});\n        }\n        return results;\n    }\n\n    static fn update_{field}({entity}: node<{EntityName}>, new_{field}: {Type}) {\n        // Update index if needed\n        {index_update_logic}\n\n        // Update field\n        {entity}->{field} = new_{field};\n    }\n\n    static fn delete({entity}: node<{EntityName}>) {\n        {entity_plural}_by_id.remove({entity}->id);\n        {additional_index_removals}\n    }\n}\n```\n\n**Example for Device**:\n\n```gcl\n// Device service - business logic and CRUD operations\nabstract type DeviceService {\n\n    static fn create(name: String, lat: float, lng: float, status: String?): node<Device> {\n        // Validation\n        if (devices_by_name.get(name) != null) {\n            throw \"Device with name '${name}' already exists\";\n        }\n\n        // Generate ID\n        var id = (device_id_counter.resolve() ?? 0) + 1;\n        device_id_counter.set(id);\n\n        // Create device\n        var device = node<Device>{ Device {\n            id: id,\n            name: name,\n            location: geo { lat: lat, lng: lng },\n            status: status,\n            created_at: Time::now()\n        }};\n\n        // Store in indices\n        devices_by_id.set(id, device);\n        devices_by_name.set(name, device);\n\n        return device;\n    }\n\n    static fn find_by_id(id: int): node<Device>? {\n        return devices_by_id.get(id);\n    }\n\n    static fn find_by_name(name: String): node<Device>? {\n        return devices_by_name.get(name);\n    }\n\n    static fn list_all(): Array<node<Device>> {\n        var results = Array<node<Device>> {};\n        for (id, device in devices_by_id) {\n            results.add(device);\n        }\n        return results;\n    }\n\n    static fn update_name(device: node<Device>, new_name: String) {\n        // Remove from name index\n        devices_by_name.remove(device->name);\n\n        // Update field\n        device->name = new_name;\n\n        // Re-add to name index\n        devices_by_name.set(new_name, device);\n    }\n\n    static fn update_status(device: node<Device>, new_status: String?) {\n        device->status = new_status;\n    }\n\n    static fn delete(device: node<Device>) {\n        devices_by_id.remove(device->id);\n        devices_by_name.remove(device->name);\n    }\n}\n```\n\n**Use Write tool** to create the file.\n\n### C. Generate API File\n\n**File**: `backend/src/api/{entity_name_snake}_api.gcl`\n\n**Template**:\n\n```gcl\n// {EntityName} REST API endpoints\n\n// Request/Response types\n@volatile type {EntityName}View {\n    {field1}: {Type1};\n    {field2}: {Type2};\n    // ... all fields\n}\n\n@volatile type {EntityName}Create {\n    {field1}: {Type1};\n    {field2}: {Type2};\n    // ... fields except id, created_at\n}\n\n@volatile type {EntityName}Update {\n    {field1}?: {Type1};\n    {field2}?: {Type2};\n    // ... updatable fields as nullable\n}\n\n// Endpoints\n@expose\n@permission(\"public\")\nfn get_{entity_plural}(): Array<{EntityName}View> {\n    var views = Array<{EntityName}View> {};\n    var {entity_plural} = {EntityName}Service::list_all();\n\n    for ({entity} in {entity_plural}) {\n        views.add({EntityName}View {\n            {field_mappings}\n        });\n    }\n\n    return views;\n}\n\n@expose\n@permission(\"public\")\nfn get_{entity}_by_id(id: int): {EntityName}View {\n    var {entity} = {EntityName}Service::find_by_id(id);\n    if ({entity} == null) {\n        throw \"{EntityName} not found\";\n    }\n\n    return {EntityName}View {\n        {field_mappings}\n    };\n}\n\n@expose\n@permission(\"admin\")\nfn create_{entity}(data: {EntityName}Create): {EntityName}View {\n    var {entity} = {EntityName}Service::create({param_list});\n\n    return {EntityName}View {\n        {field_mappings}\n    };\n}\n\n@expose\n@permission(\"admin\")\nfn update_{entity}(id: int, data: {EntityName}Update): {EntityName}View {\n    var {entity} = {EntityName}Service::find_by_id(id);\n    if ({entity} == null) {\n        throw \"{EntityName} not found\";\n    }\n\n    {update_calls}\n\n    return {EntityName}View {\n        {field_mappings}\n    };\n}\n\n@expose\n@permission(\"admin\")\nfn delete_{entity}(id: int) {\n    var {entity} = {EntityName}Service::find_by_id(id);\n    if ({entity} == null) {\n        throw \"{EntityName} not found\";\n    }\n\n    {EntityName}Service::delete({entity});\n}\n```\n\n**Example for Device**:\n\n```gcl\n// Device REST API endpoints\n\n// Request/Response types\n@volatile type DeviceView {\n    id: int;\n    name: String;\n    location: geo;\n    status: String?;\n    created_at: time;\n}\n\n@volatile type DeviceCreate {\n    name: String;\n    lat: float;\n    lng: float;\n    status: String?;\n}\n\n@volatile type DeviceUpdate {\n    name: String?;\n    status: String?;\n}\n\n// Endpoints\n@expose\n@permission(\"public\")\nfn get_devices(): Array<DeviceView> {\n    var views = Array<DeviceView> {};\n    var devices = DeviceService::list_all();\n\n    for (device in devices) {\n        views.add(DeviceView {\n            id: device->id,\n            name: device->name,\n            location: device->location,\n            status: device->status,\n            created_at: device->created_at\n        });\n    }\n\n    return views;\n}\n\n@expose\n@permission(\"public\")\nfn get_device_by_id(id: int): DeviceView {\n    var device = DeviceService::find_by_id(id);\n    if (device == null) {\n        throw \"Device not found\";\n    }\n\n    return DeviceView {\n        id: device->id,\n        name: device->name,\n        location: device->location,\n        status: device->status,\n        created_at: device->created_at\n    };\n}\n\n@expose\n@permission(\"admin\")\nfn create_device(data: DeviceCreate): DeviceView {\n    var device = DeviceService::create(data.name, data.lat, data.lng, data.status);\n\n    return DeviceView {\n        id: device->id,\n        name: device->name,\n        location: device->location,\n        status: device->status,\n        created_at: device->created_at\n    };\n}\n\n@expose\n@permission(\"admin\")\nfn update_device(id: int, data: DeviceUpdate): DeviceView {\n    var device = DeviceService::find_by_id(id);\n    if (device == null) {\n        throw \"Device not found\";\n    }\n\n    if (data.name != null) {\n        DeviceService::update_name(device, data.name!!);\n    }\n\n    if (data.status != null) {\n        DeviceService::update_status(device, data.status);\n    }\n\n    return DeviceView {\n        id: device->id,\n        name: device->name,\n        location: device->location,\n        status: device->status,\n        created_at: device->created_at\n    };\n}\n\n@expose\n@permission(\"admin\")\nfn delete_device(id: int) {\n    var device = DeviceService::find_by_id(id);\n    if (device == null) {\n        throw \"Device not found\";\n    }\n\n    DeviceService::delete(device);\n}\n```\n\n**Use Write tool** to create the file.\n\n### D. Generate Test File\n\n**File**: `backend/test/{entity_name_snake}_test.gcl`\n\n**Template**:\n\n```gcl\n// {EntityName} tests\n\n@test\nfn test_{entity}_create() {\n    var {entity} = {EntityName}Service::create({test_params});\n\n    Assert::isNotNull({entity});\n    Assert::equals({entity}->field1, expected_value1);\n    Assert::equals({entity}->field2, expected_value2);\n}\n\n@test\nfn test_{entity}_find_by_id() {\n    var {entity} = {EntityName}Service::create({test_params});\n    var found = {EntityName}Service::find_by_id({entity}->id);\n\n    Assert::isNotNull(found);\n    Assert::equals(found->id, {entity}->id);\n}\n\n@test\nfn test_{entity}_find_{unique_field}() {\n    var {entity} = {EntityName}Service::create({test_params});\n    var found = {EntityName}Service::find_by_{field}({entity}->{field});\n\n    Assert::isNotNull(found);\n    Assert::equals(found->{field}, {entity}->{field});\n}\n\n@test\nfn test_{entity}_list_all() {\n    var {entity}1 = {EntityName}Service::create({test_params1});\n    var {entity}2 = {EntityName}Service::create({test_params2});\n\n    var all = {EntityName}Service::list_all();\n\n    Assert::isTrue(all.size() >= 2);\n}\n\n@test\nfn test_{entity}_update() {\n    var {entity} = {EntityName}Service::create({test_params});\n\n    {EntityName}Service::update_field({entity}, new_value);\n\n    Assert::equals({entity}->field, new_value);\n}\n\n@test\nfn test_{entity}_delete() {\n    var {entity} = {EntityName}Service::create({test_params});\n    var id = {entity}->id;\n\n    {EntityName}Service::delete({entity});\n\n    var found = {EntityName}Service::find_by_id(id);\n    Assert::isNull(found);\n}\n\n@test\nfn test_{entity}_duplicate_validation() {\n    var {entity}1 = {EntityName}Service::create({test_params});\n\n    var failed = false;\n    try {\n        var {entity}2 = {EntityName}Service::create({test_params});  // Same unique field\n    } catch (ex) {\n        failed = true;\n    }\n\n    Assert::isTrue(failed);\n}\n```\n\n**Example for Device**:\n\n```gcl\n// Device tests\n\n@test\nfn test_device_create() {\n    var device = DeviceService::create(\"Test Device\", 48.8566, 2.3522, \"active\");\n\n    Assert::isNotNull(device);\n    Assert::equals(device->name, \"Test Device\");\n    Assert::equals(device->status, \"active\");\n}\n\n@test\nfn test_device_find_by_id() {\n    var device = DeviceService::create(\"Test Device\", 48.8566, 2.3522, \"active\");\n    var found = DeviceService::find_by_id(device->id);\n\n    Assert::isNotNull(found);\n    Assert::equals(found->id, device->id);\n}\n\n@test\nfn test_device_find_by_name() {\n    var device = DeviceService::create(\"Unique Device\", 48.8566, 2.3522, \"active\");\n    var found = DeviceService::find_by_name(\"Unique Device\");\n\n    Assert::isNotNull(found);\n    Assert::equals(found->name, \"Unique Device\");\n}\n\n@test\nfn test_device_list_all() {\n    var device1 = DeviceService::create(\"Device 1\", 48.8566, 2.3522, \"active\");\n    var device2 = DeviceService::create(\"Device 2\", 51.5074, -0.1278, \"inactive\");\n\n    var all = DeviceService::list_all();\n\n    Assert::isTrue(all.size() >= 2);\n}\n\n@test\nfn test_device_update_name() {\n    var device = DeviceService::create(\"Old Name\", 48.8566, 2.3522, \"active\");\n\n    DeviceService::update_name(device, \"New Name\");\n\n    Assert::equals(device->name, \"New Name\");\n\n    var found = DeviceService::find_by_name(\"New Name\");\n    Assert::isNotNull(found);\n}\n\n@test\nfn test_device_update_status() {\n    var device = DeviceService::create(\"Test Device\", 48.8566, 2.3522, \"active\");\n\n    DeviceService::update_status(device, \"inactive\");\n\n    Assert::equals(device->status, \"inactive\");\n}\n\n@test\nfn test_device_delete() {\n    var device = DeviceService::create(\"To Delete\", 48.8566, 2.3522, \"active\");\n    var id = device->id;\n\n    DeviceService::delete(device);\n\n    var found = DeviceService::find_by_id(id);\n    Assert::isNull(found);\n}\n\n@test\nfn test_device_duplicate_name() {\n    var device1 = DeviceService::create(\"Duplicate\", 48.8566, 2.3522, \"active\");\n\n    var failed = false;\n    try {\n        var device2 = DeviceService::create(\"Duplicate\", 51.5074, -0.1278, \"active\");\n    } catch (ex) {\n        failed = true;\n    }\n\n    Assert::isTrue(failed);\n}\n```\n\n**Use Write tool** to create the file.\n\n---\n\n## Step 6: Run greycat-lang lint --fix\n\n**Run linter immediately** to catch any errors:\n\n```bash\necho \"================================================================================\"\necho \"RUNNING LINTER\"\necho \"================================================================================\"\necho \"\"\n\ngreycat-lang lint --fix\n\nLINT_EXIT=$?\n\nif [ $LINT_EXIT -eq 0 ]; then\n    echo \"\"\n    echo \"✓ All files passed lint\"\nelse\n    echo \"\"\n    echo \"⚠ Lint found errors - please review and fix\"\nfi\n```\n\n---\n\n## Step 7: Generate Report\n\n**Summarize generated files**:\n\n```\n===============================================================================\nSCAFFOLD COMPLETE\n===============================================================================\n\nGenerated files for entity: Device\n\n✓ backend/src/model/device.gcl (32 lines)\n  - Device type with 5 fields\n  - 2 global indices (by_id, by_name)\n  - ID counter\n\n✓ backend/src/service/device_service.gcl (87 lines)\n  - create, find_by_id, find_by_name, list_all\n  - update_name, update_status\n  - delete with validation\n\n✓ backend/src/api/device_api.gcl (98 lines)\n  - 3 volatile types (DeviceView, DeviceCreate, DeviceUpdate)\n  - 5 API endpoints (@expose):\n    - GET get_devices() [@permission(\"public\")]\n    - GET get_device_by_id(id) [@permission(\"public\")]\n    - POST create_device(data) [@permission(\"admin\")]\n    - PUT update_device(id, data) [@permission(\"admin\")]\n    - DELETE delete_device(id) [@permission(\"admin\")]\n\n✓ backend/test/device_test.gcl (112 lines)\n  - 8 test cases covering CRUD and validation\n\n===============================================================================\n\nLint: ✓ All files passed\n\nNext steps:\n  1. Review generated code and customize as needed\n  2. Run tests: greycat test backend/test/device_test.gcl\n  3. Start server: greycat serve\n  4. Test endpoints:\n     curl http://localhost:8080/create_device -d '{\"name\":\"Test\",\"lat\":48.8,\"lng\":2.3,\"status\":\"active\"}'\n     curl http://localhost:8080/get_devices\n\n===============================================================================\n```\n\n---\n\n## Template Variations\n\n### Time-series Collector Template\n\n**Model differences**:\n```gcl\ntype Sensor {\n    id: String;\n    location: geo;\n    readings: nodeTime<float>;  // Time-series data\n}\n\nvar sensors_by_id: nodeIndex<String, node<Sensor>>;\n```\n\n**Service additions**:\n```gcl\nstatic fn record_reading(sensor: node<Sensor>, value: float, timestamp: time) {\n    sensor->readings.setAt(timestamp, value);\n}\n\nstatic fn get_readings(sensor: node<Sensor>, start: time, end: time): Array<Tuple<time, float>> {\n    var results = Array<Tuple<time, float>> {};\n    for (t: time, val: float in sensor->readings[start..end]) {\n        results.add(Tuple { first: t, second: val });\n    }\n    return results;\n}\n\nstatic fn get_average(sensor: node<Sensor>, start: time, end: time): float {\n    var sum = 0.0;\n    var count = 0;\n    for (t: time, val: float in sensor->readings[start..end]) {\n        sum = sum + val;\n        count = count + 1;\n    }\n    return if (count > 0) { sum / count } else { 0.0 };\n}\n```\n\n### Graph Traversal Template\n\n**Model with relationships**:\n```gcl\ntype City {\n    id: int;\n    name: String;\n    country: node<Country>;\n    streets: nodeList<node<Street>>;\n}\n\ntype Street {\n    id: int;\n    name: String;\n    city: node<City>;\n    buildings: nodeList<node<Building>>;\n}\n```\n\n**Traversal queries**:\n```gcl\nstatic fn get_city_with_streets(city: node<City>): CityWithStreetsView {\n    var street_views = Array<StreetView> {};\n    for (i, street in city->streets) {\n        street_views.add(StreetView {\n            id: street->id,\n            name: street->name\n        });\n    }\n\n    return CityWithStreetsView {\n        id: city->id,\n        name: city->name,\n        streets: street_views\n    };\n}\n```\n\n---\n\n## Success Criteria\n\n✓ **All files generated** with proper structure\n✓ **Follows GreyCat conventions** (naming, initialization, persistence)\n✓ **greycat-lang lint --fix passes** with 0 errors\n✓ **Tests comprehensive** covering CRUD and edge cases\n✓ **API layer proper** (@volatile types, never return nodeList)\n✓ **Service validation** included\n✓ **Indices consistent** maintained across operations\n\n---\n\n## Notes\n\n- **Customization encouraged**: Generated code is a starting point\n- **Existing code style**: Command detects and matches project conventions\n- **Multiple indices**: Maintains consistency across all indices\n- **Validation**: Includes duplicate checks for unique fields\n- **Permissions**: Uses @permission(\"public\") for reads, \"admin\" for writes\n- **Error handling**: Throws exceptions for not found / validation failures\n",
        "plugins/greycat/commands/tutorial.md": "---\nname: tutorial\ndescription: Interactive learning modules for GreyCat concepts - from basics to advanced patterns\nallowed-tools: AskUserQuestion, Read, Write, Bash, Grep, Glob\n---\n\n# GreyCat Interactive Tutorial\n\n**Purpose**: Progressive, hands-on learning of GreyCat concepts with real code examples and validation\n\n**Run When**: Onboarding new developers, learning specific features, refreshing knowledge\n\n---\n\n## Overview\n\n10 sequential modules covering GreyCat from basics to advanced:\n\n1. **Basics** (20 min) - Types, nullability, functions\n2. **Persistence** (25 min) - Nodes, when to persist\n3. **Collections** (30 min) - Indexed collections, when to use each\n4. **Modeling** (25 min) - Data models with relationships\n5. **Services** (20 min) - Business logic patterns\n6. **APIs** (25 min) - @expose endpoints, @volatile types\n7. **Testing** (20 min) - Writing comprehensive tests\n8. **Parallelization** (25 min) - Jobs and async patterns\n9. **Time & Geo** (30 min) - Time-series and spatial data\n10. **Advanced** (30 min) - Inheritance, complex patterns\n\n**Total**: ~4 hours of interactive learning\n\n---\n\n## Progress Tracking\n\nTutorial creates `.greycat-tutorial-progress` file to track completion:\n\n```json\n{\n  \"started\": \"2026-01-09T10:30:00Z\",\n  \"current_module\": 5,\n  \"completed_modules\": [1, 2, 3, 4],\n  \"last_session\": \"2026-01-09T12:00:00Z\"\n}\n```\n\n---\n\n## Step 1: Check Progress & Choose Module\n\n**Read progress file**:\n\n```bash\nPROGRESS_FILE=\".greycat-tutorial-progress\"\n\nif [ -f \"$PROGRESS_FILE\" ]; then\n    echo \"Welcome back to GreyCat Tutorial!\"\n    echo \"\"\n\n    # Parse progress (simple grep/sed approach)\n    CURRENT=$(grep \"current_module\" \"$PROGRESS_FILE\" | sed 's/.*: *\\([0-9]*\\).*/\\1/')\n    COMPLETED=$(grep \"completed_modules\" \"$PROGRESS_FILE\" | sed 's/.*\\[\\(.*\\)\\].*/\\1/' | tr ',' ' ' | wc -w)\n\n    echo \"Progress: $COMPLETED/10 modules complete\"\n    echo \"Current: Module $CURRENT\"\n    echo \"\"\nelse\n    echo \"Welcome to GreyCat Tutorial!\"\n    echo \"\"\n    echo \"This interactive tutorial will teach you GreyCat through hands-on exercises.\"\n    echo \"Estimated time: 4 hours (can pause/resume anytime)\"\n    echo \"\"\n\n    # Create progress file\n    cat > \"$PROGRESS_FILE\" <<EOF\n{\n  \"started\": \"$(date -u +%Y-%m-%dT%H:%M:%SZ)\",\n  \"current_module\": 1,\n  \"completed_modules\": [],\n  \"last_session\": \"$(date -u +%Y-%m-%dT%H:%M:%SZ)\"\n}\nEOF\n    CURRENT=1\nfi\n```\n\n**Ask user**:\n\n```typescript\nAskUserQuestion({\n  questions: [{\n    question: \"How would you like to proceed?\",\n    header: \"Tutorial\",\n    multiSelect: false,\n    options: [\n      {\n        label: `Continue from Module ${CURRENT}`,\n        description: \"Resume where you left off\"\n      },\n      {\n        label: \"Start from beginning\",\n        description: \"Reset progress and start Module 1\"\n      },\n      {\n        label: \"Jump to specific module\",\n        description: \"Choose any module to practice\"\n      },\n      {\n        label: \"Exit tutorial\",\n        description: \"Come back later\"\n      }\n    ]\n  }]\n})\n```\n\n---\n\n## Module 1: Basics (20 min)\n\n### Concept: Types, Nullability, Functions\n\n**Explain**:\n\n```\nGreyCat is a statically-typed language with non-null defaults.\n\nKey concepts:\n- Primitives: int, float, bool, String, char, time\n- Non-null by default, use ? for nullable\n- Functions with return types (no void)\n- For loops (no C-style for)\n```\n\n**Example**:\n\n```gcl\n// Types and nullability\nfn greet(name: String?): String {\n    if (name == null) {\n        return \"Hello, stranger!\";\n    }\n    return \"Hello, ${name}!\";\n}\n\n// Using the function\nfn main() {\n    var msg1 = greet(\"Alice\");      // \"Hello, Alice!\"\n    var msg2 = greet(null);          // \"Hello, stranger!\"\n\n    info(msg1);\n    info(msg2);\n}\n```\n\n**Hands-on Exercise**:\n\n\"Now you try! Write a function that calculates a person's age from birth year.\"\n\n**Generate template** in `tutorial/module1_basics.gcl`:\n\n```gcl\n// Module 1: Basics Exercise\n// TODO: Implement this function\nfn calculate_age(birth_year: int, current_year: int): int {\n    // Your code here\n    return 0;  // Replace this\n}\n\n// Tests (don't modify)\n@test\nfn test_calculate_age() {\n    Assert::equals(calculate_age(1990, 2024), 34);\n    Assert::equals(calculate_age(2000, 2024), 24);\n}\n\nfn main() {\n    info(\"Age for 1990: ${calculate_age(1990, 2024)}\");\n}\n```\n\n**Run & Validate**:\n\n```bash\necho \"Run your code with: greycat run main\"\necho \"Test with: greycat test tutorial/module1_basics.gcl\"\necho \"\"\nread -p \"Press Enter when you've completed the exercise...\"\n\n# Run tests\ngreycat test tutorial/module1_basics.gcl\n\nif [ $? -eq 0 ]; then\n    echo \"✓ Exercise complete! Moving to next module.\"\nelse\n    echo \"⚠ Tests failed. Review and try again.\"\n    echo \"Hint: Age = current_year - birth_year\"\nfi\n```\n\n**Checkpoint**:\n\n```\n✓ Module 1 Complete: Basics\n\nYou learned:\n  - Type system (int, String, bool, time)\n  - Nullability (? for nullable, ?? for coalescing)\n  - Functions and return types\n  - String interpolation\n\nNext: Module 2 - Persistence Fundamentals (25 min)\n```\n\n---\n\n## Module 2: Persistence Fundamentals (25 min)\n\n### Concept: node<T> vs Plain Objects\n\n**Explain**:\n\n```\nGreyCat separates transient (RAM) from persistent (storage) data.\n\nPlain object:\n  type User { name: String; }\n  var u = User { name: \"Alice\" };  // Lives in RAM only\n\nPersistent node:\n  var n = node<User>{ User { name: \"Alice\" } };  // Saved to gcdata/\n\nWhen to use node<T>:\n  ✓ Module-level variables (global data)\n  ✓ Type fields (relationships)\n  ✗ Local variables (temporary data)\n  ✗ Function parameters/returns (unless passing persisted refs)\n```\n\n**Example**:\n\n```gcl\ntype Country { name: String; code: String; }\n\n// Global index (persisted)\nvar countries_by_code: nodeIndex<String, node<Country>>;\n\n// Create and persist\nfn create_country(name: String, code: String): node<Country> {\n    var country = node<Country>{ Country {\n        name: name,\n        code: code\n    }};\n\n    countries_by_code.set(code, country);\n    return country;\n}\n\n// Read from persistence\nfn find_country(code: String): node<Country>? {\n    return countries_by_code.get(code);\n}\n\nfn main() {\n    var lux = create_country(\"Luxembourg\", \"LU\");\n    info(\"Created: ${lux->name}\");\n\n    var found = find_country(\"LU\");\n    if (found != null) {\n        info(\"Found: ${found->name}\");\n    }\n}\n```\n\n**Hands-on Exercise**:\n\n\"Create a simple product catalog with persistence.\"\n\n**Generate** `tutorial/module2_persistence.gcl`:\n\n```gcl\n// Module 2: Persistence Exercise\n\ntype Product {\n    id: int;\n    name: String;\n    price: float;\n}\n\n// TODO: Create a global index for products\n// var products_by_id: ???\n\n// TODO: Implement this function\nfn create_product(id: int, name: String, price: float): node<Product> {\n    // 1. Create a node<Product>\n    // 2. Store it in the index\n    // 3. Return the node\n    return null!!;  // Replace this\n}\n\n// TODO: Implement this function\nfn find_product(id: int): node<Product>? {\n    // Look up product by ID in the index\n    return null;  // Replace this\n}\n\n// Tests\n@test\nfn test_product_persistence() {\n    var p = create_product(1, \"Laptop\", 999.99);\n    Assert::isNotNull(p);\n    Assert::equals(p->name, \"Laptop\");\n\n    var found = find_product(1);\n    Assert::isNotNull(found);\n    Assert::equals(found->price, 999.99);\n}\n\nfn main() {\n    var laptop = create_product(1, \"Laptop\", 999.99);\n    var phone = create_product(2, \"Phone\", 599.99);\n\n    info(\"Created products:\");\n    var found = find_product(1);\n    if (found != null) {\n        info(\"  ${found->name}: $${found->price}\");\n    }\n}\n```\n\n**Validation** includes checking for nodeIndex declaration.\n\n---\n\n## Module 3: Indexed Collections (30 min)\n\n### Concept: nodeList, nodeIndex, nodeTime, nodeGeo\n\n**Explain**:\n\n```\nGreyCat provides specialized persistent collections:\n\nnodeList<node<T>>    - Ordered by integer index (0, 1, 2...)\nnodeIndex<K, V>      - Hash-based key-value lookup\nnodeTime<T>          - Time-series data (keyed by time)\nnodeGeo<node<T>>     - Geo-spatial queries (keyed by geo)\n\nLocal alternatives:\nArray<T>, Map<K,V>   - For temporary data\n```\n\n**Example**:\n\n```gcl\ntype City {\n    name: String;\n    population: int;\n    streets: nodeList<node<Street>>;  // One-to-many\n}\n\ntype Street {\n    name: String;\n}\n\nvar cities_by_name: nodeIndex<String, node<City>>;\n\nfn create_city(name: String, pop: int): node<City> {\n    var city = node<City>{ City {\n        name: name,\n        population: pop,\n        streets: nodeList<node<Street>>{}  // ⚠️ MUST initialize!\n    }};\n    cities_by_name.set(name, city);\n    return city;\n}\n\nfn add_street(city: node<City>, street_name: String) {\n    var street = node<Street>{ Street { name: street_name }};\n    city->streets.add(street);\n}\n\nfn main() {\n    var paris = create_city(\"Paris\", 2_200_000);\n    add_street(paris, \"Champs-Élysées\");\n    add_street(paris, \"Rue de Rivoli\");\n\n    info(\"${paris->name} has ${paris->streets.size()} streets\");\n}\n```\n\n**Hands-on Exercise**:\n\n\"Build a school system with students and courses (many-to-many).\"\n\n**Generate** `tutorial/module3_collections.gcl` with TODOs for implementing:\n- Student and Course types\n- nodeIndex for both\n- Enrollment (adding to both student->courses and course->students)\n\n---\n\n## Module 4: Data Modeling (25 min)\n\n### Concept: Relationships and Indices\n\n**Explain**:\n\n```\nBest practices for data modeling:\n\n1. Store node<T> refs for relationships (not embedded objects)\n2. Create global indices for all primary lookups\n3. Initialize collection attributes in constructors\n4. Keep model files separate from services\n\nFile structure:\n  backend/src/model/user.gcl       - type User + indices\n  backend/src/service/user_service.gcl - business logic\n```\n\n**Example** - City/Country hierarchy with complete model.\n\n**Hands-on Exercise**:\n\n\"Model a blog system: User writes Posts, Posts have Comments.\"\n\n---\n\n## Module 5: Services & Business Logic (20 min)\n\n### Concept: Abstract Type Services Pattern\n\n**Explain**:\n\n```\nServices encapsulate business logic using abstract types with static functions.\n\nPattern:\n  abstract type XxxService {\n      static fn create(...): node<Xxx> { }\n      static fn find(...): node<Xxx>? { }\n      static fn update(...) { }\n      static fn delete(...) { }\n  }\n\nBenefits:\n  - Centralized business logic\n  - Validation in one place\n  - Reusable from APIs\n```\n\n**Example** with full CRUD service.\n\n**Hands-on Exercise**:\n\n\"Implement UserService with validation (email uniqueness, etc).\"\n\n---\n\n## Module 6: API Development (25 min)\n\n### Concept: @expose, @permission, @volatile\n\n**Explain**:\n\n```\nAPI Layer best practices:\n\n1. Use @volatile for request/response types\n2. Never return nodeList/nodeIndex from APIs\n3. Always return Array<XxxView>\n4. Use @expose to make function available via HTTP\n5. Use @permission for access control\n\nPattern:\n  @volatile type UserView { ... }\n\n  @expose\n  @permission(\"public\")\n  fn get_users(): Array<UserView> { ... }\n```\n\n**Example** with complete CRUD API.\n\n**Hands-on Exercise**:\n\n\"Build REST API for blog system from Module 4.\"\n\n---\n\n## Module 7: Testing (20 min)\n\n### Concept: @test Functions\n\n**Explain**:\n\n```\nGreyCat testing:\n\n@test fn test_name() {\n    // Arrange\n    var user = UserService::create(\"test@example.com\");\n\n    // Act\n    var found = UserService::find(\"test@example.com\");\n\n    // Assert\n    Assert::isNotNull(found);\n    Assert::equals(found->email, \"test@example.com\");\n}\n\nAssertions: equals, isTrue, isFalse, isNull, isNotNull\nSetup/teardown: fn setup() { } fn teardown() { }\n```\n\n**Hands-on Exercise**:\n\n\"Write comprehensive tests for UserService.\"\n\n---\n\n## Module 8: Parallelization (25 min)\n\n### Concept: Job Pattern\n\n**Explain**:\n\n```\nParallel processing with Jobs:\n\nvar jobs = Array<Job<Result>> {};\nfor (item in items) {\n    jobs.add(Job<Result> {\n        function: process_fn,\n        arguments: [item]\n    });\n}\nawait(jobs, MergeStrategy::last_wins);\n\nfor (job in jobs) {\n    var result = job.result();\n}\n```\n\n**Hands-on Exercise**:\n\n\"Parallelize data processing for 1000 items.\"\n\n---\n\n## Module 9: Time-Series & Geo (30 min)\n\n### Concept: nodeTime and nodeGeo\n\n**Explain**:\n\n```\nTime-series with nodeTime:\n  var temps: nodeTime<float>;\n  temps.setAt(timestamp, value);\n  for (t: time, v: float in temps[start..end]) { }\n\nGeo-spatial with nodeGeo:\n  var devices: nodeGeo<node<Device>>;\n  devices.set(geo{lat, lng}, device);\n  for (pos: geo, d in devices.filter(GeoBox{...})) { }\n```\n\n**Hands-on Exercise**:\n\n\"Build temperature monitoring with sensor readings over time.\"\n\n---\n\n## Module 10: Advanced Patterns (30 min)\n\n### Concept: Inheritance & Polymorphism\n\n**Explain**:\n\n```\nAbstract types with inheritance:\n\nabstract type Animal {\n    name: String;\n    fn makeSound(): String;  // Abstract\n}\n\ntype Dog extends Animal {\n    fn makeSound(): String { return \"Woof!\"; }\n}\n\ntype Cat extends Animal {\n    fn makeSound(): String { return \"Meow!\"; }\n}\n\n// Store polymorphically\nvar animals: nodeIndex<String, node<Animal>>;\n```\n\n**Hands-on Exercise**:\n\n\"Implement payment system with multiple payment types (Card, Cash, Crypto).\"\n\n---\n\n## Module Navigation\n\n**Between modules**:\n\n```typescript\nAskUserQuestion({\n  questions: [{\n    question: \"Module ${N} complete! What next?\",\n    header: \"Progress\",\n    multiSelect: false,\n    options: [\n      {\n        label: \"Continue to Module ${N+1}\",\n        description: \"Next topic: ${next_topic}\"\n      },\n      {\n        label: \"Review this module\",\n        description: \"Re-read explanation and try exercise again\"\n      },\n      {\n        label: \"Take a break\",\n        description: \"Save progress and exit\"\n      }\n    ]\n  }]\n})\n```\n\n**Update progress file** after each module completion.\n\n---\n\n## Completion Certificate\n\n**After Module 10**:\n\n```\n===============================================================================\n🎓 CONGRATULATIONS! 🎓\n===============================================================================\n\nYou've completed the GreyCat Tutorial!\n\nModules completed (10/10):\n  ✓ Module 1: Basics\n  ✓ Module 2: Persistence Fundamentals\n  ✓ Module 3: Indexed Collections\n  ✓ Module 4: Data Modeling\n  ✓ Module 5: Services & Business Logic\n  ✓ Module 6: API Development\n  ✓ Module 7: Testing\n  ✓ Module 8: Parallelization\n  ✓ Module 9: Time-Series & Geo\n  ✓ Module 10: Advanced Patterns\n\nTime invested: ${hours} hours\nTutorial files: ./tutorial/ directory\n\nNext steps:\n  1. Build a real project using what you learned\n  2. Explore advanced topics: /greycat:scaffold, /greycat:migrate\n  3. Read references: frontend.md, concurrency.md, LIBRARIES.md\n  4. Join the community: https://greycat.io/community\n\nKeep learning! 🚀\n\n===============================================================================\n```\n\n---\n\n## Tutorial Directory Structure\n\n```\ntutorial/\n├── module1_basics.gcl\n├── module2_persistence.gcl\n├── module3_collections.gcl\n├── module4_modeling.gcl\n├── module5_services.gcl\n├── module6_apis.gcl\n├── module7_testing.gcl\n├── module8_parallelization.gcl\n├── module9_timeseries_geo.gcl\n└── module10_advanced.gcl\n```\n\n---\n\n## Success Criteria\n\n✓ **All modules accessible** with clear explanations\n✓ **Hands-on exercises** with validation\n✓ **Progress tracked** and resumable\n✓ **Self-paced** with pause/resume\n✓ **Real code** that runs with greycat run/test\n✓ **Completion certificate** at end\n\n---\n\n## Notes\n\n- **Interactive learning**: Each module combines theory + practice\n- **Build skills progressively**: Each module builds on previous\n- **Real GreyCat code**: All examples are valid, runnable code\n- **Validation**: Tests ensure correct understanding\n- **Flexible pace**: Can pause/resume anytime\n- **Comprehensive**: Covers all major GreyCat concepts\n",
        "plugins/greycat/commands/typecheck.md": "---\nname: typecheck\ndescription: Advanced type safety checks beyond greycat-lang lint\nallowed-tools: Bash, Read, Grep, Glob\n---\n\n# Type Safety Checker\n\n**Purpose**: Catch type safety issues and anti-patterns that greycat-lang lint doesn't detect\n\n**Run After**: Each sprint, before releases, when modifying type definitions\n\n---\n\n## Overview\n\nThis command performs advanced type safety analysis in 5 categories:\n\n1. **Volatile Checking** - @volatile on API response types\n2. **Collection Type Safety** - Persistent vs non-persistent collections\n3. **Null Safety** - Potential null pointer issues\n4. **Type Consistency** - Proper type usage patterns\n5. **Generic Types** - Static functions with generics (not allowed)\n\n---\n\n## Phase 1: Volatile Decorator Checking\n\n### Objective\nEnsure all API response types are marked @volatile.\n\n### Step 1.1: Find @expose Functions\n\n```bash\n# Extract all @expose functions and their return types\ngrep -rn \"@expose\" backend/src/api/ --include=\"*.gcl\" -A 5 | grep \"fn.*:\" | sed 's/.*: //' | sed 's/ {.*//'\n```\n\n### Step 1.2: Check Each Return Type\n\nFor each return type found:\n1. Check if it's a basic type (int, String, bool, float) → OK\n2. Check if it's an Array/Map → Check element type\n3. Check if it's a custom type → Verify has @volatile\n\n### Example Output\n\n```\n📍 backend/src/api/search_api.gcl:63\n\n⚠️ MISSING @volatile: API response type not marked as volatile\n\nFunction:\n  @expose\n  fn semanticSearch(...): PaginatedResult<SearchResultView> {\n\nType Definition (backend/src/api/api_types.gcl:45):\n  type PaginatedResult<T> {  ← Missing @volatile\n      items: Array<T>;\n      total: int;\n  }\n\nIssue:\n  - API response type should be @volatile (not persisted)\n  - Missing decorator causes unnecessary database storage\n\nFix:\n  @volatile\n  type PaginatedResult<T> {\n      items: Array<T>;\n      total: int;\n  }\n\nPriority: MEDIUM\nImpact: Database bloat, performance\n```\n\n---\n\n## Phase 2: Collection Type Safety\n\n### Objective\nDetect misuse of persistent collections (nodeList/nodeIndex/nodeTime/nodeGeo) in temporary contexts.\n\n### Step 2.1: Find All Collection Usages\n\n```bash\n# Find all persistent collection declarations\ngrep -rn \"nodeList\\|nodeIndex\\|nodeTime\\|nodeGeo\" backend/src/ --include=\"*.gcl\" -B 3 -A 3\n```\n\n### Step 2.2: Classify Usage Context\n\nFor each occurrence, determine context:\n\n**✅ LEGITIMATE (Global variable)**:\n```gcl\n// In backend/src/model/data.gcl (module-level)\nvar documents: nodeList<node<Document>>;\n```\n\n**✅ LEGITIMATE (Type field)**:\n```gcl\ntype Document {\n    chunks: nodeIndex<node<Chunk>, bool>;\n}\n```\n\n**⚠️ SUSPICIOUS (Local variable)**:\n```gcl\nfn buildResults() {\n    var results = nodeList<T> {};  ← Should be Array\n}\n```\n\n**⚠️ SUSPICIOUS (Function parameter)**:\n```gcl\nfn process(items: nodeList<T>) {  ← Should be Array\n    // ...\n}\n```\n\n### Example Output\n\n```\n📍 backend/src/service/builder.gcl:34\n\n⚠️ TYPE SAFETY: Local variable using persistent collection\n\nCode:\n  32: fn buildResults(items: Array<Item>): Array<Result> {\n  33:     var results = nodeList<node<Result>> {};\n  34:     for (i, item in items) {\n  35:         results.add(node<Result>{ ... });\n  36:     }\n  37:     return results;  ← Type error!\n  38: }\n\nProblem:\n  - Using nodeList for temporary local variable\n  - Function returns Array but creates nodeList\n  - Unnecessary persistence overhead\n\nFix:\n  var results = Array<node<Result>> {};\n\nPriority: HIGH\nImpact: Type mismatch, performance overhead\n```\n\n---\n\n## Phase 3: Null Safety Analysis\n\n### Objective\nFind potential null pointer dereferences and missing null checks.\n\n### Step 3.1: Find Chained Dereferences\n\n```bash\n# Find .resolve() chains without null checks\ngrep -rn \"\\.resolve()\\..*\\.\" backend/src/ --include=\"*.gcl\"\n\n# Find arrow operator chains\ngrep -rn \"->.*->\" backend/src/ --include=\"*.gcl\"\n```\n\n### Step 3.2: Analyze Safety\n\nCheck if null checking exists before dereference:\n- Look for `if (x == null)` before usage\n- Look for optional chaining `?.`\n- Look for null assertion `!!`\n\n### Example Output\n\n```\n📍 backend/src/service/document_service.gcl:67\n\n⚠️ NULL SAFETY: Potential null pointer dereference\n\nCode:\n  65: fn getDocumentAuthor(docId: String): String {\n  66:     var doc_node = documents_by_id.get(docId);\n  67:     return doc_node.resolve().author.resolve().name;\n  68: }\n\nProblem:\n  - No null check on doc_node (docId might not exist)\n  - No null check on author (document might not have author)\n  - Will throw runtime error if any step fails\n\nFix Option 1 (Null checks):\n  fn getDocumentAuthor(docId: String): String {\n      var doc_node = documents_by_id.get(docId);\n      if (doc_node == null) {\n          throw \"Document not found: ${docId}\";\n      }\n      var doc = doc_node.resolve();\n      if (doc.author == null) {\n          throw \"Document has no author\";\n      }\n      return doc.author.resolve().name;\n  }\n\nFix Option 2 (Optional chaining):\n  fn getDocumentAuthor(docId: String): String? {\n      return documents_by_id.get(docId)?.resolve().author?.resolve().name;\n  }\n\nPriority: MEDIUM\nImpact: Runtime errors\n```\n\n---\n\n## Phase 4: Type Consistency\n\n### Objective\nEnsure proper type usage patterns across codebase.\n\n### Step 4.1: Check Collection Initialization\n\nFind non-nullable collections that might not be initialized:\n\n```bash\n# Find type fields with collections\ngrep -rn \"^\\s*[a-z_]*:\\s*\\(Array\\|Map\\|nodeList\\|nodeIndex\\)\" backend/src/model/ --include=\"*.gcl\"\n```\n\nCheck if they're nullable or always initialized.\n\n### Example Output\n\n```\n📍 backend/src/model/document.gcl:12\n\n⚠️ TYPE SAFETY: Non-nullable collection not always initialized\n\nType:\n  type Document {\n      id: String;\n      chunks: nodeList<node<Chunk>>;  ← Non-nullable\n  }\n\nUsage (backend/src/service/import.gcl:45):\n  var doc = Document {\n      id: \"123\"\n      // chunks not initialized ← RUNTIME ERROR\n  };\n\nProblem:\n  - Non-nullable collection must be initialized on creation\n  - Missing initialization causes runtime error\n\nFix Option 1 (Make nullable):\n  chunks: nodeList<node<Chunk>>?;\n\nFix Option 2 (Always initialize):\n  var doc = Document {\n      id: \"123\",\n      chunks: nodeList<node<Chunk>> {}\n  };\n\nPriority: HIGH\nImpact: Runtime errors\n```\n\n### Step 4.2: Check Node Storage\n\nFind collections storing objects instead of node references:\n\n```bash\n# Find nodeList/nodeIndex not storing nodes\ngrep -rn \"nodeList<[^n]\" backend/src/ --include=\"*.gcl\"\ngrep -rn \"nodeIndex<.*,\\s*[^n]\" backend/src/ --include=\"*.gcl\"\n```\n\n### Example Output\n\n```\n📍 backend/src/model/data.gcl:8\n\n⚠️ TYPE SAFETY: Storing objects directly in persistent collection\n\nCode:\n  var items: nodeList<Item>;  ← Should be nodeList<node<Item>>\n\nProblem:\n  - nodeList should store node references, not objects\n  - Direct object storage breaks persistence model\n\nFix:\n  var items: nodeList<node<Item>>;\n\n  // When adding:\n  items.add(node<Item>{ Item { ... } });\n\nPriority: HIGH\nImpact: Persistence errors\n```\n\n---\n\n## Phase 5: Generic Type Safety\n\n### Objective\nDetect generic type parameters on static functions (not allowed in GreyCat).\n\n### Step 5.1: Find Static Functions with Generics\n\n```bash\n# Find static fn with <T> syntax\ngrep -rn \"static fn.*<.*>.*(\" backend/src/ --include=\"*.gcl\"\n```\n\n### Example Output\n\n```\n📍 backend/src/service/utils.gcl:12\n\n❌ TYPE ERROR: Generic type parameter on static function\n\nCode:\n  abstract type Utils {\n      static fn process<T>(value: T): T {  ← NOT ALLOWED\n          return value;\n      }\n  }\n\nProblem:\n  - Generic type parameters (<T>) not allowed on static functions in GreyCat\n  - Will cause compilation error\n\nFix Option 1 (Remove static):\n  type Utils<T> {\n      fn process(value: T): T {\n          return value;\n      }\n  }\n  // Usage: Utils<int>{}.process(42)\n\nFix Option 2 (Make concrete):\n  abstract type Utils {\n      static fn processInt(value: int): int { ... }\n      static fn processString(value: String): String { ... }\n  }\n\nPriority: CRITICAL\nImpact: Compilation error\n```\n\n---\n\n## Output Format\n\n### Executive Summary\n\n```\n===============================================================================\nGREYCAT TYPE SAFETY ANALYSIS\n===============================================================================\n\nFiles Analyzed: 45 .gcl files\nAnalysis Date: 2026-01-08\n\nISSUES FOUND:\n\nCRITICAL (Compilation Errors):\n  [ ] 2 static functions with generic type parameters\n\nHIGH (Runtime Errors):\n  [ ] 5 uninitialized non-nullable collections\n  [ ] 4 incorrect node storage in persistent collections\n  [ ] 3 local variables using persistent types\n\nMEDIUM (Safety Issues):\n  [ ] 8 missing @volatile on API response types\n  [ ] 6 potential null pointer dereferences\n  [ ] 4 unsafe type casts\n\nLOW (Best Practices):\n  [ ] 12 missing type annotations (could be inferred)\n  [ ] 7 overly broad types (Object vs specific)\n\nTOTAL ISSUES: 51\n\nESTIMATED FIX TIME:\n  Critical: 30 minutes  (fix immediately)\n  High:     2 hours     (fix this sprint)\n  Medium:   3 hours     (fix next sprint)\n  Low:      1 hour      (nice to have)\n\n===============================================================================\n```\n\n### Detailed Report\n\n```\n===============================================================================\nCRITICAL ISSUES (Fix Immediately)\n===============================================================================\n\n1. COMPILATION ERROR: Generic static function\n   📍 backend/src/service/utils.gcl:12\n   [Details above]\n\n2. COMPILATION ERROR: Generic static function\n   📍 backend/src/service/helper.gcl:34\n   [Details above]\n\n===============================================================================\nHIGH PRIORITY ISSUES\n===============================================================================\n\n1. RUNTIME ERROR: Uninitialized collection\n   📍 backend/src/model/document.gcl:12\n   [Details above]\n\n...\n\n===============================================================================\n```\n\n---\n\n## Verification Commands\n\nAfter fixes, verify:\n\n```bash\n# 1. Run greycat-lang lint (should pass)\ngreycat-lang lint\n\n# 2. Run this type check again\n/typecheck\n\n# 3. Run tests\ngreycat test\n\n# 4. Build project\ngreycat build\n```\n\n---\n\n## Success Criteria\n\n✓ **All files scanned** (.gcl files in backend/src/)\n✓ **Volatile decorators checked** on API response types\n✓ **Collection usage validated** (persistent vs non-persistent)\n✓ **Null safety analyzed** (potential NPEs)\n✓ **Type consistency verified** (initialization, node storage)\n✓ **Generic types checked** (no generics on static functions)\n✓ **Report generated** with prioritized issues\n✓ **Zero CRITICAL issues** before release\n\n---\n\n## Notes\n\n- **Complements greycat-lang lint**: Catches issues the linter misses\n- **Run regularly**: After type changes, before releases\n- **Fix CRITICAL first**: These will cause compilation errors\n- **Test after fixes**: Always run tests after type changes\n- **Non-blocking**: MEDIUM/LOW can be addressed incrementally\n\n---\n\n## Example Workflow\n\n```bash\n# 1. Run type safety check\n/typecheck\n\n# 2. Review report (51 issues found)\n# - 2 CRITICAL\n# - 12 HIGH\n# - 18 MEDIUM\n# - 19 LOW\n\n# 3. Fix CRITICAL immediately\n# - Remove generic params from static functions\n\n# 4. Verify fix\ngreycat-lang lint  # ✓ Passes\ngreycat build      # ✓ Builds successfully\n\n# 5. Fix HIGH priority issues\n# - Initialize collections\n# - Fix node storage\n# - Replace nodeList with Array in local vars\n\n# 6. Re-run check\n/typecheck\n# → CRITICAL: 0\n# → HIGH: 0\n# → MEDIUM: 18 (to address next sprint)\n\n# 7. Commit\ngit add backend/\ngit commit -m \"fix: resolve type safety issues (CRITICAL+HIGH)\"\n```\n\n---\n\n## Integration with Other Commands\n\n**Use Before**:\n- `/apicheck` - Fix type issues before API review\n- `/backend` - Type check before general cleanup\n- `/docs` - Ensure types are correct before documentation\n\n**Use After**:\n- Major refactoring\n- Type definition changes\n- Library updates (might introduce type issues)\n\n---\n\n## Common Patterns\n\n### Pattern 1: API Response Type Missing @volatile\n\n**Detection**: Function returns custom type, type lacks @volatile\n**Fix**: Add @volatile decorator to type\n**Frequency**: Common in new API endpoints\n\n### Pattern 2: Local Variable Using nodeList\n\n**Detection**: Variable declared inside function with nodeList type\n**Fix**: Change to Array\n**Frequency**: Very common antipattern\n\n### Pattern 3: Uninitialized Collection\n\n**Detection**: Non-nullable collection field, object created without initializing it\n**Fix**: Either make nullable or always initialize\n**Frequency**: Common in model types\n\n### Pattern 4: Null Dereference Chain\n\n**Detection**: Multiple .resolve() or -> without null checks\n**Fix**: Add null checks or use optional chaining\n**Frequency**: Common in service layer\n\n### Pattern 5: Generic Static Function\n\n**Detection**: static fn with <T> syntax\n**Fix**: Remove static or remove generics\n**Frequency**: Rare, but critical error\n",
        "plugins/greycat/commands/upgrade.md": "---\nname: upgrade\ndescription: Update GreyCat libraries to latest available versions\nallowed-tools: Bash(*), Read, Edit, Grep\n---\n\n# Update GreyCat Libraries\n\n**Purpose**: Upgrade all GreyCat libraries to the latest versions from get.greycat.io\n\n**Run After**: Monthly maintenance, before major releases, when breaking changes are announced\n\n---\n\n## Overview\n\nThis command upgrades GreyCat libraries by:\n\n1. Fetching latest versions from get.greycat.io\n2. Detecting which libraries are currently used in `project.gcl`\n3. Comparing current vs latest versions (exit early if up-to-date)\n4. **Asking user confirmation before applying changes**\n5. Updating `project.gcl` with latest versions\n6. Installing updated libraries\n7. Verifying with `greycat-lang lint`\n\n---\n\n## Available GreyCat Libraries\n\n### Core Libraries\n- **std** - Standard library (required, core functionality)\n- **explorer** - Graph explorer UI (recommended for development)\n\n### Pro Libraries (Shared Version)\nAll pro libraries share the same version:\n- **ai** - LLM/embedding support (llama.cpp integration)\n- **algebra** - ML pipelines, neural networks, patterns, transforms\n- **kafka** - Apache Kafka integration\n- **sql** - SQL database integration (PostgreSQL)\n- **s3** - Amazon S3 integration\n- **finance** - Financial data structures and operations\n- **powerflow** - Power flow analysis\n- **opcua** - OPC UA industrial protocol\n- **useragent** - User agent parsing and detection\n\n---\n\n## Step 1: Fetch Latest Versions\n\nFetch the latest library versions from get.greycat.io:\n\n```bash\necho \"================================================================================\"\necho \"FETCHING LATEST LIBRARY VERSIONS\"\necho \"================================================================================\"\necho \"\"\n\n# Fetch core libraries\necho \"Core libraries:\"\nSTD_VERSION=$(curl -s \"https://get.greycat.io/files/core/dev/latest\" | cut -d'/' -f2)\nEXPLORER_VERSION=$(curl -s \"https://get.greycat.io/files/explorer/dev/latest\" | cut -d'/' -f2)\n\n# If std version is empty, use explorer version as fallback\nif [ -z \"$STD_VERSION\" ]; then\n  STD_VERSION=\"$EXPLORER_VERSION\"\n  echo \"  Note: std version endpoint returned empty, using explorer version as fallback\"\nfi\n\necho \"  std:      $STD_VERSION\"\necho \"  explorer: $EXPLORER_VERSION\"\necho \"\"\n\n# Fetch pro libraries version (all share same version)\necho \"Pro libraries (all share same version):\"\nPRO_VERSION=$(curl -s \"https://get.greycat.io/files/algebra/dev/latest\" | cut -d'/' -f2)\necho \"  Version: $PRO_VERSION\"\necho \"  Libraries: ai, algebra, kafka, sql, s3, finance, powerflow, opcua, useragent\"\necho \"\"\n```\n\n**Note**: All pro libraries (ai, algebra, kafka, sql, s3, finance, powerflow, opcua, useragent) share the same version. We only need to fetch one endpoint.\n\n---\n\n## Step 2: Detect Current Libraries\n\nFind which libraries are currently declared in `project.gcl`:\n\n```bash\necho \"================================================================================\"\necho \"CURRENT LIBRARIES IN project.gcl\"\necho \"================================================================================\"\necho \"\"\n\n# Detect which libraries are currently in project.gcl\nCURRENT_LIBS=$(grep -o '@library(\"[^\"]*\"' project.gcl | sed 's/@library(\"//;s/\"//' | sort -u)\n\nif [ -z \"$CURRENT_LIBS\" ]; then\n    echo \"ERROR: No @library declarations found in project.gcl\"\n    exit 1\nfi\n\necho \"Detected libraries:\"\necho \"$CURRENT_LIBS\" | sed 's/^/  - /'\necho \"\"\n```\n\n---\n\n## Step 3: Compare Versions\n\nCompare current versions with latest versions:\n\n```bash\necho \"================================================================================\"\necho \"VERSION COMPARISON\"\necho \"================================================================================\"\necho \"\"\n\nUPDATES_NEEDED=false\n\n# Check each library that's currently in project.gcl\nfor lib in $CURRENT_LIBS; do\n    CURRENT_VERSION=$(grep \"@library(\\\"$lib\\\"\" project.gcl | sed -n \"s/.*@library(\\\"$lib\\\", \\\"\\([^\\\"]*\\)\\\").*/\\1/p\")\n\n    # Determine the latest version for this library\n    case \"$lib\" in\n        std)\n            LATEST_VERSION=\"$STD_VERSION\"\n            ;;\n        explorer)\n            LATEST_VERSION=\"$EXPLORER_VERSION\"\n            ;;\n        ai|algebra|kafka|sql|s3|finance|powerflow|opcua|useragent)\n            LATEST_VERSION=\"$PRO_VERSION\"\n            ;;\n        *)\n            echo \"  WARNING: Unknown library '$lib' - skipping\"\n            continue\n            ;;\n    esac\n\n    # Compare versions\n    if [ \"$CURRENT_VERSION\" = \"$LATEST_VERSION\" ]; then\n        echo \"  ✓ $lib: $CURRENT_VERSION (up-to-date)\"\n    else\n        echo \"  ↑ $lib: $CURRENT_VERSION → $LATEST_VERSION (update available)\"\n        UPDATES_NEEDED=true\n    fi\ndone\n\necho \"\"\n\n# Exit early if no updates needed\nif [ \"$UPDATES_NEEDED\" = false ]; then\n    echo \"================================================================================\"\n    echo \"✓ All libraries are already up-to-date. No changes needed.\"\n    echo \"================================================================================\"\n    exit 0\nfi\n\n```\n\n**Early Exit**: If all versions match, command exits here. Otherwise, continues with confirmation.\n\n---\n\n## Step 4: User Confirmation\n\nBefore applying any changes, confirm with the user:\n\n**Present Update Summary**:\n```\n===============================================================================\nUPDATE SUMMARY\n===============================================================================\n\nThe following library updates are available:\n\n[List each library with current → latest version]\n  std:      7.5.125-dev → 7.6.10-dev\n  explorer: 7.5.3-dev   → 7.6.10-dev\n  ai:       7.5.51-dev  → 7.6.10-dev\n\nActions that will be performed:\n  1. Update project.gcl with new versions\n  2. Run 'greycat install' to download libraries\n  3. Run 'greycat-lang lint' to verify compatibility\n\nProceed with update?\n```\n\n**Use AskUserQuestion tool**:\n```typescript\nAskUserQuestion({\n  questions: [{\n    question: \"Proceed with library update?\",\n    header: \"Confirm Update\",\n    multiSelect: false,\n    options: [\n      {\n        label: \"Yes, update all libraries (Recommended)\",\n        description: \"Update project.gcl, install libraries, and verify with lint\"\n      },\n      {\n        label: \"Show me what will change in project.gcl first\",\n        description: \"Display the exact changes before applying them\"\n      },\n      {\n        label: \"Cancel - don't update\",\n        description: \"Keep current library versions\"\n      }\n    ]\n  }]\n})\n```\n\n**Handle User Response**:\n\n**If \"Cancel\"**:\n```bash\necho \"================================================================================\"\necho \"UPDATE CANCELLED\"\necho \"================================================================================\"\necho \"\"\necho \"Library versions remain unchanged:\"\nfor lib in $CURRENT_LIBS; do\n    CURRENT_VERSION=$(grep \"@library(\\\"$lib\\\"\" project.gcl | sed -n \"s/.*@library(\\\"$lib\\\", \\\"\\([^\\\"]*\\)\\\").*/\\1/p\")\n    echo \"  $lib: $CURRENT_VERSION\"\ndone\nexit 0\n```\n\n**If \"Show me what will change\"**:\n```bash\necho \"================================================================================\"\necho \"CHANGES TO project.gcl\"\necho \"================================================================================\"\necho \"\"\necho \"Current library declarations:\"\ngrep '@library(' project.gcl\necho \"\"\necho \"After update:\"\nfor lib in $CURRENT_LIBS; do\n    case \"$lib\" in\n        std) NEW_VERSION=\"$STD_VERSION\" ;;\n        explorer) NEW_VERSION=\"$EXPLORER_VERSION\" ;;\n        ai|algebra|kafka|sql|s3|finance|powerflow|opcua|useragent) NEW_VERSION=\"$PRO_VERSION\" ;;\n    esac\n    echo \"@library(\\\"$lib\\\", \\\"$NEW_VERSION\\\");\"\ndone\necho \"\"\n# Ask again: \"Proceed with these changes?\"\n```\n\n**If \"Yes, update all libraries\"**:\n```bash\necho \"================================================================================\"\necho \"PROCEEDING WITH UPDATE\"\necho \"================================================================================\"\necho \"\"\n# Continue to Step 5\n```\n\n---\n\n## Step 5: Update project.gcl\n\nUpdate `project.gcl` with latest versions:\n\n```bash\necho \"Updating project.gcl...\"\necho \"\"\n\n# Update each library that's currently in project.gcl\nfor lib in $CURRENT_LIBS; do\n    # Determine the latest version for this library\n    case \"$lib\" in\n        std)\n            LATEST_VERSION=\"$STD_VERSION\"\n            ;;\n        explorer)\n            LATEST_VERSION=\"$EXPLORER_VERSION\"\n            ;;\n        ai|algebra|kafka|sql|s3|finance|powerflow|opcua|useragent)\n            LATEST_VERSION=\"$PRO_VERSION\"\n            ;;\n        *)\n            echo \"  Skipping unknown library: $lib\"\n            continue\n            ;;\n    esac\n\n    # Update the version in project.gcl using Edit tool\n    # Find old version and replace with new version\n    OLD_LINE=$(grep \"@library(\\\"$lib\\\"\" project.gcl)\n    NEW_LINE=\"@library(\\\"$lib\\\", \\\"$LATEST_VERSION\\\");\"\n\n    # Use sed for in-place replacement\n    sed -i \"s/@library(\\\"$lib\\\", \\\"[^\\\"]*\\\")/@library(\\\"$lib\\\", \\\"$LATEST_VERSION\\\")/\" project.gcl\n\n    echo \"  ✓ Updated $lib to $LATEST_VERSION\"\ndone\n\necho \"\"\necho \"Updated library declarations in project.gcl:\"\ngrep '@library(' project.gcl\necho \"\"\n```\n\n---\n\n## Step 6: Install Libraries\n\nDownload the updated libraries to `./lib` directory:\n\n```bash\necho \"================================================================================\"\necho \"INSTALLING LIBRARIES\"\necho \"================================================================================\"\necho \"\"\n\ngreycat install\n\necho \"\"\necho \"✓ Libraries installed to ./lib/\"\necho \"\"\n```\n\n---\n\n## Step 7: Verify with Lint\n\nEnsure updated libraries don't introduce compatibility issues:\n\n```bash\necho \"================================================================================\"\necho \"VERIFYING WITH LINT\"\necho \"================================================================================\"\necho \"\"\n\ngreycat-lang lint\n\nLINT_EXIT_CODE=$?\n\necho \"\"\nif [ $LINT_EXIT_CODE -eq 0 ]; then\n    echo \"================================================================================\"\n    echo \"✓ UPDATE COMPLETE - All libraries updated and verified successfully!\"\n    echo \"================================================================================\"\nelse\n    echo \"================================================================================\"\n    echo \"⚠ UPDATE COMPLETE - But lint found errors that need to be fixed\"\n    echo \"================================================================================\"\n    echo \"\"\n    echo \"The libraries were updated, but there are compatibility issues.\"\n    echo \"Please review the lint errors above and fix them.\"\nfi\n\necho \"\"\necho \"Updated libraries:\"\nfor lib in $CURRENT_LIBS; do\n    NEW_VERSION=$(grep \"@library(\\\"$lib\\\"\" project.gcl | sed -n \"s/.*@library(\\\"$lib\\\", \\\"\\([^\\\"]*\\)\\\").*/\\1/p\")\n    echo \"  $lib: $NEW_VERSION\"\ndone\necho \"\"\n```\n\n---\n\n## Adding New Libraries\n\nTo add a new library to your project:\n\n### Step 1: Add Library Declaration\n\nEdit `project.gcl` and add the library declaration:\n\n```gcl\n// Example: Adding Kafka support\n@library(\"kafka\", \"7.6.10-dev\");\n```\n\n### Step 2: Run Update Command\n\nRun this command - it will automatically detect and update the new library:\n\n```bash\n/upgrade\n```\n\n### Step 3: Verify Installation\n\n```bash\n# Check library is installed\nls -la lib/ | grep kafka\n\n# Verify lint passes\ngreycat-lang lint\n\n# Test the library\ngreycat serve\n```\n\n---\n\n## Library Version Channels\n\nGreyCat libraries are available in two channels:\n\n### Dev Channel (Recommended for Development)\n- **URL**: `https://get.greycat.io/files/{library}/dev/latest`\n- **Stability**: Latest features, may have breaking changes\n- **Use When**: Active development, testing new features\n- **Example**: `@library(\"std\", \"7.6.10-dev\")`\n\n### Release Channel (Production)\n- **URL**: `https://get.greycat.io/files/{library}/releases/latest`\n- **Stability**: Stable, production-ready\n- **Use When**: Production deployments\n- **Example**: `@library(\"std\", \"7.5.0\")`\n\n**This Command Uses**: Dev channel (`/dev/latest`) by default\n\nTo switch to release channel, manually edit the curl commands to use `/releases/latest` instead of `/dev/latest`.\n\n---\n\n## Troubleshooting\n\n### Lint Fails After Update\n\n**Symptom**: `greycat-lang lint` reports errors after update\n\n**Causes**:\n1. Breaking API changes in new library version\n2. Deprecated function usage\n3. Type signature changes\n\n**Solutions**:\n\n**A. Check Changelog**:\n```bash\n# Visit GreyCat documentation for breaking changes\n# https://doc.greycat.io/changelog\n```\n\n**B. Review Lint Errors**:\n```bash\ngreycat-lang lint 2>&1 | tee lint-errors.txt\n\n# Look for patterns:\n# - \"function not found\" → API renamed/removed\n# - \"type mismatch\" → Signature changed\n# - \"unknown decorator\" → Decorator deprecated\n```\n\n**C. Rollback if Needed**:\n```bash\n# Revert project.gcl to previous version\ngit checkout project.gcl\n\n# Reinstall old versions\ngreycat install\n\n# Verify\ngreycat-lang lint\n```\n\n**D. Update Code Incrementally**:\n```bash\n# Update one library at a time\n# Edit project.gcl manually to update only \"std\"\ngreycat install\ngreycat-lang lint\n\n# If passes, update next library\n# Edit project.gcl to update \"explorer\"\ngreycat install\ngreycat-lang lint\n\n# Continue until all updated or issue found\n```\n\n### Library Fetch Fails\n\n**Symptom**: Curl returns empty or error\n\n**Solutions**:\n\n```bash\n# 1. Check internet connection\nping get.greycat.io\n\n# 2. Test URL manually\ncurl -v \"https://get.greycat.io/files/std/dev/latest\"\n\n# 3. Check if library exists\n# Visit https://get.greycat.io in browser\n\n# 4. Try alternative library for version\n# If std fails, use explorer version as fallback (they match)\n```\n\n### Unknown Library Warning\n\n**Symptom**: `WARNING: Unknown library 'xyz' - skipping`\n\n**Cause**: Custom or third-party library not in standard list\n\n**Solution**: Safe to ignore. Custom libraries won't auto-update. Update manually:\n\n```gcl\n// In project.gcl\n@library(\"custom-lib\", \"1.2.3\");  // Update version manually\n```\n\n### Installation Fails\n\n**Symptom**: `greycat install` fails\n\n**Solutions**:\n\n```bash\n# 1. Clear lib directory\nrm -rf lib/\ngreycat install\n\n# 2. Check disk space\ndf -h .\n\n# 3. Check permissions\nls -la lib/\n\n# 4. Try manual download\ncurl \"https://get.greycat.io/files/std/dev/7.6.10-dev/std-7.6.10-dev.jar\" -o lib/std.jar\n```\n\n---\n\n## Migration Guide\n\n### Handling Breaking Changes\n\nWhen lint fails after update, follow this process:\n\n**1. Identify Breaking Changes**\n\nCommon breaking change patterns:\n\n```gcl\n// BEFORE (old version)\nfn process(data: String): Result {\n    return DataService::parse(data);\n}\n\n// AFTER (new version - API renamed)\nfn process(data: String): Result {\n    return DataService::parseString(data);  // Function renamed\n}\n```\n\n**2. Search for Deprecated Usage**\n\n```bash\n# Find all usages of deprecated function\ngrep -r \"DataService::parse\" backend/ --include=\"*.gcl\"\n\n# Replace across all files\nfind backend/ -name \"*.gcl\" -exec sed -i 's/DataService::parse/DataService::parseString/g' {} \\;\n\n# Verify\ngreycat-lang lint\n```\n\n**3. Test Thoroughly**\n\n```bash\n# Run all tests\ngreycat test\n\n# Manual testing\ngreycat serve\n# Test critical paths in UI\n```\n\n**4. Commit Changes**\n\n```bash\ngit add project.gcl lib/\ngit commit -m \"Update GreyCat libraries to latest versions\n\n- std: 7.5.0-dev → 7.6.10-dev\n- explorer: 7.5.0-dev → 7.6.10-dev\n- ai: 7.5.0-dev → 7.6.10-dev\n\nBreaking changes:\n- Updated DataService::parse → DataService::parseString\n- Updated deprecated @tag decorator usage\"\n\ngit push\n```\n\n---\n\n## Version Pinning (Advanced)\n\nFor production stability, pin specific versions instead of using `latest`:\n\n### Option 1: Manual Version Pinning\n\n```gcl\n// project.gcl\n@library(\"std\", \"7.6.10-dev\");        // Pinned to specific version\n@library(\"explorer\", \"7.6.10-dev\");   // Pinned\n@library(\"ai\", \"7.6.10-dev\");         // Pinned\n```\n\nUpdate manually when ready to upgrade.\n\n### Option 2: Version Lock File\n\nCreate `.greycat-lock` to track versions:\n\n```json\n{\n  \"locked\": \"2024-01-15T10:30:00Z\",\n  \"libraries\": {\n    \"std\": \"7.6.10-dev\",\n    \"explorer\": \"7.6.10-dev\",\n    \"ai\": \"7.6.10-dev\"\n  }\n}\n```\n\nUpdate command can respect lock file (future enhancement).\n\n---\n\n## Success Criteria\n\n✓ **Latest versions fetched** from get.greycat.io\n✓ **Current libraries detected** in project.gcl\n✓ **Version comparison complete** (or early exit if up-to-date)\n✓ **project.gcl updated** with latest versions\n✓ **Libraries installed** via `greycat install`\n✓ **Lint passes** with 0 errors (or warnings documented)\n\n---\n\n## Verification Commands\n\n```bash\n# 1. Check updated versions in project.gcl\ngrep '@library(' project.gcl\n\n# 2. Verify libraries installed\nls -la lib/\n\n# 3. Check for lint issues\ngreycat-lang lint\n\n# 4. Run tests\ngreycat test\n\n# 5. Start server to verify runtime\ngreycat serve\n```\n\n---\n\n## Best Practices\n\n### When to Update\n\n**✅ Good Times**:\n- Start of development sprint\n- After completing major features\n- Monthly maintenance window\n- Before planning new features (get latest capabilities)\n\n**❌ Bad Times**:\n- During active feature development\n- Right before production deployment\n- When team is unavailable to fix issues\n- During critical bug fixes\n\n### Update Strategy\n\n**Option A: Update All (Aggressive)**\n```bash\n# Update all libraries at once\n/upgrade\n\n# Fix all compatibility issues\ngreycat-lang lint\n# ... fix errors\n\ngreycat test\n# ... fix failing tests\n```\n\n**Option B: Update Incrementally (Safe)**\n```bash\n# 1. Update only std library\n# Edit project.gcl manually\ngreycat install && greycat-lang lint\n\n# 2. Update explorer\n# Edit project.gcl manually\ngreycat install && greycat-lang lint\n\n# 3. Update pro libraries\n# Edit project.gcl manually\ngreycat install && greycat-lang lint\n```\n\n**Option C: Test Before Update (Safest)**\n```bash\n# 1. Create test branch\ngit checkout -b update-greycat-libs\n\n# 2. Run update\n/upgrade\n\n# 3. Fix issues\ngreycat-lang lint\ngreycat test\n\n# 4. Manual testing\ngreycat serve\n\n# 5. Merge if successful\ngit checkout main\ngit merge update-greycat-libs\n```\n\n### Monitoring for Updates\n\nCheck for new versions monthly:\n\n```bash\n# Quick version check (without updating)\nSTD_LATEST=$(curl -s \"https://get.greycat.io/files/core/dev/latest\" | cut -d'/' -f2)\nSTD_CURRENT=$(grep '@library(\"std\"' project.gcl | sed -n 's/.*\"\\([^\"]*\\)\".*/\\1/p')\n\necho \"Current std: $STD_CURRENT\"\necho \"Latest std:  $STD_LATEST\"\n\nif [ \"$STD_CURRENT\" != \"$STD_LATEST\" ]; then\n    echo \"⚠ Update available!\"\nelse\n    echo \"✓ Up to date\"\nfi\n```\n\n---\n\n## Example Output\n\n```\n================================================================================\nFETCHING LATEST LIBRARY VERSIONS\n================================================================================\n\nCore libraries:\n  std:      7.6.10-dev\n  explorer: 7.6.10-dev\n\nPro libraries (all share same version):\n  Version: 7.6.10-dev\n  Libraries: ai, algebra, kafka, sql, s3, finance, powerflow, opcua, useragent\n\n================================================================================\nCURRENT LIBRARIES IN project.gcl\n================================================================================\n\nDetected libraries:\n  - ai\n  - explorer\n  - std\n\n================================================================================\nVERSION COMPARISON\n================================================================================\n\n  ↑ std: 7.5.125-dev → 7.6.10-dev (update available)\n  ↑ explorer: 7.5.3-dev → 7.6.10-dev (update available)\n  ↑ ai: 7.5.51-dev → 7.6.10-dev (update available)\n\n================================================================================\nUPDATE SUMMARY\n================================================================================\n\nThe following library updates are available:\n\n  std:      7.5.125-dev → 7.6.10-dev\n  explorer: 7.5.3-dev   → 7.6.10-dev\n  ai:       7.5.51-dev  → 7.6.10-dev\n\nActions that will be performed:\n  1. Update project.gcl with new versions\n  2. Run 'greycat install' to download libraries\n  3. Run 'greycat-lang lint' to verify compatibility\n\n[User is prompted to confirm via AskUserQuestion]\nOptions:\n  → Yes, update all libraries (Recommended)\n  → Show me what will change in project.gcl first\n  → Cancel - don't update\n\n[User selects: \"Yes, update all libraries (Recommended)\"]\n\n================================================================================\nPROCEEDING WITH UPDATE\n================================================================================\n\nUpdating project.gcl...\n\n  ✓ Updated std to 7.6.10-dev\n  ✓ Updated explorer to 7.6.10-dev\n  ✓ Updated ai to 7.6.10-dev\n\nUpdated library declarations in project.gcl:\n@library(\"std\", \"7.6.10-dev\");\n@library(\"explorer\", \"7.6.10-dev\");\n@library(\"ai\", \"7.6.10-dev\");\n\n================================================================================\nINSTALLING LIBRARIES\n================================================================================\n\nDownloading std-7.6.10-dev.jar...\nDownloading explorer-7.6.10-dev.jar...\nDownloading ai-7.6.10-dev.jar...\n\n✓ Libraries installed to ./lib/\n\n================================================================================\nVERIFYING WITH LINT\n================================================================================\n\nLinting backend files...\n✓ All files passed\n\n================================================================================\n✓ UPDATE COMPLETE - All libraries updated and verified successfully!\n================================================================================\n\nUpdated libraries:\n  std: 7.6.10-dev\n  explorer: 7.6.10-dev\n  ai: 7.6.10-dev\n```\n\n---\n\n## Notes\n\n- **No Code Changes**: This command only updates library versions, not your code\n- **Data Preservation**: Data in `gcdata/` remains untouched\n- **Rollback Available**: Use git to revert if needed\n- **Dev Channel**: Uses latest dev versions (more features, less stable)\n- **Shared Versions**: All pro libraries use the same version number\n",
        "plugins/greycat/skills/greycat/SKILL.md": "---\nname: greycat\ndescription: \"Use when working with .gcl files or GreyCat projects - efficient language with unified temporal/graph/vector database, built-in web server, native MCP for billion-scale digital twins\"\n---\n\n# GreyCat\n\nUnified language + database (temporal/graph/vector) + web server + MCP. Built for billion-scale digital twins.\n\n**Nav**: [Types](#types) • [Nullability](#nullability) • [Nodes](#nodes-persistence) • [Collections](#indexed-collections) • [Commands](#commands) • [Testing](#testing) • [Pitfalls](#common-pitfalls)\n\n**Quick Start**:\n```gcl\n// Model + index\nvar users_by_id: nodeIndex<int, node<User>>;\ntype User { name: String; email: String; }\n\n// CRUD service\nabstract type UserService {\n    static fn create(name: String): node<User> { var u = node<User>{User{name}}; users_by_id.set(u->id, u); return u; }\n    static fn find(id: int): node<User>? { return users_by_id.get(id); }\n}\n\n// API endpoint\n@expose @permission(\"public\") fn getUsers(): Array<UserView> { /* ... */ }\n\n// Time-series query\nfor (t: time, temp: float in temperatures[start..end]) { info(\"${t}: ${temp}\"); }\n\n// Parallel processing\nvar jobs = Array<Job<Result>> {};\nfor (item in items) { jobs.add(Job<Result>{function: process, arguments: [item]}); }\nawait(jobs, MergeStrategy::last_wins);\n```\n\n## Installation\n\nVerify with `which greycat` or `greycat --version`. If not found, confirm with user before installing:\n\n**Linux/Mac/FreeBSD**: `curl -fsSL https://get.greycat.io/install.sh | bash -s dev`\n**Windows**: `iwr https://get.greycat.io/install_dev.ps1 -useb | iex`\n\nVerify with `greycat --version`, restart shell if needed.\n\n## Commands\n\n| Command | Description | Key Options |\n|---------|-------------|-------------|\n| `greycat build` | Compile project | `--log`, `--cache` |\n| `greycat serve` | Start server (HTTP + MCP) | `--port=8080`, `--workers=N`, `--user=1` (dev only) |\n| `greycat run` | Execute main() or function | `greycat run myFunction` |\n| `greycat test` | Run @test functions | Exit 0 on success |\n| `greycat install` | Download dependencies | From project.gcl @library |\n| `greycat codegen` | Generate typed headers | TS, Python, C, Rust |\n| `greycat defrag` | Compact storage | Safe anytime |\n| `greycat-lang lint --fix` | **Auto-fix errors** | **Run after code changes** |\n| `greycat-lang lint` | Check only | CI/CD pipelines |\n| `greycat-lang fmt` | Format files | In-place |\n| `greycat-lang server` | Start LSP | `--stdio` for IDE |\n\n**Environment**: All `--options` have `GREYCAT_*` env equivalents. Use `.env` next to `project.gcl`.\n\n**⚠️ CRITICAL**: After generating/modifying .gcl files, IMMEDIATELY run `greycat-lang lint --fix` and verify 0 errors.\n\n**Dev mode**: `--user=1` bypasses auth (NEVER in production).\n\n## Development Workflow Commands\n\nUse `/greycat:command-name` in Claude Code:\n\n| Command | Purpose | When |\n|---------|---------|------|\n| `/greycat:init` | Initialize CLAUDE.md | New projects |\n| `/greycat:tutorial` | Interactive learning | Onboarding, learning |\n| `/greycat:scaffold` | Generate models, services, APIs, tests | Starting features |\n| `/greycat:migrate` | Schema evolution, imports, storage | Schema changes, bulk ops |\n| `/greycat:upgrade` | Update libraries | Monthly maintenance |\n| `/greycat:backend` | Backend review (dead code, anti-patterns) | Before releases |\n| `/greycat:optimize` | Auto-fix performance issues | Quick checks |\n| `/greycat:apicheck` | Review @expose endpoints | After endpoints added |\n| `/greycat:coverage` | Test coverage + suggestions | After sprints |\n| `/greycat:frontend` | React/TS frontend review | Frontend features |\n| `/greycat:docs` | Generate README, API docs | Before releases |\n| `/greycat:typecheck` | Advanced type safety | After type changes |\n\n## Language Server (LSP)\n\n**[references/lsp.md](references/lsp.md)** - IDE integration (VS Code, Neovim, Emacs), diagnostics, programmatic clients.\n\n**Quick start**: `greycat-lang server --stdio` for IDE features (completion, go-to-def, hover, diagnostics, formatting).\n\n**CLI reference**: [references/cli.md](references/cli.md)\n\n## Architecture\n\n**Dirs**: `project.gcl` (entry), `backend/src/model/` (models+indices), `backend/src/service/` (XxxService), `backend/src/api/` (@expose), `backend/src/edi/` (import/export)\n\n**project.gcl**:\n```gcl\n@library(\"std\", \"7.6.152-dev\");           // required\n@library(\"explorer\", \"7.6.11-dev\");      // graph UI /explorer (dev)\n@include(\"backend\");                     // ⚠️ project.gcl only - includes ALL .gcl\n\n@permission(\"app.admin\", \"description\");\n@role(\"admin\", \"app.admin\", \"public\", \"admin\", \"api\", \"files\");\n\n@format_indent(4); @format_line_width(280);\nfn main() { }\n```\n\n**Conventions**: snake_case files, PascalCase types, `_prefix` unused, `*_test.gcl` tests\n\n## Types\n\n**Primitives**: `int` (64-bit, `1_000_000`), `float` (`3.14`), `bool`, `char`, `String` (`\"${name}\"`)\n\n**Casting**: Float→int rounds (≥0.5 up, <0.5 down): `var i = 42.9 as int; // 43`\n\n**Time**: `time` (μs epoch), `duration` (`1_us`, `500_ms`, `5_s`, `30_min`, `7_hour`, `2_day`), `Date` (UI, needs timezone)\n\n**Geo**: `geo{lat, lng}` | Shapes: `GeoBox`, `GeoCircle`, `GeoPoly` (`.contains(geo)`)\n\n```gcl\nvar list = Array<String>{}; var map = Map<String, int>{};  // ✅ use {}, NOT ::new()\n@volatile type ApiResponse { data: String; }  // non-persisted\n```\n\n## Nullability\n\nNon-null by default. Use `?` for nullable:\n```gcl\nvar city: City?;  // nullable\ncity?.name?.size(); city?.name ?? \"Unknown\"; data.get(\"key\")!!;\nif (country == null) { return null; }\nreturn country->name;  // ✅ no !! after null check\n```\n\n**⚠️ Parens for cast + coalescing**: `(answer as String?) ?? \"default\"` NOT `answer as String? ?? \"default\"`\n\n**⚠️ NO TERNARY** — use if/else: `if (valid) { result = \"yes\"; } else { result = \"no\"; }`\n\n## Nodes (Persistence)\n\n64-bit refs to persistent containers:\n```gcl\ntype Country { name: String; code: int; }\nvar obj = Country { name: \"LU\", code: 352 };  // RAM\nvar n = node<Country>{obj};                    // persisted\n*n; n->name; n.resolve(); n->name = \"X\"; node<int>{0}.set(5);\n```\n\n**Sharing**: `type City { country: node<Country>; }` (64-bit ref) vs embedded (heavy)\n\n**Multi-index ownership** (objects belong to ONE node, store refs):\n```gcl\nvar by_id = nodeList<node<Item>>{}; var by_name = nodeIndex<String, node<Item>>{};\nvar item = node<Item>{ Item{} };\nby_id.set(1, item); by_name.set(\"x\", item);  // both point to same\n```\n\n**Transactions**: Atomic per function, rollback on error. **Patterns, multi-index, transaction safety** → [references/nodes.md](references/nodes.md)\n\n## Indexed Collections\n\n| Persisted | Key | In-Memory |\n|-----------|-----|-----------|\n| `node<T>` | — | `Array<T>`, `Map<K,V>` |\n| `nodeList<node<T>>` | int | `Stack<T>`, `Queue<T>` |\n| `nodeIndex<K, node<V>>` | hash | `Set<T>`, `Tuple<A,B>` |\n| `nodeTime<node<T>>` | time | `Buffer`, `Table`, `Tensor` |\n| `nodeGeo<node<T>>` | geo | `TimeWindow`, `SlidingWindow` |\n\n```gcl\nvar temps = nodeTime<float>{}; temps.setAt(t1, 20.5);\nfor (t: time, v: float in temps[from..to]) { }\n\nvar idx = nodeIndex<String, node<X>>{}; idx.set(\"key\", val); idx.get(\"key\");  // ⚠️ set/get, NOT add\nvar list = nodeList<node<X>>{}; for (i: int, v in list[0..100]) { }\nvar geo_idx = nodeGeo<node<B>>{}; for (pos: geo, b in geo_idx.filter(GeoBox{...})) { }\n```\n\n**Sampling**: `nodeTime::sample([series], start, end, 1000, SamplingMode::adaptative, null, null)` — Modes: `fixed`, `fixed_reg`, `adaptative`, `dense`\n\n**Sort**: `cities.sort_by(City::population, SortOrder::desc);`\n\n**⚠️ CRITICAL**: Non-nullable `nodeList`, `nodeIndex`, `nodeTime`, `Array` attributes MUST initialize:\n```gcl\nvar city = node<City>{ City{ name: \"Paris\", country: country_node,\n    streets: nodeList<node<Street>>{}  }};  // ⚠️ MUST init!\n```\n\n## Module Variables\n\nRoot-level vars must be nodes/indexes (auto-persisted):\n```gcl\nvar count: node<int?>; fn main() { count.set((count.resolve() ?? 0) + 1); }\n```\n\n**Global indices auto-initialize**: Module `nodeIndex`/`nodeList`/`nodeTime`/`nodeGeo` — no `{}` needed. Collection ATTRIBUTES in types still need `{}`.\n\n## Model vs API Types\n\n**Models** — store node refs, global indices first:\n```gcl\nvar cities_by_name: nodeIndex<String, node<City>>;\ntype City { name: String; country: node<Country>; streets: nodeList<node<Street>>; }\n```\n\n**API** — return `Array<XxxView>` with `@volatile`, never nodeList:\n```gcl\n@volatile type CityView { name: String; country_name: String; street_count: int; }\n@expose @permission(\"public\") fn getCities(): Array<CityView> { ... }  // ⚠️ REQUIRES @expose for HTTP\n```\n\n**MCP exposure** (sparingly — only high-value APIs):\n```gcl\n@expose @tag(\"mcp\") @permission(\"public\") fn searchCities(query: String): Array<CityView> { ... }\n```\n\n## Functions & Control Flow\n\n```gcl\nfn add(x: int): int { return x + 2; }; fn noReturn() { }  // no void type\nvar lambda = fn(x: int): int { x * 2 };\nfor (k: K, v: V in map) { }; for (i, v in nullable?) { }  // ✅ use ? for nullable\n```\n\n## Services & Patterns\n\n```gcl\n// Service pattern: static functions for business logic\nabstract type CountryService {\n    static fn create(name: String): node<Country> { ... }\n    static fn find(name: String): node<Country>? { return countries_by_name.get(name); }\n}\n\n// Inheritance: abstract methods, polymorphism\nabstract type Building { address: String; fn calculateTax(): float; }\ntype House extends Building { fn calculateTax(): float { return value * 0.01; } }\n```\n\n**Patterns, CRUD, inheritance, polymorphism** → [references/patterns.md](references/patterns.md)\n\n## Logging & Error Handling\n\n```gcl\ninfo(\"msg ${var}\"); warn(\"msg\"); error(\"msg\");\ntry { op(); } catch (ex) { error(\"${ex}\"); }\n```\n\n## Parallelization\n\n```gcl\nvar jobs = Array<Job<ResultType>> {};\nfor (item in items) { jobs.add(Job<ResultType> { function: processFn, arguments: [item] }); }\nawait(jobs, MergeStrategy::last_wins);\nfor (job in jobs) { results.add(job.result()); }\n```\n\n**Key**: `Job<T>` generic, `MergeStrategy::last_wins`, no nested await. **Worker pools, PeriodicTask, async HTTP, patterns** → [references/concurrency.md](references/concurrency.md)\n\n## Testing\n\nRun `greycat test`. Test files: `*_test.gcl` in `./backend/test/`.\n\n```gcl\n@test fn test_city_creation() {\n    var city = City::create(\"Paris\", country_node);\n    Assert::equals(city->name, \"Paris\");\n}\nfn setup() { /* runs before tests */ }\nfn teardown() { /* cleanup after tests */ }\n```\n\n**Assert**: `equals(a, b)`, `equalsd(a, b, epsilon)`, `isTrue(v)`, `isFalse(v)`, `isNull(v)`, `isNotNull(v)`. **Organization, mocking, fixtures, CI/CD** → [references/testing.md](references/testing.md)\n\n## Common Pitfalls\n\n**⚠️ Reserved Keywords**: `limit`, `node`, `type`, `var`, `fn` — do NOT use as variable/attribute names:\n```gcl\n// ❌ WRONG\ntype User { limit: int; type: String; }  // reserved!\nfn process(node: String) { }             // reserved!\n\n// ✅ CORRECT\ntype User { max_limit: int; user_type: String; }\nfn process(node_name: String) { }\n```\n\n| ❌ Wrong | ✅ Correct |\n|----------|-----------|\n| `Array<T>::new()` | `Array<T>{}` |\n| `(*node)->field` | `node->field` |\n| `@permission(public)` | `@permission(\"public\")` |\n| `@permission(\"api\") fn getX()` | `@expose @permission(\"api\") fn getX()` |\n| `for(i=0;i<n;i++)` | `for (i, v in list)` |\n| `nodeList<City>` | `nodeList<node<City>>` |\n| `fn getX(): nodeList<...>` | `fn getX(): Array<XxxView>` |\n| `nodeIndex.add(k, v)` | `nodeIndex.set(k, v)` |\n| `for(i, v in nullable_list)` | `for(i, v in nullable_list?)` |\n| `fn doX(): void` | `fn doX()` |\n| `City{name: \"X\"}` | `City{name: \"X\", streets: nodeList<...>{}}` |\n\n**Double-bang OK** for global registry lookups: `var config = ConfigRegistry::getConfig(key)!!;`\n\n## ABI & Database\n\n**DEV**: Delete deprecated fields. Reset `gcdata/` on schema changes. Add non-nullable → make nullable first: `newField: int?`\n```bash\nrm -rf gcdata && greycat run import  # ⚠️ DELETES DATA - confirm first\n```\n\n## Full-Stack Development\n\n**[references/frontend.md](references/frontend.md)** - React+GreyCat guide (1,013 lines): @greycat/web SDK, TypeScript codegen, auth, React Query, error handling.\n\n## Local LLM Integration\n\n**[references/ai/llm.md](references/ai/llm.md)** - llama.cpp integration: model loading, text gen, chat, embeddings, LoRA.\n```gcl\n@library(\"ai\", \"7.6.60-dev\");\nvar model = Model::load(\"llama\", \"./model.gguf\", ModelParams { n_gpu_layers: -1 });\nvar result = model.chat([ChatMessage { role: \"user\", content: \"Hello!\" }], null, null);\n```\n\n## Library References\n\n**[references/LIBRARIES.md](references/LIBRARIES.md)** - Complete catalog: std, ai, algebra, kafka, sql, s3, opcua, finance, powerflow, useragent.\n\n**CLI**: [references/cli.md](references/cli.md) | **Docs**: https://doc.greycat.io/\n",
        "plugins/greycat/skills/greycat/references/LIBRARIES.md": "# GreyCat Library References\n\nComplete type definitions for all available GreyCat libraries. These files contain the full API surface, method signatures, and documentation for each library.\n\n## Standard Library (std) - Required\n\nCore types, collections, I/O, and runtime utilities:\n\n- **[std/core.gcl](std/core.gcl)** — Core types: any, null, type, field, function, bool, char, int, float, nodeTime, nodeList, nodeIndex, nodeGeo, String, geo, time, duration, Date, TimeZone, Array, Map, Set, Stack, Queue, Tuple, Tensor, Table, Buffer, Gaussian, SamplingMode, SortOrder\n- **[std/io.gcl](std/io.gcl)** — I/O types: Reader, Writer, BinReader, GcbReader/Writer, TextReader/Writer, XmlReader, JsonReader/Writer, Json, CsvReader/Writer, CsvFormat, CsvStatistics, Csv, File, FileWalker, Url, Http, HttpRequest, HttpResponse, HttpMethod, Email, Smtp, SmtpMode, SmtpAuth\n- **[std/runtime.gcl](std/runtime.gcl)** — Runtime types: Runtime, Task, PeriodicTask, Job, StoreStat, License, UserCredential, UserRole, UserGroupPolicyType, SystemInfo, Spi\n- **[std/util.gcl](std/util.gcl)** — Utility types: Assert, Histogram, GeoBox, GeoCircle, GeoPoly, BoxWhisker, Quantile, Random, TimeWindow, SlidingWindow, Crypto, HashMode, BitSet, StringBuilder\n\n## AI Library (ai)\n\nLocal LLM inference via llama.cpp for text generation, chat, embeddings, and LoRA adapters:\n\n- **[llm.md](ai/llm.md)** | [llm.gcl](ai/llm.gcl) — Complete LLM API: Model loading, text generation, chat completion, embeddings, tokenization, Context API, Sampler API, LoRA adapters\n\n## Algebra Library (algebra)\n\n**[📚 ALGEBRA Library Guide](algebra/README.md)** — Comprehensive machine learning, neural networks, and numerical computing with 163+ types for building production ML/NN applications.\n\n### Machine Learning & Neural Networks\n\n- **[ml.md](algebra/ml.md)** | [ml.gcl](algebra/ml.gcl) — Machine learning utilities (GaussianND, PCA, Polynomial regression, TimeSeriesDecomposition)\n- **[compute.md](algebra/compute.md)** | [compute.gcl](algebra/compute.gcl) — ComputeEngine, optimizers (Adam, SGD, RMSprop), layers (Dense, LSTM, Activation), initializers\n- **[nn.md](algebra/nn.md)** | [nn.gcl](algebra/nn.gcl) — Neural networks (RegressionNetwork, ClassificationNetwork, AutoEncoderNetwork, training/inference)\n\n### Pattern Recognition & Analysis\n\n- **[patterns.md](algebra/patterns.md)** | [patterns.gcl](algebra/patterns.gcl) — Pattern detection (Euclidean, DTW, FFT, SAX) for time series matching\n- **[transforms.md](algebra/transforms.md)** | [transforms.gcl](algebra/transforms.gcl) — FFT operations, frequency analysis, time series forecasting\n- **[kmeans.md](algebra/kmeans.md)** | [kmeans.gcl](algebra/kmeans.gcl) — K-means clustering with meta-learning and automatic cluster selection\n\n### Specialized Algorithms\n\n- **[climate.md](algebra/climate.md)** | [climate.gcl](algebra/climate.gcl) — UTCI climate index calculation and thermal comfort assessment\n- **[nn_layers_names.gcl](algebra/nn_layers_names.gcl)** — Neural network layer naming conventions\n\n## Integration Libraries\n\nExternal system integrations for databases, messaging, storage, and industrial protocols:\n\n- **[kafka.md](kafka/kafka.md)** | [kafka.gcl](kafka/kafka.gcl) — Apache Kafka event streaming with type-safe producers/consumers (KafkaReader, KafkaWriter, KafkaConf with 172+ parameters)\n- **[postgres.md](sql/postgres.md)** | [postgres.gcl](sql/postgres.gcl) — PostgreSQL database with transactions, SQL execution, CSV import/export via COPY command (Postgres)\n- **[s3.md](s3/s3.md)** | [s3.gcl](s3/s3.gcl) — S3-compatible object storage for AWS S3 and MinIO (S3, S3Object, S3Bucket, upload/download/list)\n- **[opcua.md](opcua/opcua.md)** | [opcua.gcl](opcua/opcua.gcl) — OPC UA industrial automation protocol with read/write/subscribe, historical data, security modes (OpcuaClient)\n- **[useragent.md](useragent/useragent.md)** | [useragent.gcl](useragent/useragent.gcl) — User agent parsing for browser, OS, and device detection (UserAgent::parse)\n\n## Domain-Specific Libraries\n\nSpecialized libraries for specific industries and applications:\n\n- **[finance.md](finance/finance.md)** | [finance.gcl](finance/finance.gcl) — IBAN parsing and validation for payment processing (Iban::parse, country/bank/account extraction)\n- **[powerflow.md](powerflow/powerflow.md)** | [powerflow.gcl](powerflow/powerflow.gcl) — Electrical power network analysis and load flow computation (PowerNetwork, bus/line results, grid optimization)\n\n## Using Libraries in Your Project\n\nAdd libraries to your `project.gcl`:\n\n```gcl\n@library(\"std\", \"7.6.152-dev\");        // Standard library (required)\n@library(\"ai\", \"7.6.60-dev\");        // AI/LLM support\n@library(\"algebra\", \"7.6.60-dev\");   // ML and numerical computing\n@library(\"kafka\", \"7.6.60-dev\");     // Kafka integration\n@library(\"sql\", \"7.6.60-dev\");       // PostgreSQL support (postgres library)\n@library(\"s3\", \"7.6.60-dev\");        // S3 storage\n@library(\"finance\", \"7.6.60-dev\");   // Financial utilities\n@library(\"powerflow\", \"7.6.60-dev\"); // Power flow analysis\n@library(\"opcua\", \"7.6.60-dev\");     // OPC UA integration\n@library(\"useragent\", \"7.6.60-dev\"); // User agent parsing\n@library(\"explorer\", \"7.6.11-dev\");   // Graph UI (dev only)\n```\n\n**Library Installation:**\n```bash\ngreycat install    # downloads all declared @library dependencies\n```\n",
        "plugins/greycat/skills/greycat/references/ai/llm.md": "# LLM - Local Language Model API\n\nGreyCat's LLM API provides comprehensive access to llama.cpp for local language model inference. Load GGUF models, generate text, compute embeddings, and build chat applications.\n\n## Quick Start\n\n```gcl\n@library(\"ai\", \"7.6.60-dev\");\n\n// Load a model\nvar model = Model::load(\"llama3\", \"./Llama-3.2-1B.gguf\", ModelParams { n_gpu_layers: -1 });\n\n// Get model info\nvar info = model.info();\nprintln(\"Loaded ${info.description} with ${info.n_params} parameters\");\n\n// Generate text\nvar result = model.generate(\"Once upon a time\", GenerationParams { max_tokens: 100 }, null);\nprintln(result.text);\n\n// Chat completion\nvar messages = [\n    ChatMessage { role: \"system\", content: \"You are a helpful assistant.\" },\n    ChatMessage { role: \"user\", content: \"What is the capital of France?\" }\n];\nvar response = model.chat(messages, GenerationParams { max_tokens: 100 }, null);\nprintln(response.text);\n\n// Generate embeddings\nvar emb = model.embed(\"Hello world\", TensorType::f32, null);\nprintln(\"Embedding dimension: ${emb.size()}\");\n\n// Clean up (optional - GC handles this)\nmodel.free();\n```\n\n## Model API\n\n### Loading Models\n\n```gcl\n// Basic loading\nvar model = Model::load(\"name\", \"./model.gguf\", null);\n\n// With GPU offload\nvar model = Model::load(\"name\", \"./model.gguf\", ModelParams {\n    n_gpu_layers: -1,  // All layers to GPU\n    use_mmap: true,\n    use_mlock: false\n});\n\n// Retrieve loaded model by name\nvar model = Model::get(\"name\");\n\n// Load split model files\nvar model = Model::load_from_splits(\"name\", [\n    \"./model-00001-of-00003.gguf\",\n    \"./model-00002-of-00003.gguf\",\n    \"./model-00003-of-00003.gguf\"\n], null);\n```\n\n**ModelParams Fields:**\n- `n_gpu_layers: int?` - Layers to GPU (0 = CPU, -1 = all)\n- `split_mode: SplitMode?` - Multi-GPU split mode (none, layer, row)\n- `main_gpu: int?` - GPU index when split_mode is none\n- `vocab_only: bool?` - Load vocabulary only (for tokenization)\n- `use_mmap: bool?` - Use memory-mapped files\n- `use_mlock: bool?` - Prevent swapping\n\n### Text Generation\n\n```gcl\n// Basic generation\nvar result = model.generate(\"Once upon a time\", null, null);\nprintln(result.text);\n\n// With parameters\nvar result = model.generate(\n    \"Write a story:\",\n    GenerationParams {\n        max_tokens: 200,\n        temperature: 0.8,\n        top_p: 0.95,\n        top_k: 40\n    },\n    ContextParams { n_ctx: 2048 }\n);\n\n// Streaming generation\nmodel.generate_stream(\n    \"Tell me a joke:\",\n    fn(token: String, is_final: bool): bool {\n        print(token);  // Print each token\n        return true;   // Return false to stop\n    },\n    GenerationParams { max_tokens: 100 },\n    null\n);\n```\n\n**GenerationParams Fields:**\n- `max_tokens: int?` - Maximum tokens to generate\n- `temperature: float?` - Randomness (0.0 = deterministic, 0.7-1.0 = creative)\n- `top_p: float?` - Nucleus sampling threshold (0.9-0.95 typical)\n- `top_k: int?` - Top-K sampling (40-50 typical)\n- `stop_sequences: Array<String>?` - Stop generation on these strings\n- `grammar: String?` - GBNF grammar constraint\n- `sampler: SamplerParams?` - Advanced sampler configuration\n\n### Chat Completion\n\n```gcl\nvar messages = [\n    ChatMessage { role: \"system\", content: \"You are a helpful assistant.\" },\n    ChatMessage { role: \"user\", content: \"Hello!\" }\n];\n\n// Basic chat\nvar result = model.chat(messages, GenerationParams { max_tokens: 100 }, null);\nprintln(result.text);\n\n// Streaming chat\nmodel.chat_stream(\n    messages,\n    fn(token: String, is_final: bool): bool {\n        print(token);\n        return true;\n    },\n    null, null\n);\n\n// Manual formatting\nvar prompt = model.format_chat(messages, true);  // add_assistant = true\n```\n\n### Embeddings\n\n```gcl\n// Single text\nvar emb = model.embed(\"Hello world\", TensorType::f32, ContextParams {\n    pooling_type: PoolingType::mean,\n    normalize: true\n});\n\n// Batch processing (more efficient)\nvar texts = [\"Hello\", \"World\", \"Test\"];\nvar embeddings = model.embed_batch(texts, TensorType::f32, null);\n\n// Compute similarity\nfn cosine_similarity(a: Tensor, b: Tensor): float {\n    var dot = 0.0;\n    for (var i = 0; i < a.size(); i++) {\n        dot = dot + (a[i] * b[i]);\n    }\n    return dot;  // Assumes normalized embeddings\n}\n```\n\n### Tokenization\n\n```gcl\n// Tokenize\nvar tokens = model.tokenize(\"Hello world\", true, false);\nprintln(\"Token count: ${tokens.size()}\");\n\n// Detokenize\nvar text = model.detokenize(tokens, true, false);\n\n// Single token operations\nvar piece = model.token_to_text(token_id);\nvar score = model.token_score(token_id);\nvar is_eog = model.is_eog_token(token_id);\n```\n\n### Model Information\n\n```gcl\nvar info = model.info();\nprintln(\"Description: ${info.description}\");\nprintln(\"Parameters: ${info.n_params}\");\nprintln(\"Context size: ${info.n_ctx_train}\");\nprintln(\"Vocab size: ${info.n_vocab}\");\nprintln(\"Has encoder: ${info.has_encoder}\");\n\n// Get metadata\nvar name = model.meta(\"general.name\");\nvar author = model.meta(\"general.author\");\n\n// Get chat template\nvar template = model.chat_template(null);\n```\n\n### Performance Metrics\n\n```gcl\nvar result = model.generate(prompt, params, null);\n\nvar perf = result.perf.context;\nprintln(\"Generated ${perf.n_eval} tokens in ${perf.t_eval_ms}ms\");\nprintln(\"Speed: ${perf.tokens_per_second} tok/s\");\nprintln(\"Prompt: ${perf.n_p_eval} tokens at ${perf.prompt_tokens_per_second} tok/s\");\n```\n\n## Advanced: Context API\n\nFor low-level control over KV cache, sequences, and batching. Most users should use the high-level Model API.\n\n```gcl\nvar model = Model::load(\"model\", \"./model.gguf\", null);\nvar ctx = Context::create(model, ContextParams { n_ctx: 4096 });\n\n// Tokenize and decode\nvar tokens = model.tokenize(\"Hello world\", true, false);\nvar batch = Batch::from_array(tokens, 0, 0);\nctx.decode(batch);\n\n// Get logits\nvar logits = ctx.get_logits(-1);\n\n// KV cache management\nctx.kv_cache_clear();\nctx.kv_cache_seq_rm(SeqId { id: 0 }, 0, 100);  // Remove tokens 0-100\nctx.kv_cache_seq_cp(SeqId { id: 0 }, SeqId { id: 1 }, 0, -1);  // Copy sequence\n\n// State save/restore\nctx.save_state(\"./checkpoint.state\");\nctx.load_state(\"./checkpoint.state\");\n\nctx.free();\n```\n\n**Context Methods:**\n- `decode(batch)` / `encode(batch)` - Process tokens\n- `get_logits(i)` / `get_embeddings(i)` - Get outputs\n- `kv_cache_clear()` / `kv_cache_seq_rm()` - Cache management\n- `get_state()` / `set_state()` - State persistence\n- `apply_lora_adapter()` / `remove_lora_adapter()` - LoRA management\n\n## Advanced: Sampler API\n\nFor custom sampling chains. Most users should use GenerationParams instead.\n\n```gcl\n// Create sampler chain\nvar chain = SamplerChain::create(null);\nchain.add(Sampler::penalties(64, 1.1, 0.0, 0.0));\nchain.add(Sampler::top_k(40));\nchain.add(Sampler::top_p(0.95, 1));\nchain.add(Sampler::temp(0.8));\nchain.add(Sampler::dist(12345));\n\n// Sample from context\nvar token = chain.sample(ctx, -1);\nchain.accept(token);\n\nchain.free();\n```\n\n**Available Samplers:**\n- `Sampler::greedy()` - Always select highest probability\n- `Sampler::dist(seed)` - Sample from distribution\n- `Sampler::top_k(k)` - Keep top K tokens\n- `Sampler::top_p(p, min_keep)` - Nucleus sampling\n- `Sampler::min_p(p, min_keep)` - Min-P sampling\n- `Sampler::temp(t)` - Temperature scaling\n- `Sampler::penalties(last_n, repeat, freq, present)` - Repetition penalties\n- `Sampler::grammar(model, grammar, root)` - Grammar constraint\n- `Sampler::mirostat_v2(seed, tau, eta)` - Mirostat v2\n\n## LoRA Adapters\n\nApply fine-tuning adapters without modifying base weights.\n\n```gcl\nvar model = Model::load(\"base\", \"./base-model.gguf\", null);\nvar ctx = Context::create(model, null);\n\n// Load and apply adapter\nvar lora = LoraAdapter::load(model, \"./medical-lora.gguf\", 1.0, null);\nctx.apply_lora_adapter(lora, 1.0);\n\n// Generate with adapter\nvar result = model.generate(\"Symptoms of diabetes:\", null, null);\n\n// Remove or switch adapters\nctx.remove_lora_adapter(lora);\nctx.clear_lora_adapters();\n\n// Multiple adapters\nvar code_lora = LoraAdapter::load(model, \"./code-lora.gguf\", 1.0, null);\nvar math_lora = LoraAdapter::load(model, \"./math-lora.gguf\", 1.0, null);\nctx.apply_lora_adapter(code_lora, 1.0);\nctx.apply_lora_adapter(math_lora, 0.5);  // 50% strength\n```\n\n## LLM Utility Functions\n\n```gcl\n// System info\nprintln(LLM::system_info());\n\n// Check capabilities\nif (LLM::supports_gpu()) {\n    println(\"GPU acceleration available\");\n}\nprintln(\"Max devices: ${LLM::max_devices()}\");\n\n// Enable logging\nLLM::logging(true);\n\n// Built-in chat templates\nvar templates = LLM::chat_templates();\n```\n\n## Types Reference\n\n### Enums\n\n**SplitMode** - GPU distribution: `none`, `layer`, `row`\n\n**PoolingType** - Embedding pooling: `unspecified`, `none`, `mean`, `cls`, `last`, `rank`\n\n**GgmlType** - Quantization formats: `f32`, `f16`, `bf16`, `q4_0`, `q4_k`, `q5_k`, `q6_k`, `q8_0`, etc.\n\n**StopReason** - Generation stop: `max_tokens`, `eog_token`, `aborted`, `error`\n\n### Result Types\n\n**GenerationResult:**\n```gcl\ntype GenerationResult {\n    text: String;              // Generated text\n    tokens: Array<int>;        // Token IDs\n    n_tokens: int;             // Token count\n    stop_reason: StopReason;   // Why generation stopped\n    perf: PerfData;            // Performance metrics\n}\n```\n\n**ModelInfo:**\n```gcl\ntype ModelInfo {\n    description: String;       // Model description\n    n_params: int;             // Parameter count\n    n_ctx_train: int;          // Training context size\n    n_vocab: int;              // Vocabulary size\n    n_embd: int;               // Embedding dimension\n    n_layer: int;              // Layer count\n    has_encoder: bool;         // Encoder-decoder model\n    has_decoder: bool;\n    // ... many more fields\n}\n```\n\n**ChatMessage:**\n```gcl\ntype ChatMessage {\n    role: String;     // \"system\", \"user\", \"assistant\"\n    content: String;  // Message content\n}\n```\n\n### Advanced Sampler Parameters\n\n**SamplerParams** - Full control over sampling:\n```gcl\ntype SamplerParams {\n    temperature: float?;       // Randomness (0.0-2.0)\n    top_k: int?;               // Top-K filter\n    top_p: float?;             // Nucleus sampling\n    min_p: float?;             // Min-P filter\n    typical_p: float?;         // Typical sampling\n    penalty: PenaltyParams?;   // Repetition penalties\n    dry: DryParams?;           // DRY sampling\n    mirostat_v2: MirostatV2Params?;\n    grammar: String?;          // GBNF grammar\n    logit_bias: Array<LogitBias>?;\n    seed: int?;                // Random seed\n}\n```\n\n## Common Patterns\n\n### Factual/Technical Output\n\n```gcl\nvar params = GenerationParams {\n    max_tokens: 200,\n    temperature: 0.2,\n    top_p: 0.9,\n    top_k: 10\n};\n```\n\n### Creative Writing\n\n```gcl\nvar params = GenerationParams {\n    max_tokens: 500,\n    temperature: 0.9,\n    top_p: 0.95,\n    sampler: SamplerParams {\n        penalty: PenaltyParams {\n            last_n: 128,\n            repeat: 1.2,\n            freq: 0.1\n        }\n    }\n};\n```\n\n### JSON Output with Grammar\n\n```gcl\nvar json_grammar = `\nroot ::= object\nobject ::= \"{\" ws members ws \"}\"\nmembers ::= pair (ws \",\" ws pair)*\npair ::= string ws \":\" ws value\nstring ::= \"\\\"\" [^\"]* \"\\\"\"\nvalue ::= string | number | \"true\" | \"false\" | \"null\"\nnumber ::= [0-9]+\nws ::= [ \\t\\n]*\n`;\n\nvar params = GenerationParams {\n    max_tokens: 300,\n    grammar: json_grammar,\n    temperature: 0.7\n};\n```\n\n### Embedding Similarity Search\n\n```gcl\nvar model = Model::load(\"embedder\", \"./all-minilm-l6-v2.gguf\", null);\n\n// Index documents\nvar docs = [\"Document 1...\", \"Document 2...\", \"Document 3...\"];\nvar doc_embs = model.embed_batch(docs, TensorType::f32, ContextParams {\n    pooling_type: PoolingType::mean,\n    normalize: true\n});\n\n// Query\nvar query_emb = model.embed(\"search query\", TensorType::f32, ContextParams {\n    pooling_type: PoolingType::mean,\n    normalize: true\n});\n\n// Find most similar\nvar best_idx = 0;\nvar best_score = -1.0;\nfor (var i = 0; i < doc_embs.size(); i++) {\n    var score = cosine_similarity(query_emb, doc_embs[i]);\n    if (score > best_score) {\n        best_score = score;\n        best_idx = i;\n    }\n}\nprintln(\"Best match: ${docs[best_idx]}\");\n```\n\n## Best Practices\n\n- **GPU Offload**: Use `n_gpu_layers: -1` for maximum performance\n- **Reuse Models**: Load once, reuse - loading is expensive\n- **Batch Embeddings**: Use `embed_batch()` instead of multiple `embed()` calls\n- **Temperature**: 0.1-0.3 for factual, 0.7-0.9 for creative\n- **Context Size**: Set `n_ctx` based on needs - larger uses more memory\n- **Thread Count**: Match `n_threads` to CPU cores\n- **Stop Sequences**: Use to control output boundaries\n- **Check GPU**: Use `LLM::supports_gpu()` before enabling GPU offload\n- **Memory**: Call `free()` explicitly for immediate cleanup\n- **Quantization**: Use q4_k or q5_k for balance of quality and speed\n",
        "plugins/greycat/skills/greycat/references/algebra/climate.md": "# Climate Modeling\n\nClimate data modeling utilities, currently providing UTCI (Universal Thermal Climate Index) calculation for thermal comfort assessment.\n\n## Overview\n\nThe Climate module provides specialized algorithms for climate and environmental data analysis. Currently includes:\n\n- **UTCI**: Universal Thermal Climate Index for human thermal comfort assessment\n\n## UTCI - Universal Thermal Climate Index\n\n### Function\n\n```typescript\nnative fn utci(\n  temp_outside: float,  // Outside temperature (°C)\n  wind_avg: float,      // Average wind speed (m/s)\n  temp_rad: float,      // Mean radiant temperature (°C)\n  hygro_per: float      // Relative humidity (%)\n): float                // UTCI value (°C)\n```\n\n### Description\n\nThe Universal Thermal Climate Index (UTCI) is a comprehensive thermal comfort index that combines:\n- Air temperature\n- Wind speed\n- Radiant temperature\n- Humidity\n\nIt provides an equivalent temperature that represents how the human body perceives the thermal environment.\n\n### Parameters\n\n**temp_outside** (°C)\n- Air temperature measured in shade\n- Typical range: -50°C to 50°C\n\n**wind_avg** (m/s)\n- Wind speed at 10m height\n- Adjusted for body height in calculation\n- Typical range: 0 to 30 m/s\n\n**temp_rad** (°C)\n- Mean radiant temperature\n- Average temperature of surrounding surfaces\n- Accounts for solar radiation and thermal radiation from surroundings\n- Often approximates air temperature in shade\n\n**hygro_per** (%)\n- Relative humidity\n- Range: 0 to 100\n\n### Returns\n\nUTCI equivalent temperature (°C) representing perceived thermal comfort:\n\n| UTCI Range (°C) | Thermal Stress Category |\n|-----------------|------------------------|\n| > +46 | Extreme heat stress |\n| +38 to +46 | Very strong heat stress |\n| +32 to +38 | Strong heat stress |\n| +26 to +32 | Moderate heat stress |\n| +9 to +26 | No thermal stress |\n| 0 to +9 | Slight cold stress |\n| -13 to 0 | Moderate cold stress |\n| -27 to -13 | Strong cold stress |\n| -40 to -27 | Very strong cold stress |\n| < -40 | Extreme cold stress |\n\n### Examples\n\n#### Basic Usage\n\n```typescript\n// Summer day in Paris\nvar utci = utci(\n  temp_outside: 30.0,   // 30°C air temperature\n  wind_avg: 3.0,        // 3 m/s wind\n  temp_rad: 35.0,       // 35°C mean radiant (sunny)\n  hygro_per: 60.0       // 60% humidity\n);\n\nprintln(\"UTCI: ${utci}°C\"); // Output: ~34-36°C (Strong heat stress)\n```\n\n#### Winter Conditions\n\n```typescript\n// Cold winter day\nvar utci = utci(\n  temp_outside: -5.0,   // -5°C air temperature\n  wind_avg: 10.0,       // 10 m/s wind (strong wind)\n  temp_rad: -5.0,       // -5°C radiant (cloudy)\n  hygro_per: 80.0       // 80% humidity\n);\n\nprintln(\"UTCI: ${utci}°C\"); // Output: ~-15°C (Strong cold stress)\n// Wind chill makes it feel much colder\n```\n\n#### Indoor vs Outdoor\n\n```typescript\n// Shade (indoor-like)\nvar utci_shade = utci(25.0, 1.0, 25.0, 50.0);\n\n// Direct sun\nvar utci_sun = utci(25.0, 1.0, 40.0, 50.0);\n\nprintln(\"Shade feels like: ${utci_shade}°C\");\nprintln(\"Sun feels like: ${utci_sun}°C\");\n// Radiant temperature significantly affects perceived comfort\n```\n\n### Time Series Analysis\n\n#### Daily Comfort Profile\n\n```typescript\n// Calculate UTCI for each hour of the day\nvar hourly_utci = nodeTime<float> {};\n\nfor (hour in 0..24) {\n  var t = time::new(2024, 7, 15, hour, 0, 0);\n\n  // Get weather data for this hour\n  var temp = temperature_data.getAt(t);\n  var wind = wind_data.getAt(t);\n  var humidity = humidity_data.getAt(t);\n\n  // Estimate radiant temperature\n  // (Simple model: +10°C in sun hours, equal to air temp otherwise)\n  var is_sunny = hour >= 6 && hour <= 18;\n  var rad_temp = is_sunny ? temp + 10.0 : temp;\n\n  var utci_val = utci(temp, wind, rad_temp, humidity);\n  hourly_utci.setAt(t, utci_val);\n}\n\n// Find peak discomfort\nvar max_utci = -Infinity;\nvar max_time: time? = null;\nfor (t, val in hourly_utci) {\n  if (val > max_utci) {\n    max_utci = val;\n    max_time = t;\n  }\n}\n\nprintln(\"Peak thermal stress at ${max_time}: ${max_utci}°C UTCI\");\n```\n\n#### Seasonal Comparison\n\n```typescript\n// Compare thermal comfort across seasons\nvar seasons = Map<String, Array<float>> {};\nseasons.set(\"Spring\", Array<float> {});\nseasons.set(\"Summer\", Array<float> {});\nseasons.set(\"Fall\", Array<float> {});\nseasons.set(\"Winter\", Array<float> {});\n\nfor (t, temp in yearly_temperature) {\n  var date = Date { time: t, tz: TimeZone::UTC };\n  var month = date.month();\n\n  var wind = wind_data.getAt(t) ?? 2.0;\n  var humidity = humidity_data.getAt(t) ?? 50.0;\n  var rad_temp = temp; // Simplified\n\n  var utci_val = utci(temp, wind, rad_temp, humidity);\n\n  if (month >= 3 && month <= 5) {\n    seasons.get(\"Spring\")!!.add(utci_val);\n  } else if (month >= 6 && month <= 8) {\n    seasons.get(\"Summer\")!!.add(utci_val);\n  } else if (month >= 9 && month <= 11) {\n    seasons.get(\"Fall\")!!.add(utci_val);\n  } else {\n    seasons.get(\"Winter\")!!.add(utci_val);\n  }\n}\n\n// Average comfort by season\nfor (season, values in seasons) {\n  var avg = sum(values) / values.size();\n  println(\"${season} avg UTCI: ${avg}°C\");\n}\n```\n\n### Climate Analysis Applications\n\n#### 1. Heat Wave Detection\n\n```typescript\n// Identify dangerous heat conditions\nvar heat_wave_threshold = 38.0; // Strong heat stress\nvar consecutive_hours = 0;\nvar heat_wave_start: time? = null;\n\nfor (t, temp in temperature_data) {\n  var utci_val = utci(\n    temp,\n    wind_data.getAt(t) ?? 2.0,\n    temp + 5.0, // Estimate radiant\n    humidity_data.getAt(t) ?? 50.0\n  );\n\n  if (utci_val > heat_wave_threshold) {\n    if (heat_wave_start == null) {\n      heat_wave_start = t;\n    }\n    consecutive_hours++;\n  } else {\n    if (consecutive_hours >= 6) {\n      println(\"Heat wave from ${heat_wave_start} (${consecutive_hours}h)\");\n    }\n    consecutive_hours = 0;\n    heat_wave_start = null;\n  }\n}\n```\n\n#### 2. Outdoor Activity Scheduling\n\n```typescript\n// Find optimal times for outdoor activities\nfn isSafeForActivity(utci_val: float, activity: String): bool {\n  if (activity == \"running\") {\n    return utci_val >= 0.0 && utci_val <= 26.0; // No thermal stress\n  } else if (activity == \"construction\") {\n    return utci_val >= 5.0 && utci_val <= 32.0; // Slight cold to moderate heat\n  } else if (activity == \"tourism\") {\n    return utci_val >= 9.0 && utci_val <= 26.0; // Comfort range\n  }\n  return false;\n}\n\nvar safe_hours = Array<time> {};\nfor (t, temp in forecast_data) {\n  var utci_val = utci(temp, wind_forecast.getAt(t), temp, humidity_forecast.getAt(t));\n\n  if (isSafeForActivity(utci_val, \"running\")) {\n    safe_hours.add(t);\n  }\n}\n\nprintln(\"Safe running hours: ${safe_hours.size()}\");\n```\n\n#### 3. Building Climate Control\n\n```typescript\n// Optimize HVAC based on UTCI\nfor (t, indoor_temp in indoor_temperature) {\n  var outdoor_temp = outdoor_temperature.getAt(t);\n  var outdoor_wind = wind_data.getAt(t);\n  var outdoor_humidity = humidity_data.getAt(t);\n\n  // Calculate outdoor UTCI\n  var outdoor_utci = utci(outdoor_temp, outdoor_wind, outdoor_temp, outdoor_humidity);\n\n  // Calculate indoor UTCI (low wind indoors)\n  var indoor_utci = utci(indoor_temp, 0.1, indoor_temp, indoor_humidity.getAt(t));\n\n  // Adjust HVAC to maintain comfort (UTCI 18-26°C)\n  if (indoor_utci > 26.0) {\n    // Cool down\n    hvac_setpoint.setAt(t, indoor_temp - 1.0);\n  } else if (indoor_utci < 18.0) {\n    // Heat up\n    hvac_setpoint.setAt(t, indoor_temp + 1.0);\n  }\n}\n```\n\n#### 4. Climate Zone Classification\n\n```typescript\n// Classify locations by thermal comfort\nfn classifyClimate(yearly_utci_data: nodeTime<float>): String {\n  var hot_days = 0;\n  var cold_days = 0;\n  var comfortable_days = 0;\n\n  for (_, utci_val in yearly_utci_data) {\n    if (utci_val > 32.0) {\n      hot_days++;\n    } else if (utci_val < 0.0) {\n      cold_days++;\n    } else if (utci_val >= 9.0 && utci_val <= 26.0) {\n      comfortable_days++;\n    }\n  }\n\n  var total_days = yearly_utci_data.size();\n\n  if (comfortable_days > total_days * 0.7) {\n    return \"Temperate\";\n  } else if (hot_days > total_days * 0.5) {\n    return \"Hot\";\n  } else if (cold_days > total_days * 0.5) {\n    return \"Cold\";\n  } else {\n    return \"Variable\";\n  }\n}\n\nvar classification = classifyClimate(utci_time_series);\nprintln(\"Climate classification: ${classification}\");\n```\n\n### Validation and Constraints\n\n```typescript\n// UTCI is valid for specific ranges\nfn validateUTCIInputs(temp: float, wind: float, rad_temp: float, humidity: float): bool {\n  if (temp < -50.0 || temp > 50.0) {\n    warn(\"Temperature out of range: ${temp}°C\");\n    return false;\n  }\n\n  if (wind < 0.0 || wind > 30.0) {\n    warn(\"Wind speed out of range: ${wind} m/s\");\n    return false;\n  }\n\n  if (humidity < 0.0 || humidity > 100.0) {\n    warn(\"Humidity out of range: ${humidity}%\");\n    return false;\n  }\n\n  return true;\n}\n\n// Safe UTCI calculation with validation\nfn safeUTCI(temp: float, wind: float, rad_temp: float, humidity: float): float? {\n  if (validateUTCIInputs(temp, wind, rad_temp, humidity)) {\n    return utci(temp, wind, rad_temp, humidity);\n  }\n  return null;\n}\n```\n\n### Estimating Mean Radiant Temperature\n\nWhen direct measurements are unavailable:\n\n```typescript\n// Simple model for outdoor conditions\nfn estimateRadiantTemp(air_temp: float, solar_radiation: float?, cloud_cover: float?): float {\n  if (solar_radiation == null || cloud_cover == null) {\n    // No data: assume equal to air temp\n    return air_temp;\n  }\n\n  // Estimate based on solar radiation\n  // Typical: +5 to +15°C above air temp in direct sun\n  var radiation_factor = (1.0 - cloud_cover) * (solar_radiation / 1000.0);\n  var rad_temp = air_temp + radiation_factor * 15.0;\n\n  return rad_temp;\n}\n\n// Usage\nvar rad_temp = estimateRadiantTemp(\n  air_temp: 25.0,\n  solar_radiation: 800.0,  // W/m²\n  cloud_cover: 0.3         // 30% cloud cover\n);\n// Result: ~28°C\n```\n\n### Integration with Weather APIs\n\n```typescript\n// Example: Process weather API data\ntype WeatherData {\n  time: time;\n  temperature: float;      // °C\n  wind_speed: float;       // m/s\n  humidity: float;         // %\n  solar_radiation: float?; // W/m²\n  cloud_cover: float?;     // 0-1\n}\n\nfn calculateUTCIFromWeather(weather: WeatherData): float {\n  // Estimate radiant temperature\n  var rad_temp = estimateRadiantTemp(\n    weather.temperature,\n    weather.solar_radiation,\n    weather.cloud_cover\n  );\n\n  return utci(\n    weather.temperature,\n    weather.wind_speed,\n    rad_temp,\n    weather.humidity\n  );\n}\n\n// Process forecast\nvar comfort_forecast = nodeTime<float> {};\nfor (weather in forecast) {\n  var utci_val = calculateUTCIFromWeather(weather);\n  comfort_forecast.setAt(weather.time, utci_val);\n}\n```\n\n---\n\n## Best Practices\n\n### Data Quality\n- Ensure wind speed is at standard 10m height\n- Use radiant temperature measurements when available\n- For estimates, account for solar angle and surface reflectance\n\n### Interpretation\n- UTCI represents thermal perception, not actual temperature\n- Consider individual factors (age, clothing, activity) not captured by UTCI\n- Use category thresholds, not exact values, for decision-making\n\n### Application Context\n- Outdoor thermal comfort assessment\n- Heat/cold stress risk evaluation\n- Urban planning and building design\n- Health impact studies\n- Climate change impact analysis\n\n### Limitations\n- Assumes standard clothing and metabolic rates\n- Most accurate for outdoor environments\n- May need calibration for extreme climates\n- Does not account for acclimatization\n\n---\n\n## References\n\n- UTCI official documentation: http://www.utci.org\n- Blazejczyk et al. (2013): \"An introduction to the Universal Thermal Climate Index (UTCI)\"\n- ISO 7933:2004: Ergonomics of the thermal environment\n\n---\n\n## Future Additions\n\nThe Climate module may expand to include:\n- Heat Index and Humidex calculations\n- Wet Bulb Globe Temperature (WBGT)\n- Predicted Mean Vote (PMV) / Predicted Percentage Dissatisfied (PPD)\n- Climate zone classification algorithms\n- Degree-day calculations (heating/cooling)\n- Thermal comfort time series analysis utilities\n\n## See Also\n\n- [Machine Learning (ml.md)](ml.md) - Statistical analysis of climate data\n- [Transforms (transforms.md)](transforms.md) - Frequency analysis of climate patterns\n- [README.md](README.md) - Library overview\n",
        "plugins/greycat/skills/greycat/references/algebra/compute.md": "# Compute Engine & Operations\n\nThe Compute module provides the low-level computational infrastructure for neural networks, including the execution engine, optimizers, layers, operations, and initializers.\n\n## Overview\n\nThis module is the foundation of the ALGEBRA library's neural network capabilities. It provides:\n\n- **ComputeEngine**: Execution engine for compiling and running computational graphs\n- **Optimizers**: 10 optimization algorithms (Adam, SGD, RMSprop, etc.)\n- **Layers**: Building blocks for neural networks (Dense, LSTM, Activation, etc.)\n- **Operations**: Element-wise and tensor operations\n- **Initializers**: Weight initialization strategies\n\n## Architecture\n\n```\nComputeModel (layers, sequences)\n       ↓\nComputeEngine (compile, initialize)\n       ↓\nExecution Loop (forward, backward, optimize)\n       ↓\nComputeState (save/load)\n```\n\n## Type Index\n\n### Core Engine\n| Type | Purpose |\n|------|---------|\n| `ComputeEngine` | Main execution engine |\n| `ComputeModel` | Model definition with layers |\n| `ComputeState` | Saveable model state |\n| `ComputeCounter` | Training progress tracking |\n\n### Optimizers\n| Type | Algorithm | Best For |\n|------|-----------|----------|\n| `ComputeOptimizerSgd` | Stochastic Gradient Descent | Simple problems |\n| `ComputeOptimizerMomentum` | SGD + Momentum | Accelerated convergence |\n| `ComputeOptimizerNesterov` | Nesterov Momentum | Look-ahead momentum |\n| `ComputeOptimizerAdam` | Adaptive Moment Estimation | General purpose (default) |\n| `ComputeOptimizerAdaDelta` | Adaptive Learning Rate | No manual LR tuning |\n| `ComputeOptimizerAdaGrad` | Adaptive Gradient | Sparse data |\n| `ComputeOptimizerAdaMax` | Adam variant (infinity norm) | Time-variant processes |\n| `ComputeOptimizerNadam` | Adam + Nesterov | Faster convergence |\n| `ComputeOptimizerRmsProp` | Root Mean Square Propagation | RNNs |\n| `ComputeOptimizerFtrl` | Follow The Regularized Leader | Large sparse features |\n\n### Layers\n| Type | Purpose |\n|------|---------|\n| `ComputeLayerLinear` | Linear transformation (y = Wx + b) |\n| `ComputeLayerDense` | Linear + Activation |\n| `ComputeLayerLSTM` | Long Short-Term Memory RNN |\n| `ComputeLayerActivation` | Standalone activation |\n| `ComputeLayerLoss` | Loss computation |\n| `ComputeLayerMinMaxScaler` | Min-max normalization |\n| `ComputeLayerStandardScaler` | Standard (z-score) normalization |\n| `ComputeLayerPCAScaler` | PCA transformation |\n| `ComputeLayerFilter` | Feature filtering |\n| `ComputeLayerClassification` | Classification output |\n| `ComputeLayerConfusion` | Confusion matrix |\n| `ComputeLayerCustom` | Custom operations |\n| `ComputeLayerSeq` | Sequence of layer calls |\n\n### Activation Functions\n| Type | Formula |\n|------|---------|\n| `ComputeActivationRelu` | max(0, x) |\n| `ComputeActivationLeakyRelu` | max(αx, x) |\n| `ComputeActivationSigmoid` | 1 / (1 + e^-x) |\n| `ComputeActivationTanh` | (e^x - e^-x) / (e^x + e^-x) |\n| `ComputeActivationSoftmax` | e^xi / Σe^xj |\n| `ComputeActivationSoftplus` | log(1 + e^x) |\n| `ComputeActivationSoftSign` | x / (1 + \\|x\\|) |\n| `ComputeActivationSelu` | λ(α(e^x - 1)) if x<0 else λx |\n| `ComputeActivationElu` | α(e^x - 1) if x<0 else x |\n| `ComputeActivationCelu` | max(0,x) + min(0,α(e^(x/α)-1)) |\n| `ComputeActivationHardSigmoid` | clip(slope*x + shift, 0, 1) |\n| `ComputeActivationExp` | e^x |\n\n---\n\n## ComputeEngine\n\nThe main execution engine for neural network computation.\n\n### Description\n\n`ComputeEngine` compiles computational graphs from `ComputeModel` definitions and executes forward/backward passes with optimization. It manages memory allocation, tensor operations, and training state.\n\n### Methods\n\n#### configure(forwardOnly: bool)\nConfigures the engine for training or inference.\n\n**Parameters:**\n- `forwardOnly`: `true` for inference (no gradients), `false` for training\n\n**Example:**\n```typescript\nvar engine = ComputeEngine {};\nengine.configure(false); // Training mode\n```\n\n#### compile(model: ComputeModel, maxBatchSize: int): int\nCompiles model and allocates memory for given batch size.\n\n**Parameters:**\n- `model`: The model to compile\n- `maxBatchSize`: Maximum batch size\n\n**Returns:** Memory size in bytes allocated\n\n**Example:**\n```typescript\nvar memory = engine.compile(model, 32);\nprintln(\"Allocated ${memory} bytes for batch size 32\");\n```\n\n#### compileUsing(model: ComputeModel, capacity: int): int\nCompiles model using a specified memory capacity.\n\n**Parameters:**\n- `model`: The model to compile\n- `capacity`: Memory capacity in bytes\n\n**Returns:** Actual batch size that fits in memory\n\n**Example:**\n```typescript\nvar batch = engine.compileUsing(model, 100 * 1024 * 1024); // 100MB\nprintln(\"Batch size: ${batch}\");\n```\n\n#### memorySize(): int\nReturns current memory allocation in bytes.\n\n#### initialize()\nInitializes all weights and biases according to their initializers.\n\n**Must call after:** `compile()`\n\n#### setSeed(seed: int)\nSets random seed for reproducible weight initialization.\n\n**Parameters:**\n- `seed`: Random seed\n\n**Must call before:** `initialize()`\n\n#### getSeed(): int\nReturns current random seed.\n\n#### resize(batchSize: int)\nDynamically changes batch size without recompiling.\n\n**Parameters:**\n- `batchSize`: New batch size (must be ≤ maxBatchSize)\n\n#### getVar(layer_name: String, var_name: String): Tensor?\nRetrieves a tensor variable from a layer.\n\n**Parameters:**\n- `layer_name`: Name of the layer\n- `var_name`: Name of the variable\n\n**Returns:** Tensor or null if not found\n\n**Example:**\n```typescript\nvar weights = engine.getVar(\"layer_1\", \"weight\");\nvar output = engine.getVar(\"layer_1\", \"output\");\n```\n\n#### getGrad(layer_name: String, var_name: String): Tensor?\nRetrieves gradient tensor for a variable.\n\n**Parameters:**\n- `layer_name`: Name of the layer\n- `var_name`: Name of the variable\n\n**Returns:** Gradient tensor or null\n\n**Example:**\n```typescript\nvar weight_grad = engine.getGrad(\"layer_1\", \"weight\");\n```\n\n#### forward(layer_name: String)\nExecutes forward pass for a layer or sequence.\n\n**Parameters:**\n- `layer_name`: Name of layer or sequence to execute\n\n**Example:**\n```typescript\nengine.forward(\"seq_predict\"); // Execute prediction sequence\n```\n\n#### derive(layer_name: String)\nComputes derivatives of loss with respect to outputs.\n\n**Parameters:**\n- `layer_name`: Name of sequence (usually training sequence)\n\n#### backward(layer_name: String)\nExecutes backward pass (backpropagation).\n\n**Parameters:**\n- `layer_name`: Name of sequence\n\n#### optimize(layer_name: String)\nUpdates weights using gradients and optimizer.\n\n**Parameters:**\n- `layer_name`: Name of sequence\n\n#### endEpoch(layer_name: String)\nSignals end of epoch (updates optimizer state for epoch-based schedules).\n\n**Parameters:**\n- `layer_name`: Name of sequence\n\n#### getCounters(layer_name: String): ComputeCounter\nReturns training progress counters.\n\n**Parameters:**\n- `layer_name`: Name of sequence\n\n**Returns:** ComputeCounter with epoch, optimization steps, etc.\n\n#### saveState(target: ComputeState)\nSaves current model weights and optimizer state.\n\n**Parameters:**\n- `target`: ComputeState object to store state\n\n**Example:**\n```typescript\nvar state = ComputeState {};\nengine.saveState(state);\n```\n\n#### saveStateString(): String\nSaves state as serialized string.\n\n**Returns:** String representation of state\n\n#### loadState(target: ComputeState)\nLoads model weights and optimizer state.\n\n**Parameters:**\n- `target`: ComputeState object to load from\n\n#### loadStateString(input: String)\nLoads state from serialized string.\n\n**Parameters:**\n- `input`: String representation of state\n\n### Complete Training Example\n\n```typescript\n// 1. Configure and compile\nvar engine = ComputeEngine {};\nengine.configure(false); // Training mode\n\nvar model = create_model(); // Your model definition\nvar memory = engine.compile(model, 32);\n\n// 2. Initialize\nengine.setSeed(42);\nengine.initialize();\n\n// 3. Training loop\nfor (epoch in 0..100) {\n  // Fill input/output tensors\n  engine.getVar(\"placeholders\", \"input\")!!.fill(X_train);\n  engine.getVar(\"placeholders\", \"target\")!!.fill(y_train);\n\n  // Forward pass\n  engine.forward(\"seq_train\");\n\n  // Compute derivatives\n  engine.derive(\"seq_train\");\n\n  // Backward pass\n  engine.backward(\"seq_train\");\n\n  // Update weights\n  engine.optimize(\"seq_train\");\n\n  // Get loss\n  var loss = engine.getVar(\"loss_layer\", \"loss\");\n  println(\"Epoch ${epoch}: Loss = ${loss}\");\n\n  // End epoch\n  engine.endEpoch(\"seq_train\");\n}\n\n// 4. Save trained model\nvar state = ComputeState {};\nengine.saveState(state);\n```\n\n---\n\n## Optimizers\n\nAll optimizers extend `ComputeOptimizer` with a `learning_rate` field.\n\n### ComputeOptimizerSgd\n\nStochastic Gradient Descent - simplest optimizer.\n\n**Fields:**\n```typescript\nstatic learning_rate_def: float = 0.01;\nlearning_rate: float?;\n```\n\n**Update rule:**\n```\nw = w - learning_rate * gradient\n```\n\n**Use when:** Simple problems, baseline comparison\n\n**Example:**\n```typescript\nvar optimizer = ComputeOptimizerSgd {\n  learning_rate: 0.01\n};\n```\n\n### ComputeOptimizerMomentum\n\nSGD with momentum for faster convergence.\n\n**Fields:**\n```typescript\nstatic learning_rate_def: float = 0.001;\nstatic decay_rate_def: float = 0.9;\nlearning_rate: float?;\ndecay_rate: float; // Momentum coefficient (typically 0.9)\n```\n\n**Update rule:**\n```\nv = decay_rate * v + gradient\nw = w - learning_rate * v\n```\n\n**Use when:** Accelerating convergence, escaping local minima\n\n### ComputeOptimizerAdam\n\nAdaptive Moment Estimation - most popular optimizer.\n\n**Fields:**\n```typescript\nstatic learning_rate_def: float = 0.001;\nstatic beta1_def: float = 0.9;      // 1st moment decay\nstatic beta2_def: float = 0.999;    // 2nd moment decay\nstatic smooth_epsilon_def: float = 1e-07;\n\nlearning_rate: float?;\nbeta1: float?;\nbeta2: float?;\nsmooth_epsilon: float?;\n```\n\n**Update rule:**\n```\nm = beta1 * m + (1 - beta1) * gradient         // 1st moment\nv = beta2 * v + (1 - beta2) * gradient²        // 2nd moment\nm_hat = m / (1 - beta1^t)                       // Bias correction\nv_hat = v / (1 - beta2^t)\nw = w - learning_rate * m_hat / (√v_hat + ε)\n```\n\n**Use when:** Default choice for most problems\n\n**Example:**\n```typescript\nvar optimizer = ComputeOptimizerAdam {\n  learning_rate: 0.001,\n  beta1: 0.9,\n  beta2: 0.999,\n  smooth_epsilon: 1e-07\n};\n```\n\n### ComputeOptimizerRmsProp\n\nRoot Mean Square Propagation - good for RNNs.\n\n**Fields:**\n```typescript\nstatic learning_rate_def: float = 0.001;\nstatic decay_rate_def: float = 0.9;\nstatic smooth_epsilon_def: float = 1e-07;\n\nlearning_rate: float?;\ndecay_rate: float?;\nsmooth_epsilon: float?;\n```\n\n**Update rule:**\n```\nv = decay_rate * v + (1 - decay_rate) * gradient²\nw = w - learning_rate * gradient / (√v + ε)\n```\n\n**Use when:** RNNs, non-stationary objectives\n\n### ComputeOptimizerAdaGrad\n\nAdaptive Gradient - adapts learning rate per parameter.\n\n**Fields:**\n```typescript\nstatic learning_rate_def: float = 0.001;\nstatic initial_accumulator_def: float = 0.1;\nstatic smooth_epsilon_def: float = 1e-07;\n\nlearning_rate: float?;\ninitial_accumulator: float?;\nsmooth_epsilon: float?;\n```\n\n**Update rule:**\n```\nv = v + gradient²\nw = w - learning_rate * gradient / (√v + ε)\n```\n\n**Use when:** Sparse data, different feature scales\n\n**Warning:** Learning rate continually decreases - may stop learning\n\n### Optimizer Comparison\n\n| Optimizer | LR Tuning | Memory | Convergence | Best For |\n|-----------|-----------|--------|-------------|----------|\n| SGD | Hard | Low | Slow | Baselines |\n| Momentum | Medium | Low | Fast | Acceleration |\n| Adam | Easy | Medium | Fast | General (default) |\n| RMSprop | Easy | Low | Fast | RNNs |\n| AdaGrad | Medium | Low | Moderate | Sparse data |\n| AdaDelta | None | Medium | Moderate | No LR tuning |\n\n**Recommendation:** Start with Adam, switch to RMSprop for RNNs.\n\n---\n\n## Layers\n\n### ComputeLayerLinear\n\nLinear transformation: `output = input × weight + bias`\n\n**Fields:**\n```typescript\nstatic var_input_name: String = \"input\";\nstatic var_output_name: String = \"output\";\nstatic var_weight_name: String = \"weight\";\nstatic var_bias_name: String = \"bias\";\n\nname: String;\ntype: TensorType;\ninputs: int;              // Input features\noutputs: int;             // Output features\nuse_bias: bool;           // Whether to add bias\nweight_initializer: ComputeInitializer?;\nweight_regularizer: ComputeRegularizer?;\nbias_initializer: ComputeInitializer?;\nbias_regularizer: ComputeRegularizer?;\n```\n\n**Tensor shapes:**\n- Input: `[batch, inputs]`\n- Weight: `[inputs, outputs]`\n- Bias: `[outputs]`\n- Output: `[batch, outputs]`\n\n**Example:**\n```typescript\nvar layer = ComputeLayerLinear {\n  name: \"linear_1\",\n  type: TensorType::f32,\n  inputs: 784,\n  outputs: 128,\n  use_bias: true,\n  weight_initializer: ComputeInitializerXavier {},\n  weight_regularizer: null,\n  bias_initializer: ComputeInitializerConstant { value: 0.0 },\n  bias_regularizer: null\n};\n```\n\n### ComputeLayerDense\n\nLinear transformation with activation: `output = activation(input × weight + bias)`\n\n**Fields:**\nSame as `ComputeLayerLinear` plus:\n```typescript\nactivation: ComputeActivation?;  // Activation function\nstatic var_pre_activation_name: String = \"pre_activation\";\n```\n\n**Most common layer type** in neural networks.\n\n**Example:**\n```typescript\nvar layer = ComputeLayerDense {\n  name: \"dense_1\",\n  type: TensorType::f32,\n  inputs: 128,\n  outputs: 64,\n  use_bias: true,\n  activation: ComputeActivationRelu {},\n  weight_initializer: ComputeInitializerRelu {},\n  bias_initializer: ComputeInitializerConstant { value: 0.0 }\n};\n```\n\n### ComputeLayerLSTM\n\nLong Short-Term Memory recurrent layer.\n\n**Fields:**\n```typescript\nstatic var_input_name: String = \"input\";\nstatic var_output_name: String = \"output\";\nstatic var_hx_name: String = \"hx\";  // Hidden state input\nstatic var_cx_name: String = \"cx\";  // Cell state input\nstatic var_hy_name: String = \"hy\";  // Hidden state output\nstatic var_cy_name: String = \"cy\";  // Cell state output\n\nname: String;\ntype: TensorType;\ninputs: int;              // Input dimension per timestep\noutputs: int;             // Output dimension per timestep\nlayers: int;              // Number of stacked LSTM layers\nsequences: int;           // Sequence length\nuse_bias: bool?;          // Default: true\nreturn_sequences: bool?;  // Default: true (return full sequence)\nbidirectional: bool?;     // Default: false\nauto_init_states: bool?;  // Default: true (zero-init h/c states)\n```\n\n**Tensor shapes:**\n- Input: `[sequences, batch, inputs]`\n- Output (return_sequences=true): `[sequences, batch, outputs]`\n- Output (return_sequences=false): `[batch, outputs]`\n- hx/hy: `[layers * directions, batch, outputs]` where directions=2 if bidirectional\n\n**LSTM Gates:**\n```\ni = sigmoid(W_ii * x + b_ii + W_hi * h + b_hi)  // Input gate\nf = sigmoid(W_if * x + b_if + W_hf * h + b_hf)  // Forget gate\ng = tanh(W_ig * x + b_ig + W_hg * h + b_hg)     // Cell gate\no = sigmoid(W_io * x + b_io + W_ho * h + b_ho)  // Output gate\nc' = f ⊙ c + i ⊙ g                               // New cell state\nh' = o ⊙ tanh(c')                                // New hidden state\n```\n\n**Example:**\n```typescript\nvar layer = ComputeLayerLSTM {\n  name: \"lstm_1\",\n  type: TensorType::f32,\n  inputs: 50,          // 50 features per timestep\n  outputs: 128,        // 128 hidden units\n  layers: 2,           // 2-layer stacked LSTM\n  sequences: 10,       // 10 timesteps\n  use_bias: true,\n  return_sequences: false,  // Return only last timestep\n  bidirectional: true,      // Process forward and backward\n  auto_init_states: true\n};\n```\n\n### ComputeLayerActivation\n\nStandalone activation layer.\n\n**Fields:**\n```typescript\nstatic var_input_name: String = \"input\";\nstatic var_output_name: String = \"output\";\n\nname: String;\nactivation: ComputeActivation;\n```\n\n**Example:**\n```typescript\nvar layer = ComputeLayerActivation {\n  name: \"relu_1\",\n  activation: ComputeActivationRelu {}\n};\n```\n\n### Scaling Layers\n\n#### ComputeLayerMinMaxScaler\nApplies min-max normalization: `(x - min) / (max - min)`\n\n**Fields:**\n```typescript\nstatic var_input_name: String = \"input\";\nstatic var_output_name: String = \"output\";\nstatic var_min_name: String = \"min\";\nstatic var_max_name: String = \"max\";\n\nname: String;\ntype: TensorType;\ninverse_transform: bool;  // false: scale, true: inverse scale\n```\n\n#### ComputeLayerStandardScaler\nApplies z-score normalization: `(x - μ) / σ`\n\n**Fields:**\n```typescript\nstatic var_input_name: String = \"input\";\nstatic var_output_name: String = \"output\";\nstatic var_avg_name: String = \"avg\";\nstatic var_std_name: String = \"std\";\n\nname: String;\ntype: TensorType;\ninverse_transform: bool;\n```\n\n#### ComputeLayerPCAScaler\nApplies PCA transformation.\n\n**Fields:**\n```typescript\nstatic var_input_name: String = \"input\";\nstatic var_output_name: String = \"output\";\nstatic var_avg_name: String = \"avg\";\nstatic var_std_name: String = \"std\";\nstatic var_space_name: String = \"space\";\n\nname: String;\ntype: TensorType;\ninverse_transform: bool;\n```\n\n### Loss Layers\n\n#### ComputeLayerLossRegression\nRegression loss (Square or Absolute).\n\n**Fields:**\n```typescript\nstatic var_computed_name: String = \"computed\";\nstatic var_expected_name: String = \"expected\";\nstatic var_loss_name: String = \"loss\";\n\nname: String;\nloss_type: ComputeRegressionLoss;  // square or abs\nreduction: ComputeReduction?;       // auto, none, sum, mean\n```\n\n**Loss formulas:**\n- Square: `(computed - expected)²`\n- Abs: `|computed - expected|`\n\n#### ComputeLayerLossClassification\nClassification loss (Cross-entropy).\n\n**Fields:**\n```typescript\nstatic var_computed_name: String = \"computed\";\nstatic var_expected_name: String = \"expected\";\nstatic var_loss_name: String = \"loss\";\nstatic var_class_weights_name: String = \"class_weights\";\nstatic var_predicted_classes_name: String = \"predicted_classes\";\nstatic var_probabilities_name: String = \"probabilities\";\n\nname: String;\nloss_type: ComputeClassificationLoss;  // categorical or sparse\nhas_class_weights: bool;\ncalculate_probabilities: bool;\nfrom_logits: bool;\nreduction: ComputeReduction?;\n```\n\n**Loss types:**\n- `categorical_cross_entropy`: Expects one-hot encoded targets\n- `sparse_categorical_cross_entropy`: Expects integer class indices\n\n---\n\n## Initializers\n\nWeight initialization strategies.\n\n### ComputeInitializerConstant\nInitializes all weights to a constant value.\n\n```typescript\nvar init = ComputeInitializerConstant { value: 0.0 };\n```\n\n### ComputeInitializerNormal\nInitializes from normal distribution N(μ, σ).\n\n```typescript\nvar init = ComputeInitializerNormal {\n  avg: 0.0,\n  std: 0.1\n};\n```\n\n### ComputeInitializerUniform\nInitializes from uniform distribution U(min, max).\n\n```typescript\nvar init = ComputeInitializerUniform {\n  min: -0.1,\n  max: 0.1\n};\n```\n\n### ComputeInitializerXavier (Glorot)\nGood for sigmoid/tanh activations.\n\n```typescript\nvar init = ComputeInitializerXavier {};\n```\n\n**Formula:** `U(-√(6/(fan_in + fan_out)), √(6/(fan_in + fan_out)))`\n\n### ComputeInitializerRelu (He)\nOptimized for ReLU activations.\n\n```typescript\nvar init = ComputeInitializerRelu {};\n```\n\n**Formula:** `N(0, √(2/fan_in))`\n\n### Initializer Guidelines\n\n| Activation | Recommended Initializer |\n|------------|------------------------|\n| ReLU, LeakyReLU | ComputeInitializerRelu |\n| Sigmoid, Tanh | ComputeInitializerXavier |\n| Linear | ComputeInitializerXavier |\n| LSTM | ComputeInitializerLSTM |\n\n---\n\n## Operations\n\nLow-level tensor operations used in custom layers.\n\n### Element-wise Operations\n\n```typescript\n// Arithmetic\nComputeOperationAdd { input, input2, output }\nComputeOperationSub { input, input2, output }\nComputeOperationMul { input, input2, output }\nComputeOperationDiv { input, input2, output }\nComputeOperationPow { input, input2, output }\n\n// Mathematical\nComputeOperationAbs { input, output }\nComputeOperationExp { input, output }\nComputeOperationLog { input, output }\nComputeOperationSqrt { input, output }\nComputeOperationNeg { input, output }\nComputeOperationSign { input, output }\n\n// Trigonometric\nComputeOperationSin { input, output }\nComputeOperationCos { input, output }\nComputeOperationTan { input, output }\n// ... and their inverses/hyperbolic variants\n```\n\n### Matrix Operations\n\n```typescript\nComputeOperationMatMul {\n  input: String,\n  input2: String,\n  output: String,\n  transposeA: bool,\n  transposeB: bool,\n  alpha: float,  // Scaling factor\n  beta: float    // Output accumulation factor\n}\n// Computes: output = alpha * matmul(A, B) + beta * output\n```\n\n### Reduction Operations\n\n```typescript\nComputeOperationSum {\n  input: String,\n  output: String,\n  axis: int?  // Axis to reduce (null = all)\n}\n\nComputeOperationArgMax {\n  input: String,\n  output: String,   // Indices\n  output2: String   // Values\n}\n\nComputeOperationArgMin {\n  input: String,\n  output: String,   // Indices\n  output2: String   // Values\n}\n```\n\n### Utility Operations\n\n```typescript\nComputeOperationFill {\n  input: String,\n  value: any  // Fill tensor with constant\n}\n\nComputeOperationClip {\n  input: String,\n  output: String,\n  min: float?,\n  max: float?\n}\n\nComputeOperationScale {\n  input: String,\n  output: String,\n  alpha: float  // Multiply by constant\n}\n```\n\n---\n\n## Custom Layers\n\nCreate custom layers with arbitrary operations.\n\n```typescript\nvar custom_layer = ComputeLayerCustom {\n  name: \"custom\",\n  vars: Array<ComputeVariable> {\n    ComputeVarInOut { name: \"input\", with_grad: true, shape: Array<int> {0, 10}, type: TensorType::f32 },\n    ComputeVar { name: \"temp\" },\n    ComputeVarInOut { name: \"output\", with_grad: true, shape: Array<int> {0, 10}, type: TensorType::f32 }\n  },\n  ops: Array<ComputeOperation> {\n    ComputeOperationScale { input: \"input\", output: \"temp\", alpha: 2.0 },\n    ComputeOperationRelu { input: \"temp\", output: \"output\" }\n  }\n};\n```\n\n---\n\n## Complete Model Building Example\n\n```typescript\n// Build a complete regression model manually\nfn create_regression_model(inputs: int, outputs: int): ComputeModel {\n  var placeholders = ComputeLayerCustom {\n    name: \"placeholders\",\n    vars: Array<ComputeVariable> {\n      ComputeVarInOut { name: \"input\", with_grad: false, shape: Array<int> {0, inputs}, type: TensorType::f32 },\n      ComputeVarInOut { name: \"target\", with_grad: false, shape: Array<int> {0, outputs}, type: TensorType::f32 }\n    },\n    ops: Array<ComputeOperation> {}\n  };\n\n  var hidden1 = ComputeLayerDense {\n    name: \"hidden_1\",\n    type: TensorType::f32,\n    inputs: inputs,\n    outputs: 128,\n    use_bias: true,\n    activation: ComputeActivationRelu {},\n    weight_initializer: ComputeInitializerRelu {},\n    bias_initializer: ComputeInitializerConstant { value: 0.0 }\n  };\n\n  var hidden2 = ComputeLayerDense {\n    name: \"hidden_2\",\n    type: TensorType::f32,\n    inputs: 128,\n    outputs: 64,\n    use_bias: true,\n    activation: ComputeActivationRelu {},\n    weight_initializer: ComputeInitializerRelu {},\n    bias_initializer: ComputeInitializerConstant { value: 0.0 }\n  };\n\n  var output_layer = ComputeLayerLinear {\n    name: \"output\",\n    type: TensorType::f32,\n    inputs: 64,\n    outputs: outputs,\n    use_bias: true,\n    weight_initializer: ComputeInitializerXavier {},\n    bias_initializer: ComputeInitializerConstant { value: 0.0 }\n  };\n\n  var loss = ComputeLayerLossRegression {\n    name: \"loss\",\n    loss_type: ComputeRegressionLoss::square,\n    reduction: ComputeReduction::mean\n  };\n\n  // Create training sequence\n  var train_seq = ComputeLayerSeq {\n    name: \"train\",\n    calls: Array<ComputeLayerCall> {\n      ComputeLayerCall {\n        layer_name: \"hidden_1\",\n        bindings: Array<ComputeBinding> {\n          ComputeBinding { src_layer_name: \"placeholders\", src_var_name: \"input\", target_var_name: \"input\" }\n        }\n      },\n      ComputeLayerCall {\n        layer_name: \"hidden_2\",\n        bindings: Array<ComputeBinding> {\n          ComputeBinding { src_layer_name: \"hidden_1\", src_var_name: \"output\", target_var_name: \"input\" }\n        }\n      },\n      ComputeLayerCall {\n        layer_name: \"output\",\n        bindings: Array<ComputeBinding> {\n          ComputeBinding { src_layer_name: \"hidden_2\", src_var_name: \"output\", target_var_name: \"input\" }\n        }\n      },\n      ComputeLayerCall {\n        layer_name: \"loss\",\n        bindings: Array<ComputeBinding> {\n          ComputeBinding { src_layer_name: \"output\", src_var_name: \"output\", target_var_name: \"computed\" },\n          ComputeBinding { src_layer_name: \"placeholders\", src_var_name: \"target\", target_var_name: \"expected\" }\n        }\n      }\n    },\n    optimizer: ComputeOptimizerAdam {\n      learning_rate: 0.001,\n      beta1: 0.9,\n      beta2: 0.999,\n      smooth_epsilon: 1e-07\n    }\n  };\n\n  return ComputeModel {\n    layers: Array<ComputeLayer> {\n      placeholders,\n      hidden1,\n      hidden2,\n      output_layer,\n      loss,\n      train_seq\n    }\n  };\n}\n```\n\n---\n\n## Best Practices\n\n### Memory Management\n```typescript\n// Option 1: Fixed batch size\nvar memory = engine.compile(model, 32);\n\n// Option 2: Fixed memory budget\nvar batch = engine.compileUsing(model, 100 * 1024 * 1024); // 100MB\n\n// Resize during execution (must be ≤ compiled size)\nengine.resize(16);\n```\n\n### Optimizer Selection\n```typescript\n// Start with Adam\nvar optimizer = ComputeOptimizerAdam {};\n\n// Switch to RMSprop for RNNs\nvar optimizer = ComputeOptimizerRmsProp {};\n\n// Use AdaGrad for sparse data\nvar optimizer = ComputeOptimizerAdaGrad {};\n```\n\n### Weight Initialization\n```typescript\n// ReLU networks\nweight_initializer: ComputeInitializerRelu {}\n\n// Sigmoid/Tanh networks\nweight_initializer: ComputeInitializerXavier {}\n\n// Always use zero bias\nbias_initializer: ComputeInitializerConstant { value: 0.0 }\n```\n\n## See Also\n\n- [Neural Networks (nn.md)](nn.md) - High-level network APIs\n- [Machine Learning (ml.md)](ml.md) - Preprocessing and PCA\n- [README.md](README.md) - Library overview\n",
        "plugins/greycat/skills/greycat/references/algebra/kmeans.md": "# K-means Clustering\n\nEfficient K-means clustering implementation with meta-learning for automatic cluster count selection and comprehensive cluster analysis.\n\n## Overview\n\nThe K-means module provides:\n- **Standard K-means**: Lloyd's algorithm for clustering\n- **Meta-learning**: Multiple runs to find best clustering\n- **Meta-meta-learning**: Automatic cluster count selection\n- **Cluster Analysis**: Inter-cluster distances, silhouette-like metrics\n- **Inference Engine**: Fast cluster assignment for new data\n\n## Quick Start\n\n```typescript\n// Automatic clustering with optimal cluster count\nvar result = Kmeans::meta_meta_learning(\n  tensor: data,              // [samples, features]\n  maxClusters: 10,           // Try 2-10 clusters\n  stopRatio: 0.1,            // Stop if improvement < 10%\n  seed: 42,\n  metaRounds: 100,\n  rounds: 20,\n  calculateInterClusterStats: true\n);\n\nvar centroids = result.bestResult!!.centroids;\nvar assignments = result.bestResult!!.assignment;\nprintln(\"Optimal clusters: ${centroids.shape()[0]}\");\n```\n\n## Type Index\n\n| Type | Purpose |\n|------|---------|\n| `Kmeans` | Static methods for K-means clustering |\n| `KmeanResult` | Single clustering result |\n| `KmeanMetaResult` | Meta-learning result (multiple runs) |\n\n---\n\n## Kmeans\n\nStatic methods for K-means clustering operations.\n\n### Configuration\n\n#### configure(nb_clusters: int, nb_features: int, tensor_type: TensorType, features_min: float, features_max: float, calculateInterClusterStats: bool): ComputeModel (static)\nCreates K-means computational model.\n\n**Parameters:**\n- `nb_clusters`: Number of clusters (k)\n- `nb_features`: Number of features per sample\n- `tensor_type`: f32 or f64\n- `features_min`: Min value for centroid initialization\n- `features_max`: Max value for centroid initialization\n- `calculateInterClusterStats`: Compute inter-cluster distances\n\n**Returns:** ComputeModel for K-means\n\n**Example:**\n```typescript\nvar model = Kmeans::configure(\n  nb_clusters: 5,\n  nb_features: 10,\n  tensor_type: TensorType::f64,\n  features_min: 0.0,\n  features_max: 1.0,\n  calculateInterClusterStats: true\n);\n```\n\n### Initialization\n\n#### initialize(engine: ComputeEngine, seed: int) (static)\nInitializes K-means engine.\n\n**Parameters:**\n- `engine`: ComputeEngine\n- `seed`: Random seed for centroid initialization\n\n**Example:**\n```typescript\nvar engine = ComputeEngine {};\nengine.compile(model, batch_size);\nKmeans::initialize(engine, 42);\n```\n\n### Training Operations\n\n#### init_round(engine: ComputeEngine) (static)\nInitializes a new clustering round (resets accumulators).\n\n**Call before:** Processing each round of data\n\n#### learn(engine: ComputeEngine, mini_batch: Tensor) (static)\nProcesses a mini-batch of data.\n\n**Parameters:**\n- `engine`: ComputeEngine\n- `mini_batch`: Data tensor [batch, features]\n\n**Example:**\n```typescript\nKmeans::init_round(engine);\nKmeans::learn(engine, data_batch);\nKmeans::end_round(engine);\n```\n\n#### end_round(engine: ComputeEngine) (static)\nFinalizes a round (updates centroids from accumulated data).\n\n#### calculate_stats(engine: ComputeEngine) (static)\nComputes final clustering statistics.\n\n**Call after:** All rounds complete\n\n### Inference\n\n#### cluster(engine: ComputeEngine, mini_batch: Tensor): Tensor (static)\nAssigns clusters to new data.\n\n**Parameters:**\n- `mini_batch`: Data tensor [batch, features]\n\n**Returns:** Cluster assignments tensor [batch]\n\n**Example:**\n```typescript\nvar assignments = Kmeans::cluster(engine, test_data);\n```\n\n### Retrieving Results\n\n#### getDistances(engine: ComputeEngine): Tensor (static)\nGets distance matrix [batch, clusters] of last mini-batch to all centroids.\n\n#### getAssignment(engine: ComputeEngine): Tensor (static)\nGets cluster assignments [batch] of last mini-batch.\n\n#### getBestDistances(engine: ComputeEngine): Tensor (static)\nGets distance [batch] to nearest centroid for last mini-batch.\n\n#### getSumOfDistances(engine: ComputeEngine): Tensor (static)\nGets total within-cluster distance (loss metric).\n\n**Example:**\n```typescript\nvar loss = Kmeans::getSumOfDistances(engine).get(Array<int> {0}) as float;\n```\n\n#### getClustersCentroids(engine: ComputeEngine): Tensor (static)\nGets centroid positions [clusters, features].\n\n#### getClustersCounts(engine: ComputeEngine): Tensor (static)\nGets number of samples per cluster [clusters].\n\n#### getClustersSumOfDistances(engine: ComputeEngine): Tensor (static)\nGets within-cluster distance sum per cluster [clusters].\n\n#### getClustersAvgOfDistances(engine: ComputeEngine): Tensor (static)\nGets average within-cluster distance per cluster [clusters].\n\n**Requires:** `calculateInterClusterStats=true` and `calculate_stats()` called\n\n#### getClustersDistancesToEachOther(engine: ComputeEngine): Tensor (static)\nGets inter-cluster distance matrix [clusters, clusters].\n\n**Requires:** `calculateInterClusterStats=true` and `calculate_stats()` called\n\n### Utility Methods\n\n#### replaceEmptyClusters(engine: ComputeEngine): bool (static)\nReplaces empty clusters with random samples.\n\n**Returns:** true if empty clusters were found\n\n#### sortClusters(engine: ComputeEngine) (static)\nSorts clusters by centroid sum (for consistent ordering).\n\n---\n\n## High-Level Learning Methods\n\n### learning(tensor: Tensor, nbClusters: int, seed: int, rounds: int?, featuresMin: float, featuresMax: float, calculateInterClusterStats: bool): KmeanResult (static)\nRuns standard K-means for fixed cluster count and seed.\n\n**Parameters:**\n- `tensor`: Data [samples, features]\n- `nbClusters`: Number of clusters\n- `seed`: Random seed\n- `rounds`: Number of iterations (default: 20)\n- `featuresMin`, `featuresMax`: Feature ranges\n- `calculateInterClusterStats`: Compute inter-cluster stats\n\n**Returns:** KmeanResult with centroids, assignments, loss, etc.\n\n**Example:**\n```typescript\nvar result = Kmeans::learning(\n  data,\n  nbClusters: 5,\n  seed: 42,\n  rounds: 30,\n  featuresMin: 0.0,\n  featuresMax: 1.0,\n  calculateInterClusterStats: true\n);\n\nprintln(\"Loss: ${result.loss}\");\nprintln(\"Centroids: ${result.centroids}\");\n```\n\n### meta_learning(tensor: Tensor, nbClusters: int, seed: int, metaRounds: int?, rounds: int?, calculateInterClusterStats: bool): KmeanMetaResult (static)\nRuns multiple K-means with different initializations, returns best.\n\n**Parameters:**\n- `tensor`: Data\n- `nbClusters`: Fixed cluster count\n- `seed`: Base random seed\n- `metaRounds`: Number of random restarts (default: 100)\n- `rounds`: Iterations per run (default: 20)\n- `calculateInterClusterStats`: Compute stats\n\n**Returns:** KmeanMetaResult with best result and all run losses\n\n**Example:**\n```typescript\nvar result = Kmeans::meta_learning(\n  data,\n  nbClusters: 5,\n  seed: 42,\n  metaRounds: 100,\n  rounds: 20,\n  calculateInterClusterStats: true\n);\n\nprintln(\"Best loss: ${result.bestResult!!.loss}\");\nprintln(\"Run losses: ${result.runDistances}\");\n```\n\n### meta_meta_learning(tensor: Tensor, maxClusters: int, stopRatio: float, seed: int, metaRounds: int?, rounds: int?, calculateInterClusterStats: bool): KmeanMetaResult (static)\nAutomatically finds optimal cluster count.\n\n**Parameters:**\n- `tensor`: Data\n- `maxClusters`: Maximum clusters to try\n- `stopRatio`: Stop if improvement < this ratio (0.0-1.0)\n- `seed`: Base seed\n- `metaRounds`, `rounds`: As above\n\n**Returns:** KmeanMetaResult for optimal cluster count\n\n**Algorithm:**\n1. Try k=2, 3, 4, ...\n2. For each k, run meta-learning\n3. Calculate improvement ratio: `(loss[k-1] - loss[k]) / loss[k-1]`\n4. Stop when improvement < stopRatio\n\n**Example:**\n```typescript\nvar result = Kmeans::meta_meta_learning(\n  data,\n  maxClusters: 15,\n  stopRatio: 0.1,     // Stop if improvement < 10%\n  seed: 42,\n  metaRounds: 100,\n  rounds: 20,\n  calculateInterClusterStats: true\n);\n\nvar k = result.bestResult!!.centroids.shape()[0];\nprintln(\"Optimal clusters: ${k}\");\n```\n\n---\n\n## Results\n\n### KmeanResult\n\nSingle clustering result.\n\n```typescript\ntype KmeanResult {\n  loss: float;                         // Total within-cluster distance\n  roundsDistances: Array<float>;       // Loss per round\n  centroids: Tensor?;                  // [clusters, features]\n  clusters_count: Tensor?;             // [clusters] - samples per cluster\n  clusters_sum_distance: Tensor?;      // [clusters] - total distance\n  clusters_avg_distance: Tensor?;      // [clusters] - avg distance\n  assignment: Tensor?;                 // [samples] - cluster IDs\n  distances: Tensor?;                  // [samples] - distance to centroid\n  clusterInterDistances: Tensor?;      // [clusters, clusters]\n}\n```\n\n**Example:**\n```typescript\nprintln(\"Cluster sizes: ${result.clusters_count}\");\nprintln(\"Cluster compactness: ${result.clusters_avg_distance}\");\n\n// Find most compact cluster\nvar min_avg_dist = float::max;\nvar best_cluster = -1;\nfor (i in 0..result.clusters_count!!.size()) {\n  var avg_dist = result.clusters_avg_distance!!.get(Array<int> {i}) as float;\n  if (avg_dist < min_avg_dist) {\n    min_avg_dist = avg_dist;\n    best_cluster = i;\n  }\n}\nprintln(\"Most compact cluster: ${best_cluster}\");\n```\n\n### KmeanMetaResult\n\nMeta-learning result.\n\n```typescript\ntype KmeanMetaResult {\n  runDistances: Array<float>;  // Loss for each run\n  bestResult: KmeanResult?;    // Best clustering\n}\n```\n\n**Example:**\n```typescript\nprintln(\"Tried ${result.runDistances.size()} initializations\");\nprintln(\"Best loss: ${result.bestResult!!.loss}\");\nprintln(\"Worst loss: ${max(result.runDistances)}\");\n```\n\n---\n\n## Inference Engine\n\n### getInferenceEngine(bestResult: KmeanResult, maxBatchSize: int, calculateInterClusterStats: bool): ComputeEngine (static)\nCreates an engine for fast cluster assignment.\n\n**Parameters:**\n- `bestResult`: Trained clustering result\n- `maxBatchSize`: Max batch size for inference\n- `calculateInterClusterStats`: Compute stats\n\n**Returns:** ComputeEngine with loaded centroids\n\n**Example:**\n```typescript\n// Train\nvar train_result = Kmeans::meta_learning(train_data, 5, 42, 100, 20, false);\n\n// Create inference engine\nvar infer_engine = Kmeans::getInferenceEngine(\n  train_result.bestResult!!,\n  maxBatchSize: 1000,\n  calculateInterClusterStats: false\n);\n\n// Assign new data\nvar new_assignments = Kmeans::cluster(infer_engine, new_data);\n```\n\n---\n\n## Complete Examples\n\n### Basic K-means\n\n```typescript\n// Prepare data\nvar data = Tensor {};\ndata.init(TensorType::f64, Array<int> {1000, 10}); // 1000 samples, 10 features\n// ... fill data ...\n\n// Find min/max for initialization\nvar arr = data.initPos();\nvar min_val = data.get(arr) as float;\nvar max_val = data.get(arr) as float;\nwhile (data.incPos(arr)) {\n  var val = data.get(arr) as float;\n  min_val = min(min_val, val);\n  max_val = max(max_val, val);\n}\n\n// Run K-means with k=5\nvar result = Kmeans::learning(\n  data,\n  nbClusters: 5,\n  seed: 42,\n  rounds: 30,\n  featuresMin: min_val,\n  featuresMax: max_val,\n  calculateInterClusterStats: true\n);\n\nprintln(\"Final loss: ${result.loss}\");\nprintln(\"Convergence: ${result.roundsDistances}\");\n\n// Analyze clusters\nfor (i in 0..5) {\n  var count = result.clusters_count!!.get(Array<int> {i});\n  var avg_dist = result.clusters_avg_distance!!.get(Array<int> {i});\n  println(\"Cluster ${i}: ${count} samples, avg distance ${avg_dist}\");\n}\n```\n\n### Automatic Cluster Selection\n\n```typescript\n// Let algorithm find optimal k\nvar result = Kmeans::meta_meta_learning(\n  data,\n  maxClusters: 20,\n  stopRatio: 0.05,    // Stop if improvement < 5%\n  seed: 42,\n  metaRounds: 50,     // 50 random restarts per k\n  rounds: 20,         // 20 iterations per restart\n  calculateInterClusterStats: true\n);\n\nvar k = result.bestResult!!.centroids.shape()[0];\nprintln(\"Optimal number of clusters: ${k}\");\n\n// Visualize elbow\nprintln(\"\\nLoss by cluster count:\");\nfor (i, loss in result.runDistances) {\n  println(\"k=${i+2}: ${loss}\");\n}\n```\n\n### Customer Segmentation\n\n```typescript\n// Segment customers by behavior\nvar customer_features = Tensor {}; // [customers, features]\n// Features: [purchase_frequency, avg_order_value, recency, ...]\n\nvar result = Kmeans::meta_meta_learning(\n  customer_features,\n  maxClusters: 10,\n  stopRatio: 0.1,\n  seed: 42,\n  metaRounds: 100,\n  rounds: 30,\n  calculateInterClusterStats: true\n);\n\nvar k = result.bestResult!!.centroids.shape()[0];\nprintln(\"Found ${k} customer segments\");\n\n// Analyze segments\nvar centroids = result.bestResult!!.centroids;\nfor (i in 0..k) {\n  println(\"\\nSegment ${i}:\");\n  for (f in 0..centroids.shape()[1]) {\n    var feature_val = centroids.get(Array<int> {i, f});\n    println(\"  Feature ${f}: ${feature_val}\");\n  }\n\n  var size = result.bestResult!!.clusters_count!!.get(Array<int> {i});\n  println(\"  Size: ${size} customers\");\n}\n\n// Assign new customers\nvar new_customers = Tensor {}; // New customer data\nvar infer_engine = Kmeans::getInferenceEngine(result.bestResult!!, 100, false);\nvar segments = Kmeans::cluster(infer_engine, new_customers);\n```\n\n### Image Color Quantization\n\n```typescript\n// Reduce image to k dominant colors\nvar image_pixels = Tensor {}; // [width*height, 3] - RGB values\n\nvar result = Kmeans::learning(\n  image_pixels,\n  nbClusters: 16,     // 16 colors\n  seed: 42,\n  rounds: 50,\n  featuresMin: 0.0,\n  featuresMax: 255.0,\n  calculateInterClusterStats: false\n);\n\n// Centroids are the dominant colors\nvar palette = result.centroids; // [16, 3]\n\n// Reconstruct image using palette\nvar assignments = result.assignment; // [width*height]\n// Each pixel → nearest palette color\n```\n\n### Anomaly Detection\n\n```typescript\n// Use K-means for outlier detection\nvar normal_data = Tensor {}; // Normal samples\n\nvar result = Kmeans::meta_learning(\n  normal_data,\n  nbClusters: 5,\n  seed: 42,\n  metaRounds: 100,\n  rounds: 20,\n  calculateInterClusterStats: true\n);\n\n// Create inference engine\nvar engine = Kmeans::getInferenceEngine(result.bestResult!!, 1000, false);\n\n// Check new samples\nvar test_samples = Tensor {};\nKmeans::learn(engine, test_samples);\nvar distances = Kmeans::getBestDistances(engine);\n\n// Samples far from all centroids = anomalies\nvar threshold = 3.0 * mean(result.bestResult!!.clusters_avg_distance);\nfor (i in 0..distances.size()) {\n  var dist = distances.get(Array<int> {i}) as float;\n  if (dist > threshold) {\n    println(\"Anomaly detected at sample ${i}, distance: ${dist}\");\n  }\n}\n```\n\n---\n\n## Performance Tips\n\n### Choosing Parameters\n\n```typescript\n// metaRounds: More = better but slower\n// - Small datasets (<1000): 50-100\n// - Large datasets (>10000): 10-30\n\n// rounds: Iterations per run\n// - Simple data: 10-20\n// - Complex data: 30-50\n\n// stopRatio for meta_meta_learning:\n// - Aggressive (fewer clusters): 0.15-0.20\n// - Balanced: 0.05-0.10\n// - Conservative (more clusters): 0.01-0.05\n```\n\n### Memory Management\n\n```typescript\n// For large datasets, use mini-batch approach\nvar batch_size = 1000;\nfor (batch_start in 0..data.shape()[0] step batch_size) {\n  var batch_end = min(batch_start + batch_size, data.shape()[0]);\n  var batch = data.slice(batch_start, batch_end);\n  Kmeans::learn(engine, batch);\n}\n```\n\n### Initialization Strategy\n\n```typescript\n// Use data range for better initialization\nvar stats = GaussianND {};\nstats.learn(data);\nvar min_vals = stats.min;\nvar max_vals = stats.max;\n\n// Or use K-means++ initialization (manual implementation)\n```\n\n---\n\n## Best Practices\n\n### Data Preparation\n```typescript\n// Normalize features before K-means\nvar gaussian = GaussianND {};\ngaussian.learn(data);\nvar normalized = gaussian.standard_scaling(data);\n\n// Run K-means on normalized data\nvar result = Kmeans::meta_meta_learning(normalized, ...);\n```\n\n### Validation\n```typescript\n// Use silhouette score or elbow method\n// Check cluster balance\nvar counts = result.bestResult!!.clusters_count;\nfor (i in 0..counts.size()) {\n  var ratio = counts.get(Array<int> {i}) / total_samples;\n  if (ratio < 0.01) {\n    warn(\"Cluster ${i} very small: ${ratio * 100}%\");\n  }\n}\n```\n\n### Reproducibility\n```typescript\n// Always set seed for reproducible results\nvar result = Kmeans::meta_learning(data, 5, 42, 100, 20, true);\n// Same result every time with seed=42\n```\n\n## See Also\n\n- [Machine Learning (ml.md)](ml.md) - GaussianND for normalization\n- [README.md](README.md) - Library overview\n",
        "plugins/greycat/skills/greycat/references/algebra/ml.md": "# Machine Learning Utilities\n\nThe ML module provides fundamental machine learning algorithms including statistical analysis, dimensionality reduction, polynomial regression, and time series decomposition.\n\n## Overview\n\nThis module focuses on classical machine learning techniques that serve as building blocks for more complex models:\n\n- **Statistical Analysis**: Multi-dimensional Gaussian profiling\n- **Dimensionality Reduction**: Principal Component Analysis (PCA)\n- **Regression**: Polynomial fitting and time series compression\n- **Time Series**: Decomposition and aggregation utilities\n\n## Type Index\n\n| Type | Purpose |\n|------|---------|\n| `GaussianND` | N-dimensional Gaussian distribution analysis and normalization |\n| `PCA` | Principal Component Analysis for dimensionality reduction |\n| `Solver` | Linear equation solver |\n| `Polynomial` | Polynomial regression and time series compression |\n| `TimeSeriesDecomposition` | Time series aggregation and decomposition |\n\n---\n\n## GaussianND\n\nMulti-dimensional Gaussian distribution for statistical profiling and data normalization.\n\n### Description\n\n`GaussianND` learns statistical properties (mean, standard deviation, covariance) from multi-dimensional data and provides normalization methods. Each row represents one observation, each column represents one feature/dimension.\n\n### Fields\n\n```typescript\nprivate total: int?;         // Number of observations\nprivate min: Tensor?;        // Minimum values per dimension\nprivate max: Tensor?;        // Maximum values per dimension\nprivate sum: Tensor?;        // Sum of values per dimension\nprivate sum_square: Tensor?; // Sum of squared values per dimension\n```\n\n### Methods\n\n#### learn(input: Tensor)\nLearns statistical properties from input data.\n\n**Parameters:**\n- `input`: Tensor of shape `[BatchSize, N]` where each row is an observation\n\n**Example:**\n```typescript\nvar gaussian = GaussianND {};\nvar data = Tensor {};\ndata.init(TensorType::f64, Array<int> {100, 5}); // 100 samples, 5 features\n// ... fill data ...\ngaussian.learn(data);\n```\n\n#### avg(): Tensor?\nReturns the average value for each dimension.\n\n**Returns:** 1D Tensor of shape `[N]` with average values\n\n**Example:**\n```typescript\nvar averages = gaussian.avg();\nprintln(\"Feature averages: ${averages}\");\n```\n\n#### std(): Tensor?\nReturns the standard deviation for each dimension.\n\n**Returns:** 1D Tensor of shape `[N]` with standard deviations\n\n#### covariance(): Tensor?\nCalculates the covariance matrix showing how dimensions vary together.\n\n**Returns:** 2D Tensor of shape `[N, N]` where element `[i,j]` is covariance of dimension i with dimension j\n\n**Mathematical Definition:**\n```\nCov(X,Y) = E[(X - E[X])(Y - E[Y])]\n```\n\n#### correlation(): Tensor?\nCalculates the correlation matrix (normalized covariance).\n\n**Returns:** 2D Tensor of shape `[N, N]` with correlation coefficients in range `[-1, 1]`\n\n**Mathematical Definition:**\n```\nCorr(X,Y) = Cov(X,Y) / (σ_X * σ_Y)\n```\n\n#### dimensions(): int\nReturns the number of dimensions in the N-dimensional space.\n\n#### clear()\nResets all internal state for memory reuse.\n\n#### min_max_scaling(input: Tensor): Tensor\nNormalizes data to range `[0, 1]` using min-max scaling.\n\n**Parameters:**\n- `input`: Tensor of shape `[batch, N]`\n\n**Returns:** Normalized tensor of same shape\n\n**Formula:**\n```\nx_normalized = (x - min) / (max - min)\n```\n\n**Example:**\n```typescript\nvar normalized = gaussian.min_max_scaling(test_data);\n```\n\n#### inverse_min_max_scaling(input: Tensor): Tensor\nReverses min-max scaling to get original values.\n\n**Parameters:**\n- `input`: Normalized tensor of shape `[batch, N]`\n\n**Returns:** Denormalized tensor\n\n**Formula:**\n```\nx_original = x_normalized * (max - min) + min\n```\n\n#### standard_scaling(input: Tensor): Tensor\nStandardizes data to zero mean and unit variance.\n\n**Parameters:**\n- `input`: Tensor of shape `[batch, N]`\n\n**Returns:** Standardized tensor\n\n**Formula:**\n```\nx_standardized = (x - μ) / σ\n```\n\n**Example:**\n```typescript\nvar standardized = gaussian.standard_scaling(train_data);\n```\n\n#### inverse_standard_scaling(input: Tensor): Tensor\nReverses standard scaling.\n\n**Parameters:**\n- `input`: Standardized tensor\n\n**Returns:** Original scale tensor\n\n**Formula:**\n```\nx_original = x_standardized * σ + μ\n```\n\n#### crop(from: int, to: int): GaussianND\nCreates a new GaussianND with a subset of features.\n\n**Parameters:**\n- `from`: Starting feature index (inclusive)\n- `to`: Ending feature index (exclusive)\n\n**Returns:** New GaussianND with features `[from, to)`\n\n**Example:**\n```typescript\n// Extract features 2-5 from 10-feature dataset\nvar subset = gaussian.crop(2, 5);\n```\n\n### Complete Example\n\n```typescript\n// 1. Create and learn from data\nvar gaussian = GaussianND {};\nvar training_data = Tensor {};\ntraining_data.init(TensorType::f64, Array<int> {1000, 20}); // 1000 samples, 20 features\n// ... populate training_data ...\n\ngaussian.learn(training_data);\n\n// 2. Analyze statistics\nvar means = gaussian.avg();\nvar stds = gaussian.std();\nvar corr = gaussian.correlation();\n\nprintln(\"Feature means: ${means}\");\nprintln(\"Feature stds: ${stds}\");\n\n// 3. Normalize new data\nvar test_data = Tensor {};\ntest_data.init(TensorType::f64, Array<int> {100, 20});\n// ... populate test_data ...\n\nvar normalized = gaussian.standard_scaling(test_data);\n\n// 4. Use normalized data for ML, then denormalize predictions\nvar predictions_normalized = model.predict(normalized);\nvar predictions = gaussian.inverse_standard_scaling(predictions_normalized);\n```\n\n---\n\n## PCA\n\nPrincipal Component Analysis for dimensionality reduction.\n\n### Description\n\nPCA identifies the principal components (directions of maximum variance) in high-dimensional data and projects it to a lower-dimensional space while retaining most of the variance.\n\n### Fields\n\n```typescript\nstatic threshold_def: float = 0.95; // Default variance retention threshold\n\nprivate eigen_vectors: Tensor?;     // Eigenvectors of correlation matrix\nprivate eigen_values: Tensor?;      // Eigenvalues (variance explained)\nprivate avg: Tensor?;               // Mean of original features\nprivate std: Tensor?;               // Std deviation of original features\nprivate correlation: Tensor?;       // Correlation matrix\nprivate explained_variance: Tensor?; // Cumulative variance explained\nprivate space_origin: Tensor?;      // Origin in reduced space\nprivate dimension_info: Tensor?;    // Information about dimensions\n\nbest_dimension: int?;    // Best dimension for threshold\nselected_dimension: int?; // User-selected dimension\nthreshold: float?;        // Variance retention threshold\nspace: Tensor?;          // Projection matrix\n```\n\n### Methods\n\n#### learn(correlation: Tensor, avg: Tensor, std: Tensor, threshold: float?)\nLearns PCA transformation from correlation matrix.\n\n**Prerequisites:** Create a `GaussianND`, learn from data, then extract correlation, avg, std\n\n**Parameters:**\n- `correlation`: Correlation matrix from GaussianND\n- `avg`: Average values per feature\n- `std`: Standard deviations per feature\n- `threshold`: Variance retention ratio (default: 0.95 = 95%)\n\n**Example:**\n```typescript\nvar gaussian = GaussianND {};\ngaussian.learn(data);\n\nvar pca = PCA {};\npca.learn(\n  gaussian.correlation()!!,\n  gaussian.avg()!!,\n  gaussian.std()!!,\n  0.95  // Retain 95% of variance\n);\n\nprintln(\"Best dimension: ${pca.best_dimension}\");\n```\n\n#### set_dimension(dimension: int)\nSets the target dimensionality for reduction.\n\n**Parameters:**\n- `dimension`: Number of dimensions to reduce to (must be ≤ original dimensions)\n\n**Example:**\n```typescript\npca.set_dimension(10); // Reduce to 10 dimensions\n```\n\n#### transform(input: Tensor): Tensor\nProjects high-dimensional data to reduced space.\n\n**Parameters:**\n- `input`: Tensor of shape `[batch, N]` (original dimensions)\n\n**Returns:** Tensor of shape `[batch, dim]` (reduced dimensions)\n\n**Must call:** `learn()` and `set_dimension()` first\n\n**Example:**\n```typescript\nvar reduced = pca.transform(original_data);\n// [1000, 50] → [1000, 10]\n```\n\n#### inverse_transform(input: Tensor): Tensor\nReconstructs original dimensions from reduced space.\n\n**Parameters:**\n- `input`: Tensor of shape `[batch, dim]` (reduced dimensions)\n\n**Returns:** Tensor of shape `[batch, N]` (original dimensions)\n\n**Note:** Reconstruction is approximate due to information loss\n\n**Example:**\n```typescript\nvar reconstructed = pca.inverse_transform(reduced);\n// [1000, 10] → [1000, 50]\n```\n\n#### get_dimension(threshold: float): int\nCalculates minimum dimensions needed to retain a given variance.\n\n**Parameters:**\n- `threshold`: Variance retention ratio (e.g., 0.90 = 90%)\n\n**Returns:** Number of dimensions needed\n\n**Example:**\n```typescript\nvar dims_90 = pca.get_dimension(0.90); // Dims for 90% variance\nvar dims_95 = pca.get_dimension(0.95); // Dims for 95% variance\nvar dims_99 = pca.get_dimension(0.99); // Dims for 99% variance\n```\n\n### Complete PCA Workflow\n\n```typescript\n// 1. Prepare data with Gaussian profiling\nvar gaussian = GaussianND {};\ngaussian.learn(training_data); // [10000, 100] - 100 features\n\n// 2. Learn PCA\nvar pca = PCA {};\npca.learn(\n  gaussian.correlation()!!,\n  gaussian.avg()!!,\n  gaussian.std()!!,\n  0.95  // Retain 95% of variance\n);\n\n// 3. Find optimal dimensions\nprintln(\"Dimensions for 90% variance: ${pca.get_dimension(0.90)}\");\nprintln(\"Dimensions for 95% variance: ${pca.best_dimension}\");\nprintln(\"Dimensions for 99% variance: ${pca.get_dimension(0.99)}\");\n\n// 4. Set dimension and transform\npca.set_dimension(20); // Reduce from 100 to 20 features\n\nvar reduced_train = pca.transform(training_data);   // [10000, 20]\nvar reduced_test = pca.transform(test_data);        // [1000, 20]\n\n// 5. Train model on reduced data\n// model.train(reduced_train, labels);\n\n// 6. Optional: Reconstruct for visualization\nvar reconstructed = pca.inverse_transform(reduced_test);\nvar reconstruction_error = compute_error(test_data, reconstructed);\n```\n\n### PCA for Preprocessing\n\n```typescript\n// Use PCA as preprocessing for neural networks\nvar nn = RegressionNetwork::new(20, 1, TensorType::f32, false, 32, 42);\nnn.setPreProcess(PreProcessType::pca_scaling, pca);\n\n// Network will automatically apply PCA transform\nvar model = nn.build(true);\n```\n\n---\n\n## Solver\n\nStatic linear equation solver.\n\n### Description\n\nSolves linear systems of equations: `X * w = Y` for `w`.\n\n### Methods\n\n#### solve(X: Tensor, Y: Tensor): Tensor (static)\nSolves the linear system X*w = Y.\n\n**Parameters:**\n- `X`: Coefficient matrix of shape `[m, n]`\n- `Y`: Target values of shape `[m, k]`\n\n**Returns:** Solution tensor `w` of shape `[n, k]`\n\n**Example:**\n```typescript\nvar X = Tensor {}; // [100, 10] - 100 equations, 10 unknowns\nvar Y = Tensor {}; // [100, 1]  - 100 targets\n\nvar weights = Solver::solve(X, Y); // [10, 1] - solution\n```\n\n---\n\n## Polynomial\n\nPolynomial regression for curve fitting and time series compression.\n\n### Description\n\nFits polynomial curves to data and provides compression for time series using polynomial approximation.\n\n### Fields\n\n```typescript\ndegree: int?;              // Polynomial degree\ncoefficients: Array<float>?; // Polynomial coefficients\nx_start: float?;           // Starting X value\nx_step: float?;            // X step size\n```\n\n### Methods\n\n#### learn(degrees: int, X: Tensor, Y: Tensor): float\nFits a polynomial of specified degree to data.\n\n**Parameters:**\n- `degrees`: Polynomial degree (0 = constant, 1 = linear, 2 = quadratic, etc.)\n- `X`: Input values (1D tensor)\n- `Y`: Target values (1D tensor)\n\n**Returns:** Fitting error (RMSE)\n\n**Example:**\n```typescript\nvar poly = Polynomial {};\nvar X = Tensor {}; // [100] - input values\nvar Y = Tensor {}; // [100] - target values\n\nvar error = poly.learn(3, X, Y); // Fit cubic polynomial\nprintln(\"Fitting error: ${error}\");\n```\n\n#### predict(X: Tensor): Tensor\nPredicts values using learned polynomial.\n\n**Parameters:**\n- `X`: Input tensor (1D)\n\n**Returns:** Predicted values tensor (1D)\n\n#### predictValue(x: float): float\nPredicts a single value.\n\n**Parameters:**\n- `x`: Input value\n\n**Returns:** Predicted value\n\n**Example:**\n```typescript\nvar y_pred = poly.predictValue(5.0);\n```\n\n#### getDegrees(): int\nReturns the degree of the fitted polynomial.\n\n### Static Methods for Time Series Compression\n\n#### compress(originalTS: nodeTime<float>, polynomialTS: nodeTime<Polynomial>, maxDegree: int, maxError: float, maxBufferSize: int) (static)\nCompresses a time series using adaptive polynomial fitting.\n\n**Parameters:**\n- `originalTS`: Input time series\n- `polynomialTS`: Output compressed representation\n- `maxDegree`: Maximum polynomial degree allowed\n- `maxError`: Maximum acceptable error\n- `maxBufferSize`: Maximum buffer size for fitting\n\n**How it works:**\n1. Fits polynomial to sliding window of data\n2. Increases degree if error exceeds threshold\n3. Splits into new polynomial if max degree can't achieve error target\n4. Stores only polynomial coefficients instead of all points\n\n**Example:**\n```typescript\nvar original = nodeTime<float> {};\nvar compressed = nodeTime<Polynomial> {};\n\nPolynomial::compress(\n  originalTS: original,\n  polynomialTS: compressed,\n  maxDegree: 5,\n  maxError: 0.1,\n  maxBufferSize: 1000\n);\n\nprintln(\"Compression ratio: ${original.size() / compressed.size()}\");\n```\n\n#### decompress(originalTS: nodeTime<float>, polynomialTS: nodeTime<Polynomial>, maxError: float, decompressedTS: nodeTime<float>, errorTS: nodeTime<float>) (static)\nDecompresses polynomial representation back to time series.\n\n**Parameters:**\n- `originalTS`: Original time series (for validation)\n- `polynomialTS`: Compressed polynomial representation\n- `maxError`: Maximum acceptable error (warns if exceeded)\n- `decompressedTS`: Output decompressed time series\n- `errorTS`: Output error time series\n\n**Example:**\n```typescript\nvar decompressed = nodeTime<float> {};\nvar errors = nodeTime<float> {};\n\nPolynomial::decompress(\n  original,\n  compressed,\n  0.1,\n  decompressed,\n  errors\n);\n\n// Validate compression quality\nfor (t, error in errors) {\n  if (error > 0.1) {\n    warn(\"High error at ${t}: ${error}\");\n  }\n}\n```\n\n### Complete Polynomial Compression Example\n\n```typescript\n// Simulate sensor data with noise\nvar sensor_data = nodeTime<float> {};\nfor (hour in 0..24*30) { // 30 days of hourly data\n  var t = time::new(2024, 1, 1, 0, 0, 0) + duration::new(hour, DurationUnit::hours);\n  var value = sin(hour * 0.1) * 10 + random_noise();\n  sensor_data.setAt(t, value);\n}\n\n// Compress\nvar compressed = nodeTime<Polynomial> {};\nPolynomial::compress(sensor_data, compressed, 5, 0.5, 100);\n\nprintln(\"Original size: ${sensor_data.size()} points\");\nprintln(\"Compressed size: ${compressed.size()} polynomials\");\nprintln(\"Compression ratio: ${sensor_data.size() / compressed.size()}x\");\n\n// Decompress and validate\nvar decompressed = nodeTime<float> {};\nvar errors = nodeTime<float> {};\nPolynomial::decompress(sensor_data, compressed, 0.5, decompressed, errors);\n\n// Calculate average error\nvar total_error = 0.0;\nfor (_, error in errors) {\n  total_error = total_error + error;\n}\nprintln(\"Average error: ${total_error / errors.size()}\");\n```\n\n---\n\n## TimeSeriesDecomposition\n\nTime series aggregation and decomposition utilities.\n\n### Description\n\nProvides methods to decompose high-frequency time series into lower-frequency aggregates (hourly → daily → weekly → monthly → yearly).\n\n### Static Methods\n\nAll methods are static and work with `nodeTime` structures.\n\n#### calculate(originalTS: nodeTime<any>, destinationTS: nodeTime<any>, tz: TimeZone, calendarUnit: CalendarUnit, lastUpdatedTime: time?)\nAggregates time series to coarser granularity.\n\n**Parameters:**\n- `originalTS`: High-frequency source time series\n- `destinationTS`: Low-frequency destination time series\n- `tz`: TimeZone for calendar calculations\n- `calendarUnit`: Aggregation unit (hour, day, month, year)\n- `lastUpdatedTime`: Optional - only update from this time forward\n\n**Example:**\n```typescript\nvar hourly = nodeTime<float> {};\nvar daily = nodeTime<float> {};\n\n// Aggregate hourly to daily\nTimeSeriesDecomposition::calculate(\n  hourly,\n  daily,\n  TimeZone::Europe_Paris,\n  CalendarUnit::day,\n  null  // Process all data\n);\n```\n\n#### calculateWeekly(originalTS: nodeTime<any>, destinationTS: nodeTime<any>, tz: TimeZone, lastUpdatedTime: time?)\nAggregates to weekly granularity (special handling for week boundaries).\n\n#### calculateWithFactor(originalTS: nodeTime<any>, destinationTS: nodeTime<any>, tz: TimeZone, calendarUnit: CalendarUnit, factor: float, lastUpdatedTime: time?)\nAggregates with a multiplication factor.\n\n**Use case:** Unit conversion during aggregation (e.g., Wh → kWh with factor 0.001)\n\n#### calculateAll(instant: nodeTime<any>, hourly: nodeTime<any>?, daily: nodeTime<any>?, weekly: nodeTime<any>?, monthly: nodeTime<any>?, yearly: nodeTime<any>?, tz: TimeZone, lastUpdatedTime: time?)\nCascading aggregation from instant to all time scales.\n\n**Parameters:**\n- `instant`: High-frequency source (e.g., per-minute)\n- `hourly`, `daily`, `weekly`, `monthly`, `yearly`: Optional destination time series\n- `tz`: TimeZone\n- `lastUpdatedTime`: Optional incremental update\n\n**Example:**\n```typescript\nvar instant = nodeTime<float> {};  // Per-minute data\nvar hourly = nodeTime<float> {};\nvar daily = nodeTime<float> {};\nvar monthly = nodeTime<float> {};\n\nTimeSeriesDecomposition::calculateAll(\n  instant,\n  hourly,\n  daily,\n  null,    // No weekly\n  monthly,\n  null,    // No yearly\n  TimeZone::UTC,\n  null     // Process all\n);\n```\n\n#### reset(ts: nodeTime<any>, tz: TimeZone, calendarUnit: CalendarUnit, lastUpdatedTime: time?)\nResets time series values to zero.\n\n#### resetWeekly(ts: nodeTime<any>, tz: TimeZone, lastUpdatedTime: time?)\nResets weekly time series.\n\n### Complete Energy Monitoring Example\n\n```typescript\n// Energy consumption monitoring with multiple granularities\nvar instant_consumption = nodeTime<float> {};  // kWh per minute\nvar hourly_consumption = nodeTime<float> {};\nvar daily_consumption = nodeTime<float> {};\nvar monthly_consumption = nodeTime<float> {};\n\n// Simulate 30 days of per-minute data\nvar start_time = time::new(2024, 1, 1, 0, 0, 0);\nfor (minute in 0..30*24*60) {\n  var t = start_time + duration::new(minute, DurationUnit::minutes);\n  var consumption = random() * 2.0; // 0-2 kWh per minute\n  instant_consumption.setAt(t, consumption);\n}\n\n// Decompose to all granularities\nTimeSeriesDecomposition::calculateAll(\n  instant_consumption,\n  hourly_consumption,\n  daily_consumption,\n  null,  // Skip weekly\n  monthly_consumption,\n  null,  // Skip yearly\n  TimeZone::Europe_Paris,\n  null\n);\n\n// Report\nprintln(\"Total consumption (30 days): ${sum(monthly_consumption)} kWh\");\nprintln(\"Average daily consumption: ${avg(daily_consumption)} kWh\");\nprintln(\"Peak hour: ${max_time(hourly_consumption)}\");\n\n// Incremental update (add new day)\nvar last_update = time::new(2024, 1, 30, 23, 59, 59);\n// ... add new minute data ...\nTimeSeriesDecomposition::calculateAll(\n  instant_consumption,\n  hourly_consumption,\n  daily_consumption,\n  null,\n  monthly_consumption,\n  null,\n  TimeZone::Europe_Paris,\n  last_update  // Only update from this time\n);\n```\n\n---\n\n## Best Practices\n\n### Data Normalization\n\n```typescript\n// Always normalize before ML\nvar gaussian = GaussianND {};\ngaussian.learn(train_data);\n\n// Choose normalization method:\n// - Min-Max: When you need bounded [0,1] range\nvar normalized = gaussian.min_max_scaling(data);\n\n// - Standard: When you need zero mean, unit variance (better for most ML)\nvar normalized = gaussian.standard_scaling(data);\n```\n\n### PCA Guidelines\n\n```typescript\n// 1. Always standardize before PCA\nvar gaussian = GaussianND {};\ngaussian.learn(data);\nvar standardized = gaussian.standard_scaling(data);\n\n// 2. Then learn PCA\nvar pca = PCA {};\npca.learn(gaussian.correlation(), gaussian.avg(), gaussian.std(), 0.95);\n\n// 3. Choose dimensions based on variance retained\nvar dim_90 = pca.get_dimension(0.90);  // Faster, less accurate\nvar dim_95 = pca.get_dimension(0.95);  // Balanced\nvar dim_99 = pca.get_dimension(0.99);  // Slower, more accurate\n```\n\n### Time Series Compression\n\n```typescript\n// Choose parameters based on data characteristics:\n// - maxDegree: Higher for complex patterns (3-7 typical)\n// - maxError: Based on acceptable data loss (1-5% of signal range)\n// - maxBufferSize: Larger for smoother signals (100-1000)\n\nPolynomial::compress(\n  sensor_data,\n  compressed,\n  maxDegree: 5,           // Quintic polynomials\n  maxError: 0.1,          // 0.1 unit error tolerance\n  maxBufferSize: 500      // 500 points per polynomial\n);\n```\n\n## See Also\n\n- [Neural Networks (nn.md)](nn.md) - Use GaussianND for preprocessing\n- [Compute Engine (compute.md)](compute.md) - Low-level tensor operations\n- [README.md](README.md) - Complete library overview\n",
        "plugins/greycat/skills/greycat/references/algebra/nn.md": "# Neural Networks\n\nHigh-level neural network APIs for regression, classification, and autoencoders with comprehensive preprocessing, training, and inference capabilities.\n\n## Overview\n\nThe Neural Networks module provides complete ML workflows:\n- **RegressionNetwork**: Continuous value prediction\n- **ClassificationNetwork**: Categorical prediction with probabilities\n- **AutoEncoderNetwork**: Unsupervised encoding/decoding\n\nAll networks support:\n- Automatic preprocessing (min-max, standard, PCA scaling)\n- Automatic postprocessing (inverse scaling)\n- Dense, LSTM, and custom layers\n- Multiple optimizers and loss functions\n- State management for model persistence\n\n## Network Types\n\n| Type | Use Case | Output | Loss Functions |\n|------|----------|--------|----------------|\n| `RegressionNetwork` | Predicting continuous values | Float tensor | Square, Absolute |\n| `ClassificationNetwork` | Predicting categories | Class probabilities | Categorical/Sparse cross-entropy |\n| `AutoEncoderNetwork` | Dimensionality reduction, denoising | Reconstruction | Square, Absolute |\n\n---\n\n## RegressionNetwork\n\nPredicts continuous values (e.g., price, temperature, stock value).\n\n### Creation\n\n```typescript\nstatic fn new(\n  inputs: int,                  // Number of input features\n  outputs: int,                 // Number of output values\n  tensor_type: TensorType,      // f32 or f64\n  inputs_gradients: bool?,      // Track input gradients (default: false)\n  fixed_batch_size: int?,       // 0 for dynamic (default)\n  seed: int?                    // Random seed (default: random)\n): RegressionNetwork\n```\n\n**Example:**\n```typescript\nvar nn = RegressionNetwork::new(\n  inputs: 10,                   // 10 features\n  outputs: 1,                   // 1 predicted value\n  tensor_type: TensorType::f32,\n  inputs_gradients: false,\n  fixed_batch_size: 32,\n  seed: 42\n);\n```\n\n### Building the Network\n\n#### addLinearLayer(output: int, use_bias: bool, config: InitializerConfig?)\nAdds linear transformation layer.\n\n```typescript\nnn.addLinearLayer(64, true, null);\n```\n\n#### addDenseLayer(output: int, use_bias: bool, activation: ComputeActivation?, config: InitializerConfig?)\nAdds dense layer with activation.\n\n```typescript\nnn.addDenseLayer(\n  128,                           // 128 neurons\n  true,                          // Use bias\n  ComputeActivationRelu {},      // ReLU activation\n  null                           // Default initializers\n);\n```\n\n#### addLSTMLayer(output: int, layers: int, sequences: int, use_bias: bool, return_sequences: bool, bidirectional: bool, config: InitializerConfig?)\nAdds LSTM recurrent layer.\n\n```typescript\nnn.addLSTMLayer(\n  output: 64,                    // 64 hidden units\n  layers: 2,                     // 2-layer stacked LSTM\n  sequences: 10,                 // 10 timesteps\n  use_bias: true,\n  return_sequences: false,       // Return only last output\n  bidirectional: true,           // Bidirectional LSTM\n  config: null\n);\n```\n\n### Configuration\n\n#### setPreProcess(preProcess: PreProcessType, object: any?)\nSets input preprocessing (normalization).\n\n```typescript\nvar gaussian = GaussianND {};\ngaussian.learn(training_data);\n\n// Standard scaling (recommended)\nnn.setPreProcess(PreProcessType::standard_scaling, gaussian);\n\n// Or min-max scaling\nnn.setPreProcess(PreProcessType::min_max_scaling, gaussian);\n\n// Or PCA\nvar pca = PCA {};\n// ... configure pca ...\nnn.setPreProcess(PreProcessType::pca_scaling, pca);\n```\n\n#### setPostProcess(postProcess: PostProcessType, object: any?)\nSets output postprocessing.\n\n```typescript\nvar output_gaussian = GaussianND {};\noutput_gaussian.learn(training_targets);\nnn.setPostProcess(PostProcessType::standard_scaling, output_gaussian);\n```\n\n#### setOptimizer(optimizer: ComputeOptimizer?)\nSets the optimization algorithm.\n\n```typescript\nnn.setOptimizer(ComputeOptimizerAdam {\n  learning_rate: 0.001,\n  beta1: 0.9,\n  beta2: 0.999,\n  smooth_epsilon: 1e-07\n});\n```\n\n#### setLoss(loss_type: ComputeRegressionLoss?, reduction: ComputeReduction?)\nSets loss function.\n\n```typescript\nnn.setLoss(ComputeRegressionLoss::square, ComputeReduction::mean);\n// Or\nnn.setLoss(ComputeRegressionLoss::abs, ComputeReduction::mean);\n```\n\n### Training\n\n#### build(learningMode: bool): ComputeModel\nBuilds the computational graph.\n\n```typescript\nvar model = nn.build(true); // true = training mode\n```\n\n#### initWithBatch(model: ComputeModel?, engine: ComputeEngine, state: ComputeState?, batch: int): int\nInitializes with fixed batch size.\n\n```typescript\nvar engine = ComputeEngine {};\nvar max_batch = nn.initWithBatch(null, engine, null, 32);\n```\n\n#### initWithMemory(model: ComputeModel?, engine: ComputeEngine, state: ComputeState?, maxMemory: int): int\nInitializes with memory constraint.\n\n```typescript\nvar engine = ComputeEngine {};\nvar batch_size = nn.initWithMemory(null, engine, null, 100 * 1024 * 1024); // 100MB\n```\n\n#### getInput(engine: ComputeEngine): Tensor\nGets input tensor to fill with data.\n\n```typescript\nnn.getInput(engine).fill(X_batch);\n```\n\n#### getTarget(engine: ComputeEngine): Tensor\nGets target tensor to fill with labels.\n\n```typescript\nnn.getTarget(engine).fill(y_batch);\n```\n\n#### train(engine: ComputeEngine): Tensor\nExecutes one training step (forward + backward + optimize).\n\n```typescript\nvar loss = nn.train(engine);\n```\n\n#### miniBatch(engine: ComputeEngine): Tensor\nExecutes forward + backward (no optimization).\n\n```typescript\nvar loss = nn.miniBatch(engine); // Accumulate gradients\n```\n\n#### optimize(engine: ComputeEngine)\nApplies accumulated gradients.\n\n```typescript\nnn.optimize(engine);\n```\n\n#### validation(engine: ComputeEngine): Tensor\nComputes loss without training.\n\n```typescript\nvar val_loss = nn.validation(engine);\n```\n\n#### endEpoch(engine: ComputeEngine)\nSignals end of epoch.\n\n```typescript\nnn.endEpoch(engine);\n```\n\n### Inference\n\n#### initForPrediction(model: ComputeModel?, engine: ComputeEngine, state: ComputeState, batchSize: int)\nInitializes for inference (no gradients).\n\n```typescript\nvar pred_engine = ComputeEngine {};\nnn.initForPrediction(null, pred_engine, trained_state, 1);\n```\n\n#### predict(engine: ComputeEngine, input: Tensor?): Tensor\nMakes predictions.\n\n```typescript\nvar predictions = nn.predict(engine, test_data);\n```\n\n#### getPrediction(engine: ComputeEngine): Tensor\nGets last prediction (after calling predict with null input).\n\n```typescript\nnn.getInput(engine).fill(X_test);\nvar _ = nn.predict(engine, null);\nvar predictions = nn.getPrediction(engine);\n```\n\n### Complete Regression Example\n\n```typescript\n// 1. Prepare data\nvar gaussian = GaussianND {};\ngaussian.learn(X_train);\n\n// 2. Create network\nvar nn = RegressionNetwork::new(10, 1, TensorType::f32, false, 32, 42);\n\n// 3. Configure\nnn.setPreProcess(PreProcessType::standard_scaling, gaussian);\nnn.addDenseLayer(64, true, ComputeActivationRelu {}, null);\nnn.addDenseLayer(32, true, ComputeActivationRelu {}, null);\nnn.addDenseLayer(1, true, null, null);\nnn.setOptimizer(ComputeOptimizerAdam {});\nnn.setLoss(ComputeRegressionLoss::square, ComputeReduction::mean);\n\n// 4. Train\nvar engine = ComputeEngine {};\nnn.initWithBatch(nn.build(true), engine, null, 32);\n\nfor (epoch in 0..100) {\n  nn.getInput(engine).fill(X_train);\n  nn.getTarget(engine).fill(y_train);\n  var loss = nn.train(engine);\n\n  if (epoch % 10 == 0) {\n    println(\"Epoch ${epoch}: Loss = ${loss}\");\n  }\n}\n\n// 5. Save state\nvar state = ComputeState {};\nengine.saveState(state);\n\n// 6. Predict\nvar predictions = nn.predict(engine, X_test);\n```\n\n---\n\n## ClassificationNetwork\n\nPredicts categorical classes with probabilities.\n\n### Creation\n\n```typescript\nstatic fn new(\n  inputs: int,                      // Input features\n  classes: int,                     // Number of classes\n  tensor_type: TensorType,\n  inputs_gradients: bool?,\n  fixed_batch_size: int?,\n  seed: int?,\n  calculate_probabilities: bool,   // Return probabilities\n  from_logits: bool,                // Input is logits (pre-softmax)\n  has_class_weights: bool           // Use class weights\n): ClassificationNetwork\n```\n\n**Example:**\n```typescript\nvar nn = ClassificationNetwork::new(\n  inputs: 784,                      // 28x28 flattened image\n  classes: 10,                      // 10 digit classes\n  tensor_type: TensorType::f32,\n  inputs_gradients: false,\n  fixed_batch_size: 32,\n  seed: 42,\n  calculate_probabilities: true,   // Return probabilities\n  from_logits: true,                // Raw scores from network\n  has_class_weights: false          // No class weights\n);\n```\n\n### Additional Methods\n\n#### setLoss(loss_type: ComputeClassificationLoss?, reduction: ComputeReduction?)\nSets classification loss.\n\n```typescript\n// Sparse (integer labels)\nnn.setLoss(\n  ComputeClassificationLoss::sparse_categorical_cross_entropy,\n  ComputeReduction::mean\n);\n\n// Categorical (one-hot encoded labels)\nnn.setLoss(\n  ComputeClassificationLoss::categorical_cross_entropy,\n  ComputeReduction::mean\n);\n```\n\n#### getPrediction(engine: ComputeEngine): Tensor\nGets predicted class indices.\n\n```typescript\nvar classes = nn.getPrediction(engine); // [batch, 1] - class indices\n```\n\n#### getProbability(engine: ComputeEngine): Tensor\nGets class probabilities (if calculate_probabilities=true).\n\n```typescript\nvar probs = nn.getProbability(engine); // [batch, classes]\n```\n\n#### getClassWeights(engine: ComputeEngine): Tensor\nGets/sets class weights for imbalanced datasets.\n\n```typescript\nvar weights = nn.getClassWeights(engine);\nweights.fill(class_weight_tensor);\n```\n\n#### getConfusion(engine: ComputeEngine): Tensor\nGets confusion matrix.\n\n```typescript\nvar confusion = nn.getConfusion(engine); // [classes, classes]\n```\n\n#### resetConfusion(engine: ComputeEngine)\nResets confusion matrix.\n\n```typescript\nnn.resetConfusion(engine);\n```\n\n### Static Methods\n\n#### getClassificationMetrics(confusionMatrix: Tensor): ClassificationMetrics\nComputes precision, recall, F1-score from confusion matrix.\n\n```typescript\nvar metrics = ClassificationNetwork::getClassificationMetrics(confusion);\nprintln(\"Precision: ${metrics.precision}\");\nprintln(\"Recall: ${metrics.recall}\");\nprintln(\"F1-Score: ${metrics.f1Score}\");\n```\n\n**Returns:**\n```typescript\ntype ClassificationMetrics {\n  precision: Array<float?>;  // Per-class + average\n  recall: Array<float?>;     // Per-class + average\n  f1Score: Array<float?>;    // Per-class + average\n}\n```\n\n#### getDefaultClassWeights(classDistribution: Array<int>, normalize: bool): Array<float>\nComputes class weights for imbalanced datasets.\n\n```typescript\nvar class_dist = Array<int> {100, 500, 50}; // Class counts\nvar weights = ClassificationNetwork::getDefaultClassWeights(class_dist, true);\n// Returns: higher weights for rare classes\n```\n\n### Complete Classification Example\n\n```typescript\n// 1. Create network\nvar nn = ClassificationNetwork::new(\n  784, 10, TensorType::f32, false, 32, 42, true, true, false\n);\n\n// 2. Build architecture\nnn.addDenseLayer(128, true, ComputeActivationRelu {}, null);\nnn.addDenseLayer(64, true, ComputeActivationRelu {}, null);\nnn.addDenseLayer(10, true, null, null); // Output logits\n\n// 3. Configure\nnn.setOptimizer(ComputeOptimizerAdam {});\nnn.setLoss(\n  ComputeClassificationLoss::sparse_categorical_cross_entropy,\n  ComputeReduction::mean\n);\n\n// 4. Train\nvar engine = ComputeEngine {};\nnn.initWithBatch(nn.build(true), engine, null, 32);\n\nfor (epoch in 0..epochs) {\n  nn.resetConfusion(engine);\n\n  for (batch_X, batch_y in training_data) {\n    nn.getInput(engine).fill(batch_X);\n    nn.getTarget(engine).fill(batch_y);\n    var loss = nn.train(engine);\n  }\n\n  // Validation\n  for (val_X, val_y in validation_data) {\n    nn.getInput(engine).fill(val_X);\n    nn.getTarget(engine).fill(val_y);\n    var val_loss = nn.validation(engine);\n  }\n\n  // Metrics\n  var confusion = nn.getConfusion(engine);\n  var metrics = ClassificationNetwork::getClassificationMetrics(confusion);\n  println(\"Epoch ${epoch}: F1=${metrics.f1Score[metrics.f1Score.size()-1]}\");\n}\n\n// 5. Inference\nvar probs = nn.predict(engine, test_X);    // Probabilities\nvar classes = nn.getPrediction(engine);    // Class indices\n```\n\n---\n\n## AutoEncoderNetwork\n\nUnsupervised learning for encoding/decoding data.\n\n### Creation\n\n```typescript\nstatic fn new(\n  inputs: int,              // Input/output size (same)\n  tensor_type: TensorType,\n  inputs_gradients: bool?,\n  fixed_batch_size: int?,\n  seed: int?\n): AutoEncoderNetwork\n```\n\n**Example:**\n```typescript\nvar nn = AutoEncoderNetwork::new(\n  inputs: 784,              // 28x28 image\n  tensor_type: TensorType::f32,\n  inputs_gradients: false,\n  fixed_batch_size: 32,\n  seed: 42\n);\n```\n\n### Configuration\n\n#### setEncoderLayer(layerIndex: int)\nSets which layer is the encoding (latent) representation.\n\n```typescript\n// Add 3 encoder layers + 3 decoder layers\nnn.addDenseLayer(256, true, ComputeActivationRelu {}, null); // 0\nnn.addDenseLayer(128, true, ComputeActivationRelu {}, null); // 1\nnn.addDenseLayer(64, true, ComputeActivationRelu {}, null);  // 2 - encoding\nnn.addDenseLayer(128, true, ComputeActivationRelu {}, null); // 3\nnn.addDenseLayer(256, true, ComputeActivationRelu {}, null); // 4\nnn.addDenseLayer(784, true, null, null);                     // 5 - output\n\nnn.setEncoderLayer(2); // Layer 2 is the latent representation\n```\n\n### Training\n\nSame as RegressionNetwork, but target = input (reconstruction).\n\n```typescript\nnn.getInput(engine).fill(X_batch);\nnn.getTarget(engine).fill(X_batch); // Reconstruct input\nvar loss = nn.train(engine);\n```\n\n### Inference\n\n#### encode(engine: ComputeEngine, input: Tensor?): Tensor\nEncodes input to latent representation.\n\n```typescript\nvar encoded = nn.encode(engine, X_test); // [batch, 64]\n```\n\n#### decode(engine: ComputeEngine, input: Tensor?): Tensor\nDecodes latent representation to output.\n\n```typescript\nvar reconstructed = nn.decode(engine, encoded); // [batch, 784]\n```\n\n#### getEncoderInput(engine: ComputeEngine): Tensor\nGets encoder input tensor.\n\n#### getDecoderInput(engine: ComputeEngine): Tensor\nGets decoder input tensor.\n\n#### getEncoding(engine: ComputeEngine): Tensor\nGets latent encoding.\n\n#### getDecoding(engine: ComputeEngine): Tensor\nGets decoded output.\n\n### Complete AutoEncoder Example\n\n```typescript\n// 1. Create autoencoder\nvar nn = AutoEncoderNetwork::new(784, TensorType::f32, false, 32, 42);\n\n// 2. Build symmetric architecture\nnn.addDenseLayer(256, true, ComputeActivationRelu {}, null);\nnn.addDenseLayer(128, true, ComputeActivationRelu {}, null);\nnn.addDenseLayer(64, true, ComputeActivationRelu {}, null);  // Encoding\nnn.addDenseLayer(128, true, ComputeActivationRelu {}, null);\nnn.addDenseLayer(256, true, ComputeActivationRelu {}, null);\nnn.addDenseLayer(784, true, ComputeActivationSigmoid {}, null); // Output [0,1]\n\nnn.setEncoderLayer(2); // 64-dim encoding\n\n// 3. Train\nvar engine = ComputeEngine {};\nnn.initWithBatch(nn.build(true), engine, null, 32);\n\nfor (epoch in 0..epochs) {\n  for (X_batch in training_data) {\n    nn.getInput(engine).fill(X_batch);\n    nn.getTarget(engine).fill(X_batch); // Reconstruct input\n    var loss = nn.train(engine);\n  }\n}\n\n// 4. Use encoder\nvar encoded = nn.encode(engine, X_test);        // Compress\nvar reconstructed = nn.decode(engine, encoded); // Decompress\n\n// 5. Anomaly detection\nvar reconstruction_error = mse(X_test, reconstructed);\n// High error = anomaly\n```\n\n---\n\n## Advanced Features\n\n### LSTM Sequence Models\n\n```typescript\n// Time series prediction\nvar nn = RegressionNetwork::new(50, 1, TensorType::f32, false, 32, 42);\n\nnn.addLSTMLayer(\n  output: 128,\n  layers: 2,\n  sequences: 10,        // 10 timesteps\n  use_bias: true,\n  return_sequences: false,\n  bidirectional: true,\n  config: null\n);\nnn.addDenseLayer(64, true, ComputeActivationRelu {}, null);\nnn.addDenseLayer(1, true, null, null);\n\n// Input shape: [10, 32, 50] - (sequences, batch, features)\n// Output shape: [32, 1] - (batch, predictions)\n```\n\n### Multi-class Text Classification\n\n```typescript\nvar nn = ClassificationNetwork::new(\n  inputs: 512,          // Embedding dimension\n  classes: 20,          // 20 categories\n  tensor_type: TensorType::f32,\n  inputs_gradients: false,\n  fixed_batch_size: 64,\n  seed: 42,\n  calculate_probabilities: true,\n  from_logits: true,\n  has_class_weights: true // Handle class imbalance\n);\n\nnn.addLSTMLayer(256, 1, 100, true, false, true, null); // Bidirectional\nnn.addDenseLayer(128, true, ComputeActivationRelu {}, null);\nnn.addDenseLayer(20, true, null, null);\n\n// Set class weights\nvar class_counts = Array<int> {100, 500, 200, ...}; // Per class\nvar weights = ClassificationNetwork::getDefaultClassWeights(class_counts, true);\nnn.getClassWeights(engine).fill(weights_tensor);\n```\n\n### Transfer Learning\n\n```typescript\n// 1. Train base model\nvar base_nn = RegressionNetwork::new(...);\n// ... train base_nn ...\nvar base_state = ComputeState {};\nengine.saveState(base_state);\n\n// 2. Create fine-tuning model with similar architecture\nvar fine_tune_nn = RegressionNetwork::new(...);\n// ... same architecture ...\n\n// 3. Load pre-trained weights\nvar fine_tune_engine = ComputeEngine {};\nfine_tune_nn.initWithBatch(null, fine_tune_engine, base_state, 32);\n\n// 4. Fine-tune with lower learning rate\nfine_tune_nn.setOptimizer(ComputeOptimizerAdam { learning_rate: 0.0001 });\n// ... train on new task ...\n```\n\n---\n\n## Best Practices\n\n### Data Preparation\n```typescript\n// Always normalize inputs\nvar gaussian = GaussianND {};\ngaussian.learn(X_train);\nnn.setPreProcess(PreProcessType::standard_scaling, gaussian);\n```\n\n### Architecture Guidelines\n- Start simple, add complexity as needed\n- Use ReLU for hidden layers\n- Use 64-256 neurons per layer\n- Add dropout for regularization (via custom layers)\n\n### Training Tips\n```typescript\n// Early stopping\nvar best_loss = float::max;\nvar patience = 0;\nvar best_state = ComputeState {};\n\nfor (epoch in 0..max_epochs) {\n  var val_loss = nn.validation(engine);\n\n  if (val_loss < best_loss) {\n    best_loss = val_loss;\n    patience = 0;\n    engine.saveState(best_state);\n  } else {\n    patience++;\n    if (patience > 10) {\n      break; // Early stop\n    }\n  }\n}\n\n// Restore best\nengine.loadState(best_state);\n```\n\n### Learning Rate Scheduling\n```typescript\n// Reduce LR on plateau\nif (epoch > 0 && epoch % 30 == 0) {\n  optimizer.learning_rate = optimizer.learning_rate * 0.5;\n  nn.setOptimizer(optimizer);\n}\n```\n\n## See Also\n\n- [Compute Engine (compute.md)](compute.md) - Low-level operations\n- [Machine Learning (ml.md)](ml.md) - Preprocessing utilities\n- [README.md](README.md) - Library overview\n",
        "plugins/greycat/skills/greycat/references/algebra/patterns.md": "# Pattern Recognition\n\nTime series pattern detection algorithms including Euclidean distance, Dynamic Time Warping (DTW), FFT, and Symbolic Aggregate approXimation (SAX).\n\n## Overview\n\nThe Pattern Recognition module enables finding similar patterns in time series data using multiple detection algorithms:\n\n- **Euclidean**: Fast distance-based matching\n- **DTW**: Flexible time-aligned matching\n- **FFT**: Frequency domain analysis\n- **SAX**: Symbolic representation for large-scale search\n- **Random**: Baseline/testing detector\n\n## Quick Start\n\n```typescript\n// 1. Create detector\nvar detector = EuclideanPatternDetector {};\nvar engine = detector.getEngine(timeseries);\nengine.setState(PatternDetectionEngineState::new());\n\n// 2. Define patterns to search for\nengine.addPattern(\n  time::new(2024, 1, 1, 0, 0, 0),\n  time::new(2024, 1, 2, 0, 0, 0)\n);\n\n// 3. Find similar patterns\nengine.initScoring();\nengine.computeScores(null);\n\n// 4. Extract detections\nvar detections = engine.detect(PatternDetectionSensitivity {\n  threshold: 0.85,     // Similarity threshold\n  overlap: 0.2         // Maximum overlap between detections\n}, null);\n```\n\n## Type Index\n\n| Type | Purpose |\n|------|---------|\n| `PatternDetectionEngine` | Base engine for pattern detection |\n| `EuclideanPatternDetectionEngine` | Euclidean distance detector |\n| `DTWPatternDetectionEngine` | Dynamic Time Warping detector |\n| `FFTPatternDetectionEngine` | Frequency domain detector |\n| `SaxPatternDetectionEngine` | Symbolic detector |\n| `PatternDetectionEngineState` | Engine state management |\n| `Detection` / `OverlappingDetection` | Detection results |\n| `PatternDetectionSensitivity` | Detection parameters |\n\n## Enums\n\n### PatternDetectors\n```typescript\nenum PatternDetectors {\n  euclidean(\"Euclidean\");\n  fft(\"FFT\");\n  dtw(\"DTW\");\n  sax(\"SAX\");\n  random(\"Random\");\n}\n```\n\n### SamplingPolicy\n```typescript\nenum SamplingPolicy {\n  as_is(\"As-is\");              // Use original sampling\n  average_frequency(\"Average frequency\");\n  highest_frequency(\"Highest frequency\");\n}\n```\n\n### PatternNullStrategy\n```typescript\nenum PatternNullStrategy {\n  replace(\"Replace\");          // Replace with constant\n  interpolate(\"Interpolate\");  // Linear interpolation\n  previous(\"Previous\");        // Use previous non-null\n  next(\"Next\");                // Use next non-null\n  none(\"None\");                // Error on null\n}\n```\n\n### MatchingNormalisation\n```typescript\nenum MatchingNormalisation {\n  as_is(\"As-is\");                      // Raw values\n  shift(\"Vertical shift\");             // Remove mean\n  scaling(\"Vertical scaling\");         // Normalize by std\n  shift_and_scaling(\"Vertical shift and scaling\"); // Z-score\n}\n```\n\n---\n\n## PatternDetectionEngine (Abstract)\n\nBase class for all pattern detectors.\n\n### Common Fields\n\n```typescript\ntimeseries: nodeTime;                     // Time series to search\nstate: PatternDetectionEngineState?;      // Engine state\nnullStrategy: PatternNullStrategy?;       // How to handle nulls\nnullReplaceConstant: float?;              // Replacement value\nsamplingPolicy: SamplingPolicy?;          // Resampling policy\n```\n\n### Methods\n\n#### setState(newState: PatternDetectionEngineState)\nSets engine state.\n\n#### addPattern(from: time, to: time)\nAdds a pattern to search for.\n\n**Parameters:**\n- `from`: Pattern start time\n- `to`: Pattern end time\n\n**Example:**\n```typescript\nengine.addPattern(\n  time::new(2024, 1, 1, 12, 0, 0),\n  time::new(2024, 1, 1, 18, 0, 0)\n); // 6-hour pattern\n```\n\n#### setPatternsFromMarks(marks: nodeTime<bool?>)\nAdds patterns from boolean marks.\n\n**Format:**\n- `true`: Pattern start\n- `null` (or next `true`): Pattern end\n\n**Example:**\n```typescript\nvar marks = nodeTime<bool?> {};\nmarks.setAt(time::new(2024, 1, 1, 0, 0, 0), true);   // Start 1\nmarks.setAt(time::new(2024, 1, 2, 0, 0, 0), null);   // End 1\nmarks.setAt(time::new(2024, 2, 1, 0, 0, 0), true);   // Start 2\nmarks.setAt(time::new(2024, 2, 2, 0, 0, 0), null);   // End 2\n\nengine.setPatternsFromMarks(marks);\n```\n\n#### initScoring()\nInitializes scoring (detector-specific preparation).\n\n#### computeScores(path: String?): String?\nComputes similarity scores across time series.\n\n**Parameters:**\n- `path`: Optional path to save scores to GCB file (for large datasets)\n\n**Returns:** Path if file was written, null otherwise\n\n**Example:**\n```typescript\n// In-memory (small datasets)\nengine.computeScores(null);\n\n// File-based (large datasets)\nvar scores_path = engine.computeScores(\"/tmp/scores.gcb\");\n```\n\n#### detect(sensitivity: PatternDetectionSensitivity, path: String?): String?\nExtracts detections from scores.\n\n**Parameters:**\n- `sensitivity`: Detection thresholds\n- `path`: Optional path to save detections\n\n**Returns:** Path if file was written\n\n**Example:**\n```typescript\nvar detections_path = engine.detect(\n  PatternDetectionSensitivity {\n    threshold: 0.85,   // Minimum similarity (0-1)\n    overlap: 0.3       // Max overlap ratio (0-1)\n  },\n  null\n);\n```\n\n### score(index: int, pattern: TimeWindow, timeWindow: TimeWindow): float (abstract)\nComputes similarity score (implemented by subclasses).\n\n**Returns:** Score in range [0, 1] where 1 = perfect match\n\n---\n\n## EuclideanPatternDetectionEngine\n\nFast distance-based pattern detection.\n\n### Description\n\nUses Euclidean distance in time-aligned windows. Fast and intuitive but requires aligned patterns.\n\n### Additional Fields\n\n```typescript\npattern_tensors: Array<Tensor>?;          // Precomputed pattern tensors\nwindow_tensors: Array<Tensor>?;           // Reusable window tensors\nstd: float?;                              // Time series standard deviation\nmatchingNormalisation: MatchingNormalisation?; // Normalization mode\n```\n\n### Score Formula\n\n```typescript\n// Without normalization:\nscore = 1 - euclidean_distance / (std * pattern_size)\n\n// With normalization:\nscore = 1 - euclidean_distance / pattern_size\n```\n\n### Example\n\n```typescript\nvar detector = EuclideanPatternDetector {};\nvar engine = detector.getEngine(timeseries) as EuclideanPatternDetectionEngine;\n\n// Configure\nengine.setState(PatternDetectionEngineState::new());\nengine.matchingNormalisation = MatchingNormalisation::shift_and_scaling;\nengine.nullStrategy = PatternNullStrategy::interpolate;\n\n// Add patterns\nengine.addPattern(pattern_start, pattern_end);\n\n// Detect\nengine.initScoring();\nengine.computeScores(null);\nvar detections = engine.state?.detections;\n```\n\n---\n\n## DTWPatternDetectionEngine\n\nDynamic Time Warping for flexible time-aligned matching.\n\n### Description\n\nAllows patterns to match even with time distortions (compression/stretching). More robust but slower than Euclidean.\n\n### Additional Fields\n\n```typescript\nstd: float?;                              // Time series standard deviation\nmatchingNormalisation: MatchingNormalisation?;\n```\n\n### DTW Algorithm\n\nComputes optimal alignment using dynamic programming:\n\n```\nDTW[i,j] = distance(pattern[i], window[j]) +\n           min(DTW[i-1,j], DTW[i,j-1], DTW[i-1,j-1])\n```\n\n### Example\n\n```typescript\nvar detector = DTWPatternDetector {};\nvar engine = detector.getEngine(timeseries) as DTWPatternDetectionEngine;\nengine.setState(PatternDetectionEngineState::new());\nengine.matchingNormalisation = MatchingNormalisation::shift_and_scaling;\n\nengine.addPattern(pattern_start, pattern_end);\nengine.initScoring();\nengine.computeScores(null);\n```\n\n### Performance Note\n\nDTW is O(n*m) where n = pattern length, m = window length. Use for:\n- Patterns with variable speed\n- When alignment is uncertain\n- Smaller datasets (< 10k points)\n\n---\n\n## FFTPatternDetectionEngine\n\nFrequency domain pattern detection.\n\n### Description\n\nConverts patterns and windows to frequency domain using FFT, then compares frequency components. Robust to amplitude shifts.\n\n### Additional Fields\n\n```typescript\nlow_pass_filter_ratio: float;            // Frequency filter (0-1)\npattern_frequencies: Array<Tensor>?;     // Pattern FFT\nwindow_frequency: Tensor?;                // Window FFT\npattern_distance_profiles: Array<Gaussian<float>>?;\nresults: nodeTime<FFTResult>;             // Intermediate results\n```\n\n### Example\n\n```typescript\nvar detector = FFTPatternDetector {};\nvar engine = detector.getEngine(timeseries) as FFTPatternDetectionEngine;\nengine.setState(PatternDetectionEngineState::new());\nengine.low_pass_filter_ratio = 0.9; // Retain 90% of frequency energy\n\nengine.addPattern(pattern_start, pattern_end);\nengine.initScoring();\nengine.computeScores(null);\n```\n\n### Use Cases\n\n- Periodic patterns (daily/weekly cycles)\n- Patterns with phase shifts\n- Noise-robust matching\n\n---\n\n## SaxPatternDetectionEngine\n\nSymbolic Aggregate approXimation for efficient large-scale search.\n\n### Description\n\nConverts time series to symbolic strings, enabling fast approximate search. Ideal for very large datasets.\n\n### Additional Fields\n\n```typescript\nalphabet_size: int;                      // Number of symbols (2-62)\nalphabet_boundaries: Array<float>;       // Symbol boundaries\nfingerprint_length: int;                 // Reduced resolution\npattern_fingerprints: Array<String>;     // Pattern symbols\nlookup_table: nodeIndex<char, nodeIndex<char, float>>; // Distance table\n```\n\n### Creation\n\n```typescript\nvar detector = SaxPatternDetector {\n  alphabet_size: 10,        // 10 symbols\n  fingerprint_length: 20    // 20 points per pattern\n};\n```\n\n### SAX Process\n\n1. **PAA (Piecewise Aggregate Approximation)**: Reduce resolution\n   - Pattern → `fingerprint_length` segments\n   - Each segment = average value\n\n2. **Discretization**: Map to symbols\n   - Values → symbols based on histogram percentiles\n\n3. **Distance**: Compare symbolic strings using lookup table\n\n### Example\n\n```typescript\nvar detector = SaxPatternDetector {\n  alphabet_size: 10,\n  fingerprint_length: 20\n};\nvar engine = detector.getEngine(timeseries) as SaxPatternDetectionEngine;\nengine.setState(PatternDetectionEngineState::new());\n\nengine.addPattern(pattern_start, pattern_end);\nengine.initScoring();\nengine.computeScores(null);\n```\n\n### Use Cases\n\n- Very large time series (> 1M points)\n- Approximate search\n- Real-time detection\n\n---\n\n## PatternDetectionEngineState\n\nManages detector state and results.\n\n### Fields\n\n```typescript\nhasScores: bool;                          // Scores computed?\nhasDetections: bool;                      // Detections extracted?\npatterns: Array<TimeWindow>;              // Defined patterns\nscores: nodeList<any?>;                   // Computed scores\ndetections: nodeTime<OverlappingDetection>; // Extracted detections\n```\n\n### Methods\n\n#### resetPatterns()\nClears all patterns and scores.\n\n#### resetScores()\nClears scores and detections.\n\n#### resetDetections()\nClears detections only.\n\n**Example:**\n```typescript\nvar state = PatternDetectionEngineState::new();\nengine.setState(state);\n\n// After first run\nengine.resetScores(); // Re-compute with different parameters\n```\n\n---\n\n## Detection Types\n\n### Detection\n```typescript\nabstract type Detection {\n  score: float;            // Similarity score [0, 1]\n  best_pattern: int;       // Which pattern matched\n  timespan: duration;      // Detection duration\n}\n```\n\n### OverlappingDetection\n```typescript\ntype OverlappingDetection extends Detection {\n  overlap: duration;       // Overlap with other detections\n}\n```\n\n### Accessing Detections\n\n```typescript\nfor (timestamp: time, detection: OverlappingDetection in engine.state?.detections?) {\n  println(\"Detection at ${timestamp}:\");\n  println(\"  Pattern: ${detection.best_pattern}\");\n  println(\"  Score: ${detection.score}\");\n  println(\"  Duration: ${detection.timespan}\");\n  println(\"  Overlap: ${detection.overlap}\");\n}\n```\n\n---\n\n## Complete Workflow Example\n\n```typescript\n// 1. Prepare time series\nvar sensor_data = nodeTime<float> {};\n// ... populate with sensor readings ...\n\n// 2. Choose detector\nvar detector = EuclideanPatternDetector {}; // Fast\n// var detector = DTWPatternDetector {};    // Flexible\n// var detector = FFTPatternDetector {};    // Periodic\n// var detector = SaxPatternDetector { alphabet_size: 10, fingerprint_length: 20 }; // Large-scale\n\nvar engine = detector.getEngine(sensor_data);\nengine.setState(PatternDetectionEngineState::new());\n\n// 3. Configure\nengine.nullStrategy = PatternNullStrategy::interpolate;\nengine.samplingPolicy = SamplingPolicy::average_frequency;\n\nif (engine is EuclideanPatternDetectionEngine) {\n  (engine as EuclideanPatternDetectionEngine).matchingNormalisation =\n    MatchingNormalisation::shift_and_scaling;\n}\n\n// 4. Define patterns (known anomalies, events, etc.)\nengine.addPattern(\n  time::new(2024, 1, 15, 10, 0, 0),\n  time::new(2024, 1, 15, 12, 0, 0)\n); // Morning spike pattern\n\nengine.addPattern(\n  time::new(2024, 2, 20, 14, 0, 0),\n  time::new(2024, 2, 20, 16, 0, 0)\n); // Afternoon dip pattern\n\n// 5. Compute scores\nengine.initScoring();\nengine.computeScores(null);\n\n// 6. Extract detections\nvar detections = engine.detect(\n  PatternDetectionSensitivity {\n    threshold: 0.85,    // High confidence only\n    overlap: 0.2        // Allow 20% overlap\n  },\n  null\n);\n\n// 7. Process results\nprintln(\"Found ${engine.state?.detections?.size()} detections\");\n\nfor (t: time, det: OverlappingDetection in engine.state?.detections?) {\n  println(\"${t}: Pattern ${det.best_pattern}, Score ${det.score}\");\n\n  // Take action\n  if (det.score > 0.95) {\n    // High confidence match\n    alert(\"Pattern detected at ${t}\");\n  }\n}\n```\n\n---\n\n## Detector Comparison\n\n| Detector | Speed | Flexibility | Memory | Best For |\n|----------|-------|-------------|--------|----------|\n| **Euclidean** | Fast | Low | Low | Aligned patterns, quick search |\n| **DTW** | Slow | High | Medium | Variable-speed patterns |\n| **FFT** | Medium | Medium | Medium | Periodic patterns, noise |\n| **SAX** | Fast | Low | Low | Large-scale approximate search |\n\n### Recommendations\n\n- **Start with Euclidean** for most cases\n- **Use DTW** if patterns have time distortions\n- **Use FFT** for periodic/frequency-based patterns\n- **Use SAX** for very large datasets (> 100k points)\n\n---\n\n## Advanced Usage\n\n### Multi-pattern Search\n\n```typescript\n// Add multiple patterns\nfor (known_pattern in known_patterns) {\n  engine.addPattern(known_pattern.start, known_pattern.end);\n}\n\n// Detections will identify which pattern matched\nfor (t, det in engine.state?.detections?) {\n  var pattern_name = pattern_names[det.best_pattern];\n  println(\"Matched pattern: ${pattern_name}\");\n}\n```\n\n### Sliding Window Detection\n\n```typescript\n// For real-time detection, incrementally add data\nvar window_size = 1_h; // 1 hour window\n\nfor (new_data_point in stream) {\n  sensor_data.setAt(new_data_point.time, new_data_point.value);\n\n  if (should_detect()) {\n    engine.resetScores();\n    engine.computeScores(null);\n    engine.detect(sensitivity, null);\n\n    // Check latest detection\n    var latest = engine.state?.detections?.last();\n    if (latest != null) {\n      handle_detection(latest);\n    }\n  }\n}\n```\n\n### Custom Sensitivity Tuning\n\n```typescript\n// Experiment with thresholds\nfor (threshold in [0.7, 0.8, 0.9]) {\n  engine.resetDetections();\n  engine.detect(\n    PatternDetectionSensitivity { threshold: threshold, overlap: 0.2 },\n    null\n  );\n\n  println(\"Threshold ${threshold}: ${engine.state?.detections?.size()} detections\");\n}\n```\n\n---\n\n## Performance Tips\n\n### For Large Datasets\n```typescript\n// Use file-based scoring\nvar scores_path = engine.computeScores(\"/tmp/scores.gcb\");\nvar detections_path = engine.detect(sensitivity, \"/tmp/detections.gcb\");\n```\n\n### Memory Management\n```typescript\n// Clear old detections\nengine.resetDetections();\n\n// Limit pattern count\n// More patterns = longer computation\n```\n\n### Sampling\n```typescript\n// Downsample for faster detection\nengine.samplingPolicy = SamplingPolicy::average_frequency;\n```\n\n## See Also\n\n- [Transforms (transforms.md)](transforms.md) - FFT operations\n- [README.md](README.md) - Library overview\n",
        "plugins/greycat/skills/greycat/references/algebra/transforms.md": "# FFT & Signal Processing\n\nFast Fourier Transform (FFT) operations for frequency domain analysis, signal processing, and time series forecasting.\n\n## Overview\n\nThe Transforms module provides:\n- **FFT**: Bidirectional Fast Fourier Transform\n- **FFTModel**: High-level interface for time series analysis\n- **Frequency Analysis**: Extract dominant frequencies and harmonics\n- **Extrapolation**: Forecast time series using frequency components\n\n## Type Index\n\n| Type | Purpose |\n|------|---------|\n| `FFT` | Core FFT engine (forward/inverse transforms) |\n| `FFTModel` | High-level time series FFT analysis and forecasting |\n\n---\n\n## FFT\n\nCore FFT engine for frequency domain transformations.\n\n### Static Fields\n\n```typescript\nstatic log_min: float = -6666.0; // Minimum log value for dB conversion\n```\n\n### Creation\n\n#### new(nb_samples: int, inverse: bool): FFT (static)\nCreates an FFT engine.\n\n**Parameters:**\n- `nb_samples`: Number of samples (should use `get_next_fast_size()`)\n- `inverse`: `false` for time→frequency, `true` for frequency→time\n\n**Example:**\n```typescript\nvar size = FFT::get_next_fast_size(1000); // Get optimal size ≥ 1000\nvar fft = FFT::new(size, false);          // Time to frequency\nvar ifft = FFT::new(size, true);          // Frequency to time\n```\n\n### Core Transform\n\n#### transform(timeseries_complex: Tensor, frequency_complex: Tensor)\nPerforms FFT transformation.\n\n**Parameters:**\n- `timeseries_complex`: Input tensor (complex128)\n- `frequency_complex`: Output tensor (complex128)\n\n**Direction:**\n- If `inverse=false`: time → frequency (forward FFT)\n- If `inverse=true`: frequency → time (inverse FFT)\n\n**Example:**\n```typescript\nvar time_data = Tensor {};\ntime_data.init(TensorType::c128, Array<int> {1024});\n// ... fill time_data ...\n\nvar freq_data = Tensor {};\nfft.transform(time_data, freq_data);\n```\n\n### High-Level Analysis\n\n#### transform_table(timeseries: Table, timeseries_complex: Tensor, frequency_complex: Tensor): Table\nTransforms a time series table and returns frequency analysis.\n\n**Parameters:**\n- `timeseries`: Input table with time series data\n- `timeseries_complex`: Working tensor for complex time data\n- `frequency_complex`: Working tensor for complex frequency data\n\n**Returns:** Table with columns:\n- `frequency` (Hz)\n- `period` (duration)\n- `frequency_abs_power`: Magnitude\n- `power_db`: Power in decibels\n- `phase_angle`: Phase (radians)\n- `cumulated_ratio`: Cumulative energy ratio\n\n**Example:**\n```typescript\nvar fft = FFT::new(1024, false);\nvar time_complex = Tensor {};\nvar freq_complex = Tensor {};\n\nvar freq_table = fft.transform_table(timeseries_table, time_complex, freq_complex);\n\n// Analyze dominant frequencies\nfor (row in 0..freq_table.rows()) {\n  var freq = freq_table.get_cell(row, 0);\n  var power = freq_table.get_cell(row, 2);\n  println(\"Frequency: ${freq} Hz, Power: ${power}\");\n}\n```\n\n### Frequency Analysis\n\n#### get_frequency_labels(sampling_step: duration, nb_samples: int): Tensor (static)\nReturns frequency labels for FFT output.\n\n**Parameters:**\n- `sampling_step`: Time between samples\n- `nb_samples`: Number of samples\n\n**Returns:** Tensor with frequencies in Hz\n\n**Example:**\n```typescript\nvar labels = FFT::get_frequency_labels(\n  duration::new(1, DurationUnit::seconds),\n  1024\n);\n```\n\n#### get_frequency_spectrum(frequency_complex: Tensor, frequency_spectrum: Tensor, to_decibel: bool, low_pass_filter: int?)\nExtracts frequency spectrum (magnitudes).\n\n**Parameters:**\n- `frequency_complex`: Complex frequency tensor\n- `frequency_spectrum`: Output spectrum tensor\n- `to_decibel`: Convert to dB (20*log10)\n- `low_pass_filter`: Optional - only return first N frequencies\n\n**Example:**\n```typescript\nvar spectrum = Tensor {};\nFFT::get_frequency_spectrum(freq_complex, spectrum, true, 100);\n// Returns first 100 frequencies in dB\n```\n\n#### get_frequency_table(frequency_complex: Tensor, sampling_step: duration): Table (static)\nExtracts detailed frequency analysis table.\n\n**Returns:** Table with frequency, power, phase, period, dB, cumulative ratio\n\n### Filtering\n\n#### apply_low_pass_filter(frequency_complex: Tensor, destination_frequency_complex: Tensor, low_pass_filter: int)\nApplies low-pass filter (retains low frequencies, zeros high frequencies).\n\n**Parameters:**\n- `frequency_complex`: Input frequency tensor\n- `destination_frequency_complex`: Filtered output\n- `low_pass_filter`: Number of frequencies to keep\n\n**Example:**\n```typescript\nvar filtered = Tensor {};\nFFT::apply_low_pass_filter(freq_data, filtered, 50); // Keep 50 lowest frequencies\n```\n\n#### get_low_pass_filter_size(frequency_complex: Tensor, ratio: float): int (static)\nDetermines filter size to retain a ratio of signal energy.\n\n**Parameters:**\n- `frequency_complex`: Frequency tensor\n- `ratio`: Energy ratio to retain (0.0-1.0)\n\n**Returns:** Number of frequencies to keep\n\n**Example:**\n```typescript\nvar filter_size = FFT::get_low_pass_filter_size(freq_data, 0.95);\n// Returns N frequencies that contain 95% of signal energy\n```\n\n### Extrapolation\n\n#### extrapolate(frequency_complex: Tensor, sampling_step: duration, timeseries_start: time, t: time, low_pass_filter: int?): float (static)\nPredicts a single value at time t.\n\n**Parameters:**\n- `frequency_complex`: Learned frequency representation\n- `sampling_step`: Original sampling step\n- `timeseries_start`: Start time of training data\n- `t`: Time to predict\n- `low_pass_filter`: Optional frequency limit\n\n**Returns:** Predicted value\n\n#### extrapolate_table(timeseries_complex: Tensor, sampling_step: duration, timeseries_start: time, from: time, to: time, skip_elements: int?): Table (static)\nPredicts multiple values over a time range.\n\n**Parameters:**\n- `timeseries_complex`: Time domain representation\n- `sampling_step`: Sampling step\n- `timeseries_start`: Training start time\n- `from`: Prediction start\n- `to`: Prediction end\n- `skip_elements`: Optional downsampling\n\n**Returns:** Table with predicted time series\n\n### Utility Methods\n\n#### get_next_fast_size(nb_samples: int): int (static)\nReturns next power-of-2 or highly composite number ≥ nb_samples for efficient FFT.\n\n**Example:**\n```typescript\nvar optimal = FFT::get_next_fast_size(1000); // Returns 1024\n```\n\n#### frequency_to_period(frequency: float): duration (static)\nConverts frequency (Hz) to period.\n\n**Returns:** `0_s` for DC component (frequency=0)\n\n#### period_to_frequency(period: duration): float (static)\nConverts period to frequency (Hz).\n\n#### power_to_db(power: float): float (static)\nConverts power to decibels (20*log10).\n\n**Returns:** `log_min` (-6666.0) for power ≤ 0\n\n---\n\n## FFTModel\n\nHigh-level interface for FFT-based time series analysis and forecasting.\n\n### Fields\n\n```typescript\nnt: nodeTime;              // Time series data\nsampling_step: duration;   // Time between samples\ntime_complex: Tensor;      // Time domain representation\nfrequency_complex: Tensor; // Frequency domain representation\nfrequency_table: Table;    // Frequency analysis\nstart_time: time;          // Training start time\nbest_size: int;            // Optimal FFT size\n```\n\n### Training\n\n#### train(nt: nodeTime, from: time, to: time): FFTModel (static)\nTrains FFT model on time series data.\n\n**Parameters:**\n- `nt`: Time series\n- `from`: Training start time\n- `to`: Training end time\n\n**Returns:** Trained FFTModel\n\n**Example:**\n```typescript\nvar sensor_data = nodeTime<float> {};\n// ... populate sensor_data ...\n\nvar model = FFTModel::train(\n  sensor_data,\n  time::new(2024, 1, 1, 0, 0, 0),\n  time::new(2024, 2, 1, 0, 0, 0)\n);\n\n// Analyze dominant frequencies\nvar freq_table = model.frequency_table;\n```\n\n### Forecasting\n\n#### extrapolate_value(t: time, low_pass_filter_ratio: float?, nb_harmonics: int?): float\nPredicts a single value.\n\n**Parameters:**\n- `t`: Time to predict\n- `low_pass_filter_ratio`: Energy ratio to use (0.0-1.0)\n- `nb_harmonics`: Number of harmonics to use\n\n**Example:**\n```typescript\n// Predict next hour\nvar next_hour = time::new(2024, 2, 1, 1, 0, 0);\nvar prediction = model.extrapolate_value(next_hour, 0.95, null);\n```\n\n#### extrapolate(from: time, to: time, low_pass_filter_ratio: float?, nb_harmonics: int?, skip_elements: int?): Table\nPredicts multiple values.\n\n**Parameters:**\n- `from`: Prediction start\n- `to`: Prediction end\n- `low_pass_filter_ratio`: Energy ratio\n- `nb_harmonics`: Number of harmonics\n- `skip_elements`: Downsampling factor\n\n**Returns:** Table with predictions\n\n**Example:**\n```typescript\nvar predictions = model.extrapolate(\n  time::new(2024, 2, 1, 0, 0, 0),\n  time::new(2024, 2, 8, 0, 0, 0),\n  0.95,    // Use 95% of energy\n  null,    // All harmonics\n  null     // No downsampling\n);\n```\n\n---\n\n## Complete Examples\n\n### Frequency Analysis\n\n```typescript\n// Analyze sensor data for dominant frequencies\nvar sensor_data = nodeTime<float> {};\n// ... populate with hourly readings for 30 days ...\n\nvar model = FFTModel::train(\n  sensor_data,\n  time::new(2024, 1, 1, 0, 0, 0),\n  time::new(2024, 1, 31, 0, 0, 0)\n);\n\n// Find dominant frequencies\nvar freq_table = model.frequency_table;\n\nprintln(\"Top 5 frequencies:\");\nfor (row in 0..min(5, freq_table.rows())) {\n  var freq = freq_table.get_cell(row, 0) as float;\n  var period = freq_table.get_cell(row, 4) as duration;\n  var power_db = freq_table.get_cell(row, 5) as float;\n\n  println(\"  ${freq} Hz (period: ${period}): ${power_db} dB\");\n}\n\n// Detect daily/weekly cycles\nfor (row in 0..freq_table.rows()) {\n  var period = freq_table.get_cell(row, 4) as duration;\n  var period_hours = period.to(DurationUnit::hours);\n\n  if (period_hours > 23 && period_hours < 25) {\n    println(\"Daily cycle detected!\");\n  } else if (period_hours > 167 && period_hours < 169) {\n    println(\"Weekly cycle detected!\");\n  }\n}\n```\n\n### Time Series Forecasting\n\n```typescript\n// Train on historical data\nvar electricity_usage = nodeTime<float> {};\n// ... 6 months of hourly data ...\n\nvar model = FFTModel::train(\n  electricity_usage,\n  time::new(2024, 1, 1, 0, 0, 0),\n  time::new(2024, 6, 30, 23, 0, 0)\n);\n\n// Forecast next week\nvar forecast = model.extrapolate(\n  time::new(2024, 7, 1, 0, 0, 0),\n  time::new(2024, 7, 7, 23, 0, 0),\n  0.95,    // 95% energy retention\n  10,      // Top 10 harmonics\n  null\n);\n\n// Store forecasts\nvar forecast_ts = nodeTime<float> {};\nfor (row in 0..forecast.rows()) {\n  var t = forecast.get_cell(row, 0) as time;\n  var value = forecast.get_cell(row, 1) as float;\n  forecast_ts.setAt(t, value);\n}\n```\n\n### Signal Filtering\n\n```typescript\n// Remove high-frequency noise\nvar noisy_signal = nodeTime<float> {};\n// ... populate ...\n\nvar model = FFTModel::train(noisy_signal, start, end);\n\n// Apply low-pass filter\nvar filter_size = FFT::get_low_pass_filter_size(\n  model.frequency_complex,\n  0.9  // Retain 90% of energy\n);\n\nvar filtered_freq = Tensor {};\nFFT::apply_low_pass_filter(\n  model.frequency_complex,\n  filtered_freq,\n  filter_size\n);\n\n// Inverse FFT to get filtered signal\nvar ifft = FFT::new(model.best_size, true);\nvar filtered_time = Tensor {};\nifft.transform(filtered_freq, filtered_time);\n\n// Extract filtered values\nvar filtered_signal = nodeTime<float> {};\n// ... convert tensor to nodeTime ...\n```\n\n### Harmonic Analysis\n\n```typescript\n// Analyze specific harmonics (e.g., daily, weekly patterns)\nvar model = FFTModel::train(temperature_data, start, end);\n\n// Get frequency table sorted by power\nvar freq_table = model.frequency_table;\n\n// Extract 1st harmonic (fundamental frequency)\nvar fundamental_freq = freq_table.get_cell(0, 0) as float;\nvar fundamental_period = freq_table.get_cell(0, 4) as duration;\n\n// Reconstruct using only fundamental\nvar fundamental_only = model.extrapolate(\n  start,\n  end,\n  null,\n  1,     // Only 1st harmonic\n  null\n);\n\n// Reconstruct using first 3 harmonics\nvar three_harmonics = model.extrapolate(\n  start,\n  end,\n  null,\n  3,     // 3 harmonics\n  null\n);\n```\n\n### Anomaly Detection\n\n```typescript\n// Use FFT reconstruction error for anomaly detection\nvar normal_data = nodeTime<float> {};\nvar test_data = nodeTime<float> {};\n\n// Train on normal data\nvar model = FFTModel::train(normal_data, start, end);\n\n// Reconstruct test data\nvar reconstructed = model.extrapolate(test_start, test_end, 0.95, null, null);\n\n// Compare with actual\nfor (row in 0..reconstructed.rows()) {\n  var t = reconstructed.get_cell(row, 0) as time;\n  var predicted = reconstructed.get_cell(row, 1) as float;\n  var actual = test_data.getAt(t);\n\n  var error = abs(predicted - actual);\n  var threshold = 3.0; // 3 sigma rule\n\n  if (error > threshold) {\n    println(\"Anomaly detected at ${t}: error=${error}\");\n  }\n}\n```\n\n---\n\n## Mathematical Background\n\n### FFT Formula\n\n**Forward FFT (time → frequency):**\n```\nX[k] = Σ(n=0 to N-1) x[n] * e^(-2πikn/N)\n```\n\n**Inverse FFT (frequency → time):**\n```\nx[n] = (1/N) * Σ(k=0 to N-1) X[k] * e^(2πikn/N)\n```\n\n### Frequency Resolution\n\n```\nΔf = 1 / (N * Δt)\n```\n\nWhere:\n- `Δf`: Frequency resolution (Hz)\n- `N`: Number of samples\n- `Δt`: Sampling period (seconds)\n\n### Nyquist Frequency\n\nMaximum detectable frequency:\n```\nf_max = 1 / (2 * Δt)\n```\n\n---\n\n## Performance Tips\n\n### Optimal Sample Size\n```typescript\n// Always use optimal sizes for faster FFT\nvar size = FFT::get_next_fast_size(actual_samples);\n// Pads data to next power-of-2 or highly composite number\n```\n\n### Memory Usage\n```typescript\n// Reuse tensors for multiple transforms\nvar time_data = Tensor {};\nvar freq_data = Tensor {};\ntime_data.init(TensorType::c128, Array<int> {1024});\nfreq_data.init(TensorType::c128, Array<int> {1024});\n\nfor (segment in segments) {\n  // Fill time_data with segment\n  fft.transform(time_data, freq_data);\n  // Process freq_data\n}\n```\n\n### Low-Pass Filtering\n```typescript\n// Reduce computation by filtering\nvar filter_size = FFT::get_low_pass_filter_size(freq_data, 0.9);\n// Typically 10-20% of original size for 90% energy\n```\n\n---\n\n## Use Cases\n\n### 1. Periodic Pattern Detection\n```typescript\n// Find daily/weekly/monthly cycles\nvar freq_table = model.frequency_table;\n// Examine dominant frequencies and their periods\n```\n\n### 2. Signal Denoising\n```typescript\n// Remove high-frequency noise\nFFT::apply_low_pass_filter(freq_data, filtered, filter_size);\n```\n\n### 3. Time Series Forecasting\n```typescript\n// Extrapolate based on learned frequencies\nvar forecast = model.extrapolate(future_start, future_end, 0.95, null, null);\n```\n\n### 4. Compression\n```typescript\n// Store only dominant harmonics\nvar compressed = model.extrapolate(start, end, null, 5, null);\n// 5 harmonics instead of all data points\n```\n\n### 5. Similarity Analysis\n```typescript\n// Compare frequency signatures of two time series\nvar model1 = FFTModel::train(ts1, start, end);\nvar model2 = FFTModel::train(ts2, start, end);\n\nvar distance = frequency_distance(\n  model1.frequency_complex,\n  model2.frequency_complex\n);\n```\n\n---\n\n## Best Practices\n\n### Sample Size Selection\n```typescript\n// Need enough samples for desired frequency resolution\nvar min_period = 1_h;  // Want to detect hourly patterns\nvar samples_needed = 24 * 7; // At least 1 week for stability\nvar optimal_size = FFT::get_next_fast_size(samples_needed);\n```\n\n### Windowing\n```typescript\n// For non-periodic signals, apply windowing to reduce spectral leakage\n// (Currently not built-in, apply manually to input data)\n```\n\n### Handling Gaps\n```typescript\n// FFT requires evenly sampled data\n// Use interpolation for gaps before FFT\n```\n\n## See Also\n\n- [Pattern Recognition (patterns.md)](patterns.md) - FFT-based pattern detection\n- [README.md](README.md) - Library overview\n",
        "plugins/greycat/skills/greycat/references/cli.md": "# GreyCat CLI Reference\n\nComplete CLI documentation. All projects use `project.gcl`. Run from project root.\n\n## Commands\n\n| Command | Purpose | Param | Exit |\n|---------|---------|-------|------|\n| `build` | Compile | - | 0 |\n| `run` | Execute | Name (def: main) | 0 |\n| `serve` | Start server | - | ∞ |\n| `test` | Run @test | - | 0=pass |\n| `install` | Download libs | - | 0 |\n| `print` | Show .gcb | Path | 0 |\n| `codegen` | Gen headers | - | 0 |\n| `bytecode` | Show bytecode | - | 0 |\n| `defrag` | Compact | - | 0 |\n| `build-version` | Project ver | - | 0 |\n| `build-version-full` | Ver+git | - | 0 |\n| `version` | GreyCat ver | - | 0 |\n\n## Common Options\n\nAll commands accept these options (CLI / Environment Variable):\n\n| Option | Env Var | Default | Description |\n|--------|---------|---------|-------------|\n| `--log=<level>` | `GREYCAT_LOG` | info | trace/debug/info/warn/error |\n| `--logfile` | `GREYCAT_LOGFILE` | false | Log to file |\n| `--cache=<MB>` | `GREYCAT_CACHE` | 8192 | Cache size in MB |\n| `--store=<MB>` | `GREYCAT_STORE` | 1000 | Store size in MB |\n| `--store_paths=<path>` | `GREYCAT_STORE_PATHS` | ./gcdata | Storage directory |\n| `--workers=<N>` | `GREYCAT_WORKERS` | 8 | Worker threads |\n| `--user=<id>` | `GREYCAT_USER` | 0 | User ID (1 bypasses auth, **DEV ONLY**) |\n| `--tz=<str>` | `GREYCAT_TZ` | - | Timezone (e.g., Europe/Luxembourg) |\n\n## greycat serve\n\nStart server with HTTP API + MCP.\n\n```bash\ngreycat serve                           # Default: port 8080, secure\ngreycat serve --port=3000 --workers=8   # Custom config\ngreycat serve --user=1 --unsecure      # Dev mode (UNSAFE!)\n```\n\n**Essential Options:**\n\n| Option | Env Var | Default | Description |\n|--------|---------|---------|-------------|\n| `--port=<N>` | `GREYCAT_PORT` | 8080 | HTTP port |\n| `--workers=<N>` | `GREYCAT_WORKERS` | 8 | Worker threads |\n| `--user=<id>` | `GREYCAT_USER` | 0 | **DEV ONLY**: bypass auth |\n| `--unsecure` | `GREYCAT_UNSECURE` | false | **DEV ONLY**: allow HTTP |\n| `--http_threads=<N>` | `GREYCAT_HTTP_THREADS` | 3 | HTTP handler threads |\n| `--req_workers=<N>` | `GREYCAT_REQ_WORKERS` | 2 | Request workers |\n| `--task_pool_capacity=<N>` | `GREYCAT_TASK_POOL_CAPACITY` | 10000 | Task queue size |\n| `--request_pool_capacity=<N>` | `GREYCAT_REQUEST_POOL_CAPACITY` | 512 | Request queue size |\n| `--defrag_ratio=<f64>` | `GREYCAT_DEFRAG_RATIO` | 1.00 | Defrag threshold (<0=off) |\n| `--webroot=<str>` | `GREYCAT_WEBROOT` | webroot | Static files path |\n| `--keep_alive` | `GREYCAT_KEEP_ALIVE` | false | HTTP keep-alive |\n| `--validity=<sec>` | `GREYCAT_VALIDITY` | 86400 | Token validity (seconds) |\n\n**Security/SSO Options:**\n\n| Option | Env Var | Description |\n|--------|---------|-------------|\n| `--key=<str>` | `GREYCAT_KEY` | Private key path |\n| `--keysafe=<str>` | `GREYCAT_KEYSAFE` | Key password |\n| `--oid_client_id=<str>` | `GREYCAT_OID_CLIENT_ID` | OpenID client ID |\n| `--oid_config_url=<str>` | `GREYCAT_OID_CONFIG_URL` | OpenID config URL |\n| `--oid_keys_url=<str>` | `GREYCAT_OID_KEYS_URL` | OpenID keys URL |\n| `--oid_public_key=<str>` | `GREYCAT_OID_PUBLIC_KEY` | OpenID public key |\n\n**Backup Options:**\n\n| Option | Env Var | Default | Description |\n|--------|---------|---------|-------------|\n| `--backup_path=<str>` | `GREYCAT_BACKUP_PATH` | backup | Backup directory |\n| `--max_backup_files=<N>` | `GREYCAT_MAX_BACKUP_FILES` | 3 | Max backup files |\n\n**Behavior**: Executes main() as task, serves /webroot/, enables /explorer (if @library(\"explorer\")), starts MCP server.\n\n**⚠️ Security**: `--user=<id>` bypasses ALL auth (NEVER prod). `--unsecure` allows HTTP (dev only).\n\n**SSO Example:**\n```bash\ngreycat serve --oid_client_id=abc123 --oid_config_url=https://login.microsoftonline.com/{TENANT}/v2.0/.well-known/openid-configuration\n```\n\n## Other Commands\n\n**greycat test** - Run all @test functions. Exit 0 if pass, non-zero if fail.\n```bash\ngreycat test\ngreycat test --log=debug\n```\nFinds all @test functions in `*_test.gcl`, runs `setup()` before, `teardown()` after. See [testing.md](testing.md).\n\n**greycat run** - Execute main() or specified function.\n```bash\ngreycat run             # main()\ngreycat run import_data # project::import_data()\ngreycat run --user=1    # As user ID 1\n```\n\n**greycat install** - Download libraries from @library directives in project.gcl.\n\n**greycat codegen** - Generate typed headers: TypeScript, Python, C, Rust. See [frontend.md](frontend.md).\n\n**greycat defrag** - Compact storage (atomic, safe anytime). When: After deleting data, optimize disk usage.\n```bash\ngreycat defrag\ngreycat defrag --store_paths=./gcdata\n```\n\n## greycat-lang Commands\n\n**greycat-lang lint** - Check GCL for errors. **⚠️ CRITICAL**: Always run after generating/modifying .gcl files. Exit 0=no errors, 1=errors found.\n```bash\ngreycat-lang lint\ngreycat-lang lint --project=./backend/project.gcl\n```\n\n**greycat-lang fmt** - Format GCL files in-place. Respects @format_indent, @format_line_width.\n\n**greycat-lang server** - Start LSP server for IDE integration. Features: Completion, go-to-definition, find-references, hover, diagnostics, formatting.\n```bash\ngreycat-lang server --stdio    # VS Code, Neovim\ngreycat-lang server --tcp=6008 # Network clients\n```\n\n## .env File Support\n\nGreyCat loads `.env` next to project.gcl. CLI options override env vars.\n\n**Dev .env:**\n```bash\nGREYCAT_PORT=3000\nGREYCAT_WORKERS=4\nGREYCAT_LOG=debug\nGREYCAT_USER=1       # UNSAFE - don't commit\nGREYCAT_UNSECURE=true # UNSAFE - don't commit\nGREYCAT_TZ=Europe/Luxembourg\n```\n\n**Prod .env:**\n```bash\nGREYCAT_PORT=8080\nGREYCAT_WORKERS=16\nGREYCAT_LOG=warn\nGREYCAT_LOGFILE=true\nGREYCAT_STORE=10000\nGREYCAT_CACHE=16384\nGREYCAT_STORE_PATHS=/var/lib/greycat/data\nGREYCAT_OID_CLIENT_ID=prod-client-id\nGREYCAT_OID_CONFIG_URL=https://login.microsoftonline.com/TENANT/v2.0/.well-known/openid-configuration\nGREYCAT_HTTP_THREADS=8\nGREYCAT_REQ_WORKERS=4\nGREYCAT_BACKUP_PATH=/var/backups/greycat\nGREYCAT_MAX_BACKUP_FILES=7\nGREYCAT_TZ=UTC\n```\n\n**⚠️ Security**: Always add `.env` to `.gitignore`. Never commit secrets. Never commit GREYCAT_USER or GREYCAT_UNSECURE=true for prod.\n\n## Common Workflows\n\n**Dev Server:** `greycat serve` (with .env: GREYCAT_USER=1, GREYCAT_UNSECURE=true, GREYCAT_LOG=debug)\n\n**CI/CD:** `greycat-lang lint && greycat test --log=info && greycat build-version-full > version.txt`\n\n**Prod Deploy:** `greycat build --log=warn && greycat test && greycat serve`\n\n**Data Reset (Dev):** `rm -rf gcdata && greycat run import` (**⚠️ DELETES DATA**)\n\n**Quality Check:** `greycat-lang lint && greycat-lang fmt && greycat test`\n\n## Troubleshooting\n\n**Build Failures**: `greycat build --log=debug && greycat-lang lint` — Check unresolved symbols, missing @include/@library directives.\n\n**Port In Use**: `lsof -i :8080` or use different port: `greycat serve --port=8081`\n\n**Auth Failures**: Dev: `--user=1 --unsecure`. Prod: verify OpenID config with `--log=debug`.\n\n**Performance**: Increase workers/cache: `greycat serve --workers=16 --cache=16384 --task_pool_capacity=20000`. Compact: `greycat defrag`.\n\n**LSP Not Working**: Verify `which greycat-lang && greycat-lang --version`. Test: `greycat-lang server --stdio`. Install plugin: `/install greycat-lsp`.\n\n**Storage Issues**: Check size: `du -sh gcdata/`. Compact: `greycat defrag`. Dev only: `rm -rf gcdata && greycat run import`.\n\n## Help\n\n```bash\ngreycat --help\ngreycat serve --help\ngreycat -v\ngreycat -vv  # Full version + git hash\n```\n\n## See Also\n\n- [testing.md](testing.md) - Comprehensive testing guide\n- [frontend.md](frontend.md) - TypeScript integration, codegen\n- [permissions.md](permissions.md) - Auth, @permission, OpenID\n- [concurrency.md](concurrency.md) - Jobs, async, PeriodicTask\n",
        "plugins/greycat/skills/greycat/references/concurrency.md": "# Concurrency & Tasks\n\n## Jobs (Parallel Execution)\n\nJobs enable parallel computation with fork-join pattern:\n\n```gcl\nfn long_computation(max: int): int {\n    var count = 0;\n    for (var i = 0; i < max; i++) { count++; }\n    return count;\n}\n\nfn main() {\n    var jobs = Array<Job>{\n        Job { function: project::long_computation, arguments: [100_000] },\n        Job { function: project::long_computation, arguments: [100_000] }\n    };\n\n    await(jobs);  // Blocks until all complete\n\n    for (_, job in jobs) { var result = job.result(); }\n}\n```\n\n> Jobs only run in parallel when executed within a task context.\n\n### Error Handling\n\n`await` throws if any job fails. Handle individually:\n\n```gcl\nfn main() {\n    var jobs = Array<Job>{\n        Job { function: foo, arguments: [10_s, false] },\n        Job { function: foo, arguments: [1_s, true] }  // will fail\n    };\n\n    try {\n        await(jobs);\n    } catch (err) {\n        for (i, job in jobs) {\n            var res = job.result();\n            if (res is Error) { println(\"Job ${i} failed\"); }\n            else { println(\"Job ${i} finished\"); }\n        }\n    }\n}\n```\n\n### Parallel Writes\n\nCan write to different nodes in parallel, but NOT to the same node:\n\n```gcl\nvar sensor_list: nodeList<node<Sensor>>;\n\nfn main() {\n    var jobs = Array<Job>{ Job { function: project::import }, Job { function: project::import } };\n    await(jobs);\n\n    // Aggregate results after await\n    for (_, job in jobs) {\n        var sensors = job.result();\n        for (_, sensor: node<Sensor> in sensors) { sensor_list.add(sensor); }\n    }\n}\n\nfn import(): Array<node<Sensor>> {\n    var sensors = Array<node<Sensor>>{};\n    for (var i = 0; i < 10; i++) {\n        sensors.add(node<Sensor>{ Sensor { history: nodeTime<int>{} }});\n    }\n    return sensors;\n}\n```\n\n### Await Limitations\n\nObjects resolved before `await` become invalid after:\n\n```gcl\n// ❌ Wrong - resolved_foo becomes stale after await\nfn task(foo: node<Foo>) {\n    var resolved_foo = foo.resolve();\n    await(jobs);\n    resolved_foo.status = \"Done\";  // ERROR\n}\n\n// ✅ Correct - use arrow operator or set to null before await\nfn task(foo: node<Foo>) {\n    var resolved_foo = foo.resolve();\n    resolved_foo = null;  // Clear before await\n    await(jobs);\n    foo->status = \"Done\";  // Use arrow operator\n}\n```\n\n## Tasks (Async HTTP)\n\nExecute functions asynchronously via HTTP header:\n\n```bash\ncurl -H \"task:''\" -X POST -d '[]' http://localhost:8080/project::long_computation\n# Returns Task object immediately\n```\n\n### Task Object\n\n| Field | Type | Description |\n|-------|------|-------------|\n| user_id | int | User who spawned task |\n| task_id | int | Unique task ID |\n| mod | String? | Task module |\n| fun | String? | Function name |\n| creation | time | When spawned |\n| start | time? | When started |\n| duration | duration? | How long |\n| status | TaskStatus | Current status |\n\n**Task Status**: `empty → waiting → running → await → ended/error/cancelled/ended_with_errors`\n\n**Check Status**:\n```bash\ncurl -X POST -d '[1,1]' http://localhost:8080/runtime::Task::info\n```\n\n**Retrieve Result**:\n```bash\ncurl -X GET 'http://localhost:8080/files/0/tasks/1/result.gcb?json'\n```\n\n## Periodic Tasks\n\nSchedule recurring tasks using the `Scheduler` API:\n\n```gcl\nfn my_task() { println(\"Current time: ${time::now()}\"); }\n\nfn main() {\n    // Daily at midnight\n    Scheduler::add(project::my_task, DailyPeriodicity {}, null);\n\n    // Fixed interval\n    Scheduler::add(project::my_task, FixedPeriodicity { every: 1_day }, PeriodicOptions { start: time::now() });\n}\n```\n\n### Periodicity Types\n\n| Type | Description | Example |\n|------|-------------|---------|\n| `FixedPeriodicity` | Fixed intervals | `{ every: 30min }`, `{ every: 2hour }` |\n| `DailyPeriodicity` | Daily at specific time | `{ hour: 14, minute: 30 }`, `{ hour: 9, timezone: TimeZone::\"Europe/Luxembourg\" }` |\n| `WeeklyPeriodicity` | Specific weekdays | `{ days: [DayOfWeek::Mon, DayOfWeek::Fri], daily: DailyPeriodicity { hour: 9 } }` |\n| `MonthlyPeriodicity` | Specific days of month | `{ days: [15], daily: DailyPeriodicity { hour: 14 } }` |\n| `YearlyPeriodicity` | Specific calendar dates | `{ dates: [DateTuple { day: 1, month: Month::Jan }], daily: DailyPeriodicity { hour: 0 } }` |\n\n### Periodic Options\n\n```gcl\nPeriodicOptions {\n    activated: true,              // Task is active (default: true)\n    start: time::now() + 1hour,   // Delay first execution\n    max_duration: 30s             // Timeout per execution\n}\n```\n\n### Manage Scheduled Tasks\n\n```gcl\nvar tasks = Scheduler::list();  // List all tasks\nvar task = Scheduler::find(project::my_task);  // Find specific task\nScheduler::deactivate(project::my_task);  // Deactivate\nScheduler::activate(project::my_task);    // Activate\n```\n\n> Adding a task with the same function replaces the existing configuration.\n\n### PeriodicTask Fields\n\n| Field | Type | Description |\n|-------|------|-------------|\n| function | function | The scheduled function |\n| periodicity | Periodicity | Periodicity configuration |\n| options | PeriodicOptions | Applied options |\n| is_active | bool | Whether task is active |\n| next_execution | time | Next scheduled time |\n| execution_count | int | Total executions |\n",
        "plugins/greycat/skills/greycat/references/data_structures.md": "# Data Structures\n\n## Tuple\n\nSimple association for couple of values:\n\n```gcl\nvar tupleA = Tuple{x:0.5, y:\"a\"};\nvar tupleB = (0.5, \"b\");  // Shorthand\n```\n\n## Array & Map\n\nIn-memory structures for small data:\n\n```gcl\nvar arr = Array<float>{1.2, 3.4, 5.0};\nvar arrB = [1.2, 3.4];  // Shorthand (typing unknown)\n\nfor (k, v in arr) { println(\"Index: ${k}, value ${v}\"); }\n\nvar map = Map<String, int>{\"Hello\": 5, \"Test\": 2};\nmap.set(\"Key\", 42);\nprintln(map.get(\"Test\"));\n```\n\n**Removing elements**:\n```gcl\n// Array - remove by index\nvar arr = Array<int>{1, 2, 3, 4, 5};\narr.remove(2);  // Removes element at index 2\n\n// Map - remove by key\nvar map = Map<String, int>{\"a\": 1, \"b\": 2, \"c\": 3};\nmap.remove(\"b\");  // Removes key \"b\"\n```\n\n> Use `remove` to delete elements. No `unset` method. Use `pprint` for readable console output.\n\n## Windows (FIFO with Statistics)\n\n**TimeWindow** - Collect values within time period, auto-discard old:\n```gcl\nvar tw = TimeWindow<float>{ span: 5s };\ntw.add(time::new(t, DurationUnit::seconds), value as float);\n\nprintln(\"Average: ${tw.avg()}, Min: ${tw.min()}, Max: ${tw.max()}, Size: ${tw.size()}\");\n```\n\n**SlidingWindow** - Fixed number of elements:\n```gcl\nvar sw = SlidingWindow<float>{ span: 5 };  // 5 elements max\nsw.add(value as float);\nprintln(\"Average over ${sw.size()}: ${sw.avg()}\");\n```\n\n## Table\n\n2D container for result sets, sampling, web components:\n\n```gcl\nvar t = Table{};\nt.init(2, 4);  // 2 rows, 4 columns\n\nt.set_cell(0, 1, \"value\");\nt.set_cell(0, 2, time::now());\nt.set_row(1, [\"a\", 0.0, time::now()]);\n\nt.sort(1, SortOrder::asc);  // Sort by column 1\nt.remove_row(0);\n\ninfo(t.rows());\ninfo(t.get_cell(0, 0));\n```\n\n**Table Column Mappings** - Transform tables by extracting nested fields:\n```gcl\nvar mappings = Array<TableColumnMapping>{\n    TableColumnMapping { column: 0, extractors: Array<any>{\"*\", \"a\"} },  // resolve node, get attr\n    TableColumnMapping { column: 1, extractors: Array<any>{\"a\"} },       // resolve field\n    TableColumnMapping { column: 2, extractors: Array<any>{0} }          // array index\n};\nvar newTable = Table::applyMappings(t, mappings);\n```\n\n## Tensor\n\nMulti-dimensional numerical arrays:\n\n```gcl\nvar t = Tensor::new([2, 3, 2]); // Shape: 2x3x2 tensor (12 elements)\nt.set(0, 1.0);\nvar value = t.get(0);\n```\n\n**Operations**: `flatten()`, `reshape()`, `min()`, `max()`, `avg()`, `sum()`, `matmul()`\n\n**Shapes**: `[2, 3]` = 2x3 matrix, `[2, 3, 4]` = 2x3x4 3D array\n\n## Buffer\n\nTyped byte arrays (C-like):\n\n```gcl\nvar b = Buffer::new(1024);  // 1KB\nb.write_i32(0, 42);\nvar value = b.read_i32(0);\n```\n\n**Types**: `i8`, `i16`, `i32`, `i64`, `f32`, `f64`, `bool` (read/write)\n\n## Stack & Queue\n\n```gcl\nvar stack = Stack<int>{}; stack.push(1); stack.push(2); var top = stack.pop();\nvar queue = Queue<int>{}; queue.enqueue(1); queue.enqueue(2); var front = queue.dequeue();\n```\n\n## Set\n\nUnordered unique elements:\n\n```gcl\nvar set = Set<String>{}; set.add(\"a\"); set.add(\"b\"); set.add(\"a\");  // Only one \"a\"\nvar hasA = set.contains(\"a\");  // true\nset.remove(\"a\");\n```\n",
        "plugins/greycat/skills/greycat/references/finance/finance.md": "# Finance Library\n\nFinancial data types and validation for GreyCat.\n\n## Overview\n\nThe Finance library provides specialized types and utilities for working with financial data in GreyCat applications. Currently focused on International Bank Account Number (IBAN) parsing and validation, it enables applications to handle banking information with proper validation and structure.\n\nKey features include:\n- **IBAN parsing** with automatic validation\n- **Country code extraction** from IBAN strings\n- **Bank code identification** for routing\n- **Account number parsing** from IBAN format\n- **Format validation** ensuring IBAN compliance\n\nThis library is ideal for payment processing systems, banking applications, financial data validation, accounting software, and any system that handles international bank transfers.\n\n## Installation\n\nAdd the Finance library to your GreyCat project:\n\n```gcl\n@library(\"finance\", \"7.6.60-dev\")\n```\n\n## Quick Start\n\n### Parse and Validate IBAN\n\n```gcl\n// Parse a valid IBAN\nvar iban = Iban::parse(\"GB82WEST12345698765432\");\n\nif (iban != null) {\n  print(\"Country: ${iban.country}\");\n  print(\"Bank: ${iban.bank}\");\n  print(\"Account: ${iban.account}\");\n  print(\"Full IBAN: ${iban.iban}\");\n} else {\n  print(\"Invalid IBAN\");\n}\n```\n\n### Validate User Input\n\n```gcl\nfn processPayment(ibanString: String, amount: float) {\n  var iban = Iban::parse(ibanString);\n\n  if (iban == null) {\n    throw \"Invalid IBAN: ${ibanString}\";\n  }\n\n  print(\"Processing payment of ${amount} to ${iban.country} account\");\n  // ... payment logic\n}\n```\n\n## Types\n\n### Iban\n\nRepresents a validated International Bank Account Number.\n\n**Fields:**\n- `iban: String` - Full IBAN string in standard format\n- `country: String` - ISO 3166-1 alpha-2 country code (e.g., \"GB\", \"DE\", \"FR\")\n- `bank: String` - Bank identifier code\n- `account: String` - Account number\n\n**Static Methods:**\n- `parse(iban: String): Iban?` - Parse and validate IBAN string\n\n**Example:**\n\n```gcl\nvar iban = Iban::parse(\"DE89370400440532013000\");\n\nprint(iban.iban);      // \"DE89370400440532013000\"\nprint(iban.country);   // \"DE\"\nprint(iban.bank);      // \"37040044\"\nprint(iban.account);   // \"0532013000\"\n```\n\n## Methods\n\n### parse()\n\nParses and validates an IBAN string, returning a structured Iban object.\n\n**Signature:** `static fn parse(iban: String): Iban?`\n\n**Parameters:**\n- `iban: String` - IBAN string to parse (with or without spaces)\n\n**Returns:**\n- `Iban` object if valid\n- `null` if invalid or malformed\n\n**Validation Performed:**\n- Length validation (varies by country)\n- Character validation (alphanumeric only)\n- Check digit validation using modulo-97 algorithm\n- Country code validation\n\n**Example:**\n\n```gcl\n// Valid IBANs from different countries\nvar gbIban = Iban::parse(\"GB82WEST12345698765432\");\nvar deIban = Iban::parse(\"DE89370400440532013000\");\nvar frIban = Iban::parse(\"FR1420041010050500013M02606\");\nvar esIban = Iban::parse(\"ES9121000418450200051332\");\n\n// IBAN with spaces (automatically normalized)\nvar spacedIban = Iban::parse(\"GB82 WEST 1234 5698 7654 32\");\nprint(spacedIban.iban); // \"GB82WEST12345698765432\" (spaces removed)\n\n// Invalid IBANs return null\nvar invalid1 = Iban::parse(\"INVALID\");           // null\nvar invalid2 = Iban::parse(\"GB82WEST123456\");    // null (wrong length)\nvar invalid3 = Iban::parse(\"GB00WEST12345698765432\"); // null (bad check digit)\n```\n\n## Common Use Cases\n\n### Payment Processing\n\n```gcl\ntype PaymentRequest {\n  sender_iban: String;\n  recipient_iban: String;\n  amount: float;\n  currency: String;\n}\n\nfn validateAndProcessPayment(request: PaymentRequest) {\n  // Validate sender IBAN\n  var senderIban = Iban::parse(request.sender_iban);\n  if (senderIban == null) {\n    throw \"Invalid sender IBAN: ${request.sender_iban}\";\n  }\n\n  // Validate recipient IBAN\n  var recipientIban = Iban::parse(request.recipient_iban);\n  if (recipientIban == null) {\n    throw \"Invalid recipient IBAN: ${request.recipient_iban}\";\n  }\n\n  // Check for same-country transfer\n  var domestic = senderIban.country == recipientIban.country;\n  var fee = domestic ? 1.0 : 5.0;\n\n  print(\"Transfer type: ${domestic ? 'Domestic' : 'International'}\");\n  print(\"Fee: ${fee} ${request.currency}\");\n  print(\"From: ${senderIban.country} - ${senderIban.account}\");\n  print(\"To: ${recipientIban.country} - ${recipientIban.account}\");\n\n  // Process payment\n  executeTransfer(senderIban, recipientIban, request.amount, fee);\n}\n```\n\n### Batch IBAN Validation\n\n```gcl\nvar ibanList = [\n  \"GB82WEST12345698765432\",\n  \"DE89370400440532013000\",\n  \"INVALID_IBAN\",\n  \"FR1420041010050500013M02606\",\n  \"GB00WEST12345698765432\" // Invalid check digit\n];\n\nvar validIbans = Array<Iban>{};\nvar invalidIbans = Array<String>{};\n\nfor (ibanString in ibanList) {\n  var iban = Iban::parse(ibanString);\n\n  if (iban != null) {\n    validIbans.add(iban);\n  } else {\n    invalidIbans.add(ibanString);\n  }\n}\n\nprint(\"Valid IBANs: ${validIbans.size()}\");\nprint(\"Invalid IBANs: ${invalidIbans.size()}\");\n\nfor (invalidIban in invalidIbans) {\n  print(\"  - ${invalidIban}\");\n}\n```\n\n### Country-Based Routing\n\n```gcl\nfn routePayment(ibanString: String, amount: float) {\n  var iban = Iban::parse(ibanString);\n\n  if (iban == null) {\n    throw \"Invalid IBAN\";\n  }\n\n  // Route to appropriate payment processor based on country\n  if (iban.country == \"GB\") {\n    processFasterPayment(iban, amount);\n  } else if (iban.country == \"DE\" || iban.country == \"FR\" || iban.country == \"ES\") {\n    processSEPAPayment(iban, amount);\n  } else if (iban.country == \"US\") {\n    throw \"US does not use IBAN, use ACH routing\";\n  } else {\n    processSWIFTPayment(iban, amount);\n  }\n}\n\nfn processSEPAPayment(iban: Iban, amount: float) {\n  print(\"Processing SEPA payment to ${iban.country}\");\n  print(\"Bank code: ${iban.bank}\");\n  print(\"Account: ${iban.account}\");\n  print(\"Amount: ${amount} EUR\");\n}\n```\n\n### Account Masking for Security\n\n```gcl\nfn maskIban(iban: Iban): String {\n  // Show only last 4 digits of account\n  var accountLength = iban.account.length();\n  var masked = \"\";\n\n  if (accountLength > 4) {\n    for (i in 0..(accountLength - 4)) {\n      masked = masked + \"*\";\n    }\n    masked = masked + iban.account.substring(accountLength - 4, accountLength);\n  } else {\n    masked = iban.account;\n  }\n\n  return \"${iban.country}** **** **** ${masked}\";\n}\n\nvar iban = Iban::parse(\"GB82WEST12345698765432\");\nprint(maskIban(iban)); // \"GB** **** **** 5432\"\n```\n\n### Database Storage\n\n```gcl\ntype BankAccount {\n  id: int;\n  iban_string: String;\n  country: String;\n  bank_code: String;\n  account_number: String;\n  validated: bool;\n}\n\nfn saveBankAccount(ibanString: String): BankAccount? {\n  var iban = Iban::parse(ibanString);\n\n  if (iban == null) {\n    print(\"Cannot save invalid IBAN: ${ibanString}\");\n    return null;\n  }\n\n  var account = BankAccount {\n    id: generateId(),\n    iban_string: iban.iban,\n    country: iban.country,\n    bank_code: iban.bank,\n    account_number: iban.account,\n    validated: true\n  };\n\n  // Save to database\n  db.execute(\n    \"INSERT INTO bank_accounts (id, iban, country, bank_code, account_number, validated) VALUES (${account.id}, '${account.iban_string}', '${account.country}', '${account.bank_code}', '${account.account_number}', ${account.validated})\"\n  );\n\n  return account;\n}\n```\n\n### Payment Report Generation\n\n```gcl\ntype Payment {\n  id: String;\n  iban: String;\n  amount: float;\n  date: time;\n}\n\nfn generatePaymentReport(payments: Array<Payment>) {\n  var byCountry = Map<String, float>{};\n  var invalidCount = 0;\n\n  for (payment in payments) {\n    var iban = Iban::parse(payment.iban);\n\n    if (iban == null) {\n      invalidCount = invalidCount + 1;\n      continue;\n    }\n\n    var current = byCountry.get(iban.country);\n    if (current == null) {\n      byCountry.put(iban.country, payment.amount);\n    } else {\n      byCountry.put(iban.country, current + payment.amount);\n    }\n  }\n\n  print(\"=== Payment Report ===\");\n  for (entry in byCountry.entries()) {\n    print(\"${entry.key}: ${entry.value}\");\n  }\n  print(\"Invalid IBANs: ${invalidCount}\");\n}\n```\n\n## Best Practices\n\n### Validation Before Processing\n\nAlways validate IBANs before using them in financial operations:\n\n```gcl\n// Good: Validate first\nfn processPayment(ibanString: String) {\n  var iban = Iban::parse(ibanString);\n  if (iban == null) {\n    throw \"Invalid IBAN\";\n  }\n  // Safe to proceed\n  executeTransfer(iban);\n}\n\n// Bad: Assuming validity\nfn processPayment(ibanString: String) {\n  // No validation - risky!\n  executeTransfer(ibanString);\n}\n```\n\n### User Input Handling\n\nAccept IBANs with or without spaces, but normalize before storage:\n\n```gcl\n// Accept user input with spaces\nvar userInput = \"GB82 WEST 1234 5698 7654 32\";\nvar iban = Iban::parse(userInput);\n\nif (iban != null) {\n  // Store normalized version without spaces\n  saveToDatabase(iban.iban); // \"GB82WEST12345698765432\"\n}\n```\n\n### Error Messages\n\nProvide clear feedback for invalid IBANs:\n\n```gcl\nfn validateIbanWithFeedback(ibanString: String): Iban? {\n  var iban = Iban::parse(ibanString);\n\n  if (iban == null) {\n    // Could add more specific validation to identify issue\n    if (ibanString.length() < 15) {\n      print(\"IBAN too short\");\n    } else if (ibanString.length() > 34) {\n      print(\"IBAN too long\");\n    } else {\n      print(\"Invalid IBAN format or check digit\");\n    }\n  }\n\n  return iban;\n}\n```\n\n### Null Checks\n\nAlways check for null after parsing:\n\n```gcl\n// Good: Null check\nvar iban = Iban::parse(input);\nif (iban != null) {\n  print(iban.country);\n}\n\n// Bad: May crash if invalid\nvar iban = Iban::parse(input);\nprint(iban.country); // Error if iban is null!\n```\n\n### Country-Specific Logic\n\nDifferent countries have different banking systems:\n\n```gcl\nfn getPaymentMethod(iban: Iban): String {\n  // SEPA countries\n  var sepaCountries = [\"AT\", \"BE\", \"DE\", \"ES\", \"FR\", \"IT\", \"NL\", \"PT\"];\n\n  if (sepaCountries.contains(iban.country)) {\n    return \"SEPA\";\n  } else if (iban.country == \"GB\") {\n    return \"Faster Payments\";\n  } else if (iban.country == \"CH\") {\n    return \"Swiss Interbank Clearing\";\n  } else {\n    return \"SWIFT\";\n  }\n}\n```\n\n### Performance\n\nFor bulk validation, batch operations when possible:\n\n```gcl\n// Efficient: Batch validation\nfn validateBatch(ibans: Array<String>): Map<String, Iban?> {\n  var results = Map<String, Iban?>{};\n\n  for (ibanString in ibans) {\n    results.put(ibanString, Iban::parse(ibanString));\n  }\n\n  return results;\n}\n```\n\n### Gotchas\n\n- **Null return**: `parse()` returns `null` for invalid IBANs, always check\n- **Spaces are ignored**: Input can have spaces, output `iban` field will not\n- **Case sensitivity**: IBANs should be uppercase, but parser may be tolerant\n- **Country codes**: 2-letter ISO codes, not 3-letter or country names\n- **Length varies**: Different countries have different IBAN lengths (15-34 characters)\n- **Not all countries use IBAN**: USA, Canada, Australia, and others use different systems\n\n### IBAN Format Reference\n\nCommon IBAN formats by country:\n\n| Country | Code | Length | Example |\n|---------|------|--------|---------|\n| United Kingdom | GB | 22 | GB82WEST12345698765432 |\n| Germany | DE | 22 | DE89370400440532013000 |\n| France | FR | 27 | FR1420041010050500013M02606 |\n| Spain | ES | 24 | ES9121000418450200051332 |\n| Italy | IT | 27 | IT60X0542811101000000123456 |\n| Netherlands | NL | 18 | NL91ABNA0417164300 |\n| Belgium | BE | 16 | BE68539007547034 |\n\nFor a complete list, see the [IBAN Registry](https://www.swift.com/standards/data-standards/iban).\n\n### Security Considerations\n\n- **Don't log full IBANs**: Mask account numbers in logs\n- **Encrypt in storage**: IBANs are sensitive financial data\n- **Validate on server**: Don't rely solely on client-side validation\n- **Audit trail**: Log who accessed or modified IBAN data\n\n```gcl\n// Secure logging\nfn logPayment(iban: Iban, amount: float) {\n  var masked = \"${iban.country}**************${iban.account.substring(iban.account.length() - 4)}\";\n  print(\"Payment to ${masked}: ${amount}\");\n}\n```\n\n### Future Extensions\n\nThis library currently focuses on IBAN validation. Future versions may include:\n\n- **BIC/SWIFT code** validation and parsing\n- **Credit card** validation (Luhn algorithm)\n- **Currency** types and conversion\n- **Money** type with precision handling\n- **Tax ID** validation (VAT, EIN, etc.)\n\n## Error Handling\n\nThe library uses null returns instead of exceptions:\n\n```gcl\n// Pattern 1: Null check with error\nvar iban = Iban::parse(input);\nif (iban == null) {\n  throw \"Invalid IBAN: ${input}\";\n}\n// Continue with valid iban\n\n// Pattern 2: Early return\nvar iban = Iban::parse(input);\nif (iban == null) {\n  return;\n}\n// Continue with valid iban\n\n// Pattern 3: Default value\nvar iban = Iban::parse(input);\nvar country = iban != null ? iban.country : \"UNKNOWN\";\n```\n\n## IBAN Structure\n\nUnderstanding IBAN components:\n\n```\nGB82 WEST 1234 5698 7654 32\n│ │  │    │              │\n│ │  │    │              └─ Account Number\n│ │  │    └──────────────── Bank Identifier\n│ │  └─────────────────────┤\n│ └──────────────────────── Check Digits (modulo-97)\n└────────────────────────── Country Code (ISO 3166-1)\n```\n\nThe check digits ensure IBAN integrity using the modulo-97 algorithm, which detects typos and transposition errors with high accuracy.\n",
        "plugins/greycat/skills/greycat/references/frontend.md": "# Frontend Integration with React\n\nReact + GreyCat integration using `@greycat/web` SDK.\n\n## Overview\n\n**@greycat/web SDK provides**: Auto-generated TypeScript types, auth/session management, typed API communication, error handling.\n\n**Flow**: `Backend (GCL) → greycat codegen ts → project.d.ts → Frontend (TS/React) → Global gc namespace`\n\n## Installation & Setup\n\n```bash\n# Install SDK (check https://get.greycat.io for latest)\nnpm install https://get.greycat.io/files/sdk/web/dev/7.6/7.6.6-dev.tgz\n```\n\n**Dependencies**: `@greycat/web`, `react@^18.3`, `react-dom@^18.3`, `@tanstack/react-query@^5`, `react-router-dom@^6`\n\n**vite.config.ts:**\n```typescript\nimport { defineConfig } from 'vite';\nimport react from '@vitejs/plugin-react';\nimport greycat from '@greycat/web/vite-plugin';\n\nexport default defineConfig(() => ({\n  plugins: [react(), greycat({ greycat: process.env.VITE_GREYCAT_URL || 'http://127.0.0.1:8080' })],\n  server: { port: 3000 },\n}));\n```\n\n**tsconfig.json:**\n```json\n{\n  \"compilerOptions\": { \"target\": \"ES2020\", \"lib\": [\"ES2020\", \"DOM\", \"DOM.Iterable\"], \"module\": \"ESNext\", \"moduleResolution\": \"bundler\", \"jsx\": \"react-jsx\", \"strict\": true },\n  \"include\": [\"src\", \"../project.d.ts\"]\n}\n```\n\n**src/vite-env.d.ts:**\n```typescript\n/// <reference types=\"vite/client\" />\n/// <reference types=\"@greycat/web/sdk\" />\n```\n\n## SDK Initialization\n\n**⚠️ CRITICAL**: SDK must initialize BEFORE React renders.\n\n**index.tsx:**\n```typescript\nimport '@greycat/web';\n\ngc.sdk.init({\n  numFmt: new Intl.NumberFormat('en-GB', { notation: 'compact', maximumSignificantDigits: 5 }),\n}).then(() => import('./main'));\n```\n\n**main.tsx:**\n```typescript\nimport React from 'react';\nimport ReactDOM from 'react-dom/client';\nimport { QueryClient, QueryClientProvider } from '@tanstack/react-query';\nimport App from './App';\n\nconst queryClient = new QueryClient({\n  defaultOptions: {\n    queries: { staleTime: 60_000, gcTime: 5 * 60 * 1000, refetchOnWindowFocus: true, retry: 1 }\n  }\n});\n\nReactDOM.createRoot(document.getElementById('root')!).render(\n  <React.StrictMode>\n    <QueryClientProvider client={queryClient}><App /></QueryClientProvider>\n  </React.StrictMode>\n);\n```\n\n## Backend to Frontend Workflow\n\n### 1. Backend API Definition\n\n**backend/src/api/country_api.gcl:**\n```gcl\n@expose @permission(\"public\")\nfn getStats(name: String): StatsView { /* ... */ }\n\n@expose @permission(\"public\")\nfn listCountries(offset: int, maxCount: int): Array<CountryView> { /* ... */ }\n```\n\n### 2. Generate TypeScript Types\n\n```bash\ngreycat codegen ts  # Generates project.d.ts\n```\n\n**package.json automation:**\n```json\n{\n  \"scripts\": {\n    \"types\": \"cd .. && greycat codegen ts\",\n    \"dev\": \"npm run types && vite\",\n    \"build\": \"npm run types && tsc && vite build\"\n  }\n}\n```\n\n**Generated types:**\n```typescript\ndeclare namespace gc {\n  namespace country_api {\n    const getStats: gc.sdk.ExposedFn<[string], gc.api_types.StatsView>;\n    const listCountries: gc.sdk.ExposedFn<[number, number], Array<gc.api_types.CountryView>>;\n  }\n  export import getStats = gc.country_api.getStats;\n  export import listCountries = gc.country_api.listCountries;\n}\n```\n\n**⚠️ NAMESPACE STRUCTURE**:\n- `gc.project.*` - Database structure (Root, indices)\n- `gc.<api_name>.*` - API function namespaces\n- `gc.<function>()` - Top-level convenience exports (recommended)\n\n### 3. HTTP API Call Format\n\n**⚠️ CRITICAL**: All calls use **POST** with parameters as **JSON array**.\n\n**Format**: `POST /api/<namespace>::<function_name>`\n\n```bash\n# Single parameter\ncurl -X POST http://localhost:8080/api/country_api::getStats -H \"Content-Type: application/json\" -d '[\"France\"]'\n\n# Multiple parameters\ncurl -X POST http://localhost:8080/api/country_api::listCountries -H \"Content-Type: application/json\" -d '[0, 20]'\n\n# No parameters\ncurl -X POST http://localhost:8080/api/someApi::noParams -H \"Content-Type: application/json\" -d '[]'\n```\n\n### 4. TypeScript Usage\n\n```typescript\n// Direct usage - types are global (no import needed)\nconst stats: gc.api_types.StatsView = await gc.getStats(\"France\");\nconst countries: Array<gc.api_types.CountryView> = await gc.listCountries(0, 20);\n\n// TypeScript catches errors at compile time\n// ❌ gc.getStats(123);           // Error: Expected string\n// ✅ gc.getStats(\"France\");       // Correct\n```\n\n**Naming Convention**: Backend snake_case (`get_city_by_id`) → Frontend camelCase (`getCityById`).\n\n## Return Types\n\n### Simple Array\n```gcl\n@expose @permission(\"public\")\nfn listCountries(offset: int, maxCount: int): Array<CountryView> { /* ... */ }\n```\n\n### PaginatedResult\n```gcl\n@expose @permission(\"public\")\nfn getDocumentsByYear(year: int, yearType: YearType, filters: DocumentFilters?, offset: int, maxResults: int): PaginatedResult<DocumentView> {\n    return PaginatedResult<DocumentView> {\n        items: Array<DocumentView> {},\n        offset: offset,\n        limit: maxResults,\n        total: totalCount,\n        hasMore: (offset + maxResults) < totalCount,\n        page: (offset / maxResults) + 1,\n        totalPages: (totalCount + maxResults - 1) / maxResults,\n    };\n}\n```\n\n**Frontend:**\n```typescript\nconst result = await gc.getDocumentsByYear(2020, gc.vocabulary.YearType.case, null, 0, 20);\nconsole.log(result.items, result.total, result.hasMore, result.page, result.totalPages);\n```\n\n## Handling GreyCat Enums\n\n**Enum Serialization**: Use `.key!` to serialize enums.\n\n```typescript\n// ✅ CORRECT\nconst data = { role: user.role.key! };  // \"Admin\" as string\n\n// ❌ INCORRECT\nconst data = { role: user.role };  // Sends full object\n\n// Display\n<div>Role: {user.role.key}</div>\n\n// Map keys\nconst countsByRole = new Map<string, number>();\nfor (const user of users) { countsByRole.set(user.role.key!, (countsByRole.get(user.role.key!) ?? 0) + 1); }\n```\n\n## Authentication\n\n**Backend:**\n```gcl\ntype Person { email: String; firstName: String; lastName: String; userId: int; }\nvar persons_by_id: nodeIndex<int, node<Person>>;\n\nabstract type PersonService {\n    static fn create(email: String, firstName: String, password: String): node<Person> {\n        var userId = UserGroup::Default.add(UserRole::Admin, email, password);\n        var person = node<Person>{ Person { email, firstName, lastName: \"\", userId } };\n        persons_by_id.set(userId, person);\n        return person;\n    }\n}\n\n@volatile type PersonView { email: String; firstName: String; lastName: String; }\n@expose @permission(\"public\") fn getCurrentPerson(): PersonView? {\n    var user = User::current(); if (user == null) { return null; }\n    var person = persons_by_id.get(user.id); if (person == null) { return null; }\n    return PersonView { email: person->email, firstName: person->firstName, lastName: person->lastName };\n}\n```\n\n**Frontend Auth Service:**\n```typescript\n// src/services/auth.ts\nexport const authService = {\n  login: async (username: string, password: string) => {\n    await gc.sdk.login({ username, password, use_cookie: true });\n    return await gc.getCurrentPerson();\n  },\n  logout: async () => { await gc.sdk.logout(); },\n  getCurrentUser: async () => await gc.getCurrentPerson(),\n  isAuthenticated: () => gc.sdk.token !== null,\n};\n\n// src/hooks/usePerson.ts\nexport function usePerson() {\n  return useQuery({\n    queryKey: ['currentPerson'],\n    queryFn: authService.getCurrentUser,\n    staleTime: 5 * 60 * 1000,\n    enabled: authService.isAuthenticated(),\n  });\n}\n```\n\n## Service Layer Pattern\n\n**Recommended pattern**: Create service layer wrapping gc calls for retry logic and error handling.\n\n**src/services/apiUtils.ts:**\n```typescript\nexport interface RetryOptions {\n  maxRetries?: number;\n  delayMs?: number;\n  exponentialBackoff?: boolean;\n}\n\nexport async function withRetry<T>(fn: () => Promise<T>, options: RetryOptions = {}): Promise<T> {\n  const { maxRetries = 3, delayMs = 1000, exponentialBackoff = true } = options;\n  let lastError: unknown;\n\n  for (let attempt = 0; attempt <= maxRetries; attempt++) {\n    try {\n      return await fn();\n    } catch (error) {\n      lastError = error;\n      if (attempt === maxRetries) break;\n\n      // Don't retry auth errors\n      const statusCode = typeof error === 'object' && error !== null && 'status' in error\n        ? (error as { status?: number }).status : undefined;\n      if (statusCode === 401 || statusCode === 403) throw error;\n\n      const delay = exponentialBackoff ? delayMs * Math.pow(2, attempt) : delayMs;\n      await new Promise(resolve => setTimeout(resolve, delay));\n    }\n  }\n  throw lastError;\n}\n```\n\n**src/services/countryService.ts:**\n```typescript\nimport { withRetry } from './apiUtils';\n\nexport const CountryService = {\n  getStats: (name: string): Promise<gc.api_types.StatsView> =>\n    withRetry(() => gc.getStats(name)),\n\n  listCountries: (offset: number, maxCount: number): Promise<Array<gc.api_types.CountryView>> =>\n    withRetry(() => gc.listCountries(offset, maxCount)),\n};\n```\n\n## React Component Usage\n\n```typescript\nimport { useQuery, useMutation, useQueryClient } from '@tanstack/react-query';\nimport { CountryService } from '../services/countryService';\nimport { useState } from 'react';\n\nexport function CountryPage() {\n  const [page, setPage] = useState(0);\n  const pageSize = 20;\n\n  // Query\n  const { data: stats, isLoading, error } = useQuery({\n    queryKey: ['countryStats', \"France\"],\n    queryFn: () => CountryService.getStats(\"France\"),\n  });\n\n  // List query with pagination\n  const { data: countries } = useQuery({\n    queryKey: ['countries', page, pageSize],\n    queryFn: () => CountryService.listCountries(page * pageSize, pageSize),\n  });\n\n  // Mutation\n  const queryClient = useQueryClient();\n  const createCity = useMutation({\n    mutationFn: (params: { name: string; countryId: number }) => gc.createCity(params.name, params.countryId),\n    onSuccess: (newCity) => {\n      queryClient.invalidateQueries({ queryKey: ['cities', newCity.countryId] });\n      queryClient.setQueryData(['city', newCity.id], newCity);\n    },\n  });\n\n  if (isLoading) return <div>Loading...</div>;\n  if (error) return <div>Error: {error.message}</div>;\n\n  return (\n    <div>\n      <h1>France</h1>\n      <p>Population: {stats?.population}</p>\n\n      <ul>{countries?.map(c => <li key={c.id}>{c.name}</li>)}</ul>\n      <button onClick={() => setPage(p => p - 1)} disabled={page === 0}>Previous</button>\n      <button onClick={() => setPage(p => p + 1)} disabled={!countries || countries.length < pageSize}>Next</button>\n\n      <button onClick={() => createCity.mutate({ name: \"Paris\", countryId: 1 })}>Add City</button>\n    </div>\n  );\n}\n```\n\n## Complex Example with Enums\n\n**Backend:**\n```gcl\nenum YearType { case, judgment, decision, published, filed }\n\n@volatile type DocumentFilters { formexTypes: Array<FormexType>?; }\n\n@expose @permission(\"public\")\nfn getDocumentsByYear(year: int, yearType: YearType, filters: DocumentFilters?, offset: int, maxResults: int): PaginatedResult<DocumentView> { /* ... */ }\n```\n\n**Frontend Service:**\n```typescript\nexport const StatsService = {\n  getDocumentsByYear: (\n    year: number,\n    yearType?: gc.vocabulary.YearType | null,\n    offset = 0,\n    maxResults = 20\n  ): Promise<gc.pagination_service.PaginatedResult> =>\n    withRetry(() =>\n      gc.getDocumentsByYear(\n        year,\n        yearType ?? gc.vocabulary.YearType.case,\n        null,\n        offset,\n        maxResults\n      )\n    ),\n};\n```\n\n**Frontend Component:**\n```typescript\nexport default function StatsPage() {\n  const [yearType, setYearType] = useState<gc.vocabulary.YearType>(gc.vocabulary.YearType.case);\n  const [selectedYear, setSelectedYear] = useState<number | null>(null);\n\n  const { data: yearDocuments } = useQuery({\n    queryKey: ['documentsByYear', selectedYear, yearType.key],\n    queryFn: () => StatsService.getDocumentsByYear(selectedYear!, yearType, 0, 50),\n    enabled: selectedYear !== null,\n  });\n\n  return (\n    <div>\n      <select value={yearType.key} onChange={(e) => {\n        const key = e.target.value as gc.vocabulary.YearType.Field;\n        setYearType(gc.vocabulary.YearType[key]);\n        setSelectedYear(null);\n      }}>\n        <option value=\"case\">Case Year</option>\n        <option value=\"judgment\">Judgment Date</option>\n      </select>\n\n      {yearDocuments && (\n        <div>\n          <p>Total: {yearDocuments.total} documents</p>\n          <p>Page {yearDocuments.page} of {yearDocuments.totalPages}</p>\n          <ul>{yearDocuments.items.map((doc: any) => <li key={doc.id}>{doc.celex}</li>)}</ul>\n        </div>\n      )}\n    </div>\n  );\n}\n```\n\n## Error Handling\n\n**Try-Catch:**\n```typescript\ntry {\n  const city = await gc.getCity(cityId);\n  setCity(city);\n} catch (error) {\n  console.error('Failed to fetch city:', error);\n  toast.error(error instanceof Error ? error.message : 'Unknown error');\n}\n```\n\n**React Query:**\n```typescript\nconst { data, error, isError } = useQuery({\n  queryKey: ['city', cityId],\n  queryFn: () => gc.getCity(cityId),\n  retry: (failureCount, error) => {\n    if (error.message?.includes('404')) return false;\n    return failureCount < 3;\n  },\n});\n\nif (isError) return <div>Error: {error.message}</div>;\n```\n\n## Time Handling\n\nGreyCat `time` type (μs epoch) needs conversion:\n\n```typescript\n// Backend\n@volatile type EventView { timestamp: time; }\n\n// Frontend\nconst event = await gc.getEvents()[0];\nconst date = new Date(event.timestamp / 1000);  // Convert μs → ms\n\n// Display component\nfunction EventTime({ timestamp }: { timestamp: number }) {\n  const date = new Date(timestamp / 1000);\n  return <time dateTime={date.toISOString()}>{date.toLocaleString()}</time>;\n}\n```\n\n## Best Practices\n\n1. **Always run `greycat codegen ts`** after backend changes\n2. **Initialize SDK before React**: `gc.sdk.init().then(() => import('./main'))`\n3. **Use top-level `gc` namespace**: Prefer `gc.getStats()` over `gc.stats_api.getStats()`\n4. **Serialize enums**: Use `.key!` when sending to backend/APIs\n5. **Service layer**: Wrap gc calls for retry logic, error handling\n6. **React Query**: Use for caching, auto-refetching, optimistic updates\n7. **Type safety**: Leverage generated types, avoid `any`\n8. **Environment variables**: Use `VITE_GREYCAT_URL` for backend URL\n\n## Common Pitfalls\n\n| ❌ Wrong | ✅ Correct |\n|----------|-----------|\n| `gc.project.getStats()` | `gc.getStats()` (APIs not in gc.project) |\n| `{ role: user.role }` | `{ role: user.role.key! }` |\n| React renders before SDK init | `gc.sdk.init().then(() => import('./main'))` |\n| Forgot `greycat codegen ts` | Run after every backend change |\n| Direct gc calls in components | Use service layer + React Query |\n| `new Map<Enum, V>()` | `new Map<string, V>()` with `.key!` |\n| Missing enum defaults | Use `?? gc.vocabulary.EnumName.default` |\n\n## Integration Checklist\n\n- [ ] Install @greycat/web SDK\n- [ ] Configure Vite plugin + tsconfig.json\n- [ ] Add vite-env.d.ts reference types\n- [ ] Initialize SDK before React\n- [ ] Run `greycat codegen ts` after backend changes\n- [ ] Update package.json scripts\n- [ ] Implement auth service + usePerson hook\n- [ ] Create service layer for API calls\n- [ ] Setup React Query with QueryClientProvider\n- [ ] Serialize enums with `.key!`\n- [ ] Test type safety: No `any` types\n- [ ] Verify error handling\n- [ ] Convert time (μs → ms) for display\n",
        "plugins/greycat/skills/greycat/references/io.md": "# File I/O\n\n## CSV Reading\n\n### Basic CsvReader\n\n```gcl\nvar format = CsvFormat {\n    separator: ';',\n    string_delimiter: '\\'',\n    decimal_separator: ','\n};\n\nvar reader = CsvReader { path: \"./data.csv\", format: format };\nwhile (reader.can_read()) {\n    var row = reader.read();  // Array<any?>\n}\n```\n\n### Typed CsvReader\n\n```gcl\n@volatile\ntype Entry {\n    id: int;\n    name: String;\n    values: Array<int>;  // Greedy - consumes remaining columns\n}\n\nvar reader = CsvReader<Entry> {\n    path: \"files/entries.csv\",\n    format: CsvFormat { header_lines: 1 }\n};\n\nwhile (reader.can_read()) {\n    var entry = reader.read();  // Entry object\n}\n```\n\n### Special CSV Types\n\n```gcl\ntype Record {\n    position: geo;           // Consumes 2 cols: lat, lng\n    skip: null;              // Skip column\n    @format(\"%d/%m/%y %H:%M\")\n    date: time;              // Parse with format\n    @format(DurationUnit::hours)\n    elapsed: duration;       // Parse as hours\n}\n```\n\n### CsvFormat Options\n\n| Attribute | Type | Description |\n|-----------|------|-------------|\n| header_lines | int? | Lines to skip |\n| separator | char? | Column separator (default `,`) |\n| decimal_separator | char? | Decimal point (default `.`) |\n| thousands_separator | char? | Thousands separator |\n| string_delimiter | char? | String quote char |\n| format | String? | Date format |\n| tz | TimeZone? | Timezone |\n\n### CsvWriter\n\n```gcl\nvar writer = CsvWriter {\n    path: \"./data.csv\",\n    format: CsvFormat { separator: ';', decimal_separator: ',' }\n};\nwriter.write([\"John\", \"Doe\", time::now(), 56]);\n```\n\n## JSON\n\n### Read JSON File\n\n```gcl\nvar reader = JsonReader { path: \"data.json\" };\nwhile (reader.available() > 0) {\n    var obj = reader.read();  // Map with key-value pairs\n}\n```\n\n### Parse JSON String\n\n```gcl\nvar reader = Json{};\nvar parsed = reader.parse(jsonString as String);\n```\n\n### Typed JSON Reading\n\n```gcl\nabstract type Geometry {}\ntype Point extends Geometry { coordinates: Tuple<float, float>; }\ntype LineString extends Geometry { coordinates: Array<Tuple<float, float>>; }\n\ntype Feature { geometry: Geometry; }\n\nvar reader = JsonReader<Feature> { path: \"data.json\" };\nwhile (reader.can_read()) {\n    pprint(reader.read());  // Polymorphic dispatch\n}\n```\n\n### Write JSON\n\n```gcl\nvar writer = JsonWriter { path: \"./out.json\" };\nwriter.write(Foo { name: \"John\", age: 42 });\nwriter.writeln([true, false, null]);  // With newline\n\n// Append mode\nvar appendWriter = JsonWriter { path: \"./out.json\", append: true };\n```\n\n## Files & Folders\n\n### List Files\n\n```gcl\nvar files = File::ls(\"data\", \"csv\", true);  // dir, extension, recursive\n```\n\n### FileWalker\n\n```gcl\nvar walker = FileWalker { path: \"./dataFolder\" };\nwhile (!walker.isEmpty()) {\n    var file = walker.next();\n    if (file.isDir()) {\n        // Handle directory\n    } else {\n        // Handle file\n    }\n}\n```\n\n### File Operations\n\n```gcl\nFile::mkdir(\"./path/to/dir\");\nFile::copy(\"./origin.txt\", \"./copy.txt\");\nFile::delete(\"./file.txt\");\nFile::rename(\"./old\", \"./new\");\n\nFile::baseDir();     // Path to files folder\nFile::userDir();     // Path to user directory\nFile::workingDir();  // Path for current task/request\n```\n\n## Network/HTTP\n\n### GET Request\n\n```gcl\nvar page = Http::get(\"http://example.com\", null);\nvar data = Http::get(\"http://api.com\", [\n    HttpHeader { name: \"Accept\", value: \"application/json\" },\n    HttpHeader { name: \"Bearer\", value: \"${token}\" }\n]);\n```\n\n### Download File\n\n```gcl\nHttp::getFile(\"https://example.com/file\", \"./local.json\", null);\n```\n\n### POST/PUT Request\n\n```gcl\nvar request = { sampling: [\"live\"], ids: [uuid] };\nvar result = Http::post(endpoint, request, headers);\nvar parsed = JsonReader::parse(result as String);\n```\n\n### Generic HTTP Client\n\n```gcl\nvar client = Http{};\nvar req = HttpRequest {\n    method: HttpMethod::GET,\n    url: \"https://api.example.com?p=1\",\n    headers: Map<String, String>{ \"Content-Type\": \"application/json\" },\n    body: \"data\"\n};\nvar response = client.send(req);\n// response.status_code, response.headers, response.content\n```\n\n### Typed HTTP Client\n\n```gcl\ntype ApiResponse { path: String?; }\n\nvar client = Http<ApiResponse>{};\nvar response = client.send(req);\nprintln(response.content?.path);  // Typed access\n```\n\n### URL Utilities\n\n```gcl\nvar url = Url::parse(\"https://example.com/path?p1=true#section\");\n// url.protocol, url.host, url.path, url.params, url.hash\n\nvar encoded = Url::encode(FormFields { name: \"John\", age: 42 });\n// \"name=John&age=42\"\n```\n\n## SMTP Email\n\n```gcl\nvar smtp = Smtp {\n    host: \"smtp.company.com\",\n    port: 587,\n    mode: SmtpMode::starttls,\n    authenticate: SmtpAuth::login,\n    user: \"user\",\n    pass: \"pass\"\n};\n\nvar email = Email {\n    from: \"\\\"John\\\" <john@company.com>\",\n    to: [\"boss@company.com\"],\n    cc: [\"team@company.com\"],\n    subject: \"Report\",\n    body: \"Content here\",\n    body_is_html: false\n};\n\nsmtp.send(email);\n```\n",
        "plugins/greycat/skills/greycat/references/kafka/kafka.md": "# Kafka Integration\n\nApache Kafka producer/consumer integration for event streaming in GreyCat.\n\n## Overview\n\nThe Kafka library provides type-safe, high-performance messaging capabilities for GreyCat applications. Built on Apache Kafka's distributed streaming platform, it enables real-time data pipelines and event-driven architectures with automatic serialization and deserialization of GreyCat types.\n\nKey features include:\n- **Generic reader/writer types** supporting any GreyCat type `T`\n- **Full Kafka configuration** with 172+ parameters for fine-grained control\n- **Automatic serialization** of complex GreyCat types to Kafka messages\n- **Topic/partition/offset tracking** for precise message positioning\n- **Configurable timeouts** to handle network latency and blocking scenarios\n\nThis library is ideal for building microservices that communicate via event streams, implementing CQRS patterns, processing real-time data feeds, or integrating GreyCat with existing Kafka-based infrastructures.\n\n## Installation\n\nAdd the Kafka library to your GreyCat project:\n\n```gcl\n@library(\"kafka\", \"7.6.60-dev\")\n```\n\n## Quick Start\n\n### Simple Consumer\n\n```gcl\n// Configure Kafka connection\nvar conf = KafkaConf {\n  \"bootstrap.servers\": \"localhost:9092\",\n  \"group.id\": \"my-consumer-group\",\n  \"auto.offset.reset\": \"earliest\"\n};\n\n// Create a reader for String messages\nvar reader = KafkaReader<String> {\n  conf: conf,\n  topics: [\"my-topic\"],\n  timeout: 10s\n};\n\n// Read messages\nvar msg = reader.read();\nprint(\"Received: ${msg.value} from ${msg.topic}:${msg.partition}@${msg.offset}\");\n```\n\n### Simple Producer\n\n```gcl\n// Configure Kafka connection\nvar conf = KafkaConf {\n  \"bootstrap.servers\": \"localhost:9092\"\n};\n\n// Create a writer for String messages\nvar writer = KafkaWriter<String> {\n  conf: conf,\n  topic: \"my-topic\"\n};\n\n// Write messages\nwriter.write(\"Hello, Kafka!\");\nwriter.flush(); // Ensure delivery\n```\n\n## Types\n\n### KafkaReader&lt;T&gt;\n\nGeneric consumer for reading messages from Kafka topics.\n\n**Fields:**\n- `conf: KafkaConf` (private) - Kafka client configuration\n- `topics: Array<String>` (private) - List of topics to subscribe to\n- `timeout: duration?` - Maximum time `read()` will block (default: `10s`)\n\n**Methods:**\n- `read(): KafkaMsg<T>` - Blocks until a message is available or timeout occurs\n\n**Example:**\n\n```gcl\ntype SensorReading {\n  sensor_id: String;\n  temperature: float;\n  timestamp: time;\n}\n\nvar reader = KafkaReader<SensorReading> {\n  conf: KafkaConf {\n    \"bootstrap.servers\": \"kafka-cluster.example.com:9092\",\n    \"group.id\": \"sensor-processors\",\n    \"auto.offset.reset\": \"latest\"\n  },\n  topics: [\"sensor-readings\", \"sensor-alerts\"],\n  timeout: 30s\n};\n\n// Continuous consumption\nwhile (true) {\n  var msg = reader.read();\n  print(\"Sensor ${msg.value.sensor_id}: ${msg.value.temperature}°C\");\n}\n```\n\n### KafkaMsg&lt;T&gt;\n\nContainer for messages read from Kafka with metadata.\n\n**Fields:**\n- `topic: String` - Source topic name\n- `partition: int` - Partition number within the topic\n- `offset: int` - Unique position within the partition\n- `value: T` - The actual message payload\n\n**Example:**\n\n```gcl\nvar msg = reader.read();\n\n// Access message metadata\nprint(\"Topic: ${msg.topic}\");\nprint(\"Partition: ${msg.partition}\");\nprint(\"Offset: ${msg.offset}\");\n\n// Process the payload\nprocess_data(msg.value);\n\n// Use metadata for logging/tracking\nlog_message(msg.topic, msg.partition, msg.offset, msg.value);\n```\n\n### KafkaWriter&lt;T&gt;\n\nGeneric producer for writing messages to a Kafka topic.\n\n**Fields:**\n- `conf: KafkaConf` (private) - Kafka client configuration\n- `topic: String` (private) - Target topic name\n\n**Methods:**\n- `write(value: T)` - Asynchronously writes a message to the topic\n- `flush()` - Blocks until all pending messages are delivered\n\n**Example:**\n\n```gcl\ntype OrderEvent {\n  order_id: String;\n  user_id: String;\n  amount: float;\n  status: String;\n}\n\nvar writer = KafkaWriter<OrderEvent> {\n  conf: KafkaConf {\n    \"bootstrap.servers\": \"kafka1:9092,kafka2:9092,kafka3:9092\",\n    \"acks\": \"all\",\n    \"compression.type\": \"snappy\",\n    \"retries\": \"10\"\n  },\n  topic: \"order-events\"\n};\n\n// Write an order event\nwriter.write(OrderEvent {\n  order_id: \"ORD-12345\",\n  user_id: \"USR-789\",\n  amount: 99.99,\n  status: \"created\"\n});\n\n// Ensure message delivery before shutdown\nwriter.flush();\n```\n\n### KafkaConf\n\nComprehensive Kafka client configuration with 172+ parameters.\n\n**Required Fields:**\n- `\"bootstrap.servers\": String` - Initial broker list (e.g., `\"host1:9092,host2:9092\"`)\n\n**Common Optional Fields:**\n\n| Field | Description | Example |\n|-------|-------------|---------|\n| `\"group.id\"` | Consumer group identifier | `\"my-app-consumers\"` |\n| `\"auto.offset.reset\"` | Where to start consuming | `\"earliest\"` or `\"latest\"` |\n| `\"enable.auto.commit\"` | Auto-commit offsets | `\"true\"` |\n| `\"acks\"` | Acknowledgment level | `\"all\"`, `\"1\"`, `\"0\"` |\n| `\"compression.type\"` | Message compression | `\"snappy\"`, `\"gzip\"`, `\"lz4\"` |\n| `\"max.poll.interval.ms\"` | Consumer heartbeat timeout | `\"300000\"` (5 min) |\n| `\"batch.size\"` | Producer batch size in bytes | `\"16384\"` |\n| `\"linger.ms\"` | Producer batching delay | `\"10\"` |\n| `\"retries\"` | Number of retries on failure | `\"10\"` |\n| `\"request.timeout.ms\"` | Request timeout | `\"30000\"` (30s) |\n\n**Security Configuration:**\n\n```gcl\nvar secureConf = KafkaConf {\n  \"bootstrap.servers\": \"secure-kafka.example.com:9093\",\n  \"security.protocol\": \"SASL_SSL\",\n  \"sasl.mechanism\": \"PLAIN\",\n  \"sasl.username\": \"kafka-user\",\n  \"sasl.password\": \"secret-password\",\n  \"ssl.ca.location\": \"/etc/ssl/certs/ca-cert.pem\"\n};\n```\n\n**Performance Tuning:**\n\n```gcl\nvar perfConf = KafkaConf {\n  \"bootstrap.servers\": \"localhost:9092\",\n  // Producer performance\n  \"batch.size\": \"32768\",\n  \"linger.ms\": \"20\",\n  \"compression.type\": \"snappy\",\n  \"buffer.memory\": \"67108864\",\n  // Consumer performance\n  \"fetch.min.bytes\": \"1024\",\n  \"fetch.max.wait.ms\": \"500\",\n  \"max.partition.fetch.bytes\": \"1048576\"\n};\n```\n\n**All Available Fields:**\n\nThe `KafkaConf` type supports all standard Kafka configuration parameters including:\n\n**Connection:** `bootstrap.servers`, `client.id`, `broker.address.family`, `socket.timeout.ms`, `socket.connection.setup.timeout.ms`\n\n**Consumer:** `group.id`, `auto.offset.reset`, `enable.auto.commit`, `auto.commit.interval.ms`, `max.poll.interval.ms`, `session.timeout.ms`, `heartbeat.interval.ms`, `partition.assignment.strategy`, `isolation.level`\n\n**Producer:** `acks`, `retries`, `batch.size`, `linger.ms`, `buffer.memory`, `compression.type`, `max.in.flight.requests.per.connection`, `enable.idempotence`, `transactional.id`\n\n**Security:** `security.protocol`, `sasl.mechanism`, `sasl.username`, `sasl.password`, `ssl.key.location`, `ssl.certificate.location`, `ssl.ca.location`\n\n**Advanced:** `fetch.min.bytes`, `fetch.max.bytes`, `max.partition.fetch.bytes`, `request.timeout.ms`, `metadata.max.age.ms`, `reconnect.backoff.ms`\n\nSee the official [Apache Kafka documentation](https://kafka.apache.org/documentation/#configuration) for complete parameter details.\n\n## Methods\n\n### KafkaReader.read()\n\nReads a single message from subscribed topics.\n\n**Signature:** `fn read(): KafkaMsg<T>`\n\n**Behavior:**\n- Blocks until a message is available or `timeout` is reached\n- Throws an error if timeout expires\n- Automatically deserializes to type `T`\n- Commits offsets based on consumer configuration\n\n**Example:**\n\n```gcl\nvar reader = KafkaReader<String> {\n  conf: KafkaConf {\n    \"bootstrap.servers\": \"localhost:9092\",\n    \"group.id\": \"my-group\"\n  },\n  topics: [\"logs\"],\n  timeout: 5s\n};\n\ntry {\n  var msg = reader.read();\n  print(\"Message: ${msg.value}\");\n} catch (e) {\n  print(\"Timeout or error: ${e}\");\n}\n```\n\n### KafkaWriter.write()\n\nWrites a message to the configured topic.\n\n**Signature:** `fn write(value: T)`\n\n**Behavior:**\n- Asynchronously sends the message (non-blocking)\n- Automatically serializes the value\n- Messages may be batched for performance\n- No guarantee of delivery until `flush()` is called\n\n**Example:**\n\n```gcl\nvar writer = KafkaWriter<int> {\n  conf: KafkaConf { \"bootstrap.servers\": \"localhost:9092\" },\n  topic: \"metrics\"\n};\n\nfor (i in 0..100) {\n  writer.write(i);\n}\n```\n\n### KafkaWriter.flush()\n\nBlocks until all buffered messages are delivered.\n\n**Signature:** `fn flush()`\n\n**Behavior:**\n- Waits for acknowledgment of all pending messages\n- Throws an error if any message fails to deliver\n- Should be called before application shutdown\n\n**Example:**\n\n```gcl\nvar writer = KafkaWriter<String> {\n  conf: KafkaConf {\n    \"bootstrap.servers\": \"localhost:9092\",\n    \"linger.ms\": \"100\" // Buffer for up to 100ms\n  },\n  topic: \"events\"\n};\n\nwriter.write(\"event1\");\nwriter.write(\"event2\");\nwriter.write(\"event3\");\n\n// Ensure all 3 messages are sent\nwriter.flush();\nprint(\"All messages delivered\");\n```\n\n## Common Use Cases\n\n### Event Sourcing\n\n```gcl\ntype UserEvent {\n  user_id: String;\n  event_type: String;\n  payload: Map<String, any>;\n  timestamp: time;\n}\n\n// Producer side\nvar eventWriter = KafkaWriter<UserEvent> {\n  conf: KafkaConf {\n    \"bootstrap.servers\": \"kafka:9092\",\n    \"acks\": \"all\",\n    \"enable.idempotence\": \"true\"\n  },\n  topic: \"user-events\"\n};\n\nfn recordUserAction(userId: String, action: String, data: Map<String, any>) {\n  eventWriter.write(UserEvent {\n    user_id: userId,\n    event_type: action,\n    payload: data,\n    timestamp: time::now()\n  });\n}\n\n// Consumer side - rebuild state from events\nvar eventReader = KafkaReader<UserEvent> {\n  conf: KafkaConf {\n    \"bootstrap.servers\": \"kafka:9092\",\n    \"group.id\": \"state-rebuilder\",\n    \"auto.offset.reset\": \"earliest\"\n  },\n  topics: [\"user-events\"],\n  timeout: 60s\n};\n\nvar userStates = Map<String, any>{};\nwhile (true) {\n  var msg = eventReader.read();\n  applyEvent(userStates, msg.value);\n}\n```\n\n### Multi-Topic Consumer\n\n```gcl\ntype LogEntry {\n  level: String;\n  message: String;\n  source: String;\n  time: time;\n}\n\nvar multiReader = KafkaReader<LogEntry> {\n  conf: KafkaConf {\n    \"bootstrap.servers\": \"localhost:9092\",\n    \"group.id\": \"log-aggregator\"\n  },\n  topics: [\"app-logs\", \"system-logs\", \"audit-logs\"],\n  timeout: 15s\n};\n\nwhile (true) {\n  var msg = multiReader.read();\n\n  // Process based on source topic\n  if (msg.topic == \"audit-logs\") {\n    storeToDatabase(msg.value);\n  } else {\n    writeToFile(msg.topic, msg.value);\n  }\n}\n```\n\n### High-Throughput Producer\n\n```gcl\ntype Metric {\n  name: String;\n  value: float;\n  tags: Map<String, String>;\n  timestamp: time;\n}\n\nvar metricsWriter = KafkaWriter<Metric> {\n  conf: KafkaConf {\n    \"bootstrap.servers\": \"kafka1:9092,kafka2:9092\",\n    \"compression.type\": \"lz4\",\n    \"batch.size\": \"65536\",\n    \"linger.ms\": \"50\",\n    \"acks\": \"1\",\n    \"max.in.flight.requests.per.connection\": \"5\"\n  },\n  topic: \"metrics\"\n};\n\n// Write 10k metrics\nfor (i in 0..10000) {\n  metricsWriter.write(Metric {\n    name: \"cpu.usage\",\n    value: random(0.0, 100.0),\n    tags: Map<String, String>{ \"host\": \"server-${i % 10}\" },\n    timestamp: time::now()\n  });\n}\n\n// Ensure all sent before shutdown\nmetricsWriter.flush();\n```\n\n### Request-Response Pattern\n\n```gcl\ntype Request {\n  request_id: String;\n  operation: String;\n  params: Map<String, any>;\n}\n\ntype Response {\n  request_id: String;\n  result: any;\n  error: String?;\n}\n\n// Send request\nvar requestWriter = KafkaWriter<Request> {\n  conf: KafkaConf { \"bootstrap.servers\": \"localhost:9092\" },\n  topic: \"requests\"\n};\n\nrequestWriter.write(Request {\n  request_id: \"req-123\",\n  operation: \"calculate\",\n  params: Map<String, any>{ \"x\": 10, \"y\": 20 }\n});\n\n// Wait for response\nvar responseReader = KafkaReader<Response> {\n  conf: KafkaConf {\n    \"bootstrap.servers\": \"localhost:9092\",\n    \"group.id\": \"client-123\"\n  },\n  topics: [\"responses\"],\n  timeout: 30s\n};\n\nvar resp = responseReader.read();\nif (resp.value.request_id == \"req-123\") {\n  print(\"Result: ${resp.value.result}\");\n}\n```\n\n## Best Practices\n\n### Connection Management\n\n- **Reuse readers/writers**: Create once per application, not per message\n- **Set appropriate timeouts**: Match `timeout` to your latency requirements\n- **Use connection pooling**: Share `KafkaConf` instances when possible\n\n```gcl\n// Good: Single reader instance\nvar reader = createReader();\nfor (i in 0..1000) {\n  var msg = reader.read();\n  process(msg);\n}\n\n// Bad: Creating reader per read\nfor (i in 0..1000) {\n  var reader = createReader(); // Wasteful!\n  var msg = reader.read();\n  process(msg);\n}\n```\n\n### Error Handling\n\n- **Catch timeout exceptions** when reading\n- **Flush before shutdown** to avoid data loss\n- **Monitor consumer lag** in production\n\n```gcl\nvar running = true;\n\nwhile (running) {\n  try {\n    var msg = reader.read();\n    process(msg);\n  } catch (TimeoutException e) {\n    // Expected during low traffic\n    continue;\n  } catch (KafkaException e) {\n    print(\"Fatal error: ${e}\");\n    running = false;\n  }\n}\n```\n\n### Performance Optimization\n\n- **Batch writes**: Use `linger.ms` to batch small messages\n- **Compress data**: Enable `compression.type` for large payloads\n- **Tune fetch sizes**: Adjust `fetch.min.bytes` and `fetch.max.wait.ms` for consumers\n\n```gcl\n// Optimized for throughput\nvar highThroughputConf = KafkaConf {\n  \"bootstrap.servers\": \"kafka:9092\",\n  \"batch.size\": \"32768\",\n  \"linger.ms\": \"20\",\n  \"compression.type\": \"snappy\",\n  \"acks\": \"1\"\n};\n\n// Optimized for latency\nvar lowLatencyConf = KafkaConf {\n  \"bootstrap.servers\": \"kafka:9092\",\n  \"linger.ms\": \"0\",\n  \"acks\": \"1\",\n  \"compression.type\": \"none\"\n};\n```\n\n### Consumer Groups\n\n- **Use meaningful group IDs**: Makes monitoring easier\n- **Scale consumers horizontally**: Add more consumers to the same group\n- **Partition count**: Ensure partitions >= consumer count for parallelism\n\n```gcl\n// Multiple consumers in same group share load\nvar conf = KafkaConf {\n  \"bootstrap.servers\": \"kafka:9092\",\n  \"group.id\": \"order-processors\" // Same group = load balanced\n};\n\n// Consumer 1\nvar reader1 = KafkaReader<Order> { conf: conf, topics: [\"orders\"] };\n\n// Consumer 2 (can run on different machine)\nvar reader2 = KafkaReader<Order> { conf: conf, topics: [\"orders\"] };\n```\n\n### Security\n\n- **Use SSL/TLS**: Encrypt data in transit\n- **Enable SASL**: Authenticate clients\n- **Avoid hardcoding credentials**: Load from environment or config files\n\n```gcl\n// Load from environment\nvar conf = KafkaConf {\n  \"bootstrap.servers\": env(\"KAFKA_BROKERS\"),\n  \"security.protocol\": \"SASL_SSL\",\n  \"sasl.mechanism\": \"SCRAM-SHA-512\",\n  \"sasl.username\": env(\"KAFKA_USER\"),\n  \"sasl.password\": env(\"KAFKA_PASSWORD\")\n};\n```\n\n### Gotchas\n\n- **Type safety**: The generic `T` must be the same type used by producers\n- **Serialization**: Complex nested types are supported but may impact performance\n- **Offset management**: Auto-commit may lead to message loss on crashes; consider manual commits for critical data\n- **Timeout errors**: Don't confuse timeout (no messages) with connection errors\n- **Flush is blocking**: May take significant time with large batches\n",
        "plugins/greycat/skills/greycat/references/lsp.md": "# GreyCat Language Server Protocol (LSP)\n\n**Version**: 7.6.0-dev | **Protocol**: JSON-RPC 2.0 over stdio | **Language**: GreyCat (GCL)\n\n## Overview\n\nThe GreyCat LSP provides IDE-like features for `.gcl` files through the [Language Server Protocol](https://microsoft.github.io/language-server-protocol/). Works with VS Code, IntelliJ, Neovim, Emacs, and any LSP-compatible editor.\n\n**Architecture**: `Editor (Client) ◄─── JSON-RPC over stdio ───► greycat-lang server ──► Project Files (*.gcl)`\n\n**Benefits**:\n| Feature | Without LSP | With LSP |\n|---------|-------------|----------|\n| Error Detection | After `greycat-lang lint` | As you type |\n| Type Information | Manual docs | Hover tooltip |\n| Navigation | Text search | Go to definition (Ctrl+Click) |\n| Refactoring | Find/replace (risky) | Safe rename across project |\n| Code Discovery | Browse files | Autocomplete members |\n| Formatting | Manual | Auto-format on save |\n\n## Quick Start\n\n### Starting the Server\n\n**IDE Integration** (most common):\n```bash\ngreycat-lang server --stdio\n```\nServer communicates via stdin/stdout using JSON-RPC 2.0. Logs go to stderr.\n\n**Background Service** (for programmatic access):\n```bash\nmkfifo /tmp/greycat-lsp-{in,out}\ngreycat-lang server --stdio < /tmp/greycat-lsp-in > /tmp/greycat-lsp-out 2> /tmp/greycat-lsp.log &\nLSP_PID=$!\n\n# Cleanup: kill $LSP_PID && rm /tmp/greycat-lsp-{in,out,log}\n```\n\n**When to use background LSP**:\n- ✅ Programmatic queries during development (Claude Code integration)\n- ✅ Custom tooling needing real-time code intelligence\n- ✅ Batch processing (avoid startup overhead)\n- ❌ Regular IDE usage (IDE manages lifecycle)\n- ❌ CI/CD (use `greycat-lang lint` instead)\n\n### IDE Configuration\n\n**VS Code**:\n```json\n{\n  \"greycat.languageServer.path\": \"/path/to/greycat-lang\",\n  \"greycat.languageServer.args\": [\"server\", \"--stdio\"]\n}\n```\n\n**Neovim** (nvim-lspconfig):\n```lua\nrequire'lspconfig'.greycat.setup{\n  cmd = {\"greycat-lang\", \"server\", \"--stdio\"},\n  filetypes = {\"greycat\"},\n  root_dir = require'lspconfig'.util.root_pattern(\"project.gcl\", \".git\")\n}\n```\n\n**Emacs** (lsp-mode):\n```elisp\n(lsp-register-client\n (make-lsp-client :new-connection (lsp-stdio-connection '(\"greycat-lang\" \"server\" \"--stdio\"))\n                  :major-modes '(greycat-mode)\n                  :server-id 'greycat))\n```\n\n### Verify Installation\n```bash\ngreycat-lang --version\necho 'Content-Length: 2\\r\\n\\r\\n{}' | greycat-lang server --stdio  # Should return JSON-RPC\n```\n\n## LSP Capabilities\n\n**Core capabilities**:\n1. **textDocumentSync** - Real-time file tracking (incremental sync)\n2. **completionProvider** - Type-aware autocomplete (`.`, `>`, `:`, `@` triggers)\n3. **hoverProvider** - Inline documentation\n4. **definitionProvider** - Go to definition\n5. **referencesProvider** - Find all references\n6. **renameProvider** - Safe project-wide renaming\n7. **documentSymbolProvider** - File outline\n8. **documentFormattingProvider** - Code formatting\n9. **signatureHelpProvider** - Parameter hints\n10. **codeActionProvider** - Quick fixes\n11. **inlayHintProvider** - Inline type annotations\n12. **codeLensProvider** - Actionable insights\n13. **semanticTokensProvider** - AST-based syntax highlighting\n14. **workspace** - Multi-file support, file watching\n\n## Protocol Communication\n\n### Message Format\nAll messages use Content-Length headers:\n```\nContent-Length: 123\\r\\n\n\\r\\n\n{\"jsonrpc\":\"2.0\",\"id\":1,\"method\":\"initialize\",\"params\":{...}}\n```\n\n### Initialization Sequence\n\n**1. Client → Server: initialize**\n```json\n{\n  \"jsonrpc\": \"2.0\", \"id\": 1, \"method\": \"initialize\",\n  \"params\": {\n    \"processId\": 12345,\n    \"rootUri\": \"file:///home/user/project\",\n    \"capabilities\": {\"textDocument\": {\"completion\": {}, \"hover\": {}, \"definition\": {}}}\n  }\n}\n```\n\n**2. Server → Client: capabilities**\n```json\n{\n  \"jsonrpc\": \"2.0\", \"id\": 1,\n  \"result\": {\n    \"serverInfo\": {\"name\": \"GreyCat LSP\", \"version\": \"7.6.0-dev\"},\n    \"capabilities\": {\n      \"completionProvider\": {\"triggerCharacters\": [\".\", \">\", \":\", \"@\"]},\n      \"hoverProvider\": true,\n      \"definitionProvider\": true\n    }\n  }\n}\n```\n\n**3. Client → Server: initialized (notification)**\n```json\n{\"jsonrpc\": \"2.0\", \"method\": \"initialized\", \"params\": {}}\n```\n\n**4. Server → Client: diagnostics (automatic)**\n```json\n{\n  \"jsonrpc\": \"2.0\",\n  \"method\": \"textDocument/publishDiagnostics\",\n  \"params\": {\n    \"uri\": \"file:///path/to/file.gcl\",\n    \"diagnostics\": [{\n      \"range\": {\"start\": {\"line\": 10, \"character\": 5}, \"end\": {\"line\": 10, \"character\": 15}},\n      \"severity\": 1,  // 1=Error, 2=Warning, 3=Info, 4=Hint\n      \"message\": \"Type mismatch: expected String, got int\"\n    }]\n  }\n}\n```\n\n### Shutdown Sequence\n```json\n// 1. Client → Server: shutdown\n{\"jsonrpc\": \"2.0\", \"id\": 99, \"method\": \"shutdown\"}\n\n// 2. Server → Client: acknowledge\n{\"jsonrpc\": \"2.0\", \"id\": 99, \"result\": null}\n\n// 3. Client → Server: exit (no response)\n{\"jsonrpc\": \"2.0\", \"method\": \"exit\"}\n```\n\n## Programmatic Integration\n\n### Python Client Example\n\n```python\nimport subprocess, json, time, select\n\nclass GreyCatLSP:\n    def __init__(self, root_path):\n        self.proc = subprocess.Popen(\n            [\"greycat-lang\", \"server\", \"--stdio\"],\n            stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE, bufsize=0\n        )\n        self.message_id = 0\n        self.root_uri = f\"file://{root_path}\"\n        self._initialize()\n\n    def _send(self, message):\n        content = json.dumps(message)\n        header = f\"Content-Length: {len(content)}\\r\\n\\r\\n\"\n        self.proc.stdin.write((header + content).encode())\n        self.proc.stdin.flush()\n\n    def _read_responses(self, timeout=2.0):\n        responses = []\n        end_time = time.time() + timeout\n        buffer = b\"\"\n\n        while time.time() < end_time:\n            ready, _, _ = select.select([self.proc.stdout], [], [], 0.1)\n            if ready:\n                chunk = self.proc.stdout.read(4096)\n                if not chunk: break\n                buffer += chunk\n\n                while b\"Content-Length:\" in buffer:\n                    header_end = buffer.find(b\"\\r\\n\\r\\n\")\n                    if header_end == -1: break\n\n                    header = buffer[:header_end].decode()\n                    length = int(header.split(\":\")[1].strip())\n                    content_start = header_end + 4\n                    content_end = content_start + length\n\n                    if len(buffer) < content_end: break\n\n                    content = buffer[content_start:content_end].decode()\n                    responses.append(json.loads(content))\n                    buffer = buffer[content_end:]\n\n            if responses and time.time() > end_time - 0.5: break\n\n        return responses\n\n    def _initialize(self):\n        self.message_id += 1\n        self._send({\n            \"jsonrpc\": \"2.0\", \"id\": self.message_id, \"method\": \"initialize\",\n            \"params\": {\n                \"processId\": None, \"rootUri\": self.root_uri,\n                \"capabilities\": {\"textDocument\": {\"completion\": {}, \"hover\": {}, \"definition\": {}, \"references\": {}}}\n            }\n        })\n        self._read_responses(2.0)\n        self._send({\"jsonrpc\": \"2.0\", \"method\": \"initialized\", \"params\": {}})\n        time.sleep(0.3)\n\n    def open_file(self, file_path, content):\n        self._send({\n            \"jsonrpc\": \"2.0\", \"method\": \"textDocument/didOpen\",\n            \"params\": {\n                \"textDocument\": {\"uri\": f\"file://{file_path}\", \"languageId\": \"greycat\", \"version\": 1, \"text\": content}\n            }\n        })\n        time.sleep(0.3)\n        return self._read_responses(1.0)\n\n    def get_completion(self, file_path, line, character):\n        self.message_id += 1\n        msg_id = self.message_id\n        self._send({\n            \"jsonrpc\": \"2.0\", \"id\": msg_id, \"method\": \"textDocument/completion\",\n            \"params\": {\"textDocument\": {\"uri\": f\"file://{file_path}\"}, \"position\": {\"line\": line, \"character\": character}}\n        })\n        responses = self._read_responses(2.0)\n        for resp in responses:\n            if resp.get(\"id\") == msg_id and \"result\" in resp:\n                return resp[\"result\"]\n        return None\n\n    def get_hover(self, file_path, line, character):\n        self.message_id += 1\n        msg_id = self.message_id\n        self._send({\n            \"jsonrpc\": \"2.0\", \"id\": msg_id, \"method\": \"textDocument/hover\",\n            \"params\": {\"textDocument\": {\"uri\": f\"file://{file_path}\"}, \"position\": {\"line\": line, \"character\": character}}\n        })\n        responses = self._read_responses(2.0)\n        for resp in responses:\n            if resp.get(\"id\") == msg_id and \"result\" in resp:\n                return resp[\"result\"]\n        return None\n\n    def shutdown(self):\n        self._send({\"jsonrpc\": \"2.0\", \"id\": 99, \"method\": \"shutdown\"})\n        time.sleep(0.2)\n        self.proc.terminate()\n```\n\n## Common Use Cases\n\n### Claude Code Integration\n```python\nclass ClaudeLSPHelper:\n    \"\"\"Helper for Claude Code to query LSP during development\"\"\"\n\n    def __init__(self, project_root):\n        self.lsp = GreyCatLSP(project_root)\n\n    def get_type_at_position(self, file_path, line, char):\n        with open(file_path) as f:\n            content = f.read()\n        self.lsp.open_file(file_path, content)\n        hover = self.lsp.get_hover(file_path, line, char)\n        return hover[\"contents\"][\"value\"] if hover and hover.get(\"contents\") else None\n\n    def get_completions_for_member(self, file_path, line, char):\n        with open(file_path) as f:\n            content = f.read()\n        self.lsp.open_file(file_path, content)\n        result = self.lsp.get_completion(file_path, line, char)\n        items = result if isinstance(result, list) else result.get(\"items\", [])\n        return [item.get(\"label\") for item in items]\n\n    def verify_no_errors(self, file_path):\n        with open(file_path) as f:\n            content = f.read()\n        diagnostics = self.lsp.open_file(file_path, content)\n        for response in diagnostics:\n            if response.get(\"method\") == \"textDocument/publishDiagnostics\":\n                errors = [d for d in response[\"params\"].get(\"diagnostics\", []) if d.get(\"severity\") == 1]\n                if errors:\n                    return False, errors\n        return True, []\n```\n\n**Benefits**:\n- ✅ Real-time type checking without `greycat-lang lint`\n- ✅ Intelligent API/field suggestions\n- ✅ Catch errors before committing\n- ✅ Faster development iteration\n\n### Pre-Commit Validation\n```bash\n#!/bin/bash\n# pre-commit hook: validate .gcl files with LSP\n\nfor file in $(git diff --cached --name-only --diff-filter=ACM | grep '\\.gcl$'); do\n  echo \"Validating $file...\"\n  python3 validate_gcl.py \"$file\" || exit 1\ndone\necho \"✅ All files validated\"\n```\n\n## Troubleshooting\n\n### Common Issues\n\n**LSP not starting**:\n```bash\nwhich greycat-lang\ngreycat-lang --version\necho 'Content-Length: 2\\r\\n\\r\\n{}' | greycat-lang server --stdio  # Test manually\n```\n\n**No completions/hover**:\n- Ensure file opened via `textDocument/didOpen`\n- Check trigger characters: `.`, `>`, `:`, `@`\n- Verify workspace root is correct\n- Allow 2-5 seconds for initial indexing\n\n**Diagnostics not updating**:\n- Check file sync: `textDocument/didChange` after edits\n- Verify `textDocumentSync` capability enabled\n- Check stderr logs for errors\n\n**Performance issues**:\n- Large projects (>1000 files) take time to index\n- Consider splitting into multiple workspaces\n- Check memory limits\n\n### Debug Logging\n```bash\nexport GREYCAT_LSP_DEBUG=1\ngreycat-lang server --stdio 2> lsp-debug.log\n```\n\n## LSP vs. CLI Tools\n\n| Feature | LSP (Real-time) | CLI (`greycat-lang`) |\n|---------|----------------|---------------------|\n| Error Detection | As you type | After command |\n| Performance | Continuous daemon | One-shot execution |\n| Context | Full workspace | Single file/command |\n| Use Case | Interactive development | CI/CD, scripts |\n| Integration | Editor plugins | Shell scripts |\n| Validation | Advisory warnings | Authoritative errors |\n\n**Best Practice**:\n- Use **LSP** during development for instant feedback\n- Use **CLI** (`greycat-lang lint`) as final verification before commit/CI\n- LSP diagnostics ≠ compile errors (may show stricter warnings)\n\n## Symbol Kinds (LSP Spec)\n\n| Kind | Name | Description |\n|------|------|-------------|\n| 1 | File | File-level scope |\n| 2 | Module | Module/namespace |\n| 5 | Class | Type definition |\n| 6 | Method | Type method |\n| 12 | Function | Top-level function |\n| 14 | Variable | Global/local variable |\n| 7 | Property | Type field/property |\n\n## Resources\n\n- **LSP Spec**: https://microsoft.github.io/language-server-protocol/\n- **GreyCat Docs**: https://doc.greycat.io/\n- **VS Code Extension**: GreyCat Language Support\n\n**Last Updated**: January 2026 | **Server Version**: 7.6.0-dev | **Protocol Version**: LSP 3.17\n",
        "plugins/greycat/skills/greycat/references/nodes.md": "# Nodes Deep Dive\n\n## Definition\n\nNodes are persistent pointers (64-bit IDs) to locations in the Graph. Enable disc persistence for objects that would otherwise exist only in RAM.\n\n```gcl\ntype Country { name: String; phoneCode: int; location: geo; }\n\nfn main() {\n    var luxembourg = Country { name: \"Luxembourg\", phoneCode: 352, location: geo{49.8153, 6.1296} };\n    // RAM only - not persisted\n\n    var n_lux = node<Country>{luxembourg};  // Now persisted to graph\n    println(n_lux);  // {\"_type\":\"core.node\",\"ref\":\"0100000000000000\"}\n}\n```\n\n## Node Operations\n\n```gcl\n*n;              // dereference (resolve)\nn->name;         // arrow: deref + field access (NOT (*n)->name)\nn.resolve();     // explicit method\nn->name = \"X\";   // modify object field\nnode<int>{0}.set(5);  // primitives use .set()\n```\n\n**Operators**:\n| Operator | Description |\n|----------|-------------|\n| `.` | Access fields/methods on instance |\n| `::` | Static access operator |\n| `->` | Arrow: traverses graph to access attribute/function |\n\n## Transactions\n\nChanges only committed after function completes successfully. Errors cause full rollback:\n\n```gcl\nvar init: node<bool?>;\n\n@expose\nfn update_with_fail() {\n    init.set(true);              // Attempt modification\n    throw \"An Error Occurred\";   // Causes rollback - init stays null\n}\n```\n\n## Object (heavy) vs Node (light)\n\n```gcl\n// Heavy - embeds full Country object in each City\ntype City { name: String; country: Country; }\n\n// Light - only 64-bit reference, shared across cities\ntype City { name: String; country: node<Country>; }\n```\n\n## Multi-References with Nodes\n\nObjects can only belong to ONE container. For multiple indices, store node refs:\n\n```gcl\nvar t_by_id = nodeList<node<T>>{};\nvar t_by_name = nodeIndex<String, node<T>>{};\n\nvar johnNode = node<T>{ T { id: 25473, name: \"John\" } };\nt_by_id.set(johnNode->id, johnNode);\nt_by_name.set(johnNode->name, johnNode);  // Both point to same node\n```\n\n## Modifying Node Content\n\nObjects are passed by reference, primitives by value:\n\n```gcl\n// Objects - modifications persist\nnCountry->name = \"Foo\";  // Works\n\n// Primitives - must use .set()\nvar val_ref = node<int>{ 0 };\nvar resolved_val = val_ref.resolve();\nresolved_val = 5;              // NO effect\nval_ref.set(5);                // Works\n```\n\n## Indexed Collections\n\n| Persisted | Key | In-Memory |\n|-----------|-----|-----------|\n| `node<T>` | — | `Array<T>`, `Map<K,V>` |\n| `nodeList<node<T>>` | int | `Stack<T>`, `Queue<T>` |\n| `nodeIndex<K, node<V>>` | hash | `Set<T>`, `Tuple<A,B>` |\n| `nodeTime<node<T>>` | time | `Buffer`, `Table`, `Tensor` |\n| `nodeGeo<node<T>>` | geo | `TimeWindow`, `SlidingWindow` |\n\n**nodeTime** - Indexes by time with interpolation:\n```gcl\nvar temps = nodeTime<float>{};\ntemps.setAt(t1, 20.5);\n\nfor (t: time, temp: float in temps[fromTime..toTime]) { println(\"Temperature was ${temp} at ${t}\"); }\n\ntemps.remove(t1);  // Remove by time\n```\n\n**nodeList** - Indexes by integer (64-bit):\n```gcl\nvar myStock = nodeList<Palette>{};\nfor (position: int, content: Palette in myStock[54..78]) { }\n\nmyStock.remove(55);  // Remove by index\n```\n\n**nodeGeo** - Indexes by geographical position:\n```gcl\nvar buildings = nodeGeo<node<Building>>{};\nfor (position: geo, building: Building in buildings.filter(GeoBox{...})) { }\n\nbuildings.remove(position);  // Remove by geo position\n```\n\n**nodeIndex** - Indexes by any hashable key (usually String):\n```gcl\nvar collaborators = nodeIndex<String, node<Person>>{};\ncollaborators.set(\"john\", johnNode);\nfor (name: String, collab: node<Person> in collaborators) { }\n\ncollaborators.remove(\"john\");  // Remove by key\n```\n\n> Use `remove` to delete entries. No `unset` method.\n\n## Sampling Large Collections\n\nAll node structures support static sampling for performance:\n\n```gcl\nvar result = nodeTime::sample(\n    [timeSeries],      // array of node structures\n    start, end,        // range\n    1000,              // max points\n    SamplingMode::adaptative,\n    null, null         // maxDephasing, timezone\n);\n```\n\n**SamplingMode**: `fixed` (fixed delta), `fixed_reg` (fixed + linear interpolation), `adaptative` (skip to limit results), `dense` (all elements, no sampling)\n",
        "plugins/greycat/skills/greycat/references/opcua/opcua.md": "# OPC UA Integration\n\nIndustrial automation integration with OPC UA servers for GreyCat.\n\n## Overview\n\nThe OPC UA (Open Platform Communications Unified Architecture) library enables GreyCat applications to communicate with industrial control systems, SCADA systems, PLCs, and IoT devices. OPC UA is the industry-standard protocol for industrial automation and data exchange.\n\nKey features include:\n- **Read/write operations** for single and multiple nodes\n- **Historical data access** with time-range queries\n- **Real-time subscriptions** for live data monitoring\n- **Event monitoring** for alarms and notifications\n- **Metadata browsing** to discover server capabilities\n- **Security support** with authentication, encryption, and certificates\n- **Server exploration** to traverse node hierarchies\n\nThis library is ideal for Industry 4.0 applications, SCADA integration, IoT data collection, predictive maintenance, and building automation systems.\n\n## Installation\n\nAdd the OPC UA library to your GreyCat project:\n\n```gcl\n@library(\"opcua\", \"7.6.60-dev\")\n```\n\n## Quick Start\n\n### Basic Read Operation\n\n```gcl\nvar client = OpcuaClient {\n  url: \"opc.tcp://localhost:4840\",\n  security_mode: OpcuaSecurityMode::None,\n  timeout: 30s\n};\n\n// Read a single value\nvar temperature = client.read(\"ns=2;s=Temperature\");\nprint(\"Temperature: ${temperature}\");\n\n// Read multiple values at once\nvar values = client.read_all([\n  \"ns=2;s=Temperature\",\n  \"ns=2;s=Pressure\",\n  \"ns=2;s=FlowRate\"\n]);\n```\n\n### Subscribe to Live Data\n\n```gcl\nvar client = OpcuaClient {\n  url: \"opc.tcp://192.168.1.100:4840\",\n  security_mode: OpcuaSecurityMode::None\n};\n\n// Subscribe to node changes\nvar subscriptionId = client.subscribe(\n  [\"ns=2;s=ProductionCounter\"],\n  fn(data) {\n    print(\"New value: ${data}\");\n  },\n  fn(error) {\n    print(\"Error: ${error}\");\n  }\n);\n\n// Later: cancel subscription\nclient.cancel_subscription(subscriptionId);\n```\n\n## Types\n\n### OpcuaClient\n\nMain client for connecting to OPC UA servers.\n\n**Fields:**\n- `url: String` (private) - Server endpoint URL (e.g., `\"opc.tcp://localhost:4840\"`)\n- `security_mode: OpcuaSecurityMode` (private) - Security level\n- `security_policy: OpcuaSecurityPolicy?` (private) - Encryption policy\n- `credentials: OpcuaCredentials?` (private) - Username/password authentication\n- `certificate: OpcuaCertificate?` (private) - Certificate-based authentication\n- `timeout: duration?` (private) - Operation timeout (default varies by operation)\n\n**Read Methods:**\n- `read(nodeId: String): any?` - Read single node value\n- `read_all(nodeIds: Array<String>): Array<any?>` - Read multiple nodes\n- `read_with_time(nodeId: String): OpcuaValueDetails?` - Read with timestamps\n- `read_all_with_time(nodeIds: Array<String>): Array<OpcuaValueDetails?>` - Read multiple with timestamps\n- `read_history(nodeId: String, from: time?, to: time?): Array<OpcuaValueDetails>?` - Read historical data\n\n**Write Methods:**\n- `write(nodeId: String, value: any)` - Write value to node\n\n**Browse Methods:**\n- `read_metas(nodesIds: Array<String>): Array<OpcuaMeta?>` - Read node metadata\n- `get_children(nodeId: String): Array<OpcuaMeta>` - Get child nodes\n- `explore(rootId: String): Array<Array<OpcuaMeta>>` - Explore node tree\n\n**Subscription Methods:**\n- `subscribe(nodeIds: Array<String>, callback_data: function, callback_error: function): int` - Subscribe to data changes\n- `read_events(nodeIds: Array<String>, callback_data: function, callback_error: function): int` - Subscribe to events\n- `cancel_subscription(subscription_id: int): bool` - Cancel subscription\n\n**Utility Methods:**\n- `call(nodeId: String, parameters: Array<any?>): any?` - Call server method\n- `is_connected(): bool` - Check connection status\n- `get_address_spaces(): Array` - Get server address spaces\n- `get_server_time(): time` - Get server timestamp\n- `find_endpoints(url: String, security_mode?, security_policy?): Array<OpcuaEndpointDescription>` (static) - Discover server endpoints\n\n**Example:**\n\n```gcl\n// Basic connection\nvar client = OpcuaClient {\n  url: \"opc.tcp://plc.factory.com:4840\",\n  security_mode: OpcuaSecurityMode::None\n};\n\n// Secure connection with credentials\nvar secureClient = OpcuaClient {\n  url: \"opc.tcp://secure-server.com:4840\",\n  security_mode: OpcuaSecurityMode::SignAndEncrypt,\n  security_policy: OpcuaSecurityPolicy::Basic256Sha256,\n  credentials: OpcuaCredentials {\n    login: \"operator\",\n    password: \"secret123\"\n  },\n  timeout: 60s\n};\n```\n\n### OpcuaValueDetails\n\nDetailed value information including timestamps and status.\n\n**Fields:**\n- `value: any` - The actual value\n- `source_time: time?` - When the value was generated at the source\n- `server_time: time?` - When the server processed the value\n- `status_code: int?` - OPC UA status code (0 = good)\n\n**Example:**\n\n```gcl\nvar details = client.read_with_time(\"ns=2;s=Sensor1\");\nprint(\"Value: ${details.value}\");\nprint(\"Source time: ${details.source_time}\");\nprint(\"Server time: ${details.server_time}\");\nprint(\"Status: ${details.status_code}\");\n```\n\n### OpcuaMeta\n\nNode metadata including properties and attributes.\n\n**Fields:**\n- `node_id: String` - Node identifier\n- `node_class: int?` - Node class (1=Object, 2=Variable, 4=Method, etc.)\n- `browse_name: Tuple?` - Node's browse name\n- `display_name: Tuple?` - Human-readable name\n- `description: Tuple?` - Node description\n- `data_type: String?` - Data type for variable nodes\n- `value: any?` - Current value\n- `access_level: int?` - Read/write permissions\n- `historizing: bool?` - Whether history is recorded\n\n**Example:**\n\n```gcl\nvar metas = client.read_metas([\"ns=2;s=Temperature\"]);\nvar meta = metas[0];\n\nprint(\"Node ID: ${meta.node_id}\");\nprint(\"Display Name: ${meta.display_name}\");\nprint(\"Data Type: ${meta.data_type}\");\nprint(\"Value: ${meta.value}\");\n```\n\n### OpcuaEvent\n\nEvent notification structure for alarms and conditions.\n\n**Fields:**\n- `\"EventId\": String` - Unique event identifier\n- `\"EventType\": String?` - Type of event\n- `\"Message\": Tuple?` - Event message\n- `\"SourceNode\": String` - Node that generated the event\n- `\"SourceName\": String` - Source name\n- `\"Time\": time` - Event timestamp\n- `\"ReceiveTime\": time?` - When event was received\n- `\"Severity\": int` - Severity level (1-1000)\n- `\"ConditionName\": String?` - Condition name for alarms\n- `\"ActiveState\": Tuple?` - Whether condition is active\n- `\"Retain\": bool?` - Whether event should be retained\n\n**Example:**\n\n```gcl\nclient.read_events(\n  [\"ns=2;i=5001\"], // Server object for all events\n  fn(event: OpcuaEvent) {\n    print(\"Event: ${event.'Message'}\");\n    print(\"Severity: ${event.'Severity'}\");\n    print(\"Source: ${event.'SourceName'}\");\n  },\n  fn(error) {\n    print(\"Event error: ${error}\");\n  }\n);\n```\n\n### OpcuaCredentials\n\nUsername/password authentication.\n\n**Fields:**\n- `login: String` - Username\n- `password: String` - Password\n\n**Example:**\n\n```gcl\nvar creds = OpcuaCredentials {\n  login: \"admin\",\n  password: env(\"OPCUA_PASSWORD\")\n};\n\nvar client = OpcuaClient {\n  url: \"opc.tcp://server:4840\",\n  security_mode: OpcuaSecurityMode::Sign,\n  credentials: creds\n};\n```\n\n### OpcuaCertificate\n\nCertificate-based authentication configuration.\n\n**Fields:**\n- `path: String?` - Path to client certificate file\n- `private_key_path: String?` - Path to private key file\n- `application_uri: String?` - Application URI\n- `allow_self_signed: bool?` - Accept self-signed certificates\n\n**Example:**\n\n```gcl\nvar cert = OpcuaCertificate {\n  path: \"/etc/opcua/client-cert.pem\",\n  private_key_path: \"/etc/opcua/client-key.pem\",\n  application_uri: \"urn:myapp:client\",\n  allow_self_signed: false\n};\n\nvar client = OpcuaClient {\n  url: \"opc.tcp://server:4840\",\n  security_mode: OpcuaSecurityMode::SignAndEncrypt,\n  security_policy: OpcuaSecurityPolicy::Aes256_Sha256_RsaPss,\n  certificate: cert\n};\n```\n\n### OpcuaSecurityMode (Enum)\n\nSecurity mode levels.\n\n**Values:**\n- `None` - No security (plain text)\n- `Sign` - Message signing (integrity)\n- `SignAndEncrypt` - Signing + encryption (confidentiality + integrity)\n- `Invalid` - Invalid mode\n\n**Example:**\n\n```gcl\n// Development: No security\nvar devClient = OpcuaClient {\n  url: \"opc.tcp://localhost:4840\",\n  security_mode: OpcuaSecurityMode::None\n};\n\n// Production: Full security\nvar prodClient = OpcuaClient {\n  url: \"opc.tcp://production-server:4840\",\n  security_mode: OpcuaSecurityMode::SignAndEncrypt\n};\n```\n\n### OpcuaSecurityPolicy (Enum)\n\nEncryption and signing algorithms.\n\n**Values:**\n- `None` - No encryption\n- `Basic128Rsa15` - 128-bit encryption (deprecated)\n- `Basic256` - 256-bit encryption (deprecated)\n- `Basic256Sha256` - 256-bit with SHA-256\n- `Aes128_Sha256_RsaOaep` - AES-128 with SHA-256\n- `Aes256_Sha256_RsaPss` - AES-256 with SHA-256 (strongest)\n\n**Example:**\n\n```gcl\nvar client = OpcuaClient {\n  url: \"opc.tcp://server:4840\",\n  security_mode: OpcuaSecurityMode::SignAndEncrypt,\n  security_policy: OpcuaSecurityPolicy::Aes256_Sha256_RsaPss // Strongest\n};\n```\n\n### OpcuaEndpointDescription\n\nServer endpoint information from discovery.\n\n**Fields:**\n- `url: String` - Endpoint URL\n- `discovery_urls: Array` - Discovery URLs\n- `certificate: String?` - Server certificate\n- `security_mode: OpcuaSecurityMode` - Security mode\n- `security_policy: OpcuaSecurityPolicy` - Security policy\n- `user_identity_token_ids: Array` - Supported authentication methods\n- `transport_profile_uri: String` - Transport protocol\n- `security_level: int` - Security level\n\n**Example:**\n\n```gcl\nvar endpoints = OpcuaClient::find_endpoints(\"opc.tcp://server:4840\", null, null);\n\nfor (endpoint in endpoints) {\n  print(\"URL: ${endpoint.url}\");\n  print(\"Security Mode: ${endpoint.security_mode}\");\n  print(\"Security Policy: ${endpoint.security_policy}\");\n  print(\"Security Level: ${endpoint.security_level}\");\n}\n```\n\n## Methods\n\n### read()\n\nReads the current value of a single node.\n\n**Signature:** `fn read(nodeId: String): any?`\n\n**Returns:** The node's value (type depends on node), or `null` if not readable\n\n**Example:**\n\n```gcl\nvar temp = client.read(\"ns=2;s=Temperature\") as float;\nvar running = client.read(\"ns=2;s=MotorRunning\") as bool;\nvar name = client.read(\"ns=2;s=DeviceName\") as String;\n```\n\n### read_all()\n\nReads multiple nodes in a single request.\n\n**Signature:** `fn read_all(nodeIds: Array<String>): Array<any?>`\n\n**Returns:** Array of values in same order as input node IDs\n\n**Example:**\n\n```gcl\nvar nodeIds = [\n  \"ns=2;s=Temperature\",\n  \"ns=2;s=Pressure\",\n  \"ns=2;s=FlowRate\"\n];\n\nvar values = client.read_all(nodeIds);\nvar temp = values[0] as float;\nvar pressure = values[1] as float;\nvar flow = values[2] as float;\n\nprint(\"Temp: ${temp}, Pressure: ${pressure}, Flow: ${flow}\");\n```\n\n### write()\n\nWrites a value to a node.\n\n**Signature:** `fn write(nodeId: String, value: any)`\n\n**Example:**\n\n```gcl\n// Write different types\nclient.write(\"ns=2;s=Setpoint\", 75.5);\nclient.write(\"ns=2;s=Enable\", true);\nclient.write(\"ns=2;s=Mode\", \"AUTO\");\n\n// Control a valve\nclient.write(\"ns=2;s=ValvePosition\", 50); // 50% open\n```\n\n### read_with_time()\n\nReads a node value with timestamp information.\n\n**Signature:** `fn read_with_time(nodeId: String): OpcuaValueDetails?`\n\n**Example:**\n\n```gcl\nvar details = client.read_with_time(\"ns=2;s=Sensor\");\n\nprint(\"Value: ${details.value}\");\nprint(\"Source timestamp: ${details.source_time}\");\nprint(\"Server timestamp: ${details.server_time}\");\nprint(\"Quality: ${details.status_code}\"); // 0 = good\n```\n\n### read_history()\n\nReads historical data for a node within a time range.\n\n**Signature:** `fn read_history(nodeId: String, from: time?, to: time?): Array<OpcuaValueDetails>?`\n\n**Parameters:**\n- `nodeId: String` - Node to query\n- `from: time?` - Start time (null = beginning)\n- `to: time?` - End time (null = now)\n\n**Example:**\n\n```gcl\n// Last hour of data\nvar history = client.read_history(\n  \"ns=2;s=ProductionRate\",\n  time::now() - 1h,\n  time::now()\n);\n\nfor (entry in history) {\n  print(\"${entry.source_time}: ${entry.value}\");\n}\n\n// All historical data\nvar allHistory = client.read_history(\"ns=2;s=TotalCount\", null, null);\n```\n\n### subscribe()\n\nSubscribes to node value changes for real-time monitoring.\n\n**Signature:** `fn subscribe(nodeIds: Array<String>, callback_data: function, callback_error: function): int`\n\n**Returns:** Subscription ID for later cancellation\n\n**Example:**\n\n```gcl\nvar subId = client.subscribe(\n  [\"ns=2;s=Temperature\", \"ns=2;s=Pressure\"],\n  fn(data) {\n    print(\"Data update: ${data}\");\n  },\n  fn(error) {\n    print(\"Subscription error: ${error}\");\n  }\n);\n\n// Keep running to receive updates\nwhile (client.is_connected()) {\n  // Your application logic\n  sleep(1s);\n}\n\n// Cleanup\nclient.cancel_subscription(subId);\n```\n\n### read_events()\n\nSubscribes to OPC UA events (alarms, notifications).\n\n**Signature:** `fn read_events(nodeIds: Array<String>, callback_data: function, callback_error: function): int`\n\n**Example:**\n\n```gcl\nvar eventSubId = client.read_events(\n  [\"ns=2;i=5001\"], // Server object\n  fn(event: OpcuaEvent) {\n    if (event.'Severity' > 500) {\n      print(\"HIGH SEVERITY EVENT!\");\n      print(\"Message: ${event.'Message'}\");\n      print(\"Source: ${event.'SourceName'}\");\n    }\n  },\n  fn(error) {\n    print(\"Event error: ${error}\");\n  }\n);\n```\n\n### get_children()\n\nGets all child nodes of a given node.\n\n**Signature:** `fn get_children(nodeId: String): Array<OpcuaMeta>`\n\n**Example:**\n\n```gcl\n// Discover structure\nvar children = client.get_children(\"ns=2;s=Building\");\n\nfor (child in children) {\n  print(\"Child: ${child.display_name} (${child.node_id})\");\n}\n```\n\n### explore()\n\nRecursively explores the node tree from a root node.\n\n**Signature:** `fn explore(rootId: String): Array<Array<OpcuaMeta>>`\n\n**Returns:** Array of paths, where each path is an array of metadata from root to leaf\n\n**Example:**\n\n```gcl\n// Explore entire device tree\nvar paths = client.explore(\"ns=2;s=Factory\");\n\nfor (path in paths) {\n  var pathStr = \"\";\n  for (node in path) {\n    pathStr = pathStr + \" / \" + node.display_name;\n  }\n  print(pathStr);\n}\n```\n\n### call()\n\nCalls a method on the OPC UA server.\n\n**Signature:** `fn call(nodeId: String, parameters: Array<any?>): any?`\n\n**Example:**\n\n```gcl\n// Call a method with parameters\nvar result = client.call(\n  \"ns=2;s=Calculator.Add\",\n  [10, 20]\n);\nprint(\"10 + 20 = ${result}\");\n\n// Start a process\nclient.call(\"ns=2;s=Process.Start\", []);\n```\n\n## Common Use Cases\n\n### Real-Time Monitoring Dashboard\n\n```gcl\nvar client = OpcuaClient {\n  url: \"opc.tcp://factory-plc:4840\",\n  security_mode: OpcuaSecurityMode::None\n};\n\n// Subscribe to critical parameters\nvar sensors = [\n  \"ns=2;s=Line1.Temperature\",\n  \"ns=2;s=Line1.Pressure\",\n  \"ns=2;s=Line1.Speed\",\n  \"ns=2;s=Line1.Status\"\n];\n\nvar subId = client.subscribe(\n  sensors,\n  fn(data) {\n    // Update dashboard\n    updateDashboard(data);\n\n    // Check thresholds\n    if (data.temperature > 100.0) {\n      sendAlert(\"Temperature too high: ${data.temperature}\");\n    }\n  },\n  fn(error) {\n    print(\"Monitoring error: ${error}\");\n    reconnect();\n  }\n);\n```\n\n### Historical Data Analysis\n\n```gcl\nvar client = OpcuaClient {\n  url: \"opc.tcp://scada-server:4840\",\n  security_mode: OpcuaSecurityMode::None\n};\n\n// Analyze production efficiency over last 24 hours\nvar history = client.read_history(\n  \"ns=2;s=ProductionLine.Output\",\n  time::now() - 24h,\n  time::now()\n);\n\nvar total = 0.0;\nvar count = 0;\n\nfor (entry in history) {\n  total = total + (entry.value as float);\n  count = count + 1;\n}\n\nvar average = total / count;\nprint(\"24h Average Output: ${average}\");\n```\n\n### Multi-Server Data Collection\n\n```gcl\ntype PlcConnection {\n  name: String;\n  client: OpcuaClient;\n  nodes: Array<String>;\n}\n\nvar plcs = [\n  PlcConnection {\n    name: \"Line 1\",\n    client: OpcuaClient {\n      url: \"opc.tcp://plc1:4840\",\n      security_mode: OpcuaSecurityMode::None\n    },\n    nodes: [\"ns=2;s=Speed\", \"ns=2;s=Count\"]\n  },\n  PlcConnection {\n    name: \"Line 2\",\n    client: OpcuaClient {\n      url: \"opc.tcp://plc2:4840\",\n      security_mode: OpcuaSecurityMode::None\n    },\n    nodes: [\"ns=2;s=Speed\", \"ns=2;s=Count\"]\n  }\n];\n\n// Poll all PLCs\nfor (plc in plcs) {\n  var values = plc.client.read_all(plc.nodes);\n  print(\"${plc.name}: Speed=${values[0]}, Count=${values[1]}\");\n}\n```\n\n### Automated Control Logic\n\n```gcl\nvar client = OpcuaClient {\n  url: \"opc.tcp://control-system:4840\",\n  security_mode: OpcuaSecurityMode::None\n};\n\n// Automated temperature control\nwhile (true) {\n  var temp = client.read(\"ns=2;s=Reactor.Temperature\") as float;\n  var setpoint = client.read(\"ns=2;s=Reactor.Setpoint\") as float;\n\n  if (temp > setpoint + 5.0) {\n    // Too hot, increase cooling\n    client.write(\"ns=2;s=CoolingValve\", 100);\n  } else if (temp < setpoint - 5.0) {\n    // Too cold, reduce cooling\n    client.write(\"ns=2;s=CoolingValve\", 0);\n  } else {\n    // Within range, proportional control\n    var valve = ((setpoint - temp) / 10.0) * 100;\n    client.write(\"ns=2;s=CoolingValve\", valve);\n  }\n\n  sleep(5s);\n}\n```\n\n## Best Practices\n\n### Connection Management\n\n- **Check connection status**: Use `is_connected()` before operations\n- **Handle disconnections**: Implement reconnection logic\n- **Reuse clients**: Create one client per server, not per operation\n\n```gcl\nvar client = createOpcuaClient();\n\nfn readSafely(nodeId: String): any? {\n  if (!client.is_connected()) {\n    print(\"Not connected, attempting reconnect...\");\n    client = createOpcuaClient();\n  }\n\n  try {\n    return client.read(nodeId);\n  } catch (e) {\n    print(\"Read failed: ${e}\");\n    return null;\n  }\n}\n```\n\n### Security\n\n- **Use encryption in production**: Never use `None` security mode over public networks\n- **Validate certificates**: Set `allow_self_signed: false` in production\n- **Store credentials securely**: Use environment variables or secret management\n\n```gcl\n// Production-grade security\nvar prodClient = OpcuaClient {\n  url: \"opc.tcp://production.server:4840\",\n  security_mode: OpcuaSecurityMode::SignAndEncrypt,\n  security_policy: OpcuaSecurityPolicy::Aes256_Sha256_RsaPss,\n  credentials: OpcuaCredentials {\n    login: env(\"OPCUA_USER\"),\n    password: env(\"OPCUA_PASSWORD\")\n  },\n  certificate: OpcuaCertificate {\n    path: \"/etc/opcua/client-cert.pem\",\n    private_key_path: \"/etc/opcua/client-key.pem\",\n    allow_self_signed: false\n  }\n};\n```\n\n### Node ID Format\n\n- **Namespace index**: `ns=<number>` (e.g., `ns=2`)\n- **String identifier**: `s=<string>` (e.g., `ns=2;s=Temperature`)\n- **Numeric identifier**: `i=<number>` (e.g., `ns=0;i=2258`)\n- **GUID identifier**: `g=<guid>` (e.g., `ns=2;g=12345678-...`)\n\n```gcl\n// Different node ID formats\nvar temp1 = client.read(\"ns=2;s=Sensor.Temperature\"); // String\nvar temp2 = client.read(\"ns=2;i=1001\"); // Numeric\nvar serverTime = client.read(\"ns=0;i=2258\"); // Standard node\n```\n\n### Performance\n\n- **Batch reads**: Use `read_all()` instead of multiple `read()` calls\n- **Subscription over polling**: More efficient for real-time data\n- **Limit history queries**: Use time ranges to avoid large result sets\n\n```gcl\n// Good: Single batch read\nvar values = client.read_all([\n  \"ns=2;s=Sensor1\",\n  \"ns=2;s=Sensor2\",\n  \"ns=2;s=Sensor3\"\n]);\n\n// Bad: Multiple individual reads\nvar val1 = client.read(\"ns=2;s=Sensor1\");\nvar val2 = client.read(\"ns=2;s=Sensor2\");\nvar val3 = client.read(\"ns=2;s=Sensor3\");\n```\n\n### Error Handling\n\n- **Type casting**: Read values are `any`, cast to expected type\n- **Null checks**: `read()` can return `null`\n- **Subscription errors**: Always provide error callback\n\n```gcl\ntry {\n  var temp = client.read(\"ns=2;s=Temperature\");\n\n  if (temp == null) {\n    print(\"Temperature not available\");\n  } else {\n    var tempValue = temp as float;\n    print(\"Temperature: ${tempValue}°C\");\n  }\n} catch (e) {\n  print(\"Read error: ${e}\");\n}\n```\n\n### Gotchas\n\n- **Subscription callbacks run asynchronously**: Don't block in callbacks\n- **Node IDs are server-specific**: Vary between vendors and configurations\n- **Historical data requires server support**: Not all servers provide history\n- **Write permissions**: Check `access_level` before attempting writes\n- **Security policies must match**: Client and server must agree on policy\n- **Timeout configuration**: Set appropriate timeout for slow/remote servers\n- **Cleanup subscriptions**: Always cancel when done to free server resources\n\n### Discovery\n\nUse `find_endpoints()` to discover server capabilities:\n\n```gcl\nvar endpoints = OpcuaClient::find_endpoints(\n  \"opc.tcp://unknown-server:4840\",\n  null,\n  null\n);\n\nprint(\"Server supports ${endpoints.size()} endpoints:\");\nfor (endpoint in endpoints) {\n  print(\"  ${endpoint.security_mode} / ${endpoint.security_policy}\");\n}\n\n// Connect with best security available\nvar bestEndpoint = endpoints[0]; // Often sorted by security level\nvar client = OpcuaClient {\n  url: bestEndpoint.url,\n  security_mode: bestEndpoint.security_mode,\n  security_policy: bestEndpoint.security_policy\n};\n```\n",
        "plugins/greycat/skills/greycat/references/patterns.md": "# GreyCat Design Patterns\n\nCommon design patterns and best practices for GreyCat development.\n\n## Service Pattern\n\nServices encapsulate business logic using `abstract type` with static functions. Separates domain operations from API layer and provides clean CRUD interfaces.\n\n### Basic Service\n\n```gcl\n// Model\ntype Country { name: String; code: String; population: int; }\n\n// Global indices\nvar countries_by_name: nodeIndex<String, node<Country>>;\nvar countries_by_code: nodeIndex<String, node<Country>>;\n\n// Service\nabstract type CountryService {\n    static fn create(name: String, code: String): node<Country> {\n        var country = node<Country>{ Country { name: name, code: code, population: 0 }};\n        countries_by_name.set(name, country);\n        countries_by_code.set(code, country);\n        return country;\n    }\n\n    static fn find_by_name(name: String): node<Country>? { return countries_by_name.get(name); }\n    static fn find_by_code(code: String): node<Country>? { return countries_by_code.get(code); }\n\n    static fn update(country: node<Country>, name: String?, code: String?) {\n        if (name != null) { country->name = name; }\n        if (code != null) { country->code = code; }\n    }\n\n    static fn delete(country: node<Country>) {\n        countries_by_name.remove(country->name);\n        countries_by_code.remove(country->code);\n    }\n\n    static fn list_all(): Array<node<Country>> {\n        var results = Array<node<Country>> {};\n        for (name, country in countries_by_name) { results.add(country); }\n        return results;\n    }\n}\n```\n\n### API Layer Usage\n\n```gcl\n// API types with @volatile\n@volatile type CountryView { name: String; code: String; population: int; }\n@volatile type CountryCreate { name: String; code: String; }\n\n// API endpoints use service\n@expose @permission(\"public\")\nfn get_countries(): Array<CountryView> {\n    var views = Array<CountryView> {};\n    for (country in CountryService::list_all()) {\n        views.add(CountryView { name: country->name, code: country->code, population: country->population });\n    }\n    return views;\n}\n\n@expose @permission(\"admin\")\nfn create_country(data: CountryCreate): CountryView {\n    var country = CountryService::create(data.name, data.code);\n    return CountryView { name: country->name, code: country->code, population: country->population };\n}\n```\n\n**Key principles**:\n- Service functions return `node<T>` (internal)\n- API functions return `Array<XxxView>` with `@volatile` types (external)\n- Never return `nodeList` or `nodeIndex` from API endpoints\n- Keep business logic in services, thin API layer\n\n## Abstract Types & Inheritance\n\nAbstract types enable polymorphism with both concrete and abstract methods.\n\n```gcl\n// Base abstract type\nabstract type Building {\n    address: String;\n    year_built: int;\n\n    fn calculate_tax(): float;  // Abstract - must be implemented\n    fn get_age(): int { return 2024 - year_built; }  // Concrete - shared\n}\n\n// Concrete implementations\ntype House extends Building {\n    bedrooms: int;\n    fn calculate_tax(): float { return bedrooms * 100.0; }\n}\n\ntype Commercial extends Building {\n    square_meters: float;\n    fn calculate_tax(): float { return square_meters * 5.0; }\n}\n```\n\n### Polymorphic Storage\n\n```gcl\nvar buildings_by_address: nodeIndex<String, node<Building>>;\n\n// Add different types\nvar house = node<House>{ House { address: \"123 Main St\", year_built: 2000, bedrooms: 3 }};\nbuildings_by_address.set(house->address, house);\n\nvar shop = node<Commercial>{ Commercial { address: \"456 Market St\", year_built: 2010, square_meters: 150.0 }};\nbuildings_by_address.set(shop->address, shop);\n\n// Iterate polymorphically\nfor (address, building in buildings_by_address) {\n    var tax = building->calculate_tax();  // Calls correct implementation\n    var age = building->get_age();        // Shared method\n}\n\n// Type checking\nfn process_building(building: node<Building>) {\n    if (building is House) {\n        var house = building as node<House>;\n        info(\"House with ${house->bedrooms} bedrooms\");\n    } else if (building is Commercial) {\n        var shop = building as node<Commercial>;\n        info(\"Commercial with ${shop->square_meters} sqm\");\n    }\n    var tax = building->calculate_tax();  // Polymorphic call\n}\n```\n\n**Key principles**:\n- Abstract types can have both abstract and concrete methods\n- Concrete methods cannot be overridden\n- Use `is` for type checking, `as` for casting\n- Store heterogeneous types with `node<BaseType>`\n\n## CRUD Service Pattern\n\nComplete CRUD with validation and error handling:\n\n```gcl\ntype User { id: int; email: String; name: String; created_at: time; }\n\nvar users_by_id: nodeIndex<int, node<User>>;\nvar users_by_email: nodeIndex<String, node<User>>;\nvar user_id_counter: node<int?>;\n\nabstract type UserService {\n    // Create with validation\n    static fn create(email: String, name: String): node<User> {\n        if (users_by_email.get(email) != null) { throw \"User with email ${email} already exists\"; }\n\n        var id = (user_id_counter.resolve() ?? 0) + 1;\n        user_id_counter.set(id);\n\n        var user = node<User>{ User { id: id, email: email, name: name, created_at: Time::now() }};\n        users_by_id.set(id, user);\n        users_by_email.set(email, user);\n        return user;\n    }\n\n    // Read\n    static fn find_by_id(id: int): node<User>? { return users_by_id.get(id); }\n    static fn find_by_email(email: String): node<User>? { return users_by_email.get(email); }\n    static fn list_all(): Array<node<User>> {\n        var results = Array<node<User>> {};\n        for (id, user in users_by_id) { results.add(user); }\n        return results;\n    }\n\n    // Update\n    static fn update_email(user: node<User>, new_email: String) {\n        users_by_email.remove(user->email);\n        user->email = new_email;\n        users_by_email.set(new_email, user);\n    }\n\n    static fn update_name(user: node<User>, new_name: String) { user->name = new_name; }\n\n    // Delete\n    static fn delete(user: node<User>) {\n        users_by_id.remove(user->id);\n        users_by_email.remove(user->email);\n    }\n}\n```\n\n## Relationship Patterns\n\n### One-to-Many\n\n```gcl\ntype City {\n    name: String;\n    country: node<Country>;  // Reference\n    streets: nodeList<node<Street>>;  // Collection\n}\n\ntype Street { name: String; city: node<City>; }  // Back-reference\n\nvar cities_by_name: nodeIndex<String, node<City>>;\nvar streets_by_name: nodeIndex<String, node<Street>>;\n\nabstract type CityService {\n    static fn create(name: String, country: node<Country>): node<City> {\n        var city = node<City>{ City { name: name, country: country, streets: nodeList<node<Street>>{} }};\n        cities_by_name.set(name, city);\n        return city;\n    }\n\n    static fn add_street(city: node<City>, street_name: String): node<Street> {\n        var street = node<Street>{ Street { name: street_name, city: city }};\n        city->streets.add(street);\n        streets_by_name.set(street_name, street);\n        return street;\n    }\n}\n```\n\n### Many-to-Many\n\n```gcl\ntype Student { name: String; courses: nodeList<node<Course>>; }\ntype Course { name: String; students: nodeList<node<Student>>; }\n\nvar students_by_name: nodeIndex<String, node<Student>>;\nvar courses_by_name: nodeIndex<String, node<Course>>;\n\nabstract type EnrollmentService {\n    static fn enroll(student: node<Student>, course: node<Course>) {\n        student->courses.add(course);\n        course->students.add(student);\n    }\n\n    static fn unenroll(student: node<Student>, course: node<Course>) {\n        // Rebuild lists without the removed item\n        var new_courses = nodeList<node<Course>> {};\n        for (i, c in student->courses) { if (c != course) { new_courses.add(c); } }\n        student->courses = new_courses;\n\n        var new_students = nodeList<node<Student>> {};\n        for (i, s in course->students) { if (s != student) { new_students.add(s); } }\n        course->students = new_students;\n    }\n}\n```\n\n## Time-Series Pattern\n\n```gcl\ntype Sensor {\n    id: String;\n    location: geo;\n    readings: nodeTime<float>;\n}\n\nvar sensors_by_id: nodeIndex<String, node<Sensor>>;\n\nabstract type SensorService {\n    static fn create(id: String, lat: float, lng: float): node<Sensor> {\n        var sensor = node<Sensor>{ Sensor { id: id, location: geo { lat: lat, lng: lng }, readings: nodeTime<float>{} }};\n        sensors_by_id.set(id, sensor);\n        return sensor;\n    }\n\n    static fn record_reading(sensor: node<Sensor>, value: float, timestamp: time) {\n        sensor->readings.setAt(timestamp, value);\n    }\n\n    static fn get_readings(sensor: node<Sensor>, start: time, end: time): Array<Tuple<time, float>> {\n        var results = Array<Tuple<time, float>> {};\n        for (t: time, value: float in sensor->readings[start..end]) {\n            results.add(Tuple { first: t, second: value });\n        }\n        return results;\n    }\n\n    static fn get_average(sensor: node<Sensor>, start: time, end: time): float {\n        var sum = 0.0;\n        var count = 0;\n        for (t: time, value: float in sensor->readings[start..end]) {\n            sum = sum + value;\n            count = count + 1;\n        }\n        return if (count > 0) { sum / count } else { 0.0 };\n    }\n}\n```\n\n## Key Takeaways\n\n1. **Services**: Use `abstract type` with static functions for business logic\n2. **API Layer**: Thin layer that calls services, returns `@volatile` types\n3. **Persistence**: Use `node<T>` for persistent objects, `Array<T>` for temporary\n4. **Relationships**: Store `node<T>` refs, not embedded objects\n5. **Inheritance**: Abstract types for polymorphism, concrete methods shared\n6. **Collections**: Always initialize `nodeList`, `nodeIndex`, `nodeTime` in constructors\n7. **Validation**: Perform in service layer before persistence\n8. **Indices**: Maintain consistency across multiple indices\n",
        "plugins/greycat/skills/greycat/references/permissions.md": "# Authentication & Permissions\n\n## Identity\n\nHTTP requests use cookie/bearer tokens with User ID + permissions.\n\n**Dev modes**: `--unsecure` (HTTP), `--user=1` (bypass auth, NEVER prod). See [cli.md](cli.md).\n\n## Built-in Permissions\n\n| Permission | Description |\n|------------|-------------|\n| `public` | Anonymous default |\n| `admin` | Full administration |\n| `api` | Exposed functions |\n| `debug` | Graph manipulation |\n| `files` | /files/ and /webroot/ |\n\n## Default Roles\n\n| Role | Permissions |\n|------|-------------|\n| `public` | public |\n| `admin` | public, admin, api, debug, files |\n| `user` | public, api, files |\n\n## Defining Permissions\n\n```gcl\n@permission(\"app.admin\", \"desc\"); @permission(\"app.user\", \"desc\");\n@role(\"custom\", \"app.admin\", \"app.user\");\n```\n\n## Function Decorators\n\n```gcl\n@expose @permission(\"api\") fn myFunction(): String { return \"Hello\"; }\n@permission(\"super\", \"normal\") fn test() {  // OR logic\n    if (User::hasPermission(\"normal\")) { /* normal */ } else { /* super */ }\n}\n```\n\n## Public Access\n\n```gcl\n@role(\"public\", \"api\", \"files\");  // Allow anonymous\n@expose @permission(\"public\") fn hello(name: String): String { return \"Hello ${name}\"; }\n```\n\n## Runtime Check\n\n```gcl\nif (User::hasPermission(\"admin\")) { /* admin logic */ }\n```\n\n## Security Files\n\nGenerated in `gcdata/security/`: `password`, `private_key`, `user_policy.gcb`. **DON'T share private_key/user_policy.gcb between DEV/PROD.**\n\n## External SSO\n\nOpenID Connect (Azure AD, Keycloak):\n```bash\ngreycat serve --oid_client_id=<ID> --oid_config_url=https://login.microsoftonline.com/{TENANT}/v2.0/.well-known/openid-configuration\n```\nMaps SSO roles to internal roles automatically.\n\n## Explorer Admin\n\nAdmin UI: `http://127.0.0.1:8080/explorer` - Manage roles/permissions via web interface.\n",
        "plugins/greycat/skills/greycat/references/powerflow/powerflow.md": "# Power Flow Analysis\n\nElectrical power system analysis and simulation for GreyCat.\n\n## Overview\n\nThe PowerFlow library provides computational tools for analyzing electrical power networks using power flow calculations (also known as load flow analysis). It enables simulation and analysis of steady-state electrical power systems, calculating voltages, currents, and power flows throughout a network.\n\nKey features include:\n- **Power network modeling** with buses, lines, loads, and external grids\n- **Load flow computation** using Newton-Raphson or similar algorithms\n- **Bus result analysis** including voltage magnitudes, angles, and currents\n- **Line result analysis** including power flows, losses, and loading percentages\n- **Configurable solver parameters** for convergence tolerance and iteration limits\n\nThis library is ideal for power system planning, grid optimization, renewable energy integration studies, distribution network analysis, and electrical engineering simulations.\n\n## Installation\n\nAdd the PowerFlow library to your GreyCat project:\n\n```gcl\n@library(\"powerflow\", \"7.6.60-dev\")\n```\n\n## Quick Start\n\n### Simple 3-Bus System\n\n```gcl\nvar network = PowerNetwork {\n  tolerance: 0.0001,\n  max_iteration: 100\n};\n\n// Configure network size\nnetwork.configure(\n  nb_bus: 3,\n  nb_lines: 2,\n  nb_ext_grids: 1\n);\n\n// Create buses\nnetwork.createBus(0, 110.0); // Bus 0: 110 kV\nnetwork.createBus(1, 110.0); // Bus 1: 110 kV\nnetwork.createBus(2, 110.0); // Bus 2: 110 kV\n\n// Create external grid (slack bus)\nnetwork.createExtGrid(0, 1.0); // Bus 0 at 1.0 pu voltage\n\n// Create load at bus 2\nnetwork.createLoad(2, 50.0, 20.0); // 50 MW, 20 MVar\n\n// Create transmission lines\nnetwork.createLine(\n  0,        // line_id\n  0, 1,     // from bus 0 to bus 1\n  50.0,     // 50 km length\n  0.12,     // 0.12 ohm/km resistance\n  0.4,      // 0.4 ohm/km reactance\n  10.0,     // 10 nF/km capacitance\n  0.5       // 0.5 kA max current\n);\n\nnetwork.createLine(1, 1, 2, 30.0, 0.12, 0.4, 10.0, 0.5);\n\n// Run power flow calculation\nnetwork.compute();\n\n// Get results\nvar busResult = network.getBusResult(2);\nprint(\"Bus 2 voltage: ${busResult.voltage} pu\");\nprint(\"Bus 2 angle: ${busResult.angle_radians} rad\");\n\nvar lineResult = network.getLineResult(0);\nprint(\"Line 0 loading: ${lineResult.loading_percent}%\");\n```\n\n## Types\n\n### PowerNetwork\n\nMain type for power system modeling and analysis.\n\n**Fields:**\n- `tolerance: float?` (private) - Convergence tolerance for iterative solver (default: 1e-6)\n- `max_iteration: int?` (private) - Maximum solver iterations (default: 100)\n\n**Methods:**\n- `configure(nb_bus: int, nb_lines: int, nb_ext_grids: int)` - Initialize network size\n- `createBus(bus_id: int, vn_kv: float)` - Create a bus\n- `createLoad(bus_id: int, p_mw: float, q_mvar: float)` - Create a load at a bus\n- `createLine(line_id: int, from_bus_id: int, to_bus_id: int, length_km: float, r_ohm_per_km: float, x_ohm_per_km: float, c_n_f_per_km: float, max_i_ka: float)` - Create a transmission line\n- `createExtGrid(bus_id: int, vm_p_u: float)` - Create external grid (slack bus)\n- `compute()` - Run power flow calculation\n- `getBusResult(bus_id: int): PowerBusResult` - Get bus calculation results\n- `getLineResult(line_id: int): PowerLineResult` - Get line calculation results\n- `getCheckSum(): Array<float>` - Get checksum for validation\n\n**Example:**\n\n```gcl\nvar network = PowerNetwork {\n  tolerance: 0.0001,      // Tighter convergence\n  max_iteration: 50       // Limit iterations\n};\n\nnetwork.configure(nb_bus: 10, nb_lines: 15, nb_ext_grids: 2);\n```\n\n### PowerBusResult\n\nResults of power flow calculation for a bus.\n\n**Fields:**\n- `abs: float` - Voltage magnitude (absolute value)\n- `angle_radians: float` - Voltage angle in radians\n- `voltage: float` - Voltage in per-unit (pu)\n- `voltage_img: float` - Imaginary part of voltage\n- `current: float` - Current magnitude\n- `current_img: float` - Imaginary part of current\n\n**Example:**\n\n```gcl\nnetwork.compute();\n\nvar result = network.getBusResult(5);\nprint(\"Voltage magnitude: ${result.voltage} pu\");\nprint(\"Voltage angle: ${result.angle_radians * 180 / 3.14159} degrees\");\nprint(\"Current: ${result.current} kA\");\n```\n\n### PowerLineResult\n\nResults of power flow calculation for a transmission line.\n\n**Fields:**\n- `p_from_mw: float` - Active power flow into line at \"from\" bus [MW]\n- `q_from_mvar: float` - Reactive power flow into line at \"from\" bus [MVar]\n- `p_to_mw: float` - Active power flow into line at \"to\" bus [MW]\n- `q_to_mvar: float` - Reactive power flow into line at \"to\" bus [MVar]\n- `pl_mw: float` - Active power losses [MW]\n- `ql_mvar: float` - Reactive power consumption [MVar]\n- `i_from_ka: float` - Current at \"from\" bus [kA]\n- `i_to_ka: float` - Current at \"to\" bus [kA]\n- `i_ka: float` - Maximum of i_from_ka and i_to_ka [kA]\n- `vm_from_pu: float` - Voltage magnitude at \"from\" bus [pu]\n- `vm_to_pu: float` - Voltage magnitude at \"to\" bus [pu]\n- `va_from_radians: float` - Voltage angle at \"from\" bus [radians]\n- `va_to_radians: float` - Voltage angle at \"to\" bus [radians]\n- `loading_percent: float` - Line loading [%]\n\n**Example:**\n\n```gcl\nvar lineResult = network.getLineResult(3);\n\nprint(\"Power flow: ${lineResult.p_from_mw} MW\");\nprint(\"Power losses: ${lineResult.pl_mw} MW\");\nprint(\"Line loading: ${lineResult.loading_percent}%\");\n\nif (lineResult.loading_percent > 90.0) {\n  print(\"WARNING: Line 3 is heavily loaded!\");\n}\n```\n\n## Methods\n\n### configure()\n\nInitializes the network with specified component counts.\n\n**Signature:** `fn configure(nb_bus: int, nb_lines: int, nb_ext_grids: int)`\n\n**Parameters:**\n- `nb_bus: int` - Number of buses in the network\n- `nb_lines: int` - Number of transmission lines\n- `nb_ext_grids: int` - Number of external grids (slack buses)\n\n**Note:** Must be called before creating any components.\n\n**Example:**\n\n```gcl\nvar network = PowerNetwork {};\n\n// Network with 20 buses, 25 lines, 1 slack bus\nnetwork.configure(nb_bus: 20, nb_lines: 25, nb_ext_grids: 1);\n```\n\n### createBus()\n\nCreates a bus (node) in the power network.\n\n**Signature:** `fn createBus(bus_id: int, vn_kv: float)`\n\n**Parameters:**\n- `bus_id: int` - Unique bus identifier (0-based)\n- `vn_kv: float` - Nominal voltage in kilovolts (e.g., 110.0, 220.0, 400.0)\n\n**Example:**\n\n```gcl\n// Create buses at different voltage levels\nnetwork.createBus(0, 400.0);  // 400 kV transmission\nnetwork.createBus(1, 220.0);  // 220 kV sub-transmission\nnetwork.createBus(2, 110.0);  // 110 kV distribution\nnetwork.createBus(3, 33.0);   // 33 kV distribution\n```\n\n### createLoad()\n\nCreates a load at a specified bus.\n\n**Signature:** `fn createLoad(bus_id: int, p_mw: float, q_mvar: float)`\n\n**Parameters:**\n- `bus_id: int` - Bus where load is connected\n- `p_mw: float` - Active power in megawatts\n- `q_mvar: float` - Reactive power in megavolt-amperes reactive\n\n**Example:**\n\n```gcl\n// Residential load\nnetwork.createLoad(5, 10.0, 3.0); // 10 MW, 3 MVar\n\n// Industrial load\nnetwork.createLoad(8, 50.0, 20.0); // 50 MW, 20 MVar (0.93 power factor)\n\n// Capacitive load (negative reactive power)\nnetwork.createLoad(10, 5.0, -2.0); // 5 MW, -2 MVar\n```\n\n### createLine()\n\nCreates a transmission line between two buses.\n\n**Signature:** `fn createLine(line_id: int, from_bus_id: int, to_bus_id: int, length_km: float, r_ohm_per_km: float, x_ohm_per_km: float, c_n_f_per_km: float, max_i_ka: float)`\n\n**Parameters:**\n- `line_id: int` - Unique line identifier\n- `from_bus_id: int` - Starting bus\n- `to_bus_id: int` - Ending bus\n- `length_km: float` - Line length in kilometers\n- `r_ohm_per_km: float` - Resistance in ohms per kilometer\n- `x_ohm_per_km: float` - Reactance in ohms per kilometer\n- `c_n_f_per_km: float` - Capacitance in nanofarads per kilometer\n- `max_i_ka: float` - Maximum current rating in kiloamperes\n\n**Example:**\n\n```gcl\n// 400 kV overhead line\nnetwork.createLine(\n  0,          // line_id\n  0, 1,       // from bus 0 to bus 1\n  100.0,      // 100 km\n  0.03,       // 0.03 ohm/km\n  0.3,        // 0.3 ohm/km\n  12.0,       // 12 nF/km\n  2.0         // 2 kA max current\n);\n\n// 110 kV underground cable\nnetwork.createLine(\n  1,          // line_id\n  2, 3,       // from bus 2 to bus 3\n  20.0,       // 20 km\n  0.2,        // 0.2 ohm/km (higher resistance)\n  0.12,       // 0.12 ohm/km (lower reactance)\n  250.0,      // 250 nF/km (much higher capacitance)\n  0.8         // 0.8 kA max current\n);\n```\n\n### createExtGrid()\n\nCreates an external grid connection (slack bus) that maintains voltage.\n\n**Signature:** `fn createExtGrid(bus_id: int, vm_p_u: float)`\n\n**Parameters:**\n- `bus_id: int` - Bus where external grid connects\n- `vm_p_u: float` - Voltage magnitude in per-unit (typically 1.0)\n\n**Note:** At least one external grid (slack bus) is required for power flow calculation.\n\n**Example:**\n\n```gcl\n// Standard slack bus at nominal voltage\nnetwork.createExtGrid(0, 1.0); // 1.0 pu = 100% of nominal\n\n// Slack bus with voltage support\nnetwork.createExtGrid(0, 1.05); // 1.05 pu = 105% of nominal\n```\n\n### compute()\n\nExecutes the power flow calculation.\n\n**Signature:** `fn compute()`\n\n**Behavior:**\n- Solves power flow equations iteratively\n- Uses Newton-Raphson or similar method\n- Respects `tolerance` and `max_iteration` settings\n- Throws error if solution doesn't converge\n\n**Example:**\n\n```gcl\n// Build network\nnetwork.configure(nb_bus: 5, nb_lines: 4, nb_ext_grids: 1);\n// ... create buses, lines, loads, ext_grid ...\n\n// Compute power flow\ntry {\n  network.compute();\n  print(\"Power flow converged\");\n} catch (e) {\n  print(\"Power flow did not converge: ${e}\");\n}\n```\n\n### getBusResult()\n\nRetrieves calculation results for a specific bus.\n\n**Signature:** `fn getBusResult(bus_id: int): PowerBusResult`\n\n**Note:** Must call `compute()` first.\n\n**Example:**\n\n```gcl\nnetwork.compute();\n\nfor (i in 0..5) {\n  var result = network.getBusResult(i);\n  print(\"Bus ${i}: ${result.voltage} pu, ${result.angle_radians} rad\");\n}\n```\n\n### getLineResult()\n\nRetrieves calculation results for a specific line.\n\n**Signature:** `fn getLineResult(line_id: int): PowerLineResult`\n\n**Note:** Must call `compute()` first.\n\n**Example:**\n\n```gcl\nnetwork.compute();\n\nvar totalLosses = 0.0;\n\nfor (i in 0..4) {\n  var result = network.getLineResult(i);\n  totalLosses = totalLosses + result.pl_mw;\n  print(\"Line ${i} loading: ${result.loading_percent}%\");\n}\n\nprint(\"Total system losses: ${totalLosses} MW\");\n```\n\n### getCheckSum()\n\nReturns validation checksums for the network.\n\n**Signature:** `fn getCheckSum(): Array<float>`\n\n**Returns:** Array of checksum values for validation purposes\n\n**Example:**\n\n```gcl\nnetwork.compute();\nvar checksum = network.getCheckSum();\nprint(\"Network checksum: ${checksum}\");\n```\n\n## Common Use Cases\n\n### Distribution Network Analysis\n\n```gcl\nvar network = PowerNetwork {\n  tolerance: 0.0001,\n  max_iteration: 100\n};\n\n// 11 kV distribution network with 10 buses\nnetwork.configure(nb_bus: 10, nb_lines: 9, nb_ext_grids: 1);\n\n// Substation (slack bus)\nnetwork.createBus(0, 11.0);\nnetwork.createExtGrid(0, 1.0);\n\n// Distribution buses\nfor (i in 1..10) {\n  network.createBus(i, 11.0);\n}\n\n// Radial feeder lines\nfor (i in 0..9) {\n  network.createLine(\n    i,\n    i, i + 1,\n    2.0,      // 2 km between buses\n    0.32,     // Typical 11 kV line resistance\n    0.35,     // Typical 11 kV line reactance\n    11.0,     // Typical 11 kV line capacitance\n    0.4       // 400 A rating\n  );\n}\n\n// Loads at each bus\nnetwork.createLoad(1, 0.5, 0.2);\nnetwork.createLoad(2, 0.8, 0.3);\nnetwork.createLoad(3, 1.2, 0.5);\n// ... etc\n\nnetwork.compute();\n\n// Check voltage profile\nfor (i in 0..10) {\n  var result = network.getBusResult(i);\n  if (result.voltage < 0.95) {\n    print(\"WARNING: Bus ${i} voltage too low: ${result.voltage} pu\");\n  }\n}\n```\n\n### Transmission Line Loading Analysis\n\n```gcl\nvar network = PowerNetwork {};\nnetwork.configure(nb_bus: 4, nb_lines: 4, nb_ext_grids: 1);\n\n// Create buses\nnetwork.createBus(0, 400.0);\nnetwork.createBus(1, 400.0);\nnetwork.createBus(2, 400.0);\nnetwork.createBus(3, 400.0);\n\n// Slack bus\nnetwork.createExtGrid(0, 1.0);\n\n// Mesh network\nnetwork.createLine(0, 0, 1, 100.0, 0.03, 0.3, 12.0, 2.0);\nnetwork.createLine(1, 1, 2, 100.0, 0.03, 0.3, 12.0, 2.0);\nnetwork.createLine(2, 2, 3, 100.0, 0.03, 0.3, 12.0, 2.0);\nnetwork.createLine(3, 3, 0, 100.0, 0.03, 0.3, 12.0, 2.0);\n\n// Loads\nnetwork.createLoad(1, 200.0, 50.0);\nnetwork.createLoad(3, 200.0, 50.0);\n\nnetwork.compute();\n\n// Analyze line loading\nvar overloadedLines = Array<int>{};\n\nfor (i in 0..4) {\n  var result = network.getLineResult(i);\n\n  if (result.loading_percent > 100.0) {\n    overloadedLines.add(i);\n    print(\"ALERT: Line ${i} overloaded at ${result.loading_percent}%\");\n  } else if (result.loading_percent > 80.0) {\n    print(\"WARNING: Line ${i} heavily loaded at ${result.loading_percent}%\");\n  }\n}\n\nif (overloadedLines.size() > 0) {\n  print(\"Network requires reinforcement!\");\n}\n```\n\n### Renewable Integration Study\n\n```gcl\nvar network = PowerNetwork {};\nnetwork.configure(nb_bus: 6, nb_lines: 5, nb_ext_grids: 1);\n\n// Create network\nfor (i in 0..6) {\n  network.createBus(i, 110.0);\n}\n\nnetwork.createExtGrid(0, 1.0);\n\n// Transmission lines\nnetwork.createLine(0, 0, 1, 50.0, 0.12, 0.4, 10.0, 0.6);\nnetwork.createLine(1, 1, 2, 50.0, 0.12, 0.4, 10.0, 0.6);\nnetwork.createLine(2, 2, 3, 50.0, 0.12, 0.4, 10.0, 0.6);\nnetwork.createLine(3, 1, 4, 30.0, 0.12, 0.4, 10.0, 0.6);\nnetwork.createLine(4, 4, 5, 30.0, 0.12, 0.4, 10.0, 0.6);\n\n// Base load scenario\nnetwork.createLoad(3, 40.0, 15.0);\nnetwork.createLoad(5, 30.0, 10.0);\n\n// Scenario 1: Without solar\nnetwork.compute();\nvar result1 = network.getLineResult(0);\nvar basePower = result1.p_from_mw;\n\n// Scenario 2: With solar generation (negative load)\nnetwork.createLoad(2, -20.0, -5.0); // 20 MW solar farm\nnetwork.compute();\nvar result2 = network.getLineResult(0);\nvar withSolarPower = result2.p_from_mw;\n\nprint(\"Power from grid - Base: ${basePower} MW\");\nprint(\"Power from grid - With solar: ${withSolarPower} MW\");\nprint(\"Grid power reduction: ${basePower - withSolarPower} MW\");\n```\n\n### N-1 Contingency Analysis\n\n```gcl\nfn analyzeContingency(network: PowerNetwork, lineToRemove: int) {\n  // Simulate line outage by setting very high impedance\n  // (Proper implementation would require network reconfiguration)\n\n  network.compute();\n\n  var overloaded = false;\n  for (i in 0..network.nb_lines) {\n    if (i == lineToRemove) continue;\n\n    var result = network.getLineResult(i);\n    if (result.loading_percent > 100.0) {\n      print(\"Line ${i} overloaded: ${result.loading_percent}%\");\n      overloaded = true;\n    }\n  }\n\n  return overloaded;\n}\n\n// Test each line outage\nfor (lineId in 0..nb_lines) {\n  print(\"Testing outage of line ${lineId}\");\n  var failed = analyzeContingency(network, lineId);\n  if (failed) {\n    print(\"CRITICAL: System unstable with line ${lineId} out\");\n  }\n}\n```\n\n## Best Practices\n\n### Network Modeling\n\n- **Use realistic parameters**: Get line impedances from manufacturer data or standards\n- **Proper voltage levels**: Ensure bus nominal voltages match connected equipment\n- **Include all loads**: Missing loads lead to inaccurate results\n- **Model reactive power**: Both P and Q are important for accurate voltage calculations\n\n```gcl\n// Good: Complete model\nnetwork.createLoad(5, 50.0, 20.0); // P and Q specified\n\n// Bad: Ignoring reactive power\n// network.createLoad(5, 50.0, 0.0); // Unrealistic\n```\n\n### Convergence Issues\n\n- **Start with slack bus**: At least one external grid is required\n- **Check topology**: Ensure network is connected (no isolated buses)\n- **Adjust tolerance**: Tighter tolerance may fail to converge\n- **Increase iterations**: Complex networks may need more iterations\n\n```gcl\n// For difficult cases\nvar network = PowerNetwork {\n  tolerance: 0.001,     // Relax tolerance\n  max_iteration: 200    // Allow more iterations\n};\n```\n\n### Units and Conventions\n\n- **Voltage**: kV (kilovolts) for buses, pu (per-unit) for results\n- **Power**: MW (megawatts) and MVar (megavolt-amperes reactive)\n- **Current**: kA (kiloamperes)\n- **Impedance**: Ohms per km\n- **Capacitance**: Nanofarads per km\n- **Angles**: Radians (convert to degrees: `angle * 180 / 3.14159`)\n\n### Result Interpretation\n\n- **Voltage limits**: Typically 0.95-1.05 pu is acceptable\n- **Line loading**: Above 80% indicates need for reinforcement\n- **Power losses**: Sum `pl_mw` across all lines for total losses\n- **Voltage angles**: Large angles indicate long transmission distances or high impedance\n\n```gcl\nnetwork.compute();\n\nvar busResult = network.getBusResult(10);\n\n// Check voltage\nif (busResult.voltage < 0.95 || busResult.voltage > 1.05) {\n  print(\"Voltage violation at bus 10\");\n}\n\n// Convert angle to degrees\nvar angleDegrees = busResult.angle_radians * 180.0 / 3.14159;\nprint(\"Voltage angle: ${angleDegrees} degrees\");\n```\n\n### Performance\n\n- **Pre-allocate with configure()**: Specify exact component counts\n- **Reuse network objects**: Recreate only when topology changes\n- **Batch result queries**: Get all results after single compute()\n\n```gcl\n// Efficient\nnetwork.compute();\nvar results = Array<PowerBusResult>{};\nfor (i in 0..nb_buses) {\n  results.add(network.getBusResult(i));\n}\n\n// Inefficient\nfor (i in 0..nb_buses) {\n  network.compute(); // Don't recompute!\n  var result = network.getBusResult(i);\n}\n```\n\n### Gotchas\n\n- **Bus/line IDs are 0-based**: First bus is 0, not 1\n- **Must configure before creating**: Call `configure()` first\n- **Compute before results**: `getBusResult()` requires prior `compute()`\n- **Line parameters are per km**: Total impedance = parameter * length\n- **Capacitance units**: nF/km, not μF/km\n- **Convergence failures**: May indicate unrealistic network configuration\n- **Sign conventions**: Positive P/Q = consumption, negative = generation\n\n### Validation\n\nAlways validate results:\n\n```gcl\nnetwork.compute();\n\n// Check power balance\nvar totalGeneration = 0.0;\nvar totalLoad = 0.0;\nvar totalLosses = 0.0;\n\nfor (i in 0..nb_lines) {\n  var lineResult = network.getLineResult(i);\n  totalLosses = totalLosses + lineResult.pl_mw;\n}\n\n// Generation = Load + Losses (approximately)\nvar error = totalGeneration - totalLoad - totalLosses;\nif (abs(error) > 1.0) {\n  print(\"WARNING: Power balance error: ${error} MW\");\n}\n```\n",
        "plugins/greycat/skills/greycat/references/s3/s3.md": "# S3 Object Storage\n\nAmazon S3-compatible object storage integration for GreyCat.\n\n## Overview\n\nThe S3 library provides seamless integration with Amazon S3 and S3-compatible object storage services (MinIO, Wasabi, DigitalOcean Spaces, etc.). It enables GreyCat applications to store and retrieve objects, manage buckets, and build scalable cloud storage solutions.\n\nKey features include:\n- **S3-compatible API** supporting AWS S3, MinIO, and other providers\n- **Object operations** including upload, download, list, and delete\n- **Bucket management** for creating and listing buckets\n- **Credential authentication** using access/secret key pairs\n- **Virtual path support** for organizing objects with directory-like structures\n\nThis library is ideal for storing large files, backups, user-generated content, data lakes, or building cloud-native applications with distributed object storage.\n\n## Installation\n\nAdd the S3 library to your GreyCat project:\n\n```gcl\n@library(\"s3\", \"7.6.60-dev\")\n```\n\n## Quick Start\n\n### Connect and Upload a File\n\n```gcl\nvar s3 = S3 {\n  host: \"s3.amazonaws.com\",\n  region: \"us-east-1\",\n  credentials: S3BasicCredentials {\n    access_key: \"AKIAIOSFODNN7EXAMPLE\",\n    secret_key: \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\"\n  }\n};\n\n// Upload a file\ns3.put_object(\"my-bucket\", \"/local/path/document.pdf\", \"documents/2024/document.pdf\");\n\n// Download it back\ns3.get_object(\"my-bucket\", \"documents/2024/document.pdf\", \"/local/path/downloaded.pdf\");\n```\n\n### List Objects in a Bucket\n\n```gcl\nvar objects = s3.list_objects(\"my-bucket\", null, null, null);\n\nfor (obj in objects) {\n  print(\"${obj.key} - ${obj.size} bytes - ${obj.last_modified}\");\n}\n```\n\n## Types\n\n### S3\n\nMain connection type representing an S3 client.\n\n**Fields:**\n- `host: String` - S3 server address (e.g., `\"s3.amazonaws.com\"`, `\"localhost:9000\"`)\n- `region: String` - AWS region (e.g., `\"us-east-1\"`, `\"eu-west-1\"`)\n- `credentials: S3BasicCredentials` - Authentication credentials\n- `force_path_style: bool?` - Use path-style URLs instead of virtual-hosted-style (required for MinIO)\n\n**Methods:**\n- `list_objects(bucket, prefix?, start_after?, max_keys?): Array<S3Object>`\n- `get_object(bucket, key, filepath)`\n- `put_object(bucket, filepath, key)`\n- `delete_object(bucket, key)`\n- `create_bucket(bucket)`\n- `list_buckets(prefix?): Array<S3Bucket>`\n\n**Example:**\n\n```gcl\n// AWS S3\nvar awsS3 = S3 {\n  host: \"s3.amazonaws.com\",\n  region: \"us-west-2\",\n  credentials: S3BasicCredentials {\n    access_key: env(\"AWS_ACCESS_KEY\"),\n    secret_key: env(\"AWS_SECRET_KEY\")\n  }\n};\n\n// MinIO (local development)\nvar minioS3 = S3 {\n  host: \"localhost:9000\",\n  region: \"us-east-1\",\n  credentials: S3BasicCredentials {\n    access_key: \"minioadmin\",\n    secret_key: \"minioadmin\"\n  },\n  force_path_style: true // Required for MinIO\n};\n```\n\n### S3Object\n\nRepresents an object stored in S3.\n\n**Fields:**\n- `key: String` - Object key/path (e.g., `\"24/07/02/record_1234.txt\"`)\n- `last_modified: time` - Last modification timestamp\n- `size: int` - Object size in bytes\n- `etag: String` - Entity tag (MD5 hash of the object)\n\n**Example:**\n\n```gcl\nvar objects = s3.list_objects(\"my-bucket\", \"logs/\", null, 10);\n\nfor (obj in objects) {\n  print(\"File: ${obj.key}\");\n  print(\"  Size: ${obj.size} bytes\");\n  print(\"  Modified: ${obj.last_modified}\");\n  print(\"  ETag: ${obj.etag}\");\n}\n```\n\n### S3Bucket\n\nRepresents a bucket in S3.\n\n**Fields:**\n- `name: String` - Bucket name\n- `creation_date: time` - When the bucket was created\n\n**Example:**\n\n```gcl\nvar buckets = s3.list_buckets(null);\n\nfor (bucket in buckets) {\n  print(\"Bucket: ${bucket.name} (created ${bucket.creation_date})\");\n}\n```\n\n### S3BasicCredentials\n\nAuthentication credentials for S3 access.\n\n**Fields:**\n- `access_key: String` - AWS access key ID or equivalent\n- `secret_key: String` - AWS secret access key or equivalent\n\n**Example:**\n\n```gcl\nvar creds = S3BasicCredentials {\n  access_key: env(\"S3_ACCESS_KEY\"),\n  secret_key: env(\"S3_SECRET_KEY\")\n};\n\nvar s3 = S3 {\n  host: \"s3.amazonaws.com\",\n  region: \"us-east-1\",\n  credentials: creds\n};\n```\n\n## Methods\n\n### list_objects()\n\nLists objects in a bucket with optional filtering and pagination.\n\n**Signature:** `fn list_objects(bucket: String, prefix: String?, start_after: String?, max_keys: int?): Array<S3Object>`\n\n**Parameters:**\n- `bucket: String` - Bucket name to list from\n- `prefix: String?` - Only return keys beginning with this prefix (null = no filter)\n- `start_after: String?` - Start listing after this key (for pagination)\n- `max_keys: int?` - Maximum number of keys to return (default: 1000, max: 1000)\n\n**Returns:** Array of `S3Object` instances\n\n**Example:**\n\n```gcl\n// List all objects (up to 1000)\nvar all = s3.list_objects(\"my-bucket\", null, null, null);\n\n// List objects with prefix (folder-like filtering)\nvar logs = s3.list_objects(\"my-bucket\", \"logs/2024/\", null, null);\n\n// Paginate through large result sets\nvar page1 = s3.list_objects(\"my-bucket\", null, null, 100);\nvar lastKey = page1[page1.size() - 1].key;\nvar page2 = s3.list_objects(\"my-bucket\", null, lastKey, 100);\n\n// List up to 50 objects starting with \"data/\"\nvar data = s3.list_objects(\"my-bucket\", \"data/\", null, 50);\n```\n\n### get_object()\n\nDownloads an object from S3 to a local file.\n\n**Signature:** `fn get_object(bucket: String, key: String, filepath: String)`\n\n**Parameters:**\n- `bucket: String` - Source bucket name\n- `key: String` - Object key to download\n- `filepath: String` - Local destination file path\n\n**Behavior:**\n- Downloads the entire object\n- Overwrites existing local file\n- Creates parent directories if needed\n- Throws error if object doesn't exist\n\n**Example:**\n\n```gcl\n// Download a file\ns3.get_object(\"my-bucket\", \"images/photo.jpg\", \"/tmp/photo.jpg\");\n\n// Download from virtual directory\ns3.get_object(\"backups\", \"2024/01/15/database.sql.gz\", \"/restore/database.sql.gz\");\n\n// Download with error handling\ntry {\n  s3.get_object(\"my-bucket\", \"important-file.txt\", \"/tmp/important.txt\");\n  print(\"Download successful\");\n} catch (e) {\n  print(\"Download failed: ${e}\");\n}\n```\n\n### put_object()\n\nUploads a local file to S3.\n\n**Signature:** `fn put_object(bucket: String, filepath: String, key: String)`\n\n**Parameters:**\n- `bucket: String` - Target bucket name\n- `filepath: String` - Local source file path\n- `key: String` - Object key in S3 (can include virtual directories)\n\n**Behavior:**\n- Uploads the entire file\n- Overwrites existing S3 object with same key\n- Automatically sets content type based on file extension\n- Throws error if local file doesn't exist\n\n**Example:**\n\n```gcl\n// Upload a file\ns3.put_object(\"my-bucket\", \"/local/report.pdf\", \"reports/monthly/jan-2024.pdf\");\n\n// Upload to root of bucket\ns3.put_object(\"my-bucket\", \"/data/config.json\", \"config.json\");\n\n// Upload with timestamp in key\nvar timestamp = time::now();\ns3.put_object(\"logs\", \"/var/log/app.log\", \"logs/${timestamp}/app.log\");\n\n// Batch upload\nvar files = [\"file1.txt\", \"file2.txt\", \"file3.txt\"];\nfor (file in files) {\n  s3.put_object(\"uploads\", \"/tmp/${file}\", \"batch-upload/${file}\");\n}\n```\n\n### delete_object()\n\nRemoves an object from S3.\n\n**Signature:** `fn delete_object(bucket: String, key: String)`\n\n**Parameters:**\n- `bucket: String` - Bucket containing the object\n- `key: String` - Key of the object to delete\n\n**Behavior:**\n- Permanently deletes the object\n- No error if object doesn't exist (idempotent)\n- Cannot be undone (unless versioning is enabled in S3)\n\n**Example:**\n\n```gcl\n// Delete a single object\ns3.delete_object(\"my-bucket\", \"old-file.txt\");\n\n// Delete objects by prefix (requires listing first)\nvar oldLogs = s3.list_objects(\"logs\", \"2023/\", null, null);\nfor (obj in oldLogs) {\n  s3.delete_object(\"logs\", obj.key);\n}\n\n// Cleanup temporary files\ns3.delete_object(\"temp-bucket\", \"temp/processing-${session_id}.tmp\");\n```\n\n### create_bucket()\n\nCreates a new S3 bucket.\n\n**Signature:** `fn create_bucket(bucket: String)`\n\n**Parameters:**\n- `bucket: String` - Name of the bucket to create\n\n**Behavior:**\n- Creates a bucket in the configured region\n- Bucket names must be globally unique (AWS) or unique within the service\n- Must follow bucket naming rules: lowercase, no underscores, 3-63 characters\n- Error if bucket already exists\n\n**Example:**\n\n```gcl\n// Create a bucket\ns3.create_bucket(\"my-new-bucket\");\n\n// Create with error handling\ntry {\n  s3.create_bucket(\"analytics-data-2024\");\n  print(\"Bucket created\");\n} catch (e) {\n  print(\"Failed to create bucket: ${e}\");\n}\n\n// Create multiple buckets for different purposes\nvar buckets = [\"logs\", \"backups\", \"uploads\", \"exports\"];\nfor (name in buckets) {\n  try {\n    s3.create_bucket(\"myapp-${name}\");\n  } catch (e) {\n    print(\"Bucket ${name} already exists or failed: ${e}\");\n  }\n}\n```\n\n### list_buckets()\n\nLists all buckets owned by the authenticated user.\n\n**Signature:** `fn list_buckets(prefix: String?): Array<S3Bucket>`\n\n**Parameters:**\n- `prefix: String?` - Only return buckets starting with this prefix (null = all buckets)\n\n**Returns:** Array of `S3Bucket` instances\n\n**Example:**\n\n```gcl\n// List all buckets\nvar buckets = s3.list_buckets(null);\nfor (bucket in buckets) {\n  print(\"${bucket.name} - created ${bucket.creation_date}\");\n}\n\n// List buckets with prefix\nvar appBuckets = s3.list_buckets(\"myapp-\");\nprint(\"Found ${appBuckets.size()} application buckets\");\n\n// Check if specific bucket exists\nvar allBuckets = s3.list_buckets(null);\nvar exists = false;\nfor (bucket in allBuckets) {\n  if (bucket.name == \"important-bucket\") {\n    exists = true;\n    break;\n  }\n}\n```\n\n## Common Use Cases\n\n### Backup and Restore\n\n```gcl\nvar s3 = S3 {\n  host: \"s3.amazonaws.com\",\n  region: \"us-west-1\",\n  credentials: S3BasicCredentials {\n    access_key: env(\"AWS_ACCESS_KEY\"),\n    secret_key: env(\"AWS_SECRET_KEY\")\n  }\n};\n\n// Backup: Upload database dump\nvar timestamp = time::now();\nvar backupFile = \"/tmp/db-backup-${timestamp}.sql.gz\";\n\n// Create backup (external command)\n// exec(\"pg_dump mydb | gzip > ${backupFile}\");\n\n// Upload to S3\ns3.put_object(\n  \"database-backups\",\n  backupFile,\n  \"backups/${timestamp}/database.sql.gz\"\n);\n\nprint(\"Backup uploaded to S3\");\n\n// Restore: Download specific backup\ns3.get_object(\n  \"database-backups\",\n  \"backups/2024-01-15T10:00:00/database.sql.gz\",\n  \"/tmp/restore.sql.gz\"\n);\n\n// Restore database (external command)\n// exec(\"gunzip < /tmp/restore.sql.gz | psql mydb\");\n\nprint(\"Restore complete\");\n```\n\n### File Processing Pipeline\n\n```gcl\nvar s3 = S3 {\n  host: \"s3.amazonaws.com\",\n  region: \"eu-west-1\",\n  credentials: S3BasicCredentials {\n    access_key: env(\"AWS_ACCESS_KEY\"),\n    secret_key: env(\"AWS_SECRET_KEY\")\n  }\n};\n\n// List unprocessed files\nvar files = s3.list_objects(\"uploads\", \"incoming/\", null, 100);\n\nfor (file in files) {\n  // Download file\n  var localPath = \"/tmp/${file.key}\";\n  s3.get_object(\"uploads\", file.key, localPath);\n\n  // Process file (example: convert, analyze, etc.)\n  var processedPath = processFile(localPath);\n\n  // Upload processed result\n  s3.put_object(\"uploads\", processedPath, \"processed/${file.key}\");\n\n  // Move original to archive\n  s3.put_object(\"uploads\", localPath, \"archive/${file.key}\");\n  s3.delete_object(\"uploads\", file.key);\n\n  print(\"Processed: ${file.key}\");\n}\n```\n\n### Multi-Region Replication\n\n```gcl\nvar sourceS3 = S3 {\n  host: \"s3.amazonaws.com\",\n  region: \"us-east-1\",\n  credentials: S3BasicCredentials {\n    access_key: env(\"AWS_ACCESS_KEY\"),\n    secret_key: env(\"AWS_SECRET_KEY\")\n  }\n};\n\nvar targetS3 = S3 {\n  host: \"s3.amazonaws.com\",\n  region: \"eu-west-1\",\n  credentials: S3BasicCredentials {\n    access_key: env(\"AWS_ACCESS_KEY\"),\n    secret_key: env(\"AWS_SECRET_KEY\")\n  }\n};\n\n// Replicate bucket contents\nvar objects = sourceS3.list_objects(\"source-bucket\", null, null, null);\n\nfor (obj in objects) {\n  // Download from source region\n  var tmpPath = \"/tmp/replication/${obj.key}\";\n  sourceS3.get_object(\"source-bucket\", obj.key, tmpPath);\n\n  // Upload to target region\n  targetS3.put_object(\"target-bucket\", tmpPath, obj.key);\n\n  print(\"Replicated: ${obj.key}\");\n}\n```\n\n### Static Website Hosting\n\n```gcl\nvar s3 = S3 {\n  host: \"s3.amazonaws.com\",\n  region: \"us-east-1\",\n  credentials: S3BasicCredentials {\n    access_key: env(\"AWS_ACCESS_KEY\"),\n    secret_key: env(\"AWS_SECRET_KEY\")\n  }\n};\n\n// Upload website files\nvar websiteFiles = [\n  [\"index.html\", \"index.html\"],\n  [\"about.html\", \"about.html\"],\n  [\"style.css\", \"css/style.css\"],\n  [\"app.js\", \"js/app.js\"],\n  [\"logo.png\", \"images/logo.png\"]\n];\n\nfor (file in websiteFiles) {\n  var localPath = \"/website/${file[0]}\";\n  var s3Key = file[1];\n  s3.put_object(\"my-website-bucket\", localPath, s3Key);\n}\n\nprint(\"Website deployed\");\n```\n\n## Best Practices\n\n### Credential Management\n\n- **Never hardcode credentials**: Use environment variables or secure configuration\n- **Use IAM roles**: In AWS environments, prefer IAM roles over access keys\n- **Rotate keys regularly**: Update credentials periodically for security\n\n```gcl\n// Good: Load from environment\nvar s3 = S3 {\n  host: env(\"S3_HOST\"),\n  region: env(\"S3_REGION\"),\n  credentials: S3BasicCredentials {\n    access_key: env(\"S3_ACCESS_KEY\"),\n    secret_key: env(\"S3_SECRET_KEY\")\n  }\n};\n\n// Bad: Hardcoded credentials\nvar s3 = S3 {\n  host: \"s3.amazonaws.com\",\n  region: \"us-east-1\",\n  credentials: S3BasicCredentials {\n    access_key: \"AKIAIOSFODNN7EXAMPLE\", // NEVER DO THIS!\n    secret_key: \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\"\n  }\n};\n```\n\n### Object Naming\n\n- **Use virtual directories**: Organize with `/` separators (e.g., `\"2024/01/file.txt\"`)\n- **Include timestamps**: Makes finding objects easier\n- **Avoid special characters**: Stick to alphanumeric, hyphens, underscores, slashes\n\n```gcl\n// Good naming conventions\ns3.put_object(\"logs\", \"/app.log\", \"logs/2024/01/15/app.log\");\ns3.put_object(\"uploads\", \"/photo.jpg\", \"users/user-123/photos/2024-01-15-photo.jpg\");\n\n// Avoid\ns3.put_object(\"data\", \"/file.txt\", \"file with spaces.txt\"); // Spaces problematic\ns3.put_object(\"data\", \"/file.txt\", \"file@#$.txt\"); // Special chars\n```\n\n### Performance\n\n- **Batch operations**: Process multiple files in loops\n- **Use appropriate max_keys**: Don't fetch more than you need\n- **Paginate large listings**: Use `start_after` for buckets with many objects\n\n```gcl\n// Efficient pagination\nvar allObjects = Array<S3Object>{};\nvar startAfter = null;\n\nwhile (true) {\n  var batch = s3.list_objects(\"huge-bucket\", null, startAfter, 1000);\n  allObjects.add_all(batch);\n\n  if (batch.size() < 1000) {\n    break; // Last page\n  }\n\n  startAfter = batch[batch.size() - 1].key;\n}\n```\n\n### Error Handling\n\n- **Wrap operations in try-catch**: Network issues, permissions, missing objects\n- **Verify uploads**: Check file size or re-download to confirm\n- **Handle missing objects gracefully**: `get_object()` fails if key doesn't exist\n\n```gcl\ntry {\n  s3.get_object(\"my-bucket\", \"important-file.txt\", \"/tmp/file.txt\");\n} catch (e) {\n  print(\"Failed to download: ${e}\");\n  // Handle missing file or permission error\n}\n```\n\n### Storage Optimization\n\n- **Delete old objects**: Clean up temporary files regularly\n- **Use object lifecycle policies**: Automate archival and deletion (configure in S3)\n- **Compress before upload**: Save bandwidth and storage costs\n\n```gcl\n// Clean up files older than 30 days\nvar oldFiles = s3.list_objects(\"temp-bucket\", null, null, null);\nvar cutoff = time::now() - 30d;\n\nfor (file in oldFiles) {\n  if (file.last_modified < cutoff) {\n    s3.delete_object(\"temp-bucket\", file.key);\n    print(\"Deleted old file: ${file.key}\");\n  }\n}\n```\n\n### Gotchas\n\n- **force_path_style**: Required for MinIO and some S3-compatible services\n- **Bucket naming**: AWS has strict naming rules (lowercase, no underscores, globally unique)\n- **max_keys limit**: Returns at most 1000 objects per call, must paginate for more\n- **Object overwrites**: `put_object()` silently replaces existing objects\n- **Case sensitivity**: Object keys are case-sensitive (`\"file.txt\"` != `\"File.txt\"`)\n- **Deletion is permanent**: No trash/recycle bin unless versioning enabled\n- **Regional endpoints**: Ensure `host` and `region` match your bucket's location\n\n### Security\n\n- **Use HTTPS**: Production deployments should use encrypted connections\n- **Bucket policies**: Configure access policies in S3 console/API\n- **Audit access**: Enable logging to track object access\n\n```gcl\n// Production configuration\nvar prodS3 = S3 {\n  host: \"s3.amazonaws.com\", // Uses HTTPS by default\n  region: \"us-east-1\",\n  credentials: S3BasicCredentials {\n    access_key: env(\"S3_ACCESS_KEY\"),\n    secret_key: env(\"S3_SECRET_KEY\")\n  }\n};\n```\n\n### MinIO Configuration\n\nMinIO requires `force_path_style: true`:\n\n```gcl\nvar minio = S3 {\n  host: \"localhost:9000\",\n  region: \"us-east-1\", // MinIO ignores region but field is required\n  credentials: S3BasicCredentials {\n    access_key: \"minioadmin\",\n    secret_key: \"minioadmin\"\n  },\n  force_path_style: true // REQUIRED for MinIO\n};\n```\n",
        "plugins/greycat/skills/greycat/references/sql/postgres.md": "# PostgreSQL Integration\n\nNative PostgreSQL database integration for GreyCat with transaction support and CSV import/export.\n\n## Overview\n\nThe PostgreSQL library provides direct database connectivity for GreyCat applications, enabling seamless integration with PostgreSQL databases. It supports standard SQL operations, transaction management, and high-performance bulk data operations through CSV import/export.\n\nKey features include:\n- **Transaction support** with begin, commit, and rollback operations\n- **Query execution** with automatic result mapping to GreyCat Tables\n- **CSV import** using PostgreSQL's native COPY command for high-speed data loading\n- **CSV export** for efficient data extraction and reporting\n- **Type safety** with native GreyCat type system integration\n\nThis library is ideal for applications that need persistent storage, complex queries, data warehousing, ETL pipelines, or integration with existing PostgreSQL databases.\n\n## Installation\n\nAdd the PostgreSQL library to your GreyCat project:\n\n```gcl\n@library(\"sql\", \"7.6.60-dev\")\n```\n\n**Note:** Ensure PostgreSQL client libraries are available in your runtime environment.\n\n## Quick Start\n\n### Basic Connection and Query\n\n```gcl\nvar db = Postgres {\n  url: \"localhost\",\n  port: \"5432\",\n  db_name: \"myapp\",\n  login: \"postgres\",\n  password: \"secret\"\n};\n\n// Execute a simple query\nvar result = db.execute(\"SELECT * FROM users WHERE active = true\");\nprint(result);\n```\n\n### Transaction Example\n\n```gcl\nvar db = Postgres {\n  url: \"localhost\",\n  port: \"5432\",\n  db_name: \"banking\",\n  login: \"bank_user\",\n  password: \"secure_password\"\n};\n\n// Transfer money between accounts\ndb.begin();\ntry {\n  db.execute(\"UPDATE accounts SET balance = balance - 100 WHERE id = 1\");\n  db.execute(\"UPDATE accounts SET balance = balance + 100 WHERE id = 2\");\n  db.commit();\n  print(\"Transfer completed\");\n} catch (e) {\n  db.rollback();\n  print(\"Transfer failed: ${e}\");\n}\n```\n\n## Types\n\n### Postgres\n\nMain database connection type with transaction and query capabilities.\n\n**Fields:**\n- `url: String` (private) - PostgreSQL server hostname or IP address\n- `port: String` (private) - Server port number (typically `\"5432\"`)\n- `db_name: String` (private) - Name of the database to connect to\n- `login: String?` (private) - Username for authentication (optional)\n- `password: String?` (private) - Password for authentication (optional)\n\n**Methods:**\n- `begin()` - Start a new transaction\n- `commit()` - Commit the current transaction\n- `rollback()` - Rollback the current transaction\n- `execute(query: String): any?` - Execute SQL query, returns Table for SELECT or null\n- `csv_to_table(path: String, table_name: String, cols_name: Array<String>?, sep: char)` - Import CSV to table\n- `query_to_csv(path: String, query: String, sep: char)` - Export query results to CSV\n\n**Example:**\n\n```gcl\nvar db = Postgres {\n  url: \"db.example.com\",\n  port: \"5432\",\n  db_name: \"production_db\",\n  login: \"app_user\",\n  password: \"app_password\"\n};\n\n// Check connection\nvar version = db.execute(\"SELECT version()\");\nprint(version);\n```\n\n## Methods\n\n### begin()\n\nStarts a new database transaction.\n\n**Signature:** `fn begin()`\n\n**Behavior:**\n- Initiates a new transaction context\n- All subsequent `execute()` calls are part of this transaction\n- Must be followed by `commit()` or `rollback()`\n- Cannot use CSV operations while in a transaction\n\n**Example:**\n\n```gcl\ndb.begin();\ndb.execute(\"INSERT INTO orders (id, amount) VALUES (1, 100)\");\ndb.execute(\"INSERT INTO order_items (order_id, product_id) VALUES (1, 42)\");\ndb.commit();\n```\n\n### commit()\n\nCommits the current transaction, making all changes permanent.\n\n**Signature:** `fn commit()`\n\n**Behavior:**\n- Persists all changes made since `begin()`\n- Releases locks and resources\n- Throws an error if no transaction is active\n- Ends the transaction context\n\n**Example:**\n\n```gcl\ndb.begin();\ntry {\n  db.execute(\"UPDATE inventory SET quantity = quantity - 10 WHERE product_id = 5\");\n  db.execute(\"INSERT INTO sales (product_id, quantity) VALUES (5, 10)\");\n  db.commit();\n} catch (e) {\n  db.rollback();\n  throw e;\n}\n```\n\n### rollback()\n\nRolls back the current transaction, discarding all changes.\n\n**Signature:** `fn rollback()`\n\n**Behavior:**\n- Undoes all changes made since `begin()`\n- Releases locks and resources\n- Safe to call even if transaction is already rolled back\n- Ends the transaction context\n\n**Example:**\n\n```gcl\ndb.begin();\ndb.execute(\"DELETE FROM users WHERE last_login < '2020-01-01'\");\n\nvar count = db.execute(\"SELECT COUNT(*) FROM users\") as Table;\nif (count.rows[0][0] as int < 100) {\n  // Too many users deleted, rollback\n  db.rollback();\n  print(\"Rollback: would have deleted too many users\");\n} else {\n  db.commit();\n}\n```\n\n### execute()\n\nExecutes a SQL query and returns results for SELECT statements.\n\n**Signature:** `fn execute(query: String): any?`\n\n**Parameters:**\n- `query: String` - SQL statement to execute\n\n**Returns:**\n- `Table` for SELECT queries containing result rows\n- `null` for INSERT, UPDATE, DELETE, CREATE, etc.\n\n**Example:**\n\n```gcl\n// SELECT query returns Table\nvar users = db.execute(\"SELECT id, name, email FROM users ORDER BY name\") as Table;\nfor (row in users.rows) {\n  var id = row[0] as int;\n  var name = row[1] as String;\n  var email = row[2] as String;\n  print(\"User: ${id} - ${name} (${email})\");\n}\n\n// INSERT returns null\ndb.execute(\"INSERT INTO logs (message, level) VALUES ('Started', 'INFO')\");\n\n// UPDATE returns null\ndb.execute(\"UPDATE users SET last_login = NOW() WHERE id = 42\");\n\n// DDL returns null\ndb.execute(\"CREATE TABLE IF NOT EXISTS sessions (id SERIAL PRIMARY KEY, token TEXT)\");\n```\n\n### csv_to_table()\n\nImports data from a CSV file into a PostgreSQL table using the COPY command.\n\n**Signature:** `fn csv_to_table(path: String, table_name: String, cols_name: Array<String>?, sep: char)`\n\n**Parameters:**\n- `path: String` - Absolute path to the CSV file\n- `table_name: String` - Target table name\n- `cols_name: Array<String>?` - Column names to import (null = all columns)\n- `sep: char` - Column separator character (e.g., `','`, `'\\t'`, `';'`)\n\n**Requirements:**\n- User must have COPY privileges in PostgreSQL\n- Cannot be used inside a transaction (call `commit()` or `rollback()` first)\n- CSV file must be readable by PostgreSQL server process\n- Table must already exist with matching schema\n\n**Example:**\n\n```gcl\nvar db = Postgres {\n  url: \"localhost\",\n  port: \"5432\",\n  db_name: \"analytics\",\n  login: \"etl_user\",\n  password: \"etl_password\"\n};\n\n// Ensure no active transaction\ndb.execute(\"CREATE TABLE IF NOT EXISTS sales (\n  date DATE,\n  product_id INT,\n  quantity INT,\n  revenue DECIMAL(10,2)\n)\");\n\n// Import all columns from comma-separated file\ndb.csv_to_table(\"/data/sales_2024.csv\", \"sales\", null, ',');\n\n// Import only specific columns from tab-separated file\ndb.csv_to_table(\n  \"/data/products.tsv\",\n  \"products\",\n  [\"sku\", \"name\", \"price\"],\n  '\\t'\n);\n\n// Import semicolon-separated European format\ndb.csv_to_table(\"/data/customers.csv\", \"customers\", null, ';');\n```\n\n### query_to_csv()\n\nExports the results of a SQL query to a CSV file.\n\n**Signature:** `fn query_to_csv(path: String, query: String, sep: char)`\n\n**Parameters:**\n- `path: String` - Output CSV file path\n- `query: String` - SQL SELECT query to execute\n- `sep: char` - Column separator character\n\n**Behavior:**\n- Executes the query and writes results to file\n- Overwrites existing files\n- Creates parent directories if they don't exist\n- Efficient for large result sets\n\n**Example:**\n\n```gcl\n// Export monthly sales report\ndb.query_to_csv(\n  \"/reports/sales_january.csv\",\n  \"SELECT date, product, sum(revenue) FROM sales WHERE month = 1 GROUP BY date, product\",\n  ','\n);\n\n// Export user data with semicolon separator\ndb.query_to_csv(\n  \"/exports/active_users.csv\",\n  \"SELECT id, email, created_at FROM users WHERE active = true\",\n  ';'\n);\n\n// Export complex join query\ndb.query_to_csv(\n  \"/tmp/order_details.csv\",\n  \"\"\"\n  SELECT o.id, o.date, u.email, p.name, oi.quantity\n  FROM orders o\n  JOIN users u ON o.user_id = u.id\n  JOIN order_items oi ON o.id = oi.order_id\n  JOIN products p ON oi.product_id = p.id\n  WHERE o.date >= '2024-01-01'\n  \"\"\",\n  ','\n);\n```\n\n## Common Use Cases\n\n### ETL Pipeline\n\n```gcl\nvar db = Postgres {\n  url: \"warehouse.example.com\",\n  port: \"5432\",\n  db_name: \"data_warehouse\",\n  login: \"etl_service\",\n  password: \"etl_secret\"\n};\n\n// Extract: Export data from source\ndb.query_to_csv(\n  \"/tmp/raw_events.csv\",\n  \"SELECT * FROM events WHERE processed = false\",\n  ','\n);\n\n// Transform: Process CSV (external tool or GreyCat logic)\n// ...\n\n// Load: Import transformed data\ndb.execute(\"CREATE TEMP TABLE staging_events (LIKE events)\");\ndb.csv_to_table(\"/tmp/transformed_events.csv\", \"staging_events\", null, ',');\n\n// Merge into main table\ndb.begin();\ndb.execute(\"INSERT INTO events SELECT * FROM staging_events\");\ndb.execute(\"UPDATE events SET processed = true WHERE id IN (SELECT id FROM staging_events)\");\ndb.commit();\n```\n\n### Database Migration\n\n```gcl\nvar sourceDb = Postgres {\n  url: \"old-server.example.com\",\n  port: \"5432\",\n  db_name: \"legacy_db\",\n  login: \"admin\",\n  password: \"admin_pwd\"\n};\n\nvar targetDb = Postgres {\n  url: \"new-server.example.com\",\n  port: \"5432\",\n  db_name: \"new_db\",\n  login: \"admin\",\n  password: \"admin_pwd\"\n};\n\n// Export from source\nsourceDb.query_to_csv(\"/tmp/users.csv\", \"SELECT * FROM users\", ',');\nsourceDb.query_to_csv(\"/tmp/orders.csv\", \"SELECT * FROM orders\", ',');\n\n// Create schema on target\ntargetDb.execute(\"CREATE TABLE users (id SERIAL PRIMARY KEY, name TEXT, email TEXT)\");\ntargetDb.execute(\"CREATE TABLE orders (id SERIAL PRIMARY KEY, user_id INT, amount DECIMAL)\");\n\n// Import to target\ntargetDb.csv_to_table(\"/tmp/users.csv\", \"users\", null, ',');\ntargetDb.csv_to_table(\"/tmp/orders.csv\", \"orders\", null, ',');\n\nprint(\"Migration complete\");\n```\n\n### Transactional Business Logic\n\n```gcl\nvar db = Postgres {\n  url: \"localhost\",\n  port: \"5432\",\n  db_name: \"ecommerce\",\n  login: \"app\",\n  password: \"app_secret\"\n};\n\nfn processOrder(userId: int, items: Array<OrderItem>) {\n  db.begin();\n\n  try {\n    // Create order\n    var orderResult = db.execute(\n      \"INSERT INTO orders (user_id, status, created_at) VALUES (${userId}, 'pending', NOW()) RETURNING id\"\n    ) as Table;\n    var orderId = orderResult.rows[0][0] as int;\n\n    // Add items and update inventory\n    for (item in items) {\n      // Add order item\n      db.execute(\n        \"INSERT INTO order_items (order_id, product_id, quantity, price) VALUES (${orderId}, ${item.productId}, ${item.quantity}, ${item.price})\"\n      );\n\n      // Decrease inventory\n      db.execute(\n        \"UPDATE products SET stock = stock - ${item.quantity} WHERE id = ${item.productId}\"\n      );\n\n      // Check if stock went negative\n      var stock = db.execute(\"SELECT stock FROM products WHERE id = ${item.productId}\") as Table;\n      if (stock.rows[0][0] as int < 0) {\n        throw \"Insufficient stock for product ${item.productId}\";\n      }\n    }\n\n    // Update order status\n    db.execute(\"UPDATE orders SET status = 'confirmed' WHERE id = ${orderId}\");\n\n    db.commit();\n    return orderId;\n\n  } catch (e) {\n    db.rollback();\n    print(\"Order failed: ${e}\");\n    throw e;\n  }\n}\n```\n\n### Reporting and Analytics\n\n```gcl\nvar db = Postgres {\n  url: \"analytics.example.com\",\n  port: \"5432\",\n  db_name: \"reporting\",\n  login: \"analyst\",\n  password: \"analyst_pwd\"\n};\n\n// Generate daily sales report\nvar sales = db.execute(\"\"\"\n  SELECT\n    date_trunc('day', created_at) as day,\n    count(*) as num_orders,\n    sum(amount) as total_revenue,\n    avg(amount) as avg_order_value\n  FROM orders\n  WHERE created_at >= NOW() - INTERVAL '30 days'\n  GROUP BY day\n  ORDER BY day\n\"\"\") as Table;\n\n// Export to CSV for spreadsheet analysis\ndb.query_to_csv(\n  \"/reports/daily_sales_last_30_days.csv\",\n  \"\"\"\n  SELECT\n    date_trunc('day', created_at) as day,\n    count(*) as num_orders,\n    sum(amount) as total_revenue,\n    avg(amount) as avg_order_value\n  FROM orders\n  WHERE created_at >= NOW() - INTERVAL '30 days'\n  GROUP BY day\n  ORDER BY day\n  \"\"\",\n  ','\n);\n\nprint(\"Report generated\");\n```\n\n## Best Practices\n\n### Connection Management\n\n- **Reuse connections**: Create one `Postgres` instance per database, reuse across queries\n- **Close transactions**: Always pair `begin()` with `commit()` or `rollback()`\n- **Use connection pooling**: In production, consider external connection poolers like PgBouncer\n\n```gcl\n// Good: Single connection instance\nvar db = createDbConnection();\nfor (i in 0..100) {\n  db.execute(\"INSERT INTO events (data) VALUES ('event-${i}')\");\n}\n\n// Bad: Creating connection per query\nfor (i in 0..100) {\n  var db = createDbConnection(); // Wasteful!\n  db.execute(\"INSERT INTO events (data) VALUES ('event-${i}')\");\n}\n```\n\n### Transaction Safety\n\n- **Always use try-catch** with transactions\n- **Keep transactions short**: Long-running transactions block other operations\n- **Avoid CSV operations in transactions**: They are mutually exclusive\n\n```gcl\ndb.begin();\ntry {\n  // Do work\n  db.execute(\"UPDATE accounts SET balance = balance - 100 WHERE id = 1\");\n  db.commit();\n} catch (e) {\n  db.rollback();\n  throw e;\n}\n```\n\n### SQL Injection Prevention\n\n- **Use parameterized queries** when possible (with external libraries)\n- **Validate inputs** before embedding in SQL strings\n- **Escape special characters** in user-provided data\n\n```gcl\n// Dangerous: SQL injection risk\nvar userInput = getUserInput();\ndb.execute(\"SELECT * FROM users WHERE name = '${userInput}'\"); // BAD!\n\n// Better: Validate input first\nvar userInput = getUserInput();\nif (isValidUsername(userInput)) {\n  db.execute(\"SELECT * FROM users WHERE name = '${userInput}'\");\n} else {\n  throw \"Invalid username\";\n}\n```\n\n### CSV Performance\n\n- **Use CSV for bulk operations**: Much faster than individual INSERTs\n- **Ensure correct permissions**: COPY requires superuser or specific grants\n- **Validate CSV format**: Mismatched columns cause import failures\n\n```gcl\n// Slow: 10k individual inserts\nfor (i in 0..10000) {\n  db.execute(\"INSERT INTO data (value) VALUES (${i})\");\n}\n\n// Fast: Single CSV import\n// (assuming data is already in CSV file)\ndb.csv_to_table(\"/data/bulk_data.csv\", \"data\", null, ',');\n```\n\n### Error Handling\n\n- **Check for null results**: Not all queries return data\n- **Handle connection failures**: Network issues, credential problems\n- **Log transaction failures**: For debugging and auditing\n\n```gcl\ntry {\n  var result = db.execute(\"SELECT * FROM users WHERE id = 999\");\n\n  if (result == null) {\n    print(\"Query executed but returned no data\");\n  } else {\n    var table = result as Table;\n    if (table.rows.size() == 0) {\n      print(\"No users found\");\n    } else {\n      print(\"Found ${table.rows.size()} users\");\n    }\n  }\n\n} catch (e) {\n  print(\"Database error: ${e}\");\n  // Log, alert, or retry\n}\n```\n\n### Gotchas\n\n- **Transactions and COPY are mutually exclusive**: Call `commit()` or `rollback()` before CSV operations\n- **File paths for CSV**: Must be accessible by PostgreSQL server (not just GreyCat process)\n- **Column count mismatch**: CSV columns must match table schema or specified `cols_name`\n- **Type conversions**: Query results need explicit casting from `any` to specific types\n- **Transaction isolation**: Default isolation level may cause unexpected behavior with concurrent transactions\n- **Result table format**: SELECT results are returned as generic `Table` type, requiring knowledge of column order\n\n### Security\n\n- **Never commit credentials**: Load from environment variables or secure configuration\n- **Use least privilege**: Grant only necessary database permissions\n- **Enable SSL**: Use encrypted connections in production\n\n```gcl\n// Load credentials from environment\nvar db = Postgres {\n  url: env(\"DB_HOST\"),\n  port: env(\"DB_PORT\"),\n  db_name: env(\"DB_NAME\"),\n  login: env(\"DB_USER\"),\n  password: env(\"DB_PASSWORD\")\n};\n```\n\n### Schema Management\n\n- **Version your schema**: Use migration tools\n- **Test DDL changes**: Create/alter tables outside transactions\n- **Document table structures**: Keep schema documentation updated\n\n```gcl\n// Initialize schema if needed\ndb.execute(\"\"\"\n  CREATE TABLE IF NOT EXISTS schema_version (\n    version INT PRIMARY KEY,\n    applied_at TIMESTAMP DEFAULT NOW()\n  )\n\"\"\");\n\nvar version = db.execute(\"SELECT MAX(version) FROM schema_version\") as Table;\nif (version.rows.size() == 0) {\n  // Apply initial schema\n  db.execute(\"CREATE TABLE users (...)\");\n  db.execute(\"CREATE TABLE orders (...)\");\n  db.execute(\"INSERT INTO schema_version (version) VALUES (1)\");\n}\n```\n",
        "plugins/greycat/skills/greycat/references/std/core.md": "# [std](index.html) > core\n\n[Source](core.source.html)\n\nCore primitive types, data structures, and fundamental operations for GreyCat. This module provides the foundation for all GreyCat applications including primitives (int, float, bool), collection types (Array, Map, Table), graph nodes (node, nodeTime, nodeIndex, nodeList, nodeGeo), and mathematical/temporal utilities.\n\n## Primitive Types\n\n### any\nWildcard meta type for storing any value, including primitives. Used when type flexibility is needed.\n\n### null\nMeta type representing null placeholders.\n\n### type\nMeta type for reflection and introspection. Provides access to type metadata, fields, and enum values.\n\n```gcl\n// Get type information\nvar my_type = type::of(some_object);\nvar field_count = my_type.nb_fields();\nvar fields = my_type.fields();\n\n// Access enum by name or offset\nvar error_code = type::enum_by_name<ErrorCode>(typeof ErrorCode, \"runtime_error\");\nvar code_name = type::enum_name(ErrorCode::timeout); // \"timeout\"\nvar code_offset = type::enum_offset(ErrorCode::timeout); // 6\n```\n\n### field\nDescriptor of a type field for meta programming.\n\n```gcl\n// Inspect field metadata\nvar field_info = my_type.field_by_name(\"username\");\nvar field_name = field_info?.name();\nvar field_type = field_info?.type();\nvar is_nullable = field_info?.is_nullable();\n```\n\n### bool\nBoolean type with `true` or `false` literals.\n\n### char\nCharacter type supporting ASCII and UTF-8 codepoints.\n\n### int\nSigned 64-bit integer (-9223372036854775808 to 9223372036854775807).\n\n```gcl\n// Static bounds\nAssert::equals(int::min, -9223372036854775808);\nAssert::equals(int::max, 9223372036854775807);\n\n// Format with thousand separators\nvar formatted = int::to_string(1000000, ','); // \"1,000,000\"\n```\n\n### float\n64-bit floating point (-1.7976931348623157e+308 to 1.7976931348623157e+308).\n\n```gcl\n// Format float with precision\nvar formatted = float::to_string(\n    1234.5678,\n    '.', // decimal separator\n    ',', // thousand separator (optional)\n    2,   // max digits after decimal\n    false // e-notation disabled\n); // \"1,234.57\"\n```\n\n### FloatPrecision\nEnum defining compression precision levels for float values (p1 to p10000000000).\n\n```gcl\nvar precision = FloatPrecision::p1000; // 0.001 precision\n```\n\n### String\nImmutable UTF-8 string type with comprehensive manipulation methods.\n\n```gcl\n// String comparison and searching\nvar text = \"Hello World\";\nAssert::isTrue(text.startsWith(\"Hello\"));\nAssert::isTrue(text.endsWith(\"World\"));\nAssert::isTrue(text.contains(\"lo Wo\"));\nAssert::equals(text.indexOf('W'), 6);\nAssert::equals(text.indexOfString(\"World\"), 6);\n\n// String transformation\nvar lower = text.lowercase(); // \"hello world\"\nvar upper = text.uppercase(); // \"HELLO WORLD\"\nvar trimmed = \"  text  \".trim(); // \"text\"\nvar replaced = text.replace(\"World\", \"GreyCat\"); // \"Hello GreyCat\"\n\n// Slicing and splitting\nvar part = text.slice(0, 5); // \"Hello\"\nvar words = text.split(' '); // [\"Hello\", \"World\"]\n\n// String similarity metrics\nvar similarity = \"hello\".jaro(\"hallo\"); // Jaro distance\nvar jw_similarity = \"hello\".jarowinkler(\"hallo\"); // Jaro-Winkler\nvar distance = \"hello\".levenshtein(\"hallo\"); // Levenshtein distance\n\n// Normalization\nvar normalized = \"Café\".nfkd_casefold(); // NFD normalized, casefolded\n```\n\n### Buffer\nBinary blob manipulation for building strings efficiently.\n\n```gcl\nvar buffer = Buffer {};\nbuffer.add(\"Hello\");\nbuffer.add(\" \");\nbuffer.add(\"World\");\nAssert::equals(buffer.toString(), \"Hello World\");\nAssert::equals(buffer.size(), 11);\nbuffer.clear();\n```\n\n## Collections\n\n### Array<T>\nGeneric ordered container with sorting, searching, and manipulation methods.\n\n```gcl\n// Initialize and populate\nvar numbers = Array<int> {};\nnumbers.fill(5, 0); // [0, 0, 0, 0, 0]\nnumbers.set(0, 10);\nnumbers.add(20);\nnumbers.add_all([30, 40, 50]);\n\n// Accessing and searching\nAssert::equals(numbers.get(0), 10);\nAssert::equals(numbers.size(), 8);\nAssert::equals(numbers.index_of(20), 1);\n\n// Manipulation\nnumbers.swap(0, 1);\nnumbers.sort(SortOrder::asc);\nnumbers.remove(0);\nnumbers.remove_first();\nnumbers.remove_last();\n\n// Capacity management\nnumbers.set_capacity(100);\n\n// Range comparison\nAssert::isTrue(numbers.range_equals(0, 2, 30));\n```\n\n### Map<K, V>\nKey-value store with iteration support.\n\n```gcl\nvar map = Map<String, int> {};\nmap.set(\"one\", 1);\nmap.set(\"two\", 2);\n\nAssert::equals(map.get(\"one\"), 1);\nAssert::isTrue(map.contains(\"two\"));\nAssert::equals(map.size(), 2);\n\nvar values = map.values(); // [1, 2]\nmap.remove(\"one\");\n```\n\n### Table<T>\nTwo-dimensional data structure with column-based operations.\n\n```gcl\n// Create and populate table\nvar table = Table<Person> {};\ntable.init(10, 5); // pre-allocate capacity\n\n// Set individual cells\ntable.set_cell(0, 0, \"John\");\ntable.set_cell(0, 1, 30);\n\n// Set entire rows from objects\ntable.set_row(0, Person { name: \"Alice\", age: 25 });\ntable.add_row(Person { name: \"Bob\", age: 35 });\n\n// Access data\nvar cell_value = table.get_cell(0, 0);\nvar row_obj = table.get_row(0);\n\n// Sorting\ntable.sort(1, SortOrder::desc); // sort by column 1 (age)\ntable.sort_by(0, person_name_field, SortOrder::asc);\n\n// Metadata\nAssert::equals(table.cols(), 5);\nAssert::equals(table.rows(), 2);\n\n// Remove rows\ntable.remove_row(0);\n\n// Apply column mappings for data transformation\nvar mapped = Table::applyMappings(table, mappings);\n```\n\n### Tuple<T, U>\nSimple two-element association structure.\n\n```gcl\nvar pair = Tuple<String, int> { x: \"answer\", y: 42 };\nAssert::equals(pair.x, \"answer\");\nAssert::equals(pair.y, 42);\n```\n\n## Graph Node Types\n\n### node<T>\nPersistent singleton value stored in the graph.\n\n```gcl\nvar user_node: node<User>;\nuser_node.set(User { name: \"Alice\", email: \"alice@example.com\" });\nvar user = user_node.resolve();\nAssert::equals(user.name, \"Alice\");\n\n// Batch resolution\nvar users = node::resolve_all([node1, node2, node3]);\n```\n\n### nodeTime<T>\nTime-series data structure with temporal indexing.\n\n```gcl\nvar temperature: nodeTime<float>;\n\n// Add timestamped values\ntemperature.setAt('2025-01-01T00:00:00Z', 20.5);\ntemperature.setAt('2025-01-01T01:00:00Z', 21.0);\ntemperature.setAt('2025-01-01T02:00:00Z', 21.5);\n\n// Batch insert with delta\ntemperature.setAll('2025-01-02T00:00:00Z', [18.0, 19.0, 20.0], 1h);\n\n// Time-based queries\nvar current = temperature.resolve(); // latest value\nvar at_time = temperature.resolveAt('2025-01-01T00:30:00Z'); // closest previous\nvar exact = temperature.getAt('2025-01-01T01:00:00Z');\nvar time_value = temperature.resolveTimeValueAt('2025-01-01T00:30:00Z');\n\n// Navigation\nvar first_time = temperature.firstTime();\nvar last_time = temperature.lastTime();\nvar first_val = temperature.first();\nvar last_val = temperature.last();\nvar prev_time = temperature.prev('2025-01-01T01:00:00Z');\nvar next_time = temperature.next('2025-01-01T01:00:00Z');\n\n// Range operations\nvar count = temperature.rangeSize('2025-01-01T00:00:00Z', '2025-01-01T23:59:59Z');\nvar total_size = temperature.size();\n\n// Removal\ntemperature.removeAt('2025-01-01T00:00:00Z');\ntemperature.removeAll();\n\n// Sampling (for visualization/analysis)\nvar sample = nodeTime::sample(\n    [temperature],\n    '2025-01-01T00:00:00Z',\n    '2025-01-01T23:59:59Z',\n    1000, // max rows\n    SamplingMode::adaptative,\n    1min, // max dephasing\n    TimeZone::UTC\n);\n\n// Get metadata\nvar info = nodeTime::info([temperature]);\n```\n\n### nodeTimeCursor<T>\nIterator for walking through nodeTime series.\n\n```gcl\nvar cursor = nodeTimeCursor<float> { n: temperature };\ncursor.first();\nwhile (cursor.currentTime() != null) {\n    var t = cursor.currentTime();\n    var v = cursor.current();\n    println(\"${t}: ${v}\");\n    cursor.next();\n}\n\n// Advanced navigation\ncursor.lessOrEq('2025-01-01T12:00:00Z');\ncursor.skip_values(10);\ncursor.skip_duration(1h);\n```\n\n### nodeIndex<K, V>\nPersistent key-value store with O(log n) retrieval.\n\n```gcl\nvar user_index: nodeIndex<String, User>;\n\n// Set and get\nuser_index.set(\"alice\", User { name: \"Alice\", age: 30 });\nuser_index.set(\"bob\", User { name: \"Bob\", age: 25 });\n\nvar user = user_index.get(\"alice\");\nAssert::equals(user?.name, \"Alice\");\n\n// Searching\nvar results = user_index.search(\"ali\", 10); // fuzzy search, max 10 results\nvar closest = nodeIndex::search_closest(user_index, \"alex\", 5);\n\n// Management\nAssert::equals(user_index.size(), 2);\nuser_index.remove(\"alice\");\nuser_index.removeAll();\n\n// Sampling\nvar sample = nodeIndex::sample([user_index], \"a\", 100, SamplingMode::dense);\n\n// Metadata\nvar info = nodeIndex::info([user_index]);\n```\n\n### nodeList<T>\nSparse list with efficient large-scale storage.\n\n```gcl\nvar measurements: nodeList<float>;\n\n// Add and set values\nmeasurements.add(10.5); // append\nmeasurements.add(20.5);\nmeasurements.set(100, 99.9); // sparse indexing\n\n// Access\nvar val = measurements.get(0);\nvar resolved = measurements.resolve(50); // closest previous if not found\nvar entry = measurements.resolveEntry(50); // returns Tuple<int, T>\n\n// Navigation\nvar first_idx = measurements.firstIndex();\nvar last_idx = measurements.lastIndex();\nvar first_val = measurements.first();\nvar last_val = measurements.last();\n\n// Range operations\nvar count = measurements.rangeSize(0, 100);\nAssert::equals(measurements.size(), 3); // sparse: only 3 values set\n\n// Removal\nmeasurements.remove(100);\nmeasurements.removeAll();\n\n// Sampling\nvar sample = nodeList::sample(\n    [measurements],\n    0,\n    1000,\n    500, // max rows\n    SamplingMode::fixed,\n    10 // max dephasing\n);\n\n// Metadata\nvar info = nodeList::info([measurements]);\n```\n\n### nodeGeo<T>\nGeographic indexed values with spatial queries.\n\n```gcl\nvar poi: nodeGeo<String>; // points of interest\n\n// Add locations\npoi.set(geo{48.8566, 2.3522}, \"Eiffel Tower\");\npoi.set(geo{51.5074, -0.1278}, \"Big Ben\");\n\n// Query by location\nvar location = poi.get(geo{48.8566, 2.3522});\nvar resolved = poi.resolve(geo{48.86, 2.35}); // nearest\n\n// Range queries\nvar box_count = poi.rangeSize(\n    geo{48.0, 2.0}, // SW corner\n    geo{49.0, 3.0}  // NE corner\n);\n\n// Navigation\nvar sw_point = poi.firstIndex(); // most south-west\nvar ne_point = poi.lastIndex(); // most north-east\n\n// Management\npoi.remove(geo{48.8566, 2.3522});\npoi.removeAll();\n\n// Sampling\nvar sample = nodeGeo::sample(\n    [poi],\n    geo{48.0, 2.0},\n    geo{49.0, 3.0},\n    1000,\n    SamplingMode::dense\n);\n```\n\n## Tensor & Numerical Types\n\n### Tensor\nMulti-dimensional array for numerical computations.\n\n```gcl\n// Create tensor\nvar tensor = Tensor {};\ntensor.init(TensorType::f64, [3, 4]); // 3x4 matrix of f64\n\n// Set and get values\ntensor.set([0, 0], 1.5);\ntensor.set([0, 1], 2.5);\nvar value = tensor.get([0, 0]);\n\n// Complex tensors\ntensor.setImag([0, 0], 0.5); // imaginary part\nvar imag_val = tensor.getImag([0, 0]);\n\n// Operations\ntensor.add([0, 0], 0.5); // add to existing value\ntensor.fill(0.0); // fill all with value\nvar total = tensor.sum();\n\n// Iteration\nvar pos = tensor.initPos();\nwhile (tensor.incPos(pos)) {\n    var val = tensor.get(pos);\n    // process value\n}\n\n// Transformations\nvar complex_t = tensor.to_complex_tensor();\nvar real_part = complex_t.get_real_tensor();\nvar imag_part = var_imaginary_tensor();\nvar absolute = complex_t.get_absolute_tensor();\nvar phase = complex_t.get_phase_tensor(false); // radians\n\n// Reshaping and slicing\ntensor.reshape([12]); // reshape to 1D\nvar slice = tensor.slice([0, 0], [2, 2]); // 2x2 sub-tensor\ntensor.slide(5); // sliding window\n\n// Scaling\nvar scaled = tensor.scale(2.0, 1.0); // scale by 2.0\n\n// Distance calculation\nvar dist = tensor.distance(other_tensor, TensorDistance::euclidean);\nvar l2_dist = tensor.distance(other_tensor, TensorDistance::l2sq);\nvar cos_dist = tensor.distance(other_tensor, TensorDistance::cosine);\n\n// Utilities\nAssert::equals(tensor.size(), 12);\nAssert::equals(tensor.dim(), 2);\nvar shape = tensor.shape(); // [3, 4]\nvar tensor_type = tensor.type();\n\n// Conversion\nvar as_table = tensor.toTable(); // 1D or 2D only\nvar as_string = tensor.toString();\n\n// Static constructors\nvar vec = Tensor::wrap_1d(TensorType::f64, [1.0, 2.0, 3.0]);\n```\n\n### TensorType\nEnum defining tensor data types: i32, i64, f32, f64, c64, c128.\n\n### TensorDistance\nDistance metrics: euclidean (l2), l2sq, cosine.\n\n### VectorIndex<T>\nHigh-performance vector similarity search using HNSW algorithm.\n\n```gcl\nvar index = VectorIndex<String> {\n    distance: TensorDistance::cosine,\n    rng: Random { seed: 42 }\n};\n\n// Index vectors\nvar vec1 = Tensor::wrap_1d(TensorType::f32, [0.1, 0.2, 0.3]);\nvar vec2 = Tensor::wrap_1d(TensorType::f32, [0.4, 0.5, 0.6]);\n\nindex.add(vec1_node, \"item1\");\nindex.add(vec2_node, \"item2\");\n\n// Search for similar vectors\nvar query = Tensor::wrap_1d(TensorType::f32, [0.15, 0.25, 0.35]);\nvar results = index.search(query, 10); // top 10 results\n\nfor (_, result in results) {\n    println(\"${result.value}: distance=${result.distance}\");\n}\n\nAssert::equals(index.size(), 2);\n```\n\n## Geographic Types\n\n### geo\nLatitude/longitude coordinates with distance calculations.\n\n```gcl\n// Create geo point\nvar paris = geo{48.8566, 2.3522};\nvar london = geo{51.5074, -0.1278};\n\n// Access components\nvar latitude = paris.lat();\nvar longitude = paris.lng();\n\n// Distance in meters\nvar distance = paris.distance(london); // ~344,000 meters\n\n// String representations\nvar coords = paris.toString(); // \"48.8566,2.3522\"\nvar geohash = paris.toGeohash(); // GeoHash encoding\n\n// Bounds\nAssert::equals(geo::min, geo{-85.0511287602, -179.9999999581});\nAssert::equals(geo::max, geo{85.0511287602, 179.9999999581});\n```\n\n### GeoCircle\nCircular geographic region.\n\n```gcl\nvar circle = GeoCircle {\n    center: geo{48.8566, 2.3522},\n    radius: 10000.0 // 10km radius\n};\n\nvar point = geo{48.86, 2.35};\nAssert::isTrue(circle.contains(point));\n\n// Bounding box\nvar sw = circle.sw();\nvar ne = circle.ne();\n```\n\n### GeoBox\nRectangular geographic region.\n\n```gcl\nvar box = GeoBox {\n    sw: geo{48.8, 2.3},\n    ne: geo{48.9, 2.4}\n};\n\nAssert::isTrue(box.contains(geo{48.85, 2.35}));\nAssert::isFalse(box.contains(geo{49.0, 2.5}));\n```\n\n### GeoPoly\nPolygonal geographic region.\n\n```gcl\nvar polygon = GeoPoly {\n    points: [\n        geo{48.8, 2.3},\n        geo{48.9, 2.3},\n        geo{48.9, 2.4},\n        geo{48.8, 2.4}\n    ]\n};\n\nAssert::isTrue(polygon.contains(geo{48.85, 2.35}));\nvar sw = polygon.sw();\nvar ne = polygon.ne();\n```\n\n## Time & Duration\n\n### time\nAbsolute moment in time with calendar operations.\n\n```gcl\n// Create time\nvar now = time::now();\nvar specific = time::new(1704067200, DurationUnit::seconds);\nvar parsed = time::parse(\"2025-01-01T00:00:00Z\", null); // ISO 8601\n\n// Arithmetic with durations\nvar later = now + 1h;\nvar earlier = now - 30min;\n\n// Calendar operations\nvar next_day = now.calendar_add(1, CalendarUnit::day, TimeZone::UTC);\nvar next_month = now.calendar_add(1, CalendarUnit::month, null);\nvar start_of_day = now.calendar_floor(CalendarUnit::day, null);\nvar end_of_day = now.calendar_ceiling(CalendarUnit::day, null);\nvar week_start = now.startOfWeek(null);\nvar week_end = now.endOfWeek(null);\n\n// Calendar queries\nvar day_of_year = now.dayOfYear(null); // 0-365\nvar day_of_week = now.dayOfWeek(null); // 0 (Sunday) - 6 (Saturday)\nvar week_of_year = now.weekOfYear(null); // 1-53\nvar days_in_month = now.daysInMonth(null);\n\nAssert::isTrue(time::isLeap(2024));\nAssert::equals(time::totalDaysInYear(2024), 366);\nAssert::equals(time::totalDaysInMonth(2, 2024), 29);\n\n// Conversion\nvar epoch_millis = now.to(DurationUnit::milliseconds);\nvar floored = now.floor(DurationUnit::hours);\n\n// Formatting\nvar formatted = now.format(\"%Y-%m-%d %H:%M:%S\", TimeZone::UTC);\nvar iso = now.format(\"%+\", null); // ISO 8601\n\n// Date conversion\nvar date = now.toDate(TimeZone::UTC);\n\n// Bounds\nAssert::equals(time::min, -9223372036854775808_time);\nAssert::equals(time::max, 9223372036854775807_time);\n\n// Context\nvar context_time = time::current(); // contextual time (defaults to time::min)\n```\n\n### duration\nTime interval with flexible units.\n\n```gcl\n// Create durations\nvar one_hour = duration::new(1, DurationUnit::hours);\nvar thirty_mins = duration::new(30, DurationUnit::minutes);\nvar precise = duration::newf(1.5, DurationUnit::hours);\n\n// Conversion\nvar in_minutes = one_hour.to(DurationUnit::minutes); // 60\nvar in_seconds = one_hour.to(DurationUnit::seconds); // 3600\nvar in_hours_float = precise.tof(DurationUnit::hours); // 1.5\n\n// Arithmetic\nvar extended = one_hour.add(30, DurationUnit::minutes); // 90 minutes\nvar reduced = one_hour.subtract(15, DurationUnit::minutes); // 45 minutes\n\n// Literal syntax\nvar d1 = 1h;\nvar d2 = 30min;\nvar d3 = 5s;\n```\n\n### Date\nCalendar date with timezone awareness.\n\n```gcl\n// Create dates\nvar date = Date {\n    year: 2025,\n    month: 1,\n    day: 1,\n    hour: 12,\n    minute: 30,\n    second: 0,\n    microsecond: 0\n};\n\nvar from_time = Date::from_time(time::now(), TimeZone::UTC);\nvar parsed_date = Date::parse(\"2025-01-01\", \"%Y-%m-%d\");\n\n// Convert to time\nvar as_time = date.to_time(TimeZone::UTC);\nvar nearest = date.to_nearest_time(TimeZone::\"Europe/Paris\"); // handles DST\n```\n\n### DurationUnit\nEnum: microseconds, milliseconds, seconds, minutes, hours, days.\n\n### CalendarUnit\nEnum: year, month, day, hour, minute, second, microsecond.\n\n### TimeZone\nComprehensive timezone enumeration covering all IANA timezones.\n\n```gcl\nvar paris_time = now.toDate(TimeZone::\"Europe/Paris\");\nvar ny_time = now.toDate(TimeZone::\"America/New_York\");\nvar utc_time = now.toDate(TimeZone::UTC);\n```\n\n## Enumerations\n\n### ErrorCode\nRuntime error codes: none(0), interrupted(1), await(2), timeout(6), forbidden(7), runtime_error(8).\n\n### SamplingMode\nData sampling strategies:\n- fixed(0): Fixed delta between indices\n- fixed_reg(1): Fixed with linear regression interpolation\n- adaptative(2): Adaptive sampling ensuring minimum skip\n- dense(3): All elements in range\n\n```gcl\nvar sample = nodeTime::sample(\n    [timeseries],\n    from_time,\n    to_time,\n    1000,\n    SamplingMode::adaptative,\n    max_dephase,\n    tz\n);\n```\n\n### SortOrder\nSorting direction: asc, desc.\n\n```gcl\narray.sort(SortOrder::desc);\ntable.sort(column_idx, SortOrder::asc);\n```\n\n## Primitive Tuples\n\nCompact primitive types for efficient storage and indexing.\n\n### t2, t3, t4\nInteger tuples (2-4 dimensions).\n\n```gcl\nvar point2d: t2; // stores 2 ints as single primitive\nvar x = point2d.x0();\nvar y = point2d.x1();\n\nvar point3d: t3;\nvar z = point3d.x2();\n\nvar point4d: t4;\nvar w = point4d.x3();\n```\n\n### t2f, t3f, t4f\nFloat tuples (2-4 dimensions).\n\n```gcl\nvar vector: t3f;\nvar x = vector.x0();\nvar y = vector.x1();\nvar z = vector.x2();\n```\n\n### str\nCompact string (max 10 lowercase ASCII chars) stored as primitive. Efficient for indexing.\n\n```gcl\nvar code: str = \"product123\"; // error: too long\nvar valid: str = \"item42\"; // OK\n```\n\n## Error Handling\n\n### Error\nError type with stack traces.\n\n```gcl\ntry {\n    // risky operation\n    throw Error { message: \"Something went wrong\" };\n} catch (e: Error) {\n    println(\"Error: ${e.message}\");\n    for (_, frame in e.stack) {\n        println(\"  at ${frame.function} (${frame.module}:${frame.line}:${frame.column})\");\n    }\n}\n```\n\n### ErrorFrame\nStack frame information: module, function, line, column.\n\n## Utility Functions\n\n### Mathematical Functions\n\n```gcl\n// Exponential and logarithmic\nvar e_x = exp(2.0);\nvar natural_log = log(10.0);\nvar log_base2 = log2(8.0);\nvar log_base10 = log10(100.0);\nvar power = pow(2.0, 8.0); // 256.0\n\n// Trigonometric\nvar sine = sin(MathConstants::pi / 2); // 1.0\nvar cosine = cos(0.0); // 1.0\nvar tangent = tan(MathConstants::pi / 4); // 1.0\n\n// Inverse trigonometric\nvar arc_sine = asin(1.0); // π/2\nvar arc_cosine = acos(0.0); // π/2\nvar arc_tangent = atan(1.0); // π/4\n\n// Hyperbolic\nvar sinh_val = sinh(1.0);\nvar cosh_val = cosh(1.0);\nvar tanh_val = tanh(1.0);\n\n// Rounding\nvar floored = floor(3.7); // 3.0\nvar ceiled = ceil(3.2); // 4.0\nvar rounded = round(3.5); // 4.0\nvar truncated = trunc(3.9); // 3.0\nvar precise = roundp(3.14159, 2); // 3.14\n\n// Other\nvar square_root = sqrt(16.0); // 4.0\nvar absolute = abs(-5); // 5\nvar minimum = min(10, 20); // 10\nvar maximum = max(10, 20); // 20\n\nAssert::isTrue(isNaN(0.0 / 0.0));\n```\n\n### MathConstants\nMathematical constants: e, log_2e, log_10e, ln2, ln10, pi, pi_2, pi_4, m1_pi, m2_pi, m2_sqrt_pi, sqrt2, sqrt1_2.\n\n```gcl\nvar circle_area = MathConstants::pi * r * r;\nvar half_pi = MathConstants::pi_2;\n```\n\n### Other Utility Functions\n\n```gcl\n// Parsing\nvar number = parseNumber(\"42\"); // int or float\nvar hex_val = parseHex(\"FF\"); // 255\n\n// Cloning\nvar copy = clone(original_object);\n\n// Enum value access\nvar code_value = valueOf(ErrorCode::timeout); // 6\n\n// Printing\nprintln(\"Hello World\"); // with newline\nprint(my_object); // pretty print with formatting\npprint(\"text\"); // print without newline\n\n// Logging\nerror(\"Critical error occurred\");\nwarn(\"This is a warning\");\ninfo(\"Informational message\");\nperf(\"Performance metric\");\ntrace(\"Detailed trace information\");\n```\n\n## Supporting Types\n\n### NodeInfo<T>\nStatistics about graph nodes.\n\n```gcl\ntype NodeInfo<T> {\n    size: int;    // number of elements\n    from: T?;     // first value (inclusive)\n    to: T?;       // last value (inclusive)\n}\n\nvar info = nodeTime::info([timeseries]);\nprintln(\"Size: ${info[0].size}, From: ${info[0].from}, To: ${info[0].to}\");\n```\n\n### SearchResult<K, V>\nResult from vector/index search operations.\n\n```gcl\nvar results = vector_index.search(query_vector, 10);\nfor (_, result in results) {\n    println(\"Key: ${result.key}, Value: ${result.value}, Distance: ${result.distance}\");\n}\n```\n\n### TableColumnMapping\nDescribes column extractors for table transformations.\n\n```gcl\nvar mapping = TableColumnMapping {\n    column: 0,\n    extractors: [\"*\", \"name\"] // resolve node, then get 'name' field\n};\nvar transformed = Table::applyMappings(original_table, [mapping]);\n```\n",
        "plugins/greycat/skills/greycat/references/std/io.md": "# [std](index.html) > io\n\n[Source](io.source.html)\n\nInput/output module providing file operations, data serialization, network communication, and email functionality for GreyCat applications.\n\n## Writers & Readers\n\nAll reader types implement position tracking and support streaming operations for memory-efficient processing of large files. Writers support both append and overwrite modes. CSV operations include advanced analysis capabilities for automatic type inference and code generation.\n\n### GcbWriter / GcbReader\nBinary format using GreyCat's ABI encoding for efficient serialization of any GreyCat type.\n\n```gcl\n// Write binary data\nvar writer = GcbWriter<MyType> { path: \"/path/to/data.gcb\" };\nwriter.write(my_object);\nwriter.flush();\n\n// Read binary data back\nvar reader = GcbReader<MyType> { path: \"/path/to/data.gcb\" };\nwhile (reader.can_read()) {\n    var obj = reader.read();\n    Assert::isNotNull(obj);\n}\n```\n\n### BinReader\nLow-level binary file reader for reading primitive types and tensors directly from binary files.\n\n```gcl\n// Read raw binary data (integers, floats, tensors)\nvar reader = BinReader { path: \"/path/to/data.bin\" };\n\n// Read primitive types\nvar int32_val = reader.read_i32();   // Read 32-bit integer\nvar int64_val = reader.read_i64();   // Read 64-bit integer\nvar float32_val = reader.read_f32(); // Read IEEE 754 binary32\nvar float64_val = reader.read_f64(); // Read IEEE 754 binary64\n\n// Read tensor with specified type and shape\nvar tensor = reader.read_tensor(TensorType::f32, [3, 3]);\n\n// Check available data\nwhile (reader.can_read()) {\n    var bytes_left = reader.available();\n    // Process remaining data...\n}\n```\n\n### XmlReader\nXML file reader for parsing XML documents into typed GreyCat objects.\n\n```gcl\n// Read XML data\nvar reader = XmlReader<MyXmlType> { path: \"/path/to/data.xml\" };\nwhile (reader.can_read()) {\n    var obj = reader.read();\n    // Process XML element...\n}\n```\n\n### TextWriter / TextReader\nUTF-8 text file operations with line-based I/O.\n\n```gcl\n// Write text lines\nvar writer = TextWriter<String> { path: \"/path/to/output.txt\" };\nwriter.writeln(\"First line\");\nwriter.writeln(\"Second line\");\nwriter.flush();\n\n// Read text lines\nvar reader = TextReader { path: \"/path/to/output.txt\" };\nAssert::equals(reader.read(), \"First line\");\nAssert::equals(reader.read(), \"Second line\");\nAssert::isFalse(reader.can_read());\n```\n\n### JsonWriter / JsonReader\nJSON serialization supporting both single objects and NDJSON (newline-delimited JSON) streams.\n\n```gcl\n// Write JSON objects\nvar writer = JsonWriter<Person> { path: \"/path/to/people.json\" };\nwriter.writeln(person1); // Each call writes one JSON object per line\nwriter.writeln(person2);\nwriter.flush();\n\n// Read JSON stream\nvar reader = JsonReader<Person> { path: \"/path/to/people.json\" };\nvar first_person = reader.read();\nAssert::equals(first_person.name, \"John\");\n\n// Parse JSON strings directly\nvar parsed = Json<Person> {}.parse(\"{\\\"name\\\":\\\"Alice\\\",\\\"age\\\":30}\");\nAssert::equals(parsed.name, \"Alice\");\n```\n\n### CsvWriter / CsvReader\nCSV file operations with automatic header generation and configurable formatting.\n\n```gcl\n// Configure CSV format\nvar format = CsvFormat {\n    header_lines: 1,\n    separator: ',',\n    string_delimiter: '\"'\n};\n\n// Write structured data as CSV\nvar writer = CsvWriter<Employee> { path: \"/path/to/employees.csv\", format: format };\nwriter.write(employee1); // Headers written automatically on first write\nwriter.write(employee2);\nwriter.flush();\n\n// Read CSV with type validation\nvar reader = CsvReader<Employee> { path: \"/path/to/employees.csv\", format: format };\nwhile (reader.can_read()) {\n    var emp = reader.read();\n    Assert::isNotNull(emp.name);\n}\n\n// Re-use reader for multiple files\nreader.set_path(\"/path/to/other_employees.csv\");\nAssert::isTrue(reader.can_read());\n```\n\n## CSV Analysis & Code Generation\n\n### Csv\nStatic utility for analyzing CSV files and generating GreyCat types.\n\n```gcl\n// Analyze CSV structure\nvar config = CsvAnalysisConfig {\n    row_limit: 1000,\n    enumerable_limit: 50\n};\n\nvar files = Array<File> {File::open(\"/path/to/sales.csv\")!!};\nvar stats = Csv::analyze(files, config);\n\n// Generate GreyCat types based on analysis\nvar type_definitions = Csv::generate(stats);\nAssert::isTrue(type_definitions.contains(\"type\"));\n\n// Sample data for preview\nvar reader = CsvReader<any> { path: \"/path/to/sales.csv\" };\nvar sample_table = Csv::sample(reader, 100);\nAssert::equals(sample_table.rows(), 100);\n```\n\n## File System Operations\n\n### File\nComprehensive file system utilities for file and directory management.\n\n```gcl\n// File discovery and metadata\nvar csv_files = File::ls(\"/path/to/data\", \".csv\", true); // Recursive search\nAssert::isTrue(csv_files.size() > 0);\n\nvar file = File::open(\"/path/to/important.txt\")!!;\nAssert::isFalse(file.isDir());\nAssert::equals(file.extension()!!, \"txt\");\nAssert::isNotNull(file.sha256()!!);\n\n// File operations\nAssert::isTrue(File::copy(\"/path/to/source.txt\", \"/path/to/dest.txt\"));\nAssert::isTrue(File::rename(\"/path/to/old.txt\", \"/path/to/new.txt\"));\nAssert::isTrue(File::delete(\"/path/to/unwanted.txt\"));\nAssert::isTrue(File::mkdir(\"/path/to/directory\"));\n\n// Working directories\nvar base_dir = File::baseDir();\nvar user_dir = File::userDir();\nvar work_dir = File::workingDir();\nAssert::isNotNull(base_dir);\n```\n\n### FileWalker\nIterator for traversing file system hierarchies.\n\n```gcl\n// Walk through directory structure\nvar walker = FileWalker { path: \".\" };\nvar file_count = 0;\n\nwhile (!walker.isEmpty()) {\n    var file = walker.next();\n    if (file != null && !file.isDir()) {\n        file_count++;\n    }\n}\n\nprintln(\"file count = ${file_count}\");\n```\n\n## Network & Web\n\n### Url\nURL parsing and manipulation utility.\n\n```gcl\n// Parse URL components\nvar url = Url::parse(\"https://api.example.com:8080/users?active=true#section1\");\n\nAssert::equals(url.protocol, \"https\");\nAssert::equals(url.host, \"api.example.com\");\nAssert::equals(url.port, 8080);\nAssert::equals(url.path, \"/users\");\nAssert::equals(url.params?.get(\"active\"), \"true\");\nAssert::equals(url.hash, \"section1\");\n```\n\n### Http\nHTTP client for REST API communication and file downloads.\n\n```gcl\n// HTTP GET with custom headers (Map<String, String>)\nvar headers = Map<String, String> {\n    [\"Authorization\"] = \"Bearer token123\",\n    [\"Accept\"] = \"application/json\"\n};\n\nvar response = Http<String> {}.get(\"https://api.example.com/users\", headers);\nAssert::isNotNull(response);\n\n// Download file directly\nHttp<any> {}.getFile(\"https://example.com/data.csv\", \"/path/to/local.csv\", null);\nvar downloaded = File::open(\"/path/to/local.csv\")!!;\nAssert::isTrue(downloaded.size!! > 0);\n\n// POST data\nvar payload = User { name: \"John\", email: \"john@example.com\" };\nvar result = Http<User> {}.post(\"https://api.example.com/users\", payload, headers);\nAssert::isNotNull(result);\n```\n\n## Email & Communication\n\n### Email & Smtp\nEmail composition and SMTP delivery.\n\n```gcl\n// Configure SMTP server\nvar smtp = Smtp {\n    host: \"smtp.example.com\",\n    port: 587,\n    mode: SmtpMode::starttls,\n    authenticate: SmtpAuth::plain,\n    user: \"sender@example.com\",\n    pass: \"password123\"\n};\n\n// Compose and send email\nvar email = Email {\n    from: \"sender@example.com\",\n    to: [\"recipient@example.com\"],\n    cc: [\"cc@example.com\"],\n    subject: \"Test Email\",\n    body: \"<h1>Hello World</h1>\",\n    body_is_html: true\n};\n\n// Send email (would throw exception on failure)\nsmtp.send(email);\n```\n",
        "plugins/greycat/skills/greycat/references/std/runtime.md": "# [std](index.html) > runtime\n\n[Source](runtime.source.html)\n\nRuntime module providing task execution, security, scheduling, system operations, and server management for GreyCat applications.\n\n## Task Execution & Parallelism\n\n### Job<T>\nRepresents a unit of computation executed in parallel, awaited by a parent Task.\n\n```gcl\n// Create jobs for parallel execution\nvar job1 = Job<int> { function: compute_sum, arguments: [array1] };\nvar job2 = Job<int> { function: compute_sum, arguments: [array2] };\n\n// Execute jobs in parallel\nawait([job1, job2], MergeStrategy::strict);\n\n// Retrieve results\nvar result1 = job1.result();\nvar result2 = job2.result();\n```\n\n### MergeStrategy\nControls how node updates from jobs merge with the main graph.\n\n- **strict**: All or nothing. Any conflict throws an exception. Provides strongest consistency guarantee (recommended default).\n- **first_wins**: Partial strategy. Conflicts resolved using previously inserted values. Non-blocking.\n- **last_wins**: Partial strategy. Conflicts resolved by overriding with current values. Non-blocking.\n\n```gcl\n// Safe merge: fail on conflicts\nawait(jobs, MergeStrategy::strict);\n\n// Optimistic merge: keep first value on conflict\nawait(jobs, MergeStrategy::first_wins);\n\n// Force merge: always use latest value\nawait(jobs, MergeStrategy::last_wins);\n```\n\n### Task\nRepresents an executing or completed task with metadata and status tracking.\n\n```gcl\n// Get current task information\nvar task_id = Task::id();\nvar parent_id = Task::parentId();\n\n// Report progress (0.0 to 1.0)\nTask::progress(0.5); // 50% complete\n\n// Query running tasks\nvar active_tasks = Task::running();\nfor (_, task in active_tasks) {\n    println(\"Task ${task.task_id}: ${task.status}\");\n    println(\"  User: ${task.user_id}, Duration: ${task.duration}\");\n}\n\n// Get task history\nvar past_tasks = Task::history(0, 100); // offset, max\n\n// Check if specific task is running\nvar is_active = Task::is_running(task_id);\n\n// Cancel a running task\nvar cancelled = Task::cancel(task_id);\n```\n\n### TaskStatus\nEnum representing task lifecycle states: empty, waiting, running, await, cancelled, error, ended, ended_with_errors, breakpoint.\n\n## Scheduler\n\nThe scheduler manages recurring tasks that execute automatically without user intervention. Essential for maintenance operations, data processing, and system health checks.\n\n### Basic Scheduling\n\n```gcl\nfn backup_database() {\n    // Perform database backup\n    Runtime::backup_delta();\n}\n\nfn schedule_backups() {\n    // Schedule backup every day at 2 AM\n    Scheduler::add(\n        backup_database,\n        DailyPeriodicity { hour: 2 },\n        null\n    );\n}\n```\n\n### Advanced Scheduling\n\n```gcl\nfn health_check() {\n    // Check system health and log results\n    var info = Runtime::info();\n    info(\"Memory usage: ${info.mem_worker} / ${info.mem_total}\");\n}\n\nfn schedule_health_checks() {\n    // Schedule health checks every 5 minutes, starting in 1 hour\n    Scheduler::add(\n        health_check,\n        FixedPeriodicity { every: 5min },\n        PeriodicOptions {\n            start: time::now() + 1hour,\n            max_duration: 30s,\n            activated: true\n        }\n    );\n}\n```\n\n### Managing Scheduled Tasks\n\n```gcl\nfn manage_scheduled_tasks() {\n    // Find a specific task\n    var ptask = Scheduler::find(health_check);\n    if (ptask != null) {\n        println(\"Health check runs every ${ptask.periodicity.every}\");\n        println(\"Next execution: ${ptask.next_execution}\");\n        println(\"Executed ${ptask.execution_count} times\");\n    }\n\n    // Temporarily disable a task\n    Scheduler::deactivate(backup_database);\n\n    // Re-enable it later\n    Scheduler::activate(backup_database);\n\n    // List all scheduled tasks\n    var all_tasks = Scheduler::list();\n    for (_, ptask in all_tasks) {\n        println(\"${ptask.function}: active=${ptask.is_active}\");\n    }\n}\n```\n\n### Periodicity Types\n\n#### FixedPeriodicity\nTasks that repeat at fixed intervals.\n\n```gcl\n// Every 30 minutes\nFixedPeriodicity { every: 30min }\n\n// Every 2 hours\nFixedPeriodicity { every: 2hour }\n\n// Every day (24 hours)\nFixedPeriodicity { every: 24hour }\n```\n\n#### DailyPeriodicity\nTasks executed daily at specific times. All fields default to 0 (midnight).\n\n```gcl\n// Run at 2:30 PM every day\nDailyPeriodicity { hour: 14, minute: 30 }\n\n// Run at midnight (all defaults)\nDailyPeriodicity {}\n\n// Run at 9 AM Europe/Luxembourg time\nDailyPeriodicity {\n    hour: 9,\n    timezone: TimeZone::\"Europe/Luxembourg\"\n}\n```\n\n#### WeeklyPeriodicity\nTasks that run on specific days of the week.\n\n```gcl\n// Every Monday and Friday at 9 AM\nWeeklyPeriodicity {\n    days: [DayOfWeek::Mon, DayOfWeek::Fri],\n    daily: DailyPeriodicity { hour: 9 }\n}\n\n// Every weekday at midnight\nWeeklyPeriodicity {\n    days: [\n        DayOfWeek::Mon,\n        DayOfWeek::Tue,\n        DayOfWeek::Wed,\n        DayOfWeek::Thu,\n        DayOfWeek::Fri\n    ]\n}\n```\n\n#### MonthlyPeriodicity\nTasks that run monthly on specific days.\n\n```gcl\n// 15th of every month at 2 PM\nMonthlyPeriodicity {\n    days: [15],\n    daily: DailyPeriodicity { hour: 14 }\n}\n\n// First and last day of month at midnight\nMonthlyPeriodicity {\n    days: [1, -1] // -1 means last day\n}\n\n// Three days a month at 9:30 PM\nMonthlyPeriodicity {\n    days: [1, 15, -1],\n    daily: DailyPeriodicity { hour: 9, minute: 30 }\n}\n```\n\n#### YearlyPeriodicity\nTasks that run on specific calendar dates each year.\n\n```gcl\n// New Year's Day and Christmas\nYearlyPeriodicity {\n    dates: [\n        DateTuple { day: 1, month: Month::Jan },\n        DateTuple { day: 25, month: Month::Dec }\n    ]\n}\n\n// Quarterly reports (1st of each quarter)\nYearlyPeriodicity {\n    dates: [\n        DateTuple { day: 1, month: Month::Jan },\n        DateTuple { day: 1, month: Month::Apr },\n        DateTuple { day: 1, month: Month::Jul },\n        DateTuple { day: 1, month: Month::Oct }\n    ]\n}\n```\n\n## Runtime & System Information\n\n### Runtime\nSystem runtime operations and configuration.\n\n```gcl\n// Get runtime information\nvar info = Runtime::info();\nprintln(\"GreyCat version: ${info.version}\");\nprintln(\"Program version: ${info.program_version}\");\nprintln(\"Architecture: ${info.arch}\");\nprintln(\"Timezone: ${info.timezone}\");\nprintln(\"License: ${info.license.type} (${info.license.company})\");\nprintln(\"IO threads: ${info.io_threads}\");\nprintln(\"Background threads: ${info.bg_threads}\");\nprintln(\"Foreground threads: ${info.fg_threads}\");\nprintln(\"Total memory: ${info.mem_total} MB\");\nprintln(\"Worker memory: ${info.mem_worker} MB\");\nprintln(\"Disk data: ${info.disk_data_bytes} bytes\");\n\n// Access graph root\nvar root = Runtime::root();\n\n// Get ABI (Application Binary Interface)\nvar abi_info = Runtime::abi();\n\n// Sleep current thread\nRuntime::sleep(1s);\n\n// Backup operations\nRuntime::backup_full(); // Full backup\nRuntime::backup_delta(); // Incremental backup\n\n// Defragmentation\nRuntime::defrag();\n```\n\n### System\nSystem-level operations including process execution and environment access.\n\n```gcl\n// Execute command and wait for result\nvar output = System::exec(\"/usr/bin/date\", [\"+%Y-%m-%d\"]);\nprintln(\"Current date: ${output}\");\n\n// Spawn background process\nvar child = System::spawn(\"/path/to/script.sh\", [\"--option\", \"value\"]);\n\n// Wait for completion\nvar result = child.wait();\nprintln(\"Exit code: ${result.code}\");\nprintln(\"Output: ${result.stdout}\");\nprintln(\"Errors: ${result.stderr}\");\n\n// Or kill the process\nchild.kill();\n\n// Get timezone\nvar tz = System::tz();\n\n// Access environment variables\nvar home = System::getEnv(\"HOME\");\nvar path = System::getEnv(\"PATH\");\n```\n\n### License\nLicense information structure.\n\n```gcl\nvar license = Runtime::info().license;\nprintln(\"Type: ${license.type}\"); // community, enterprise, testing\nprintln(\"Name: ${license.name}\");\nprintln(\"Company: ${license.company}\");\nprintln(\"Valid from: ${license.start}\");\nprintln(\"Valid until: ${license.end}\");\nprintln(\"Max memory: ${license.max_memory} MB\");\n```\n\n## Security & Authentication\n\n### User\nUser management and authentication.\n\n```gcl\n// Login with credentials (returns session token)\nvar token = User::login(\"username:password\", true); // use_cookie = true\n\n// JWT token login\nvar session_token = User::tokenLogin(jwt_token, true);\n\n// Logout\nUser::logout();\n\n// Renew session\nvar new_token = User::renew(true);\n\n// Get current user info\nvar current_user_id = User::current();\nvar me = User::me();\nprintln(\"Logged in as: ${me.full_name} (${me.email})\");\nprintln(\"Role: ${me.role}\");\n\n// Check permissions\nvar user_permissions = User::permissions();\nvar has_admin = User::hasPermission(\"admin\");\nvar has_debug = User::hasPermission(\"debug\");\n\n// Get user by name or ID\nvar user = User::getByName(\"alice\");\nvar user_by_id = User::get(1);\n\n// Password management\nUser::setPassword(\"alice\", \"new_secure_password\");\nvar password_valid = User::checkPassword(\"alice\", \"test_password\");\n```\n\n### UserGroup\nGroups for organizing users with shared permissions.\n\n```gcl\n// All security entities (users and groups)\nvar entities = SecurityEntity::all();\nfor (_, entity in entities) {\n    println(\"${entity.name}: active=${entity.activated}\");\n}\n\n// Create or update entity\nvar group = UserGroup {\n    name: \"developers\",\n    activated: true\n};\nvar group_id = SecurityEntity::set(group);\n```\n\n### UserGroupPolicy\nPolicies defining group access levels.\n\n```gcl\nvar policy = UserGroupPolicy {\n    group_id: 1,\n    type: UserGroupPolicyType::write\n};\n\n// Policy types: read, write, execute\n```\n\n### OpenIDConnect\nOpenID Connect authentication configuration.\n\n```gcl\n// Get current OIDC configuration\nvar oidc_config = OpenIDConnect::config();\nif (oidc_config != null) {\n    println(\"OIDC URL: ${oidc_config.url}\");\n    println(\"Client ID: ${oidc_config.clientId}\");\n}\n```\n\n### Permission & Role\nAccess control definitions (defined at module level with @permission and @role annotations).\n\n```gcl\n// Predefined permissions\n// @permission(\"public\", \"default, associated with anonymous users\")\n// @permission(\"admin\", \"allows to administrate anything on the server\")\n// @permission(\"api\", \"allows access to exposed functions and webroot files\")\n// @permission(\"debug\", \"allows access to low-level graph manipulation functions\")\n// @permission(\"files\", \"allows access to files under /files/* or webroot according to ACL\")\n\n// Predefined roles\n// @role(\"public\", \"public\")\n// @role(\"admin\", \"public\", \"admin\", \"api\", \"debug\", \"files\")\n// @role(\"user\", \"public\", \"api\", \"files\")\n\n// Query all permissions and roles\nvar all_permissions = Permission::all();\nvar all_roles = Role::all();\n```\n\n## Logging\n\n### Log Types\n\n```gcl\n// Log levels\nenum LogLevel {\n    error;\n    warn;\n    info;\n    perf;\n    trace;\n}\n\n// Log structure\ntype Log {\n    level: LogLevel;\n    time: time;\n    user_id: int?;\n    id: int?;\n    id2: int?;\n    src: function?;\n    data: any?;\n}\n\n// Usage data logging\ntype LogDataUsage {\n    read_bytes: int;\n    read_hits: int;\n    read_wasted: int;\n    write_bytes: int;\n    write_hits: int;\n    cache_bytes: int;\n    cache_hits: int;\n}\n```\n\n## Debugging\n\n### Debug\nCheckpoint snapshots for debugging suspended tasks.\n\n```gcl\n// Get all checkpoint IDs\nvar checkpoint_ids = Debug::all();\n\n// Get specific checkpoint\nvar debug_info = Debug::get(checkpoint_id);\nprintln(\"Frames: ${debug_info.frames.size()}\");\n\nfor (_, frame in debug_info.frames) {\n    println(\"${frame.module}::${frame.type}::${frame.function}\");\n    println(\"  ${frame.src}:${frame.line}:${frame.column}\");\n\n    for (_, var in frame.scope) {\n        println(\"  ${var.name} = ${var.value}\");\n    }\n}\n\n// Resume execution from checkpoint\nDebug::resume(checkpoint_id);\n```\n\n## API Documentation\n\n### OpenApi\nGenerate OpenAPI v3 specifications from exposed functions.\n\n```gcl\n// Get OpenAPI specification\nvar spec = OpenApi::v3();\nprintln(\"OpenAPI version: ${spec.openapi}\");\nprintln(\"API title: ${spec.info.title}\");\nprintln(\"API version: ${spec.info.version}\");\n\n// Iterate through paths\nif (spec.paths != null) {\n    for (path, item in spec.paths) {\n        println(\"Path: ${path}\");\n        if (item.post != null) {\n            println(\"  Description: ${item.post.description}\");\n        }\n    }\n}\n```\n\n## Model Context Protocol (MCP)\n\nGreyCat supports the Model Context Protocol for LLM integration. MCP enables AI assistants to interact with GreyCat tools and resources.\n\n### MCP Initialization\n\n```gcl\n// Initialize MCP session\nvar init_params = McpInitializeParams {\n    protocolVersion: \"2025-06-18\",\n    capabilities: McpClientCapabilities {\n        roots: McpClientRoots { listChanged: true }\n    },\n    clientInfo: McpImplementation {\n        name: \"my-client\",\n        version: \"1.0.0\"\n    }\n};\n\nvar result = mcp_initialize(init_params);\nprintln(\"Server protocol version: ${result.protocolVersion}\");\nprintln(\"Server: ${result.serverInfo.name} ${result.serverInfo.version}\");\n```\n\n### MCP Tools\n\n```gcl\n// List available tools\nvar tools_list = mcp_tools_list(null);\nfor (_, tool in tools_list.tools) {\n    println(\"Tool: ${tool.name}\");\n    println(\"  Title: ${tool.title}\");\n    println(\"  Description: ${tool.description}\");\n}\n\n// Call a tool\nvar call_params = McpToolsCallParams {\n    name: \"my_tool\",\n    arguments: { param1: \"value1\", param2: 42 }\n};\n\nvar call_result = mcp_tools_call(call_params);\nif (call_result.isError) {\n    error(\"Tool call failed\");\n} else {\n    for (_, content in call_result.content) {\n        if (content.type == McpContentType::text) {\n            var text_content = content as McpTextContent;\n            println(text_content.text);\n        }\n    }\n}\n```\n\n### MCP Content Types\n\nMCP supports various content types for tool results:\n\n```gcl\n// Text content\nvar text = McpTextContent {\n    type: McpContentType::text,\n    text: \"Hello from GreyCat!\"\n};\n\n// Image content (base64 encoded)\nvar image = McpImageContent {\n    type: McpContentType::image,\n    data: base64_image_data,\n    mimeType: \"image/png\"\n};\n\n// Audio content (base64 encoded)\nvar audio = McpAudioContent {\n    type: McpContentType::audio,\n    data: base64_audio_data,\n    mimeType: \"audio/mp3\"\n};\n\n// Resource content\nvar resource = McpResourceContent {\n    type: McpContentType::resource,\n    uri: \"file:///path/to/resource\",\n    description: \"Important data file\",\n    mimeType: \"application/json\",\n    size: 1024\n};\n```\n\n### MCP Annotations\n\n```gcl\n// Add metadata to content\nvar annotations = McpAnnotations {\n    audience: [McpRole::user, McpRole::assistant],\n    priority: McpPriority::MostImportant,\n    lastModified: \"2025-01-03T10:30:00Z\"\n};\n\nvar annotated_text = McpTextContent {\n    type: McpContentType::text,\n    text: \"Critical information\",\n    annotations: annotations\n};\n```\n\n## Supporting Types\n\n### PeriodicTask\nRepresents a scheduled periodic task.\n\n```gcl\ntype PeriodicTask {\n    function: function;          // Function to execute\n    periodicity: Periodicity;    // Schedule configuration\n    options: PeriodicOptions;    // Execution options\n    is_active: bool;            // Currently active?\n    next_execution: time;       // Next scheduled time\n    execution_count: int;       // Total executions\n}\n```\n\n### PeriodicOptions\nConfiguration for periodic task behavior.\n\n```gcl\ntype PeriodicOptions {\n    activated: bool?;        // Initially active (default: true)\n    start: time?;           // Lifecycle start time (default: now)\n    max_duration: duration?; // Max execution time (default: null = unlimited)\n}\n```\n\n### SecurityPolicy\nComplete security policy structure.\n\n```gcl\ntype SecurityPolicy {\n    entities: Array<SecurityEntity>?;\n    credentials: Map<String, UserCredential>?;\n    fields: SecurityFields?;\n    keys: Map<String, String>?;\n    keys_last_refresh: time?;\n}\n```\n\n### SecurityFields\nCustomizable security-related user fields.\n\n```gcl\n// Get current security fields configuration\nvar fields = SecurityFields::get();\n\n// Set security fields configuration\nSecurityFields::set(SecurityFields {\n    email: \"example@company.com\",\n    name: \"Company LDAP\",\n    first_name: \"givenName\",\n    last_name: \"sn\",\n    roles: roles_map,\n    groups: groups_map\n});\n```\n",
        "plugins/greycat/skills/greycat/references/std/util.md": "# [std](index.html) > util\n\n[Source](util.source.html)\n\nUtility module providing fundamental data structures and helper types for GreyCat applications.\n\n## Collections\n\n### Queue\nFIFO collection with optional capacity bounds. When capacity is reached, front elements are automatically dropped.\n\n```gcl\n// Create a bounded queue that keeps only the last 3 items\nvar queue = Queue<String> { capacity: 3 };\nqueue.push(\"first\");\nqueue.push(\"second\");\nqueue.push(\"third\");\nqueue.push(\"fourth\"); // \"first\" gets dropped automatically\n\nAssert::equals(queue.pop(), \"second\");\nAssert::equals(queue.front(), \"third\");\nAssert::equals(queue.front(), \"third\");\nAssert::equals(queue.back(), \"fourth\");\nAssert::equals(queue.back(), \"fourth\");\n```\n\n### Stack\nStandard LIFO collection for last-in-first-out operations.\n\n```gcl\n// Basic stack operations\nvar stack = Stack<int> {};\nstack.push(10);\nstack.push(20);\nstack.push(30);\n\nAssert::equals(stack.pop(), 30);\nAssert::equals(stack.last(), 20); // peek without removing\nAssert::equals(stack.first(), 10); // bottom element\n```\n\n### SlidingWindow\nFixed-size FIFO collection that maintains statistical aggregates (avg, std, median, min, max) over the most recent N values. Perfect for streaming analytics.\n\n```gcl\n// Moving average over last 3 values\nvar window = SlidingWindow<float> { span: 3 };\n\n// Add streaming data\nwindow.add(10.0);\nwindow.add(20.0);\nwindow.add(30.0);\n\nAssert::equalsd(window.avg()!!, 20.0, 0.001);\nAssert::equals(window.size(), 3);\n\n// Add another value, pushing out the first\nwindow.add(40.0);\nAssert::equalsd(window.avg()!!, 30.0, 0.001); // (20+30+40)/3\nAssert::equals(window.min(), 20.0);\nAssert::equals(window.max(), 40.0);\n```\n\n### TimeWindow\nTime-based sliding window that maintains values within a duration span, automatically expiring old entries. Includes statistical functions for time-series analysis.\n\n```gcl\n// Keep only values from the last 5 minutes\nvar time_window = TimeWindow<float> { span: 5min };\n\n// Add timestamped data\ntime_window.add(time::now(), temperature);\ntime_window.add(time::now() + 30s, next_temp);\n\n// Get statistics for recent data only\nvar recent_avg = time_window.avg();\nvar min_reading = time_window.min(); // returns Tuple<time, float>\n```\n\n## Statistics & Analysis\n\n### Gaussian\nLive statistical profile that tracks running mean, standard deviation, and distribution properties. Supports normalization, standardization, and probability calculations (PDF/CDF).\n\n```gcl\n// Build statistical profile incrementally\nvar profile = Gaussian<float> {};\n\n// Add data points\nprofile.add(10.0);\nprofile.add(20.0);\nprofile.add(30.0);\n\nAssert::equalsd(profile.avg()!!, 20.0, 0.001);\nAssert::equals(profile.min, 10.0);\nAssert::equals(profile.max, 30.0);\n\n// Normalization: (value-min)/(max-min)\nAssert::equalsd(profile.normalize(15.0)!!, 0.25, 0.001);\n\n// Standardization: (value-avg)/std\nvar standardized = profile.standardize(25.0);\nAssert::isTrue(standardized > 0.0); // above average\n```\n\n### Histogram\nBinned data distribution analyzer with configurable quantizers. Provides percentile calculations, ratio analysis, and comprehensive statistical summaries.\n\n```gcl\n// Create histogram with 20 uniform bins between 0-100\nvar quantizer = LinearQuantizer<float> { min: 0.0, max: 100.0, bins: 20 };\nvar histogram = Histogram<float> { quantizer: quantizer };\n\n// Add data points\nfor (_, score in test_scores) {\n    histogram.add(score);\n}\n\n// Analyze distribution\nvar median = histogram.percentile(0.5); // 50th percentile\nvar top_10_percent = histogram.percentile(0.9);\nvar below_passing = histogram.ratio_under(60.0); // fraction below 60\nvar stats = histogram.stats(); // comprehensive statistics\n```\n\n### GaussianProfile\nMulti-dimensional Gaussian statistics indexed by quantized keys for categorical analysis.\n\n```gcl\n// Profile statistics by category\nvar quantizer = LinearQuantizer<int> { min: 0, max: 100, bins: 10 };\nvar profile = GaussianProfile<int> { quantizer: quantizer, precision: FloatPrecision::p1000 };\n\n// Add data points with categories\nprofile.add(age_group, salary);\nprofile.add(age_group, another_salary);\n\n// Get statistics per category\nvar avg_salary_for_group = profile.avg(age_group);\nvar salary_std_for_group = profile.std(age_group);\n```\n\n## Quantizers\n\n### LinearQuantizer\nUniform binning with equal-width intervals\n\n```gcl\n// 10 equal bins from 0 to 100\nvar linear = LinearQuantizer<float> { min: 0.0, max: 100.0, bins: 10 };\n\nAssert::equals(linear.size(), 10);\nAssert::equals(linear.quantize(25.0), 2); // 25.0 falls in bin 2\nAssert::equals(linear.quantize(95.0), 9); // 95.0 falls in bin 9\n\n// Get bounds for bin 2\nvar bounds = linear.bounds(2);\nAssert::equals(bounds.min, 20.0);\nAssert::equals(bounds.max, 30.0);\nAssert::equals(bounds.center, 25.0);\n```\n\n### LogQuantizer\nLogarithmic binning for exponential data distributions\n\n```gcl\n// Logarithmic bins for data with exponential distribution\nvar log_quantizer = LogQuantizer<float> { min: 1.0, max: 1000.0, bins: 10 };\nvar bin = log_quantizer.quantize(50.0); // maps to appropriate log bin\n```\n\n### CustomQuantizer\nUser-defined bin boundaries for irregular distributions\n\n```gcl\n// Custom age groups: 0-18, 18-25, 25-40, 40-65, 65+\nvar age_quantizer = CustomQuantizer<int> {\n    min: 0,\n    max: 100,\n    step_starts: [0, 18, 25, 40, 65]\n};\nvar age_group = age_quantizer.quantize(32); // returns appropriate bin\n```\n\n### MultiQuantizer\nMulti-dimensional quantization for complex data structures\n\n```gcl\n// Quantize multi-dimensional data like [age, income, score]\nvar quantizers = Array<Quantizer<float>> {\n    LinearQuantizer<float> { min: 0.0, max: 100.0, bins: 5 },    // age groups\n    LogQuantizer<float> { min: 1000.0, max: 200000.0, bins: 8 }, // income brackets\n    LinearQuantizer<float> { min: 0.0, max: 100.0, bins: 10 }    // score ranges\n};\nvar multi = MultiQuantizer<float> { quantizers: quantizers };\nvar slot = multi.quantize([35.0, 45000.0, 87.5]); // single slot index\nvar vector = multi.slot_vector(slot); // [age_bin, income_bin, score_bin]\n```\n\n## Utilities\n\n### Random\nSeeded random number generator with uniform, normal, and Gaussian distributions. Supports various data types including geo coordinates.\n\n```gcl\n// Reproducible random numbers with fixed seed\nvar rng = Random { seed: 12345 };\n\n// Test uniform distribution bounds\nvar roll = rng.uniform(1, 7); // 1-6 inclusive\nAssert::isTrue(roll >= 1 && roll < 7);\n\nvar probability = rng.uniformf(0.0, 1.0);\nAssert::isTrue(probability >= 0.0 && probability < 1.0);\n\n// Normal distribution should center around mean\nvar samples = Array<float> {};\nrng.fill(samples, 1000, 50.0, 60.0);\nAssert::equals(samples.size(), 1000);\n```\n\n### Assert\nTesting utility with type-aware equality checks and boolean assertions.\n\n```gcl\n// Unit testing helpers\nAssert::equals(calculated_result, expected_value);\nAssert::equalsd(pi_approximation, 3.14159, 0.001); // float comparison with epsilon\nAssert::equalst(tensor_a, tensor_b, 0.01); // tensor comparison with epsilon\nAssert::isTrue(validation_passed);\nAssert::isNotNull(database_connection);\n```\n\n### ProgressTracker\nPerformance monitoring for long-running operations with speed and ETA calculations.\n\n```gcl\n// Track progress of batch processing\nvar tracker = ProgressTracker { start: time::now(), total: 1000 };\n\n// Simulate processing 300 items\ntracker.update(300);\n\nAssert::equalsd(tracker.progress!!, 0.3, 0.001); // 30% complete\nAssert::equals(tracker.counter, 300);\nAssert::isNotNull(tracker.speed);\nAssert::isNotNull(tracker.remaining);\n\n// Complete the task\ntracker.update(1000);\nAssert::equalsd(tracker.progress, 1.0, 0.001); // 100% complete\n```\n\n### Crypto\nCryptographic functions including SHA hashing, Base64 encoding, URL encoding, and PKCS1 signing.\n\n```gcl\n// Hash functions\nvar input = \"hello world\";\nvar sha1_result = Crypto::sha1hex(input);\nvar sha256_result = Crypto::sha256hex(input);\n\nAssert::isNotNull(sha1_result);\nAssert::isNotNull(sha256_result);\nAssert::isTrue(sha1_result.size() > 0);\nAssert::isTrue(sha256_result.size() > sha1_result.size());\n\n// Encoding/decoding round trip\nvar original = \"test string with spaces\";\nvar encoded = Crypto::base64_encode(original);\nvar decoded = Crypto::base64_decode(encoded);\nAssert::equals(decoded, original);\n\n// URL encoding round trip\nvar url_encoded = Crypto::url_encode(\"param with spaces & symbols\");\nvar url_decoded = Crypto::url_decode(url_encoded);\nAssert::equals(url_decoded, \"param with spaces & symbols\");\n```\n\n### Plot\nBasic plotting functionality for scatter plots from tabular data.\n\n```gcl\n// Simple temperature data over months\nvar data_table = Table {};\ndata_table.set_row(0, [\"Jan\", 1, -2, 8]);\ndata_table.set_row(1, [\"Feb\", 2, 1, 12]);\ndata_table.set_row(2, [\"Mar\", 3, 8, 18]);\ndata_table.set_row(3, [\"Apr\", 4, 15, 22]);\ndata_table.set_row(4, [\"May\", 5, 22, 28]);\ndata_table.set_row(5, [\"Jun\", 6, 28, 32]);\ndata_table.set_row(6, [\"Jul\", 7, 31, 35]);\ndata_table.set_row(7, [\"Aug\", 8, 29, 33]);\ndata_table.set_row(8, [\"Sep\", 9, 23, 28]);\ndata_table.set_row(9, [\"Oct\", 10, 16, 21]);\ndata_table.set_row(10, [\"Nov\", 11, 7, 14]);\ndata_table.set_row(11, [\"Dec\", 12, 2, 9]);\n\n// Plot month (x) vs min_temp and max_temp (y series)\n// Columns: [month_name, month_number, min_temp, max_temp]\nPlot::scatter_plot(data_table, 1, [2, 3], \"temperature_trends.png\");\n```\n",
        "plugins/greycat/skills/greycat/references/testing.md": "# Testing\n\nRun: `greycat test` ([cli.md](cli.md) for options)\n\n## Test Functions\n\n```gcl\n@test fn my_test_function() { Assert::isNull(null); }\n// Output: project::my_test_function ok (5us)\n//         tests success: 1, failed: 0, skipped: 0\n```\n\n## Multiple Tests\n\nExecute in definition order. Share module context (changes persist between tests, not saved to disk).\n\n## Setup & Teardown\n\nRun once per module:\n```gcl\nvar n: node<int?>;\nfn setup() { n.set(1); }  // Before tests\nfn teardown() { }         // After tests\n@test fn some_test() { Assert::equals(*n, 1); n.set(42); }\n@test fn following_test() { Assert::equals(*n, 42); }  // Sees prior change\n```\n\n## Test Files\n\n`*_test.gcl` excluded from `greycat build`:\n```\nsrc/model.gcl + src/model_test.gcl  # Unit tests with source\ntest/api_test.gcl                    # Integration tests\n```\n\n## Assert Methods\n\n`equals(a, b)`, `equalsd(a, b, epsilon)`, `equalst(a, b, epsilon)`, `isTrue(v)`, `isFalse(v)`, `isNull(v)`, `isNotNull(v)`\n\n## Exit Codes\n\n`greycat test; echo $?` → 0=success, non-zero=failure\n\n## Example\n\n```gcl\n@test fn test_country_service() {\n    var country = CountryService::create(\"Luxembourg\", \"LU\");\n    Assert::isNotNull(country); Assert::equals(country->name, \"Luxembourg\");\n    var found = CountryService::find(\"Luxembourg\");\n    Assert::isNotNull(found); Assert::equals(found->code, \"LU\");\n}\n@test fn test_fail() { throw \"Expected failure\"; }  // Reports as failed\n```\n",
        "plugins/greycat/skills/greycat/references/time.md": "# Time Handling\n\n## time\n\n64-bit signed integer representing microseconds since 01/01/1970 UTC.\n\n| Property | Value |\n|----------|-------|\n| Symbol | `time` |\n| Min | 21/12/-290308 |\n| Max | 10/01/294247 |\n| Precision | 1 microsecond |\n\n### Initialize Time\n\n```gcl\nvar t1 = time::now();                                    // Current system time\nvar t2 = 5_time;                                         // 5 μs after epoch\nvar t3 = -1000000_time;                                  // 1s before epoch\nvar t4 = time::new(23, DurationUnit::hours);            // 23 hours after epoch\nvar t5 = time::new(1684163705, DurationUnit::seconds);  // POSIX epoch\nvar t6 = time::parse(\"07/06/2023 10:57:32\", \"%d/%m/%Y %H:%M:%S\");\n```\n\n### Format Time\n\n```gcl\nprintln(t.format(\"%d/%m/%Y %H:%M:%S\", TimeZone::\"Europe/Luxembourg\"));\n```\n\n### Time Operations\n\n```gcl\nif (t5 > t4) { }           // Compare\nvar d = t1 - t2;           // Subtract → duration\nvar newT = t1 + 10_s;      // Add duration\n\n// Loop with time\nfor (var t = 0_time; t < 1000_time; t = t + 1_us) { }\n```\n\n## Date\n\nHuman-friendly representation for UI display. Resource-intensive to create.\n\n### Create Date\n\n```gcl\nvar date = t.toDate(null);                              // UTC\nvar dateLux = t.toDate(TimeZone::\"Europe/Luxembourg\");  // Specific TZ\n\nvar d1 = Date::from_time(time::now(), TimeZone::\"Europe/Luxembourg\");\nvar d2 = Date::parse(\"07/06/2023 10:57:32\", \"%d/%m/%Y %H:%M:%S\");\n```\n\n### Convert Back to Time\n\n```gcl\nvar t = date.to_time(null);  // Convert to UTC time\n\n// Handle invalid dates (DST gaps)\nvar t = date.to_nearest_time(tz);  // Finds nearest valid time\n```\n\n## Duration\n\nSpan between two time points. Internal unit: microseconds.\n\n```gcl\nvar d = t2 - t1;                                    // From time subtraction\nvar d1 = duration::new(5, DurationUnit::microseconds);\nvar d2 = duration::new(2, DurationUnit::days);\nvar d3 = duration::newf(1.5, DurationUnit::years);  // Float value\n\n// Shorthand literals (suffix notation)\nvar d4 = 1_us;      // 1 microsecond\nvar d5 = 500_ms;    // 500 milliseconds\nvar d6 = 5.6_s;     // 5.6 seconds\nvar d7 = 30_min;    // 30 minutes\nvar d8 = 7_hour;    // 7 hours\nvar d9 = 2_day;     // 2 days\n\n// ⚠️ Only these suffixes are valid:\n//    _us, _ms, _s, _min, _hour, _day\n// ❌ These do NOT work:\n//    _microsecond, _millisecond, _second, _minute (verbose forms not supported)\n//    _month, _year (ambiguous - use smaller denominations like 30_day or 365_day)\n```\n\n### Duration Operations\n\n```gcl\nif (d4 > d5) { }    // Compare\nvar sum = 10_s + 10_s;   // Add\nvar diff = 10_s - 5_s;   // Subtract\n\nprintln(d.to(DurationUnit::seconds));   // Integer seconds\nprintln(d.tof(DurationUnit::hours));    // Fractional hours\n```\n\n## DurationUnit\n\nFixed microsecond values:\n\n| Unit | Microseconds | Shorthand | Example |\n|------|--------------|-----------|---------|\n| microseconds | 1 | `_us` | `1_us`, `100_us` |\n| milliseconds | 1e3 | `_ms` | `500_ms`, `1_ms` |\n| seconds | 1e6 | `_s` | `30_s`, `1.5_s` |\n| minutes | 60e6 | `_min` | `5_min`, `30_min` |\n| hours | 3600e6 | `_hour` | `1_hour`, `24_hour` |\n| days | 86400e6 | `_day` | `1_day`, `7_day` |\n\n**Note**: Maximum duration is `_day`. For longer periods, use multiples (e.g., `30_day`, `365_day`) or CalendarUnit for calendar-aware operations.\n\n## CalendarUnit\n\nRespects Gregorian calendar and timezones:\n\n- microseconds, seconds, minutes, hours\n- days, months, years\n\n### Calendar Functions\n\n```gcl\n// Shift time by calendar units\nt.calendar_add(1, CalendarUnit::months, tz);  // Add 1 month\n\n// Floor to unit boundary\nt.calendar_floor(CalendarUnit::years, tz);    // Jan 1st 00:00:00\n\n// Ceil to unit boundary\nt.calendar_ceiling(CalendarUnit::years, tz);  // Dec 31st 23:59:59\n```\n\n## Date Format Specifiers\n\n| Specifier | Meaning | Example |\n|-----------|---------|---------|\n| %d | Day zero-padded (01-31) | 22 |\n| %D | Short MM/DD/YY | 07/30/09 |\n| %e | Day space-padded | 22 |\n| %H | Hour 24h (00-23) | 16 |\n| %I | Hour 12h (01-12) | 08 |\n| %m | Month (01-12) | 08 |\n| %M | Minute (00-59) | 52 |\n| %S | Second (00-61) | 06 |\n| %s | Unix epoch seconds | 1455803239 |\n| %y | Year 2-digit | 11 |\n| %Y | Year 4-digit | 2016 |\n\n```gcl\nvar d = Date::parse(\"31.01.2022-01:30:30\", \"%d.%m.%Y-%H:%M:%S\");\n```\n",
        "plugins/greycat/skills/greycat/references/useragent/useragent.md": "# User Agent Parser\n\nHTTP User Agent string parsing for browser and device detection in GreyCat.\n\n## Overview\n\nThe User Agent library provides parsing and analysis of HTTP User-Agent strings, enabling applications to identify browsers, operating systems, and devices from web traffic. Built on industry-standard user agent parsing, it extracts structured information from the complex User-Agent header sent by web browsers and applications.\n\nKey features include:\n- **Browser identification** including family, version (major, minor, patch)\n- **Operating system detection** with version information\n- **Device detection** including family, brand, and model\n- **Comprehensive version parsing** with granular version components\n- **Support for modern and legacy user agents** across desktop, mobile, and IoT devices\n\nThis library is ideal for web analytics, device-specific content delivery, compatibility detection, security analysis, and understanding user demographics in web applications.\n\n## Installation\n\nAdd the User Agent library to your GreyCat project:\n\n```gcl\n@library(\"useragent\", \"7.6.60-dev\")\n```\n\n## Quick Start\n\n### Parse a User Agent String\n\n```gcl\nvar uaString = \"Mozilla/5.0 (Linux; Android 4.4.2; QMV7A Build/KOT49H) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.135 Safari/537.36\";\n\nvar ua = UserAgent::parse(uaString);\n\nprint(\"Browser: ${ua.browserFamily} ${ua.browserMajor}.${ua.browserMinor}\");\nprint(\"OS: ${ua.osFamily} ${ua.osMajor}.${ua.osMinor}\");\nprint(\"Device: ${ua.deviceBrand} ${ua.deviceModel}\");\n```\n\n### Detect Mobile Devices\n\n```gcl\nfn isMobile(uaString: String): bool {\n  var ua = UserAgent::parse(uaString);\n\n  var mobileFamilies = [\"Android\", \"iOS\", \"Windows Phone\", \"BlackBerry\"];\n\n  return mobileFamilies.contains(ua.osFamily);\n}\n```\n\n## Types\n\n### UserAgent\n\nParsed user agent information with browser, OS, and device details.\n\n**Fields:**\n\n**Browser Information:**\n- `browserFamily: String?` - Browser name (e.g., \"Chrome\", \"Firefox\", \"Safari\")\n- `browserMajor: String?` - Browser major version (e.g., \"36\")\n- `browserMinor: String?` - Browser minor version (e.g., \"0\")\n- `browserPatch: String?` - Browser patch version (e.g., \"1985\")\n\n**Operating System Information:**\n- `osFamily: String?` - OS name (e.g., \"Android\", \"iOS\", \"Windows\", \"Mac OS X\")\n- `osMajor: String?` - OS major version (e.g., \"4\")\n- `osMinor: String?` - OS minor version (e.g., \"4\")\n- `osPatch: String?` - OS patch version (e.g., \"2\")\n- `osPatchMinor: String?` - OS minor patch version (e.g., \"4\")\n\n**Device Information:**\n- `deviceFamily: String?` - Device family (e.g., \"iPhone\", \"Samsung Galaxy\")\n- `deviceBrand: String?` - Device manufacturer (e.g., \"Apple\", \"Samsung\")\n- `deviceModel: String?` - Device model (e.g., \"11\", \"S21\")\n\n**Static Methods:**\n- `parse(userAgent: String): UserAgent` - Parse a user agent string\n\n**Note:** All fields are optional (`?`) as not all information may be present in every user agent string.\n\n**Example:**\n\n```gcl\nvar ua = UserAgent::parse(\"Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Mobile/15E148 Safari/604.1\");\n\nprint(ua.browserFamily);  // \"Mobile Safari\"\nprint(ua.browserMajor);   // \"14\"\nprint(ua.osFamily);       // \"iOS\"\nprint(ua.osMajor);        // \"14\"\nprint(ua.osMinor);        // \"6\"\nprint(ua.deviceFamily);   // \"iPhone\"\nprint(ua.deviceBrand);    // \"Apple\"\n```\n\n## Methods\n\n### parse()\n\nParses a User-Agent string into structured components.\n\n**Signature:** `static fn parse(userAgent: String): UserAgent`\n\n**Parameters:**\n- `userAgent: String` - The User-Agent string from HTTP headers\n\n**Returns:** `UserAgent` object with parsed information\n\n**Example:**\n\n```gcl\n// Desktop browser\nvar desktop = UserAgent::parse(\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\");\n\nprint(\"Browser: ${desktop.browserFamily} ${desktop.browserMajor}\");\n// \"Browser: Chrome 120\"\n\nprint(\"OS: ${desktop.osFamily} ${desktop.osMajor}.${desktop.osMinor}\");\n// \"OS: Windows 10.0\"\n\n// Mobile browser\nvar mobile = UserAgent::parse(\"Mozilla/5.0 (Linux; Android 13; SM-S918B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Mobile Safari/537.36\");\n\nprint(\"Device: ${mobile.deviceBrand} ${mobile.deviceModel}\");\n// \"Device: Samsung SM-S918B\"\n\n// Bot/Crawler\nvar bot = UserAgent::parse(\"Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)\");\n\nprint(\"Browser: ${bot.browserFamily}\");\n// \"Browser: Googlebot\"\n```\n\n## Common Use Cases\n\n### Browser-Specific Features\n\n```gcl\nfn supportsWebP(uaString: String): bool {\n  var ua = UserAgent::parse(uaString);\n\n  // WebP supported in Chrome 23+, Firefox 65+, Edge 18+\n  if (ua.browserFamily == \"Chrome\") {\n    var major = ua.browserMajor as int;\n    return major >= 23;\n  } else if (ua.browserFamily == \"Firefox\") {\n    var major = ua.browserMajor as int;\n    return major >= 65;\n  } else if (ua.browserFamily == \"Edge\") {\n    var major = ua.browserMajor as int;\n    return major >= 18;\n  }\n\n  return false;\n}\n\nfn serveImage(uaString: String, imageName: String): String {\n  if (supportsWebP(uaString)) {\n    return \"/images/${imageName}.webp\";\n  } else {\n    return \"/images/${imageName}.jpg\";\n  }\n}\n```\n\n### Analytics Dashboard\n\n```gcl\ntype VisitorStats {\n  total: int;\n  byBrowser: Map<String, int>;\n  byOS: Map<String, int>;\n  byDevice: Map<String, int>;\n}\n\nfn analyzeVisitors(userAgents: Array<String>): VisitorStats {\n  var stats = VisitorStats {\n    total: userAgents.size(),\n    byBrowser: Map<String, int>{},\n    byOS: Map<String, int>{},\n    byDevice: Map<String, int>{}\n  };\n\n  for (uaString in userAgents) {\n    var ua = UserAgent::parse(uaString);\n\n    // Count browsers\n    if (ua.browserFamily != null) {\n      var count = stats.byBrowser.get(ua.browserFamily);\n      stats.byBrowser.put(ua.browserFamily, count == null ? 1 : count + 1);\n    }\n\n    // Count operating systems\n    if (ua.osFamily != null) {\n      var count = stats.byOS.get(ua.osFamily);\n      stats.byOS.put(ua.osFamily, count == null ? 1 : count + 1);\n    }\n\n    // Count device brands\n    if (ua.deviceBrand != null) {\n      var count = stats.byDevice.get(ua.deviceBrand);\n      stats.byDevice.put(ua.deviceBrand, count == null ? 1 : count + 1);\n    }\n  }\n\n  return stats;\n}\n\n// Generate report\nvar stats = analyzeVisitors(visitorUserAgents);\nprint(\"=== Visitor Statistics ===\");\nprint(\"Total visitors: ${stats.total}\");\nprint(\"\\nTop Browsers:\");\nfor (entry in stats.byBrowser.entries()) {\n  var percentage = (entry.value as float / stats.total) * 100;\n  print(\"  ${entry.key}: ${entry.value} (${percentage}%)\");\n}\n```\n\n### Mobile vs Desktop Detection\n\n```gcl\ntype DeviceType {\n  mobile: bool;\n  tablet: bool;\n  desktop: bool;\n}\n\nfn detectDeviceType(uaString: String): DeviceType {\n  var ua = UserAgent::parse(uaString);\n\n  var mobileOS = [\"Android\", \"iOS\", \"Windows Phone\", \"BlackBerry\"];\n  var tabletDevices = [\"iPad\", \"Android\"]; // Simplified\n\n  var isMobile = false;\n  var isTablet = false;\n\n  if (ua.osFamily != null && mobileOS.contains(ua.osFamily)) {\n    isMobile = true;\n\n    // Check if it's actually a tablet\n    if (ua.deviceFamily != null) {\n      if (ua.deviceFamily.contains(\"iPad\") || ua.deviceFamily.contains(\"Tablet\")) {\n        isTablet = true;\n        isMobile = false;\n      }\n    }\n  }\n\n  return DeviceType {\n    mobile: isMobile,\n    tablet: isTablet,\n    desktop: !isMobile && !isTablet\n  };\n}\n\n// Serve appropriate content\nfn getTemplate(uaString: String): String {\n  var deviceType = detectDeviceType(uaString);\n\n  if (deviceType.mobile) {\n    return \"mobile-template.html\";\n  } else if (deviceType.tablet) {\n    return \"tablet-template.html\";\n  } else {\n    return \"desktop-template.html\";\n  }\n}\n```\n\n### Bot Detection\n\n```gcl\nfn isBot(uaString: String): bool {\n  var ua = UserAgent::parse(uaString);\n\n  if (ua.browserFamily == null) {\n    return false;\n  }\n\n  var botNames = [\n    \"Googlebot\",\n    \"Bingbot\",\n    \"Slurp\",        // Yahoo\n    \"DuckDuckBot\",\n    \"Baiduspider\",\n    \"YandexBot\",\n    \"facebookexternalhit\"\n  ];\n\n  for (botName in botNames) {\n    if (ua.browserFamily.contains(botName)) {\n      return true;\n    }\n  }\n\n  return false;\n}\n\nfn handleRequest(uaString: String) {\n  if (isBot(uaString)) {\n    // Serve static, SEO-optimized content\n    return serveBotContent();\n  } else {\n    // Serve interactive JavaScript application\n    return serveAppContent();\n  }\n}\n```\n\n### Browser Version Compatibility\n\n```gcl\ntype BrowserSupport {\n  supported: bool;\n  reason: String?;\n}\n\nfn checkBrowserSupport(uaString: String): BrowserSupport {\n  var ua = UserAgent::parse(uaString);\n\n  if (ua.browserFamily == null || ua.browserMajor == null) {\n    return BrowserSupport {\n      supported: false,\n      reason: \"Unable to detect browser\"\n    };\n  }\n\n  var major = ua.browserMajor as int;\n\n  // Define minimum versions\n  if (ua.browserFamily == \"Chrome\" && major < 90) {\n    return BrowserSupport {\n      supported: false,\n      reason: \"Chrome 90+ required\"\n    };\n  } else if (ua.browserFamily == \"Firefox\" && major < 88) {\n    return BrowserSupport {\n      supported: false,\n      reason: \"Firefox 88+ required\"\n    };\n  } else if (ua.browserFamily == \"Safari\" && major < 14) {\n    return BrowserSupport {\n      supported: false,\n      reason: \"Safari 14+ required\"\n    };\n  } else if (ua.browserFamily == \"IE\") {\n    return BrowserSupport {\n      supported: false,\n      reason: \"Internet Explorer is not supported\"\n    };\n  }\n\n  return BrowserSupport {\n    supported: true,\n    reason: null\n  };\n}\n\n// Usage in web application\nfn renderPage(uaString: String) {\n  var support = checkBrowserSupport(uaString);\n\n  if (!support.supported) {\n    return renderUpgradePage(support.reason);\n  }\n\n  return renderApplication();\n}\n```\n\n### Geographic and Localization\n\n```gcl\nfn getLocaleFromUA(uaString: String): String {\n  var ua = UserAgent::parse(uaString);\n\n  // Some user agents include locale information\n  // This is a simplified example\n  if (ua.osFamily == \"iOS\") {\n    return \"en-US\"; // Could parse from UA string\n  } else if (ua.osFamily == \"Android\") {\n    return \"en-US\"; // Could parse from UA string\n  }\n\n  return \"en-US\"; // Default\n}\n```\n\n### Security Analysis\n\n```gcl\ntype SecurityRisk {\n  level: String;\n  issues: Array<String>;\n}\n\nfn assessSecurityRisk(uaString: String): SecurityRisk {\n  var ua = UserAgent::parse(uaString);\n  var issues = Array<String>{};\n  var level = \"LOW\";\n\n  // Check for very old browsers\n  if (ua.browserFamily == \"IE\") {\n    issues.add(\"Internet Explorer detected (unsupported)\");\n    level = \"HIGH\";\n  }\n\n  if (ua.browserFamily == \"Chrome\" && ua.browserMajor != null) {\n    var major = ua.browserMajor as int;\n    if (major < 100) {\n      issues.add(\"Outdated Chrome version\");\n      level = \"MEDIUM\";\n    }\n  }\n\n  // Check for uncommon user agents (potential scrapers)\n  if (ua.browserFamily == null && ua.osFamily == null) {\n    issues.add(\"Incomplete user agent (potential bot)\");\n    level = \"MEDIUM\";\n  }\n\n  return SecurityRisk {\n    level: level,\n    issues: issues\n  };\n}\n```\n\n## Best Practices\n\n### Null Checks\n\nAlways check for null values since not all fields are populated:\n\n```gcl\n// Good: Null-safe\nvar ua = UserAgent::parse(uaString);\n\nif (ua.browserFamily != null) {\n  print(\"Browser: ${ua.browserFamily}\");\n} else {\n  print(\"Browser: Unknown\");\n}\n\n// Bad: May cause error\nvar ua = UserAgent::parse(uaString);\nprint(\"Browser: ${ua.browserFamily}\"); // Could be null!\n```\n\n### Version Comparison\n\nWhen comparing versions, handle null and convert to int:\n\n```gcl\nfn isModernChrome(ua: UserAgent): bool {\n  if (ua.browserFamily != \"Chrome\" || ua.browserMajor == null) {\n    return false;\n  }\n\n  var major = ua.browserMajor as int;\n  return major >= 100;\n}\n```\n\n### Caching Results\n\nParse once and reuse:\n\n```gcl\n// Good: Parse once\nvar ua = UserAgent::parse(uaString);\nvar isMobile = checkMobile(ua);\nvar browser = getBrowserName(ua);\nvar os = getOSName(ua);\n\n// Bad: Parse multiple times\nvar isMobile = checkMobile(UserAgent::parse(uaString));\nvar browser = getBrowserName(UserAgent::parse(uaString));\nvar os = getOSName(UserAgent::parse(uaString));\n```\n\n### Fallback Values\n\nProvide defaults for unknown values:\n\n```gcl\nfn getBrowserDisplay(ua: UserAgent): String {\n  var browser = ua.browserFamily != null ? ua.browserFamily : \"Unknown Browser\";\n  var version = ua.browserMajor != null ? ua.browserMajor : \"?\";\n\n  return \"${browser} ${version}\";\n}\n```\n\n### Feature Detection Over UA Sniffing\n\nWhen possible, use feature detection instead of user agent sniffing:\n\n```gcl\n// Better: Feature detection (client-side)\n// if (window.WebGLRenderingContext) { ... }\n\n// Acceptable: UA parsing for analytics or server-side decisions\nvar ua = UserAgent::parse(uaString);\nlogBrowserStats(ua);\n```\n\n### Gotchas\n\n- **All fields are optional**: Any field can be `null`\n- **Version strings**: Versions are strings, not numbers (cast when comparing)\n- **Browser aliases**: Same browser may have different names (e.g., \"Chrome Mobile\")\n- **Device detection limitations**: Not all devices are accurately detected\n- **User agent spoofing**: Clients can send fake user agents\n- **Empty strings vs null**: Some fields may be empty strings rather than null\n- **Case sensitivity**: Browser/OS names are case-sensitive\n- **Bot detection**: Simple bot detection is unreliable; use additional signals\n\n### Common User Agent Patterns\n\nDesktop browsers:\n```\nChrome (Windows): Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\nFirefox (Mac): Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:121.0) Gecko/20100101 Firefox/121.0\nSafari (Mac): Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15\n```\n\nMobile browsers:\n```\nChrome (Android): Mozilla/5.0 (Linux; Android 13) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.6099.43 Mobile Safari/537.36\nSafari (iOS): Mozilla/5.0 (iPhone; CPU iPhone OS 17_1_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Mobile/15E148 Safari/604.1\n```\n\nBots:\n```\nGooglebot: Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)\n```\n\n### Security Considerations\n\n- **Don't trust user agents**: They can be spoofed\n- **Combine with other signals**: IP address, behavior analysis\n- **Log suspicious patterns**: Unusual or malformed user agents\n- **Rate limiting**: Use UA parsing to help identify bots\n\n```gcl\nfn isLikelySuspicious(uaString: String): bool {\n  var ua = UserAgent::parse(uaString);\n\n  // Very short or missing critical fields\n  if (uaString.length() < 20) {\n    return true;\n  }\n\n  // No browser family detected\n  if (ua.browserFamily == null && ua.osFamily == null) {\n    return true;\n  }\n\n  return false;\n}\n```\n\n### Performance\n\nParsing is relatively fast, but consider caching for high-traffic applications:\n\n```gcl\nvar uaCache = Map<String, UserAgent>{};\n\nfn getCachedUserAgent(uaString: String): UserAgent {\n  var cached = uaCache.get(uaString);\n\n  if (cached == null) {\n    cached = UserAgent::parse(uaString);\n    uaCache.put(uaString, cached);\n  }\n\n  return cached;\n}\n```\n\n### Browser Family Values\n\nCommon values you might encounter:\n\n**Desktop Browsers:**\n- `\"Chrome\"`, `\"Firefox\"`, `\"Safari\"`, `\"Edge\"`, `\"Opera\"`, `\"IE\"`\n\n**Mobile Browsers:**\n- `\"Mobile Safari\"`, `\"Chrome Mobile\"`, `\"Firefox Mobile\"`, `\"Samsung Internet\"`\n\n**Bots/Crawlers:**\n- `\"Googlebot\"`, `\"Bingbot\"`, `\"Baiduspider\"`, `\"YandexBot\"`\n\n### OS Family Values\n\nCommon operating systems:\n- `\"Windows\"`, `\"Mac OS X\"`, `\"Linux\"`, `\"Ubuntu\"`\n- `\"Android\"`, `\"iOS\"`, `\"Chrome OS\"`\n- `\"Windows Phone\"`, `\"BlackBerry\"`\n\n### Device Brand Values\n\nCommon device manufacturers:\n- `\"Apple\"`, `\"Samsung\"`, `\"Huawei\"`, `\"Xiaomi\"`, `\"Google\"`, `\"LG\"`, `\"Sony\"`\n\n## Example: Complete Request Handler\n\n```gcl\ntype RequestInfo {\n  ua: UserAgent;\n  isMobile: bool;\n  isBot: bool;\n  browserSupported: bool;\n  deviceType: String;\n}\n\nfn analyzeRequest(uaString: String): RequestInfo {\n  var ua = UserAgent::parse(uaString);\n\n  // Determine device type\n  var deviceType = \"desktop\";\n  if (ua.osFamily != null) {\n    if ([\"Android\", \"iOS\"].contains(ua.osFamily)) {\n      deviceType = ua.deviceFamily != null && ua.deviceFamily.contains(\"iPad\") ? \"tablet\" : \"mobile\";\n    }\n  }\n\n  // Check if bot\n  var isBot = ua.browserFamily != null &&\n    (ua.browserFamily.contains(\"bot\") || ua.browserFamily.contains(\"Bot\"));\n\n  // Check browser support\n  var supported = true;\n  if (ua.browserFamily == \"IE\") {\n    supported = false;\n  } else if (ua.browserMajor != null) {\n    var major = ua.browserMajor as int;\n    if (ua.browserFamily == \"Chrome\" && major < 90) {\n      supported = false;\n    }\n  }\n\n  return RequestInfo {\n    ua: ua,\n    isMobile: deviceType == \"mobile\",\n    isBot: isBot,\n    browserSupported: supported,\n    deviceType: deviceType\n  };\n}\n\n// Use in request handling\nfn handleWebRequest(uaString: String) {\n  var info = analyzeRequest(uaString);\n\n  print(\"Device: ${info.deviceType}\");\n  print(\"Browser: ${info.ua.browserFamily} ${info.ua.browserMajor}\");\n  print(\"OS: ${info.ua.osFamily}\");\n\n  if (info.isBot) {\n    return serveBotContent();\n  } else if (!info.browserSupported) {\n    return serveUpgradePage();\n  } else if (info.isMobile) {\n    return serveMobilePage();\n  } else {\n    return serveDesktopPage();\n  }\n}\n```\n",
        "plugins/llamacpp/.claude-plugin/plugin.json": "{\n  \"name\": \"llamacpp\",\n  \"version\": \"1.7.6\",\n  \"description\": \"Complete llama.cpp C/C++ API reference (v b7572) covering 163 functions: model loading, inference, text generation, embeddings, chat, advanced sampling (XTC, DRY, infill), per-sequence state management, model type detection, and more. For GGUF models, local LLM inference, and C/C++ AI development.\",\n  \"author\": {\n    \"name\": \"Datathings\",\n    \"email\": \"contact@datathings.com\"\n  },\n  \"license\": \"MIT\",\n  \"repository\": \"https://github.com/datathings/marketplace\",\n  \"keywords\": [\"llm\", \"llama.cpp\", \"llamacpp\", \"inference\", \"c-api\", \"cpp\", \"c\", \"embeddings\", \"text-generation\", \"chat\", \"gguf\", \"local-ai\", \"model-inference\", \"quantization\", \"lora\", \"developer-tools\"],\n  \"skills\": \"./skills/\"\n}\n",
        "plugins/llamacpp/README.md": "# llama.cpp Skill\n\nA comprehensive skill for working with the llama.cpp C API. Provides complete API reference, working examples, and best practices for building LLM applications with llama.cpp.\n\n## Table of Contents\n\n- [What This Skill Provides](#what-this-skill-provides)\n- [When to Use](#when-to-use)\n- [Skill Triggers](#skill-triggers)\n- [Repository Structure](#repository-structure)\n- [Installation](#installation)\n- [Features](#features)\n- [Usage](#usage)\n- [Packaging the Skill](#packaging-the-skill)\n- [Development](#development)\n- [Contributing](#contributing)\n- [License](#license)\n- [Links](#links)\n- [Version History](#version-history)\n\n## What This Skill Provides\n\n- **Complete API Reference**: All non-deprecated llama.cpp C functions organized by category\n- **Working Examples**: Production-ready code for common tasks (text generation, chat, embeddings, batch processing, streaming)\n- **Workflow Guides**: Step-by-step examples for typical use cases\n- **Best Practices**: Patterns for efficient and correct API usage\n\n## When to Use\n\nUse this skill to:\n- Look up llama.cpp C API functions and their signatures\n- Generate code that uses the llama.cpp library\n- Implement text generation, embeddings extraction, or chat applications\n- Work with batches, sequences, KV cache management, LoRA adapters, or state persistence\n- Get answers about llama.cpp API usage\n\n## Skill Triggers\n\nThis skill automatically activates when you:\n\n### Direct llama.cpp References\n- Mention \"llama.cpp\" or \"llama-cpp\" in your question\n- Reference the ggerganov/llama.cpp repository\n- Ask about GGUF model files or formats\n- Use llama.cpp function names (e.g., `llama_model_load_from_file`)\n\n### LLM Development Tasks\n- Ask \"how do I use llama.cpp to...\"\n- Request code for local LLM inference\n- Need C/C++ code for AI model integration\n- Want to implement text generation, chat, or embeddings locally\n\n### Specific Features\n- Questions about model loading, tokenization, or inference\n- Batching, KV cache, or sequence management questions\n- LoRA adapter integration\n- State persistence or session management\n- Sampling strategies and configuration\n\n### Common Query Patterns\n- \"How do I load a GGUF model?\"\n- \"Write C code to generate text with llama.cpp\"\n- \"What function extracts embeddings?\"\n- \"How to implement chat with llama.cpp?\"\n- \"Troubleshooting llama.cpp inference issues\"\n\n**Note:** The skill uses keyword matching on queries and code context to determine when to activate.\n\n## Repository Structure\n\n```\nplugins/llamacpp/                  # Plugin directory\n├── README.md                      # This file - public documentation\n├── package.sh                     # Script to package skill into .skill file\n│\n├── llama.cpp/                     # Upstream llama.cpp (separate git repo)\n│   ├── .git/                      # llama.cpp's own git history\n│   ├── include/llama.h            # Main C API header (source of truth)\n│   ├── include/llama-cpp.h        # C++ wrapper header\n│   └── ...                        # Full llama.cpp source code\n│\n└── skills/llamacpp/               # Skill content directory\n    ├── SKILL.md                   # Main skill entry point\n    └── references/                # Detailed API documentation (split by category)\n        ├── api-core.md            # Initialization, parameters, model loading\n        ├── api-model-info.md      # Model properties and architecture\n        ├── api-context.md         # Context, memory (KV cache), state\n        ├── api-inference.md       # Batch, inference, tokenization, chat\n        ├── api-sampling.md        # Sampling strategies (XTC, DRY, etc.)\n        ├── api-advanced.md        # LoRA, performance, training\n        ├── api.md                 # Legacy complete API reference\n        └── workflows.md           # Working examples and patterns\n```\n\n**Important:** The `llama.cpp/` directory is a **separate git repository** cloned from https://github.com/ggml-org/llama.cpp. It is NOT a git submodule. This allows us to easily update to new llama.cpp versions and reference the source API headers.\n\n## Installation\n\n### Option 1: Install from Marketplace\n\nThe skill can be installed directly from [skillsmp.com](https://skillsmp.com).\n\n### Option 2: Manual Installation\n\n1. Clone the marketplace repository:\n   ```bash\n   git clone https://github.com/datathings/marketplace.git\n   cd marketplace/plugins/llamacpp\n   ```\n\n2. Package the skill:\n   ```bash\n   ./package.sh\n   ```\n\n3. Install the generated `llamacpp.skill` file in your skills directory.\n\n## Features\n\n### API Reference (references/api.md)\n\nComplete documentation of all non-deprecated llama.cpp functions organized into 15 categories:\n- Initialization & Backend\n- Parameter Helpers\n- Model Loading & Management\n- Model Properties & Metadata\n- Context Management\n- Memory (KV Cache) Management\n- State & Session Management\n- Batch Operations\n- Inference & Decoding\n- Vocabulary & Tokenization\n- Chat Templates\n- Sampling\n- LoRA Adapters\n- Performance & Utilities\n- Training\n\n### Workflow Examples (references/workflows.md)\n\nWorking examples demonstrating:\n- Basic text generation\n- Chat with system prompts\n- Embeddings extraction\n- Batch processing\n- Multiple sequence management\n- LoRA adapter usage\n- State save/load\n- Custom sampling strategies\n- Encoder-decoder models\n- Memory management patterns\n\n### Production Examples (references/examples.md)\n\nComplete, production-ready applications:\n- Simple text generation (minimal example)\n- Interactive chat application (conversation history)\n- Embeddings extraction (similarity computation)\n- Batch text processing (parallel sequences)\n- Streaming generation (real-time delivery)\n\n## Compatibility\n\nThis skill documents the llama.cpp C API as of **January 2026** (version b7709).\n\n### Version Support\n\n- **Recommended:** llama.cpp b3000 or newer\n- **API Coverage:** 173 non-deprecated functions across 16 categories\n- **Deprecated Functions:** 30 (excluded from documentation)\n- **New in b7709:** Gemma3n multimodal support with MobileNetV5 vision encoder, pooling type improvements\n- **New in b7681:** Direct I/O support, per-device memory margins, improved parameter fitting status reporting\n\n### Version Notes\n\nIf using older llama.cpp builds (<b3000), some function signatures may differ. Refer to the llama.cpp repository for version-specific documentation.\n\n### Keeping Updated\n\nTo update this skill for newer llama.cpp versions:\n\n1. **Update llama.cpp to latest tag:**\n   ```bash\n   cd llama.cpp\n   git fetch --tags\n   LATEST_TAG=$(git tag --sort=-version:refname | grep \"^b\" | head -n 1)\n   git checkout \"$LATEST_TAG\"\n   cd ..\n   ```\n\n2. **Analyze API changes** by reading `llama.cpp/include/llama.h`\n3. **Update skill documentation** in `llamacpp/references/` to match new API\n4. **Update version** in `llamacpp/SKILL.md` and `marketplace.json`\n5. **Re-package the skill** with `./package.sh`\n\nFor a systematic update process, use the `/update` command in Claude Code.\n\n## Usage\n\nOnce installed, this skill will automatically activate when you ask questions about llama.cpp or request code that uses the llama.cpp C API.\n\nExample queries:\n- \"How do I load a model in llama.cpp?\"\n- \"Write a C program to generate text using llama.cpp\"\n- \"What function do I use to extract embeddings?\"\n- \"Show me how to implement chat with llama.cpp\"\n- \"How do I manage the KV cache for multiple sequences?\"\n\n## Packaging the Skill\n\nTo create a distributable `.skill` file:\n\n```bash\n./package.sh\n```\n\nThis will create `llamacpp.skill` in the current directory. You can optionally specify an output directory:\n\n```bash\n./package.sh -o /path/to/output\n```\n\n## Development\n\n### Updating the Skill\n\n1. Modify files in the `llamacpp/` directory\n2. Re-package the skill: `./package.sh`\n3. Test with your AI assistant\n\n### File Organization\n\n- **SKILL.md**: Main skill file with overview and navigation. Keep this concise (< 300 lines)\n- **references/api.md**: Complete API function reference\n- **references/workflows.md**: Workflow examples showing how to accomplish tasks\n- **references/examples.md**: Production-ready complete applications\n\n## Contributing\n\nWhen contributing to this skill:\n\n1. Keep SKILL.md lean - detailed content goes in reference files\n2. Ensure all code examples compile and work correctly\n3. Update the API reference when llama.cpp adds new functions\n4. Include error handling in examples\n5. Follow the existing organization pattern\n\n## License\n\nThis skill documentation is provided as-is. Please refer to the llama.cpp project for the library's license.\n\n## Links\n\n- llama.cpp: https://github.com/ggerganov/llama.cpp\n- Datathings: https://datathings.com/\n- Skills Marketplace: https://skillsmp.com/\n- Repository: https://github.com/datathings/marketplace\n\n## Version History\n\n- **1.0.0**: Initial release with complete API reference and examples\n",
        "plugins/llamacpp/skills/llamacpp/SKILL.md": "---\nname: llamacpp\ndescription: \"Complete llama.cpp C/C++ API reference covering model loading, inference, text generation, embeddings, chat, tokenization, sampling, batching, KV cache, LoRA adapters, and state management. Triggers on: llama.cpp questions, LLM inference code, GGUF models, local AI/ML inference, C/C++ LLM integration, \\\"how do I use llama.cpp\\\", API function lookups, implementation questions, troubleshooting llama.cpp issues, and any llama-cpp or ggerganov/llama.cpp mentions.\"\n---\n\n# llama.cpp C API Guide\n\nComprehensive reference for the llama.cpp C API, documenting all non-deprecated functions and common usage patterns.\n\n## Overview\n\nllama.cpp is a C/C++ implementation for LLM inference with minimal dependencies and state-of-the-art performance. This skill provides:\n\n- **Complete API Reference**: All non-deprecated functions organized by category\n- **Common Workflows**: Working examples for typical use cases\n- **Best Practices**: Patterns for efficient and correct API usage\n\n## Quick Start\n\nSee **[references/workflows.md](references/workflows.md)** for complete working examples. Basic workflow:\n\n1. `llama_backend_init()` - Initialize backend\n2. `llama_model_load_from_file()` - Load model\n3. `llama_init_from_model()` - Create context\n4. `llama_tokenize()` - Convert text to tokens\n5. `llama_decode()` - Process tokens\n6. `llama_sampler_sample()` - Sample next token\n7. Cleanup in reverse order\n\n## When to Use This Skill\n\nUse this skill when:\n\n1. **API Lookup**: You need to find a specific function (e.g., \"How do I load a model?\", \"What function creates a context?\")\n2. **Code Generation**: You're writing C code that uses llama.cpp\n3. **Workflow Guidance**: You need to understand the steps for a task (e.g., text generation, embeddings, chat)\n4. **Advanced Features**: You're working with batches, sequences, LoRA adapters, state management, or custom sampling\n5. **Migration**: You're updating code from deprecated functions to current API\n\n## Core Concepts\n\n### Key Objects\n\n- **`llama_model`**: Loaded model weights and architecture\n- **`llama_context`**: Inference state (KV cache, compute buffers)\n- **`llama_batch`**: Input tokens and positions for processing\n- **`llama_sampler`**: Token sampling configuration\n- **`llama_vocab`**: Vocabulary and tokenizer\n- **`llama_memory_t`**: KV cache memory handle\n\n### Typical Flow\n\n1. **Initialize**: `llama_backend_init()`\n2. **Load Model**: `llama_model_load_from_file()`\n3. **Create Context**: `llama_init_from_model()`\n4. **Tokenize**: `llama_tokenize()`\n5. **Process**: `llama_encode()` or `llama_decode()`\n6. **Sample**: `llama_sampler_sample()`\n7. **Generate**: Repeat steps 5-6\n8. **Cleanup**: Free in reverse order\n\n## API Reference\n\nFor detailed API documentation, the complete API is split across 6 files for efficient targeted loading. Start with **[references/api-core.md](references/api-core.md)** which links to all other sections.\n\n**API Files:**\n\n- **[api-core.md](references/api-core.md)** (220 lines) - Initialization, parameters, model loading\n- **[api-model-info.md](references/api-model-info.md)** (193 lines) - Model properties, architecture detection **NEW**\n- **[api-context.md](references/api-context.md)** (412 lines) - Context, memory (KV cache), state management\n- **[api-inference.md](references/api-inference.md)** (417 lines) - Batch operations, inference, tokenization, chat\n- **[api-sampling.md](references/api-sampling.md)** (490 lines) - All 26+ sampling strategies (incl. adaptive-p) + backend sampling API\n- **[api-advanced.md](references/api-advanced.md)** (359 lines) - LoRA adapters, performance, training\n\n**Total:** 197 active functions (b7871) across 6 organized files\n\n### Quick Function Lookup\n\nMost common: `llama_backend_init()`, `llama_model_load_from_file()`, `llama_init_from_model()`, `llama_tokenize()`, `llama_decode()`, `llama_sampler_sample()`, `llama_vocab_is_eog()`, `llama_memory_clear()`\n\nSee **[references/api.md](references/api.md)** for all 197 function signatures and detailed usage.\n\n## Common Workflows\n\nSee **[references/workflows.md](references/workflows.md)** for 13 complete working examples: basic text generation, chat, embeddings, batch processing, multi-sequence, LoRA, state save/load, custom sampling (XTC/DRY), encoder-decoder models, model detection, and memory management patterns.\n\n\n## Best Practices\n\nSee **[references/workflows.md](references/workflows.md)** for detailed best practices. Key points:\n\n- Always use default parameter functions (`llama_model_default_params()`, etc.)\n- Check return values for errors\n- Free resources in reverse order of creation\n- Handle dynamic buffer sizes for tokenization\n- Query actual context size after creation (`llama_n_ctx()`)\n- Check for end-of-generation with `llama_vocab_is_eog()`\n\n## Common Patterns\n\nEnd-of-generation check (`llama_vocab_is_eog()`), logits retrieval (`llama_get_logits_ith()`), batch creation (`llama_batch_get_one()`), tokenization buffer handling. See **[references/workflows.md](references/workflows.md)** for complete code examples.\n\n## Troubleshooting\n\n### Common Issues\n\n**Model loading fails:**\n- Verify file path and GGUF format validity\n- Check available RAM/VRAM for model size\n- Reduce `n_gpu_layers` if GPU memory insufficient\n\n**Tokenization returns negative value:**\n- Buffer too small; reallocate with `-n` size and retry\n- See tokenization pattern in [Common Patterns](#common-patterns)\n\n**Decode/encode returns non-zero:**\n- Verify batch initialization (`llama_batch_get_one()` or `llama_batch_init()`)\n- Check context capacity (`llama_n_ctx()`)\n- Ensure positions within context window\n\n**Silent failures / no output:**\n- Check if `llama_vocab_is_eog()` immediately returns true\n- Verify sampler initialization\n- Enable logging: `llama_log_set()`\n\n**Performance issues:**\n- Increase `n_threads` for CPU\n- Set `n_gpu_layers` for GPU offloading\n- Use larger `n_batch` for prompts\n- See [Performance & Utilities](references/api.md#performance--utilities)\n\n**Sliding Window Attention (SWA) issues:**\n- If using Mistral-style models with SWA, set `ctx_params.swa_full = true` to access beyond attention window\n- Check: `llama_model_n_swa(model)` to detect SWA size and configuration needs\n- Symptoms: Token positions beyond window size causing decode errors\n\n**Per-sequence state errors:**\n- Ensure sequence ID matches when loading: `llama_state_seq_load_file(ctx, \"file\", dest_seq_id, ...)`\n- Verify token buffer is large enough for loaded tokens\n- Check sequence wasn't cleared or removed before loading state\n\n**Model type detection:**\n- Use `llama_model_has_encoder()` before assuming decoder-only architecture\n- For recurrent models (Mamba/RWKV), KV cache behavior differs from standard transformers\n- Encoder-decoder models require `llama_encode()` then `llama_decode()` workflow\n\nFor advanced issues: https://github.com/ggerganov/llama.cpp/discussions\n\n## Resources\n\n- **API Reference** (6 files, 2,086 lines total) - Complete API reference split by category for targeted loading:\n  - [api-core.md](references/api-core.md) - Initialization, parameters, model loading\n  - [api-model-info.md](references/api-model-info.md) - Model properties, architecture detection\n  - [api-context.md](references/api-context.md) - Context, memory, state management\n  - [api-inference.md](references/api-inference.md) - Batch, inference, tokenization, chat\n  - [api-sampling.md](references/api-sampling.md) - All 25+ sampling strategies + backend sampling API\n  - [api-advanced.md](references/api-advanced.md) - LoRA, performance, training\n- **[references/workflows.md](references/workflows.md)** (1,616 lines) - 15 complete working examples: basic workflows (text generation, chat, embeddings, batching, sequences), intermediate (LoRA, state, sampling, encoder-decoder, memory), advanced features (XTC/DRY, per-sequence state, model detection), and production applications (interactive chat, streaming).\n\n## Key Differences from Deprecated API\n\nIf you're updating old code:\n\n- Use `llama_model_load_from_file()` instead of `llama_load_model_from_file()`\n- Use `llama_model_free()` instead of `llama_free_model()`\n- Use `llama_init_from_model()` instead of `llama_new_context_with_model()`\n- Use `llama_vocab_*()` functions instead of `llama_token_*()`\n- Use `llama_state_*()` functions instead of deprecated state functions\n\nSee the API reference for complete mappings.\n",
        "plugins/llamacpp/skills/llamacpp/references/api-advanced.md": "# llama.cpp C API Reference - Advanced Features\n\nPart 6 of 6 | [Core](api-core.md) | [Model Info](api-model-info.md) | [Context](api-context.md) | [Inference](api-inference.md) | [Sampling](api-sampling.md) | **Advanced**\n\nThis file covers:\n- LoRA Adapters - Load and apply LoRA adapters, control vectors\n- Performance & Utilities - Performance measurement, logging, system info\n- Training - Fine-tuning and training functions\n- Important Constants - Key constants and enums\n- Key Data Structures - Core struct definitions\n\nFor complete API navigation, see [api-core.md](api-core.md).\n\n---\n\n## LoRA Adapters\n\n### llama_adapter_lora_init\n```c\nstruct llama_adapter_lora * llama_adapter_lora_init(\n    struct llama_model * model,\n    const char * path_lora);\n```\nLoad a LoRA adapter from file.\n\n**Important:**\n- The adapter is valid as long as the associated model is not freed\n- All adapters must be loaded before context creation\n\n### llama_adapter_lora_free [DEPRECATED]\n```c\nvoid llama_adapter_lora_free(struct llama_adapter_lora * adapter);\n```\n**DEPRECATED:** Adapters are now automatically freed together with the associated model. This function is kept for backwards compatibility but should not be used in new code.\n\n### llama_adapter_meta_val_str\n```c\nint32_t llama_adapter_meta_val_str(\n    const struct llama_adapter_lora * adapter,\n    const char * key,\n    char * buf,\n    size_t buf_size);\n```\nGet adapter metadata value as a string by key.\n\n### llama_adapter_meta_count\n```c\nint32_t llama_adapter_meta_count(const struct llama_adapter_lora * adapter);\n```\nGet the number of metadata key/value pairs.\n\n### llama_adapter_meta_key_by_index\n```c\nint32_t llama_adapter_meta_key_by_index(\n    const struct llama_adapter_lora * adapter,\n    int32_t i,\n    char * buf,\n    size_t buf_size);\n```\nGet metadata key name by index.\n\n### llama_adapter_meta_val_str_by_index\n```c\nint32_t llama_adapter_meta_val_str_by_index(\n    const struct llama_adapter_lora * adapter,\n    int32_t i,\n    char * buf,\n    size_t buf_size);\n```\nGet metadata value as a string by index.\n\n### llama_adapter_get_alora_n_invocation_tokens\n```c\nuint64_t llama_adapter_get_alora_n_invocation_tokens(\n    const struct llama_adapter_lora * adapter);\n```\nGet the number of invocation tokens if this is an A-LoRA adapter.\n\n### llama_adapter_get_alora_invocation_tokens\n```c\nconst llama_token * llama_adapter_get_alora_invocation_tokens(\n    const struct llama_adapter_lora * adapter);\n```\nGet the invocation tokens if this is an A-LoRA adapter.\n\n### llama_set_adapter_lora\n```c\nint32_t llama_set_adapter_lora(\n    struct llama_context * ctx,\n    struct llama_adapter_lora * adapter,\n    float scale);\n```\nAdd a loaded LoRA adapter to the context. Does not modify model weights.\n\n**Parameters:**\n- `ctx`: Context\n- `adapter`: LoRA adapter\n- `scale`: Scaling factor\n\n### llama_rm_adapter_lora\n```c\nint32_t llama_rm_adapter_lora(\n    struct llama_context * ctx,\n    struct llama_adapter_lora * adapter);\n```\nRemove a specific LoRA adapter from the context. Returns -1 if not present.\n\n### llama_clear_adapter_lora\n```c\nvoid llama_clear_adapter_lora(struct llama_context * ctx);\n```\nRemove all LoRA adapters from the context.\n\n### llama_apply_adapter_cvec\n```c\nint32_t llama_apply_adapter_cvec(\n    struct llama_context * ctx,\n    const float * data,\n    size_t len,\n    int32_t n_embd,\n    int32_t il_start,\n    int32_t il_end);\n```\nApply a loaded control vector to the context. If `data` is NULL, clear the currently loaded vector.\n\n**Parameters:**\n- `ctx`: Context\n- `data`: Control vector data (n_embd x n_layers buffer starting from layer 1), or NULL to clear\n- `len`: Length of data\n- `n_embd`: Size of a single layer's control\n- `il_start`: Start layer (inclusive)\n- `il_end`: End layer (inclusive)\n\n---\n\n## Performance & Utilities\n\n### System Information\n\n```c\nconst char * llama_print_system_info(void);\n```\nGet system information as a string.\n\n```c\nint64_t llama_time_us(void);\n```\nGet current time in microseconds.\n\n```c\nsize_t llama_max_devices(void);\n```\nGet the maximum number of devices.\n\n```c\nsize_t llama_max_parallel_sequences(void);\n```\nGet the maximum number of parallel sequences.\n\n```c\nsize_t llama_max_tensor_buft_overrides(void);\n```\nGet the maximum number of tensor buffer type overrides.\n\n```c\nbool llama_supports_mmap(void);\n```\nCheck if mmap is supported.\n\n```c\nbool llama_supports_mlock(void);\n```\nCheck if mlock is supported.\n\n```c\nbool llama_supports_gpu_offload(void);\n```\nCheck if GPU offload is supported.\n\n```c\nbool llama_supports_rpc(void);\n```\nCheck if RPC is supported.\n\n### Performance Measurement\n\n```c\nstruct llama_perf_context_data llama_perf_context(\n    const struct llama_context * ctx);\n```\nGet performance data for the context.\n\n```c\nvoid llama_perf_context_print(const struct llama_context * ctx);\n```\nPrint performance statistics for the context.\n\n```c\nvoid llama_perf_context_reset(struct llama_context * ctx);\n```\nReset performance counters for the context.\n\n```c\nstruct llama_perf_sampler_data llama_perf_sampler(\n    const struct llama_sampler * chain);\n```\nGet performance data for the sampler chain. **Note:** Only works with samplers constructed via `llama_sampler_chain_init()`.\n\n```c\nvoid llama_perf_sampler_print(const struct llama_sampler * chain);\n```\nPrint performance statistics for the sampler.\n\n```c\nvoid llama_perf_sampler_reset(struct llama_sampler * chain);\n```\nReset performance counters for the sampler.\n\n```c\nvoid llama_memory_breakdown_print(const struct llama_context * ctx);\n```\nPrint a breakdown of per-device memory use via `LLAMA_LOG`.\n\n### Logging\n\n```c\nvoid llama_log_get(ggml_log_callback * log_callback, void ** user_data);\n```\nGet the current log callback and user data.\n\n```c\nvoid llama_log_set(ggml_log_callback log_callback, void * user_data);\n```\nSet callback for all future logging events. If NULL, everything is output on stderr. **Note:** Logger state is global, so these functions are NOT thread-safe.\n\n### Model Split Utilities\n\n```c\nint32_t llama_split_path(\n    char * split_path,\n    size_t maxlen,\n    const char * path_prefix,\n    int32_t split_no,\n    int32_t split_count);\n```\nBuild a split GGUF file path for a chunk.\n\n**Example:**\n```c\nchar split_path[256];\nllama_split_path(split_path, 256, \"/models/ggml-model-q4_0\", 2, 4);\n// Result: \"/models/ggml-model-q4_0-00002-of-00004.gguf\"\n```\n\n```c\nint32_t llama_split_prefix(\n    char * split_prefix,\n    size_t maxlen,\n    const char * split_path,\n    int32_t split_no,\n    int32_t split_count);\n```\nExtract the path prefix from a split path if and only if split_no and split_count match.\n\n### Flash Attention\n\n```c\nconst char * llama_flash_attn_type_name(enum llama_flash_attn_type flash_attn_type);\n```\nGet the name of a flash attention type.\n\n---\n\n## Training\n\n### llama_opt_param_filter_all\n```c\nbool llama_opt_param_filter_all(\n    const struct ggml_tensor * tensor,\n    void * userdata);\n```\nParameter filter that always returns true (all tensors contain trainable parameters).\n\n### llama_opt_init\n```c\nvoid llama_opt_init(\n    struct llama_context * lctx,\n    struct llama_model * model,\n    struct llama_opt_params lopt_params);\n```\nInitialize optimization/training for a model.\n\n**Parameters:**\n- `lctx`: Context\n- `model`: Model to train\n- `lopt_params`: Optimization parameters\n\n### llama_opt_epoch\n```c\nvoid llama_opt_epoch(\n    struct llama_context * lctx,\n    ggml_opt_dataset_t dataset,\n    ggml_opt_result_t result_train,\n    ggml_opt_result_t result_eval,\n    int64_t idata_split,\n    ggml_opt_epoch_callback callback_train,\n    ggml_opt_epoch_callback callback_eval);\n```\nRun a training epoch.\n\n**Parameters:**\n- `lctx`: Context\n- `dataset`: Training dataset\n- `result_train`: Training results\n- `result_eval`: Evaluation results\n- `idata_split`: Data split index\n- `callback_train`: Training callback\n- `callback_eval`: Evaluation callback\n\n---\n\n## Important Constants\n\n```c\n#define LLAMA_DEFAULT_SEED 0xFFFFFFFF\n#define LLAMA_TOKEN_NULL -1\n```\n\n## Key Data Structures\n\n### llama_batch\nInput data for `llama_encode`/`llama_decode`:\n- `n_tokens`: Number of tokens in the batch\n- `token`: Token IDs (used when `embd` is NULL)\n- `embd`: Token embeddings (used when `token` is NULL)\n- `pos`: Token positions (NULL for automatic tracking)\n- `seq_id`: Sequence IDs for each token (NULL defaults to sequence 0)\n- `logits`: Whether to output logits for each token (NULL outputs last token only for generation)\n\n### llama_model_params\nModel loading parameters (get defaults via `llama_model_default_params()`):\n- `devices`: NULL-terminated list of devices for offloading\n- `n_gpu_layers`: Number of layers to store in VRAM\n- `split_mode`: How to split the model across GPUs\n- `vocab_only`: Only load vocabulary, no weights\n- `use_mmap`: Use mmap if possible\n- `use_direct_io`: Use direct I/O when supported (takes precedence over use_mmap)\n- `use_mlock`: Force system to keep model in RAM\n\n### llama_context_params\nContext parameters (get defaults via `llama_context_default_params()`):\n- `n_ctx`: Text context size (0 = from model)\n- `n_batch`: Logical maximum batch size\n- `n_ubatch`: Physical maximum batch size\n- `n_seq_max`: Max number of sequences\n- `n_threads`: Threads for generation\n- `n_threads_batch`: Threads for batch processing\n- `embeddings`: Extract embeddings\n- `rope_scaling_type`: RoPE scaling type\n- `pooling_type`: Pooling type\n- `attention_type`: Attention type\n- `flash_attn_type`: Flash attention configuration\n\n### llama_token_data / llama_token_data_array\nUsed for sampling:\n- `llama_token_data`: Contains token ID, logit, and probability\n- `llama_token_data_array`: Array of token data with selection index and sorted flag\n",
        "plugins/llamacpp/skills/llamacpp/references/api-context.md": "# llama.cpp C API Reference - Context & Memory\n\nPart 3 of 6 | [Core](api-core.md) | [Model Info](api-model-info.md) | **Context** | [Inference](api-inference.md) | [Sampling](api-sampling.md) | [Advanced](api-advanced.md)\n\nThis file covers:\n- Context Management - Create and manage inference contexts\n- Memory (KV Cache) Management - Manipulate KV cache sequences\n- State & Session Management - Save/load full and per-sequence states\n\nFor complete API navigation, see [api-core.md](api-core.md).\n\n---\n\n## Context Management\n\n### llama_init_from_model\n```c\nstruct llama_context * llama_init_from_model(\n    struct llama_model * model,\n    struct llama_context_params params);\n```\nCreate a new context from a loaded model. Returns NULL on failure.\n\n**Usage:**\n```c\nstruct llama_context_params params = llama_context_default_params();\nparams.n_ctx = 4096;\nparams.n_batch = 512;\nstruct llama_context * ctx = llama_init_from_model(model, params);\nif (!ctx) {\n    // Handle error\n}\n```\n\n**New parameters in b7631:**\n\n- `samplers` (`struct llama_sampler_seq_config *`) - [EXPERIMENTAL] Backend sampler chain configuration. Enables GPU-accelerated sampling as part of the computation graph. The caller must keep the sampler chains alive. Samplers must be sampler chains (use `llama_sampler_chain_init`). Default: NULL.\n\n- `n_samplers` (`size_t`) - Number of sampler configurations in the `samplers` array. Default: 0.\n\n**New parameters in b7572:**\n\n- `kv_unified` (bool) - Use unified KV cache buffer (experimental). Enables a more memory-efficient cache layout. Default: false.\n\n- `swa_full` (bool) - For models with Sliding Window Attention (SWA), allocate full context size instead of just the attention window. Set to true when you need to access tokens outside the SWA window. Check `llama_model_n_swa()` to detect if a model uses SWA. Default: false.\n\n**Example with Backend Sampling:**\n```c\n// Create sampler chain for backend sampling\nstruct llama_sampler * chain = llama_sampler_chain_init(\n    llama_sampler_chain_default_params());\nllama_sampler_chain_add(chain, llama_sampler_init_top_k(50));\nllama_sampler_chain_add(chain, llama_sampler_init_dist(42));\n\n// Configure backend sampling\nstruct llama_sampler_seq_config sampler_configs[] = {\n    { .seq_id = 0, .sampler = chain }\n};\n\nstruct llama_context_params params = llama_context_default_params();\nparams.samplers = sampler_configs;\nparams.n_samplers = 1;\n\nstruct llama_context * ctx = llama_init_from_model(model, params);\n```\n\n**Example with SWA:**\n```c\nstruct llama_context_params params = llama_context_default_params();\nparams.n_ctx = 32768;\n\n// Check if model uses Sliding Window Attention\nint32_t swa_size = llama_model_n_swa(model);\nif (swa_size > 0) {\n    printf(\"Model uses SWA with window: %d\\n\", swa_size);\n    params.swa_full = true;  // Enable full context access\n}\n\nstruct llama_context * ctx = llama_init_from_model(model, params);\n```\n\n### llama_free\n```c\nvoid llama_free(struct llama_context * ctx);\n```\nFree all allocated memory for a context. Always call this when done with a context.\n\n**Usage:**\n```c\nllama_free(ctx);\n```\n\n### llama_params_fit\n```c\nenum llama_params_fit_status llama_params_fit(\n    const char * path_model,\n    struct llama_model_params * mparams,\n    struct llama_context_params * cparams,\n    float * tensor_split,\n    struct llama_model_tensor_buft_override * tensor_buft_overrides,\n    size_t * margins,\n    uint32_t n_ctx_min,\n    enum ggml_log_level log_level);\n```\nFits model and context parameters to available device memory. Returns a status enum (SUCCESS, FAILURE, or ERROR). This function is NOT thread-safe. Only parameters matching defaults are modified, except context size which is always modified when set to 0.\n\n**Return Values:**\n- `LLAMA_PARAMS_FIT_STATUS_SUCCESS (0)`: Found allocations that are projected to fit\n- `LLAMA_PARAMS_FIT_STATUS_FAILURE (1)`: Could not find allocations that fit\n- `LLAMA_PARAMS_FIT_STATUS_ERROR (2)`: Hard error occurred (e.g., model not found)\n\n**Parameters:**\n- `path_model`: Path to model file\n- `mparams`: Writable model params (will be modified)\n- `cparams`: Writable context params (will be modified)\n- `tensor_split`: Writable buffer for tensor split (needs at least `llama_max_devices()` elements)\n- `tensor_buft_overrides`: Writable buffer for overrides (needs at least `llama_max_tensor_buft_overrides()` elements)\n- `margins`: Margins of memory to leave per device in bytes (array with `llama_max_devices()` elements)\n- `n_ctx_min`: Minimum context size to set when trying to reduce memory use\n- `log_level`: Minimum log level to print during fitting\n\n### llama_get_model\n```c\nconst struct llama_model * llama_get_model(const struct llama_context * ctx);\n```\nGet the model associated with a context.\n\n### llama_get_memory\n```c\nllama_memory_t llama_get_memory(const struct llama_context * ctx);\n```\nGet the memory handle for a context.\n\n### llama_pooling_type\n```c\nenum llama_pooling_type llama_pooling_type(const struct llama_context * ctx);\n```\nGet the pooling type used by the context.\n\n### llama_n_ctx\n```c\nuint32_t llama_n_ctx(const struct llama_context * ctx);\n```\nGet the actual context size. After creating a context, query this to get the actual value (may differ from requested).\n\n### llama_n_ctx_seq\n```c\nuint32_t llama_n_ctx_seq(const struct llama_context * ctx);\n```\nGet the context size for sequences.\n\n### llama_n_batch\n```c\nuint32_t llama_n_batch(const struct llama_context * ctx);\n```\nGet the logical maximum batch size.\n\n### llama_n_ubatch\n```c\nuint32_t llama_n_ubatch(const struct llama_context * ctx);\n```\nGet the physical maximum batch size.\n\n### llama_n_seq_max\n```c\nuint32_t llama_n_seq_max(const struct llama_context * ctx);\n```\nGet the maximum number of sequences.\n\n---\n\n## Memory (KV Cache) Management\n\nThe memory functions operate on the KV cache and allow for advanced sequence management.\n\n### llama_memory_clear\n```c\nvoid llama_memory_clear(llama_memory_t mem, bool data);\n```\nClear the memory contents.\n\n**Parameters:**\n- `mem`: Memory handle\n- `data`: If true, data buffers will also be cleared together with metadata\n\n### llama_memory_seq_rm\n```c\nbool llama_memory_seq_rm(\n    llama_memory_t mem,\n    llama_seq_id seq_id,\n    llama_pos p0,\n    llama_pos p1);\n```\nRemove all tokens that belong to the specified sequence and have positions in [p0, p1). Returns false if a partial sequence cannot be removed. Removing a whole sequence never fails.\n\n**Parameters:**\n- `mem`: Memory handle\n- `seq_id`: Sequence ID (< 0: match any sequence)\n- `p0`: Start position (< 0: [0, p1])\n- `p1`: End position (< 0: [p0, inf))\n\n### llama_memory_seq_cp\n```c\nvoid llama_memory_seq_cp(\n    llama_memory_t mem,\n    llama_seq_id seq_id_src,\n    llama_seq_id seq_id_dst,\n    llama_pos p0,\n    llama_pos p1);\n```\nCopy all tokens that belong to the specified sequence to another sequence.\n\n**Parameters:**\n- `mem`: Memory handle\n- `seq_id_src`: Source sequence ID\n- `seq_id_dst`: Destination sequence ID\n- `p0`: Start position (< 0: [0, p1])\n- `p1`: End position (< 0: [p0, inf))\n\n### llama_memory_seq_keep\n```c\nvoid llama_memory_seq_keep(llama_memory_t mem, llama_seq_id seq_id);\n```\nRemove all tokens that do not belong to the specified sequence.\n\n### llama_memory_seq_add\n```c\nvoid llama_memory_seq_add(\n    llama_memory_t mem,\n    llama_seq_id seq_id,\n    llama_pos p0,\n    llama_pos p1,\n    llama_pos delta);\n```\nAdd relative position \"delta\" to all tokens that belong to the specified sequence and have positions in [p0, p1).\n\n**Parameters:**\n- `mem`: Memory handle\n- `seq_id`: Sequence ID\n- `p0`: Start position (< 0: [0, p1])\n- `p1`: End position (< 0: [p0, inf))\n- `delta`: Position delta to add\n\n### llama_memory_seq_div\n```c\nvoid llama_memory_seq_div(\n    llama_memory_t mem,\n    llama_seq_id seq_id,\n    llama_pos p0,\n    llama_pos p1,\n    int d);\n```\nInteger division of the positions by factor of `d > 1`.\n\n**Parameters:**\n- `mem`: Memory handle\n- `seq_id`: Sequence ID\n- `p0`: Start position (< 0: [0, p1])\n- `p1`: End position (< 0: [p0, inf))\n- `d`: Divisor (must be > 1)\n\n### llama_memory_seq_pos_min\n```c\nllama_pos llama_memory_seq_pos_min(llama_memory_t mem, llama_seq_id seq_id);\n```\nGet the smallest position present in the memory for the specified sequence. Returns -1 if the sequence is empty. Typically non-zero only for SWA caches.\n\n### llama_memory_seq_pos_max\n```c\nllama_pos llama_memory_seq_pos_max(llama_memory_t mem, llama_seq_id seq_id);\n```\nGet the largest position present in the memory for the specified sequence. Returns -1 if the sequence is empty.\n\n### llama_memory_can_shift\n```c\nbool llama_memory_can_shift(llama_memory_t mem);\n```\nCheck if the memory supports shifting.\n\n---\n\n## State & Session Management\n\n### llama_state_get_size\n```c\nsize_t llama_state_get_size(struct llama_context * ctx);\n```\nGet the actual size in bytes of the state (logits, embedding, and memory). Only use when saving the state.\n\n### llama_state_get_data\n```c\nsize_t llama_state_get_data(\n    struct llama_context * ctx,\n    uint8_t * dst,\n    size_t size);\n```\nCopy the state to the specified destination address. Returns the number of bytes copied.\n\n**Parameters:**\n- `ctx`: Context\n- `dst`: Destination buffer (must have enough memory allocated)\n- `size`: Size of destination buffer\n\n### llama_state_set_data\n```c\nsize_t llama_state_set_data(\n    struct llama_context * ctx,\n    const uint8_t * src,\n    size_t size);\n```\nSet the state from the specified address. Returns the number of bytes read.\n\n### llama_state_load_file\n```c\nbool llama_state_load_file(\n    struct llama_context * ctx,\n    const char * path_session,\n    llama_token * tokens_out,\n    size_t n_token_capacity,\n    size_t * n_token_count_out);\n```\nLoad session from file.\n\n### llama_state_save_file\n```c\nbool llama_state_save_file(\n    struct llama_context * ctx,\n    const char * path_session,\n    const llama_token * tokens,\n    size_t n_token_count);\n```\nSave session to file.\n\n### llama_state_seq_get_size\n```c\nsize_t llama_state_seq_get_size(\n    struct llama_context * ctx,\n    llama_seq_id seq_id);\n```\nGet the exact size needed to copy the state of a single sequence.\n\n### llama_state_seq_get_data\n```c\nsize_t llama_state_seq_get_data(\n    struct llama_context * ctx,\n    uint8_t * dst,\n    size_t size,\n    llama_seq_id seq_id);\n```\nCopy the state of a single sequence into the specified buffer.\n\n### llama_state_seq_set_data\n```c\nsize_t llama_state_seq_set_data(\n    struct llama_context * ctx,\n    const uint8_t * src,\n    size_t size,\n    llama_seq_id dest_seq_id);\n```\nCopy sequence data into the specified sequence. Returns positive on success, zero on failure.\n\n### llama_state_seq_save_file\n```c\nsize_t llama_state_seq_save_file(\n    struct llama_context * ctx,\n    const char * filepath,\n    llama_seq_id seq_id,\n    const llama_token * tokens,\n    size_t n_token_count);\n```\nSave sequence state to file.\n\n### llama_state_seq_load_file\n```c\nsize_t llama_state_seq_load_file(\n    struct llama_context * ctx,\n    const char * filepath,\n    llama_seq_id dest_seq_id,\n    llama_token * tokens_out,\n    size_t n_token_capacity,\n    size_t * n_token_count_out);\n```\nLoad sequence state from file.\n\n### llama_state_seq_get_size_ext\n```c\nsize_t llama_state_seq_get_size_ext(\n    struct llama_context * ctx,\n    llama_seq_id seq_id,\n    llama_state_seq_flags flags);\n```\nGet size of sequence state with flags (e.g., `LLAMA_STATE_SEQ_FLAGS_PARTIAL_ONLY` for SWA/recurrent cache only).\n\n### llama_state_seq_get_data_ext\n```c\nsize_t llama_state_seq_get_data_ext(\n    struct llama_context * ctx,\n    uint8_t * dst,\n    size_t size,\n    llama_seq_id seq_id,\n    llama_state_seq_flags flags);\n```\nGet sequence state data with flags.\n\n### llama_state_seq_set_data_ext\n```c\nsize_t llama_state_seq_set_data_ext(\n    struct llama_context * ctx,\n    const uint8_t * src,\n    size_t size,\n    llama_seq_id dest_seq_id,\n    llama_state_seq_flags flags);\n```\nSet sequence state data with flags.\n\n---\n\n",
        "plugins/llamacpp/skills/llamacpp/references/api-core.md": "# llama.cpp C API Reference - Core Functions\n\nPart 1 of 6 | **Core** | [Model Info](api-model-info.md) | [Context](api-context.md) | [Inference](api-inference.md) | [Sampling](api-sampling.md) | [Advanced](api-advanced.md)\n\nThis is the core API reference covering initialization, parameters, and model loading. The complete API is split across 6 files for efficient targeted loading.\n\n**Other API sections:**\n- [api-model-info.md](api-model-info.md) - Model properties, architecture detection\n- [api-context.md](api-context.md) - Context, memory (KV cache), state management\n- [api-inference.md](api-inference.md) - Batch operations, inference, tokenization, chat templates\n- [api-sampling.md](api-sampling.md) - All 25+ sampling strategies\n- [api-advanced.md](api-advanced.md) - LoRA, performance, training, constants, structures\n\n## Table of Contents\n\n1. [Initialization & Backend](#initialization--backend)\n2. [Parameter Helpers](#parameter-helpers)\n3. [Model Loading & Management](#model-loading--management)\n4. [Model Properties & Metadata](#model-properties--metadata)\n5. [Context Management](#context-management)\n6. [Memory (KV Cache) Management](#memory-kv-cache-management)\n7. [State & Session Management](#state--session-management)\n8. [Batch Operations](#batch-operations)\n9. [Inference & Decoding](#inference--decoding)\n10. [Vocabulary & Tokenization](#vocabulary--tokenization)\n11. [Chat Templates](#chat-templates)\n12. [Sampling](#sampling)\n13. [LoRA Adapters](#lora-adapters)\n14. [Performance & Utilities](#performance--utilities)\n15. [Training](#training)\n\n## Quick Reference\n\n### Most Common Functions\n\n**Model & Context:**\n- `llama_backend_init()` - Initialize backend\n- `llama_model_load_from_file()` - Load GGUF model\n- `llama_init_from_model()` - Create inference context\n- `llama_model_free()`, `llama_free()` - Cleanup\n\n**Tokenization:**\n- `llama_tokenize()` - Text → tokens\n- `llama_detokenize()` - Tokens → text\n- `llama_token_to_piece()` - Single token → text\n\n**Inference:**\n- `llama_decode()` - Process token batch\n- `llama_get_logits_ith()` - Get token probabilities\n- `llama_get_embeddings_ith()` - Extract embeddings\n\n**Sampling:**\n- `llama_sampler_chain_init()` - Create sampler\n- `llama_sampler_sample()` - Sample next token\n- `llama_vocab_is_eog()` - Check for end-of-generation\n\n**Memory Management:**\n- `llama_memory_clear()` - Clear KV cache\n- `llama_memory_seq_rm()` - Remove sequence\n- `llama_memory_seq_cp()` - Copy sequence\n\nSee categories below for complete function listings.\n\n---\n\n## Initialization & Backend\n\n### llama_backend_init\n```c\nvoid llama_backend_init(void);\n```\nInitialize the llama + ggml backend. Call once at the start of the program.\n\n**Usage:**\n```c\nllama_backend_init();\n```\n\n### llama_backend_free\n```c\nvoid llama_backend_free(void);\n```\nFree backend resources. Call once at the end of the program. Currently only used for MPI.\n\n### llama_numa_init\n```c\nvoid llama_numa_init(enum ggml_numa_strategy numa);\n```\nOptional: Initialize NUMA optimizations.\n\n**Parameters:**\n- `numa`: NUMA strategy to use (from ggml)\n\n### llama_attach_threadpool\n```c\nvoid llama_attach_threadpool(\n    struct llama_context * ctx,\n    ggml_threadpool_t threadpool,\n    ggml_threadpool_t threadpool_batch);\n```\nOptional: Attach a custom threadpool. An auto threadpool is created in ggml if not passed explicitly.\n\n**Parameters:**\n- `ctx`: Context to attach threadpool to\n- `threadpool`: Threadpool for single-token generation\n- `threadpool_batch`: Threadpool for batch processing\n\n### llama_detach_threadpool\n```c\nvoid llama_detach_threadpool(struct llama_context * ctx);\n```\nDetach threadpool from context.\n\n---\n\n## Parameter Helpers\n\n### llama_model_default_params\n```c\nstruct llama_model_params llama_model_default_params(void);\n```\nGet default model parameters. Always use this to initialize `llama_model_params` before modifying specific fields.\n\n**Usage:**\n```c\nstruct llama_model_params params = llama_model_default_params();\nparams.n_gpu_layers = 32;  // Override specific fields\n```\n\n### llama_context_default_params\n```c\nstruct llama_context_params llama_context_default_params(void);\n```\nGet default context parameters. Always use this to initialize `llama_context_params`.\n\n**Usage:**\n```c\nstruct llama_context_params params = llama_context_default_params();\nparams.n_ctx = 4096;\nparams.n_batch = 512;\n```\n\n### llama_sampler_chain_default_params\n```c\nstruct llama_sampler_chain_params llama_sampler_chain_default_params(void);\n```\nGet default sampler chain parameters.\n\n### llama_model_quantize_default_params\n```c\nstruct llama_model_quantize_params llama_model_quantize_default_params(void);\n```\nGet default quantization parameters.\n\n---\n\n## Model Loading & Management\n\n### llama_model_load_from_file\n```c\nstruct llama_model * llama_model_load_from_file(\n    const char * path_model,\n    struct llama_model_params params);\n```\nLoad a model from a file. If the file is split into multiple parts, the file name must follow this pattern: `<name>-%05d-of-%05d.gguf`. Returns NULL on failure.\n\n**Parameters:**\n- `path_model`: Path to the GGUF model file\n- `params`: Model loading parameters (get from `llama_model_default_params()`)\n\n**Usage:**\n```c\nstruct llama_model_params params = llama_model_default_params();\nparams.n_gpu_layers = 32;\nstruct llama_model * model = llama_model_load_from_file(\"model.gguf\", params);\nif (!model) {\n    // Handle error\n}\n```\n\n### llama_model_load_from_splits\n```c\nstruct llama_model * llama_model_load_from_splits(\n    const char ** paths,\n    size_t n_paths,\n    struct llama_model_params params);\n```\nLoad a model from multiple split files (supports custom naming schemes). The paths must be in the correct order.\n\n**Parameters:**\n- `paths`: Array of paths to split files\n- `n_paths`: Number of split files\n- `params`: Model loading parameters\n\n### llama_model_save_to_file\n```c\nvoid llama_model_save_to_file(\n    const struct llama_model * model,\n    const char * path_model);\n```\nSave a model to a file.\n\n### llama_model_free\n```c\nvoid llama_model_free(struct llama_model * model);\n```\nFree a loaded model. Always call this when done with a model.\n\n**Usage:**\n```c\nllama_model_free(model);\n```\n\n### llama_model_quantize\n```c\nuint32_t llama_model_quantize(\n    const char * fname_inp,\n    const char * fname_out,\n    const llama_model_quantize_params * params);\n```\nQuantize a model. Returns 0 on success.\n\n**Parameters:**\n- `fname_inp`: Input model file path\n- `fname_out`: Output model file path\n- `params`: Quantization parameters\n\n---\n\n",
        "plugins/llamacpp/skills/llamacpp/references/api-inference.md": "# llama.cpp C API Reference - Inference & Tokenization\n\nPart 4 of 6 | [Core](api-core.md) | [Model Info](api-model-info.md) | [Context](api-context.md) | **Inference** | [Sampling](api-sampling.md) | [Advanced](api-advanced.md)\n\nThis file covers:\n- Batch Operations - Create and manage token batches\n- Inference & Decoding - Run inference, get logits and embeddings\n- Vocabulary & Tokenization - Tokenize text, convert tokens\n- Chat Templates - Format chat conversations\n\nFor complete API navigation, see [api-core.md](api-core.md).\n\n---\n\n## Batch Operations\n\n### llama_batch_get_one\n```c\nstruct llama_batch llama_batch_get_one(\n    llama_token * tokens,\n    int32_t n_tokens);\n```\nReturn batch for single sequence of tokens. The sequence ID will be fixed to 0. The position of tokens will be tracked automatically. This is a helper function to facilitate transition to the new batch API - avoid using it for new code.\n\n**Usage:**\n```c\nllama_token tokens[] = {1, 2, 3, 4, 5};\nstruct llama_batch batch = llama_batch_get_one(tokens, 5);\nllama_decode(ctx, batch);\n```\n\n### llama_batch_init\n```c\nstruct llama_batch llama_batch_init(\n    int32_t n_tokens,\n    int32_t embd,\n    int32_t n_seq_max);\n```\nAllocate a batch of tokens on the heap. Must be freed with `llama_batch_free()`.\n\n**Parameters:**\n- `n_tokens`: Maximum number of tokens\n- `embd`: If != 0, allocate `llama_batch.embd` with size `n_tokens * embd * sizeof(float)`. Otherwise, allocate `llama_batch.token` to store `n_tokens` token IDs\n- `n_seq_max`: Maximum number of sequences each token can be assigned to\n\n**Usage:**\n```c\nstruct llama_batch batch = llama_batch_init(512, 0, 1);\n// Use batch...\nllama_batch_free(batch);\n```\n\n### llama_batch_free\n```c\nvoid llama_batch_free(struct llama_batch batch);\n```\nFree a batch allocated with `llama_batch_init()`.\n\n---\n\n## Inference & Decoding\n\n### llama_encode\n```c\nint32_t llama_encode(\n    struct llama_context * ctx,\n    struct llama_batch batch);\n```\nProcess a batch of tokens without using KV cache. For encoder-decoder models, processes the batch using the encoder and stores the output internally for later use by the decoder's cross-attention layers.\n\n**Returns:**\n- `0`: Success\n- `< 0`: Error (memory state is restored)\n\n### llama_decode\n```c\nint32_t llama_decode(\n    struct llama_context * ctx,\n    struct llama_batch batch);\n```\nProcess a batch of tokens. Requires the context to have memory. For encoder-decoder models, processes using the decoder.\n\n**Returns:**\n- `0`: Success\n- `1`: Could not find a KV slot (try reducing batch size or increase context)\n- `2`: Aborted (processed ubatches remain in memory)\n- `-1`: Invalid input batch\n- `< -1`: Fatal error (processed ubatches remain in memory)\n\n**Usage:**\n```c\nstruct llama_batch batch = llama_batch_get_one(tokens, n_tokens);\nint ret = llama_decode(ctx, batch);\nif (ret != 0) {\n    // Handle error\n}\n```\n\n### llama_set_n_threads\n```c\nvoid llama_set_n_threads(\n    struct llama_context * ctx,\n    int32_t n_threads,\n    int32_t n_threads_batch);\n```\nSet the number of threads used for decoding.\n\n**Parameters:**\n- `ctx`: Context\n- `n_threads`: Number of threads for generation (single token)\n- `n_threads_batch`: Number of threads for batch processing (multiple tokens)\n\n### llama_n_threads\n```c\nint32_t llama_n_threads(struct llama_context * ctx);\n```\nGet the number of threads used for generation of a single token.\n\n### llama_n_threads_batch\n```c\nint32_t llama_n_threads_batch(struct llama_context * ctx);\n```\nGet the number of threads used for batch processing.\n\n### llama_set_embeddings\n```c\nvoid llama_set_embeddings(struct llama_context * ctx, bool embeddings);\n```\nSet whether the context outputs embeddings or not.\n\n### llama_set_causal_attn\n```c\nvoid llama_set_causal_attn(struct llama_context * ctx, bool causal_attn);\n```\nSet whether to use causal attention or not. If set to true, the model will only attend to past tokens.\n\n### llama_set_warmup\n```c\nvoid llama_set_warmup(struct llama_context * ctx, bool warmup);\n```\nSet whether the model is in warmup mode. If true, all model tensors are activated during `llama_decode()` to load and cache their weights.\n\n### llama_set_abort_callback\n```c\nvoid llama_set_abort_callback(\n    struct llama_context * ctx,\n    ggml_abort_callback abort_callback,\n    void * abort_callback_data);\n```\nSet abort callback. If it returns true, execution will be aborted (currently only works with CPU execution).\n\n### llama_synchronize\n```c\nvoid llama_synchronize(struct llama_context * ctx);\n```\nWait until all computations are finished. Automatically done when obtaining results, not usually necessary to call explicitly.\n\n### llama_get_logits\n```c\nfloat * llama_get_logits(struct llama_context * ctx);\n```\nGet token logits from the last `llama_decode()` call. Logits for which `llama_batch.logits[i] != 0` are stored contiguously.\n\n**Returns:** Pointer to logits array. Shape: `[n_outputs, n_vocab]`\n\n### llama_get_logits_ith\n```c\nfloat * llama_get_logits_ith(struct llama_context * ctx, int32_t i);\n```\nGet logits for the i-th token. Negative indices access logits in reverse order (-1 is the last token). Returns NULL for invalid IDs.\n\n**Usage:**\n```c\nfloat * logits = llama_get_logits_ith(ctx, -1);  // Get logits for last token\n```\n\n### llama_get_embeddings\n```c\nfloat * llama_get_embeddings(struct llama_context * ctx);\n```\nGet all output token embeddings. Returns NULL when `pooling_type == LLAMA_POOLING_TYPE_NONE` with generative models.\n\n**Returns:** Pointer to embeddings array. Shape: `[n_outputs * n_embd]`\n\n### llama_get_embeddings_ith\n```c\nfloat * llama_get_embeddings_ith(struct llama_context * ctx, int32_t i);\n```\nGet embeddings for the i-th token. Negative indices can be used (-1 is last). Returns NULL for invalid IDs.\n\n**Returns:** Shape: `[n_embd]`\n\n### llama_get_embeddings_seq\n```c\nfloat * llama_get_embeddings_seq(\n    struct llama_context * ctx,\n    llama_seq_id seq_id);\n```\nGet embeddings for a sequence ID. Returns NULL if `pooling_type` is `LLAMA_POOLING_TYPE_NONE`. For `LLAMA_POOLING_TYPE_RANK`, returns `float[n_cls_out]` with rank(s).\n\n**Returns:** Shape: `[n_embd]` or `[n_cls_out]` for ranking models\n\n---\n\n## Vocabulary & Tokenization\n\n### llama_vocab_type\n```c\nenum llama_vocab_type llama_vocab_type(const struct llama_vocab * vocab);\n```\nGet the vocabulary type (SPM, BPE, WPM, UGM, RWKV, PLAMO2).\n\n### llama_vocab_n_tokens\n```c\nint32_t llama_vocab_n_tokens(const struct llama_vocab * vocab);\n```\nGet the number of tokens in the vocabulary.\n\n### llama_vocab_get_text\n```c\nconst char * llama_vocab_get_text(\n    const struct llama_vocab * vocab,\n    llama_token token);\n```\nGet the text representation of a token.\n\n### llama_vocab_get_score\n```c\nfloat llama_vocab_get_score(\n    const struct llama_vocab * vocab,\n    llama_token token);\n```\nGet the score of a token.\n\n### llama_vocab_get_attr\n```c\nenum llama_token_attr llama_vocab_get_attr(\n    const struct llama_vocab * vocab,\n    llama_token token);\n```\nGet the attributes of a token (bitfield of `llama_token_attr`).\n\n### llama_vocab_is_eog\n```c\nbool llama_vocab_is_eog(\n    const struct llama_vocab * vocab,\n    llama_token token);\n```\nCheck if the token is an end-of-generation token (EOS, EOT, etc.).\n\n### llama_vocab_is_control\n```c\nbool llama_vocab_is_control(\n    const struct llama_vocab * vocab,\n    llama_token token);\n```\nCheck if the token is a control token or a renderable token.\n\n### Special Token Functions\n\nGet special token IDs:\n\n```c\nllama_token llama_vocab_bos(const struct llama_vocab * vocab);   // beginning-of-sentence\nllama_token llama_vocab_eos(const struct llama_vocab * vocab);   // end-of-sentence\nllama_token llama_vocab_eot(const struct llama_vocab * vocab);   // end-of-turn\nllama_token llama_vocab_sep(const struct llama_vocab * vocab);   // sentence separator\nllama_token llama_vocab_nl(const struct llama_vocab * vocab);    // next-line\nllama_token llama_vocab_pad(const struct llama_vocab * vocab);   // padding\nllama_token llama_vocab_mask(const struct llama_vocab * vocab);  // mask\n```\n\nCheck if special tokens should be added:\n\n```c\nbool llama_vocab_get_add_bos(const struct llama_vocab * vocab);\nbool llama_vocab_get_add_eos(const struct llama_vocab * vocab);\nbool llama_vocab_get_add_sep(const struct llama_vocab * vocab);\n```\n\nFill-in-the-middle tokens:\n\n```c\nllama_token llama_vocab_fim_pre(const struct llama_vocab * vocab);\nllama_token llama_vocab_fim_suf(const struct llama_vocab * vocab);\nllama_token llama_vocab_fim_mid(const struct llama_vocab * vocab);\nllama_token llama_vocab_fim_pad(const struct llama_vocab * vocab);\nllama_token llama_vocab_fim_rep(const struct llama_vocab * vocab);\nllama_token llama_vocab_fim_sep(const struct llama_vocab * vocab);\n```\n\n### llama_tokenize\n```c\nint32_t llama_tokenize(\n    const struct llama_vocab * vocab,\n    const char * text,\n    int32_t text_len,\n    llama_token * tokens,\n    int32_t n_tokens_max,\n    bool add_special,\n    bool parse_special);\n```\nConvert text into tokens.\n\n**Parameters:**\n- `vocab`: Vocabulary\n- `text`: Text to tokenize\n- `text_len`: Length of text\n- `tokens`: Output buffer (must be large enough)\n- `n_tokens_max`: Maximum number of tokens\n- `add_special`: Allow adding BOS and EOS tokens if model is configured to do so\n- `parse_special`: Allow tokenizing special/control tokens (otherwise treated as plaintext)\n\n**Returns:**\n- Positive: Number of tokens (no more than `n_tokens_max`)\n- Negative: Number of tokens that would have been returned (buffer too small)\n- `INT32_MIN`: Overflow\n\n**Usage:**\n```c\nconst char * text = \"Hello, world!\";\nllama_token tokens[128];\nint n = llama_tokenize(vocab, text, strlen(text), tokens, 128, true, false);\nif (n < 0) {\n    // Buffer too small, need -n tokens\n}\n```\n\n### llama_token_to_piece\n```c\nint32_t llama_token_to_piece(\n    const struct llama_vocab * vocab,\n    llama_token token,\n    char * buf,\n    int32_t length,\n    int32_t lstrip,\n    bool special);\n```\nConvert a token ID to text. Does not write null terminator.\n\n**Parameters:**\n- `vocab`: Vocabulary\n- `token`: Token ID\n- `buf`: Output buffer\n- `length`: Buffer length\n- `lstrip`: Number of leading spaces to skip (useful when encoding/decoding multiple tokens)\n- `special`: If true, special tokens are rendered\n\n### llama_detokenize\n```c\nint32_t llama_detokenize(\n    const struct llama_vocab * vocab,\n    const llama_token * tokens,\n    int32_t n_tokens,\n    char * text,\n    int32_t text_len_max,\n    bool remove_special,\n    bool unparse_special);\n```\nConvert tokens back into text.\n\n**Parameters:**\n- `vocab`: Vocabulary\n- `tokens`: Array of tokens\n- `n_tokens`: Number of tokens\n- `text`: Output buffer\n- `text_len_max`: Maximum text length\n- `remove_special`: Remove BOS and EOS tokens if model is configured to do so\n- `unparse_special`: If true, special tokens are rendered\n\n**Returns:**\n- Positive: Number of chars/bytes (no more than `text_len_max`)\n- Negative: Number of chars/bytes that would have been returned\n\n---\n\n## Chat Templates\n\n### llama_chat_apply_template\n```c\nint32_t llama_chat_apply_template(\n    const char * tmpl,\n    const struct llama_chat_message * chat,\n    size_t n_msg,\n    bool add_ass,\n    char * buf,\n    int32_t length);\n```\nApply chat template to format a conversation. Does not use a Jinja parser - only supports a pre-defined list of templates.\n\n**Parameters:**\n- `tmpl`: Jinja template (NULL to use model's default)\n- `chat`: Array of chat messages\n- `n_msg`: Number of messages\n- `add_ass`: Whether to end prompt with assistant message start token(s)\n- `buf`: Output buffer (recommended size: 2 * total characters of all messages)\n- `length`: Buffer size\n\n**Returns:** Total number of bytes of the formatted prompt\n\n**Usage:**\n```c\nllama_chat_message messages[] = {\n    {\"system\", \"You are a helpful assistant.\"},\n    {\"user\", \"Hello!\"}\n};\nchar buf[1024];\nint len = llama_chat_apply_template(NULL, messages, 2, true, buf, 1024);\n```\n\n### llama_chat_builtin_templates\n```c\nint32_t llama_chat_builtin_templates(const char ** output, size_t len);\n```\nGet list of built-in chat templates.\n\n---\n\n",
        "plugins/llamacpp/skills/llamacpp/references/api-model-info.md": "# llama.cpp C API Reference - Model Properties\n\nPart 2 of 6 | [Core](api-core.md) | **Model Info** | [Context](api-context.md) | [Inference](api-inference.md) | [Sampling](api-sampling.md) | [Advanced](api-advanced.md)\n\nThis file covers:\n- Model Properties & Metadata - Query model architecture and metadata\n\nFor complete API navigation, see [api-core.md](api-core.md).\n\n---\n\n## Model Properties & Metadata\n\n### llama_model_get_vocab\n```c\nconst struct llama_vocab * llama_model_get_vocab(const struct llama_model * model);\n```\nGet the model's vocabulary.\n\n### llama_model_rope_type\n```c\nenum llama_rope_type llama_model_rope_type(const struct llama_model * model);\n```\nGet the model's RoPE type.\n\n### llama_model_n_ctx_train\n```c\nint32_t llama_model_n_ctx_train(const struct llama_model * model);\n```\nGet the context size the model was trained with.\n\n### llama_model_n_embd\n```c\nint32_t llama_model_n_embd(const struct llama_model * model);\n```\nGet the embedding dimension.\n\n### llama_model_n_embd_inp\n```c\nint32_t llama_model_n_embd_inp(const struct llama_model * model);\n```\nGet the input embedding dimension.\n\n### llama_model_n_embd_out\n```c\nint32_t llama_model_n_embd_out(const struct llama_model * model);\n```\nGet the output embedding dimension.\n\n### llama_model_n_layer\n```c\nint32_t llama_model_n_layer(const struct llama_model * model);\n```\nGet the number of layers in the model.\n\n### llama_model_n_head\n```c\nint32_t llama_model_n_head(const struct llama_model * model);\n```\nGet the number of attention heads.\n\n### llama_model_n_head_kv\n```c\nint32_t llama_model_n_head_kv(const struct llama_model * model);\n```\nGet the number of KV heads (for grouped-query attention).\n\n### llama_model_n_swa\n```c\nint32_t llama_model_n_swa(const struct llama_model * model);\n```\nGet the sliding window attention size.\n\n### llama_model_rope_freq_scale_train\n```c\nfloat llama_model_rope_freq_scale_train(const struct llama_model * model);\n```\nGet the model's RoPE frequency scaling factor.\n\n### llama_model_n_cls_out\n```c\nuint32_t llama_model_n_cls_out(const struct llama_model * model);\n```\nGet the number of classifier outputs (only valid for classifier models). Undefined behavior for non-classifier models.\n\n### llama_model_cls_label\n```c\nconst char * llama_model_cls_label(const struct llama_model * model, uint32_t i);\n```\nGet the label of a classifier output by index. Returns NULL if no label provided.\n\n### llama_model_meta_val_str\n```c\nint32_t llama_model_meta_val_str(\n    const struct llama_model * model,\n    const char * key,\n    char * buf,\n    size_t buf_size);\n```\nGet metadata value as a string by key name. Returns the length of the string on success, or -1 on failure. The output string is always null-terminated.\n\n### llama_model_meta_count\n```c\nint32_t llama_model_meta_count(const struct llama_model * model);\n```\nGet the number of metadata key/value pairs.\n\n### llama_model_meta_key_str\n```c\nconst char * llama_model_meta_key_str(enum llama_model_meta_key key);\n```\nGet sampling metadata key name. Returns NULL if the key is invalid.\n\n### llama_model_meta_key_by_index\n```c\nint32_t llama_model_meta_key_by_index(\n    const struct llama_model * model,\n    int32_t i,\n    char * buf,\n    size_t buf_size);\n```\nGet metadata key name by index.\n\n### llama_model_meta_val_str_by_index\n```c\nint32_t llama_model_meta_val_str_by_index(\n    const struct llama_model * model,\n    int32_t i,\n    char * buf,\n    size_t buf_size);\n```\nGet metadata value as a string by index.\n\n### llama_model_desc\n```c\nint32_t llama_model_desc(\n    const struct llama_model * model,\n    char * buf,\n    size_t buf_size);\n```\nGet a string describing the model type.\n\n### llama_model_size\n```c\nuint64_t llama_model_size(const struct llama_model * model);\n```\nGet the total size of all tensors in the model in bytes.\n\n### llama_model_n_params\n```c\nuint64_t llama_model_n_params(const struct llama_model * model);\n```\nGet the total number of parameters in the model.\n\n### llama_model_has_encoder\n```c\nbool llama_model_has_encoder(const struct llama_model * model);\n```\nReturns true if the model contains an encoder that requires `llama_encode()` call.\n\n### llama_model_has_decoder\n```c\nbool llama_model_has_decoder(const struct llama_model * model);\n```\nReturns true if the model contains a decoder that requires `llama_decode()` call.\n\n### llama_model_decoder_start_token\n```c\nllama_token llama_model_decoder_start_token(const struct llama_model * model);\n```\nFor encoder-decoder models, returns the token ID that must be provided to the decoder to start generating. Returns -1 for other models.\n\n### llama_model_is_recurrent\n```c\nbool llama_model_is_recurrent(const struct llama_model * model);\n```\nReturns true if the model is recurrent (like Mamba, RWKV, etc.).\n\n### llama_model_is_hybrid\n```c\nbool llama_model_is_hybrid(const struct llama_model * model);\n```\nReturns true if the model is hybrid (like Jamba, Granite, etc.).\n\n### llama_model_is_diffusion\n```c\nbool llama_model_is_diffusion(const struct llama_model * model);\n```\nReturns true if the model is diffusion-based (like LLaDA, Dream, etc.).\n\n### llama_model_chat_template\n```c\nconst char * llama_model_chat_template(\n    const struct llama_model * model,\n    const char * name);\n```\nGet the default chat template. Returns NULL if not available. If `name` is NULL, returns the default chat template.\n\n---\n\n",
        "plugins/llamacpp/skills/llamacpp/references/api-sampling.md": "# llama.cpp C API Reference - Sampling\n\nPart 5 of 6 | [Core](api-core.md) | [Model Info](api-model-info.md) | [Context](api-context.md) | [Inference](api-inference.md) | **Sampling** | [Advanced](api-advanced.md)\n\nThis file covers:\n- Sampling - All 26+ sampling strategies including XTC, DRY, adaptive-p, penalties, top-k/p, temperature, etc.\n- Backend Sampling API [EXPERIMENTAL] - GPU-accelerated sampling\n\nFor complete API navigation, see [api-core.md](api-core.md).\n\n---\n\n## Sampling\n\nSampling in llama.cpp uses a chain architecture where multiple samplers can be combined.\n\n### Core Sampler Functions\n\n```c\nstruct llama_sampler * llama_sampler_init(\n    struct llama_sampler_i * iface,\n    llama_sampler_context_t ctx);\n```\nInitialize a custom sampler (for advanced users implementing custom sampling).\n\n```c\nconst char * llama_sampler_name(const struct llama_sampler * smpl);\n```\nGet the name of a sampler.\n\n```c\nvoid llama_sampler_accept(struct llama_sampler * smpl, llama_token token);\n```\nAccept a token (updates sampler state).\n\n```c\nvoid llama_sampler_apply(\n    struct llama_sampler * smpl,\n    llama_token_data_array * cur_p);\n```\nApply the sampler to modify the token data array.\n\n```c\nvoid llama_sampler_reset(struct llama_sampler * smpl);\n```\nReset the sampler state.\n\n```c\nstruct llama_sampler * llama_sampler_clone(const struct llama_sampler * smpl);\n```\nClone a sampler.\n\n```c\nvoid llama_sampler_free(struct llama_sampler * smpl);\n```\nFree a sampler. **Important:** Do not free if added to a chain via `llama_sampler_chain_add()`.\n\n### Sampler Chain\n\n```c\nstruct llama_sampler * llama_sampler_chain_init(\n    struct llama_sampler_chain_params params);\n```\nInitialize a sampler chain.\n\n**Usage:**\n```c\nstruct llama_sampler_chain_params params = llama_sampler_chain_default_params();\nstruct llama_sampler * chain = llama_sampler_chain_init(params);\n```\n\n```c\nvoid llama_sampler_chain_add(\n    struct llama_sampler * chain,\n    struct llama_sampler * smpl);\n```\nAdd a sampler to the chain. **Important:** The chain takes ownership and will free the sampler.\n\n```c\nstruct llama_sampler * llama_sampler_chain_get(\n    struct llama_sampler * chain,\n    int32_t i);\n```\nGet the i-th sampler in the chain. Returns NULL if:\n- the sampler is NULL\n- the sampler is not a `llama_sampler_chain`\n- the index is out of bounds, unless i == -1\n- if i == -1, returns the chain itself (can be used to check if the sampler is a chain)\n\n```c\nint llama_sampler_chain_n(const struct llama_sampler * chain);\n```\nGet the number of samplers in the chain.\n\n```c\nstruct llama_sampler * llama_sampler_chain_remove(\n    struct llama_sampler * chain,\n    int32_t i);\n```\nRemove a sampler from the chain. The chain no longer owns it and will not free it.\n\n### Built-in Samplers\n\n#### Basic Samplers\n\n```c\nstruct llama_sampler * llama_sampler_init_greedy(void);\n```\nGreedy sampling (always pick the most likely token).\n\n```c\nstruct llama_sampler * llama_sampler_init_dist(uint32_t seed);\n```\nSample from the probability distribution.\n\n**Parameters:**\n- `seed`: Random seed. Use `LLAMA_DEFAULT_SEED` (0xFFFFFFFF) to use a random seed.\n\n#### Top-K Sampling\n\n```c\nstruct llama_sampler * llama_sampler_init_top_k(int32_t k);\n```\nTop-K sampling. Setting k <= 0 makes this a noop.\n\n**Reference:** \"The Curious Case of Neural Text Degeneration\" (https://arxiv.org/abs/1904.09751)\n\n#### Top-P (Nucleus) Sampling\n\n```c\nstruct llama_sampler * llama_sampler_init_top_p(float p, size_t min_keep);\n```\nNucleus sampling.\n\n**Parameters:**\n- `p`: Cumulative probability threshold\n- `min_keep`: Minimum number of tokens to keep\n\n**Reference:** \"The Curious Case of Neural Text Degeneration\" (https://arxiv.org/abs/1904.09751)\n\n#### Min-P Sampling\n\n```c\nstruct llama_sampler * llama_sampler_init_min_p(float p, size_t min_keep);\n```\nMinimum P sampling.\n\n**Reference:** https://github.com/ggml-org/llama.cpp/pull/3841\n\n#### Typical Sampling\n\n```c\nstruct llama_sampler * llama_sampler_init_typical(float p, size_t min_keep);\n```\nLocally Typical Sampling.\n\n**Reference:** https://arxiv.org/abs/2202.00666\n\n#### Temperature\n\n```c\nstruct llama_sampler * llama_sampler_init_temp(float t);\n```\nTemperature sampling. Updates logits: `l_i' = l_i/t`. When t <= 0, max logit is kept, rest set to -inf.\n\n```c\nstruct llama_sampler * llama_sampler_init_temp_ext(\n    float t,\n    float delta,\n    float exponent);\n```\nDynamic temperature (entropy-based).\n\n**Reference:** https://arxiv.org/abs/2309.02772\n\n#### XTC Sampler\n\n```c\nstruct llama_sampler * llama_sampler_init_xtc(\n    float p,\n    float t,\n    size_t min_keep,\n    uint32_t seed);\n```\nXTC sampler.\n\n**Reference:** https://github.com/oobabooga/text-generation-webui/pull/6335\n\n#### Top-nσ Sampling\n\n```c\nstruct llama_sampler * llama_sampler_init_top_n_sigma(float n);\n```\nTop-nσ sampling.\n\n**Reference:** \"Top-nσ: Not All Logits Are You Need\" (https://arxiv.org/pdf/2411.07641)\n\n#### Mirostat\n\n```c\nstruct llama_sampler * llama_sampler_init_mirostat(\n    int32_t n_vocab,\n    uint32_t seed,\n    float tau,\n    float eta,\n    int32_t m);\n```\nMirostat 1.0 algorithm.\n\n**Parameters:**\n- `n_vocab`: Vocabulary size\n- `seed`: Random seed\n- `tau`: Target cross-entropy\n- `eta`: Learning rate\n- `m`: Number of tokens considered (paper uses m=100)\n\n**Reference:** https://arxiv.org/abs/2007.14966\n\n```c\nstruct llama_sampler * llama_sampler_init_mirostat_v2(\n    uint32_t seed,\n    float tau,\n    float eta);\n```\nMirostat 2.0 algorithm.\n\n#### Grammar\n\n```c\nstruct llama_sampler * llama_sampler_init_grammar(\n    const struct llama_vocab * vocab,\n    const char * grammar_str,\n    const char * grammar_root);\n```\nInitialize a GBNF grammar sampler. Returns NULL if parsing fails.\n\n**Parameters:**\n- `vocab`: Vocabulary\n- `grammar_str`: Production rules as a string\n- `grammar_root`: Name of the start symbol\n\n```c\nstruct llama_sampler * llama_sampler_init_grammar_lazy_patterns(\n    const struct llama_vocab * vocab,\n    const char * grammar_str,\n    const char * grammar_root,\n    const char ** trigger_patterns,\n    size_t num_trigger_patterns,\n    const llama_token * trigger_tokens,\n    size_t num_trigger_tokens);\n```\nLazy grammar sampler (triggers based on patterns or tokens).\n\n**Reference:** https://github.com/ggml-org/llama.cpp/pull/9639\n\n#### Penalties\n\n```c\nstruct llama_sampler * llama_sampler_init_penalties(\n    int32_t penalty_last_n,\n    float penalty_repeat,\n    float penalty_freq,\n    float penalty_present);\n```\nApply repetition penalties. **Note:** Avoid using on full vocabulary (slow). Apply top-k or top-p first.\n\n**Parameters:**\n- `penalty_last_n`: Last n tokens to penalize (0 = disabled, -1 = context size)\n- `penalty_repeat`: Repeat penalty (1.0 = disabled)\n- `penalty_freq`: Frequency penalty (0.0 = disabled)\n- `penalty_present`: Presence penalty (0.0 = disabled)\n\n#### DRY Sampler\n\n```c\nstruct llama_sampler * llama_sampler_init_dry(\n    const struct llama_vocab * vocab,\n    int32_t n_ctx_train,\n    float dry_multiplier,\n    float dry_base,\n    int32_t dry_allowed_length,\n    int32_t dry_penalty_last_n,\n    const char ** seq_breakers,\n    size_t num_breakers);\n```\nDRY (Don't Repeat Yourself) sampler.\n\n**Reference:** https://github.com/oobabooga/text-generation-webui/pull/5677\n\n#### Adaptive-P Sampler\n\n```c\nstruct llama_sampler * llama_sampler_init_adaptive_p(\n    float target,\n    float decay,\n    uint32_t seed);\n```\nAdaptive-P sampler - selects tokens near a configurable target probability over time.\n\nThe sampler transforms the token probability distribution to favor tokens near a user-configurable probability target. Internally maintains an exponential moving average (EMA) of original probabilities of selected tokens, using this to compute an adapted target at each step.\n\n**Parameters:**\n- `target`: Select tokens near this probability (valid range 0.0 to 1.0; negative = disabled)\n- `decay`: EMA decay for adaptation; history ≈ 1/(1-decay) tokens (valid range 0.0 - 0.99)\n- `seed`: Random seed. Use `LLAMA_DEFAULT_SEED` for a random seed.\n\n**Important:** This sampler selects a token ID (like mirostat, dist, greedy), so it must be **last in the sampler chain**. Only mild truncation before this sampler is recommended - use min-p as the only other active sampler.\n\n**Reference:** https://github.com/ggml-org/llama.cpp/pull/17927\n\n#### Logit Bias\n\n```c\nstruct llama_sampler * llama_sampler_init_logit_bias(\n    int32_t n_vocab,\n    int32_t n_logit_bias,\n    const llama_logit_bias * logit_bias);\n```\nApply logit biases to specific tokens.\n\n#### Infill Sampler\n\n```c\nstruct llama_sampler * llama_sampler_init_infill(\n    const struct llama_vocab * vocab);\n```\nFill-in-the-middle infilling sampler. Use after top-k + top-p sampling.\n\n### Sampling Functions\n\n```c\nuint32_t llama_sampler_get_seed(const struct llama_sampler * smpl);\n```\nGet the seed used by the sampler (if applicable), otherwise `LLAMA_DEFAULT_SEED`.\n\n```c\nllama_token llama_sampler_sample(\n    struct llama_sampler * smpl,\n    struct llama_context * ctx,\n    int32_t idx);\n```\nSample and accept a token from the idx-th output of the last evaluation.\n\n**Usage:**\n```c\n// Setup sampler chain\nstruct llama_sampler * sampler = llama_sampler_chain_init(\n    llama_sampler_chain_default_params());\nllama_sampler_chain_add(sampler, llama_sampler_init_top_k(50));\nllama_sampler_chain_add(sampler, llama_sampler_init_top_p(0.9, 1));\nllama_sampler_chain_add(sampler, llama_sampler_init_temp(0.8));\nllama_sampler_chain_add(sampler, llama_sampler_init_dist(LLAMA_DEFAULT_SEED));\n\n// Decoding loop\nwhile (...) {\n    llama_decode(ctx, batch);\n    llama_token token = llama_sampler_sample(sampler, ctx, -1);\n    // Use token...\n}\n\nllama_sampler_free(sampler);\n```\n\n---\n\n## Backend Sampling API [EXPERIMENTAL]\n\nBackend sampling allows sampling operations to be performed directly on the GPU as part of the computation graph. This can improve performance by reducing data transfer between device and host memory.\n\n**Note:** Use only if the `llama_context` was created with at least one `llama_sampler_seq_config`.\n\n### Configuration Struct\n\n```c\nstruct llama_sampler_seq_config {\n    llama_seq_id           seq_id;\n    struct llama_sampler * sampler;\n};\n```\nConfiguration for per-sequence backend sampling. Add to `llama_context_params.samplers` when creating context.\n\n### Attaching Samplers\n\n```c\nbool llama_set_sampler(\n    struct llama_context * ctx,\n    llama_seq_id seq_id,\n    struct llama_sampler * smpl);\n```\nAttach a sampler to the context for a specific sequence.\n\n**Notes:**\n- Prefer initializing the context with `llama_context_params.samplers` when possible\n- The sampler must be a sampler chain (use `llama_sampler_chain_init`)\n\n### Retrieving Sampled Results\n\n```c\nllama_token llama_get_sampled_token_ith(\n    struct llama_context * ctx,\n    int32_t i);\n```\nGet the backend sampled token for the i-th token. Returns `LLAMA_TOKEN_NULL` if no token was sampled.\n\n```c\nfloat * llama_get_sampled_probs_ith(struct llama_context * ctx, int32_t i);\nuint32_t llama_get_sampled_probs_count_ith(struct llama_context * ctx, int32_t i);\n```\nGet the backend sampled probabilities for the i-th token. Returns NULL if no probabilities were generated.\n\n```c\nfloat * llama_get_sampled_logits_ith(struct llama_context * ctx, int32_t i);\nuint32_t llama_get_sampled_logits_count_ith(struct llama_context * ctx, int32_t i);\n```\nGet the backend sampled logits for the i-th token. Returns NULL if no logits were sampled.\n\n```c\nllama_token * llama_get_sampled_candidates_ith(struct llama_context * ctx, int32_t i);\nuint32_t llama_get_sampled_candidates_count_ith(struct llama_context * ctx, int32_t i);\n```\nGet the backend sampled candidates (token ids) for the i-th token. These are needed to map probability/logit indices to vocab token ids. Returns NULL if no candidates were sampled.\n\n### Custom Sampler Interface for Backend\n\nWhen implementing custom samplers with backend support, the `llama_sampler_i` interface includes:\n\n```c\nstruct llama_sampler_data {\n    struct ggml_tensor * logits;\n    struct ggml_tensor * probs;\n    struct ggml_tensor * sampled;\n    struct ggml_tensor * candidates;\n};\n```\nData structure for backend sampling operations.\n\n**Interface methods for backend sampling:**\n\n```c\n// Return true if the backend supports all ops needed by the sampler\nbool (*backend_init)(struct llama_sampler * smpl, ggml_backend_buffer_type_t buft);\n\n// Called after backend_apply()\nvoid (*backend_accept)(\n    struct llama_sampler * smpl,\n    struct ggml_context  * ctx,\n    struct ggml_cgraph   * gf,\n    struct ggml_tensor   * selected_token);\n\n// Called after backend_init()\nvoid (*backend_apply)(\n    struct llama_sampler      * smpl,\n    struct ggml_context       * ctx,\n    struct ggml_cgraph        * gf,\n    struct llama_sampler_data * data);\n\n// Called before graph execution to set inputs for the current ubatch\nvoid (*backend_set_input)(struct llama_sampler * smpl);\n```\n\n**Usage Example:**\n```c\n// Create sampler chain for backend sampling\nstruct llama_sampler * chain = llama_sampler_chain_init(\n    llama_sampler_chain_default_params());\nllama_sampler_chain_add(chain, llama_sampler_init_top_k(50));\nllama_sampler_chain_add(chain, llama_sampler_init_dist(42));\n\n// Configure backend sampling in context params\nstruct llama_sampler_seq_config sampler_configs[] = {\n    { .seq_id = 0, .sampler = chain }\n};\n\nstruct llama_context_params ctx_params = llama_context_default_params();\nctx_params.samplers = sampler_configs;\nctx_params.n_samplers = 1;\n\n// Create context with backend sampling\nstruct llama_context * ctx = llama_init_from_model(model, ctx_params);\n\n// After decode, retrieve backend-sampled token\nllama_decode(ctx, batch);\nllama_token token = llama_get_sampled_token_ith(ctx, -1);\nif (token != LLAMA_TOKEN_NULL) {\n    // Token was sampled on backend\n}\n```\n\n---\n\n",
        "plugins/llamacpp/skills/llamacpp/references/api.md": "# llama.cpp C API Reference\n\nThis document provides a comprehensive reference for all non-deprecated functions in the llama.cpp C API. Functions are organized by category for easy navigation.\n\n## Table of Contents\n\n1. [Initialization & Backend](#initialization--backend)\n2. [Parameter Helpers](#parameter-helpers)\n3. [Model Loading & Management](#model-loading--management)\n4. [Model Properties & Metadata](#model-properties--metadata)\n5. [Context Management](#context-management)\n6. [Memory (KV Cache) Management](#memory-kv-cache-management)\n7. [State & Session Management](#state--session-management)\n8. [Batch Operations](#batch-operations)\n9. [Inference & Decoding](#inference--decoding)\n10. [Vocabulary & Tokenization](#vocabulary--tokenization)\n11. [Chat Templates](#chat-templates)\n12. [Sampling](#sampling)\n13. [LoRA Adapters](#lora-adapters)\n14. [Performance & Utilities](#performance--utilities)\n15. [Training](#training)\n\n## Quick Reference\n\n### Most Common Functions\n\n**Model & Context:**\n- `llama_backend_init()` - Initialize backend\n- `llama_model_load_from_file()` - Load GGUF model\n- `llama_init_from_model()` - Create inference context\n- `llama_model_free()`, `llama_free()` - Cleanup\n\n**Tokenization:**\n- `llama_tokenize()` - Text → tokens\n- `llama_detokenize()` - Tokens → text\n- `llama_token_to_piece()` - Single token → text\n\n**Inference:**\n- `llama_decode()` - Process token batch\n- `llama_get_logits_ith()` - Get token probabilities\n- `llama_get_embeddings_ith()` - Extract embeddings\n\n**Sampling:**\n- `llama_sampler_chain_init()` - Create sampler\n- `llama_sampler_sample()` - Sample next token\n- `llama_vocab_is_eog()` - Check for end-of-generation\n\n**Memory Management:**\n- `llama_memory_clear()` - Clear KV cache\n- `llama_memory_seq_rm()` - Remove sequence\n- `llama_memory_seq_cp()` - Copy sequence\n\nSee categories below for complete function listings.\n\n---\n\n## Initialization & Backend\n\n### llama_backend_init\n```c\nvoid llama_backend_init(void);\n```\nInitialize the llama + ggml backend. Call once at the start of the program.\n\n**Usage:**\n```c\nllama_backend_init();\n```\n\n### llama_backend_free\n```c\nvoid llama_backend_free(void);\n```\nFree backend resources. Call once at the end of the program. Currently only used for MPI.\n\n### llama_numa_init\n```c\nvoid llama_numa_init(enum ggml_numa_strategy numa);\n```\nOptional: Initialize NUMA optimizations.\n\n**Parameters:**\n- `numa`: NUMA strategy to use (from ggml)\n\n### llama_attach_threadpool\n```c\nvoid llama_attach_threadpool(\n    struct llama_context * ctx,\n    ggml_threadpool_t threadpool,\n    ggml_threadpool_t threadpool_batch);\n```\nOptional: Attach a custom threadpool. An auto threadpool is created in ggml if not passed explicitly.\n\n**Parameters:**\n- `ctx`: Context to attach threadpool to\n- `threadpool`: Threadpool for single-token generation\n- `threadpool_batch`: Threadpool for batch processing\n\n### llama_detach_threadpool\n```c\nvoid llama_detach_threadpool(struct llama_context * ctx);\n```\nDetach threadpool from context.\n\n---\n\n## Parameter Helpers\n\n### llama_model_default_params\n```c\nstruct llama_model_params llama_model_default_params(void);\n```\nGet default model parameters. Always use this to initialize `llama_model_params` before modifying specific fields.\n\n**Usage:**\n```c\nstruct llama_model_params params = llama_model_default_params();\nparams.n_gpu_layers = 32;  // Override specific fields\n```\n\n### llama_context_default_params\n```c\nstruct llama_context_params llama_context_default_params(void);\n```\nGet default context parameters. Always use this to initialize `llama_context_params`.\n\n**Usage:**\n```c\nstruct llama_context_params params = llama_context_default_params();\nparams.n_ctx = 4096;\nparams.n_batch = 512;\n```\n\n### llama_sampler_chain_default_params\n```c\nstruct llama_sampler_chain_params llama_sampler_chain_default_params(void);\n```\nGet default sampler chain parameters.\n\n### llama_model_quantize_default_params\n```c\nstruct llama_model_quantize_params llama_model_quantize_default_params(void);\n```\nGet default quantization parameters.\n\n---\n\n## Model Loading & Management\n\n### llama_model_load_from_file\n```c\nstruct llama_model * llama_model_load_from_file(\n    const char * path_model,\n    struct llama_model_params params);\n```\nLoad a model from a file. If the file is split into multiple parts, the file name must follow this pattern: `<name>-%05d-of-%05d.gguf`. Returns NULL on failure.\n\n**Parameters:**\n- `path_model`: Path to the GGUF model file\n- `params`: Model loading parameters (get from `llama_model_default_params()`)\n\n**Usage:**\n```c\nstruct llama_model_params params = llama_model_default_params();\nparams.n_gpu_layers = 32;\nstruct llama_model * model = llama_model_load_from_file(\"model.gguf\", params);\nif (!model) {\n    // Handle error\n}\n```\n\n### llama_model_load_from_splits\n```c\nstruct llama_model * llama_model_load_from_splits(\n    const char ** paths,\n    size_t n_paths,\n    struct llama_model_params params);\n```\nLoad a model from multiple split files (supports custom naming schemes). The paths must be in the correct order.\n\n**Parameters:**\n- `paths`: Array of paths to split files\n- `n_paths`: Number of split files\n- `params`: Model loading parameters\n\n### llama_model_save_to_file\n```c\nvoid llama_model_save_to_file(\n    const struct llama_model * model,\n    const char * path_model);\n```\nSave a model to a file.\n\n### llama_model_free\n```c\nvoid llama_model_free(struct llama_model * model);\n```\nFree a loaded model. Always call this when done with a model.\n\n**Usage:**\n```c\nllama_model_free(model);\n```\n\n### llama_model_quantize\n```c\nuint32_t llama_model_quantize(\n    const char * fname_inp,\n    const char * fname_out,\n    const llama_model_quantize_params * params);\n```\nQuantize a model. Returns 0 on success.\n\n**Parameters:**\n- `fname_inp`: Input model file path\n- `fname_out`: Output model file path\n- `params`: Quantization parameters\n\n---\n\n## Model Properties & Metadata\n\n### llama_model_get_vocab\n```c\nconst struct llama_vocab * llama_model_get_vocab(const struct llama_model * model);\n```\nGet the model's vocabulary.\n\n### llama_model_rope_type\n```c\nenum llama_rope_type llama_model_rope_type(const struct llama_model * model);\n```\nGet the model's RoPE type.\n\n### llama_model_n_ctx_train\n```c\nint32_t llama_model_n_ctx_train(const struct llama_model * model);\n```\nGet the context size the model was trained with.\n\n### llama_model_n_embd\n```c\nint32_t llama_model_n_embd(const struct llama_model * model);\n```\nGet the embedding dimension.\n\n### llama_model_n_embd_inp\n```c\nint32_t llama_model_n_embd_inp(const struct llama_model * model);\n```\nGet the input embedding dimension.\n\n### llama_model_n_layer\n```c\nint32_t llama_model_n_layer(const struct llama_model * model);\n```\nGet the number of layers in the model.\n\n### llama_model_n_head\n```c\nint32_t llama_model_n_head(const struct llama_model * model);\n```\nGet the number of attention heads.\n\n### llama_model_n_head_kv\n```c\nint32_t llama_model_n_head_kv(const struct llama_model * model);\n```\nGet the number of KV heads (for grouped-query attention).\n\n### llama_model_n_swa\n```c\nint32_t llama_model_n_swa(const struct llama_model * model);\n```\nGet the sliding window attention size.\n\n### llama_model_rope_freq_scale_train\n```c\nfloat llama_model_rope_freq_scale_train(const struct llama_model * model);\n```\nGet the model's RoPE frequency scaling factor.\n\n### llama_model_n_cls_out\n```c\nuint32_t llama_model_n_cls_out(const struct llama_model * model);\n```\nGet the number of classifier outputs (only valid for classifier models). Undefined behavior for non-classifier models.\n\n### llama_model_cls_label\n```c\nconst char * llama_model_cls_label(const struct llama_model * model, uint32_t i);\n```\nGet the label of a classifier output by index. Returns NULL if no label provided.\n\n### llama_model_meta_val_str\n```c\nint32_t llama_model_meta_val_str(\n    const struct llama_model * model,\n    const char * key,\n    char * buf,\n    size_t buf_size);\n```\nGet metadata value as a string by key name. Returns the length of the string on success, or -1 on failure. The output string is always null-terminated.\n\n### llama_model_meta_count\n```c\nint32_t llama_model_meta_count(const struct llama_model * model);\n```\nGet the number of metadata key/value pairs.\n\n### llama_model_meta_key_str\n```c\nconst char * llama_model_meta_key_str(enum llama_model_meta_key key);\n```\nGet sampling metadata key name. Returns NULL if the key is invalid.\n\n### llama_model_meta_key_by_index\n```c\nint32_t llama_model_meta_key_by_index(\n    const struct llama_model * model,\n    int32_t i,\n    char * buf,\n    size_t buf_size);\n```\nGet metadata key name by index.\n\n### llama_model_meta_val_str_by_index\n```c\nint32_t llama_model_meta_val_str_by_index(\n    const struct llama_model * model,\n    int32_t i,\n    char * buf,\n    size_t buf_size);\n```\nGet metadata value as a string by index.\n\n### llama_model_desc\n```c\nint32_t llama_model_desc(\n    const struct llama_model * model,\n    char * buf,\n    size_t buf_size);\n```\nGet a string describing the model type.\n\n### llama_model_size\n```c\nuint64_t llama_model_size(const struct llama_model * model);\n```\nGet the total size of all tensors in the model in bytes.\n\n### llama_model_n_params\n```c\nuint64_t llama_model_n_params(const struct llama_model * model);\n```\nGet the total number of parameters in the model.\n\n### llama_model_has_encoder\n```c\nbool llama_model_has_encoder(const struct llama_model * model);\n```\nReturns true if the model contains an encoder that requires `llama_encode()` call.\n\n### llama_model_has_decoder\n```c\nbool llama_model_has_decoder(const struct llama_model * model);\n```\nReturns true if the model contains a decoder that requires `llama_decode()` call.\n\n### llama_model_decoder_start_token\n```c\nllama_token llama_model_decoder_start_token(const struct llama_model * model);\n```\nFor encoder-decoder models, returns the token ID that must be provided to the decoder to start generating. Returns -1 for other models.\n\n### llama_model_is_recurrent\n```c\nbool llama_model_is_recurrent(const struct llama_model * model);\n```\nReturns true if the model is recurrent (like Mamba, RWKV, etc.).\n\n### llama_model_is_hybrid\n```c\nbool llama_model_is_hybrid(const struct llama_model * model);\n```\nReturns true if the model is hybrid (like Jamba, Granite, etc.).\n\n### llama_model_is_diffusion\n```c\nbool llama_model_is_diffusion(const struct llama_model * model);\n```\nReturns true if the model is diffusion-based (like LLaDA, Dream, etc.).\n\n### llama_model_chat_template\n```c\nconst char * llama_model_chat_template(\n    const struct llama_model * model,\n    const char * name);\n```\nGet the default chat template. Returns NULL if not available. If `name` is NULL, returns the default chat template.\n\n---\n\n## Context Management\n\n### llama_init_from_model\n```c\nstruct llama_context * llama_init_from_model(\n    struct llama_model * model,\n    struct llama_context_params params);\n```\nCreate a new context from a loaded model. Returns NULL on failure.\n\n**Usage:**\n```c\nstruct llama_context_params params = llama_context_default_params();\nparams.n_ctx = 4096;\nparams.n_batch = 512;\nstruct llama_context * ctx = llama_init_from_model(model, params);\nif (!ctx) {\n    // Handle error\n}\n```\n\n**New parameters in b7572:**\n\n- `kv_unified` (bool) - Use unified KV cache buffer (experimental). Enables a more memory-efficient cache layout. Default: false.\n\n- `swa_full` (bool) - For models with Sliding Window Attention (SWA), allocate full context size instead of just the attention window. Set to true when you need to access tokens outside the SWA window. Check `llama_model_n_swa()` to detect if a model uses SWA. Default: false.\n\n**Example with SWA:**\n```c\nstruct llama_context_params params = llama_context_default_params();\nparams.n_ctx = 32768;\n\n// Check if model uses Sliding Window Attention\nint32_t swa_size = llama_model_n_swa(model);\nif (swa_size > 0) {\n    printf(\"Model uses SWA with window: %d\\n\", swa_size);\n    params.swa_full = true;  // Enable full context access\n}\n\nstruct llama_context * ctx = llama_init_from_model(model, params);\n```\n\n### llama_free\n```c\nvoid llama_free(struct llama_context * ctx);\n```\nFree all allocated memory for a context. Always call this when done with a context.\n\n**Usage:**\n```c\nllama_free(ctx);\n```\n\n### llama_params_fit\n```c\nenum llama_params_fit_status llama_params_fit(\n    const char * path_model,\n    struct llama_model_params * mparams,\n    struct llama_context_params * cparams,\n    float * tensor_split,\n    struct llama_model_tensor_buft_override * tensor_buft_overrides,\n    size_t * margins,\n    uint32_t n_ctx_min,\n    enum ggml_log_level log_level);\n```\nFits model and context parameters to available device memory. Returns a status enum (SUCCESS, FAILURE, or ERROR). This function is NOT thread-safe. Only parameters matching defaults are modified, except context size which is always modified when set to 0.\n\n**Return Values:**\n- `LLAMA_PARAMS_FIT_STATUS_SUCCESS (0)`: Found allocations that are projected to fit\n- `LLAMA_PARAMS_FIT_STATUS_FAILURE (1)`: Could not find allocations that fit\n- `LLAMA_PARAMS_FIT_STATUS_ERROR (2)`: Hard error occurred (e.g., model not found)\n\n**Parameters:**\n- `path_model`: Path to model file\n- `mparams`: Writable model params (will be modified)\n- `cparams`: Writable context params (will be modified)\n- `tensor_split`: Writable buffer for tensor split (needs at least `llama_max_devices()` elements)\n- `tensor_buft_overrides`: Writable buffer for overrides (needs at least `llama_max_tensor_buft_overrides()` elements)\n- `margins`: Margins of memory to leave per device in bytes (array with `llama_max_devices()` elements)\n- `n_ctx_min`: Minimum context size to set when trying to reduce memory use\n- `log_level`: Minimum log level to print during fitting\n\n### llama_get_model\n```c\nconst struct llama_model * llama_get_model(const struct llama_context * ctx);\n```\nGet the model associated with a context.\n\n### llama_get_memory\n```c\nllama_memory_t llama_get_memory(const struct llama_context * ctx);\n```\nGet the memory handle for a context.\n\n### llama_pooling_type\n```c\nenum llama_pooling_type llama_pooling_type(const struct llama_context * ctx);\n```\nGet the pooling type used by the context.\n\n### llama_n_ctx\n```c\nuint32_t llama_n_ctx(const struct llama_context * ctx);\n```\nGet the actual context size. After creating a context, query this to get the actual value (may differ from requested).\n\n### llama_n_ctx_seq\n```c\nuint32_t llama_n_ctx_seq(const struct llama_context * ctx);\n```\nGet the context size for sequences.\n\n### llama_n_batch\n```c\nuint32_t llama_n_batch(const struct llama_context * ctx);\n```\nGet the logical maximum batch size.\n\n### llama_n_ubatch\n```c\nuint32_t llama_n_ubatch(const struct llama_context * ctx);\n```\nGet the physical maximum batch size.\n\n### llama_n_seq_max\n```c\nuint32_t llama_n_seq_max(const struct llama_context * ctx);\n```\nGet the maximum number of sequences.\n\n---\n\n## Memory (KV Cache) Management\n\nThe memory functions operate on the KV cache and allow for advanced sequence management.\n\n### llama_memory_clear\n```c\nvoid llama_memory_clear(llama_memory_t mem, bool data);\n```\nClear the memory contents.\n\n**Parameters:**\n- `mem`: Memory handle\n- `data`: If true, data buffers will also be cleared together with metadata\n\n### llama_memory_seq_rm\n```c\nbool llama_memory_seq_rm(\n    llama_memory_t mem,\n    llama_seq_id seq_id,\n    llama_pos p0,\n    llama_pos p1);\n```\nRemove all tokens that belong to the specified sequence and have positions in [p0, p1). Returns false if a partial sequence cannot be removed. Removing a whole sequence never fails.\n\n**Parameters:**\n- `mem`: Memory handle\n- `seq_id`: Sequence ID (< 0: match any sequence)\n- `p0`: Start position (< 0: [0, p1])\n- `p1`: End position (< 0: [p0, inf))\n\n### llama_memory_seq_cp\n```c\nvoid llama_memory_seq_cp(\n    llama_memory_t mem,\n    llama_seq_id seq_id_src,\n    llama_seq_id seq_id_dst,\n    llama_pos p0,\n    llama_pos p1);\n```\nCopy all tokens that belong to the specified sequence to another sequence.\n\n**Parameters:**\n- `mem`: Memory handle\n- `seq_id_src`: Source sequence ID\n- `seq_id_dst`: Destination sequence ID\n- `p0`: Start position (< 0: [0, p1])\n- `p1`: End position (< 0: [p0, inf))\n\n### llama_memory_seq_keep\n```c\nvoid llama_memory_seq_keep(llama_memory_t mem, llama_seq_id seq_id);\n```\nRemove all tokens that do not belong to the specified sequence.\n\n### llama_memory_seq_add\n```c\nvoid llama_memory_seq_add(\n    llama_memory_t mem,\n    llama_seq_id seq_id,\n    llama_pos p0,\n    llama_pos p1,\n    llama_pos delta);\n```\nAdd relative position \"delta\" to all tokens that belong to the specified sequence and have positions in [p0, p1).\n\n**Parameters:**\n- `mem`: Memory handle\n- `seq_id`: Sequence ID\n- `p0`: Start position (< 0: [0, p1])\n- `p1`: End position (< 0: [p0, inf))\n- `delta`: Position delta to add\n\n### llama_memory_seq_div\n```c\nvoid llama_memory_seq_div(\n    llama_memory_t mem,\n    llama_seq_id seq_id,\n    llama_pos p0,\n    llama_pos p1,\n    int d);\n```\nInteger division of the positions by factor of `d > 1`.\n\n**Parameters:**\n- `mem`: Memory handle\n- `seq_id`: Sequence ID\n- `p0`: Start position (< 0: [0, p1])\n- `p1`: End position (< 0: [p0, inf))\n- `d`: Divisor (must be > 1)\n\n### llama_memory_seq_pos_min\n```c\nllama_pos llama_memory_seq_pos_min(llama_memory_t mem, llama_seq_id seq_id);\n```\nGet the smallest position present in the memory for the specified sequence. Returns -1 if the sequence is empty. Typically non-zero only for SWA caches.\n\n### llama_memory_seq_pos_max\n```c\nllama_pos llama_memory_seq_pos_max(llama_memory_t mem, llama_seq_id seq_id);\n```\nGet the largest position present in the memory for the specified sequence. Returns -1 if the sequence is empty.\n\n### llama_memory_can_shift\n```c\nbool llama_memory_can_shift(llama_memory_t mem);\n```\nCheck if the memory supports shifting.\n\n---\n\n## State & Session Management\n\n### llama_state_get_size\n```c\nsize_t llama_state_get_size(struct llama_context * ctx);\n```\nGet the actual size in bytes of the state (logits, embedding, and memory). Only use when saving the state.\n\n### llama_state_get_data\n```c\nsize_t llama_state_get_data(\n    struct llama_context * ctx,\n    uint8_t * dst,\n    size_t size);\n```\nCopy the state to the specified destination address. Returns the number of bytes copied.\n\n**Parameters:**\n- `ctx`: Context\n- `dst`: Destination buffer (must have enough memory allocated)\n- `size`: Size of destination buffer\n\n### llama_state_set_data\n```c\nsize_t llama_state_set_data(\n    struct llama_context * ctx,\n    const uint8_t * src,\n    size_t size);\n```\nSet the state from the specified address. Returns the number of bytes read.\n\n### llama_state_load_file\n```c\nbool llama_state_load_file(\n    struct llama_context * ctx,\n    const char * path_session,\n    llama_token * tokens_out,\n    size_t n_token_capacity,\n    size_t * n_token_count_out);\n```\nLoad session from file.\n\n### llama_state_save_file\n```c\nbool llama_state_save_file(\n    struct llama_context * ctx,\n    const char * path_session,\n    const llama_token * tokens,\n    size_t n_token_count);\n```\nSave session to file.\n\n### llama_state_seq_get_size\n```c\nsize_t llama_state_seq_get_size(\n    struct llama_context * ctx,\n    llama_seq_id seq_id);\n```\nGet the exact size needed to copy the state of a single sequence.\n\n### llama_state_seq_get_data\n```c\nsize_t llama_state_seq_get_data(\n    struct llama_context * ctx,\n    uint8_t * dst,\n    size_t size,\n    llama_seq_id seq_id);\n```\nCopy the state of a single sequence into the specified buffer.\n\n### llama_state_seq_set_data\n```c\nsize_t llama_state_seq_set_data(\n    struct llama_context * ctx,\n    const uint8_t * src,\n    size_t size,\n    llama_seq_id dest_seq_id);\n```\nCopy sequence data into the specified sequence. Returns positive on success, zero on failure.\n\n### llama_state_seq_save_file\n```c\nsize_t llama_state_seq_save_file(\n    struct llama_context * ctx,\n    const char * filepath,\n    llama_seq_id seq_id,\n    const llama_token * tokens,\n    size_t n_token_count);\n```\nSave sequence state to file.\n\n### llama_state_seq_load_file\n```c\nsize_t llama_state_seq_load_file(\n    struct llama_context * ctx,\n    const char * filepath,\n    llama_seq_id dest_seq_id,\n    llama_token * tokens_out,\n    size_t n_token_capacity,\n    size_t * n_token_count_out);\n```\nLoad sequence state from file.\n\n### llama_state_seq_get_size_ext\n```c\nsize_t llama_state_seq_get_size_ext(\n    struct llama_context * ctx,\n    llama_seq_id seq_id,\n    llama_state_seq_flags flags);\n```\nGet size of sequence state with flags (e.g., `LLAMA_STATE_SEQ_FLAGS_PARTIAL_ONLY` for SWA/recurrent cache only).\n\n### llama_state_seq_get_data_ext\n```c\nsize_t llama_state_seq_get_data_ext(\n    struct llama_context * ctx,\n    uint8_t * dst,\n    size_t size,\n    llama_seq_id seq_id,\n    llama_state_seq_flags flags);\n```\nGet sequence state data with flags.\n\n### llama_state_seq_set_data_ext\n```c\nsize_t llama_state_seq_set_data_ext(\n    struct llama_context * ctx,\n    const uint8_t * src,\n    size_t size,\n    llama_seq_id dest_seq_id,\n    llama_state_seq_flags flags);\n```\nSet sequence state data with flags.\n\n---\n\n## Batch Operations\n\n### llama_batch_get_one\n```c\nstruct llama_batch llama_batch_get_one(\n    llama_token * tokens,\n    int32_t n_tokens);\n```\nReturn batch for single sequence of tokens. The sequence ID will be fixed to 0. The position of tokens will be tracked automatically. This is a helper function to facilitate transition to the new batch API - avoid using it for new code.\n\n**Usage:**\n```c\nllama_token tokens[] = {1, 2, 3, 4, 5};\nstruct llama_batch batch = llama_batch_get_one(tokens, 5);\nllama_decode(ctx, batch);\n```\n\n### llama_batch_init\n```c\nstruct llama_batch llama_batch_init(\n    int32_t n_tokens,\n    int32_t embd,\n    int32_t n_seq_max);\n```\nAllocate a batch of tokens on the heap. Must be freed with `llama_batch_free()`.\n\n**Parameters:**\n- `n_tokens`: Maximum number of tokens\n- `embd`: If != 0, allocate `llama_batch.embd` with size `n_tokens * embd * sizeof(float)`. Otherwise, allocate `llama_batch.token` to store `n_tokens` token IDs\n- `n_seq_max`: Maximum number of sequences each token can be assigned to\n\n**Usage:**\n```c\nstruct llama_batch batch = llama_batch_init(512, 0, 1);\n// Use batch...\nllama_batch_free(batch);\n```\n\n### llama_batch_free\n```c\nvoid llama_batch_free(struct llama_batch batch);\n```\nFree a batch allocated with `llama_batch_init()`.\n\n---\n\n## Inference & Decoding\n\n### llama_encode\n```c\nint32_t llama_encode(\n    struct llama_context * ctx,\n    struct llama_batch batch);\n```\nProcess a batch of tokens without using KV cache. For encoder-decoder models, processes the batch using the encoder and stores the output internally for later use by the decoder's cross-attention layers.\n\n**Returns:**\n- `0`: Success\n- `< 0`: Error (memory state is restored)\n\n### llama_decode\n```c\nint32_t llama_decode(\n    struct llama_context * ctx,\n    struct llama_batch batch);\n```\nProcess a batch of tokens. Requires the context to have memory. For encoder-decoder models, processes using the decoder.\n\n**Returns:**\n- `0`: Success\n- `1`: Could not find a KV slot (try reducing batch size or increase context)\n- `2`: Aborted (processed ubatches remain in memory)\n- `-1`: Invalid input batch\n- `< -1`: Fatal error (processed ubatches remain in memory)\n\n**Usage:**\n```c\nstruct llama_batch batch = llama_batch_get_one(tokens, n_tokens);\nint ret = llama_decode(ctx, batch);\nif (ret != 0) {\n    // Handle error\n}\n```\n\n### llama_set_n_threads\n```c\nvoid llama_set_n_threads(\n    struct llama_context * ctx,\n    int32_t n_threads,\n    int32_t n_threads_batch);\n```\nSet the number of threads used for decoding.\n\n**Parameters:**\n- `ctx`: Context\n- `n_threads`: Number of threads for generation (single token)\n- `n_threads_batch`: Number of threads for batch processing (multiple tokens)\n\n### llama_n_threads\n```c\nint32_t llama_n_threads(struct llama_context * ctx);\n```\nGet the number of threads used for generation of a single token.\n\n### llama_n_threads_batch\n```c\nint32_t llama_n_threads_batch(struct llama_context * ctx);\n```\nGet the number of threads used for batch processing.\n\n### llama_set_embeddings\n```c\nvoid llama_set_embeddings(struct llama_context * ctx, bool embeddings);\n```\nSet whether the context outputs embeddings or not.\n\n### llama_set_causal_attn\n```c\nvoid llama_set_causal_attn(struct llama_context * ctx, bool causal_attn);\n```\nSet whether to use causal attention or not. If set to true, the model will only attend to past tokens.\n\n### llama_set_warmup\n```c\nvoid llama_set_warmup(struct llama_context * ctx, bool warmup);\n```\nSet whether the model is in warmup mode. If true, all model tensors are activated during `llama_decode()` to load and cache their weights.\n\n### llama_set_abort_callback\n```c\nvoid llama_set_abort_callback(\n    struct llama_context * ctx,\n    ggml_abort_callback abort_callback,\n    void * abort_callback_data);\n```\nSet abort callback. If it returns true, execution will be aborted (currently only works with CPU execution).\n\n### llama_synchronize\n```c\nvoid llama_synchronize(struct llama_context * ctx);\n```\nWait until all computations are finished. Automatically done when obtaining results, not usually necessary to call explicitly.\n\n### llama_get_logits\n```c\nfloat * llama_get_logits(struct llama_context * ctx);\n```\nGet token logits from the last `llama_decode()` call. Logits for which `llama_batch.logits[i] != 0` are stored contiguously.\n\n**Returns:** Pointer to logits array. Shape: `[n_outputs, n_vocab]`\n\n### llama_get_logits_ith\n```c\nfloat * llama_get_logits_ith(struct llama_context * ctx, int32_t i);\n```\nGet logits for the i-th token. Negative indices access logits in reverse order (-1 is the last token). Returns NULL for invalid IDs.\n\n**Usage:**\n```c\nfloat * logits = llama_get_logits_ith(ctx, -1);  // Get logits for last token\n```\n\n### llama_get_embeddings\n```c\nfloat * llama_get_embeddings(struct llama_context * ctx);\n```\nGet all output token embeddings. Returns NULL when `pooling_type == LLAMA_POOLING_TYPE_NONE` with generative models.\n\n**Returns:** Pointer to embeddings array. Shape: `[n_outputs * n_embd]`\n\n### llama_get_embeddings_ith\n```c\nfloat * llama_get_embeddings_ith(struct llama_context * ctx, int32_t i);\n```\nGet embeddings for the i-th token. Negative indices can be used (-1 is last). Returns NULL for invalid IDs.\n\n**Returns:** Shape: `[n_embd]`\n\n### llama_get_embeddings_seq\n```c\nfloat * llama_get_embeddings_seq(\n    struct llama_context * ctx,\n    llama_seq_id seq_id);\n```\nGet embeddings for a sequence ID. Returns NULL if `pooling_type` is `LLAMA_POOLING_TYPE_NONE`. For `LLAMA_POOLING_TYPE_RANK`, returns `float[n_cls_out]` with rank(s).\n\n**Returns:** Shape: `[n_embd]` or `[n_cls_out]` for ranking models\n\n---\n\n## Vocabulary & Tokenization\n\n### llama_vocab_type\n```c\nenum llama_vocab_type llama_vocab_type(const struct llama_vocab * vocab);\n```\nGet the vocabulary type (SPM, BPE, WPM, UGM, RWKV, PLAMO2).\n\n### llama_vocab_n_tokens\n```c\nint32_t llama_vocab_n_tokens(const struct llama_vocab * vocab);\n```\nGet the number of tokens in the vocabulary.\n\n### llama_vocab_get_text\n```c\nconst char * llama_vocab_get_text(\n    const struct llama_vocab * vocab,\n    llama_token token);\n```\nGet the text representation of a token.\n\n### llama_vocab_get_score\n```c\nfloat llama_vocab_get_score(\n    const struct llama_vocab * vocab,\n    llama_token token);\n```\nGet the score of a token.\n\n### llama_vocab_get_attr\n```c\nenum llama_token_attr llama_vocab_get_attr(\n    const struct llama_vocab * vocab,\n    llama_token token);\n```\nGet the attributes of a token (bitfield of `llama_token_attr`).\n\n### llama_vocab_is_eog\n```c\nbool llama_vocab_is_eog(\n    const struct llama_vocab * vocab,\n    llama_token token);\n```\nCheck if the token is an end-of-generation token (EOS, EOT, etc.).\n\n### llama_vocab_is_control\n```c\nbool llama_vocab_is_control(\n    const struct llama_vocab * vocab,\n    llama_token token);\n```\nCheck if the token is a control token or a renderable token.\n\n### Special Token Functions\n\nGet special token IDs:\n\n```c\nllama_token llama_vocab_bos(const struct llama_vocab * vocab);   // beginning-of-sentence\nllama_token llama_vocab_eos(const struct llama_vocab * vocab);   // end-of-sentence\nllama_token llama_vocab_eot(const struct llama_vocab * vocab);   // end-of-turn\nllama_token llama_vocab_sep(const struct llama_vocab * vocab);   // sentence separator\nllama_token llama_vocab_nl(const struct llama_vocab * vocab);    // next-line\nllama_token llama_vocab_pad(const struct llama_vocab * vocab);   // padding\nllama_token llama_vocab_mask(const struct llama_vocab * vocab);  // mask\n```\n\nCheck if special tokens should be added:\n\n```c\nbool llama_vocab_get_add_bos(const struct llama_vocab * vocab);\nbool llama_vocab_get_add_eos(const struct llama_vocab * vocab);\nbool llama_vocab_get_add_sep(const struct llama_vocab * vocab);\n```\n\nFill-in-the-middle tokens:\n\n```c\nllama_token llama_vocab_fim_pre(const struct llama_vocab * vocab);\nllama_token llama_vocab_fim_suf(const struct llama_vocab * vocab);\nllama_token llama_vocab_fim_mid(const struct llama_vocab * vocab);\nllama_token llama_vocab_fim_pad(const struct llama_vocab * vocab);\nllama_token llama_vocab_fim_rep(const struct llama_vocab * vocab);\nllama_token llama_vocab_fim_sep(const struct llama_vocab * vocab);\n```\n\n### llama_tokenize\n```c\nint32_t llama_tokenize(\n    const struct llama_vocab * vocab,\n    const char * text,\n    int32_t text_len,\n    llama_token * tokens,\n    int32_t n_tokens_max,\n    bool add_special,\n    bool parse_special);\n```\nConvert text into tokens.\n\n**Parameters:**\n- `vocab`: Vocabulary\n- `text`: Text to tokenize\n- `text_len`: Length of text\n- `tokens`: Output buffer (must be large enough)\n- `n_tokens_max`: Maximum number of tokens\n- `add_special`: Allow adding BOS and EOS tokens if model is configured to do so\n- `parse_special`: Allow tokenizing special/control tokens (otherwise treated as plaintext)\n\n**Returns:**\n- Positive: Number of tokens (no more than `n_tokens_max`)\n- Negative: Number of tokens that would have been returned (buffer too small)\n- `INT32_MIN`: Overflow\n\n**Usage:**\n```c\nconst char * text = \"Hello, world!\";\nllama_token tokens[128];\nint n = llama_tokenize(vocab, text, strlen(text), tokens, 128, true, false);\nif (n < 0) {\n    // Buffer too small, need -n tokens\n}\n```\n\n### llama_token_to_piece\n```c\nint32_t llama_token_to_piece(\n    const struct llama_vocab * vocab,\n    llama_token token,\n    char * buf,\n    int32_t length,\n    int32_t lstrip,\n    bool special);\n```\nConvert a token ID to text. Does not write null terminator.\n\n**Parameters:**\n- `vocab`: Vocabulary\n- `token`: Token ID\n- `buf`: Output buffer\n- `length`: Buffer length\n- `lstrip`: Number of leading spaces to skip (useful when encoding/decoding multiple tokens)\n- `special`: If true, special tokens are rendered\n\n### llama_detokenize\n```c\nint32_t llama_detokenize(\n    const struct llama_vocab * vocab,\n    const llama_token * tokens,\n    int32_t n_tokens,\n    char * text,\n    int32_t text_len_max,\n    bool remove_special,\n    bool unparse_special);\n```\nConvert tokens back into text.\n\n**Parameters:**\n- `vocab`: Vocabulary\n- `tokens`: Array of tokens\n- `n_tokens`: Number of tokens\n- `text`: Output buffer\n- `text_len_max`: Maximum text length\n- `remove_special`: Remove BOS and EOS tokens if model is configured to do so\n- `unparse_special`: If true, special tokens are rendered\n\n**Returns:**\n- Positive: Number of chars/bytes (no more than `text_len_max`)\n- Negative: Number of chars/bytes that would have been returned\n\n---\n\n## Chat Templates\n\n### llama_chat_apply_template\n```c\nint32_t llama_chat_apply_template(\n    const char * tmpl,\n    const struct llama_chat_message * chat,\n    size_t n_msg,\n    bool add_ass,\n    char * buf,\n    int32_t length);\n```\nApply chat template to format a conversation. Does not use a Jinja parser - only supports a pre-defined list of templates.\n\n**Parameters:**\n- `tmpl`: Jinja template (NULL to use model's default)\n- `chat`: Array of chat messages\n- `n_msg`: Number of messages\n- `add_ass`: Whether to end prompt with assistant message start token(s)\n- `buf`: Output buffer (recommended size: 2 * total characters of all messages)\n- `length`: Buffer size\n\n**Returns:** Total number of bytes of the formatted prompt\n\n**Usage:**\n```c\nllama_chat_message messages[] = {\n    {\"system\", \"You are a helpful assistant.\"},\n    {\"user\", \"Hello!\"}\n};\nchar buf[1024];\nint len = llama_chat_apply_template(NULL, messages, 2, true, buf, 1024);\n```\n\n### llama_chat_builtin_templates\n```c\nint32_t llama_chat_builtin_templates(const char ** output, size_t len);\n```\nGet list of built-in chat templates.\n\n---\n\n## Sampling\n\nSampling in llama.cpp uses a chain architecture where multiple samplers can be combined.\n\n### Core Sampler Functions\n\n```c\nstruct llama_sampler * llama_sampler_init(\n    struct llama_sampler_i * iface,\n    llama_sampler_context_t ctx);\n```\nInitialize a custom sampler (for advanced users implementing custom sampling).\n\n```c\nconst char * llama_sampler_name(const struct llama_sampler * smpl);\n```\nGet the name of a sampler.\n\n```c\nvoid llama_sampler_accept(struct llama_sampler * smpl, llama_token token);\n```\nAccept a token (updates sampler state).\n\n```c\nvoid llama_sampler_apply(\n    struct llama_sampler * smpl,\n    llama_token_data_array * cur_p);\n```\nApply the sampler to modify the token data array.\n\n```c\nvoid llama_sampler_reset(struct llama_sampler * smpl);\n```\nReset the sampler state.\n\n```c\nstruct llama_sampler * llama_sampler_clone(const struct llama_sampler * smpl);\n```\nClone a sampler.\n\n```c\nvoid llama_sampler_free(struct llama_sampler * smpl);\n```\nFree a sampler. **Important:** Do not free if added to a chain via `llama_sampler_chain_add()`.\n\n### Sampler Chain\n\n```c\nstruct llama_sampler * llama_sampler_chain_init(\n    struct llama_sampler_chain_params params);\n```\nInitialize a sampler chain.\n\n**Usage:**\n```c\nstruct llama_sampler_chain_params params = llama_sampler_chain_default_params();\nstruct llama_sampler * chain = llama_sampler_chain_init(params);\n```\n\n```c\nvoid llama_sampler_chain_add(\n    struct llama_sampler * chain,\n    struct llama_sampler * smpl);\n```\nAdd a sampler to the chain. **Important:** The chain takes ownership and will free the sampler.\n\n```c\nstruct llama_sampler * llama_sampler_chain_get(\n    struct llama_sampler * chain,\n    int32_t i);\n```\nGet the i-th sampler in the chain. Returns NULL if:\n- the sampler is NULL\n- the sampler is not a `llama_sampler_chain`\n- the index is out of bounds, unless i == -1\n- if i == -1, returns the chain itself (can check if sampler is a chain)\n\n```c\nint llama_sampler_chain_n(const struct llama_sampler * chain);\n```\nGet the number of samplers in the chain.\n\n```c\nstruct llama_sampler * llama_sampler_chain_remove(\n    struct llama_sampler * chain,\n    int32_t i);\n```\nRemove a sampler from the chain. The chain no longer owns it and will not free it.\n\n### Built-in Samplers\n\n#### Basic Samplers\n\n```c\nstruct llama_sampler * llama_sampler_init_greedy(void);\n```\nGreedy sampling (always pick the most likely token).\n\n```c\nstruct llama_sampler * llama_sampler_init_dist(uint32_t seed);\n```\nSample from the probability distribution.\n\n#### Top-K Sampling\n\n```c\nstruct llama_sampler * llama_sampler_init_top_k(int32_t k);\n```\nTop-K sampling. Setting k <= 0 makes this a noop.\n\n**Reference:** \"The Curious Case of Neural Text Degeneration\" (https://arxiv.org/abs/1904.09751)\n\n#### Top-P (Nucleus) Sampling\n\n```c\nstruct llama_sampler * llama_sampler_init_top_p(float p, size_t min_keep);\n```\nNucleus sampling.\n\n**Parameters:**\n- `p`: Cumulative probability threshold\n- `min_keep`: Minimum number of tokens to keep\n\n**Reference:** \"The Curious Case of Neural Text Degeneration\" (https://arxiv.org/abs/1904.09751)\n\n#### Min-P Sampling\n\n```c\nstruct llama_sampler * llama_sampler_init_min_p(float p, size_t min_keep);\n```\nMinimum P sampling.\n\n**Reference:** https://github.com/ggml-org/llama.cpp/pull/3841\n\n#### Typical Sampling\n\n```c\nstruct llama_sampler * llama_sampler_init_typical(float p, size_t min_keep);\n```\nLocally Typical Sampling.\n\n**Reference:** https://arxiv.org/abs/2202.00666\n\n#### Temperature\n\n```c\nstruct llama_sampler * llama_sampler_init_temp(float t);\n```\nTemperature sampling. Updates logits: `l_i' = l_i/t`. When t <= 0, max logit is kept, rest set to -inf.\n\n```c\nstruct llama_sampler * llama_sampler_init_temp_ext(\n    float t,\n    float delta,\n    float exponent);\n```\nDynamic temperature (entropy-based).\n\n**Reference:** https://arxiv.org/abs/2309.02772\n\n#### XTC Sampler\n\n```c\nstruct llama_sampler * llama_sampler_init_xtc(\n    float p,\n    float t,\n    size_t min_keep,\n    uint32_t seed);\n```\nXTC sampler.\n\n**Reference:** https://github.com/oobabooga/text-generation-webui/pull/6335\n\n#### Top-nσ Sampling\n\n```c\nstruct llama_sampler * llama_sampler_init_top_n_sigma(float n);\n```\nTop-nσ sampling.\n\n**Reference:** \"Top-nσ: Not All Logits Are You Need\" (https://arxiv.org/pdf/2411.07641)\n\n#### Mirostat\n\n```c\nstruct llama_sampler * llama_sampler_init_mirostat(\n    int32_t n_vocab,\n    uint32_t seed,\n    float tau,\n    float eta,\n    int32_t m);\n```\nMirostat 1.0 algorithm.\n\n**Parameters:**\n- `n_vocab`: Vocabulary size\n- `seed`: Random seed\n- `tau`: Target cross-entropy\n- `eta`: Learning rate\n- `m`: Number of tokens considered (paper uses m=100)\n\n**Reference:** https://arxiv.org/abs/2007.14966\n\n```c\nstruct llama_sampler * llama_sampler_init_mirostat_v2(\n    uint32_t seed,\n    float tau,\n    float eta);\n```\nMirostat 2.0 algorithm.\n\n#### Grammar\n\n```c\nstruct llama_sampler * llama_sampler_init_grammar(\n    const struct llama_vocab * vocab,\n    const char * grammar_str,\n    const char * grammar_root);\n```\nInitialize a GBNF grammar sampler. Returns NULL if parsing fails.\n\n**Parameters:**\n- `vocab`: Vocabulary\n- `grammar_str`: Production rules as a string\n- `grammar_root`: Name of the start symbol\n\n```c\nstruct llama_sampler * llama_sampler_init_grammar_lazy_patterns(\n    const struct llama_vocab * vocab,\n    const char * grammar_str,\n    const char * grammar_root,\n    const char ** trigger_patterns,\n    size_t num_trigger_patterns,\n    const llama_token * trigger_tokens,\n    size_t num_trigger_tokens);\n```\nLazy grammar sampler (triggers based on patterns or tokens).\n\n**Reference:** https://github.com/ggml-org/llama.cpp/pull/9639\n\n#### Penalties\n\n```c\nstruct llama_sampler * llama_sampler_init_penalties(\n    int32_t penalty_last_n,\n    float penalty_repeat,\n    float penalty_freq,\n    float penalty_present);\n```\nApply repetition penalties. **Note:** Avoid using on full vocabulary (slow). Apply top-k or top-p first.\n\n**Parameters:**\n- `penalty_last_n`: Last n tokens to penalize (0 = disabled, -1 = context size)\n- `penalty_repeat`: Repeat penalty (1.0 = disabled)\n- `penalty_freq`: Frequency penalty (0.0 = disabled)\n- `penalty_present`: Presence penalty (0.0 = disabled)\n\n#### DRY Sampler\n\n```c\nstruct llama_sampler * llama_sampler_init_dry(\n    const struct llama_vocab * vocab,\n    int32_t n_ctx_train,\n    float dry_multiplier,\n    float dry_base,\n    int32_t dry_allowed_length,\n    int32_t dry_penalty_last_n,\n    const char ** seq_breakers,\n    size_t num_breakers);\n```\nDRY (Don't Repeat Yourself) sampler.\n\n**Reference:** https://github.com/oobabooga/text-generation-webui/pull/5677\n\n#### Adaptive-P Sampler\n\n```c\nstruct llama_sampler * llama_sampler_init_adaptive_p(\n    float target,\n    float decay,\n    uint32_t seed);\n```\nAdaptive-P sampler - selects tokens near a configurable target probability over time.\n\nThe sampler transforms the token probability distribution to favor tokens near a user-configurable probability target. Internally maintains an exponential moving average (EMA) of original probabilities of selected tokens, using this to compute an adapted target at each step.\n\n**Parameters:**\n- `target`: Select tokens near this probability (valid range 0.0 to 1.0; negative = disabled)\n- `decay`: EMA decay for adaptation; history ≈ 1/(1-decay) tokens (valid range 0.0 - 0.99)\n- `seed`: Random seed. Use `LLAMA_DEFAULT_SEED` for a random seed.\n\n**Important:** This sampler selects a token ID (like mirostat, dist, greedy), so it must be **last in the sampler chain**. Only mild truncation before this sampler is recommended - use min-p as the only other active sampler.\n\n**Reference:** https://github.com/ggml-org/llama.cpp/pull/17927\n\n#### Logit Bias\n\n```c\nstruct llama_sampler * llama_sampler_init_logit_bias(\n    int32_t n_vocab,\n    int32_t n_logit_bias,\n    const llama_logit_bias * logit_bias);\n```\nApply logit biases to specific tokens.\n\n#### Infill Sampler\n\n```c\nstruct llama_sampler * llama_sampler_init_infill(\n    const struct llama_vocab * vocab);\n```\nFill-in-the-middle infilling sampler. Use after top-k + top-p sampling.\n\n### Sampling Functions\n\n```c\nuint32_t llama_sampler_get_seed(const struct llama_sampler * smpl);\n```\nGet the seed used by the sampler (if applicable), otherwise `LLAMA_DEFAULT_SEED`.\n\n```c\nllama_token llama_sampler_sample(\n    struct llama_sampler * smpl,\n    struct llama_context * ctx,\n    int32_t idx);\n```\nSample and accept a token from the idx-th output of the last evaluation.\n\n**Usage:**\n```c\n// Setup sampler chain\nstruct llama_sampler * sampler = llama_sampler_chain_init(\n    llama_sampler_chain_default_params());\nllama_sampler_chain_add(sampler, llama_sampler_init_top_k(50));\nllama_sampler_chain_add(sampler, llama_sampler_init_top_p(0.9, 1));\nllama_sampler_chain_add(sampler, llama_sampler_init_temp(0.8));\nllama_sampler_chain_add(sampler, llama_sampler_init_dist(LLAMA_DEFAULT_SEED));\n\n// Decoding loop\nwhile (...) {\n    llama_decode(ctx, batch);\n    llama_token token = llama_sampler_sample(sampler, ctx, -1);\n    // Use token...\n}\n\nllama_sampler_free(sampler);\n```\n\n### Backend Sampling API [EXPERIMENTAL]\n\nBackend sampling allows sampling operations to be performed directly on the GPU as part of the computation graph.\n\n**Note:** Use only if the `llama_context` was created with at least one `llama_sampler_seq_config`.\n\n#### llama_set_sampler\n```c\nbool llama_set_sampler(\n    struct llama_context * ctx,\n    llama_seq_id seq_id,\n    struct llama_sampler * smpl);\n```\nAttach a sampler to the context for a specific sequence. Prefer initializing the context with `llama_context_params.samplers` when possible.\n\n#### llama_get_sampled_token_ith\n```c\nllama_token llama_get_sampled_token_ith(struct llama_context * ctx, int32_t i);\n```\nGet the backend sampled token for the i-th token. Returns `LLAMA_TOKEN_NULL` if no token was sampled.\n\n#### llama_get_sampled_probs_ith / llama_get_sampled_probs_count_ith\n```c\nfloat * llama_get_sampled_probs_ith(struct llama_context * ctx, int32_t i);\nuint32_t llama_get_sampled_probs_count_ith(struct llama_context * ctx, int32_t i);\n```\nGet the backend sampled probabilities for the i-th token. Returns NULL if no probabilities were generated.\n\n#### llama_get_sampled_logits_ith / llama_get_sampled_logits_count_ith\n```c\nfloat * llama_get_sampled_logits_ith(struct llama_context * ctx, int32_t i);\nuint32_t llama_get_sampled_logits_count_ith(struct llama_context * ctx, int32_t i);\n```\nGet the backend sampled logits for the i-th token. Returns NULL if no logits were sampled.\n\n#### llama_get_sampled_candidates_ith / llama_get_sampled_candidates_count_ith\n```c\nllama_token * llama_get_sampled_candidates_ith(struct llama_context * ctx, int32_t i);\nuint32_t llama_get_sampled_candidates_count_ith(struct llama_context * ctx, int32_t i);\n```\nGet the backend sampled candidates (token ids) for the i-th token. Returns NULL if no candidates were sampled.\n\n---\n\n## LoRA Adapters\n\n### llama_adapter_lora_init\n```c\nstruct llama_adapter_lora * llama_adapter_lora_init(\n    struct llama_model * model,\n    const char * path_lora);\n```\nLoad a LoRA adapter from file.\n\n### llama_adapter_lora_free [DEPRECATED]\n```c\nvoid llama_adapter_lora_free(struct llama_adapter_lora * adapter);\n```\n**DEPRECATED:** Adapters are now automatically freed together with the associated model. This function is kept for backwards compatibility but should not be used in new code.\n\n### llama_adapter_meta_val_str\n```c\nint32_t llama_adapter_meta_val_str(\n    const struct llama_adapter_lora * adapter,\n    const char * key,\n    char * buf,\n    size_t buf_size);\n```\nGet adapter metadata value as a string by key.\n\n### llama_adapter_meta_count\n```c\nint32_t llama_adapter_meta_count(const struct llama_adapter_lora * adapter);\n```\nGet the number of metadata key/value pairs.\n\n### llama_adapter_meta_key_by_index\n```c\nint32_t llama_adapter_meta_key_by_index(\n    const struct llama_adapter_lora * adapter,\n    int32_t i,\n    char * buf,\n    size_t buf_size);\n```\nGet metadata key name by index.\n\n### llama_adapter_meta_val_str_by_index\n```c\nint32_t llama_adapter_meta_val_str_by_index(\n    const struct llama_adapter_lora * adapter,\n    int32_t i,\n    char * buf,\n    size_t buf_size);\n```\nGet metadata value as a string by index.\n\n### llama_adapter_get_alora_n_invocation_tokens\n```c\nuint64_t llama_adapter_get_alora_n_invocation_tokens(\n    const struct llama_adapter_lora * adapter);\n```\nGet the number of invocation tokens if this is an A-LoRA adapter.\n\n### llama_adapter_get_alora_invocation_tokens\n```c\nconst llama_token * llama_adapter_get_alora_invocation_tokens(\n    const struct llama_adapter_lora * adapter);\n```\nGet the invocation tokens if this is an A-LoRA adapter.\n\n### llama_set_adapter_lora\n```c\nint32_t llama_set_adapter_lora(\n    struct llama_context * ctx,\n    struct llama_adapter_lora * adapter,\n    float scale);\n```\nAdd a loaded LoRA adapter to the context. Does not modify model weights.\n\n**Parameters:**\n- `ctx`: Context\n- `adapter`: LoRA adapter\n- `scale`: Scaling factor\n\n### llama_rm_adapter_lora\n```c\nint32_t llama_rm_adapter_lora(\n    struct llama_context * ctx,\n    struct llama_adapter_lora * adapter);\n```\nRemove a specific LoRA adapter from the context. Returns -1 if not present.\n\n### llama_clear_adapter_lora\n```c\nvoid llama_clear_adapter_lora(struct llama_context * ctx);\n```\nRemove all LoRA adapters from the context.\n\n### llama_apply_adapter_cvec\n```c\nint32_t llama_apply_adapter_cvec(\n    struct llama_context * ctx,\n    const float * data,\n    size_t len,\n    int32_t n_embd,\n    int32_t il_start,\n    int32_t il_end);\n```\nApply a loaded control vector to the context. If `data` is NULL, clear the currently loaded vector.\n\n**Parameters:**\n- `ctx`: Context\n- `data`: Control vector data (n_embd x n_layers buffer starting from layer 1), or NULL to clear\n- `len`: Length of data\n- `n_embd`: Size of a single layer's control\n- `il_start`: Start layer (inclusive)\n- `il_end`: End layer (inclusive)\n\n---\n\n## Performance & Utilities\n\n### System Information\n\n```c\nconst char * llama_print_system_info(void);\n```\nGet system information as a string.\n\n```c\nint64_t llama_time_us(void);\n```\nGet current time in microseconds.\n\n```c\nsize_t llama_max_devices(void);\n```\nGet the maximum number of devices.\n\n```c\nsize_t llama_max_parallel_sequences(void);\n```\nGet the maximum number of parallel sequences.\n\n```c\nsize_t llama_max_tensor_buft_overrides(void);\n```\nGet the maximum number of tensor buffer type overrides.\n\n```c\nbool llama_supports_mmap(void);\n```\nCheck if mmap is supported.\n\n```c\nbool llama_supports_mlock(void);\n```\nCheck if mlock is supported.\n\n```c\nbool llama_supports_gpu_offload(void);\n```\nCheck if GPU offload is supported.\n\n```c\nbool llama_supports_rpc(void);\n```\nCheck if RPC is supported.\n\n### Performance Measurement\n\n```c\nstruct llama_perf_context_data llama_perf_context(\n    const struct llama_context * ctx);\n```\nGet performance data for the context.\n\n```c\nvoid llama_perf_context_print(const struct llama_context * ctx);\n```\nPrint performance statistics for the context.\n\n```c\nvoid llama_perf_context_reset(struct llama_context * ctx);\n```\nReset performance counters for the context.\n\n```c\nstruct llama_perf_sampler_data llama_perf_sampler(\n    const struct llama_sampler * chain);\n```\nGet performance data for the sampler chain. **Note:** Only works with samplers constructed via `llama_sampler_chain_init()`.\n\n```c\nvoid llama_perf_sampler_print(const struct llama_sampler * chain);\n```\nPrint performance statistics for the sampler.\n\n```c\nvoid llama_perf_sampler_reset(struct llama_sampler * chain);\n```\nReset performance counters for the sampler.\n\n```c\nvoid llama_memory_breakdown_print(const struct llama_context * ctx);\n```\nPrint a breakdown of per-device memory use via `LLAMA_LOG`.\n\n### Logging\n\n```c\nvoid llama_log_get(ggml_log_callback * log_callback, void ** user_data);\n```\nGet the current log callback and user data.\n\n```c\nvoid llama_log_set(ggml_log_callback log_callback, void * user_data);\n```\nSet callback for all future logging events. If NULL, everything is output on stderr. **Note:** Logger state is global, so these functions are NOT thread-safe.\n\n### Model Split Utilities\n\n```c\nint32_t llama_split_path(\n    char * split_path,\n    size_t maxlen,\n    const char * path_prefix,\n    int32_t split_no,\n    int32_t split_count);\n```\nBuild a split GGUF file path for a chunk.\n\n**Example:**\n```c\nchar split_path[256];\nllama_split_path(split_path, 256, \"/models/ggml-model-q4_0\", 2, 4);\n// Result: \"/models/ggml-model-q4_0-00002-of-00004.gguf\"\n```\n\n```c\nint32_t llama_split_prefix(\n    char * split_prefix,\n    size_t maxlen,\n    const char * split_path,\n    int32_t split_no,\n    int32_t split_count);\n```\nExtract the path prefix from a split path if and only if split_no and split_count match.\n\n### Flash Attention\n\n```c\nconst char * llama_flash_attn_type_name(enum llama_flash_attn_type flash_attn_type);\n```\nGet the name of a flash attention type.\n\n---\n\n## Training\n\n### llama_opt_param_filter_all\n```c\nbool llama_opt_param_filter_all(\n    const struct ggml_tensor * tensor,\n    void * userdata);\n```\nParameter filter that always returns true (all tensors contain trainable parameters).\n\n### llama_opt_init\n```c\nvoid llama_opt_init(\n    struct llama_context * lctx,\n    struct llama_model * model,\n    struct llama_opt_params lopt_params);\n```\nInitialize optimization/training for a model.\n\n**Parameters:**\n- `lctx`: Context\n- `model`: Model to train\n- `lopt_params`: Optimization parameters\n\n### llama_opt_epoch\n```c\nvoid llama_opt_epoch(\n    struct llama_context * lctx,\n    ggml_opt_dataset_t dataset,\n    ggml_opt_result_t result_train,\n    ggml_opt_result_t result_eval,\n    int64_t idata_split,\n    ggml_opt_epoch_callback callback_train,\n    ggml_opt_epoch_callback callback_eval);\n```\nRun a training epoch.\n\n**Parameters:**\n- `lctx`: Context\n- `dataset`: Training dataset\n- `result_train`: Training results\n- `result_eval`: Evaluation results\n- `idata_split`: Data split index\n- `callback_train`: Training callback\n- `callback_eval`: Evaluation callback\n\n---\n\n## Important Constants\n\n```c\n#define LLAMA_DEFAULT_SEED 0xFFFFFFFF\n#define LLAMA_TOKEN_NULL -1\n```\n\n## Key Data Structures\n\n### llama_batch\nInput data for `llama_encode`/`llama_decode`:\n- `n_tokens`: Number of tokens in the batch\n- `token`: Token IDs (used when `embd` is NULL)\n- `embd`: Token embeddings (used when `token` is NULL)\n- `pos`: Token positions (NULL for automatic tracking)\n- `seq_id`: Sequence IDs for each token (NULL defaults to sequence 0)\n- `logits`: Whether to output logits for each token (NULL outputs last token only for generation)\n\n### llama_model_params\nModel loading parameters (get defaults via `llama_model_default_params()`):\n- `devices`: NULL-terminated list of devices for offloading\n- `n_gpu_layers`: Number of layers to store in VRAM\n- `split_mode`: How to split the model across GPUs\n- `vocab_only`: Only load vocabulary, no weights\n- `use_mmap`: Use mmap if possible\n- `use_direct_io`: Use direct I/O when supported (takes precedence over use_mmap)\n- `use_mlock`: Force system to keep model in RAM\n\n### llama_context_params\nContext parameters (get defaults via `llama_context_default_params()`):\n- `n_ctx`: Text context size (0 = from model)\n- `n_batch`: Logical maximum batch size\n- `n_ubatch`: Physical maximum batch size\n- `n_seq_max`: Max number of sequences\n- `n_threads`: Threads for generation\n- `n_threads_batch`: Threads for batch processing\n- `embeddings`: Extract embeddings\n- `rope_scaling_type`: RoPE scaling type\n- `pooling_type`: Pooling type\n- `attention_type`: Attention type\n- `flash_attn_type`: Flash attention configuration\n\n### llama_token_data / llama_token_data_array\nUsed for sampling:\n- `llama_token_data`: Contains token ID, logit, and probability\n- `llama_token_data_array`: Array of token data with selection index and sorted flag\n",
        "plugins/llamacpp/skills/llamacpp/references/workflows.md": "# llama.cpp Workflows and Examples\n\nComplete working examples for common llama.cpp tasks. This document combines workflow patterns with production-ready applications.\n\n## Table of Contents\n\n### Basic Workflows (1-5)\n1. [Basic Text Generation](#1-basic-text-generation)\n2. [Chat with System Prompt](#2-chat-with-system-prompt)\n3. [Embeddings Extraction](#3-embeddings-extraction)\n4. [Batch Processing](#4-batch-processing)\n5. [Multiple Sequences](#5-multiple-sequences)\n\n### Intermediate Workflows (6-10)\n6. [Using LoRA Adapters](#6-using-lora-adapters)\n7. [State Management](#7-state-management)\n8. [Custom Sampling](#8-custom-sampling)\n9. [Encoder-Decoder Models](#9-encoder-decoder-models)\n10. [Memory Management Patterns](#10-memory-management-patterns)\n\n### Advanced Workflows - b7572 Features (11-13)\n11. [Advanced Sampling: XTC + DRY](#11-advanced-sampling-xtc--dry)\n12. [Per-Sequence State Management](#12-per-sequence-state-management)\n13. [Model Architecture Detection](#13-model-architecture-detection)\n\n### Production Examples (14-15)\n14. [Interactive Chat Application](#14-interactive-chat-application)\n15. [Streaming Generation](#15-streaming-generation)\n\n### Reference\n- [Best Practices](#best-practices)\n- [Compilation](#compilation)\n\n---\n\n## 1. Basic Text Generation\n\nComplete example of loading a model and generating text.\n\n```c\n#include \"llama.h\"\n#include <stdio.h>\n#include <string.h>\n#include <stdlib.h>\n\nint main() {\n    // 1. Initialize backend\n    llama_backend_init();\n\n    // 2. Load model\n    struct llama_model_params model_params = llama_model_default_params();\n    model_params.n_gpu_layers = 32;  // Offload layers to GPU\n\n    struct llama_model * model = llama_model_load_from_file(\n        \"model.gguf\",\n        model_params\n    );\n\n    if (!model) {\n        fprintf(stderr, \"Failed to load model\\n\");\n        return 1;\n    }\n\n    // 3. Create context\n    struct llama_context_params ctx_params = llama_context_default_params();\n    ctx_params.n_ctx = 4096;       // Context size\n    ctx_params.n_batch = 512;      // Batch size for prompt processing\n    ctx_params.n_threads = 8;      // Number of threads\n\n    struct llama_context * ctx = llama_init_from_model(model, ctx_params);\n\n    if (!ctx) {\n        fprintf(stderr, \"Failed to create context\\n\");\n        llama_model_free(model);\n        return 1;\n    }\n\n    // 4. Tokenize input\n    const char * prompt = \"Once upon a time\";\n    const struct llama_vocab * vocab = llama_model_get_vocab(model);\n\n    llama_token tokens[512];\n    int n_tokens = llama_tokenize(\n        vocab,\n        prompt,\n        strlen(prompt),\n        tokens,\n        512,\n        true,   // add_special (add BOS if needed)\n        false   // parse_special\n    );\n\n    if (n_tokens < 0) {\n        fprintf(stderr, \"Tokenization failed\\n\");\n        llama_free(ctx);\n        llama_model_free(model);\n        return 1;\n    }\n\n    // 5. Setup sampler\n    struct llama_sampler * sampler = llama_sampler_chain_init(\n        llama_sampler_chain_default_params()\n    );\n\n    llama_sampler_chain_add(sampler, llama_sampler_init_top_k(40));\n    llama_sampler_chain_add(sampler, llama_sampler_init_top_p(0.95, 1));\n    llama_sampler_chain_add(sampler, llama_sampler_init_temp(0.8));\n    llama_sampler_chain_add(sampler, llama_sampler_init_dist(1234));\n\n    // 6. Process prompt\n    struct llama_batch batch = llama_batch_get_one(tokens, n_tokens);\n\n    if (llama_decode(ctx, batch) != 0) {\n        fprintf(stderr, \"Failed to decode prompt\\n\");\n        llama_sampler_free(sampler);\n        llama_free(ctx);\n        llama_model_free(model);\n        return 1;\n    }\n\n    // 7. Generate tokens\n    int n_gen = 0;\n    int max_tokens = 100;\n\n    while (n_gen < max_tokens) {\n        // Sample next token\n        llama_token new_token = llama_sampler_sample(sampler, ctx, -1);\n\n        // Check for EOS\n        if (llama_vocab_is_eog(vocab, new_token)) {\n            break;\n        }\n\n        // Convert token to text and print\n        char buf[256];\n        int len = llama_token_to_piece(vocab, new_token, buf, 256, 0, false);\n        if (len > 0) {\n            fwrite(buf, 1, len, stdout);\n            fflush(stdout);\n        }\n\n        // Prepare batch for next token\n        batch = llama_batch_get_one(&new_token, 1);\n\n        if (llama_decode(ctx, batch) != 0) {\n            fprintf(stderr, \"Failed to decode\\n\");\n            break;\n        }\n\n        n_gen++;\n    }\n\n    printf(\"\\n\");\n\n    // 8. Cleanup\n    llama_sampler_free(sampler);\n    llama_free(ctx);\n    llama_model_free(model);\n    llama_backend_free();\n\n    return 0;\n}\n```\n\n---\n\n## 2. Chat with System Prompt\n\nUsing chat templates for conversational AI.\n\n```c\n#include \"llama.h\"\n#include <stdio.h>\n#include <string.h>\n\nint main() {\n    llama_backend_init();\n\n    // Load model (same as basic example)\n    struct llama_model_params model_params = llama_model_default_params();\n    struct llama_model * model = llama_model_load_from_file(\"model.gguf\", model_params);\n\n    struct llama_context_params ctx_params = llama_context_default_params();\n    ctx_params.n_ctx = 4096;\n    struct llama_context * ctx = llama_init_from_model(model, ctx_params);\n\n    // Setup chat messages\n    llama_chat_message messages[] = {\n        {\"system\", \"You are a helpful assistant.\"},\n        {\"user\", \"What is the capital of France?\"}\n    };\n\n    // Apply chat template\n    char prompt[2048];\n    int prompt_len = llama_chat_apply_template(\n        NULL,        // Use model's default template\n        messages,\n        2,           // Number of messages\n        true,        // Add assistant start token\n        prompt,\n        sizeof(prompt)\n    );\n\n    if (prompt_len < 0) {\n        fprintf(stderr, \"Chat template failed\\n\");\n        return 1;\n    }\n\n    printf(\"Formatted prompt:\\n%s\\n\", prompt);\n\n    // Tokenize and generate (same as basic example)\n    const struct llama_vocab * vocab = llama_model_get_vocab(model);\n    llama_token tokens[512];\n    int n_tokens = llama_tokenize(vocab, prompt, prompt_len, tokens, 512, true, false);\n\n    // ... continue with generation ...\n\n    llama_free(ctx);\n    llama_model_free(model);\n    llama_backend_free();\n\n    return 0;\n}\n```\n\n---\n\n## 3. Embeddings Extraction\n\nExtract embeddings for semantic search, similarity, and clustering tasks.\n\n```cpp\n#include \"llama.h\"\n#include <cstdio>\n#include <cmath>\n#include <vector>\n#include <string>\n\n// Normalize embeddings to unit length\nstatic void normalize_embeddings(float * emb, int n_embd) {\n    float norm = 0.0f;\n    for (int i = 0; i < n_embd; i++) {\n        norm += emb[i] * emb[i];\n    }\n    norm = sqrtf(norm);\n\n    if (norm > 0.0f) {\n        for (int i = 0; i < n_embd; i++) {\n            emb[i] /= norm;\n        }\n    }\n}\n\n// Compute cosine similarity\nstatic float cosine_similarity(const float * a, const float * b, int n_embd) {\n    float dot = 0.0f;\n    for (int i = 0; i < n_embd; i++) {\n        dot += a[i] * b[i];\n    }\n    return dot;\n}\n\nint main(int argc, char ** argv) {\n    const char * model_path = \"model.gguf\";\n\n    // Multiple texts to embed\n    std::vector<std::string> texts = {\n        \"The quick brown fox jumps over the lazy dog\",\n        \"A fast auburn fox leaps above an idle canine\",\n        \"Machine learning is a subset of artificial intelligence\",\n        \"Neural networks are inspired by biological neurons\"\n    };\n\n    ggml_backend_load_all();\n\n    // 1. Load model\n    llama_model_params model_params = llama_model_default_params();\n    model_params.n_gpu_layers = 99;\n\n    llama_model * model = llama_model_load_from_file(model_path, model_params);\n    if (!model) {\n        fprintf(stderr, \"Failed to load model\\n\");\n        return 1;\n    }\n\n    const llama_vocab * vocab = llama_model_get_vocab(model);\n    const int n_embd = llama_model_n_embd(model);\n\n    // 2. Create context with embeddings enabled\n    llama_context_params ctx_params = llama_context_default_params();\n    ctx_params.n_ctx = 2048;\n    ctx_params.n_batch = 2048;\n    ctx_params.embeddings = true;  // CRITICAL: Enable embeddings\n    ctx_params.pooling_type = LLAMA_POOLING_TYPE_MEAN;  // Mean pooling\n    ctx_params.attention_type = LLAMA_ATTENTION_TYPE_CAUSAL;\n\n    llama_context * ctx = llama_init_from_model(model, ctx_params);\n    if (!ctx) {\n        fprintf(stderr, \"Failed to create context\\n\");\n        llama_model_free(model);\n        return 1;\n    }\n\n    const llama_pooling_type pooling = llama_pooling_type(ctx);\n    llama_memory_t mem = llama_get_memory(ctx);\n\n    // 3. Process texts in batches\n    const int n_texts = texts.size();\n    std::vector<float> all_embeddings(n_texts * n_embd);\n\n    for (int i = 0; i < n_texts; i++) {\n        // Clear KV cache (not needed for embeddings)\n        llama_memory_clear(mem, true);\n\n        // Tokenize\n        const int n_tokens = -llama_tokenize(\n            vocab,\n            texts[i].c_str(),\n            texts[i].size(),\n            NULL,\n            0,\n            true,\n            true\n        );\n\n        std::vector<llama_token> tokens(n_tokens);\n        if (llama_tokenize(vocab, texts[i].c_str(), texts[i].size(),\n                          tokens.data(), tokens.size(), true, true) < 0) {\n            fprintf(stderr, \"Failed to tokenize text %d\\n\", i);\n            continue;\n        }\n\n        // Create batch for this sequence\n        llama_batch batch = llama_batch_get_one(tokens.data(), tokens.size());\n\n        // Decode\n        if (llama_decode(ctx, batch) < 0) {\n            fprintf(stderr, \"Failed to decode text %d\\n\", i);\n            continue;\n        }\n\n        // Get embeddings\n        float * emb = nullptr;\n\n        if (pooling == LLAMA_POOLING_TYPE_NONE) {\n            // Get token embeddings (last token)\n            emb = llama_get_embeddings_ith(ctx, -1);\n        } else {\n            // Get sequence embeddings (pooled)\n            emb = llama_get_embeddings_seq(ctx, 0);\n        }\n\n        if (!emb) {\n            fprintf(stderr, \"Failed to get embeddings for text %d\\n\", i);\n            continue;\n        }\n\n        // Copy and normalize\n        float * out = &all_embeddings[i * n_embd];\n        for (int j = 0; j < n_embd; j++) {\n            out[j] = emb[j];\n        }\n        normalize_embeddings(out, n_embd);\n\n        printf(\"Embedded text %d: \\\"%s\\\"\\n\", i, texts[i].c_str());\n        printf(\"  First 5 dimensions: \");\n        for (int j = 0; j < 5 && j < n_embd; j++) {\n            printf(\"%.4f \", out[j]);\n        }\n        printf(\"...\\n\");\n    }\n\n    // 4. Compute similarity matrix\n    printf(\"\\nCosine Similarity Matrix:\\n\");\n    printf(\"     \");\n    for (int i = 0; i < n_texts; i++) {\n        printf(\"  T%d  \", i);\n    }\n    printf(\"\\n\");\n\n    for (int i = 0; i < n_texts; i++) {\n        printf(\"T%d: \", i);\n        for (int j = 0; j < n_texts; j++) {\n            float sim = cosine_similarity(\n                &all_embeddings[i * n_embd],\n                &all_embeddings[j * n_embd],\n                n_embd\n            );\n            printf(\" %.3f\", sim);\n        }\n        printf(\"\\n\");\n    }\n\n    printf(\"\\nText pairs with high similarity (> 0.8):\\n\");\n    for (int i = 0; i < n_texts; i++) {\n        for (int j = i + 1; j < n_texts; j++) {\n            float sim = cosine_similarity(\n                &all_embeddings[i * n_embd],\n                &all_embeddings[j * n_embd],\n                n_embd\n            );\n            if (sim > 0.8f) {\n                printf(\"  Text %d <-> Text %d: %.3f\\n\", i, j, sim);\n                printf(\"    \\\"%s\\\"\\n\", texts[i].c_str());\n                printf(\"    \\\"%s\\\"\\n\", texts[j].c_str());\n            }\n        }\n    }\n\n    llama_free(ctx);\n    llama_model_free(model);\n\n    return 0;\n}\n```\n\n**Key Features:**\n- Embeddings context configuration\n- Support for different pooling types\n- Embedding normalization\n- Similarity computation\n\n---\n\n## 4. Batch Processing\n\nProcess multiple prompts in a single batch for efficiency.\n\n```c\n#include \"llama.h\"\n#include <stdio.h>\n#include <string.h>\n#include <stdlib.h>\n\nint main() {\n    llama_backend_init();\n\n    struct llama_model_params model_params = llama_model_default_params();\n    struct llama_model * model = llama_model_load_from_file(\"model.gguf\", model_params);\n\n    struct llama_context_params ctx_params = llama_context_default_params();\n    ctx_params.n_ctx = 2048;\n    ctx_params.n_batch = 512;\n    struct llama_context * ctx = llama_init_from_model(model, ctx_params);\n\n    const struct llama_vocab * vocab = llama_model_get_vocab(model);\n\n    // Multiple prompts\n    const char * prompts[] = {\n        \"The weather today is\",\n        \"In the year 2050,\",\n        \"Once upon a time\"\n    };\n    int n_prompts = 3;\n\n    // Allocate batch manually for multiple sequences\n    struct llama_batch batch = llama_batch_init(512, 0, n_prompts);\n\n    for (int i = 0; i < n_prompts; i++) {\n        llama_token tokens[128];\n        int n = llama_tokenize(vocab, prompts[i], strlen(prompts[i]), tokens, 128, true, false);\n\n        // Add tokens to batch\n        for (int j = 0; j < n; j++) {\n            batch.token[batch.n_tokens] = tokens[j];\n            batch.pos[batch.n_tokens] = j;\n            batch.n_seq_id[batch.n_tokens] = 1;\n            batch.seq_id[batch.n_tokens] = &((llama_seq_id[]){i})[0];  // Sequence i\n            batch.logits[batch.n_tokens] = (j == n - 1);  // Only last token outputs logits\n            batch.n_tokens++;\n        }\n    }\n\n    // Decode batch\n    if (llama_decode(ctx, batch) != 0) {\n        fprintf(stderr, \"Failed to decode batch\\n\");\n        return 1;\n    }\n\n    // Sample from each sequence\n    struct llama_sampler * sampler = llama_sampler_chain_init(\n        llama_sampler_chain_default_params()\n    );\n    llama_sampler_chain_add(sampler, llama_sampler_init_dist(1234));\n\n    for (int i = 0; i < n_prompts; i++) {\n        float * logits = llama_get_logits_ith(ctx, i);\n        int n_vocab = llama_vocab_n_tokens(vocab);\n        llama_token_data_array candidates = {\n            .data = malloc(n_vocab * sizeof(llama_token_data)),\n            .size = n_vocab,\n            .selected = -1,\n            .sorted = false\n        };\n\n        for (int j = 0; j < n_vocab; j++) {\n            candidates.data[j].id = j;\n            candidates.data[j].logit = logits[j];\n            candidates.data[j].p = 0.0f;\n        }\n\n        llama_sampler_apply(sampler, &candidates);\n        llama_token token = candidates.data[candidates.selected].id;\n\n        // Print result\n        char buf[256];\n        int len = llama_token_to_piece(vocab, token, buf, 256, 0, false);\n        printf(\"Prompt %d next token: %.*s\\n\", i, len, buf);\n\n        free(candidates.data);\n    }\n\n    llama_batch_free(batch);\n    llama_sampler_free(sampler);\n    llama_free(ctx);\n    llama_model_free(model);\n    llama_backend_free();\n\n    return 0;\n}\n```\n\n---\n\n## 5. Multiple Sequences\n\nProcess multiple independent sequences in parallel using the same context.\n\n```c\n#include \"llama.h\"\n#include <stdio.h>\n#include <string.h>\n\nint main() {\n    llama_backend_init();\n\n    struct llama_model_params model_params = llama_model_default_params();\n    struct llama_model * model = llama_model_load_from_file(\"model.gguf\", model_params);\n\n    struct llama_context_params ctx_params = llama_context_default_params();\n    ctx_params.n_ctx = 2048;\n    ctx_params.n_seq_max = 4;  // Support up to 4 sequences\n\n    struct llama_context * ctx = llama_init_from_model(model, ctx_params);\n    llama_memory_t mem = llama_get_memory(ctx);\n\n    // Clear all sequences\n    llama_memory_clear(mem, false);\n\n    // Process sequence 0\n    const char * prompt1 = \"Hello\";\n    // ... tokenize and decode for seq_id = 0 ...\n\n    // Process sequence 1 independently\n    const char * prompt2 = \"Goodbye\";\n    // ... tokenize and decode for seq_id = 1 ...\n\n    // Copy sequence 0 to sequence 2 (for speculative decoding or similar)\n    llama_memory_seq_cp(mem, 0, 2, -1, -1);  // Copy all positions\n\n    // Remove sequence 1\n    llama_memory_seq_rm(mem, 1, -1, -1);  // Remove all positions\n\n    // Keep only sequence 0, remove all others\n    llama_memory_seq_keep(mem, 0);\n\n    llama_free(ctx);\n    llama_model_free(model);\n    llama_backend_free();\n\n    return 0;\n}\n```\n\n---\n\n## 6. Using LoRA Adapters\n\nLoad and apply LoRA adapters to customize model behavior.\n\n```c\n#include \"llama.h\"\n#include <stdio.h>\n\nint main() {\n    llama_backend_init();\n\n    // Load base model\n    struct llama_model_params model_params = llama_model_default_params();\n    struct llama_model * model = llama_model_load_from_file(\"base-model.gguf\", model_params);\n\n    struct llama_context_params ctx_params = llama_context_default_params();\n    struct llama_context * ctx = llama_init_from_model(model, ctx_params);\n\n    // Load LoRA adapter\n    struct llama_adapter_lora * lora = llama_adapter_lora_init(\n        model,\n        \"lora-adapter.gguf\"\n    );\n\n    if (!lora) {\n        fprintf(stderr, \"Failed to load LoRA adapter\\n\");\n        return 1;\n    }\n\n    // Apply LoRA to context\n    float scale = 1.0;  // LoRA scaling factor\n    if (llama_set_adapter_lora(ctx, lora, scale) < 0) {\n        fprintf(stderr, \"Failed to apply LoRA adapter\\n\");\n        return 1;\n    }\n\n    // Use context with LoRA applied...\n    // ... generate text ...\n\n    // Remove LoRA adapter\n    llama_rm_adapter_lora(ctx, lora);\n\n    // Or clear all adapters\n    llama_clear_adapter_lora(ctx);\n\n    // Note: LoRA adapters are automatically freed with the model\n    // llama_adapter_lora_free() is deprecated\n\n    llama_free(ctx);\n    llama_model_free(model);\n    llama_backend_free();\n\n    return 0;\n}\n```\n\n---\n\n## 7. State Management\n\nSave and restore inference state for resuming generation.\n\n```c\n#include \"llama.h\"\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n\nint main() {\n    llama_backend_init();\n\n    struct llama_model_params model_params = llama_model_default_params();\n    struct llama_model * model = llama_model_load_from_file(\"model.gguf\", model_params);\n\n    struct llama_context_params ctx_params = llama_context_default_params();\n    ctx_params.n_ctx = 2048;\n    struct llama_context * ctx = llama_init_from_model(model, ctx_params);\n\n    const struct llama_vocab * vocab = llama_model_get_vocab(model);\n\n    // Process some tokens\n    const char * prompt = \"The quick brown fox\";\n    llama_token tokens[128];\n    int n_tokens = llama_tokenize(vocab, prompt, strlen(prompt), tokens, 128, true, false);\n\n    struct llama_batch batch = llama_batch_get_one(tokens, n_tokens);\n    llama_decode(ctx, batch);\n\n    // Save state to memory\n    size_t state_size = llama_state_get_size(ctx);\n    uint8_t * state_data = malloc(state_size);\n\n    size_t written = llama_state_get_data(ctx, state_data, state_size);\n    printf(\"Saved %zu bytes of state\\n\", written);\n\n    // Save state to file\n    bool success = llama_state_save_file(ctx, \"state.bin\", tokens, n_tokens);\n    if (!success) {\n        fprintf(stderr, \"Failed to save state to file\\n\");\n    }\n\n    // Clear context\n    llama_memory_t mem = llama_get_memory(ctx);\n    llama_memory_clear(mem, true);\n\n    // Restore state from memory\n    size_t read = llama_state_set_data(ctx, state_data, state_size);\n    printf(\"Restored %zu bytes of state\\n\", read);\n\n    // Or restore from file\n    llama_token loaded_tokens[128];\n    size_t n_loaded_tokens;\n    success = llama_state_load_file(\n        ctx,\n        \"state.bin\",\n        loaded_tokens,\n        128,\n        &n_loaded_tokens\n    );\n\n    if (success) {\n        printf(\"Loaded %zu tokens from state file\\n\", n_loaded_tokens);\n    }\n\n    free(state_data);\n    llama_free(ctx);\n    llama_model_free(model);\n    llama_backend_free();\n\n    return 0;\n}\n```\n\n---\n\n## 8. Custom Sampling\n\nImplement custom sampling strategies.\n\n```c\n#include \"llama.h\"\n#include <stdio.h>\n#include <string.h>\n\nint main() {\n    llama_backend_init();\n\n    struct llama_model_params model_params = llama_model_default_params();\n    struct llama_model * model = llama_model_load_from_file(\"model.gguf\", model_params);\n\n    struct llama_context_params ctx_params = llama_context_default_params();\n    struct llama_context * ctx = llama_init_from_model(model, ctx_params);\n\n    // Advanced sampler chain\n    struct llama_sampler * sampler = llama_sampler_chain_init(\n        llama_sampler_chain_default_params()\n    );\n\n    // Add penalties for repetition\n    llama_sampler_chain_add(sampler, llama_sampler_init_penalties(\n        64,     // penalty_last_n (last 64 tokens)\n        1.1,    // penalty_repeat (1.1x penalty)\n        0.0,    // penalty_freq (disabled)\n        0.0     // penalty_present (disabled)\n    ));\n\n    // Add top-k filtering\n    llama_sampler_chain_add(sampler, llama_sampler_init_top_k(40));\n\n    // Add top-p (nucleus) filtering\n    llama_sampler_chain_add(sampler, llama_sampler_init_top_p(0.95, 1));\n\n    // Add min-p filtering\n    llama_sampler_chain_add(sampler, llama_sampler_init_min_p(0.05, 1));\n\n    // Add temperature\n    llama_sampler_chain_add(sampler, llama_sampler_init_temp(0.8));\n\n    // Final sampler (required)\n    llama_sampler_chain_add(sampler, llama_sampler_init_dist(1234));\n\n    // Use sampler in generation loop...\n    // llama_token token = llama_sampler_sample(sampler, ctx, -1);\n\n    // Get sampler performance stats\n    llama_perf_sampler_print(sampler);\n\n    llama_sampler_free(sampler);\n    llama_free(ctx);\n    llama_model_free(model);\n    llama_backend_free();\n\n    return 0;\n}\n```\n\n---\n\n## 9. Encoder-Decoder Models\n\nUse encoder-decoder models (like T5, BART).\n\n```c\n#include \"llama.h\"\n#include <stdio.h>\n#include <string.h>\n\nint main() {\n    llama_backend_init();\n\n    struct llama_model_params model_params = llama_model_default_params();\n    struct llama_model * model = llama_model_load_from_file(\"encoder-decoder.gguf\", model_params);\n\n    // Check if model has encoder and decoder\n    bool has_encoder = llama_model_has_encoder(model);\n    bool has_decoder = llama_model_has_decoder(model);\n\n    printf(\"Model has encoder: %s\\n\", has_encoder ? \"yes\" : \"no\");\n    printf(\"Model has decoder: %s\\n\", has_decoder ? \"yes\" : \"no\");\n\n    struct llama_context_params ctx_params = llama_context_default_params();\n    struct llama_context * ctx = llama_init_from_model(model, ctx_params);\n\n    const struct llama_vocab * vocab = llama_model_get_vocab(model);\n\n    // 1. Encode input with llama_encode (for encoder)\n    const char * input = \"Translate to French: Hello, how are you?\";\n    llama_token input_tokens[128];\n    int n_input = llama_tokenize(vocab, input, strlen(input), input_tokens, 128, true, false);\n\n    struct llama_batch encoder_batch = llama_batch_get_one(input_tokens, n_input);\n\n    if (llama_encode(ctx, encoder_batch) != 0) {\n        fprintf(stderr, \"Encoding failed\\n\");\n        return 1;\n    }\n\n    // 2. Start decoder with special start token\n    llama_token decoder_start = llama_model_decoder_start_token(model);\n    llama_token decoder_tokens[256];\n    decoder_tokens[0] = decoder_start;\n    int n_decoded = 1;\n\n    // 3. Decode with llama_decode (for decoder)\n    struct llama_sampler * sampler = llama_sampler_chain_init(\n        llama_sampler_chain_default_params()\n    );\n    llama_sampler_chain_add(sampler, llama_sampler_init_greedy());\n\n    while (n_decoded < 256) {\n        struct llama_batch decoder_batch = llama_batch_get_one(\n            &decoder_tokens[n_decoded - 1],\n            1\n        );\n\n        if (llama_decode(ctx, decoder_batch) != 0) {\n            fprintf(stderr, \"Decoding failed\\n\");\n            break;\n        }\n\n        llama_token token = llama_sampler_sample(sampler, ctx, -1);\n\n        if (llama_vocab_is_eog(vocab, token)) {\n            break;\n        }\n\n        decoder_tokens[n_decoded++] = token;\n\n        // Print token\n        char buf[256];\n        int len = llama_token_to_piece(vocab, token, buf, 256, 0, false);\n        fwrite(buf, 1, len, stdout);\n        fflush(stdout);\n    }\n\n    printf(\"\\n\");\n\n    llama_sampler_free(sampler);\n    llama_free(ctx);\n    llama_model_free(model);\n    llama_backend_free();\n\n    return 0;\n}\n```\n\n---\n\n## 10. Memory Management Patterns\n\nAdvanced KV cache manipulation for efficient inference.\n\n```c\n#include \"llama.h\"\n#include <stdio.h>\n\nint main() {\n    llama_backend_init();\n\n    struct llama_model_params model_params = llama_model_default_params();\n    struct llama_model * model = llama_model_load_from_file(\"model.gguf\", model_params);\n\n    struct llama_context_params ctx_params = llama_context_default_params();\n    ctx_params.n_ctx = 2048;\n    ctx_params.n_seq_max = 2;\n\n    struct llama_context * ctx = llama_init_from_model(model, ctx_params);\n    llama_memory_t mem = llama_get_memory(ctx);\n\n    // Pattern 1: Sliding window - remove old tokens when cache is full\n    llama_pos max_pos = llama_memory_seq_pos_max(mem, 0);\n    if (max_pos >= 2000) {\n        // Remove first 500 tokens\n        llama_memory_seq_rm(mem, 0, 0, 500);\n        // Shift remaining tokens down by 500 positions\n        llama_memory_seq_add(mem, 0, 500, -1, -500);\n    }\n\n    // Pattern 2: Context shifting - divide positions to fit more context\n    if (llama_memory_can_shift(mem)) {\n        // Compress positions by 2x (keep every other position)\n        llama_memory_seq_div(mem, 0, 0, -1, 2);\n    }\n\n    // Pattern 3: Fork sequence for parallel paths (e.g., speculative decoding)\n    llama_memory_seq_cp(mem, 0, 1, -1, -1);\n\n    // Pattern 4: Prefix caching - keep common prefix, fork for variants\n    llama_memory_seq_cp(mem, 0, 1, -1, -1);\n    llama_memory_seq_cp(mem, 0, 2, -1, -1);\n\n    llama_free(ctx);\n    llama_model_free(model);\n    llama_backend_free();\n\n    return 0;\n}\n```\n\n---\n\n## 11. Advanced Sampling: XTC + DRY\n\n**NEW in b7572** - Demonstrates new XTC and DRY samplers for reducing repetition and increasing diversity.\n\n```c\n#include \"llama.h\"\n#include <stdio.h>\n#include <string.h>\n\nint main() {\n    llama_backend_init();\n\n    struct llama_model_params model_params = llama_model_default_params();\n    struct llama_model * model = llama_model_load_from_file(\"model.gguf\", model_params);\n\n    struct llama_context_params ctx_params = llama_context_default_params();\n    ctx_params.n_ctx = 2048;\n    struct llama_context * ctx = llama_init_from_model(model, ctx_params);\n\n    struct llama_vocab * vocab = llama_model_get_vocab(model);\n\n    // Create advanced sampler chain with XTC + DRY\n    struct llama_sampler * sampler = llama_sampler_chain_init(\n        llama_sampler_chain_default_params()\n    );\n\n    // Add DRY sampler (reduces repetition)\n    const char * seq_breakers[] = {\"\\n\", \".\", \"?\", \"!\", \",\", \":\", \";\", \")\"};\n    llama_sampler_chain_add(sampler,\n        llama_sampler_init_dry(\n            vocab,\n            llama_model_n_ctx_train(model),\n            0.8,    // dry_multiplier\n            1.75,   // dry_base\n            2,      // dry_allowed_length\n            256,    // dry_penalty_last_n\n            seq_breakers, 8\n        )\n    );\n\n    // Add top-k\n    llama_sampler_chain_add(sampler, llama_sampler_init_top_k(40));\n\n    // Add XTC (excludes top choices for diversity)\n    llama_sampler_chain_add(sampler,\n        llama_sampler_init_xtc(\n            0.1,    // probability\n            0.5,    // threshold\n            1,      // min_keep\n            1234    // seed\n        )\n    );\n\n    // Add temperature\n    llama_sampler_chain_add(sampler, llama_sampler_init_temp(0.8));\n\n    // Add final dist sampler\n    llama_sampler_chain_add(sampler, llama_sampler_init_dist(1234));\n\n    // Tokenize and generate...\n    const char * prompt = \"Write a creative story about\";\n    llama_token tokens[2048];\n    int n_tokens = llama_tokenize(vocab, prompt, strlen(prompt), tokens, 2048, true, false);\n\n    struct llama_batch batch = llama_batch_get_one(tokens, n_tokens);\n    llama_decode(ctx, batch);\n\n    printf(\"%s\", prompt);\n    for (int i = 0; i < 200; i++) {\n        llama_token new_token = llama_sampler_sample(sampler, ctx, -1);\n        if (llama_vocab_is_eog(vocab, new_token)) break;\n\n        char buf[256];\n        int n = llama_token_to_piece(vocab, new_token, buf, 256, 0, false);\n        if (n > 0) {\n            fwrite(buf, 1, n, stdout);\n            fflush(stdout);\n        }\n\n        batch = llama_batch_get_one(&new_token, 1);\n        llama_decode(ctx, batch);\n    }\n    printf(\"\\n\");\n\n    llama_sampler_free(sampler);\n    llama_free(ctx);\n    llama_model_free(model);\n    llama_backend_free();\n\n    return 0;\n}\n```\n\n**Key Points:**\n- DRY sampler prevents repetition using sequence breakers\n- XTC adds diversity by occasionally excluding top tokens\n- Combine with traditional samplers for best results\n\n---\n\n## 12. Per-Sequence State Management\n\n**NEW in b7572** - Save and load state for individual sequences.\n\n```c\n#include \"llama.h\"\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n\nint main() {\n    llama_backend_init();\n\n    struct llama_model_params model_params = llama_model_default_params();\n    struct llama_model * model = llama_model_load_from_file(\"model.gguf\", model_params);\n\n    struct llama_context_params ctx_params = llama_context_default_params();\n    ctx_params.n_ctx = 4096;\n    struct llama_context * ctx = llama_init_from_model(model, ctx_params);\n\n    struct llama_vocab * vocab = llama_model_get_vocab(model);\n\n    // Simulate conversation with Alice (sequence 0)\n    const char * alice_msg = \"Hi, I'm Alice. Tell me about quantum physics.\";\n    llama_token alice_tokens[256];\n    int n_alice = llama_tokenize(vocab, alice_msg, strlen(alice_msg), alice_tokens, 256, true, false);\n\n    // Process Alice's sequence\n    struct llama_batch batch = llama_batch_init(256, 0, 1);\n    for (int i = 0; i < n_alice; i++) {\n        batch.token[batch.n_tokens] = alice_tokens[i];\n        batch.pos[batch.n_tokens] = i;\n        batch.n_seq_id[batch.n_tokens] = 1;\n        batch.seq_id[batch.n_tokens][0] = 0;  // sequence 0\n        batch.logits[batch.n_tokens] = (i == n_alice - 1);\n        batch.n_tokens++;\n    }\n    llama_decode(ctx, batch);\n\n    // Save Alice's sequence state to file\n    printf(\"Saving Alice's conversation state...\\n\");\n    size_t saved_bytes = llama_state_seq_save_file(\n        ctx, \"alice.state\", 0, alice_tokens, n_alice\n    );\n    printf(\"Saved %zu bytes\\n\", saved_bytes);\n\n    // Clear memory and work with Bob\n    llama_memory_clear(llama_get_memory(ctx));\n\n    // Later: restore Alice's conversation\n    printf(\"\\nRestoring Alice's conversation...\\n\");\n    llama_token restored_tokens[1024];\n    size_t restored_count;\n    size_t loaded_bytes = llama_state_seq_load_file(\n        ctx, \"alice.state\", 0, restored_tokens, 1024, &restored_count\n    );\n    printf(\"Loaded %zu bytes, %zu tokens\\n\", loaded_bytes, restored_count);\n\n    llama_batch_free(batch);\n    llama_free(ctx);\n    llama_model_free(model);\n    llama_backend_free();\n\n    return 0;\n}\n```\n\n**Use Cases:**\n- Multi-user chat applications with context switching\n- Branching conversation trees\n- Efficient memory management for multiple conversations\n\n---\n\n## 13. Model Architecture Detection\n\n**NEW in b7572** - Detect and handle different model architectures dynamically.\n\n```c\n#include \"llama.h\"\n#include <stdio.h>\n\nvoid handle_model(struct llama_model * model) {\n    printf(\"=== Model Architecture Analysis ===\\n\\n\");\n\n    if (llama_model_has_encoder(model)) {\n        printf(\"✓ Encoder-decoder model (T5/BART/Flan-T5)\\n\");\n        llama_token decoder_start = llama_model_decoder_start_token(model);\n        printf(\"  Decoder start token: %d\\n\", decoder_start);\n        printf(\"  Usage: Use llama_encode() for input, llama_decode() for generation\\n\\n\");\n\n    } else if (llama_model_is_recurrent(model)) {\n        printf(\"✓ Recurrent model (Mamba/RWKV)\\n\");\n        printf(\"  Features: Different KV cache behavior, efficient long contexts\\n\\n\");\n\n    } else if (llama_model_is_hybrid(model)) {\n        printf(\"✓ Hybrid model (Jamba/Granite MoE)\\n\");\n        printf(\"  Features: Mix of attention and RNN/SSM layers\\n\\n\");\n\n    } else {\n        printf(\"✓ Standard transformer model\\n\\n\");\n\n        int32_t swa_size = llama_model_n_swa(model);\n        if (swa_size > 0) {\n            printf(\"  Sliding Window Attention (SWA) detected\\n\");\n            printf(\"  Window size: %d tokens\\n\", swa_size);\n            printf(\"  Tip: Set ctx_params.swa_full = true for full context access\\n\\n\");\n        }\n\n        enum llama_rope_type rope = llama_model_rope_type(model);\n        const char * rope_names[] = {\n            \"None\", \"Normal\", \"NeoX\", \"Multi-Res\", \"Improved Multi-Res\", \"Vision\"\n        };\n        printf(\"  RoPE type: %s\\n\", rope_names[rope]);\n        if (rope == LLAMA_ROPE_TYPE_VISION) {\n            printf(\"  Note: Vision model - designed for multimodal input\\n\");\n        }\n        printf(\"\\n\");\n    }\n\n    int32_t n_classes = llama_model_n_cls_out(model);\n    if (n_classes > 0) {\n        printf(\"✓ Classifier model detected\\n\");\n        printf(\"  Output classes: %d\\n\", n_classes);\n        for (int i = 0; i < n_classes && i < 10; i++) {\n            const char * label = llama_model_cls_label(model, i);\n            if (label) {\n                printf(\"    Class %d: %s\\n\", i, label);\n            }\n        }\n        printf(\"\\n\");\n    }\n\n    printf(\"=== Model Properties ===\\n\");\n    printf(\"Embedding dimension: %d\\n\", llama_model_n_embd(model));\n    printf(\"Layers: %d\\n\", llama_model_n_layer(model));\n    printf(\"Training context: %d tokens\\n\", llama_model_n_ctx_train(model));\n}\n\nint main() {\n    llama_backend_init();\n\n    struct llama_model_params params = llama_model_default_params();\n    struct llama_model * model = llama_model_load_from_file(\"model.gguf\", params);\n\n    if (!model) {\n        fprintf(stderr, \"Failed to load model\\n\");\n        llama_backend_free();\n        return 1;\n    }\n\n    handle_model(model);\n\n    llama_model_free(model);\n    llama_backend_free();\n    return 0;\n}\n```\n\n**Use Cases:**\n- Dynamic model loading systems\n- Automatic optimization based on architecture\n- Debugging and model analysis tools\n\n---\n\n## 14. Interactive Chat Application\n\nComplete chat application with conversation history management. Demonstrates proper use of chat templates and incremental prompting.\n\n```cpp\n#include \"llama.h\"\n#include <iostream>\n#include <string>\n#include <vector>\n#include <cstring>\n\nint main(int argc, char ** argv) {\n    const char * model_path = \"model.gguf\";\n    const int n_ctx = 4096;\n    const int n_gpu_layers = 99;\n\n    // Suppress non-error logs\n    llama_log_set([](enum ggml_log_level level, const char * text, void * /*user_data*/) {\n        if (level >= GGML_LOG_LEVEL_ERROR) {\n            fprintf(stderr, \"%s\", text);\n        }\n    }, nullptr);\n\n    ggml_backend_load_all();\n\n    llama_model_params model_params = llama_model_default_params();\n    model_params.n_gpu_layers = n_gpu_layers;\n\n    llama_model * model = llama_model_load_from_file(model_path, model_params);\n    if (!model) {\n        fprintf(stderr, \"Failed to load model\\n\");\n        return 1;\n    }\n\n    const llama_vocab * vocab = llama_model_get_vocab(model);\n\n    llama_context_params ctx_params = llama_context_default_params();\n    ctx_params.n_ctx = n_ctx;\n    ctx_params.n_batch = n_ctx;\n\n    llama_context * ctx = llama_init_from_model(model, ctx_params);\n    if (!ctx) {\n        fprintf(stderr, \"Failed to create context\\n\");\n        llama_model_free(model);\n        return 1;\n    }\n\n    llama_sampler * sampler = llama_sampler_chain_init(\n        llama_sampler_chain_default_params()\n    );\n    llama_sampler_chain_add(sampler, llama_sampler_init_min_p(0.05f, 1));\n    llama_sampler_chain_add(sampler, llama_sampler_init_temp(0.8f));\n    llama_sampler_chain_add(sampler, llama_sampler_init_dist(LLAMA_DEFAULT_SEED));\n\n    std::vector<llama_chat_message> messages;\n    std::vector<char> formatted_prompt(n_ctx);\n    int prev_formatted_len = 0;\n\n    const char * chat_template = llama_model_chat_template(model, nullptr);\n\n    printf(\"Chat started. Type your message and press Enter. Empty line to exit.\\n\\n\");\n\n    while (true) {\n        printf(\"\\033[32mYou> \\033[0m\");\n        std::string user_input;\n        std::getline(std::cin, user_input);\n\n        if (user_input.empty()) {\n            break;\n        }\n\n        messages.push_back({\"user\", strdup(user_input.c_str())});\n\n        int new_formatted_len = llama_chat_apply_template(\n            chat_template,\n            messages.data(),\n            messages.size(),\n            true,\n            formatted_prompt.data(),\n            formatted_prompt.size()\n        );\n\n        if (new_formatted_len > (int)formatted_prompt.size()) {\n            formatted_prompt.resize(new_formatted_len);\n            new_formatted_len = llama_chat_apply_template(\n                chat_template,\n                messages.data(),\n                messages.size(),\n                true,\n                formatted_prompt.data(),\n                formatted_prompt.size()\n            );\n        }\n\n        if (new_formatted_len < 0) {\n            fprintf(stderr, \"Failed to apply chat template\\n\");\n            break;\n        }\n\n        std::string prompt(\n            formatted_prompt.begin() + prev_formatted_len,\n            formatted_prompt.begin() + new_formatted_len\n        );\n\n        llama_memory_t mem = llama_get_memory(ctx);\n        bool is_first_turn = (llama_memory_seq_pos_max(mem, 0) == -1);\n\n        const int n_tokens = -llama_tokenize(\n            vocab,\n            prompt.c_str(),\n            prompt.size(),\n            NULL,\n            0,\n            is_first_turn,\n            true\n        );\n\n        std::vector<llama_token> tokens(n_tokens);\n        llama_tokenize(vocab, prompt.c_str(), prompt.size(),\n                      tokens.data(), tokens.size(),\n                      is_first_turn, true);\n\n        printf(\"\\033[33mAssistant> \\033[0m\");\n        std::string response;\n\n        llama_batch batch = llama_batch_get_one(tokens.data(), tokens.size());\n\n        while (true) {\n            int n_ctx_used = llama_memory_seq_pos_max(mem, 0) + 1;\n            if (n_ctx_used + batch.n_tokens > n_ctx) {\n                printf(\"\\n\\033[31mContext size exceeded!\\033[0m\\n\");\n                goto cleanup;\n            }\n\n            if (llama_decode(ctx, batch) != 0) {\n                fprintf(stderr, \"Failed to decode\\n\");\n                goto cleanup;\n            }\n\n            llama_token new_token = llama_sampler_sample(sampler, ctx, -1);\n\n            if (llama_vocab_is_eog(vocab, new_token)) {\n                break;\n            }\n\n            char buf[256];\n            int n = llama_token_to_piece(vocab, new_token, buf, sizeof(buf), 0, true);\n            std::string piece(buf, n);\n            printf(\"%s\", piece.c_str());\n            fflush(stdout);\n            response += piece;\n\n            batch = llama_batch_get_one(&new_token, 1);\n        }\n\n        printf(\"\\n\\033[0m\\n\");\n\n        messages.push_back({\"assistant\", strdup(response.c_str())});\n\n        prev_formatted_len = llama_chat_apply_template(\n            chat_template,\n            messages.data(),\n            messages.size(),\n            false,\n            nullptr,\n            0\n        );\n    }\n\ncleanup:\n    for (auto & msg : messages) {\n        free(const_cast<char *>(msg.content));\n    }\n\n    llama_sampler_free(sampler);\n    llama_free(ctx);\n    llama_model_free(model);\n\n    return 0;\n}\n```\n\n**Key Features:**\n- Incremental prompting (only processes new messages)\n- Proper conversation history management\n- Context size checking\n- Colored terminal output\n\n---\n\n## 15. Streaming Generation\n\nStream tokens as they're generated, useful for real-time applications and servers.\n\n```cpp\n#include \"llama.h\"\n#include <cstdio>\n#include <string>\n#include <vector>\n#include <chrono>\n\ntypedef void (*token_callback_t)(const char * text, int length, void * user_data);\n\nstruct streaming_context {\n    llama_context * ctx;\n    llama_sampler * sampler;\n    const llama_vocab * vocab;\n    token_callback_t callback;\n    void * user_data;\n};\n\nvoid generate_streaming(\n    streaming_context * stream_ctx,\n    const char * prompt,\n    int max_tokens\n) {\n    const llama_vocab * vocab = stream_ctx->vocab;\n    llama_context * ctx = stream_ctx->ctx;\n    llama_sampler * sampler = stream_ctx->sampler;\n\n    int n = -llama_tokenize(vocab, prompt, strlen(prompt), NULL, 0, true, true);\n    std::vector<llama_token> tokens(n);\n    llama_tokenize(vocab, prompt, strlen(prompt), tokens.data(), tokens.size(), true, true);\n\n    llama_batch batch = llama_batch_get_one(tokens.data(), tokens.size());\n    if (llama_decode(ctx, batch) != 0) {\n        fprintf(stderr, \"Failed to decode prompt\\n\");\n        return;\n    }\n\n    auto start_time = std::chrono::high_resolution_clock::now();\n    int n_generated = 0;\n\n    for (int i = 0; i < max_tokens; i++) {\n        llama_token token = llama_sampler_sample(sampler, ctx, -1);\n\n        if (llama_vocab_is_eog(vocab, token)) {\n            break;\n        }\n\n        char buf[256];\n        int len = llama_token_to_piece(vocab, token, buf, sizeof(buf), 0, true);\n        if (len > 0) {\n            if (stream_ctx->callback) {\n                stream_ctx->callback(buf, len, stream_ctx->user_data);\n            }\n        }\n\n        n_generated++;\n\n        batch = llama_batch_get_one(&token, 1);\n        if (llama_decode(ctx, batch) != 0) {\n            break;\n        }\n    }\n\n    auto end_time = std::chrono::high_resolution_clock::now();\n    auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(end_time - start_time);\n\n    fprintf(stderr, \"\\n[Generated %d tokens in %lld ms, %.2f tokens/s]\\n\",\n            n_generated, duration.count(),\n            n_generated * 1000.0 / duration.count());\n}\n\nvoid print_token(const char * text, int length, void * user_data) {\n    fwrite(text, 1, length, stdout);\n    fflush(stdout);\n}\n\nvoid append_to_string(const char * text, int length, void * user_data) {\n    std::string * output = (std::string*)user_data;\n    output->append(text, length);\n}\n\nint main(int argc, char ** argv) {\n    const char * model_path = \"model.gguf\";\n    const char * prompt = \"Write a short story about a robot:\";\n\n    ggml_backend_load_all();\n\n    llama_model_params model_params = llama_model_default_params();\n    model_params.n_gpu_layers = 99;\n\n    llama_model * model = llama_model_load_from_file(model_path, model_params);\n    if (!model) return 1;\n\n    llama_context_params ctx_params = llama_context_default_params();\n    ctx_params.n_ctx = 2048;\n\n    llama_context * ctx = llama_init_from_model(model, ctx_params);\n    if (!ctx) {\n        llama_model_free(model);\n        return 1;\n    }\n\n    llama_sampler * sampler = llama_sampler_chain_init(\n        llama_sampler_chain_default_params()\n    );\n    llama_sampler_chain_add(sampler, llama_sampler_init_top_p(0.95f, 1));\n    llama_sampler_chain_add(sampler, llama_sampler_init_temp(0.8f));\n    llama_sampler_chain_add(sampler, llama_sampler_init_dist(1234));\n\n    // Example 1: Stream to stdout\n    printf(\"Prompt: %s\\n\\n\", prompt);\n    printf(\"Streaming output:\\n\");\n\n    streaming_context stream_ctx = {\n        ctx,\n        sampler,\n        llama_model_get_vocab(model),\n        print_token,\n        nullptr\n    };\n\n    generate_streaming(&stream_ctx, prompt, 200);\n\n    printf(\"\\n\\n\");\n\n    // Example 2: Stream to string\n    printf(\"Capturing to string:\\n\");\n    std::string captured;\n\n    stream_ctx.callback = append_to_string;\n    stream_ctx.user_data = &captured;\n\n    llama_memory_clear(llama_get_memory(ctx), true);\n\n    generate_streaming(&stream_ctx, \"The meaning of life is\", 50);\n\n    printf(\"\\nCaptured text (%zu bytes): %s\\n\", captured.size(), captured.c_str());\n\n    llama_sampler_free(sampler);\n    llama_free(ctx);\n    llama_model_free(model);\n\n    return 0;\n}\n```\n\n**Key Features:**\n- Callback-based streaming architecture\n- Real-time token delivery\n- Performance measurement\n- Multiple streaming targets\n\n---\n\n## Best Practices\n\n### 1. Always Initialize Backend\n```c\nllama_backend_init();\n// ... your code ...\nllama_backend_free();\n```\n\n### 2. Check Return Values\n```c\nif (!model || !ctx) {\n    fprintf(stderr, \"Initialization failed\\n\");\n    // Cleanup and exit\n}\n```\n\n### 3. Free Resources in Reverse Order\n```c\nllama_sampler_free(sampler);  // 1. Free samplers first\nllama_free(ctx);              // 2. Free context\nllama_model_free(model);      // 3. Free model\nllama_backend_free();         // 4. Free backend last\n```\n\n### 4. Use Default Params as Base\n```c\nstruct llama_model_params params = llama_model_default_params();\n// Override only what you need\nparams.n_gpu_layers = 32;\n```\n\n### 5. Query Actual Context Size\n```c\nctx_params.n_ctx = 4096;\nstruct llama_context * ctx = llama_init_from_model(model, ctx_params);\nuint32_t actual_ctx = llama_n_ctx(ctx);  // May differ from requested\n```\n\n### 6. Handle Tokenization Buffer Sizes\n```c\nint n = llama_tokenize(vocab, text, len, tokens, max_tokens, true, false);\nif (n < 0) {\n    // Buffer too small, need -n tokens\n    tokens = realloc(tokens, -n * sizeof(llama_token));\n    n = llama_tokenize(vocab, text, len, tokens, -n, true, false);\n}\n```\n\n### 7. Check for End-of-Generation\n```c\nif (llama_vocab_is_eog(vocab, token)) {\n    break;  // Stop generation\n}\n```\n\n### 8. Performance Monitoring\n```c\nllama_perf_context_print(ctx);\nllama_perf_sampler_print(sampler);\n```\n\n---\n\n## Compilation\n\nAll examples can be compiled with:\n\n```bash\n# C examples\ngcc example.c -o example -I../include -L../build -lllama -lm\n\n# C++ examples\ng++ -std=c++11 example.cpp -o example -I../include -L../build -lllama\n\n# With CMake\ncmake -B build\ncmake --build build\n./build/example -m model.gguf\n```\n\n**Link flags:**\n- `-I../include` - llama.cpp header directory\n- `-L../build` - llama.cpp library directory\n- `-lllama` - Link against llama library\n- `-lm` - Math library (for C examples)\n"
      },
      "plugins": [
        {
          "name": "llamacpp",
          "source": "./plugins/llamacpp",
          "description": "llama.cpp C API reference skill - Complete documentation for 163 functions covering local LLM inference",
          "version": "1.7.6",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add datathings/marketplace",
            "/plugin install llamacpp@datathings"
          ]
        },
        {
          "name": "greycat-c",
          "source": "./plugins/greycat-c",
          "description": "GreyCat C API & Standard Library skill - Native C SDK documentation for GreyCat",
          "version": "1.7.6",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add datathings/marketplace",
            "/plugin install greycat-c@datathings"
          ]
        },
        {
          "name": "greycat",
          "source": "./plugins/greycat",
          "description": "Full-stack GreyCat development skill - GCL language, backend development, and React frontend integration",
          "version": "1.7.6",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add datathings/marketplace",
            "/plugin install greycat@datathings"
          ]
        },
        {
          "name": "greycat-lsp",
          "source": "./plugins/greycat-lsp",
          "description": "GreyCat Language Server Protocol plugin - LSP support for .gcl files",
          "version": "1.7.6",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add datathings/marketplace",
            "/plugin install greycat-lsp@datathings"
          ]
        }
      ]
    }
  ]
}