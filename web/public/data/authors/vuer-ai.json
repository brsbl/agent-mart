{
  "author": {
    "id": "vuer-ai",
    "display_name": "Vuer.ai",
    "type": "Organization",
    "avatar_url": "https://avatars.githubusercontent.com/u/150614266?v=4",
    "url": "https://github.com/vuer-ai",
    "bio": "The Vuer XR Visualization Framework for Scene Representation and Robotics.",
    "stats": {
      "total_marketplaces": 1,
      "total_plugins": 1,
      "total_commands": 0,
      "total_skills": 7,
      "total_stars": 331,
      "total_forks": 20
    }
  },
  "marketplaces": [
    {
      "name": "vuer",
      "version": null,
      "description": "3D visualization toolkit for robotics and GenAI",
      "owner_info": {
        "name": "Ge Yang"
      },
      "keywords": [],
      "repo_full_name": "vuer-ai/vuer",
      "repo_url": "https://github.com/vuer-ai/vuer",
      "repo_description": "Vuer is a 3D visualization tool for robotics and VR applications.",
      "homepage": "https://docs.vuer.ai",
      "signals": {
        "stars": 331,
        "forks": 20,
        "pushed_at": "2026-01-26T00:17:01Z",
        "created_at": "2023-11-16T08:05:33Z",
        "license": "MIT"
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 274
        },
        {
          "path": ".claude-plugin/plugin.json",
          "type": "blob",
          "size": 366
        },
        {
          "path": ".claude",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/skills/params-proto.md",
          "type": "blob",
          "size": 6002
        },
        {
          "path": "README.md",
          "type": "blob",
          "size": 5521
        },
        {
          "path": "docs",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/_static",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/_static/go1",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/_static/go1/README.md",
          "type": "blob",
          "size": 682
        },
        {
          "path": "docs/_static/mujoco_scenes",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/_static/mujoco_scenes/adhesion",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/_static/mujoco_scenes/adhesion/README.md",
          "type": "blob",
          "size": 287
        },
        {
          "path": "docs/_static/mujoco_scenes/agility_cassie",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/_static/mujoco_scenes/agility_cassie/README.md",
          "type": "blob",
          "size": 904
        },
        {
          "path": "docs/_static/mujoco_scenes/cube",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/_static/mujoco_scenes/cube/README.md",
          "type": "blob",
          "size": 349
        },
        {
          "path": "docs/_static/mujoco_scenes/humanoid",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/_static/mujoco_scenes/humanoid/README.md",
          "type": "blob",
          "size": 1299
        },
        {
          "path": "docs/_static/mujoco_scenes/plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/_static/mujoco_scenes/plugin/sdf",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/_static/mujoco_scenes/plugin/sdf/asset",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/_static/mujoco_scenes/plugin/sdf/asset/README.md",
          "type": "blob",
          "size": 179
        },
        {
          "path": "docs/_static/mujoco_scenes/replicate",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/_static/mujoco_scenes/replicate/README.md",
          "type": "blob",
          "size": 691
        },
        {
          "path": "docs/_static/mujoco_scenes/shadow_hand",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/_static/mujoco_scenes/shadow_hand/README.md",
          "type": "blob",
          "size": 1719
        },
        {
          "path": "docs/_static/perseverance",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/_static/perseverance/README.md",
          "type": "blob",
          "size": 535
        },
        {
          "path": "docs/_static/perseverance/rover",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/_static/perseverance/rover/meshes",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/_static/perseverance/rover/meshes/Textures",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/_static/perseverance/rover/meshes/Textures/README.md",
          "type": "blob",
          "size": 163
        },
        {
          "path": "docs/tutorials",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/tutorials/camera",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/tutorials/camera/README.md",
          "type": "blob",
          "size": 299
        },
        {
          "path": "docs/tutorials/physics",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/tutorials/physics/assets",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/tutorials/physics/assets/agility_cassie",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/tutorials/physics/assets/agility_cassie/README.md",
          "type": "blob",
          "size": 904
        },
        {
          "path": "skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/client",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/client/SKILL.md",
          "type": "blob",
          "size": 976
        },
        {
          "path": "skills/components",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/components/SKILL.md",
          "type": "blob",
          "size": 1901
        },
        {
          "path": "skills/events",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/events/SKILL.md",
          "type": "blob",
          "size": 1607
        },
        {
          "path": "skills/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/examples/SKILL.md",
          "type": "blob",
          "size": 1774
        },
        {
          "path": "skills/getting-started",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/getting-started/SKILL.md",
          "type": "blob",
          "size": 1034
        },
        {
          "path": "skills/server",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/server/SKILL.md",
          "type": "blob",
          "size": 1254
        },
        {
          "path": "skills/xr",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/xr/SKILL.md",
          "type": "blob",
          "size": 1710
        }
      ],
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"name\": \"vuer\",\n  \"owner\": {\n    \"name\": \"Ge Yang\"\n  },\n  \"description\": \"3D visualization toolkit for robotics and GenAI\",\n  \"plugins\": [\n    {\n      \"name\": \"vuer\",\n      \"source\": \"./\",\n      \"description\": \"vuer skills for 3D visualization and robotics\"\n    }\n  ]\n}\n",
        ".claude-plugin/plugin.json": "{\n  \"name\": \"vuer\",\n  \"version\": \"0.0.79\",\n  \"description\": \"3D visualization toolkit for robotics and GenAI\",\n  \"author\": {\n    \"name\": \"Ge Yang\"\n  },\n  \"repository\": \"https://github.com/vuer-ai/vuer\",\n  \"homepage\": \"https://docs.vuer.ai\",\n  \"license\": \"MIT\",\n  \"keywords\": [\"3d-visualization\", \"robotics\", \"genai\", \"webxr\", \"vr\", \"ar\"],\n  \"skills\": \"../skills/\"\n}\n",
        ".claude/skills/params-proto.md": "# ML-Dash with params-proto\n\n## Basic Class Object Support\n\nPass configuration classes directly to `params.log()` or `params.set()`:\n\n```python\nfrom ml_dash import Experiment\n\nclass TrainingConfig:\n    learning_rate = 0.001\n    batch_size = 32\n    epochs = 100\n\nclass ModelConfig:\n    architecture = \"resnet50\"\n    hidden_size = 768\n    num_layers = 12\n\nwith Experiment(prefix=\"alice/cv/my-experiment\").run as experiment:\n    # Pass classes directly - attributes extracted automatically\n    experiment.params.log(training=TrainingConfig, model=ModelConfig)\n\n    # Stored as:\n    # training.learning_rate = 0.001\n    # training.batch_size = 32\n    # training.epochs = 100\n    # model.architecture = \"resnet50\"\n    # model.hidden_size = 768\n    # model.num_layers = 12\n```\n\n## params-proto Integration\n\nSeamless integration with `params-proto` classes:\n\n```python\nfrom params_proto import ParamsProto\n\nclass TrainingParams(ParamsProto):\n    learning_rate: float = 0.001\n    batch_size: int = 32\n    epochs: int = 100\n    warmup_steps: int = 1000\n\nclass ModelParams(ParamsProto):\n    architecture: str = \"transformer\"\n    hidden_size: int = 768\n    num_layers: int = 12\n    dropout: float = 0.1\n\nwith Experiment(prefix=\"alice/nlp/transformer-train\").run as experiment:\n    # Works seamlessly with params-proto classes\n    experiment.params.log(\n        training=TrainingParams,\n        model=ModelParams\n    )\n```\n\n## Automatic Attribute Extraction\n\nFeatures of automatic attribute extraction:\n\n- **Private attributes skipped**: Attributes starting with `_` are ignored\n- **Magic methods ignored**: `__init__`, `__repr__`, etc. are skipped\n- **Callable attributes ignored**: Methods are automatically excluded\n- **Nested structures**: Supports deeply nested configurations\n- **Flatten to dot notation**: `training.learning_rate`, `model.hidden_size`, etc.\n\n```python\nclass Config:\n    # Public attributes - included\n    lr = 0.001\n    batch_size = 32\n\n    # Private attributes - automatically skipped\n    _internal_state = None\n    _cache = {}\n\n    # Methods - automatically skipped\n    def get_lr_schedule(self):\n        return None\n\nwith Experiment(prefix=\"alice/ml/test\").run as experiment:\n    experiment.params.log(config=Config)\n    # Only public attributes are logged\n    # config.lr = 0.001\n    # config.batch_size = 32\n```\n\n## Multiple Configurations\n\nCombine multiple configuration classes in a single call:\n\n```python\nclass DataConfig:\n    dataset = \"imagenet\"\n    train_size = 1000000\n    val_size = 50000\n    num_workers = 4\n\nclass OptimizerConfig:\n    type = \"adamw\"\n    lr = 0.001\n    beta1 = 0.9\n    beta2 = 0.999\n    weight_decay = 0.01\n\nclass AugmentationConfig:\n    random_crop = True\n    flip_prob = 0.5\n    color_jitter = 0.4\n\nwith Experiment(prefix=\"alice/ml/resnet-training\").run as experiment:\n    experiment.params.log(\n        data=DataConfig,\n        optimizer=OptimizerConfig,\n        augmentation=AugmentationConfig\n    )\n```\n\n## Mixed Configuration Styles\n\nCombine class objects with dictionaries and flat parameters:\n\n```python\nclass TrainingArgs:\n    learning_rate = 0.001\n    batch_size = 32\n\nwith Experiment(prefix=\"alice/ml/hybrid-config\").run as experiment:\n    experiment.params.log(\n        training=TrainingArgs,  # Class object\n        model={\"name\": \"resnet50\", \"layers\": 50},  # Dict\n        run_id=\"exp-001\"  # Flat param\n    )\n\n    # All stored with dot notation:\n    # training.learning_rate = 0.001\n    # training.batch_size = 32\n    # model.name = \"resnet50\"\n    # model.layers = 50\n    # run_id = \"exp-001\"\n```\n\n## Retrieving Parameters\n\nGet parameters from logged class objects:\n\n```python\nwith Experiment(prefix=\"alice/ml/test\").run as experiment:\n    experiment.params.log(training=TrainingConfig)\n\n    # Retrieve flattened parameters\n    params = experiment.params.get()\n    print(params)\n    # -> {\"training.learning_rate\": 0.001, \"training.batch_size\": 32, ...}\n\n    # Retrieve nested structure\n    params_nested = experiment.params.get(flatten=False)\n    print(params_nested)\n    # -> {\"training\": {\"learning_rate\": 0.001, \"batch_size\": 32, ...}}\n\n    # Access specific parameter\n    lr = params[\"training.learning_rate\"]\n```\n\n## Updated Parameters During Training\n\nUpdate class-based parameters during training:\n\n```python\nwith Experiment(prefix=\"alice/ml/lr-decay\").run as experiment:\n    # Initial parameters\n    experiment.params.log(config=InitialConfig)\n\n    for epoch in range(100):\n        if epoch == 50:\n            # Update learning rate\n            experiment.params.log(**{\"config.learning_rate\": 0.0001})\n\n        train()\n```\n\n## Common Patterns\n\n### Configuration Class Pattern\n\n```python\nclass Config:\n    # Model\n    model_name = \"bert-base\"\n    hidden_size = 768\n    num_layers = 12\n\n    # Training\n    learning_rate = 1e-5\n    batch_size = 32\n    num_epochs = 3\n\n    # Data\n    max_length = 512\n    train_split = 0.9\n\nwith Experiment(prefix=\"alice/ml/bert-finetune\").run as experiment:\n    experiment.params.log(config=Config)\n```\n\n### params-proto with Type Hints\n\n```python\nfrom params_proto import ParamsProto\n\nclass HyperParams(ParamsProto):\n    lr: float = 1e-3\n    weight_decay: float = 0.01\n    dropout: float = 0.1\n    num_layers: int = 12\n\nwith Experiment(prefix=\"alice/ml/typed-config\").run as experiment:\n    experiment.params.log(hyper=HyperParams)\n```\n\n### Environment-Specific Config\n\n```python\nimport os\n\nclass BaseConfig:\n    lr = 0.001\n    batch_size = 32\n\nclass ProdConfig(BaseConfig):\n    batch_size = 128\n    num_workers = 8\n\nconfig = ProdConfig if os.getenv(\"ENV\") == \"prod\" else BaseConfig\n\nwith Experiment(prefix=\"alice/ml/env-aware\").run as experiment:\n    experiment.params.log(config=config)\n```\n\n## Storage and Access\n\n### Local Storage\nParameters are stored in `parameters.json`:\n\n```json\n{\n  \"training.learning_rate\": 0.001,\n  \"training.batch_size\": 32,\n  \"model.architecture\": \"resnet50\"\n}\n```\n\n### Remote Storage\nIn remote mode, stored in MongoDB with the same flattened structure for easy querying.\n",
        "README.md": "<h2>Vuer: An Event-Driven, Declarative Visualization Toolkit for GenAI and Robotics\n<br/>\n<img src=\"https://api.netlify.com/api/v1/badges/2df7f3ba-1a26-4047-b76a-d7401f907bb5/deploy-status\" alt=\"Production\">\n<a href=\"https://pypi.org/project/vuer/\">\n<img src=\"https://img.shields.io/pypi/v/vuer.svg\" alt=\"pypi\">\n</a>\n<a href=\"https://docs.vuer.ai\">\n<img src=\"https://readthedocs.org/projects/vuer-py/badge/?version=latest\">\n</a>\n</h2>\n<p>\n<strong><code>pip install vuer</code></strong>\n&nbsp;&nbsp;⬝&nbsp;&nbsp;\n<a href=\"https://docs.vuer.ai\">docs</a>\n&nbsp;&nbsp;⬝&nbsp;&nbsp;\n<a href=\"#development\">development</a>\n</p>\n\nVuer is a light-weight visualization toolkit for interacting with dynamic 3D and robotics data. It is\nVR and AR ready, and can be run on mobile devices.\n\n## Latest Updates\n\n- **2025-11-28**: All documentation examples are now executable with [downloadable assets](https://drive.google.com/file/d/1sx2-UckFTwEpXZwuSWSc4b2f8z0JAF1F/view?usp=sharing).\n\n## Installation\n\nYou can install `vuer` with `pip`:\n\n```shell\npip install -U vuer\n```\n\nHere is an example that loads a URDF file and displays it in the browser. For more examples, see the\nthe [examples](https://docs.vuer.ai/en/latest/examples/meshes/mesh_loading.html) page.\n\n```python\nfrom vuer import Vuer\nfrom vuer.schemas import DefaultScene, Urdf, OrbitControls\n\napp = Vuer()\n\n@app.spawn(start=True)\nasync def main(sess):\n    sess.set @ DefaultScene(\n        Urdf(src=\"https://raw.githubusercontent.com/nasa-jpl/m2020-urdf-models/main/rover/m2020.urdf\"),\n        up=[0, 0, -1],  # Z-down coordinate system\n        bgChildren=[OrbitControls(key=\"OrbitControls\")]\n    )\n\n    await sess.forever()\n```\n\n[![Click for Live Demo](./assets/curiosity.png)](https://vuer.ai?collapseMenu=True&background=131416,fff&initCamPos=2.8,2.2,2.5&ws=ws%3A%2F%2Flocalhost%3A8012&scene=3gAJqGNoaWxkcmVukd4ABKhjaGlsZHJlbpHeAAaoY2hpbGRyZW6Qo3RhZ6RVcmRmo2tleaExo3NyY9lSaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL25hc2EtanBsL20yMDIwLXVyZGYtbW9kZWxzL21haW4vcm92ZXIvbTIwMjAudXJkZqtqb2ludFZhbHVlc94AAKhyb3RhdGlvbpPLQAkeuGAAAAAAAKN0YWenTW92YWJsZaNrZXmhMqhwb3NpdGlvbpMAAMs%2FwzMzQAAAAKN0YWelU2NlbmWja2V5oTOidXCTAAABpGdyaWTDqHNob3dMZXZhwqtyYXdDaGlsZHJlbpLeAASoY2hpbGRyZW6Qo3RhZ6xBbWJpZW50TGlnaHSja2V5tWRlZmF1bHRfYW1iaWVudF9saWdodKlpbnRlbnNpdHkB3gAFqGNoaWxkcmVukKN0YWewRGlyZWN0aW9uYWxMaWdodKNrZXm5ZGVmYXVsdF9kaXJlY3Rpb25hbF9saWdodKlpbnRlbnNpdHkBpmhlbHBlcsOsaHRtbENoaWxkcmVukLJiYWNrZ3JvdW5kQ2hpbGRyZW6Q\")\n\nTo get a quick overview of what you can do with `vuer`, check out the following:\n\n- browse the example gallery [here](https://docs.vuer.ai/en/latest/examples/meshes/mesh_loading.html)\n- try the demo showing a Unitree Go1 robot in front of a staircase [here](https://docs.vuer.ai/en/latest/examples/urdf_go1_stairs.html)\n\nFor more details:\n\n- A full list of visualization components: [API documentation on Components](https://docs.vuer.ai/en/latest/api/vuer.html).\n\n- A full list of data types: [API documentation on Data Types](https://docs.vuer.ai/en/latest/api/types.html).\n\n## Using Vuer with Claude Code\n\nVuer includes a Claude Code plugin that teaches Claude how to use the library. To install:\n\n```\n/plugin marketplace add vuer-ai/vuer\n/plugin install vuer@vuer\n```\n\nSee the [full guide](https://docs.vuer.ai/en/latest/guides/claude_skill.html) for details.\n\n## Examples\n\nTo run the examples, you'll need to download the required assets:\n\n1. Download `vuer_doc_assets` from [this Google Drive link](https://drive.google.com/file/d/1sx2-UckFTwEpXZwuSWSc4b2f8z0JAF1F/view?usp=sharing)\n2. Unzip the downloaded file\n3. Place the `vuer_doc_assets` folder alongside the project directory and rename it to `assets`.\n\n```\nparent_directory/\n├── vuer/                 # This project\n│   ├── docs/\n│   ├── vuer/\n│   └── README.md\n└── assets/      # Downloaded assets folder\n```\n\n4. Run the examples:\n\n```bash\ncd docs/examples/meshes\npython mesh_loading.py\n```\n\n## Development\n\n### Setup\n\n**Using uv (recommended):**\n```bash\nuv sync --group dev\nsource .venv/bin/activate\n```\n\n**Using pip:**\n```bash\npip install -e '.[dev]'\n```\n\n### Common Tasks\n\n```bash\nmake docs     # Build documentation\nmake preview  # Build and live preview at http://localhost:8000/\nmake test     # Run tests\nmake clean    # Clean build artifacts\n```\n\n### Contributing\n\nWe welcome contributions! See [CONTRIBUTING.md](CONTRIBUTING.md) for detailed guidelines on:\n- Setting up your development environment\n- Code quality standards (ruff formatting and linting)\n- Documentation workflow\n- Publishing releases\n\n## Running Vuer from the Browser\n\nVuer can run directly in the web browser via PyScript and Pyodide. To support this, server dependencies (`websockets`, `aiohttp`) are automatically excluded when `platform_system == 'Emscripten'`.\n\n```python\nimport micropip\nawait micropip.install(\"vuer\")\n\nfrom vuer.schemas import Scene, Box, Sphere, Urdf\n```\n\nThis enables use cases like:\n- Building interactive 3D scene editors in the browser\n- Running Python-based scene generation in JupyterLite notebooks\n- Prototyping robotics visualizations without a local Python server\n\n## To Cite\n\n```bibtex\n@software{vuer,\n  author = {Ge Yang},\n  title = {{VUER}: An Event-Driven, Declarative Visualization Toolkit for GenAI and Robotics},\n  version = {},\n  publisher = {GitHub},\n  url = {https://github.com/vuer-ai/vuer},\n  year = {2025}\n}\n```\n\n## About Us\n\nVuer is built by researchers at MIT and UCSD in fields including robotics, computer vision, and computer graphics.\n\n\n\n",
        "docs/_static/go1/README.md": "# Go1 URDF with Simplified Mesh\n\nThe original Unitree Go1 URDF contains dae mesh files that are very heavy, causing the\nwebGL frontend to effectively freeze.\n\nIn this version, we replace the dae mesh files with simplified stl files. I also correct\nsome of the mistakes in the older gabe_go1 model.\n\n### Folder Structure\n\n```shell\n├── README.md\n├── meshes\n│   ├── calf.stl\n│   ├── hip.stl\n│   ├── picknik_ur5_realsense_camera_adapter_rev2.3mf\n│   ├── thigh.stl\n│   ├── thigh_mirror.stl\n│   └── trunk.stl\n├── urdf\n│   └── go1.urdf\n└── xml\n    └── go1.xml\n\n4 directories, 9 files\n```\n\n",
        "docs/_static/mujoco_scenes/adhesion/README.md": "# Active adhesion example\n\nThis example model shows how to use adhesion actuators.\n\nThe video below is a screen capture of a user interacting with the model:\n\n\n[![Active adhesion example model](https://img.youtube.com/vi/BcHZ5BFeTmU/0.jpg)](https://www.youtube.com/watch?v=BcHZ5BFeTmU)\n\n",
        "docs/_static/mujoco_scenes/agility_cassie/README.md": "# Agility Cassie Description (MJCF)\n\n## Overview\n\nThis package contains a simplified robot description (MJCF) of the Cassie\nbipedal robot. The original MJCF and assets were provided directly by\n[Agility Robotics](http://www.agilityrobotics.com/) under an\n[MIT License](LICENSE).\n\n<p float=\"left\">\n  <img src=\"cassie.png\" width=\"400\">\n</p>\n\n## Modifications made to the original model\n\n1. Replaced single quotes with double quotes.\n2. Made collision geoms visible, put them in hidden group 3.\n3. Removed `nuser_actuator` and `nuser_sensor` (automatically inferred since\n   MuJoCo 2.1.2).\n4. Changed solver from PGS to Newton.\n5. Removed `<visual>` clause.\n6. Removed attribute specs which are already default.\n7. Improved collision geometry.\n8. Added `scene.xml` which includes the robot, with a textured groundplane, skybox, and haze.\n\n## License\n\nThis model is released under an [MIT License](LICENSE).\n",
        "docs/_static/mujoco_scenes/cube/README.md": "# Cube\n\nThis model was contributed by Kevin Zakka. For details regarding its creation, see the associated [repository](https://github.com/kevinzakka/mujoco_cube).\n\nClick the video below to see a user interacting with the cube:\n\n[![3x3x3 cube example model](https://img.youtube.com/vi/ZppeDArq6AU/0.jpg)](https://www.youtube.com/watch?v=ZppeDArq6AU)\n",
        "docs/_static/mujoco_scenes/humanoid/README.md": "# Humanoid\n\nThis simplified humanoid model, introduced in [1], is designed for bipedal locomotion\nbehaviours. While several variants of it exist in the wild, this version is based on the model\nin the DeepMind Control Suite [2], which has fairly realistic actuator gains.\n\n* Degrees of Freedom: 27\n* Actuators: 21\n\n<p float=\"left\">\n  <img src=\"humanoid.png\" width=\"400\">\n</p>\n\n## Changelog\n\n* 08-10-2024: Moved tracking light from the torso to the world.\n* 20-02-2024: Sorted actuators in the same order as the joints.\n* 02-01-2024: Add more keyframes.\n* 27-11-2023: Move humanoid geoms to group 1.\n* 05-04-2023: Fix typo in texture size.\n* 20-09-2022: Use default class for left_upper_arm geom.\n* 17-09-2022: Increase offscreen render buffer resolution of the humanoid to 2560x1440.\n* 12-09-2022:\n  * Increased maximum hip flexion.\n  * Symmetrised shoulder and ankle joints.\n  * Added hamstring tendons which couple the hip and knee at large flexion values.\n  * Moved duplicated values into defaults.\n  * Added two keyframes.\n  * Improved lighting.\n  * Changed naming convention.\n\n## References\n\n[1] [Synthesis and Stabilization of Complex Behaviors through Online Trajectory Optimization](https://doi.org/10.1109/IROS.2012.6386025).\n\n[2] [DeepMind Control Suite](https://arxiv.org/abs/1801.00690).\n",
        "docs/_static/mujoco_scenes/plugin/sdf/asset/README.md": "The spot assets were taken from https://www.cs.cmu.edu/~kmcrane/Projects/ModelRepository/ and are\nreleased under the CC0 1.0 Universal (CC0 1.0) Public Domain Dedication license.\n",
        "docs/_static/mujoco_scenes/replicate/README.md": "# Replicate\n\nModels in this directory provide usage examples of the `<replicate>` meta-element, see\n[documentation](https://mujoco.readthedocs.io/en/stable/XMLreference.html#replicate-r).\n\nThe models contain several categories:\n\n1. Constructing static geometries:\n   - cylinder.xml\n   - bowl.xml\n   - helix.xml\n   - container.xml\n\n2. Replicating moving bodies:\n   - particle.xml\n   - bunnies.xml\n   - leaves.xml\n   - stonehenge.xml\n\n3. Replicas that include tendons:\n   - newton_cradle.xml\n   - tendon.xml\n\n4. Replicas with multiple referencing elements:\n   - references.xml\n\n[![Replicate showcase](https://img.youtube.com/vi/5k0_wsIRAFc/0.jpg)](https://www.youtube.com/watch?v=5k0_wsIRAFc)\n",
        "docs/_static/mujoco_scenes/shadow_hand/README.md": "# Shadow Hand E3M5 Description (MJCF)\n\n## Overview\n\nThis package contains assets of the \"E3M5\" version of the Shadow Hand robot,\nincluding both right-handed and left-handed versions.\nThe original URDF and assets were provided directly by\n[Shadow Robot Company](https://www.shadowrobot.com/) under the\n[Apache 2.0 License](LICENSE).\n\n<p float=\"left\">\n  <img src=\"shadow_hand.png\" width=\"400\">\n</p>\n\n## URDF → MJCF derivation steps\n\n1. Converted the DAE [mesh\n   files](https://github.com/shadow-robot/sr_common/tree/noetic-devel/sr_description/meshes/)\n   to OBJ format using [Blender](https://www.blender.org/).\n2. Processed `.obj` files with [`obj2mjcf`](https://github.com/kevinzakka/obj2mjcf).\n3. Added `<mujoco> <compiler discardvisual=\"false\"/> </mujoco>` to the\n   [URDF](https://github.com/shadow-robot/sr_common/blob/noetic-devel/sr_description/hand/xacro/forearm/forearm_e.urdf.xacro)'s\n   `<robot>` clause in order to preserve visual geometries.\n4. Loaded the URDF into MuJoCo and saved a corresponding MJCF.\n5. Removed `_E3M5` suffix from mesh names.\n6. Added forearm body and its corresponding inertial specs.\n7. Removed 2 artifact boxes left from the URDF conversion.\n8. Manually edited the MJCF to extract common properties into the `<default>` section.\n9. Added `<exclude>` clauses to prevent collisions between the forearm and the\n    wrist and thumb bodies.\n10. Added position-controlled actuators.\n11. Added `impratio=10` for better noslip.\n12. Hardened the contacts on the hand geoms.\n13. Added `scene_left.xml` and `scene_right.xml` which include the robot, with\n    an object, textured groundplane, skybox, and haze.\n\n## License\n\nThese models are released under an [Apache-2.0 License](LICENSE).\n",
        "docs/_static/perseverance/README.md": "# Perseverance Rover and Mars Helicopter\n\nM2020 Perseverance Rover and Helicopter URDF models used for operations visualization. Models courtesy of the Mars 2020 Perseverance and Ingenuity teams and URDF conversion by JPL RSVP team.\n\nThis work was carried out at the Jet Propulsion Laboratory, California Institute of Technology, under a contract with the National Aeronautics and Space Administration\n\nReleased June 10th, 2022\n\nRelease Ids URS307049, URS309682\n\nCredit NASA/JPL-Caltech\n\nRover modeling and texturing by Zareh Gorjian\n\n",
        "docs/_static/perseverance/rover/meshes/Textures/README.md": "M20 URDF Textures provided in resolutions 1k, 2k, 4k, and 8k.\n\nTo change to a different resolution texture rename the target texture to \"M2020_Rover_Texture.jpg\".\n",
        "docs/tutorials/camera/README.md": "# Virtual Cameras\n\nYou can add and move virtual camera objects. See the following two examples:\n\n```{eval-rst}\n.. toctree::\n   :maxdepth: 1\n    \n   record_camera_movement.md\n   move_camera.md\n   grab_render_virtual_camera.md\n   render_queue.md\n   frustum_transformation.md\n   grab_heightmap.md\n\n```\n",
        "docs/tutorials/physics/assets/agility_cassie/README.md": "# Agility Cassie Description (MJCF)\n\n## Overview\n\nThis package contains a simplified robot description (MJCF) of the Cassie\nbipedal robot. The original MJCF and assets were provided directly by\n[Agility Robotics](http://www.agilityrobotics.com/) under an\n[MIT License](LICENSE).\n\n<p float=\"left\">\n  <img src=\"cassie.png\" width=\"400\">\n</p>\n\n## Modifications made to the original model\n\n1. Replaced single quotes with double quotes.\n2. Made collision geoms visible, put them in hidden group 3.\n3. Removed `nuser_actuator` and `nuser_sensor` (automatically inferred since\n   MuJoCo 2.1.2).\n4. Changed solver from PGS to Newton.\n5. Removed `<visual>` clause.\n6. Removed attribute specs which are already default.\n7. Improved collision geometry.\n8. Added `scene.xml` which includes the robot, with a textured groundplane, skybox, and haze.\n\n## License\n\nThis model is released under an [MIT License](LICENSE).\n",
        "skills/client/SKILL.md": "---\nname: client\ndescription: VuerClient for connecting to Vuer servers and sending custom events (plugin:vuer@vuer)\n---\n\n# VuerClient\n\n## Basic Usage\n\n```python\nimport asyncio\nfrom vuer import VuerClient\nfrom vuer.events import ClientEvent\n\nclass MyEvent(ClientEvent):\n    etype = \"MY_EVENT\"\n\nasync def main():\n    async with VuerClient(URI=\"ws://localhost:8012\") as client:\n        client.send @ MyEvent(value={\"data\": 123})  # Fire-and-forget\n        await client.send(MyEvent(value=\"x\"))       # Awaitable\n\nasyncio.run(main())\n```\n\n## Custom Events\n\n```python\nclass SetPositionEvent(ClientEvent):\n    etype = \"SET_POSITION\"\n    value = [0, 0, 0]  # Optional default\n```\n\n## Receiving\n\n```python\nevent = await client.recv(timeout=5.0)\n\nasync for event in client:\n    print(event.etype, event.value)\n```\n\n## Server Handler\n\n```python\n@app.add_handler(\"MY_EVENT\")\nasync def on_event(event, session: VuerSession):\n    session.upsert @ Box(key=\"box\", position=event.value)\n```\n",
        "skills/components/SKILL.md": "---\nname: components\ndescription: Vuer 3D component reference - primitives, meshes, URDF, cameras, lighting, images (plugin:vuer@vuer)\n---\n\n# Vuer Components\n\nImport: `from vuer.schemas import Box, Sphere, TriMesh, Urdf, PointCloud, ...`\n\n## Common Properties\n\nAll components: `key`, `position=[x,y,z]`, `rotation=[x,y,z]`, `scale`, `matrix` (4x4), `children`\n\n## Primitives\n\n```python\nBox(key=\"box\", args=[w, h, d], color=\"red\")\nSphere(key=\"s\", args=[radius, wSegs, hSegs])\nCylinder(key=\"c\", args=[rTop, rBot, height, segs])\nCapsule(key=\"cap\", args=[radius, height, capSegs, radSegs])\nCone, Plane, Ring, Torus, TorusKnot\n```\n\n## Custom Geometry\n\n```python\nTriMesh(key=\"mesh\", vertices=np.array, faces=np.array, colors=np.array)\nPointCloud(key=\"pcd\", vertices=np.array, colors=np.array, size=0.01)\n```\n\n## Models\n\n```python\nUrdf(key=\"robot\", src=\"/static/robot.urdf\", jointValues={\"joint1\": 0.5})\nGlb(key=\"model\", src=\"/static/model.glb\", scale=0.1)\nObj, Ply, Stl, Fbx, Dae  # Same pattern\nSplat(key=\"splat\", src=\"/static/scene.splat\")  # Gaussian splatting\n```\n\n## Cameras & Lighting\n\n```python\nPerspectiveCamera(key=\"cam\", fov=75, position=[5,5,5], makeDefault=True)\nFrustum(key=\"f\", fov=60, aspect=1.6, near=0.1, far=10, showFrustum=True)\nAmbientLight, DirectionalLight, PointLight, SpotLight\n```\n\n## Images\n\n```python\nImg(src=\"url\", key=\"img\")                    # DOM image\nImg(np_array, key=\"np\")                      # From numpy\nImage(img_array, position=[0,1,-2], key=\"p\") # 3D plane\nImageBackground(src=\"bg.jpg\", distance=10)   # Camera-facing\n```\n\n## Interaction\n\n```python\nMovable(key=\"m\", children=[Box(key=\"box\")])  # Drag & drop\nGripper(key=\"g\", pinchWidth=0.04)            # Gripper viz\n```\n\n## Organization\n\n```python\ngroup(key=\"grp\", children=[...])\nScene(children=[...], bgChildren=[Grid()], rawChildren=[AmbientLight()])\nDefaultScene(...)  # Includes default lighting/controls\n```\n",
        "skills/events/SKILL.md": "---\nname: events\ndescription: Vuer event system - session APIs, custom events, handlers, RPC, streaming (plugin:vuer@vuer)\n---\n\n# Vuer Events\n\n## Session APIs\n\n```python\nsession.set @ DefaultScene(Box(key=\"box\"))           # Init once\nsession.upsert @ Box(key=\"box\", position=[1,0,0])    # Update or insert\nsession.upsert @ [Box(key=\"b1\"), Sphere(key=\"s1\")]   # Multiple\nsession.upsert(Box(key=\"child\"), to=\"parent-key\")    # To parent\nsession.update @ Box(key=\"box\", color=\"blue\")        # Update only\nsession.add @ Box(key=\"new\")                         # Add new\nsession.remove @ \"key\"                               # Remove\nsession.remove @ [\"k1\", \"k2\"]                        # Multiple\n```\n\n## Custom Events\n\n```python\nfrom vuer.events import ClientEvent\n\nclass SetPositionEvent(ClientEvent):\n    etype = \"SET_POSITION\"\n\n# From VuerClient\nclient.send @ SetPositionEvent(value=[1, 2, 3])\n```\n\n## Handlers\n\n```python\n@app.add_handler(\"CAMERA_MOVE\")\nasync def on_camera(event, session):\n    print(event.value)  # position, rotation, matrix\n\ncleanup = app.add_handler(\"EVENT\", fn)\ncleanup()  # Remove later\n```\n\nEvents: `CAMERA_MOVE`, `HAND_MOVE`, `OBJECT_MOVE`, `CLICK`, `UPLOAD`\n\n## RPC\n\n```python\nrender = await session.grab_render(quality=0.9)\nimage = render.value  # PIL.Image\n\nmesh_data = await session.get_webxr_mesh(key=\"webxr-mesh\")\n```\n\n## Streaming\n\n```python\n@app.bind(start=True)\nasync def streaming(session: VuerSession):\n    session.set @ DefaultScene()\n    t = 0\n    while True:\n        t += 0.016\n        yield Frame(Update(Box(key=\"box\", position=[np.sin(t), 0, 0])), frame_rate=60)\n```\n",
        "skills/examples/SKILL.md": "---\nname: examples\ndescription: Common Vuer patterns - animation, URDF, point clouds, interactivity, batch updates (plugin:vuer@vuer)\n---\n\n# Vuer Examples\n\n## Animation Loop\n\n```python\n@app.spawn(start=True)\nasync def main(session: VuerSession):\n    session.set @ DefaultScene()\n    t = 0\n    while True:\n        t += 0.05\n        session.upsert @ Box(key=\"box\", position=[np.sin(t), 0.5, np.cos(t)], rotation=[0, t, 0])\n        await asyncio.sleep(0.016)\n```\n\n## URDF Robot\n\n```python\nsession.set @ DefaultScene(\n    Urdf(key=\"robot\", src=\"/static/panda.urdf\",\n         jointValues={\"panda_joint1\": 0, \"panda_joint2\": -0.5, \"panda_joint4\": -2.0})\n)\n```\n\n## Point Cloud\n\n```python\nvertices = np.random.randn(10000, 3).astype(np.float16) * 2\ncolors = (np.random.rand(10000, 3) * 255).astype(np.uint8)\nsession.set @ DefaultScene(PointCloud(key=\"pcd\", vertices=vertices, colors=colors, size=0.02))\n```\n\n## Interactive Movable\n\n```python\nsession.set @ DefaultScene(Movable(key=\"m\", children=[Box(key=\"box\", color=\"green\")]))\n\n@app.add_handler(\"OBJECT_MOVE\")\nasync def on_move(event, session):\n    print(event.value)\n```\n\n## Batch Updates\n\n```python\nwhile True:\n    session.upsert @ [Box(key=f\"box-{i}\", position=pos) for i, pos in enumerate(positions)]\n    await asyncio.sleep(0.016)\n```\n\n## Frame Capture\n\n```python\nawait asyncio.sleep(1)\nrender = await session.grab_render(quality=0.9)\nrender.value.save(\"screenshot.png\")\n```\n\n## Hierarchical Scene\n\n```python\nsession.set @ DefaultScene(\n    group(key=\"arm\", children=[\n        Box(key=\"base\", args=[0.5, 0.1, 0.5]),\n        group(key=\"link1\", position=[0, 0.1, 0], children=[\n            Cylinder(key=\"j1\", args=[0.05, 0.05, 0.3]),\n            group(key=\"link2\", position=[0, 0.3, 0], children=[...])\n        ])\n    ])\n)\n```\n",
        "skills/getting-started/SKILL.md": "---\nname: getting-started\ndescription: Vuer quick start and basic usage patterns for real-time 3D visualization in robotics and AI applications (plugin:vuer@vuer)\n---\n\n# Vuer Quick Start\n\nVuer: Python async backend (aiohttp) <--WebSocket--> Browser client (Three.js)\n\n## Minimal Example\n\n```python\nfrom vuer import Vuer, VuerSession\nfrom vuer.schemas import DefaultScene, Box, Sphere\n\napp = Vuer()\n\n@app.spawn(start=True)\nasync def main(session: VuerSession):\n    session.set @ DefaultScene(\n        Box(key=\"box\", args=[0.2, 0.2, 0.2]),\n        Sphere(key=\"sphere\", args=[0.1], position=[0.5, 0, 0]),\n    )\n    await session.forever()\n```\n\nView at https://vuer.ai (connects to ws://localhost:8012).\n\n## Session APIs\n\n| API | Purpose |\n|-----|---------|\n| `session.set @ Scene(...)` | Initialize root scene (once) |\n| `session.upsert @ Element(...)` | Update or insert element |\n| `session.update @ Element(...)` | Update existing only |\n| `session.add @ Element(...)` | Add new element |\n| `session.remove @ \"key\"` | Remove by key |\n",
        "skills/server/SKILL.md": "---\nname: server\ndescription: Vuer server configuration, decorators, event handlers, SSL/TLS for VR (plugin:vuer@vuer)\n---\n\n# Vuer Server\n\n## Configuration\n\n```python\napp = Vuer(\n    port=8012,              # WebSocket port\n    static_root=\".\",        # Static files at /static/\n    cors=\"*\",               # CORS origins\n    free_port=False,        # Auto-kill process on port\n)\n```\n\nEnvironment variables: `VUER_PORT`, `VUER_DOMAIN`, `VUER_CORS`, `VUER_STATIC_ROOT`\n\n## Decorators\n\n```python\n# Main handler (start=True auto-starts)\n@app.spawn(start=True)\nasync def main(session: VuerSession):\n    session.set @ DefaultScene()\n    await session.forever()\n\n# Generator for streaming\n@app.bind(start=True)\nasync def handler(session: VuerSession):\n    while True:\n        yield Frame(Update(...), frame_rate=60)\n```\n\n## Event Handlers\n\n```python\n@app.add_handler(\"CAMERA_MOVE\")\nasync def on_camera(event: ClientEvent, session: VuerSession):\n    print(event.value)\n```\n\nEvents: `CAMERA_MOVE`, `HAND_MOVE`, `CLICK`, `UPLOAD`, `OBJECT_MOVE`\n\n## SSL for VR\n\nVR requires wss://. Use ngrok:\n\n```bash\nngrok http 8012\n# Visit: https://vuer.ai?ws=wss://xxxx.ngrok.io\n```\n\nOr configure SSL:\n```python\napp = Vuer(cert=\"/path/to/cert.pem\", key=\"/path/to/key.pem\")\n```\n",
        "skills/xr/SKILL.md": "---\nname: xr\ndescription: Vuer VR/AR/WebXR - hand tracking, controllers, AR mesh, haptics (plugin:vuer@vuer)\n---\n\n# Vuer XR\n\nRequires wss://. Use ngrok: `ngrok http 8012` then `https://vuer.ai?ws=wss://xxxx.ngrok.io`\n\n## Hand Tracking\n\n```python\nsession.set @ DefaultScene(Hands(key=\"hands\", stream=True))\n\n@app.add_handler(\"HAND_MOVE\")\nasync def on_hands(event, session):\n    left = event.value.get('left')\n    if left:\n        wrist = left['wrist']\n        pinch = left.get('pinchStrength', 0)\n```\n\nJoints: `wrist`, `thumbTip`, `indexTip`, `middleTip`, `ringTip`, `pinkyTip`, `pinchStrength`, `matrix`\n\n## Controllers\n\n```python\nsession.set @ DefaultScene(MotionController(key=\"ctrl\", stream=True))\n\n@app.add_handler(\"CONTROLLER_MOVE\")\nasync def on_ctrl(event, session):\n    right = event.value.get('right')\n    if right:\n        pos, rot, buttons = right['position'], right['rotation'], right['buttons']\n```\n\n## Gripper\n\n```python\n@app.add_handler(\"HAND_MOVE\")\nasync def update_gripper(event, session):\n    r = event.value.get('right')\n    if r:\n        session.upsert @ Gripper(key=\"g\", position=r['wrist'],\n                                  pinchWidth=0.08 * (1 - r.get('pinchStrength', 0)))\n```\n\n## AR Mesh\n\n```python\nsession.set @ DefaultScene(WebXRMesh(key=\"mesh\", stream=False))\nawait asyncio.sleep(2)\nmesh_data = await session.get_webxr_mesh(key=\"mesh\")\nfor m in mesh_data.value.get('meshes', []):\n    vertices, indices, label = m['vertices'], m['indices'], m.get('semanticLabel')\n```\n\n## Haptics\n\n```python\nfrom vuer.events import HapticActuatorPulse\nsession @ HapticActuatorPulse(left={\"strength\": 0.5, \"duration\": 100},\n                               right={\"strength\": 1.0, \"duration\": 200})\n```\n"
      },
      "plugins": [
        {
          "name": "vuer",
          "source": "./",
          "description": "vuer skills for 3D visualization and robotics",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add vuer-ai/vuer",
            "/plugin install vuer@vuer"
          ]
        }
      ]
    }
  ]
}