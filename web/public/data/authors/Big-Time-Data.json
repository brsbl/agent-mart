{
  "author": {
    "id": "Big-Time-Data",
    "display_name": "big-time-data",
    "type": "Organization",
    "avatar_url": "https://avatars.githubusercontent.com/u/78001431?v=4",
    "url": "https://github.com/Big-Time-Data",
    "bio": "Big Timing",
    "stats": {
      "total_marketplaces": 1,
      "total_plugins": 1,
      "total_commands": 5,
      "total_skills": 1,
      "total_stars": 0,
      "total_forks": 0
    }
  },
  "marketplaces": [
    {
      "name": "dts-marketplace",
      "version": null,
      "description": "DTS (Data Tool Suite) plugins for dbt project management and LLM-observable data workflows",
      "owner_info": {
        "name": "Big Time Data",
        "email": "contact@bigtimedata.com"
      },
      "keywords": [],
      "repo_full_name": "Big-Time-Data/homebrew-dts",
      "repo_url": "https://github.com/Big-Time-Data/homebrew-dts",
      "repo_description": null,
      "homepage": null,
      "signals": {
        "stars": 0,
        "forks": 0,
        "pushed_at": "2026-01-27T17:48:06Z",
        "created_at": "2025-05-07T12:36:41Z",
        "license": null
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 728
        },
        {
          "path": "plugins",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dts/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dts/.claude-plugin/marketplace.json",
          "type": "blob",
          "size": 717
        },
        {
          "path": "plugins/dts/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 1202
        },
        {
          "path": "plugins/dts/README.md",
          "type": "blob",
          "size": 1096
        },
        {
          "path": "plugins/dts/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dts/commands/annotations.md",
          "type": "blob",
          "size": 3583
        },
        {
          "path": "plugins/dts/commands/dts-prime.md",
          "type": "blob",
          "size": 5393
        },
        {
          "path": "plugins/dts/commands/explore-data.md",
          "type": "blob",
          "size": 6388
        },
        {
          "path": "plugins/dts/commands/query.md",
          "type": "blob",
          "size": 5156
        },
        {
          "path": "plugins/dts/commands/validate.md",
          "type": "blob",
          "size": 8023
        },
        {
          "path": "plugins/dts/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dts/skills/dts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dts/skills/dts/SKILL.md",
          "type": "blob",
          "size": 12196
        },
        {
          "path": "plugins/dts/skills/dts/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dts/skills/dts/references/ANNOTATION_TYPES.md",
          "type": "blob",
          "size": 4383
        },
        {
          "path": "plugins/dts/skills/dts/references/ENVIRONMENTS.md",
          "type": "blob",
          "size": 6965
        },
        {
          "path": "plugins/dts/skills/dts/references/INTERPRETATION.md",
          "type": "blob",
          "size": 4207
        },
        {
          "path": "plugins/dts/skills/dts/references/QUERY_PATTERNS.md",
          "type": "blob",
          "size": 10037
        },
        {
          "path": "plugins/dts/skills/dts/references/VISUALIZATION.md",
          "type": "blob",
          "size": 3841
        }
      ],
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"name\": \"dts-marketplace\",\n  \"owner\": {\n    \"name\": \"Big Time Data\",\n    \"email\": \"contact@bigtimedata.com\"\n  },\n  \"metadata\": {\n    \"description\": \"DTS (Data Tool Suite) plugins for dbt project management and LLM-observable data workflows\",\n    \"version\": \"1.0.0\",\n    \"tags\": [\"dbt\", \"data-engineering\", \"observability\", \"mcp\"],\n    \"repository\": \"https://github.com/Big-Time-Data/data-tools\",\n    \"documentation\": \"https://github.com/Big-Time-Data/data-tools/blob/main/dts-backend/claude-plugin/README.md\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"dts\",\n      \"description\": \"Data Tool Suite - LLM-observable workflows for dbt projects with query execution and interpretation\",\n      \"source\": \"./plugins/dts\"\n    }\n  ]\n}\n",
        "plugins/dts/.claude-plugin/marketplace.json": "{\n  \"name\": \"dts-marketplace\",\n  \"owner\": {\n    \"name\": \"Big Time Data\",\n    \"email\": \"contact@bigtimedata.com\"\n  },\n  \"metadata\": {\n    \"description\": \"DTS (Data Tool Suite) plugins for dbt project management and LLM-observable data workflows\",\n    \"version\": \"1.0.0\",\n    \"tags\": [\"dbt\", \"data-engineering\", \"observability\", \"mcp\"],\n    \"repository\": \"https://github.com/Big-Time-Data/data-tools\",\n    \"documentation\": \"https://github.com/Big-Time-Data/data-tools/blob/main/dts-backend/claude-plugin/README.md\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"dts\",\n      \"description\": \"Data Tool Suite - LLM-observable workflows for dbt projects with query execution and interpretation\",\n      \"source\": \"./\"\n    }\n  ]\n}\n",
        "plugins/dts/.claude-plugin/plugin.json": "{\n  \"name\": \"dts\",\n  \"description\": \"Data Tool Suite - LLM-observable workflows for dbt projects including query execution, interpretation, and annotation handling.\",\n  \"version\": \"2.0.10\",\n  \"author\": {\n    \"name\": \"Alex Dovenmuehle\",\n    \"url\": \"https://github.com/adoven\"\n  },\n  \"repository\": \"https://github.com/Big-Time-Data/data-tools\",\n  \"license\": \"MIT\",\n  \"keywords\": [\"dbt\", \"data-engineering\", \"observability\", \"llm\", \"dts\"],\n  \"commands\": \"./commands/\",\n  \"skills\": \"./skills/\",\n  \"mcpServers\": {\n    \"dts\": {\n      \"command\": \"dts_mcp\",\n      \"args\": []\n    }\n  },\n  \"hooks\": {\n    \"SessionStart\": [\n      {\n        \"matcher\": \"\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"dts_mcp prime\"\n          }\n        ]\n      }\n    ],\n    \"SessionEnd\": [\n      {\n        \"matcher\": \"\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"dts_mcp session-end\"\n          }\n        ]\n      }\n    ],\n    \"PreToolUse\": [\n      {\n        \"matcher\": \"mcp__plugin_dts_dts__*\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"dts_mcp inject-session\"\n          }\n        ]\n      }\n    ]\n  }\n}\n",
        "plugins/dts/README.md": "# DTS Query Workflow Plugin\n\nClaude Code plugin for LLM-observable data exploration workflows. Orchestrates reasoning, query execution, interpretation, and annotation handling through the DTS MCP server.\n\n## Features\n\n- **Commands**: `/query`, `/explore-data`, `/validate`, `/annotations`, `/dts-prime`\n- **Auto-initialization**: `dts-prime` runs at session start to establish schema context\n- **LLM Observability**: All queries tracked with intent, reasoning recorded via observer\n- **Cross-environment**: Compare dev vs prod with fully-qualified table names\n\n## Commands\n\n| Command | Description |\n|---------|-------------|\n| `/query <model>` | Start a query workflow against a model or topic |\n| `/explore-data <model>` | Rich exploration with data quality checks |\n| `/validate <model> against <source>` | Validate data against expectations/CSV/prod |\n| `/annotations` | Check for pending human annotations |\n| `/dts-prime` | Re-initialize schema context |\n\n## Requirements\n\n- dbt project with `dbt_project.yml` and accessible `profiles.yml`\n- Database credentials configured in dbt profiles\n",
        "plugins/dts/commands/annotations.md": "---\ndescription: Check for and handle pending human annotations from the observer\nargument-hint: [--all] [--session <id>] [--query <id>]\n---\n\n# Annotations Command\n\nRetrieve and process human feedback (comments) from the observer system.\n\n## Arguments\n\n- **--all**: Show all annotations, not just pending\n- **--session <id>**: Filter to specific session\n- **--query <id>**: Filter to specific query\n\n## Examples\n\n```\n/annotations                           # Check pending annotations\n/annotations --all                     # Show all annotations\n/annotations --query abc-123           # Annotations for specific query\n/annotations --session current         # Current session only\n```\n\n---\n\n## Workflow\n\n### Step 1: Retrieve Annotations\n\nFetch pending annotations from the observer:\n\n```\nobserver_get_annotations(pending_only: true)\n```\n\nIf specific filters requested:\n```\nobserver_get_annotations(\n  session_id: \"<session_id>\",  # if --session provided\n  query_id: \"<query_id>\"       # if --query provided\n)\n```\n\n### Step 2: Process Comments\n\nFor each annotation, acknowledge and incorporate the feedback:\n\n```\nobserver_context(\n  context_type: \"observation\",\n  content: \"Reviewer feedback: <annotation_content>\",\n  tags: [\"feedback-received\"]\n)\n```\n\nDetermine appropriate response based on comment content:\n- If it suggests a correction → offer to re-run affected queries\n- If it provides context → note for future analysis\n- If it asks a question → respond directly\n\n### Step 3: Return Summary\n\nProvide a summary of all annotations processed:\n\n```markdown\n## Annotation Summary\n\n### Pending Annotations: <count>\n\n### Comments\n\n#### Comment #1\n- **Query**: abc-123 (\"Check weekly orders\")\n- **Reviewer**: Alex\n- **Content**: \"Should exclude test orders\"\n- **Suggested Action**: Re-run with filter `WHERE is_test = false`\n\n#### Comment #2\n- **Query**: def-456 (\"Revenue breakdown\")\n- **Reviewer**: Sarah\n- **Content**: \"Good analysis, matches finance report\"\n\n### Recommended Actions\n1. Re-run query abc-123 with test order filter\n2. No other action required\n```\n\n---\n\n## Handling No Annotations\n\nIf no pending annotations exist:\n\n```markdown\n## Annotation Check Complete\n\nNo pending annotations found.\n\n**Session**: <current_session_id>\n**Queries in session**: <count>\n**Last check**: <timestamp>\n\nAll previous analyses are awaiting review. To request feedback, share the observer dashboard with reviewers.\n```\n\n---\n\n## Annotation Lifecycle\n\n```\nQuery executed\n    │\n    ▼\nResults viewed in Observer UI\n    │\n    ▼\nHuman adds comment\n    │\n    ▼\nAnnotation stored (pending)\n    │\n    ▼\n/annotations retrieves it ◄─── You are here\n    │\n    ▼\nProcess and respond\n    │\n    ▼\nMark as delivered\n```\n\n---\n\n## Integration with Other Commands\n\nAfter processing annotations:\n\n- **If correction suggested**: Run `/query` again with adjustments\n- **If context provided**: Note for `/explore-data` or next `/query`\n\nExample flow:\n\n```\nUser: /annotations\nAssistant: Found 1 comment - \"Exclude test accounts from analysis\"\n\nUser: /query customers\nAssistant: Running analysis with test accounts excluded based on previous feedback...\n```\n\n---\n\n## Error Handling\n\n**If observer not initialized:**\n> Observer not available. Please ensure DTS is configured with observer enabled.\n\n**If session not found:**\n> Session <id> not found. Use `/annotations --all` to see all annotations or start a new query workflow.\n\n**If no annotations table:**\n> No annotations have been recorded yet. Annotations are created when reviewers add feedback through the Observer UI.\n",
        "plugins/dts/commands/dts-prime.md": "---\ndescription: Initialize database/schema context at session start for fully-qualified table names\nargument-hint: [--force]\n---\n\n# DTS Prime Command\n\nEstablishes database and schema context at session start by auto-detecting dev and prod targets from dbt configuration. This context is referenced by other commands like `/query` and `/explore-data` for fully-qualified table names.\n\n## Arguments\n\n- **--force**: Skip auto-detection and prompt for manual input\n\n## Examples\n\n```\n/dts-prime                # Auto-detect from dbt config\n/dts-prime --force        # Manually specify database.schema pairs\n```\n\n---\n\n## Detection Workflow\n\n### Step 1: Find dbt Project Configuration\n\nLook for `dbt_project.yml` in the current directory or parent directories:\n\n```\n1. Search for dbt_project.yml starting from current directory\n2. Extract the `profile` name from the file\n3. If not found, proceed to Step 4 (manual input)\n```\n\n**Example dbt_project.yml:**\n```yaml\nname: analytics\nprofile: 'analytics'\n...\n```\n\n### Step 2: Locate profiles.yml\n\nThe profiles.yml location varies. Use `dbt debug --config-dir` to find it:\n\n```bash\ndbt debug --config-dir\n# Output: To view your profiles.yml file, run:\n# open /Users/alice/.dbt\n```\n\n**Common locations (in order of precedence):**\n1. `DBT_PROFILES_DIR` environment variable\n2. Current directory (`./profiles.yml`)\n3. Default: `~/.dbt/profiles.yml`\n\n**Detection steps:**\n```\n1. Run: dbt debug --config-dir (parse output for profiles directory)\n2. If dbt not available, check $DBT_PROFILES_DIR\n3. Check current directory for profiles.yml\n4. Fall back to ~/.dbt/profiles.yml\n5. If none found, proceed to manual input\n```\n\n### Step 3: Parse Target Configuration\n\nRead profiles.yml and extract database/schema for each target:\n\n**Example profiles.yml structure:**\n```yaml\nanalytics:\n  target: dev\n  outputs:\n    dev:\n      type: snowflake\n      database: ANALYTICS_DEV\n      schema: DEV_{{ env_var('USER') | upper }}\n      ...\n    prod:\n      type: snowflake\n      database: ANALYTICS\n      schema: MARTS\n      ...\n```\n\n**Dev Target:**\n- Extract `database` and `schema` fields\n- Resolve Jinja templates (e.g., `{{ env_var('USER') }}` -> actual username)\n- Result: `DATABASE.SCHEMA` (e.g., `ANALYTICS_DEV.DEV_ADOVEN`)\n\n**Prod Target:**\n- Extract `database` and `schema` fields\n- Note: Prod may use a different database entirely, not just a different schema\n- Result: `DATABASE.SCHEMA` (e.g., `ANALYTICS.MARTS`)\n\n**Multi-Schema Projects:**\nSome projects use multiple schemas in prod (e.g., by domain). To detect:\n1. Check dbt_project.yml for custom schema configurations\n2. Look for `+schema:` overrides in model configs\n3. Scan models/ directory structure for schema hints\n4. Common patterns: `ANALYTICS.MARTS`, `ANALYTICS.STAGING`, `ANALYTICS.FINANCE`\n\n### Step 4: Handle Detection Failure\n\nIf auto-detection fails at any step, prompt the user ONCE:\n\n```\nAskUserQuestion:\n  questions:\n    - question: \"What's your dev database.schema? (e.g., ANALYTICS_DEV.DEV_ADOVEN)\"\n      header: \"Dev Target\"\n      options:\n        - label: \"Enter manually\"\n          description: \"Provide in DATABASE.SCHEMA format\"\n\n    - question: \"What are your prod database.schema(s)? (comma-separated if multiple)\"\n      header: \"Prod Target(s)\"\n      options:\n        - label: \"Enter manually\"\n          description: \"e.g., ANALYTICS.MARTS or ANALYTICS.MARTS, ANALYTICS.FINANCE\"\n```\n\n### Step 5: Store and Output Context\n\nStore the detected/entered context for the session and output a summary:\n\n```markdown\n## Schema Context Loaded\n\n**Dev**: `ANALYTICS_DEV.DEV_ADOVEN`\n**Prod**:\n- `ANALYTICS.MARTS`\n- `ANALYTICS.FINANCE`\n\nThis context will be used for:\n- Fully-qualified table names in queries\n- Environment-aware data comparisons\n- Source-to-target validation\n```\n\n---\n\n## Context Usage by Other Commands\n\nOther commands should reference this schema context:\n\n| Command | Usage |\n|---------|-------|\n| `/query` | Use prod schemas for data exploration queries |\n| `/explore-data` | Default to prod; support `--dev` flag for dev schema |\n| Data comparisons | Compare dev vs prod using respective schemas |\n\n**Example fully-qualified reference:**\n```sql\n-- Instead of:\nSELECT * FROM orders\n\n-- Use:\nSELECT * FROM ANALYTICS.MARTS.orders\n-- or for dev:\nSELECT * FROM ANALYTICS_DEV.DEV_ADOVEN.orders\n```\n\n---\n\n## Output Format\n\nOn successful completion, return:\n\n```markdown\n## DTS Prime: Context Initialized\n\n| Environment | Database.Schema |\n|-------------|-----------------|\n| Dev | `ANALYTICS_DEV.DEV_ADOVEN` |\n| Prod | `ANALYTICS.MARTS` |\n| Prod | `ANALYTICS.FINANCE` |\n\n**Profile**: analytics\n**Profiles Location**: ~/.dbt/profiles.yml\n**Detection**: auto-detected | manual\n\nReady for queries. Use `/query` or `/explore-data` to begin.\n```\n\n---\n\n## Error Scenarios\n\n| Scenario | Response |\n|----------|----------|\n| No dbt_project.yml found | Proceed to manual input |\n| `dbt debug --config-dir` fails | Check env var, then common locations |\n| Profile not found in profiles.yml | Proceed to manual input |\n| profiles.yml not found anywhere | Proceed to manual input |\n| Invalid format entered | Re-prompt with DATABASE.SCHEMA format hint |\n\n---\n\n## Session Persistence\n\nThe schema context is session-scoped:\n- Stored in memory for the duration of the session\n- Other commands can reference it without re-detection\n- Use `--force` to re-initialize if schemas change during session\n",
        "plugins/dts/commands/explore-data.md": "---\ndescription: Explore data with rich context, visualizations, and data quality analysis\nargument-hint: <model_or_topic> [--deep] [--quality] [--issue <issue-id>]\n---\n\n# Explore Data Command\n\nA richer version of `/query` designed for open-ended data exploration. Automatically includes data quality checks and visualization suggestions.\n\n## Arguments\n\n- **$ARGUMENTS**: The model name, topic, or question to explore\n- **--deep**: Include full lineage analysis and related models\n- **--quality**: Run comprehensive data quality checks\n- **--issue**: Link this workflow to a beads/issue tracker ID (e.g., `data-tools-123`).\n  When provided, all observer calls include this reference for traceability.\n\n## Environment\n\nThe agent determines which environment(s) to query based on the task:\n- **Exploration** → dev (default)\n- **Production analysis** → prod\n- **Validation/comparison** → both dev and prod\n\nSchema context is established by `dts-prime` at session start. Always use fully-qualified table names (e.g., `ANALYTICS.MARTS.orders`).\n\n## Examples\n\n```\n/explore-data orders                    # Basic exploration of orders model\n/explore-data orders --deep             # Include upstream/downstream analysis\n/explore-data \"customer behavior\"       # Explore a topic across models\n/explore-data inventory --quality       # Focus on data quality\n/explore-data orders --issue data-tools-456  # Link to issue tracker\n```\n\n---\n\n## Exploration Framework\n\nThis command follows a structured exploration approach:\n\n### 1. Model Discovery\n\nUnderstand what data is available:\n\n```\n# Find relevant models\ndbt_list(selector: \"<topic>\")\n\n# Get lineage context\ndbt_lineage(node: \"<model>\", direction: \"both\")\n\n# Get schema details\ndb_get_schemata(level: \"columns\", table: \"<model>\")\n```\n\nRecord your initial understanding:\n```\nobserver_context(\n  context_type: \"reasoning\",\n  content: \"Exploring $ARGUMENTS. Available models: [...]. Starting with [model] because [reason].\",\n  tags: [\"exploration\", \"<topic>\"],\n  issue_ref: \"<issue-id>\"  # If --issue provided\n)\n```\n\n### 2. Overview Analysis\n\nAlways start with high-level metrics:\n\n**Volume and Recency:**\n```sql\nSELECT\n  COUNT(*) as total_rows,\n  MIN(created_at) as earliest_record,\n  MAX(created_at) as latest_record,\n  MAX(updated_at) as last_update\nFROM <model>\n```\n\n**Key Distributions:**\n```sql\nSELECT\n  <categorical_column>,\n  COUNT(*) as count,\n  ROUND(100.0 * COUNT(*) / SUM(COUNT(*)) OVER(), 1) as pct\nFROM <model>\nGROUP BY 1\nORDER BY 2 DESC\nLIMIT 10\n```\n\n### 3. Data Quality Scan (Default)\n\nAlways include basic quality checks:\n\n**Null Analysis:**\n```sql\nSELECT\n  COUNT(*) as total,\n  COUNT(<column_1>) as has_col1,\n  COUNT(<column_2>) as has_col2,\n  -- etc for key columns\nFROM <model>\n```\n\n**Recency Check:**\n```sql\nSELECT\n  CASE\n    WHEN MAX(created_at) < CURRENT_DATE - INTERVAL '7 days' THEN 'stale'\n    WHEN MAX(created_at) < CURRENT_DATE - INTERVAL '1 day' THEN 'delayed'\n    ELSE 'current'\n  END as freshness_status\nFROM <model>\n```\n\n### 4. Deep Analysis (if --deep)\n\nWhen deep exploration is requested:\n\n**Upstream Dependencies:**\n```\ndbt_lineage(node: \"<model>\", direction: \"upstream\")\n# Then explore each upstream model for data quality issues\n```\n\n**Downstream Impact:**\n```\ndbt_lineage(node: \"<model>\", direction: \"downstream\")\n# Identify which downstream models would be affected by issues\n```\n\n**Cross-Model Joins:**\n```sql\n-- Check join integrity with parent tables\nSELECT\n  COUNT(*) as total,\n  COUNT(b.id) as matched,\n  COUNT(*) - COUNT(b.id) as orphaned\nFROM <model> a\nLEFT JOIN <parent_model> b ON a.<fk_column> = b.id\n```\n\n### 5. Data Quality Focus (if --quality)\n\nWhen quality focus is requested:\n\n**Duplicate Detection:**\n```sql\nSELECT\n  <natural_key_columns>,\n  COUNT(*) as occurrences\nFROM <model>\nGROUP BY <natural_key_columns>\nHAVING COUNT(*) > 1\nORDER BY occurrences DESC\nLIMIT 20\n```\n\n**Value Range Validation:**\n```sql\nSELECT\n  '<column>' as column_name,\n  MIN(<column>) as min_val,\n  MAX(<column>) as max_val,\n  AVG(<column>) as avg_val,\n  COUNT(CASE WHEN <column> < 0 THEN 1 END) as negative_count,\n  COUNT(CASE WHEN <column> IS NULL THEN 1 END) as null_count\nFROM <model>\n```\n\n**Referential Integrity:**\n```sql\nSELECT\n  '<foreign_key>' as fk_column,\n  COUNT(*) as total,\n  COUNT(CASE WHEN <foreign_key> IS NOT NULL\n             AND NOT EXISTS (SELECT 1 FROM <ref_table> WHERE id = <foreign_key>)\n             THEN 1 END) as orphaned\nFROM <model>\n```\n\n---\n\n## Output Structure\n\nExploration results should be comprehensive:\n\n```markdown\n## Data Exploration: $ARGUMENTS\n\n**Issue**: `<issue-id>` (if --issue provided)\n\n### Model Overview\n- **Table**: <schema.table>\n- **Rows**: <count>\n- **Date Range**: <earliest> to <latest>\n- **Freshness**: <status>\n\n### Key Metrics\n| Metric | Value |\n|--------|-------|\n| Total records | X |\n| Unique <entity> | Y |\n| Date range | A to B |\n\n### Data Quality Summary\n| Check | Status | Details |\n|-------|--------|---------|\n| Completeness | <status> | <X>% of records have all required fields |\n| Freshness | <status> | Last update: <date> |\n| Duplicates | <status> | <N> potential duplicates found |\n| Orphans | <status> | <N> records missing foreign keys |\n\n### Distribution Analysis\n<key categorical breakdowns>\n\n### Visualizations\n- [Chart 1]: <description>\n- [Chart 2]: <description>\n\n### Lineage Context (if --deep)\n- **Upstream**: <models>\n- **Downstream**: <models>\n\n### Issues Found\n- <issue 1>\n- <issue 2>\n\n### Recommendations\n- <recommendation 1>\n- <recommendation 2>\n```\n\n---\n\n## Visualization Suggestions\n\nAlways suggest visualizations for:\n\n1. **Time trends** (if date columns exist):\n```json\n{\"chart_type\": \"line\", \"x_column\": \"date\", \"y_column\": \"count\", \"title\": \"Volume Over Time\"}\n```\n\n2. **Category distributions**:\n```json\n{\"chart_type\": \"bar\", \"x_column\": \"category\", \"y_column\": \"count\", \"title\": \"Distribution by Category\"}\n```\n\n3. **Quality metrics**:\n```json\n{\"chart_type\": \"table\", \"columns\": [\"check\", \"status\", \"value\"], \"title\": \"Data Quality Summary\"}\n```\n\n---\n\n## Comparison to /query\n\n| Aspect | /query | /explore-data |\n|--------|--------|---------------|\n| Focus | Answer specific question | Open-ended exploration |\n| Quality checks | Only if relevant | Always included |\n| Visualizations | When helpful | Always suggested |\n| Lineage | If needed | Included with --deep |\n| Output | Targeted findings | Comprehensive overview |\n",
        "plugins/dts/commands/query.md": "---\ndescription: Start a query workflow against a dbt model or data topic\nargument-hint: <model_or_topic> [question...] [--issue <issue-id>]\n---\n\n# Query Command\n\nExecute a complete query workflow with LLM observability. This command orchestrates context loading, reasoning, query execution, interpretation, and annotation checking.\n\n## Arguments\n\n- **$ARGUMENTS**: The model name, topic, or question to investigate\n- **--issue**: Link this workflow to a beads/issue tracker ID (e.g., `data-tools-123`).\n  When provided, all observer calls include this reference for traceability.\n\n## Environment\n\nThe agent determines which environment(s) to query based on the task:\n- **Exploration** → dev (default)\n- **Production analysis** → prod\n- **Validation/comparison** → both dev and prod\n\nSchema context is established by `dts-prime` at session start. Always use fully-qualified table names (e.g., `ANALYTICS.MARTS.orders`).\n\n## Examples\n\n```\n/query orders                          # Explore the orders model\n/query \"why are orders down this week\" # Answer a specific question\n/query customers churn                 # Investigate customer churn\n/query orders --issue data-tools-123   # Link queries to issue tracker\n```\n\n---\n\n## Workflow Execution\n\n### Step 1: Check for Previous Feedback\n\nThe database connection auto-initializes on first query - no explicit load needed.\n\n```\n1. Call observer_get_annotations to check for pending feedback from previous sessions\n2. If annotations exist, acknowledge them before proceeding\n```\n\n### Step 2: Gather Model Context\n\nBased on the target argument:\n\n**If model name:**\n```\n1. Call dbt_lineage(node: \"<model>\", direction: \"both\") to understand relationships\n2. Call db_get_schemata(level: \"columns\", table: \"<model>\") for column details\n```\n\n**If question or topic:**\n```\n1. Call dbt_list to find relevant models\n2. Call db_get_schemata for schema overview\n3. Identify which models are most relevant to the question\n```\n\n### Step 3: Document Your Plan\n\nBefore any queries, record your investigation plan:\n\n```\nobserver_context(\n  context_type: \"plan\",\n  content: \"Investigation plan for: $ARGUMENTS\n\n  1. [First query purpose]\n  2. [Second query purpose]\n  3. [Expected outcome]\",\n  tags: [\"<topic>\", \"query-plan\"],\n  issue_ref: \"<issue-id>\"  # If --issue provided\n)\n```\n\n### Step 4: Execute Query Loop\n\nFor each query needed:\n\n**4a. Record reasoning:**\n```\nobserver_context(\n  context_type: \"reasoning\",\n  content: \"Running this query because...\",\n  tags: [\"<topic>\"],\n  issue_ref: \"<issue-id>\"  # If --issue provided\n)\n```\n\n**4b. Execute query:**\n```\ndb_exec(\n  sql: \"<your SQL>\",\n  intent: \"<brief description of why>\",\n  tags: [\"<topic>\", \"<query-type>\"],\n  issue_ref: \"<issue-id>\"  # If --issue provided\n)\n```\n\n**4c. Interpret results:**\n```\ndb_interpret(\n  query_id: \"<from db_exec result>\",\n  interpretation: \"<what you learned>\",\n  visualization_hints: [\n    {\"chart_type\": \"<type>\", \"x_column\": \"<col>\", \"y_column\": \"<col>\", \"title\": \"<title>\"}\n  ],\n  tags: [\"<insight-type>\"]\n)\n```\n\n**4d. Decide: continue or return**\n- If question not fully answered: continue to next query\n- If max queries reached (5): return with partial results\n- If blocking annotation received: stop and address\n\n### Step 5: Check for Annotations\n\nBefore returning, check if any annotations arrived during execution:\n\n```\nobserver_get_annotations(pending_only: true)\n```\n\nHandle any feedback before finalizing.\n\n### Step 6: Return Summary\n\nProvide a structured summary to the main thread:\n\n```markdown\n## Query Workflow Complete\n\n**Issue**: `<issue-id>` (if --issue provided)\n**Session**: <session-id>\n**Topic**: $ARGUMENTS\n\n### Results Summary\n- <key finding 1>\n- <key finding 2>\n- <key finding 3>\n\n### Queries Executed\n1. **<intent-1>**: <brief result>\n2. **<intent-2>**: <brief result>\n\n### Visualizations Suggested\n- <chart-1>: <what it shows>\n\n### Data Quality Notes\n- <any issues found>\n\n### Recommended Follow-ups\n- <suggestion-1>\n- <suggestion-2>\n\n### Human Review Requested\n- [ ] <item needing review>\n```\n\n---\n\n## Error Handling\n\n**If query fails:**\n1. Log error with `observer_context(context_type: \"observation\", content: \"Query failed: <error>\")`\n2. Attempt alternative approach if possible\n3. Return partial results with clear error description\n\n**If no relevant data found:**\n1. Document what was searched\n2. Suggest alternative approaches\n3. Ask user for clarification if needed\n\n**If rate limited or timeout:**\n1. Return what was gathered so far\n2. Note where to resume\n\n---\n\n## Annotation Response\n\nIf annotations (comments) are received during workflow, read the content and determine the appropriate response:\n\n- If it suggests a correction → acknowledge, adjust approach, re-run affected queries\n- If it provides context → incorporate into interpretation\n- If it asks a question → address before continuing\n\n---\n\n## Output Format\n\nThe workflow should return enough context for the main thread to:\n1. Understand what was investigated\n2. Know what queries were run and why\n3. See the key findings and interpretations\n4. Know what follow-up actions are suggested\n5. Identify if any human review is needed\n",
        "plugins/dts/commands/validate.md": "---\ndescription: Validate data against expectations, CSV file, or another table\nargument-hint: <model> against <source> [--issue <issue-id>]\n---\n\n# Validate Command\n\nValidate data against expectations, a reference CSV file, or another table (typically prod). Returns a structured validation report with pass/fail status for each check.\n\n## Arguments\n\n- **\\<model\\>**: The model or table to validate (e.g., `orders`, `dev.customers`)\n- **against \\<source\\>**: What to validate against (see Validation Types below)\n- **--issue**: Link this workflow to a beads/issue tracker ID (e.g., `data-tools-123`).\n  When provided, all observer calls include this reference for traceability.\n\n## Environment\n\nSchema context is established by `dts-prime` at session start. Always use fully-qualified table names.\n\nFor cross-environment validation, the command automatically queries both environments using the dev and prod schemas from context.\n\n## Examples\n\n```\n/validate orders against \"no nulls in order_id, >1M rows\"\n/validate orders against /path/to/expected.csv\n/validate orders against prod\n/validate dev.customers against prod.customers\n/validate orders against prod --issue data-tools-123\n```\n\n---\n\n## Validation Types\n\n### 1. Description-Based Validation\n\nWhen the source is a quoted string with expectations:\n\n```\n/validate orders against \"no nulls in order_id, >1M rows, unique customer_id\"\n```\n\n**Parsing expectations:**\n- `no nulls in <column>` → NULL count check\n- `>N rows` or `<N rows` → Row count threshold\n- `unique <column>` → Uniqueness check\n- `<column> between X and Y` → Range check\n- `<column> in (a, b, c)` → Value set check\n\n**Generated SQL pattern:**\n```sql\nSELECT\n  COUNT(*) as total_rows,\n  COUNT(order_id) as non_null_order_id,\n  COUNT(DISTINCT customer_id) as unique_customers\nFROM ANALYTICS.MARTS.orders\n```\n\n### 2. CSV File Validation\n\nWhen the source is a file path:\n\n```\n/validate orders against /path/to/expected.csv\n```\n\n**Workflow:**\n1. Use the Read tool to load the CSV file\n2. Parse expected values (typically aggregates or sample rows)\n3. Generate queries to compute actual values\n4. Compare actual vs expected\n\n**CSV formats supported:**\n\n*Aggregate expectations:*\n```csv\nmetric,expected\ntotal_rows,1500000\nunique_customers,45000\navg_order_value,125.50\n```\n\n*Sample row validation:*\n```csv\norder_id,customer_id,status,amount\n12345,C001,completed,99.99\n12346,C002,pending,150.00\n```\n\n### 3. Cross-Table/Environment Validation\n\nWhen the source references another table or `prod`:\n\n```\n/validate orders against prod\n/validate dev.customers against prod.customers\n```\n\n**Comparison queries:**\n\n*Row count comparison:*\n```sql\nSELECT 'dev' as env, COUNT(*) as row_count\nFROM ANALYTICS_DEV.DEV_ADOVEN.orders\nUNION ALL\nSELECT 'prod' as env, COUNT(*) as row_count\nFROM ANALYTICS.MARTS.orders\n```\n\n*Schema comparison:*\n- Column names and types match\n- No missing columns in dev\n\n*Value distribution comparison:*\n```sql\nWITH dev_dist AS (\n  SELECT status, COUNT(*) as cnt\n  FROM ANALYTICS_DEV.DEV_ADOVEN.orders\n  GROUP BY status\n),\nprod_dist AS (\n  SELECT status, COUNT(*) as cnt\n  FROM ANALYTICS.MARTS.orders\n  GROUP BY status\n)\nSELECT\n  COALESCE(d.status, p.status) as status,\n  d.cnt as dev_count,\n  p.cnt as prod_count,\n  ABS(COALESCE(d.cnt, 0) - COALESCE(p.cnt, 0)) as diff\nFROM dev_dist d\nFULL OUTER JOIN prod_dist p ON d.status = p.status\nORDER BY diff DESC\n```\n\n---\n\n## Workflow Execution\n\n### Step 1: Parse Validation Request\n\nDetermine validation type from arguments:\n\n```\nobserver_context(\n  context_type: \"plan\",\n  content: \"Validation plan for: <model> against <source>\n\n  Validation type: <description|csv|cross-table>\n  Checks to perform:\n  1. <check-1>\n  2. <check-2>\n  3. <check-3>\",\n  tags: [\"validation\", \"<model>\"],\n  issue_ref: \"<issue-id>\"  # If --issue provided\n)\n```\n\n### Step 2: Gather Schema Context\n\n```\n1. Call db_get_schemata(level: \"columns\", table: \"<model>\") for column details\n2. For cross-table: also get schema of comparison table\n3. Identify key columns, data types, and constraints\n```\n\n### Step 3: Generate Validation Queries\n\nBased on validation type, generate appropriate checks:\n\n**Standard checks (always include):**\n- Row count\n- Null counts for key columns\n- Primary key uniqueness\n\n**Type-specific checks:**\n- Description: Parse and generate per expectation\n- CSV: Generate queries matching expected metrics\n- Cross-table: Generate comparison queries\n\n### Step 4: Execute Validation Queries\n\nFor each validation check:\n\n```\ndb_exec(\n  sql: \"<validation SQL>\",\n  intent: \"validate: <check description>\",\n  tags: [\"validation\", \"<check-type>\"],\n  issue_ref: \"<issue-id>\"  # If --issue provided\n)\n```\n\n### Step 5: Interpret Results\n\nFor each check, determine pass/fail:\n\n```\ndb_interpret(\n  query_id: \"<from db_exec>\",\n  interpretation: \"CHECK <PASSED|FAILED>: <details>\",\n  tags: [\"validation-result\"],\n  issue_ref: \"<issue-id>\"  # If --issue provided\n)\n```\n\n### Step 6: Return Validation Report\n\n```markdown\n## Validation Report\n\n**Issue**: `<issue-id>` (if --issue provided)\n**Model**: <model>\n**Validated Against**: <source>\n**Timestamp**: <datetime>\n\n### Summary\n- **Total Checks**: N\n- **Passed**: X\n- **Failed**: Y\n- **Status**: <PASSED|FAILED>\n\n### Check Results\n\n| Check | Status | Expected | Actual | Details |\n|-------|--------|----------|--------|---------|\n| Row count | PASS | >1M | 1,523,456 | |\n| No nulls in order_id | PASS | 0 | 0 | |\n| Unique customer_id | FAIL | unique | 47 duplicates | See details below |\n\n### Failed Check Details\n\n#### Unique customer_id\n**Expected**: All customer_id values unique\n**Actual**: 47 duplicate customer_id values found\n\nSample duplicates:\n| customer_id | count |\n|-------------|-------|\n| C12345 | 3 |\n| C67890 | 2 |\n\n**Suggested action**: Investigate duplicate customer records\n\n### Data Quality Notes\n- <any additional observations>\n\n### Recommended Follow-ups\n- [ ] <action item if checks failed>\n```\n\n---\n\n## Validation Check Reference\n\n### Row Count Checks\n\n| Expectation | SQL Pattern |\n|-------------|-------------|\n| `>N rows` | `SELECT COUNT(*) > N as passed FROM ...` |\n| `<N rows` | `SELECT COUNT(*) < N as passed FROM ...` |\n| `=N rows` | `SELECT COUNT(*) = N as passed FROM ...` |\n| `between N and M rows` | `SELECT COUNT(*) BETWEEN N AND M as passed FROM ...` |\n\n### Null Checks\n\n| Expectation | SQL Pattern |\n|-------------|-------------|\n| `no nulls in <col>` | `SELECT COUNT(*) - COUNT(<col>) = 0 as passed FROM ...` |\n| `<X% nulls in <col>` | `SELECT 100.0 * (COUNT(*) - COUNT(<col>)) / COUNT(*) < X as passed FROM ...` |\n\n### Uniqueness Checks\n\n| Expectation | SQL Pattern |\n|-------------|-------------|\n| `unique <col>` | `SELECT COUNT(*) = COUNT(DISTINCT <col>) as passed FROM ...` |\n| `unique (<col1>, <col2>)` | `SELECT COUNT(*) = COUNT(DISTINCT <col1> || <col2>) as passed FROM ...` |\n\n### Range Checks\n\n| Expectation | SQL Pattern |\n|-------------|-------------|\n| `<col> >= N` | `SELECT MIN(<col>) >= N as passed FROM ...` |\n| `<col> between X and Y` | `SELECT MIN(<col>) >= X AND MAX(<col>) <= Y as passed FROM ...` |\n\n### Referential Integrity\n\n| Expectation | SQL Pattern |\n|-------------|-------------|\n| `<fk> references <table>` | `SELECT COUNT(*) = 0 as passed FROM <model> WHERE <fk> NOT IN (SELECT id FROM <table>)` |\n\n---\n\n## Error Handling\n\n**If validation query fails:**\n1. Log error with `observer_context(context_type: \"observation\", content: \"Validation query failed: <error>\")`\n2. Mark check as ERROR (not FAIL)\n3. Continue with remaining checks\n4. Include error in report\n\n**If CSV file not found:**\n1. Return error asking user to verify path\n2. Suggest using description-based validation instead\n\n**If comparison table doesn't exist:**\n1. Return error with available tables\n2. Suggest correct table name if similar one exists\n\n---\n\n## Output Format\n\nThe validation report should clearly communicate:\n1. Overall pass/fail status (prominent)\n2. Individual check results (table format)\n3. Details for any failures (actionable)\n4. Suggested remediation steps\n",
        "plugins/dts/skills/dts/SKILL.md": "---\nname: dts\ndescription: Data Tool Suite - LLM-observable workflows for dbt project data exploration\nversion: 1.1.0\ntags: [dbt, observability, data-analysis, dts]\nactivation:\n  - when: \"user asks about data, models, or tables\"\n  - when: \"user wants to explore or analyze data\"\n  - when: \"user asks 'why' questions about metrics\"\n  - when: \"user requests validation or comparison between environments\"\n  - when: \"user mentions dbt model names or database tables\"\n---\n\n# DTS Query Workflow\n\nThis skill provides a structured workflow for executing and interpreting database queries with full LLM observability. All reasoning, queries, and interpretations are tracked via the DTS observer system for later review and human annotation.\n\n## Quick Start\n\nWhen exploring data or answering questions about dbt models:\n\n1. **Think first** - Record your reasoning with `observer_context`\n2. **Query with intent** - Execute queries with `db_exec` including why you're running them\n3. **ALWAYS interpret** - Record what you learned with `db_interpret` **(REQUIRED)**\n4. **Check for feedback** - Look for human annotations with `observer_get_annotations`\n\n## Environment Context\n\nThe `dts-prime` command (auto-runs at session start) establishes schema context from dbt configuration. This context is critical for writing correct queries.\n\n### Available Variables\n\nAfter `dts-prime` runs, you have access to:\n\n| Variable | Description | Example |\n|----------|-------------|---------|\n| `dev_schema` | Your development database.schema | `ANALYTICS_DEV.DEV_ADOVEN` |\n| `prod_schemas` | Production database.schema(s) | `ANALYTICS.MARTS`, `ANALYTICS.FINANCE` |\n\n### Query Patterns by Environment\n\n**Dev Only** (testing your changes):\n```sql\nSELECT * FROM ANALYTICS_DEV.DEV_ADOVEN.orders LIMIT 100\n```\n\n**Prod Only** (production analysis):\n```sql\nSELECT * FROM ANALYTICS.MARTS.orders LIMIT 100\n```\n\n**Cross-Environment** (comparing dev to prod):\n```sql\nSELECT 'dev' as env, COUNT(*) as row_count\nFROM ANALYTICS_DEV.DEV_ADOVEN.orders\nUNION ALL\nSELECT 'prod' as env, COUNT(*) as row_count\nFROM ANALYTICS.MARTS.orders\n```\n\n### Always Use Fully-Qualified Names\n\n**Do this:**\n```sql\nSELECT * FROM ANALYTICS.MARTS.orders\n```\n\n**Not this:**\n```sql\nSELECT * FROM orders  -- Relies on session context, will fail\n```\n\nSee [references/ENVIRONMENTS.md](references/ENVIRONMENTS.md) for detailed patterns.\n\n## Issue Context\n\nWhen working on a tracked issue (GitHub, Linear, Jira, beads, etc.), include the issue reference for traceability.\n\n### Detecting Issue Reference\n\nAt workflow start, check for issue references in the conversation:\n- Look for patterns: `data-tools-xxx`, `PROJ-123`, `#456`, or explicit issue mentions\n- Store the issue reference for the session\n- If no issue is mentioned, omit `issue_ref` - it's optional\n\n### Passing Issue Reference\n\nWhen an issue is active, include `issue_ref` in observer calls:\n\n```\ndb_exec(\n  sql: \"SELECT ...\",\n  intent: \"...\",\n  issue_ref: \"data-tools-yf9\"  # Link query to issue\n)\n\nobserver_context(\n  context_type: \"reasoning\",\n  content: \"...\",\n  issue_ref: \"data-tools-yf9\"  # Link reasoning to issue\n)\n```\n\n### Return Summary\n\nWhen returning results, include the issue reference:\n\n```markdown\n## Query Workflow Summary\n\n**Issue**: `data-tools-yf9` - Update SKILL.md with env, issue, activation triggers\n**Session ID**: abc-123\n...\n```\n\n## When to Use This Workflow\n\nThis workflow should be used **proactively** when data exploration is needed. Don't wait for explicit `/query` commands.\n\n### Activate When\n\n- User asks about data, models, or tables\n- User wants to explore or analyze data\n- User asks \"why\" questions about metrics (e.g., \"why did orders drop?\")\n- User requests validation or comparison between environments\n- User mentions dbt model names or database tables\n- User asks about data quality or discrepancies\n\n### How to Activate\n\nWhen you recognize these patterns, proactively engage:\n\n```\n\"I'll investigate this using the query workflow. Let me first understand the data model...\"\n```\n\nThen proceed with the phases: Context Loading → Reasoning → Query → Interpret → Iterate → Return.\n\n### Don't Wait For\n\n- Explicit `/query` command\n- User asking you to \"use the query workflow\"\n- Permission to explore data\n\nIf the user's question involves data, start the workflow.\n\n## Available MCP Tools\n\nThe DTS MCP server provides these tools for the workflow:\n\n### Context & Reasoning Tools\n\n| Tool | Purpose |\n|------|---------|\n| (auto-init) | Database connection auto-initializes on first query |\n| `dbt_list` | List dbt nodes with selection criteria |\n| `dbt_lineage` | Get upstream/downstream lineage for a model |\n| `db_get_schemata` | Get schema, table, and column information |\n\n### Query Execution Tools\n\n| Tool | Purpose |\n|------|---------|\n| `db_exec` | Execute SQL with intent tracking |\n| `db_interpret` | Record interpretation of query results |\n| `observer_context` | Record reasoning, plans, or observations |\n| `observer_get_annotations` | Retrieve human feedback |\n\n---\n\n## Workflow Phases\n\n### Phase 1: Context Loading\n\nBefore querying, gather context about the data environment:\n\n```\n# Database connection auto-initializes on first query - no explicit load needed!\n\n# Get lineage for relevant models\ndbt_lineage(node: \"model_name\", direction: \"both\")\n\n# Get schema details\ndb_get_schemata(level: \"columns\", schema: \"schema_name\", table: \"table_name\")\n\n# Check for pending annotations from previous sessions\nobserver_get_annotations()\n```\n\n### Phase 2: Reasoning (Recorded)\n\nBefore generating SQL, document your reasoning:\n\n```\nobserver_context(\n  context_type: \"reasoning\",\n  content: \"To understand why orders decreased, I need to:\n    1. Compare this week to last week\n    2. Check by product category\n    3. Look for any data quality issues\",\n  tags: [\"domain:orders\", \"analysis:trend\"]\n)\n```\n\n**Context Types:**\n- `reasoning` - Your thought process and decision logic\n- `plan` - Intended sequence of actions\n- `observation` - Insights or findings without immediate action\n\n### Phase 3: Query Execution\n\nExecute queries with full context tracking:\n\n```\ndb_exec(\n  sql: \"SELECT date, COUNT(*) as order_count FROM orders WHERE date >= '2024-01-01' GROUP BY date\",\n  intent: \"Get daily order counts to identify when the decrease started\",\n  tags: [\"domain:orders\", \"analysis:trend\"]\n)\n```\n\n**Returns:**\n```json\n{\n  \"query_id\": \"abc-123-def\",\n  \"columns\": [\"date\", \"order_count\"],\n  \"rows\": [...],\n  \"status\": \"success\",\n  \"_hint\": \"Record your analysis with db_interpret(query_id: \\\"abc-123-def\\\")\"\n}\n```\n\nThe `query_id` links all subsequent operations (interpretation, annotations) to this query.\n\n> **IMPORTANT**: After analyzing query results, you MUST call `db_interpret` to record your interpretation. This is required for observability - skipping this step means your analysis cannot be reviewed or annotated by humans.\n\n### Phase 4: Interpretation (Recorded)\n\nAfter receiving results, record your interpretation:\n\n```\ndb_interpret(\n  query_id: \"abc-123-def\",\n  interpretation: \"Orders dropped 23% starting January 15th, coinciding with the website redesign launch. The decrease is concentrated in mobile orders.\",\n  visualization_hints: [\n    {\n      \"chart_type\": \"line\",\n      \"x_column\": \"date\",\n      \"y_column\": \"order_count\",\n      \"title\": \"Daily Order Trend\"\n    }\n  ],\n  tags: [\"orders:trend\", \"viz:line\"],\n  issue_ref: \"data-tools-yf9\"  # Include if working on a tracked issue (GitHub, Linear, beads, etc.)\n)\n```\n\n**Visualization Hint Types:**\n- `line` - Time series, trends\n- `bar` - Comparisons, categories\n- `pie` - Proportions, distributions\n- `table` - Detailed breakdowns\n\n**Tag Guidelines:**\n\nUse namespaced tags to keep interpretations organized and searchable. Limit to **2-3 tags** per interpretation.\n\n| Prefix | Purpose | Examples |\n|--------|---------|----------|\n| `domain:` | Data domain being analyzed | `domain:orders`, `domain:users`, `domain:billing` |\n| `analysis:` | Type of analysis | `analysis:trend`, `analysis:comparison`, `analysis:anomaly` |\n| `viz:` | Visualization type | `viz:line`, `viz:bar`, `viz:table` |\n| `quality:` | Data quality focus | `quality:nulls`, `quality:duplicates` |\n| `period:` | Time period focus | `period:weekly`, `period:nov-2024` |\n\n**Good tags:**\n- `domain:teamwork`, `analysis:trend`, `period:nov-2024`\n- `domain:orders`, `analysis:comparison`, `viz:bar`\n\n**Avoid:**\n- Redundant tags: `november-analysis`, `feature-breakdown`, `engagement-analysis`\n- Over-specific: `dashboard-requirements`, `weekly-engagement`\n- Too many tags: more than 3 per interpretation\n\n### Phase 5: Iteration Decision\n\nDecide whether to continue investigating or return results:\n\n**Continue when:**\n- Results are incomplete or raise new questions\n- Data quality issues need investigation\n- User's question isn't fully answered\n\n**Stop when:**\n- Question is fully answered\n- Maximum query depth reached (default: 5 queries)\n- Blocking feedback received (e.g., reviewer says to stop)\n\nDocument your decision:\n\n```\nobserver_context(\n  context_type: \"observation\",\n  content: \"Initial analysis shows mobile orders down 23%. Need to investigate:\n    1. Mobile vs desktop breakdown\n    2. Whether specific pages are affected\n    Proceeding with follow-up queries.\",\n  tags: [\"domain:orders\", \"analysis:comparison\"]\n)\n```\n\n### Phase 6: Return to Main Thread\n\nWhen returning results, provide a structured summary:\n\n```markdown\n## Query Workflow Summary\n\n### Session Context\n- Session ID: [from observer]\n- Queries Executed: 3\n- Models Analyzed: orders, customers, products\n\n### Key Findings\n- Orders dropped 23% starting January 15th\n- Mobile orders most affected (-31%)\n- Desktop orders relatively stable (-8%)\n\n### Data Quality Notes\n- 47 orders missing customer_id (0.3% of total)\n\n### Suggested Follow-ups\n- Investigate mobile checkout conversion rates\n- Check for correlating support tickets\n\n### Pending Actions\n- [ ] Review findings with product team\n- [ ] Check mobile analytics data\n```\n\n---\n\n## Annotation Handling\n\nHuman annotations (comments) can arrive at any time. Check for them:\n\n1. **At workflow start** - Previous session feedback\n2. **Before major decisions** - Recent feedback\n3. **At workflow end** - Final review\n\n```\nobserver_get_annotations(pending_only: true)\n```\n\nRead the comment content to determine the appropriate response:\n- If it suggests a correction → re-evaluate approach, possibly re-run queries\n- If it provides context → acknowledge and incorporate\n- If it validates findings → note increased confidence\n- If it says to stop → halt current approach, ask for guidance\n\nExample handling:\n\n```\n# Check for annotations\nannotations = observer_get_annotations(pending_only: true)\n\nfor annotation in annotations:\n    observer_context(\n      context_type: \"reasoning\",\n      content: f\"Received feedback: {annotation.content}. Evaluating response.\",\n      tags: [\"feedback-received\"]\n    )\n    # Determine response based on comment content\n```\n\n---\n\n## Best Practices\n\n### Query Design\n- Start broad, then narrow down\n- Include relevant filters in WHERE clause\n- Use appropriate aggregations for the question\n- Limit result sets for exploration (LIMIT 100)\n\n### Interpretation\n- State findings clearly and quantitatively\n- Note any assumptions or limitations\n- Suggest visualizations that highlight key patterns\n- Flag data quality concerns\n\n### Context Recording\n- Be specific about reasoning\n- Link related queries through tags\n- Document decision points\n- Include enough context for human reviewers\n\n### Error Handling\n- If query fails, log with `observer_context` type \"observation\"\n- Try alternative approaches\n- Return partial results with clear error description\n- Don't silently ignore failures\n\n---\n\n## Reference Files\n\nFor detailed guidance on specific topics:\n\n- [references/QUERY_PATTERNS.md](references/QUERY_PATTERNS.md) - SQL patterns for data exploration\n- [references/INTERPRETATION.md](references/INTERPRETATION.md) - How to interpret query results\n- [references/VISUALIZATION.md](references/VISUALIZATION.md) - Chart selection guidelines\n- [references/ANNOTATION_TYPES.md](references/ANNOTATION_TYPES.md) - Handling different feedback types\n",
        "plugins/dts/skills/dts/references/ANNOTATION_TYPES.md": "# Handling Annotations\n\nHow to respond to human feedback (comments) from the observer.\n\n## Overview\n\nAnnotations are comments left by human reviewers on your queries and interpretations. All annotations are of type `comment` - read the content to understand what response is needed.\n\n---\n\n## Common Feedback Patterns\n\n### Corrections\n\n**Example comments:**\n- \"The orders table doesn't include returns - need to join with returns table\"\n- \"Customer count is wrong - should filter out test accounts\"\n- \"Date filter should be fiscal year, not calendar year\"\n\n**How to respond:**\n\n1. **Acknowledge** the feedback:\n```\nobserver_context(\n  context_type: \"observation\",\n  content: \"Received feedback: Need to exclude test accounts from customer count. Adjusting approach.\",\n  tags: [\"feedback-response\"]\n)\n```\n\n2. **Re-run** affected queries with corrected approach:\n```\ndb_exec(\n  sql: \"SELECT COUNT(*) FROM customers WHERE is_test = false\",\n  intent: \"Re-count customers excluding test accounts (per feedback)\",\n  tags: [\"feedback-rerun\"]\n)\n```\n\n3. **Update interpretation** with corrected findings\n\n---\n\n### Additional Context\n\n**Example comments:**\n- \"FYI: We had a marketing campaign on Jan 15 that might explain the spike\"\n- \"The EMEA region uses a different order system\"\n- \"These numbers look consistent with what finance reported\"\n\n**How to respond:**\n\n1. **Acknowledge** in your reasoning:\n```\nobserver_context(\n  context_type: \"observation\",\n  content: \"Note from reviewer: Marketing campaign on Jan 15 may explain the order spike. Incorporating this context.\",\n  tags: [\"context-added\"]\n)\n```\n\n2. **Incorporate** into interpretation:\n> The spike in orders on January 15th (+47%) aligns with the marketing campaign noted by the reviewer.\n\n3. **No need to re-run queries** unless the comment reveals data gaps\n\n---\n\n### Validations\n\n**Example comments:**\n- \"Confirmed - these numbers match our internal dashboard\"\n- \"Good analysis, this aligns with what we expected\"\n- \"Verified the SQL logic is correct\"\n\n**How to respond:**\n\n1. **Note increased confidence**:\n```\nobserver_context(\n  context_type: \"observation\",\n  content: \"Analysis validated by reviewer. Confidence level: high.\",\n  tags: [\"validated\"]\n)\n```\n\n2. **Note in summary:**\n> **Validation:** Key findings have been reviewed and confirmed by reviewer.\n\n---\n\n### Stop Requests\n\n**Example comments:**\n- \"This analysis is using the wrong data source entirely\"\n- \"Stop - we can't share this data externally\"\n- \"The question is about something different\"\n\n**How to respond:**\n\n1. **Stop** current analysis:\n```\nobserver_context(\n  context_type: \"observation\",\n  content: \"Received feedback to stop. Requesting guidance.\",\n  tags: [\"stopped\", \"needs-guidance\"]\n)\n```\n\n2. **Document** what was attempted and why it was stopped\n\n3. **Request clarification** from user before proceeding\n\n---\n\n## Checking for Annotations\n\n### When to Check\n\n1. **At workflow start** - Previous session feedback\n2. **After long-running queries** - Feedback may have arrived\n3. **Before returning results** - Final review\n\n### How to Check\n\n```\n# Get all pending annotations\nobserver_get_annotations(pending_only: true)\n\n# Get annotations for specific query\nobserver_get_annotations(query_id: \"abc-123\")\n\n# Get all session annotations\nobserver_get_annotations(session_id: \"<current_session>\")\n```\n\n---\n\n## Example Annotation Handling Flow\n\n```\n# Check for annotations\nannotations = observer_get_annotations(pending_only: true)\n\nfor annotation in annotations:\n    # Read the comment content to determine response\n    content = annotation.content.lower()\n\n    if \"stop\" in content or \"wrong\" in content:\n        # May need to stop and ask for guidance\n        observer_context(\n          context_type: \"observation\",\n          content: f\"Feedback received: {annotation.content}\"\n        )\n        # Consider stopping if the feedback indicates serious issues\n\n    elif \"should\" in content or \"need to\" in content:\n        # Likely a correction - re-run affected queries\n        observer_context(\n          context_type: \"reasoning\",\n          content: f\"Adjusting for feedback: {annotation.content}\"\n        )\n        # Identify and re-run affected queries\n\n    else:\n        # General context or validation\n        observer_context(\n          context_type: \"observation\",\n          content: f\"Reviewer feedback: {annotation.content}\"\n        )\n```\n",
        "plugins/dts/skills/dts/references/ENVIRONMENTS.md": "# Working with Environments\n\nUnderstanding dev vs prod environments and how to query them effectively.\n\n## How dts-prime Detects Schemas\n\nThe `/dts-prime` command auto-detects database and schema context from dbt configuration files.\n\n### Detection Chain\n\n```\ndbt_project.yml → profile name → profiles.yml → target configs\n```\n\n**Step 1: Find dbt_project.yml**\n```yaml\n# dbt_project.yml\nname: analytics\nprofile: 'analytics'  # ← This is the profile name\n```\n\n**Step 2: Locate profiles.yml**\n\nSearch order (first match wins):\n1. `DBT_PROFILES_DIR` environment variable\n2. Current directory (`./profiles.yml`)\n3. Default: `~/.dbt/profiles.yml`\n\nUse `dbt debug --config-dir` to find the active profiles directory.\n\n**Step 3: Extract Target Configurations**\n```yaml\n# profiles.yml\nanalytics:\n  target: dev\n  outputs:\n    dev:\n      type: snowflake\n      database: ANALYTICS_DEV\n      schema: DEV_{{ env_var('USER') | upper }}\n    prod:\n      type: snowflake\n      database: ANALYTICS\n      schema: MARTS\n```\n\n**Result:**\n- Dev: `ANALYTICS_DEV.DEV_ADOVEN`\n- Prod: `ANALYTICS.MARTS`\n\n---\n\n## Table Naming Patterns\n\n### Snowflake\n\nSnowflake uses three-part fully-qualified names:\n\n```\nDATABASE.SCHEMA.TABLE\n```\n\n**Examples:**\n```sql\nSELECT * FROM ANALYTICS.MARTS.orders\nSELECT * FROM ANALYTICS_DEV.DEV_ADOVEN.orders\n```\n\n### Other Warehouses\n\n| Warehouse | Pattern | Example |\n|-----------|---------|---------|\n| Snowflake | `DATABASE.SCHEMA.TABLE` | `ANALYTICS.MARTS.orders` |\n| BigQuery | `PROJECT.DATASET.TABLE` | `my-project.analytics.orders` |\n| Postgres | `SCHEMA.TABLE` | `public.orders` |\n| Redshift | `SCHEMA.TABLE` | `analytics.orders` |\n| Databricks | `CATALOG.SCHEMA.TABLE` | `main.analytics.orders` |\n\n---\n\n## Environment Structure\n\n### Typical Setup\n\nDev and prod often live in different databases, not just different schemas:\n\n```\nPROD                          DEV\n├── ANALYTICS                 ├── ANALYTICS_DEV\n│   ├── MARTS                 │   └── DEV_ADOVEN\n│   │   └── orders            │       └── orders\n│   ├── STAGING               │\n│   │   └── stg_orders        │\n│   └── FINANCE               │\n│       └── fct_revenue       │\n```\n\n### Multi-Schema Prod\n\nProduction often has multiple schemas organized by domain:\n\n| Schema | Purpose | Example Models |\n|--------|---------|----------------|\n| `MARTS` | Business-facing models | `orders`, `customers` |\n| `STAGING` | Intermediate transforms | `stg_orders`, `stg_customers` |\n| `FINANCE` | Finance-specific models | `fct_revenue`, `dim_accounts` |\n| `MARKETING` | Marketing models | `fct_campaigns`, `dim_channels` |\n\nDev typically consolidates everything into a single schema per user.\n\n---\n\n## When to Query Which Environment\n\n### Dev Only\n\nUse dev for:\n- Testing changes you're developing\n- Exploring your modified models\n- Debugging transformations\n- Verifying your dbt runs\n\n```sql\n-- Check if your changes work\nSELECT * FROM ANALYTICS_DEV.DEV_ADOVEN.orders LIMIT 10\n```\n\n### Prod Only\n\nUse prod for:\n- Production analysis and reporting\n- Understanding current state of data\n- Answering business questions\n- Baseline comparisons\n\n```sql\n-- Production analysis\nSELECT status, COUNT(*)\nFROM ANALYTICS.MARTS.orders\nGROUP BY status\n```\n\n### Both Environments\n\nUse both for:\n- Validating dev changes against prod baseline\n- Comparing row counts before/after changes\n- Finding differences introduced by your changes\n- Ensuring data consistency\n\n---\n\n## Cross-Environment Query Patterns\n\n### Compare Row Counts\n\n```sql\nSELECT 'dev' as env, COUNT(*) as row_count\nFROM ANALYTICS_DEV.DEV_ADOVEN.orders\nUNION ALL\nSELECT 'prod' as env, COUNT(*) as row_count\nFROM ANALYTICS.MARTS.orders\n```\n\n### Find Rows Only in Dev (Additions)\n\n```sql\nSELECT * FROM ANALYTICS_DEV.DEV_ADOVEN.orders\nEXCEPT\nSELECT * FROM ANALYTICS.MARTS.orders\nLIMIT 100\n```\n\n### Find Rows Only in Prod (Deletions)\n\n```sql\nSELECT * FROM ANALYTICS.MARTS.orders\nEXCEPT\nSELECT * FROM ANALYTICS_DEV.DEV_ADOVEN.orders\nLIMIT 100\n```\n\n### Compare Aggregates\n\n```sql\nSELECT\n  'dev' as env,\n  COUNT(*) as total_orders,\n  COUNT(DISTINCT customer_id) as unique_customers,\n  SUM(amount) as total_revenue\nFROM ANALYTICS_DEV.DEV_ADOVEN.orders\n\nUNION ALL\n\nSELECT\n  'prod' as env,\n  COUNT(*) as total_orders,\n  COUNT(DISTINCT customer_id) as unique_customers,\n  SUM(amount) as total_revenue\nFROM ANALYTICS.MARTS.orders\n```\n\n### Side-by-Side Comparison\n\n```sql\nWITH dev_stats AS (\n  SELECT\n    status,\n    COUNT(*) as count,\n    SUM(amount) as revenue\n  FROM ANALYTICS_DEV.DEV_ADOVEN.orders\n  GROUP BY status\n),\nprod_stats AS (\n  SELECT\n    status,\n    COUNT(*) as count,\n    SUM(amount) as revenue\n  FROM ANALYTICS.MARTS.orders\n  GROUP BY status\n)\nSELECT\n  COALESCE(d.status, p.status) as status,\n  d.count as dev_count,\n  p.count as prod_count,\n  d.count - p.count as count_diff,\n  d.revenue as dev_revenue,\n  p.revenue as prod_revenue,\n  d.revenue - p.revenue as revenue_diff\nFROM dev_stats d\nFULL OUTER JOIN prod_stats p ON d.status = p.status\nORDER BY ABS(COALESCE(d.count, 0) - COALESCE(p.count, 0)) DESC\n```\n\n---\n\n## Common Gotchas\n\n### Always Use Fully-Qualified Names\n\n```sql\n-- Bad: Relies on session context\nSELECT * FROM orders\n\n-- Good: Explicit and portable\nSELECT * FROM ANALYTICS.MARTS.orders\n```\n\n### Dev Can Usually See Prod (Read-Only)\n\nMost warehouse configurations grant dev users read access to prod:\n- Your dev connection can query prod tables\n- This enables cross-environment comparisons\n- Writes are restricted to your dev schema\n\n### Schema Case Sensitivity\n\n| Warehouse | Case Behavior |\n|-----------|---------------|\n| Snowflake | Case-insensitive (stored uppercase) |\n| BigQuery | Case-sensitive |\n| Postgres | Case-insensitive (stored lowercase) |\n\n```sql\n-- Snowflake: These are equivalent\nSELECT * FROM ANALYTICS.MARTS.orders\nSELECT * FROM analytics.marts.orders\n\n-- BigQuery: Case matters\nSELECT * FROM `my-project.Analytics.Orders`  -- May fail if actual case differs\n```\n\n### Different Databases = Different Connections\n\nIf dev and prod are in separate databases, verify your connection can reach both:\n\n```sql\n-- Test prod access from dev connection\nSELECT 1 FROM ANALYTICS.INFORMATION_SCHEMA.TABLES LIMIT 1\n```\n\n### Prod May Have Stale Data\n\nDevelopment data is refreshed by your dbt runs. Production data follows a schedule:\n- Check when prod was last refreshed\n- Account for data lag in comparisons\n- Filter to comparable time windows\n\n```sql\n-- Filter to avoid comparing today's dev data to yesterday's prod\nWHERE order_date < CURRENT_DATE\n```\n\n---\n\n## Quick Reference\n\n| Task | Environment | Example |\n|------|-------------|---------|\n| Test my changes | Dev | `ANALYTICS_DEV.DEV_ADOVEN.orders` |\n| Production analysis | Prod | `ANALYTICS.MARTS.orders` |\n| Validate changes | Both | UNION ALL or EXCEPT queries |\n| Debug transformation | Dev | Query staging tables |\n| Business reporting | Prod | Query marts tables |\n",
        "plugins/dts/skills/dts/references/INTERPRETATION.md": "# Interpreting Query Results\n\nGuidelines for analyzing and explaining query results effectively.\n\n## Interpretation Framework\n\n### 1. State the Finding Clearly\n\nStart with the key takeaway:\n\n**Good:**\n> Orders decreased 23% week-over-week, from 1,247 to 961.\n\n**Avoid:**\n> The data shows some changes in the orders table.\n\n### 2. Provide Context\n\nCompare to baselines or expectations:\n\n> This 23% decrease is significant - the typical week-over-week variance is under 5%. The last time we saw a decrease this large was during the holiday shutdown in December.\n\n### 3. Quantify Impact\n\nUse specific numbers:\n\n| Metric | Value |\n|--------|-------|\n| Absolute change | -286 orders |\n| Percentage change | -23% |\n| Revenue impact | -$28,600 (estimated) |\n\n### 4. Note Limitations\n\nBe explicit about data constraints:\n\n> Note: This analysis only includes orders with status 'completed'. Pending orders (currently 47) are excluded and may affect final numbers.\n\n---\n\n## Common Interpretation Scenarios\n\n### Trends\n\nWhen interpreting time series:\n- Identify the direction (increasing, decreasing, stable)\n- Note the magnitude (slight, moderate, significant)\n- Compare to historical patterns\n- Look for inflection points\n\n**Example:**\n> Revenue has been declining steadily since January 15th, with a cumulative 18% drop. The decline accelerated in the past week (-8% vs -3% the week prior). This coincides with the mobile app update on January 14th.\n\n### Distributions\n\nWhen interpreting categorical breakdowns:\n- Identify the dominant categories\n- Note any surprises vs expectations\n- Look for concentration or dispersion\n\n**Example:**\n> Three products account for 67% of revenue: Widget Pro (32%), Widget Basic (21%), and Widget Plus (14%). This concentration has increased from 58% last quarter, suggesting customers are consolidating around fewer products.\n\n### Comparisons\n\nWhen comparing periods or segments:\n- State the comparison clearly\n- Quantify the difference\n- Test statistical significance if applicable\n\n**Example:**\n> Mobile orders (-31%) declined nearly 4x faster than desktop (-8%). This disparity suggests a mobile-specific issue rather than a general demand problem.\n\n### Anomalies\n\nWhen identifying outliers or unusual patterns:\n- Describe what's unexpected\n- Quantify the deviation\n- Suggest possible causes\n\n**Example:**\n> Customer #4521 has 847 orders this month, which is 12 standard deviations above the mean (avg: 3.2 orders/customer). This appears to be either a test account or a data quality issue - recommend investigation.\n\n---\n\n## Red Flags to Highlight\n\nAlways call out:\n\n- **Data quality issues**: NULLs, duplicates, orphaned records\n- **Unexpected values**: Negatives where positives expected, future dates\n- **Volume anomalies**: Sudden spikes or drops\n- **Concentration risks**: Over-reliance on single customers/products\n- **Trend changes**: Inflection points or reversals\n\n---\n\n## Interpretation Checklist\n\nBefore finalizing your interpretation:\n\n- [ ] Did I state the main finding in the first sentence?\n- [ ] Did I include specific numbers (not just \"increased\" or \"decreased\")?\n- [ ] Did I compare to a relevant baseline?\n- [ ] Did I note any data quality concerns?\n- [ ] Did I mention what's NOT in the data?\n- [ ] Did I suggest next steps if warranted?\n\n---\n\n## Example Full Interpretation\n\n**Query intent:** Understand why orders are down this week\n\n**Results:**\n| period | orders | revenue |\n|--------|--------|---------|\n| this_week | 961 | $96,100 |\n| last_week | 1,247 | $131,800 |\n\n**Interpretation:**\n\n> Orders decreased 23% this week (961 vs 1,247), with revenue down 27% ($96,100 vs $131,800). The larger revenue drop suggests higher-value orders were disproportionately affected.\n>\n> Looking at the breakdown by channel:\n> - Mobile: -31% (478 → 330)\n> - Desktop: -8% (769 → 631)\n>\n> The disparity strongly suggests a mobile-specific issue. This coincides with the iOS app update released on Monday.\n>\n> **Data quality note:** 12 orders are missing channel attribution and were excluded.\n>\n> **Recommended follow-up:**\n> 1. Check mobile checkout completion rates\n> 2. Review iOS app store reviews for reported issues\n> 3. Compare Android vs iOS performance\n",
        "plugins/dts/skills/dts/references/QUERY_PATTERNS.md": "# Query Patterns for Data Exploration\n\nCommon SQL patterns for exploring data models effectively.\n\n## Exploration Patterns\n\n### Overview Query\nGet a quick sense of the data:\n```sql\nSELECT\n  COUNT(*) as total_rows,\n  COUNT(DISTINCT customer_id) as unique_customers,\n  MIN(created_at) as earliest,\n  MAX(created_at) as latest\nFROM orders\n```\n\n### Distribution Analysis\nUnderstand value distributions:\n```sql\nSELECT\n  status,\n  COUNT(*) as count,\n  ROUND(100.0 * COUNT(*) / SUM(COUNT(*)) OVER(), 2) as pct\nFROM orders\nGROUP BY status\nORDER BY count DESC\n```\n\n### Time Series Trend\nAnalyze trends over time:\n```sql\nSELECT\n  DATE_TRUNC('day', created_at) as date,\n  COUNT(*) as count,\n  SUM(amount) as total\nFROM orders\nWHERE created_at >= CURRENT_DATE - INTERVAL '30 days'\nGROUP BY 1\nORDER BY 1\n```\n\n### Period Comparison\nCompare this period to last:\n```sql\nSELECT\n  CASE WHEN created_at >= CURRENT_DATE - INTERVAL '7 days' THEN 'this_week' ELSE 'last_week' END as period,\n  COUNT(*) as orders,\n  SUM(amount) as revenue\nFROM orders\nWHERE created_at >= CURRENT_DATE - INTERVAL '14 days'\nGROUP BY 1\n```\n\n---\n\n## Data Quality Patterns\n\n### NULL Analysis\nFind columns with missing values:\n```sql\nSELECT\n  COUNT(*) as total,\n  COUNT(customer_id) as has_customer,\n  COUNT(*) - COUNT(customer_id) as missing_customer,\n  ROUND(100.0 * (COUNT(*) - COUNT(customer_id)) / COUNT(*), 2) as pct_missing\nFROM orders\n```\n\n### Duplicate Detection\nFind potential duplicates:\n```sql\nSELECT\n  customer_id,\n  order_date,\n  COUNT(*) as occurrences\nFROM orders\nGROUP BY customer_id, order_date\nHAVING COUNT(*) > 1\nORDER BY occurrences DESC\nLIMIT 20\n```\n\n### Orphaned Records\nFind records without required relationships:\n```sql\nSELECT o.id, o.customer_id\nFROM orders o\nLEFT JOIN customers c ON o.customer_id = c.id\nWHERE c.id IS NULL\nLIMIT 20\n```\n\n### Value Range Check\nValidate expected ranges:\n```sql\nSELECT\n  MIN(amount) as min_amount,\n  MAX(amount) as max_amount,\n  AVG(amount) as avg_amount,\n  COUNT(CASE WHEN amount < 0 THEN 1 END) as negative_amounts,\n  COUNT(CASE WHEN amount > 10000 THEN 1 END) as large_amounts\nFROM orders\n```\n\n---\n\n## Diagnostic Patterns\n\n### Recent Changes\nSee most recent activity:\n```sql\nSELECT *\nFROM orders\nORDER BY updated_at DESC\nLIMIT 10\n```\n\n### Cohort Analysis\nGroup by acquisition cohort:\n```sql\nSELECT\n  DATE_TRUNC('month', c.created_at) as cohort_month,\n  COUNT(DISTINCT o.customer_id) as customers,\n  COUNT(o.id) as orders,\n  SUM(o.amount) as revenue\nFROM orders o\nJOIN customers c ON o.customer_id = c.id\nGROUP BY 1\nORDER BY 1\n```\n\n### Funnel Analysis\nTrack conversion through stages:\n```sql\nSELECT\n  COUNT(*) as total,\n  COUNT(CASE WHEN status IN ('pending', 'confirmed', 'shipped', 'delivered') THEN 1 END) as confirmed,\n  COUNT(CASE WHEN status IN ('shipped', 'delivered') THEN 1 END) as shipped,\n  COUNT(CASE WHEN status = 'delivered' THEN 1 END) as delivered\nFROM orders\nWHERE created_at >= CURRENT_DATE - INTERVAL '30 days'\n```\n\n---\n\n## Validation Patterns\n\nPatterns for validating data against expectations. Use with `/validate` command.\n\n### Row Count Validation\nVerify expected row counts:\n```sql\n-- Validate minimum row count\nSELECT\n  COUNT(*) as actual_rows,\n  COUNT(*) >= 1000000 as passed\nFROM ANALYTICS.MARTS.orders\n\n-- Validate row count in range\nSELECT\n  COUNT(*) as actual_rows,\n  COUNT(*) BETWEEN 900000 AND 1100000 as passed\nFROM ANALYTICS.MARTS.orders\n```\n\n### Null Check Validation\nVerify no nulls in required columns:\n```sql\n-- Single column null check\nSELECT\n  COUNT(*) as total_rows,\n  COUNT(*) - COUNT(order_id) as null_count,\n  COUNT(*) = COUNT(order_id) as passed\nFROM ANALYTICS.MARTS.orders\n\n-- Multi-column null check\nSELECT\n  'order_id' as column_name,\n  COUNT(*) - COUNT(order_id) as null_count\nFROM ANALYTICS.MARTS.orders\nUNION ALL\nSELECT\n  'customer_id' as column_name,\n  COUNT(*) - COUNT(customer_id) as null_count\nFROM ANALYTICS.MARTS.orders\n```\n\n### Uniqueness Validation\nVerify primary key or unique constraints:\n```sql\n-- Single column uniqueness\nSELECT\n  COUNT(*) as total_rows,\n  COUNT(DISTINCT order_id) as unique_values,\n  COUNT(*) = COUNT(DISTINCT order_id) as passed\nFROM ANALYTICS.MARTS.orders\n\n-- Composite key uniqueness\nSELECT\n  COUNT(*) as total_rows,\n  COUNT(DISTINCT customer_id || '-' || order_date) as unique_combinations,\n  COUNT(*) = COUNT(DISTINCT customer_id || '-' || order_date) as passed\nFROM ANALYTICS.MARTS.orders\n\n-- Find duplicates if validation fails\nSELECT\n  order_id,\n  COUNT(*) as occurrences\nFROM ANALYTICS.MARTS.orders\nGROUP BY order_id\nHAVING COUNT(*) > 1\nORDER BY occurrences DESC\nLIMIT 20\n```\n\n### Value Range Validation\nVerify values fall within expected bounds:\n```sql\n-- Numeric range validation\nSELECT\n  MIN(amount) as min_value,\n  MAX(amount) as max_value,\n  MIN(amount) >= 0 AND MAX(amount) <= 100000 as passed\nFROM ANALYTICS.MARTS.orders\n\n-- Date range validation (no future dates)\nSELECT\n  MAX(created_at) as max_date,\n  MAX(created_at) <= CURRENT_TIMESTAMP as passed\nFROM ANALYTICS.MARTS.orders\n\n-- Enum/allowed values validation\nSELECT\n  COUNT(*) as total,\n  COUNT(CASE WHEN status IN ('pending', 'confirmed', 'shipped', 'delivered', 'cancelled') THEN 1 END) as valid,\n  COUNT(*) = COUNT(CASE WHEN status IN ('pending', 'confirmed', 'shipped', 'delivered', 'cancelled') THEN 1 END) as passed\nFROM ANALYTICS.MARTS.orders\n```\n\n### Referential Integrity Validation\nVerify foreign key relationships:\n```sql\n-- All orders have valid customers\nSELECT\n  COUNT(*) as total_orders,\n  COUNT(CASE WHEN c.id IS NULL THEN 1 END) as orphaned_orders,\n  COUNT(CASE WHEN c.id IS NULL THEN 1 END) = 0 as passed\nFROM ANALYTICS.MARTS.orders o\nLEFT JOIN ANALYTICS.MARTS.customers c ON o.customer_id = c.id\n\n-- Find orphaned records if validation fails\nSELECT o.id, o.customer_id\nFROM ANALYTICS.MARTS.orders o\nLEFT JOIN ANALYTICS.MARTS.customers c ON o.customer_id = c.id\nWHERE c.id IS NULL\nLIMIT 20\n```\n\n---\n\n## Cross-Environment Comparison\n\nPatterns for comparing dev to prod. Use with `/validate against prod`.\n\n### Row Count Comparison\nCompare row counts between environments:\n```sql\nSELECT 'dev' as env, COUNT(*) as row_count\nFROM ANALYTICS_DEV.DEV_ADOVEN.orders\nUNION ALL\nSELECT 'prod' as env, COUNT(*) as row_count\nFROM ANALYTICS.MARTS.orders\n```\n\n### Schema Comparison\nCompare column structure (Snowflake example):\n```sql\nSELECT\n  d.column_name,\n  d.data_type as dev_type,\n  p.data_type as prod_type,\n  CASE WHEN d.data_type = p.data_type THEN 'MATCH' ELSE 'MISMATCH' END as status\nFROM ANALYTICS_DEV.INFORMATION_SCHEMA.COLUMNS d\nFULL OUTER JOIN ANALYTICS.INFORMATION_SCHEMA.COLUMNS p\n  ON d.column_name = p.column_name\nWHERE d.table_name = 'ORDERS' AND p.table_name = 'ORDERS'\n  AND d.table_schema = 'DEV_ADOVEN' AND p.table_schema = 'MARTS'\nORDER BY d.ordinal_position\n```\n\n### Data Diff - Rows Only in Dev (Additions)\nFind new rows in dev not present in prod:\n```sql\nSELECT * FROM ANALYTICS_DEV.DEV_ADOVEN.orders\nEXCEPT\nSELECT * FROM ANALYTICS.MARTS.orders\nLIMIT 100\n```\n\n### Data Diff - Rows Only in Prod (Deletions)\nFind rows in prod missing from dev:\n```sql\nSELECT * FROM ANALYTICS.MARTS.orders\nEXCEPT\nSELECT * FROM ANALYTICS_DEV.DEV_ADOVEN.orders\nLIMIT 100\n```\n\n### Data Diff - By Primary Key\nMore efficient comparison using keys:\n```sql\n-- Rows in dev but not prod\nSELECT d.*\nFROM ANALYTICS_DEV.DEV_ADOVEN.orders d\nWHERE NOT EXISTS (\n  SELECT 1 FROM ANALYTICS.MARTS.orders p\n  WHERE p.order_id = d.order_id\n)\nLIMIT 100\n\n-- Rows in prod but not dev\nSELECT p.*\nFROM ANALYTICS.MARTS.orders p\nWHERE NOT EXISTS (\n  SELECT 1 FROM ANALYTICS_DEV.DEV_ADOVEN.orders d\n  WHERE d.order_id = p.order_id\n)\nLIMIT 100\n```\n\n### Aggregate Comparison\nCompare summary statistics:\n```sql\nSELECT\n  'dev' as env,\n  COUNT(*) as total_orders,\n  COUNT(DISTINCT customer_id) as unique_customers,\n  SUM(amount) as total_revenue,\n  AVG(amount) as avg_order_value\nFROM ANALYTICS_DEV.DEV_ADOVEN.orders\n\nUNION ALL\n\nSELECT\n  'prod' as env,\n  COUNT(*) as total_orders,\n  COUNT(DISTINCT customer_id) as unique_customers,\n  SUM(amount) as total_revenue,\n  AVG(amount) as avg_order_value\nFROM ANALYTICS.MARTS.orders\n```\n\n### Distribution Comparison\nCompare value distributions side-by-side:\n```sql\nWITH dev_dist AS (\n  SELECT status, COUNT(*) as cnt\n  FROM ANALYTICS_DEV.DEV_ADOVEN.orders\n  GROUP BY status\n),\nprod_dist AS (\n  SELECT status, COUNT(*) as cnt\n  FROM ANALYTICS.MARTS.orders\n  GROUP BY status\n)\nSELECT\n  COALESCE(d.status, p.status) as status,\n  d.cnt as dev_count,\n  p.cnt as prod_count,\n  d.cnt - p.cnt as diff,\n  ROUND(100.0 * (d.cnt - p.cnt) / NULLIF(p.cnt, 0), 2) as pct_change\nFROM dev_dist d\nFULL OUTER JOIN prod_dist p ON d.status = p.status\nORDER BY ABS(COALESCE(d.cnt, 0) - COALESCE(p.cnt, 0)) DESC\n```\n\n### Time-Bounded Comparison\nCompare only overlapping time periods:\n```sql\n-- Get overlapping date range\nWITH date_bounds AS (\n  SELECT\n    GREATEST(\n      (SELECT MIN(created_at) FROM ANALYTICS_DEV.DEV_ADOVEN.orders),\n      (SELECT MIN(created_at) FROM ANALYTICS.MARTS.orders)\n    ) as start_date,\n    LEAST(\n      (SELECT MAX(created_at) FROM ANALYTICS_DEV.DEV_ADOVEN.orders),\n      (SELECT MAX(created_at) FROM ANALYTICS.MARTS.orders)\n    ) as end_date\n)\nSELECT\n  'dev' as env,\n  COUNT(*) as row_count\nFROM ANALYTICS_DEV.DEV_ADOVEN.orders, date_bounds\nWHERE created_at BETWEEN start_date AND end_date\n\nUNION ALL\n\nSELECT\n  'prod' as env,\n  COUNT(*) as row_count\nFROM ANALYTICS.MARTS.orders, date_bounds\nWHERE created_at BETWEEN start_date AND end_date\n```\n\n---\n\n## Best Practices\n\n### Always Include\n- `LIMIT` clause for exploration queries (start with 100)\n- Date filters to bound the query scope\n- `ORDER BY` for meaningful result ordering\n\n### Avoid\n- `SELECT *` without LIMIT\n- Cross joins without conditions\n- Unbounded date ranges on large tables\n\n### Query Comments\nInclude intent in your queries:\n```sql\n-- Check: Are mobile orders declining faster than desktop?\nSELECT\n  device_type,\n  DATE_TRUNC('week', created_at) as week,\n  COUNT(*) as orders\nFROM orders\nWHERE created_at >= CURRENT_DATE - INTERVAL '8 weeks'\nGROUP BY 1, 2\nORDER BY 2, 1\n```\n",
        "plugins/dts/skills/dts/references/VISUALIZATION.md": "# Visualization Selection Guidelines\n\nChoose the right chart type for your data and message.\n\n## Chart Selection Matrix\n\n| Data Type | Purpose | Recommended Chart |\n|-----------|---------|-------------------|\n| Time series | Show trend | `line` |\n| Categories | Compare values | `bar` |\n| Parts of whole | Show composition | `pie` (if <7 categories) |\n| Detailed breakdown | Exact values | `table` |\n| Distribution | Show spread | `bar` (histogram) |\n| Correlation | Show relationship | `line` (scatter) |\n\n---\n\n## Chart Type Details\n\n### Line Chart\n\n**Best for:** Time series, trends, continuous data\n\n**When to use:**\n- Showing change over time\n- Comparing multiple series over same time period\n- Highlighting trend direction\n\n**Visualization hint format:**\n```json\n{\n  \"chart_type\": \"line\",\n  \"x_column\": \"date\",\n  \"y_column\": \"order_count\",\n  \"title\": \"Daily Orders - Last 30 Days\"\n}\n```\n\n**Tips:**\n- Use for 7+ data points\n- Consider dual y-axis for different scales\n- Highlight key inflection points\n\n---\n\n### Bar Chart\n\n**Best for:** Categorical comparisons, discrete values\n\n**When to use:**\n- Comparing values across categories\n- Showing rankings\n- Period-over-period comparisons\n\n**Visualization hint format:**\n```json\n{\n  \"chart_type\": \"bar\",\n  \"x_column\": \"product_category\",\n  \"y_column\": \"revenue\",\n  \"title\": \"Revenue by Product Category\"\n}\n```\n\n**Tips:**\n- Sort by value (largest first) unless there's natural ordering\n- Limit to 10-12 bars maximum\n- Use horizontal bars for long category names\n\n---\n\n### Pie Chart\n\n**Best for:** Parts of a whole, proportions\n\n**When to use:**\n- Showing percentage breakdown\n- When parts sum to 100%\n- 6 or fewer categories\n\n**Visualization hint format:**\n```json\n{\n  \"chart_type\": \"pie\",\n  \"label_column\": \"status\",\n  \"value_column\": \"count\",\n  \"title\": \"Order Status Distribution\"\n}\n```\n\n**Tips:**\n- Avoid if more than 6 categories\n- Order slices by size\n- Consider bar chart as alternative (often clearer)\n\n---\n\n### Table\n\n**Best for:** Detailed data, exact values\n\n**When to use:**\n- Multiple metrics per category\n- Precise values matter\n- Detailed breakdown needed\n\n**Visualization hint format:**\n```json\n{\n  \"chart_type\": \"table\",\n  \"columns\": [\"product\", \"orders\", \"revenue\", \"avg_order_value\"],\n  \"title\": \"Product Performance Summary\"\n}\n```\n\n**Tips:**\n- Limit rows to keep scannable\n- Right-align numbers\n- Include totals row if appropriate\n\n---\n\n## Multiple Visualizations\n\nFor complex analyses, suggest multiple complementary charts:\n\n```json\n{\n  \"visualizations\": [\n    {\n      \"chart_type\": \"line\",\n      \"x_column\": \"date\",\n      \"y_column\": \"orders\",\n      \"title\": \"Order Trend\"\n    },\n    {\n      \"chart_type\": \"bar\",\n      \"x_column\": \"channel\",\n      \"y_column\": \"orders\",\n      \"title\": \"Orders by Channel\"\n    }\n  ]\n}\n```\n\n---\n\n## Visualization Principles\n\n### Do\n- Choose chart type based on the question being answered\n- Title charts to highlight the key insight\n- Label axes clearly\n- Use consistent colors for same categories across charts\n\n### Don't\n- Use pie charts for more than 6 categories\n- Start y-axis at non-zero without noting it\n- Use 3D effects (they distort perception)\n- Overcrowd with too many data series\n\n---\n\n## Examples by Question Type\n\n**\"What's the trend?\"** → Line chart\n```json\n{\"chart_type\": \"line\", \"x_column\": \"week\", \"y_column\": \"revenue\", \"title\": \"Weekly Revenue Trend\"}\n```\n\n**\"How do segments compare?\"** → Bar chart\n```json\n{\"chart_type\": \"bar\", \"x_column\": \"region\", \"y_column\": \"sales\", \"title\": \"Sales by Region\"}\n```\n\n**\"What's the breakdown?\"** → Pie or bar\n```json\n{\"chart_type\": \"pie\", \"label_column\": \"category\", \"value_column\": \"pct\", \"title\": \"Revenue Mix\"}\n```\n\n**\"What are the details?\"** → Table\n```json\n{\"chart_type\": \"table\", \"columns\": [\"product\", \"qty\", \"revenue\", \"margin\"], \"title\": \"Product Details\"}\n```\n"
      },
      "plugins": [
        {
          "name": "dts",
          "description": "Data Tool Suite - LLM-observable workflows for dbt projects with query execution and interpretation",
          "source": "./",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add Big-Time-Data/homebrew-dts",
            "/plugin install dts@dts-marketplace"
          ]
        }
      ]
    }
  ]
}