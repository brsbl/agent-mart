{
  "author": {
    "id": "EricGrill",
    "display_name": "Eric Grill",
    "avatar_url": "https://avatars.githubusercontent.com/u/694055?v=4"
  },
  "marketplaces": [
    {
      "name": "agents-skills-plugins",
      "version": null,
      "description": "A curated collection of Claude Code plugins, agents, and skills",
      "repo_full_name": "EricGrill/agents-skills-plugins",
      "repo_url": "https://github.com/EricGrill/agents-skills-plugins",
      "repo_description": "A curated collection of Claude Code skills and agents",
      "signals": {
        "stars": 5,
        "forks": 0,
        "pushed_at": "2026-01-25T02:48:12Z"
      },
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"$schema\": \"https://anthropic.com/claude-code/marketplace.schema.json\",\n  \"name\": \"agents-skills-plugins\",\n  \"owner\": {\n    \"name\": \"Eric Grill\",\n    \"email\": \"ericgrill@example.com\"\n  },\n  \"metadata\": {\n    \"description\": \"A curated collection of Claude Code plugins, agents, and skills\",\n    \"version\": \"1.0.0\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"nano-banana\",\n      \"source\": \"./plugins/nano-banana\",\n      \"description\": \"Image generation using Google's Gemini API\",\n      \"version\": \"1.0.0\",\n      \"author\": { \"name\": \"Eric Grill\" },\n      \"category\": \"ai\"\n    },\n    {\n      \"name\": \"mcp-proxmox-admin\",\n      \"source\": \"./plugins/mcp-proxmox-admin\",\n      \"description\": \"Proxmox VE infrastructure management via MCP with VM, container, and snapshot control\",\n      \"version\": \"1.0.0\",\n      \"author\": { \"name\": \"Eric Grill\" },\n      \"category\": \"devops\"\n    },\n    {\n      \"name\": \"mcp-multi-agent-ssh\",\n      \"source\": \"./plugins/mcp-multi-agent-ssh\",\n      \"description\": \"Persistent SSH connections with encrypted credential storage and SFTP support\",\n      \"version\": \"1.0.0\",\n      \"author\": { \"name\": \"Eric Grill\" },\n      \"category\": \"devops\"\n    },\n    {\n      \"name\": \"mcp-kali-orchestration\",\n      \"source\": \"./plugins/mcp-kali-orchestration\",\n      \"description\": \"Kali Linux orchestration with 50+ security tools for authorized pentesting\",\n      \"version\": \"1.0.0\",\n      \"author\": { \"name\": \"Eric Grill\" },\n      \"category\": \"security\"\n    },\n    {\n      \"name\": \"mcp-multi-agent-server-delegation\",\n      \"source\": \"./plugins/mcp-multi-agent-server-delegation\",\n      \"description\": \"Task delegation to isolated Proxmox VMs with automatic cleanup and callbacks\",\n      \"version\": \"1.0.0\",\n      \"author\": { \"name\": \"Eric Grill\" },\n      \"category\": \"devops\"\n    },\n    {\n      \"name\": \"mcp-predictive-market\",\n      \"source\": \"./plugins/mcp-predictive-market\",\n      \"description\": \"Query 5 prediction markets with arbitrage detection and comparative analysis\",\n      \"version\": \"1.0.0\",\n      \"author\": { \"name\": \"Eric Grill\" },\n      \"category\": \"ai\"\n    },\n    {\n      \"name\": \"mcp-bitcoin-cli\",\n      \"source\": \"./plugins/mcp-bitcoin-cli\",\n      \"description\": \"Bitcoin OP_RETURN operations for documents, timestamps, and BRC-20 tokens\",\n      \"version\": \"1.0.0\",\n      \"author\": { \"name\": \"Eric Grill\" },\n      \"category\": \"development\"\n    },\n    {\n      \"name\": \"mcp-civic-data\",\n      \"source\": \"./plugins/mcp-civic-data\",\n      \"description\": \"Access 7 government APIs for weather, census, NASA, and economic data\",\n      \"version\": \"1.0.0\",\n      \"author\": { \"name\": \"Eric Grill\" },\n      \"category\": \"productivity\"\n    },\n    {\n      \"name\": \"mcp-memvid-state-service\",\n      \"source\": \"./plugins/mcp-memvid-state-service\",\n      \"description\": \"AI memory layer with vector search, full-text search, and temporal queries\",\n      \"version\": \"1.0.0\",\n      \"author\": { \"name\": \"Eric Grill\" },\n      \"category\": \"ai\"\n    },\n    {\n      \"name\": \"superpowers\",\n      \"source\": \"./plugins/superpowers\",\n      \"description\": \"Core skills library: TDD, debugging, collaboration patterns\",\n      \"version\": \"4.1.0\",\n      \"author\": { \"name\": \"Jesse Vincent\" },\n      \"category\": \"development\"\n    },\n    {\n      \"name\": \"agent-orchestration\",\n      \"source\": \"./plugins/agent-orchestration\",\n      \"description\": \"Context management and multi-agent orchestration\",\n      \"version\": \"1.0.0\",\n      \"author\": { \"name\": \"wshobson\" },\n      \"category\": \"ai\"\n    },\n    {\n      \"name\": \"blockchain-web3\",\n      \"source\": \"./plugins/blockchain-web3\",\n      \"description\": \"Blockchain development with Solidity security, DeFi, NFTs\",\n      \"version\": \"1.0.0\",\n      \"author\": { \"name\": \"wshobson\" },\n      \"category\": \"development\"\n    },\n    {\n      \"name\": \"business-analytics\",\n      \"source\": \"./plugins/business-analytics\",\n      \"description\": \"Business analysis with data storytelling and KPI dashboards\",\n      \"version\": \"1.0.0\",\n      \"author\": { \"name\": \"wshobson\" },\n      \"category\": \"productivity\"\n    },\n    {\n      \"name\": \"code-documentation\",\n      \"source\": \"./plugins/code-documentation\",\n      \"description\": \"Code documentation with automated doc generation\",\n      \"version\": \"1.0.0\",\n      \"author\": { \"name\": \"wshobson\" },\n      \"category\": \"development\"\n    },\n    {\n      \"name\": \"comprehensive-review\",\n      \"source\": \"./plugins/comprehensive-review\",\n      \"description\": \"Comprehensive code review with architecture and security\",\n      \"version\": \"1.0.0\",\n      \"author\": { \"name\": \"wshobson\" },\n      \"category\": \"development\"\n    },\n    {\n      \"name\": \"seo-analysis-monitoring\",\n      \"source\": \"./plugins/seo-analysis-monitoring\",\n      \"description\": \"SEO analysis with authority building and content refresh\",\n      \"version\": \"1.0.0\",\n      \"author\": { \"name\": \"wshobson\" },\n      \"category\": \"marketing\"\n    },\n    {\n      \"name\": \"seo-content-creation\",\n      \"source\": \"./plugins/seo-content-creation\",\n      \"description\": \"SEO content creation with auditing and planning\",\n      \"version\": \"1.0.0\",\n      \"author\": { \"name\": \"wshobson\" },\n      \"category\": \"marketing\"\n    },\n    {\n      \"name\": \"seo-technical-optimization\",\n      \"source\": \"./plugins/seo-technical-optimization\",\n      \"description\": \"Technical SEO with keyword strategy and meta optimization\",\n      \"version\": \"1.0.0\",\n      \"author\": { \"name\": \"wshobson\" },\n      \"category\": \"marketing\"\n    },\n    {\n      \"name\": \"team-collaboration\",\n      \"source\": \"./plugins/team-collaboration\",\n      \"description\": \"Team collaboration with DX optimization and standups\",\n      \"version\": \"1.0.0\",\n      \"author\": { \"name\": \"wshobson\" },\n      \"category\": \"productivity\"\n    },\n    {\n      \"name\": \"unit-testing\",\n      \"source\": \"./plugins/unit-testing\",\n      \"description\": \"Unit testing with debugging and test automation\",\n      \"version\": \"1.0.0\",\n      \"author\": { \"name\": \"wshobson\" },\n      \"category\": \"development\"\n    },\n    {\n      \"name\": \"python-development\",\n      \"source\": \"./plugins/python-development\",\n      \"description\": \"Python development with Django, FastAPI, async patterns\",\n      \"version\": \"1.0.0\",\n      \"author\": { \"name\": \"wshobson\" },\n      \"category\": \"development\"\n    },\n    {\n      \"name\": \"llm-application-dev\",\n      \"source\": \"./plugins/llm-application-dev\",\n      \"description\": \"LLM application development with RAG and embeddings\",\n      \"version\": \"1.0.0\",\n      \"author\": { \"name\": \"wshobson\" },\n      \"category\": \"ai\"\n    },\n    {\n      \"name\": \"javascript-typescript\",\n      \"source\": \"./plugins/javascript-typescript\",\n      \"description\": \"JavaScript and TypeScript development with modern patterns\",\n      \"version\": \"1.0.0\",\n      \"author\": { \"name\": \"wshobson\" },\n      \"category\": \"development\"\n    },\n    {\n      \"name\": \"git-pr-workflows\",\n      \"source\": \"./plugins/git-pr-workflows\",\n      \"description\": \"Git and PR workflows with code review and onboarding\",\n      \"version\": \"1.0.0\",\n      \"author\": { \"name\": \"wshobson\" },\n      \"category\": \"development\"\n    },\n    {\n      \"name\": \"game-development\",\n      \"source\": \"./plugins/game-development\",\n      \"description\": \"Game development with Unity, Godot, and Minecraft\",\n      \"version\": \"1.0.0\",\n      \"author\": { \"name\": \"wshobson\" },\n      \"category\": \"development\"\n    },\n    {\n      \"name\": \"full-stack-orchestration\",\n      \"source\": \"./plugins/full-stack-orchestration\",\n      \"description\": \"Full-stack orchestration with deployment and security\",\n      \"version\": \"1.0.0\",\n      \"author\": { \"name\": \"wshobson\" },\n      \"category\": \"development\"\n    },\n    {\n      \"name\": \"content-marketing\",\n      \"source\": \"./plugins/content-marketing\",\n      \"description\": \"Content marketing with strategy and search agents\",\n      \"version\": \"1.0.0\",\n      \"author\": { \"name\": \"wshobson\" },\n      \"category\": \"marketing\"\n    },\n    {\n      \"name\": \"context-management\",\n      \"source\": \"./plugins/context-management\",\n      \"description\": \"Context management with save and restore capabilities\",\n      \"version\": \"1.0.0\",\n      \"author\": { \"name\": \"wshobson\" },\n      \"category\": \"productivity\"\n    },\n    {\n      \"name\": \"customer-sales-automation\",\n      \"source\": \"./plugins/customer-sales-automation\",\n      \"description\": \"Customer support and sales automation agents\",\n      \"version\": \"1.0.0\",\n      \"author\": { \"name\": \"wshobson\" },\n      \"category\": \"productivity\"\n    },\n    {\n      \"name\": \"database-design\",\n      \"source\": \"./plugins/database-design\",\n      \"description\": \"Database architecture and SQL optimization\",\n      \"version\": \"1.0.0\",\n      \"author\": { \"name\": \"wshobson\" },\n      \"category\": \"development\"\n    },\n    {\n      \"name\": \"data-validation-suite\",\n      \"source\": \"./plugins/data-validation-suite\",\n      \"description\": \"Data validation and backend security coding\",\n      \"version\": \"1.0.0\",\n      \"author\": { \"name\": \"wshobson\" },\n      \"category\": \"development\"\n    },\n    {\n      \"name\": \"deployment-strategies\",\n      \"source\": \"./plugins/deployment-strategies\",\n      \"description\": \"Deployment engineering with Terraform and IaC\",\n      \"version\": \"1.0.0\",\n      \"author\": { \"name\": \"wshobson\" },\n      \"category\": \"devops\"\n    },\n    {\n      \"name\": \"developer-essentials\",\n      \"source\": \"./plugins/developer-essentials\",\n      \"description\": \"Essential developer skills for monorepos and debugging\",\n      \"version\": \"1.0.0\",\n      \"author\": { \"name\": \"wshobson\" },\n      \"category\": \"development\"\n    },\n    {\n      \"name\": \"documentation-generation\",\n      \"source\": \"./plugins/documentation-generation\",\n      \"description\": \"Documentation generation with API docs and diagrams\",\n      \"version\": \"1.0.0\",\n      \"author\": { \"name\": \"wshobson\" },\n      \"category\": \"development\"\n    },\n    {\n      \"name\": \"frontend-mobile-development\",\n      \"source\": \"./plugins/frontend-mobile-development\",\n      \"description\": \"Frontend and mobile development with React and Tailwind\",\n      \"version\": \"1.0.0\",\n      \"author\": { \"name\": \"wshobson\" },\n      \"category\": \"development\"\n    },\n    {\n      \"name\": \"frontend-mobile-security\",\n      \"source\": \"./plugins/frontend-mobile-security\",\n      \"description\": \"Frontend and mobile security with XSS scanning\",\n      \"version\": \"1.0.0\",\n      \"author\": { \"name\": \"wshobson\" },\n      \"category\": \"security\"\n    },\n    {\n      \"name\": \"awesome-claude-skills\",\n      \"source\": \"./plugins/awesome-claude-skills\",\n      \"description\": \"27 practical Claude Skills for documents and productivity\",\n      \"version\": \"1.0.0\",\n      \"author\": { \"name\": \"ComposioHQ\" },\n      \"category\": \"productivity\"\n    },\n    {\n      \"name\": \"ios-simulator-skill\",\n      \"source\": \"./plugins/ios-simulator-skill\",\n      \"description\": \"iOS Simulator automation with 21 scripts for testing\",\n      \"version\": \"1.0.0\",\n      \"author\": { \"name\": \"conorluddy\" },\n      \"category\": \"development\"\n    },\n    {\n      \"name\": \"multi-agent-patterns\",\n      \"source\": \"./plugins/multi-agent-patterns\",\n      \"description\": \"Multi-agent architecture patterns for context isolation\",\n      \"version\": \"1.0.0\",\n      \"author\": { \"name\": \"muratcankoylan\" },\n      \"category\": \"ai\"\n    },\n    {\n      \"name\": \"beautiful-prose\",\n      \"source\": \"./plugins/beautiful-prose\",\n      \"description\": \"Hard-edged writing style skill for forceful prose\",\n      \"version\": \"1.0.0\",\n      \"author\": { \"name\": \"SHADOWPR0\" },\n      \"category\": \"productivity\"\n    },\n    {\n      \"name\": \"ralph-wiggum-marketer\",\n      \"source\": \"./plugins/ralph-wiggum-marketer\",\n      \"description\": \"Autonomous AI copywriter for SaaS content marketing\",\n      \"version\": \"1.0.0\",\n      \"author\": { \"name\": \"muratcankoylan\" },\n      \"category\": \"marketing\"\n    },\n    {\n      \"name\": \"ai-investigator\",\n      \"source\": \"./plugins/ai-investigator\",\n      \"description\": \"Enterprise AI case study analyzer with Claude and Firecrawl\",\n      \"version\": \"1.0.0\",\n      \"author\": { \"name\": \"muratcankoylan\" },\n      \"category\": \"ai\"\n    },\n    {\n      \"name\": \"rosetta-prompt\",\n      \"source\": \"./plugins/rosetta-prompt\",\n      \"description\": \"Prompt optimization for different AI providers using multi-agent ReAct\",\n      \"version\": \"1.0.0\",\n      \"author\": { \"name\": \"muratcankoylan\" },\n      \"category\": \"ai\"\n    },\n    {\n      \"name\": \"readwren\",\n      \"source\": \"./plugins/readwren\",\n      \"description\": \"Multi-agent literary interview for extracting reading profiles\",\n      \"version\": \"1.0.0\",\n      \"author\": { \"name\": \"muratcankoylan\" },\n      \"category\": \"productivity\"\n    },\n    {\n      \"name\": \"food-tour-planner\",\n      \"source\": \"./plugins/food-tour-planner\",\n      \"description\": \"AI food tour planner with DeepAgents and Google Maps\",\n      \"version\": \"1.0.0\",\n      \"author\": { \"name\": \"muratcankoylan\" },\n      \"category\": \"productivity\"\n    },\n    {\n      \"name\": \"actual-code\",\n      \"source\": \"./plugins/actual-code\",\n      \"description\": \"7-agent code assessment generator from GitHub repos\",\n      \"version\": \"1.0.0\",\n      \"author\": { \"name\": \"muratcankoylan\" },\n      \"category\": \"development\"\n    },\n    {\n      \"name\": \"book-training\",\n      \"source\": \"./plugins/book-training\",\n      \"description\": \"Style transfer pipeline for author-style LLM training with LoRA\",\n      \"version\": \"1.0.0\",\n      \"author\": { \"name\": \"muratcankoylan\" },\n      \"category\": \"ai\"\n    },\n    {\n      \"name\": \"linkedin-analyzer\",\n      \"source\": \"./plugins/linkedin-analyzer\",\n      \"description\": \"LinkedIn profile analysis with Cohere Command R+\",\n      \"version\": \"1.0.0\",\n      \"author\": { \"name\": \"muratcankoylan\" },\n      \"category\": \"productivity\"\n    },\n    {\n      \"name\": \"feed2context\",\n      \"source\": \"./plugins/feed2context\",\n      \"description\": \"One-click feed to research report for LinkedIn and X\",\n      \"version\": \"1.0.0\",\n      \"author\": { \"name\": \"muratcankoylan\" },\n      \"category\": \"productivity\"\n    }\n  ]\n}\n",
        ".claude-plugin/plugin.json": "{\n  \"name\": \"agent-exchange\",\n  \"description\": \"Marketplace management plugin with an agent to find and add Claude Code plugins\",\n  \"version\": \"1.0.0\",\n  \"author\": {\n    \"name\": \"Eric Grill\"\n  },\n  \"repository\": \"EricGrill/agents-skills-plugins\",\n  \"keywords\": [\"marketplace\", \"plugin-discovery\", \"agent-exchange\"]\n}\n",
        "README.md": "<p align=\"center\">\n  <h1 align=\"center\">Claude Code Plugin Marketplace</h1>\n  <p align=\"center\">\n    <strong>Open-source plugins, agents, and skills for Claude Code CLI</strong>\n  </p>\n  <p align=\"center\">\n    <a href=\"https://github.com/EricGrill/agents-skills-plugins/blob/main/LICENSE\"><img src=\"https://img.shields.io/badge/license-MIT-blue.svg\" alt=\"MIT License\"></a>\n    <img src=\"https://img.shields.io/badge/plugins-49-green.svg\" alt=\"49 Plugins\">\n    <img src=\"https://img.shields.io/badge/agents-70+-purple.svg\" alt=\"70+ Agents\">\n    <img src=\"https://img.shields.io/badge/skills-110+-orange.svg\" alt=\"110+ Skills\">\n  </p>\n  <p align=\"center\">\n    <a href=\"#-quick-start\">Quick Start</a> |\n    <a href=\"#-plugin-catalog\">Catalog</a> |\n    <a href=\"#-all-plugins\">Full List</a> |\n    <a href=\"#-contributing\">Contributing</a>\n  </p>\n</p>\n\n## What is this?\n\nA community-maintained collection of plugins that extend Claude Code with specialized capabilities. Each plugin adds agents, skills, or commands that help with specific development tasks—from TDD and debugging to document generation and DevOps automation.\n\n**This marketplace aggregates plugins from multiple authors** so you can discover, install, and manage them from one place.\n\n---\n\n## Quick Start\n\nGet started in two commands:\n\n```bash\n# Add the marketplace to Claude Code\n/plugin marketplace add EricGrill/agents-skills-plugins\n\n# Install any plugin from the catalog\n/plugin install <plugin-name>@agents-skills-plugins\n```\n\n**New to Claude Code plugins?** Start with **superpowers** - it includes essential skills for TDD, debugging, and code review that work with any project.\n\n---\n\n## Why Use a Plugin Marketplace?\n\n| Benefit | Description |\n|---------|-------------|\n| **One-command install** | No manual setup. Install plugins directly within Claude Code |\n| **Curated collection** | Plugins are reviewed and organized by use case |\n| **Auto-sync** | The marketplace updates weekly from upstream sources |\n| **Community-driven** | Contributions welcome from any plugin author |\n\n---\n\n## Plugin Catalog\n\n### By Category\n\n| Category | Plugins | Best For |\n|----------|---------|----------|\n| [Core & Workflows](#-core--workflows) | superpowers, developer-essentials, git-pr-workflows | Every developer |\n| [Documents & Productivity](#-documents--productivity) | awesome-claude-skills (27 skills!) | Office docs, media, productivity |\n| [Languages](#-languages) | python-development, javascript-typescript | Language-specific development |\n| [AI & LLM](#-ai--llm-development) | llm-application-dev, agent-orchestration | Building AI applications |\n| [Code Quality](#-code-quality) | comprehensive-review, unit-testing, code-documentation | Better code |\n| [Frontend & Mobile](#-frontend--mobile) | frontend-mobile-development, frontend-mobile-security | Web & mobile apps |\n| [DevOps](#-devops--infrastructure) | deployment-strategies, full-stack-orchestration | Infrastructure & deployment |\n| [MCP Servers](#-mcp-servers) | mcp-proxmox-admin, mcp-kali-orchestration, mcp-bitcoin-cli | Claude tool integrations |\n| [SEO & Marketing](#-seo--marketing) | seo-content-creation, content-marketing | Content & SEO |\n| [Data & Backend](#-data--backend) | database-design, data-validation-suite | Data management |\n| [Specialized](#-specialized) | blockchain-web3, game-development | Domain-specific |\n\n---\n\n## All Plugins\n\n### Core & Workflows\n\nEssential plugins that every developer should consider.\n\n<details>\n<summary><b>superpowers</b> - Core skills library (14 skills, 3 commands)</summary>\n\nThe foundation for effective Claude Code usage. Includes TDD, debugging, code review, and collaboration patterns.\n\n> **Read the guide:** [Brainstorming Superpower Guide](https://ericgrill.com/blog/superpowers-brainstorming-guide) — How to use the brainstorming skill to explore ideas before writing code.\n\n```\n/plugin install superpowers@agents-skills-plugins\n```\n\n| Skills | Commands |\n|--------|----------|\n| brainstorming, dispatching-parallel-agents, executing-plans, finishing-a-development-branch, receiving-code-review, requesting-code-review, subagent-driven-development, systematic-debugging, test-driven-development, using-git-worktrees, using-superpowers, verification-before-completion, writing-plans, writing-skills | `/brainstorm` `/write-plan` `/execute-plan` |\n\n*Forked from [obra/superpowers](https://github.com/obra/superpowers)*\n\n</details>\n\n<details>\n<summary><b>developer-essentials</b> - Monorepos, debugging, testing (11 skills)</summary>\n\nEssential patterns for modern development workflows including monorepo management, build optimization, and advanced git.\n\n> **Read the guide:** [Developer-Essentials Monorepo Guide](https://ericgrill.com/blog/developer-essentials-monorepo-guide) — Setting up monorepo architecture with Nx, Turborepo, and Bazel.\n\n```\n/plugin install developer-essentials@agents-skills-plugins\n```\n\n| Agents | Skills |\n|--------|--------|\n| monorepo-architect | auth-implementation-patterns, bazel-build-optimization, code-review-excellence, debugging-strategies, e2e-testing-patterns, error-handling-patterns, git-advanced-workflows, monorepo-management, nx-workspace-patterns, sql-optimization-patterns, turborepo-caching |\n\n</details>\n\n<details>\n<summary><b>git-pr-workflows</b> - Git and PR automation (3 commands)</summary>\n\nStreamline your git workflow with code review, onboarding, and PR enhancement tools.\n\n> **Read the guide:** [Git-PR-Workflows Plugin](https://ericgrill.com/blog/git-pr-workflows-plugin) — Automate code reviews and PR enhancements with Claude Code.\n\n```\n/plugin install git-pr-workflows@agents-skills-plugins\n```\n\n| Agents | Commands |\n|--------|----------|\n| code-reviewer | `/git-workflow` `/onboard` `/pr-enhance` |\n\n</details>\n\n<details>\n<summary><b>team-collaboration</b> - DX optimization and standups (2 commands)</summary>\n\nImprove team workflows with issue tracking and standup automation.\n\n> **Read the guide:** [Team-Collaboration Plugin](https://ericgrill.com/blog/team-collaboration-plugin) — Streamline standups and issue tracking for dev teams.\n\n```\n/plugin install team-collaboration@agents-skills-plugins\n```\n\n| Agents | Commands |\n|--------|----------|\n| dx-optimizer | `/issue` `/standup-notes` |\n\n</details>\n\n<details>\n<summary><b>context-management</b> - Save and restore context (2 commands)</summary>\n\nNever lose your place. Save and restore conversation context across sessions.\n\n> **Read the guide:** [Context Management Plugin](https://ericgrill.com/blog/context-management-plugin) — Save and restore Claude Code conversation context.\n\n```\n/plugin install context-management@agents-skills-plugins\n```\n\n| Agents | Commands |\n|--------|----------|\n| context-manager | `/context-save` `/context-restore` |\n\n</details>\n\n---\n\n### Documents & Productivity\n\nWork with Office documents, create media, and boost productivity.\n\n<details>\n<summary><b>awesome-claude-skills</b> - 27 practical skills from ComposioHQ</summary>\n\nA comprehensive collection covering documents, creative media, development tools, business, and productivity.\n\n> **Read the guides:**\n> - [Document Skills](https://ericgrill.com/blog/anthropic-document-skills) — Word, PDF, PowerPoint, and Excel automation\n> - [Design Skills](https://ericgrill.com/blog/anthropic-design-skills) — Canvas design, image enhancement, and themes\n> - [Builder Skills](https://ericgrill.com/blog/anthropic-builder-skills) — MCP servers, artifacts, and webapp testing\n> - [Communication Skills](https://ericgrill.com/blog/anthropic-communication-skills) — Brand guidelines and internal comms\n\n```\n/plugin install awesome-claude-skills@agents-skills-plugins\n```\n\n**Document Skills:**\n| Skill | Description |\n|-------|-------------|\n| docx | Create, edit, analyze Word docs with tracked changes |\n| pdf | Extract text, tables, metadata, merge & annotate PDFs |\n| pptx | Read, generate, and adjust slides and layouts |\n| xlsx | Spreadsheet manipulation: formulas, charts, data |\n\n**Creative & Media:**\n| Skill | Description |\n|-------|-------------|\n| canvas-design | Creates visual art in PNG and PDF |\n| image-enhancer | Improves resolution, sharpness, clarity |\n| slack-gif-creator | Animated GIFs optimized for Slack |\n| theme-factory | Professional font and color themes |\n| video-downloader | Download videos from YouTube |\n\n**Development:**\n| Skill | Description |\n|-------|-------------|\n| artifacts-builder | Multi-component HTML with React & Tailwind |\n| changelog-generator | Git commits to release notes |\n| mcp-builder | Create MCP servers for LLM integrations |\n| skill-creator | Guide to building Claude Skills |\n| webapp-testing | Test apps with Playwright |\n\n**Business & Marketing:**\n| Skill | Description |\n|-------|-------------|\n| brand-guidelines | Apply brand standards to artifacts |\n| competitive-ads-extractor | Analyze competitor ads |\n| domain-name-brainstormer | Generate and check domain names |\n| internal-comms | Newsletters, FAQs, status reports |\n| lead-research-assistant | Identify and qualify leads |\n\n**Productivity:**\n| Skill | Description |\n|-------|-------------|\n| file-organizer | Organize files, find duplicates |\n| invoice-organizer | Automate invoice organization |\n| content-research-writer | Research and refine content |\n| meeting-insights-analyzer | Analyze meeting transcripts |\n| raffle-winner-picker | Secure random selection |\n\n*From [ComposioHQ/awesome-claude-skills](https://github.com/ComposioHQ/awesome-claude-skills)*\n\n</details>\n\n---\n\n### Languages\n\nLanguage-specific development tools and patterns.\n\n<details>\n<summary><b>python-development</b> - Django, FastAPI, async Python (5 skills)</summary>\n\nComplete Python development toolkit with framework support and modern tooling.\n\n> **Read the guide:** [Python Development Plugin](https://ericgrill.com/blog/python-development-plugin) — Django, FastAPI, and async Python patterns with uv package manager.\n\n```\n/plugin install python-development@agents-skills-plugins\n```\n\n| Agents | Skills | Commands |\n|--------|--------|----------|\n| django-pro, fastapi-pro, python-pro | async-python-patterns, python-packaging, python-performance-optimization, python-testing-patterns, uv-package-manager | `/python-scaffold` |\n\n</details>\n\n<details>\n<summary><b>javascript-typescript</b> - Modern JS/TS patterns (4 skills)</summary>\n\nTypeScript and JavaScript development with modern patterns and Node.js backend support.\n\n```\n/plugin install javascript-typescript@agents-skills-plugins\n```\n\n| Agents | Skills | Commands |\n|--------|--------|----------|\n| javascript-pro, typescript-pro | javascript-testing-patterns, modern-javascript-patterns, nodejs-backend-patterns, typescript-advanced-types | `/typescript-scaffold` |\n\n</details>\n\n---\n\n### AI & LLM Development\n\nBuild intelligent applications with RAG, embeddings, and prompt engineering.\n\n<details>\n<summary><b>llm-application-dev</b> - RAG, embeddings, LangChain (8 skills)</summary>\n\nEverything you need to build production LLM applications.\n\n> **Read the guide:** [LLM-Application-Dev Plugin](https://ericgrill.com/blog/llm-application-dev-plugin) — Build RAG systems, embeddings, and LangChain agents.\n\n```\n/plugin install llm-application-dev@agents-skills-plugins\n```\n\n| Agents | Skills | Commands |\n|--------|--------|----------|\n| ai-engineer, prompt-engineer, vector-database-engineer | embedding-strategies, hybrid-search-implementation, langchain-architecture, llm-evaluation, prompt-engineering-patterns, rag-implementation, similarity-search-patterns, vector-index-tuning | `/ai-assistant` `/langchain-agent` `/prompt-optimize` |\n\n</details>\n\n<details>\n<summary><b>agent-orchestration</b> - Multi-agent coordination (2 commands)</summary>\n\nOrchestrate complex multi-agent workflows with context management and performance optimization.\n\n> **Read the guide:** [Agent-Orchestration Plugin](https://ericgrill.com/blog/agent-orchestration-plugin) — Coordinate multi-agent workflows and optimize performance.\n\n```\n/plugin install agent-orchestration@agents-skills-plugins\n```\n\n| Agents | Commands |\n|--------|----------|\n| context-manager | `/improve-agent` `/multi-agent-optimize` |\n\n</details>\n\n<details>\n<summary><b>multi-agent-patterns</b> - Architecture patterns for multi-agent systems</summary>\n\nDesign and implement multi-agent architectures with supervisor, swarm, and hierarchical patterns.\n\n```\n/plugin install multi-agent-patterns@agents-skills-plugins\n```\n\n**Patterns Covered:**\n| Pattern | Description |\n|---------|-------------|\n| Supervisor/Orchestrator | Central control, delegating to specialists |\n| Peer-to-Peer/Swarm | Direct agent communication, flexible handoffs |\n| Hierarchical | Layered abstraction and coordination |\n\n**Key Concepts:** Context isolation, token economics, parallelization strategies, consensus mechanisms, failure mode handling.\n\n*From [muratcankoylan/Agent-Skills-for-Context-Engineering](https://github.com/muratcankoylan/Agent-Skills-for-Context-Engineering)*\n\n</details>\n\n<details>\n<summary><b>ai-investigator</b> - Enterprise AI case study analyzer</summary>\n\nAnalyze enterprise AI case studies using Claude and Firecrawl APIs with automatic discovery and report generation.\n\n```\n/plugin install ai-investigator@agents-skills-plugins\n```\n\n**Reports Generated:**\n| Report Type | Description |\n|-------------|-------------|\n| Individual | Executive summary, AI strategy, tech implementation, business impact |\n| Cross-Case | Patterns, success factors, technology trends, ROI metrics |\n| Executive Dashboard | Company profiles, tech stacks, success metrics |\n\n**Modes:** CSV analysis (specific URLs) or Website discovery (automatic crawl)\n\n*From [muratcankoylan/AI-Investigator](https://github.com/muratcankoylan/AI-Investigator)*\n\n</details>\n\n---\n\n### Code Quality\n\nWrite better code with reviews, testing, and documentation.\n\n<details>\n<summary><b>comprehensive-review</b> - Architecture and security review (2 commands)</summary>\n\nDeep code review covering architecture, security, and PR quality.\n\n```\n/plugin install comprehensive-review@agents-skills-plugins\n```\n\n| Agents | Commands |\n|--------|----------|\n| architect-review, code-reviewer, security-auditor | `/full-review` `/pr-enhance` |\n\n</details>\n\n<details>\n<summary><b>unit-testing</b> - Testing and debugging (1 command)</summary>\n\nAutomated test generation and systematic debugging.\n\n> **Read the guide:** [Unit-Testing Plugin](https://ericgrill.com/blog/unit-testing-plugin) — Generate tests and debug systematically with Claude Code.\n\n```\n/plugin install unit-testing@agents-skills-plugins\n```\n\n| Agents | Commands |\n|--------|----------|\n| debugger, test-automator | `/test-generate` |\n\n</details>\n\n<details>\n<summary><b>code-documentation</b> - Docs and tutorials (2 commands)</summary>\n\nGenerate documentation, explain code, and create tutorials.\n\n```\n/plugin install code-documentation@agents-skills-plugins\n```\n\n| Agents | Commands |\n|--------|----------|\n| code-reviewer, docs-architect, tutorial-engineer | `/code-explain` `/doc-generate` |\n\n</details>\n\n<details>\n<summary><b>documentation-generation</b> - API docs and diagrams (3 skills)</summary>\n\nProfessional documentation with API specs, Mermaid diagrams, and architecture records.\n\n```\n/plugin install documentation-generation@agents-skills-plugins\n```\n\n| Agents | Skills | Commands |\n|--------|--------|----------|\n| api-documenter, docs-architect, mermaid-expert, reference-builder, tutorial-engineer | architecture-decision-records, changelog-automation, openapi-spec-generation | `/doc-generate` |\n\n</details>\n\n---\n\n### Frontend & Mobile\n\nBuild modern web and mobile applications.\n\n<details>\n<summary><b>frontend-mobile-development</b> - React, Next.js, React Native (4 skills)</summary>\n\nFull-stack frontend development with modern frameworks and design systems.\n\n```\n/plugin install frontend-mobile-development@agents-skills-plugins\n```\n\n| Agents | Skills | Commands |\n|--------|--------|----------|\n| frontend-developer, mobile-developer | nextjs-app-router-patterns, react-native-architecture, react-state-management, tailwind-design-system | `/component-scaffold` |\n\n</details>\n\n<details>\n<summary><b>frontend-mobile-security</b> - XSS scanning and secure coding (1 command)</summary>\n\nSecurity-focused frontend development with vulnerability scanning.\n\n```\n/plugin install frontend-mobile-security@agents-skills-plugins\n```\n\n| Agents | Commands |\n|--------|----------|\n| frontend-developer, frontend-security-coder, mobile-security-coder | `/xss-scan` |\n\n</details>\n\n<details>\n<summary><b>ios-simulator-skill</b> - iOS testing automation (21 scripts)</summary>\n\nProduction-ready iOS simulator automation with semantic navigation using accessibility APIs.\n\n```\n/plugin install ios-simulator-skill@agents-skills-plugins\n```\n\n**Categories:**\n| Category | Scripts |\n|----------|---------|\n| Build & Dev | build_and_test.py, log_monitor.py |\n| Navigation | screen_mapper.py, navigator.py, gesture.py, keyboard.py, app_launcher.py |\n| Testing | accessibility_audit.py, visual_diff.py, test_recorder.py, app_state_capture.py |\n| Permissions | clipboard.py, status_bar.py, push_notification.py, privacy_manager.py |\n| Lifecycle | simctl_boot.py, simctl_shutdown.py, simctl_create.py, simctl_delete.py, simctl_erase.py |\n\n**Features:** Semantic navigation (find by meaning, not coordinates), 96% token reduction, WCAG compliance checking, CI/CD ready with JSON output.\n\n*From [conorluddy/ios-simulator-skill](https://github.com/conorluddy/ios-simulator-skill)*\n\n</details>\n\n---\n\n### DevOps & Infrastructure\n\nDeploy, monitor, and manage infrastructure.\n\n<details>\n<summary><b>full-stack-orchestration</b> - End-to-end deployment (1 command)</summary>\n\nCoordinate deployment, performance, security, and testing across your stack.\n\n```\n/plugin install full-stack-orchestration@agents-skills-plugins\n```\n\n| Agents | Commands |\n|--------|----------|\n| deployment-engineer, performance-engineer, security-auditor, test-automator | `/full-stack-feature` |\n\n</details>\n\n<details>\n<summary><b>deployment-strategies</b> - Terraform and IaC</summary>\n\nInfrastructure as code with Terraform expertise.\n\n```\n/plugin install deployment-strategies@agents-skills-plugins\n```\n\n| Agents |\n|--------|\n| deployment-engineer, terraform-specialist |\n\n</details>\n\n---\n\n### MCP Servers\n\nModel Context Protocol servers that give Claude direct access to external systems and APIs.\n\n<details>\n<summary><b>mcp-proxmox-admin</b> - Proxmox VE infrastructure management</summary>\n\nManage Proxmox virtual machines, containers, and infrastructure through Claude with 16 tools across VM control, snapshots, and monitoring.\n\n> **Read the guide:** [Claude Code Rescued My Proxmox Cluster](https://ericgrill.com/blog/claude-code-proxmox-cluster-rescue)\n\n```\n/plugin install mcp-proxmox-admin@agents-skills-plugins\n```\n\n| Tools | Features |\n|-------|----------|\n| VM control (start, stop, restart) | Hybrid SSH/REST transport |\n| LXC container management | Optional read-only safe mode |\n| Snapshot operations | API token or SSH auth |\n| Node/storage/cluster monitoring | |\n\n*From [EricGrill/mcp-proxmox-admin](https://github.com/EricGrill/mcp-proxmox-admin)*\n\n</details>\n\n<details>\n<summary><b>mcp-multi-agent-ssh</b> - Persistent SSH connections</summary>\n\nMaintain persistent SSH connections with encrypted credential storage instead of opening/closing for each command. 10-minute idle timeout with auto-reconnection.\n\n```\n/plugin install mcp-multi-agent-ssh@agents-skills-plugins\n```\n\n| Tools | Security |\n|-------|----------|\n| `ssh_exec` - Remote command execution | AES-256-GCM encryption |\n| SFTP upload/download | PBKDF2 key derivation (100k iterations) |\n| Connection pooling & status | Master password protection |\n| Credential management | File permissions (600) |\n\n*From [EricGrill/mcp-multi-agent-ssh](https://github.com/EricGrill/mcp-multi-agent-ssh)*\n\n</details>\n\n<details>\n<summary><b>mcp-kali-orchestration</b> - Kali Linux security tools (50+)</summary>\n\nSpin up Kali Linux instances and access professional security tools for authorized pentesting, CTFs, and security research.\n\n> **Read the guide:** [I Gave Claude Code 50+ Kali Linux Tools](https://ericgrill.com/blog/kali-orchestration-ai-hacker)\n\n```\n/plugin install mcp-kali-orchestration@agents-skills-plugins\n```\n\n| Category | Tools |\n|----------|-------|\n| Reconnaissance | nmap, amass, DNS enumeration |\n| Web Application | sqlmap, nuclei, gobuster |\n| Exploitation | Metasploit, msfvenom |\n| Password Attacks | hydra, john, hashcat |\n| Post-Exploitation | impacket, crackmapexec, BloodHound |\n| Network | tcpdump, Wireshark, responder |\n\n**Backends:** Docker (fast, local) or Proxmox (full VM isolation)\n\n*From [EricGrill/mcp-kali-orchestration](https://github.com/EricGrill/mcp-kali-orchestration)*\n\n</details>\n\n<details>\n<summary><b>mcp-multi-agent-server-delegation</b> - Isolated VM task execution</summary>\n\nDelegate tasks to isolated Proxmox VMs for secure, sandboxed execution with automatic cleanup and HTTP callback status reporting.\n\n```\n/plugin install mcp-multi-agent-server-delegation@agents-skills-plugins\n```\n\n| Features | Agent Types |\n|----------|-------------|\n| Complete VM isolation per job | Claude CLI |\n| Automatic cleanup after completion | Shell scripts |\n| HTTP webhook status callbacks | Custom binaries |\n| Timeout protection | |\n| CPU/memory/disk quotas | |\n\n*From [EricGrill/mcp-multi-agent-server-delegation](https://github.com/EricGrill/mcp-multi-agent-server-delegation)*\n\n</details>\n\n<details>\n<summary><b>mcp-predictive-market</b> - 5 prediction markets aggregator</summary>\n\nQuery prediction markets simultaneously with arbitrage detection and comparative analysis across Manifold, Polymarket, Metaculus, PredictIt, and Kalshi.\n\n> **Read the guide:** [Decentralized Predictive Market Bitcoin](https://ericgrill.com/blog/decentralized-predictive-market-bitcoin)\n\n```\n/plugin install mcp-predictive-market@agents-skills-plugins\n```\n\n| Tools | Features |\n|-------|----------|\n| Multi-platform search | Arbitrage detection |\n| Market discovery | Price comparison |\n| Odds tracking | Watchlist building |\n| 8 integrated tools | 134 passing tests |\n\n*From [EricGrill/mcp-predictive-market](https://github.com/EricGrill/mcp-predictive-market)*\n\n</details>\n\n<details>\n<summary><b>mcp-bitcoin-cli</b> - Bitcoin OP_RETURN operations</summary>\n\nEmbed and read data on the Bitcoin blockchain through Claude with document storage, timestamping, and BRC-20 token support.\n\n> **Read the guide:** [From OP_RETURN to Lightning](https://ericgrill.com/blog/bitcoin-opreturn-to-lightning-journey)\n\n```\n/plugin install mcp-bitcoin-cli@agents-skills-plugins\n```\n\n| Capabilities | Safety |\n|--------------|--------|\n| Document storage (up to 100KB) | Testnet default |\n| SHA-256/SHA3 timestamping | Dry-run mode |\n| BRC-20 deploy/mint/transfer | Fee warnings |\n| Custom BTCD envelope protocol | Data size validation |\n\n**Networks:** mainnet, testnet, signet, regtest\n\n*From [EricGrill/mcp-bitcoin-cli](https://github.com/EricGrill/mcp-bitcoin-cli)*\n\n</details>\n\n<details>\n<summary><b>mcp-civic-data</b> - 7 government APIs</summary>\n\nAccess free government and open data APIs for weather, census, NASA, and economic indicators. Most features require no API keys.\n\n> **Read the guide:** [I Got Tired of Hunting for API Code](https://ericgrill.com/blog/civic-data-api-wrapper)\n\n```\n/plugin install mcp-civic-data@agents-skills-plugins\n```\n\n| Data Source | Examples |\n|-------------|----------|\n| NOAA Weather | US forecasts and alerts |\n| US Census | Population, demographics |\n| NASA | Astronomy photos, Mars rover imagery |\n| World Bank | GDP, economic indicators |\n| Data.gov | 300,000+ US datasets |\n| EU Open Data | European datasets |\n\n**22 tools** with zero-config setup and graceful fallback.\n\n*From [EricGrill/mcp-civic-data](https://github.com/EricGrill/mcp-civic-data)*\n\n</details>\n\n<details>\n<summary><b>mcp-memvid-state-service</b> - AI memory with vector search</summary>\n\nSingle-file AI memory layer with vector search, full-text search, and temporal queries stored in portable `.mv2` files.\n\n> **Read the guide:** [One File to Rule Them All: Portable AI Memory](https://ericgrill.com/blog/memvid-portable-ai-memory)\n\n```\n/plugin install mcp-memvid-state-service@agents-skills-plugins\n```\n\n| Search Types | Embedding Options |\n|--------------|-------------------|\n| Semantic (HNSW vectors) | Local models (bge, nomic, gte) |\n| Full-text (BM25) | Ollama integration |\n| Temporal queries | OpenAI API |\n| Smart auto-select | |\n\n**10 MCP tools** - No Redis, Postgres, or external vector DB required.\n\n*From [EricGrill/mcp-memvid-state-service](https://github.com/EricGrill/mcp-memvid-state-service)*\n\n</details>\n\n---\n\n### SEO & Marketing\n\nOptimize content for search and marketing.\n\n<details>\n<summary><b>seo-content-creation</b> - SEO writing and planning</summary>\n\nCreate SEO-optimized content with auditing, planning, and writing agents.\n\n```\n/plugin install seo-content-creation@agents-skills-plugins\n```\n\n| Agents |\n|--------|\n| seo-content-auditor, seo-content-planner, seo-content-writer |\n\n</details>\n\n<details>\n<summary><b>seo-analysis-monitoring</b> - Authority and content health</summary>\n\nMonitor SEO performance with authority building and content refresh detection.\n\n```\n/plugin install seo-analysis-monitoring@agents-skills-plugins\n```\n\n| Agents |\n|--------|\n| seo-authority-builder, seo-cannibalization-detector, seo-content-refresher |\n\n</details>\n\n<details>\n<summary><b>seo-technical-optimization</b> - Keywords and meta tags</summary>\n\nTechnical SEO with keyword strategy, meta optimization, and site structure.\n\n```\n/plugin install seo-technical-optimization@agents-skills-plugins\n```\n\n| Agents |\n|--------|\n| seo-keyword-strategist, seo-meta-optimizer, seo-snippet-hunter, seo-structure-architect |\n\n</details>\n\n<details>\n<summary><b>content-marketing</b> - Content strategy</summary>\n\nContent marketing with strategy and search specialist agents.\n\n```\n/plugin install content-marketing@agents-skills-plugins\n```\n\n| Agents |\n|--------|\n| content-marketer, search-specialist |\n\n</details>\n\n<details>\n<summary><b>ralph-wiggum-marketer</b> - Autonomous AI copywriter (4 commands)</summary>\n\nAutonomous AI copywriter for SaaS content using the Ralph Wiggum pattern - iterative loops that ship content while you sleep.\n\n```\n/plugin install ralph-wiggum-marketer@agents-skills-plugins\n```\n\n| Commands | Description |\n|----------|-------------|\n| `/ralph-init` | Initialize a new content project |\n| `/ralph-marketer` | Start the autonomous copywriter loop |\n| `/ralph-status` | Check content pipeline and progress |\n| `/ralph-cancel` | Cancel the active loop |\n\n**The Ralph Loop:** Read PRD → Check Progress → Pick Task → Execute → Verify → Commit → Update → Repeat\n\n*From [muratcankoylan/ralph-wiggum-marketer](https://github.com/muratcankoylan/ralph-wiggum-marketer)*\n\n</details>\n\n<details>\n<summary><b>beautiful-prose</b> - Forceful writing without AI tics</summary>\n\nA hard-edged writing style skill for clean, exact, muscular prose free of modern AI cadence.\n\n```\n/plugin install beautiful-prose@agents-skills-plugins\n```\n\n**Registers:**\n| Register | Style |\n|----------|-------|\n| founding_fathers | Formal, spare, civic gravity |\n| literary_modern | Vivid, lean imagery (default) |\n| cold_steel | Severe compression, punchy |\n| journalistic | Crisp, factual, narrative clarity |\n\n**Controls:** `DENSITY: lean|standard|dense`, `HEAT: cool|warm|hot`, `LENGTH: micro|short|medium|long`\n\n*From [SHADOWPR0/beautiful_prose](https://github.com/SHADOWPR0/beautiful_prose)*\n\n</details>\n\n---\n\n### Data & Backend\n\nDatabase design, validation, and backend services.\n\n<details>\n<summary><b>database-design</b> - SQL and PostgreSQL (1 skill)</summary>\n\nDatabase architecture and query optimization.\n\n> **Read the guide:** [Database-Design Plugin](https://ericgrill.com/blog/database-design-plugin) — PostgreSQL architecture and SQL query optimization.\n\n```\n/plugin install database-design@agents-skills-plugins\n```\n\n| Agents | Skills |\n|--------|--------|\n| database-architect, sql-pro | postgresql |\n\n</details>\n\n<details>\n<summary><b>data-validation-suite</b> - Backend security</summary>\n\nData validation and secure backend coding practices.\n\n```\n/plugin install data-validation-suite@agents-skills-plugins\n```\n\n| Agents |\n|--------|\n| backend-security-coder |\n\n</details>\n\n<details>\n<summary><b>customer-sales-automation</b> - Support and sales</summary>\n\nAutomate customer support and sales workflows.\n\n```\n/plugin install customer-sales-automation@agents-skills-plugins\n```\n\n| Agents |\n|--------|\n| customer-support, sales-automator |\n\n</details>\n\n<details>\n<summary><b>business-analytics</b> - KPIs and dashboards (2 skills)</summary>\n\nBusiness analysis with data storytelling and dashboard design.\n\n```\n/plugin install business-analytics@agents-skills-plugins\n```\n\n| Agents | Skills |\n|--------|--------|\n| business-analyst | data-storytelling, kpi-dashboard-design |\n\n</details>\n\n---\n\n### Specialized\n\nDomain-specific plugins for specialized development.\n\n<details>\n<summary><b>blockchain-web3</b> - Solidity, DeFi, NFTs (4 skills)</summary>\n\nWeb3 development with smart contract security and DeFi protocols.\n\n```\n/plugin install blockchain-web3@agents-skills-plugins\n```\n\n| Agents | Skills |\n|--------|--------|\n| blockchain-developer | defi-protocol-templates, nft-standards, solidity-security, web3-testing |\n\n</details>\n\n<details>\n<summary><b>game-development</b> - Unity, Godot, Minecraft (2 skills)</summary>\n\nGame development across multiple engines and platforms.\n\n```\n/plugin install game-development@agents-skills-plugins\n```\n\n| Agents | Skills |\n|--------|--------|\n| minecraft-bukkit-pro, unity-developer | godot-gdscript-patterns, unity-ecs-patterns |\n\n</details>\n\n<details>\n<summary><b>nano-banana</b> - AI image generation (MCP)</summary>\n\nGenerate images using Google's Gemini API.\n\n```\n/plugin install nano-banana@agents-skills-plugins\n```\n\n| Tools | Requires |\n|-------|----------|\n| `generate_image`, `generate_blog_images` | `GEMINI_API_KEY` |\n\n</details>\n\n<details>\n<summary><b>rosetta-prompt</b> - Multi-provider prompt optimization</summary>\n\nAdapts prompts for different AI providers using multi-agent ReAct loops with LangChain.\n\n```\n/plugin install rosetta-prompt@agents-skills-plugins\n```\n\n**Architecture:** Orchestrator spawns parallel optimizer agents for OpenAI, Anthropic, Google, etc.\n\n*From [muratcankoylan/The-Rosetta-Prompt](https://github.com/muratcankoylan/The-Rosetta-Prompt)*\n\n</details>\n\n<details>\n<summary><b>readwren</b> - Literary DNA extraction</summary>\n\nMulti-agent interview system that extracts your literary preferences and generates reading profiles.\n\n```\n/plugin install readwren@agents-skills-plugins\n```\n\n**Features:** 12-turn adaptive interview, taste anchors, style signature mapping, vocabulary analysis.\n\n*From [muratcankoylan/readwren](https://github.com/muratcankoylan/readwren)*\n\n</details>\n\n<details>\n<summary><b>food-tour-planner</b> - AI food tour planning</summary>\n\nMulti-agent food tour planner using LangChain DeepAgents and Google Maps API.\n\n```\n/plugin install food-tour-planner@agents-skills-plugins\n```\n\n**Agents:** Restaurant Finder, Neighborhood Researcher, Dashboard Creator.\n\n*From [muratcankoylan/Food-tour-planner-agent](https://github.com/muratcankoylan/Food-tour-planner-agent)*\n\n</details>\n\n<details>\n<summary><b>actual-code</b> - 7-agent code assessment generator</summary>\n\nAnalyzes GitHub repos and generates personalized coding challenges using Google Gemini.\n\n```\n/plugin install actual-code@agents-skills-plugins\n```\n\n**Pipeline:** Scanner → Code/PR/Issue/Dependency Analyzers → Problem Creator → QA Validator\n\n*From [muratcankoylan/actual_code](https://github.com/muratcankoylan/actual_code)*\n\n</details>\n\n<details>\n<summary><b>book-training</b> - Author style transfer with LoRA</summary>\n\nComplete pipeline for training LLMs to write in specific author styles using SFT.\n\n```\n/plugin install book-training@agents-skills-plugins\n```\n\n**Results:** 97% loss reduction, ~50-70% human score on AI detectors.\n\n*From [muratcankoylan/book-training](https://github.com/muratcankoylan/book-training)*\n\n</details>\n\n<details>\n<summary><b>linkedin-analyzer</b> - LinkedIn profile insights</summary>\n\nAnalyze LinkedIn profiles using Cohere Command R+ for professional insights.\n\n```\n/plugin install linkedin-analyzer@agents-skills-plugins\n```\n\n**Features:** Post analysis, engagement metrics, professional growth recommendations.\n\n*From [muratcankoylan/linkedin-analyzer](https://github.com/muratcankoylan/linkedin-analyzer)*\n\n</details>\n\n<details>\n<summary><b>feed2context</b> - Feed to research report</summary>\n\nOne-click transformation of LinkedIn/X posts into detailed research reports.\n\n```\n/plugin install feed2context@agents-skills-plugins\n```\n\n**Pipeline:** Extract post → Build query (Kimi-K2) → Search & reason (Groq Compound) → Report\n\n*From [muratcankoylan/feed2context](https://github.com/muratcankoylan/feed2context)*\n\n</details>\n\n---\n\n## Marketplace Stats\n\n| Metric | Count |\n|--------|-------|\n| Plugins | 49 |\n| Agents | 70+ |\n| Skills | 110+ |\n| Commands | 40+ |\n| MCP Tools | 100+ |\n| Contributors | 10+ |\n\n---\n\n## Attribution\n\nPlugins in this marketplace come from:\n\n- **[obra/superpowers](https://github.com/obra/superpowers)** by Jesse Vincent - Core skills library\n- **[wshobson/agents](https://github.com/wshobson/agents)** - Specialized agents and skills\n- **[ComposioHQ/awesome-claude-skills](https://github.com/ComposioHQ/awesome-claude-skills)** - Document & productivity skills\n- **[conorluddy/ios-simulator-skill](https://github.com/conorluddy/ios-simulator-skill)** - iOS testing automation\n- **[muratcankoylan](https://github.com/muratcankoylan)** - 10 plugins including multi-agent patterns, rosetta-prompt, readwren, actual-code, book-training, and more\n- **[SHADOWPR0/beautiful_prose](https://github.com/SHADOWPR0/beautiful_prose)** - Writing style skill\n- **[EricGrill](https://github.com/EricGrill)** - 9 plugins: nano-banana, mcp-proxmox-admin, mcp-multi-agent-ssh, mcp-kali-orchestration, mcp-multi-agent-server-delegation, mcp-predictive-market, mcp-bitcoin-cli, mcp-civic-data, mcp-memvid-state-service\n\nAll plugins are MIT licensed.\n\n---\n\n## Contributing\n\nWant to add your plugin to the marketplace?\n\n1. **Fork this repository**\n2. **Add your plugin** to the `plugins/` directory\n3. **Update `marketplace.json`** with your plugin metadata\n4. **Open a PR** with a brief description\n\nThe marketplace auto-syncs weekly from upstream sources to pull in updates.\n\n### Plugin Requirements\n\n- Must be compatible with Claude Code's plugin format\n- Include a `plugin.json` manifest\n- MIT or compatible open-source license preferred\n\n---\n\n## Related Resources\n\n**Learn Claude Code:**\n- [Claude Code Intro](https://ericgrill.com/blog/claude-code-intro) - Getting started with Claude Code CLI\n- [Claude Code Skills](https://ericgrill.com/blog/claude-code-skills) - Understanding and using skills\n- [Claude Code Agents](https://ericgrill.com/blog/claude-code-agents) - Working with specialized agents\n- [Claude Code Hooks](https://ericgrill.com/blog/claude-code-hooks) - Automating workflows with hooks\n\n**Official Documentation:**\n- [Claude Code Documentation](https://docs.anthropic.com/en/docs/claude-code) - Official docs for Claude Code CLI\n- [Plugin Development Guide](https://docs.anthropic.com/en/docs/claude-code/plugins) - How to build your own plugins\n\n---\n\n## License\n\nMIT\n",
        "plugins/nano-banana/README.md": "# Nano Banana\n\nImage generation MCP server using Google's Gemini API.\n\n## Requirements\n\n- Node.js 18+\n- Google Gemini API key\n\n## Setup\n\n1. Get a Gemini API key from [Google AI Studio](https://aistudio.google.com/)\n2. Set your API key:\n   ```bash\n   export GEMINI_API_KEY=\"your-api-key\"\n   ```\n\n## Tools\n\n### generate_image\n\nGenerate a single image from a text prompt.\n\n**Parameters:**\n- `prompt` (required) - Description of the image to generate\n- `filename` (required) - Output filename (without extension)\n- `outputDir` - Output directory (default: `public/blog`)\n- `aspectRatio` - One of: `1:1`, `16:9`, `9:16`, `4:3`, `3:4`, `3:2`, `2:3`\n- `model` - `flash` (fast) or `pro` (higher quality)\n\n### generate_blog_images\n\nGenerate a complete set of images for a blog post.\n\n**Parameters:**\n- `slug` (required) - Blog post slug for output directory\n- `heroPrompt` (required) - Prompt for the hero image\n- `sectionPrompts` - Array of `{name, prompt}` for section images\n- `style` - Style guidelines appended to all prompts\n\n## Environment Variables\n\n- `GEMINI_API_KEY` - Your Gemini API key (required)\n- `NANO_BANANA_OUTPUT_DIR` - Default output directory (default: `./public/blog`)\n\n## License\n\nMIT\n",
        "plugins/mcp-proxmox-admin/README.md": "<p align=\"center\">\n  <h1 align=\"center\">MCP Proxmox Admin</h1>\n  <p align=\"center\">\n    <strong>Manage Proxmox VE infrastructure through Claude</strong>\n  </p>\n  <p align=\"center\">\n    <a href=\"https://github.com/EricGrill/mcp-proxmox-admin/blob/master/LICENSE\"><img src=\"https://img.shields.io/badge/license-MIT-blue.svg\" alt=\"MIT License\"></a>\n    <img src=\"https://img.shields.io/badge/tools-16-green.svg\" alt=\"16 Tools\">\n    <img src=\"https://img.shields.io/badge/transport-SSH%20%2B%20API-purple.svg\" alt=\"SSH + API\">\n    <img src=\"https://img.shields.io/badge/node-%3E%3D18-orange.svg\" alt=\"Node >= 18\">\n  </p>\n  <p align=\"center\">\n    <a href=\"#-quick-start\">Quick Start</a> |\n    <a href=\"#-available-tools\">Tools</a> |\n    <a href=\"#%EF%B8%8F-configuration\">Configuration</a> |\n    <a href=\"#-contributing\">Contributing</a>\n  </p>\n</p>\n\n---\n\n## What is this?\n\nAn MCP (Model Context Protocol) server that enables Claude to manage your Proxmox VE infrastructure. Control VMs, containers, snapshots, and monitor cluster health through natural language.\n\n**Works with Claude Desktop, Cursor, and any MCP-compatible client.**\n\n---\n\n## Quick Start\n\n```bash\n# Install globally\nnpm install -g mcp-proxmox-admin\n\n# Or run directly with npx\nnpx mcp-proxmox-admin\n```\n\nAdd to your Claude Desktop config and start managing your Proxmox cluster:\n\n> \"Show me all running VMs on my Proxmox cluster\"\n\n---\n\n## Why Use This?\n\n| Feature | Description |\n|---------|-------------|\n| **Full VM Control** | Start, stop, shutdown, restart virtual machines |\n| **Container Management** | Manage LXC containers with the same ease |\n| **Snapshot Operations** | Create, restore, and delete snapshots |\n| **Hybrid Transport** | Auto-selects SSH or REST API based on operation |\n| **Safe Mode** | Optional read-only mode for monitoring |\n\n---\n\n## Available Tools\n\n### Virtual Machines\n\n| Tool | Description |\n|------|-------------|\n| `proxmox_vm_list` | List all VMs across nodes |\n| `proxmox_vm_start` | Start a VM |\n| `proxmox_vm_stop` | Stop a VM immediately |\n| `proxmox_vm_shutdown` | Graceful ACPI shutdown |\n| `proxmox_vm_restart` | Restart a VM |\n\n### Containers\n\n| Tool | Description |\n|------|-------------|\n| `proxmox_ct_list` | List all LXC containers |\n| `proxmox_ct_start` | Start a container |\n| `proxmox_ct_stop` | Stop a container |\n| `proxmox_ct_restart` | Restart a container |\n\n### Infrastructure\n\n| Tool | Description |\n|------|-------------|\n| `proxmox_node_list` | List cluster nodes |\n| `proxmox_node_status` | Get node CPU, memory, disk |\n| `proxmox_storage_list` | List storage pools |\n\n### Snapshots\n\n| Tool | Description |\n|------|-------------|\n| `proxmox_snapshot_list` | List snapshots for VM/container |\n| `proxmox_snapshot_create` | Create a new snapshot |\n| `proxmox_snapshot_restore` | Restore to a snapshot |\n| `proxmox_snapshot_delete` | Delete a snapshot (requires confirm) |\n\n---\n\n## Configuration\n\n### Claude Desktop Setup\n\nAdd to your Claude Desktop config:\n\n| Platform | Config Path |\n|----------|-------------|\n| macOS | `~/Library/Application Support/Claude/claude_desktop_config.json` |\n| Windows | `%APPDATA%\\Claude\\claude_desktop_config.json` |\n| Linux | `~/.config/Claude/claude_desktop_config.json` |\n\n```json\n{\n  \"mcpServers\": {\n    \"proxmox\": {\n      \"command\": \"npx\",\n      \"args\": [\"mcp-proxmox-admin\"],\n      \"env\": {\n        \"PROXMOX_HOST\": \"192.168.1.100\",\n        \"PROXMOX_API_TOKEN_ID\": \"user@pam!mytoken\",\n        \"PROXMOX_API_TOKEN_SECRET\": \"your-secret-here\",\n        \"PROXMOX_SSH_USER\": \"root\",\n        \"PROXMOX_SSH_KEY_PATH\": \"~/.ssh/id_rsa\"\n      }\n    }\n  }\n}\n```\n\n### Environment Variables\n\n| Variable | Description | Default |\n|----------|-------------|---------|\n| `PROXMOX_HOST` | Proxmox host/IP | *required* |\n| `PROXMOX_PORT` | API port | `8006` |\n| `PROXMOX_API_TOKEN_ID` | API token (format: `user@realm!tokenid`) | |\n| `PROXMOX_API_TOKEN_SECRET` | API token secret | |\n| `PROXMOX_VERIFY_SSL` | Verify SSL certificates | `true` |\n| `PROXMOX_SSH_USER` | SSH username | |\n| `PROXMOX_SSH_PORT` | SSH port | `22` |\n| `PROXMOX_SSH_KEY_PATH` | Path to SSH private key | |\n| `PROXMOX_SSH_PASSWORD` | SSH password (alternative to key) | |\n| `PROXMOX_DEFAULT_TRANSPORT` | `ssh`, `api`, or `auto` | `auto` |\n| `PROXMOX_DEFAULT_NODE` | Default node name | |\n| `PROXMOX_TIMEOUT` | Request timeout (ms) | `30000` |\n| `PROXMOX_SAFE_MODE` | Disable destructive operations | `false` |\n\n### Config Priority\n\n1. **MCP client settings** (highest priority)\n2. **Environment variables**\n3. **proxmox-config.json file** (lowest priority)\n\n---\n\n## Transport Modes\n\n| Mode | Description |\n|------|-------------|\n| `auto` | Best transport per operation (recommended) |\n| `api` | REST API only |\n| `ssh` | SSH commands only |\n\nIn `auto` mode: API for reads, SSH for snapshots and config changes.\n\n---\n\n## Examples\n\n<details>\n<summary><b>VM Management</b></summary>\n\n```\n\"Show me all running VMs\"\n\"Start VM 100 on node pve\"\n\"Gracefully shutdown VM 101\"\n\"Stop VM 102 immediately\"\n```\n\n</details>\n\n<details>\n<summary><b>Container Management</b></summary>\n\n```\n\"List all LXC containers\"\n\"Start container 200\"\n\"Restart container 201\"\n```\n\n</details>\n\n<details>\n<summary><b>Snapshots</b></summary>\n\n```\n\"Create a snapshot of VM 100 named 'before-upgrade'\"\n\"List snapshots for VM 100\"\n\"Restore VM 100 to snapshot 'before-upgrade'\"\n\"Delete snapshot 'old-backup' from VM 100\"\n```\n\n</details>\n\n<details>\n<summary><b>Cluster Monitoring</b></summary>\n\n```\n\"What's the status of node pve?\"\n\"Show me CPU and memory usage for all nodes\"\n\"List all storage pools and available space\"\n```\n\n</details>\n\n---\n\n## Authentication\n\n### API Token (Recommended)\n\n1. Proxmox UI → Datacenter → Permissions → API Tokens\n2. Create token for your user\n3. Assign `PVEVMAdmin` role for full VM control\n4. Add token ID and secret to config\n\n### SSH Key\n\n```bash\n# Generate key if needed\nssh-keygen -t ed25519\n\n# Copy to Proxmox\nssh-copy-id root@proxmox-host\n```\n\n---\n\n## Safe Mode\n\nEnable for read-only monitoring:\n\n```bash\nPROXMOX_SAFE_MODE=true\n```\n\nDisables: snapshot deletion and other destructive operations.\n\n---\n\n## Development\n\n```bash\n# Clone\ngit clone https://github.com/EricGrill/mcp-proxmox-admin.git\ncd mcp-proxmox-admin\n\n# Install & build\nnpm install\nnpm run build\n\n# Test\nnpm test\n\n# Dev mode (watch)\nnpm run dev\n```\n\n---\n\n## Troubleshooting\n\n<details>\n<summary><b>Cannot connect to Proxmox</b></summary>\n\n1. Verify host and port\n2. Check connectivity: `ping <proxmox-host>`\n3. Test API: `curl -k https://<proxmox-host>:8006/api2/json`\n\n</details>\n\n<details>\n<summary><b>Authentication failed</b></summary>\n\n1. API token format: `user@realm!tokenid`\n2. Verify token hasn't expired\n3. SSH key permissions: `chmod 600 ~/.ssh/id_rsa`\n\n</details>\n\n<details>\n<summary><b>Permission denied</b></summary>\n\nRequired Proxmox permissions:\n- `VM.PowerMgmt` - start/stop/restart\n- `VM.Snapshot` - snapshot operations\n- `Sys.Audit` - node/storage info\n\n</details>\n\n<details>\n<summary><b>SSL certificate errors</b></summary>\n\nFor self-signed certs:\n- Set `PROXMOX_VERIFY_SSL=false` (dev only)\n- Or add Proxmox CA to system trust store\n\n</details>\n\n---\n\n## Contributing\n\nContributions welcome!\n\n1. Fork the repository\n2. Create feature branch: `git checkout -b feature/my-feature`\n3. Make changes and test: `npm test`\n4. Commit: `git commit -m 'Add my feature'`\n5. Push: `git push origin feature/my-feature`\n6. Open a Pull Request\n\n---\n\n## License\n\nMIT\n",
        "plugins/mcp-multi-agent-ssh/README.md": "<p align=\"center\">\n  <h1 align=\"center\">MCP Multi-Agent SSH</h1>\n  <p align=\"center\">\n    <strong>Stateful SSH connections for Claude Code via MCP</strong>\n  </p>\n  <p align=\"center\">\n    <a href=\"https://github.com/EricGrill/mcp-multi-agent-ssh/blob/main/LICENSE\"><img src=\"https://img.shields.io/badge/license-MIT-blue.svg\" alt=\"MIT License\"></a>\n    <img src=\"https://img.shields.io/badge/python-3.10+-green.svg\" alt=\"Python 3.10+\">\n    <img src=\"https://img.shields.io/badge/tools-10-purple.svg\" alt=\"10 Tools\">\n    <img src=\"https://img.shields.io/badge/MCP-compatible-orange.svg\" alt=\"MCP Compatible\">\n  </p>\n  <p align=\"center\">\n    <a href=\"#-quick-start\">Quick Start</a> |\n    <a href=\"#-tools\">Tools</a> |\n    <a href=\"#-examples\">Examples</a> |\n    <a href=\"#-security\">Security</a>\n  </p>\n</p>\n\n---\n\n## What is this?\n\nAn MCP server that gives Claude Code persistent SSH connections. Instead of opening and closing connections for every command, connections stay alive for 10 minutes—making remote server management fast and seamless.\n\n**Part of the [Claude Code Plugin Marketplace](https://github.com/EricGrill/agents-skills-plugins)** — discover more plugins, agents, and skills for Claude Code.\n\n---\n\n## Quick Start\n\n### Using NPX\n\n```bash\nnpx mcp-multi-agent-ssh\n```\n\n### Using Docker\n\n```bash\ndocker run -it --rm \\\n  -v ~/.mcp-multi-agent-ssh:/root/.mcp-multi-agent-ssh \\\n  -e MCP_SSH_MASTER_PASSWORD=your-password \\\n  mcp-multi-agent-ssh\n```\n\n---\n\n## Claude Code Setup\n\nAdd to your Claude Code MCP configuration:\n\n**NPX Method:**\n\n```json\n{\n  \"mcpServers\": {\n    \"ssh\": {\n      \"command\": \"npx\",\n      \"args\": [\"mcp-multi-agent-ssh\"],\n      \"env\": {\n        \"MCP_SSH_MASTER_PASSWORD\": \"your-master-password\"\n      }\n    }\n  }\n}\n```\n\n**Docker Method:**\n\n```json\n{\n  \"mcpServers\": {\n    \"ssh\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\", \"-i\", \"--rm\",\n        \"-v\", \"~/.mcp-multi-agent-ssh:/root/.mcp-multi-agent-ssh\",\n        \"-e\", \"MCP_SSH_MASTER_PASSWORD\",\n        \"mcp-multi-agent-ssh\"\n      ],\n      \"env\": {\n        \"MCP_SSH_MASTER_PASSWORD\": \"your-master-password\"\n      }\n    }\n  }\n}\n```\n\n---\n\n## Features\n\n| Feature | Description |\n|---------|-------------|\n| **Persistent Connections** | SSH connections stay open for 10 minutes of inactivity |\n| **Encrypted Credentials** | Per-host credentials stored with AES-256-GCM encryption |\n| **Auto-Reconnect** | Transparently reconnects when connections expire or drop |\n| **SFTP Support** | Upload, download, and list files on remote servers |\n| **Host-Based Auth** | Credentials automatically matched by hostname |\n\n---\n\n## Tools\n\n### Connection Management\n\n| Tool | Description |\n|------|-------------|\n| `ssh_connect` | Connect to an SSH server. Stores credentials for future use. |\n| `ssh_disconnect` | Close connection to a specific host. |\n| `ssh_list_connections` | List all active connections with idle time. |\n\n### Command Execution\n\n| Tool | Description |\n|------|-------------|\n| `ssh_exec` | Run a command on a remote server. Auto-connects if needed. |\n\n### File Operations (SFTP)\n\n| Tool | Description |\n|------|-------------|\n| `sftp_upload` | Upload a local file to a remote server. |\n| `sftp_download` | Download a file from a remote server. |\n| `sftp_list` | List files in a remote directory. |\n\n### Credential Management\n\n| Tool | Description |\n|------|-------------|\n| `ssh_list_credentials` | List hosts with stored credentials. |\n| `ssh_delete_credentials` | Remove stored credentials for a host. |\n\n---\n\n## Examples\n\n### Connect and Run Commands\n\n```\nUser: Connect to my server at example.com as user \"deploy\" with password \"secret123\"\n\nClaude: [calls ssh_connect]\nConnected! Credentials saved for future use.\n\nUser: What's the disk usage?\n\nClaude: [calls ssh_exec with command=\"df -h\"]\nFilesystem      Size  Used Avail Use% Mounted on\n/dev/sda1       100G   45G   55G  45% /\n```\n\n### Transfer Files\n\n```\nUser: Upload my config file to the server\n\nClaude: [calls sftp_upload]\nUploaded 2.3 KB to /etc/app/config.yml\n```\n\n### Auto-Reconnect\n\nAfter 10 minutes of inactivity, connections automatically close with a notification. The next command reconnects using stored credentials:\n\n```\n[mcp-multi-agent-ssh] Connection to example.com:22 expired after 10 minutes of inactivity\n\nUser: Check the server status\n\nClaude: [calls ssh_exec — automatically reconnects]\n● app.service - My Application\n   Active: active (running)\n```\n\n---\n\n## Security\n\n### Credential Storage\n\n| Aspect | Implementation |\n|--------|----------------|\n| **Location** | `~/.mcp-multi-agent-ssh/credentials.enc` |\n| **Encryption** | AES-256-GCM |\n| **Key Derivation** | PBKDF2 with 100,000 iterations |\n| **File Permissions** | 600 (owner read/write only) |\n\n### Master Password\n\nThe master password encrypts/decrypts stored credentials:\n\n1. **Environment Variable** (recommended):\n   ```bash\n   export MCP_SSH_MASTER_PASSWORD=\"your-password\"\n   ```\n\n2. **Interactive Prompt**: If not set, you'll be prompted on first run.\n\n---\n\n## Development\n\n### Prerequisites\n\n- Python 3.10+\n- Node.js 18+ (for NPX launcher)\n\n### Local Setup\n\n```bash\n# Clone the repository\ngit clone https://github.com/ericgrill/mcp-multi-agent-ssh.git\ncd mcp-multi-agent-ssh\n\n# Create virtual environment\npython -m venv venv\nsource venv/bin/activate\n\n# Install in development mode\npip install -e \".[dev]\"\n\n# Run tests\npytest\n```\n\n### Project Structure\n\n```\nmcp-multi-agent-ssh/\n├── src/mcp_multi_agent_ssh/\n│   ├── __init__.py\n│   ├── server.py          # MCP server entry point\n│   ├── connection_pool.py # SSH connection management\n│   ├── credentials.py     # Encrypted credential storage\n│   └── types.py           # Pydantic models\n├── bin/\n│   └── launcher.js        # NPX launcher script\n├── tests/\n├── Dockerfile\n├── docker-compose.yml\n├── package.json\n├── pyproject.toml\n└── README.md\n```\n\n---\n\n## Troubleshooting\n\n| Issue | Solution |\n|-------|----------|\n| **\"Master password required\"** | Set `MCP_SSH_MASTER_PASSWORD` environment variable |\n| **\"Python 3.10+ not found\"** | Install Python 3.10+ from https://www.python.org/ |\n| **Connection timeouts** | Check network, verify port (default: 22), check firewall |\n| **\"Failed to decrypt credentials\"** | Wrong password. Delete `~/.mcp-multi-agent-ssh/credentials.enc` and `salt` to reset |\n\n---\n\n## Related\n\n**[Claude Code Plugin Marketplace](https://github.com/EricGrill/agents-skills-plugins)** — Discover 40+ plugins, 70+ agents, and 110+ skills for Claude Code including:\n\n- **superpowers** — TDD, debugging, code review skills\n- **python-development** — Django, FastAPI, async Python\n- **llm-application-dev** — RAG, embeddings, LangChain\n\n---\n\n## License\n\nMIT\n",
        "plugins/mcp-kali-orchestration/README.md": "# MCP-Kali-Orchestration\n\n**Orchestrate Kali Linux instances on-demand with 50+ security tools exposed via MCP**\n\n[![MIT License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)\n[![50+ Tools](https://img.shields.io/badge/Tools-50%2B-blue.svg)](#-tool-catalog)\n[![Docker](https://img.shields.io/badge/Backend-Docker-2496ED.svg)](https://www.docker.com/)\n[![Proxmox](https://img.shields.io/badge/Backend-Proxmox-E57000.svg)](https://www.proxmox.com/)\n\n[Quick Start](#-quick-start) | [Tool Catalog](#-tool-catalog) | [All Tools](#-all-tools) | [Configuration](#-configuration) | [Contributing](#-contributing)\n\n---\n\n## What is this?\n\nAn MCP (Model Context Protocol) server that spins up Kali Linux instances and exposes professional security tools directly to Claude Code or any MCP-compatible client. Run nmap scans, exploit vulnerabilities with Metasploit, crack passwords, and perform full penetration tests—all through natural language.\n\n> Part of the [Claude Code Plugin Marketplace](https://github.com/EricGrill/agents-skills-plugins) ecosystem.\n\n---\n\n## Quick Start\n\n**1. Add the marketplace and install:**\n\n```bash\nclaude mcp add-json mcp-kali-orchestration '{\"command\":\"node\",\"args\":[\"/path/to/mcp-kali-orchestration/dist/index.js\"]}'\n```\n\n**2. Build the Kali image and server:**\n\n```bash\ngit clone https://github.com/EricGrill/mcp-kali-orchestration.git\ncd mcp-kali-orchestration\nnpm install && npm run build\nnpm run build:image  # Builds Docker image with all tools\n```\n\n---\n\n## Why Use MCP-Kali-Orchestration?\n\n| Feature | Description |\n|---------|-------------|\n| **On-demand instances** | Spin up fresh Kali containers/VMs per engagement, tear down when done |\n| **50+ tools exposed** | Reconnaissance, web testing, exploitation, passwords, post-exploitation, network |\n| **Dual backend** | Docker (fast, local) or Proxmox (full VM isolation) |\n| **Natural language** | \"Scan example.com for vulnerabilities\" → Claude handles the rest |\n\n---\n\n## Tool Catalog\n\n| Category | Tools | Best For |\n|----------|-------|----------|\n| **Lifecycle** | 6 | Instance management (`kali_start`, `kali_stop`, `kali_exec`) |\n| **Reconnaissance** | 9 | Network discovery, DNS enum, OSINT (`nmap`, `amass`, `theharvester`) |\n| **Web Testing** | 12 | Web app security (`sqlmap`, `nikto`, `nuclei`, `gobuster`) |\n| **Exploitation** | 4 | Exploits and payloads (`metasploit`, `msfvenom`, `searchsploit`) |\n| **Password Attacks** | 7 | Credential attacks (`hydra`, `john`, `hashcat`) |\n| **Post-Exploitation** | 7 | Lateral movement (`impacket`, `crackmapexec`, `bloodhound`) |\n| **Network** | 7 | Network attacks and analysis (`responder`, `bettercap`, `tcpdump`) |\n\n---\n\n## All Tools\n\n### Lifecycle Management\n\n| Tool | Description |\n|------|-------------|\n| `kali_start` | Spin up a new Kali instance (Docker or Proxmox) |\n| `kali_stop` | Stop and remove an instance |\n| `kali_list` | List all running instances with status and IPs |\n| `kali_exec` | Execute arbitrary command in instance |\n| `kali_upload` | Upload file from host to instance |\n| `kali_download` | Download file from instance to host |\n\n### Reconnaissance\n\n| Tool | Description |\n|------|-------------|\n| `nmap_scan` | Port scanning with customizable flags |\n| `nmap_vuln_scan` | Vulnerability scanning with NSE scripts |\n| `masscan_scan` | High-speed port scanner for large ranges |\n| `whois_lookup` | Domain registration information |\n| `dig_lookup` | DNS queries (A, MX, NS, TXT, etc.) |\n| `dnsrecon_scan` | DNS enumeration and zone transfers |\n| `theharvester_search` | Email, subdomain, and host harvesting |\n| `amass_enum` | Subdomain enumeration (passive/active) |\n| `sublist3r_scan` | Fast subdomain enumeration |\n\n### Web Application Testing\n\n| Tool | Description |\n|------|-------------|\n| `nikto_scan` | Web server vulnerability scanner |\n| `dirb_scan` | Directory brute-forcing |\n| `gobuster_dir` | Directory/file enumeration |\n| `gobuster_dns` | DNS subdomain brute-forcing |\n| `ffuf_fuzz` | Fast web fuzzer |\n| `sqlmap_scan` | SQL injection detection and exploitation |\n| `wpscan_scan` | WordPress vulnerability scanner |\n| `whatweb_scan` | Web technology fingerprinting |\n| `wafw00f_detect` | WAF detection |\n| `nuclei_scan` | Template-based vulnerability scanner |\n| `xsser_scan` | Cross-site scripting scanner |\n| `commix_scan` | Command injection exploitation |\n\n### Exploitation\n\n| Tool | Description |\n|------|-------------|\n| `metasploit_search` | Search Metasploit modules |\n| `metasploit_run` | Execute Metasploit exploits/auxiliary modules |\n| `searchsploit_search` | Search Exploit-DB offline database |\n| `msfvenom_generate` | Generate encoded payloads |\n\n### Password Attacks\n\n| Tool | Description |\n|------|-------------|\n| `hydra_attack` | Online password cracking (SSH, FTP, HTTP, SMB, etc.) |\n| `medusa_attack` | Parallel modular login brute-forcer |\n| `john_crack` | Offline hash cracking (John the Ripper) |\n| `hashcat_crack` | GPU-accelerated hash cracking |\n| `hash_identifier` | Identify hash types |\n| `cewl_generate` | Custom wordlist from website spidering |\n| `crunch_generate` | Pattern-based wordlist generator |\n\n### Post-Exploitation\n\n| Tool | Description |\n|------|-------------|\n| `impacket_secretsdump` | Dump NTLM hashes from Windows targets |\n| `impacket_psexec` | Remote execution via SMB |\n| `impacket_smbexec` | SMB-based command execution |\n| `impacket_wmiexec` | WMI-based command execution |\n| `evil_winrm` | WinRM shell access |\n| `crackmapexec_run` | Swiss army knife for network attacks |\n| `bloodhound_collect` | Active Directory enumeration |\n\n### Network\n\n| Tool | Description |\n|------|-------------|\n| `netcat_connect` | Network connections and listeners |\n| `tcpdump_capture` | Packet capture |\n| `wireshark_cli` | tshark packet analysis |\n| `responder_run` | LLMNR/NBT-NS/mDNS poisoner |\n| `bettercap_run` | Network attack framework |\n| `socat_relay` | Advanced network relay/tunnel |\n| `aircrack_crack` | WPA/WPA2 handshake cracking |\n\n---\n\n## Usage Example\n\n```\nHuman: Start a Kali instance and scan 192.168.1.0/24 for open ports\n\nClaude: I'll spin up a Kali instance and run an nmap scan.\n\n[Uses kali_start]\nInstance started: kali-1704825600, IP: 172.17.0.2\n\n[Uses nmap_scan with target=192.168.1.0/24, ports=1-1000]\nNmap scan report for 192.168.1.0/24\nHost: 192.168.1.1   Ports: 22/open/tcp, 80/open/tcp, 443/open/tcp\nHost: 192.168.1.50  Ports: 22/open/tcp, 3389/open/tcp\n...\n\nFound 2 hosts with open ports. Would you like me to run vulnerability scans?\n```\n\n---\n\n## Configuration\n\nCopy `.env.example` to `.env`:\n\n```bash\n# Backend: \"docker\" or \"proxmox\"\nKALI_BACKEND=docker\n\n# Docker settings\nDOCKER_SOCKET=/var/run/docker.sock\nKALI_IMAGE=mcp-kali:latest\n\n# Proxmox settings (when KALI_BACKEND=proxmox)\nPROXMOX_HOST=192.168.1.100\nPROXMOX_PORT=8006\nPROXMOX_API_TOKEN_ID=root@pam!mcp-kali\nPROXMOX_API_TOKEN_SECRET=xxxx\nPROXMOX_SSH_USER=root\nPROXMOX_SSH_KEY_PATH=~/.ssh/id_rsa\nPROXMOX_KALI_TEMPLATE=local:vztmpl/kali-template\nPROXMOX_TARGET_NODE=pve\n```\n\n---\n\n## Security Notice\n\nThis tool is intended for **authorized use only**:\n\n- Authorized penetration testing engagements\n- CTF competitions\n- Security research in lab environments\n- Educational purposes\n\n**Always ensure you have explicit written authorization before using these tools against any target.**\n\n---\n\n## Related Resources\n\n- [Claude Code Plugin Marketplace](https://github.com/EricGrill/agents-skills-plugins) - Discover more plugins, agents, and skills\n- [MCP Proxmox Admin](https://github.com/EricGrill/mcp-proxmox-admin) - Proxmox VM management via MCP\n\n---\n\n## Contributing\n\n1. Fork this repository\n2. Add your tool wrapper to `src/tools/`\n3. Register it in `src/server.ts`\n4. Submit a pull request\n\n---\n\n## License\n\nMIT\n",
        "plugins/mcp-multi-agent-server-delegation/README.md": "# MCP-Multi-Agent-Server-Delegation\n\n**Delegate tasks to isolated Proxmox VMs for secure, sandboxed execution**\n\n[![MIT License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)\n[![5 Tools](https://img.shields.io/badge/Tools-5-blue.svg)](#-tool-catalog)\n[![TypeScript](https://img.shields.io/badge/TypeScript-5.0-3178C6.svg)](https://www.typescriptlang.org/)\n[![Proxmox](https://img.shields.io/badge/Proxmox-VE-E57000.svg)](https://www.proxmox.com/)\n[![MCP](https://img.shields.io/badge/MCP-Server-purple.svg)](https://modelcontextprotocol.io/)\n\n[Quick Start](#-quick-start) | [Tool Catalog](#-tool-catalog) | [Agent Types](#-agent-types) | [Configuration](#-configuration) | [Architecture](#-architecture)\n\n---\n\n## What is this?\n\nAn MCP (Model Context Protocol) server that delegates tasks to isolated Proxmox VMs. Run untrusted code, long builds, or user-submitted jobs in complete isolation with automatic cleanup and HTTP callback status reporting.\n\nPerfect for:\n- **Untrusted code execution** - Sandboxed VM isolation\n- **Build/test pipelines** - Clean environments every time\n- **Agent orchestration** - Spawn Claude or custom agents in isolated VMs\n\n> Requires [mcp-proxmox-admin](https://github.com/EricGrill/mcp-proxmox-admin) for VM management.\n\n---\n\n## Quick Start\n\n**1. Add to Claude Code:**\n\n```json\n{\n  \"mcpServers\": {\n    \"delegation\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/dist/index.js\"],\n      \"env\": {\n        \"CALLBACK_PORT\": \"8765\",\n        \"PROXMOX_ADMIN_PATH\": \"/path/to/mcp-proxmox-admin/dist/index.js\"\n      }\n    }\n  }\n}\n```\n\n**2. Or install manually:**\n\n```bash\ngit clone https://github.com/EricGrill/mcp-multi-agent-server-delegation.git\ncd mcp-multi-agent-server-delegation\nnpm install && npm run build\nnode dist/index.js\n```\n\n---\n\n## Why Use This?\n\n| Feature | Description |\n|---------|-------------|\n| **Full VM isolation** | Each job runs in its own Proxmox VM - complete sandboxing |\n| **Agent flexibility** | Run Claude, shell scripts, or custom binaries |\n| **Automatic cleanup** | Ephemeral VMs destroyed after job completion |\n| **Status callbacks** | Real-time updates via HTTP webhooks from VMs |\n| **Timeout protection** | Jobs killed and VMs destroyed on timeout |\n\n---\n\n## Tool Catalog\n\n| Tool | Description |\n|------|-------------|\n| `submit_job` | Submit a job manifest for execution in an isolated VM |\n| `get_job_status` | Check current status, progress, and timing of a job |\n| `get_job_result` | Retrieve output, artifacts, and errors from completed job |\n| `cancel_job` | Cancel a running job and destroy its VM |\n| `list_jobs` | List all jobs with optional status filter |\n\n---\n\n## All Tools\n\n### Job Submission\n\n| Tool | Parameters | Returns |\n|------|------------|---------|\n| `submit_job` | `manifest` (task, agentType, files, env, resources, timeout) | `job_id` |\n\n### Job Monitoring\n\n| Tool | Parameters | Returns |\n|------|------------|---------|\n| `get_job_status` | `job_id` | status, progress, timestamps |\n| `get_job_result` | `job_id` | output, artifacts, duration, errors |\n| `list_jobs` | `status?` (filter) | Array of job summaries |\n\n### Job Control\n\n| Tool | Parameters | Returns |\n|------|------------|---------|\n| `cancel_job` | `job_id` | confirmation |\n\n---\n\n## Agent Types\n\n| Type | Command | Use Case |\n|------|---------|----------|\n| `claude` | Claude CLI | AI-powered task execution |\n| `script` | Bash/Python | Build scripts, automation |\n| `custom` | Any binary | Specialized tools, compilers |\n\n### Example: Claude Agent\n\n```json\n{\n  \"task\": \"Review this codebase and create a summary\",\n  \"agentType\": \"claude\",\n  \"files\": [\n    { \"path\": \"/workspace/code.py\", \"content\": \"base64...\" }\n  ],\n  \"timeout\": 600\n}\n```\n\n### Example: Script Agent\n\n```json\n{\n  \"task\": \"npm test\",\n  \"agentType\": \"script\",\n  \"agent\": { \"command\": \"bash -c 'cd /workspace && npm install && npm test'\" },\n  \"timeout\": 300\n}\n```\n\n---\n\n## Configuration\n\n### Environment Variables\n\n| Variable | Description | Default |\n|----------|-------------|---------|\n| `CALLBACK_PORT` | Port for HTTP callback server | `8765` |\n| `CALLBACK_HOST` | Host/IP for callback URL | `0.0.0.0` |\n| `PROXMOX_ADMIN_PATH` | Path to mcp-proxmox-admin | — |\n| `DEFAULT_VM_TEMPLATE` | Proxmox template name | `agent-template` |\n| `DEFAULT_TIMEOUT` | Job timeout in seconds | `3600` |\n| `DEFAULT_CPU` | VM CPU cores | `2` |\n| `DEFAULT_MEMORY` | VM memory | `2G` |\n| `DEFAULT_DISK` | VM disk size | `10G` |\n| `HEARTBEAT_THRESHOLD_SECONDS` | Stale job detection | `120` |\n| `CLEANUP_INTERVAL_SECONDS` | Cleanup check interval | `30` |\n\n### Job Manifest Schema\n\n```typescript\ninterface JobManifest {\n  task: string;                    // What to do\n  agentType: 'claude' | 'script' | 'custom';\n  agent?: {\n    command?: string;              // For script/custom\n    claudeModel?: string;          // For claude\n  };\n  files?: Array<{ path: string; content: string }>;\n  env?: Record<string, string>;\n  resources?: { cpu?: number; memory?: string; disk?: string };\n  timeout?: number;\n  lifecycle?: 'ephemeral' | 'persistent';\n  statusMode?: 'simple' | 'detailed' | 'streaming';\n}\n```\n\n---\n\n## Architecture\n\n```\n┌─────────────────────────────────────────────────────────────────┐\n│                        Claude / Agent                           │\n└─────────────────────────────┬───────────────────────────────────┘\n                              │ MCP Protocol\n┌─────────────────────────────▼───────────────────────────────────┐\n│                 MCP Delegation Server                           │\n│  ┌─────────────┐  ┌─────────────┐  ┌─────────────────────────┐  │\n│  │ Job Manager │  │ Status Store│  │ Callback HTTP Server    │  │\n│  └─────────────┘  └─────────────┘  └─────────────────────────┘  │\n└─────────────────────────────┬───────────────────────────────────┘\n                              │ MCP Protocol\n┌─────────────────────────────▼───────────────────────────────────┐\n│                    mcp-proxmox-admin                            │\n└─────────────────────────────┬───────────────────────────────────┘\n                              │\n┌─────────────────────────────▼───────────────────────────────────┐\n│                     Proxmox VMs                                 │\n│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐          │\n│  │   Job VM 1   │  │   Job VM 2   │  │   Job VM 3   │          │\n│  │  → Agent     │  │  → Agent     │  │  → Agent     │          │\n│  │  → Callback  │  │  → Callback  │  │  → Callback  │          │\n│  └──────────────┘  └──────────────┘  └──────────────┘          │\n└─────────────────────────────────────────────────────────────────┘\n```\n\n### Job Lifecycle\n\n```\nsubmit_job → pending → provisioning → running → success/failed/timeout\n                                         ↓\n                              VM destroyed (if ephemeral)\n```\n\n---\n\n## VM Template Requirements\n\nYour Proxmox VM template should include:\n\n- **Base OS**: Ubuntu/Debian minimal\n- **Packages**: `curl`, `jq`, `bash`\n- **Claude CLI**: For `claude` agent type\n- **Python/Node**: For script execution\n- **Network**: Access to callback server URL\n\n---\n\n## Development\n\n```bash\nnpm run dev      # Run in development mode\nnpm test         # Run tests\nnpm run build    # Build for production\n```\n\n### Project Structure\n\n```\nsrc/\n├── index.ts           # Entry point\n├── server.ts          # MCP server with tools\n├── store.ts           # In-memory job store\n├── callback-server.ts # HTTP callback receiver\n├── proxmox-client.ts  # Proxmox MCP client wrapper\n├── types.ts           # TypeScript types\n├── schemas.ts         # Zod validation schemas\n└── config.ts          # Configuration loader\n```\n\n---\n\n## Security Considerations\n\n| Protection | Implementation |\n|------------|----------------|\n| **VM Isolation** | Each job in separate Proxmox VM |\n| **Network Segmentation** | VMs in isolated VLAN (configure in Proxmox) |\n| **Timeouts** | Automatic job termination on timeout |\n| **Ephemeral VMs** | Destroyed after completion by default |\n| **Resource Limits** | CPU/memory/disk quotas via Proxmox |\n\n---\n\n## Related Projects\n\n- [mcp-proxmox-admin](https://github.com/EricGrill/mcp-proxmox-admin) - Required for VM management\n- [Model Context Protocol](https://modelcontextprotocol.io/) - MCP specification\n\n---\n\n## License\n\nMIT\n",
        "plugins/mcp-predictive-market/README.md": "<p align=\"center\">\n  <h1 align=\"center\">MCP Predictive Market</h1>\n  <p align=\"center\">\n    <strong>Query and analyze prediction markets through Claude</strong>\n  </p>\n  <p align=\"center\">\n    <a href=\"https://github.com/EricGrill/mcp-predictive-market/blob/master/LICENSE\"><img src=\"https://img.shields.io/badge/license-MIT-blue.svg\" alt=\"MIT License\"></a>\n    <img src=\"https://img.shields.io/badge/tools-8-green.svg\" alt=\"8 Tools\">\n    <img src=\"https://img.shields.io/badge/platforms-5-purple.svg\" alt=\"5 Platforms\">\n    <img src=\"https://img.shields.io/badge/python-%3E%3D3.11-orange.svg\" alt=\"Python >= 3.11\">\n    <img src=\"https://img.shields.io/badge/tests-134%20passing-brightgreen.svg\" alt=\"134 Tests\">\n  </p>\n  <p align=\"center\">\n    <a href=\"#-quick-start\">Quick Start</a> |\n    <a href=\"#-available-tools\">Tools</a> |\n    <a href=\"#-supported-platforms\">Platforms</a> |\n    <a href=\"#%EF%B8%8F-configuration\">Configuration</a> |\n    <a href=\"https://github.com/EricGrill/agents-skills-plugins\">Plugin Marketplace</a>\n  </p>\n</p>\n\n---\n\n## What is this?\n\nAn MCP (Model Context Protocol) server that aggregates prediction market data from **5 major platforms**. Search markets, compare odds, detect arbitrage opportunities, and track predictions through natural language.\n\n**Works with Claude Desktop, Claude Code, Cursor, and any MCP-compatible client.**\n\n**Part of the [Claude Code Plugin Marketplace](https://github.com/EricGrill/agents-skills-plugins).**\n\n---\n\n## Quick Start\n\n```bash\n# Clone and install\ngit clone https://github.com/EricGrill/mcp-predictive-market.git\ncd mcp-predictive-market\nuv sync\n\n# Run the server\nuv run python -m mcp_predictive_market.server\n```\n\nAdd to your Claude config and start querying markets:\n\n> \"Find prediction markets about AI regulation\"\n\n---\n\n## Why Use This?\n\n| Feature | Description |\n|---------|-------------|\n| **Multi-Platform Search** | Query 5 prediction markets simultaneously |\n| **Arbitrage Detection** | Find price discrepancies across platforms |\n| **Market Tracking** | Build watchlists and monitor odds changes |\n| **Platform Comparison** | Side-by-side odds for similar questions |\n| **Unified Data Model** | Consistent market schema across all platforms |\n\n---\n\n## Available Tools\n\n### Search & Discovery\n\n| Tool | Description |\n|------|-------------|\n| `search_markets` | Search markets across all platforms by keyword |\n| `list_categories` | Get available market categories |\n| `browse_category` | Browse markets in a specific category |\n\n### Market Data\n\n| Tool | Description |\n|------|-------------|\n| `get_market_odds` | Get current odds for a specific market |\n| `compare_platforms` | Side-by-side comparison of similar markets |\n\n### Tracking\n\n| Tool | Description |\n|------|-------------|\n| `track_market` | Add a market to your watchlist |\n| `get_tracked_markets` | View all tracked markets with current prices |\n\n### Analysis\n\n| Tool | Description |\n|------|-------------|\n| `find_arbitrage` | Detect price discrepancies between platforms |\n\n---\n\n## Supported Platforms\n\n| Platform | URL | Specialization |\n|----------|-----|----------------|\n| **Manifold Markets** | [manifold.markets](https://manifold.markets) | Play money, wide variety |\n| **Polymarket** | [polymarket.com](https://polymarket.com) | Crypto, high liquidity |\n| **Metaculus** | [metaculus.com](https://metaculus.com) | Science, long-term forecasts |\n| **PredictIt** | [predictit.org](https://predictit.org) | US politics |\n| **Kalshi** | [kalshi.com](https://kalshi.com) | CFTC-regulated, real money |\n\n---\n\n## Configuration\n\n### Claude Desktop Setup\n\nAdd to your Claude Desktop config:\n\n| Platform | Config Path |\n|----------|-------------|\n| macOS | `~/Library/Application Support/Claude/claude_desktop_config.json` |\n| Windows | `%APPDATA%\\Claude\\claude_desktop_config.json` |\n| Linux | `~/.config/Claude/claude_desktop_config.json` |\n\n```json\n{\n  \"mcpServers\": {\n    \"prediction-market\": {\n      \"command\": \"uv\",\n      \"args\": [\"run\", \"--directory\", \"/path/to/mcp-predictive-market\", \"python\", \"-m\", \"mcp_predictive_market.server\"]\n    }\n  }\n}\n```\n\n### Claude Code Setup\n\nAdd to your project `.mcp.json` or `~/.config/claude-code/mcp.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"prediction-market\": {\n      \"command\": \"uv\",\n      \"args\": [\"run\", \"--directory\", \"/path/to/mcp-predictive-market\", \"python\", \"-m\", \"mcp_predictive_market.server\"]\n    }\n  }\n}\n```\n\n---\n\n## Examples\n\n<details>\n<summary><b>Search & Discovery</b></summary>\n\n```\n\"Find prediction markets about AI\"\n\"What categories of markets are available?\"\n\"Show me crypto markets on Polymarket\"\n\"Browse politics markets\"\n```\n\n</details>\n\n<details>\n<summary><b>Market Analysis</b></summary>\n\n```\n\"Get current odds for Manifold market abc123\"\n\"Compare odds for 'Will Bitcoin hit $100k?' across all platforms\"\n\"Show me the probability of a 2024 recession on different platforms\"\n```\n\n</details>\n\n<details>\n<summary><b>Arbitrage Detection</b></summary>\n\n```\n\"Find arbitrage opportunities with at least 10% spread\"\n\"Are there any markets with significantly different odds across platforms?\"\n\"Show me the biggest price discrepancies right now\"\n```\n\n</details>\n\n<details>\n<summary><b>Market Tracking</b></summary>\n\n```\n\"Track the Polymarket election market\"\n\"Show all my tracked markets\"\n\"What are the current prices on my watchlist?\"\n```\n\n</details>\n\n---\n\n## Development\n\n```bash\n# Clone\ngit clone https://github.com/EricGrill/mcp-predictive-market.git\ncd mcp-predictive-market\n\n# Install with dev dependencies\nuv sync --extra dev\n\n# Run tests\nuv run pytest -v\n\n# Run specific test file\nuv run pytest tests/test_integration.py -v\n```\n\n### Project Structure\n\n```\nsrc/mcp_predictive_market/\n├── server.py           # MCP server entry point\n├── tools.py            # Tool handler implementations\n├── schema.py           # Unified market data models\n├── errors.py           # Custom exceptions\n├── rate_limiter.py     # Per-platform rate limiting\n├── adapters/           # Platform-specific adapters\n│   ├── base.py         # Adapter protocol\n│   ├── manifold.py\n│   ├── polymarket.py\n│   ├── metaculus.py\n│   ├── predictit.py\n│   └── kalshi.py\n├── analysis/           # Market analysis modules\n│   ├── matching.py     # Cross-platform market matching\n│   └── arbitrage.py    # Arbitrage detection\n└── state/              # State management\n    └── memvid_client.py\n```\n\n---\n\n## Troubleshooting\n\n<details>\n<summary><b>No results from a platform</b></summary>\n\n1. Platform API may be rate-limited - wait and retry\n2. Check platform is online: visit the website directly\n3. Some platforms filter certain market types\n\n</details>\n\n<details>\n<summary><b>Arbitrage opportunities not found</b></summary>\n\n1. Lower the `min_spread` parameter (default is 5%)\n2. Try broader search terms\n3. Fewer opportunities exist in efficient markets\n\n</details>\n\n<details>\n<summary><b>Market not found</b></summary>\n\n1. Verify the market ID format (varies by platform)\n2. Ensure the market hasn't been resolved/closed\n3. Check you're using the correct platform name\n\n</details>\n\n---\n\n## Related Projects\n\n- [Claude Code Plugin Marketplace](https://github.com/EricGrill/agents-skills-plugins) - Discover more MCP plugins\n- [MCP Proxmox Admin](https://github.com/EricGrill/mcp-proxmox-admin) - Manage Proxmox VE through Claude\n- [MCP Memvid State Service](https://github.com/EricGrill/mcp-memvid-state-service) - Persistent state for MCP servers\n\n---\n\n## Contributing\n\nContributions welcome!\n\n1. Fork the repository\n2. Create feature branch: `git checkout -b feature/my-feature`\n3. Make changes and test: `uv run pytest`\n4. Commit: `git commit -m 'Add my feature'`\n5. Push: `git push origin feature/my-feature`\n6. Open a Pull Request\n\n---\n\n## License\n\nMIT\n",
        "plugins/mcp-bitcoin-cli/README.md": "<p align=\"center\">\n  <h1 align=\"center\">MCP Bitcoin CLI</h1>\n  <p align=\"center\">\n    <strong>Embed and read data on the Bitcoin blockchain through Claude</strong>\n  </p>\n  <p align=\"center\">\n    <a href=\"https://github.com/EricGrill/mcp-bitcoin-cli/blob/master/LICENSE\"><img src=\"https://img.shields.io/badge/license-MIT-blue.svg\" alt=\"MIT License\"></a>\n    <img src=\"https://img.shields.io/badge/tools-16-green.svg\" alt=\"16 Tools\">\n    <img src=\"https://img.shields.io/badge/OP__RETURN-100KB-orange.svg\" alt=\"OP_RETURN 100KB\">\n    <img src=\"https://img.shields.io/badge/python-%3E%3D3.11-blue.svg\" alt=\"Python >= 3.11\">\n  </p>\n  <p align=\"center\">\n    <a href=\"#-quick-start\">Quick Start</a> |\n    <a href=\"#-available-tools\">Tools</a> |\n    <a href=\"#%EF%B8%8F-configuration\">Configuration</a> |\n    <a href=\"#-contributing\">Contributing</a>\n  </p>\n</p>\n\n---\n\n## What is this?\n\nAn [MCP (Model Context Protocol)](https://modelcontextprotocol.io/) server that enables Claude to interact with Bitcoin's OP_RETURN functionality. Store documents, create timestamps, deploy tokens, and build custom protocols—all through natural language.\n\n**Works with Claude Desktop, Cursor, and any MCP-compatible client.**\n\n**Supports Bitcoin Core v30+ with up to ~100KB OP_RETURN data.**\n\n---\n\n## Quick Start\n\n```bash\n# Install from source\ngit clone https://github.com/EricGrill/mcp-bitcoin-cli.git\ncd mcp-bitcoin-cli\npip install -e .\n\n# Run the server\nmcp-bitcoin-cli\n```\n\nAdd to your Claude Desktop config and start working with Bitcoin:\n\n> \"Create a timestamp for this document on Bitcoin testnet\"\n\n---\n\n## Why Use This?\n\n| Feature | Description |\n|---------|-------------|\n| **Document Storage** | Embed documents up to 100KB directly on-chain |\n| **Timestamping** | Create immutable SHA-256/SHA3 hash commitments |\n| **BRC-20 Tokens** | Deploy, mint, and transfer tokens using the BRC-20 standard |\n| **Custom Protocols** | Build your own OP_RETURN protocols with the BTCD envelope format |\n| **Offline-Capable** | Encode/decode data without a running Bitcoin node |\n| **Safety First** | Testnet default, dry-run mode, fee warnings |\n\n---\n\n## Available Tools\n\n### Low-Level Primitives\n\nOffline-capable tools for data encoding and transaction building.\n\n| Tool | Description |\n|------|-------------|\n| `encode_op_return` | Encode arbitrary data into OP_RETURN script format |\n| `decode_op_return` | Parse and extract data from OP_RETURN scripts |\n| `build_op_return_transaction` | Construct transactions with OP_RETURN outputs |\n| `parse_envelope` | Parse BTCD envelope structure from raw bytes |\n\n### Bitcoin Core Interface\n\nTools for interacting with a running Bitcoin node.\n\n| Tool | Description |\n|------|-------------|\n| `get_node_info` | Check connection status and network info |\n| `list_utxos` | List available UTXOs for funding transactions |\n| `broadcast_transaction` | Send signed transactions (dry-run by default) |\n| `get_transaction` | Fetch and decode transaction details |\n| `search_op_returns` | Scan blocks for OP_RETURN transactions |\n\n### Token Operations (BRC-20)\n\nCreate and manage tokens using the [BRC-20 standard](https://domo-2.gitbook.io/brc-20-experiment/).\n\n| Tool | Description |\n|------|-------------|\n| `create_token_deploy` | Deploy a new BRC-20 token |\n| `create_token_mint` | Mint tokens from an existing deployment |\n| `create_token_transfer` | Create a transfer inscription |\n\n### Document Storage\n\nStore and retrieve documents on the blockchain.\n\n| Tool | Description |\n|------|-------------|\n| `embed_document` | Prepare documents for on-chain storage |\n| `read_document` | Parse and extract documents from transactions |\n\n### Timestamping & Attestation\n\nCreate cryptographic proofs of existence.\n\n| Tool | Description |\n|------|-------------|\n| `create_timestamp` | Create SHA-256/SHA3 hash commitments |\n| `verify_timestamp` | Verify data against on-chain timestamps |\n\n---\n\n## Data Envelope Format\n\nAll data uses the **BTCD envelope format** for discoverability and proper parsing:\n\n```\n┌─────────────────────────────────────────────────────────┐\n│ OP_RETURN Envelope (variable size, up to ~100KB)        │\n├──────────┬──────────┬──────────┬────────────────────────┤\n│ Magic    │ Version  │ Type     │ Payload                │\n│ (4 bytes)│ (1 byte) │ (1 byte) │ (variable)             │\n├──────────┼──────────┼──────────┼────────────────────────┤\n│ \"BTCD\"   │ 0x01     │ See below│ Type-specific data     │\n└──────────┴──────────┴──────────┴────────────────────────┘\n```\n\n| Type | Hex | Description |\n|------|-----|-------------|\n| RAW | `0x00` | Raw bytes, no structure |\n| TEXT | `0x01` | UTF-8 text |\n| JSON | `0x02` | JSON document |\n| HASH | `0x03` | Hash commitment (timestamp) |\n| TOKEN | `0x04` | Token operation (BRC-20) |\n| FILE | `0x05` | File with content-type |\n| CUSTOM | `0x80+` | User-defined protocols |\n\n---\n\n## Configuration\n\n### Claude Desktop Setup\n\nAdd to your Claude Desktop config:\n\n| Platform | Config Path |\n|----------|-------------|\n| macOS | `~/Library/Application Support/Claude/claude_desktop_config.json` |\n| Windows | `%APPDATA%\\Claude\\claude_desktop_config.json` |\n| Linux | `~/.config/Claude/claude_desktop_config.json` |\n\n```json\n{\n  \"mcpServers\": {\n    \"bitcoin\": {\n      \"command\": \"mcp-bitcoin-cli\",\n      \"env\": {\n        \"BITCOIN_NETWORK\": \"testnet\",\n        \"BITCOIN_CLI_PATH\": \"/usr/local/bin/bitcoin-cli\"\n      }\n    }\n  }\n}\n```\n\n### Configuration File\n\nCreate `~/.mcp-bitcoin-cli/config.toml`:\n\n```toml\n[connection]\nmethod = \"cli\"              # \"cli\" or \"rpc\"\nnetwork = \"testnet\"         # \"mainnet\", \"testnet\", \"signet\", \"regtest\"\n\n[cli]\npath = \"bitcoin-cli\"        # Path to bitcoin-cli binary\ndatadir = \"\"                # Optional: custom datadir\n\n[rpc]\nhost = \"127.0.0.1\"\nport = 18332                # Testnet default\nuser = \"\"\npassword = \"\"\n\n[safety]\nrequire_confirmation = true # Prompt before broadcast\ndry_run_default = true      # Always dry-run first\nmax_data_size = 102400      # 100KB limit\n```\n\n### Network Ports\n\n| Network | Default RPC Port |\n|---------|------------------|\n| Mainnet | 8332 |\n| Testnet | 18332 |\n| Signet | 38332 |\n| Regtest | 18443 |\n\n---\n\n## Examples\n\n<details>\n<summary><b>Timestamping</b></summary>\n\n```\n\"Create a SHA-256 timestamp for this contract\"\n\"Verify this document against timestamp in transaction abc123...\"\n\"Create a SHA3-256 hash commitment for my research paper\"\n```\n\n</details>\n\n<details>\n<summary><b>Document Storage</b></summary>\n\n```\n\"Embed this JSON configuration on the blockchain\"\n\"Store this text document with content-type text/plain\"\n\"Read the document from transaction def456...\"\n```\n\n</details>\n\n<details>\n<summary><b>BRC-20 Tokens</b></summary>\n\n```\n\"Deploy a new token called TEST with max supply 21 million\"\n\"Mint 1000 TEST tokens\"\n\"Create a transfer inscription for 500 TEST\"\n```\n\n</details>\n\n<details>\n<summary><b>Raw Data</b></summary>\n\n```\n\"Encode this hex data into an OP_RETURN script\"\n\"Decode the OP_RETURN from this transaction\"\n\"Build a transaction with this message embedded\"\n```\n\n</details>\n\n---\n\n## Architecture\n\n```\n┌─────────────────────────────────────────────────────────┐\n│                    MCP Server                           │\n├─────────────────────────────────────────────────────────┤\n│  High-Level Tools                                       │\n│  ┌─────────────┐ ┌─────────────┐ ┌─────────────────┐   │\n│  │ BRC-20 Ops  │ │ Document    │ │ Timestamp/      │   │\n│  │ deploy/mint │ │ Storage     │ │ Attestation     │   │\n│  └──────┬──────┘ └──────┬──────┘ └────────┬────────┘   │\n│         │               │                  │            │\n│  ───────┴───────────────┴──────────────────┴─────────  │\n│                                                         │\n│  Low-Level Primitives                                   │\n│  ┌─────────────┐ ┌─────────────┐ ┌─────────────────┐   │\n│  │ encode_     │ │ decode_     │ │ build_op_return │   │\n│  │ op_return   │ │ op_return   │ │ _transaction    │   │\n│  └─────────────┘ └─────────────┘ └─────────────────┘   │\n├─────────────────────────────────────────────────────────┤\n│  Bitcoin Core Interface (configurable)                  │\n│  ┌──────────────────┐  ┌────────────────────────────┐  │\n│  │ bitcoin-cli      │  │ JSON-RPC (direct)          │  │\n│  │ (subprocess)     │  │                            │  │\n│  └──────────────────┘  └────────────────────────────┘  │\n└─────────────────────────────────────────────────────────┘\n```\n\n---\n\n## Safety Features\n\n| Feature | Description |\n|---------|-------------|\n| **Testnet Default** | Network locked to testnet unless explicitly configured |\n| **Dry-Run Mode** | Transactions validated before broadcast by default |\n| **Fee Warnings** | Alerts for unusually high fees |\n| **Size Validation** | Rejects data exceeding configured max before building |\n| **Network Lock** | Can't switch networks mid-session |\n\n---\n\n## Development\n\n```bash\n# Clone\ngit clone https://github.com/EricGrill/mcp-bitcoin-cli.git\ncd mcp-bitcoin-cli\n\n# Install with dev dependencies\npip install -e \".[dev]\"\n\n# Run tests\npytest -v\n\n# Run tests with coverage\npytest --cov=mcp_bitcoin_cli\n```\n\n### Project Structure\n\n```\nsrc/mcp_bitcoin_cli/\n├── __init__.py          # Public exports\n├── server.py            # MCP server with 16 tools\n├── envelope.py          # BTCD envelope encoding/decoding\n├── primitives.py        # OP_RETURN script encoding/decoding\n├── config.py            # Configuration loading\n├── node/\n│   ├── interface.py     # Abstract node interface\n│   ├── cli.py           # bitcoin-cli subprocess\n│   └── rpc.py           # JSON-RPC direct connection\n└── protocols/\n    ├── base.py          # Base protocol class\n    └── brc20.py         # BRC-20 token protocol\n```\n\n---\n\n## Troubleshooting\n\n<details>\n<summary><b>Cannot connect to Bitcoin Core</b></summary>\n\n1. Verify Bitcoin Core is running: `bitcoin-cli getblockchaininfo`\n2. Check network matches config (testnet vs mainnet)\n3. Verify RPC credentials if using JSON-RPC mode\n\n</details>\n\n<details>\n<summary><b>Transaction rejected</b></summary>\n\n1. Use `broadcast_transaction` with `dry_run=true` first\n2. Check fee rate is sufficient\n3. Verify UTXOs have enough confirmations\n\n</details>\n\n<details>\n<summary><b>Data too large</b></summary>\n\n- Bitcoin Core v30+ supports up to ~100KB OP_RETURN\n- Older versions limited to 80 bytes\n- Check `max_data_size` in config\n\n</details>\n\n<details>\n<summary><b>Import errors</b></summary>\n\n```bash\n# Verify installation\npython -c \"import mcp_bitcoin_cli; print(mcp_bitcoin_cli.__version__)\"\n\n# Reinstall if needed\npip install -e \".[dev]\"\n```\n\n</details>\n\n---\n\n## Contributing\n\nContributions welcome!\n\n1. Fork the repository\n2. Create feature branch: `git checkout -b feature/my-feature`\n3. Make changes and test: `pytest`\n4. Commit: `git commit -m 'Add my feature'`\n5. Push: `git push origin feature/my-feature`\n6. Open a Pull Request\n\n---\n\n## Related Projects\n\n- [MCP Proxmox Admin](https://github.com/EricGrill/mcp-proxmox-admin) - Manage Proxmox VE through Claude\n- [Model Context Protocol](https://modelcontextprotocol.io/) - The protocol specification\n- [BRC-20 Standard](https://domo-2.gitbook.io/brc-20-experiment/) - Bitcoin token standard\n\n---\n\n## License\n\nMIT\n",
        "plugins/mcp-civic-data/README.md": "# mcp-civic-data\n\n**Access free government and open data APIs through Claude**\n\n[![MIT License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)\n[![22 Tools](https://img.shields.io/badge/Tools-22-blue.svg)](#-tool-catalog)\n[![7 APIs](https://img.shields.io/badge/APIs-7-orange.svg)](#-included-apis)\n[![Python 3.11+](https://img.shields.io/badge/Python-3.11+-yellow.svg)](https://python.org)\n\n[Quick Start](#-quick-start) | [Tool Catalog](#-tool-catalog) | [Configuration](#-configuration) | [Examples](#-examples)\n\n---\n\n## 🌐 What is this?\n\nAn MCP (Model Context Protocol) server that gives Claude access to **7 free government and open data APIs** - weather forecasts, census demographics, NASA imagery, economic indicators, and more. No API keys required for most features.\n\n> Part of the [Claude Code Plugin Marketplace](https://github.com/EricGrill/agents-skills-plugins) ecosystem.\n\n---\n\n## 🚀 Quick Start\n\n**Add to Claude Desktop:**\n\n```json\n{\n  \"mcpServers\": {\n    \"civic-data\": {\n      \"command\": \"python\",\n      \"args\": [\"-m\", \"mcp_govt_api\"],\n      \"env\": {\n        \"OPENWEATHER_API_KEY\": \"optional-for-global-weather\",\n        \"NASA_API_KEY\": \"optional-for-higher-limits\"\n      }\n    }\n  }\n}\n```\n\n**Or install manually:**\n\n```bash\npip install mcp-civic-data\n```\n\n---\n\n## 📡 Included APIs\n\n| API | Coverage | Key Required |\n|-----|----------|--------------|\n| **NOAA Weather** | US forecasts, alerts, radar | No |\n| **OpenWeather** | Global weather conditions | Yes |\n| **US Census** | Population, demographics, housing | No |\n| **NASA** | APOD, Mars rovers, image library | No (optional) |\n| **World Bank** | GDP, poverty, country indicators | No |\n| **Data.gov** | 300,000+ US government datasets | No |\n| **EU Open Data** | European Union datasets | No |\n\n---\n\n## 💡 Why Use This?\n\n| Feature | Description |\n|---------|-------------|\n| **Zero config** | Works immediately - most APIs need no keys |\n| **Graceful fallback** | Missing keys? Those tools just won't appear |\n| **Real data** | Live government sources, not cached or stale |\n| **22 tools** | From quick lookups to raw API access |\n| **Well-documented** | Every tool has clear parameters and examples |\n\n---\n\n## 📦 Tool Catalog\n\n| Category | Tools | What You Can Do |\n|----------|-------|-----------------|\n| **Weather** | 5 | US forecasts, alerts, global conditions |\n| **Census** | 4 | Population, demographics, housing stats |\n| **NASA** | 4 | Astronomy photos, Mars rovers, image search |\n| **Economics** | 3 | Country GDP, poverty, comparisons |\n| **Data.gov** | 3 | Search/explore US government datasets |\n| **EU Data** | 3 | Search/explore European datasets |\n\n---\n\n## 🔧 All Tools\n\n### Weather (NOAA + OpenWeather)\n\n| Tool | Description |\n|------|-------------|\n| `get_weather_forecast` | 7-day forecast for US coordinates |\n| `get_weather_alerts` | Active alerts by state (CA, TX, NY...) |\n| `get_global_weather` | Current weather for any city worldwide |\n| `query_noaa` | Raw NOAA API access |\n| `query_openweather` | Raw OpenWeather API access |\n\n### US Census\n\n| Tool | Description |\n|------|-------------|\n| `get_population` | Population by state or county |\n| `get_demographics` | Age, race, income breakdown |\n| `get_housing_stats` | Home values, rent, vacancy rates |\n| `query_census` | Raw Census API with custom variables |\n\n### NASA\n\n| Tool | Description |\n|------|-------------|\n| `get_astronomy_photo` | Astronomy Picture of the Day |\n| `get_mars_rover_photos` | Curiosity, Perseverance photos |\n| `search_nasa_images` | Search NASA's image/video library |\n| `query_nasa` | Raw NASA API access |\n\n### World Bank Economics\n\n| Tool | Description |\n|------|-------------|\n| `get_country_indicators` | GDP, population, poverty for any country |\n| `compare_countries` | Compare indicators across countries |\n| `query_worldbank` | Raw World Bank API access |\n\n### Data.gov\n\n| Tool | Description |\n|------|-------------|\n| `search_datasets` | Search 300,000+ US government datasets |\n| `get_dataset_info` | Metadata and download links |\n| `query_datagov` | Raw CKAN API access |\n\n### EU Open Data\n\n| Tool | Description |\n|------|-------------|\n| `search_eu_datasets` | Search European Union datasets |\n| `get_eu_dataset_info` | Dataset details and distributions |\n| `query_eu_data` | Raw EU Data Portal API access |\n\n---\n\n## ⚙️ Configuration\n\n### Environment Variables\n\n| Variable | Required | Description |\n|----------|----------|-------------|\n| `OPENWEATHER_API_KEY` | For global weather | [Get free key](https://openweathermap.org/api) |\n| `NASA_API_KEY` | Optional | Higher rate limits (1000/hr vs 30/hr) |\n| `API_TIMEOUT` | Optional | Request timeout in seconds (default: 30) |\n\n### API Availability on Startup\n\n```\nAPI Availability:\n  ✓ NOAA (no key required)\n  ✓ Census (no key required)\n  ✓ NASA (no key, limited to 30 req/hour)\n  ✗ OpenWeather (OPENWEATHER_API_KEY not set)\n  ✓ World Bank (no key required)\n  ✓ Data.gov (no key required)\n  ✓ EU Open Data (no key required)\n```\n\n---\n\n## 📝 Examples\n\n### Get weather forecast\n\n```\n\"What's the weather forecast for Washington DC?\"\n→ Uses get_weather_forecast(38.8894, -77.0352)\n```\n\n### Check demographics\n\n```\n\"What's the population and median income in California?\"\n→ Uses get_demographics(\"CA\")\n```\n\n### Explore Mars\n\n```\n\"Show me recent photos from the Perseverance rover\"\n→ Uses get_mars_rover_photos(rover=\"perseverance\")\n```\n\n### Compare economies\n\n```\n\"Compare GDP between USA, China, and India\"\n→ Uses compare_countries([\"USA\", \"CHN\", \"IND\"])\n```\n\n### Find government data\n\n```\n\"Find datasets about climate change on Data.gov\"\n→ Uses search_datasets(\"climate change\")\n```\n\n---\n\n## 🏗️ Development\n\n```bash\n# Clone and install\ngit clone https://github.com/EricGrill/mcp-civic-data.git\ncd mcp-civic-data\npip install -e .\n\n# Run locally\npython -m mcp_govt_api\n```\n\n---\n\n## 🤝 Contributing\n\n1. Fork the repository\n2. Create your feature branch (`git checkout -b feature/amazing`)\n3. Commit changes (`git commit -m 'Add amazing feature'`)\n4. Push to branch (`git push origin feature/amazing`)\n5. Open a Pull Request\n\n---\n\n## 📜 License\n\nMIT License - see [LICENSE](LICENSE) for details.\n\n---\n\n<p align=\"center\">\n  <a href=\"https://github.com/EricGrill/agents-skills-plugins\">\n    <img src=\"https://img.shields.io/badge/Part%20of-Claude%20Code%20Plugin%20Marketplace-blueviolet?style=for-the-badge\" alt=\"Plugin Marketplace\">\n  </a>\n</p>\n",
        "plugins/mcp-memvid-state-service/README.md": "# MCP-Memvid-State-Service\n\n**Single-file AI memory layer with vector search, full-text search, and temporal queries**\n\n[![MIT License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)\n[![10 Tools](https://img.shields.io/badge/Tools-10-blue.svg)](#-tool-catalog)\n[![Ollama](https://img.shields.io/badge/Embeddings-Ollama-white.svg)](https://ollama.ai/)\n[![OpenAI](https://img.shields.io/badge/Embeddings-OpenAI-412991.svg)](https://openai.com/)\n[![Local](https://img.shields.io/badge/Embeddings-Local-orange.svg)](#embedding-providers)\n\n[Quick Start](#-quick-start) | [Tool Catalog](#-tool-catalog) | [Embedding Providers](#-embedding-providers) | [Configuration](#-configuration) | [Examples](#-examples)\n\n---\n\n## 🧠 What is this?\n\nAn MCP (Model Context Protocol) server wrapping [memvid](https://memvid.com) - a Rust-based memory system that stores everything in a single portable `.mv2` file. Replace Redis for caching, Qdrant/Pinecone for vector search, and SQLite for structured queries—all without external infrastructure.\n\n> Part of the [Claude Code Plugin Marketplace](https://github.com/EricGrill/agents-skills-plugins) ecosystem.\n\n---\n\n## 🚀 Quick Start\n\n**1. Add to Claude Code:**\n\n```json\n{\n  \"mcpServers\": {\n    \"memvid\": {\n      \"command\": \"npx\",\n      \"args\": [\"mcp-memvid\"],\n      \"env\": {\n        \"OLLAMA_HOST\": \"http://localhost:11434\"\n      }\n    }\n  }\n}\n```\n\n**2. Or install and run manually:**\n\n```bash\ngit clone https://github.com/EricGrill/mcp-memvid-state-service.git\ncd mcp-memvid-state-service\nnpm install && npm run build\nnode dist/index.js\n```\n\n---\n\n## 💡 Why Use MCP-Memvid?\n\n| Feature | Description |\n|---------|-------------|\n| **Single-file storage** | All data, indices, and metadata in one portable `.mv2` file |\n| **No infrastructure** | No Redis, no Postgres, no vector DB cluster to manage |\n| **Triple search** | Semantic (vector), lexical (BM25), and temporal queries |\n| **Local-first** | Built-in embedding models work offline on Linux/macOS |\n| **Ollama support** | Use local LLMs for embeddings without API costs |\n\n---\n\n## 📦 Tool Catalog\n\n| Category | Tools | Description |\n|----------|-------|-------------|\n| **Storage** | 2 | Store and delete memories (`store_memory`, `delete_capsule`) |\n| **Search** | 4 | Vector, keyword, smart, and temporal (`semantic_search`, `text_search`, `smart_search`, `recent_memories`) |\n| **Management** | 3 | Capsule lifecycle (`list_capsules`, `create_capsule`, `capsule_info`) |\n| **Config** | 1 | View embedding status (`embedding_config`) |\n\n---\n\n## 🔧 All Tools\n\n### Storage\n\n| Tool | Description |\n|------|-------------|\n| `store_memory` | Store text with title, tags, metadata, and optional embeddings |\n| `delete_capsule` | Permanently delete a capsule file (requires confirmation) |\n\n### Search\n\n| Tool | Description |\n|------|-------------|\n| `semantic_search` | Find by meaning using vector embeddings (HNSW) |\n| `text_search` | Find by exact keywords using BM25 ranking |\n| `smart_search` | Auto-select best search mode based on query |\n| `recent_memories` | Retrieve memories in chronological order |\n\n### Capsule Management\n\n| Tool | Description |\n|------|-------------|\n| `list_capsules` | List all available memory capsules |\n| `create_capsule` | Create a new empty capsule |\n| `capsule_info` | Get storage path and existence status |\n\n### Configuration\n\n| Tool | Description |\n|------|-------------|\n| `embedding_config` | Show current embedding model, Ollama status, API keys |\n\n---\n\n## 🤖 Embedding Providers\n\n| Provider | Setup | Models | Best For |\n|----------|-------|--------|----------|\n| **Local** | None needed | `bge-small`, `bge-base`, `nomic`, `gte-large` | Offline, privacy-first |\n| **Ollama** | `OLLAMA_HOST=http://localhost:11434` | Any via OpenAI API | Local LLMs, no API costs |\n| **OpenAI** | `OPENAI_API_KEY=sk-...` | `openai-small`, `openai-large` | Best quality, cloud |\n\n### Ollama Setup\n\n```bash\n# Install Ollama\ncurl -fsSL https://ollama.ai/install.sh | sh\n\n# Pull an embedding model\nollama pull nomic-embed-text\n\n# Set environment variable\nexport OLLAMA_HOST=http://localhost:11434\n```\n\n---\n\n## ⚙️ Configuration\n\n### Environment Variables\n\n| Variable | Description | Default |\n|----------|-------------|---------|\n| `OLLAMA_HOST` | Ollama server URL | — |\n| `OPENAI_API_KEY` | OpenAI API key | — |\n| `OPENAI_BASE_URL` | Custom OpenAI-compatible endpoint | — |\n| `MEMVID_EMBEDDING_MODEL` | Default embedding model | `bge-small` |\n| `XDG_DATA_HOME` | Base storage directory | `~/.local/share` |\n\n### Storage Location\n\n```\n$XDG_DATA_HOME/memvid/capsules/\n├── agent-context.mv2\n├── knowledge-base.mv2\n└── session-cache.mv2\n```\n\n---\n\n## 📝 Examples\n\n### Store a memory with embeddings\n\n```javascript\nstore_memory({\n  capsule: \"knowledge-base\",\n  text: \"The API uses JWT tokens with 24-hour expiry. Refresh tokens last 7 days.\",\n  title: \"Auth Architecture\",\n  tags: [\"api\", \"security\", \"jwt\"],\n  enable_embedding: true,\n  embedding_model: \"bge-small\"\n})\n```\n\n### Semantic search\n\n```javascript\nsemantic_search({\n  capsule: \"knowledge-base\",\n  query: \"how long do authentication tokens last\",\n  limit: 5\n})\n```\n\n### Get recent context\n\n```javascript\nrecent_memories({\n  capsule: \"agent-context\",\n  limit: 10\n})\n```\n\n### Check embedding configuration\n\n```javascript\nembedding_config()\n// Returns:\n// {\n//   \"defaultModel\": \"bge-small\",\n//   \"ollamaHost\": \"http://localhost:11434\",\n//   \"openaiBaseUrl\": \"http://localhost:11434/v1\",\n//   ...\n// }\n```\n\n---\n\n## 🖥️ Platform Support\n\n| Platform | Local Embeddings | Notes |\n|----------|------------------|-------|\n| Linux x64 | ✅ Yes | Full support |\n| macOS ARM64 | ✅ Yes | Full support (Apple Silicon) |\n| macOS x64 | ✅ Yes | Full support (Intel) |\n| Windows x64 | ❌ No | Use Ollama or OpenAI |\n\n---\n\n## 🤝 Contributing\n\n1. Fork the repository\n2. Create your feature branch (`git checkout -b feature/amazing`)\n3. Commit changes (`git commit -m 'Add amazing feature'`)\n4. Push to branch (`git push origin feature/amazing`)\n5. Open a Pull Request\n\n---\n\n## 📜 License\n\nMIT License - see [LICENSE](LICENSE) for details.\n\n---\n\n<p align=\"center\">\n  <a href=\"https://github.com/EricGrill/agents-skills-plugins\">\n    <img src=\"https://img.shields.io/badge/Part%20of-Claude%20Code%20Plugin%20Marketplace-blueviolet?style=for-the-badge\" alt=\"Plugin Marketplace\">\n  </a>\n</p>\n",
        "plugins/superpowers/README.md": "# Superpowers\n\nCore skills library for Claude Code: TDD, debugging, collaboration patterns, and proven techniques.\n\n**Original Author:** [Jesse Vincent](https://github.com/obra) ([@obra](https://github.com/obra))\n**Original Repository:** https://github.com/obra/superpowers\n**License:** MIT\n\nThis is a fork of the original superpowers plugin, included in this marketplace for convenience.\n\n## Skills (14)\n\n- **brainstorming** - Turn ideas into fully formed designs through collaborative dialogue\n- **dispatching-parallel-agents** - Run multiple independent tasks concurrently\n- **executing-plans** - Execute implementation plans with review checkpoints\n- **finishing-a-development-branch** - Complete development work with merge/PR options\n- **receiving-code-review** - Handle code review feedback with technical rigor\n- **requesting-code-review** - Verify work meets requirements before merging\n- **subagent-driven-development** - Execute plans with independent subtasks\n- **systematic-debugging** - Debug bugs and test failures methodically\n- **test-driven-development** - Implement features with TDD workflow\n- **using-git-worktrees** - Create isolated git worktrees for feature work\n- **using-superpowers** - Introduction to finding and using skills\n- **verification-before-completion** - Verify work before claiming completion\n- **writing-plans** - Design implementation plans for multi-step tasks\n- **writing-skills** - Create and test new skills\n\n## Agents (1)\n\n- **code-reviewer** - Reviews code against plans and coding standards\n\n## Commands (3)\n\n- `/brainstorm` - Start a brainstorming session\n- `/write-plan` - Create an implementation plan\n- `/execute-plan` - Execute an existing plan\n\n## Installation\n\n```\n/plugin install superpowers@agents-skills-plugins\n```\n\n## Attribution\n\nAll credit for the original superpowers plugin goes to Jesse Vincent. This fork is distributed under the same MIT license.\n",
        "plugins/awesome-claude-skills/README.md": "<h1 align=\"center\">Awesome Claude Skills</h1>\n\n<p align=\"center\">\n<a href=\"https://platform.composio.dev/?utm_source=Github&utm_medium=Youtube&utm_campaign=2025-11&utm_content=AwesomeSkills\">\n  <img width=\"1280\" height=\"640\" alt=\"Composio banner\" src=\"https://github.com/user-attachments/assets/adb3f57a-2706-4329-856f-059a32059d48\">\n</a>\n\n\n</p>\n\n<p align=\"center\">\n  <a href=\"https://awesome.re\">\n    <img src=\"https://awesome.re/badge.svg\" alt=\"Awesome\" />\n  </a>\n  <a href=\"https://makeapullrequest.com\">\n    <img src=\"https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square\" alt=\"PRs Welcome\" />\n  </a>\n  <a href=\"https://www.apache.org/licenses/LICENSE-2.0\">\n    <img src=\"https://img.shields.io/badge/License-Apache_2.0-blue.svg?style=flat-square\" alt=\"License: Apache-2.0\" />\n  </a>\n</p>\n<div>\n<p align=\"center\">\n  <a href=\"https://twitter.com/composio\">\n    <img src=\"https://img.shields.io/badge/Follow on X-000000?style=for-the-badge&logo=x&logoColor=white\" alt=\"Follow on X\" />\n  </a>\n  <a href=\"https://www.linkedin.com/company/composiohq/\">\n    <img src=\"https://img.shields.io/badge/Follow on LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white\" alt=\"Follow on LinkedIn\" />\n  </a>\n  <a href=\"https://discord.com/invite/composio\">\n    <img src=\"https://img.shields.io/badge/Join our Discord-5865F2?style=for-the-badge&logo=discord&logoColor=white\" alt=\"Join our Discord\" />\n  </a>\n  </p>\n</div>\n\nA curated list of practical Claude Skills for enhancing productivity across Claude.ai, Claude Code, and the Claude API.\n\n\n> If you want your skills to take actions across 500+ apps, wire them up with [Composio](https://platform.composio.dev/?utm_source=Github&utm_medium=Youtube&utm_campaign=2025-11&utm_content=AwesomeSkills)\n\n\n## Contents\n\n- [What Are Claude Skills?](#what-are-claude-skills)\n- [Skills](#skills)\n  - [Document Processing](#document-processing)\n  - [Development & Code Tools](#development--code-tools)\n  - [Data & Analysis](#data--analysis)\n  - [Business & Marketing](#business--marketing)\n  - [Communication & Writing](#communication--writing)\n  - [Creative & Media](#creative--media)\n  - [Productivity & Organization](#productivity--organization)\n  - [Collaboration & Project Management](#collaboration--project-management)\n  - [Security & Systems](#security--systems)\n- [Getting Started](#getting-started)\n- [Creating Skills](#creating-skills)\n- [Contributing](#contributing)\n- [Resources](#resources)\n- [License](#license)\n\n## What Are Claude Skills?\n\nClaude Skills are customizable workflows that teach Claude how to perform specific tasks according to your unique requirements. Skills enable Claude to execute tasks in a repeatable, standardized manner across all Claude platforms.\n\n## Skills\n\n### Document Processing\n\n- [docx](https://github.com/anthropics/skills/tree/main/skills/docx) - Create, edit, analyze Word docs with tracked changes, comments, formatting.\n- [pdf](https://github.com/anthropics/skills/tree/main/skills/pdf) - Extract text, tables, metadata, merge & annotate PDFs.\n- [pptx](https://github.com/anthropics/skills/tree/main/skills/pptx) - Read, generate, and adjust slides, layouts, templates.\n- [xlsx](https://github.com/anthropics/skills/tree/main/skills/xlsx) - Spreadsheet manipulation: formulas, charts, data transformations.\n- [Markdown to EPUB Converter](https://github.com/smerchek/claude-epub-skill) - Converts markdown documents and chat summaries into professional EPUB ebook files. *By [@smerchek](https://github.com/smerchek)*\n\n### Development & Code Tools\n\n- [artifacts-builder](https://github.com/anthropics/skills/tree/main/web-artifacts-builder) - Suite of tools for creating elaborate, multi-component claude.ai HTML artifacts using modern frontend web technologies (React, Tailwind CSS, shadcn/ui).\n- [aws-skills](https://github.com/zxkane/aws-skills) - AWS development with CDK best practices, cost optimization MCP servers, and serverless/event-driven architecture patterns.\n- [Changelog Generator](./changelog-generator/) - Automatically creates user-facing changelogs from git commits by analyzing history and transforming technical commits into customer-friendly release notes.\n- [Claude Code Terminal Title](https://github.com/bluzername/claude-code-terminal-title) - Gives each Claud-Code terminal window a dynamic title that describes the work being done so you don't lose track of what window is doing what.\n- [D3.js Visualization](https://github.com/chrisvoncsefalvay/claude-d3js-skill) - Teaches Claude to produce D3 charts and interactive data visualizations. *By [@chrisvoncsefalvay](https://github.com/chrisvoncsefalvay)*\n- [FFUF Web Fuzzing](https://github.com/jthack/ffuf_claude_skill) - Integrates the ffuf web fuzzer so Claude can run fuzzing tasks and analyze results for vulnerabilities. *By [@jthack](https://github.com/jthack)*\n- [finishing-a-development-branch](https://github.com/obra/superpowers/tree/main/skills/finishing-a-development-branch) - Guides completion of development work by presenting clear options and handling chosen workflow.\n- [iOS Simulator](https://github.com/conorluddy/ios-simulator-skill) - Enables Claude to interact with iOS Simulator for testing and debugging iOS applications. *By [@conorluddy](https://github.com/conorluddy)*\n- [MCP Builder](./mcp-builder/) - Guides creation of high-quality MCP (Model Context Protocol) servers for integrating external APIs and services with LLMs using Python or TypeScript.\n- [move-code-quality-skill](https://github.com/1NickPappas/move-code-quality-skill) - Analyzes Move language packages against the official Move Book Code Quality Checklist for Move 2024 Edition compliance and best practices.\n- [Playwright Browser Automation](https://github.com/lackeyjb/playwright-skill) - Model-invoked Playwright automation for testing and validating web applications. *By [@lackeyjb](https://github.com/lackeyjb)*\n- [prompt-engineering](https://github.com/NeoLabHQ/context-engineering-kit/tree/master/plugins/customaize-agent/skills/prompt-engineering) - Teaches well-known prompt engineering techniques and patterns, including Anthropic best practices and agent persuasion principles.\n- [pypict-claude-skill](https://github.com/omkamal/pypict-claude-skill) - Design comprehensive test cases using PICT (Pairwise Independent Combinatorial Testing) for requirements or code, generating optimized test suites with pairwise coverage.\n- [Skill Creator](./skill-creator/) - Provides guidance for creating effective Claude Skills that extend capabilities with specialized knowledge, workflows, and tool integrations.\n- [Skill Seekers](https://github.com/yusufkaraaslan/Skill_Seekers) - Automatically converts any documentation website into a Claude AI skill in minutes. *By [@yusufkaraaslan](https://github.com/yusufkaraaslan)*\n- [software-architecture](https://github.com/NeoLabHQ/context-engineering-kit/tree/master/plugins/ddd/skills/software-architecture) - Implements design patterns including Clean Architecture, SOLID principles, and comprehensive software design best practices.\n- [subagent-driven-development](https://github.com/NeoLabHQ/context-engineering-kit/tree/master/plugins/sadd/skills/subagent-driven-development) - Dispatches independent subagents for individual tasks with code review checkpoints between iterations for rapid, controlled development.\n- [test-driven-development](https://github.com/obra/superpowers/tree/main/skills/test-driven-development) - Use when implementing any feature or bugfix, before writing implementation code.\n- [using-git-worktrees](https://github.com/obra/superpowers/blob/main/skills/using-git-worktrees/) - Creates isolated git worktrees with smart directory selection and safety verification.\n- [Webapp Testing](./webapp-testing/) - Tests local web applications using Playwright for verifying frontend functionality, debugging UI behavior, and capturing screenshots.\n\n### Data & Analysis\n\n- [CSV Data Summarizer](https://github.com/coffeefuelbump/csv-data-summarizer-claude-skill) - Automatically analyzes CSV files and generates comprehensive insights with visualizations without requiring user prompts. *By [@coffeefuelbump](https://github.com/coffeefuelbump)*\n- [postgres](https://github.com/sanjay3290/ai-skills/tree/main/skills/postgres) - Execute safe read-only SQL queries against PostgreSQL databases with multi-connection support and defense-in-depth security. *By [@sanjay3290](https://github.com/sanjay3290)*\n- [root-cause-tracing](https://github.com/obra/superpowers/tree/main/skills/root-cause-tracing) - Use when errors occur deep in execution and you need to trace back to find the original trigger.\n\n### Business & Marketing\n\n- [Brand Guidelines](./brand-guidelines/) - Applies Anthropic's official brand colors and typography to artifacts for consistent visual identity and professional design standards.\n- [Competitive Ads Extractor](./competitive-ads-extractor/) - Extracts and analyzes competitors' ads from ad libraries to understand messaging and creative approaches that resonate.\n- [Domain Name Brainstormer](./domain-name-brainstormer/) - Generates creative domain name ideas and checks availability across multiple TLDs including .com, .io, .dev, and .ai extensions.\n- [Internal Comms](./internal-comms/) - Helps write internal communications including 3P updates, company newsletters, FAQs, status reports, and project updates using company-specific formats.\n- [Lead Research Assistant](./lead-research-assistant/) - Identifies and qualifies high-quality leads by analyzing your product, searching for target companies, and providing actionable outreach strategies.\n\n### Communication & Writing\n\n- [article-extractor](https://github.com/michalparkola/tapestry-skills-for-claude-code/tree/main/article-extractor) - Extract full article text and metadata from web pages.\n- [brainstorming](https://github.com/obra/superpowers/tree/main/skills/brainstorming) - Transform rough ideas into fully-formed designs through structured questioning and alternative exploration.\n- [Content Research Writer](./content-research-writer/) - Assists in writing high-quality content by conducting research, adding citations, improving hooks, and providing section-by-section feedback.\n- [family-history-research](https://github.com/emaynard/claude-family-history-research-skill) - Provides assistance with planning family history and genealogy research projects.\n- [Meeting Insights Analyzer](./meeting-insights-analyzer/) - Analyzes meeting transcripts to uncover behavioral patterns including conflict avoidance, speaking ratios, filler words, and leadership style.\n- [NotebookLM Integration](https://github.com/PleasePrompto/notebooklm-skill) - Lets Claude Code chat directly with NotebookLM for source-grounded answers based exclusively on uploaded documents. *By [@PleasePrompto](https://github.com/PleasePrompto)*\n\n### Creative & Media\n\n- [Canvas Design](./canvas-design/) - Creates beautiful visual art in PNG and PDF documents using design philosophy and aesthetic principles for posters, designs, and static pieces.\n- [imagen](https://github.com/sanjay3290/ai-skills/tree/main/skills/imagen) - Generate images using Google Gemini's image generation API for UI mockups, icons, illustrations, and visual assets. *By [@sanjay3290](https://github.com/sanjay3290)*\n- [Image Enhancer](./image-enhancer/) - Improves image and screenshot quality by enhancing resolution, sharpness, and clarity for professional presentations and documentation.\n- [Slack GIF Creator](./slack-gif-creator/) - Creates animated GIFs optimized for Slack with validators for size constraints and composable animation primitives.\n- [Theme Factory](./theme-factory/) - Applies professional font and color themes to artifacts including slides, docs, reports, and HTML landing pages with 10 pre-set themes.\n- [Video Downloader](./video-downloader/) - Downloads videos from YouTube and other platforms for offline viewing, editing, or archival with support for various formats and quality options.\n- [youtube-transcript](https://github.com/michalparkola/tapestry-skills-for-claude-code/tree/main/youtube-transcript) - Fetch transcripts from YouTube videos and prepare summaries.\n\n### Productivity & Organization\n\n- [File Organizer](./file-organizer/) - Intelligently organizes files and folders by understanding context, finding duplicates, and suggesting better organizational structures.\n- [Invoice Organizer](./invoice-organizer/) - Automatically organizes invoices and receipts for tax preparation by reading files, extracting information, and renaming consistently.\n- [kaizen](https://github.com/NeoLabHQ/context-engineering-kit/tree/master/plugins/kaizen/skills/kaizen) - Applies continuous improvement methodology with multiple analytical approaches, based on Japanese Kaizen philosophy and Lean methodology.\n- [n8n-skills](https://github.com/haunchen/n8n-skills) - Enables AI assistants to directly understand and operate n8n workflows.\n- [Raffle Winner Picker](./raffle-winner-picker/) - Randomly selects winners from lists, spreadsheets, or Google Sheets for giveaways and contests with cryptographically secure randomness.\n- [ship-learn-next](https://github.com/michalparkola/tapestry-skills-for-claude-code/tree/main/ship-learn-next) - Skill to help iterate on what to build or learn next, based on feedback loops.\n- [tapestry](https://github.com/michalparkola/tapestry-skills-for-claude-code/tree/main/tapestry) - Interlink and summarize related documents into knowledge networks.\n\n### Collaboration & Project Management\n\n- [git-pushing](https://github.com/mhattingpete/claude-skills-marketplace/tree/main/engineering-workflow-plugin/skills/git-pushing) - Automate git operations and repository interactions.\n- [review-implementing](https://github.com/mhattingpete/claude-skills-marketplace/tree/main/engineering-workflow-plugin/skills/review-implementing) - Evaluate code implementation plans and align with specs.\n- [test-fixing](https://github.com/mhattingpete/claude-skills-marketplace/tree/main/engineering-workflow-plugin/skills/test-fixing) - Detect failing tests and propose patches or fixes.\n\n### Security & Systems\n\n- [computer-forensics](https://github.com/mhattingpete/claude-skills-marketplace/tree/main/computer-forensics-skills/skills/computer-forensics) - Digital forensics analysis and investigation techniques.\n- [file-deletion](https://github.com/mhattingpete/claude-skills-marketplace/tree/main/computer-forensics-skills/skills/file-deletion) - Secure file deletion and data sanitization methods.\n- [metadata-extraction](https://github.com/mhattingpete/claude-skills-marketplace/tree/main/computer-forensics-skills/skills/metadata-extraction) - Extract and analyze file metadata for forensic purposes.\n- [threat-hunting-with-sigma-rules](https://github.com/jthack/threat-hunting-with-sigma-rules-skill) - Use Sigma detection rules to hunt for threats and analyze security events.\n\n## Getting Started\n\n### Using Skills in Claude.ai\n\n1. Click the skill icon (🧩) in your chat interface.\n2. Add skills from the marketplace or upload custom skills.\n3. Claude automatically activates relevant skills based on your task.\n\n### Using Skills in Claude Code\n\n1. Place the skill in `~/.config/claude-code/skills/`:\n   ```bash\n   mkdir -p ~/.config/claude-code/skills/\n   cp -r skill-name ~/.config/claude-code/skills/\n   ```\n\n2. Verify skill metadata:\n   ```bash\n   head ~/.config/claude-code/skills/skill-name/SKILL.md\n   ```\n\n3. Start Claude Code:\n   ```bash\n   claude\n   ```\n\n4. The skill loads automatically and activates when relevant.\n\n### Using Skills via API\n\nUse the Claude Skills API to programmatically load and manage skills:\n\n```python\nimport anthropic\n\nclient = anthropic.Anthropic(api_key=\"your-api-key\")\n\nresponse = client.messages.create(\n    model=\"claude-3-5-sonnet-20241022\",\n    skills=[\"skill-id-here\"],\n    messages=[{\"role\": \"user\", \"content\": \"Your prompt\"}]\n)\n```\n\nSee the [Skills API documentation](https://docs.claude.com/en/api/skills-guide) for details.\n\n## Creating Skills\n\n### Skill Structure\n\nEach skill is a folder containing a `SKILL.md` file with YAML frontmatter:\n\n```\nskill-name/\n├── SKILL.md          # Required: Skill instructions and metadata\n├── scripts/          # Optional: Helper scripts\n├── templates/        # Optional: Document templates\n└── resources/        # Optional: Reference files\n```\n\n### Basic Skill Template\n\n```markdown\n---\nname: my-skill-name\ndescription: A clear description of what this skill does and when to use it.\n---\n\n# My Skill Name\n\nDetailed description of the skill's purpose and capabilities.\n\n## When to Use This Skill\n\n- Use case 1\n- Use case 2\n- Use case 3\n\n## Instructions\n\n[Detailed instructions for Claude on how to execute this skill]\n\n## Examples\n\n[Real-world examples showing the skill in action]\n```\n\n### Skill Best Practices\n\n- Focus on specific, repeatable tasks\n- Include clear examples and edge cases\n- Write instructions for Claude, not end users\n- Test across Claude.ai, Claude Code, and API\n- Document prerequisites and dependencies\n- Include error handling guidance\n\n## Contributing\n\nWe welcome contributions! Please read our [Contributing Guidelines](CONTRIBUTING.md) for details on:\n\n- How to submit new skills\n- Skill quality standards\n- Pull request process\n- Code of conduct\n\n### Quick Contribution Steps\n\n1. Ensure your skill is based on a real use case\n2. Check for duplicates in existing skills\n3. Follow the skill structure template\n4. Test your skill across platforms\n5. Submit a pull request with clear documentation\n\n## Resources\n\n### Official Documentation\n\n- [Claude Skills Overview](https://www.anthropic.com/news/skills) - Official announcement and features\n- [Skills User Guide](https://support.claude.com/en/articles/12512180-using-skills-in-claude) - How to use skills in Claude\n- [Creating Custom Skills](https://support.claude.com/en/articles/12512198-creating-custom-skills) - Skill development guide\n- [Skills API Documentation](https://docs.claude.com/en/api/skills-guide) - API integration guide\n- [Agent Skills Blog Post](https://anthropic.com/engineering/equipping-agents-for-the-real-world-with-agent-skills) - Engineering deep dive\n\n### Community Resources\n\n- [Anthropic Skills Repository](https://github.com/anthropics/skills) - Official example skills\n- [Claude Community](https://community.anthropic.com) - Discuss skills with other users\n- [Skills Marketplace](https://claude.ai/marketplace) - Discover and share skills\n\n### Inspiration & Use Cases\n\n- [Lenny's Newsletter](https://www.lennysnewsletter.com/p/everyone-should-be-using-claude-code) - 50 ways people use Claude Code\n- [Notion Skills](https://www.notion.so/notiondevs/Notion-Skills-for-Claude-28da4445d27180c7af1df7d8615723d0) - Notion integration skills\n\n\n## Join the Community\n\n- Have questions about integrating Composio with your auth setup? [Hop on a quick call with us](https://calendly.com/thomas-composio/composio-enterprise-setup)\n- [Follow us on Twitter](https://x.com/composio)\n- [Join our Discord](https://discord.com/invite/composio)\n\n## License\n\nThis repository is licensed under the Apache License 2.0.\n\nIndividual skills may have different licenses - please check each skill's folder for specific licensing information.\n\n---\n\n**Note**: Claude Skills work across Claude.ai, Claude Code, and the Claude API. Once you create a skill, it's portable across all platforms, making your workflows consistent everywhere you use Claude.\n\n- [AgentsKB](https://agentskb.com) - Upgrade your AI with researched answers. We did the research so your AI gets it right the first time.\n",
        "plugins/ios-simulator-skill/README.md": "# iOS Simulator Skill for Claude Code\n\nProduction-ready automation for iOS app testing and building. 21 scripts optimized for both human developers and AI agents.\n\nThis is basically a Skill version of my XCode MCP: [https://github.com/conorluddy/xc-mcp](https://github.com/conorluddy/xc-mcp)\n\nMCPs load a lot of tokens into the context window when they're active, but also seem to work really well. Skills don't load in any context. I'll make a plugin next and try to find the balance...\n\nUpdated: The Plugin version lets you easily disable MCPs for different tool groups. Optimise your context window by only enabling the tools you're actively using, such as xcodebuild: [https://github.com/conorluddy/xclaude-plugin](https://github.com/conorluddy/xclaude-plugin)\n\n## What It Does\n\nInstead of pixel-based navigation that breaks when UI changes:\n\n```bash\n# Fragile - breaks if UI changes\nidb ui tap 320 400\n\n# Robust - finds by meaning\npython scripts/navigator.py --find-text \"Login\" --tap\n```\n\nUses semantic navigation on accessibility APIs to interact with elements by their meaning, not coordinates. Works across different screen sizes and survives UI redesigns.\n\n## Features\n\n- **21 production scripts** for building, testing, and automation\n- **Semantic navigation** - find elements by text, type, or ID\n- **Token optimized** - 96% reduction vs raw tools (3-5 lines default)\n- **Zero configuration** - works immediately on macOS with Xcode\n- **Structured output** - JSON and formatted text, easy to parse\n- **Auto-UDID detection** - no need to specify device each time\n- **Batch operations** - boot, delete, erase multiple simulators at once\n- **Comprehensive testing** - WCAG compliance, visual diffs, accessibility audits\n- **CI/CD ready** - JSON output, exit codes, automated device lifecycle\n\n## Installation\n\n### As Claude Code Skill\n\n```bash\n# Personal installation\ngit clone https://github.com/conorluddy/ios-simulator-skill.git ~/.claude/skills/ios-simulator-skill\n\n# Project installation\ngit clone https://github.com/conorluddy/ios-simulator-skill.git .claude/skills/ios-simulator-skill\n```\n\nRestart Claude Code. The skill loads automatically.\n\n### From Release\n\n```bash\n# Download latest release\ncurl -L https://github.com/conorluddy/ios-simulator-skill/releases/download/vX.X.X/ios-simulator-skill-vX.X.X.zip -o skill.zip\n\n# Extract\nunzip skill.zip -d ~/.claude/skills/ios-simulator-skill\n```\n\n## Prerequisites\n\n- macOS 12+\n- Xcode Command Line Tools (`xcode-select --install`)\n- Python 3\n- IDB (optional, for interactive features: `brew tap facebook/fb && brew install idb-companion`)\n\n## Quick Start\n\n```bash\n# 1. Check environment\nbash ~/.claude/skills/ios-simulator-skill/scripts/sim_health_check.sh\n\n# 2. Launch your app\npython ~/.claude/skills/ios-simulator-skill/scripts/app_launcher.py --launch com.example.app\n\n# 3. See what's on screen\npython ~/.claude/skills/ios-simulator-skill/scripts/screen_mapper.py\n# Output:\n# Screen: LoginViewController (45 elements, 7 interactive)\n# Buttons: \"Login\", \"Cancel\", \"Forgot Password\"\n# TextFields: 2 (0 filled)\n\n# 4. Tap login button\npython ~/.claude/skills/ios-simulator-skill/scripts/navigator.py --find-text \"Login\" --tap\n\n# 5. Enter text\npython ~/.claude/skills/ios-simulator-skill/scripts/navigator.py --find-type TextField --enter-text \"user@test.com\"\n\n# 6. Check accessibility\npython ~/.claude/skills/ios-simulator-skill/scripts/accessibility_audit.py\n```\n\n## 21 Scripts Organized by Category\n\n### Build & Development\n- **build_and_test.py** - Build projects, run tests, parse results\n- **log_monitor.py** - Real-time log monitoring\n\n### Navigation & Interaction\n- **screen_mapper.py** - Analyze current screen\n- **navigator.py** - Find and interact with elements\n- **gesture.py** - Swipes, scrolls, pinches\n- **keyboard.py** - Text input and hardware buttons\n- **app_launcher.py** - App lifecycle control\n\n### Testing & Analysis\n- **accessibility_audit.py** - WCAG compliance checking\n- **visual_diff.py** - Screenshot comparison\n- **test_recorder.py** - Automated test documentation\n- **app_state_capture.py** - Debugging snapshots\n- **sim_health_check.sh** - Environment verification\n\n### Advanced Testing & Permissions\n- **clipboard.py** - Clipboard management\n- **status_bar.py** - Status bar control\n- **push_notification.py** - Push notifications\n- **privacy_manager.py** - Permission management\n\n### Device Lifecycle\n- **simctl_boot.py** - Boot simulator\n- **simctl_shutdown.py** - Shutdown simulator\n- **simctl_create.py** - Create simulator\n- **simctl_delete.py** - Delete simulator\n- **simctl_erase.py** - Factory reset\n\nSee **SKILL.md** for complete reference.\n\n## How It Works with Claude Code\n\nClaude Code automatically detects when to use this skill based on your request. You don't need to manually invoke it.\n\n**Example conversation:**\n\n```\nYou: \"Set up my iOS app for testing\"\nClaude: [Uses simctl_boot.py and app_launcher.py automatically]\n\nYou: \"Tap the login button\"\nClaude: [Uses navigator.py to find and tap]\n\nYou: \"Check if the form is accessible\"\nClaude: [Uses accessibility_audit.py]\n```\n\nYou can also run scripts manually when needed.\n\n## Usage Examples\n\n### Example 1: Login Flow\n\n```bash\n# Launch app\npython scripts/app_launcher.py --launch com.example.app\n\n# Map screen to find fields\npython scripts/screen_mapper.py\n\n# Enter credentials\npython scripts/navigator.py --find-type TextField --index 0 --enter-text \"user@test.com\"\npython scripts/navigator.py --find-type SecureTextField --enter-text \"password\"\n\n# Tap login\npython scripts/navigator.py --find-text \"Login\" --tap\n\n# Verify accessibility\npython scripts/accessibility_audit.py\n```\n\n### Example 2: Test Documentation\n\n```bash\n# Record test execution\npython scripts/test_recorder.py --test-name \"Login Flow\" --output test-reports/\n\n# Generates:\n# - Screenshots per step\n# - Accessibility trees\n# - Markdown report with timing\n```\n\n### Example 3: Visual Testing\n\n```bash\n# Capture baseline\npython scripts/app_state_capture.py --output baseline/\n\n# Make changes...\n\n# Compare\npython scripts/visual_diff.py baseline/screenshot.png current/screenshot.png\n```\n\n### Example 4: Permission Testing\n\n```bash\n# Grant permissions\npython scripts/privacy_manager.py --bundle-id com.example.app --grant camera,location\n\n# Test app behavior with permissions...\n\n# Revoke permissions\npython scripts/privacy_manager.py --bundle-id com.example.app --revoke camera,location\n```\n\n### Example 5: Device Lifecycle in CI/CD\n\n```bash\n# Create test device\nDEVICE_ID=$(python scripts/simctl_create.py --device \"iPhone 16 Pro\" --json | jq -r '.new_udid')\n\n# Run tests\npython scripts/build_and_test.py --project MyApp.xcodeproj\n\n# Clean up\npython scripts/simctl_delete.py --udid $DEVICE_ID --yes\n```\n\n## Design Principles\n\n**Semantic Navigation**: Find elements by meaning (text, type, ID) not pixel coordinates. Survives UI changes and works across device sizes.\n\n**Token Efficiency**: Default output is 3-5 lines. Use `--verbose` for details or `--json` for machine parsing. 96% reduction vs raw tools.\n\n**Accessibility-First**: Built on iOS accessibility APIs for reliability. Better for users with accessibility needs and more robust for automation.\n\n**Zero Configuration**: Works immediately on any macOS with Xcode. No complex setup, no configuration files.\n\n**Structured Data**: Scripts output JSON or formatted text, not raw logs. Easy to parse, integrate, and understand.\n\n**Auto-Learning**: Build system learns your device preference and remembers it for next time.\n\n## Requirements\n\n**System:**\n- macOS 12 or later\n- Xcode Command Line Tools\n- Python 3\n\n**Optional:**\n- IDB (for interactive features)\n- Pillow (for visual_diff.py: `pip3 install pillow`)\n\n## Documentation\n\n- **SKILL.md** - Complete script reference and table of contents\n- **CLAUDE.md** - Architecture and developer guide\n- **references/** - Deep documentation on specific topics\n- **examples/** - Complete automation workflows\n\n## Output Efficiency\n\nAll scripts minimize output by default:\n\n| Task | Raw Tools | This Skill | Savings |\n|------|-----------|-----------|---------|\n| Screen analysis | 200+ lines | 5 lines | 97.5% |\n| Find & tap button | 100+ lines | 1 line | 99% |\n| Enter text | 50+ lines | 1 line | 98% |\n| Login flow | 400+ lines | 15 lines | 96% |\n\nThis efficiency keeps AI agent conversations focused and cost-effective.\n\n## Troubleshooting\n\n### Environment Issues\n\n```bash\n# Run health check\nbash ~/.claude/skills/ios-simulator-skill/scripts/sim_health_check.sh\n\n# Checks: macOS, Xcode, simctl, IDB, Python, simulators, packages\n```\n\n### Script Help\n\n```bash\n# All scripts support --help\npython scripts/navigator.py --help\npython scripts/accessibility_audit.py --help\n```\n\n### Not Finding Elements\n\n```bash\n# Use verbose mode to see all elements\npython scripts/screen_mapper.py --verbose\n\n# Check for exact text match\npython scripts/navigator.py --find-text \"Exact Button Text\" --tap\n```\n\n## Contributing\n\nContributions should:\n- Maintain token efficiency (minimal default output)\n- Follow accessibility-first design\n- Support `--help` documentation\n- Support `--json` for CI/CD\n- Pass Black formatter and Ruff linter\n- Include type hints\n- Update SKILL.md\n\n## License\n\nMIT License - Allows commercial use and distribution.\n\n## Support\n\n- **Issues**: Create GitHub issue with reproduction steps\n- **Documentation**: See SKILL.md and references/\n- **Examples**: Check examples/ directory\n- **Skills Docs**: https://docs.claude.com/en/docs/claude-code/skills\n\n---\n\n**Built for AI agents. Optimized for developers.**\n",
        "plugins/ralph-wiggum-marketer/README.md": "# Ralph Wiggum Marketer\n\nA **Claude Code Plugin** that provides an autonomous AI copywriter for SaaS content marketing.\n\nUses the [Ralph Wiggum pattern](https://ghuntley.com/ralph/) - an iterative AI loop that ships content while you sleep.\n\n## Installation\n\n### Option 1: Add as Marketplace (Recommended)\n\n```bash\n# In Claude Code, add the repo as a marketplace:\n/plugin marketplace add muratcankoylan/ralph-wiggum-marketer\n\n# Then install the plugin:\n/plugin install ralph-wiggum-marketer@muratcankoylan-ralph-wiggum-marketer\n```\n\n### Option 2: Test Locally (For Development)\n\n```bash\n# Clone the repo\ngit clone https://github.com/muratcankoylan/ralph-wiggum-marketer.git\n\n# Run Claude Code with the plugin directory\nclaude --plugin-dir ./ralph-wiggum-marketer\n```\n\n### Option 3: Interactive Plugin Manager\n\n```bash\n# Open the plugin manager:\n/plugin\n\n# Browse, search, and install from the interactive UI\n```\n\n## Quick Start\n\n```bash\n# 1. Initialize a new content project\n/ralph-init\n\n# 2. Check progress anytime\n/ralph-status\n\n# 3. Cancel if needed\n/ralph-cancel\n```\n\n## Commands\n\n| Command | Description |\n|---------|-------------|\n| `/ralph-init` | Initialize a new content project in current directory |\n| `/ralph-marketer` | Start the autonomous copywriter loop |\n| `/ralph-status` | Check content pipeline and progress |\n| `/ralph-cancel` | Cancel the active loop |\n\n## How It Works\n\n```\n┌──────────────────────────────────────────────────────────────────┐\n│                     MULTI-AGENT ECOSYSTEM                        │\n├──────────────────────────────────────────────────────────────────┤\n│                                                                  │\n│  ┌─────────────┐   ┌─────────────┐   ┌─────────────┐             │\n│  │ TrendScout  │   │  Research   │   │  Product/   │             │\n│  │   Agent     │   │   Agent     │   │  Marketing  │             │\n│  └──────┬──────┘   └──────┬──────┘   └──────┬──────┘             │\n│         │                 │                 │                    │\n│         ▼                 ▼                 ▼                    │\n│  ┌────────────────────────────────────────────────────┐          │\n│  │              SQLite Content Database               │          │\n│  │  • trends     • research     • communications      │          │\n│  └────────────────────────┬───────────────────────────┘          │\n│                           │                                      │\n│                           ▼                                      │\n│  ┌────────────────────────────────────────────────────┐          │\n│  │           RALPH THE COPYWRITER                     │          │\n│  │                                                     │         │\n│  │   Reads inputs → Plans content → Writes drafts     │          │\n│  │   → Reviews & iterates → Publishes                 │          │\n│  │                                                     │         │\n│  │   Memory: git commits + progress.txt + prd.json    │          │\n│  └────────────────────────────────────────────────────┘          │\n│                           │                                      │\n│                           ▼                                      │\n│                    Published Content                             │\n│              (blogs, case studies, social, newsletters)          │\n└──────────────────────────────────────────────────────────────────┘\n```\n\n### The Ralph Loop\n\n1. **Read PRD**: Check `scripts/ralph/prd.json` for tasks\n2. **Check Progress**: Read `scripts/ralph/progress.txt` for learnings\n3. **Pick Task**: Find highest priority story where `passes: false`\n4. **Execute**: Complete the task following acceptance criteria\n5. **Verify**: Run tests to ensure quality\n6. **Commit**: Save progress to git\n7. **Update**: Mark task done, log learnings\n8. **Repeat**: Loop until all tasks complete\n\nEach iteration is a fresh context window. Memory persists through files.\n\n## Plugin Structure\n\n```\nralph-wiggum-marketer/\n├── .claude-plugin/\n│   └── plugin.json          # Plugin manifest\n├── commands/\n│   ├── ralph-marketer.md    # Main loop command\n│   ├── ralph-init.md        # Project initialization\n│   ├── ralph-status.md      # Status check\n│   └── ralph-cancel.md      # Cancel loop\n├── skills/\n│   └── copywriter/\n│       └── SKILL.md         # Copywriter skill\n├── hooks/\n│   ├── hooks.json           # Hook configuration\n│   └── stop-hook.sh         # Loop continuation hook\n├── scripts/\n│   └── src/                 # Database & utility scripts\n├── templates/\n│   ├── prd.json             # Task template\n│   ├── progress.txt         # Progress log template\n│   ├── prompt.md            # Agent instructions template\n│   └── package.json         # Project package.json template\n└── README.md\n```\n\n## Database Schema\n\n### Input Tables (from other agents)\n\n```sql\n-- Trends from TrendScout\ntrends (topic, description, source, relevance_score, status)\n\n-- Research from Research Agent\nresearch (title, summary, key_findings, data_points, category, status)\n\n-- Communications from Product/Marketing\ncommunications (type, title, details, key_messages, target_audience, priority, status)\n```\n\n### Ralph's Workspace\n\n```sql\n-- Content planning\ncontent_plan (content_type, title, brief, target_keywords, status)\n\n-- Work in progress\ndrafts (plan_id, version, content, word_count, feedback)\n\n-- Final content\npublished (plan_id, final_content, meta_description)\n\n-- Activity tracking\nagent_log (action, details, created_at)\n```\n\n## Customizing\n\n### Add Your Own Content Sources\n\nEdit `src/db/seed.js`:\n\n```javascript\n// Add a trend\ninsertTrend.run(\n  'Your Trend Topic',\n  'Description of the trend',\n  'Source',\n  85  // relevance score\n);\n\n// Add a communication\ninsertComm.run(\n  'product_update',\n  'Your Product Launch',\n  'Details about what it does',\n  JSON.stringify(['Key message 1', 'Key message 2']),\n  'Target audience',\n  1  // priority\n);\n```\n\n### Add Your Own Tasks\n\nEdit `scripts/ralph/prd.json`:\n\n```json\n{\n  \"id\": \"WRITE-004\",\n  \"title\": \"Write your custom blog\",\n  \"acceptanceCriteria\": [\n    \"At least 1000 words\",\n    \"Includes 3 data points\",\n    \"Has compelling CTA\"\n  ],\n  \"priority\": 5,\n  \"passes\": false\n}\n```\n\n## Sample Tasks\n\nThe default PRD includes 12 stories:\n\n1. **SETUP-001**: Initialize database\n2. **PLAN-001**: Plan product launch blog\n3. **WRITE-001**: Write launch blog draft\n4. **PLAN-002**: Plan thought leadership blog\n5. **WRITE-002**: Write data-driven blog\n6. **REVIEW-001**: Review and improve draft\n7. **PUBLISH-001**: Publish launch blog\n8. **PLAN-003**: Plan case study\n9. **WRITE-003**: Write case study\n10. **SOCIAL-001**: Create social posts\n11. **NEWSLETTER-001**: Draft newsletter\n12. **METRICS-001**: Log final metrics\n\n## The Ralph Philosophy\n\n> \"Ralph is a Bash loop. Memory persists only through git history and text files. Each iteration is a fresh context window.\"\n\nKey principles:\n- **Small stories** - Must complete in one iteration\n- **Explicit criteria** - No ambiguity\n- **Fast feedback** - Tests every iteration\n- **Compound learnings** - Patterns accumulate\n- **Persistence wins** - Keep iterating\n\n## Credits\n\n- Original Ralph concept: [@GeoffreyHuntley](https://ghuntley.com/ralph/)\n- Official Ralph plugin: [claude-plugins-official](https://github.com/anthropics/claude-plugins-official/tree/main/plugins/ralph-loop)\n- Video walkthrough: [@mattpocockuk](https://twitter.com/mattpocockuk)\n\n## License\n\nMIT\n",
        "plugins/ai-investigator/README.md": "# AI Enterprise Case Study Analyzer\n\nAn intelligent system for analyzing enterprise AI case studies using the Claude 3.5 Sonnet API. The system supports two main modes of operation:\n1. Analyzing case studies from provided URLs in a CSV file.\n2. Discovering and analyzing case studies from company websites using the Firecrawl API.\n\n<img width=\"946\" alt=\"Screenshot 2024-11-05 at 4 58 41 AM\" src=\"https://github.com/user-attachments/assets/95be2e76-12bd-4dea-bd91-1b7d309f0f6d\">\n<img width=\"1153\" alt=\"Screenshot 2024-11-05 at 4 58 49 AM\" src=\"https://github.com/user-attachments/assets/7b935a1b-b79e-4fb3-85c7-cb18d48601bb\">\n<img width=\"1153\" alt=\"Screenshot 2024-11-05 at 4 58 49 AM\" src=\"https://github.com/user-attachments/assets/1669df59-a81a-4aab-b62b-149e1480a82a\">\n<img width=\"1141\" alt=\"Screenshot 2024-11-05 at 5 03 37 AM\" src=\"https://github.com/user-attachments/assets/370e2f63-fd1c-4af4-ae78-130b99fe4b0b\">\n\n\n\n## Core Features\n\n### 1. Case Study Discovery & Analysis\n- **CSV Mode**: Analyze specific case study URLs provided in a CSV file.\n- **Website Mode**: Automatically discover and analyze case studies from company websites using Firecrawl's map endpoint.\n- Intelligent case study identification powered by Claude 3.5 Sonnet.\n- Content extraction handled by Firecrawl's scrape endpoint.\n\n### 2. Content Processing Pipeline\n- **Content Extraction** (via Firecrawl API):\n  - **Map endpoint** (`/v1/map`): Discovers links on the website.\n  - **Scrape endpoint** (`/v1/scrape`): Extracts content in markdown format and retrieves metadata for context.\n- **Case Study Identification**:\n  - Uses Claude to identify potential case study links.\n  - Filters content to ensure only relevant case studies are processed.\n- **Content Analysis**:\n  - Checks for enterprise AI qualification.\n  - Performs a detailed, multi-section analysis.\n  - Assesses business impact and technology stack.\n\n### 3. Report Generation\nThe system creates three types of reports:\n\n#### a. Individual Case Study Reports (`reports/individual/`)\n- Executive Summary\n- AI Strategy Analysis\n- Technical Implementation Details\n- Business Impact Assessment\n- Key Success Factors\n- Lessons Learned\n\n#### b. Cross-Case Analysis (`reports/cross_case_analysis/`)\n- Patterns across multiple implementations.\n- Common success factors.\n- Technology trends.\n- ROI metrics and implementation challenges.\n\n#### c. Executive Dashboard (`reports/executive_dashboard/`)\n- Company profiles\n- Technology stacks\n- Success metrics and implementation scales\n- Overall trends in enterprise AI adoption\n\n## Technical Architecture\n\n### 1. Firecrawl Integration\n- **Map Endpoint** (`/v1/map`):\n  ```python\n  map_result = app.map_url(website_url, params={'includeSubdomains': True})\n  ```\n  Used for discovering all links on a website.\n\n- **Scrape Endpoint** (`/v1/scrape`):\n  ```python\n  params = {\n      \"url\": url,\n      \"onlyMainContent\": True,\n      \"formats\": [\"markdown\"],\n      \"timeout\": 30000\n  }\n  ```\n  Used for content extraction from specific pages.\n\n### 2. Claude 3.5 Sonnet Integration\n- **Link Analysis**: Identifies relevant case study URLs.\n- **Content Analysis**: Checks for enterprise AI relevance.\n- **Report Generation**: Produces comprehensive, structured analysis reports.\n\n### 3. Data Processing Workflow\nInput (CSV/Website) → Firecrawl Map → Link Analysis → Content Extraction → Claude Analysis → Report Generation\n\n## Project Structure\n```\nproject/\n├── src/\n│   ├── scrapers/\n│   │   ├── website_crawler.py  # Firecrawl map integration\n│   │   └── web_loader.py       # Firecrawl scrape integration\n│   ├── processors/\n│   │   └── claude_processor.py # Claude API integration\n│   ├── config.py               # Configuration settings\n│   └── main.py                 # Main application logic\n├── input/                      # Input CSV files\n├── raw_content/                # Extracted raw content\n│   └── case_[id]/\n│       ├── raw_content.txt\n│       ├── structured_content.json\n│       └── metadata.json\n├── reports/\n│   ├── individual/             # Individual reports\n│   ├── cross_case_analysis/    # Cross-case analysis\n│   └── executive_dashboard/    # Executive dashboard\n└── logs/                       # Processing logs\n```\n\n## Installation & Setup\n\n1. **Clone the repository**:\n   ```bash\n   git clone https://github.com/yourusername/ai-case-study-analyzer.git\n   cd ai-case-study-analyzer\n   ```\n\n2. **Create a virtual environment**:\n   ```bash\n   python -m venv .venv\n   source .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n   ```\n\n3. **Install dependencies**:\n   ```bash\n   pip install -r requirements.txt\n   ```\n\n4. **Set up environment variables in `.env`**:\n   ```\n   ANTHROPIC_API_KEY=your_claude_api_key\n   FIRECRAWL_API_KEY=your_firecrawl_api_key\n   ```\n\n## Usage\n\n### 1. CSV Analysis Mode\n- Place your CSV file in the `input/` directory with a column named `url` containing case study URLs.\n\n### 2. Website Analysis Mode\n- Provide a company website URL to:\n  1. Map all website links using Firecrawl.\n  2. Identify and analyze case study content using Claude.\n  3. Extract content and generate comprehensive reports.\n\n**Run the analyzer**:\n```bash\npython -m src.main\n```\n\n## API Integration Details\n\n### Firecrawl API\n1. **Map Endpoint**:\n   - Discovers all links on a website.\n   - Parameters: `includeSubdomains: true`, `ignoreSitemap: false`, `limit: 5000`.\n\n2. **Scrape Endpoint**:\n   - Extracts main content from individual pages.\n   - Parameters: `onlyMainContent: true`, `formats: [\"markdown\"]`, `timeout: 30000`.\n\n### Claude 3.5 Sonnet API\n1. **Link Analysis**:\n   - Model: `claude-3-5-sonnet-20241022`.\n   - Temperature: `0.2`.\n   - Max tokens: `4096`.\n\n2. **Content Analysis**:\n   - Checks for enterprise AI qualification.\n   - Performs multi-section analysis and report generation.\n\n## Output Examples\n\n### Individual Case Study Report\n```markdown\n# Enterprise AI Implementation Report: [Company Name]\n1. **Executive Summary**\n   [Summary of implementation and outcomes]\n\n2. **AI Strategy Analysis**\n   [Detailed analysis of AI strategy]\n```\n\n### Cross-Case Analysis\n```json\n{\n  \"case_1\": {\n    \"company\": {...},\n    \"technologies\": [...],\n    \"success_factors\": {...},\n    \"business_impact\": {...}\n  }\n}\n```\n\n## Star History\n\n<a href=\"https://star-history.com/#muratcankoylan/AI-Investigator&Date\">\n <picture>\n   <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://api.star-history.com/svg?repos=muratcankoylan/AI-Investigator&type=Date&theme=dark\" />\n   <source media=\"(prefers-color-scheme: light)\" srcset=\"https://api.star-history.com/svg?repos=muratcankoylan/AI-Investigator&type=Date\" />\n   <img alt=\"Star History Chart\" src=\"https://api.star-history.com/svg?repos=muratcankoylan/AI-Investigator&type=Date\" />\n </picture>\n</a>\n\n## Contributing\nContributions are welcome!\n\n## License\nThis project is licensed under the MIT License.\n",
        "plugins/rosetta-prompt/README.md": "# The Rosetta Prompt\n\nA prompt optimization system that adapts your prompts for different AI providers. \n\n[![LangChain v1](https://img.shields.io/badge/LangChain-v1.0-blue)](https://docs.langchain.com/oss/python/releases/langchain-v1)\n[![Python 3.11+](https://img.shields.io/badge/Python-3.11+-green)](https://python.org)\n\n## What Makes This Agentic?\n\nThis is **not** a simple prompt-in/prompt-out system. Each **Optimizer Agent** is a true autonomous agent that:\n\n1. **Discovers knowledge** - Uses `list_provider_docs` tool to find available documentation\n2. **Reads selectively** - Uses `read_provider_doc` tool to retrieve specific guidelines (12K+ chars each)\n3. **Applies learning** - Transforms prompts based on provider-specific patterns it learned\n4. **Reports changes** - Uses `submit_optimization` to return structured results with detailed changelog\n\nThe agent makes **autonomous decisions** in a ReAct loop (Reason → Act → Observe → Repeat).\n\n## Architecture\n\n```\n┌─────────────────────────────────────────────────────────────────────────────────────────┐\n│                                    AGENTIC SYSTEM                                       │\n├─────────────────────────────────────────────────────────────────────────────────────────┤\n│                                                                                         │\n│   ┌───────────────────────────────────────────────────────────────────────────────┐     │\n│   │                           ORCHESTRATOR AGENT                                  │     │\n│   │                                                                               │     │\n│   │   • Validates providers against docs/ directory                               │     |\n│   │   • Spawns parallel optimizer agents (asyncio.gather)                         │     │\n│   │   • Aggregates results from all agents                                        │     │\n│   └───────────────────────────────────────────────────────────────────────────────┘     │\n│                                       │                                                 │\n│               ┌───────────────────────┼───────────────────────┐                         │\n│               │                       │                       │                         │\n│               ▼                       ▼                       ▼                         │\n│   ┌─────────────────────┐ ┌─────────────────────┐ ┌─────────────────────┐               │\n│   │   OPTIMIZER AGENT   │ │   OPTIMIZER AGENT   │ │   OPTIMIZER AGENT   │               │\n│   │      (OpenAI)       │ │    (Anthropic)      │ │     (Google)        │               │\n│   │                     │ │                     │ │                     │               │\n│   │  ┌───────────────┐  │ │  ┌───────────────┐  │ │  ┌───────────────┐  │               │ \n│   │  │ ReAct Loop    │  │ │  │ ReAct Loop    │  │ │  │ ReAct Loop    │  │               │\n│   │  │               │  │ │  │               │  │ │  │               │  │               │\n│   │  │ 1. Reason     │  │ │  │ 1. Reason     │  │ │  │ 1. Reason     │  │               │\n│   │  │ 2. Act (Tool) │  │ │  │ 2. Act (Tool) │  │ │  │ 2. Act (Tool) │  │               │\n│   │  │ 3. Observe    │  │ │  │ 3. Observe    │  │ │  │ 3. Observe    │  │               │\n│   │  │ 4. Repeat     │  │ │  │ 4. Repeat     │  │ │  │ 4. Repeat     │  │               │\n│   │  └───────┬───────┘  │ │  └───────┬───────┘  │ │  └───────┬───────┘  │               │\n│   │          │          │ │          │          │ │          │          │               │\n│   │     [FileLogger]    │ │     [FileLogger]    │ │     [FileLogger]    │               │\n│   └──────────┼──────────┘ └──────────┼──────────┘ └──────────┼──────────┘               │\n│              │                       │                       │                          │\n│              └───────────────────────┴───────────────────────┘                          │\n│                                      │                                                  │\n│                                      ▼                                                  │\n│   ┌─────────────────────────────────────────────────────────────────────────────┐       │\n│   │                              TOOL LAYER                                     │       │\n│   │                                                                             │       │\n│   │   list_provider_docs(provider) → [\"index.md\", \"prompting.md\"]               │       │\n│   │   read_provider_doc(provider, doc_name) → \"12K chars of guidelines...\"      │       │ \n│   │   submit_optimization(prompt, changes) → Final structured result            │       │\n│   │                                                                             │       │\n│   └─────────────────────────────────────┬───────────────────────────────────────┘       │\n│                                         │                                               │\n│                                         ▼                                               │\n│   ┌─────────────────────────────────────────────────────────────────────────────┐       │\n│   │                           KNOWLEDGE BASE (docs/)                            │       │\n│   │                                                                             │       │\n│   │   ├── openai/prompting.md      (Official prompting guide)                   │       │\n│   │   ├── anthropic/prompting.md   (Be clear, direct, detailed)                 │       │\n│   │   ├── google/prompting.md      (Prompt design strategies)                   │       │\n│   │   └── kimi/prompting.md        (Kimi-specific guidelines)                   │       │\n│   │                                                                             │       │\n│   │   → Auto-detected on startup (add folder = new provider)                    │       │\n│   └─────────────────────────────────────────────────────────────────────────────┘       │\n│                                                                                         │\n└─────────────────────────────────────────────────────────────────────────────────────────┘\n```\n\n## Agent System Deep Dive\n\n### The ReAct Agent Loop (`agents/optimizer.py`)\n\nEach optimizer runs an autonomous **ReAct loop** (Reasoning + Acting):\n\n```python\nclass OptimizerAgent:\n    \"\"\"\n    Agentic optimizer using ReAct pattern.\n    \n    The agent autonomously decides which documents to read,\n    rather than having context pre-loaded (prevents context rot).\n    \"\"\"\n    \n    def __init__(self):\n        self.llm = ChatOpenAI(\n            model=PRIMARY_MODEL,\n            api_key=OPENROUTER_API_KEY,\n            base_url=OPENROUTER_BASE_URL,\n            max_tokens=16384,  # Prevent output truncation\n        )\n    \n    async def _run_agent_loop(self, task, provider, original, log, file_log):\n        \"\"\"\n        The core ReAct loop:\n        1. Send messages to LLM\n        2. Parse tool call from response\n        3. Execute tool, get result\n        4. Add result to conversation\n        5. Repeat until submission\n        \"\"\"\n        messages = [\n            SystemMessage(content=AGENT_SYSTEM_PROMPT),\n            HumanMessage(content=task),\n        ]\n        \n        for iteration in range(max_iterations):\n            # REASON: LLM decides what to do\n            response = await self.llm.ainvoke(messages)\n            \n            # Check for final submission\n            if \"submit_optimization\" in response.content:\n                return self._parse_final_submission(response.content, ...)\n            \n            # ACT: Parse and execute tool\n            tool_call = self._parse_tool_call(response.content)\n            if tool_call:\n                name, args = tool_call\n                result = self._execute_tool(name, args)  # Tool execution\n                \n                # OBSERVE: Add result to conversation\n                messages.append(AIMessage(content=response.content))\n                messages.append(HumanMessage(content=f\"TOOL RESULT:\\n{result}\"))\n```\n\n### Tool Definitions\n\nThe agent uses a simple text-based tool calling format:\n\n```python\n# Tool format the agent uses:\n# TOOL: list_provider_docs | ARGS: provider=openai\n# TOOL: read_provider_doc | ARGS: provider=openai, doc_name=prompting.md\n# TOOL: submit_optimization | ARGS: done\n\ndef _list_provider_docs(provider: str) -> str:\n    \"\"\"List available docs for a provider.\"\"\"\n    provider_path = Path(DOCS_BASE_PATH) / provider.lower()\n    files = [f.name for f in provider_path.iterdir() if f.suffix == \".md\"]\n    return f\"Available docs for {provider.upper()}: {', '.join(files)}\"\n\ndef _read_provider_doc(provider: str, doc_name: str) -> str:\n    \"\"\"Read specific documentation file (returns full content ~12K chars).\"\"\"\n    doc_path = Path(DOCS_BASE_PATH) / provider.lower() / doc_name\n    content = doc_path.read_text()\n    return f\"=== {provider.upper()}: {doc_name} ===\\n\\n{content}\"\n```\n\n### Agent Execution Flow (Real Example)\n\n```\nUser: \"Optimize 'You are a helpful assistant' for Anthropic\"\n                    │\n                    ▼\n┌─────────────────────────────────────────────────────────────────┐\n│ ITERATION 1 (0ms)                                               │\n│                                                                 │\n│ LLM Response: \"TOOL: list_provider_docs | ARGS: provider=anthropic\"\n│                                                                 │\n│ Tool Result: \"Available docs for ANTHROPIC: index.md, prompting.md\"\n└─────────────────────────────────────────────────────────────────┘\n                    │\n                    ▼\n┌─────────────────────────────────────────────────────────────────┐\n│ ITERATION 2 (2.5s)                                              │\n│                                                                 │\n│ LLM Response: \"TOOL: read_provider_doc | ARGS: provider=anthropic, doc_name=prompting.md\"\n│                                                                 │\n│ Tool Result: \"=== ANTHROPIC: prompting.md ===                   │\n│                                                                 │\n│ Prompt engineering                                              │\n│ Be clear, direct, and detailed                                  │\n│                                                                 │\n│ When interacting with Claude, think of it as a brilliant but    │\n│ very new employee (with amnesia) who needs explicit instructions│\n│ ...\" (12,082 characters)                                        │\n└─────────────────────────────────────────────────────────────────┘\n                    │\n                    ▼\n┌─────────────────────────────────────────────────────────────────┐\n│ ITERATION 3 (8.8s)                                              │\n│                                                                 │\n│ LLM Response: \"TOOL: read_provider_doc | ARGS: provider=anthropic, doc_name=index.md\"\n│                                                                 │\n│ Tool Result: (416 characters of index content)                  │\n└─────────────────────────────────────────────────────────────────┘\n                    │\n                    ▼\n┌─────────────────────────────────────────────────────────────────┐\n│ ITERATION 4 (11.8s) - FINAL SUBMISSION                          │\n│                                                                 │\n│ LLM Response:                                                   │\n│ \"Based on my review of Anthropic's prompting guidelines...      │\n│                                                                 │\n│ TOOL: submit_optimization | ARGS: done                          │\n│                                                                 │\n│ OPTIMIZED_PROMPT:                                               │\n│ ```                                                             │\n│ You are a helpful assistant designed to provide clear,          │\n│ accurate, and thoughtful responses to user questions...         │\n│ ```                                                             │\n│                                                                 │\n│ CHANGES:                                                        │\n│ 1. [clarity] - Added explicit description following the         │\n│    guideline to be specific about what you want Claude to do\"   │\n└─────────────────────────────────────────────────────────────────┘\n                    │\n                    ▼\n              OPTIMIZATION COMPLETE (26s total, 4 iterations)\n```\n\n## Comprehensive Logging\n\nEvery agent execution is logged in two ways:\n\n### 1. API Response Logs (`agent_logs`)\n\nEach optimization result includes detailed logs in the JSON response:\n\n```json\n{\n  \"agent_logs\": [\n    {\"timestamp\": \"...\", \"elapsed_ms\": 0, \"type\": \"system\", \"content\": \"Starting optimization for ANTHROPIC\"},\n    {\"timestamp\": \"...\", \"elapsed_ms\": 2475, \"type\": \"tool_call\", \"content\": \"Calling tool: list_provider_docs\", \"metadata\": {\"args\": {\"provider\": \"anthropic\"}}},\n    {\"timestamp\": \"...\", \"elapsed_ms\": 2476, \"type\": \"tool_result\", \"content\": \"Available docs for ANTHROPIC: index.md, prompting.md\"},\n    {\"timestamp\": \"...\", \"elapsed_ms\": 8790, \"type\": \"tool_call\", \"content\": \"Calling tool: read_provider_doc\", \"metadata\": {\"args\": {\"provider\": \"anthropic\", \"doc_name\": \"prompting.md\"}}},\n    {\"timestamp\": \"...\", \"elapsed_ms\": 8792, \"type\": \"tool_result\", \"content\": \"=== ANTHROPIC: prompting.md ===...\", \"metadata\": {\"result_length\": 12082}},\n    {\"timestamp\": \"...\", \"elapsed_ms\": 26142, \"type\": \"submit\", \"content\": \"Agent submitting final result\"}\n  ]\n}\n```\n\n### 2. Local File Logs (`logs/`)\n\nFull execution traces are saved to `rosetta_prompt/logs/`:\n\n```bash\n$ ls rosetta_prompt/logs/\n20251205_055106_859143_anthropic.log  # 37KB\n20251205_055106_861382_google.log     # 37KB\n\n$ cat rosetta_prompt/logs/20251205_055106_859143_anthropic.log\n================================================================================\nROSETTA PROMPT - AGENT EXECUTION LOG\n================================================================================\nProvider: ANTHROPIC\nStarted: 2025-12-05T05:51:06.859193\n================================================================================\n\n------------------------------------------------------------\n[2025-12-05T05:51:06.859383] [0.000s] SYSTEM\n------------------------------------------------------------\nStarting optimization for ANTHROPIC\nModel: anthropic/claude-opus-4.5\nOriginal prompt length: 28 chars\n\n------------------------------------------------------------\n[2025-12-05T05:51:06.859441] [0.000s] TASK_INPUT\n------------------------------------------------------------\n## TASK: Optimize for ANTHROPIC\n...\n\n------------------------------------------------------------\n[2025-12-05T05:51:15.651451] [8.792s] TOOL_RESULT\n------------------------------------------------------------\nTool: read_provider_doc\nResult (12082 chars):\n=== ANTHROPIC: prompting.md ===\n\nPrompt engineering\nBe clear, direct, and detailed\n...\n```\n\nLog files contain:\n- **Full system prompt** sent to LLM\n- **Complete LLM responses** (not truncated)\n- **Full tool results** (12K+ chars of documentation)\n- **Timing data** for each step\n- **Final parsed output**\n\n## API Usage\n\n### Optimize Endpoint\n\n```bash\ncurl -X POST http://localhost:8000/optimize \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"prompt\": \"You are a helpful assistant.\",\n    \"providers\": [\"openai\", \"anthropic\", \"google\"]\n  }'\n```\n\nResponse:\n\n```json\n{\n  \"original\": \"You are a helpful assistant.\",\n  \"optimized\": {\n    \"openai\": {\n      \"provider\": \"openai\",\n      \"prompt\": \"# Identity\\nYou are an AI assistant designed to help...\",\n      \"changes\": [\n        {\"category\": \"structure\", \"description\": \"Added markdown sections...\"},\n        {\"category\": \"formatting\", \"description\": \"Included examples...\"}\n      ],\n      \"success\": true,\n      \"agent_logs\": [...]\n    },\n    \"anthropic\": {\n      \"provider\": \"anthropic\", \n      \"prompt\": \"You are a helpful assistant designed to provide clear...\",\n      \"changes\": [...],\n      \"success\": true,\n      \"agent_logs\": [...]\n    }\n  }\n}\n```\n\n### Get Available Providers\n\n```bash\ncurl http://localhost:8000/providers\n# [\"anthropic\", \"google\", \"kimi\", \"openai\"]\n```\n\n## Technology Stack\n\n| Component | Technology | Why |\n|-----------|------------|-----|\n| **LLM** | OpenRouter (free tier) | Zero cost to experiment |\n| **Agent Pattern** | ReAct (Reason + Act) | Industry standard for tool-using agents |\n| **Messages** | LangChain `SystemMessage`, `HumanMessage`, `AIMessage` | Clean conversation management |\n| **LLM Client** | `langchain_openai.ChatOpenAI` | OpenRouter compatible |\n| **API** | FastAPI | Async support for parallel agents |\n| **Frontend** | React + Three.js | 3D visualization of results |\n| **State** | Zustand | Minimal React state management |\n\n### Key LangChain Components Used\n\n```python\nfrom langchain_openai import ChatOpenAI\nfrom langchain.messages import SystemMessage, HumanMessage, AIMessage\n\n# LLM client compatible with OpenRouter\nllm = ChatOpenAI(\n    model=\"amazon/nova-2-lite-v1:free\",\n    api_key=OPENROUTER_API_KEY,\n    base_url=\"https://openrouter.ai/api/v1\",\n)\n\n# Async invocation\nresponse = await llm.ainvoke([\n    SystemMessage(content=\"You are an optimizer agent...\"),\n    HumanMessage(content=\"Optimize this prompt for OpenAI...\"),\n])\n```\n\n## Project Structure\n\n```\nTheRosettaPrompt/\n├── rosetta_prompt/\n│   ├── main.py                    # FastAPI endpoints\n│   ├── config.py                  # LLM + provider config\n│   │\n│   ├── agents/\n│   │   ├── orchestrator.py        # Parallel agent coordination\n│   │   └── optimizer.py           # ReAct agent with tool loop\n│   │\n│   ├── utils/\n│   │   └── logger.py              # FileLogger for local logs\n│   │\n│   ├── models/\n│   │   └── schemas.py             # Pydantic models + AgentLogEntry\n│   │\n│   ├── logs/                      # Agent execution logs (auto-created)\n│   │   └── *.log\n│   │\n│   └── docs/                      # Knowledge base (auto-detected)\n│       ├── openai/prompting.md\n│       ├── anthropic/prompting.md\n│       ├── google/prompting.md\n│       └── kimi/prompting.md\n│\n├── updater/                       # Claude Agent SDK doc updater\n│   ├── agent.py                   # Main updater agent\n│   ├── tools.py                   # Custom tools (Firecrawl, file ops)\n│   ├── config.py                  # Provider URLs configuration\n│   └── scheduler.py               # Weekly update scheduler\n│\n└── ui/\n    └── src/\n        ├── components/\n        │   ├── InputScreen.js     # Prompt input + provider selection\n        │   ├── ProcessingScreen.js # Live agent logs\n        │   └── ResultsScreen.js   # 3D card carousel\n        └── store.js               # API calls + Zustand state\n```\n\n## Adding New Providers\n\nProviders are **auto-detected** from `docs/`. To add one:\n\n```bash\n# 1. Create provider directory\nmkdir rosetta_prompt/docs/mistral\n\n# 2. Add documentation (scrape from official docs)\ncat > rosetta_prompt/docs/mistral/prompting.md << 'EOF'\n# Mistral Prompting Guidelines\n\n## Best Practices\n- Use clear, structured instructions\n- Mistral models respond well to...\nEOF\n\n# 3. Restart server - new provider appears automatically\n```\n\nThe agent will now:\n1. `list_provider_docs(\"mistral\")` → `[\"prompting.md\"]`\n2. `read_provider_doc(\"mistral\", \"prompting.md\")` → Full guidelines\n3. Apply Mistral-specific patterns to optimize prompts\n\n## Automatic Documentation Updates\n\nThe `updater/` directory contains an autonomous agent that automatically updates prompting guides by scraping provider documentation using **Firecrawl** and synthesizing content with **Claude Opus**.\n\n### Updater Architecture\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│                    Scheduler (Weekly)                        │\n│                         │                                    │\n│                         ▼                                    │\n│  ┌─────────────────────────────────────────────────────┐    │\n│  │         Claude Opus (Anthropic SDK)                 │    │\n│  │         Native Tool Calling + ReAct Loop            │    │\n│  │                                                     │    │\n│  │  Tools:                                             │    │\n│  │  • list_providers → Get configured providers        │    │\n│  │  • batch_scrape_urls (Firecrawl) → Fetch all docs   │    │\n│  │  • read_current_guide → Compare with existing       │    │\n│  │  • update_guide → Write synthesized content         │    │\n│  │  • write_update_log → Record update status          │    │\n│  └──────────────────────┬──────────────────────────────┘    │\n│                         │                                    │\n│                         ▼                                    │\n│              rosetta_prompt/docs/*.md                        │\n└─────────────────────────────────────────────────────────────┘\n```\n\n### Running the Updater\n\n```bash\ncd updater\npip install -r requirements.txt\n\n# Manual update (all providers)\npython agent.py\n\n# Update specific providers\npython agent.py anthropic openai\n\n# Update multiple providers\npython agent.py google kimi\n\n# Weekly scheduler\npython scheduler.py\n```\n\n### Configuration\n\nAdd URLs for new providers in `updater/config.py`:\n\n```python\nPROVIDER_CONFIGS = {\n    \"mistral\": {\n        \"name\": \"Mistral\",\n        \"urls\": [\"https://docs.mistral.ai/capabilities/completion/\"],\n        \"doc_file\": \"prompting.md\"\n    }\n}\n\nCLAUDE_MODEL = \"claude-opus-4-5-20251101\"  # Model for synthesis\nMAX_TURNS = 5  # Max agent iterations\n```\n\nRequires `ANTHROPIC_API_KEY` and `FIRECRAWL_API_KEY` in `.env`.\n\n## Setup\n\n```bash\n# Backend\ncd rosetta_prompt\npip install -r requirements.txt\necho \"OPENROUTER_API_KEY=your_key\" > .env\nuvicorn main:app --reload --port 8000\n\n# Frontend\ncd ui\nnpm install\nnpm start\n```\n\n## License\n\nMIT\n",
        "plugins/readwren/README.md": "# WREN: AI Literary Interview Agent\n\n**An adaptive multi-agent system that extracts your literary DNA through conversation and generates actionable reading profiles.**\n\nWREN solves a critical problem for LLM users: you know what you like, but explaining your literary taste to an AI is hard. WREN's interview agent asks the right questions, listens deeply, and builds a structured profile that any LLM can use to generate precisely targeted content.\n\nBuilt with **LangGraph**, **LangChain**, and **Kimi K2 Thinking models**.\n\n---\n\n## Why WREN?\n\n**The Problem**: Kimi K2 is a great writer, but users struggle to articulate their literary preferences in a way LLMs can act on. Vague prompts like \"write me something good\" produce generic results.\n\n**The Solution**: WREN conducts a 12-turn adaptive interview that:\n- Extracts taste anchors (what you love/hate and why)\n- Maps your style signature (prose density, pacing, tone preferences)\n- Identifies narrative desires (story types you wish existed)\n- Captures implicit signals (vocabulary richness, engagement patterns)\n- Generates a structured, machine-readable profile\n\n**The Result**: A profile you can hand to any LLM to get content that matches your exact taste.\n\n---\n\n## Architecture: Multi-Agent System\n\nWREN uses a **specialized multi-agent architecture** with distinct roles:\n\n```\n┌────────────────────────────────────────────────────────────────────┐\n│                          CLI Interface                             │\n│                       (cli_interview.py)                           │\n└────────────────────┬───────────────────────────────────────────────┘\n                     │\n       ┌─────────────┴──────────────┐\n       │                            │\n┌──────▼─────────────────┐   ┌──────▼──────────────────────┐\n│   InterviewAgent       │   │   ProfileGeneratorAgent     │\n│ (kimi-k2-thinking-     │   │   (kimi-k2-thinking)        │\n│       turbo)           │   │                             │\n│                        │   │ Tools:                      │\n│ Tools:                 │   │ ┌─────────────────────────┐ │\n│ ┌────────────────────┐ │   │ │ ReasoningExtractor      │ │\n│ │ ProfileAnalyzer    │ │   │ │ - Extract thinking      │ │\n│ │ - Vocab richness   │ │   │ │ - Format reasoning      │ │\n│ │ - Response brevity │ │   │ └─────────────────────────┘ │\n│ │ - Engagement level │ │   │                             │\n│ └────────────────────┘ │   │ ┌─────────────────────────┐ │\n│                        │   │ │ ProfileFormatter        │ │\n│ ┌────────────────────┐ │   │ │ - JSON → Markdown       │ │\n│ │ConversationAnalyzer│ │   │ │ - Shareable text        │ │\n│ │ - Turn tracking    │ │   │ │ - Human-readable        │ │\n│ │ - Coverage check   │ │   │ └─────────────────────────┘ │\n│ │ - Readiness score  │ │   │                             │\n│ └────────────────────┘ │   │ ┌─────────────────────────┐ │\n└────────┬───────────────┘   │ │ ProfileSaver            │ │\n         │                   │ │ - Create user folders   │ │\n         │                   │ │ - Save logs + profiles  │ │\n         │                   │ │ - Multiple formats      │ │\n         │                   │ └─────────────────────────┘ │\n         │                   └──────┬──────────────────────┘\n         │                          │\n┌────────▼──────────────────────────▼─────────────────────────┐\n│                   LangGraph StateGraph                      │\n│  ┌──────────────────────────────────────────────────────┐   │\n│  │                                                      │   │\n│  │  [analyze_node]                                     │   │\n│  │      ↓                                              │   │\n│  │  Run ProfileAnalyzer + ConversationAnalyzer         │   │\n│  │      ↓                                              │   │\n│  │  [_should_continue]                                 │   │\n│  │      ↓                    ↓                         │   │\n│  │  turn < 12           turn >= 12                     │   │\n│  │      ↓                    ↓                         │   │\n│  │  [generate_question]  [generate_profile]            │   │\n│  │                                                      │   │\n│  └──────────────────────────────────────────────────────┘   │\n│                                                              │\n│  State: {messages, turn_count, analysis, profile_data}      │\n└────────┬─────────────────────────────────────────────────────┘\n         │\n┌────────▼──────────────┐\n│ RedisCheckpointSaver  │\n│ (State Persistence)   │\n│                       │\n│ - Pickle serialization│\n│ - 24h TTL             │\n│ - Resume sessions     │\n└───────────────────────┘\n```\n\n### Agent 1: InterviewAgent\n\n**Role**: Conversational interviewer that adapts to user responses\n\n**Model**: `kimi-k2-thinking-turbo` (fast, conversational)\n\n**Capabilities**:\n- Conducts 12-turn structured interview\n- References previous answers (shows it's listening)\n- Adjusts question depth based on response style\n- Tracks coverage across 5 dimensions\n- Uses LangGraph state machine for conversation flow\n\n**Key Innovation**: Uses real-time analysis tools to adapt questioning:\n- `ProfileAnalyzerTool`: Measures vocabulary richness, brevity, engagement\n- `ConversationAnalyzerTool`: Tracks coverage and determines readiness\n\n### Agent 2: ProfileGeneratorAgent\n\n**Role**: Deep analyst that transforms conversation into structured profile\n\n**Model**: `kimi-k2-thinking` (extended reasoning for analysis)\n\n**Capabilities**:\n- Parses full conversation transcript\n- Generates JSON profile with 40+ data points\n- Scores style preferences on 0-100 scales\n- Provides human-readable explanations\n- Extracts its own reasoning process\n\n**Key Innovation**: Single-purpose agent runs once, uses expensive model only when needed, includes explanations in second-person for easy sharing.\n\n### Agent 3: ReasoningExtractor\n\n**Role**: Extracts and formats Kimi K2's internal thinking\n\n**Capabilities**:\n- Pulls `reasoning_content` from model responses\n- Formats for human readability\n- Saves separately for transparency\n- Enables debugging and insight\n\n---\n\n## LangGraph State Machine\n\nWREN uses **LangGraph** for stateful conversation management:\n\n```python\nclass InterviewState(TypedDict):\n    messages: Annotated[List[BaseMessage], add]  # Conversation history\n    turn_count: int                              # Current turn\n    profile_data: Dict[str, Any]                 # Generated profile\n    is_complete: bool                            # Completion flag\n    current_analysis: Dict[str, Any]             # Real-time metrics\n```\n\n### Graph Flow\n\n```\nUser Input\n    ↓\n[analyze_node]\n├─> ProfileAnalyzerTool: Analyze response style\n├─> ConversationAnalyzerTool: Check coverage\n└─> Update state with analysis\n    ↓\n[_should_continue]\n├─> turn_count >= 12? → generate_profile\n└─> turn_count < 12?  → generate_question\n    ↓\n[generate_question_node]\n├─> Build prompt with turn context + analysis\n├─> Invoke Kimi K2 Thinking Turbo\n├─> Extract reasoning from response\n└─> Return AIMessage\n    ↓\nState persisted to Redis → Ready for next turn\n```\n\n**Why LangGraph?**\n- Built-in state persistence (Redis or in-memory)\n- Clean separation of analysis → decision → generation\n- Resumable sessions (pick up where you left off)\n- Type-safe state transitions\n\n---\n\n## Redis Integration\n\nWREN implements a **custom Redis checkpointer** for LangGraph:\n\n```python\nclass RedisCheckpointSaver(BaseCheckpointSaver):\n    def put(self, config, checkpoint, metadata, new_versions):\n        # Serializes full state with pickle (handles Python objects)\n        serialized = pickle.dumps({\n            \"checkpoint\": checkpoint,\n            \"metadata\": metadata,\n            \"config\": config\n        })\n        self.redis.setex(key, self.ttl, serialized)  # 24h TTL\n```\n\n**Why Custom?**\n- LangGraph doesn't include Redis checkpointer out of the box\n- Standard JSON serialization fails on Python objects\n- Pickle handles complex state including functions/lambdas\n\n**Benefits**:\n- Sessions persist across restarts\n- Resume interrupted interviews\n- Inspect state at any point\n- Auto-expiration after 24 hours\n\n---\n\n## Prompt Engineering\n\n### Adaptive System Prompt\n\n```python\nSYSTEM_PROMPT = \"\"\"You are a world-class literary profiler conducting \nan adaptive interview.\n\nCORE PRINCIPLES:\n- Ask ONE question at a time\n- Always reference their specific previous answers\n- Adapt follow-ups based on response depth and style\n- Continue asking questions until turn 12\n\nSTRICT RULES:\n- CURRENT TURN: {turn_count} of 12\n- If turn < 12: Ask another question (do NOT mention completion)\n- If turn = 12: Only then offer to generate their profile\n\"\"\"\n```\n\n**Key Features**:\n- Dynamic turn injection prevents premature completion\n- Explicit rules override model's tendency to end early\n- References previous answers (shows listening)\n- Adapts energy level to user responses\n\n### Profile Generation Prompt\n\nThe system dynamically loads scoring guidelines from `PROFILE_RUBRIC.md`:\n\n```python\n@staticmethod\ndef get_summary_prompt(conversation: str, include_rubric: bool = True):\n    # Load rubric scales from file\n    rubric_section = _load_rubric_section()\n    \n    return f\"\"\"Generate JSON profile with:\n    \n    JSON SCHEMA: [detailed structure]\n    \n    SCORING GUIDELINES:\n    {rubric_section}  ← Dynamically loaded from PROFILE_RUBRIC.md\n    \n    Conversation:\n    {conversation}\n    \"\"\"\n```\n\n**Why Dynamic Loading?**\n- Single source of truth (update rubric → prompts update automatically)\n- LLM sees detailed scoring guidance\n- Consistent scoring across all profiles\n\n---\n\n## Tools & Analysis\n\n### ProfileAnalyzerTool\n\n```python\ndef _run(self, response_text: str, conversation_history: List) -> Dict:\n    \"\"\"Analyzes individual responses for implicit signals.\"\"\"\n    \n    # Calculate metrics\n    vocabulary_richness = unique_words / total_words\n    response_brevity = 1 / (word_count / 100)  # Normalized\n    engagement_level = heuristic(examples, emotion_words, depth)\n    \n    return {\n        \"vocabulary_richness\": 0.0-1.0,\n        \"response_brevity\": 0.0-1.0,\n        \"engagement_level\": 0.0-1.0\n    }\n```\n\n### ConversationAnalyzerTool\n\n```python\ndef _run(self, conversation_history: List[Dict]) -> Dict:\n    \"\"\"Tracks coverage across 5 dimensions.\"\"\"\n    \n    coverage = {\n        \"taste_anchors\": check_keywords([\"book\", \"author\", \"story\"]),\n        \"style_preference\": check_keywords([\"prose\", \"writing\", \"style\"]),\n        \"narrative_desire\": check_keywords([\"wish\", \"want\", \"story\"]),\n        \"consumption_habit\": check_keywords([\"read\", \"time\", \"daily\"])\n    }\n    \n    return {\n        \"turn_count\": len(user_messages),\n        \"coverage\": coverage,\n        \"coverage_score\": sum(coverage.values()) / len(coverage),\n        \"ready_for_summary\": turn >= 8 and coverage_score >= 0.75\n    }\n```\n\n**These tools feed into the agent's decision-making**, enabling adaptive questioning based on what's been covered.\n\n---\n\n## Output: Structured Profiles\n\nWREN generates **4 output formats**:\n\n### 1. JSON Profile (`profile_TIMESTAMP.json`)\n\n```json\n{\n  \"taste_anchors\": {\n    \"loves\": [\"The Remains of the Day\", \"Beloved\"],\n    \"hates\": [\"Finnegans Wake\"],\n    \"inferred_genres\": [\"modernist fiction\", \"trauma narratives\"]\n  },\n  \"style_signature\": {\n    \"prose_density\": 70,    // 0-100 scale\n    \"pacing\": 60,\n    \"tone\": 10,\n    \"worldbuilding\": 20,\n    \"character_focus\": 90\n  },\n  \"narrative_desires\": {\n    \"wish\": \"Language fracturing to contain catastrophe\",\n    \"preferred_ending\": \"transcendent\",\n    \"themes\": [\"language limits\", \"trauma\", \"alienation\"]\n  },\n  \"consumption\": {\n    \"daily_time_minutes\": 75,\n    \"delivery_frequency\": \"every_few_days\",\n    \"pages_per_delivery\": 30\n  },\n  \"implicit\": {\n    \"vocabulary_richness\": 0.95,\n    \"response_brevity_score\": 0.2,\n    \"engagement_index\": 0.95\n  },\n  \"explanations\": {\n    \"prose_density\": \"You appreciate complex prose when it reveals emotion...\",\n    \"reading_philosophy\": \"You read as an act of emotional archaeology...\"\n  },\n  \"reader_archetype\": \"Fracture Dweller\"\n}\n```\n\n### 2. Markdown Profile (`profile_TIMESTAMP.md`)\n\nHuman-readable with sections and formatting.\n\n### 3. Shareable Profile (`profile_TIMESTAMP_SHAREABLE.txt`)\n\nOptimized for social media / documentation sharing.\n\n### 4. Conversation Log (`conversation_TIMESTAMP.json`)\n\nFull transcript with reasoning content from Kimi K2.\n\n---\n\n## Quick Start\n\n### Installation\n\n```bash\n# Clone repository\ngit clone https://github.com/muratcankoylan/readwren.git\ncd readwren\n\n# Install dependencies\npip install -r requirements.txt\n\n# Set up environment\ncp env.example .env\n# Edit .env with your Moonshot API key\n```\n\n### Configuration\n\n```bash\n# .env file\nMOONSHOT_API_KEY=sk-your-api-key-here\nMOONSHOT_BASE_URL=https://api.moonshot.ai/v1\n\n# Optional: Redis for persistent sessions\nREDIS_HOST=your-redis-host.com\nREDIS_PORT=17887\nREDIS_PASSWORD=your-password\n```\n\nGet your Moonshot API key: https://platform.moonshot.ai/\n\n### Run Interview\n\n```bash\n./run_interview.sh\n```\n\nOr directly:\n\n```bash\npython cli_interview.py\n```\n\n### Example Session\n\n```\nLet's start simple. Name 3 books you've loved, and 1 you couldn't finish.\n\nYour response: I love The Remains of the Day for its devastating restraint, \nBeloved for how it makes the unspeakable tangible, and The Metamorphosis \nfor crystallizing alienation. I couldn't finish Finnegans Wake.\n\nAgent: Your taste gravitates toward emotional precision over linguistic \nspectacle. When you say \"devastating restraint,\" what specific moment \nin Ishiguro's novel best exemplifies this for you?\n\nProgress: ●○○○○○○○○○○○ (1/12)\n```\n\nAfter 12 turns:\n\n```\n✓ Profile saved to: user_profiles/cli_20251108_145739/\n  - conversation_20251108_150303.json (full transcript)\n  - profile_20251108_150303.json (structured data)\n  - profile_20251108_150303.md (human-readable)\n  - profile_20251108_150303_SHAREABLE.txt (social sharing)\n```\n\n---\n\n## Using Your Profile\n\nOnce generated, give your profile to any LLM:\n\n```\nClaude/GPT/Grok/Kimi, write a short story for me using these preferences:\n\n- Prose density: 70/100 (between Morrison and Ishiguro)\n- Tone: 10/100 (dark, serious, restrained)\n- Character focus: 90/100 (psychological depth over plot)\n- Theme: Language failing to contain private catastrophe\n- Ending: Transcendent (through remaining broken)\n- Avoid: Linguistic performance for its own sake\n```\n\n**Result**: Precisely targeted content matching your exact taste.\n\n---\n\n## Project Structure\n\n```\nreadwren/\n├── src/                     # Core application\n│   ├── agents/              # AI agents\n│   │   ├── interview_agent.py       # Main interviewer (LangGraph)\n│   │   ├── profile_generator.py     # Profile analyst\n│   │   ├── redis_checkpointer.py    # Custom Redis persistence\n│   │   └── reasoning_extractor.py   # Kimi K2 reasoning handler\n│   ├── tools/               # Analysis tools\n│   │   ├── profile_tools.py         # Response analyzers\n│   │   ├── profile_saver.py         # File management\n│   │   └── profile_formatter.py     # Output formatting\n│   ├── prompts/             # Prompt engineering\n│   │   └── interview_prompts.py     # System prompts + rubric loader\n│   └── config/              # Configuration\n│       └── settings.py\n├── docs/                    # Documentation\n│   ├── TECHNICAL_DOCUMENTATION.md   # Complete technical reference\n│   ├── PROFILE_RUBRIC.md            # Scoring system (loaded dynamically)\n│   ├── RUBRIC_INTEGRATION.md        # Rubric usage guide\n│   └── REDIS_GUIDE.md               # Redis setup and usage\n├── scripts/                 # Utility scripts\n│   ├── view_redis_sessions.py       # List active sessions\n│   ├── view_session_conversation.py # Decode Redis checkpoints\n│   ├── view_conversation_log.py     # Display conversation logs\n│   └── retrieve_profile.py          # Retrieve and edit profiles\n├── examples/                # Example outputs\n│   └── example_session/             # Complete mock interview\n│       ├── logs/                    # Conversation transcript\n│       └── profiles/                # Generated profiles\n├── cli_interview.py         # Interactive CLI entry point\n├── run_interview.sh         # Startup script\n├── requirements.txt         # Python dependencies\n├── env.example              # Environment template\n├── LICENSE                  # MIT License\n├── README.md                # This file\n└── user_profiles/           # Your generated outputs (gitignored)\n```\n\n---\n\n## Technical Details\n\n### Technology Stack\n\n- **LangGraph**: State machine for conversation flow\n- **LangChain**: LLM abstraction layer\n- **Moonshot AI**: Kimi K2 models (turbo + thinking)\n- **Redis**: Session persistence (optional)\n- **Python 3.11+**\n\n### Model Strategy\n\n| Aspect | InterviewAgent | ProfileGenerator |\n|--------|----------------|------------------|\n| Model | kimi-k2-thinking-turbo | kimi-k2-thinking |\n| Tokens | 800 | 4000 |\n| Temperature | 0.8 | 0.7 |\n| Speed | ~2-3s | ~10-15s |\n| Reasoning | Basic thinking | Extended reasoning |\n| Total calls | 12+ | 1 |\n\n**Strategy**: Use thinking-turbo for fast conversational turns, reserve full thinking model for deep analysis at the end.\n\n### State Management\n\n```python\n# LangGraph state with Redis persistence\nstate = {\n    \"messages\": [HumanMessage, AIMessage, ...],\n    \"turn_count\": 8,\n    \"current_analysis\": {\n        \"coverage_score\": 0.85,\n        \"vocabulary_richness\": 0.92\n    },\n    \"is_complete\": False\n}\n\n# Automatically saved to Redis after each turn\n# TTL: 24 hours (configurable)\n```\n\n---\n\n## Advanced Features\n\n### Session Management\n\n```bash\n# View all active Redis sessions\npython scripts/view_redis_sessions.py\n\n# View specific conversation\npython scripts/view_conversation_log.py user_profiles/cli_20251108_145739/logs/conversation.json\n\n# Retrieve session from Redis\npython scripts/retrieve_profile.py\n```\n\n### Kimi K2 Reasoning\n\nWREN extracts and displays Kimi K2's internal thinking:\n\n```\n💭 REASONING:\nLet me analyze their preference for restraint. They mentioned Ishiguro \nand Morrison, both use controlled prose. I should probe if they prefer \nbrevity or just emotional control. The Finnegans Wake rejection suggests \nthey dislike when complexity becomes performative...\n```\n\nEnable in CLI:\n\n```bash\nShow Kimi K2 reasoning? (y/N): y\n```\n\n### Custom Rubric\n\nEdit `PROFILE_RUBRIC.md` to adjust scoring scales. Changes automatically propagate to profile generation prompts.\n\n---\n\n## Documentation\n\n- **[TECHNICAL_DOCUMENTATION.md](docs/TECHNICAL_DOCUMENTATION.md)**: Complete architecture, agents, tools, prompts, data flow\n- **[PROFILE_RUBRIC.md](docs/PROFILE_RUBRIC.md)**: Scoring system and metric definitions\n- **[RUBRIC_INTEGRATION.md](docs/RUBRIC_INTEGRATION.md)**: How rubric is used in code\n- **[REDIS_GUIDE.md](docs/REDIS_GUIDE.md)**: Redis setup and session management\n\n---\n\n## Utilities\n\n- `scripts/view_redis_sessions.py`: List all active sessions with metadata\n- `scripts/view_session_conversation.py`: Decode Redis checkpoint for a session\n- `scripts/view_conversation_log.py`: Display saved conversation logs\n- `scripts/retrieve_profile.py`: Retrieve and manually edit profiles\n\n---\n\n## Use Cases\n\n1. **Content Creators**: Generate stories/essays matching your style\n2. **Readers**: Get precise book recommendations\n3. **Writers**: Articulate your voice for AI writing assistants\n4. **Product Teams**: Build user profiles for personalization\n5. **Researchers**: Study literary preferences and taste formation\n\n---\n\n## Why Multi-Agent?\n\n**Single-agent approach** (naive):\n- One LLM does everything: interview + analyze + generate profile\n- Expensive model runs 12+ times\n- Can't optimize for different tasks\n- Mixed concerns (conversation vs analysis)\n\n**Multi-agent approach** (WREN):\n- **InterviewAgent**: Fast, conversational model (turbo)\n- **ProfileGenerator**: Deep analysis model (thinking)\n- **Each agent optimized for its role**\n- Thinking model runs once (cost-efficient)\n- Clean separation of concerns\n- Tools feed into agent decision-making\n\nThis architecture is **reusable**: same pattern works for medical history, design preferences, dietary taste, etc.\n\n---\n\n## Contributing\n\nWREN is open source. Contributions welcome:\n\n1. Fork the repository\n2. Create a feature branch\n3. Make your changes\n4. Submit a pull request\n\nAreas for contribution:\n- Additional analysis tools\n- New output formats\n- Web UI\n- Multi-language support\n- Alternative LLM support\n\n---\n\n## License\n\nOpen Source - MIT License\n\n---\n\n## Credits\n\n**Built by**: Muratcan Koylan ([@koylanai](https://twitter.com/koylanai))\n\n**Powered by**:\n- [Moonshot AI](https://platform.moonshot.ai/) (Kimi K2 models)\n- [LangChain](https://www.langchain.com/)\n- [LangGraph](https://langchain-ai.github.io/langgraph/)\n\n---\n\n## Questions?\n\n- **Twitter**: [@koylanai](https://twitter.com/koylanai)\n- **GitHub Issues**: [readwren/issues](https://github.com/muratcankoylan/readwren/issues)\n- **Documentation**: See [TECHNICAL_DOCUMENTATION.md](https://github.com/muratcankoylan/readwren/blob/main/docs/TECHNICAL_DOCUMENTATION.md)\n\n---\n\n**WREN**: Because explaining your taste shouldn't be harder than having it.\n\n",
        "plugins/food-tour-planner/README.md": "# DeepAgent Food Tours\n\nAI-powered food tour planner using [LangChain DeepAgents](https://github.com/langchain-ai/deepagents), Google Maps API, and Tavily research.\n\n## Screenshots\n\n### Interactive Map Interface\n<img src=\"screenshots/Screenshot%202025-11-05%20at%2012.32.22%E2%80%AFAM.png\" width=\"800\" alt=\"Map interface with search points\">\n\n### DeepAgent Planning Process\n<img src=\"screenshots/Screenshot%202025-11-05%20at%2012.47.05%E2%80%AFAM.png\" width=\"800\" alt=\"Adding search points to the map\">\n<img src=\"screenshots/Screenshot%202025-11-05%20at%2012.48.38%E2%80%AFAM.png\" width=\"800\" alt=\"Natural language AI prompt\">\n<img src=\"screenshots/Screenshot%202025-11-05%20at%2012.49.23%E2%80%AFAM.png\" width=\"800\" alt=\"AI agents planning the tour\">\n<img src=\"screenshots/Screenshot%202025-11-05%20at%2012.50.43%E2%80%AFAM.png\" width=\"800\" alt=\"Reviews and neighborhood insights\">\n\n\n### Generated Tour Dashboard\n<img src=\"screenshots/Screenshot%202025-11-05%20at%2012.49.50%E2%80%AFAM.png\" width=\"800\" alt=\"Beautiful HTML tour dashboard header\">\n<img src=\"screenshots/Screenshot%202025-11-05%20at%2012.50.06%E2%80%AFAM.png\" width=\"800\" alt=\"Establishment details with photos and reviews\">\n<img src=\"screenshots/Screenshot%202025-11-05%20at%2012.50.14%E2%80%AFAM.png\" width=\"800\" alt=\"Personalized tour recommendations\">\n\n\n## Features\n\n- **Interactive Map Interface**: Click to add search points and visualize coverage areas\n- **Smart Search**: Scan neighborhoods with multiple overlapping search radii for complete coverage\n- **AI Planning**: Use natural language prompts to let AI plan personalized food tours\n- **Lightweight Data Collection**: Efficient API usage with minimal requests\n- **Beautiful Dashboards**: Auto-generated HTML reports with establishment details and research findings\n- **Flexible Export**: Preview results and select only the establishments you want before detailed export\n\n## Architecture\n\nThe project uses a multi-agent architecture powered by LangChain DeepAgents:\n\n- **Scan Manager** (Node.js): Web UI and basic scan functionality\n- **Dashboard Server** (Node.js): Serves generated HTML tour reports\n- **DeepAgent API** (Python): Coordinates AI agents for intelligent tour planning\n  - **Restaurant Finder Agent**: Searches and evaluates food establishments\n  - **Neighborhood Researcher Agent**: Analyzes local culture and food trends\n  - **Dashboard Creator Agent**: Generates beautiful HTML reports\n\n## Prerequisites\n\n- **Node.js** >= 18.0.0\n- **Python** >= 3.9\n- **npm** >= 9.0.0\n\n## API Keys Required\n\n1. **Google Maps API Key** ([Get it here](https://console.cloud.google.com/google/maps-apis))\n   - Enable: Places API, Geocoding API, Maps JavaScript API\n   \n2. **Tavily API Key** ([Get it here](https://tavily.com))\n   - For neighborhood research and web data\n   \n3. **Anthropic API Key** ([Get it here](https://console.anthropic.com)) OR **OpenAI API Key** ([Get it here](https://platform.openai.com))\n   - For AI agent reasoning\n\n## Installation\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/muratcankoylan/deepagent-food-tours.git\ncd deepagent-food-tours\n```\n\n2. Install Node.js dependencies:\n```bash\nnpm install\n```\n\n3. Install Python dependencies:\n```bash\npip3 install -r requirements.txt\n```\n\n4. Create `.env` file from template:\n```bash\ncp .env.example .env\n```\n\n5. Edit `.env` and add your API keys:\n```bash\nGOOGLE_MAPS_API_KEY=your_actual_key_here\nTAVILY_API_KEY=your_actual_key_here\nANTHROPIC_API_KEY=your_actual_key_here  # or OPENAI_API_KEY\n```\n\n## Usage\n\n### Quick Start\n\nRun all three services at once:\n```bash\n./start.sh\n```\n\nThis will start:\n- Scan Manager UI: http://localhost:3001\n- Dashboard Server: http://localhost:3002\n- DeepAgent API: http://localhost:5001\n\n### Manual Start (Individual Services)\n\nIf you prefer to run services individually:\n\n```bash\n# Terminal 1: Scan Manager\nnpm start\n\n# Terminal 2: Dashboard Server\nnpm run dashboard-server\n\n# Terminal 3: DeepAgent API\nnpm run deepagent-api\n```\n\n### Basic Scan (No AI)\n\n1. Open http://localhost:3001\n2. Click on the map to add search points\n3. Configure radius and categories\n4. Click \"Create Scan Task\"\n5. Wait for scanning to complete\n6. Click \"View Results & Export\"\n7. Select establishments and export\n\n### AI-Powered Tour Planning\n\n1. Open http://localhost:3001\n2. Click on the map to add search points for the neighborhood\n3. Enter an AI prompt like:\n   - \"I want a fun evening exploring local food\"\n   - \"Plan a romantic dinner date in this area\"\n   - \"Find the best brunch spots for a Sunday morning\"\n4. Click \"Create Scan Task\"\n5. Wait 1-2 minutes for the AI to plan your tour\n6. Click \"View AI Dashboard\" to see the generated tour report\n\n## Project Structure\n\n```\ndeepagent-food-tours/\n├── src/\n│   ├── services/\n│   │   ├── places.js                    # Google Places API wrapper\n│   │   ├── neighborhood-analyzer.js     # Multi-point scan logic\n│   │   └── geographic-sorter.js         # Route optimization\n│   ├── agents/\n│   │   ├── food_tour_agent.py           # Main DeepAgent\n│   │   └── tools/\n│   │       ├── places_lightweight.py    # Lightweight Places API\n│   │       ├── tavily_research.py       # Neighborhood research\n│   │       └── dashboard_generator.py   # HTML report generator\n│   ├── scan-manager.js                  # Web UI backend\n│   ├── dashboard-server.js              # Dashboard hosting\n│   └── deepagent-api.py                 # Python API bridge\n├── public/\n│   └── index.html                       # Web UI frontend\n├── dashboards/                          # Generated HTML reports\n├── output/                              # Export JSON files\n├── package.json\n├── requirements.txt\n├── start.sh\n└── .env.example\n```\n\n## How It Works\n\n### Basic Scanning\n\n1. User adds search points on the map\n2. System performs circular searches at each point\n3. Automatic deduplication removes overlapping results\n4. User previews results and selects items for detailed export\n5. Full details fetched only for selected items (saves API calls)\n\n### AI-Powered Planning\n\n1. User provides location and natural language prompt\n2. DeepAgent creates task breakdown\n3. Restaurant Finder agent searches for relevant establishments\n4. Neighborhood Researcher agent analyzes the area\n5. Dashboard Creator agent generates an HTML report\n6. User views the complete tour plan at http://localhost:3002\n\n\n## Troubleshooting\n\n### Port Already in Use\n\nIf you see \"address already in use\" errors:\n```bash\n# Kill processes on ports 3001, 3002, 5001\nlsof -ti:3001,3002,5001 | xargs kill -9\n```\n\n### Python Import Errors\n\nMake sure you're in the project root when running:\n```bash\ncd /path/to/deepagent-food-tours\npython3 src/deepagent-api.py\n```\n\n### Missing API Keys\n\nCheck your `.env` file has all required keys:\n```bash\ncat .env\n```\n\n### DeepAgent Timeout\n\nFor large neighborhoods, increase the timeout in `src/scan-manager.js` (currently 5 minutes).\n\n## Contributing\n\nContributions welcome. Please open an issue first to discuss proposed changes.\n\n## License\n\nMIT\n\n## Acknowledgments\n\n- Built with [LangChain DeepAgents](https://github.com/langchain-ai/deepagents)\n- Uses Google Maps Platform APIs\n- Powered by Tavily Research API\n\nFeel free to use this as a template for your own DeepAgent projects.\n\n",
        "plugins/actual-code/README.md": "# 🎯 ActualCode - AI-Powered Code Assessment Generator\n\n[![Watch this video](https://img.youtube.com/vi/jnIFJ8-syio/0.jpg)](https://www.youtube.com/watch?v=jnIFJ8-syio)\n\nTechnical details: https://github.com/muratcankoylan/actual_code/blob/main/ActualCode-TechnicalDeepDiveforJury.md\n\n**Transform GitHub repositories into realistic coding challenges using multi-agent AI**\n\n[![Built with Google Gemini](https://img.shields.io/badge/Built%20with-Google%20Gemini-4285F4?logo=google)](https://cloud.google.com/vertex-ai)\n[![Python 3.11+](https://img.shields.io/badge/Python-3.11+-3776AB?logo=python)](https://www.python.org/)\n[![A2A Protocol](https://img.shields.io/badge/Protocol-A2A-FF6B6B)](https://github.com/google/adk)\n\n---\n\n## 🌟 Overview\n\nActualCode is a code assessment platform that analyzes **real GitHub repositories** and generates **personalized, realistic coding challenges** using a **7-agent AI architecture** powered by Google's Gemini models and the A2A (Agent-to-Agent) protocol.\n\n### The Problem We Solve\n\n- **LeetCode is too generic** - Candidates solve abstract algorithms, not real-world problems\n- **Hiring is time-consuming** - Creating repository-specific assessments takes hours\n- **Context gap** - Candidates who ace LeetCode still struggle with actual codebases\n\n### Our Solution\n\n1. **Input**: Any GitHub repository URL + difficulty level\n2. **AI Magic**: 7 specialized AI agents collaborate using A2A protocol\n3. **Output**: Realistic, implementable coding problem in **~2 minutes**\n\n---\n\n## 🏗️ Architecture\n\n```\nUser Input (GitHub Repo)\n        ↓\n   Agent 1: Scanner (GitHub API)\n        ↓\n   Agents 2-5: Parallel Analysis\n     • Code Analyzer (Gemini 2.5 Pro)\n     • PR Analyzer (Gemini 2.5 Flash)\n     • Issue Analyzer (Gemini 2.5 Flash)\n     • Dependency Analyzer (Gemini 2.5 Flash)\n        ↓\n   Agent 6: Problem Creator (Gemini 2.5 Pro)\n        ↓\n   Agent 7: QA Validator (Gemini 2.5 Flash)\n        ↓\n   Personalized Assessment ✨\n```\n\n### Multi-Agent System Features\n\n- **7 Specialized Agents** - Each with unique expertise\n- **A2A Protocol** - Google's Agent-to-Agent communication\n- **Single-Pass Analysis** - Optimized for speed (2 min vs 4+ min)\n- **QA Validation** - Automated quality scoring with feedback\n- **Repository-Specific** - Problems tailored to actual codebase\n\n---\n\n## 🚀 Quick Start\n\n### Prerequisites\n\n1. **Python 3.11+**\n2. **GitHub Personal Access Token** - [Get here](https://github.com/settings/tokens)\n3. **Google Cloud Account** - With Vertex AI enabled\n4. **Service Account Key** - For Google Cloud authentication\n\n### Installation\n\n```bash\n# Clone the repository\ngit clone https://github.com/muratcankoylan/actual_code.git\ncd actual_code\n\n# Create virtual environment\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n\n# Install dependencies\npip install -r requirements.txt\n\n# Set up environment variables\ncp .env.example .env\n# Edit .env with your credentials\n```\n\n### Configuration\n\nCreate a `.env` file with:\n\n```bash\n# GitHub Token\nGITHUB_TOKEN=your_github_personal_access_token\n\n# Google Cloud\nGOOGLE_CLOUD_PROJECT=your-project-id\nGOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account-key.json\nGOOGLE_CLOUD_REGION=us-central1\nGOOGLE_GENAI_USE_VERTEXAI=True\n```\n\n### Run\n\n```bash\n# Activate virtual environment\nsource venv/bin/activate\n\n# Run the CLI\npython cli_runner.py\n```\n\nFollow the interactive prompts to generate your first assessment!\n\n---\n\n## 📖 Usage\n\n### Interactive CLI\n\n```bash\n$ python cli_runner.py\n\nGitHub Repository URL: facebook/react\nSelect Difficulty: [2] medium\nSelect Problem Type: [1] feature\nTime Limit: [3] 180 minutes\nProceed? y\n\n[AI agents analyze the repository...]\n\n✅ Assessment Generated Successfully!\nProblem Title: Implement Error Boundary with Recovery\nTech Stack: JavaScript, React, TypeScript\nQA Score: 85/100\n\n✅ Assessment saved to: assessment_20250930_153045.json\n✅ Detailed logs saved to: DETAILED_RUN_20250930_153045.txt\n```\n\n### Output Files\n\n1. **`assessment_{timestamp}.json`** - Complete assessment with:\n   - Problem statement\n   - Requirements & acceptance criteria\n   - Starter code\n   - Hints\n   - Evaluation rubric\n   - QA validation scores\n\n2. **`DETAILED_RUN_{timestamp}.txt`** - Complete logs with:\n   - Repository data (all files)\n   - Agent analysis details\n   - Problem generation process\n   - QA validation feedback\n\n---\n\n## 🎯 Features\n\n### Real GitHub Integration\n\n- ✅ Fetches actual repository data via GitHub API\n- ✅ Analyzes real code structure, PRs, issues\n- ✅ Uses actual tech stack and dependencies\n- ✅ References real codebase patterns\n\n### Multi-Agent AI Pipeline\n\n- ✅ **7 Specialized Agents** working in concert\n- ✅ **A2A Protocol** for agent communication\n- ✅ **Parallel Processing** for speed\n- ✅ **Single-Pass Analysis** (optimized)\n- ✅ **QA Validation** with automated scoring\n\n### Repository-Specific Problems\n\n- ✅ Problems match the input repository's tech stack\n- ✅ Addresses actual weaknesses in the codebase\n- ✅ Uses repository's architecture patterns\n- ✅ Realistic and implementable within time limit\n\n### Quality Assurance\n\n- ✅ 4-dimension validation (Feasibility, Quality, Technical, Educational)\n- ✅ Automated scoring (0-100)\n- ✅ Specific feedback for improvement\n- ✅ Single-pass validation with refinement\n\n---\n\n## 🧪 Example\n\n### Input\n\n```\nRepository: https://github.com/expressjs/express\nDifficulty: medium\nType: feature\nTime: 180 minutes\n```\n\n### Output\n\n```json\n{\n  \"problem\": {\n    \"title\": \"Implement Advanced Middleware Error Handling\",\n    \"description\": \"Add comprehensive error handling middleware to Express...\",\n    \"tech_stack\": [\"JavaScript\", \"Express\", \"Node.js\"],\n    \"requirements\": [\n      \"Create custom error classes\",\n      \"Implement middleware chain\",\n      \"Add error logging\",\n      ...\n    ],\n    \"acceptance_criteria\": [...],\n    \"starter_code\": [...],\n    \"hints\": [...],\n    \"estimated_time\": 180,\n    \"difficulty\": \"medium\",\n    \"evaluation_rubric\": [...]\n  },\n  \"validation\": {\n    \"overall_score\": 85,\n    \"scores\": {\n      \"feasibility\": 90,\n      \"quality\": 85,\n      \"technical\": 82,\n      \"educational\": 83\n    }\n  }\n}\n```\n\n---\n\n## 📚 Documentation\n\n- **[QUICK_START.md](QUICK_START.md)** - 5-minute setup guide\n- **[CLI_GUIDE.md](CLI_GUIDE.md)** - Complete CLI documentation\n- **[PRODUCTION_READY.md](PRODUCTION_READY.md)** - Architecture details\n- **[SETUP_GITHUB.md](SETUP_GITHUB.md)** - GitHub token setup\n- **[ALL_ISSUES_RESOLVED.md](ALL_ISSUES_RESOLVED.md)** - Development changelog\n- **[final_docs/](final_docs/)** - Complete technical documentation\n\n---\n\n## 🏗️ Technical Stack\n\n### AI & Cloud\n\n- **Google Vertex AI** - AI platform\n- **Gemini 2.5 Pro** - Code analysis & problem creation\n- **Gemini 2.5 Flash** - PR/Issue/Dependency analysis & QA validation\n- **Google ADK** - Agent Development Kit\n- **A2A Protocol** - Agent-to-Agent communication\n\n### Backend\n\n- **Python 3.11+** - Core language\n- **aiohttp** - Async HTTP for GitHub API\n- **structlog** - Structured logging\n\n### Integration\n\n- **GitHub API** - Repository data fetching\n- **Vertex AI API** - AI model access\n\n---\n\n## 📊 Performance\n\n- **Repository Fetch**: 5-15 seconds\n- **Agent Analysis**: ~60 seconds (single-pass)\n- **Problem Creation**: 30-45 seconds\n- **QA Validation**: 10-15 seconds\n- **Refinement**: 20-35 seconds\n\n**Total**: **~2 minutes** (optimized from 4+ minutes)\n\n---\n\n## 🔧 Project Structure\n\n```\nhackathon_code/\n├── cli_runner.py              # Interactive CLI interface\n├── orchestrator.py            # Multi-agent coordinator\n├── agents/                    # 7 AI agents\n│   ├── scanner_agent.py       # GitHub repository scanner\n│   ├── code_analyzer_agent.py # Code architecture analyzer\n│   ├── pr_analyzer_agent.py   # Pull request analyzer\n│   ├── issue_analyzer_agent.py# Issue tracker analyzer\n│   ├── dependency_analyzer_agent.py # Tech stack analyzer\n│   ├── problem_creator_agent.py # Problem generator\n│   └── qa_validator_agent.py  # Quality validator\n├── utils/                     # Utilities\n│   ├── github_mcp.py          # GitHub API integration\n│   ├── a2a_protocol.py        # A2A protocol implementation\n│   ├── monitoring.py          # Performance monitoring\n│   └── json_parser.py         # Robust JSON parsing\n├── final_docs/                # Complete documentation\n└── requirements.txt           # Python dependencies\n```\n\n---\n\n## 🎨 Key Innovations\n\n### 1. Multi-Agent A2A Protocol\n\nFirst production implementation of Google's A2A protocol with 7 specialized agents communicating seamlessly.\n\n### 2. Repository-Specific Problems\n\nUnlike generic platforms, problems are tailored to:\n- Actual tech stack used\n- Real code patterns found\n- Specific weaknesses identified\n- Genuine opportunities discovered\n\n### 3. Single-Pass Optimization\n\nOptimized from 3-loop analysis to single-pass:\n- **2x faster** generation\n- **66% fewer API calls**\n- Same quality output\n\n### 4. Quality Assurance\n\nBuilt-in QA agent validates on 4 dimensions:\n- Feasibility (time, context, dependencies)\n- Quality (clarity, testability)\n- Technical (stack match, patterns)\n- Educational (skill assessment value)\n\n---\n\n## 🛠️ Development\n\n### Running Tests\n\n```bash\n# Test GitHub connection\npython test_github_connection.py\n\n# Test with your repository\npython test_my_repo.py\n\n# Verify setup\n./verify_setup.sh\n```\n\n### Key Scripts\n\n- `cli_runner.py` - Main CLI application\n- `test_github_connection.py` - GitHub API tester\n- `test_my_repo.py` - Repository-specific tester\n- `verify_setup.sh` - Environment checker\n\n---\n\n## 🔐 Security\n\n- ✅ No tokens in code or repository\n- ✅ Environment variables for secrets\n- ✅ .gitignore for sensitive files\n- ✅ Service account keys excluded\n- ✅ API rate limiting handled\n\n---\n\n## 📝 Contributing\n\nThis project was built for the Google AI Hackathon showcasing:\n- Google Gemini 2.5 Pro/Flash\n- Vertex AI integration\n- A2A Protocol implementation\n- Multi-agent architecture\n\n---\n\n## 📄 License\n\nMIT License - See LICENSE file for details\n\n---\n\n## 🙏 Acknowledgments\n\n- **Google Vertex AI** - For powerful AI models\n- **Google ADK** - For agent development framework\n- **A2A Protocol** - For agent interoperability\n\n---\n\n## 📞 Contact\n\n**Murat Can Koylan**\n- GitHub: [@muratcankoylan](https://github.com/muratcankoylan)\n- Repository: [actual_code](https://github.com/muratcankoylan/actual_code)\n\n---\n\n## 🚀 Get Started Now!\n\n```bash\ngit clone https://github.com/muratcankoylan/actual_code.git\ncd actual_code\npip install -r requirements.txt\npython cli_runner.py\n```\n\n**Generate your first AI-powered coding assessment in 2 minutes!** 🎉\n",
        "plugins/book-training/README.md": "# Book Training: Style Transfer via SFT\n\nA complete pipeline for training language models to write in a specific author's style using Supervised Fine-Tuning (SFT) with LoRA.\n\n## Project Overview\n\nThis repository demonstrates how to:\n1. Extract and segment text from EPUB books\n2. Generate diverse instruction-response pairs using LLMs\n3. Train a small model (Qwen3-8B-Base) with LoRA\n4. Validate genuine style transfer vs. memorization\n\n## Results\n\nTrained on Gertrude Stein's \"Three Lives\" (1909):\n\n| Metric | Value |\n|--------|-------|\n| Training Examples | 591 |\n| Final Test Loss | 213 (from 7,584) |\n| Loss Reduction | 97% |\n| Training Time | ~15 min |\n| AI Detector Score | ~50-70% Human (Pangram) |\n\nThe model applies Stein's style to modern scenarios not in training data, confirming style transfer rather than memorization.\n\n## Repository Structure\n\n```\n├── skills/book-sft-pipeline/     # Reusable skill for AI agents\n│   ├── SKILL.md                  # Main skill definition\n│   ├── references/               # Segmentation, training docs\n│   ├── scripts/                  # Example pipeline code\n│   └── examples/gertrude-stein/  # Complete case study\n├── src/\n│   ├── pipeline.ts               # Dataset generation pipeline\n│   └── train-tinker.ts           # Tinker training setup\n├── train_tinker.py               # LoRA training script\n├── chat.py                       # Interactive model testing\n├── test_style_transfer.py        # Style validation tests\n├── hf-dataset/                   # HuggingFace dataset files\n│   ├── train.jsonl               # 591 training examples\n│   ├── test.jsonl                # 49 test examples\n│   └── README.md                 # Dataset card\n└── three-lives-sft-dataset.jsonl # Full generated dataset\n```\n\n## Quick Start\n\n### 1. Generate Dataset\n\n```bash\nnpm install\nnpx tsx src/pipeline.ts\n```\n\n### 2. Train with Tinker\n\n```bash\npip install tinker python-dotenv\npython train_tinker.py\n```\n\n### 3. Test the Model\n\n```bash\npython chat.py\n```\n\n## Dataset\n\nAvailable on HuggingFace: [MuratcanKoylan/gertrude-stein-style-sft](https://huggingface.co/datasets/MuratcanKoylan/gertrude-stein-style-sft)\n\n```python\nfrom datasets import load_dataset\ndataset = load_dataset(\"MuratcanKoylan/gertrude-stein-style-sft\")\n```\n\n## Key Techniques\n\n### Prompt Diversity\n15 prompt templates × 5 system prompts = 75 unique combinations per chunk. Prevents overfitting to specific phrasings.\n\n### Fine-Grained Segmentation\n150-400 word chunks with overlap. Smaller chunks capture more stylistic patterns.\n\n### Modern Scenario Testing\nValidate style transfer by prompting about topics the author never wrote about (real estate offices, smartphones, etc.).\n\n## Skills for AI Agents\n\nThe `/skills/book-sft-pipeline` folder contains a complete skill definition that enables AI agents to replicate this pipeline for any book:\n\n```\nskills/book-sft-pipeline/\n├── SKILL.md              # Agent-readable instructions\n├── references/           # Technical documentation\n├── scripts/              # Example implementations\n└── examples/             # Case studies with results\n```\n\n## Sample Output\n\n**Prompt**: Write about a tech startup founder in the style of Gertrude Stein.\n\n**Output**:\n> She was always working, always she was working on her startup. The investors they would come and they would go and she would talk to them about the product. She was a good founder, always she was a good founder. She had this way of explaining things, simple and direct, and the investors they would listen.\n\n## License\n\nMIT\n\nSource text \"Three Lives\" by Gertrude Stein (1909) is in the public domain.\n\n",
        "plugins/linkedin-analyzer/README.md": "# LinkedIn Profile Insight Tool\n\n**Creator**: Muratcan Koylan [Profile Link](https://twitter.com/koylanai)\n\n## Overview\n\nThe LinkedIn Profile Insight Tool is designed to leverage advanced AI models, such as Cohere Command R+, to provide detailed analysis and insights into LinkedIn profiles. This tool extracts data from LinkedIn profiles and posts, offering users a comprehensive look at professional behaviors, trends, and potential growth areas.\n\n## Features\n\n- **User Detail Extraction**: Retrieves detailed information from LinkedIn profiles, such as roles, experiences, and recent activities.\n- **Post Analysis**: Analyzes LinkedIn posts to determine engagement levels, areas of expertise, and prevalent topics.\n- **Professional Insights**: Generates insights into a user's professional interests and industry influence.\n- **Engagement Metrics**: Assesses the impact of LinkedIn activities through engagement metrics like likes, comments, and shares.\n\n## Workflow\n\n1. **Data Retrieval**: The tool fetches user details and posts from LinkedIn.\n2. **Data Analysis**: Applies AI models to analyze the data.\n3. **Report Generation**: Outputs a structured report detailing insights and recommendations for professional development.\n\n## Setup\n\nEnsure you have valid API keys for LinkedIn and OpenRouter's AI services. Insert these keys where indicated in the code.\n\n## Usage\n\n1. Open `linkedin_profile_insight.ipynb` in Google Colab.\n2. Input your API keys where indicated.\n3. Execute the notebook cells sequentially.\n4. Enter the LinkedIn profile URL when prompted.\n5. Review the comprehensive report generated by the tool.\n\n## Security and Compliance\n\n- **API Key Security**: Store your API keys securely to prevent unauthorized access.\n- **Data Privacy**: Adhere to relevant data protection laws, such as GDPR, when handling personal data.\n\n## Disclaimer\n\nThis tool is intended for educational and informational purposes only. It is not a substitute for professional career advice. Always respect privacy and use judgment when analyzing LinkedIn profiles.\n\n## License\n\nThis project is open-sourced under the MIT License which allows for free use and modification with proper attribution.\n\n## Contributing\n\nContributions are welcome! Enhance functionality, address bugs, or suggest improvements by:\n- **Submitting Pull Requests**: Contribute directly to code enhancements or fixes.\n- **Reporting Issues**: Provide feedback and suggest new features or enhancements.\n\n## Contact\n\nFor further information, collaboration, or inquiries, you can reach me directly on LinkedIn:\n\n- **Muratcan Koylan**: [Profile Link](https://twitter.com/koylanai)\n\n## Additional Resources\n\nWe can use this tool to enhance our agentic workflows, let me know if you want to collaborate.\n",
        "plugins/feed2context/README.md": "# Feed2Context\n\nOne‑click feed → report for LinkedIn and X. Capture a post, auto‑shape a research query, search with reasoning, and get a detailed research report in seconds.\n\n- LinkedIn extraction: Groq Compound Mini visits the URL and returns only the post text (fallback to extension DOM capture).\n- X extraction: Browser Use drives a real browser to read the post (X blocks scraping reliably).\n- Query building: Kimi‑K2 compresses the goal into a single effective search query.\n- Research and answer: Groq Compound searches and reasons to produce an instant research report.\n\n<img width=\"436\" height=\"482\" alt=\"Screenshot 2025-09-08 at 10 11 36 PM\" src=\"https://github.com/user-attachments/assets/082afb4b-89dc-48e8-b840-9a358283b4bc\" />\n\n\nGroq Compound: https://groq.com/blog/introducing-the-next-generation-of-compound-on-groqcloud\n\n## Quick start\n\n1) Create a virtual env and install dependencies\n\n```bash\npython -m venv .venv && source .venv/bin/activate\npip install -r requirements.txt\n# For the X pipeline, install Browser Use as well (optional for LinkedIn):\npip install browser-use\n```\n\n2) Set environment variables\n\nCreate a `.env` file (sample – do not commit real secrets):\n\n```\nGROQ_API_KEY=sk_...\nOPENAI_API_KEY=sk_...\nCOMPOSIO_API_KEY=sk_...\nCOMPOSIO_AUTH_CONFIG_ID=ac_...\nCOMPOSIO_EXTERNAL_USER_ID=user_123\nREPORTS_EMAIL_TO=recipient@example.com\n```\n\n3) Run the API (serves both LinkedIn and X pipelines)\n\n```bash\npython app2.py\n# app2 serves both ports via the same FastAPI app:\n#   - X pipeline UI/API at        http://127.0.0.1:8000/\n#   - LinkedIn pipeline UI/API at http://127.0.0.1:8001/\n# Both show the same notebook UI and read the same saved data.\n```\n\nOpen either port in the browser to see the minimal “notebook” UI which loads reports from `/reports` and renders the latest answer.\n\n## Why Browser Use for X\n\nX employs strong anti‑scraping measures that can break static HTTP fetches. The X pipeline uses `browser_use.Agent` with `ChatGroq` to control a real browser, load the tweet, click if needed, and extract the author + main tweet text reliably.\n\nLinkedIn extraction does not need a local browser and uses `groq/compound-mini` to visit the URL and return only the core post text.\n\n## Architecture\n\nTop‑level components:\n\n- `app2.py` — FastAPI server implementing both pipelines and serving a tiny UI\n- `extension/` — Chrome extension (Manifest V3) injecting the action button on LinkedIn and X\n- `data/reports.jsonl` — Local append‑only JSONL store for all results\n\n### Server (`app2.py`)\n\nEndpoints:\n\n- `GET /` — minimal, client‑side UI that lists and renders saved reports\n- `GET /reports` — returns the latest saved reports as JSON (up to 200)\n- `POST /trigger` — accepts `{ url, note }`, detects the source, runs the pipeline, and persists the result\n\nSource detection:\n\n- `linkedin` if the URL contains `linkedin.com`\n- `x` if it contains `x.com` or `twitter.com`\n- `unknown` otherwise\n\nPipeline by source:\n\n- X (`x`)\n  - `browser_use.Agent` + `ChatGroq` opens the tweet and extracts:\n    - Author display name\n    - Main tweet text (optionally a brief media description)\n  - Output is plain text, fed to query building\n\n- LinkedIn (`linkedin`)\n  - `groq/compound-mini` (Compound Mini) visits the URL and returns:\n    - `{ \"post_text\": \"...\" }` — only the post text, with “see more” expanded when possible\n\nThen for both:\n\n1) Query building with `moonshotai/kimi-k2-instruct` → returns `{ \"query\": \"...\" }`\n2) Research with `groq/compound` (streamed, buffered server‑side) → final `compound_answer` (Markdown)\n3) Persist to `data/reports.jsonl`:\n\n```json\n{\n  \"timestamp\": \"2024-01-01T00:00:00Z\",\n  \"post_url\": \"...\",\n  \"user_note\": \"...\",\n  \"source\": \"x | linkedin | unknown\",\n  \"post_text\": \"...\",               \n  \"query\": \"...\",\n  \"compound_answer\": \"...\"\n}\n```\n\n### Extension (`extension/`)\n\n- `manifest.json` — permissions for `x.com`, `linkedin.com`, and local hosts `127.0.0.1:8000/8001`\n- `content.js` — injects a “Save + Analyze” button on X posts; sends messages to the background\n- `content_linkedin.js` — injects a “Save + Analyze” button on LinkedIn posts, resolves stable permalinks, extracts visible post text, and captures author name + profile URL\n- `background.js` — calls the local API:\n  - `http://127.0.0.1:8000/trigger` for X (`TRIGGER_ANALYSIS`)\n  - `http://127.0.0.1:8001/trigger` for LinkedIn (`TRIGGER_ANALYSIS_LI`)\n\nAfter clicking the button on a post, you’ll be prompted for a short note. The extension sends `{ url, note }` (and when available `{ raw_text, author_name, author_url }`) to the local server, which runs the pipeline and saves the report.\n\n## Prompts \n\nExtraction prompt for Compound Mini (LinkedIn and X fallback):\n\n```text\nYou are PostExtractor. Visit the given social post URL (LinkedIn or X) and return ONLY the main post text.\nInput: The user will provide a URL directly.\nRules:\n- Return ONLY JSON: {\"post_text\": \"...\"}\n- Exclude reactions, counts, and comments; include text from 'see more' / collapsed content if applicable\n- If the page is not directly accessible, infer the gist from any preview/snippet and user-visible text\n- No markdown, no extra text\n```\n\nQuery builder prompt (Kimi‑K2):\n\n```text\nYou are QueryBuilder. Build one detailed but concise research query from the extracted post text and user note.\nInputs:\n- post_text: extracted social post text (LinkedIn or X)\n- user_note: user's intent\nProcess:\n- Identify entities and intent from post_text and user_note\n- Compose a single concise research query (<= 50 words)\nOutput:\nReturn ONLY JSON: {\"query\": \"...\"}\nNo markdown or extra text.\n```\n\nNote: The X pipeline uses `browser_use.Agent` with a speed‑optimized `BrowserProfile` (short waits, `headless=False`) and a `ChatGroq` model to extract author + tweet text from the actual page.\n\n### Email formatting and delivery (Composio)\n\n- The server formats each report’s Markdown answer into an email body using OpenAI (default model `gpt-5`).\n- Tables are rewritten into headings, short paragraphs, and bullet lists; no `<table>` tags are used.\n- Composio’s Gmail toolkit sends the email to `REPORTS_EMAIL_TO`. You can reuse an existing connection via `COMPOSIO_CONNECTED_ACCOUNT_ID`, or run `python composio/gmail_demo.py` once to approve OAuth and cache it locally.\n\n## API\n\nBase URLs (served by the same app instance):\n\n- X: `http://127.0.0.1:8000`\n- LinkedIn: `http://127.0.0.1:8001`\n\nRoutes:\n\n- `GET /` — minimal notebook UI\n- `GET /reports` — list of recent reports (JSON array)\n- `POST /trigger` — body `{ \"url\": \"...\", \"note\": \"...\" }`\n\nExamples:\n\n```bash\n# LinkedIn example\ncurl -s http://127.0.0.1:8001/trigger \\\n  -H 'Content-Type: application/json' \\\n  -d '{\"url\":\"https://www.linkedin.com/feed/update/urn:li:activity:...\",\"note\":\"analyze the company and founders\"}' | jq .\n\n# X example\ncurl -s http://127.0.0.1:8000/trigger \\\n  -H 'Content-Type: application/json' \\\n  -d '{\"url\":\"https://x.com/username/status/1234567890123456789\",\"note\":\"context and risks?\"}' | jq .\n```\n\n## Data and UI\n\n- Data is persisted as JSONL at `data/reports.jsonl` (append‑only)\n- The home page fetches `/reports` and renders the latest answer; click items in the left list to switch\n\n## Load the Chrome extension\n\nSteps (Manifest V3):\n\n1. Open Chrome → `chrome://extensions/`\n2. Enable “Developer mode” (top‑right)\n3. Click “Load unpacked” → select the `extension/` folder\n4. Ensure the server is running (`python app2.py`):\n   - `http://127.0.0.1:8000/` (X)\n   - `http://127.0.0.1:8001/` (LinkedIn)\n5. Open LinkedIn and X in new tabs; you should see a “Save + Analyze” button on posts\n\nNotes:\n\n- Permissions: `activeTab`, `scripting`, `storage`, hosts for X, LinkedIn, and `127.0.0.1`\n- After editing files in `extension/`, click the refresh icon next to the extension in `chrome://extensions/`\n\n## Troubleshooting\n\n- UI says “Loading…”\n  - Ensure the server is running on 8000/8001; hard refresh the page (Cmd+Shift+R)\n\n- Extension alerts “Failed to send”\n  - Verify ports: X calls 8000, LinkedIn calls 8001\n  - Confirm host permissions in `extension/manifest.json`\n\n- GROQ key errors or empty results\n  - Ensure `.env` contains `GROQ_API_KEY` and the key is valid\n  - Some features require “latest” model headers (handled in `app2.py`)\n\n- X pipeline not extracting\n  - Install `browser-use` (`pip install browser-use`)\n  - A real browser window will open (`headless=False`); keep it visible until extraction finishes\n\n- LinkedIn permalink not detected\n  - Click the post’s timestamp/permalink, then use the button again (the content script prefers stable URNs)\n\n## Notes\n\n- Built with Groq Compound and Kimi‑K2.\n- Local by default: the extension posts to `127.0.0.1` and the server stores results in `data/reports.jsonl`.\n- Answers are concise by design, optimized for speed and relevance.\n\n## License\n\nMIT\n"
      },
      "plugins": [
        {
          "name": "nano-banana",
          "source": "./plugins/nano-banana",
          "description": "Image generation using Google's Gemini API",
          "version": "1.0.0",
          "author": {
            "name": "Eric Grill"
          },
          "category": "ai",
          "categories": [
            "ai"
          ],
          "install_commands": [
            "/plugin marketplace add EricGrill/agents-skills-plugins",
            "/plugin install nano-banana@agents-skills-plugins"
          ]
        },
        {
          "name": "mcp-proxmox-admin",
          "source": "./plugins/mcp-proxmox-admin",
          "description": "Proxmox VE infrastructure management via MCP with VM, container, and snapshot control",
          "version": "1.0.0",
          "author": {
            "name": "Eric Grill"
          },
          "category": "devops",
          "categories": [
            "devops"
          ],
          "install_commands": [
            "/plugin marketplace add EricGrill/agents-skills-plugins",
            "/plugin install mcp-proxmox-admin@agents-skills-plugins"
          ]
        },
        {
          "name": "mcp-multi-agent-ssh",
          "source": "./plugins/mcp-multi-agent-ssh",
          "description": "Persistent SSH connections with encrypted credential storage and SFTP support",
          "version": "1.0.0",
          "author": {
            "name": "Eric Grill"
          },
          "category": "devops",
          "categories": [
            "devops"
          ],
          "install_commands": [
            "/plugin marketplace add EricGrill/agents-skills-plugins",
            "/plugin install mcp-multi-agent-ssh@agents-skills-plugins"
          ]
        },
        {
          "name": "mcp-kali-orchestration",
          "source": "./plugins/mcp-kali-orchestration",
          "description": "Kali Linux orchestration with 50+ security tools for authorized pentesting",
          "version": "1.0.0",
          "author": {
            "name": "Eric Grill"
          },
          "category": "security",
          "categories": [
            "security"
          ],
          "install_commands": [
            "/plugin marketplace add EricGrill/agents-skills-plugins",
            "/plugin install mcp-kali-orchestration@agents-skills-plugins"
          ]
        },
        {
          "name": "mcp-multi-agent-server-delegation",
          "source": "./plugins/mcp-multi-agent-server-delegation",
          "description": "Task delegation to isolated Proxmox VMs with automatic cleanup and callbacks",
          "version": "1.0.0",
          "author": {
            "name": "Eric Grill"
          },
          "category": "devops",
          "categories": [
            "devops"
          ],
          "install_commands": [
            "/plugin marketplace add EricGrill/agents-skills-plugins",
            "/plugin install mcp-multi-agent-server-delegation@agents-skills-plugins"
          ]
        },
        {
          "name": "mcp-predictive-market",
          "source": "./plugins/mcp-predictive-market",
          "description": "Query 5 prediction markets with arbitrage detection and comparative analysis",
          "version": "1.0.0",
          "author": {
            "name": "Eric Grill"
          },
          "category": "ai",
          "categories": [
            "ai"
          ],
          "install_commands": [
            "/plugin marketplace add EricGrill/agents-skills-plugins",
            "/plugin install mcp-predictive-market@agents-skills-plugins"
          ]
        },
        {
          "name": "mcp-bitcoin-cli",
          "source": "./plugins/mcp-bitcoin-cli",
          "description": "Bitcoin OP_RETURN operations for documents, timestamps, and BRC-20 tokens",
          "version": "1.0.0",
          "author": {
            "name": "Eric Grill"
          },
          "category": "development",
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add EricGrill/agents-skills-plugins",
            "/plugin install mcp-bitcoin-cli@agents-skills-plugins"
          ]
        },
        {
          "name": "mcp-civic-data",
          "source": "./plugins/mcp-civic-data",
          "description": "Access 7 government APIs for weather, census, NASA, and economic data",
          "version": "1.0.0",
          "author": {
            "name": "Eric Grill"
          },
          "category": "productivity",
          "categories": [
            "productivity"
          ],
          "install_commands": [
            "/plugin marketplace add EricGrill/agents-skills-plugins",
            "/plugin install mcp-civic-data@agents-skills-plugins"
          ]
        },
        {
          "name": "mcp-memvid-state-service",
          "source": "./plugins/mcp-memvid-state-service",
          "description": "AI memory layer with vector search, full-text search, and temporal queries",
          "version": "1.0.0",
          "author": {
            "name": "Eric Grill"
          },
          "category": "ai",
          "categories": [
            "ai"
          ],
          "install_commands": [
            "/plugin marketplace add EricGrill/agents-skills-plugins",
            "/plugin install mcp-memvid-state-service@agents-skills-plugins"
          ]
        },
        {
          "name": "superpowers",
          "source": "./plugins/superpowers",
          "description": "Core skills library: TDD, debugging, collaboration patterns",
          "version": "4.1.0",
          "author": {
            "name": "Jesse Vincent"
          },
          "category": "development",
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add EricGrill/agents-skills-plugins",
            "/plugin install superpowers@agents-skills-plugins"
          ]
        },
        {
          "name": "agent-orchestration",
          "source": "./plugins/agent-orchestration",
          "description": "Context management and multi-agent orchestration",
          "version": "1.0.0",
          "author": {
            "name": "wshobson"
          },
          "category": "ai",
          "categories": [
            "ai"
          ],
          "install_commands": [
            "/plugin marketplace add EricGrill/agents-skills-plugins",
            "/plugin install agent-orchestration@agents-skills-plugins"
          ]
        },
        {
          "name": "blockchain-web3",
          "source": "./plugins/blockchain-web3",
          "description": "Blockchain development with Solidity security, DeFi, NFTs",
          "version": "1.0.0",
          "author": {
            "name": "wshobson"
          },
          "category": "development",
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add EricGrill/agents-skills-plugins",
            "/plugin install blockchain-web3@agents-skills-plugins"
          ]
        },
        {
          "name": "business-analytics",
          "source": "./plugins/business-analytics",
          "description": "Business analysis with data storytelling and KPI dashboards",
          "version": "1.0.0",
          "author": {
            "name": "wshobson"
          },
          "category": "productivity",
          "categories": [
            "productivity"
          ],
          "install_commands": [
            "/plugin marketplace add EricGrill/agents-skills-plugins",
            "/plugin install business-analytics@agents-skills-plugins"
          ]
        },
        {
          "name": "code-documentation",
          "source": "./plugins/code-documentation",
          "description": "Code documentation with automated doc generation",
          "version": "1.0.0",
          "author": {
            "name": "wshobson"
          },
          "category": "development",
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add EricGrill/agents-skills-plugins",
            "/plugin install code-documentation@agents-skills-plugins"
          ]
        },
        {
          "name": "comprehensive-review",
          "source": "./plugins/comprehensive-review",
          "description": "Comprehensive code review with architecture and security",
          "version": "1.0.0",
          "author": {
            "name": "wshobson"
          },
          "category": "development",
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add EricGrill/agents-skills-plugins",
            "/plugin install comprehensive-review@agents-skills-plugins"
          ]
        },
        {
          "name": "seo-analysis-monitoring",
          "source": "./plugins/seo-analysis-monitoring",
          "description": "SEO analysis with authority building and content refresh",
          "version": "1.0.0",
          "author": {
            "name": "wshobson"
          },
          "category": "marketing",
          "categories": [
            "marketing"
          ],
          "install_commands": [
            "/plugin marketplace add EricGrill/agents-skills-plugins",
            "/plugin install seo-analysis-monitoring@agents-skills-plugins"
          ]
        },
        {
          "name": "seo-content-creation",
          "source": "./plugins/seo-content-creation",
          "description": "SEO content creation with auditing and planning",
          "version": "1.0.0",
          "author": {
            "name": "wshobson"
          },
          "category": "marketing",
          "categories": [
            "marketing"
          ],
          "install_commands": [
            "/plugin marketplace add EricGrill/agents-skills-plugins",
            "/plugin install seo-content-creation@agents-skills-plugins"
          ]
        },
        {
          "name": "seo-technical-optimization",
          "source": "./plugins/seo-technical-optimization",
          "description": "Technical SEO with keyword strategy and meta optimization",
          "version": "1.0.0",
          "author": {
            "name": "wshobson"
          },
          "category": "marketing",
          "categories": [
            "marketing"
          ],
          "install_commands": [
            "/plugin marketplace add EricGrill/agents-skills-plugins",
            "/plugin install seo-technical-optimization@agents-skills-plugins"
          ]
        },
        {
          "name": "team-collaboration",
          "source": "./plugins/team-collaboration",
          "description": "Team collaboration with DX optimization and standups",
          "version": "1.0.0",
          "author": {
            "name": "wshobson"
          },
          "category": "productivity",
          "categories": [
            "productivity"
          ],
          "install_commands": [
            "/plugin marketplace add EricGrill/agents-skills-plugins",
            "/plugin install team-collaboration@agents-skills-plugins"
          ]
        },
        {
          "name": "unit-testing",
          "source": "./plugins/unit-testing",
          "description": "Unit testing with debugging and test automation",
          "version": "1.0.0",
          "author": {
            "name": "wshobson"
          },
          "category": "development",
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add EricGrill/agents-skills-plugins",
            "/plugin install unit-testing@agents-skills-plugins"
          ]
        },
        {
          "name": "python-development",
          "source": "./plugins/python-development",
          "description": "Python development with Django, FastAPI, async patterns",
          "version": "1.0.0",
          "author": {
            "name": "wshobson"
          },
          "category": "development",
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add EricGrill/agents-skills-plugins",
            "/plugin install python-development@agents-skills-plugins"
          ]
        },
        {
          "name": "llm-application-dev",
          "source": "./plugins/llm-application-dev",
          "description": "LLM application development with RAG and embeddings",
          "version": "1.0.0",
          "author": {
            "name": "wshobson"
          },
          "category": "ai",
          "categories": [
            "ai"
          ],
          "install_commands": [
            "/plugin marketplace add EricGrill/agents-skills-plugins",
            "/plugin install llm-application-dev@agents-skills-plugins"
          ]
        },
        {
          "name": "javascript-typescript",
          "source": "./plugins/javascript-typescript",
          "description": "JavaScript and TypeScript development with modern patterns",
          "version": "1.0.0",
          "author": {
            "name": "wshobson"
          },
          "category": "development",
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add EricGrill/agents-skills-plugins",
            "/plugin install javascript-typescript@agents-skills-plugins"
          ]
        },
        {
          "name": "git-pr-workflows",
          "source": "./plugins/git-pr-workflows",
          "description": "Git and PR workflows with code review and onboarding",
          "version": "1.0.0",
          "author": {
            "name": "wshobson"
          },
          "category": "development",
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add EricGrill/agents-skills-plugins",
            "/plugin install git-pr-workflows@agents-skills-plugins"
          ]
        },
        {
          "name": "game-development",
          "source": "./plugins/game-development",
          "description": "Game development with Unity, Godot, and Minecraft",
          "version": "1.0.0",
          "author": {
            "name": "wshobson"
          },
          "category": "development",
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add EricGrill/agents-skills-plugins",
            "/plugin install game-development@agents-skills-plugins"
          ]
        },
        {
          "name": "full-stack-orchestration",
          "source": "./plugins/full-stack-orchestration",
          "description": "Full-stack orchestration with deployment and security",
          "version": "1.0.0",
          "author": {
            "name": "wshobson"
          },
          "category": "development",
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add EricGrill/agents-skills-plugins",
            "/plugin install full-stack-orchestration@agents-skills-plugins"
          ]
        },
        {
          "name": "content-marketing",
          "source": "./plugins/content-marketing",
          "description": "Content marketing with strategy and search agents",
          "version": "1.0.0",
          "author": {
            "name": "wshobson"
          },
          "category": "marketing",
          "categories": [
            "marketing"
          ],
          "install_commands": [
            "/plugin marketplace add EricGrill/agents-skills-plugins",
            "/plugin install content-marketing@agents-skills-plugins"
          ]
        },
        {
          "name": "context-management",
          "source": "./plugins/context-management",
          "description": "Context management with save and restore capabilities",
          "version": "1.0.0",
          "author": {
            "name": "wshobson"
          },
          "category": "productivity",
          "categories": [
            "productivity"
          ],
          "install_commands": [
            "/plugin marketplace add EricGrill/agents-skills-plugins",
            "/plugin install context-management@agents-skills-plugins"
          ]
        },
        {
          "name": "customer-sales-automation",
          "source": "./plugins/customer-sales-automation",
          "description": "Customer support and sales automation agents",
          "version": "1.0.0",
          "author": {
            "name": "wshobson"
          },
          "category": "productivity",
          "categories": [
            "productivity"
          ],
          "install_commands": [
            "/plugin marketplace add EricGrill/agents-skills-plugins",
            "/plugin install customer-sales-automation@agents-skills-plugins"
          ]
        },
        {
          "name": "database-design",
          "source": "./plugins/database-design",
          "description": "Database architecture and SQL optimization",
          "version": "1.0.0",
          "author": {
            "name": "wshobson"
          },
          "category": "development",
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add EricGrill/agents-skills-plugins",
            "/plugin install database-design@agents-skills-plugins"
          ]
        },
        {
          "name": "data-validation-suite",
          "source": "./plugins/data-validation-suite",
          "description": "Data validation and backend security coding",
          "version": "1.0.0",
          "author": {
            "name": "wshobson"
          },
          "category": "development",
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add EricGrill/agents-skills-plugins",
            "/plugin install data-validation-suite@agents-skills-plugins"
          ]
        },
        {
          "name": "deployment-strategies",
          "source": "./plugins/deployment-strategies",
          "description": "Deployment engineering with Terraform and IaC",
          "version": "1.0.0",
          "author": {
            "name": "wshobson"
          },
          "category": "devops",
          "categories": [
            "devops"
          ],
          "install_commands": [
            "/plugin marketplace add EricGrill/agents-skills-plugins",
            "/plugin install deployment-strategies@agents-skills-plugins"
          ]
        },
        {
          "name": "developer-essentials",
          "source": "./plugins/developer-essentials",
          "description": "Essential developer skills for monorepos and debugging",
          "version": "1.0.0",
          "author": {
            "name": "wshobson"
          },
          "category": "development",
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add EricGrill/agents-skills-plugins",
            "/plugin install developer-essentials@agents-skills-plugins"
          ]
        },
        {
          "name": "documentation-generation",
          "source": "./plugins/documentation-generation",
          "description": "Documentation generation with API docs and diagrams",
          "version": "1.0.0",
          "author": {
            "name": "wshobson"
          },
          "category": "development",
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add EricGrill/agents-skills-plugins",
            "/plugin install documentation-generation@agents-skills-plugins"
          ]
        },
        {
          "name": "frontend-mobile-development",
          "source": "./plugins/frontend-mobile-development",
          "description": "Frontend and mobile development with React and Tailwind",
          "version": "1.0.0",
          "author": {
            "name": "wshobson"
          },
          "category": "development",
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add EricGrill/agents-skills-plugins",
            "/plugin install frontend-mobile-development@agents-skills-plugins"
          ]
        },
        {
          "name": "frontend-mobile-security",
          "source": "./plugins/frontend-mobile-security",
          "description": "Frontend and mobile security with XSS scanning",
          "version": "1.0.0",
          "author": {
            "name": "wshobson"
          },
          "category": "security",
          "categories": [
            "security"
          ],
          "install_commands": [
            "/plugin marketplace add EricGrill/agents-skills-plugins",
            "/plugin install frontend-mobile-security@agents-skills-plugins"
          ]
        },
        {
          "name": "awesome-claude-skills",
          "source": "./plugins/awesome-claude-skills",
          "description": "27 practical Claude Skills for documents and productivity",
          "version": "1.0.0",
          "author": {
            "name": "ComposioHQ"
          },
          "category": "productivity",
          "categories": [
            "productivity"
          ],
          "install_commands": [
            "/plugin marketplace add EricGrill/agents-skills-plugins",
            "/plugin install awesome-claude-skills@agents-skills-plugins"
          ]
        },
        {
          "name": "ios-simulator-skill",
          "source": "./plugins/ios-simulator-skill",
          "description": "iOS Simulator automation with 21 scripts for testing",
          "version": "1.0.0",
          "author": {
            "name": "conorluddy"
          },
          "category": "development",
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add EricGrill/agents-skills-plugins",
            "/plugin install ios-simulator-skill@agents-skills-plugins"
          ]
        },
        {
          "name": "multi-agent-patterns",
          "source": "./plugins/multi-agent-patterns",
          "description": "Multi-agent architecture patterns for context isolation",
          "version": "1.0.0",
          "author": {
            "name": "muratcankoylan"
          },
          "category": "ai",
          "categories": [
            "ai"
          ],
          "install_commands": [
            "/plugin marketplace add EricGrill/agents-skills-plugins",
            "/plugin install multi-agent-patterns@agents-skills-plugins"
          ]
        },
        {
          "name": "beautiful-prose",
          "source": "./plugins/beautiful-prose",
          "description": "Hard-edged writing style skill for forceful prose",
          "version": "1.0.0",
          "author": {
            "name": "SHADOWPR0"
          },
          "category": "productivity",
          "categories": [
            "productivity"
          ],
          "install_commands": [
            "/plugin marketplace add EricGrill/agents-skills-plugins",
            "/plugin install beautiful-prose@agents-skills-plugins"
          ]
        },
        {
          "name": "ralph-wiggum-marketer",
          "source": "./plugins/ralph-wiggum-marketer",
          "description": "Autonomous AI copywriter for SaaS content marketing",
          "version": "1.0.0",
          "author": {
            "name": "muratcankoylan"
          },
          "category": "marketing",
          "categories": [
            "marketing"
          ],
          "install_commands": [
            "/plugin marketplace add EricGrill/agents-skills-plugins",
            "/plugin install ralph-wiggum-marketer@agents-skills-plugins"
          ]
        },
        {
          "name": "ai-investigator",
          "source": "./plugins/ai-investigator",
          "description": "Enterprise AI case study analyzer with Claude and Firecrawl",
          "version": "1.0.0",
          "author": {
            "name": "muratcankoylan"
          },
          "category": "ai",
          "categories": [
            "ai"
          ],
          "install_commands": [
            "/plugin marketplace add EricGrill/agents-skills-plugins",
            "/plugin install ai-investigator@agents-skills-plugins"
          ]
        },
        {
          "name": "rosetta-prompt",
          "source": "./plugins/rosetta-prompt",
          "description": "Prompt optimization for different AI providers using multi-agent ReAct",
          "version": "1.0.0",
          "author": {
            "name": "muratcankoylan"
          },
          "category": "ai",
          "categories": [
            "ai"
          ],
          "install_commands": [
            "/plugin marketplace add EricGrill/agents-skills-plugins",
            "/plugin install rosetta-prompt@agents-skills-plugins"
          ]
        },
        {
          "name": "readwren",
          "source": "./plugins/readwren",
          "description": "Multi-agent literary interview for extracting reading profiles",
          "version": "1.0.0",
          "author": {
            "name": "muratcankoylan"
          },
          "category": "productivity",
          "categories": [
            "productivity"
          ],
          "install_commands": [
            "/plugin marketplace add EricGrill/agents-skills-plugins",
            "/plugin install readwren@agents-skills-plugins"
          ]
        },
        {
          "name": "food-tour-planner",
          "source": "./plugins/food-tour-planner",
          "description": "AI food tour planner with DeepAgents and Google Maps",
          "version": "1.0.0",
          "author": {
            "name": "muratcankoylan"
          },
          "category": "productivity",
          "categories": [
            "productivity"
          ],
          "install_commands": [
            "/plugin marketplace add EricGrill/agents-skills-plugins",
            "/plugin install food-tour-planner@agents-skills-plugins"
          ]
        },
        {
          "name": "actual-code",
          "source": "./plugins/actual-code",
          "description": "7-agent code assessment generator from GitHub repos",
          "version": "1.0.0",
          "author": {
            "name": "muratcankoylan"
          },
          "category": "development",
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add EricGrill/agents-skills-plugins",
            "/plugin install actual-code@agents-skills-plugins"
          ]
        },
        {
          "name": "book-training",
          "source": "./plugins/book-training",
          "description": "Style transfer pipeline for author-style LLM training with LoRA",
          "version": "1.0.0",
          "author": {
            "name": "muratcankoylan"
          },
          "category": "ai",
          "categories": [
            "ai"
          ],
          "install_commands": [
            "/plugin marketplace add EricGrill/agents-skills-plugins",
            "/plugin install book-training@agents-skills-plugins"
          ]
        },
        {
          "name": "linkedin-analyzer",
          "source": "./plugins/linkedin-analyzer",
          "description": "LinkedIn profile analysis with Cohere Command R+",
          "version": "1.0.0",
          "author": {
            "name": "muratcankoylan"
          },
          "category": "productivity",
          "categories": [
            "productivity"
          ],
          "install_commands": [
            "/plugin marketplace add EricGrill/agents-skills-plugins",
            "/plugin install linkedin-analyzer@agents-skills-plugins"
          ]
        },
        {
          "name": "feed2context",
          "source": "./plugins/feed2context",
          "description": "One-click feed to research report for LinkedIn and X",
          "version": "1.0.0",
          "author": {
            "name": "muratcankoylan"
          },
          "category": "productivity",
          "categories": [
            "productivity"
          ],
          "install_commands": [
            "/plugin marketplace add EricGrill/agents-skills-plugins",
            "/plugin install feed2context@agents-skills-plugins"
          ]
        }
      ]
    }
  ]
}