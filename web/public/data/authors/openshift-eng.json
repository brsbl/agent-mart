{
  "author": {
    "id": "openshift-eng",
    "display_name": "OpenShift Eng",
    "avatar_url": "https://avatars.githubusercontent.com/u/84759374?v=4"
  },
  "marketplaces": [
    {
      "name": "ai-helpers",
      "version": null,
      "description": "Git Plugin",
      "repo_full_name": "openshift-eng/ai-helpers",
      "repo_url": "https://github.com/openshift-eng/ai-helpers",
      "repo_description": "Developer productivity tools for Claude Code & other AI assistants",
      "signals": {
        "stars": 38,
        "forks": 207,
        "pushed_at": "2026-02-13T18:49:08Z"
      },
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"name\": \"ai-helpers\",\n  \"owner\": {\n    \"name\": \"openshift-eng\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"git\",\n      \"source\": \"./plugins/git\",\n      \"description\": \"Git Plugin\",\n      \"version\": \"0.0.4\"\n    },\n    {\n      \"name\": \"hello-world\",\n      \"source\": \"./plugins/hello-world\",\n      \"description\": \"Hello World Plugin\",\n      \"version\": \"1.0.1\"\n    },\n    {\n      \"name\": \"jira\",\n      \"source\": \"./plugins/jira\",\n      \"description\": \"A plugin to automate tasks with Jira\",\n      \"version\": \"0.3.1\"\n    },\n    {\n      \"name\": \"ci\",\n      \"source\": \"./plugins/ci\",\n      \"description\": \"A plugin to work with OpenShift CI\",\n      \"version\": \"0.0.5\"\n    },\n    {\n      \"name\": \"teams\",\n      \"source\": \"./plugins/teams\",\n      \"description\": \"Team structure knowledge and health analysis commands for OpenShift teams\",\n      \"version\": \"0.0.2\"\n    },\n    {\n      \"name\": \"doc\",\n      \"source\": \"./plugins/doc\",\n      \"description\": \"A plugin for engineering documentation and notes\",\n      \"version\": \"0.0.2\"\n    },\n    {\n      \"name\": \"session\",\n      \"source\": \"./plugins/session\",\n      \"description\": \"A plugin for Claude session management and persistence\",\n      \"version\": \"0.0.2\"\n    },\n    {\n      \"name\": \"sosreport\",\n      \"source\": \"./plugins/sosreport\",\n      \"description\": \"Analyze sosreport archives for system diagnostics and troubleshooting\",\n      \"version\": \"0.0.2\"\n    },\n    {\n      \"name\": \"utils\",\n      \"source\": \"./plugins/utils\",\n      \"description\": \"A generic utilities plugin serving as a catch-all for various helper commands\",\n      \"version\": \"0.0.5\"\n    },\n    {\n      \"name\": \"olm\",\n      \"source\": \"./plugins/olm\",\n      \"description\": \"OLM (Operator Lifecycle Manager) plugin for operator management and debugging\",\n      \"version\": \"0.1.0\"\n    },\n    {\n      \"name\": \"olm-team\",\n      \"source\": \"./plugins/olm-team\",\n      \"description\": \"OLM team development utilities and onboarding tools\",\n      \"version\": \"0.2.0\"\n    },\n    {\n      \"name\": \"prow-job\",\n      \"source\": \"./plugins/prow-job\",\n      \"description\": \"A plugin to analyze and inspect Prow CI job results\",\n      \"version\": \"0.0.6\"\n    },\n    {\n      \"name\": \"agendas\",\n      \"source\": \"./plugins/agendas\",\n      \"description\": \"A plugin to create various meeting agendas\",\n      \"version\": \"0.0.2\"\n    },\n    {\n      \"name\": \"openshift\",\n      \"source\": \"./plugins/openshift\",\n      \"description\": \"OpenShift development utilities and helpers\",\n      \"version\": \"0.0.2\"\n    },\n    {\n      \"name\": \"etcd\",\n      \"source\": \"./plugins/etcd\",\n      \"description\": \"Etcd cluster health monitoring and performance analysis utilities\",\n      \"version\": \"0.0.2\"\n    },\n    {\n      \"name\": \"yaml\",\n      \"source\": \"./plugins/yaml\",\n      \"description\": \"YAML documentation and utilities\",\n      \"version\": \"0.0.2\"\n    },\n    {\n      \"name\": \"must-gather\",\n      \"source\": \"./plugins/must-gather\",\n      \"description\": \"A plugin to analyze and report on must-gather data\",\n      \"version\": \"0.0.2\"\n    },\n    {\n      \"name\": \"lvms\",\n      \"source\": \"./plugins/lvms\",\n      \"description\": \"LVMS (Logical Volume Manager Storage) plugin for troubleshooting and debugging storage issues\",\n      \"version\": \"0.1.0\"\n    },\n    {\n      \"name\": \"metrics\",\n      \"source\": \"./plugins/metrics\",\n      \"description\": \"Anonymous metrics usage for ai-helpers\",\n      \"version\": \"0.0.2\"\n    },\n    {\n      \"name\": \"hcp\",\n      \"source\": \"./plugins/hcp\",\n      \"description\": \"Generate HyperShift cluster creation commands via hcp CLI from natural language descriptions\",\n      \"version\": \"0.0.2\"\n    },\n    {\n      \"name\": \"compliance\",\n      \"source\": \"./plugins/compliance\",\n      \"description\": \"Security compliance and vulnerability analysis tools for Go projects\",\n      \"version\": \"0.0.2\"\n    },\n    {\n      \"name\": \"test-coverage\",\n      \"source\": \"./plugins/test-coverage\",\n      \"description\": \"Analyze test coverage and identify gaps in test scenarios\",\n      \"version\": \"0.0.2\"\n    },\n    {\n      \"name\": \"node-tuning\",\n      \"source\": \"./plugins/node-tuning\",\n      \"description\": \"Generate and analyze OpenShift node tuning profiles\",\n      \"version\": \"1.0.0\"\n    },\n    {\n      \"name\": \"origin\",\n      \"source\": \"./plugins/origin\",\n      \"description\": \"Helpers for openshift/origin development.\",\n      \"version\": \"0.0.2\"\n    },\n    {\n      \"name\": \"container-image\",\n      \"source\": \"./plugins/container-image\",\n      \"description\": \"Container image inspection and analysis using skopeo and podman\",\n      \"version\": \"0.0.2\"\n    },\n    {\n      \"name\": \"node\",\n      \"source\": \"./plugins/node\",\n      \"description\": \"Kubernetes and OpenShift node health monitoring and diagnostics\",\n      \"version\": \"0.0.2\"\n    },\n    {\n      \"name\": \"bigquery\",\n      \"source\": \"./plugins/bigquery\",\n      \"description\": \"BigQuery analysis utilities\",\n      \"version\": \"0.0.2\"\n    },\n    {\n      \"name\": \"workspaces\",\n      \"source\": \"./plugins/workspaces\",\n      \"description\": \"Manage isolated git worktree workspaces for multi-repo development\",\n      \"version\": \"1.0.0\"\n    },\n    {\n      \"name\": \"gwapi\",\n      \"source\": \"./plugins/gwapi\",\n      \"description\": \"Gateway API installation utilities for Kubernetes/OpenShift clusters\",\n      \"version\": \"0.0.1\"\n    },\n    {\n      \"name\": \"code-review\",\n      \"source\": \"./plugins/code-review\",\n      \"description\": \"Automated code quality review with language-aware analysis for pre-commit verification\",\n      \"version\": \"0.0.4\"\n    }\n  ]\n}\n",
        "README.md": "# AI Helpers\n\nA collection of Claude Code plugins to automate and assist with various development tasks.\n\n[Discover available plugins](https://openshift-eng.github.io/ai-helpers/)\n\n## Installation\n\n### From the Claude Code Plugin Marketplace\n\n1. **Add the marketplace:**\n   ```bash\n   /plugin marketplace add openshift-eng/ai-helpers\n   ```\n\n2. **Install a plugin:**\n   ```bash\n   /plugin install jira@ai-helpers\n   ```\n\n3. **Use the commands:**\n   ```bash\n   /jira:solve OCPBUGS-12345 origin\n   ```\n\n## Updating Plugins\n\nTo get the latest plugin versions:\n\n1. **Update the marketplace** (fetches latest plugin catalog):\n   ```bash\n   /plugin marketplace update ai-helpers\n   ```\n\n2. **Reinstall the plugin** (downloads new version):\n   ```bash\n   /plugin install <plugin>@ai-helpers\n   ```\n\n### Automatic Catalog Sync\n\nAdd a SessionStart hook to automatically sync the marketplace catalog on each session. In your project's `.claude/settings.json`:\n\n```json\n{\n  \"hooks\": {\n    \"SessionStart\": [\n      {\n        \"command\": \"claude plugin marketplace update ai-helpers\",\n        \"timeout\": 30000\n      }\n    ]\n  }\n}\n```\n\n**Note:** This only refreshes the catalog (what's available). To actually update an installed plugin to a newer version, you still need to reinstall it with `/plugin install <plugin>@ai-helpers`.\n\n### Using Cursor\n\nCursor is able to find the various commands defined in this repo by\nmaking it available inside your `~/.cursor/commands` directory.\n\n```\n$ mkdir -p ~/.cursor/commands\n$ git clone git@github.com:openshift-eng/ai-helpers.git\n$ ln -s ai-helpers ~/.cursor/commands/ai-helpers\n```\n\n## Using the Docker Container\n\nA container is available with Claude Code and all plugins pre-installed.\n\n### Building the Container\n\n```bash\npodman build -f images/Dockerfile -t ai-helpers .\n```\n\n### Running with Vertex AI and gcloud Authentication\n\nTo use Claude Code with Google Cloud's Vertex AI, you need to pass through your gcloud credentials and set the required environment variables:\n\n```bash\npodman run -it \\\n  -e CLAUDE_CODE_USE_VERTEX=1 \\\n  -e CLOUD_ML_REGION=your-ml-region \\\n  -e ANTHROPIC_VERTEX_PROJECT_ID=your-project-id \\\n  -v ~/.config/gcloud:/home/claude/.config/gcloud:ro \\\n  -v $(pwd):/workspace \\\n  -w /workspace \\\n  ai-helpers\n```\n\n**Environment Variables:**\n- `CLAUDE_CODE_USE_VERTEX=1` - Enable Vertex AI integration\n- `CLOUD_ML_REGION` - Your GCP region (e.g., `us-east5`)\n- `ANTHROPIC_VERTEX_PROJECT_ID` - Your GCP project ID\n\n**Volume Mounts:**\n- `-v ~/.config/gcloud:/home/claude/.config/gcloud:ro` - Passes through your gcloud authentication (read-only)\n- `-v $(pwd):/workspace` - Mounts your current directory into the container\n\n### Running Commands Non-Interactively\n\nYou can execute Claude Code commands directly without entering an interactive session using the `-p` or `--print` flag:\n\n```bash\npodman run -it \\\n  -e CLAUDE_CODE_USE_VERTEX=1 \\\n  -e CLOUD_ML_REGION=your-ml-region \\\n  -e ANTHROPIC_VERTEX_PROJECT_ID=your-project-id \\\n  -v ~/.config/gcloud:/home/claude/.config/gcloud:ro \\\n  -v $(pwd):/workspace \\\n  -w /workspace \\\n  ai-helpers \\\n  --print \"/hello-world:echo Hello from Claude Code!\"\n```\n\nThis will:\n1. Start the container with your gcloud credentials\n2. Execute the `/hello-world:echo` command with the provided message\n3. Print the response and exit when complete\n\n## Available Plugins\n\nFor a complete list of all available plugins and commands, see **[PLUGINS.md](PLUGINS.md)**.\n\n## Plugin Development\n\nWant to contribute or create your own plugins? Check out the `plugins/` directory for examples.\nMake sure your commands and agents follow the conventions for the Sections structure presented in the hello-world reference implementation plugin (see [`hello-world:echo`](plugins/hello-world/commands/echo.md) for an example).\n\n### Ethical Guidelines\n\nPlugins, commands, skills, and hooks must NEVER reference real people by name, even as stylistic examples (e.g., \"in the style of <specific human>\").\n\n**Ethical rationale:**\n1. **Consent**: Individuals have not consented to have their identity or persona used in AI-generated content\n2. **Misrepresentation**: AI cannot accurately replicate a person's unique voice, style, or intent\n3. **Intellectual Property**: A person's distinctive style may be protected\n4. **Dignity**: Using someone's identity without permission diminishes their autonomy\n\n**Instead, describe specific qualities explicitly**\n\nGood examples:\n\n* \"Write commit messages that are direct, technically precise, and focused on the rationale behind changes\"\n* \"Explain using clear analogies, a sense of wonder, and accessible language for non-experts\"\n* \"Code review comments that are encouraging, constructive, and focus on collaborative improvement\"\n\nWhen you identify a desirable characteristic (clarity, brevity, formality, humor, etc.), describe it explicitly rather than using a person as proxy.\n\n### Adding New Commands\n\n**Check for overlaps first** - Before coding, validate your idea:\n\n```bash\n/utils:review-ai-helpers-overlap --idea \"brief description of your command\"\n```\n\nCollaborating on existing work instead of duplicating parallel efforts is always encouraged when overlap is found. This helps maintain a clean, non-redundant plugin collection in such an actively developed project (see [`/utils:review-ai-helpers-overlap`](plugins/utils/commands/review-ai-helpers-overlap.md) for detailed usage).\n\nWhen contributing new commands:\n\n1. **If your command fits an existing plugin**: Add it to the appropriate plugin's `commands/` directory\n2. **If your command doesn't have a clear parent plugin**: Add it to the **utils plugin** (`plugins/utils/commands/`)\n   - The utils plugin serves as a catch-all for commands that don't fit existing categories\n   - Once we accumulate several related commands in utils, they can be segregated into a new targeted plugin\n\n### Creating a New Plugin\n\nIf you're contributing several related commands that warrant their own plugin:\n\n1. Create a new directory under `plugins/` with your plugin name\n2. Create the plugin structure:\n   ```\n   plugins/your-plugin/\n   ‚îú‚îÄ‚îÄ .claude-plugin/\n   ‚îÇ   ‚îî‚îÄ‚îÄ plugin.json\n   ‚îî‚îÄ‚îÄ commands/\n       ‚îî‚îÄ‚îÄ your-command.md\n   ```\n3. Register your plugin in `.claude-plugin/marketplace.json`\n\n### Validating Plugins\n\nThis repository uses [claudelint](https://github.com/stbenjam/claudelint) to validate plugin structure:\n\n```bash\nmake lint\n```\n\n### Updating Plugin Documentation\n\nAfter adding or modifying plugins, regenerate the PLUGINS.md file:\n\n```bash\nmake update\n```\n\nThis automatically scans all plugins and regenerates the complete plugin/command documentation in PLUGINS.md.\n\n## Additional Documentation\n\n- **[PLUGINS.md](PLUGINS.md)** - Complete list of all available plugins and commands\n- **[CONTRIBUTING.md](CONTRIBUTING.md)** - Guidelines for contributing plugins, including versioning policy\n- **[AGENTS.md](AGENTS.md)** - Complete guide for AI agents working with this repository\n- **[CLAUDE.md](CLAUDE.md)** - Claude-specific configuration and notes\n\n## License\n\nSee [LICENSE](LICENSE) for details.\n",
        "plugins/git/README.md": "# Git Plugin\n\nGit workflow automation and utilities for Claude Code.\n\n## Commands\n\n### `/git:bisect`\n\nInteractive git bisect assistant with pattern detection and automation. Helps find the exact commit that introduced a specific change using binary search.\n\n### `/git:cherry-pick-by-patch`\n\nCherry-pick a git commit into the current branch using the patch command instead of git cherry-pick.\n\n### `/git:fix-robot-pr`\n\nFix a cherrypick-robot PR that needs manual intervention by creating a replacement PR with all necessary fixes applied.\n\n### `/git:commit-suggest`\n\nGenerate Conventional Commits style commit messages for staged changes or recent commits.\n\n### `/git:debt-scan`\n\nScan the codebase for technical debt markers and generate a report.\n\n### `/git:redescribe`\n\nAdapt and correct a PR description based on code diffs and commit messages.\n\n### `/git:summary`\n\nGenerate a summary of git repository changes and activity.\n\nSee the [commands/](commands/) directory for full documentation of each command.\n\n## Installation\n\n```bash\n/plugin install git@ai-helpers\n```\n\n",
        "plugins/hello-world/README.md": "# Hello World Plugin\n\nA reference implementation plugin demonstrating Claude Code plugin structure and conventions.\n\n## Commands\n\n### `/hello-world:echo`\n\nA simple echo command that demonstrates the proper structure for command definitions.\n\nThis plugin serves as a template for creating new plugins. See [commands/echo.md](commands/echo.md) for the complete command format that follows the conventions defined in AGENTS.md.\n\n## Installation\n\n```bash\n/plugin install hello-world@ai-helpers\n```\n\n## For Plugin Developers\n\nThis plugin is the canonical example of proper plugin structure:\n- Correct frontmatter format\n- Required sections (Name, Synopsis, Description, Implementation)\n- Proper command naming conventions\n- Complete documentation\n\nUse this as a reference when creating new commands and plugins.\n\n",
        "plugins/jira/README.md": "# Jira Plugin\n\nComprehensive Jira integration for Claude Code, providing AI-powered tools to analyze issues, create solutions, and generate status rollups.\n\n## Features\n\n- üîç **Issue Analysis and Solutions** - Analyze JIRA issues and create pull requests to solve them\n- üìä **Status Rollups** - Generate comprehensive status rollup comments for any Jira issue given a date range\n- üìù **Weekly Status Updates** - Automate weekly status summary updates with intelligent activity analysis and color-coded health indicators\n- üìã **Backlog Grooming** - Analyze new bugs and cards for grooming meetings\n- üè∑Ô∏è **Activity Type Categorization** - AI-powered categorization of JIRA tickets into activity types with confidence scoring\n- üß™ **Test Generation** - Generate comprehensive test steps for JIRA issues by analyzing related PRs\n- ‚ú® **Issue Creation** - Create well-formed stories, epics, features, tasks, bugs, and feature requests with guided workflows\n- üìù **Release Note Generation** - Automatically generate bug fix release notes from Jira and linked GitHub PRs\n- ü§ñ **Automated Workflows** - From issue analysis to PR creation, fully automated\n- üí¨ **Smart Comment Analysis** - Extracts blockers, risks, and key insights from comments\n\n## Prerequisites\n\n- Claude Code installed\n- Jira MCP server configured\n- Optional: `gh` CLI tools installed and configured, for GitHub access.\n\n### Setting up Jira MCP Server\n\n```bash\n# Start the atlassian mcp server using podman\npodman run -i --rm -p 8080:8080 -e \"JIRA_URL=https://issues.redhat.com\" -e \"JIRA_USERNAME\" -e \"JIRA_PERSONAL_TOKEN\" -e \"JIRA_SSL_VERIFY\" ghcr.io/sooperset/mcp-atlassian:latest --transport sse --port 8080 -vv\n```\n\nAdd the MCP server to Claude:\n\n```bash\n# Add the Atlassian MCP server\nclaude mcp add --transport sse atlassian http://localhost:8080/sse\n```\n\n#### Getting Tokens\n\nFor your Jira token, use https://issues.redhat.com/secure/ViewProfile.jspa?selectedTab=com.atlassian.pats.pats-plugin:jira-user-personal-access-tokens\n\n### Notes and tips\n\n- Do not commit real tokens. If you must keep a project-local file, prefer committing a `mcp.json.sample` with placeholders, and keep your real `mcp.json` untracked.\n- Consider using the [rh-pre-commit](https://source.redhat.com/departments/it/it_information_security/leaktk/leaktk_components/rh_pre_commit) hook to scan for secrets accidentally left in commits.\n- The `atlassian` server example uses an MCP container image: `ghcr.io/sooperset/mcp-atlassian:latest`.\n- If you prefer Docker, replace the `podman` command with `docker` (arguments are typically the same).\n- If Podman is installed via Podman Machine on macOS, ensure it is running: `podman machine start`.\n- Keep `JIRA_SSL_VERIFY` as \"true\" unless you have a specific reason to disable TLS verification.\n- Limit active MCP servers: running too many at once can degrade performance or hit limits. Use Cursor's MCP panel to disable those you don't need for the current session.\n\n## Installation\n\nEnsure you have the ai-helpers marketplace enabled, via [the instructions here](/README.md).\n\n```bash\n# Install the plugin\n/plugin install jira@ai-helpers\n```\n\n## Reference Files\n\nThis plugin uses shared reference files for progressive disclosure, following [Claude Code best practices](https://platform.claude.com/docs/en/agents-and-tools/agent-skills/best-practices#progressive-disclosure-patterns).\n\nSkills reference these files rather than duplicating content:\n\n| File | Purpose |\n|------|---------|\n| [reference/wiki-markup.md](reference/wiki-markup.md) | JIRA Wiki Markup formatting guide |\n| [reference/mcp-tools.md](reference/mcp-tools.md) | MCP tool signatures and custom fields |\n| [reference/cli-fallback.md](reference/cli-fallback.md) | jira-cli commands when MCP unavailable |\n\n**Best Practice:** Keep references one level deep. Link directly from SKILL.md to reference files. Deeply nested references may result in partial file reads.\n\n## Available Commands\n\n### `/jira:solve` - Analyze and Solve JIRA Issues\n\nAnalyze a JIRA issue and create a pull request to solve it. The command fetches issue details, analyzes the codebase, creates an implementation plan, makes the necessary changes, and creates a PR with conventional commits.\n\n**Usage:**\n```bash\n/jira:solve OCPBUGS-12345 enxebre\n```\n\nSee [commands/solve.md](commands/solve.md) for full documentation.\n\n---\n\n### `/jira:status-rollup` - Generate Weekly Status Rollups\n\nGenerate comprehensive status rollup comments for any Jira issue by recursively analyzing all child issues and their activity within a date range. The command extracts insights from changelogs and comments to create well-formatted status summaries.\n\n**Usage:**\n```bash\n/jira:status-rollup FEATURE-123 --start-date 2025-10-08 --end-date 2025-10-14\n```\n\nSee [commands/status-rollup.md](commands/status-rollup.md) for full documentation.\n\n---\n\n### `/jira:grooming` - Backlog Grooming Assistant\n\nAnalyze and organize new bugs and cards added over a specified time period to prepare for grooming meetings. The command provides automated data collection, intelligent analysis, and generates structured, actionable meeting agendas.\n\n**Usage:**\n```bash\n# Single project\n/jira:grooming OCPSTRAT last-week\n\n# Multiple OpenShift projects\n/jira:grooming \"OCPSTRAT,OCPBUGS,HOSTEDCP\" last-week\n\n# Filter by component\n/jira:grooming OCPSTRAT last-week --component \"Control Plane\"\n\n# Filter by label\n/jira:grooming OCPSTRAT last-week --label \"technical-debt\"\n\n# Combine filters\n/jira:grooming OCPSTRAT last-week --component \"Control Plane\" --label \"security\"\n```\nSee [commands/grooming.md](commands/grooming.md) for full documentation.\n\n---\n\n### `/jira:categorize-activity-type` - AI-Powered Activity Type Categorization\n\nAnalyze JIRA tickets and automatically assign Activity Type categories based on ticket content, issue type, labels, and parent Epic context. Uses AI-powered analysis with confidence scoring to ensure accurate categorizations.\n\n**Usage:**\n```bash\n# Basic usage (prompts for confirmation)\n/jira:categorize-activity-type ROX-12345\n\n# Auto-apply for high confidence categorizations\n/jira:categorize-activity-type ROX-12345 --auto-apply\n```\n\nSee [commands/categorize-activity-type.md](commands/categorize-activity-type.md) for full documentation.\n\n---\n\n### `/jira:generate-test-plan` - Generate Test Steps\n\nGenerate comprehensive test steps for a JIRA issue by analyzing related pull requests. The command supports auto-discovery of PRs from the JIRA issue or manual specification of specific PRs to analyze.\n\n**Usage:**\n```bash\n# Auto-discover all PRs from JIRA\n/jira:generate-test-plan CNTRLPLANE-205\n\n# Test only specific PRs\n/jira:generate-test-plan CNTRLPLANE-205 https://github.com/openshift/hypershift/pull/6888\n```\n\nSee [commands/generate-test-plan.md](commands/generate-test-plan.md) for full documentation.\n\n---\n\n### `/jira:create` - Create Jira Issues\n\nCreate well-formed Jira issues (stories, epics, features, tasks, bugs, feature requests) with intelligent defaults, interactive guidance, and validation. The command applies project-specific conventions, suggests components based on context, and provides templates for consistent issue creation.\n\n**Usage:**\n```bash\n# Create a story\n/jira:create story MYPROJECT \"Add user dashboard\"\n\n# Create a story with options\n/jira:create story MYPROJECT \"Add search functionality\" --component \"Frontend\" --version \"2.5.0\"\n\n# Create an epic with parent\n/jira:create epic MYPROJECT \"Mobile application redesign\" --parent MYPROJECT-100\n\n# Create a bug\n/jira:create bug MYPROJECT \"Login button doesn't work on mobile\"\n\n# Create a bug with component\n/jira:create bug MYPROJECT \"API returns 500 error\" --component \"Backend\"\n\n# Create a task\n/jira:create task MYPROJECT \"Update API documentation\" --parent MYPROJECT-456\n\n# Create a feature\n/jira:create feature MYPROJECT \"Advanced search capabilities\"\n\n# Create a feature request\n/jira:create feature-request RFE \"Support custom SSL certificates for ROSA HCP\"\n```\n\n**Key Features:**\n- **Universal requirements** - All tickets MUST include Security Level: Red Hat Employee and label: ai-generated-jira\n- **Smart defaults** - Project and team-specific conventions applied automatically\n- **Interactive templates** - Guides you through user story format, acceptance criteria, bug templates\n- **Security validation** - Scans for credentials and secrets before submission\n- **Extensible** - Supports project-specific and team-specific skills for custom workflows\n- **Hybrid workflow** - Required fields as arguments, optional fields as interactive prompts\n\n**Supported Issue Types:**\n- `story` - User stories with acceptance criteria\n- `epic` - Epics with parent feature linking\n- `feature` - Strategic features with market problem analysis\n- `task` - Technical tasks and operational work\n- `bug` - Bug reports with structured templates\n- `feature-request` - Customer-driven feature requests for RFE project with business justification\n\n**Project-Specific Conventions:**\n\nDifferent projects may have different conventions (security levels, labels, versions, components, etc.). The command automatically detects your project and applies the appropriate conventions via project-specific skills.\n\n**Team-Specific Conventions:**\n\nTeams may have additional conventions layered on top of project conventions (component selection, custom fields, workflows, etc.). The command automatically detects team context and applies team-specific skills.\n\nSee [commands/create.md](commands/create.md) for full documentation.\n\n---\n\n### `/jira:create-release-note` - Generate Bug Fix Release Notes\n\nAutomatically generate bug fix release notes by analyzing Jira bug tickets and their linked GitHub pull requests. The command extracts Cause and Consequence from the bug description, analyzes PR content (description, commits, code changes, comments), synthesizes the information into a cohesive release note, and updates the Jira ticket.\n\n**Usage:**\n```bash\n/jira:create-release-note OCPBUGS-38358\n```\n\n**What it does:**\n1. Fetches the bug ticket from Jira\n2. Extracts Cause and Consequence sections from bug description\n3. Finds all linked GitHub PRs\n4. Analyzes each PR (description, commits, diff, comments)\n5. Synthesizes Fix, Result, and Workaround information\n6. Validates content for security (no credentials)\n7. Prompts for Release Note Type selection\n8. Updates Jira ticket fields\n\n**Release Note Format:**\n```\nCause: <extracted from bug description>\nConsequence: <extracted from bug description>\nFix: <analyzed from PRs>\nResult: <analyzed from PRs>\nWorkaround: <analyzed from PRs if applicable>\n```\n\n**Prerequisites:**\n- MCP Jira server configured\n- GitHub CLI (`gh`) installed and authenticated\n- Access to linked GitHub repositories\n- Jira permissions to update Release Note fields\n\n**Example Output:**\n```\n‚úì Release Note Created for OCPBUGS-38358\n\nType: Bug Fix\n\nText:\n---\nCause: hostedcontrolplane controller crashes when hcp.Spec.Platform.AWS.CloudProviderConfig.Subnet.ID is undefined\nConsequence: control-plane-operator enters a crash loop\nFix: Added nil check for CloudProviderConfig.Subnet before accessing Subnet.ID field\nResult: The control-plane-operator no longer crashes when CloudProviderConfig.Subnet is not specified\n---\n\nUpdated: https://issues.redhat.com/browse/OCPBUGS-38358\n```\n\nSee [commands/create-release-note.md](commands/create-release-note.md) for full documentation.\n\n---\n\n### `/jira:update-weekly-status` - Update Weekly Status Summaries\n\nAutomate the process of updating weekly status summaries for Jira issues with intelligent activity analysis and color-coded health indicators. The command analyzes recent activity across tickets, GitHub PRs, and GitLab MRs to draft status updates (Red/Yellow/Green), then allows you to review and modify them before updating Jira.\n\n**Usage:**\n```bash\n# Interactive mode (prompts for project and component)\n/jira:update-weekly-status\n\n# Specify project\n/jira:update-weekly-status OCPSTRAT\n\n# Specify project and component\n/jira:update-weekly-status OCPSTRAT --component \"Control Plane\"\n\n# With label filter\n/jira:update-weekly-status OCPSTRAT --label strategic-work\n\n# With specific users (by email)\n/jira:update-weekly-status OCPBUGS antoni@redhat.com jdoe@redhat.com\n\n# With excluded users\n/jira:update-weekly-status OCPSTRAT !manager@redhat.com\n\n# Full example with all options\n/jira:update-weekly-status OCPSTRAT --component \"Control Plane\" --label strategic-work user@redhat.com\n```\n\n**Key Features:**\n- Interactive component selection from available project components\n- User filtering by email or display name (with auto-resolution)\n- Intelligent activity analysis (comments, child issues, linked PRs/MRs)\n- Recent update warnings to prevent duplicate updates (24-hour check)\n- Batch processing with selective skip options\n- Formatted status summaries with color-coded health indicators (Red/Yellow/Green)\n- Auto-detects Status Summary custom field or prompts for field ID\n\n**What it does:**\n1. Filters issues by project, component, label, and assignee\n2. Checks recent activity (comments, PR updates, child issues)\n3. Drafts color-coded status summaries with specific accomplishments\n4. Warns about recently-updated issues to avoid duplicates\n5. Allows review and modification before updating\n6. Provides comprehensive summary report with statistics\n\n**Prerequisites:**\n- Jira MCP server configured\n- GitHub CLI (`gh`) installed and authenticated (optional but recommended)\n- Jira permissions to update Status Summary field\n\nSee [commands/update-weekly-status.md](commands/update-weekly-status.md) for full documentation.\n\n---\n\n## Troubleshooting\n\n### \"Could not find issue {issue-id}\"\n- Verify the issue ID is correct\n- Ensure you have access to the issue in Jira\n- Check that your Jira MCP server is properly configured\n\nFor command-specific troubleshooting, see the individual command documentation.\n\n## Contributing\n\nContributions welcome! Please submit pull requests to the [ai-helpers repository](https://github.com/openshift-eng/ai-helpers).\n\n## License\n\nApache-2.0\n",
        "plugins/ci/README.md": "# CI Plugin\n\nA plugin for working with OpenShift CI infrastructure, providing\ncommands to analyze CI workflow,chain or data, investigate failures, and understand\nrelease quality.\n\n## Commands\n\n### ask-sippy\n\nQuery the Sippy Chat AI agent for CI/CD data analysis.  Sippy Chat has a\n[web interface](https://sippy-auth.dptools.openshift.org/sippy-ng/chat)\navailable as well.\n\n**Note:** Each query is independent with no conversation history\nmaintained between calls. Use the web interface for longer sessions\nrequiring more context.\n\nThinking steps are not currently streamed, so it may take some time to\nappear to get a result, 10-60 seconds in most cases.\n\n**Usage:**\n```bash\n/ask-sippy [question]\n```\n\n**What it does:**\n- Analyzes OpenShift release payloads and rejection reasons\n- Investigates CI job failures and patterns\n- Examines test failures, flakes, and regressions\n- Provides CI health trends and comparisons\n- Delivers release quality metrics\n\n**Prerequisites:**\n\nYou need to set a token for Sippy's authenticated instance. You can\nobtain the OAuth token by visiting\n[api.ci](https://console-openshift-console.apps.cr.j7t7.p1.openshiftapps.com/k8s/cluster/projects)\nand logging in with SSO, and displaying your API token (sha256~<something>).\n\n```bash\nexport ASK_SIPPY_API_TOKEN='your-token-here'\n```\n\n**Examples:**\n\n1. **Payload investigation:**\n   ```bash\n   /ask-sippy Why was the latest 4.21 payload rejected?\n   ```\n\n2. **Test failure analysis:**\n   ```bash\n   /ask-sippy What are the most common test failures in e2e-aws this week?\n   ```\n\n3. **CI health check:**\n   ```bash\n   /ask-sippy How is the overall CI health for 4.20 compared to last week?\n   ```\n\n4. **Specific test inquiry:**\n   ```bash\n   /ask-sippy Why is the test \"sig-network Feature:SCTP should create a Pod with SCTP HostPort\" failing?\n   ```\n\n### list-step\nLists all step references (ref) used in a specified workflow or chain.\n\n**Prerequisites:**\n\nRun this command from your local clone of the openshift/release repository.\n\n**Usage:**\n```bash\n/list-step\n```\n**Arguments:**\n- workflow-name (e.g., `hypershift-aws-e2e-external`)\n\nor \n- chain-name(e.g., `rosa-cluster-provision-chain`)\n\n### trigger-periodic\n\nTrigger a periodic gangway job with optional environment variable overrides.\n\n**Prerequisites:** Authentication to app.ci cluster (see Configuration)\n\n**Usage:**\n```bash\n/trigger-periodic\n```\n\n**Arguments (interactive):**\n- Job name (e.g., `periodic-ci-openshift-release-master-ci-4.14-e2e-aws-ovn`)\n- Optional environment variable overrides\n\n### trigger-postsubmit\n\nTrigger a postsubmit gangway job with repository refs.\n\n**Prerequisites:** Authentication to app.ci cluster (see Configuration)\n\n**Usage:**\n```bash\n/trigger-postsubmit\n```\n\n**Arguments (interactive):**\n- Job name (e.g., `branch-ci-openshift-assisted-installer-release-4.12-images`)\n- Repository organization (e.g., `openshift`)\n- Repository name (e.g., `assisted-installer`)\n- Base ref/branch (e.g., `release-4.12`)\n- Base SHA (commit hash)\n- Repository link\n- Optional base link (comparison URL)\n- Optional environment variable overrides\n\n### trigger-presubmit\n\nTrigger a presubmit gangway job.\n\n**Prerequisites:** Authentication to app.ci cluster (see Configuration)\n\n**Usage:**\n```bash\n/trigger-presubmit\n```\n\n**WARNING:** Presubmit jobs should typically be triggered using GitHub Prow commands (`/test`, `/retest`). Only use this if you have a specific reason to trigger via REST API.\n\n**Arguments (interactive):**\n- Job name\n- Pull request information (org, repo, base ref, PR number, SHAs)\n- Optional environment variable overrides\n\n### query-job-status\n\nQuery the status of a gangway job execution by ID.\n\n**Prerequisites:** Authentication to app.ci cluster (see Configuration)\n\n**Usage:**\n```bash\n/query-job-status\n```\n\n**Arguments (interactive):**\n- Execution ID (returned when a job is triggered)\n\n**Returns:**\n- Job name, type, and status (SUCCESS, FAILURE, PENDING, RUNNING, ABORTED)\n- GCS path to artifacts (if available)\n\n## Configuration\n\n### Authentication for Gangway Commands\n\nGangway commands require authentication to the app.ci cluster:\n\n1. Visit https://console-openshift-console.apps.ci.l2s4.p1.openshiftapps.com/\n2. Log in with SSO and click \"Copy login command\"\n3. Execute the `oc login` command in your terminal\n\nVerify with: `oc whoami`\n\n## Additional Resources\n\n- [Sippy Chat Web Interface](https://sippy-auth.dptools.openshift.org/sippy-ng/chat)\n- [Triggering ProwJobs via REST](https://docs.ci.openshift.org/docs/how-tos/triggering-prowjobs-via-rest/)\n- [Gangway CLI](https://github.com/openshift-eng/gangway-cli)\n",
        "plugins/teams/README.md": "# Teams Plugin\n\nTeam structure knowledge and health analysis commands for OpenShift teams.\n\n## Overview\n\nThe Teams plugin provides comprehensive information about OpenShift team structure and health analysis capabilities. It helps teams understand:\n\n1. **Team Structure**:\n   - Team component ownership mapping\n   - Repository assignments\n   - Communication channels (Slack)\n   - Team member information\n\n2. **Team Health Analysis**:\n   - Regression management metrics across team components\n   - Bug backlog health for team components\n   - Combined quality grading with actionable recommendations\n   - Trend tracking across releases\n\nThe plugin offers commands at different levels:\n- **Discovery**: List teams and their component ownership\n- **Data**: Raw regression and JIRA data for investigation\n- **Summary**: Aggregated statistics and counts\n- **Analysis**: Combined health grading with recommendations\n\n## Commands\n\n### Team Discovery\n\n#### `/teams:list-teams`\n\nList all teams from the team component mapping.\n\n**Usage:**\n```\n/teams:list-teams\n```\n\n**Use Cases:**\n- Discover available teams\n- Validate team names for other commands\n- Understanding team structure\n\n#### `/teams:list-components`\n\nList all OCPBUGS components, optionally filtered by team.\n\n**Usage:**\n```\n/teams:list-components\n/teams:list-components --team \"API Server\"\n```\n\n**Use Cases:**\n- Discover components owned by a team\n- Validate component names for JIRA queries\n- Understanding component ownership\n\n### Health Analysis\n\n#### `/teams:health-check`\n\nAnalyze and grade team or component health based on regression and JIRA bug metrics.\n\n**Usage:**\n```\n/teams:health-check <release> --team <team-name> [--project JIRAPROJECT]\n/teams:health-check <release> --components comp1 comp2 ... [--project JIRAPROJECT]\n```\n\n**Examples:**\n```\n# Analyze team health (recommended)\n/teams:health-check 4.21 --team \"API Server\"\n\n# Analyze specific components\n/teams:health-check 4.21 --components Monitoring etcd\n\n# Use alternative JIRA project\n/teams:health-check 4.21 --team \"Networking\" --project OCPBUGS\n```\n\n**Use Cases:**\n- Grade overall team quality\n- Identify components needing attention\n- Get actionable recommendations\n- Generate comprehensive health scorecards\n- Prioritize engineering investment\n- Track team health across releases\n\n**Requirements:**\n- Either `--team` or `--components` is REQUIRED\n- Analyzing all components is too much data\n\n#### `/teams:health-check-regressions`\n\nQuery and summarize just regression data with counts and metrics. Part of the overall health-check command.\n\n**Usage:**\n```\n/teams:health-check-regressions <release> [--components comp1 comp2 ...] [--start YYYY-MM-DD] [--end YYYY-MM-DD]\n/teams:health-check-regressions <release> --team <team-name> [--start YYYY-MM-DD] [--end YYYY-MM-DD]\n```\n\n**Examples:**\n```\n# Team-based summary (recommended)\n/teams:health-check-regressions 4.21 --team \"API Server\"\n\n# Component-based summary\n/teams:health-check-regressions 4.21 --components Monitoring etcd\n\n# Custom date range\n/teams:health-check-regressions 4.17 --start 2024-05-17 --end 2024-10-29\n```\n\n**Use Cases:**\n- Get quick regression counts by team or component\n- Track triage coverage and response times\n- Understanding open vs closed breakdown\n- Generate summary reports for teams\n\n#### `/teams:health-check-jiras`\n\nQuery and summarize JIRA bugs with counts by component, status, and priority. Part of the overall health-check command.\n\n**Usage:**\n```\n/teams:health-check-jiras --project <project> [--component comp1 ...] [--status status1 ...] [--include-closed] [--limit N]\n/teams:health-check-jiras --project <project> --team <team-name> [--status status1 ...] [--include-closed] [--limit N]\n```\n\n**Examples:**\n```\n# Team-based JIRA summary\n/teams:health-check-jiras --project OCPBUGS --team \"API Server\"\n\n# Component-based summary\n/teams:health-check-jiras --project OCPBUGS --component \"kube-apiserver\" \"etcd\"\n\n# Include closed bugs\n/teams:health-check-jiras --project OCPBUGS --team \"Networking\" --include-closed\n```\n\n**Use Cases:**\n- Get bug counts by team or component\n- Track recent bug flow (opened vs closed)\n- Monitor bug velocity and closure rates\n- Compare bug backlogs across teams\n\n### Raw Data\n\n#### `/teams:list-regressions`\n\nFetch and list raw regression data for OpenShift releases without summarization.\n\n**Usage:**\n```\n/teams:list-regressions <release> [--components comp1 comp2 ...] [--start YYYY-MM-DD] [--end YYYY-MM-DD]\n/teams:list-regressions <release> --team <team-name> [--start YYYY-MM-DD] [--end YYYY-MM-DD]\n```\n\n**Use Cases:**\n- Access complete regression details for investigation\n- Build custom analysis workflows\n- Export data for offline analysis\n\n#### `/teams:list-jiras`\n\nQuery and list raw JIRA bug data for a specific project.\n\n**Usage:**\n```\n/teams:list-jiras --project <project> [--component comp1 comp2 ...] [--status status1 ...] [--include-closed] [--limit N]\n```\n\n**Use Cases:**\n- Access detailed JIRA bug information\n- Custom bug analysis workflows\n- Export bug data for reporting\n\n## Team Structure Data\n\nThe plugin maintains a committed mapping file (`team_component_map.json`) that contains:\n\n- **Team names**: All OpenShift teams with OCPBUGS components\n- **Component ownership**: Which OCPBUGS components each team owns\n- **Metadata**: Component counts per team\n\n**Data Source**: The mapping data originates from https://gitlab.cee.redhat.com/hybrid-platforms/org (requires Red Hat VPN).\n\n**To update the mapping**:\n1. Submit PRs to the org repository to correct team/component assignments\n2. After merge, regenerate: `python3 plugins/teams/generate_team_component_map.py`\n3. Commit the updated mapping file\n\n## Installation\n\n### Via Marketplace (Recommended)\n\n```bash\n# Add the marketplace\n/plugin marketplace add openshift-eng/ai-helpers\n\n# Install the plugin\n/plugin install teams@ai-helpers\n\n# Use the commands\n/teams:list-teams\n/teams:health-check 4.21 --team \"API Server\"\n```\n\n### Manual Installation\n\n```bash\n# Clone the repository\nmkdir -p ~/.cursor/commands\ngit clone git@github.com:openshift-eng/ai-helpers.git\nln -s ai-helpers ~/.cursor/commands/ai-helpers\n```\n\n## Prerequisites\n\n1. **Python 3.6+**: Required to run analysis scripts\n2. **Network Access**: Required to reach component health API and JIRA\n3. **JIRA Authentication** (for bug analysis):\n   - `JIRA_URL`: Your JIRA instance URL\n   - `JIRA_PERSONAL_TOKEN`: Your JIRA bearer token\n\n## Use Cases\n\n### Analyze Team Health\n\nGet a comprehensive health scorecard for a team:\n\n```\n/teams:health-check 4.21 --team \"API Server\"\n```\n\nOutput includes:\n- Team-level regression metrics\n- Team-level bug backlog metrics\n- Per-component breakdowns\n- Health grades and recommendations\n\n### Track Team Trends\n\nCompare team health across releases:\n\n```\n/teams:health-check 4.21 --team \"Networking\"\n/teams:health-check 4.20 --team \"Networking\"\n```\n\n### Discover Team Structure\n\nFind teams and their components:\n\n```\n/teams:list-teams\n/teams:list-components --team \"API Server\"\n```\n\n### Monitor Bug Backlogs\n\nTrack team bug health:\n\n```\n/teams:health-check-jiras --project OCPBUGS --team \"API Server\"\n```\n\n## Output Format\n\n### health-check Command\n\nProvides a **Comprehensive Health Report** with:\n\n- **Team Summary** (when using --team):\n  - Overall team metrics combining all components\n  - Per-component breakdowns within the team\n  - Team-level health grade\n\n- **Component Scorecards**:\n  - Regression triage coverage (target: 90%+)\n  - Average time to triage (target: <24 hours)\n  - Bug backlog size and age\n  - Combined health grades (‚úÖ/‚ö†Ô∏è/‚ùå)\n\n- **Actionable Recommendations**:\n  - Open untriaged regressions\n  - High bug backlogs\n  - Slow triage response\n  - Growing bug trends\n\n## Contributing\n\nSee [AGENTS.md](../../AGENTS.md) for development guidelines.\n\n## Support\n\n- **Issues**: https://github.com/openshift-eng/ai-helpers/issues\n- **Repository**: https://github.com/openshift-eng/ai-helpers\n\n## License\n\nSee [LICENSE](../../LICENSE) for details.\n",
        "plugins/doc/README.md": "# Doc Plugin\n\nEngineering documentation and note-taking utilities for Claude Code.\n\n## Commands\n\n### `/doc:note`\n\nCreate and manage engineering notes and documentation.\n\nSee [commands/note.md](commands/note.md) for full documentation.\n\n## Installation\n\n```bash\n/plugin install doc@ai-helpers\n```\n\n",
        "plugins/session/README.md": "# Session Plugin\n\nClaude Code session management and persistence utilities.\n\n## Commands\n\n### `/session:save-session`\n\nSave the current conversation session to a markdown file for future continuation.\n\nThis command captures the conversation context, allowing you to resume long-running tasks across multiple sessions.\n\nSee [commands/save-session.md](commands/save-session.md) for full documentation.\n\n## Installation\n\n```bash\n/plugin install session@ai-helpers\n```\n\n",
        "plugins/sosreport/README.md": "# Sosreport Plugin\n\nAutomate sosreport analysis for system diagnostics and troubleshooting.\n\n## Overview\n\nThe sosreport plugin provides AI-powered analysis of sosreport archives, which are diagnostic data collections from Linux systems. It automatically examines logs, resource usage, network configuration, and system state to identify issues and provide actionable recommendations.\n\n## What is sosreport?\n\n[sosreport](https://github.com/sosreport/sos) is a diagnostic data collection tool used primarily in Red Hat Enterprise Linux and related distributions. It gathers system configuration, logs, and diagnostic information into a single archive for troubleshooting purposes.\n\n## Commands\n\n### `/sosreport:analyze`\n\nPerforms comprehensive analysis of a sosreport archive with support for selective analysis.\n\n**Usage:**\n```bash\n/sosreport:analyze <path-to-sosreport> [--only <areas>] [--skip <areas>]\n```\n\n**Arguments:**\n- `<path-to-sosreport>`: Path to the sosreport archive (`.tar.gz`, `.tar.xz`) or extracted directory\n- `--only <areas>`: (Optional) Run only specific analysis areas (comma-separated)\n- `--skip <areas>`: (Optional) Skip specific analysis areas (comma-separated)\n\n**Analysis Areas:**\n\nThe analysis is organized into four specialized areas, each with detailed implementation guidance:\n\n1. **`logs`** - System and Application Logs Analysis\n   - Analyzes journald logs, syslog, dmesg, and application logs\n   - Identifies errors, warnings, and critical messages\n   - Detects OOM killer events, kernel panics, segfaults\n   - Counts and categorizes errors by severity\n   - Provides timeline of critical events\n   - **Skill**: [`skills/logs-analysis/SKILL.md`](skills/logs-analysis/SKILL.md)\n\n2. **`resources`** - System Resource Usage Analysis\n   - Memory usage, swap, and pressure indicators\n   - CPU information and load averages\n   - Disk usage and filesystem capacity\n   - Process analysis (top consumers, zombies)\n   - Resource exhaustion patterns\n   - **Skill**: [`skills/resource-analysis/SKILL.md`](skills/resource-analysis/SKILL.md)\n\n3. **`network`** - Network Configuration and Connectivity\n   - Network interface status and IP addresses\n   - Routing table and default gateway\n   - Active connections and listening services\n   - Firewall rules (firewalld/iptables/nftables)\n   - DNS configuration and hostname resolution\n   - Network error detection\n   - **Skill**: [`skills/network-analysis/SKILL.md`](skills/network-analysis/SKILL.md)\n\n4. **`system-config`** - System Configuration and Security\n   - OS version and kernel information\n   - Installed package versions\n   - Systemd service status and failures\n   - SELinux/AppArmor configuration and denials\n   - Kernel parameters and resource limits\n   - **Skill**: [`skills/system-config-analysis/SKILL.md`](skills/system-config-analysis/SKILL.md)\n\n**Output:**\n- Interactive summary categorized by severity (Critical, High, Medium, Low)\n- Resource utilization metrics (when `resources` is selected)\n- Top errors and their frequency (when `logs` is selected)\n- Failed services (when `system-config` is selected)\n- Network configuration status (when `network` is selected)\n- Actionable recommendations\n- File paths for detailed investigation\n\n**Examples:**\n\n```bash\n# Comprehensive analysis (all areas)\n/sosreport:analyze /tmp/sosreport-server01-2024-01-15.tar.xz\n\n# Analyze only logs and network\n/sosreport:analyze /tmp/sosreport.tar.xz --only logs,network\n\n# Skip resource analysis\n/sosreport:analyze /tmp/sosreport.tar.xz --skip resources\n\n# Quick log-only analysis\n/sosreport:analyze /tmp/sosreport.tar.xz --only logs\n\n# Analyze extracted directory\n/sosreport:analyze /tmp/sosreport-server01-2024-01-15/ --only system-config\n```\n\nThe command automatically extracts compressed archives to `.work/sosreport-analyze/` and performs the selected analysis.\n\n### `/sosreport:ovs-db`\n\nAnalyzes Open vSwitch (OVS) database files (`conf.db`) collected in sosreports using `ovsdb-tool`.\n\n**Usage:**\n```bash\n/sosreport:ovs-db <sosreport-path> [--query <json>]\n```\n\n**What it analyzes:**\n- System information (OVS version, DPDK settings)\n- Bridges with datapath type, fail mode, STP status\n- Ports including VLAN tags, bonding, LACP configuration\n- Interfaces with types, link state, MTU, errors\n- Special interfaces (VXLAN, Geneve, GRE tunnels, DPDK ports)\n- Controllers and managers\n\n**Prerequisites:**\n- `ovsdb-tool` must be installed (from openvswitch package)\n  - Fedora/RHEL: `sudo dnf install openvswitch`\n  - Ubuntu/Debian: `sudo apt install openvswitch-common`\n\n**Examples:**\n```bash\n# Analyze from sosreport archive\n/sosreport:ovs-db /tmp/sosreport-server01-2024-01-15.tar.xz\n\n# Analyze conf.db directly\n/sosreport:ovs-db /var/lib/openvswitch/conf.db\n\n# Query VXLAN tunnels\n/sosreport:ovs-db /tmp/sosreport/ --query '[\"Open_vSwitch\", {\"op\":\"select\", \"table\":\"Interface\", \"where\":[[\"type\",\"==\",\"vxlan\"]], \"columns\":[\"name\",\"options\"]}]'\n\n# Check for interface errors\n/sosreport:ovs-db /tmp/sosreport/ --query '[\"Open_vSwitch\", {\"op\":\"select\", \"table\":\"Interface\", \"where\":[], \"columns\":[\"name\",\"error\",\"link_state\"]}]'\n```\n\nSee [`commands/ovs-db.md`](commands/ovs-db.md) for full documentation.\n\n## Analysis Skills\n\nThe sosreport plugin uses specialized analysis skills for each area. Each skill contains detailed implementation guidance with bash commands, parsing logic, error handling, and output formats.\n\n| Skill | Description | Documentation |\n|-------|-------------|---------------|\n| **Logs Analysis** | Analyzes system logs, journald, dmesg, and application logs. Identifies errors, OOM events, kernel panics, and segfaults. | [`skills/logs-analysis/SKILL.md`](skills/logs-analysis/SKILL.md) |\n| **Resource Analysis** | Analyzes memory, CPU, disk usage, and processes. Identifies resource exhaustion and performance bottlenecks. | [`skills/resource-analysis/SKILL.md`](skills/resource-analysis/SKILL.md) |\n| **Network Analysis** | Analyzes network interfaces, routing, connections, firewall rules, and DNS configuration. | [`skills/network-analysis/SKILL.md`](skills/network-analysis/SKILL.md) |\n| **System Config Analysis** | Analyzes OS info, packages, systemd services, SELinux/AppArmor, and kernel parameters. | [`skills/system-config-analysis/SKILL.md`](skills/system-config-analysis/SKILL.md) |\n| **OVS DB Analysis** | Analyzes Open vSwitch database (conf.db) for bridges, ports, interfaces, tunnels, and DPDK configuration. | [`skills/ovs-db-analysis/SKILL.md`](skills/ovs-db-analysis/SKILL.md) |\n\nEach skill document includes:\n- Step-by-step implementation instructions\n- Bash command examples with actual sosreport file paths\n- Error handling guidance\n- Output format templates\n- Common patterns and severity classifications\n- Tips for effective analysis\n\n## Installation\n\n### From Marketplace\n\n```bash\n# Add the ai-helpers marketplace\n/plugin marketplace add openshift-eng/ai-helpers\n\n# Install the sosreport plugin\n/plugin install sosreport@ai-helpers\n```\n\n### Manual Installation\n\n```bash\n# Clone the repository\ngit clone https://github.com/openshift-eng/ai-helpers.git\n\n# Add the ai-helpers marketplace from cloned directory\n/plugin marketplace add $(pwd)/ai-helpers\n\n# Install the sosreport plugin\n/plugin install sosreport@ai-helpers\n```\n\n## Typical Workflows\n\n### Full Comprehensive Analysis\n\n1. **Obtain sosreport**: Get a sosreport archive from a system (usually generated with `sosreport` or `sos report` command)\n\n2. **Run comprehensive analysis**:\n   ```bash\n   /sosreport:analyze /path/to/sosreport.tar.xz\n   ```\n\n3. **Review findings**: Examine the interactive summary for critical issues and recommendations across all areas\n\n4. **Deep dive**: Ask follow-up questions about specific findings:\n   ```bash\n   Can you show me more details about the OOM killer events?\n   What caused the httpd service to fail?\n   ```\n\n5. **Take action**: Use the recommendations to troubleshoot and resolve issues\n\n### Targeted Investigation Workflow\n\n1. **Quick log scan** (fastest):\n   ```bash\n   /sosreport:analyze /path/to/sosreport.tar.xz --only logs\n   ```\n   Quickly identify error patterns and critical events\n\n2. **Follow-up based on findings**:\n   - If memory issues found: Run `--only resources`\n   - If network errors found: Run `--only network`\n   - If service failures found: Run `--only system-config`\n\n3. **Example iterative investigation**:\n   ```bash\n   # Start with logs to identify the problem area\n   /sosreport:analyze /tmp/sos.tar.xz --only logs\n\n   # Found network timeouts, analyze network configuration\n   /sosreport:analyze /tmp/sos.tar.xz --only network\n\n   # Network looks fine, check if it's a resource issue\n   /sosreport:analyze /tmp/sos.tar.xz --only resources\n   ```\n\n### Performance-Focused Workflow\n\nWhen you know what you're looking for:\n\n```bash\n# Only interested in service configuration\n/sosreport:analyze /path/to/sos.tar.xz --only system-config\n\n# Need logs and network, skip the rest\n/sosreport:analyze /path/to/sos.tar.xz --only logs,network\n\n# Full analysis but skip time-consuming resource analysis\n/sosreport:analyze /path/to/sos.tar.xz --skip resources\n```\n\n## Use Cases\n\n- **Incident response**: Quickly identify root causes of system failures\n  - Start with `--only logs` for fastest initial assessment\n  - Follow up with targeted analysis based on log findings\n\n- **Performance troubleshooting**: Find resource bottlenecks and optimization opportunities\n  - Use `--only resources` to focus on memory, CPU, and disk metrics\n  - Combine with `--only logs` to correlate resource issues with errors\n\n- **Configuration review**: Verify system configuration and identify misconfigurations\n  - Use `--only system-config` to audit packages, services, and security settings\n  - Add `--only network` to validate network configuration\n\n- **Network troubleshooting**: Diagnose connectivity and firewall issues\n  - Use `--only network,logs` to see network config and related errors\n  - Skip resource-intensive analysis for faster results\n\n- **Proactive monitoring**: Regular analysis of production system sosreports\n  - Run comprehensive analysis for periodic health checks\n  - Use selective analysis for quick daily checks\n\n- **Knowledge transfer**: Let AI explain complex system issues to team members\n  - Use selective analysis to focus on specific areas for learning\n  - Each skill provides detailed documentation for understanding\n\n## Prerequisites\n\n- **tar**: For extracting compressed archives (usually pre-installed)\n- **Disk space**: At least 2x the size of the compressed sosreport\n\n## Tips\n\n- **Selective analysis**: Use `--only` or `--skip` to run specific analysis areas for faster results\n  - `--only logs` is the fastest option for initial investigation\n  - Combine multiple areas: `--only logs,network`\n  - Valid areas: `logs`, `resources`, `network`, `system-config`\n\n- **Archive handling**: The command works with both compressed archives (`.tar.gz`, `.tar.xz`) and extracted directories\n\n- **Performance**: For large sosreports (>1GB)\n  - Comprehensive analysis may take several minutes\n  - Use selective analysis to reduce analysis time\n  - Start with `--only logs` then add more areas as needed\n\n- **Interactive investigation**: You can ask follow-up questions to drill deeper into specific findings\n  - \"Show me more details about the OOM killer events\"\n  - \"What caused the httpd service to fail?\"\n  - \"Analyze the network timeouts in more detail\"\n\n- **Workspace**: The extracted sosreport is preserved in `.work/sosreport-analyze/` for manual investigation\n\n- **Skills documentation**: Each analysis area has detailed implementation guidance\n  - See `skills/logs-analysis/SKILL.md` for log analysis details\n  - See `skills/resource-analysis/SKILL.md` for resource analysis details\n  - See `skills/network-analysis/SKILL.md` for network analysis details\n  - See `skills/system-config-analysis/SKILL.md` for system config details\n\n## Contributing\n\nSee the main [CLAUDE.md](../../CLAUDE.md) guide for information on contributing to this plugin.\n\n## Resources\n\n- [sosreport GitHub](https://github.com/sosreport/sos)\n- [Red Hat sosreport guide](https://access.redhat.com/solutions/3592)\n- [AI Helpers Repository](https://github.com/openshift-eng/ai-helpers)\n",
        "plugins/utils/README.md": "# Utils Plugin\n\nGeneral-purpose utilities and helper commands for development workflows.\n\n## Commands\n\n### `/utils:generate-test-plan`\n\nGenerate comprehensive test steps for one or more related GitHub PRs.\n\n### `/utils:address-reviews`\n\nProcess and address code review comments on pull requests.\n\n### `/utils:process-renovate-pr`\n\nAutomate processing of Renovate dependency update PRs.\n\n### `/utils:auto-approve-konflux-prs`\n\nAutomate approving Konflux bot PRs for the given repository by adding /lgtm and /approve\n\n### `/utils:review-security`\n\nOrchestrate security scanners and provide contextual triage of findings.\n\n### `/utils:placeholder`\n\nA placeholder command for testing and development.\n\n### `/utils:review-ai-helpers-overlap`\n\nReview potential overlaps with existing ai-helpers (Claude Code Plugins, Commands, Skills, Sub-agents, or Hooks) and open PRs.\n\n## Purpose\n\nThe utils plugin serves as a catch-all for commands that don't fit into existing specialized plugins. Once we accumulate several related commands, they can be segregated into a new targeted plugin.\n\nSee the [commands/](commands/) directory for full documentation of each command.\n\n## Installation\n\n```bash\n/plugin install utils@ai-helpers\n```\n\n",
        "plugins/olm/README.md": "# OLM Plugin\n\nA comprehensive plugin for managing and debugging Operator Lifecycle Manager (OLM) in OpenShift clusters.\n\n## Overview\n\nThis plugin provides comprehensive OLM capabilities:\n\n- **Operator Discovery**: Search and discover operators across all catalog sources\n- **Lifecycle Management**: Install, upgrade, and uninstall operators with intelligent defaults\n- **Health Monitoring**: List and check detailed operator health status\n- **Update Management**: Check for and install operator updates with approval workflows\n- **Troubleshooting**: Diagnose and fix common OLM issues automatically\n- **Catalog Management**: Add, remove, and manage custom catalog sources; build and publish catalog indexes\n- **Advanced Debugging**: Troubleshoot OLM issues by correlating must-gather logs with source code and known bugs in Jira\n- **Safety Features**: Orphaned resource cleanup, stuck namespace detection, and confirmation prompts\n- **Context-Aware**: Automatic channel discovery, namespace auto-detection, and smart recommendations\n\nThe plugin supports both OLMv0 (traditional OLM) and OLMv1 (next-generation) architectures.\n\n## Prerequisites\n\n- Claude Code installed\n- OpenShift CLI (`oc`) installed and configured\n- Access to an OpenShift cluster with cluster-admin or sufficient RBAC permissions\n- `git` (required for debug command)\n- `opm` and `podman` (required for opm command to build catalogs)\n- Network access to GitHub and Jira (for debug command)\n\n## Commands\n\n### Operator Management Commands\n\n#### `/olm:search` - Search for Operators\n\nSearch for available operators in OperatorHub catalogs.\n\n**Usage:**\n```bash\n/olm:search cert-manager                              # Search by keyword\n/olm:search                                           # List all operators\n/olm:search prometheus --catalog community-operators  # Search specific catalog\n/olm:search external-secrets-operator --exact         # Exact name match\n```\n\n**What it does:**\n- Searches across all catalog sources or specific catalogs\n- Shows operator details (versions, channels, descriptions)\n- Groups results by catalog source\n- Provides install commands for each operator\n\n**Arguments:**\n- `query` (optional): Search term for filtering operators\n- `--catalog <name>` (optional): Limit search to specific catalog\n- `--exact` (optional): Only show exact name matches\n\nSee [commands/search.md](commands/search.md) for full documentation.\n\n---\n\n#### `/olm:install` - Install Operators\n\nInstall operators from OperatorHub with smart defaults and verification.\n\n**Usage:**\n```bash\n/olm:install openshift-cert-manager-operator                           # Basic install\n/olm:install openshift-cert-manager-operator my-namespace              # Custom namespace\n/olm:install openshift-cert-manager-operator ns stable-v1              # Specific channel\n/olm:install prometheus ns stable community-operators --approval=Manual # Manual approval mode\n```\n\n**What it does:**\n- Creates namespace and OperatorGroup automatically\n- Auto-discovers default channel if not specified\n- Creates Subscription with configurable approval mode\n- Monitors installation progress and verifies CSV status\n- Reports deployment and pod status\n\n**Arguments:**\n- `operator-name` (required): Name of the operator\n- `namespace` (optional): Target namespace (defaults to operator name)\n- `channel` (optional): Subscription channel (auto-discovered if not provided)\n- `source` (optional): CatalogSource name (defaults to \"redhat-operators\")\n- `--approval=Automatic|Manual` (optional): InstallPlan approval mode (default: Automatic)\n\nSee [commands/install.md](commands/install.md) for full documentation.\n\n---\n\n#### `/olm:list` - List Installed Operators\n\nView all operators installed in the cluster with health status.\n\n**Usage:**\n```bash\n/olm:list                           # List all operators\n/olm:list cert-manager-operator     # List in specific namespace\n/olm:list --all-namespaces          # Explicit cluster-wide view\n```\n\n**What it does:**\n- Shows operator status, versions, and channels\n- Identifies operators requiring attention (failed, upgrading, etc.)\n- Provides summary statistics by status and catalog\n- Suggests troubleshooting commands for problematic operators\n\n**Arguments:**\n- `namespace` (optional): Target namespace\n- `--all-namespaces` or `-A` (optional): List cluster-wide\n\nSee [commands/list.md](commands/list.md) for full documentation.\n\n---\n\n#### `/olm:status` - Check Operator Status\n\nGet comprehensive health and status information for a specific operator.\n\n**Usage:**\n```bash\n/olm:status openshift-cert-manager-operator           # Auto-discover namespace\n/olm:status external-secrets-operator my-namespace    # Specific namespace\n```\n\n**What it does:**\n- Shows CSV, Subscription, and InstallPlan status\n- Displays available updates and upgrade information\n- Lists deployments and pods with health information\n- Shows recent events and warnings\n- Checks for pending manual approvals\n- Provides context-aware troubleshooting recommendations\n\n**Arguments:**\n- `operator-name` (required): Name of the operator\n- `namespace` (optional): Namespace (auto-discovered if not provided)\n\nSee [commands/status.md](commands/status.md) for full documentation.\n\n---\n\n#### `/olm:upgrade` - Update Operators\n\nUpdate operators to the latest version or switch channels.\n\n**Usage:**\n```bash\n/olm:upgrade openshift-cert-manager-operator                      # Upgrade to latest\n/olm:upgrade cert-manager ns --channel=tech-preview               # Switch channel\n/olm:upgrade prometheus ns --approve                               # Approve pending upgrade\n```\n\n**What it does:**\n- Checks for available updates in current or different channels\n- Switches operator to different channel if requested\n- Approves pending InstallPlans for manual approval mode\n- Monitors upgrade progress with detailed feedback\n- Verifies upgrade success and reports any issues\n\n**Arguments:**\n- `operator-name` (required): Name of the operator to upgrade\n- `namespace` (optional): Namespace (auto-discovered if not provided)\n- `--channel=<channel>` (optional): Switch to specified channel\n- `--approve` (optional): Auto-approve pending InstallPlan\n\nSee [commands/upgrade.md](commands/upgrade.md) for full documentation.\n\n---\n\n#### `/olm:approve` - Approve InstallPlans\n\nApprove pending InstallPlans for operators with manual approval mode.\n\n**Usage:**\n```bash\n/olm:approve openshift-cert-manager-operator                      # Approve pending plan\n/olm:approve external-secrets-operator eso-operator               # Specific namespace\n/olm:approve cert-manager ns --all                                # Approve all pending\n```\n\n**What it does:**\n- Finds pending InstallPlans requiring manual approval\n- Shows what will be installed/upgraded before approval\n- Approves InstallPlans after user confirmation\n- Monitors installation/upgrade execution\n- Reports completion status\n\n**Arguments:**\n- `operator-name` (required): Name of the operator\n- `namespace` (optional): Namespace (auto-discovered if not provided)\n- `--all` (optional): Approve all pending InstallPlans\n\nSee [commands/approve.md](commands/approve.md) for full documentation.\n\n---\n\n#### `/olm:uninstall` - Uninstall Operators\n\nSafely uninstall operators with optional resource cleanup.\n\n**Usage:**\n```bash\n/olm:uninstall openshift-cert-manager-operator                    # Basic uninstall\n/olm:uninstall operator-name my-namespace                          # Custom namespace\n/olm:uninstall operator-name ns --remove-crds                      # Include CRDs\n/olm:uninstall operator-name ns --remove-crds --remove-namespace   # Full cleanup\n```\n\n**What it does:**\n- Removes Subscription and CSV\n- Checks for and handles orphaned custom resources\n- Removes operator deployments\n- Optionally removes CRDs (with cluster-wide impact warning)\n- Optionally removes namespace\n- Detects and handles stuck Terminating namespaces\n- Provides detailed uninstallation summary\n- Post-uninstall verification\n\n**Arguments:**\n- `operator-name` (required): Name of the operator\n- `namespace` (optional): Target namespace (defaults to operator name)\n- `--remove-crds` (optional): Remove CRDs - **CAUTION: affects entire cluster**\n- `--remove-namespace` (optional): Remove namespace\n- `--force` (optional): Skip confirmation prompts\n\nSee [commands/uninstall.md](commands/uninstall.md) for full documentation.\n\n---\n\n#### `/olm:diagnose` - Diagnose and Fix Issues\n\nDiagnose common OLM and operator issues with optional auto-fix.\n\n**Usage:**\n```bash\n/olm:diagnose                                           # Cluster-wide health check\n/olm:diagnose openshift-cert-manager-operator           # Check specific operator\n/olm:diagnose \"\" stuck-namespace --fix                  # Fix stuck namespace\n/olm:diagnose --cluster --fix                           # Full scan and fix\n```\n\n**What it does:**\n- Scans for orphaned CRDs from deleted operators\n- Detects namespaces stuck in Terminating state\n- Identifies failed operator installations\n- Checks for conflicting OperatorGroups\n- Verifies catalog source health\n- Detects Subscription/CSV mismatches\n- Lists pending manual approvals\n- Generates comprehensive troubleshooting report\n- Optionally attempts to fix detected issues\n\n**Arguments:**\n- `operator-name` (optional): Specific operator to diagnose\n- `namespace` (optional): Specific namespace to check\n- `--fix` (optional): Attempt automatic fixes with confirmation\n- `--cluster` (optional): Run cluster-wide diagnostics\n\nSee [commands/diagnose.md](commands/diagnose.md) for full documentation.\n\n---\n\n#### `/olm:catalog` - Manage Catalog Sources\n\nManage catalog sources for operator discovery and installation.\n\n**Usage:**\n```bash\n/olm:catalog list                                     # List all catalogs\n/olm:catalog add my-catalog registry.io/catalog:v1    # Add custom catalog\n/olm:catalog remove my-catalog                        # Remove catalog\n/olm:catalog refresh redhat-operators                 # Refresh catalog\n/olm:catalog status custom-catalog                    # Check catalog health\n```\n\n**What it does:**\n- Lists all catalog sources with health status\n- Adds custom or private catalog sources\n- Removes catalog sources (with operator usage warnings)\n- Refreshes catalogs to get latest operator updates\n- Checks catalog source health and connectivity\n- Shows catalog pod status and troubleshooting info\n\n**Subcommands:**\n- `list`: Show all catalog sources\n- `add <name> <image>`: Add new catalog source\n- `remove <name>`: Remove catalog source\n- `refresh <name>`: Force catalog refresh\n- `status <name>`: Check catalog health\n\nSee [commands/catalog.md](commands/catalog.md) for full documentation.\n\n---\n\n#### `/olm:opm` - Build and Manage Operator Catalogs\n\nExecute opm (Operator Package Manager) commands for building and managing operator catalogs.\n\n**Usage:**\n```bash\n/olm:opm build-index-image catalog quay.io/myorg/mycatalog:v1.0.0\n/olm:opm build-semver-index-image catalog-config.yaml quay.io/myorg/mycatalog:v1.0.0\n/olm:opm generate-semver-template quay.io/org/bundle:v1.0.0,quay.io/org/bundle:v1.0.1\n/olm:opm list packages quay.io/olmqe/nginx8518-index-test:v1\n/olm:opm list channels quay.io/olmqe/nginx8518-index-test:v1 nginx85187\n/olm:opm list bundles quay.io/olmqe/nginx8518-index-test:v1\n```\n\n**What it does:**\n- Builds operator catalog index images from catalog directories\n- Creates multi-architecture catalog indexes using semver templates\n- Generates semver template configuration files for operator catalogs\n- Lists packages, channels, and bundles in catalog indexes\n- Supports both cacheless and normal builds\n- Validates catalog configurations before building\n\n**Subcommands:**\n- `build-index-image`: Build an index from an existing catalog directory\n- `build-semver-index-image`: Build an index from a semver template\n- `generate-semver-template`: Generate a semver template file\n- `list packages`: List all packages in a catalog index\n- `list channels`: List channels for packages\n- `list bundles`: List bundles for packages\n\n**Common Options:**\n- `--cacheless`: Build cacheless image (uses scratch as base)\n- `--arch=<arch>`: Specify architecture (default: multi for multi-arch)\n- `--base-image=<image>`: Custom base image for the index\n- `--builder-image=<image>`: Custom builder image\n\nSee [commands/opm.md](commands/opm.md) for full documentation.\n\n---\n\n### Debugging Commands\n\n#### `/olm:debug` - Debug OLM Issues\n\nDebug OLM issues using must-gather logs and source code analysis.\n\n**Usage:**\n```\n/olm:debug <issue-description> <must-gather-path> [olm-version]\n```\n\n**Arguments:**\n- `issue-description`: Brief description of the OLM issue being investigated\n- `must-gather-path`: Path to the must-gather log directory\n- `olm-version`: (Optional) Either `olmv0` (default) or `olmv1`\n\n**Examples:**\n\n1. Debug a CSV stuck in pending state (OLMv0):\n   ```\n   /olm:debug \"CSV stuck in pending state\" /path/to/must-gather\n   ```\n\n2. Debug OLMv1 ClusterExtension issue:\n   ```\n   /olm:debug \"ClusterExtension installation failing\" /path/to/must-gather olmv1\n   ```\n\n3. Debug operator upgrade issue:\n   ```\n   /olm:debug \"Operator upgrade from v1.0 to v2.0 fails with dependency resolution error\" ~/Downloads/must-gather.local.123456 olmv0\n   ```\n\n**How it works:**\n\n1. **Extracts OCP version** from the must-gather logs\n2. **Clones appropriate repositories**:\n   - OLMv0: `operator-framework-olm`\n   - OLMv1: `operator-framework-operator-controller` and `cluster-olm-operator`\n3. **Checks out the correct branch** matching the OCP version (e.g., `release-4.14`)\n4. **Analyzes logs** to identify errors, warnings, and failed reconciliations\n5. **Queries Jira** for known bugs in OCPBUGS project (OLM component) matching the OCP version\n6. **Matches errors** with known bugs based on error messages and symptoms\n7. **Correlates errors with source code** to identify root causes\n8. **Generates a comprehensive analysis report** with recommendations and links to related Jira issues\n\n**Output:**\n\nThe command creates a working directory at `.work/olm-debug/<timestamp>/` containing:\n\n- `analysis.md`: Comprehensive analysis report with known bugs section\n- `relevant-logs.txt`: Extracted relevant log entries\n- `code-references.md`: Links to relevant source code\n- `known-bugs.md`: List of potentially related Jira bugs with match confidence and workarounds\n- `repos/`: Cloned repository directories\n\nSee [commands/debug.md](commands/debug.md) for full documentation.\n\n---\n\n## Example Workflows\n\n### Quick Start - Install and Monitor\n\n```bash\n# Search for operator\n/olm:search cert-manager\n\n# Install operator\n/olm:install openshift-cert-manager-operator\n\n# Check status\n/olm:status openshift-cert-manager-operator\n\n# List all operators\n/olm:list\n```\n\n### Production Workflow - Manual Approval\n\n```bash\n# Install with manual approval for better control\n/olm:install external-secrets-operator eso-operator stable-v0.10 redhat-operators --approval=Manual\n\n# Check for updates\n/olm:status external-secrets-operator eso-operator\n\n# Upgrade when ready\n/olm:upgrade external-secrets-operator eso-operator\n\n# Approve the upgrade\n/olm:approve external-secrets-operator eso-operator\n```\n\n### Troubleshooting Workflow\n\n```bash\n# Operator not working properly\n/olm:status problematic-operator\n\n# Run diagnostics\n/olm:diagnose problematic-operator\n\n# If issues found, attempt fixes\n/olm:diagnose problematic-operator namespace --fix\n```\n\n### Clean Uninstall Workflow\n\n```bash\n# Check operator status before uninstalling\n/olm:status openshift-cert-manager-operator\n\n# Uninstall with full cleanup\n/olm:uninstall openshift-cert-manager-operator cert-manager-operator --remove-crds --remove-namespace\n\n# Verify cleanup\n/olm:diagnose --cluster\n```\n\n### Catalog Management Workflow\n\n```bash\n# List available catalogs\n/olm:catalog list\n\n# Build a custom catalog index\n/olm:opm generate-semver-template quay.io/org/bundle:v1.0.0,quay.io/org/bundle:v1.1.0 --output=my-catalog.yaml\n/olm:opm build-semver-index-image my-catalog.yaml quay.io/myorg/my-catalog:v1.0\n\n# Add custom catalog\n/olm:catalog add my-operators quay.io/myorg/my-catalog:v1.0\n\n# Search for operators in new catalog\n/olm:search --catalog my-operators\n\n# Check catalog health\n/olm:catalog status my-operators\n```\n\n### Advanced Debugging Workflow\n\n```bash\n# Debug OLM issues using must-gather logs\n/olm:debug \"CSV stuck in pending\" /path/to/must-gather\n```\n\n## OLM Version Support\n\n### OLMv0\n- Used in OpenShift 4.x (traditional OLM)\n- Repository: [operator-framework-olm](https://github.com/openshift/operator-framework-olm)\n- Key resources: CSV, Subscription, InstallPlan, OperatorGroup\n\n### OLMv1\n- Next-generation OLM architecture\n- Repositories:\n  - [operator-framework-operator-controller](https://github.com/openshift/operator-framework-operator-controller)\n  - [cluster-olm-operator](https://github.com/openshift/cluster-olm-operator)\n- Key resources: ClusterExtension, Catalog\n\n## Troubleshooting\n\n### Operator Not Found\n```bash\n/olm:search <operator-name>                           # Search for operator\noc get packagemanifests -n openshift-marketplace      # List manually\n/olm:catalog list                                     # Check catalog sources\n```\n\n### Installation Issues\n```bash\n/olm:status <operator-name>                           # Check detailed status\n/olm:diagnose <operator-name>                         # Run diagnostics\noc get csv -n <namespace>                             # Check CSV manually\noc describe csv <csv-name> -n <namespace>             # Detailed CSV info\n```\n\n### Upgrade Issues\n```bash\n/olm:status <operator-name>                           # Check for pending upgrades\n/olm:approve <operator-name>                          # Approve if manual mode\n/olm:diagnose <operator-name> --fix                   # Fix issues\n```\n\n### Uninstallation Issues\n```bash\n# CSV won't delete\noc get csv <csv-name> -n <namespace> -o yaml | grep finalizers\n\n# Namespace stuck in Terminating\n/olm:diagnose \"\" <namespace> --fix\n\n# Orphaned resources\n/olm:diagnose --cluster\n```\n\n### Catalog Source Issues\n```bash\n/olm:catalog status <catalog-name>                    # Check catalog health\n/olm:catalog refresh <catalog-name>                   # Refresh catalog\noc logs -n openshift-marketplace <catalog-pod>        # Check logs\n```\n\n### Debugging Issues\n\n**Cannot determine OCP version from must-gather:**\n- **Solution**: Manually specify the OCP version when prompted, or check that the must-gather is complete\n\n**Repository clone fails:**\n- **Solution**: Check network connectivity and GitHub access. You can manually clone the repositories and point the command to them.\n\n**Branch not found for OCP version:**\n- **Solution**: The command will fall back to the `main` branch. Be aware that there may be version differences.\n\n**Jira access fails or returns no results:**\n- **Solution**: Check network connectivity to https://issues.redhat.com/. The command will continue with analysis even if Jira is unavailable.\n\n**Too many potential bug matches returned:**\n- **Solution**: Review the `known-bugs.md` file and focus on high-confidence matches. Verify each match by reading the full bug description in Jira.\n\n## Resources\n\n- [Red Hat OpenShift: Operators Documentation](https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html/operators/)\n- [Red Hat OpenShift: Administrator Tasks](https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html/operators/administrator-tasks)\n- [Red Hat OpenShift: Troubleshooting Operators](https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html/operators/administrator-tasks#olm-troubleshooting-operator-issues)\n- [Operator Lifecycle Manager Documentation](https://olm.operatorframework.io/)\n- [OperatorHub.io](https://operatorhub.io/) - Browse operators online\n- [Must-gather Documentation](https://docs.openshift.com/container-platform/latest/support/gathering-cluster-data.html)\n- [OCPBUGS Jira Project](https://issues.redhat.com/projects/OCPBUGS/)\n\n## Contributing\n\nTo add new commands to this plugin:\n\n1. Create a new `.md` file in `plugins/olm/commands/`\n2. Follow the command definition format in existing commands\n3. Update this README with the new command documentation\n4. Run `make lint` to validate the plugin structure\n\n## Support\n\nFor issues or feature requests, please file an issue at:\nhttps://github.com/openshift-eng/ai-helpers/issues\n",
        "plugins/olm-team/README.md": "# OLM Team Plugin\n\nDevelopment utilities and onboarding tools for the OLM (Operator Lifecycle Manager) team.\n\n## Overview\n\nThis plugin provides tools to help OLM team members quickly set up their development environment and understand the OLM ecosystem. It automates the process of forking, cloning, and configuring all necessary OLM repositories with proper remote configurations.\n\n## Prerequisites\n\n- Claude Code installed\n- GitHub CLI (`gh`) installed and authenticated\n- Git installed and configured\n- SSH keys configured with GitHub (or HTTPS credentials)\n\n## Commands\n\n### `/olm-team:configure-agent` - Configure k8s-ocp-olm-expert Agent\n\nCreates or updates the configuration file for the k8s-ocp-olm-expert agent with paths to local OLM repositories.\n\n**Usage:**\n```bash\n/olm-team:configure-agent\n```\n\n**What it does:**\n- Searches for OLM repositories in common locations\n- Detects which repositories you have cloned locally\n- Recommends running `/olm-team:dev-setup` if repositories are missing\n- Helps you create a configuration file at `~/.config/claude-code/olm-agent-config.json`\n- Validates all configured paths\n- Enables the k8s-ocp-olm-expert agent to provide code-aware responses\n\n**When to use:**\n- **After `/olm-team:dev-setup`**: Automatically configures the agent with cloned repository paths\n- **With existing repositories**: If you already have OLM repositories cloned, this command finds them\n- **To update paths**: If you move repositories or clone additional ones\n\n**Interactive process:**\n1. Scans common directories for OLM repositories\n2. Shows which repositories were found and which are missing\n3. If < 80% of repositories found, recommends running `/olm-team:dev-setup`\n4. Prompts for any missing repository paths\n5. Validates all paths and creates configuration file\n6. Displays summary and next steps\n\n**Configuration file created:**\n`~/.config/claude-code/olm-agent-config.json` with paths to:\n- openshift-docs\n- operator-lifecycle-manager, operator-registry, api (OLM v0 upstream)\n- operator-framework-olm, operator-marketplace (OLM v0 downstream)\n- operator-controller (OLM v1 upstream)\n- operator-framework-operator-controller, cluster-olm-operator (OLM v1 downstream)\n\nSee [commands/configure-agent.md](commands/configure-agent.md) for full documentation.\n\n---\n\n### `/olm-team:dev-setup` - Development Environment Setup\n\nAutomates the complete onboarding process for new OLM team members by setting up all development repositories.\n\n**Usage:**\n```bash\n/olm-team:dev-setup                          # Prompts for target directory\n/olm-team:dev-setup ~/go/src/github.com/     # Uses specified directory\n```\n\n**What it does:**\n- Explains the OLM repository ecosystem (upstream vs downstream, v0 vs v1)\n- Forks all OLM repositories to your GitHub account (if not already forked)\n- Clones repositories locally with \"origin\" pointing to your fork\n- Adds \"upstream\" remote pointing to the original repository\n- Provides educational context about each repository's purpose\n- Generates a comprehensive summary with next steps\n\n**Repositories configured:**\n\n**OLM v0 (Traditional Architecture)**\n- Upstream:\n  - `operator-framework/operator-registry` - Catalog and bundle management\n  - `operator-framework/operator-lifecycle-manager` - Core OLM v0 runtime\n  - `operator-framework/api` - API definitions and CRD schemas\n- Downstream:\n  - `openshift/operator-framework-olm` - OpenShift distribution of OLM v0\n  - `operator-framework/operator-marketplace` - OperatorHub integration\n\n**OLM v1 (Next-Generation Architecture)**\n- Upstream:\n  - `operator-framework/operator-controller` - Core OLM v1 runtime\n- Downstream:\n  - `openshift/operator-framework-operator-controller` - OpenShift distribution of OLM v1\n  - `openshift/cluster-olm-operator` - OLM cluster operator for OpenShift\n\n**Arguments:**\n- `target-directory` (optional): Base directory for cloning repositories\n  - Common choices: `~/go/src/github.com/`, `~/src/`, `~/code/olm/`\n  - If not provided, you'll be prompted to choose\n\n**Directory structure created:**\n```\n<target-directory>/\n  <github-username>/\n    operator-registry/\n      .git/\n        config (origin ‚Üí your fork, upstream ‚Üí operator-framework)\n    operator-lifecycle-manager/\n    api/\n    operator-framework-olm/\n    operator-marketplace/\n    operator-controller/\n    operator-framework-operator-controller/\n    cluster-olm-operator/\n```\n\nSee [commands/dev-setup.md](commands/dev-setup.md) for full documentation.\n\n---\n\n### `/olm-team:ep-watch` - Watch Enhancement PRs from Other Teams\n\nWatches open Enhancement Proposal PRs from other teams that may impact OLM.\n\n**Usage:**\n```bash\n/olm-team:ep-watch\n```\n\n**What it does:**\n- Fetches open PRs from the openshift/enhancements repository\n- Filters out PRs created by OLM team members (11 members)\n- Analyzes PR content for OLM-related topics using weighted scoring\n- Returns up to 3 most relevant PRs from other teams\n- Shows why each PR matters to OLM with impact assessment\n\n**When to use:**\n- **Weekly team meetings**: Stay aware of cross-team dependencies\n- **Planning sessions**: Identify potential impacts on OLM architecture\n- **Design reviews**: Ensure OLM perspective is considered in other teams' proposals\n\n**Output includes:**\n- PR number, title, and author\n- How long ago it was opened\n- Relevance score (HIGH/MEDIUM/LOW)\n- Why it's relevant to OLM (matched topics)\n- Potential impacts on OLM\n- Direct link to the PR for review\n\n**Example:**\n```\n/olm-team:ep-watch\n\n‚Üí Returns:\n  PR #1938: Gateway API without OLM (MEDIUM relevance)\n  - Why: Discusses removing OLM from Gateway API installation\n  - Impact: Other teams removing OLM from their workflows\n```\n\nSee [commands/ep-watch.md](commands/ep-watch.md) for full documentation.\n\n---\n\n## Understanding the OLM Ecosystem\n\n### Upstream vs Downstream\n\n**Upstream (operator-framework)**:\n- Community-driven development\n- Latest features and improvements\n- Base for all downstream distributions\n\n**Downstream (openshift)**:\n- OpenShift-specific customizations\n- Enterprise features and security patches\n- What ships in Red Hat OpenShift products\n\n### OLM v0 vs OLM v1\n\n**OLM v0 (Traditional)**:\n- Current production architecture in OpenShift\n- Uses CSV, Subscription, InstallPlan resources\n- Mature and battle-tested\n\n**OLM v1 (Next-Generation)**:\n- Simplified architecture\n- Better dependency resolution\n- Uses ClusterExtension and Catalog resources\n- Active development, future of OLM\n\n## Example Workflow\n\n### Initial Setup\n```bash\n# Set up your entire development environment\n/olm-team:dev-setup ~/go/src/github.com/\n\n# The command will:\n# 1. Fork all 8 OLM repositories to your GitHub account\n# 2. Clone them to ~/go/src/github.com/<your-username>/\n# 3. Configure origin (fork) and upstream (original) remotes\n# 4. Explain each repository's purpose\n```\n\n### After Setup - Typical Development Workflow\n```bash\n# Navigate to a repository\ncd ~/go/src/github.com/<your-username>/operator-lifecycle-manager\n\n# Create a feature branch\ngit checkout -b feature/my-feature\n\n# Make changes and commit\ngit add .\ngit commit -m \"Add new feature\"\n\n# Push to your fork\ngit push origin feature/my-feature\n\n# Create pull request (from your fork to upstream)\ngh pr create --web\n\n# Keep your fork in sync with upstream\ngit fetch upstream\ngit checkout main  # or master, depending on the repo's default branch\ngit merge upstream/HEAD\ngit push origin HEAD\n```\n\n## Development Resources\n\nAfter running `/olm-team:dev-setup`, you'll have access to:\n\n1. **Source Code**: All OLM repositories cloned and ready for development\n2. **Repository Context**: Understanding of each repository's role\n3. **Remote Configuration**: Proper setup for contributing (fork + upstream)\n4. **Next Steps Guide**: Commands and workflows for getting started\n\n## k8s-ocp-olm-expert Agent\n\nThis plugin includes the **k8s-ocp-olm-expert agent**, an elite software engineering agent with deep expertise in Kubernetes, OpenShift, and OLM.\n\n### What it does\n\nThe agent automatically engages when you:\n- Debug Kubernetes resources (pods, deployments, etc.)\n- Work with OpenShift-specific features\n- Develop or troubleshoot operators using OLM\n- Review manifests, CRDs, or operator bundles\n- Ask questions about k8s/OCP/OLM concepts\n\n### Features\n\n- **Documentation Integration**: Searches local openshift-docs and provides file references with line numbers\n- **Code-Aware Responses**: References upstream/downstream OLM v0 and v1 source code\n- **Documentation Gap Identification**: Identifies what's documented and what's missing\n- **Production-Ready Advice**: Security-minded, version-aware recommendations\n\n### Configuration Required\n\nThe agent requires configuration to know where your local repositories are located.\n\n**Quick setup:**\n```bash\n# Option 1: New team member (clones all repos + configures agent)\n/olm-team:dev-setup\n\n# Option 2: Have repos already (just configures agent)\n/olm-team:configure-agent\n```\n\n### Usage Example\n\n```\nUser: Where is catalogd CA configuration documented in openshift-docs?\n\nAgent: [k8s-ocp-olm-expert automatically engages]\n\n## Documentation Found\n\n**CA Certificate Configuration for Image Registries:**\n- **File:** `modules/images-configuration-cas.adoc` (line 8): \"Configuring additional trust stores for image registry access\"\n...\n\n## Documentation Gaps\n\n**What's missing:**\n- No specific documentation for catalogd CA certificate configuration\n...\n```\n\nSee [skills/k8s-ocp-olm-expert/README.md](skills/k8s-ocp-olm-expert/README.md) for detailed agent documentation.\n\n---\n\n## Additional Resources\n\n- **OLM Documentation**: https://olm.operatorframework.io/\n- **OpenShift OLM Docs**: https://docs.openshift.com/container-platform/latest/operators/\n- **Contributing Guides**: Check `CONTRIBUTING.md` in each repository\n- **OLM Slack**: Join the #olm channel in Kubernetes Slack\n- **Agent Skill Documentation**: [skills/k8s-ocp-olm-expert/SKILL.md](skills/k8s-ocp-olm-expert/SKILL.md)\n\n## Troubleshooting\n\n### GitHub CLI Not Authenticated\n```bash\ngh auth login\n# Follow the prompts to authenticate\n```\n\n### SSH Key Issues\n```bash\n# Test SSH connection\nssh -T git@github.com\n\n# If fails, set up SSH keys\nssh-keygen -t ed25519 -C \"your-email@example.com\"\n# Add key to GitHub: https://github.com/settings/keys\n```\n\n### Directory Permission Issues\nChoose a directory where you have write permissions, typically:\n- Your home directory: `~/`\n- Your user-specific directories: `~/go/`, `~/src/`, `~/code/`\n\nAvoid system directories like `/usr/`, `/opt/` which require elevated permissions.\n\n### Fork Already Exists\nIf you've already forked a repository, the command will detect this and skip the fork step, proceeding directly to cloning.\n\n### Repository Already Cloned\nIf a repository is already cloned in the target directory, the command will ask if you want to skip it or re-clone.\n\n## Contributing\n\nTo add new commands to this plugin:\n\n1. Create a new `.md` file in `plugins/olm-team/commands/`\n2. Follow the command definition format (see existing commands)\n3. Update this README with the new command documentation\n4. Run `make lint` to validate the plugin structure\n\n## Support\n\nFor issues or feature requests, please file an issue at:\nhttps://github.com/openshift-eng/ai-helpers/issues\n",
        "plugins/prow-job/README.md": "# Prow Job Plugin\n\nAnalyze and inspect Prow CI job results for OpenShift development.\n\n## Commands\n\n### `/prow-job:analyze-test-failure`\n\nAnalyze a failed Prow test job and generate a detailed failure report.\n\n### `/prow-job:analyze-resource`\n\nAnalyze Kubernetes resources from a Prow job to debug issues.\n\nGenerates interactive HTML reports with resource timelines, logs, and events.\n\n### `/prow-job:extract-must-gather`\n\nExtract and analyze must-gather archives from Prow job artifacts.\n\n## Skills\n\nThis plugin includes advanced skills with Python helper scripts for log analysis and report generation. See the [skills/](skills/) directory for detailed implementation guides.\n\n## Installation\n\n```bash\n/plugin install prow-job@ai-helpers\n```\n\n",
        "plugins/agendas/README.md": "# Agendas Plugin\n\nGenerate structured meeting agendas to streamline team collaboration and decision-making processes.\n\n## Features\n\n- üìã **Outcome Refinement Agendas** - Analyze OCPSTRAT outcome issues and generate structured refinement meeting agendas\n- üö® **Issue Hygiene Detection** - Automatically identify missing assignments, incorrect issue types, and other routine problems\n- üìä **Team Overload Analysis** - Detect component teams that may be overloaded based on issue assignments\n- ‚è∞ **Age-Based Prioritization** - Flag outcomes that have been open too long or stuck in \"New\" status\n- ‚úÖ **Actionable Output** - Ready-to-use Markdown agendas that can be copied directly into Confluence\n\n## Prerequisites\n\n- Claude Code installed\n- Jira MCP server configured (same as Jira plugin)\n\n### Setting up Jira MCP Server\n\n```bash\n# Add the Atlassian MCP server\nclaude mcp add atlassian npx @modelcontextprotocol/server-atlassian\n```\n\nOR you can use an already running Jira MCP Server:\n\n```bash\n# Add the Atlassian MCP server\nclaude mcp add --transport sse atlassian http https://localhost:8080/sse\n```\n\nConfigure your Jira credentials according to the [Atlassian MCP documentation](https://github.com/modelcontextprotocol/servers/tree/main/src/atlassian).\n\n### Running Jira MCP Server locally with podman\n\n```bash\n# Start the atlassian mcp server using podman\npodman run -i --rm -p 8080:8080 -e \"JIRA_URL=https://issues.redhat.com\" -e \"JIRA_USERNAME\" -e \"JIRA_API_TOKEN\" -e \"JIRA_PERSONAL_TOKEN\" -e \"JIRA_SSL_VERIFY\" ghcr.io/sooperset/mcp-atlassian:latest --transport sse --port 8080 -vv\n```\n\n#### Getting Tokens\nYou'll need to generate your own tokens:\n\n- For JIRA API TOKEN, use https://id.atlassian.com/manage-profile/security/api-tokens\n- For JIRA PERSONAL TOKEN, use https://issues.redhat.com/secure/ViewProfile.jspa?selectedTab=com.atlassian.pats.pats-plugin:jira-user-personal-access-tokens\n\n## Installation\n\n### From the OpenShift AI Helpers Marketplace\n\n```bash\n# Add the marketplace (one-time setup)\n/plugin marketplace add https://raw.githubusercontent.com/openshift-eng/ai-helpers/main/marketplace.json\n\n# Install the plugin\n/plugin install agendas\n```\n\n### Manual Installation\n\n```bash\n# Clone the repository\ngit clone https://github.com/openshift-eng/ai-helpers.git\n\n# Copy to Claude Code plugins directory\ncp -r ai-helpers/plugins/agendas ~/.claude/plugins/\n\n# Enable the plugin\n/plugin enable agendas\n```\n\n## Available Commands\n\n### `/agendas:outcome-refinement` - Outcome Refinement Meeting Agenda\n\nAnalyze OCPSTRAT outcome issues and generate a structured meeting agenda for outcome refinement sessions. The command automatically identifies common issues that require human follow-up and organizes them into actionable discussion points.\n\n**Usage:**\n```bash\n/agendas:outcome-refinement\n```\n\n**What It Checks:**\n\nThe command analyzes outcome issues and flags:\n\n- **Missing Assignments**: Outcomes without assignee, architect, QA contact, or doc contact\n- **Incorrect Child Issues**: Outcomes with non-Feature child issue types\n- **Status Mismatches**: Child issues being actively worked on while parent outcome shows wrong status\n- **Age Analysis**: Outcomes that have been open too long (especially in \"New\" status for over a year)\n- **Scope Concerns**: Outcomes with active child issues but open for years, indicating potential scope creep\n- **Team Overload**: Components commonly assigned across multiple outcomes, indicating team capacity issues\n\n**Output Format:**\n\nThe command generates a ready-to-use Markdown agenda:\n\n```markdown\n# Outcome Refinement Agenda\n**Outcome Issues**: [count]\n\n## üö® Critical Issues ([count])\n- **[OCPSTRAT-1234]** BGP integration with public clouds - *Critical, needs immediate attention*\n- **[OCPSTRAT-1235]** Consistent Ingress/Egress into OpenShift clusters - *High, assign to team lead*\n\n## üìù Needs Clarification ([count])\n- **[OCPSTRAT-1238]** Missing architect\n- **[OCPSTRAT-1239]** Component team is overloaded\n- **[OCPSTRAT-1240]** Outcome has been open for years with no delivery\n\n## üìã Action Items\n- [ ] Set architect for OCPSTRAT-1236 to SME architect (immediate)\n- [ ] Schedule review for OCPSTRAT-1236 (this week)\n```\n\nSee [commands/outcome-refinement.md](commands/outcome-refinement.md) for full documentation.\n\n## Troubleshooting\n\n### \"Could not find issues\"\n- Verify you have access to OCPSTRAT project in Jira\n- Check that your Jira MCP server is properly configured\n- Ensure your credentials have permission to query the project\n\n### Empty or incomplete agenda\n- Verify the JQL query is returning results\n- Check that the outcome issues have child issues\n- Ensure the date ranges are appropriate for your analysis\n\n## Contributing\n\nContributions welcome! Please submit pull requests to the [ai-helpers repository](https://github.com/openshift-eng/ai-helpers).\n\n## License\n\nApache-2.0\n",
        "plugins/openshift/README.md": "# OpenShift Plugin\n\nOpenShift development utilities and workflow helpers for Claude Code.\n\n## Commands\n\n### `/openshift:new-e2e-test`\n\nWrite and validate new OpenShift E2E tests using the Ginkgo framework.\n\n### `/openshift:rebase`\n\nRebase an OpenShift fork of an upstream repository to a new upstream release.\n\nThis command automates the complex process of rebasing OpenShift forks following the UPSTREAM commit conventions.\n\n### `/openshift:bump-deps`\n\nAutomates the process of bumping dependencies in OpenShift organization projects. It analyzes the dependency, determines\nthe appropriate version to bump to, updates the necessary files (go.mod, go.sum, package.json, etc.), runs tests,\nand optionally creates Jira tickets and pull requests.\n\n### `/openshift:create-cluster`\n\nExtract OpenShift installer from release image and create an OCP cluster.\n\nThis command automates the process of extracting the installer from a release image and creating a new OpenShift cluster on various platforms (AWS, Azure, GCP, vSphere, OpenStack).\n\n### `/openshift:destroy-cluster`\n\nDestroy an OpenShift cluster created by the create-cluster command.\n\nThis command safely destroys a cluster and cleans up all cloud resources. Includes safety confirmations and optional backup of cluster information.\n\n### `/openshift:ironic-status`\n\nCheck status of Ironic baremetal nodes in OpenShift cluster.\n\nSee the [commands/](commands/) directory for full documentation of each command.\n\n## Installation\n\n### From the Claude Code Plugin Marketplace\n\n1. **Add the marketplace** (if not already added):\n   ```bash\n   /plugin marketplace add openshift-eng/ai-helpers\n   ```\n\n2. **Install the openshift plugin**:\n   ```bash\n   /plugin install openshift@ai-helpers\n   ```\n\n3. **Use the commands**:\n   ```bash\n   /openshift:bump-deps k8s.io/api\n   ```\n\n## Available Commands\n\n### E2E Test Generation\n\n#### `/openshift:new-e2e-test` - Generate E2E Tests\n\nGenerate end-to-end tests for OpenShift features.\n\nSee [commands/new-e2e-test.md](commands/new-e2e-test.md) for full documentation.\n\n### Dependency Bumping\n\n#### `/openshift:bump-deps` - Bump Dependencies\n\nAutomates dependency updates in OpenShift projects with comprehensive analysis, testing, and optional Jira ticket and PR creation.\n\n**Basic Usage:**\n```bash\n# Bump to latest version\n/openshift:bump-deps k8s.io/api\n\n# Bump to specific version\n/openshift:bump-deps golang.org/x/net v0.20.0\n\n# Bump with Jira ticket\n/openshift:bump-deps github.com/spf13/cobra --create-jira\n\n# Bump with Jira ticket and PR\n/openshift:bump-deps github.com/prometheus/client_golang --create-jira --create-pr\n```\n\n**Supported Dependency Types:**\n- Go modules (go.mod)\n- npm packages (package.json)\n- Container images (Dockerfile)\n- Python packages (requirements.txt, pyproject.toml)\n\n**Key Features:**\n- Automatic version discovery and compatibility checking\n- Changelog and breaking change analysis\n- Automated testing (unit, integration, e2e)\n- Jira ticket creation with comprehensive details\n- Pull request creation with proper formatting\n- Handles direct and indirect dependencies\n- Security vulnerability detection\n- Batch updates for related dependencies\n\n**Arguments:**\n- `<dependency>` (required): Package identifier (e.g., `k8s.io/api`, `@types/node`)\n- `[version]` (optional): Target version (defaults to latest stable)\n- `--create-jira`: Create a Jira ticket for the update\n- `--create-pr`: Create a pull request (implies --create-jira)\n- `--jira-project <PROJECT>`: Specify Jira project (default: auto-detect)\n- `--component <COMPONENT>`: Specify Jira component (default: auto-detect)\n- `--skip-tests`: Skip running tests (creates draft PR)\n- `--force`: Force update even if tests fail\n\n**Examples:**\n\n1. Simple bump to latest:\n   ```bash\n   /openshift:bump-deps k8s.io/client-go\n   ```\n\n2. Bump with custom Jira project:\n   ```bash\n   /openshift:bump-deps sigs.k8s.io/controller-runtime --create-jira --jira-project OCPBUGS\n   ```\n\n3. Bump container image:\n   ```bash\n   /openshift:bump-deps registry.access.redhat.com/ubi9/ubi-minimal\n   ```\n\n4. Batch update Kubernetes dependencies:\n   ```bash\n   /openshift:bump-deps \"k8s.io/*\"\n   ```\n\nSee [commands/bump-deps.md](commands/bump-deps.md) for full documentation.\n\n### Cluster Management\n\n#### `/openshift:create-cluster` - Create OCP Clusters\n\nExtract the OpenShift installer from a release image and create a new OpenShift Container Platform cluster. This command automates installer extraction and cluster creation for development and testing purposes.\n\n**‚ö†Ô∏è Important**: This is a last-resort tool. For most workflows, use **Cluster Bot**, **Gangway**, or **Multi-PR Testing in CI** instead. Only use this when you need full control over cluster configuration or are testing installer changes.\n\n**Basic Usage:**\n```bash\n# Interactive mode (prompts for all options)\n/openshift:create-cluster\n\n# With release image and platform\n/openshift:create-cluster quay.io/openshift-release-dev/ocp-release:4.21.0-ec.2-x86_64 aws\n\n# With CI build\n/openshift:create-cluster registry.ci.openshift.org/ocp/release:4.21.0-0.ci-2025-10-27-031915 gcp\n```\n\n**Prerequisites:**\n- OpenShift CLI (`oc`) installed\n- Cloud provider credentials configured (AWS, Azure, GCP, etc.)\n- Pull secret from [Red Hat Console](https://console.redhat.com/openshift/install/pull-secret)\n- Domain/DNS configuration (e.g., Route53 hosted zone for AWS)\n\n**Supported Platforms:**\n- AWS (Amazon Web Services)\n- Azure (Microsoft Azure)\n- GCP (Google Cloud Platform)\n- vSphere (VMware vSphere)\n- OpenStack\n- none (Bare metal / platform-agnostic)\n\n**Key Features:**\n- Automatic installer extraction from release images\n- Version-specific installer caching\n- Interactive configuration generation\n- Post-installation verification\n- Cluster credentials and access information\n\n**Arguments:**\n- `[release-image]` (optional): OpenShift release image (prompted if not provided)\n- `[platform]` (optional): Target platform (prompted if not provided)\n\n**Examples:**\n\n1. Create cluster with production release on AWS:\n   ```bash\n   /openshift:create-cluster quay.io/openshift-release-dev/ocp-release:4.21.0-ec.2-x86_64 aws\n   ```\n\n2. Create cluster with CI build interactively:\n   ```bash\n   /openshift:create-cluster registry.ci.openshift.org/ocp/release:4.21.0-0.ci-2025-10-27-031915\n   ```\n\n3. Full interactive mode:\n   ```bash\n   /openshift:create-cluster\n   ```\n\nSee [commands/create-cluster.md](commands/create-cluster.md) for full documentation.\n\n#### `/openshift:destroy-cluster` - Destroy OCP Clusters\n\nSafely destroy an OpenShift Container Platform cluster that was created using `/openshift:create-cluster`. This command handles cleanup of all cloud resources with built-in safety confirmations.\n\n**‚ö†Ô∏è WARNING**: This operation is **irreversible** and permanently deletes all cluster resources and data.\n\n**Basic Usage:**\n```bash\n# Interactive mode (searches for installation directories)\n/openshift:destroy-cluster\n\n# With specific installation directory\n/openshift:destroy-cluster ./my-cluster-install-20251028-120000\n\n# With full path\n/openshift:destroy-cluster /path/to/cluster-install-dir\n```\n\n**Safety Features:**\n- Requires explicit \"yes\" confirmation before destruction\n- Displays cluster information before proceeding\n- Optional backup of cluster credentials and metadata\n- Validates installation directory and metadata\n- Provides manual cleanup instructions if automated cleanup fails\n\n**What Gets Deleted:**\n- All cluster VMs and compute resources\n- Load balancers and networking resources\n- Storage volumes and persistent data\n- DNS records (if managed by installer)\n- All cluster configuration\n\n**Arguments:**\n- `[install-dir]` (optional): Path to cluster installation directory (prompted if not provided)\n\n**Examples:**\n\n1. Destroy cluster interactively:\n   ```bash\n   /openshift:destroy-cluster\n   ```\n\n2. Destroy specific cluster:\n   ```bash\n   /openshift:destroy-cluster ./test-cluster-install-20251028-120000\n   ```\n\nSee [commands/destroy-cluster.md](commands/destroy-cluster.md) for full documentation.\n\n## Development\n\n### Adding New Commands\n\nTo add a new command to this plugin:\n\n1. Create a new markdown file in `commands/`:\n   ```bash\n   touch plugins/openshift/commands/your-command.md\n   ```\n\n2. Follow the structure from existing commands (see `commands/bump-deps.md` for reference)\n\n3. Include these sections:\n   - Name\n   - Synopsis\n   - Description\n   - Implementation\n   - Return Value\n   - Examples\n   - Arguments\n   - Error Handling\n   - Notes\n\n4. Test your command:\n   ```bash\n   /openshift:your-command\n   ```\n\n### Plugin Structure\n\n```\nplugins/openshift/\n‚îú‚îÄ‚îÄ .claude-plugin/\n‚îÇ   ‚îî‚îÄ‚îÄ marketplace.json          # Plugin metadata\n‚îú‚îÄ‚îÄ commands/\n‚îÇ   ‚îú‚îÄ‚îÄ bump-deps.md              # Dependency bumping command\n‚îÇ   ‚îú‚îÄ‚îÄ new-e2e-test.md           # E2E test generation\n‚îÇ   ‚îî‚îÄ‚îÄ ...                        # Additional commands\n‚îî‚îÄ‚îÄ README.md                      # This file\n```\n\n## Related Plugins\n\n- **utils** - General utilities including `process-renovate-pr` for processing Renovate PRs\n- **jira** - Jira automation and issue management\n- **git** - Git workflow automation\n- **ci** - OpenShift CI integration\n\n## Contributing\n\nContributions are welcome! When adding new OpenShift-related commands:\n\n1. Ensure the command is specific to OpenShift development workflows\n2. Follow the existing command structure and documentation format\n3. Include comprehensive examples and error handling\n4. Test with real OpenShift projects\n5. Update this README with new command documentation\n\n## License\n\nSee [LICENSE](../../LICENSE) for details.\n",
        "plugins/etcd/README.md": "# Etcd Plugin\n\nA Claude Code plugin for monitoring etcd cluster health and analyzing performance in OpenShift environments.\n\n## Overview\n\nThis plugin provides commands to help diagnose and troubleshoot etcd-related issues in OpenShift clusters. Etcd is the critical distributed key-value store that holds all cluster state for Kubernetes/OpenShift, and maintaining its health and performance is essential for cluster stability.\n\n## Commands\n\n### `/etcd:health-check`\n\nPerforms a comprehensive health check of the etcd cluster, examining:\n- Etcd pod status and availability\n- Cluster health and member status\n- Leadership election status\n- Database size and fragmentation\n- Disk space utilization\n- Recent error logs\n- Performance metrics (with `--verbose` flag)\n\n**Usage:**\n```\n/etcd:health-check [--verbose]\n```\n\n**Example:**\n```\n/etcd:health-check\n/etcd:health-check --verbose\n```\n\n### `/etcd:analyze-performance`\n\nAnalyzes etcd performance metrics to identify latency issues and bottlenecks, including:\n- Disk I/O performance (commit latency, fsync duration)\n- Network latency between etcd peers\n- Request/response performance by operation type\n- Leader stability and proposal metrics\n- Database size and fragmentation\n- Performance warnings from logs\n\n**Usage:**\n```\n/etcd:analyze-performance [--duration <minutes>]\n```\n\n**Example:**\n```\n/etcd:analyze-performance\n/etcd:analyze-performance --duration 15\n```\n\n## Prerequisites\n\nAll commands require:\n\n1. **OpenShift CLI (oc)** - Install from https://mirror.openshift.com/pub/openshift-v4/clients/ocp/\n2. **Active cluster connection** - Must be authenticated to an OpenShift cluster\n3. **Cluster admin permissions** - Required to access etcd pods and metrics\n4. **Running etcd pods** - At least one etcd pod must be running\n\n## Installation\n\n### From Marketplace\n\n```bash\n# Add the marketplace (if not already added)\n/plugin marketplace add openshift-eng/ai-helpers\n\n# Install the etcd plugin\n/plugin install etcd@ai-helpers\n```\n\n### Manual Installation\n\n```bash\n# Clone the repository\ngit clone https://github.com/openshift-eng/ai-helpers.git\n\n# Link to your Claude Code plugins directory\nln -s $(pwd)/ai-helpers/plugins/etcd ~/.claude/plugins/etcd\n```\n\n## Use Cases\n\n### Troubleshooting Cluster Issues\n\nWhen experiencing cluster-wide problems:\n1. Run `/etcd:health-check` to verify etcd cluster status\n2. If issues are found, run `/etcd:analyze-performance` to identify bottlenecks\n3. Follow the recommendations provided in the output\n\n### Performance Tuning\n\nFor proactive performance monitoring:\n1. Run `/etcd:analyze-performance --duration 30` for comprehensive analysis\n2. Review disk I/O and network latency metrics\n3. Compare against recommended thresholds\n4. Implement suggested optimizations\n\n### Capacity Planning\n\nBefore scaling operations:\n1. Check current database size with `/etcd:health-check`\n2. Analyze performance trends with `/etcd:analyze-performance`\n3. Identify if hardware upgrades are needed\n\n## Common Issues and Solutions\n\n### High Disk Latency\n\n**Problem:** Backend commit P99 > 100ms or WAL fsync P99 > 10ms\n\n**Solutions:**\n- Migrate to SSD or NVMe storage\n- Use dedicated disks for etcd (not shared with OS)\n- Check for competing I/O workloads\n\n### Frequent Leader Changes\n\n**Problem:** Leader changes > 5\n\n**Solutions:**\n- Check network connectivity between etcd nodes\n- Ensure nodes are in same datacenter/availability zone\n- Verify no clock skew between nodes\n\n### Large Database Size\n\n**Problem:** Database size > 8GB or high fragmentation\n\n**Solutions:**\n- Run etcd defragmentation\n- Review event retention policies\n- Check for excessive key creation\n\n## Performance Benchmarks\n\nRecommended thresholds for healthy etcd:\n- **Backend commit P99:** < 100ms\n- **WAL fsync P99:** < 10ms\n- **Peer RTT P99:** < 50ms\n- **Leader changes:** < 5 total\n- **Database size:** < 8GB\n- **Disk usage:** < 80%\n\n## Security Considerations\n\n- Commands require cluster-admin or equivalent permissions\n- Access to etcd allows viewing all cluster secrets\n- Metrics and logs may contain sensitive information\n- Performance data should be treated as confidential\n\n## Resources\n\n- **Etcd Documentation:** https://etcd.io/docs/\n- **OpenShift Etcd Docs:** https://docs.openshift.com/container-platform/latest/backup_and_restore/control_plane_backup_and_restore/\n- **Performance Tuning:** https://etcd.io/docs/latest/tuning/\n\n## Contributing\n\nTo contribute improvements or report issues:\n1. Visit https://github.com/openshift-eng/ai-helpers\n2. Open an issue or pull request\n3. Follow the contribution guidelines in the repository\n\n## License\n\nThis plugin is part of the ai-helpers project and follows the same license terms.\n",
        "plugins/yaml/README.md": "# YAML Plugin\n\nYAML documentation and utilities for Claude Code.\n\n## Commands\n\n### `/yaml:docs`\n\nGenerate or query documentation for YAML files and structures.\n\nSee [commands/docs.md](commands/docs.md) for full documentation.\n\n## Installation\n\n```bash\n/plugin install yaml@ai-helpers\n```\n\n",
        "plugins/must-gather/README.md": "# Must-Gather Analyzer Plugin\n\nClaude Code plugin for analyzing OpenShift must-gather diagnostic data.\n\n## Overview\n\nThis plugin provides tools to analyze must-gather data collected from OpenShift clusters, displaying resource status in familiar `oc`-like format and identifying cluster issues.\n\n## Features\n\n### Skills\n\n- **Must-Gather Analyzer** - Comprehensive analysis of cluster operators, pods, nodes, and network components\n  - Parses YAML resources from must-gather dumps\n  - Displays output similar to `oc get` commands\n  - Identifies and categorizes issues\n  - Provides actionable diagnostics\n\n### Analysis Scripts\n\nAll scripts located in `plugins/must-gather/skills/must-gather-analyzer/scripts/`:\n\n#### `analyze_clusterversion.py`\nAnalyzes cluster version, update status, and capabilities.\n\n```bash\n./analyze_clusterversion.py <must-gather-path>\n```\n\nOutput format matches `oc get clusterversion`:\n```\nNAME       VERSION                                            AVAILABLE   PROGRESSING   SINCE   STATUS\nversion    4.20.0-0.okd-scos-2025-08-18-130459                            False         65d\n```\n\nAlso provides detailed information:\n- Cluster ID and version hash\n- Current and desired versions\n- Conditions (Available, Progressing, Failing, etc.)\n- Update history\n- Available updates\n- Enabled capabilities\n\n#### `analyze_clusteroperators.py`\nAnalyzes cluster operator status and health.\n\n```bash\n./analyze_clusteroperators.py <must-gather-path>\n```\n\nOutput format matches `oc get clusteroperators`:\n```\nNAME                                       VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE\nauthentication                             4.18.26   True        False         False      149m\nbaremetal                                  4.18.26   True        False         False      169m\n```\n\n#### `analyze_pods.py`\nAnalyzes pod status across all namespaces.\n\n```bash\n# All pods in all namespaces\n./analyze_pods.py <must-gather-path>\n\n# Specific namespace\n./analyze_pods.py <must-gather-path> --namespace openshift-etcd\n\n# Only problematic pods\n./analyze_pods.py <must-gather-path> --problems-only\n```\n\nOutput format matches `oc get pods -A`:\n```\nNAMESPACE                              NAME                                    READY   STATUS             RESTARTS   AGE\nopenshift-kube-apiserver               kube-apiserver-master-0                 4/4     Running            0          5d\nopenshift-etcd                         etcd-master-1                           1/1     CrashLoopBackOff   15         2h\n```\n\n#### `analyze_nodes.py`\nAnalyzes node status and conditions.\n\n```bash\n# All nodes\n./analyze_nodes.py <must-gather-path>\n\n# Only nodes with issues\n./analyze_nodes.py <must-gather-path> --problems-only\n```\n\nOutput format matches `oc get nodes`:\n```\nNAME                                       STATUS                     ROLES          AGE     VERSION\nmaster-0.example.com                       Ready                      master         10d     v1.27.0+1234\nworker-1.example.com                       Ready,MemoryPressure       worker         10d     v1.27.0+1234\n```\n\n#### `analyze_network.py`\nAnalyzes network configuration and health.\n\n```bash\n./analyze_network.py <must-gather-path>\n```\n\nShows:\n- Network type (OVN-Kubernetes, OpenShift SDN)\n- Network operator status\n- OVN pod health\n- PodNetworkConnectivityCheck results\n\n#### `analyze_ovn_dbs.py`\nAnalyzes OVN Northbound and Southbound databases from clusters using `ovsdb-tool`.\n\n```bash\n# Standard analysis\n./analyze_ovn_dbs.py <must-gather-path>\n\n# Query mode (raw OVSDB queries)\n./analyze_ovn_dbs.py <must-gather-path> --query '[\"OVN_Northbound\", {...}]'\n```\n\n**Requirements:**\n- `ovsdb-tool` must be installed (`openvswitch` package)\n- Database files collected in `network_logs/ovnk_database_store.tar.gz`\n\n**Output per node:**\n```\n================================================================================\nNode: worker-node.internal\nPod:  ovnkube-node-79cbh\n================================================================================\n  Logical Switches:      4\n  Logical Switch Ports:  55\n  ACLs:                  7\n  Logical Routers:       2\n\n  POD LOGICAL SWITCH PORTS (43):\n  NAMESPACE                                POD                                           IP\n  ------------------------------------------------------------------------------------------------------------------------\n  openshift-apiserver                      apiserver-7f4f77f688-s6t7b                    10.128.0.4\n  openshift-authentication                 oauth-openshift-96688d9f8-v2l2j               10.128.0.6\n  ...\n```\n\n**Features:**\n- Automatically maps ovnkube pods to nodes by reading pod specifications\n- Per-node logical network topology\n- Filter analysis to specific nodes with `--node` flag\n- **Query Mode**: Run custom OVSDB JSON queries for specific data extraction\n- Claude can construct OVSDB queries based on natural language requests\n- Logical switches and their ports\n- Pod logical switch ports with namespace, pod name, and IP addresses\n- Access Control Lists (ACLs) with priorities, directions, and match rules\n- Logical routers and their ports\n\n**Use Cases:**\n- Verify pod network configuration and IP assignments\n- Troubleshoot connectivity issues by reviewing ACL rules\n- Understand logical network topology across zones\n- Audit network policies translated to OVN ACLs\n\n#### `analyze_events.py`\nAnalyzes cluster events sorted by last occurrence.\n\n```bash\n# Recent events (last 100)\n./analyze_events.py <must-gather-path>\n\n# Warning events only\n./analyze_events.py <must-gather-path> --type Warning\n\n# Events in specific namespace\n./analyze_events.py <must-gather-path> --namespace openshift-etcd\n\n# Show last 50 events\n./analyze_events.py <must-gather-path> --count 50\n```\n\nOutput format:\n```\nNAMESPACE                      LAST SEEN  TYPE       REASON                         OBJECT                                   MESSAGE\nopenshift-etcd                 64d        Warning    Unhealthy                      Pod/etcd-guard-ip-10-0-90-209            Readiness probe failed\nopenshift-kube-apiserver       64d        Normal     Started                        Pod/kube-apiserver-master-0              Started container\n```\n\n#### `analyze_etcd.py`\nAnalyzes etcd cluster health from etcd_info directory.\n\n```bash\n./analyze_etcd.py <must-gather-path>\n```\n\nShows:\n- Member health status\n- Member list with IDs and URLs\n- Endpoint status (leader, version, DB size)\n- Quorum status and summary\n\nOutput includes:\n```\nETCD CLUSTER SUMMARY\nTotal Members: 3\nHealthy Members: 3/3\n  ‚úÖ All members healthy\n  ‚úÖ Quorum achieved (3/2)\n```\n\n#### `analyze_pvs.py`\nAnalyzes PersistentVolumes and PersistentVolumeClaims.\n\n```bash\n# All PVs and PVCs\n./analyze_pvs.py <must-gather-path>\n\n# PVCs in specific namespace\n./analyze_pvs.py <must-gather-path> --namespace openshift-monitoring\n```\n\nOutput format:\n```\nPERSISTENT VOLUMES\nNAME                                               CAPACITY   ACCESS MODES         RECLAIM    STATUS     CLAIM\npvc-3d4a0119-b2f2-44fa-9b2f-b11c611c74f2           20Gi       ReadWriteOnce        Delete     Bound      openshift-monitoring/prometheus-data-pro\n\nPERSISTENT VOLUME CLAIMS\nNAMESPACE                      NAME                                STATUS     VOLUME                                         CAPACITY\nopenshift-monitoring           prometheus-data-prometheus-0        Bound      pvc-3d4a0119-b2f2-44fa-9b2f-b11c611c74f2       20Gi\n```\n\n#### `analyze_prometheus.py`\n\nAnalyzes Prometheus alerts.\n\n```bash\n# Alerts in all namespaces\n./analyze_prometheus.py <must-gather-path>\n\n# Alerts from a specific namespace\n./analyze_prometheus.py <must-gather-path> --namespace openshift-monitoring\n```\n\nOutput format:\n```\nALERTS\nSTATE      NAMESPACE                                          NAME                                               SEVERITY   SINCE                LABELS\nfiring     openshift-monitoring                               Watchdog                                           none       2025-10-06T09:54:21Z {}\nfiring     openshift-monitoring                               AlertmanagerReceiversNotConfigured                 warning    2025-10-06T09:54:51Z {}\n\n================================================================================\nSUMMARY\nActive alerts: 2 total (0 pending, 2 firing)\n================================================================================\n```\n\n#### `analyze_windows_logs.py`\n\nAnalyzes Windows node logs from must-gather data.\n\n```bash\n# Analyze all Windows logs\n./analyze_windows_logs.py <must-gather-path>\n\n# Analyze specific component\n./analyze_windows_logs.py <must-gather-path> --component kubelet\n\n# Summary only\n./analyze_windows_logs.py <must-gather-path> --errors-only\n```\n\n**Analyzes logs from:**\n- `kube-proxy` - Windows networking service\n- `hybrid-overlay` - OVN-Kubernetes hybrid networking\n- `kubelet` - Windows node agent\n- `containerd` - Container runtime for Windows\n- `WICD` - Windows Instance Config Daemon\n- `csi-proxy` - Storage plugin for Windows\n\nOutput format:\n```\n================================================================================\nWINDOWS NODE LOGS ANALYSIS\n================================================================================\n\nComponents analyzed: 6/8\nTotal log lines:     125,432\nTotal errors found:  23\nTotal warnings:      15\n\nCOMPONENT STATUS:\nCOMPONENT                 LINES      ERRORS     WARNINGS   STATUS\n--------------------------------------------------------------------------------\nkubelet                   45,123     12         5          ‚ùå ERRORS\ncontainerd                32,456     6          4          ‚ùå ERRORS\nhybrid-overlay            8,912      5          3          ‚ùå ERRORS\nkube-proxy                15,234     0          2          ‚úÖ OK\n\n================================================================================\nDETECTED ISSUES\n================================================================================\n\n1. [CRITICAL] HNS (Host Network Service) Failures Detected\n   Found 5 HNS-related errors. This typically causes pods to fail in ContainerCreating state.\n   ‚Üí Check Windows node networking configuration. May need to restart HNS service or reboot node.\n```\n\n**Common Issues Detected:**\n- HNS failures (pods stuck in ContainerCreating)\n- Containerd runtime errors\n- Hybrid-overlay networking issues\n- Kubelet pod management failures\n- WICD configuration errors\n- CSI-Proxy storage mount failures\n\n### Slash Commands\n\n#### `/must-gather:analyze [path] [component]`\nRuns comprehensive analysis of must-gather data.\n\n```\n/must-gather:analyze ./must-gather.local.123456789\n```\n\nExecutes all analysis scripts and provides:\n- Executive summary of cluster health\n- Critical issues and warnings\n- Actionable recommendations\n- Suggested logs to review\n\nCan also analyze specific components:\n```\n/must-gather:analyze ./must-gather.local.123456789 ovn databases\n```\n\n#### `/must-gather:ovn-dbs [path] [--node <node-name>]`\nAnalyzes OVN databases from must-gather.\n\n```\n# Analyze all nodes\n/must-gather:ovn-dbs ./must-gather.local.123456789\n\n# Analyze specific node\n/must-gather:ovn-dbs ./must-gather.local.123456789 --node ip-10-0-26-145\n\n# Analyze worker nodes (partial match)\n/must-gather:ovn-dbs ./must-gather.local.123456789 --node worker\n\n# Run custom OVSDB query (Claude constructs the JSON)\n/must-gather:ovn-dbs ./must-gather.local.123456789 --query '[\"OVN_Northbound\", {\"op\":\"select\", \"table\":\"ACL\", \"where\":[[\"priority\", \">\", 1000]], \"columns\":[\"priority\",\"match\"]}]'\n```\n\nProvides detailed analysis of:\n- Logical network topology per node (automatically mapped from pods)\n- Pod logical switch ports with IPs\n- ACL rules and priorities\n- Logical routers and switches\n- Node filtering with partial name matching\n\n**Requirements:** `ovsdb-tool` installed\n\n#### `/must-gather:windows [path] [--component COMPONENT]`\nAnalyzes Windows node logs and issues from must-gather data.\n\n```\n# Analyze all Windows logs\n/must-gather:windows ./must-gather.local.123456789\n\n# Analyze specific component\n/must-gather:windows ./must-gather.local.123456789 --component kubelet\n\n# Analyze hybrid-overlay networking\n/must-gather:windows ./must-gather.local.123456789 --component hybrid-overlay\n```\n\nProvides analysis of:\n- Windows component status (kubelet, containerd, hybrid-overlay, kube-proxy, WICD, csi-proxy)\n- Error and warning counts per component\n- Detected Windows-specific issues (HNS failures, runtime errors, networking problems)\n- Recommendations for remediation\n- Detailed error messages categorized by type\n\n**Use Cases:**\n- Troubleshooting Windows node issues\n- Investigating Windows pod failures\n- Analyzing hybrid-overlay networking between Linux and Windows nodes\n- Debugging HNS (Host Network Service) failures\n- Reviewing container runtime issues on Windows\n\n## Installation\n\n### From Local Repository\n\nIf you're working in the must-gather repository:\n\n1. The plugin is already available in `.claude-plugin/`\n2. Claude Code will automatically detect project plugins\n\n### Manual Installation\n\nTo use this plugin in other projects:\n\n1. Copy the `.claude-plugin/` directory to your desired location\n2. Add to Claude Code:\n   ```bash\n   /plugin marketplace add /path/to/.claude-plugin\n   /plugin install must-gather-analyzer\n   ```\n\n## Usage Examples\n\n### Analyzing Cluster Version\n\nAsk Claude:\n- \"What version is this cluster running?\"\n- \"Show me the cluster version\"\n- \"What's the update status?\"\n- \"What capabilities are enabled?\"\n\n### Analyzing Cluster Operators\n\nAsk Claude:\n- \"Analyze the cluster operators in this must-gather\"\n- \"Which operators are degraded?\"\n- \"Show me operator status\"\n\nClaude will automatically use the Must-Gather Analyzer skill and run `analyze_clusteroperators.py`.\n\n### Finding Pod Issues\n\nAsk Claude:\n- \"What pods are failing in this must-gather?\"\n- \"Show me crashlooping pods\"\n- \"Analyze pods in openshift-etcd namespace\"\n\n### Analyzing Events\n\nAsk Claude:\n- \"Show me warning events from this must-gather\"\n- \"What events occurred in openshift-etcd namespace?\"\n- \"Show me the last 50 events\"\n\n### Checking etcd Health\n\nAsk Claude:\n- \"Check etcd cluster health\"\n- \"What's the etcd member status?\"\n- \"Is etcd quorum healthy?\"\n\n### Analyzing Storage\n\nAsk Claude:\n- \"Show me PersistentVolumes and PVCs\"\n- \"What storage resources exist?\"\n- \"Are there any pending PVCs?\"\n\n### Complete Cluster Analysis\n\n```\n/analyze-mg ./must-gather.local.5464029130631179436\n```\n\nThis runs all analysis scripts and provides comprehensive diagnostics.\n\n## Requirements\n\n- Python 3.6+\n- PyYAML library (`pip install pyyaml`)\n\n## Must-Gather Directory Structure\n\nExpected directory structure from `oc adm must-gather` output:\n\n```\nmust-gather.local.*/\n‚îú‚îÄ‚îÄ cluster-scoped-resources/\n‚îÇ   ‚îú‚îÄ‚îÄ config.openshift.io/\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ clusteroperators/\n‚îÇ   ‚îî‚îÄ‚îÄ core/\n‚îÇ       ‚îî‚îÄ‚îÄ nodes/\n‚îú‚îÄ‚îÄ namespaces/\n‚îÇ   ‚îî‚îÄ‚îÄ <namespace>/\n‚îÇ       ‚îî‚îÄ‚îÄ core/\n‚îÇ           ‚îî‚îÄ‚îÄ pods/\n‚îî‚îÄ‚îÄ network_logs/\n```\n\n## Development\n\n### Adding New Analysis Scripts\n\n1. Create script in `skills/must-gather-analyzer/scripts/`\n2. Follow the output format pattern (matching `oc get` commands)\n3. Update `SKILL.md` with usage instructions\n4. Add to `/analyze-mg` command workflow\n\n### Output Format Guidelines\n\nAll scripts should:\n- Use tabular output matching `oc` command format\n- Handle missing resources gracefully\n- Print \"No resources found.\" when appropriate\n- Support common flags like `--namespace`, `--problems-only`\n\n## Troubleshooting\n\n### \"No resources found\"\n- Verify must-gather path is correct\n- Check that must-gather completed successfully\n- Ensure directory structure matches expected format\n\n### Scripts not executing\n- Verify scripts are executable: `chmod +x scripts/*.py`\n- Check Python 3 is available\n- Install dependencies: `pip install pyyaml`\n\n## Contributing\n\nWhen adding new analysis capabilities:\n1. Follow existing script patterns\n2. Match `oc` command output format\n3. Include error handling for missing data\n4. Update this README with new features\n\n## License\n\nThis plugin is part of the openshift/must-gather repository.\n",
        "plugins/lvms/README.md": "# LVMS Plugin\n\nComprehensive troubleshooting and debugging plugin for LVMS (Logical Volume Manager Storage).\n\n## Overview\n\nThe LVMS plugin provides powerful commands for diagnosing and troubleshooting storage issues in OpenShift clusters using LVMS. It analyzes LVMCluster resources, volume groups, PVCs, TopoLVM CSI driver, and node-level storage configuration to identify root causes of storage failures.\n\n## Commands\n\n### `/lvms:analyze`\n\nComprehensive LVMS troubleshooting that analyzes cluster health, storage resources, and identifies common issues.\n\n**Works with:**\n- Live OpenShift clusters (via `oc` CLI)\n- LVMS must-gather data (offline analysis)\n\n**Features:**\n- LVMCluster health and readiness analysis\n- Volume group status across all nodes\n- PVC/PV binding issues and pending volumes\n- LVMS operator and TopoLVM CSI driver health\n- Node-level device availability and configuration (live clusters)\n- Thin pool capacity and usage\n- Pod log analysis with error deduplication\n- Root cause analysis with specific remediation steps\n\n**Usage Examples:**\n\n```bash\n# Analyze live cluster\n/lvms:analyze --live\n\n# Analyze must-gather data\n/lvms:analyze ./must-gather/registry-ci-openshift-org-origin-4-18.../\n\n# Focus on specific component\n/lvms:analyze --live --component storage\n/lvms:analyze ./must-gather/... check pending PVCs\n\n# Analyze pod logs only\n/lvms:analyze --live --component logs\n/lvms:analyze ./must-gather/... --component logs\n```\n\n## Common Use Cases\n\n### 1. PVCs Stuck in Pending State\n\nWhen PVCs using LVMS storage classes are not binding:\n\n```bash\n/lvms:analyze --live check pending PVCs\n```\n\nThe command will:\n- Identify which PVCs are pending\n- Check volume group free space\n- Verify TopoLVM CSI driver is running\n- Check for node affinity issues\n- Provide specific remediation steps\n\n### 2. LVMCluster Not Ready\n\nWhen LVMCluster resource is not reaching Ready state:\n\n```bash\n/lvms:analyze --live analyze operator\n```\n\nThe command will:\n- Check LVMCluster status and conditions\n- Identify which nodes have volume group issues\n- Verify device availability and configuration\n- Check for conflicting filesystems on devices\n- Provide steps to clean devices and recreate VGs\n\n### 3. Volume Group Creation Failures\n\nWhen volume groups are not being created on nodes:\n\n```bash\n/lvms:analyze --live --component volumes\n```\n\nThe command will:\n- Show volume group status per node\n- Identify missing or failed volume groups\n- Check device selector configuration\n- Detect devices already in use\n- Provide commands to wipe devices and retry\n\n### 4. Must-Gather Analysis\n\nWhen analyzing a must-gather from a failed cluster:\n\n```bash\n/lvms:analyze ./must-gather/path/\n```\n\nThe command will:\n- Perform offline analysis of all LVMS resources\n- Generate comprehensive health report\n- Identify critical issues and warnings\n- Provide prioritized remediation recommendations\n- Suggest which logs to review\n\n## Installation\n\n### From Marketplace\n\n```bash\n# Add the marketplace\n/plugin marketplace add openshift-eng/ai-helpers\n\n# Install LVMS plugin\n/plugin install lvms@ai-helpers\n\n# Use the command\n/lvms:analyze --live\n```\n\n### Manual Installation\n\n```bash\n# Clone the repository\ngit clone https://github.com/openshift-eng/ai-helpers.git\n\n# Link to Claude Code plugins directory\nln -s $(pwd)/ai-helpers/plugins/lvms ~/.claude/plugins/lvms\n```\n\n## Prerequisites\n\n**For Live Cluster Analysis:**\n- `oc` CLI installed and configured\n- Active cluster connection\n- Read access to `openshift-lvm-storage` or older `openshift-storage` namespace\n- Ability to read cluster-scoped resources\n\n**For Must-Gather Analysis:**\n- Python 3.6+ (for analysis script)\n- PyYAML library: `pip install pyyaml`\n\n## What the Plugin Checks\n\n### LVMCluster Resources\n- Overall state (Ready, Progressing, Failed, Degraded)\n- Status conditions (ResourcesAvailable, VolumeGroupsReady)\n- Device class configurations\n- Node coverage and readiness\n\n### Volume Groups\n- Volume group creation status per node\n- Physical volume availability\n- Free space and capacity\n- Thin pool configuration and usage\n- Missing or failed volume groups\n\n### Storage (PVCs/PVs)\n- PVC binding status\n- Pending volume provisioning failures\n- Storage class configuration\n- Capacity issues\n- Node affinity constraints\n\n### Operator Health\n- LVMS operator deployment status\n- TopoLVM controller readiness\n- TopoLVM node daemonset coverage\n- VG-manager daemonset status\n- Pod crashes and restarts\n\n### Node Devices\n- Block device availability\n- Existing filesystems on devices\n- Device selector matches\n- Disk capacity and usage\n\n### Pod Logs\n- Error and warning messages from vg-manager pods\n- Error and warning messages from lvms-operator pod\n- Deduplication of repeated errors from reconciliation loops\n- JSON log parsing with timestamps and context\n\n## Output Format\n\nThe plugin provides structured, color-coded output:\n\n- ‚úì Green checkmarks for healthy components\n- ‚ö† Yellow warnings for non-critical issues\n- ‚ùå Red errors for critical problems\n- ‚Ñπ Blue info for additional context\n\nReports include:\n- Component-by-component health status\n- Root cause analysis\n- Prioritized recommendations\n- Specific remediation commands\n- Links to relevant documentation\n\n## Troubleshooting the Plugin\n\n**Script not found:**\n```bash\n# Verify script exists\nls plugins/lvms/skills/lvms-analyzer/scripts/analyze_lvms.py\n\n# Make executable\nchmod +x plugins/lvms/skills/lvms-analyzer/scripts/analyze_lvms.py\n```\n\n**Cannot connect to cluster:**\n```bash\n# Verify oc is configured\noc whoami\noc cluster-info\n\n# Check LVMS namespace\noc get namespace openshift-lvm-storage\n```\n\n**Must-gather path errors:**\n```bash\n# Use the correct subdirectory (the one with the hash)\nls must-gather/registry-ci-*/namespaces/openshift-lvm-storage\n\n# Not the parent directory\n```\n\n## Related Resources\n\n- [LVMS GitHub Repository](https://github.com/openshift/lvm-operator)\n- [LVMS Troubleshooting Guide](https://github.com/openshift/lvm-operator/blob/main/docs/troubleshooting.md)\n- [TopoLVM Documentation](https://github.com/topolvm/topolvm)\n- [OpenShift Storage Documentation](https://docs.openshift.com/container-platform/latest/storage/index.html)\n\n## Contributing\n\nContributions are welcome! Please see the main repository's [CLAUDE.md](../../CLAUDE.md) for guidelines on:\n- Adding new commands\n- Extending analysis capabilities\n- Improving diagnostic checks\n- Adding helper scripts\n\n## Support\n\nFor issues or feature requests:\n- GitHub Issues: https://github.com/openshift-eng/ai-helpers/issues\n- Repository: https://github.com/openshift-eng/ai-helpers\n",
        "plugins/metrics/README.md": "# Metrics Plugin\n\nAnonymous usage metrics collection for ai-helpers slash commands, skills, and sessions.\n\n## Overview\n\nThe `metrics` plugin provides anonymous usage tracking for:\n- **Events**: Individual slash commands and skill invocations\n- **Sessions**: Aggregate session-level metrics (duration, tool usage, conversation patterns)\n\nThis helps maintainers understand usage patterns and make data-driven decisions about feature development and improvements.\n\n## How It Works\n\nThe plugin uses Claude Code's [hook system](https://docs.claude.com/en/docs/claude-code/hooks) to automatically track usage:\n\n### Event Tracking (Slash Commands & Skills)\n\n1. **Hook Triggers**:\n   - `UserPromptSubmit`: Fires when you submit a prompt that starts with `/` (slash commands)\n   - `PreToolUse`: Fires when Claude invokes a Skill tool\n2. **Data Collection**: The `send_metrics.py` script extracts the command/skill name and system information\n3. **Background Transmission**: Events are sent asynchronously to the events endpoint\n4. **Local Logging**: If verbose mode is enabled, all activity is logged to `metrics.log`\n\n### Session Tracking (Session-Level Aggregates)\n\n1. **Hook Trigger**: `SessionEnd` fires when your Claude Code session ends\n2. **Transcript Parsing**: The `send_session_metrics.py` script parses the session transcript file\n3. **Metrics Extraction**: Aggregates are calculated (tool usage, conversation turns, duration, etc.)\n4. **Background Transmission**: Session metrics are sent asynchronously to the sessions endpoint\n5. **Privacy**: Only counts and aggregates are collected - no command arguments, file paths, or message content\n\n### Hook Configuration\n\nThe plugin is defined in `plugins/metrics/hooks/hooks.json`:\n\n```json\n{\n  \"description\": \"Anonymous Usage Metric Collection\",\n  \"hooks\": {\n    \"UserPromptSubmit\": [\n      {\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"${CLAUDE_PLUGIN_ROOT}/scripts/send_metrics.py\",\n            \"timeout\": 30\n          }\n        ]\n      }\n    ],\n    \"PreToolUse\": [\n      {\n        \"matcher\": {\n          \"tool\": \"Skill\"\n        },\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"${CLAUDE_PLUGIN_ROOT}/scripts/send_metrics.py\",\n            \"timeout\": 30\n          }\n        ]\n      }\n    ],\n    \"SessionEnd\": [\n      {\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"${CLAUDE_PLUGIN_ROOT}/scripts/send_session_metrics.py\",\n            \"timeout\": 30\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n## What Data is Collected\n\nThe plugin collects two types of anonymous data:\n\n### Event Metrics (Slash Commands & Skills)\n\nCollected when you use a slash command or invoke a skill:\n\n| Field | Description | Example |\n|-------|-------------|---------|\n| `type` | Metric type | `\"slash_command\"` or `\"skill\"` |\n| `name` | The command or skill name | `\"jira:solve\"` or `\"prow-job:prow-job-analyze-install-failure\"` |\n| `engine` | Always \"claude\" | `\"claude\"` |\n| `version` | Plugin version | `\"1.0\"` |\n| `timestamp` | UTC timestamp | `\"2025-10-30T12:34:56Z\"` |\n| `session_id` | Claude session identifier | `\"abc123...\"` |\n| `user_id` | Persistent anonymous UUID | `\"550e8400-e29b-41d4-a716-446655440000\"` |\n| `os` | Operating system | `\"darwin\"`, `\"linux\"`, `\"windows\"` |\n| `mac` | SHA256 hash of session_id + timestamp | `\"a1b2c3...\"` |\n| `prompt_length` | Character count of the prompt | `42` |\n\n**Privacy Guarantees:**\n- No command arguments or sensitive data are transmitted\n- No personal identifying information (PII) is collected\n- Session IDs are ephemeral and rotate between Claude sessions\n- A persistent anonymous UUID is stored locally in `.anonymous_id`, used only to correlate events across sessions\n- The `user_id` contains no PII and can be regenerated/cleared by deleting the `.anonymous_id` file\n- The `user_id` is treated as anonymous for analytics and integrity purposes\n- The MAC is used for data integrity verification only\n\n**Example payloads:**\n\nSlash command:\n```json\n{\n  \"type\": \"slash_command\",\n  \"name\": \"jira:solve\",\n  \"engine\": \"claude\",\n  \"version\": \"1.0\",\n  \"timestamp\": \"2025-10-30T12:34:56Z\",\n  \"session_id\": \"abc123...\",\n  \"user_id\": \"550e8400-e29b-41d4-a716-446655440000\",\n  \"os\": \"darwin\",\n  \"mac\": \"a1b2c3...\",\n  \"prompt_length\": 42\n}\n```\n\nSkill invocation:\n```json\n{\n  \"type\": \"skill\",\n  \"name\": \"prow-job:prow-job-analyze-install-failure\",\n  \"engine\": \"claude\",\n  \"version\": \"1.0\",\n  \"timestamp\": \"2025-10-30T12:34:56Z\",\n  \"session_id\": \"abc123...\",\n  \"user_id\": \"550e8400-e29b-41d4-a716-446655440000\",\n  \"os\": \"darwin\",\n  \"mac\": \"a1b2c3...\",\n  \"prompt_length\": 0\n}\n```\n\n### Session Metrics (Session-Level Aggregates)\n\nCollected when your Claude Code session ends:\n\n| Field | Description | Example |\n|-------|-------------|---------|\n| `session_id` | Session identifier | `\"abc123...\"` |\n| `user_id` | Persistent anonymous UUID | `\"550e8400-e29b-41d4-a716-446655440000\"` |\n| `os` | Operating system | `\"darwin\"`, `\"linux\"`, `\"windows\"` |\n| `engine` | Always \"claude\" | `\"claude\"` |\n| `start_timestamp` | Session start time (UTC) | `\"2025-10-30T12:00:00Z\"` |\n| `end_timestamp` | Session end time (UTC) | `\"2025-10-30T14:30:00Z\"` |\n| `session_duration` | Duration in seconds | `9000` |\n| `exit_reason` | How session ended | `\"clear\"`, `\"logout\"`, `\"prompt_input_exit\"`, `\"other\"` |\n| `turn_count` | User-assistant exchanges | `42` |\n| `user_message_count` | Total user messages | `45` |\n| `assistant_message_count` | Total assistant messages | `48` |\n| `total_tool_calls` | Total tool invocations | `156` |\n| `tool_error_count` | Failed tool calls | `3` |\n| `bash_call_count` | Bash tool usage | `28` |\n| `file_read_count` | Read tool usage | `42` |\n| `file_edit_count` | Edit tool usage | `18` |\n| `file_write_count` | Write tool usage | `5` |\n| `grep_call_count` | Grep tool usage | `12` |\n| `glob_call_count` | Glob tool usage | `8` |\n| `web_fetch_call_count` | WebFetch tool usage | `2` |\n| `web_search_call_count` | WebSearch tool usage | `1` |\n| `total_input_tokens` | Total input tokens (nullable) | `50000` |\n| `total_output_tokens` | Total output tokens (nullable) | `12000` |\n| `cache_creation_tokens` | Cache creation tokens (nullable) | `15000` |\n| `cache_read_tokens` | Cache read tokens (nullable) | `35000` |\n| `had_errors` | Whether errors occurred | `true` or `false` |\n\n**Privacy Guarantees for Session Metrics:**\n- **Only aggregates**: Tool usage counts, not what tools were used for\n- **No content**: Command arguments, file paths, or message content are never collected\n- **No identifiable patterns**: Git operations, specific bash commands, or file names are excluded\n- **Optional tokens**: Token metrics are only included if non-zero and can be omitted for privacy\n\n**Example session payload:**\n```json\n{\n  \"session_id\": \"abc123...\",\n  \"user_id\": \"550e8400-e29b-41d4-a716-446655440000\",\n  \"os\": \"darwin\",\n  \"engine\": \"claude\",\n  \"start_timestamp\": \"2025-10-30T12:00:00Z\",\n  \"end_timestamp\": \"2025-10-30T14:30:00Z\",\n  \"session_duration\": 9000,\n  \"exit_reason\": \"clear\",\n  \"turn_count\": 42,\n  \"user_message_count\": 45,\n  \"assistant_message_count\": 48,\n  \"total_tool_calls\": 156,\n  \"tool_error_count\": 3,\n  \"bash_call_count\": 28,\n  \"file_read_count\": 42,\n  \"file_edit_count\": 18,\n  \"file_write_count\": 5,\n  \"grep_call_count\": 12,\n  \"glob_call_count\": 8,\n  \"web_fetch_call_count\": 2,\n  \"web_search_call_count\": 1,\n  \"total_input_tokens\": 50000,\n  \"total_output_tokens\": 12000,\n  \"cache_creation_tokens\": 15000,\n  \"cache_read_tokens\": 35000,\n  \"had_errors\": false\n}\n```\n\n## Enabling the Plugin\n\nIf you've installed ai-helpers from the marketplace:\n\n```bash\n# Enable the plugin\n/plugin enable metrics@ai-helpers\n\n# Verify it's enabled\n/plugin list\n```\n\n## Disabling the Plugin\n\nIf you wish to opt out of metrics collection:\n\n```bash\n/plugin disable metrics@ai-helpers\n```\n\n## Configuration\n\nThe plugin can be configured using environment variables:\n\n### `CLAUDE_METRICS_URL`\n\nOverride the metrics endpoint URL (useful for testing or custom deployments):\n\n```bash\nexport CLAUDE_METRICS_URL=https://localhost:8080/metrics\n```\n\nThe plugin will automatically append `/events` for event metrics and `/sessions` for session metrics.\n\n**Default**: `https://us-central1-openshift-ci-data-analysis.cloudfunctions.net/metrics-upload`\n\n### `CLAUDE_METRICS_VERBOSE`\n\nEnable verbose logging to see all metrics activity in the log file:\n\n```bash\nexport CLAUDE_METRICS_VERBOSE=1\n# or\nexport CLAUDE_METRICS_VERBOSE=true\n# or\nexport CLAUDE_METRICS_VERBOSE=yes\n```\n\n**Logging behavior:**\n- **Normal mode** (default): Only errors are logged to `metrics.log` with full details including:\n  - HTTP error codes and response bodies\n  - Network/timeout errors\n  - The complete payload that failed to send\n- **Verbose mode**: All activity is logged, including:\n  - Successful transmissions\n  - API request details (URL, headers, payload)\n  - Response status codes and bodies\n\n**Log location**: `${CLAUDE_PLUGIN_ROOT}/metrics.log`\n\n**Example verbose log entry:**\n```\n[2025-10-30T12:34:56+00:00] Sending metrics: {\"type\": \"slash_command\", \"name\": \"jira:solve\", ...}\n[2025-10-30T12:34:56+00:00] API Request: POST https://...\n[2025-10-30T12:34:56+00:00] Response: HTTP 200 - {\"status\": \"ok\"}\n```\n\n**Example error log entry (always logged):**\n```\n[2025-10-30T12:34:56+00:00] ERROR: HTTP 500 - Internal Server Error\nData sent: {\n  \"type\": \"slash_command\",\n  \"name\": \"jira:solve\",\n  ...\n}\n```\n\n## Network Behavior\n\n- **Event Endpoint**: `https://us-central1-openshift-ci-data-analysis.cloudfunctions.net/metrics-upload/events`\n- **Session Endpoint**: `https://us-central1-openshift-ci-data-analysis.cloudfunctions.net/metrics-upload/sessions`\n- **Async**: Runs in a background thread so it doesn't delay command execution\n- **Resilience**: Network failures are logged but don't interrupt your work\n\n## Source Code\n\nAll metrics collection logic is open source and available in this repository:\n\n- **Hook definition**: `plugins/metrics/hooks/hooks.json`\n- **Event collection script**: `plugins/metrics/scripts/send_metrics.py`\n- **Session collection script**: `plugins/metrics/scripts/send_session_metrics.py`\n- **Plugin metadata**: `plugins/metrics/.claude-plugin/plugin.json`\n\n## Data Usage\n\nThe collected metrics help us:\n\n**Event metrics:**\n- Understand which commands and skills are most valuable to users\n- Identify commands that may need better documentation\n- Make data-driven decisions about feature prioritization and deprecations\n\n**Session metrics:**\n- Understand typical session patterns (duration, tool usage, conversation depth)\n- Identify workflow bottlenecks and optimization opportunities\n- Measure developer productivity patterns without revealing specific work content\n- Optimize tool performance and caching strategies based on actual usage\n\n**Aggregate metrics may be shared publicly** (e.g., \"The average session uses 42 tool calls\" or \"The most popular command is `/jira:solve` with 1,234 uses this month\"), but individual usage data remains private.\n",
        "plugins/hcp/README.md": "# HCP Plugin\n\nThe HCP plugin generates intelligent `hypershift create cluster` commands from natural language descriptions across multiple cloud providers and platforms.\n\n## Overview\n\nThis plugin translates natural language descriptions into precise, ready-to-execute `hypershift create cluster` commands, applying provider-specific best practices and handling complex parameter validation automatically. The plugin **generates commands for you to run** - it does not provision clusters directly.\n\n## Commands\n\n### `/hcp:generate`\n\nGenerate ready-to-execute hypershift cluster creation commands from natural language descriptions.\n\n**Usage:**\n```\n/hcp:generate <provider> <cluster-description>\n```\n\n**Supported Providers:**\n- `aws` - Amazon Web Services\n- `azure` - Microsoft Azure (self-managed control plane)\n- `kubevirt` - KubeVirt on existing Kubernetes clusters\n- `openstack` - OpenStack clouds\n- `powervs` - IBM Cloud PowerVS\n- `agent` - Bare metal and edge deployments\n\n**Examples:**\n```bash\n# AWS development cluster\n/hcp:generate aws \"development cluster for testing new features\"\n\n# High-availability KubeVirt cluster\n/hcp:generate kubevirt \"production cluster with high availability\"\n\n# Cost-optimized Azure cluster\n/hcp:generate azure \"small cluster for dev work, minimize costs\"\n\n# Disconnected bare metal cluster\n/hcp:generate agent \"airgapped cluster for secure environment\"\n```\n\n## Key Features\n\n- **Multi-Provider Support**: Works with AWS, Azure, KubeVirt, OpenStack, PowerVS, and Agent providers\n- **Smart Analysis**: Extracts requirements from natural language descriptions\n- **Interactive Prompts**: Guides users through provider-specific configurations\n- **Best Practices**: Applies provider-specific defaults and optimizations\n- **Security Validation**: Ensures safe parameter handling and credential management\n- **Network Conflict Prevention**: Especially critical for KubeVirt deployments\n\n## Provider-Specific Skills\n\nThe plugin uses specialized skills for each provider:\n\n- **`hcp-create-aws`**: AWS-specific guidance including STS credentials, IAM roles, and VPC configuration\n- **`hcp-create-azure`**: Azure identity configuration, resource groups, and region management\n- **`hcp-create-kubevirt`**: Network conflict prevention, VM sizing, and storage class management\n- **`hcp-create-openstack`**: OpenStack credentials, external networks, and flavor selection\n- **`hcp-create-powervs`**: IBM Cloud integration, processor types, and resource group management\n- **`hcp-create-agent`**: Bare metal deployment, agent management, and disconnected environments\n\n## Installation\n\n### From AI Helpers Marketplace\n\n```bash\n# Add the marketplace\n/plugin marketplace add openshift-eng/ai-helpers\n\n# Install the HCP plugin\n/plugin install hcp@ai-helpers\n\n# Use the command\n/hcp:generate aws \"development cluster for API testing\"\n```\n\n### Manual Installation (Cursor)\n\n```bash\n# Clone the repository\nmkdir -p ~/.cursor/commands\ngit clone git@github.com:openshift-eng/ai-helpers.git\nln -s ai-helpers ~/.cursor/commands/ai-helpers\n```\n\n## Common Use Cases\n\n### Development Environments\n- Quick cluster setup for testing\n- Cost-optimized configurations\n- Single replica control planes\n\n### Production Deployments\n- High-availability configurations\n- Multi-zone deployments\n- Auto-repair and monitoring enabled\n\n### Edge Computing\n- Minimal resource footprints\n- Disconnected/airgapped environments\n- Agent-based deployments\n\n### Special Requirements\n- FIPS compliance configurations\n- IPv6 and dual-stack networking\n- Custom storage and compute requirements\n\n## Architecture\n\nThe plugin follows a modular architecture with:\n\n1. **Main Command**: `/hcp:generate` acts as an orchestrator\n2. **Provider Skills**: Specialized implementation guidance for each provider\n3. **Interactive Workflows**: Guided parameter collection and validation\n4. **Smart Defaults**: Environment-specific best practices\n\nThis design ensures:\n- **Single Source of Truth**: Each provider's knowledge lives in one place\n- **Extensibility**: Easy to add new providers or update existing ones\n- **Maintainability**: Clear separation of concerns between providers\n\n## Contributing\n\nTo add support for a new provider:\n\n1. Create a new skill directory: `plugins/hcp/skills/hcp-create-<provider>/`\n2. Implement the `SKILL.md` file following the established pattern\n3. Update the main command to invoke the new skill\n4. Test the implementation and add examples\n\nSee existing skills as reference implementations.\n\n## Support\n\n- **Issues**: [GitHub Issues](https://github.com/openshift-eng/ai-helpers/issues)\n- **Documentation**: [HyperShift Documentation](https://hypershift.openshift.io/)\n- **Skills**: View individual skill files in `plugins/hcp/skills/`\n\n## License\n\nThis plugin is part of the AI Helpers project and follows the same licensing terms.",
        "plugins/compliance/README.md": "# Compliance Plugin\n\nSecurity compliance and vulnerability analysis tools for Go projects.\n\n## Command\n\n### `/compliance:analyze-cve <CVE-ID>`\n\nAnalyzes Go codebases to determine CVE impact with multi-level confidence assessment.\n\n**Example:**\n```\n/compliance:analyze-cve CVE-2024-24783\n```\n\n**Features:**\n- Fetches CVE details from NVD, MITRE, and Go Vulnerability Database\n- Multi-level verification (dependency check ‚Üí static analysis ‚Üí govulncheck ‚Üí **call graph reachability**)\n- Generates reports with confidence levels (HIGH/MEDIUM/LOW)\n- Provides exact remediation commands\n- Optionally applies fixes with approval\n\n**Output:**\n- `.work/compliance/analyze-cve/{CVE-ID}/report.md` - Full analysis with confidence assessment\n- `.work/compliance/analyze-cve/{CVE-ID}/callgraph.svg` - Visual execution path (if call graph analysis performed)\n- `.work/compliance/analyze-cve/{CVE-ID}/govulncheck-output.txt` - Scanner results\n\n## Verification Levels\n\nThe command uses multiple methods with increasing confidence:\n\n1. **Dependency check** ‚Üí Confirms package presence\n2. **Static analysis** ‚Üí Finds function usage  \n3. **govulncheck** ‚Üí Official Go vulnerability scanner\n4. **Call graph reachability** ‚Üí Proves execution path (HIGHEST confidence)\n5. **Context analysis** ‚Üí Checks security controls\n\nReports include confidence level (HIGH/MEDIUM/LOW) based on verification methods used.\n\n## Prerequisites\n\n**Required:** Go toolchain\n\n**Recommended (for higher confidence):**\n```bash\n# Go vulnerability tools\ngo install golang.org/x/vuln/cmd/govulncheck@latest\ngo install golang.org/x/tools/cmd/callgraph@latest\ngo install golang.org/x/tools/cmd/digraph@latest\n\n# Optional: For visual graphs\nbrew install graphviz  # macOS\n```\n\nThe command auto-detects available tools and uses the most comprehensive methods possible.\n\n## Fallback Mode\n\nIf internet access fails, the command prompts for manual CVE information (description, affected packages, versions, fixes). Analysis proceeds with user-provided data, clearly marked in the report.\n\n## Report Includes\n\n- **Executive Summary**: Verdict (AFFECTED/NOT AFFECTED) with confidence level\n- **Analysis Methodology**: Which verification methods were used\n- **Impact Assessment**: Evidence from codebase, call chains (if found)\n- **Remediation Steps**: Exact commands and fixes\n- **Visual Artifacts**: Call graph SVG, scanner outputs\n\n## Examples\n\n### Basic usage\n```\n/compliance:analyze-cve CVE-2024-24783\n```\nAnalyzes codebase for crypto/x509 vulnerability, provides upgrade command if affected.\n\n### High-confidence analysis\n```\n/compliance:analyze-cve CVE-2024-45338\n```\n**Result:**\n- Finds `golang.org/x/net/html v0.21.0` (vulnerable)\n- Proves execution path: `main ‚Üí HTTPHandler ‚Üí ParseHTML ‚Üí html.Parse`\n- **Confidence**: HIGH | **Verdict**: AFFECTED\n- Generates `callgraph.svg` showing call chain\n- Recommends: `go get golang.org/x/net@v0.23.0`\n",
        "plugins/test-coverage/README.md": "# Test Coverage Plugin\n\nAnalyze e2e/integration test code structure without running tests to identify coverage gaps.\n\n## Commands\n\n### `/test-coverage:analyze`\n\nAnalyze e2e/integration test code structure without running tests to identify files and functions without test coverage.\n\n### `/test-coverage:gaps`\n\nIdentify missing test coverage in OpenShift/Kubernetes test files with intelligent gap analysis for any component.\n\nSee the [commands/](commands/) directory for full documentation of each command.\n\n## Installation\n\n```bash\n/plugin install test-coverage@ai-helpers\n```\n",
        "plugins/node-tuning/README.md": "# Node Tuning Operator Plugin (node-tuning)\n\n## Overview\nThe `node-tuning` plugin automates common workflows for the OpenShift Node Tuning Operator. Use it when you need to:\n- Generate reproducible Tuned manifests (`tuned.openshift.io/v1`) that capture sysctl settings, tuned daemon sections, and recommendation rules without hand-writing YAML.\n- Audit live nodes or captured sosreports for kernel tuning gaps (CPU isolation, IRQ affinity, huge pages, net/sysctl state) and receive actionable remediation guidance.\n\n## Commands\n- `/node-tuning:generate-tuned-profile` ‚Äì Generate a Tuned profile manifest from a natural language description of the desired parameters, sections, and targeting rules. The command also supports advanced workflows such as coordinating huge pages or kernel-rt boot parameters with a dedicated MachineConfigPool.\n- `/node-tuning:analyze-node-tuning` ‚Äì Inspect a live node or sosreport snapshot for tuning signals (isolcpus, IRQ affinity, huge pages, sysctls, networking counters) and surface recommended adjustments.\n\n## Prerequisites\n- Python 3.8 or newer must be available in the execution environment (the helper script is dependency-free beyond the standard library).\n- Access to an OpenShift cluster if you plan to apply the generated manifest (`oc` CLI recommended for validation and application).\n- Extracted sosreport directories when analyzing offline diagnostics (optional).\n\n## Typical Workflow\n1. Invoke `/node-tuning:generate-tuned-profile` with a profile name, summary, and any sysctl, include, or section options.\n2. Review the rendered YAML returned by the command or written to `.work/node-tuning/<profile-name>/tuned.yaml` when using the helper script directly.\n3. Validate the manifest with `oc apply --server-dry-run=client -f <path>` if desired.\n4. Apply the manifest to the cluster or commit it to version control for automation.\n5. Use the helper‚Äôs `--list-nodes` and `--label-node` options when you need to inspect or tag nodes before generating manifests.\n6. For huge pages or other kernel boot parameters, coordinate with a dedicated MachineConfigPool as described in the advanced workflow inside `commands/generate-tuned-profile.md`.\n7. Diagnose tuning gaps with `/node-tuning:analyze-node-tuning --format markdown` and translate the recommendations into updated Tuned profiles. When you cannot SSH to the node, supply `--node <name>` (plus optional `--kubeconfig`/`--oc-binary`) and the analyzer will, by default, enter the RHCOS `toolbox` (support-tools image) via `oc debug node/<name>`, run `sosreport -e openshift ... --all-logs --plugin-timeout=600`, download the archive, and analyze it offline. Override the container image with `--toolbox-image` (or `TOOLBOX_IMAGE`) and extend/tune the sosreport flags with `--sosreport-arg`. HTTP(S) proxy variables are forwarded automatically when set, but they are entirely optional. Add `--no-collect-sosreport` if you prefer the lighter `/proc` snapshot workflow.\n\n## Related Files\n- Command definition: `commands/generate-tuned-profile.md`\n- Command definition: `commands/analyze-node-tuning.md`\n- Helper implementation: `skills/scripts/generate_tuned_profile.py`\n- Helper implementation: `skills/scripts/analyze_node_tuning.py`\n- Skill documentation: `skills/scripts/SKILL.md`\n",
        "plugins/origin/README.md": "# Origin Plugin\n\nUtilities and workflow helpers for developing and reviewing changes in the\nopenshift/origin repository.  \nThis plugin focuses on improving test quality, code consistency, and CI suite\nintegration for Origin contributions.\n\n## Commands\n\n### /origin:two-node-origin-pr-helper\n\nExpert review tool for PRs that add or modify Two Node (Fencing or Arbiter) tests\nunder test/extended/two_node/.\n\nThis command performs:\n\n- Automatic discovery of changed Two Node test files\n- Analysis of Ginkgo Describe/It blocks, suite tags, and Serial annotations\n- Review of test logic, determinism, cleanup behavior, and structure\n- Suggestions for reusing existing Origin and Kubernetes utilities\n- Identification of duplicated logic that should use shared helpers\n- Recommendations for suite placement and Serial usage\n- Recommendations for CI lane coverage in openshift/release\n- Generation of ready-to-paste PR text for both Origin and Release repositories\n\nUse this helper when contributing to Origin‚Äôs Two Node test suite or reviewing PRs\nthat affect Two Node behavior.\n\nSee the commands/ directory for full documentation.\n\n## Installation\n\n### From the Claude Code Plugin Marketplace\n\n1. Add the OpenShift ai-helpers marketplace:\n\n   /plugin marketplace add openshift-eng/ai-helpers\n\n2. Install the origin plugin:\n\n   /plugin install origin@ai-helpers\n\n3. Use the command:\n\n   /origin:two-node-origin-pr-helper\n\n## Available Commands\n\n### Two Node PR Review\n\n#### /origin:two-node-origin-pr-helper ‚Äî Review Two Node Tests in Origin\n\nThis command performs a full expert review of PRs that modify or add Two Node\n(Fencing or Arbiter) tests under test/extended/two_node/.\n\nThe helper covers:\n\n- Code correctness and logical consistency\n- Ginkgo test structure and best practices\n- Suite tagging and Serial analysis\n- Utility/helper reuse (Origin + Kubernetes)\n- CI suite and lane coverage recommendations\n- PR description generation\n\nSee commands/two-node-origin-pr-helper.md for full documentation.\n\n## Development\n\n### Adding New Commands\n\nTo add a new command to this plugin:\n\n1. Create a markdown file in commands/:\n\n   touch plugins/origin/commands/your-command.md\n\n2. Use existing commands as a template and include sections:\n\n   - Name  \n   - Synopsis  \n   - Description  \n   - Implementation behavior  \n   - Return value / output structure  \n   - Examples  \n   - Arguments  \n   - Error handling  \n   - Additional context if needed\n\n3. Test the command:\n\n   /origin:your-command\n\n## Plugin Structure\n\nplugins/origin/\n‚îú‚îÄ‚îÄ .claude-plugin/\n‚îÇ   ‚îî‚îÄ‚îÄ plugin.json\n‚îú‚îÄ‚îÄ commands/\n‚îÇ   ‚îî‚îÄ‚îÄ two-node-origin-pr-helper.md\n‚îî‚îÄ‚îÄ README.md\n\n## Related Plugins\n\n- openshift ‚Äî General OpenShift development and CI helpers\n- ci ‚Äî Prow/CI-related workflow helpers\n- git ‚Äî Git workflow helpers\n- jira ‚Äî Jira automation helpers\n- utils ‚Äî General-purpose utilities\n\n## Contributing\n\nContributions are welcome.\n\nWhen adding Origin-specific commands:\n\n- Ensure the workflow relates directly to openshift/origin\n- Follow existing documentation patterns\n- Provide actionable examples and behavior explanations\n- Use realistic Origin repository paths and test patterns\n- Update this README with any new commands\n\n## License\n\nSee [LICENSE](../../LICENSE) for details.\n",
        "plugins/container-image/README.md": "# Container Image Plugin\n\nContainer image inspection and analysis tools using skopeo and podman.\n\n## Overview\n\nThis plugin provides commands to inspect, analyze, and compare container images from any OCI-compliant registry. It leverages `skopeo` and `podman` to provide detailed insights into image structure, manifest lists, layers, and configuration without requiring full image pulls.\n\n## Features\n\n- **Image Inspection**: Detailed breakdown of image metadata, layers, and configuration\n- **Image Comparison**: Compare two images to identify differences\n- **Tag Discovery**: List and analyze available tags for a repository\n\n## Commands\n\n### `/container-image:inspect`\n\nInspect and provide detailed breakdown of a container image.\n\n**Usage:**\n```bash\n/container-image:inspect <image>\n```\n\n**Examples:**\n```bash\n/container-image:inspect quay.io/openshift-release-dev/ocp-release:4.20.0-multi\n/container-image:inspect registry.redhat.io/ubi9/ubi:latest\n/container-image:inspect docker.io/library/nginx@sha256:abc123...\n```\n\n**What it shows:**\n- Inferred image purpose and context based on metadata analysis\n- Image digest and basic metadata\n- Architecture and OS information\n- Manifest type (single image vs manifest list)\n- For multi-arch images: all available platforms with per-platform digests, sizes, and layer counts\n- Platform comparison (size ranges, architecture list)\n- Size breakdown and layer details\n- Configuration (environment, entrypoint, ports, volumes)\n- Labels and annotations\n- Usage examples for pulling specific platforms\n\nSee [commands/inspect.md](commands/inspect.md) for full documentation.\n\n### `/container-image:compare`\n\nCompare two container images to identify differences.\n\n**Usage:**\n```bash\n/container-image:compare <image1> <image2>\n```\n\n**Examples:**\n```bash\n/container-image:compare quay.io/myapp:v1.0.0 quay.io/myapp:v2.0.0\n/container-image:compare registry.prod.example.com/myapp:latest registry.staging.example.com/myapp:latest\n```\n\n**What it shows:**\n- Whether images are identical (digest match)\n- Metadata differences (creation date, size)\n- Layer analysis (added, removed, modified layers)\n- Configuration changes (environment variables, labels, entrypoint)\n- Size impact\n- Summary of significant changes\n\nSee [commands/compare.md](commands/compare.md) for full documentation.\n\n### `/container-image:tags`\n\nList and analyze available tags for a container image repository.\n\n**Usage:**\n```bash\n/container-image:tags <repository>\n```\n\n**Examples:**\n```bash\n/container-image:tags quay.io/openshift-release-dev/ocp-release\n/container-image:tags docker.io/library/nginx\n```\n\n**What it shows:**\n- All available tags for the repository\n- Tag metadata (creation date, size, architecture)\n- Tag categorization (version, date-based, special tags)\n- Recent tags and update patterns\n- Recommendations for tag selection\n- Duplicate tags (same digest, different names)\n\nSee [commands/tags.md](commands/tags.md) for full documentation.\n\n## Installation\n\n### From the Claude Code Plugin Marketplace\n\n1. **Add the marketplace** (if not already added):\n   ```bash\n   /plugin marketplace add openshift-eng/ai-helpers\n   ```\n\n2. **Install the container-image plugin**:\n   ```bash\n   /plugin install container-image@ai-helpers\n   ```\n\n3. **Use the commands**:\n   ```bash\n   /container-image:inspect quay.io/openshift-release-dev/ocp-release:4.20.0-multi\n   ```\n\n## Prerequisites\n\n### Required Tools\n\n**skopeo** - Primary tool for image inspection\n\n- Check if installed: `which skopeo`\n- Installation:\n  - RHEL/Fedora: `sudo dnf install skopeo`\n  - Ubuntu/Debian: `sudo apt-get install skopeo`\n  - macOS: `brew install skopeo`\n- Documentation: https://github.com/containers/skopeo\n\n### Optional Tools\n\n**podman** - Additional image analysis capabilities\n\n- Installation:\n  - RHEL/Fedora: `sudo dnf install podman`\n  - Ubuntu/Debian: `sudo apt-get install podman`\n  - macOS: `brew install podman`\n- Documentation: https://podman.io/\n\n**dive** - Interactive layer analysis (for `/container-image:compare`)\n\n- Installation: https://github.com/wagoodman/dive\n- Provides detailed layer-by-layer exploration\n\n### Registry Authentication\n\nFor private registries, authenticate before running commands:\n\n```bash\n# Using skopeo\nskopeo login registry.example.com\n\n# Using podman (if installed)\npodman login registry.example.com\n```\n\nAuthentication is typically stored at `~/.docker/config.json` or `${XDG_RUNTIME_DIR}/containers/auth.json`.\n\n## Use Cases\n\n### Development Workflows\n\n1. **Version Selection**: Find the right image version for your deployment\n   ```bash\n   /container-image:tags quay.io/myapp\n   /container-image:inspect quay.io/myapp:v2.1.0\n   ```\n\n2. **Multi-Arch Development**: Verify architecture support before deployment\n   ```bash\n   /container-image:inspect registry.redhat.io/ubi9/ubi:latest\n   ```\n   The inspect command automatically detects and shows all available platforms for multi-arch images.\n\n3. **Update Analysis**: Understand changes before upgrading\n   ```bash\n   /container-image:compare myapp:current myapp:latest\n   ```\n\n### Troubleshooting\n\n1. **Deployment Issues**: Verify correct image is being used\n   ```bash\n   /container-image:inspect <failing-image>\n   ```\n\n2. **Architecture Mismatches**: Check platform compatibility\n   ```bash\n   /container-image:inspect <image>\n   ```\n   For multi-arch images, this will show all available platforms and their digests.\n\n3. **Size Issues**: Identify what's consuming space\n   ```bash\n   /container-image:inspect <large-image>\n   /container-image:compare <old-image> <new-image>\n   ```\n\n### Security & Compliance\n\n1. **Image Verification**: Confirm image authenticity via digest\n   ```bash\n   /container-image:inspect myapp@sha256:abc123...\n   ```\n\n2. **Change Tracking**: Document what changed between versions\n   ```bash\n   /container-image:compare prod:v1.0.0 prod:v1.1.0\n   ```\n\n3. **Registry Migration**: Verify images copied correctly\n   ```bash\n   /container-image:compare source.registry.com/app:v1 dest.registry.com/app:v1\n   ```\n\n## Common Workflows\n\n### Upgrading an Application Image\n\n```bash\n# 1. List available versions\n/container-image:tags quay.io/myapp\n\n# 2. Inspect the new version (shows all architectures if multi-arch)\n/container-image:inspect quay.io/myapp:v2.0.0\n\n# 3. Compare with current version\n/container-image:compare quay.io/myapp:v1.5.0 quay.io/myapp:v2.0.0\n```\n\n### Verifying Multi-Architecture Support\n\n```bash\n# 1. Check if image is multi-arch and see all platforms\n/container-image:inspect quay.io/myapp:latest\n\n# 2. Inspect specific platform by digest\n/container-image:inspect quay.io/myapp@sha256:<arm64-digest>\n\n# 3. Compare platforms\n/container-image:compare quay.io/myapp@sha256:<amd64-digest> quay.io/myapp@sha256:<arm64-digest>\n```\n\n### Investigating Image Bloat\n\n```bash\n# 1. Inspect current image\n/container-image:inspect myapp:latest\n\n# 2. Compare with previous version\n/container-image:compare myapp:v1.0.0 myapp:latest\n\n# 3. Identify which layers added size\n# (Layer analysis in the comparison output)\n```\n\n## Tips & Best Practices\n\n### Image References\n\n- **Use digests for production**: `myapp@sha256:abc123...` (immutable)\n- **Use tags for development**: `myapp:latest` (convenient but mutable)\n- **Be specific**: `myapp:v1.2.3` is better than `myapp:v1`\n\n### Multi-Architecture Images\n\n- Use `/container-image:inspect` to check platform support - it automatically detects and displays all available architectures\n- Pull specific platforms when needed: `podman pull --platform=linux/arm64 <image>`\n- Verify all platforms are updated in manifest lists by comparing platform digests\n\n### Performance\n\n- `skopeo inspect` doesn't pull the full image (fast and efficient)\n- For large repositories, `/container-image:tags` may sample tags\n- Use `--filter` options to narrow results for large tag lists\n\n### Security\n\n- Always verify image digests match expectations\n- Check for unexpected configuration changes with `/container-image:compare`\n- Use `/container-image:inspect` to review labels and metadata\n\n## Plugin Structure\n\n```\nplugins/container-image/\n‚îú‚îÄ‚îÄ .claude-plugin/\n‚îÇ   ‚îî‚îÄ‚îÄ plugin.json          # Plugin metadata\n‚îú‚îÄ‚îÄ commands/\n‚îÇ   ‚îú‚îÄ‚îÄ inspect.md           # Image inspection command\n‚îÇ   ‚îú‚îÄ‚îÄ compare.md           # Image comparison command\n‚îÇ   ‚îî‚îÄ‚îÄ tags.md              # Tag listing command\n‚îî‚îÄ‚îÄ README.md                # This file\n```\n\n## Development\n\n### Adding New Commands\n\nTo add a new command to this plugin:\n\n1. Create a new markdown file in `commands/`:\n   ```bash\n   touch plugins/container-image/commands/your-command.md\n   ```\n\n2. Follow the structure from existing commands (see `commands/inspect.md`)\n\n3. Include these sections:\n   - Name\n   - Synopsis\n   - Description\n   - Prerequisites\n   - Implementation\n   - Return Value\n   - Examples\n   - Error Handling\n   - Notes\n   - Arguments\n\n4. Test your command:\n   ```bash\n   /container-image:your-command\n   ```\n\n### Testing\n\nTest commands with various image types:\n- Public images (docker.io, quay.io)\n- Private registries (requires authentication)\n- Multi-arch images (manifest lists)\n- Single-arch images\n- Large images (layer analysis)\n- Different registries (Red Hat, Quay, Docker Hub)\n\n## Contributing\n\nContributions are welcome! When adding new container image analysis commands:\n\n1. Ensure the command provides unique value not covered by existing commands\n2. Follow the existing command structure and documentation format\n3. Include comprehensive examples and error handling\n4. Test with multiple registries and image types\n5. Update this README with new command documentation\n\n## License\n\nSee [LICENSE](../../LICENSE) for details.\n",
        "plugins/node/README.md": "# Node Plugin\n\nKubernetes and OpenShift node health monitoring and diagnostics.\n\n## Overview\n\nThe Node plugin provides comprehensive health checking and diagnostic capabilities for Kubernetes and OpenShift cluster nodes. It automates the inspection of node-level components including kubelet, CRI-O container runtime, system resources, and node conditions to ensure nodes are functioning properly.\n\n## Commands\n\n### `/node:cluster-node-health-check`\n\nPerform comprehensive health check on cluster nodes and report kubelet, CRI-O, and node-level issues.\n\n**Usage:**\n```bash\n/node:cluster-node-health-check [--node <node-name>] [--verbose] [--output-format json|text]\n```\n\n**Arguments:**\n- `--node <node-name>` (optional): Name of a specific node to check. If not provided, checks all nodes in the cluster.\n- `--verbose` (optional): Enable detailed output with additional context, including resource-level details, warning conditions, and remediation suggestions.\n- `--output-format` (optional): Output format for results (`text` or `json`). Defaults to `text`.\n\n**Examples:**\n\nCheck all nodes in the cluster:\n```bash\n/node:cluster-node-health-check\n```\n\nCheck a specific node:\n```bash\n/node:cluster-node-health-check --node worker-1\n```\n\nVerbose output with detailed diagnostics:\n```bash\n/node:cluster-node-health-check --verbose\n```\n\nJSON output for automation:\n```bash\n/node:cluster-node-health-check --output-format json\n```\n\n**What it checks:**\n\n1. **Node Status and Conditions**\n   - Ready status\n   - MemoryPressure, DiskPressure, PIDPressure\n   - NetworkUnavailable condition\n   - Node taints and scheduling constraints\n\n2. **Kubelet Service Health**\n   - Service status and restart counts\n   - Certificate validity\n   - Configuration issues\n\n3. **CRI-O Container Runtime**\n   - Runtime service status\n   - Container operation errors\n   - Version compatibility\n\n4. **Resource Utilization**\n   - CPU and memory allocation\n   - Disk space usage\n   - Pod count vs capacity\n   - Ephemeral storage\n\n5. **System Services**\n   - Critical daemon status (kubelet, crio)\n   - Failed systemd units\n\n6. **Kernel Parameters**\n   - Key sysctl settings for Kubernetes\n   - SELinux status\n\n7. **Pod Health on Nodes**\n   - Running, pending, and failed pods\n   - High restart counts\n   - Resource pressure impact\n\n8. **Recent Events**\n   - Warning events for nodes\n   - Pod events on nodes\n\n**Output:**\n\nThe command provides:\n- Overall health status (Healthy ‚úÖ / Warning ‚ö†Ô∏è / Critical ‚ùå)\n- Detailed findings for each node\n- Specific issues with severity levels\n- Impact assessment\n- Recommended remediation actions\n- Diagnostic commands for further investigation\n\nSee [commands/cluster-node-health-check.md](commands/cluster-node-health-check.md) for detailed documentation.\n\n## Prerequisites\n\n- **Kubernetes/OpenShift CLI**: Either `oc` or `kubectl` must be installed\n- **Active cluster connection**: Must be connected to a running cluster\n- **Sufficient permissions**: Read access to nodes and pods, ability to create debug pods for node-level inspection\n\n## Use Cases\n\n- **Pre-deployment validation**: Verify node health before deploying applications\n- **Troubleshooting**: Diagnose node-related issues affecting workload performance\n- **Capacity planning**: Understand resource utilization across nodes\n- **Proactive monitoring**: Regular health checks to catch issues early\n- **Post-upgrade validation**: Verify node health after cluster upgrades\n- **CI/CD integration**: Automated node health verification in pipelines\n\n## Common Issues Detected\n\nThe plugin can detect and report:\n\n- Nodes in NotReady state\n- Kubelet service failures or frequent restarts\n- CRI-O runtime errors\n- Memory or disk pressure conditions\n- Network unavailability\n- High pod restart counts\n- Resource exhaustion (CPU, memory, disk)\n- Failed system services\n- Certificate expiration warnings\n- Scheduling constraints (taints, labels)\n\n## Installation\n\n### From Marketplace\n\n```bash\n# Add the ai-helpers marketplace\n/plugin marketplace add openshift-eng/ai-helpers\n\n# Install the node plugin\n/plugin install node@ai-helpers\n```\n\n### Manual Installation\n\n```bash\n# Clone the repository\ngit clone https://github.com/openshift-eng/ai-helpers.git\n\n# Link to Claude Code plugins directory\nln -s $(pwd)/ai-helpers/plugins/node ~/.claude/plugins/node\n```\n\n## Contributing\n\nContributions are welcome! Please see the main [CLAUDE.md](../../CLAUDE.md) for plugin development guidelines.\n\n## License\n\nApache License 2.0 - See [LICENSE](../../LICENSE) for details.\n",
        "plugins/bigquery/README.md": "# BigQuery Plugin\n\nBigQuery cost analysis and optimization utilities for Google Cloud Platform projects.\n\n## Overview\n\nThis plugin helps analyze BigQuery usage patterns, identify expensive queries, and provide optimization recommendations to reduce costs. It's particularly useful for:\n\n- Monitoring BigQuery spending and usage trends\n- Identifying top consumers (users, service accounts, queries)\n- Finding optimization opportunities\n- Generating usage reports for stakeholders\n- Debugging cost overruns and threshold violations\n\n## Prerequisites\n\n- Google Cloud SDK installed (`gcloud` and `bq` CLI tools)\n- Authenticated to GCP: `gcloud auth login`\n- BigQuery read access to the projects you want to analyze\n- At minimum, `bigquery.jobs.list` permission\n\n### Installation\n\n**macOS (Homebrew):**\n```bash\nbrew install google-cloud-sdk\n```\n\n**Other platforms:**\nVisit https://cloud.google.com/sdk/docs/install\n\n**Verify installation:**\n```bash\nbq version\ngcloud auth list\n```\n\n## Commands\n\n### `/bigquery:analyze-usage`\n\nAnalyze BigQuery usage and costs for a project.\n\n**Usage:**\n```\n/bigquery:analyze-usage <project-id> <timeframe>\n```\n\n**Arguments:**\n- `project-id`: GCP project ID (e.g., `openshift-ci-data-analysis`)\n- `timeframe`: Time period to analyze (e.g., \"24 hours\", \"7 days\", \"30 days\")\n\n**Examples:**\n```\n/bigquery:analyze-usage openshift-ci-data-analysis \"24 hours\"\n/bigquery:analyze-usage my-project \"7 days\"\n/bigquery:analyze-usage prod-data-warehouse \"30 days\"\n```\n\n**Output:**\n- Executive summary (total usage, costs, key findings)\n- Usage by user/service account (top consumers)\n- Top query patterns with optimization recommendations\n- Top individual queries by cost\n- Prioritized optimization recommendations\n- Option to save report to markdown file\n\n## Skills\n\n### `bigquery:analyze-usage`\n\nCore analysis skill that:\n- Queries INFORMATION_SCHEMA.JOBS for usage data\n- Analyzes query patterns and identifies optimization opportunities\n- Calculates costs and usage metrics\n- Generates actionable recommendations\n\nThis skill is automatically invoked by the `/bigquery:analyze-usage` command.\n\n## Features\n\n### Usage Analysis\n- Total queries executed\n- Total data scanned (TB/GB)\n- Estimated costs (on-demand pricing)\n- Breakdown by user and service account\n- Query frequency and patterns\n\n### Query Pattern Detection\n- Groups similar queries together\n- Identifies high-frequency patterns\n- Calculates aggregate costs per pattern\n- Highlights optimization opportunities\n\n### Optimization Recommendations\n- Tables needing partitioning or clustering\n- Queries using `SELECT *` instead of column pruning\n- Full table scans without WHERE clauses\n- High-frequency queries that could benefit from caching\n- Scheduled queries running too often\n- Estimated savings per recommendation\n\n### Report Generation\n- Clean, readable markdown format\n- Tables for easy comparison\n- Detailed analysis with context\n- Exportable for sharing with teams\n\n## Cost Calculation\n\nCosts are estimated using Google Cloud on-demand pricing:\n- **$6.25 per TB** of data scanned\n\n**Note:** Actual costs may differ if you have:\n- Flat-rate pricing\n- Reserved capacity\n- Committed use discounts\n- Different regional pricing\n\nThe reports note this and focus on relative costs for comparison.\n\n## Common Use Cases\n\n### 1. Daily Cost Monitoring\n```\n/bigquery:analyze-usage my-project \"24 hours\"\n```\nMonitor daily usage and catch cost spikes early.\n\n### 2. Weekly Review\n```\n/bigquery:analyze-usage my-project \"7 days\"\n```\nReview weekly trends and identify optimization opportunities.\n\n### 3. Monthly Reporting\n```\n/bigquery:analyze-usage my-project \"30 days\"\n```\nGenerate monthly reports for stakeholders.\n\n### 4. Debugging Cost Overruns\n```\n/bigquery:analyze-usage my-project \"1 hour\"\n```\nWhen you see a cost spike, analyze the last hour to identify the culprit.\n\n### 5. Service Account Auditing\nIdentify which service accounts are driving costs and whether their usage is expected.\n\n## Optimization Tips\n\nCommon issues the plugin identifies:\n\n### 1. SELECT * Queries\n**Problem:** Scanning entire tables when only a few columns are needed.\n**Fix:** Specify only required columns.\n**Savings:** Often 50-90% reduction in bytes scanned.\n\n### 2. Missing Partitioning\n**Problem:** Full table scans on large time-series data.\n**Fix:** Partition tables by date column, use partition filters.\n**Savings:** 90-99% reduction in bytes scanned.\n\n### 3. Missing Clustering\n**Problem:** Scanning entire partitions when querying by specific values.\n**Fix:** Cluster tables by frequently filtered columns.\n**Savings:** 50-90% reduction in bytes scanned.\n\n### 4. Unfiltered Queries\n**Problem:** No WHERE clause, scanning entire table.\n**Fix:** Add appropriate WHERE clauses with date/ID filters.\n**Savings:** 80-99% reduction in bytes scanned.\n\n### 5. High-Frequency Queries\n**Problem:** Running the same expensive query repeatedly.\n**Fix:** Use query result caching, reduce frequency, or materialize results.\n**Savings:** Depends on frequency, often 70-95% reduction.\n\n### 6. Scheduled Query Frequency\n**Problem:** Scheduled queries running more often than necessary.\n**Fix:** Reduce frequency if data doesn't change that often.\n**Savings:** Linear with frequency reduction.\n\n## Troubleshooting\n\n### Authentication Issues\n```bash\ngcloud auth login\ngcloud config set project <project-id>\n```\n\n### Permission Issues\nEnsure you have BigQuery Job User role:\n```bash\ngcloud projects get-iam-policy <project-id> --flatten=\"bindings[].members\" --filter=\"bindings.members:user:YOUR_EMAIL\"\n```\n\n### \"bq command not found\"\nInstall Google Cloud SDK (see Prerequisites section).\n\n### No Data Returned\n- Verify queries ran in the timeframe\n- Check you're using the correct region (`region-us` vs `US` vs `EU`)\n- Ensure project ID is correct (not project name)\n\n### Slow Analysis\nLonger timeframes (30 days) analyze more data and take longer. For very large projects, consider shorter timeframes or analyzing specific users.\n\n## Region Support\n\nBy default, queries use `region-us` for INFORMATION_SCHEMA. If your project uses a different region, you may need to adjust queries:\n\n- US multi-region: `region-us` or `US`\n- EU multi-region: `region-eu` or `EU`\n- Asia multi-region: `region-asia`\n\nThe plugin will attempt to detect the correct region automatically.\n\n## Privacy and Security\n\n- All analysis is read-only\n- No data is modified or exported outside your environment\n- Query text is truncated (first 200-300 chars) in reports\n- Sensitive data in query text is not filtered - review reports before sharing\n- Reports are saved locally only\n\n## Contributing\n\nThis plugin is part of the ai-helpers repository. Contributions welcome!\n\n## License\n\nSame as parent ai-helpers repository.\n",
        "plugins/workspaces/README.md": "# Workspaces Plugin\n\n**Isolated git worktree workspaces for multi-repo development.**\n\nWork on features spanning multiple repositories without branch conflicts. Each task gets its own directory with synchronized worktrees‚Äîdelete everything at once when done.\n\n```bash\n/workspaces:create New field in azure machine template\n# Creates workspace with worktrees for machine-api-operator, machine-api-provider-azure,\n# api, client-go. All on fresh branch off origin/main.\n\n/workspaces:delete new-field-in-azure-machine-template\n# Clean up when done\n```\n\n**Customizable:** Define aliases, auto-include rules (e.g., azure work ‚Üí include related repos), and naming conventions for your team.\n\n## Commands\n\n| Command | Description |\n|---------|-------------|\n| `/workspaces:create` | Create workspace with git worktrees for specified repos |\n| `/workspaces:delete` | Remove workspace (checks for uncommitted/unpushed work) |\n\n---\n\n## Setup\n\nFirst run prompts for two paths:\n- **Git repos root**: Where your repos are cloned (e.g., `~/git`)\n- **Workspaces root**: Where workspaces are created (e.g., `~/workspaces`)\n\nConfig stored in `~/.claude/plugins/config/workspaces/config.env`.\n\nTo reconfigure:\n```bash\nrm ~/.claude/plugins/config/workspaces/config.env\n# Next command will prompt for new paths\n```\n\n## How It Works\n\n**Git worktrees** let you check out multiple branches of the same repo simultaneously. This plugin:\n\n1. Creates a dedicated directory for your task\n2. Adds worktrees for each repository (all on the same feature branch)\n3. Copies template files (customize in `${CLAUDE_WORKSPACES_ROOT}/.template/`)\n4. Creates a `CLAUDE.md` with workspace context\n\n**Workspace structure:**\n```\n${CLAUDE_WORKSPACES_ROOT}/\n‚îú‚îÄ‚îÄ .template/              # Template copied to new workspaces\n‚îî‚îÄ‚îÄ TEAM-1234/              # Your workspace\n    ‚îú‚îÄ‚îÄ CLAUDE.md           # Context for Claude\n    ‚îú‚îÄ‚îÄ frontend/           # Git worktree (branch: TEAM-1234)\n    ‚îî‚îÄ‚îÄ backend/            # Git worktree (branch: TEAM-1234)\n```\n\n**Benefits:**\n- Isolated environments per task (no branch switching in main repos)\n- Atomic workspace deletion (clean up everything at once)\n- Shared branches across repos (consistent naming)\n- Template customization (add scripts, configs, etc.)\n\n## Customization\n\n**Custom aliases and auto-detect rules:**\n\nA template file `~/.claude/plugins/config/workspaces/custom-prompt.md.template` is created on first setup.\n\nTo customize for your team:\n```bash\ncd ~/.claude/plugins/config/workspaces\ncp custom-prompt.md.template custom-prompt.md\n# Edit custom-prompt.md to add your team's aliases and auto-detect rules\n```\n\nExample customizations:\n- Repository name aliases (e.g., `FE` ‚Üí `frontend`)\n- Auto-detect rules (e.g., when `frontend` is selected, also add `shared-components`)\n\n**Template customization:**\n\nAdd files to all new workspaces:\n```bash\ncd ${CLAUDE_WORKSPACES_ROOT}/.template\n# Add your .gitignore, scripts, etc.\n```\n\n## Requirements\n\n- `git` v2.5+ (worktree support)\n- `bash` v4.0+\n- `gh` CLI (optional, for PR checkout)\n- Linux or macOS\n",
        "plugins/gwapi/README.md": "# Gateway API Plugin\n\nInstall and configure Gateway API resources on Kubernetes and OpenShift clusters.\n\n## Overview\n\nThis Gateway API plugin provides utilities for installing Gateway API resources with automatic cluster configuration. It simplifies the deployment of GatewayClass and Gateway resources by applying the appropriate configuration.\n\n## Commands\n\n### `/gwapi:install`\n\nInstall Gateway API resources to a Kubernetes/OpenShift cluster.\n\nSee [commands/install.md](commands/install.md) for complete documentation.\n\n### `/gwapi:check`\n\nCheck the installed Gateway API resources in the connected cluster.\n\nSee [commands/check.md](commands/check.md) for complete documentation.\n\n### `/gwapi:delete`\n\nDelete Gateway API resources in the Kubernetes/OpenShift cluster.\n\nSee [commands/delete.md](commands/delete.md) for complete documentation.\n\n**Synopsis:**\n```bash\n/gwapi:install [namespace]\n/gwapi:check [namespace]\n/gwapi:delete [namespace]\n```\n\n**Features:**\n- Automatically detects cluster ingress domain\n- Installs GatewayClass and Gateway resources\n- Supports both OpenShift (`oc`) and Kubernetes (`kubectl`)\n- Optional namespace targeting\n- Check installed Gateway API resources\n- Delete Gateway API resources\n- Idempotent installation (safe to run multiple times)\n\n## Installation\n\n```bash\n/plugin install gwapi@ai-helpers\n```\n\n## Prerequisites\n\n- Either `oc` (OpenShift CLI) or `kubectl` (Kubernetes CLI) must be installed\n- Active connection to a Kubernetes or OpenShift cluster\n- Appropriate permissions to create cluster-scoped resources (GatewayClass) and namespaced resources (Gateway)\n\n## Resources Installed\n\nThe plugin installs, checks and deletes the following Gateway API resources:\n\n1. **GatewayClass** (`openshift-default`)\n   - Controller: `openshift.io/gateway-controller/v1`\n   - Cluster-scoped resource defining the gateway implementation\n\n2. **Gateway** (`gateway`)\n   - Namespace: `openshift-ingress` (default)\n   - Hostname pattern: `*.gwapi.${DOMAIN}` (automatically configured)\n   - Listener on port 80 (HTTP)\n   - Allows routes from all namespaces\n\n## How It Works\n\n1. Detects available CLI tool (`oc` or `kubectl`)\n2. Verifies cluster connectivity\n3. Retrieves cluster ingress domain (OpenShift) or prompts for manual input (Kubernetes)\n4. Applies GatewayClass resource\n5. Substitutes cluster domain into Gateway resource and applies it\n6. Verifies installation success\n7. Checks the installed and other related Gateway API resources\n8. Deletes all related resources after prompting the user\n\n## Notes\n\n- The Gateway resource uses `${DOMAIN}` as a placeholder that gets replaced with your cluster's actual ingress domain\n- Resources are applied idempotently - you can run the command multiple times safely\n- Original YAML files are not modified; domain substitution happens in-memory during application\n- Deleting the Gateway API resources provides warnings and disclaimers\n",
        "plugins/code-review/README.md": "# code-review Plugin\n\nAutomated code quality review with language-aware analysis for pre-commit verification.\n\n## Commands\n\n### `/code-review:pre-commit-review`\n\nPerforms a comprehensive code quality review of staged and unstaged changes before committing. Analyzes unit test coverage, idiomatic code patterns, DRY compliance, SOLID principles, and build verification.\n\n**Usage:**\n```bash\n/code-review:pre-commit-review [--language <lang>] [--profile <name>] [--skip-build] [--skip-tests]\n```\n\n**Arguments:**\n- `--language <lang>`: Language skill to load. Currently shipped: `go`. Planned: `python`, `rust`, `typescript`, `java`. Auto-detected if omitted.\n- `--profile <name>`: Project profile to load for project-specific conventions.\n- `--skip-build`: Skip build verification.\n- `--skip-tests`: Skip unit test coverage review.\n\n## Language Skills\n\nLanguage skills provide language-specific guidance for idiomatic code review, test conventions, and build commands. They are stored in `skills/lang-<lang>/SKILL.md`.\n\n### Available Languages\n\n| Language | Skill Directory | Key Features |\n|----------|----------------|--------------|\n| Go | `skills/lang-go/` | Table-driven tests, `gofmt`, error wrapping, race detection |\n\n### Adding a New Language\n\n1. Create a directory: `skills/lang-<lang>/`\n2. Create `SKILL.md` with the following sections:\n   - **When to Use This Skill**: Describe when this skill is loaded.\n   - **Test Conventions**: Language-specific test patterns, file organization, and best practices.\n   - **Idiomatic Code Checklist**: Language-specific code quality checks (formatting, naming, error handling, idioms).\n   - **Build Commands**: Priority-ordered build, test, and verification commands.\n3. Use the frontmatter format:\n   ```yaml\n   ---\n   name: \"<Language> Language Review\"\n   description: \"Language-specific review guidance for <Language> code\"\n   ---\n   ```\n\n## Profile Skills\n\nProfile skills provide project-specific conventions that layer on top of language checks. They are stored in `skills/profile-<name>/SKILL.md`.\n\n### Available Profiles\n\n| Profile | Skill Directory | Key Features |\n|---------|----------------|--------------|\n| HyperShift | `skills/profile-hypershift/` | controller-runtime patterns, `support/upsert/`, `make api`, structured logging |\n\n### Adding a New Profile\n\n1. Create a directory: `skills/profile-<name>/`\n2. Create `SKILL.md` with the following sections:\n   - **When to Use This Skill**: Describe when this profile is loaded.\n   - **Additional Test Conventions**: Project-specific test patterns that supplement the language conventions.\n   - **Project-Specific Patterns**: Architectural patterns, framework usage, and coding conventions specific to the project.\n   - **Project Utilities**: Shared packages or utilities that should be used instead of reimplementing.\n   - **Build Commands**: Project-specific build, test, and verification commands.\n   - **Additional Checks**: Any project-specific checks (e.g., API generation, commit format).\n3. Use the frontmatter format:\n   ```yaml\n   ---\n   name: \"<Project> Project Profile\"\n   description: \"Project-specific review profile for <project>\"\n   ---\n   ```\n4. All guidance must be self-contained within the SKILL.md. Do not reference external paths that may not exist in every repository.\n\n## How It Works\n\nThe command runs in a defined sequence of steps:\n\n1. **Parse arguments** and load applicable language and profile skills.\n2. **Identify changed files** from `git diff`.\n3. **Unit test coverage review** with language and profile conventions applied.\n4. **Idiomatic code review** with language-specific checklist.\n5. **DRY principle review** with profile-aware utility checks.\n6. **SOLID principles review** with profile-aware structural patterns.\n7. **Build verification** using profile, language, or auto-detected build commands.\n8. **Project-specific checks** from the profile (if loaded).\n9. **Generate report** with verdict and actionable findings.\n\n## Examples\n\n```bash\n# Auto-detect language, no profile\n/code-review:pre-commit-review\n\n# Go code with HyperShift conventions\n/code-review:pre-commit-review --language go --profile hypershift\n\n# Skip build for docs-only changes\n/code-review:pre-commit-review --skip-build\n\n# Python review without test checks\n/code-review:pre-commit-review --language python --skip-tests\n```\n"
      },
      "plugins": [
        {
          "name": "git",
          "source": "./plugins/git",
          "description": "Git Plugin",
          "version": "0.0.4",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add openshift-eng/ai-helpers",
            "/plugin install git@ai-helpers"
          ]
        },
        {
          "name": "hello-world",
          "source": "./plugins/hello-world",
          "description": "Hello World Plugin",
          "version": "1.0.1",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add openshift-eng/ai-helpers",
            "/plugin install hello-world@ai-helpers"
          ]
        },
        {
          "name": "jira",
          "source": "./plugins/jira",
          "description": "A plugin to automate tasks with Jira",
          "version": "0.3.1",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add openshift-eng/ai-helpers",
            "/plugin install jira@ai-helpers"
          ]
        },
        {
          "name": "ci",
          "source": "./plugins/ci",
          "description": "A plugin to work with OpenShift CI",
          "version": "0.0.5",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add openshift-eng/ai-helpers",
            "/plugin install ci@ai-helpers"
          ]
        },
        {
          "name": "teams",
          "source": "./plugins/teams",
          "description": "Team structure knowledge and health analysis commands for OpenShift teams",
          "version": "0.0.2",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add openshift-eng/ai-helpers",
            "/plugin install teams@ai-helpers"
          ]
        },
        {
          "name": "doc",
          "source": "./plugins/doc",
          "description": "A plugin for engineering documentation and notes",
          "version": "0.0.2",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add openshift-eng/ai-helpers",
            "/plugin install doc@ai-helpers"
          ]
        },
        {
          "name": "session",
          "source": "./plugins/session",
          "description": "A plugin for Claude session management and persistence",
          "version": "0.0.2",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add openshift-eng/ai-helpers",
            "/plugin install session@ai-helpers"
          ]
        },
        {
          "name": "sosreport",
          "source": "./plugins/sosreport",
          "description": "Analyze sosreport archives for system diagnostics and troubleshooting",
          "version": "0.0.2",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add openshift-eng/ai-helpers",
            "/plugin install sosreport@ai-helpers"
          ]
        },
        {
          "name": "utils",
          "source": "./plugins/utils",
          "description": "A generic utilities plugin serving as a catch-all for various helper commands",
          "version": "0.0.5",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add openshift-eng/ai-helpers",
            "/plugin install utils@ai-helpers"
          ]
        },
        {
          "name": "olm",
          "source": "./plugins/olm",
          "description": "OLM (Operator Lifecycle Manager) plugin for operator management and debugging",
          "version": "0.1.0",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add openshift-eng/ai-helpers",
            "/plugin install olm@ai-helpers"
          ]
        },
        {
          "name": "olm-team",
          "source": "./plugins/olm-team",
          "description": "OLM team development utilities and onboarding tools",
          "version": "0.2.0",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add openshift-eng/ai-helpers",
            "/plugin install olm-team@ai-helpers"
          ]
        },
        {
          "name": "prow-job",
          "source": "./plugins/prow-job",
          "description": "A plugin to analyze and inspect Prow CI job results",
          "version": "0.0.6",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add openshift-eng/ai-helpers",
            "/plugin install prow-job@ai-helpers"
          ]
        },
        {
          "name": "agendas",
          "source": "./plugins/agendas",
          "description": "A plugin to create various meeting agendas",
          "version": "0.0.2",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add openshift-eng/ai-helpers",
            "/plugin install agendas@ai-helpers"
          ]
        },
        {
          "name": "openshift",
          "source": "./plugins/openshift",
          "description": "OpenShift development utilities and helpers",
          "version": "0.0.2",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add openshift-eng/ai-helpers",
            "/plugin install openshift@ai-helpers"
          ]
        },
        {
          "name": "etcd",
          "source": "./plugins/etcd",
          "description": "Etcd cluster health monitoring and performance analysis utilities",
          "version": "0.0.2",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add openshift-eng/ai-helpers",
            "/plugin install etcd@ai-helpers"
          ]
        },
        {
          "name": "yaml",
          "source": "./plugins/yaml",
          "description": "YAML documentation and utilities",
          "version": "0.0.2",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add openshift-eng/ai-helpers",
            "/plugin install yaml@ai-helpers"
          ]
        },
        {
          "name": "must-gather",
          "source": "./plugins/must-gather",
          "description": "A plugin to analyze and report on must-gather data",
          "version": "0.0.2",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add openshift-eng/ai-helpers",
            "/plugin install must-gather@ai-helpers"
          ]
        },
        {
          "name": "lvms",
          "source": "./plugins/lvms",
          "description": "LVMS (Logical Volume Manager Storage) plugin for troubleshooting and debugging storage issues",
          "version": "0.1.0",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add openshift-eng/ai-helpers",
            "/plugin install lvms@ai-helpers"
          ]
        },
        {
          "name": "metrics",
          "source": "./plugins/metrics",
          "description": "Anonymous metrics usage for ai-helpers",
          "version": "0.0.2",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add openshift-eng/ai-helpers",
            "/plugin install metrics@ai-helpers"
          ]
        },
        {
          "name": "hcp",
          "source": "./plugins/hcp",
          "description": "Generate HyperShift cluster creation commands via hcp CLI from natural language descriptions",
          "version": "0.0.2",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add openshift-eng/ai-helpers",
            "/plugin install hcp@ai-helpers"
          ]
        },
        {
          "name": "compliance",
          "source": "./plugins/compliance",
          "description": "Security compliance and vulnerability analysis tools for Go projects",
          "version": "0.0.2",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add openshift-eng/ai-helpers",
            "/plugin install compliance@ai-helpers"
          ]
        },
        {
          "name": "test-coverage",
          "source": "./plugins/test-coverage",
          "description": "Analyze test coverage and identify gaps in test scenarios",
          "version": "0.0.2",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add openshift-eng/ai-helpers",
            "/plugin install test-coverage@ai-helpers"
          ]
        },
        {
          "name": "node-tuning",
          "source": "./plugins/node-tuning",
          "description": "Generate and analyze OpenShift node tuning profiles",
          "version": "1.0.0",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add openshift-eng/ai-helpers",
            "/plugin install node-tuning@ai-helpers"
          ]
        },
        {
          "name": "origin",
          "source": "./plugins/origin",
          "description": "Helpers for openshift/origin development.",
          "version": "0.0.2",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add openshift-eng/ai-helpers",
            "/plugin install origin@ai-helpers"
          ]
        },
        {
          "name": "container-image",
          "source": "./plugins/container-image",
          "description": "Container image inspection and analysis using skopeo and podman",
          "version": "0.0.2",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add openshift-eng/ai-helpers",
            "/plugin install container-image@ai-helpers"
          ]
        },
        {
          "name": "node",
          "source": "./plugins/node",
          "description": "Kubernetes and OpenShift node health monitoring and diagnostics",
          "version": "0.0.2",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add openshift-eng/ai-helpers",
            "/plugin install node@ai-helpers"
          ]
        },
        {
          "name": "bigquery",
          "source": "./plugins/bigquery",
          "description": "BigQuery analysis utilities",
          "version": "0.0.2",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add openshift-eng/ai-helpers",
            "/plugin install bigquery@ai-helpers"
          ]
        },
        {
          "name": "workspaces",
          "source": "./plugins/workspaces",
          "description": "Manage isolated git worktree workspaces for multi-repo development",
          "version": "1.0.0",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add openshift-eng/ai-helpers",
            "/plugin install workspaces@ai-helpers"
          ]
        },
        {
          "name": "gwapi",
          "source": "./plugins/gwapi",
          "description": "Gateway API installation utilities for Kubernetes/OpenShift clusters",
          "version": "0.0.1",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add openshift-eng/ai-helpers",
            "/plugin install gwapi@ai-helpers"
          ]
        },
        {
          "name": "code-review",
          "source": "./plugins/code-review",
          "description": "Automated code quality review with language-aware analysis for pre-commit verification",
          "version": "0.0.4",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add openshift-eng/ai-helpers",
            "/plugin install code-review@ai-helpers"
          ]
        }
      ]
    }
  ]
}