{
  "author": {
    "id": "jcputney",
    "display_name": "Jonathan Putney",
    "type": "User",
    "avatar_url": "https://avatars.githubusercontent.com/u/42720634?u=0534ac3b2f169f8c5b4680c79ec8266e4a254d4f&v=4",
    "url": "https://github.com/jcputney",
    "bio": "VP, Technology @noverant LMS",
    "stats": {
      "total_marketplaces": 1,
      "total_plugins": 1,
      "total_commands": 1,
      "total_skills": 1,
      "total_stars": 14,
      "total_forks": 1
    }
  },
  "marketplaces": [
    {
      "name": "agent-peer-review-marketplace",
      "version": "1.0.9",
      "description": "Marketplace for AI-to-AI peer review plugins. Multiple perspectives catch more issues than one.",
      "owner_info": {
        "name": "jcputney",
        "url": "https://github.com/jcputney"
      },
      "keywords": [],
      "repo_full_name": "jcputney/agent-peer-review",
      "repo_url": "https://github.com/jcputney/agent-peer-review",
      "repo_description": "A Claude Code plugin that validates Claude's work using OpenAI Codex CLI. Two AI perspectives catch more issues than one.",
      "homepage": null,
      "signals": {
        "stars": 14,
        "forks": 1,
        "pushed_at": "2026-01-26T21:37:38Z",
        "created_at": "2026-01-14T21:39:54Z",
        "license": "MIT"
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 887
        },
        {
          "path": "plugins",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/codex-peer-review",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/codex-peer-review/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/codex-peer-review/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 626
        },
        {
          "path": "plugins/codex-peer-review/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/codex-peer-review/agents/codex-peer-reviewer.md",
          "type": "blob",
          "size": 9022
        },
        {
          "path": "plugins/codex-peer-review/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/codex-peer-review/commands/codex-peer-review.md",
          "type": "blob",
          "size": 3984
        },
        {
          "path": "plugins/codex-peer-review/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/codex-peer-review/hooks/hooks.json",
          "type": "blob",
          "size": 1648
        },
        {
          "path": "plugins/codex-peer-review/hooks/peer-review-reminder.sh",
          "type": "blob",
          "size": 1241
        },
        {
          "path": "plugins/codex-peer-review/hooks/plan-peer-review-check.sh",
          "type": "blob",
          "size": 662
        },
        {
          "path": "plugins/codex-peer-review/hooks/stop-peer-review-check.sh",
          "type": "blob",
          "size": 1107
        },
        {
          "path": "plugins/codex-peer-review/hooks/task-peer-review-check.sh",
          "type": "blob",
          "size": 652
        },
        {
          "path": "plugins/codex-peer-review/hooks/user-prompt-check.sh",
          "type": "blob",
          "size": 889
        },
        {
          "path": "plugins/codex-peer-review/hooks/write-peer-review-check.sh",
          "type": "blob",
          "size": 804
        },
        {
          "path": "plugins/codex-peer-review/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/codex-peer-review/skills/codex-peer-review",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/codex-peer-review/skills/codex-peer-review/SKILL.md",
          "type": "blob",
          "size": 15949
        },
        {
          "path": "plugins/codex-peer-review/skills/codex-peer-review/common-mistakes.md",
          "type": "blob",
          "size": 7399
        },
        {
          "path": "plugins/codex-peer-review/skills/codex-peer-review/discussion-protocol.md",
          "type": "blob",
          "size": 6062
        },
        {
          "path": "plugins/codex-peer-review/skills/codex-peer-review/escalation-criteria.md",
          "type": "blob",
          "size": 5248
        }
      ],
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"$schema\": \"https://anthropic.com/claude-code/marketplace.schema.json\",\n  \"name\": \"agent-peer-review-marketplace\",\n  \"version\": \"1.0.9\",\n  \"description\": \"Marketplace for AI-to-AI peer review plugins. Multiple perspectives catch more issues than one.\",\n  \"owner\": {\n    \"name\": \"jcputney\",\n    \"url\": \"https://github.com/jcputney\"\n  },\n  \"repository\": \"https://github.com/jcputney/agent-peer-review\",\n  \"plugins\": [\n    {\n      \"name\": \"codex-peer-review\",\n      \"version\": \"1.0.9\",\n      \"description\": \"Peer validation system using OpenAI Codex CLI. Two AI perspectives catch more issues than one.\",\n      \"author\": {\n        \"name\": \"jcputney\",\n        \"url\": \"https://github.com/jcputney\"\n      },\n      \"source\": \"./plugins/codex-peer-review\",\n      \"category\": \"code-quality\",\n      \"tags\": [\"peer-review\", \"code-review\", \"validation\", \"codex\", \"ai-collaboration\"]\n    }\n  ]\n}\n",
        "plugins/codex-peer-review/.claude-plugin/plugin.json": "{\n  \"name\": \"codex-peer-review\",\n  \"version\": \"1.0.9\",\n  \"description\": \"Peer validation system using OpenAI Codex CLI. Validates Claude's designs and code reviews through structured discussion before presenting to user.\",\n  \"author\": {\n    \"name\": \"jcputney\",\n    \"url\": \"https://github.com/jcputney\"\n  },\n  \"repository\": \"https://github.com/jcputney/agent-peer-review\",\n  \"license\": \"MIT\",\n  \"keywords\": [\"peer-review\", \"code-review\", \"validation\", \"codex\", \"ai-collaboration\"],\n  \"commands\": [\"./commands/codex-peer-review.md\"],\n  \"agents\": [\"./agents/codex-peer-reviewer.md\"],\n  \"skills\": [\"./skills/codex-peer-review\"]\n}\n",
        "plugins/codex-peer-review/agents/codex-peer-reviewer.md": "---\nname: codex-peer-reviewer\ndescription: Use this agent to run peer review validation with Codex CLI. Dispatches to a separate context to keep the main conversation clean. Returns synthesized peer review results.\nmodel: sonnet\ncolor: cyan\npermissionMode: bypassPermissions\nskills:\n  - codex-peer-review\nallowed-tools:\n  - Bash(codex exec*)\n  - Bash(codex review*)\n  - Bash(command -v codex*)\n  - Bash(command -v jq*)\n  - Bash(jq *)\n  - Bash(grep *)\n  - Bash(mcp-cli *)\n  - Bash(tee *)\n  - Bash(cat *)\n  - Read\n  - WebSearch\n  - TaskCreate\n  - TaskUpdate\n  - TaskList\n---\n\n# Codex Peer Reviewer Agent\n\nYou are a peer review agent that validates Claude's work using OpenAI Codex CLI. You run in a separate context to keep the main conversation clean.\n\n## Your Task\n\nYou will receive one of the following from the main conversation:\n1. **Claude's design/plan** to validate\n2. **Claude's code review findings** to cross-check\n3. **An architecture recommendation** to verify\n4. **A broad technical question** Claude answered\n\n## Workflow\n\n### Step 0: Create Progress Task\n\n**IMPORTANT:** Before doing any work, create a task to show progress to the user:\n\n```\nTaskCreate:\n  subject: \"Peer review validation\"\n  description: \"Running AI-to-AI peer review with Codex CLI\"\n  activeForm: \"Running peer review...\"\n```\n\nThen immediately mark it in_progress:\n```\nTaskUpdate:\n  taskId: [the task ID]\n  status: \"in_progress\"\n```\n\nUpdate the `activeForm` as you progress through steps:\n- `\"Verifying Codex CLI...\"` → Step 1\n- `\"Getting Codex perspective...\"` → Step 2\n- `\"Comparing AI positions...\"` → Step 3\n- `\"Synthesizing results...\"` → Step 4\n\n### Step 1: Verify Prerequisites\n\nUpdate task: `activeForm: \"Verifying Codex CLI...\"`\n\n```bash\n# Check codex CLI\nif ! command -v codex &>/dev/null; then\n  echo \"ERROR: Codex CLI not installed. Cannot proceed with peer review.\"\n  exit 1\nfi\n```\n\n### Step 2: Choose the Right Codex Command\n\nUpdate task: `activeForm: \"Getting Codex perspective...\"`\n\n## DEFAULT TO `codex exec` - ALMOST ALWAYS THE RIGHT CHOICE\n\n`codex exec` is the preferred command for nearly all peer review scenarios. It gives you precise control over what gets analyzed and avoids runaway reviews of entire branches.\n\n**Only use `codex review` when:**\n- User explicitly says \"review the entire branch\" → `--base`\n- User explicitly says \"review all uncommitted changes\" → `--uncommitted`\n- User explicitly says \"review this commit\" → `--commit <sha>`\n\n**Use `codex exec` for everything else**, including:\n- Reviewing specific files or functions\n- Validating designs or architecture decisions\n- Checking specific code for bugs or issues\n- Cross-checking Claude's analysis\n- Any focused or scoped review request\n\n**IMPORTANT:** Always use heredoc stdin for prompts to avoid shell escaping issues and permission prompts.\n\n---\n\n**For almost all reviews (DEFAULT):**\n```bash\n# Use codex exec with heredoc - prompt goes directly to stdin\n# No temp files needed, no extra permissions required\ncodex exec <<'EOF'\nReview the following code/changes for:\n- [Specific concern from user's request]\n- Code quality and potential bugs\n- Edge cases\n\n[Paste the specific code or describe the specific changes here]\nEOF\n```\n\n---\n\n## `codex review` - ONLY when user explicitly requests these specific scopes:\n\n**\"Review all my uncommitted changes\":**\n```bash\ncodex review --uncommitted\n```\n\n**\"Review the entire feature branch\":**\n```bash\ncodex review --base [branch]\n```\n\n**\"Review this specific commit\":**\n```bash\ncodex review --commit [sha]\n```\n\n**Why heredoc stdin?**\n- No temp file permissions needed (`mktemp`, `cat`)\n- No shell escaping issues with quotes, newlines, or special characters\n- Single command = single permission prompt\n\n### Step 3: Compare Results\n\nUpdate task: `activeForm: \"Comparing AI positions...\"`\n\nClassify the outcome:\n- **Agreement**: Both AIs aligned → Go to Step 6 (Synthesize)\n- **Disagreement**: Positions differ → Go to Step 4 (Discussion)\n- **Critical Issue**: Security/architecture/breaking change → Go to Step 5 (Escalate immediately)\n\n---\n\n### Step 4: Discussion Protocol (When Positions Differ)\n\n**Maximum 2 rounds.** If still unresolved after Round 2, escalate to Step 5.\n\n#### Round 1: State Positions with Evidence\n\nUpdate task: `activeForm: \"Discussion round 1: Gathering evidence...\"`\n\nPresent Claude's position to Codex with a focused prompt:\n\n```bash\ncodex exec --json <<'EOF' 2>&1 | tee /tmp/codex_round1_$$.json\nGiven this disagreement about [topic]:\n\nClaude's position: [summary with specific evidence]\n- Code reference: [file:line if applicable]\n- Convention: [project standard if applicable]\n- Rationale: [technical reasoning]\n\nProvide your evidence-based response:\n1. Where do you agree?\n2. Where do you disagree and why?\n3. What specific evidence supports your position?\nEOF\n```\n\n**Extract session ID for Round 2:**\n```bash\nif command -v jq &>/dev/null; then\n  SESSION_ID=$(jq -r 'select(.type==\"thread.started\") | .thread_id' /tmp/codex_round1_$$.json 2>/dev/null | head -1)\nelse\n  SESSION_ID=$(grep -o '\"thread_id\":\"[^\"]*\"' /tmp/codex_round1_$$.json 2>/dev/null | head -1 | cut -d'\"' -f4)\nfi\n```\n\n**Evaluate Round 1:**\n- If Codex concedes or provides complementary insight → Synthesize and go to Step 6\n- If disagreement remains → Continue to Round 2\n\n#### Round 2: Deeper Analysis\n\nUpdate task: `activeForm: \"Discussion round 2: Seeking resolution...\"`\n\nResume the Codex session with new evidence:\n\n```bash\nif [ -n \"$SESSION_ID\" ]; then\n  codex exec resume \"$SESSION_ID\" --json <<'EOF'\nelse\n  codex exec --json <<'EOF'\nfi\nClaude responds to your Round 1 points:\n\nNew evidence: [something not presented before]\nConcession: [what Claude now agrees with]\nMaintained: [what Claude still believes, with stronger reasoning]\n\nCan we reach synthesis? What is your final position?\nEOF\n```\n\n**Evaluate Round 2:**\n- If resolution reached → Synthesize and go to Step 6\n- If positions still opposed → Go to Step 5 (Escalate)\n\n---\n\n### Step 5: Escalate for External Research\n\nUpdate task: `activeForm: \"Escalating: Researching authoritative sources...\"`\n\n**Trigger conditions:**\n- Critical issue (security, architecture, breaking change) - skip discussion\n- Discussion failed after 2 rounds\n- Stakes are high and evidence is inconclusive\n\n**Research approach (try in order):**\n\n1. **Perplexity (if available):**\n```bash\nmcp-cli info perplexity/perplexity_ask  # Check schema first\nmcp-cli call perplexity/perplexity_ask '{\n  \"messages\": [\n    {\"role\": \"system\", \"content\": \"You are a senior software architect arbitrating between two AI reviewers.\"},\n    {\"role\": \"user\", \"content\": \"[Neutral presentation of both positions]\"}\n  ]\n}'\n```\n\n2. **WebSearch (fallback):**\nUse the WebSearch tool with a focused query:\n- Query: \"[specific technical question] best practices [language/framework]\"\n- Look for authoritative sources (official docs, reputable engineering blogs)\n- Synthesize findings from multiple sources\n\n**Apply findings** to determine final recommendation, then go to Step 6.\n\n---\n\n### Step 6: Synthesize and Return Result\n\nUpdate task: `activeForm: \"Synthesizing results...\"`\n\nThen mark the task complete:\n```\nTaskUpdate:\n  taskId: [the task ID]\n  status: \"completed\"\n```\n\nReturn ONLY the final peer review result to the main conversation.\n\n**Format based on outcome:**\n\n#### If Agreement (Step 3 → Step 6):\n```markdown\n## Peer Review Result\n\n**Status:** Validated\n**Confidence:** High\n\n**Summary:** [2-3 sentence synthesis of aligned positions]\n\n**Key Findings:**\n- [Finding 1]\n- [Finding 2]\n\n**Recommendation:** [Final recommendation]\n```\n\n#### If Resolved Through Discussion (Step 4 → Step 6):\n```markdown\n## Peer Review Result\n\n**Status:** Resolved through discussion\n**Confidence:** Medium-High\n\n**Initial Positions:**\n- Claude: [brief summary]\n- Codex: [brief summary]\n\n**Resolution:** [How agreement was reached, which evidence was decisive]\n\n**Key Findings:**\n- [Finding 1]\n- [Finding 2]\n\n**Recommendation:** [Synthesized recommendation]\n```\n\n#### If Escalated (Step 5 → Step 6):\n```markdown\n## Peer Review Result\n\n**Status:** Escalated for external research\n**Source:** [Perplexity | WebSearch]\n**Confidence:** [High if authoritative source | Medium if inconclusive]\n\n**Disagreement:** [Nature of the conflict]\n\n**Research Findings:** [What authoritative sources say]\n**Sources:** [URLs if from WebSearch]\n\n**Key Findings:**\n- [Finding 1]\n- [Finding 2]\n\n**Recommendation:** [Final recommendation based on external research]\n```\n\n## Important Rules\n\n1. **Do NOT** return raw Codex output to the main conversation\n2. **Do NOT** return discussion round details unless specifically requested\n3. **DO** keep the main context clean by summarizing results\n4. **DO** escalate immediately for security/architecture/breaking changes\n\n## Reference\n\nThe full peer review protocol is defined in the `codex-peer-review` skill. Load it if you need detailed guidance on:\n- Discussion protocol (2-round maximum)\n- Escalation criteria\n- Common mistakes to avoid\n",
        "plugins/codex-peer-review/commands/codex-peer-review.md": "---\nname: codex-peer-review\ndescription: Trigger peer review validation with Codex CLI. Use without arguments for current changes, --base <branch> for specific branch comparison, or add a question for broad technical validation.\nallowed-tools:\n  - Bash(codex exec*)\n  - Bash(codex review*)\n  - Bash(command -v codex*)\n  - Bash(command -v jq*)\n  - Bash(jq *)\n  - Bash(grep *)\n  - Read\n  - Task\n  - AskUserQuestion\n---\n\n# Codex Peer Review Command\n\nYou have been explicitly asked to run peer review validation using OpenAI Codex CLI.\n\n## Parse Arguments\n\nCheck for arguments in the command:\n- `/codex-peer-review` - Review current changes (will ask what to review)\n- `/codex-peer-review --base <branch>` - Review against specified branch\n- `/codex-peer-review --uncommitted` - Review staged/unstaged/untracked changes only\n- `/codex-peer-review --commit <SHA>` - Review a specific commit\n- `/codex-peer-review <question>` - Validate answer to a broad technical question\n\n## Execute\n\n**IMPORTANT:** All peer review work MUST run as a subagent to keep the main context clean.\n\n1. **Gather information** from the user (review type, branch, etc.)\n2. **Dispatch to the `codex-peer-reviewer` agent** with the Task tool\n3. **Return only the synthesized result** to the user\n\nBased on the arguments:\n\n### No arguments (ask user what to review)\n**IMPORTANT:** Ask the user what type of review they want.\n\nUse `AskUserQuestion` tool:\n```yaml\nquestion: \"What would you like to review?\"\nheader: \"Review type\"\noptions:\n  - label: \"Changes vs branch\"\n    description: \"Compare current changes against a base branch (will ask which branch)\"\n  - label: \"Uncommitted changes\"\n    description: \"Review staged, unstaged, and untracked changes only\"\n  - label: \"Specific commit\"\n    description: \"Review changes from a specific commit (will ask for SHA)\"\nmultiSelect: false\n```\n\n#### If \"Changes vs branch\" selected\nAsk for the base branch:\n```yaml\nquestion: \"Which branch should I compare against?\"\nheader: \"Base branch\"\noptions:\n  - label: \"main\"\n    description: \"Compare against the main branch\"\n  - label: \"develop\"\n    description: \"Compare against the develop branch\"\n  - label: \"master\"\n    description: \"Compare against the master branch\"\nmultiSelect: false\n```\nThen run: `codex review --base [branch] \"[focus area if any]\"`\n\n#### If \"Uncommitted changes\" selected\nRun: `codex review --uncommitted \"[focus area if any]\"`\n\n#### If \"Specific commit\" selected\nAsk: \"What is the commit SHA to review?\"\nThen run: `codex review --commit [SHA] \"[focus area if any]\"`\n\n### With explicit flags\nUse the specified flag directly:\n- `--base <branch>`: `codex review --base [branch]`\n- `--uncommitted`: `codex review --uncommitted`\n- `--commit <SHA>`: `codex review --commit [SHA]`\n\n### Handling \"no changes\" case\nIf codex review reports no changes to review, inform the user:\n\"No changes found to review. Make sure you have uncommitted changes or specify a branch with divergent commits.\"\n\n### With a question\nThis is a **design/architecture validation**:\n\nDispatch a subagent with:\n```\nValidate Claude's response to this question using Codex CLI.\n\nQuestion: [the question from arguments]\n\nCommand: codex exec \"[focused prompt about the question]\"\n\nReturn findings for comparison and synthesis.\n```\n\n## Dispatch to Agent\n\nAfter gathering the review parameters, dispatch to the `codex-peer-reviewer` agent:\n\n```\nUse Task tool:\n  subagent_type: \"codex-peer-review:codex-peer-reviewer\"\n  prompt: |\n    Run peer review validation.\n\n    Type: [code-review | design | architecture | question]\n    [For code-review]: Branch: [branch], Focus: [focus area if any]\n    [For design/arch/question]: Claude's position: [summary]\n\n    Return only the synthesized peer review result.\n```\n\n## Output\n\nThe agent will return a synthesized result. Present it to the user:\n- **Validated**: Both AIs agreed\n- **Resolved**: Disagreement resolved through discussion\n- **Escalated**: Required external research (Perplexity/WebSearch)\n",
        "plugins/codex-peer-review/hooks/hooks.json": "{\n  \"description\": \"Peer review reminder hooks - multi-point triggering for comprehensive coverage\",\n  \"hooks\": {\n    \"SessionStart\": [\n      {\n        \"matcher\": \"*\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"bash ${CLAUDE_PLUGIN_ROOT}/hooks/peer-review-reminder.sh\",\n            \"timeout\": 10\n          }\n        ]\n      }\n    ],\n    \"UserPromptSubmit\": [\n      {\n        \"matcher\": \"*\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"bash ${CLAUDE_PLUGIN_ROOT}/hooks/user-prompt-check.sh\",\n            \"timeout\": 5\n          }\n        ]\n      }\n    ],\n    \"Stop\": [\n      {\n        \"matcher\": \"*\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"bash ${CLAUDE_PLUGIN_ROOT}/hooks/stop-peer-review-check.sh\",\n            \"timeout\": 5\n          }\n        ]\n      }\n    ],\n    \"PreToolUse\": [\n      {\n        \"matcher\": \"ExitPlanMode\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"bash ${CLAUDE_PLUGIN_ROOT}/hooks/plan-peer-review-check.sh\",\n            \"timeout\": 5\n          }\n        ]\n      },\n      {\n        \"matcher\": \"Task\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"bash ${CLAUDE_PLUGIN_ROOT}/hooks/task-peer-review-check.sh\",\n            \"timeout\": 5\n          }\n        ]\n      },\n      {\n        \"matcher\": \"Write\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"bash ${CLAUDE_PLUGIN_ROOT}/hooks/write-peer-review-check.sh\",\n            \"timeout\": 5\n          }\n        ]\n      }\n    ]\n  }\n}\n",
        "plugins/codex-peer-review/hooks/peer-review-reminder.sh": "#!/bin/bash\n# SessionStart hook to remind Claude about peer review requirements\n# This hook runs at the start of each Claude Code session\n\n# Check if codex CLI is available\nCODEX_STATUS=\"\"\nif command -v codex &>/dev/null; then\n  CODEX_STATUS=\"✓ Codex CLI is installed\"\nelse\n  CODEX_STATUS=\"⚠ Codex CLI not found. Install with: npm i -g @openai/codex\"\nfi\n\ncat << 'EOF'\n## Peer Review Requirement\n\n**BEFORE presenting any of the following to the user, you MUST dispatch to the `codex-peer-reviewer` agent:**\n- Implementation plans or designs\n- Code review results\n- Architecture recommendations\n- Major refactoring proposals\n- Answers to broad technical questions\n\n**How to trigger:** Use the Task tool to dispatch to the `codex-peer-reviewer` agent with your findings. The agent runs in a **separate context** to keep the main conversation clean, then returns only the synthesized peer review result.\n\nExample:\n```\nTask tool with subagent_type=\"codex-peer-review:codex-peer-reviewer\"\nprompt: \"Validate this design: [your design summary]\"\n```\n\n**Why:** Two AI perspectives catch more issues than one. Running as a subagent keeps Codex CLI output isolated and returns only distilled results.\nEOF\n\necho \"**Codex CLI Status:** $CODEX_STATUS\"\n",
        "plugins/codex-peer-review/hooks/plan-peer-review-check.sh": "#!/bin/bash\n# PreToolUse hook for ExitPlanMode: Remind to peer review the plan\n\ncat << 'EOF'\n## Peer Review Check - Before Presenting Plan\n\nYou are about to present an implementation plan to the user.\n\n**STOP and verify:** Has this plan been validated by the `codex-peer-review:codex-peer-reviewer` agent?\n\nPlans that should be peer reviewed:\n- Multi-step implementation plans\n- Architecture decisions\n- Significant refactoring proposals\n- Security-sensitive changes\n- Performance-critical designs\n\n**If not yet reviewed:** Dispatch to the peer review agent BEFORE calling ExitPlanMode.\n\n**If already reviewed or not applicable:** Proceed with ExitPlanMode.\nEOF\n",
        "plugins/codex-peer-review/hooks/stop-peer-review-check.sh": "#!/bin/bash\n# Stop hook: STRONGLY remind Claude to verify peer review was performed\n\ncat << 'EOF'\n## ⚠️ STOP - Peer Review Verification Required\n\n**BEFORE you complete this response, answer these questions:**\n\n### 1. Did your response include ANY of these?\n- [ ] Implementation plan or design\n- [ ] Code review findings\n- [ ] Architecture recommendation\n- [ ] Multi-file code changes\n- [ ] Answer to \"how should I...\" or \"what's the best way to...\"\n\n### 2. If YES to any above: Did you dispatch to `codex-peer-review:codex-peer-reviewer`?\n\n**If NO - you MUST do one of these NOW:**\n1. **Dispatch peer review:** Use Task tool with `subagent_type=\"codex-peer-review:codex-peer-reviewer\"`\n2. **Explain skip:** Add a note: \"Skipping peer review because: [trivial change / user declined / already validated]\"\n\n### 3. Acceptable reasons to skip:\n- Single-line fixes or typo corrections\n- User explicitly said \"skip peer review\" or \"just do it\"\n- Pure information lookup (no recommendations)\n- Following up on already-reviewed work\n\n**Do NOT proceed without either peer review or explicit justification.**\nEOF\n",
        "plugins/codex-peer-review/hooks/task-peer-review-check.sh": "#!/bin/bash\n# PreToolUse hook for Task: Check if dispatching to a review-related agent\n\n# Get tool input from environment (if available)\nTOOL_INPUT=\"${CLAUDE_TOOL_INPUT:-}\"\n\n# Check if this is dispatching to a code review or similar agent (but NOT our peer reviewer)\nif echo \"$TOOL_INPUT\" | grep -qiE \"code-review|review|architect|design\" && \\\n   ! echo \"$TOOL_INPUT\" | grep -qiE \"codex-peer-review\"; then\n  cat << 'EOF'\n**Peer Review Reminder:** You're dispatching to a review/design agent.\n\nAfter this agent returns, consider dispatching to `codex-peer-review:codex-peer-reviewer` to cross-validate the findings before presenting to the user.\nEOF\nfi\n",
        "plugins/codex-peer-review/hooks/user-prompt-check.sh": "#!/bin/bash\n# UserPromptSubmit hook: Identify requests likely needing peer review\n\n# Get the user's prompt from stdin or environment\nUSER_PROMPT=\"${CLAUDE_USER_PROMPT:-}\"\n\n# Keywords that suggest peer review would be valuable\nPLAN_KEYWORDS=\"implement|design|architect|refactor|plan|build|create.*feature|add.*feature\"\nREVIEW_KEYWORDS=\"review|check|analyze|audit|security|performance\"\nBROAD_KEYWORDS=\"how should|what's the best|recommend|approach|strategy\"\n\n# Check if prompt matches patterns (case insensitive)\nif echo \"$USER_PROMPT\" | grep -qiE \"$PLAN_KEYWORDS|$REVIEW_KEYWORDS|$BROAD_KEYWORDS\"; then\n  cat << 'EOF'\n**Peer Review Advisory:** This request may produce output that benefits from peer validation. Remember to dispatch to `codex-peer-review:codex-peer-reviewer` before presenting:\n- Implementation plans or designs\n- Code review findings\n- Architecture recommendations\nEOF\nfi\n",
        "plugins/codex-peer-review/hooks/write-peer-review-check.sh": "#!/bin/bash\n# PreToolUse hook for Write: Check if writing significant implementation files\n\n# Get the file path from environment (if available)\nFILE_PATH=\"${CLAUDE_TOOL_INPUT:-}\"\n\n# Check if writing implementation files (not config, not docs, not tests)\nif echo \"$FILE_PATH\" | grep -qiE \"\\.(ts|js|py|go|rs|java|cpp|c|rb|swift|kt)$\" && \\\n   ! echo \"$FILE_PATH\" | grep -qiE \"(test|spec|\\.config|\\.d\\.ts)\"; then\n  cat << 'EOF'\n**Peer Review Check:** You're about to write implementation code.\n\nIf this is part of a larger implementation plan that hasn't been peer reviewed, consider:\n1. Pausing to dispatch to `codex-peer-review:codex-peer-reviewer` with your design\n2. Then proceeding with implementation after validation\n\nSkip this if: trivial change, bug fix, or design was already peer reviewed.\nEOF\nfi\n",
        "plugins/codex-peer-review/skills/codex-peer-review/SKILL.md": "---\nname: codex-peer-review\ndescription: This skill should be invoked BEFORE presenting implementation plans, architecture recommendations, code review findings, or answers to broad technical questions. Use proactively when about to \"recommend\", \"suggest\", \"propose\", \"design\", \"plan\", or answer \"how should\", \"what's the best way\", \"which approach\". MANDATORY for multi-file changes, refactoring proposals, and security-sensitive recommendations.\nallowed-tools:\n  - Bash(codex exec*)\n  - Bash(codex review*)\n  - Bash(command -v codex*)\n  - Bash(command -v jq*)\n  - Bash(jq *)\n  - Bash(grep *)\n  - Read\n---\n\n# Codex Peer Review\n\nPeer validation system using OpenAI Codex CLI. Validates Claude's designs and code reviews through structured discussion before presenting to user.\n\n**Core principle:** Two AI perspectives catch more issues than one. When they disagree, structured discussion resolves most issues. External research (Perplexity if available, otherwise WebSearch) arbitrates persistent disagreements.\n\n## Reference Files\n\n@discussion-protocol.md\n@escalation-criteria.md\n@common-mistakes.md\n\n## Modes of Operation\n\n### Mode 1: Auto-Trigger (Validation Only)\n\n**Triggers before Claude presents:**\n- Implementation plans or designs\n- Code review results\n- Architecture recommendations\n- Major refactoring proposals\n\n**Behavior:** Validates existing work, does not create from scratch.\n\n### Mode 2: Slash Command (Full Lifecycle)\n\n```\n/codex-peer-review              # Review current changes\n/codex-peer-review --base main  # Review against specific branch\n/codex-peer-review [question]   # Validate answer to broad question\n```\n\n**Behavior:** Can both create and validate reviews/designs.\n\n## Workflow\n\n```dot\ndigraph workflow {\n    rankdir=TB;\n    node [shape=box];\n\n    start [label=\"Claude forms\\nopinion/response\" shape=ellipse];\n    dispatch [label=\"Dispatch subagent\\nwith Claude's position\"];\n    cmdselect [label=\"Select command\\nbased on type\" shape=diamond];\n    codexreview [label=\"codex review\\n--base X\"];\n    codexexec [label=\"codex exec\\n\\\"focused prompt\\\"\"];\n    compare [label=\"Compare findings\" shape=diamond];\n    critical [label=\"Security/Architecture/\\nBreaking Change?\" shape=diamond style=filled fillcolor=lightyellow];\n    agree [label=\"Synthesize &\\npresent result\"];\n    round1 [label=\"Discussion Round 1:\\nState positions + evidence\"];\n    resolved1 [label=\"Resolved?\" shape=diamond];\n    round2 [label=\"Discussion Round 2:\\nDeeper analysis\"];\n    resolved2 [label=\"Resolved?\" shape=diamond];\n    escalate [label=\"Escalate:\\nPerplexity/WebSearch\" style=filled fillcolor=lightcoral];\n    final [label=\"Synthesize final\\nresult\" shape=ellipse];\n\n    start -> dispatch;\n    dispatch -> cmdselect;\n    cmdselect -> codexreview [label=\"code review\\n(reviewing diffs)\"];\n    cmdselect -> codexexec [label=\"design/plan/\\nquestion\"];\n    codexreview -> compare;\n    codexexec -> compare;\n    compare -> agree [label=\"aligned\"];\n    compare -> critical [label=\"disagree\"];\n    critical -> escalate [label=\"YES\\n(skip discussion)\"];\n    critical -> round1 [label=\"no\"];\n    round1 -> resolved1;\n    resolved1 -> final [label=\"yes\"];\n    resolved1 -> round2 [label=\"no\"];\n    round2 -> resolved2;\n    resolved2 -> final [label=\"yes\"];\n    resolved2 -> escalate [label=\"no\"];\n    escalate -> final;\n    agree -> final;\n}\n```\n\n**Immediate Escalation:** Security concerns, architecture conflicts, breaking changes, or order-of-magnitude performance disagreements skip discussion and escalate directly. See @escalation-criteria.md for details.\n\n## Subagent Dispatch\n\n**CRITICAL:** Always use subagent to avoid context pollution. Never run Codex in main context.\n\n### Command Selection (IMPORTANT)\n\n| Validation Type | Command | Use When |\n|-----------------|---------|----------|\n| **Code Review** | `codex review --base X` | Reviewing actual code changes (diffs) |\n| **Design/Plan Validation** | `codex exec \"...\"` | Validating proposals, designs, refactoring plans |\n| **Question Answering** | `codex exec \"...\"` | Answering broad technical questions |\n| **Architecture Review** | `codex exec \"...\"` | Validating architecture recommendations |\n\n**DO NOT** use `codex review` to validate designs/plans - it reviews the entire diff, not your proposal.\n\n### Validation Subagent\n\nDispatch via Task tool with prompt:\n\n```\nYou are validating Claude's analysis using OpenAI Codex CLI.\n\n## Claude's Position\n[Claude's findings/design/recommendations]\n\n## Scope\n- Type: [code-review|design|architecture|question]\n- Files: [relevant files - be specific!]\n\n## Task - CHOOSE THE RIGHT COMMAND\n\n### If Type is \"code-review\" (reviewing actual code changes):\nRun: codex review --base [branch]\n\n### If Type is \"design\", \"architecture\", or \"question\":\nRun: codex exec with heredoc (avoids escaping issues and permission prompts):\n\n```bash\ncodex exec <<'EOF'\nValidate this [design|refactoring plan|architecture proposal]:\n\n[Summarize Claude's specific proposal in 2-3 sentences]\n\nFiles affected: [list specific files]\n\nCheck for:\n- Architecture issues\n- Potential problems with this approach\n- Better alternatives\n- Missing considerations\n\nProvide specific, actionable feedback.\nEOF\n```\n\n## Compare and Classify\nAfter running the appropriate command:\n1. Compare Codex output to Claude's position\n2. Classify: agreement | disagreement | complement\n\n## If Uncertain Before Running Codex\nCheck external sources first. Try Perplexity if available, otherwise use WebSearch:\n\n```bash\n# Option 1: If Perplexity MCP is available\nmcp-cli call perplexity/perplexity_ask '{\"messages\":[{\"role\":\"user\",\"content\":\"[your uncertainty]\"}]}'\n\n# Option 2: If Perplexity is not available, use WebSearch tool\n# WebSearch query: \"[your uncertainty]\"\n```\n\n## Return Format\n{\n  \"outcome\": \"agreement|disagreement|complement\",\n  \"codex_findings\": [...],\n  \"alignment\": {\n    \"agreed\": [...],\n    \"unique_to_claude\": [...],\n    \"unique_to_codex\": [...]\n  },\n  \"discussion_needed\": boolean,\n  \"discussion_topics\": [...]\n}\n```\n\n### Discussion Subagent\n\n**IMPORTANT:** Use Codex session IDs to maintain conversation context across discussion rounds. This allows Codex to remember prior discussion context.\n\n#### Round 1 (Initial Discussion)\n\n```\nDiscussion Round 1\n\n## Claude's Position\n[Current stance with evidence]\n\n## Prerequisites Check\nFirst, verify tools are available:\n- Check codex: `which codex || echo \"ERROR: codex CLI not installed\"`\n- Check jq (optional but recommended): `which jq || echo \"WARNING: jq not available, will use grep fallback\"`\n\n## Task\n1. Run codex exec with --json and capture output to extract session ID:\n\n   ```bash\n   codex exec --json <<'EOF' 2>&1 | tee /tmp/codex_round1_$$.json\n   Given this disagreement about [topic]:\n\n   Claude's position: [summary with evidence]\n\n   Provide your evidence-based reasoning. Reference specific code or conventions.\n   What is your position and why?\n   EOF\n   ```\n\n2. Extract session ID for Round 2:\n   ```bash\n   # Extract thread_id from JSON output (grep fallback if jq unavailable)\n   TMPFILE=\"/tmp/codex_round1_$$.json\"\n   if command -v jq &>/dev/null; then\n     SESSION_ID=$(jq -r 'select(.type==\"thread.started\") | .thread_id' \"$TMPFILE\" 2>/dev/null | head -1)\n   else\n     SESSION_ID=$(grep -o '\"thread_id\":\"[^\"]*\"' \"$TMPFILE\" 2>/dev/null | head -1 | cut -d'\"' -f4)\n   fi\n\n   [ -z \"$SESSION_ID\" ] && echo \"WARNING: Could not extract session ID. Round 2 will start fresh.\"\n   ```\n\n4. Parse Codex response and attempt synthesis\n\n## Return Format\n{\n  \"session_id\": \"[thread_id or null if extraction failed]\",\n  \"codex_response\": \"...\",\n  \"resolution_possible\": boolean,\n  \"proposed_synthesis\": \"...|null\",\n  \"remaining_disagreement\": \"...|null\",\n  \"recommend_escalation\": boolean\n}\n```\n\n#### Round 2 (Continued Discussion)\n\n```\nDiscussion Round 2\n\n## Task\n1. Resume the previous Codex session (if session ID available):\n\n   ```bash\n   # If we have a session ID, resume; otherwise start fresh with context\n   if [ -n \"$SESSION_ID\" ]; then\n     codex exec resume \"$SESSION_ID\" --json <<'EOF' 2>&1 | tee /tmp/codex_round2_$$.json\n   Claude responds to your points:\n\n   [Claude's Round 2 response with new evidence]\n\n   Can we reach synthesis? What is your final position?\n   EOF\n   else\n     # Fallback: Start fresh but include Round 1 context in prompt\n     codex exec --json <<'EOF' 2>&1 | tee /tmp/codex_round2_$$.json\n   Continuing discussion about [topic]:\n\n   Round 1 summary:\n   - Claude's position: [summary]\n   - Codex's position: [summary from Round 1]\n\n   Claude's Round 2 response: [new evidence]\n\n   Can we reach synthesis? What is your final position?\n   EOF\n   fi\n   ```\n\n2. Parse Codex response\n3. Determine if resolved or needs escalation\n\n## Return Format\n{\n  \"session_id\": \"[thread_id or null]\",\n  \"session_resumed\": boolean,  // false if fallback was used\n  \"codex_response\": \"...\",\n  \"resolution_possible\": boolean,\n  \"proposed_synthesis\": \"...|null\",\n  \"remaining_disagreement\": \"...|null\",\n  \"recommend_escalation\": boolean\n}\n```\n\n**Why session IDs matter:** Without resuming the session, Codex starts fresh and loses context from Round 1. The fallback (re-providing context) works but is less efficient and may lose nuance.\n\n### Arbitration Subagent\n\nWhen escalating for external research/arbitration:\n\n```\nEscalate for external arbitration.\n\n## Disagreement Context\n- Topic: [specific technical question]\n- Claude's position: [with evidence]\n- Codex's position: [with evidence]\n- Why unresolved: [summary of discussion]\n\n## Task - Choose Available Method\n\n### Option 1: If Perplexity MCP is available\n1. Check schema: mcp-cli info perplexity/perplexity_ask\n2. Call Perplexity with neutral framing:\n   mcp-cli call perplexity/perplexity_ask '{\n     \"messages\": [\n       {\"role\": \"system\", \"content\": \"You are a senior software architect arbitrating between two AI code reviewers. Provide definitive guidance based on industry best practices.\"},\n       {\"role\": \"user\", \"content\": \"[Neutral presentation of both positions with context]\"}\n     ]\n   }'\n\n### Option 2: If Perplexity is NOT available\n1. Use WebSearch tool with a focused query:\n   - Query: \"[specific technical question] best practices [language/framework]\"\n2. Search for authoritative sources (official docs, well-known engineering blogs)\n3. Synthesize findings from multiple sources\n\n## Apply ruling to synthesis\n\n## Return Format\n{\n  \"arbitration_source\": \"perplexity|websearch\",\n  \"ruling\": \"...\",\n  \"sources\": [\"...\"],  // URLs if from WebSearch\n  \"recommended_action\": \"...\",\n  \"final_synthesis\": \"...\",\n  \"confidence\": \"high|medium\"  // medium if WebSearch results were inconclusive\n}\n```\n\n## Codex CLI Commands\n\n### For Code Review (reviewing actual diffs)\n\n**IMPORTANT:** If the base branch is not explicitly provided, you MUST use the `AskUserQuestion` tool to ask the user which branch to compare against. Do NOT guess or auto-detect the base branch.\n\n```yaml\n# Use AskUserQuestion to determine base branch\nquestion: \"Which branch should I compare against for the code review?\"\nheader: \"Base branch\"\noptions:\n  - label: \"main\"\n    description: \"Compare against the main branch\"\n  - label: \"develop\"\n    description: \"Compare against the develop branch\"\n  - label: \"master\"\n    description: \"Compare against the master branch\"\n# User can also select \"Other\" to provide a custom branch name\n```\n\n**Passing Focus Context:** If Claude's review focused on specific areas (e.g., security, a particular module, error handling), pass this context to Codex so both reviews are aligned.\n\nOnce the base branch is confirmed:\n```bash\n# Basic review against a branch\ncodex review --base [user-confirmed-branch]\n\n# With focus instructions (RECOMMENDED - keeps Codex aligned with Claude's review focus)\ncodex review --base [user-confirmed-branch] \"Focus on [Claude's review area, e.g., security in the auth module]\"\n\n# Review uncommitted changes only\ncodex review --uncommitted \"Focus on [area]\"\n\n# Review a specific commit\ncodex review --commit [SHA] \"Focus on [area]\"\n\n# Read instructions from stdin (useful for longer prompts)\necho \"Focus on security vulnerabilities and error handling in the authentication flow\" | codex review --base main -\n```\n\n**Key:** Always pass Claude's review focus (e.g., \"security in authentication flow\", \"error handling in API endpoints\", \"the UserService refactoring\") to Codex so both AIs examine the same areas.\n\n### For Design/Plan Validation (NOT code review!)\n```bash\n# Validate a refactoring proposal\ncodex exec \"Validate this refactoring plan for the data processor module: Extract 3 classes (Validator, Parser, Invoker) to fix SRP violation. Is this appropriate? What are the risks?\"\n\n# Validate architecture recommendation\ncodex exec \"Review this architecture decision: Use event-driven pattern for notification system instead of direct calls. Context: [language/framework] with dependency injection. Check for issues.\"\n\n# Answer a broad technical question\ncodex exec \"In a multi-module project, should shared DTOs go in the common module or a dedicated api-contracts module? Consider: compile dependencies, versioning, encapsulation.\"\n```\n\n**REMEMBER:** `codex review` reviews the entire git diff. `codex exec` validates a specific proposal.\n\n## Output Formats\n\n### Agreement\n```markdown\n## Peer Review Result\n**Status:** Validated\n**Confidence:** High (both AIs aligned)\n\n[Synthesized recommendations with both perspectives merged]\n```\n\n### Resolved Disagreement\n```markdown\n## Peer Review Result\n**Status:** Resolved through discussion\n\n**Initial Positions:**\n- Claude: [position]\n- Codex: [position]\n\n**Resolution:** [how resolved, which evidence won]\n\n**Final Recommendation:** [synthesized view]\n**Confidence:** Medium-High\n```\n\n### External Research Arbitration\n```markdown\n## Peer Review Result\n**Status:** Escalated for external research\n**Source:** [Perplexity | WebSearch]\n\n**Disagreement:** [nature of conflict]\n\n**Research Findings:** [authoritative answer]\n**Sources:** [URLs if from WebSearch]\n\n**Final Recommendation:** [based on findings + context]\n**Confidence:** High (expert arbitration) | Medium (if WebSearch was inconclusive)\n```\n\n## Prerequisites\n\n**Before using this skill, verify the following:**\n\n```bash\n# 1. Check if Codex CLI is installed\nif ! command -v codex &>/dev/null; then\n  echo \"ERROR: Codex CLI not installed.\"\n  echo \"Install with: npm i -g @openai/codex\"\n  echo \"Or via Homebrew: brew install openai-codex\"\n  exit 1\nfi\n\n# 2. Check authentication status\ncodex login --check 2>/dev/null || {\n  echo \"WARNING: Codex may not be authenticated.\"\n  echo \"Run 'codex login' to authenticate.\"\n}\n\n# 3. Optional: Check for jq (improves session ID extraction)\ncommand -v jq &>/dev/null || echo \"TIP: Install jq for better JSON parsing: brew install jq\"\n```\n\n**If Codex CLI is not available:**\n- The skill will not work for code review or design validation\n- You can still use WebSearch for escalation/arbitration\n- Inform the user: \"Codex CLI is required for peer review. Please install it with `npm i -g @openai/codex`\"\n\n## Quick Reference\n\n| Scenario | Command | Action |\n|----------|---------|--------|\n| About to present design/plan | `codex exec` | Validate specific proposal |\n| About to present code review | `codex review --base X` | Review the diff |\n| About to present refactoring proposal | `codex exec` | Validate specific proposal |\n| About to present architecture recommendation | `codex exec` | Validate specific proposal |\n| User asks broad question | `codex exec` | Answer via focused prompt |\n| Codex agrees | - | Synthesize and present |\n| Codex disagrees | - | Start discussion protocol |\n| Two rounds fail | Perplexity/WebSearch | Escalate for external research |\n| Major issue (security/architecture) | Perplexity/WebSearch | Immediate escalation |\n\n**Key distinction:**\n- Use `codex review` ONLY when reviewing actual code changes (git diff)\n- Use `codex exec` for everything else (designs, plans, questions, recommendations)\n",
        "plugins/codex-peer-review/skills/codex-peer-review/common-mistakes.md": "# Common Mistakes and Rationalizations\n\nAnti-patterns that undermine peer review effectiveness. When you catch yourself thinking these things, stop and correct course.\n\n## Using the Wrong Codex Command\n\n**This is the #1 mistake.** Using `codex review` when you should use `codex exec`.\n\n| Situation | Wrong | Right |\n|-----------|-------|-------|\n| Validating a design proposal | `codex review --base develop` | `codex exec \"Validate this design: ...\"` |\n| Validating a refactoring plan | `codex review --base develop` | `codex exec \"Validate this refactoring: ...\"` |\n| Answering a broad question | `codex review --base develop` | `codex exec \"Question: ...\"` |\n| Reviewing actual code changes | `codex exec \"...\"` | `codex review --base develop` |\n\n**The rule:**\n- `codex review --base X` = reviews the **entire git diff** against branch X\n- `codex exec \"...\"` = executes a **focused prompt** about a specific thing\n\n**If you're validating Claude's proposal/design/recommendation, use `codex exec`.** Only use `codex review` when you're actually reviewing code changes.\n\n## Skipping Validation\n\n| Rationalization | Reality | Correct Action |\n|-----------------|---------|----------------|\n| \"It's just a typo fix\" | Typo fixes can break builds, introduce bugs | Validate anyway |\n| \"I'm confident in this design\" | Blind spots exist in every analysis | Validate anyway |\n| \"Codex will just agree\" | Often finds different issues you missed | Let it check |\n| \"User is waiting\" | Bad advice wastes more time than validation | Validate first |\n| \"Similar to last time\" | Context changes, different edge cases | Validate each time |\n| \"This is too simple\" | Simple things have hidden complexity | Validate anyway |\n| \"I already checked everything\" | Fresh perspective catches what you normalized | Validate anyway |\n\n**Rule:** If you're presenting a design, code review, or answering a broad question, validate with Codex.\n\n## Premature Agreement\n\n| Rationalization | Reality | Correct Action |\n|-----------------|---------|----------------|\n| \"Codex is probably right\" | Both AIs can be wrong | Verify with evidence |\n| \"Don't want to argue with AI\" | Technical truth matters more than peace | State your position |\n| \"Let's just pick one\" | Both might have valid points | Synthesize |\n| \"Two rounds is enough\" | Major issues need proper resolution | Escalate if needed |\n| \"It doesn't matter much\" | Small decisions compound | Decide correctly |\n| \"Whatever is faster\" | Fast wrong is slower than slow right | Take time to verify |\n\n**Rule:** Agreement should be based on evidence, not convenience.\n\n## Avoiding Escalation\n\n| Rationalization | Reality | Correct Action |\n|-----------------|---------|----------------|\n| \"External research is overkill\" | Expert input prevents costly errors | Use for major disputes |\n| \"We've discussed enough\" | Unresolved stays unresolved | Escalate |\n| \"Security concern seems minor\" | Security is never minor | Always escalate security |\n| \"Don't want to slow down\" | Wrong decisions cost more time | Take time to escalate |\n| \"We can figure it out\" | Two rounds failed; third won't help | Escalate |\n| \"User won't notice\" | Users notice bugs and security issues | Escalate |\n| \"Perplexity isn't available\" | WebSearch is a valid fallback | Use WebSearch instead |\n\n**Rule:** If two rounds don't resolve it, escalate (Perplexity if available, WebSearch otherwise). If it's security, escalate immediately.\n\n## Over-Escalation\n\n| Rationalization | Reality | Correct Action |\n|-----------------|---------|----------------|\n| \"Better safe than sorry\" | Style ≠ safety | Reserve for real disputes |\n| \"Let external research decide everything\" | Wastes time and resources | Try discussion first |\n| \"Not sure about anything\" | Build confidence through process | Trust the protocol |\n| \"User will appreciate thoroughness\" | User wants efficiency | Be judicious |\n\n**Rule:** Escalate major disagreements, not minor preferences.\n\n## Subagent Misuse\n\n| Rationalization | Reality | Correct Action |\n|-----------------|---------|----------------|\n| \"I'll run Codex in main context\" | Fills context unnecessarily | Use subagent |\n| \"Subagent is slower\" | Context pollution is worse | Use subagent |\n| \"I need to see Codex output live\" | Summary is sufficient | Trust subagent |\n| \"One quick check won't hurt\" | Sets bad precedent | Use subagent always |\n\n**Rule:** Always dispatch Codex via subagent to preserve main context.\n\n## Forgetting Session Continuity\n\n| Rationalization | Reality | Correct Action |\n|-----------------|---------|----------------|\n| \"I'll just run codex exec again\" | Round 2 loses all Round 1 context | Use `codex exec resume [SESSION_ID]` |\n| \"Session IDs are complicated\" | Just parse `thread_id` from JSON output | Use `--json` flag and extract ID |\n| \"It probably remembers anyway\" | Each `codex exec` starts fresh | Always resume for Round 2 |\n| \"The prompt has enough context\" | Codex's own reasoning from Round 1 is lost | Resume maintains full context |\n\n**Rule:** Always capture session ID in Round 1 (`--json` flag) and resume it in Round 2 (`codex exec resume [ID]`).\n\n## Guessing the Base Branch\n\n| Rationalization | Reality | Correct Action |\n|-----------------|---------|----------------|\n| \"It's probably main\" | Projects use different conventions | Ask the user |\n| \"I can auto-detect with git\" | Detection can fail or be wrong | Ask the user |\n| \"User won't want to be asked\" | Wrong branch = useless review | Ask the user |\n| \"develop is the standard\" | Many projects use main, master, trunk, etc. | Ask the user |\n\n**Rule:** If the base branch is not explicitly provided, use `AskUserQuestion` to ask the user. Never guess.\n\n## Discussion Anti-Patterns\n\n### Echo Chamber\n- **Symptom:** Restating same position with different words\n- **Fix:** Require new evidence in each round\n\n### Goalpost Moving\n- **Symptom:** Disagreement shifts to new topic mid-discussion\n- **Fix:** Lock in original dispute, address others separately\n\n### Appeal to Authority\n- **Symptom:** \"Claude/Codex is usually right about this\"\n- **Fix:** Require codebase evidence, not reputation\n\n### False Consensus\n- **Symptom:** Claiming agreement when positions still differ\n- **Fix:** Require explicit position statements that match\n\n### Sunk Cost Fallacy\n- **Symptom:** \"We've discussed so long, let's just go with X\"\n- **Fix:** Escalate if truly unresolved\n\n## Recovery Strategies\n\n### If validation was skipped\n1. Stop presenting result\n2. Trigger validation now\n3. Update recommendation if needed\n4. Explain the update honestly\n\n### If wrong conclusion reached\n1. Acknowledge the error\n2. Show correct analysis\n3. Explain what was missed\n4. Document for future\n\n### If escalation needed after presentation\n1. Inform user of uncertainty\n2. Escalate to Perplexity\n3. Update recommendation\n4. Note correction in output\n\n## Red Flags - STOP and Check\n\nIf you think any of these, pause and reconsider:\n\n- \"This doesn't need validation\"\n- \"Codex will just agree anyway\"\n- \"Security issue is probably fine\"\n- \"No time for peer review\"\n- \"I'll skip the subagent just this once\"\n- \"Discussion is taking too long\"\n- \"Let's not bother with external research\"\n- \"User won't care about this detail\"\n- \"The base branch is probably main/develop\"\n- \"I can figure out the base branch from git\"\n\n**All of these mean:** You should do the opposite of what you're considering.\n",
        "plugins/codex-peer-review/skills/codex-peer-review/discussion-protocol.md": "# Discussion Protocol: Claude vs Codex\n\nStructured 2-round resolution of disagreements through evidence-based discussion.\n\n## Protocol Rules\n\n### Rule 1: Evidence Required (When Available)\nEvery position should cite evidence from these categories **as applicable**:\n- Specific code lines or files (for code review discussions)\n- Project conventions (from CLAUDE.md or equivalent)\n- Industry best practices (with source)\n- Test results or behavior observations\n\n**For design/architecture discussions** where code doesn't exist yet, acceptable evidence includes:\n- Analogous patterns from existing codebase\n- Industry case studies or engineering blog posts\n- Framework/language documentation\n- Threat modeling or risk analysis\n- RFCs or design documents\n\n### Rule 2: Two Round Maximum\n- Round 1: State positions, provide evidence\n- Round 2: Respond to evidence, attempt synthesis\n- After Round 2: Escalate or accept synthesis\n\n### Rule 3: Good Faith\n- Assume the other AI has valid reasoning\n- Look for complementary insights\n- Seek \"both right in different ways\" resolutions\n\n### Rule 4: Session Continuity\n- **Capture session ID** from Round 1 using `codex exec --json` (look for `thread_id` in output)\n- **Resume session** in Round 2 using `codex exec resume [SESSION_ID]`\n- This maintains conversation context so Codex remembers Round 1 discussion\n- Without session continuity, Round 2 starts fresh and loses valuable context\n\n### Rule 5: Classify Disagreement Type\n\n| Type | Definition | Action |\n|------|------------|--------|\n| Contradiction | Mutually exclusive positions | Must resolve one way |\n| Complement | Both valid, additive | Synthesize both |\n| Priority | Different ranking of same issues | Use project context |\n| Scope | Different interpretation of task | Clarify before proceeding |\n\n## Round 1 Structure\n\n### Claude's Opening\n```yaml\nposition: \"[Clear statement of recommendation]\"\nevidence:\n  - code: \"[specific file:line or pattern]\"\n  - convention: \"[from project standards]\"\n  - rationale: \"[technical reasoning]\"\nconfidence: \"[high|medium|low]\"\nopen_to: \"[what evidence would change your mind]\"\n```\n\n### Codex's Response (via subagent)\n```yaml\nposition: \"[Clear statement]\"\nagreement_areas: \"[where aligned with Claude]\"\ndisagreement_areas: \"[where divergent]\"\ncounter_evidence:\n  - code: \"[specific reference]\"\n  - rationale: \"[technical reasoning]\"\n```\n\n### Round 1 Outcome\n```yaml\noutcome: \"[resolved|unresolved]\"\nsynthesis: \"[if resolved, the merged view]\"\nremaining_issues: \"[if unresolved, specific points to address]\"\n```\n\n## Round 2 Structure\n\n### Focus\nAddress ONLY `remaining_issues` from Round 1. No new topics.\n\n### Claude's Response\n```yaml\naddressing: \"[specific remaining issue]\"\nnew_evidence: \"[something not presented in Round 1]\"\nconcession: \"[what I now agree with from Codex]\"\nmaintained: \"[what I still believe, with stronger reasoning]\"\n```\n\n### Codex's Response\n```yaml\naddressing: \"[specific remaining issue]\"\nresponse_to_evidence: \"[how new evidence changes or confirms view]\"\nproposed_synthesis: \"[attempt to merge positions]\"\n```\n\n### Round 2 Outcome\n```yaml\noutcome: \"[resolved|escalate]\"\nfinal_synthesis: \"[if resolved, the agreed approach]\"\nescalation_reason: \"[if escalating, why discussion failed]\"\nescalation_question: \"[precise question for Perplexity]\"\n```\n\n## Example Discussion\n\n### Setup\n- Claude: \"This function should use a Result/Either type for error handling\"\n- Codex: \"The current exception-based approach is clearer for this codebase\"\n\n### Round 1\n\n**Claude:**\n```yaml\nposition: \"Use Result type for explicit error handling\"\nevidence:\n  - code: \"user_service.py:45 returns None on not-found\"\n  - convention: \"Project uses type hints extensively\"\n  - rationale: \"Result types are more expressive and statically checkable\"\nconfidence: \"high\"\nopen_to: \"Performance concerns or team familiarity arguments\"\n```\n\n**Codex:**\n```yaml\nposition: \"Keep exception-based error handling\"\nagreement_areas: \"Explicit error handling is important\"\ndisagreement_areas: \"Implementation approach\"\ncounter_evidence:\n  - code: \"Other methods in user_service use exceptions consistently\"\n  - rationale: \"Consistency within module aids readability\"\n```\n\n**Round 1 Outcome:**\n```yaml\noutcome: \"unresolved\"\nsynthesis: null\nremaining_issues:\n  - \"Module consistency vs modern idiom adoption\"\n```\n\n### Round 2\n\n**Claude:**\n```yaml\naddressing: \"Module consistency vs modern idiom\"\nnew_evidence: \"Git history shows 3 commits in last month adopting Result types elsewhere\"\nconcession: \"Consistency matters for readability\"\nmaintained: \"Migration is already underway, this continues the pattern\"\n```\n\n**Codex:**\n```yaml\naddressing: \"Module consistency vs modern idiom\"\nresponse_to_evidence: \"Migration trend is valid context I missed\"\nproposed_synthesis: \"Use Result type here, add TODO for full module migration\"\n```\n\n**Round 2 Outcome:**\n```yaml\noutcome: \"resolved\"\nfinal_synthesis: \"Use Result type with TODO comment for module-wide migration\"\n```\n\n## When Discussion Fails\n\nIf after Round 2:\n- Positions remain fundamentally opposed\n- Evidence is inconclusive on both sides\n- Stakes are high (security, architecture, breaking changes)\n\nThen escalate with precise question:\n\n```\nGiven [specific context], should [option A] or [option B] be used?\n\nContext:\n- Codebase: [language/framework]\n- Existing patterns: [relevant patterns]\n- Constraints: [any constraints]\n\nClaude's position: [summary with key evidence]\nCodex's position: [summary with key evidence]\n\nWhich approach is correct and why?\n```\n\n## Anti-Patterns\n\n### Bad Discussion\n- **Echo Chamber:** Restating positions without new evidence\n- **Goalpost Moving:** Changing the disagreement mid-discussion\n- **Appeal to Authority:** \"Codex/Claude is usually right\"\n- **False Consensus:** Claiming agreement without actual alignment\n\n### Good Discussion\n- **Evidence-Based:** Every claim backed by code or standards\n- **Focused:** One issue at a time\n- **Constructive:** Seeking synthesis, not victory\n- **Honest:** Acknowledging uncertainty and conceding valid points\n",
        "plugins/codex-peer-review/skills/codex-peer-review/escalation-criteria.md": "# Escalation Criteria\n\nWhen to escalate disagreements for external research and authoritative resolution.\n\n**Note:** Use Perplexity MCP if available. If Perplexity is not available, use WebSearch tool instead.\n\n## Immediate Escalation (Skip Discussion)\n\nThese trigger direct escalation to external research - do not attempt discussion rounds:\n\n| Category | Examples | Why Skip Discussion |\n|----------|----------|---------------------|\n| Security | Auth bypass, injection, secrets exposure | Risk too high for debate |\n| Architecture | Fundamental design conflicts | Need expert judgment |\n| Breaking Changes | Backward compatibility disputes | Impact too broad |\n| Performance | Order-of-magnitude disagreements | Need verification |\n\n**Rule:** If either AI flags a security concern, escalate immediately regardless of the other's opinion.\n\n## Escalation After Discussion\n\nAfter completing 2 discussion rounds, escalate if ANY of these apply:\n\n### 1. No Convergence\n- Positions remain fundamentally opposed\n- New evidence in Round 2 didn't shift either position\n- Both AIs maintain high confidence in conflicting views\n\n### 2. Low Confidence, High Stakes\n- Neither AI can provide strong evidence\n- But the decision has significant consequences\n- \"We're both guessing on something important\"\n\n### 3. Novel Situation\n- Neither AI has clear precedent to cite\n- Project conventions don't cover this case\n- Industry standards are ambiguous or conflicting\n\n### 4. Factual Dispute\n- Disagreement is about verifiable facts\n- One AI might be working from outdated information\n- External research can provide authoritative answer\n\n## Do NOT Escalate\n\nReserve external research for genuine disputes. Do not escalate:\n\n| Category | Why Not | Resolution |\n|----------|---------|------------|\n| Minor style disagreements | Not worth expert time | Defer to project conventions |\n| Documentation wording | Subjective preference | Defer to Claude (user context) |\n| Test coverage thresholds | Project-specific | Use project standards |\n| Naming conventions | Style guide territory | Use project standards |\n| Formatting preferences | Tooling handles this | Use formatter config |\n\n## Escalation Message Templates\n\nWhen escalating, frame the question neutrally.\n\n### Option 1: Perplexity MCP (if available)\n\n```json\n{\n  \"messages\": [\n    {\n      \"role\": \"system\",\n      \"content\": \"You are a senior software architect arbitrating between two AI code reviewers. Provide definitive guidance based on industry best practices and the specific context provided. Be direct and decisive.\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"## Disagreement Context\\n\\n**Topic:** [specific technical question]\\n**Codebase:** [language/framework/version]\\n**Project Context:** [relevant constraints or patterns]\\n\\n**Claude's Position:**\\n[position summary]\\n- Evidence: [key evidence]\\n- Reasoning: [technical reasoning]\\n\\n**Codex's Position:**\\n[position summary]\\n- Evidence: [key evidence]\\n- Reasoning: [technical reasoning]\\n\\n**Discussion Summary:**\\n[what was tried, why it didn't resolve]\\n\\n**Question:** Which approach is correct for this situation and why? Please be decisive.\"\n    }\n  ]\n}\n```\n\n### Option 2: WebSearch (if Perplexity not available)\n\nUse the WebSearch tool with focused queries:\n\n```\nPrimary query: \"[specific technical question] best practices [language/framework] [year]\"\n\nFollow-up queries if needed:\n- \"[Option A approach] vs [Option B approach] [language]\"\n- \"[specific pattern] official documentation [framework]\"\n```\n\nLook for:\n- Official documentation\n- Well-known engineering blogs (Google, Netflix, Uber, etc.)\n- Stack Overflow answers with high votes and recent activity\n- Language/framework RFCs or design documents\n\n## Handling External Research Response\n\n### Accept as Authoritative\n- For this specific case, the research ruling is final\n- Do not re-litigate after receiving response\n- Apply ruling to the synthesis\n\n### Document for Future\n- Note the ruling in the final output\n- Include source URLs when using WebSearch\n- If pattern emerges, consider adding to project standards\n- Help user understand the reasoning\n\n### If Research is Inconclusive\nIf external research can't provide a clear answer:\n1. Present both options to user with tradeoffs\n2. Note the sources consulted and why they were inconclusive\n3. Let user make final decision\n4. Do not guess or flip a coin\n\n## Escalation Flowchart\n\n```\nDisagreement detected\n        |\n        v\nIs it security/architecture/breaking?\n        |\n    Yes |  No\n        |   |\n        v   v\n   ESCALATE   Start Round 1\n   NOW              |\n                    v\n              Resolved?\n                |\n            Yes | No\n                |  |\n                v  v\n            Done   Round 2\n                      |\n                      v\n                 Resolved?\n                   |\n               Yes | No\n                   |  |\n                   v  v\n               Done   Any major issue\n                      OR no convergence?\n                         |\n                     Yes | No (minor)\n                         |  |\n                         v  v\n                    ESCALATE  Accept\n                    NOW       partial\n                              agreement\n```\n"
      },
      "plugins": [
        {
          "name": "codex-peer-review",
          "version": "1.0.9",
          "description": "Peer validation system using OpenAI Codex CLI. Two AI perspectives catch more issues than one.",
          "author": {
            "name": "jcputney",
            "url": "https://github.com/jcputney"
          },
          "source": "./plugins/codex-peer-review",
          "category": "code-quality",
          "tags": [
            "peer-review",
            "code-review",
            "validation",
            "codex",
            "ai-collaboration"
          ],
          "categories": [
            "ai-collaboration",
            "code-quality",
            "code-review",
            "codex",
            "peer-review",
            "validation"
          ],
          "install_commands": [
            "/plugin marketplace add jcputney/agent-peer-review",
            "/plugin install codex-peer-review@agent-peer-review-marketplace"
          ]
        }
      ]
    }
  ]
}