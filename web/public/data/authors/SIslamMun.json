{
  "author": {
    "id": "SIslamMun",
    "display_name": "Shazzadul Islam",
    "type": "User",
    "avatar_url": "https://avatars.githubusercontent.com/u/41265882?u=0e71001d2a4a787f2e9b10ea3feea47d3dda3901&v=4",
    "url": "https://github.com/SIslamMun",
    "bio": "studies PhD in computer science  at Illinois Institute of Technology",
    "stats": {
      "total_marketplaces": 1,
      "total_plugins": 1,
      "total_commands": 3,
      "total_skills": 0,
      "total_stars": 0,
      "total_forks": 0
    }
  },
  "marketplaces": [
    {
      "name": "iowarp-plugins",
      "version": null,
      "description": "Official plugin marketplace for IOWarp project tools and integrations",
      "owner_info": {
        "name": "IOWarp Research Team",
        "email": "contact@iowarp.org"
      },
      "keywords": [],
      "repo_full_name": "SIslamMun/iowarp-plugin",
      "repo_url": "https://github.com/SIslamMun/iowarp-plugin",
      "repo_description": null,
      "homepage": null,
      "signals": {
        "stars": 0,
        "forks": 0,
        "pushed_at": "2025-10-14T16:56:49Z",
        "created_at": "2025-10-13T09:41:56Z",
        "license": null
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 611
        },
        {
          "path": "ndp-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "ndp-plugin/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "ndp-plugin/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 536
        },
        {
          "path": "ndp-plugin/README.md",
          "type": "blob",
          "size": 8615
        },
        {
          "path": "ndp-plugin/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "ndp-plugin/agents/ndp-data-scientist.md",
          "type": "blob",
          "size": 13858
        },
        {
          "path": "ndp-plugin/agents/ndp-dataset-curator.md",
          "type": "blob",
          "size": 7529
        },
        {
          "path": "ndp-plugin/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "ndp-plugin/commands/ndp-dataset-details.md",
          "type": "blob",
          "size": 5135
        },
        {
          "path": "ndp-plugin/commands/ndp-organizations.md",
          "type": "blob",
          "size": 3676
        },
        {
          "path": "ndp-plugin/commands/ndp-search.md",
          "type": "blob",
          "size": 3485
        },
        {
          "path": "ndp-plugin/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "ndp-plugin/hooks/hooks.json",
          "type": "blob",
          "size": 1989
        },
        {
          "path": "ndp-plugin/hooks/log_ndp_events.py",
          "type": "blob",
          "size": 5916
        }
      ],
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"name\": \"iowarp-plugins\",\n  \"owner\": {\n    \"name\": \"IOWarp Research Team\",\n    \"email\": \"contact@iowarp.org\"\n  },\n  \"description\": \"Official plugin marketplace for IOWarp project tools and integrations\",\n  \"plugins\": [\n    {\n      \"name\": \"ndp-plugin\",\n      \"source\": \"./ndp-plugin\",\n      \"description\": \"National Data Platform (NDP) integration with dataset search, discovery, and workflow automation\",\n      \"version\": \"1.0.0\",\n      \"keywords\": [\n        \"ndp\",\n        \"dataset-search\",\n        \"scientific-data\",\n        \"ckan\",\n        \"chronolog\",\n        \"workflow-automation\"\n      ]\n    }\n  ]\n}\n",
        "ndp-plugin/.claude-plugin/plugin.json": "{\n  \"name\": \"ndp-plugin\",\n  \"version\": \"1.0.0\",\n  \"description\": \"National Data Platform (NDP) integration plugin with dataset search, discovery, and workflow automation\",\n  \"author\": {\n    \"name\": \"IOWarp Research Team\",\n    \"email\": \"contact@iowarp.org\"\n  },\n  \"homepage\": \"https://github.com/SIslamMun/iowarp-plugin\",\n  \"repository\": \"https://github.com/SIslamMun/iowarp-plugin\",\n  \"license\": \"MIT\",\n  \"keywords\": [\n    \"ndp\",\n    \"dataset-search\",\n    \"scientific-data\",\n    \"ckan\",\n    \"chronolog\",\n    \"workflow-automation\"\n  ]\n}\n",
        "ndp-plugin/README.md": "# NDP Plugin for Claude Code\n\nA comprehensive Claude Code plugin for the National Data Platform (NDP) that integrates MCP servers, hooks, agents, and custom commands to provide seamless dataset discovery and workflow automation capabilities.\n\n## Features\n\n- **Custom Slash Commands**: Quick access to NDP search, organization listing, and dataset details\n- **Specialized Agents**: AI assistants for data science and dataset curation workflows\n- **Event Hooks**: Automatic logging of NDP-related activities for observability\n- **MCP Integration**: Direct access to NDP API through the iowarp-mcps package\n\n## Installation\n\n### Prerequisites\n\n- Claude Code installed and configured\n- Python 3.10 or higher\n- `uvx` package manager (for running MCP servers)\n\n### Install the Plugin\n\n#### Option 1: From GitHub (Recommended)\n\n```bash\n# Add the marketplace\n/plugin marketplace add SIslamMun/iowarp-plugin\n\n# Install the plugin\n/plugin install ndp-plugin@iowarp-plugins\n```\n\n#### Option 2: Local Development\n\n1. Clone the repository to your local machine\n2. Add the plugin directory as a local marketplace:\n\n```bash\n# Navigate to the directory containing ndp-plugin\ncd /path/to/iowarp-plugin\n\n# In Claude Code, add the local marketplace\n/plugin marketplace add /path/to/iowarp-plugin\n\n# Install the plugin\n/plugin install ndp-plugin@iowarp-plugins\n```\n\n### Verify Installation\n\nAfter installation, verify the plugin is loaded:\n\n```bash\n# List all available commands (should show ndp-* commands)\n/help\n\n# List available agents (should show NDP agents)\n/agents\n```\n\n## Quick Start\n\n### 1. Search for Datasets\n\nUse the `/ndp-search` command to find datasets:\n\n```\n/ndp-search\n\n# Then ask:\n\"Find climate datasets from NOAA\"\n\"Search for oceanographic data in NetCDF format\"\n```\n\n### 2. List Organizations\n\nUse the `/ndp-organizations` command to explore data sources:\n\n```\n/ndp-organizations\n\n# Then ask:\n\"Show me all organizations with 'climate' in their name\"\n```\n\n### 3. Get Dataset Details\n\nUse the `/ndp-dataset-details` command to retrieve full metadata:\n\n```\n/ndp-dataset-details\n\n# Then provide:\n\"Get details for dataset with ID 'dataset-12345-climate-temp'\"\n```\n\n## Components\n\n### Slash Commands\n\nThe plugin provides three custom commands:\n\n1. **`/ndp-search`** - Search for datasets across NDP with advanced filtering\n2. **`/ndp-organizations`** - List and filter organizations in NDP\n3. **`/ndp-dataset-details`** - Get detailed information about specific datasets\n\n### Agents\n\nTwo specialized agents are included:\n\n1. **NDP Data Scientist** - Expert in dataset discovery and research workflows\n   - Dataset search strategies\n   - Data quality assessment\n   - Multi-source integration planning\n\n2. **NDP Dataset Curator** - Expert in metadata and publishing workflows\n   - Metadata validation\n   - CKAN standards compliance\n   - Publishing guidance\n\n### Hooks\n\nEvent logging hooks track NDP usage:\n\n- **PreToolUse**: Logs before NDP MCP tools are called\n- **PostToolUse**: Logs after NDP MCP tools complete\n- **SessionStart**: Logs when sessions begin\n- **Stop**: Logs session completion\n\nLogs are stored in `${CLAUDE_PLUGIN_ROOT}/logs/ndp_events.log`\n\n### MCP Integration\n\nThe plugin integrates with **three MCP servers**, providing comprehensive data discovery, analysis, and visualization:\n\n#### NDP MCP (Dataset Discovery)\n1. **`list_organizations`** - List organizations from NDP\n2. **`search_datasets`** - Search datasets with advanced filtering\n3. **`get_dataset_details`** - Retrieve complete dataset metadata\n\n#### Pandas MCP (Data Analysis)\n4. **`load_data`** - Load datasets from various formats\n5. **`profile_data`** - Comprehensive data profiling\n6. **`statistical_summary`** - Detailed statistical analysis\n\n#### Plot MCP (Visualization)\n7. **`line_plot`** - Create time-series visualizations\n8. **`scatter_plot`** - Show variable relationships\n9. **`heatmap_plot`** - Visualize correlation matrices\n\n## Usage Examples\n\n### Example 1: Discover Climate Data\n\n```\nUser: /ndp-search\n\nClaude: I'll help you search for datasets in the National Data Platform.\nWhat are you looking for?\n\nUser: Find climate datasets from NOAA in NetCDF format\n\nClaude: [Uses NDP Data Scientist agent]\n[Calls list_organizations to find NOAA]\n[Calls search_datasets with organization=NOAA, format=NetCDF]\n[Returns results with dataset names, descriptions, and resources]\n```\n\n### Example 2: Multi-Server Search\n\n```\nUser: Search for oceanographic datasets on both global and local servers\n\nClaude: [Uses NDP Data Scientist agent]\n[Calls list_organizations on global server]\n[Calls list_organizations on local server]\n[Calls search_datasets on both servers]\n[Compares and presents results from both sources]\n```\n\n### Example 3: Dataset Quality Review\n\n```\nUser: Review the metadata quality for dataset ID \"ocean-temp-2023\"\n\nClaude: [Uses NDP Dataset Curator agent]\n[Calls get_dataset_details for the specified dataset]\n[Analyzes metadata completeness]\n[Provides recommendations for improvements]\n```\n\n## Configuration\n\n### Environment Variables (Optional)\n\nYou can configure NDP behavior with environment variables:\n\n```bash\n# NDP MCP Configuration\nexport NDP_DEFAULT_SERVER=\"global\"  # or \"local\"\nexport NDP_RESULT_LIMIT=\"20\"        # Maximum results per search\n\n# Logging Configuration\nexport NDP_LOG_LEVEL=\"INFO\"         # DEBUG, INFO, WARN, ERROR\nexport NDP_LOG_RETENTION_DAYS=\"30\"  # Auto-cleanup old logs\n```\n\n### Viewing Logs\n\nUse the provided script to view event logs:\n\n```bash\n./scripts/view-logs.sh\n```\n\nOr manually:\n\n```bash\ncat logs/ndp_events.log | jq .\n```\n\n## Troubleshooting\n\n### Plugin Not Loading\n\n1. Check that the plugin directory structure is correct\n2. Verify `plugin.json` is valid JSON\n3. Run Claude Code with debug flag: `claude --debug`\n\n### MCP Server Not Available\n\n1. Ensure `uv` is installed: `curl -LsSf https://astral.sh/uv/install.sh | sh`\n2. Test the MCP server directly: `uvx iowarp-mcps ndp`\n3. Check that iowarp-mcps is published to PyPI\n\n### Hooks Not Executing\n\n1. Verify `log_ndp_events.py` is executable: `chmod +x hooks/log_ndp_events.py`\n2. Ensure `uv` is installed and available in PATH\n3. Review hook configuration in `hooks/hooks.json`\n\n### No Search Results\n\n1. Verify NDP servers are accessible\n2. Try different search terms or filters\n3. Check server parameter (global vs local)\n4. Increase result limit if needed\n\n## Development\n\n### Testing Changes\n\n1. Make changes to plugin files\n2. Reinstall the plugin:\n\n```bash\n/plugin uninstall ndp-plugin\n/plugin install ndp-plugin@local-marketplace\n```\n\n3. Restart Claude Code to reload changes\n\n### Adding New Commands\n\n1. Create a new markdown file in `commands/` directory\n2. Add YAML frontmatter with description\n3. Document the command usage and examples\n4. Reload the plugin\n\n### Extending Agents\n\n1. Create new agent markdown files in `agents/` directory\n2. Define capabilities in YAML frontmatter\n3. Document expertise and use cases\n4. Reload the plugin\n\n## Architecture\n\n```\nndp-plugin/\n‚îú‚îÄ‚îÄ .claude-plugin/\n‚îÇ   ‚îî‚îÄ‚îÄ plugin.json          # Plugin metadata and configuration\n‚îú‚îÄ‚îÄ commands/                 # Custom slash commands\n‚îÇ   ‚îú‚îÄ‚îÄ ndp-search.md\n‚îÇ   ‚îú‚îÄ‚îÄ ndp-organizations.md\n‚îÇ   ‚îî‚îÄ‚îÄ ndp-dataset-details.md\n‚îú‚îÄ‚îÄ agents/                   # Specialized AI assistants\n‚îÇ   ‚îú‚îÄ‚îÄ ndp-data-scientist.md\n‚îÇ   ‚îî‚îÄ‚îÄ ndp-dataset-curator.md\n‚îú‚îÄ‚îÄ hooks/                    # Event automation\n‚îÇ   ‚îú‚îÄ‚îÄ hooks.json           # Hook configuration\n‚îÇ   ‚îî‚îÄ‚îÄ log_ndp_events.py    # Event logging script\n‚îú‚îÄ‚îÄ logs/                     # Event logs (auto-created)\n‚îÇ   ‚îî‚îÄ‚îÄ ndp_events.log\n‚îú‚îÄ‚îÄ scripts/                  # Utility scripts\n‚îÇ   ‚îî‚îÄ‚îÄ view-logs.sh         # Log viewer\n‚îú‚îÄ‚îÄ .mcp.json                # MCP server configuration\n‚îî‚îÄ‚îÄ README.md                # This file\n```\n\n## Contributing\n\nContributions are welcome! Please submit issues and pull requests to the [iowarp-plugin repository](https://github.com/SIslamMun/iowarp-plugin).\n\n## License\n\nMIT License - See LICENSE file for details\n\n## Support\n\n- **Documentation**: https://docs.claude.com/en/docs/claude-code/plugins\n- **IOWarp MCP Docs**: https://github.com/iowarp/iowarp-mcps\n- **Issues**: https://github.com/SIslamMun/iowarp-plugin/issues\n\n## Changelog\n\n### Version 1.0.0 (Initial Release)\n\n- Custom slash commands for NDP search, organizations, and dataset details\n- Two specialized agents (Data Scientist and Dataset Curator)\n- Event logging hooks for observability\n- MCP integration with iowarp-mcps ndp server\n- Comprehensive documentation and examples\n",
        "ndp-plugin/agents/ndp-data-scientist.md": "---\ndescription: Specialized agent for scientific data discovery and analysis using NDP\ncapabilities:\n  - Dataset search and discovery\n  - Data source evaluation\n  - Research workflow guidance\n  - Multi-source data integration\nmcp_tools:\n  - list_organizations\n  - search_datasets\n  - get_dataset_details\n  - load_data\n  - profile_data\n  - statistical_summary\n  - line_plot\n  - scatter_plot\n  - heatmap_plot\n---\n\n# NDP Data Scientist\n\nExpert in discovering, evaluating, and recommending scientific datasets from the National Data Platform.\n\n## üìÅ Critical: Output Management\n\n**ALL outputs MUST be saved to the project's `output/` folder at the root:**\n\n```\n${CLAUDE_PROJECT_DIR}/output/\n‚îú‚îÄ‚îÄ data/          # Downloaded datasets\n‚îú‚îÄ‚îÄ plots/         # All visualizations (PNG, PDF)\n‚îú‚îÄ‚îÄ reports/       # Analysis summaries and documentation\n‚îî‚îÄ‚îÄ intermediate/  # Temporary processing files\n```\n\n**Before starting any analysis:**\n1. Create directory structure: `mkdir -p output/data output/plots output/reports`\n2. All file paths in tool calls must use `output/` prefix\n3. Example: `load_data(file_path=\"output/data/dataset.csv\")`\n4. Example: `line_plot(..., output_path=\"output/plots/trend.png\")`\n\nYou have access to three MCP tools that enable direct interaction with the National Data Platform:\n\n## Available MCP Tools\n\n### 1. `list_organizations`\nLists all organizations contributing data to NDP. Use this to:\n- Discover available data sources\n- Verify organization names before searching\n- Filter organizations by name substring\n- Query different servers (global, local, pre_ckan)\n\n**Parameters**:\n- `name_filter` (optional): Filter by name substring\n- `server` (optional): 'global' (default), 'local', or 'pre_ckan'\n\n**Usage Pattern**: Always call this FIRST when user mentions an organization or wants to explore data sources.\n\n### 2. `search_datasets`\nSearches for datasets using various criteria. Use this to:\n- Find datasets by terms, organization, format, description\n- Filter by resource format (CSV, JSON, NetCDF, HDF5, etc.)\n- Search across different servers\n- Limit results to prevent context overflow\n\n**Key Parameters**:\n- `search_terms`: List of terms to search\n- `owner_org`: Organization name (get from list_organizations first)\n- `resource_format`: Filter by format (CSV, JSON, NetCDF, etc.)\n- `dataset_description`: Search in descriptions\n- `server`: 'global' (default) or 'local'\n- `limit`: Max results (default: 20, increase if needed)\n\n**Usage Pattern**: Use after identifying correct organization names. Start with broad searches, then refine.\n\n### 3. `get_dataset_details`\nRetrieves complete metadata for a specific dataset. Use this to:\n- Get full dataset information after search\n- View all resources and download URLs\n- Check dataset completeness and quality\n- Understand resource structure\n\n**Parameters**:\n- `dataset_identifier`: Dataset ID or name (from search results)\n- `identifier_type`: 'id' (default) or 'name'\n- `server`: 'global' (default) or 'local'\n\n**Usage Pattern**: Call this after finding interesting datasets to provide detailed analysis to user.\n\n## Expertise\n\n- **Dataset Discovery**: Advanced search strategies across multiple CKAN instances\n- **Quality Assessment**: Evaluate dataset completeness, format suitability, and metadata quality\n- **Research Workflows**: Guide users through data discovery to analysis pipelines\n- **Integration Planning**: Recommend approaches for combining datasets from multiple sources\n\n## When to Invoke\n\nUse this agent when you need help with:\n- Finding datasets for specific research questions\n- Evaluating dataset quality and suitability\n- Planning data integration strategies\n- Understanding NDP organization structure\n- Optimizing search queries for better results\n\n## Recommended Workflow\n\n1. **Understand Requirements**: Ask clarifying questions about research needs\n2. **Discover Organizations**: Use `list_organizations` to find relevant data sources\n3. **Search Datasets**: Use `search_datasets` with appropriate filters\n4. **Analyze Results**: Review search results for relevance\n5. **Get Details**: Use `get_dataset_details` for interesting datasets\n6. **Provide Recommendations**: Evaluate and recommend best datasets with reasoning\n\n## MCP Tool Usage Best Practices\n\n- **Always verify organization names** with `list_organizations` before using in search\n- **Use appropriate servers**: global for public data, local for institutional data\n- **Limit results** appropriately (start with 20, increase if needed)\n- **Combine filters** for precise searches (organization + format + terms)\n- **Multi-server searches**: Query both global and local when comprehensive coverage needed\n- **Get details selectively**: Only retrieve full details for relevant datasets to manage context\n\n## Example Interactions with MCP Tool Usage\n\n### Example 1: Finding NOAA Climate Data\n**User**: \"I need climate data from NOAA for the past decade in NetCDF format\"\n\n**Agent Actions**:\n1. Call `list_organizations(name_filter=\"noaa\")` to verify organization name\n2. Call `search_datasets(owner_org=\"NOAA\", resource_format=\"NetCDF\", search_terms=[\"climate\"], limit=20)`\n3. Review results and call `get_dataset_details(dataset_identifier=\"<id>\")` for top candidates\n4. Provide recommendations with quality assessment\n\n### Example 2: Organization Discovery\n**User**: \"What organizations provide Earth observation data through NDP?\"\n\n**Agent Actions**:\n1. Call `list_organizations(name_filter=\"earth\")`\n2. Call `list_organizations(name_filter=\"observation\")`\n3. Call `list_organizations(name_filter=\"satellite\")`\n4. Summarize findings and suggest specific organizations for user's needs\n\n### Example 3: Multi-Server Comparison\n**User**: \"Compare datasets about temperature monitoring across different servers\"\n\n**Agent Actions**:\n1. Call `search_datasets(search_terms=[\"temperature\", \"monitoring\"], server=\"global\", limit=15)`\n2. Call `search_datasets(search_terms=[\"temperature\", \"monitoring\"], server=\"local\", limit=15)`\n3. Compare and contrast results (coverage, formats, organizations)\n4. Recommend best sources based on requirements\n\n### Example 4: Format-Specific Search\n**User**: \"Find the best datasets for studying coastal erosion patterns\"\n\n**Agent Actions**:\n1. Call `list_organizations(name_filter=\"coast\")` and `list_organizations(name_filter=\"ocean\")`\n2. Call `search_datasets(search_terms=[\"coastal\", \"erosion\"], resource_format=\"NetCDF\", limit=20)`\n3. Call `search_datasets(search_terms=[\"coastal\", \"erosion\"], resource_format=\"GeoTIFF\", limit=20)`\n4. Evaluate datasets for spatial resolution, temporal coverage, and data quality\n5. Provide ranked recommendations with reasoning\n\n## Additional Data Analysis & Visualization Tools\n\nYou also have access to pandas and plot MCP tools for advanced data analysis and visualization:\n\n### Pandas MCP Tools (Data Analysis)\n\n#### `load_data`\nLoad datasets from downloaded NDP resources for analysis:\n- Supports CSV, Excel, JSON, Parquet, HDF5\n- Intelligent format detection\n- Returns data with quality metrics\n\n**Usage**: After downloading dataset from NDP, load it for analysis\n\n#### `profile_data`\nComprehensive data profiling:\n- Dataset overview (shape, types, statistics)\n- Column analysis with distributions\n- Data quality metrics (missing values, duplicates)\n- Correlation analysis (optional)\n\n**Usage**: First step after loading data to understand structure\n\n#### `statistical_summary`\nDetailed statistical analysis:\n- Descriptive stats (mean, median, mode, std dev)\n- Distribution analysis (skewness, kurtosis)\n- Data profiling and outlier detection\n\n**Usage**: Deep dive into numerical columns for research insights\n\n### Plot MCP Tools (Visualization)\n\n#### `line_plot`\nCreate time-series or trend visualizations:\n- **Parameters**: file_path, x_column, y_column, title, output_path\n- Returns plot with statistical summary\n\n**Usage**: Visualize temporal trends in climate/ocean data\n\n#### `scatter_plot`\nShow relationships between variables:\n- **Parameters**: file_path, x_column, y_column, title, output_path\n- Includes correlation statistics\n\n**Usage**: Explore correlations between dataset variables\n\n#### `heatmap_plot`\nVisualize correlation matrices:\n- **Parameters**: file_path, title, output_path\n- Shows all numerical column correlations\n\n**Usage**: Identify relationships across multiple variables\n\n## Complete Research Workflow with All Tools\n\n### Output Management\n\n**CRITICAL**: All analysis outputs, visualizations, and downloaded datasets MUST be saved to the project's `output/` folder:\n\n- **Create output directory**: `mkdir -p output/` at project root if it doesn't exist\n- **Downloaded datasets**: Save to `output/data/` (e.g., `output/data/ocean_temp.csv`)\n- **Visualizations**: Save to `output/plots/` (e.g., `output/plots/temperature_trends.png`)\n- **Analysis reports**: Save to `output/reports/` (e.g., `output/reports/analysis_summary.txt`)\n- **Intermediate files**: Save to `output/intermediate/` for processing steps\n\n**Path Usage**:\n- Always use `${CLAUDE_PROJECT_DIR}/output/` for absolute paths\n- For plot tools, use `output_path` parameter: `output_path=\"output/plots/my_plot.png\"`\n- Organize by dataset or analysis type: `output/noaa_ocean/`, `output/climate_analysis/`\n\n### Discovery ‚Üí Analysis ‚Üí Visualization Pipeline\n\n**Phase 1: Dataset Discovery (NDP Tools)**\n1. `list_organizations` - Find data providers\n2. `search_datasets` - Locate relevant datasets\n3. `get_dataset_details` - Get download URLs and metadata\n\n**Phase 2: Data Acquisition**\n4. Download dataset to `output/data/` folder\n5. Verify file exists and is readable\n\n**Phase 3: Data Analysis (Pandas Tools)**\n6. `load_data` - Load from `output/data/<filename>`\n7. `profile_data` - Understand data structure and quality\n8. `statistical_summary` - Analyze distributions and statistics\n\n**Phase 4: Visualization (Plot Tools)**\n9. `line_plot` - Save to `output/plots/line_<name>.png`\n10. `scatter_plot` - Save to `output/plots/scatter_<name>.png`\n11. `heatmap_plot` - Save to `output/plots/heatmap_<name>.png`\n\n## Enhanced Example Workflows\n\n### Example 5: Complete Research Analysis\n**User**: \"Help me analyze NOAA ocean temperature data - find it, load it, analyze statistics, and create visualizations\"\n\n**Agent Actions**:\n1. **Setup**:\n   - Create output structure: `mkdir -p output/data output/plots output/reports`\n\n2. **Discovery**:\n   - `list_organizations(name_filter=\"noaa\")`\n   - `search_datasets(owner_org=\"NOAA\", search_terms=[\"ocean\", \"temperature\"], resource_format=\"CSV\")`\n   - `get_dataset_details(dataset_identifier=\"<id>\")` to get download URL\n\n3. **Data Acquisition**:\n   - Provide download instructions: `wget <url> -O output/data/ocean_temp.csv`\n   - Or use: `curl -o output/data/ocean_temp.csv <url>`\n\n4. **Analysis**:\n   - `load_data(file_path=\"output/data/ocean_temp.csv\")`\n   - `profile_data(file_path=\"output/data/ocean_temp.csv\")`\n   - `statistical_summary(file_path=\"output/data/ocean_temp.csv\", include_distributions=True)`\n\n5. **Visualization**:\n   - `line_plot(file_path=\"output/data/ocean_temp.csv\", x_column=\"date\", y_column=\"temperature\", title=\"Ocean Temperature Trends\", output_path=\"output/plots/temp_trends.png\")`\n   - `scatter_plot(file_path=\"output/data/ocean_temp.csv\", x_column=\"depth\", y_column=\"temperature\", title=\"Depth vs Temperature\", output_path=\"output/plots/depth_vs_temp.png\")`\n   - `heatmap_plot(file_path=\"output/data/ocean_temp.csv\", title=\"Variable Correlations\", output_path=\"output/plots/correlations.png\")`\n\n6. **Summary**:\n   - Create analysis report saved to `output/reports/ocean_temp_analysis.md`\n\n### Example 6: Multi-Dataset Comparison\n**User**: \"Compare temperature datasets from two different organizations\"\n\n**Agent Actions**:\n1. **Setup**: `mkdir -p output/data output/plots output/reports`\n2. Find both datasets using NDP tools\n3. Download to `output/data/dataset1.csv` and `output/data/dataset2.csv`\n4. Load both with `load_data`\n5. Profile both with `profile_data`\n6. Create comparison visualizations:\n   - `line_plot` ‚Üí `output/plots/dataset1_trends.png`\n   - `line_plot` ‚Üí `output/plots/dataset2_trends.png`\n   - `scatter_plot` ‚Üí `output/plots/comparison_scatter.png`\n7. Generate correlation analysis:\n   - `heatmap_plot` ‚Üí `output/plots/dataset1_correlations.png`\n   - `heatmap_plot` ‚Üí `output/plots/dataset2_correlations.png`\n8. Create comparison report ‚Üí `output/reports/dataset_comparison.md`\n\n## Tool Selection Guidelines\n\n**Use NDP Tools when**:\n- Searching for datasets\n- Discovering data sources\n- Getting metadata and download URLs\n- Exploring what data is available\n\n**Use Pandas Tools when**:\n- Loading downloaded datasets\n- Analyzing data structure and quality\n- Computing statistics\n- Transforming or filtering data\n\n**Use Plot Tools when**:\n- Creating visualizations\n- Exploring relationships\n- Generating publication-ready figures\n- Presenting results\n\n## Best Practices for Full Workflow\n\n1. **Always start with NDP discovery** - Don't analyze data you haven't found yet\n2. **Create output directory structure** - `mkdir -p output/data output/plots output/reports` at project root\n3. **Save everything to output/** - All files, plots, and reports go in the organized output structure\n4. **Get dataset details first** - Understand format and structure before downloading\n5. **Download to output/data/** - Keep all datasets organized in one location\n6. **Profile before analyzing** - Use `profile_data` to understand data quality\n7. **Visualize with output paths** - Always specify `output_path=\"output/plots/<name>.png\"` for plots\n8. **Create summary reports** - Save analysis summaries to `output/reports/` for documentation\n9. **Use descriptive filenames** - Name files clearly: `ocean_temp_2020_2024.csv`, not `data.csv`\n10. **Provide complete guidance** - Tell user exact paths for all inputs and outputs\n",
        "ndp-plugin/agents/ndp-dataset-curator.md": "---\ndescription: Specialized agent for dataset curation, metadata validation, and NDP publishing workflows\ncapabilities:\n  - Metadata quality assessment\n  - Dataset organization recommendations\n  - Publishing workflow guidance\n  - Resource format validation\nmcp_tools:\n  - list_organizations\n  - search_datasets\n  - get_dataset_details\n---\n\n# NDP Dataset Curator\n\nExpert in dataset curation, metadata best practices, and NDP publishing workflows.\n\nYou have access to three MCP tools for examining existing datasets and organizational structure in NDP:\n\n## Available MCP Tools\n\n### 1. `list_organizations`\nLists organizations in NDP. Use this to:\n- Understand organizational structure\n- Find examples of well-organized data providers\n- Verify organization naming conventions\n- Guide users on organization selection\n\n**Parameters**:\n- `name_filter` (optional): Filter by name substring\n- `server` (optional): 'global' (default), 'local', or 'pre_ckan'\n\n**Usage for Curation**: Examine how established organizations structure their data presence.\n\n### 2. `search_datasets`\nSearches datasets by various criteria. Use this to:\n- Find example datasets with good metadata\n- Identify metadata patterns and standards\n- Review resource format distribution\n- Analyze dataset organization practices\n\n**Key Parameters**:\n- `owner_org`: Study datasets from specific organizations\n- `resource_format`: Examine format usage patterns\n- `limit`: Control number of examples to review\n\n**Usage for Curation**: Pull example datasets to demonstrate metadata best practices.\n\n### 3. `get_dataset_details`\nRetrieves complete dataset metadata. Use this to:\n- Perform detailed metadata quality assessment\n- Evaluate completeness of metadata fields\n- Check resource documentation quality\n- Identify metadata gaps and issues\n- Provide specific improvement recommendations\n\n**Parameters**:\n- `dataset_identifier`: Dataset ID or name\n- `identifier_type`: 'id' (default) or 'name'\n- `server`: 'global' (default) or 'local'\n\n**Usage for Curation**: Deep-dive analysis of metadata quality, format compliance, documentation completeness.\n\n## Expertise\n\n- **Metadata Standards**: Ensure datasets follow CKAN and scientific metadata conventions\n- **Organization Management**: Guide dataset organization and categorization\n- **Resource Validation**: Verify resource formats, accessibility, and documentation\n- **Publishing Workflows**: Help prepare datasets for NDP publication\n\n## When to Invoke\n\nUse this agent when you need help with:\n- Preparing datasets for NDP publication\n- Validating metadata completeness and quality\n- Organizing datasets within NDP structure\n- Understanding CKAN metadata requirements\n- Reviewing dataset documentation\n\n## Metadata Quality Assessment Workflow\n\n1. **Get Dataset Details**: Use `get_dataset_details` to retrieve complete metadata\n2. **Evaluate Completeness**: Check for required and recommended CKAN fields\n3. **Assess Documentation**: Review descriptions, tags, and resource documentation\n4. **Validate Formats**: Verify resource formats are correct and standardized\n5. **Compare Best Practices**: Use `search_datasets` to find exemplary datasets\n6. **Provide Recommendations**: Specific, actionable improvements with examples\n\n## CKAN Metadata Fields to Validate\n\n### Required Fields\n- **Title**: Clear, descriptive, not redundant with organization name\n- **Description**: Comprehensive, well-formatted, includes methodology\n- **Organization**: Appropriate organization assignment\n- **Resources**: At least one resource with valid format and URL\n\n### Recommended Fields\n- **Tags**: Relevant keywords for discoverability\n- **Author/Maintainer**: Contact information\n- **License**: Clear licensing information\n- **Temporal Coverage**: Date ranges for time-series data\n- **Spatial Coverage**: Geographic extent\n- **Version**: Dataset version information\n\n### Resource Validation\n- **Format**: Standardized format names (CSV, JSON, NetCDF, HDF5, GeoTIFF)\n- **Description**: Clear explanation of resource contents\n- **URL**: Accessible download links\n- **Size**: File size information when available\n\n## MCP Tool Usage Best Practices\n\n- **Get full details** before assessment: Always use `get_dataset_details` first\n- **Find exemplars**: Use `search_datasets` to locate well-documented datasets as examples\n- **Study organizational patterns**: Use `list_organizations` to understand naming and structure\n- **Provide specific examples**: Reference actual NDP datasets when recommending improvements\n- **Validate across servers**: Check both global and local for comprehensive validation\n\n## Example Interactions with MCP Tool Usage\n\n### Example 1: Metadata Completeness Review\n**User**: \"Review dataset ID 'climate-temps-2023' for metadata completeness\"\n\n**Agent Actions**:\n1. Call `get_dataset_details(dataset_identifier=\"climate-temps-2023\")`\n2. Evaluate all metadata fields against CKAN standards\n3. Check resource completeness (formats, descriptions, URLs)\n4. Compare with similar high-quality datasets using `search_datasets(search_terms=[\"climate\"], limit=5)`\n5. Provide detailed report with specific missing/incomplete fields\n6. Recommend improvements with examples from best-practice datasets\n\n### Example 2: Organization Selection Guidance\n**User**: \"Help me choose the right organization for my oceanographic dataset\"\n\n**Agent Actions**:\n1. Call `list_organizations(name_filter=\"ocean\")`\n2. Call `list_organizations(name_filter=\"marine\")`\n3. For each relevant organization, call `search_datasets(owner_org=\"<name>\", limit=3)` to see their datasets\n4. Evaluate organizational focus, dataset types, and quality standards\n5. Recommend best-fit organization with rationale\n6. Provide examples of similar datasets in that organization\n\n### Example 3: Resource Format Validation\n**User**: \"Validate the resource formats and documentation for this dataset\"\n\n**Agent Actions**:\n1. Call `get_dataset_details(dataset_identifier=\"<id>\")`\n2. Check each resource for:\n   - Standardized format names\n   - Complete descriptions\n   - Valid URLs\n   - Appropriate file types for data\n3. Call `search_datasets(resource_format=\"NetCDF\", limit=5)` to find examples of good resource documentation\n4. Provide format-specific recommendations\n5. Suggest improvements with examples from well-documented resources\n\n### Example 4: Publication Readiness Assessment\n**User**: \"Is my dataset ready for NDP publication? Dataset name: 'satellite-imagery-pacific'\"\n\n**Agent Actions**:\n1. Call `get_dataset_details(dataset_identifier=\"satellite-imagery-pacific\", identifier_type=\"name\")`\n2. Perform comprehensive checklist:\n   - All required fields present\n   - Description quality and completeness\n   - Tags appropriate and sufficient\n   - Resources properly formatted\n   - Contact information provided\n   - License clearly stated\n3. Call `search_datasets(search_terms=[\"satellite\"], resource_format=\"GeoTIFF\", limit=3)` for comparison\n4. Provide publication readiness score with specific gaps\n5. Prioritized action items for publication preparation\n\n### Example 5: Best Practices Demonstration\n**User**: \"Show me examples of well-documented climate datasets\"\n\n**Agent Actions**:\n1. Call `search_datasets(search_terms=[\"climate\"], limit=10)`\n2. Call `get_dataset_details` for top 3 results with most complete metadata\n3. Analyze their metadata structure:\n   - Description formatting and content\n   - Tag usage\n   - Resource organization\n   - Documentation completeness\n4. Extract best practices and patterns\n5. Provide template based on these examples\n",
        "ndp-plugin/commands/ndp-dataset-details.md": "---\ndescription: Retrieve detailed information about a specific NDP dataset\n---\n\n# NDP Dataset Details\n\nGet comprehensive metadata and resource information for a specific dataset.\n\nThis command provides access to detailed dataset metadata through the NDP MCP.\n\n## Available MCP Tool\n\n### `get_dataset_details`\nRetrieves complete information for a specific dataset:\n\n**Parameters**:\n- **dataset_identifier** (required): The dataset ID or name\n  - ID: Unique identifier (e.g., \"a1b2c3d4-5678-90ef-ghij-klmnopqrstuv\")\n  - Name: Human-readable name (e.g., \"noaa-climate-temp-2023\")\n- **identifier_type** (optional): Type of identifier\n  - `'id'` (default) - Use when providing dataset ID\n  - `'name'` - Use when providing dataset name/slug\n- **server** (optional): Server to query\n  - `'global'` (default) - Global NDP server\n  - `'local'` - Local/institutional server\n\n**Returns**: Comprehensive dataset information including:\n- **Metadata**: Title, description, organization, tags, license\n- **Resources**: All files/URLs with formats, sizes, descriptions\n- **Temporal Info**: Creation date, last modified, temporal coverage\n- **Spatial Info**: Geographic coverage (if applicable)\n- **Contact Info**: Author, maintainer information\n- **Additional Fields**: Custom metadata, processing info\n\n## Usage Patterns\n\n### After Dataset Search\n```\n\"Get details for dataset ID 'climate-temps-2023'\"\n```\nUses: `get_dataset_details(dataset_identifier=\"climate-temps-2023\", identifier_type=\"id\")`\n\n### By Dataset Name\n```\n\"Show me all information about the 'ocean-temperature-pacific' dataset\"\n```\nUses: `get_dataset_details(dataset_identifier=\"ocean-temperature-pacific\", identifier_type=\"name\")`\n\n### Resource Information\n```\n\"What formats are available for this dataset?\" (after finding it in search)\n```\nUses: `get_dataset_details(dataset_identifier=\"<from_search>\")`\n\n### Quality Assessment\n```\n\"Review the metadata quality for dataset 'satellite-imagery-2024'\"\n```\nUses: `get_dataset_details(dataset_identifier=\"satellite-imagery-2024\", identifier_type=\"name\")`\n\n## Information Retrieved\n\n### Core Metadata\n- **Title**: Dataset name\n- **Description**: Detailed description with methodology\n- **Organization**: Owner organization\n- **Tags**: Keywords for discoverability\n- **License**: Usage rights and restrictions\n\n### Resource Details\nFor each resource (file/URL):\n- **Format**: File format (CSV, JSON, NetCDF, HDF5, etc.)\n- **URL**: Download link\n- **Description**: Resource-specific description\n- **Size**: File size (if available)\n- **Created/Modified**: Timestamps\n\n### Additional Information\n- **Author/Maintainer**: Contact information\n- **Temporal Coverage**: Date ranges\n- **Spatial Coverage**: Geographic extent\n- **Version**: Dataset version\n- **Related Datasets**: Links to related data\n- **Processing Info**: Data processing details\n\n## When to Use\n\n1. **After Search**: Follow up on interesting datasets from search results\n2. **Before Download**: Verify dataset contents and formats\n3. **Quality Review**: Check metadata completeness for curation\n4. **Citation Info**: Get complete information for proper attribution\n5. **Resource Selection**: Choose specific files/formats from dataset\n6. **Metadata Validation**: Assess dataset documentation quality\n\n## Workflow Integration\n\n1. **Search First**: Use `/ndp-search` to find datasets\n2. **Get IDs**: Note dataset IDs or names from search results\n3. **Retrieve Details**: Use this command for complete information\n4. **Download**: Use resource URLs from details for data access\n\n## Example Interactions\n\n### Example 1: Complete Dataset Review\n```\nUser: \"Get complete information for dataset ID 'abc123-climate'\"\nClaude uses: get_dataset_details(dataset_identifier=\"abc123-climate\")\nResult: Full metadata, all resources, download URLs, temporal/spatial coverage\n```\n\n### Example 2: Resource Exploration\n```\nUser: \"What files are included in the NOAA temperature dataset?\"\nClaude uses:\n  1. search_datasets(owner_org=\"NOAA\", search_terms=[\"temperature\"])\n  2. get_dataset_details(dataset_identifier=\"<id_from_search>\")\nResult: List of all resources with formats and descriptions\n```\n\n### Example 3: Metadata Quality Check\n```\nUser: \"Review the documentation for this oceanographic dataset\"\nClaude uses: get_dataset_details(dataset_identifier=\"<provided_id>\")\nAnalysis: Evaluates description, tags, resource documentation, contact info\n```\n\n### Example 4: Multi-Dataset Comparison\n```\nUser: \"Compare the resources available in these three datasets\"\nClaude uses: get_dataset_details() for each dataset\nResult: Side-by-side comparison of formats, sizes, documentation\n```\n\n## Tips\n\n- **Use IDs when available**: More reliable than names\n- **Check both servers**: Same dataset name might exist on multiple servers\n- **Review all resources**: Datasets often have multiple files/formats\n- **Note download URLs**: Save resource URLs for data access\n- **Check temporal coverage**: Ensure data covers your time period of interest\n- **Verify formats**: Confirm file formats are compatible with your tools\n- **Read descriptions carefully**: Important processing details often in descriptions\n",
        "ndp-plugin/commands/ndp-organizations.md": "---\ndescription: List and filter organizations in the National Data Platform\n---\n\n# NDP Organizations\n\nList all organizations contributing data to the National Data Platform.\n\nThis command provides access to organization discovery functionality through the NDP MCP.\n\n## Available MCP Tool\n\n### `list_organizations`\nLists all organizations in NDP with optional filtering:\n\n**Parameters**:\n- **name_filter** (optional): Filter organizations by name substring match\n  - Case-insensitive partial matching\n  - Example: \"climate\" matches \"Climate Research Center\", \"NOAA Climate Lab\"\n- **server** (optional): Server to query\n  - `'global'` (default) - Public global NDP server\n  - `'local'` - Local/institutional NDP server\n  - `'pre_ckan'` - Pre-production server\n\n**Returns**: List of organization names and metadata including:\n- Total count of organizations\n- Organization names matching filter\n- Server queried\n\n## Usage Patterns\n\n### Discover All Organizations\n```\n\"List all organizations in the National Data Platform\"\n```\nUses: `list_organizations()` - No filter, returns all organizations\n\n### Filter by Keyword\n```\n\"Show me all organizations with 'climate' in their name\"\n```\nUses: `list_organizations(name_filter=\"climate\")`\n\n### Multi-Server Query\n```\n\"Compare organizations on global and local servers\"\n```\nUses: `list_organizations(server=\"global\")` and `list_organizations(server=\"local\")`\n\n### Research-Specific Discovery\n```\n\"Find organizations related to oceanographic research\"\n```\nUses: `list_organizations(name_filter=\"ocean\")` and `list_organizations(name_filter=\"marine\")`\n\n## Why Use This Command\n\n1. **Verify Organization Names**: Get exact names before using in dataset searches\n2. **Explore Data Sources**: Understand what organizations contribute to NDP\n3. **Guide Searches**: Identify relevant organizations for your research domain\n4. **Server Comparison**: See organizational differences between servers\n5. **Data Coverage**: Understand breadth of data providers\n\n## Workflow Integration\n\n1. **Start Here**: Use this command before searching datasets\n2. **Identify Providers**: Find organizations relevant to your research\n3. **Use in Search**: Pass organization names to `search_datasets`\n4. **Iterate**: Refine organization filters as needed\n\n## Example Interactions\n\n### Example 1: General Exploration\n```\nUser: \"List all organizations available on the local NDP server\"\nClaude uses: list_organizations(server=\"local\")\nResult: Complete list of local organizations with count\n```\n\n### Example 2: Targeted Discovery\n```\nUser: \"Find organizations related to satellite data\"\nClaude uses: list_organizations(name_filter=\"satellite\")\nResult: Organizations with \"satellite\" in their name\n```\n\n### Example 3: Multi-Keyword Search\n```\nUser: \"Show me organizations working on Earth observation\"\nClaude uses:\n  - list_organizations(name_filter=\"earth\")\n  - list_organizations(name_filter=\"observation\")\nResult: Combined results from both searches\n```\n\n### Example 4: Before Dataset Search\n```\nUser: \"I want to search for NOAA climate data\"\nClaude uses: list_organizations(name_filter=\"noaa\")\nResult: Exact NOAA organization name(s)\nThen: Can proceed with search_datasets(owner_org=\"<verified_name>\")\n```\n\n## Tips\n\n- **Use partial names**: \"ocean\" will match \"Oceanographic Institute\", \"Ocean Research Lab\", etc.\n- **Try variations**: Search both \"climate\" and \"atmospheric\" to find all relevant organizations\n- **Check both servers**: Global and local may have different organizations\n- **Verify before searching**: Always confirm organization name before using in dataset searches\n- **Multiple keywords**: Try related terms to discover all relevant providers\n",
        "ndp-plugin/commands/ndp-search.md": "---\ndescription: Search for datasets in the National Data Platform\n---\n\n# NDP Dataset Search\n\nSearch for datasets across the National Data Platform ecosystem with advanced filtering options.\n\nThis command provides access to the NDP MCP tools for dataset discovery and exploration.\n\n## Available MCP Tools\n\nWhen you use this command, Claude can invoke these MCP tools:\n\n### `search_datasets` - Primary search tool\nSearches for datasets using various criteria:\n- **search_terms**: List of terms to search across all fields\n- **owner_org**: Filter by organization name\n- **resource_format**: Filter by format (CSV, JSON, NetCDF, HDF5, GeoTIFF, etc.)\n- **dataset_description**: Search in descriptions\n- **server**: Query 'global' (default) or 'local' server\n- **limit**: Maximum results (default: 20)\n\n### `list_organizations` - Organization discovery\nLists available organizations:\n- **name_filter**: Filter by name substring\n- **server**: Query 'global' (default), 'local', or 'pre_ckan'\n\n### `get_dataset_details` - Detailed information\nRetrieves complete metadata for a specific dataset:\n- **dataset_identifier**: Dataset ID or name from search results\n- **identifier_type**: 'id' (default) or 'name'\n- **server**: 'global' (default) or 'local'\n\n## Recommended Workflow\n\n1. **Discover Organizations**: Use `list_organizations` to find relevant data sources\n2. **Search Datasets**: Use `search_datasets` with appropriate filters\n3. **Review Results**: Claude will summarize matching datasets\n4. **Get Details**: Use `get_dataset_details` for datasets of interest\n5. **Refine Search**: Adjust filters based on results\n\n## Best Practices\n\n- **Always verify organization names** with `list_organizations` before using in search\n- **Start broad, then refine**: Begin with simple terms, add filters as needed\n- **Limit results appropriately**: Default 20 is good, increase if needed\n- **Use format filters**: Narrow to specific formats (NetCDF, CSV, etc.) when relevant\n- **Multi-server searches**: Query both global and local for comprehensive coverage\n\n## Example Queries\n\n### Basic Search\n```\n\"Find climate datasets from NOAA\"\n```\nExpected tools: `list_organizations(name_filter=\"noaa\")`, then `search_datasets(owner_org=\"NOAA\", search_terms=[\"climate\"])`\n\n### Format-Specific Search\n```\n\"Search for oceanographic data in NetCDF format\"\n```\nExpected tools: `search_datasets(search_terms=[\"oceanographic\"], resource_format=\"NetCDF\")`\n\n### Organization-Based Search\n```\n\"List all datasets from a specific research institution\"\n```\nExpected tools: `list_organizations(name_filter=\"<institution>\")`, then `search_datasets(owner_org=\"<name>\")`\n\n### Refined Search with Limit\n```\n\"Find CSV datasets about temperature monitoring, limit to 10 results\"\n```\nExpected tools: `search_datasets(search_terms=[\"temperature\", \"monitoring\"], resource_format=\"CSV\", limit=10)`\n\n### Multi-Server Comparison\n```\n\"Compare oceanographic datasets on global and local servers\"\n```\nExpected tools: `search_datasets(server=\"global\", ...)` and `search_datasets(server=\"local\", ...)`\n\n## Tips for Effective Searching\n\n1. **Use specific terminology**: Scientific terms work better than generic ones\n2. **Combine filters**: Organization + format + terms = precise results\n3. **Check multiple formats**: Try CSV, NetCDF, HDF5 for scientific data\n4. **Explore organizations first**: Understanding data providers helps target searches\n5. **Request details selectively**: Full metadata for only the most relevant datasets\n",
        "ndp-plugin/hooks/hooks.json": "{\n  \"hooks\": {\n    \"UserPromptSubmit\": [\n      {\n        \"matcher\": \"\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"${CLAUDE_PLUGIN_ROOT}/hooks/log_ndp_events.py --event-type UserPromptSubmit\"\n          }\n        ]\n      }\n    ],\n    \"PreToolUse\": [\n      {\n        \"matcher\": \"ndp\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"${CLAUDE_PLUGIN_ROOT}/hooks/log_ndp_events.py --event-type PreToolUse\"\n          }\n        ]\n      },\n      {\n        \"matcher\": \"*\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"echo \\\"$(date +%s.%N),$(ps -o %cpu= -p $$),$(ps -o rss= -p $$),$CLAUDE_TOOL_NAME,start\\\" >> ~/.claude/performance.csv\"\n          }\n        ]\n      }\n    ],\n    \"PostToolUse\": [\n      {\n        \"matcher\": \"ndp\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"${CLAUDE_PLUGIN_ROOT}/hooks/log_ndp_events.py --event-type PostToolUse\"\n          }\n        ]\n      },\n      {\n        \"matcher\": \"*\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"echo \\\"$(date +%s.%N),$(ps -o %cpu= -p $$),$(ps -o rss= -p $$),$CLAUDE_TOOL_NAME,end\\\" >> ~/.claude/performance.csv; if [[ $(wc -l < ~/.claude/performance.csv) -gt 1000 ]]; then tail -n 500 ~/.claude/performance.csv > ~/.claude/performance.csv.tmp && mv ~/.claude/performance.csv.tmp ~/.claude/performance.csv; fi\"\n          }\n        ]\n      }\n    ],\n    \"SessionStart\": [\n      {\n        \"matcher\": \"\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"${CLAUDE_PLUGIN_ROOT}/hooks/log_ndp_events.py --event-type SessionStart\"\n          }\n        ]\n      }\n    ],\n    \"Stop\": [\n      {\n        \"matcher\": \"\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"${CLAUDE_PLUGIN_ROOT}/hooks/log_ndp_events.py --event-type Stop\"\n          }\n        ]\n      }\n    ]\n  }\n}\n",
        "ndp-plugin/hooks/log_ndp_events.py": "#!/usr/bin/env -S uv run --python 3.10 --script\n# /// script\n# requires-python = \">=3.10\"\n# ///\n\"\"\"\nNDP Plugin Event Logger\nLogs Claude Code events related to NDP plugin usage to a local file.\nEnhanced to capture tool names, user input, and agent responses.\n\"\"\"\n\nimport json\nimport sys\nimport os\nimport argparse\nfrom datetime import datetime\nfrom pathlib import Path\n\ndef get_log_file_path():\n    \"\"\"Get the log file path within plugin directory\"\"\"\n    # Get plugin root directory\n    plugin_root = Path(__file__).parent.parent\n    logs_dir = plugin_root / \"logs\"\n\n    # Create logs directory if it doesn't exist\n    logs_dir.mkdir(exist_ok=True)\n\n    return logs_dir / \"ndp_events.log\"\n\ndef extract_enhanced_data(event_type: str, event_data: dict) -> dict:\n    \"\"\"Extract enhanced information from event data\"\"\"\n    enhanced = {\n        \"timestamp\": datetime.now().isoformat(),\n        \"event_type\": event_type,\n        \"session_id\": event_data.get(\"session_id\", \"unknown\"),\n    }\n    \n    # Extract tool information for PreToolUse and PostToolUse\n    if event_type in [\"PreToolUse\", \"PostToolUse\"]:\n        tool_data = event_data.get('tool', {})\n        if tool_data:\n            enhanced['tool_name'] = tool_data.get('name', 'unknown')\n            enhanced['tool_input'] = tool_data.get('input', {})\n            \n            # For PostToolUse, capture tool results\n            if event_type == \"PostToolUse\":\n                if 'result' in event_data:\n                    enhanced['tool_result'] = event_data['result']\n                if 'output' in event_data:\n                    enhanced['tool_output'] = event_data['output']\n                if 'error' in event_data:\n                    enhanced['tool_error'] = event_data['error']\n    \n    # Extract user input for UserPromptSubmit\n    if event_type == \"UserPromptSubmit\":\n        if 'text' in event_data:\n            enhanced['user_prompt'] = event_data['text']\n        if 'messages' in event_data:\n            enhanced['conversation_messages'] = event_data['messages']\n    \n    # For PostToolUse, extract agent response from transcript\n    if event_type == \"PostToolUse\" and 'transcript_path' in event_data:\n        transcript_path = event_data['transcript_path']\n        if os.path.exists(transcript_path):\n            try:\n                # Read last few messages to capture recent agent response\n                recent_chat = []\n                with open(transcript_path, 'r') as f:\n                    lines = f.readlines()\n                    # Get last 5 messages to capture context\n                    for line in lines[-5:]:\n                        line = line.strip()\n                        if line:\n                            try:\n                                msg = json.loads(line)\n                                recent_chat.append(msg)\n                            except json.JSONDecodeError:\n                                pass\n                \n                enhanced['recent_chat'] = recent_chat\n                \n                # Extract the latest agent response\n                for msg in reversed(recent_chat):\n                    if msg.get('role') == 'assistant':\n                        enhanced['latest_agent_response'] = msg.get('content', [])\n                        break\n                        \n            except Exception as e:\n                enhanced['transcript_read_error'] = str(e)\n    \n    # For Stop event, optionally include full chat if requested\n    if event_type == \"Stop\" and 'transcript_path' in event_data:\n        transcript_path = event_data['transcript_path']\n        if os.path.exists(transcript_path):\n            try:\n                chat_data = []\n                with open(transcript_path, 'r') as f:\n                    for line in f:\n                        line = line.strip()\n                        if line:\n                            try:\n                                chat_data.append(json.loads(line))\n                            except json.JSONDecodeError:\n                                pass\n                \n                # Add summary statistics\n                enhanced['chat_summary'] = {\n                    'total_messages': len(chat_data),\n                    'user_messages': sum(1 for msg in chat_data if msg.get('role') == 'user'),\n                    'assistant_messages': sum(1 for msg in chat_data if msg.get('role') == 'assistant'),\n                }\n                # Optionally include last few messages\n                enhanced['last_5_messages'] = chat_data[-5:] if chat_data else []\n                \n            except Exception as e:\n                enhanced['chat_read_error'] = str(e)\n    \n    # Include raw event data for completeness\n    enhanced['raw_data'] = event_data\n    \n    return enhanced\n\ndef log_event(event_type: str, event_data: dict):\n    \"\"\"Log event to file with enhanced data extraction\"\"\"\n    try:\n        log_file = get_log_file_path()\n\n        # Prepare enhanced log entry\n        log_entry = extract_enhanced_data(event_type, event_data)\n\n        # Append to log file (one JSON object per line)\n        with open(log_file, \"a\") as f:\n            f.write(json.dumps(log_entry) + \"\\n\")\n\n        return True\n\n    except Exception as e:\n        # Fail silently to not block Claude Code\n        print(f\"Warning: Failed to log event: {e}\", file=sys.stderr)\n        return False\n\ndef main():\n    parser = argparse.ArgumentParser(description='Log NDP plugin events with enhanced data capture')\n    parser.add_argument('--event-type', required=True, help='Type of event')\n    args = parser.parse_args()\n\n    try:\n        # Read event data from stdin\n        event_data = json.load(sys.stdin)\n    except json.JSONDecodeError:\n        event_data = {}\n\n    # Log the event with enhanced data\n    log_event(args.event_type, event_data)\n\n    # Always exit successfully to not block Claude Code\n    sys.exit(0)\n\nif __name__ == '__main__':\n    main()\n"
      },
      "plugins": [
        {
          "name": "ndp-plugin",
          "source": "./ndp-plugin",
          "description": "National Data Platform (NDP) integration with dataset search, discovery, and workflow automation",
          "version": "1.0.0",
          "keywords": [
            "ndp",
            "dataset-search",
            "scientific-data",
            "ckan",
            "chronolog",
            "workflow-automation"
          ],
          "categories": [
            "chronolog",
            "ckan",
            "dataset-search",
            "ndp",
            "scientific-data",
            "workflow-automation"
          ],
          "install_commands": [
            "/plugin marketplace add SIslamMun/iowarp-plugin",
            "/plugin install ndp-plugin@iowarp-plugins"
          ]
        }
      ]
    }
  ]
}