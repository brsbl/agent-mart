{
  "author": {
    "id": "data-goblin",
    "display_name": "Kurt",
    "type": "User",
    "avatar_url": "https://avatars.githubusercontent.com/u/97248341?u=ae77391625ffda818108918f34078d6de7d14530&v=4",
    "url": "https://github.com/data-goblin",
    "bio": "Just a goblin trying to make it to tomorrow.",
    "stats": {
      "total_marketplaces": 1,
      "total_plugins": 1,
      "total_commands": 1,
      "total_skills": 1,
      "total_stars": 17,
      "total_forks": 1
    }
  },
  "marketplaces": [
    {
      "name": "fabric-cli-plugin",
      "version": null,
      "description": "Skill that teaches Claude to use the Microsoft Fabric CLI (fab).",
      "owner_info": {
        "name": "Kurt Buhler"
      },
      "keywords": [],
      "repo_full_name": "data-goblin/fabric-cli-plugin",
      "repo_url": "https://github.com/data-goblin/fabric-cli-plugin",
      "repo_description": "Claude Code plugin for Microsoft Fabric CLI",
      "homepage": null,
      "signals": {
        "stars": 17,
        "forks": 1,
        "pushed_at": "2026-01-12T06:10:47Z",
        "created_at": "2025-11-27T12:28:08Z",
        "license": "NOASSERTION"
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 600
        },
        {
          "path": "README.md",
          "type": "blob",
          "size": 4437
        },
        {
          "path": "claude-desktop",
          "type": "tree",
          "size": null
        },
        {
          "path": "claude-desktop/README.md",
          "type": "blob",
          "size": 368
        },
        {
          "path": "commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "commands/audit-context.md",
          "type": "blob",
          "size": 4672
        },
        {
          "path": "skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/fabric-cli",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/fabric-cli/SKILL.md",
          "type": "blob",
          "size": 18985
        },
        {
          "path": "skills/fabric-cli/agent-templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/fabric-cli/agent-templates/CLAUDE.md",
          "type": "blob",
          "size": 1629
        },
        {
          "path": "skills/fabric-cli/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/fabric-cli/references/admin.md",
          "type": "blob",
          "size": 5162
        },
        {
          "path": "skills/fabric-cli/references/create-workspaces.md",
          "type": "blob",
          "size": 629
        },
        {
          "path": "skills/fabric-cli/references/fab-api.md",
          "type": "blob",
          "size": 5648
        },
        {
          "path": "skills/fabric-cli/references/notebooks.md",
          "type": "blob",
          "size": 16933
        },
        {
          "path": "skills/fabric-cli/references/querying-data.md",
          "type": "blob",
          "size": 1274
        },
        {
          "path": "skills/fabric-cli/references/quickstart.md",
          "type": "blob",
          "size": 12774
        },
        {
          "path": "skills/fabric-cli/references/reference.md",
          "type": "blob",
          "size": 25772
        },
        {
          "path": "skills/fabric-cli/references/reports.md",
          "type": "blob",
          "size": 5580
        },
        {
          "path": "skills/fabric-cli/references/semantic-models.md",
          "type": "blob",
          "size": 16252
        },
        {
          "path": "skills/fabric-cli/references/workspaces.md",
          "type": "blob",
          "size": 13313
        },
        {
          "path": "skills/fabric-cli/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/fabric-cli/scripts/README.md",
          "type": "blob",
          "size": 2546
        },
        {
          "path": "skills/fabric-cli/servers",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/fabric-cli/servers/win32",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/fabric-cli/servers/win32/Resources",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/fabric-cli/servers/win32/Resources/calendar_instructions_and_examples.md",
          "type": "blob",
          "size": 7900
        },
        {
          "path": "skills/fabric-cli/servers/win32/Resources/dax_query_instructions_and_examples.md",
          "type": "blob",
          "size": 21388
        },
        {
          "path": "skills/fabric-cli/servers/win32/Resources/dax_udf_instructions_and_examples.md",
          "type": "blob",
          "size": 8369
        },
        {
          "path": "skills/fabric-cli/servers/win32/Resources/powerbi_project_instructions.md",
          "type": "blob",
          "size": 2754
        }
      ],
      "files": {
        ".claude-plugin/marketplace.json": "{\r\n  \"name\": \"fabric-cli-plugin\",\r\n  \"owner\": {\r\n    \"name\": \"Kurt Buhler\"\r\n  },\r\n  \"metadata\": {\r\n    \"description\": \"Skill that teaches Claude to use the Microsoft Fabric CLI (fab).\",\r\n    \"version\": \"0.2.0\"\r\n  },\r\n  \"plugins\": [\r\n    {\r\n      \"name\": \"fabric-cli-plugin\",\r\n      \"description\": \"Fabric CLI context and resources for effective use of the Fabric CLI with coding agents.\",\r\n      \"source\": \"./\",\r\n      \"strict\": false,\r\n      \"skills\": [\r\n        \"./skills/fabric-cli\"\r\n      ],\r\n      \"commands\": [\r\n        \"./commands\"\r\n      ],\r\n      \"mcpServers\": \"./.mcp.json\"\r\n    }\r\n  ]\r\n}\r\n",
        "README.md": "<h1 align=\"center\">fabric-cli-plugin</h1>\r\n\r\n<p align=\"center\">\r\n  Skill and MCP server for Microsoft Fabric CLI\r\n</p>\r\n\r\n<p align=\"center\">\r\n  <img src=\"https://img.shields.io/badge/version-0.2.0-blue\" alt=\"Version\">\r\n  <img src=\"https://img.shields.io/badge/uv-required-purple\" alt=\"uv required\">\r\n  <img src=\"https://img.shields.io/badge/fab_cli-required-orange\" alt=\"Fab CLI\">\r\n  <img src=\"https://img.shields.io/badge/license-MIT-green\" alt=\"License\">\r\n</p>\r\n\r\n---\r\n\r\n## Claude Code\r\n\r\n```bash\r\n# 1. Add marketplace source\r\n/plugin marketplace add data-goblin/fabric-cli-plugin\r\n\r\n# 2. Install the plugin\r\n/plugin install fabric-cli-plugin@data-goblin\r\n```\r\n\r\nOr: `/plugin` > Browse Plugins > Select marketplace\r\n\r\n<!-- BEGIN CLAUDE_DESKTOP -->\r\n## Claude Desktop\r\n\r\n### MCP Servers\r\n\r\n> [!WARNING]\r\n> Agents with a `Bash` tool like Claude Code don't need MCP servers.\r\n> MCP servers allow agents to execute operations that may modify your Fabric environment.\r\n> Use with caution.\r\n\r\n| Server | Description |\r\n|--------|-------------|\r\n| [`fabric-cli-mcp.mcpb`](claude-desktop/fabric-cli-mcp.mcpb) | 17 tools for Fabric workspace and item operations |\r\n| [`powerbi-modeling-mcp.mcpb`](claude-desktop/powerbi-modeling-mcp.mcpb) | Semantic model operations (Windows only) - repackaged from [Microsoft](https://github.com/microsoft/powerbi-modeling-mcp) |\r\n\r\n1. Download the `.mcpb` file\r\n2. Open Claude Desktop > **Settings** > **Extensions** > drag and drop the `.mcpb`\r\n3. Read, review, and click **Install**\r\n4. *(To remove)* Settings > Extensions > (...) > Uninstall\r\n\r\n<a href=\"docs/remove-fabric-cli-mcp.png\"><img src=\"docs/remove-fabric-cli-mcp.png\" alt=\"remove-fabric-cli-mcp\" width=\"650\"></a>\r\n\r\n### Skill (Fabric CLI context)\r\n\r\n1. Download [`fabric-cli-skill.zip`](claude-desktop/fabric-cli-skill.zip)\r\n2. Open Claude Desktop > **Settings** > **Capabilities** > **Skills** > *Upload skill*\r\n3. Click **Upload skill** > select the ZIP\r\n4. *(To remove)* (Reopen) Settings > Skills > fabric-cli (...) > Delete\r\n\r\n<a href=\"docs/remove-fabric-cli-skill.png\"><img src=\"docs/remove-fabric-cli-skill.png\" alt=\"remove-fabric-cli-skill\" width=\"650\"></a>\r\n\r\nRequires: Pro, Max, Team, or Enterprise plan\r\n\r\n[Extensions Docs](https://docs.anthropic.com/en/docs/claude-desktop/extensions) | [Skills Docs](https://support.claude.com/en/articles/12512198-how-to-create-custom-skills)\r\n<!-- END CLAUDE_DESKTOP -->\r\n\r\n## Requirements\r\n\r\n- [uv](https://docs.astral.sh/uv/) for MCP server\r\n  - macOS: `brew install uv`\r\n  - Windows: `winget install astral-sh.uv`\r\n- [fab CLI](https://pypi.org/project/fabric-cli/) authenticated via `fab auth login`\r\n- Windows: may need `PYTHONIOENCODING=utf-8` with some fab commands\r\n\r\n## Testing\r\n\r\n- **macOS** (15.6.1): Claude Code 2.0.55, Claude Desktop 1.0.1217 - 2025-11-27\r\n- **Windows** (11 25H2): Claude Code 2.0.55, Claude Desktop 1.0.1217 - 2025-11-27\r\n\r\n## What's Included\r\n\r\n- **Skill**: Fab CLI guidance and patterns\r\n- **Commands**: `/audit-context` for reviewing project context files\r\n- **Agent Template**: CLAUDE.md template for Power BI/Fabric projects\r\n- **Fabric CLI MCP**: 17 tools for Fabric operations (Claude Desktop only)\r\n- **Power BI Modeling MCP**: Semantic model operations (Windows only)\r\n- **Microsoft Docs MCP**: Search official Microsoft/Azure documentation\r\n\r\n## Additional Resources\r\n\r\nThe following resources aren't included but might be useful:\r\n\r\n- [DAX Performance Tuner MCP](https://github.com/microsoft/fabric-toolbox/tree/main/tools/DAXPerformanceTunerMCPServer) - AI-assisted DAX query optimization (requires build)\r\n\r\n<br>\r\n\r\n---\r\n\r\n<p align=\"center\">\r\n  <em>All code and context are shared without any warranty or guarantees for any fellow</em><img src=\"docs/claude-goblins.svg\" alt=\"Claude Goblins\" height=\"20\" style=\"vertical-align: middle;\"><em>using Microsoft Fabric.</em>\r\n</p>\r\n\r\n<p align=\"center\">\r\n  <em>Built with assistance from <a href=\"https://claude.ai/claude-code\">Claude Code</a> donkeys. AI-generated code has been reviewed but may contain errors. Use at your own risk.</p>\r\n\r\n<p align=\"center\">\r\n  Context files are human-written and revised by Claude Code after iterative use. No AI-generated creative assets used.</em>\r\n</p>\r\n\r\n---\r\n\r\n<p align=\"center\">\r\n  <a href=\"https://github.com/data-goblin\">Kurt Buhler</a> · <a href=\"https://data-goblins.com\">Data Goblins</a> · part of <a href=\"https://tabulareditor.com\">Tabular Editor</a>\r\n</p>\r\n\r\n",
        "claude-desktop/README.md": "# Claude Desktop Assets\r\n\r\nSee [main README](../README.md#claude-desktop) for installation instructions.\r\n\r\n## Files\r\n\r\n| File | Description |\r\n|------|-------------|\r\n| `fabric-cli-mcp.mcpb` | Fabric CLI MCP server (17 tools) |\r\n| `powerbi-modeling-mcp.mcpb` | Power BI Modeling MCP (Windows only) |\r\n| `fabric-cli-skill.zip` | Fabric CLI skill for Claude Desktop |\r\n",
        "commands/audit-context.md": "---\r\ndescription: Review project context and agent configuration files (CLAUDE.md, agents.md, memory files, and others) to provide feedback and identify opportunities for improvement.\r\nargument-hint: [additional instructions]\r\nmodel: sonnet\r\n---\r\n\r\n# Audit context\r\n\r\nAudit the project's context configuration files with additional instructions $ARGUMENTS\r\n\r\nDo not modify any files. Use your `Explore` subagent if there are more than three markdowns to read.\r\n\r\n## Agent configuration and memory files\r\n\r\nSearch for and read:\r\n\r\n1. `CLAUDE.md` files (root, .claude/, nested directories)\r\n2. `AGENTS.md` or similar agent configuration files\r\n3. `.claude/settings.json` and `.claude/settings.local.json`\r\n4. Any `SKILL.md` files in the project\r\n\r\nFor each file found, report:\r\n\r\n1. Are memory and agent configuration files sufficiently short (ideally less than 500 lines) or too short (<50 lines)?\r\n2. Are the context files relevant and up-to-date for the current project?\r\n3. Do they mention when to automatically load the skills that are available, like `fabric-cli` or others?\r\n4. Are they well-structured and concise, with headings, lists, examples, and commands?\r\n5. Are they personalized for the project or generic?\r\n6. Do they mention standard conventions like:\r\n    - Recommending conciseness\r\n    - Avoiding sycophancy or agreement and favoring critique and pushback within reason\r\n    - Avoiding emojis in all responses\r\n    - Formatting data with ASCII tables\r\n    - When to ask for clarification\r\n    - Where to get trustworthy information with WebSearch and Fetch\r\n    - Avoiding excessive commenting in generated code\r\n    - Following a strict separation of concerns with liberal composition of all code files produced\r\n    - Documenting all tasks in a .claude/scratchpads/ directory and searching that directory before starting a new task\r\n    - Creating any new files ONLY in the project tmp/ unless specified otherwise\r\n    - Not creating test scripts or markdown summaries unless explicitly asked to\r\n    - Never using TODOs, partial implementations, or stubs\r\n    - Never saying \"You're absolutely right!\"\r\n    - Never providing time estimates or talking about work in \"Phases\" or \"Parts\" unless explicitly asked to\r\n    - When to use subagents, slash commands, and MCP servers, if any are configured\r\n    - How to avoid assumptions, particularly about how something works based on limited test cases\r\n7. Are they absent, too little, or clearly AI-generated (risking \"context rot\")?\r\n8. Do they follow \"progressive disclosure\" with reference of other relevant files, if they exist?\r\n\r\n## Other project context\r\n\r\nSearch for other context like `spec.md` or `task.md`, or other markdown, txt, yaml, or similar files that provide context, and evaluate:\r\n\r\n- Are they sufficiently well structured and composed?\r\n- Are they concise; providing exactly sufficient information to complete a task or implementation and no more, no less?\r\n- Are they human-written or do they seem AI-generated?\r\n- Is there a clear design direction?\r\n- Are they outdated or irrelevant to the current tasks, or are they still relevant or helpful?\r\n\r\n## Output\r\n\r\nWhen complete, provide the user a concise summary with recommendations about how to improve their context. Focus on recommendations that follow Anthropic's documentation and best practices. Retrieve and read the following articles, and evaluate the context with this input:\r\n\r\n- [Context engineering](https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents)\r\n- [Best practices for skills](https://platform.claude.com/docs/en/agents-and-tools/agent-skills/best-practices)\r\n- [Prompt engineering guide](https://platform.claude.com/docs/en/build-with-claude/prompt-engineering/overview)\r\n- [Long context tips](https://platform.claude.com/docs/en/build-with-claude/prompt-engineering/long-context-tips)\r\n- [Memory management](https://code.claude.com/docs/en/memory)\r\n- [Subagents](https://code.claude.com/docs/en/sub-agents)\r\n\r\nRemind the user that context evaluation is ambiguous unless they have evaluations (evals) with specific, re-usable prompts that they can run multiple times to assess the performance of the agent. Remind the user that agent- or AI-generated context has a high probability of being inappropriate, verbose, or even incorrect. Remind them of the risk of attractor basins/sinks based on agent training data or existing memory files, and hermeneutic reasoning circles that can arise in context and funnel outputs in unproductive directions. Remind users that creating good context is an important but iterative skill that is essential to get good outputs from AI and agents.\r\n",
        "skills/fabric-cli/SKILL.md": "---\nname: fabric-cli\ndescription: Use Microsoft Fabric CLI (fab) to manage workspaces, semantic models, reports, notebooks, lakehouses, and other Fabric resources via file-system metaphor and commands. Use when deploying Fabric items, running jobs, querying data, managing OneLake files, or automating Fabric operations. Invoke this skill automatically whenever a user mentions the Fabric CLI, fab, or Fabric.\n---\n\n# Microsoft Fabric CLI Operations\n\n> **Note:** If you have access to a Bash tool (e.g., Claude Code), execute `fab` commands directly via Bash rather than using an MCP server.\n\nExpert guidance for using the `fab` CLI to programmatically manage Fabric\n\n## When to Use This Skill\n\nActivate automatically when tasks involve:\n\n- Mention of the Fabric CLI, Fabric items, Power BI, `fab`, or `fab` commands\n- Managing workspaces, items, or resources\n- Deploying or migrating semantic models, reports, notebooks, pipelines\n- Running or scheduling jobs (notebooks, pipelines, Spark)\n- Working with lakehouse/warehouse tables and data\n- Using the Fabric, Power BI, or OneLake APIs\n- Automating Fabric operations in scripts\n\n## Critical\n\n- Before first use, ask the user if they have Fabric admin access, any API restrictions, or preferences for Fabric/Power BI API usage\n- Remind the user to add their Fabric access level and preferences to their agent memory files (e.g., CLAUDE.md) for future sessions\n- If workspace or item name is unclear, ask the user first, then verify with `fab ls` or `fab exists` before proceeding\n- The first time you use `fab` run `fab auth status` to make sure the user is authenticated. If not, ask the user to run `fab auth login` to login\n- Always use `fab --help` and `fab <command> --help` the first time you use a command to understand its syntax, first\n- Always try the simple `fab` command alone, first before piping it\n- Always use `-f` when executing command if the flag is available to do so non-interactively\n- Ensure that you avoid removing or moving items, workspaces, or definitions, or changing properties without explicit user direction\n- If a command is blocked in your permissions and you try to use it, stop and ask the user for clarification; never try to circumvent it\n- Use `fab` in non-interactive mode. Interactive mode doesn't work with coding agents\n\n## First Run\n\n```bash\nfab auth login          # Authenticate (opens browser)\nfab auth status         # Verify authentication\nfab ls                  # List your workspaces\nfab ls \"Name.Workspace\" # List items in a workspace\n```\n\n## Variable Extraction Pattern\n\nMost workflows need IDs. Extract them like this:\n\n```bash\nWS_ID=$(fab get \"ws.Workspace\" -q \"id\" | tr -d '\"')\nMODEL_ID=$(fab get \"ws.Workspace/Model.SemanticModel\" -q \"id\" | tr -d '\"')\n\n# Then use in API calls\nfab api -A powerbi \"groups/$WS_ID/datasets/$MODEL_ID/refreshes\" -X post -i '{\"type\":\"Full\"}'\n```\n\n## Quick Start\n\n**New to Fabric CLI?** Here are some references you can read:\n\n- [Quick Start Guide](./references/quickstart.md) - Copy-paste examples\n- [Querying Data](./references/querying-data.md) - Query semantic models and lakehouse tables\n- [Semantic Models](./references/semantic-models.md) - TMDL, DAX, refresh, storage mode\n- [Reports](./references/reports.md) - Export, import, visuals, fields\n- [Notebooks](./references/notebooks.md) - Job execution, parameters\n- [Workspaces](./references/workspaces.md) - Create, manage, permissions\n- [Full Command Reference](./references/reference.md) - All commands detailed\n- [Command Reference Table](#command-reference) - At-a-glance command syntax\n\n### Command Reference\n\n| Command | Purpose | Example |\n|---------|---------|---------|\n| **Finding Items** |||\n| `fab ls` | List items | `fab ls \"Sales.Workspace\"` |\n| `fab ls -l` | List with details | `fab ls \"Sales.Workspace\" -l` |\n| `fab exists` | Check if exists | `fab exists \"Sales.Workspace/Model.SemanticModel\"` |\n| `fab get` | Get item details | `fab get \"Sales.Workspace/Model.SemanticModel\"` |\n| `fab get -q` | Query specific field | `fab get \"Sales.Workspace\" -q \"id\"` |\n| **Definitions** |||\n| `fab get -q \"definition\"` | Get full definition | `fab get \"ws.Workspace/Model.SemanticModel\" -q \"definition\"` |\n| `fab export` | Export to local | `fab export \"ws.Workspace/Nb.Notebook\" -o ./backup` |\n| `fab import` | Import from local | `fab import \"ws.Workspace/Nb.Notebook\" -i ./backup/Nb.Notebook` |\n| **Running Jobs** |||\n| `fab job run` | Run synchronously | `fab job run \"ws.Workspace/ETL.Notebook\"` |\n| `fab job start` | Run asynchronously | `fab job start \"ws.Workspace/ETL.Notebook\"` |\n| `fab job run -P` | Run with params | `fab job run \"ws.Workspace/Nb.Notebook\" -P date:string=2025-01-01` |\n| `fab job run-list` | List executions | `fab job run-list \"ws.Workspace/Nb.Notebook\"` |\n| `fab job run-status` | Check status | `fab job run-status \"ws.Workspace/Nb.Notebook\" --id <job-id>` |\n| **Refreshing Models** |||\n| `fab api -A powerbi` | Trigger refresh | `fab api -A powerbi \"groups/<ws-id>/datasets/<model-id>/refreshes\" -X post -i '{\"type\":\"Full\"}'` |\n| `fab api -A powerbi` | Check refresh status | `fab api -A powerbi \"groups/<ws-id>/datasets/<model-id>/refreshes?\\$top=1\"` |\n| **DAX Queries** |||\n| `fab get -q \"definition\"` | Get model schema first | `fab get \"ws.Workspace/Model.SemanticModel\" -q \"definition\"` |\n| `fab api -A powerbi` | Execute DAX | `fab api -A powerbi \"groups/<ws-id>/datasets/<model-id>/executeQueries\" -X post -i '{\"queries\":[{\"query\":\"EVALUATE...\"}]}'` |\n| **Lakehouse** |||\n| `fab ls` | Browse files/tables | `fab ls \"ws.Workspace/LH.Lakehouse/Files\"` |\n| `fab table schema` | Get table schema | `fab table schema \"ws.Workspace/LH.Lakehouse/Tables/sales\"` |\n| `fab cp` | Upload/download | `fab cp ./local.csv \"ws.Workspace/LH.Lakehouse/Files/\"` |\n| **Management** |||\n| `fab cp` | Copy items | `fab cp \"dev.Workspace/Item.Type\" \"prod.Workspace\" -f` |\n| `fab set` | Update properties | `fab set \"ws.Workspace/Item.Type\" -q displayName -i \"New Name\"` |\n| `fab rm` | Delete item | `fab rm \"ws.Workspace/Item.Type\" -f` |\n\n## Core Concepts\n\n### Path Format\n\nFabric uses filesystem-like paths with type extensions:\n\n`/WorkspaceName.Workspace/ItemName.ItemType`\n\nFor lakehouses this is extended into files and tables:\n\n`/WorkspaceName.Workspace/LakehouseName.Lakehouse/Files/FileName.extension` or `/WorkspaceName.Workspace/LakehouseName.Lakehouse/Tables/TableName`\n\nFor Fabric capacities you have to use `fab ls .capacities`\n\nExamples:\n\n- `\"/Production.Workspace/Sales Model.SemanticModel\"`\n- `/Data.Workspace/MainLH.Lakehouse/Files/data.csv`\n- `/Data.Workspace/MainLH.Lakehouse/Tables/dbo/customers`\n\n### Common Item Types\n\n- `.Workspace` - Workspaces\n- `.SemanticModel` - Power BI datasets\n- `.Report` - Power BI reports\n- `.Notebook` - Fabric notebooks\n- `.DataPipeline` - Data pipelines\n- `.Lakehouse` / `.Warehouse` - Data stores\n- `.SparkJobDefinition` - Spark jobs\n\nFull list: 35+ types. Use `fab desc .<ItemType>` to explore.\n\n## Essential Commands\n\n### Navigation & Discovery\n\n```bash\n# List resources\nfab ls                                    # List workspaces\nfab ls \"Production.Workspace\"             # List items in workspace\nfab ls \"Production.Workspace\" -l          # Detailed listing\nfab ls \"Data.Workspace/LH.Lakehouse\"      # List lakehouse contents\n\n# Check existence\nfab exists \"Production.Workspace/Sales.SemanticModel\"\n\n# Get details\nfab get \"Production.Workspace/Sales.Report\"\nfab get \"Production.Workspace\" -q \"id\"    # Query with JMESPath\n```\n\n### Creating & Managing Resources\n\n```bash\n# Create workspace after using `fab ls .capacities` to check capacities\nfab mkdir \"NewWorkspace.Workspace\" -P capacityname=MyCapacity\n\n# Create items\nfab mkdir \"Production.Workspace/NewLakehouse.Lakehouse\"\nfab mkdir \"Production.Workspace/Pipeline.DataPipeline\"\n\n# Update properties\nfab set \"Production.Workspace/Item.Notebook\" -q displayName -i \"New Name\"\nfab set \"Production.Workspace\" -q description -i \"Production environment\"\n```\n\n### Copy, Move, Export, Import\n\n```bash\n# Copy between workspaces\nfab cp \"Dev.Workspace/Pipeline.DataPipeline\" \"Production.Workspace\"\nfab cp \"Dev.Workspace/Report.Report\" \"Production.Workspace/ProdReport.Report\"\n\n# Export to local\nfab export \"Production.Workspace/Model.SemanticModel\" -o /tmp/exports\nfab export \"Production.Workspace\" -o /tmp/backup -a  # Export all items\n\n# Import from local\nfab import \"Production.Workspace/Pipeline.DataPipeline\" -i /tmp/exports/Pipeline.DataPipeline -f\n\n# IMPORTANT: Use -f flag for non-interactive execution\n# Without -f, import/export operations expect an interactive terminal for confirmation\n# This will fail in scripts, automation, or when stdin is not a terminal\nfab import \"ws.Workspace/Item.Type\" -i ./Item.Type -f  # Required for scripts\n```\n\n### API Operations\n\nDirect REST API access with automatic authentication.\n\n**Audiences:**\n\n- `fabric` (default) - Fabric REST API\n- `powerbi` - Power BI REST API\n- `storage` - OneLake Storage API\n- `azure` - Azure Resource Manager\n\n```bash\n# Fabric API (default)\nfab api workspaces\nfab api workspaces -q \"value[?type=='Workspace']\"\nfab api \"workspaces/<workspace-id>/items\"\n\n# Power BI API (for DAX queries, dataset operations)\nfab api -A powerbi groups\nfab api -A powerbi \"datasets/<model-id>/executeQueries\" -X post -i '{\"queries\": [{\"query\": \"EVALUATE VALUES(Date[Year])\"}]}'\n\n# POST/PUT/DELETE\nfab api -X post \"workspaces/<ws-id>/items\" -i '{\"displayName\": \"New Item\", \"type\": \"Lakehouse\"}'\nfab api -X put \"workspaces/<ws-id>/items/<item-id>\" -i /tmp/config.json\nfab api -X delete \"workspaces/<ws-id>/items/<item-id>\"\n\n# OneLake Storage API\nfab api -A storage \"WorkspaceName.Workspace/LH.Lakehouse/Files\" -P resource=filesystem,recursive=false\n```\n\n### Job Management\n\n```bash\n# Run synchronously (wait for completion)\nfab job run \"Production.Workspace/ETL.Notebook\"\nfab job run \"Production.Workspace/Pipeline.DataPipeline\" --timeout 300\n\n# Run with parameters\nfab job run \"Production.Workspace/ETL.Notebook\" -P date:string=2024-01-01,batch:int=1000,debug:bool=false\n\n# Start asynchronously\nfab job start \"Production.Workspace/LongProcess.Notebook\"\n\n# Monitor\nfab job run-list \"Production.Workspace/ETL.Notebook\"\nfab job run-status \"Production.Workspace/ETL.Notebook\" --id <job-id>\n\n# Schedule\nfab job run-sch \"Production.Workspace/Pipeline.DataPipeline\" --type daily --interval 10:00,16:00 --start 2024-11-15T09:00:00 --enable\nfab job run-sch \"Production.Workspace/Pipeline.DataPipeline\" --type weekly --interval 10:00 --days Monday,Friday --enable\n\n# Cancel\nfab job run-cancel \"Production.Workspace/ETL.Notebook\" --id <job-id>\n```\n\n### Table Operations\n\n```bash\n# View schema\nfab table schema \"Data.Workspace/LH.Lakehouse/Tables/dbo/customers\"\n\n# Load data (non-schema lakehouses only)\nfab table load \"Data.Workspace/LH.Lakehouse/Tables/sales\" --file \"Data.Workspace/LH.Lakehouse/Files/daily_sales.csv\" --mode append\n\n# Optimize (lakehouses only)\nfab table optimize \"Data.Workspace/LH.Lakehouse/Tables/transactions\" --vorder --zorder customer_id,region\n\n# Vacuum (lakehouses only)\nfab table vacuum \"Data.Workspace/LH.Lakehouse/Tables/temp_data\" --retain_n_hours 48\n```\n\n## Common Workflows\n\n### Semantic Model Management\n\n```bash\n# Find models\nfab ls \"ws.Workspace\" | grep \".SemanticModel\"\n\n# Get definition\nfab get \"ws.Workspace/Model.SemanticModel\" -q definition\n\n# Trigger refresh\nfab api -A powerbi \"groups/$WS_ID/datasets/$MODEL_ID/refreshes\" -X post -i '{\"type\":\"Full\"}'\n\n# Check refresh status\nfab api -A powerbi \"groups/$WS_ID/datasets/$MODEL_ID/refreshes?\\$top=1\"\n```\n\n**Execute DAX:**\n\n```bash\nfab api -A powerbi \"groups/$WS_ID/datasets/$MODEL_ID/executeQueries\" -X post \\\n  -i '{\"queries\":[{\"query\":\"EVALUATE TOPN(5, '\\''TableName'\\'')\"}]}'\n```\n\n**DAX rules:** EVALUATE required, single quotes around tables (`'Sales'`), qualify columns (`'Sales'[Amount]`).\n\nFor full details: [semantic-models.md](./references/semantic-models.md) | [querying-data.md](./references/querying-data.md)\n\n### Report Operations\n\n```bash\n# Get report definition\nfab get \"ws.Workspace/Report.Report\" -q definition\n\n# Export to local\nfab export \"ws.Workspace/Report.Report\" -o /tmp/exports -f\n\n# Import from local\nfab import \"ws.Workspace/Report.Report\" -i /tmp/exports/Report.Report -f\n\n# Rebind to different model\nfab set \"ws.Workspace/Report.Report\" -q semanticModelId -i \"<new-model-id>\"\n```\n\nFor full details: [reports.md](./references/reports.md)\n\n### Lakehouse/Warehouse Operations\n\n```bash\n# Browse contents\nfab ls \"Data.Workspace/LH.Lakehouse/Files\"\nfab ls \"Data.Workspace/LH.Lakehouse/Tables/dbo\"\n\n# Upload/download files\nfab cp ./local-data.csv \"Data.Workspace/LH.Lakehouse/Files/data.csv\"\nfab cp \"Data.Workspace/LH.Lakehouse/Files/data.csv\" ~/Downloads/\n\n# Load and optimize tables\nfab table load \"Data.Workspace/LH.Lakehouse/Tables/sales\" --file \"Data.Workspace/LH.Lakehouse/Files/sales.csv\"\nfab table optimize \"Data.Workspace/LH.Lakehouse/Tables/sales\" --vorder --zorder customer_id\n```\n\n### Environment Migration\n\n```bash\n# Export from dev\nfab export \"Dev.Workspace\" -o /tmp/migration -a\n\n# Import to production (item by item)\nfab import \"Production.Workspace/Pipeline.DataPipeline\" -i /tmp/migration/Pipeline.DataPipeline\nfab import \"Production.Workspace/Report.Report\" -i /tmp/migration/Report.Report\n```\n\n## Cross-Workspace Search\n\n### DataHub V2 API (Recommended)\n\nUse `scripts/search_across_workspaces.py` for cross-workspace search with rich metadata not available elsewhere:\n\n```bash\n# Find all semantic models (use \"Model\" not \"SemanticModel\")\npython3 scripts/search_across_workspaces.py --type Model\n\n# Find models by name\npython3 scripts/search_across_workspaces.py --type Model --filter \"Sales\"\n\n# Find stale items (not visited in 6+ months)\npython3 scripts/search_across_workspaces.py --type Model --not-visited-since 2024-06-01\n\n# Find items by owner\npython3 scripts/search_across_workspaces.py --type PowerBIReport --owner \"kurt\"\n\n# Find Direct Lake models only\npython3 scripts/search_across_workspaces.py --type Model --storage-mode directlake\n\n# Find items in workspace\npython3 scripts/search_across_workspaces.py --type Lakehouse --workspace \"fit-data\"\n\n# Get JSON output\npython3 scripts/search_across_workspaces.py --type Model --output json\n\n# Sort by last visited (oldest first)\npython3 scripts/search_across_workspaces.py --type Model --sort last-visited --sort-order asc\n\n# List all available types\npython3 scripts/search_across_workspaces.py --list-types\n```\n\n**Unique DataHub fields** (not available via fab api or admin APIs):\n\n- `lastVisitedTimeUTC` - When item was last opened/used\n- `storageMode` - Import, DirectQuery, or DirectLake\n- `ownerUser` - Full owner details (name, email)\n- `capacitySku` - F2, F64, PP, etc.\n- `isDiscoverable` - Whether item appears in search\n\n**Important type mappings:**\n\n- Semantic models: use `--type Model` (not SemanticModel)\n- Dataflows: use `--type DataFlow` (capital F)\n- Notebooks: use `--type SynapseNotebook`\n\n### Admin APIs (Requires Admin Role)\n\nIf you have Fabric/Power BI admin access:\n\n```bash\n# Find semantic models by name (cross-workspace)\nfab api \"admin/items\" -P \"type=SemanticModel\" -q \"itemEntities[?contains(name, 'Sales')]\"\n\n# Find all notebooks\nfab api \"admin/items\" -P \"type=Notebook\" -q \"itemEntities[].{name:name,workspace:workspaceId}\"\n\n# Find all lakehouses\nfab api \"admin/items\" -P \"type=Lakehouse\"\n\n# Common types: SemanticModel, Report, Notebook, Lakehouse, Warehouse, DataPipeline, Ontology\n```\n\nFor full admin API reference: [admin.md](./references/admin.md)\n\n## Key Patterns\n\n### JMESPath Queries\n\nFilter and transform JSON responses with `-q`:\n\n```bash\n# Get single field\n-q \"id\"\n-q \"displayName\"\n\n# Get nested field\n-q \"properties.sqlEndpointProperties\"\n-q \"definition.parts[0]\"\n\n# Filter arrays\n-q \"value[?type=='Lakehouse']\"\n-q \"value[?contains(name, 'prod')]\"\n\n# Get first element\n-q \"value[0]\"\n-q \"definition.parts[?path=='model.tmdl'] | [0]\"\n```\n\n### Error Handling & Debugging\n\n```bash\n# Show response headers\nfab api workspaces --show_headers\n\n# Verbose output\nfab get \"Production.Workspace/Item\" -v\n\n# Save responses for debugging\nfab api workspaces -o /tmp/workspaces.json\n```\n\n### Performance Optimization\n\n1. **Use `ls` for fast listing** - Much faster than `get`\n2. **Use `exists` before operations** - Check before get/modify\n3. **Filter with `-q`** - Get only what you need\n4. **Use GUIDs in automation** - More stable than names\n\n## Common Flags\n\n- `-f, --force` - Skip confirmation prompts\n- `-v, --verbose` - Verbose output\n- `-l` - Long format listing\n- `-a` - Show hidden items\n- `-o, --output` - Output file path\n- `-i, --input` - Input file or JSON string\n- `-q, --query` - JMESPath query\n- `-P, --params` - Parameters (key=value)\n- `-X, --method` - HTTP method (get/post/put/delete/patch)\n- `-A, --audience` - API audience (fabric/powerbi/storage/azure)\n- `--show_headers` - Show response headers\n- `--timeout` - Timeout in seconds\n\n## Important Notes\n\n- **All examples assume `fab` is installed and authenticated**\n- **Paths require proper extensions** (`.Workspace`, `.SemanticModel`, etc.)\n- **Quote paths with spaces**: `\"My Workspace.Workspace\"`\n- **Use `-f` for non-interactive scripts** (skips prompts)\n- **Semantic model updates**: Use Power BI API (`-A powerbi`) for DAX queries and dataset operations\n\n## Need More Details?\n\nFor specific item type help:\n\n```bash\nfab desc .<ItemType>\n```\n\nFor command help:\n\n```bash\nfab --help\nfab <command> --help\n```\n\n## References\n\n**Skill references:**\n\n- [Querying Data](./references/querying-data.md) - Query semantic models and lakehouse tables\n- [Semantic Models](./references/semantic-models.md) - TMDL, DAX, refresh, storage mode\n- [Reports](./references/reports.md) - Export, import, visuals, fields\n- [Notebooks](./references/notebooks.md) - Job execution, parameters\n- [Workspaces](./references/workspaces.md) - Create, manage, permissions\n- [Admin APIs](./references/admin.md) - Cross-workspace search, tenant operations, governance\n- [API Reference](./references/fab-api.md) - Capacities, gateways, pipelines, domains, dataflows, apps\n- [Full Command Reference](./references/reference.md) - All commands detailed\n- [Quick Start Guide](./references/quickstart.md) - Copy-paste examples\n\n**External references** (request markdown when possible):\n\n- fab CLI: [GitHub Source](https://github.com/microsoft/fabric-cli) | [Docs](https://microsoft.github.io/fabric-cli/)\n- Microsoft: [Fabric CLI Learn](https://learn.microsoft.com/en-us/rest/api/fabric/articles/fabric-command-line-interface)\n- APIs: [Fabric API](https://learn.microsoft.com/en-us/rest/api/fabric/articles/) | [Power BI API](https://learn.microsoft.com/en-us/rest/api/power-bi/)\n- DAX: [dax.guide](https://dax.guide/) - use `dax.guide/<function>/` e.g. `dax.guide/addcolumns/`\n- Power Query: [powerquery.guide](https://powerquery.guide/) - use `powerquery.guide/function/<function>`\n- [Power Query Best Practices](https://learn.microsoft.com/en-us/power-query/best-practices)\n",
        "skills/fabric-cli/agent-templates/CLAUDE.md": "---\r\ntemplate: true\r\ndescription: Starting point for Power BI or Fabric projects. Copy as CLAUDE.md or append to existing CLAUDE.md / agents.md.\r\n---\r\n\r\n# Power BI / Fabric Project\r\n\r\n## Automatic Skills\r\n\r\nAlways invoke `fabric-cli` skill when:\r\n\r\n- Working with Power BI reports, semantic models, or datasets\r\n- Managing Fabric workspaces, lakehouses, notebooks, pipelines\r\n- Running `fab` CLI commands or discussing Fabric APIs\r\n- Answering questions about Power BI or Microsoft Fabric\r\n\r\n## Fabric CLI Usage\r\n\r\n```bash\r\nfab auth status        # Check authentication\r\nfab ls                 # List workspaces\r\nfab ls \"ws.Workspace\"  # List items in workspace\r\nfab --help             # Command reference\r\n```\r\n\r\n## Common Patterns\r\n\r\n- Always use `-f` flag for non-interactive execution\r\n- Quote paths with spaces: `\"My Workspace.Workspace\"`\r\n- Extract IDs for API calls: `fab get \"ws.Workspace\" -q \"id\"`\r\n- Use `-q` for JMESPath queries on output\r\n\r\n## Safety\r\n\r\n- Never remove/move items without explicit permission\r\n- Verify workspace/item names with `fab ls` or `fab exists` before operations\r\n- Check `fab auth status` before first use each session\r\n\r\n## Cross-Workspace Search\r\n\r\nUse `scripts/search_across_workspaces.py` for finding items across workspaces:\r\n\r\n```bash\r\npython3 scripts/search_across_workspaces.py --type Model --filter \"Sales\"\r\npython3 scripts/search_across_workspaces.py --list-types\r\n```\r\n\r\n## References\r\n\r\nSkill includes detailed references for:\r\n\r\n- Semantic models, reports, notebooks, lakehouses\r\n- DAX queries, Power BI API, admin operations\r\n- Full command reference at `fab <command> --help`\r\n",
        "skills/fabric-cli/references/admin.md": "# Admin API Operations\r\n\r\nGuide for Fabric/Power BI admin-level API operations using `fab`. These APIs require admin privileges and enable cross-workspace discovery, tenant-wide operations, and governance.\r\n\r\n## Prerequisites\r\n\r\n- Fabric Admin or Power BI Admin role\r\n- Or delegated admin permissions via service principal\r\n\r\nCheck your access:\r\n\r\n```bash\r\nfab api \"admin/capacities\" 2>&1 | head -5\r\n# If you see results, you have admin access\r\n```\r\n\r\n## Cross-Workspace Item Discovery\r\n\r\nThe admin API is the fastest way to find items across ALL workspaces without iterating.\r\n\r\n### Find Items by Type\r\n\r\n```bash\r\n# Find all semantic models across tenant\r\nfab api \"admin/items\" -P \"type=SemanticModel\"\r\n\r\n# Find all notebooks\r\nfab api \"admin/items\" -P \"type=Notebook\"\r\n\r\n# Find all lakehouses\r\nfab api \"admin/items\" -P \"type=Lakehouse\"\r\n\r\n# Find specific item by name pattern\r\nfab api \"admin/items\" -P \"type=SemanticModel\" -q \"itemEntities[?contains(name, 'Sales')]\"\r\n```\r\n\r\n### Available Item Types\r\n\r\n```\r\nSemanticModel    Report          Dashboard       Notebook\r\nLakehouse        Warehouse       DataPipeline    Dataflow\r\nEnvironment      SparkJobDef     CopyJob         Reflex\r\nOntology         GraphModel      Exploration     OrgApp\r\n```\r\n\r\n### Extract Item Details\r\n\r\n```bash\r\n# Get item IDs and workspace IDs\r\nfab api \"admin/items\" -P \"type=Lakehouse\" -q \"itemEntities[].{name:name,id:id,workspace:workspaceId}\"\r\n\r\n# Find item's workspace name\r\nITEM=$(fab api \"admin/items\" -P \"type=SemanticModel\" -q \"itemEntities[?name=='Sales Model'] | [0]\")\r\nWS_ID=$(echo \"$ITEM\" | jq -r '.workspaceId')\r\nfab api \"admin/workspaces/$WS_ID\" -q \"displayName\"\r\n```\r\n\r\n## Workspace Administration\r\n\r\n### List All Workspaces\r\n\r\n```bash\r\n# All workspaces in tenant\r\nfab api \"admin/workspaces\"\r\n\r\n# Filter by state\r\nfab api \"admin/workspaces\" -q \"workspaces[?state=='Active']\"\r\n\r\n# Get workspace details with users\r\nfab api \"admin/workspaces/<workspace-id>/users\"\r\n```\r\n\r\n### Workspace Governance\r\n\r\n```bash\r\n# Get workspace capacity assignment\r\nfab api \"admin/workspaces/<workspace-id>\" -q \"capacityId\"\r\n\r\n# List workspaces on a capacity\r\nfab api \"admin/capacities/<capacity-id>/workspaces\"\r\n```\r\n\r\n## Capacity Administration\r\n\r\n```bash\r\n# List all capacities\r\nfab api \"admin/capacities\"\r\n\r\n# Get capacity details\r\nfab api \"admin/capacities/<capacity-id>\"\r\n\r\n# Get capacity workloads\r\nfab api \"admin/capacities/<capacity-id>/workloads\"\r\n```\r\n\r\n## Dataset/Model Administration\r\n\r\n```bash\r\n# Get all datasets in tenant (Power BI API)\r\nfab api -A powerbi \"admin/datasets\"\r\n\r\n# Get dataset users\r\nfab api -A powerbi \"admin/datasets/<dataset-id>/users\"\r\n\r\n# Get datasources for a dataset\r\nfab api -A powerbi \"admin/datasets/<dataset-id>/datasources\"\r\n```\r\n\r\n## Report Administration\r\n\r\n```bash\r\n# Get all reports in tenant\r\nfab api -A powerbi \"admin/reports\"\r\n\r\n# Get report users\r\nfab api -A powerbi \"admin/reports/<report-id>/users\"\r\n\r\n# Get reports in a workspace\r\nfab api -A powerbi \"admin/groups/<workspace-id>/reports\"\r\n```\r\n\r\n## Common Patterns\r\n\r\n### Find Item Across Workspaces\r\n\r\n```bash\r\n# Search for a model by name\r\nfab api \"admin/items\" -P \"type=SemanticModel\" \\\r\n  -q \"itemEntities[?contains(name, 'keyword')] | [0].{name:name,id:id,workspace:workspaceId}\"\r\n```\r\n\r\n### Get Full Item Path\r\n\r\n```bash\r\n# Get workspace name + item name for fab path\r\nITEM=$(fab api \"admin/items\" -P \"type=Notebook\" -q \"itemEntities[?name=='ETL Pipeline'] | [0]\")\r\nWS_ID=$(echo \"$ITEM\" | jq -r '.workspaceId')\r\nITEM_NAME=$(echo \"$ITEM\" | jq -r '.name')\r\nWS_NAME=$(fab api \"admin/workspaces/$WS_ID\" -q \"displayName\" | tr -d '\"')\r\necho \"$WS_NAME.Workspace/$ITEM_NAME.Notebook\"\r\n```\r\n\r\n### Audit Item Modifications\r\n\r\n```bash\r\n# Get items modified recently\r\nfab api \"admin/items\" -P \"type=Report\" \\\r\n  -q \"itemEntities | sort_by(@, &lastUpdatedDate) | reverse(@) | [:10]\"\r\n```\r\n\r\n## Security & Governance\r\n\r\n### Get Item Permissions\r\n\r\n```bash\r\n# Dataset permissions\r\nfab api -A powerbi \"admin/datasets/<dataset-id>/users\"\r\n\r\n# Report permissions\r\nfab api -A powerbi \"admin/reports/<report-id>/users\"\r\n\r\n# Workspace permissions\r\nfab api \"admin/workspaces/<workspace-id>/users\"\r\n```\r\n\r\n### Encryption Keys\r\n\r\n```bash\r\n# Get tenant encryption keys\r\nfab api -A powerbi \"admin/tenantKeys\"\r\n```\r\n\r\n## Pagination\r\n\r\nAdmin APIs return paginated results. Check for continuation:\r\n\r\n```bash\r\n# First page\r\nRESULT=$(fab api \"admin/items\" -P \"type=SemanticModel\")\r\n\r\n# Check for more\r\necho \"$RESULT\" | jq '.continuationUri'\r\n\r\n# If not null, fetch next page\r\nfab api \"<continuation-uri>\"\r\n```\r\n\r\n## Error Handling\r\n\r\nCommon admin API errors:\r\n\r\n| Error | Cause | Solution |\r\n|-------|-------|----------|\r\n| 401 | Not authenticated | Run `fab auth login` |\r\n| 403 | Not admin | Request admin role |\r\n| 404 | Item not found | Check item exists |\r\n| 429 | Rate limited | Wait and retry |\r\n\r\n## Best Practices\r\n\r\n1. **Cache results** - Admin APIs can be slow; cache for repeated queries\r\n2. **Use filters** - Always filter by type when possible\r\n3. **Paginate** - Handle continuation for large tenants\r\n4. **Rate limit** - Space out bulk operations\r\n5. **Audit** - Log admin operations for compliance\r\n",
        "skills/fabric-cli/references/create-workspaces.md": "# Creating Workspaces\r\n\r\n## Create Workspace with Large Storage Format\r\n\r\n**Step 1:** List available capacities\r\n\r\n```bash\r\nfab ls .capacities\r\n```\r\n\r\n**Step 2:** Create workspace on chosen capacity\r\n\r\n```bash\r\nfab mkdir \"Workspace Name.Workspace\" -P capacityName=\"MyCapacity\"\r\n```\r\n\r\n**Step 3:** Get workspace ID\r\n\r\n```bash\r\nfab get \"Workspace Name.Workspace\" -q \"id\"\r\n```\r\n\r\n**Step 4:** Set default storage format to Large\r\n\r\n```bash\r\nfab api -A powerbi -X patch \"groups/<workspace-id>\" -i '{\"defaultDatasetStorageFormat\":\"Large\"}'\r\n```\r\n\r\nDone. The workspace now defaults to Large storage format for all new semantic models.\r\n",
        "skills/fabric-cli/references/fab-api.md": "# Fabric API Reference\r\n\r\nDirect API access via `fab api` for operations beyond standard commands.\r\n\r\n## API Basics\r\n\r\n```bash\r\n# Fabric API (default)\r\nfab api \"<endpoint>\"\r\n\r\n# Power BI API\r\nfab api -A powerbi \"<endpoint>\"\r\n\r\n# With query\r\nfab api \"<endpoint>\" -q \"value[0].id\"\r\n\r\n# POST with body\r\nfab api -X post \"<endpoint>\" -i '{\"key\":\"value\"}'\r\n```\r\n\r\n## Capacities\r\n\r\n```bash\r\n# List all capacities\r\nfab api capacities\r\n\r\n# Response includes: id, displayName, sku (F2, F64, FT1, PP3), region, state\r\n```\r\n\r\n**Pause capacity** (cost savings):\r\n\r\n```bash\r\n# CAUTION: Pausing stops all workloads on that capacity\r\n# Resume is intentionally NOT documented - too dangerous for automation\r\n# Use Azure Portal for resume operations\r\n\r\n# To pause via Azure CLI (not fab):\r\naz resource update --ids \"/subscriptions/{sub}/resourceGroups/{rg}/providers/Microsoft.Fabric/capacities/{name}\" \\\r\n  --set properties.state=Paused\r\n```\r\n\r\n## Gateways\r\n\r\n```bash\r\n# List gateways\r\nfab api -A powerbi gateways\r\n\r\n# Get gateway datasources\r\nGATEWAY_ID=\"<gateway-id>\"\r\nfab api -A powerbi \"gateways/$GATEWAY_ID/datasources\"\r\n\r\n# Get gateway users\r\nfab api -A powerbi \"gateways/$GATEWAY_ID/users\"\r\n```\r\n\r\n## Deployment Pipelines\r\n\r\n```bash\r\n# List pipelines (user)\r\nfab api -A powerbi pipelines\r\n\r\n# List pipelines (admin - all tenant)\r\nfab api -A powerbi admin/pipelines\r\n\r\n# Get pipeline stages\r\nPIPELINE_ID=\"<pipeline-id>\"\r\nfab api -A powerbi \"pipelines/$PIPELINE_ID/stages\"\r\n\r\n# Get pipeline operations\r\nfab api -A powerbi \"pipelines/$PIPELINE_ID/operations\"\r\n```\r\n\r\n**Deploy content** (use Fabric API):\r\n\r\n```bash\r\n# Assign workspace to stage\r\nfab api -X post \"deploymentPipelines/$PIPELINE_ID/stages/$STAGE_ID/assignWorkspace\" \\\r\n  -i '{\"workspaceId\":\"<workspace-id>\"}'\r\n\r\n# Deploy to next stage\r\nfab api -X post \"deploymentPipelines/$PIPELINE_ID/deploy\" -i '{\r\n  \"sourceStageOrder\": 0,\r\n  \"targetStageOrder\": 1,\r\n  \"options\": {\"allowCreateArtifact\": true, \"allowOverwriteArtifact\": true}\r\n}'\r\n```\r\n\r\n## Domains\r\n\r\n```bash\r\n# List domains\r\nfab api admin/domains\r\n\r\n# Get domain workspaces\r\nDOMAIN_ID=\"<domain-id>\"\r\nfab api \"admin/domains/$DOMAIN_ID/workspaces\"\r\n\r\n# Assign workspaces to domain\r\nfab api -X post \"admin/domains/$DOMAIN_ID/assignWorkspaces\" \\\r\n  -i '{\"workspacesIds\":[\"<ws-id-1>\",\"<ws-id-2>\"]}'\r\n```\r\n\r\n## Dataflows\r\n\r\n**Gen1** (Power BI dataflows):\r\n\r\n```bash\r\n# List all dataflows (admin)\r\nfab api -A powerbi admin/dataflows\r\n\r\n# List workspace dataflows\r\nWS_ID=\"<workspace-id>\"\r\nfab api -A powerbi \"groups/$WS_ID/dataflows\"\r\n\r\n# Refresh dataflow\r\nDATAFLOW_ID=\"<dataflow-id>\"\r\nfab api -A powerbi -X post \"groups/$WS_ID/dataflows/$DATAFLOW_ID/refreshes\"\r\n```\r\n\r\n**Gen2** (Fabric dataflows):\r\n\r\n```bash\r\n# Gen2 dataflows are Fabric items - use standard fab commands\r\nfab ls \"ws.Workspace\" | grep DataflowGen2\r\nfab get \"ws.Workspace/Flow.DataflowGen2\" -q \"id\"\r\n```\r\n\r\n## Apps\r\n\r\n**Workspace Apps** (published from workspaces):\r\n\r\n```bash\r\n# List user's apps\r\nfab api -A powerbi apps\r\n\r\n# List all apps (admin)\r\nfab api -A powerbi 'admin/apps?$top=100'\r\n\r\n# Get app details\r\nAPP_ID=\"<app-id>\"\r\nfab api -A powerbi \"apps/$APP_ID\"\r\n\r\n# Get app reports\r\nfab api -A powerbi \"apps/$APP_ID/reports\"\r\n\r\n# Get app dashboards\r\nfab api -A powerbi \"apps/$APP_ID/dashboards\"\r\n```\r\n\r\n**Org Apps** (template apps from AppSource):\r\n\r\n```bash\r\n# Org apps are installed from AppSource marketplace\r\n# They appear in the regular apps endpoint after installation\r\n# No separate API for org app catalog - use AppSource\r\n```\r\n\r\n## Admin Operations\r\n\r\n### Workspaces\r\n\r\n```bash\r\n# List all workspaces (requires $top)\r\nfab api -A powerbi 'admin/groups?$top=100'\r\n\r\n# Response includes: id, name, type, state, capacityId, pipelineId\r\n\r\n# Get workspace users\r\nfab api -A powerbi \"admin/groups/$WS_ID/users\"\r\n```\r\n\r\n### Items\r\n\r\n```bash\r\n# List all items in tenant\r\nfab api admin/items\r\n\r\n# Response includes: id, type, name, workspaceId, capacityId, creatorPrincipal\r\n```\r\n\r\n### Security Scanning\r\n\r\n```bash\r\n# Reports shared with entire org (security risk)\r\nfab api -A powerbi \"admin/widelySharedArtifacts/linksSharedToWholeOrganization\"\r\n\r\n# Reports published to web (security risk)\r\nfab api -A powerbi \"admin/widelySharedArtifacts/publishedToWeb\"\r\n```\r\n\r\n### Activity Events\r\n\r\n```bash\r\n# Get activity events (last 30 days max)\r\n# Dates must be in ISO 8601 format with quotes\r\nSTART=\"2025-11-26T00:00:00Z\"\r\nEND=\"2025-11-27T00:00:00Z\"\r\nfab api -A powerbi \"admin/activityevents?startDateTime='$START'&endDateTime='$END'\"\r\n```\r\n\r\n## Common Patterns\r\n\r\n### Extract ID for Chaining\r\n\r\n```bash\r\n# Get ID and remove quotes\r\nWS_ID=$(fab get \"ws.Workspace\" -q \"id\" | tr -d '\"')\r\nMODEL_ID=$(fab get \"ws.Workspace/Model.SemanticModel\" -q \"id\" | tr -d '\"')\r\n\r\n# Use in API call\r\nfab api -A powerbi \"groups/$WS_ID/datasets/$MODEL_ID/refreshes\" -X post -i '{\"type\":\"Full\"}'\r\n```\r\n\r\n### Pagination\r\n\r\n```bash\r\n# APIs with $top often have pagination\r\n# Check for @odata.nextLink in response\r\n\r\nfab api -A powerbi 'admin/groups?$top=100' -q \"@odata.nextLink\"\r\n# Use returned URL for next page\r\n```\r\n\r\n### Error Handling\r\n\r\n```bash\r\n# Check status_code in response\r\n# 200 = success\r\n# 400 = bad request (check parameters)\r\n# 401 = unauthorized (re-authenticate)\r\n# 403 = forbidden (insufficient permissions)\r\n# 404 = not found\r\n```\r\n\r\n## API Audiences\r\n\r\n| Audience | Flag | Base URL | Use Case |\r\n|----------|------|----------|----------|\r\n| Fabric | (default) | api.fabric.microsoft.com | Fabric items, workspaces, admin |\r\n| Power BI | `-A powerbi` | api.powerbi.com | Reports, datasets, gateways, pipelines |\r\n\r\nMost admin operations work with both APIs but return different formats.\r\n",
        "skills/fabric-cli/references/notebooks.md": "# Notebook Operations\n\nComprehensive guide for working with Fabric notebooks using the Fabric CLI.\n\n## Overview\n\nFabric notebooks are interactive documents for data engineering, data science, and analytics. They can be executed, scheduled, and managed via the CLI.\n\n## Getting Notebook Information\n\n### Basic Notebook Info\n\n```bash\n# Check if notebook exists\nfab exists \"Production.Workspace/ETL Pipeline.Notebook\"\n\n# Get notebook properties\nfab get \"Production.Workspace/ETL Pipeline.Notebook\"\n\n# Get with verbose details\nfab get \"Production.Workspace/ETL Pipeline.Notebook\" -v\n\n# Get only notebook ID\nfab get \"Production.Workspace/ETL Pipeline.Notebook\" -q \"id\"\n```\n\n### Get Notebook Definition\n\n```bash\n# Get full notebook definition\nfab get \"Production.Workspace/ETL Pipeline.Notebook\" -q \"definition\"\n\n# Save definition to file\nfab get \"Production.Workspace/ETL Pipeline.Notebook\" -q \"definition\" -o /tmp/notebook-def.json\n\n# Get notebook content (cells)\nfab get \"Production.Workspace/ETL Pipeline.Notebook\" -q \"definition.parts[?path=='notebook-content.py'].payload | [0]\"\n```\n\n## Exporting Notebooks\n\n### Export as IPYNB\n\n```bash\n# Export notebook\nfab export \"Production.Workspace/ETL Pipeline.Notebook\" -o /tmp/notebooks\n\n# This creates:\n# /tmp/notebooks/ETL Pipeline.Notebook/\n# ├── notebook-content.py (or .ipynb)\n# └── metadata files\n```\n\n### Export All Notebooks from Workspace\n\n```bash\n# Export all notebooks\nWS_ID=$(fab get \"Production.Workspace\" -q \"id\")\nNOTEBOOKS=$(fab api \"workspaces/$WS_ID/items\" -q \"value[?type=='Notebook'].displayName\")\n\nfor NOTEBOOK in $NOTEBOOKS; do\n    fab export \"Production.Workspace/$NOTEBOOK.Notebook\" -o /tmp/notebooks\ndone\n```\n\n## Importing Notebooks\n\n### Import from Local\n\n```bash\n# Import notebook from .ipynb format (default)\nfab import \"Production.Workspace/New ETL.Notebook\" -i /tmp/notebooks/ETL\\ Pipeline.Notebook\n\n# Import from .py format\nfab import \"Production.Workspace/Script.Notebook\" -i /tmp/script.py --format py\n```\n\n### Copy Between Workspaces\n\n```bash\n# Copy notebook\nfab cp \"Dev.Workspace/ETL.Notebook\" \"Production.Workspace\"\n\n# Copy with new name\nfab cp \"Dev.Workspace/ETL.Notebook\" \"Production.Workspace/Prod ETL.Notebook\"\n```\n\n## Creating Notebooks\n\n### Create Blank Notebook\n\n```bash\n# Get workspace ID first\nfab get \"Production.Workspace\" -q \"id\"\n\n# Create via API\nfab api -X post \"workspaces/<workspace-id>/notebooks\" -i '{\"displayName\": \"New Data Processing\"}'\n```\n\n### Create and Configure Query Notebook\n\nUse this workflow to create a notebook for querying lakehouse tables with Spark SQL.\n\n#### Step 1: Create the notebook\n\n```bash\n# Get workspace ID\nfab get \"Sales.Workspace\" -q \"id\"\n# Returns: 4caf7825-81ac-4c94-9e46-306b4c20a4d5\n\n# Create notebook\nfab api -X post \"workspaces/4caf7825-81ac-4c94-9e46-306b4c20a4d5/notebooks\" -i '{\"displayName\": \"Data Query\"}'\n# Returns notebook ID: 97bbd18d-c293-46b8-8536-82fb8bc9bd58\n```\n\n#### Step 2: Get lakehouse ID (required for notebook metadata)\n\n```bash\nfab get \"Sales.Workspace/SalesLH.Lakehouse\" -q \"id\"\n# Returns: ddbcc575-805b-4922-84db-ca451b318755\n```\n\n#### Step 3: Create notebook code in Fabric format\n\n```bash\ncat > /tmp/notebook.py <<'EOF'\n# Fabric notebook source\n\n# METADATA ********************\n\n# META {\n# META   \"kernel_info\": {\n# META     \"name\": \"synapse_pyspark\"\n# META   },\n# META   \"dependencies\": {\n# META     \"lakehouse\": {\n# META       \"default_lakehouse\": \"ddbcc575-805b-4922-84db-ca451b318755\",\n# META       \"default_lakehouse_name\": \"SalesLH\",\n# META       \"default_lakehouse_workspace_id\": \"4caf7825-81ac-4c94-9e46-306b4c20a4d5\"\n# META     }\n# META   }\n# META }\n\n# CELL ********************\n\n# Query lakehouse table\ndf = spark.sql(\"\"\"\n    SELECT\n        date_key,\n        COUNT(*) as num_records\n    FROM gold.sets\n    GROUP BY date_key\n    ORDER BY date_key DESC\n    LIMIT 10\n\"\"\")\n\n# IMPORTANT: Convert to pandas and print to capture output\n# display(df) will NOT show results via API\npandas_df = df.toPandas()\nprint(pandas_df)\nprint(f\"\\nLatest date: {pandas_df.iloc[0]['date_key']}\")\nEOF\n```\n\n#### Step 4: Base64 encode and create update definition\n\n```bash\nbase64 -i /tmp/notebook.py > /tmp/notebook-b64.txt\n\ncat > /tmp/update.json <<EOF\n{\n  \"definition\": {\n    \"parts\": [\n      {\n        \"path\": \"notebook-content.py\",\n        \"payload\": \"$(cat /tmp/notebook-b64.txt)\",\n        \"payloadType\": \"InlineBase64\"\n      }\n    ]\n  }\n}\nEOF\n```\n\n#### Step 5: Update notebook with code\n\n```bash\nfab api -X post \"workspaces/4caf7825-81ac-4c94-9e46-306b4c20a4d5/notebooks/97bbd18d-c293-46b8-8536-82fb8bc9bd58/updateDefinition\" -i /tmp/update.json --show_headers\n# Returns operation ID in Location header\n```\n\n#### Step 6: Check update completed\n\n```bash\nfab api \"operations/<operation-id>\"\n# Wait for status: \"Succeeded\"\n```\n\n#### Step 7: Run the notebook\n\n```bash\nfab job start \"Sales.Workspace/Data Query.Notebook\"\n# Returns job instance ID\n```\n\n#### Step 8: Check execution status\n\n```bash\nfab job run-status \"Sales.Workspace/Data Query.Notebook\" --id <job-id>\n# Wait for status: \"Completed\"\n```\n\n#### Step 9: Get results (download from Fabric UI)\n\n- Open notebook in Fabric UI after execution\n- Print output will be visible in cell outputs\n- Download .ipynb file to see printed results locally\n\n#### Critical Requirements\n\n1. **File format**: Must be `notebook-content.py` (NOT `.ipynb`)\n2. **Lakehouse ID**: Must include `default_lakehouse` ID in metadata (not just name)\n3. **Spark session**: Will be automatically available when lakehouse is attached\n4. **Capturing output**: Use `df.toPandas()` and `print()` - `display()` won't show in API\n5. **Results location**: Print output visible in UI and downloaded .ipynb, NOT in definition\n\n#### Common Issues\n\n- `NameError: name 'spark' is not defined` - Lakehouse not attached (missing default_lakehouse ID)\n- Job \"Completed\" but no results - Used display() instead of print()\n- Update fails - Used .ipynb path instead of .py\n\n### Create from Template\n\n```bash\n# Export template\nfab export \"Templates.Workspace/Template Notebook.Notebook\" -o /tmp/templates\n\n# Import as new notebook\nfab import \"Production.Workspace/Custom Notebook.Notebook\" -i /tmp/templates/Template\\ Notebook.Notebook\n```\n\n## Running Notebooks\n\n### Run Synchronously (Wait for Completion)\n\n```bash\n# Run notebook and wait\nfab job run \"Production.Workspace/ETL Pipeline.Notebook\"\n\n# Run with timeout (seconds)\nfab job run \"Production.Workspace/Long Process.Notebook\" --timeout 600\n```\n\n### Run with Parameters\n\n```bash\n# Run with basic parameters\nfab job run \"Production.Workspace/ETL Pipeline.Notebook\" -P \\\n  date:string=2024-01-01,\\\n  batch_size:int=1000,\\\n  debug_mode:bool=false,\\\n  threshold:float=0.95\n\n# Parameters must match types defined in notebook\n# Supported types: string, int, float, bool\n```\n\n### Run with Spark Configuration\n\n```bash\n# Run with custom Spark settings\nfab job run \"Production.Workspace/Big Data Processing.Notebook\" -C '{\n  \"conf\": {\n    \"spark.executor.memory\": \"8g\",\n    \"spark.executor.cores\": \"4\",\n    \"spark.dynamicAllocation.enabled\": \"true\"\n  },\n  \"environment\": {\n    \"id\": \"<environment-id>\",\n    \"name\": \"Production Environment\"\n  }\n}'\n\n# Run with default lakehouse\nfab job run \"Production.Workspace/Data Ingestion.Notebook\" -C '{\n  \"defaultLakehouse\": {\n    \"name\": \"MainLakehouse\",\n    \"id\": \"<lakehouse-id>\",\n    \"workspaceId\": \"<workspace-id>\"\n  }\n}'\n\n# Run with workspace Spark pool\nfab job run \"Production.Workspace/Analytics.Notebook\" -C '{\n  \"useStarterPool\": false,\n  \"useWorkspacePool\": \"HighMemoryPool\"\n}'\n```\n\n### Run with Combined Parameters and Configuration\n\n```bash\n# Combine parameters and configuration\nfab job run \"Production.Workspace/ETL Pipeline.Notebook\" \\\n  -P date:string=2024-01-01,batch:int=500 \\\n  -C '{\n    \"defaultLakehouse\": {\"name\": \"StagingLH\", \"id\": \"<lakehouse-id>\"},\n    \"conf\": {\"spark.sql.shuffle.partitions\": \"200\"}\n  }'\n```\n\n### Run Asynchronously\n\n```bash\n# Start notebook and return immediately\nJOB_ID=$(fab job start \"Production.Workspace/ETL Pipeline.Notebook\" | grep -o '\"id\": \"[^\"]*\"' | cut -d'\"' -f4)\n\n# Check status later\nfab job run-status \"Production.Workspace/ETL Pipeline.Notebook\" --id \"$JOB_ID\"\n```\n\n## Monitoring Notebook Executions\n\n### Get Job Status\n\n```bash\n# Check specific job\nfab job run-status \"Production.Workspace/ETL Pipeline.Notebook\" --id <job-id>\n\n# Get detailed status via API\nWS_ID=$(fab get \"Production.Workspace\" -q \"id\")\nNOTEBOOK_ID=$(fab get \"Production.Workspace/ETL Pipeline.Notebook\" -q \"id\")\nfab api \"workspaces/$WS_ID/items/$NOTEBOOK_ID/jobs/instances/<job-id>\"\n```\n\n### List Execution History\n\n```bash\n# List all job runs\nfab job run-list \"Production.Workspace/ETL Pipeline.Notebook\"\n\n# List only scheduled runs\nfab job run-list \"Production.Workspace/ETL Pipeline.Notebook\" --schedule\n\n# Get latest run status\nfab job run-list \"Production.Workspace/ETL Pipeline.Notebook\" | head -n 1\n```\n\n### Cancel Running Job\n\n```bash\nfab job run-cancel \"Production.Workspace/ETL Pipeline.Notebook\" --id <job-id>\n```\n\n## Scheduling Notebooks\n\n### Create Cron Schedule\n\n```bash\n# Run every 30 minutes\nfab job run-sch \"Production.Workspace/ETL Pipeline.Notebook\" \\\n  --type cron \\\n  --interval 30 \\\n  --start 2024-11-15T00:00:00 \\\n  --end 2025-12-31T23:59:00 \\\n  --enable\n```\n\n### Create Daily Schedule\n\n```bash\n# Run daily at 2 AM and 2 PM\nfab job run-sch \"Production.Workspace/ETL Pipeline.Notebook\" \\\n  --type daily \\\n  --interval 02:00,14:00 \\\n  --start 2024-11-15T00:00:00 \\\n  --end 2025-12-31T23:59:00 \\\n  --enable\n```\n\n### Create Weekly Schedule\n\n```bash\n# Run Monday and Friday at 9 AM\nfab job run-sch \"Production.Workspace/Weekly Report.Notebook\" \\\n  --type weekly \\\n  --interval 09:00 \\\n  --days Monday,Friday \\\n  --start 2024-11-15T00:00:00 \\\n  --enable\n```\n\n### Update Schedule\n\n```bash\n# Modify existing schedule\nfab job run-update \"Production.Workspace/ETL Pipeline.Notebook\" \\\n  --id <schedule-id> \\\n  --type daily \\\n  --interval 03:00 \\\n  --enable\n\n# Disable schedule\nfab job run-update \"Production.Workspace/ETL Pipeline.Notebook\" \\\n  --id <schedule-id> \\\n  --disable\n```\n\n## Notebook Configuration\n\n### Set Default Lakehouse\n\n```bash\n# Via notebook properties\nfab set \"Production.Workspace/ETL.Notebook\" -q lakehouse -i '{\n  \"known_lakehouses\": [{\"id\": \"<lakehouse-id>\"}],\n  \"default_lakehouse\": \"<lakehouse-id>\",\n  \"default_lakehouse_name\": \"MainLakehouse\",\n  \"default_lakehouse_workspace_id\": \"<workspace-id>\"\n}'\n```\n\n### Set Default Environment\n\n```bash\nfab set \"Production.Workspace/ETL.Notebook\" -q environment -i '{\n  \"environmentId\": \"<environment-id>\",\n  \"workspaceId\": \"<workspace-id>\"\n}'\n```\n\n### Set Default Warehouse\n\n```bash\nfab set \"Production.Workspace/Analytics.Notebook\" -q warehouse -i '{\n  \"known_warehouses\": [{\"id\": \"<warehouse-id>\", \"type\": \"Datawarehouse\"}],\n  \"default_warehouse\": \"<warehouse-id>\"\n}'\n```\n\n## Updating Notebooks\n\n### Update Display Name\n\n```bash\nfab set \"Production.Workspace/ETL.Notebook\" -q displayName -i \"ETL Pipeline v2\"\n```\n\n### Update Description\n\n```bash\nfab set \"Production.Workspace/ETL.Notebook\" -q description -i \"Daily ETL pipeline for sales data ingestion and transformation\"\n```\n\n## Deleting Notebooks\n\n```bash\n# Delete with confirmation (interactive)\nfab rm \"Dev.Workspace/Old Notebook.Notebook\"\n\n# Force delete without confirmation\nfab rm \"Dev.Workspace/Old Notebook.Notebook\" -f\n```\n\n## Advanced Workflows\n\n### Parameterized Notebook Execution\n\n```python\n# Create parametrized notebook with cell tagged as \"parameters\"\n# In notebook, create cell:\ndate = \"2024-01-01\"  # default\nbatch_size = 1000    # default\ndebug = False        # default\n\n# Execute with different parameters\nfab job run \"Production.Workspace/Parameterized.Notebook\" -P \\\n  date:string=2024-02-15,\\\n  batch_size:int=2000,\\\n  debug:bool=true\n```\n\n### Notebook Orchestration Pipeline\n\n```bash\n#!/bin/bash\n\nWORKSPACE=\"Production.Workspace\"\nDATE=$(date +%Y-%m-%d)\n\n# 1. Run ingestion notebook\necho \"Starting data ingestion...\"\nfab job run \"$WORKSPACE/1_Ingest_Data.Notebook\" -P date:string=$DATE\n\n# 2. Run transformation notebook\necho \"Running transformations...\"\nfab job run \"$WORKSPACE/2_Transform_Data.Notebook\" -P date:string=$DATE\n\n# 3. Run analytics notebook\necho \"Generating analytics...\"\nfab job run \"$WORKSPACE/3_Analytics.Notebook\" -P date:string=$DATE\n\n# 4. Run reporting notebook\necho \"Creating reports...\"\nfab job run \"$WORKSPACE/4_Reports.Notebook\" -P date:string=$DATE\n\necho \"Pipeline completed for $DATE\"\n```\n\n### Monitoring Long-Running Notebook\n\n```bash\n#!/bin/bash\n\nNOTEBOOK=\"Production.Workspace/Long Process.Notebook\"\n\n# Start job\nJOB_ID=$(fab job start \"$NOTEBOOK\" -P date:string=$(date +%Y-%m-%d) | \\\n  grep -o '\"id\": \"[^\"]*\"' | head -1 | cut -d'\"' -f4)\n\necho \"Started job: $JOB_ID\"\n\n# Poll status every 30 seconds\nwhile true; do\n  STATUS=$(fab job run-status \"$NOTEBOOK\" --id \"$JOB_ID\" | \\\n    grep -o '\"status\": \"[^\"]*\"' | cut -d'\"' -f4)\n\n  echo \"[$(date +%H:%M:%S)] Status: $STATUS\"\n\n  if [[ \"$STATUS\" == \"Completed\" ]] || [[ \"$STATUS\" == \"Failed\" ]]; then\n    break\n  fi\n\n  sleep 30\ndone\n\nif [[ \"$STATUS\" == \"Completed\" ]]; then\n  echo \"Job completed successfully\"\n  exit 0\nelse\n  echo \"Job failed\"\n  exit 1\nfi\n```\n\n### Conditional Notebook Execution\n\n```bash\n#!/bin/bash\n\nWORKSPACE=\"Production.Workspace\"\n\n# Check if data is ready\nDATA_READY=$(fab api \"workspaces/<ws-id>/lakehouses/<lh-id>/Files/ready.flag\" 2>&1 | grep -c \"200\")\n\nif [ \"$DATA_READY\" -eq 1 ]; then\n  echo \"Data ready, running notebook...\"\n  fab job run \"$WORKSPACE/Process Data.Notebook\" -P date:string=$(date +%Y-%m-%d)\nelse\n  echo \"Data not ready, skipping execution\"\nfi\n```\n\n## Notebook Definition Structure\n\nNotebook definition contains:\n\nNotebookName.Notebook/\n├── .platform                  # Git integration metadata\n├── notebook-content.py        # Python code (or .ipynb format)\n└── metadata.json             # Notebook metadata\n\n### Query Notebook Content\n\n```bash\nNOTEBOOK=\"Production.Workspace/ETL.Notebook\"\n\n# Get Python code content\nfab get \"$NOTEBOOK\" -q \"definition.parts[?path=='notebook-content.py'].payload | [0]\" | base64 -d\n\n# Get metadata\nfab get \"$NOTEBOOK\" -q \"definition.parts[?path=='metadata.json'].payload | [0]\" | base64 -d | jq .\n```\n\n## Troubleshooting\n\n### Notebook Execution Failures\n\n```bash\n# Check recent execution\nfab job run-list \"Production.Workspace/ETL.Notebook\" | head -n 5\n\n# Get detailed error\nfab job run-status \"Production.Workspace/ETL.Notebook\" --id <job-id> -q \"error\"\n\n# Common issues:\n# - Lakehouse not attached\n# - Invalid parameters\n# - Spark configuration errors\n# - Missing dependencies\n```\n\n### Parameter Type Mismatches\n\n```bash\n# Parameters must match expected types\n# ❌ Wrong: -P count:string=100    (should be int)\n# ✅ Right: -P count:int=100\n\n# Check notebook definition for parameter types\nfab get \"Production.Workspace/ETL.Notebook\" -q \"definition.parts[?path=='notebook-content.py']\"\n```\n\n### Lakehouse Access Issues\n\n```bash\n# Verify lakehouse exists and is accessible\nfab exists \"Production.Workspace/MainLakehouse.Lakehouse\"\n\n# Check notebook's lakehouse configuration\nfab get \"Production.Workspace/ETL.Notebook\" -q \"properties.lakehouse\"\n\n# Re-attach lakehouse\nfab set \"Production.Workspace/ETL.Notebook\" -q lakehouse -i '{\n  \"known_lakehouses\": [{\"id\": \"<lakehouse-id>\"}],\n  \"default_lakehouse\": \"<lakehouse-id>\",\n  \"default_lakehouse_name\": \"MainLakehouse\",\n  \"default_lakehouse_workspace_id\": \"<workspace-id>\"\n}'\n```\n\n## Performance Tips\n\n1. **Use workspace pools**: Faster startup than starter pool\n2. **Cache data in lakehouses**: Avoid re-fetching data\n3. **Parameterize notebooks**: Reuse logic with different inputs\n4. **Monitor execution time**: Set appropriate timeouts\n5. **Use async execution**: Don't block on long-running notebooks\n6. **Optimize Spark config**: Tune for specific workloads\n\n## Best Practices\n\n1. **Tag parameter cells**: Use \"parameters\" tag for injected params\n2. **Handle failures gracefully**: Add error handling and logging\n3. **Version control notebooks**: Export and commit to Git\n4. **Use descriptive names**: Clear naming for scheduled jobs\n5. **Document parameters**: Add comments explaining expected inputs\n6. **Test locally first**: Validate in development workspace\n7. **Monitor schedules**: Review execution history regularly\n8. **Clean up old notebooks**: Remove unused notebooks\n\n## Security Considerations\n\n1. **Credential management**: Use Key Vault for secrets\n2. **Workspace permissions**: Control who can execute notebooks\n3. **Parameter validation**: Sanitize inputs in notebook code\n4. **Data access**: Respect lakehouse/warehouse permissions\n5. **Logging**: Don't log sensitive information\n\n## Related Scripts\n\n- `scripts/run_notebook_pipeline.py` - Orchestrate multiple notebooks\n- `scripts/monitor_notebook.py` - Monitor long-running executions\n- `scripts/export_notebook.py` - Export with validation\n- `scripts/schedule_notebook.py` - Simplified scheduling interface\n",
        "skills/fabric-cli/references/querying-data.md": "# Querying Data\r\n\r\n## Query a Semantic Model (DAX)\r\n\r\n```bash\r\n# 1. Get workspace and model IDs\r\nfab get \"ws.Workspace\" -q \"id\"\r\nfab get \"ws.Workspace/Model.SemanticModel\" -q \"id\"\r\n\r\n# 2. Execute DAX query\r\nfab api -A powerbi \"groups/<ws-id>/datasets/<model-id>/executeQueries\" \\\r\n  -X post -i '{\"queries\":[{\"query\":\"EVALUATE TOPN(10, '\\''TableName'\\'')\"}]}'\r\n```\r\n\r\nOr use the helper script:\r\n\r\n```bash\r\npython3 scripts/execute_dax.py \"ws.Workspace/Model.SemanticModel\" -q \"EVALUATE TOPN(10, 'Table')\"\r\n```\r\n\r\n## Query a Lakehouse Table\r\n\r\nLakehouse tables cannot be queried directly via API. Create a Direct Lake semantic model first.\r\n\r\n```bash\r\n# 1. Create Direct Lake model from lakehouse table\r\npython3 scripts/create_direct_lake_model.py \\\r\n  \"src.Workspace/LH.Lakehouse\" \\\r\n  \"dest.Workspace/Model.SemanticModel\" \\\r\n  -t schema.table\r\n\r\n# 2. Query via DAX\r\npython3 scripts/execute_dax.py \"dest.Workspace/Model.SemanticModel\" -q \"EVALUATE TOPN(10, 'table')\"\r\n\r\n# 3. (Optional) Delete temporary model\r\nfab rm \"dest.Workspace/Model.SemanticModel\" -f\r\n```\r\n\r\n## Get Lakehouse SQL Endpoint\r\n\r\nFor external SQL clients:\r\n\r\n```bash\r\nfab get \"ws.Workspace/LH.Lakehouse\" -q \"properties.sqlEndpointProperties\"\r\n```\r\n\r\nReturns `connectionString` and `id` for SQL connections.\r\n",
        "skills/fabric-cli/references/quickstart.md": "# Fabric CLI Quick Start Guide\n\nReal working examples using Fabric workspaces and items. These commands are ready to copy-paste and modify for your own workspaces.\n\n## Finding Items\n\n### List Workspaces\n\n```bash\n# List all workspaces\nfab ls\n\n# List with details (shows IDs, types, etc.)\nfab ls -l\n\n# Find specific workspace\nfab ls | grep \"Sales\"\n```\n\n### List Items in Workspace\n\n```bash\n# List all items in workspace\nfab ls \"Sales.Workspace\"\n\n# List with details (shows IDs, modification dates)\nfab ls \"Sales.Workspace\" -l\n\n# Filter by type\nfab ls \"Sales.Workspace\" | grep \"Notebook\"\nfab ls \"Sales.Workspace\" | grep \"SemanticModel\"\nfab ls \"Sales.Workspace\" | grep \"Lakehouse\"\n```\n\n### Check if Item Exists\n\n```bash\n# Check workspace exists\nfab exists \"Sales.Workspace\"\n\n# Check specific item exists\nfab exists \"Sales.Workspace/Sales Model.SemanticModel\"\nfab exists \"Sales.Workspace/SalesLH.Lakehouse\"\nfab exists \"Sales.Workspace/ETL - Extract.Notebook\"\n```\n\n### Get Item Details\n\n```bash\n# Get basic properties\nfab get \"Sales.Workspace/Sales Model.SemanticModel\"\n\n# Get all properties (verbose)\nfab get \"Sales.Workspace/Sales Model.SemanticModel\" -v\n\n# Get specific property (workspace ID)\nfab get \"Sales.Workspace\" -q \"id\"\n\n# Get specific property (model ID)\nfab get \"Sales.Workspace/Sales Model.SemanticModel\" -q \"id\"\n\n# Get display name\nfab get \"Sales.Workspace/Sales Model.SemanticModel\" -q \"displayName\"\n```\n\n## Working with Semantic Models\n\n### Get Model Information\n\n```bash\n# Get model definition (full TMDL structure)\nfab get \"Sales.Workspace/Sales Model.SemanticModel\" -q \"definition\"\n\n# Save definition to file\nfab get \"Sales.Workspace/Sales Model.SemanticModel\" -q \"definition\" > sales-model-definition.json\n\n# Get model creation date\nfab get \"Sales.Workspace/Sales Model.SemanticModel\" -q \"properties.createdDateTime\"\n\n# Get model type (DirectLake, Import, etc.)\nfab get \"Sales.Workspace/Sales Model.SemanticModel\" -q \"properties.mode\"\n```\n\n### Check Refresh Status\n\n```bash\n# First, get the workspace ID\nfab get \"Sales.Workspace\" -q \"id\"\n# Returns: a1b2c3d4-e5f6-7890-abcd-ef1234567890\n\n# Then get the model ID\nfab get \"Sales.Workspace/Sales Model.SemanticModel\" -q \"id\"\n# Returns: 12345678-abcd-ef12-3456-789abcdef012\n\n# Now use those IDs to get latest refresh (put $top in the URL)\nfab api -A powerbi \"groups/a1b2c3d4-e5f6-7890-abcd-ef1234567890/datasets/12345678-abcd-ef12-3456-789abcdef012/refreshes?\\$top=1\"\n\n# Get full refresh history\nfab api -A powerbi \"groups/a1b2c3d4-e5f6-7890-abcd-ef1234567890/datasets/12345678-abcd-ef12-3456-789abcdef012/refreshes\"\n```\n\n### Query Model with DAX\n\n```bash\n# First, get the model definition to see table/column names\nfab get \"Sales.Workspace/Sales Model.SemanticModel\" -q \"definition\"\n\n# Get the workspace and model IDs\nfab get \"Sales.Workspace\" -q \"id\"\nfab get \"Sales.Workspace/Sales Model.SemanticModel\" -q \"id\"\n\n# Execute DAX query (using proper table qualification)\nfab api -A powerbi \"groups/a1b2c3d4-e5f6-7890-abcd-ef1234567890/datasets/12345678-abcd-ef12-3456-789abcdef012/executeQueries\" -X post -i '{\"queries\":[{\"query\":\"EVALUATE TOPN(1, '\\''Orders'\\'', '\\''Orders'\\''[OrderDate], DESC)\"}],\"serializerSettings\":{\"includeNulls\":true}}'\n\n# Query top 5 records from a table\nfab api -A powerbi \"groups/a1b2c3d4-e5f6-7890-abcd-ef1234567890/datasets/12345678-abcd-ef12-3456-789abcdef012/executeQueries\" -X post -i '{\"queries\":[{\"query\":\"EVALUATE TOPN(5, '\\''Orders'\\'')\"}],\"serializerSettings\":{\"includeNulls\":true}}'\n```\n\n### Trigger Model Refresh\n\n```bash\n# Get workspace and model IDs\nfab get \"Sales.Workspace\" -q \"id\"\nfab get \"Sales.Workspace/Sales Model.SemanticModel\" -q \"id\"\n\n# Trigger full refresh\nfab api -A powerbi \"groups/a1b2c3d4-e5f6-7890-abcd-ef1234567890/datasets/12345678-abcd-ef12-3456-789abcdef012/refreshes\" -X post -i '{\"type\": \"Full\", \"commitMode\": \"Transactional\"}'\n\n# Monitor refresh status\nfab api -A powerbi \"groups/a1b2c3d4-e5f6-7890-abcd-ef1234567890/datasets/12345678-abcd-ef12-3456-789abcdef012/refreshes?\\$top=1\"\n```\n\n## Working with Notebooks\n\n### List Notebooks\n\n```bash\n# List all notebooks in workspace\nfab ls \"Sales.Workspace\" | grep \"Notebook\"\n\n# Get specific notebook details\nfab get \"Sales.Workspace/ETL - Extract.Notebook\"\n\n# Get notebook ID\nfab get \"Sales.Workspace/ETL - Extract.Notebook\" -q \"id\"\n```\n\n### Run Notebook\n\n```bash\n# Run notebook synchronously (wait for completion)\nfab job run \"Sales.Workspace/ETL - Extract.Notebook\"\n\n# Run with timeout (300 seconds = 5 minutes)\nfab job run \"Sales.Workspace/ETL - Extract.Notebook\" --timeout 300\n\n# Run with parameters\nfab job run \"Sales.Workspace/ETL - Extract.Notebook\" -P \\\n  date:string=2025-10-17,\\\n  debug:bool=true\n```\n\n### Run Notebook Asynchronously\n\n```bash\n# Start notebook and return immediately\nfab job start \"Sales.Workspace/ETL - Extract.Notebook\"\n\n# Check execution history\nfab job run-list \"Sales.Workspace/ETL - Extract.Notebook\"\n\n# Check specific job status (replace <job-id> with actual ID)\nfab job run-status \"Sales.Workspace/ETL - Extract.Notebook\" --id <job-id>\n```\n\n### Get Notebook Definition\n\n```bash\n# Get full notebook definition\nfab get \"Sales.Workspace/ETL - Extract.Notebook\" -q \"definition\"\n\n# Save to file\nfab get \"Sales.Workspace/ETL - Extract.Notebook\" -q \"definition\" > etl-extract-notebook.json\n\n# Get notebook code content\nfab get \"Sales.Workspace/ETL - Extract.Notebook\" -q \"definition.parts[?path=='notebook-content.py'].payload | [0]\" | base64 -d\n```\n\n## Working with Lakehouses\n\n### Browse Lakehouse\n\n```bash\n# List lakehouse contents\nfab ls \"Sales.Workspace/SalesLH.Lakehouse\"\n\n# List Files directory\nfab ls \"Sales.Workspace/SalesLH.Lakehouse/Files\"\n\n# List specific folder in Files\nfab ls \"Sales.Workspace/SalesLH.Lakehouse/Files/2025/10\"\n\n# List Tables\nfab ls \"Sales.Workspace/SalesLH.Lakehouse/Tables\"\n\n# List with details (shows sizes, modified dates)\nfab ls \"Sales.Workspace/SalesLH.Lakehouse/Tables\" -l\n\n# List specific schema tables\nfab ls \"Sales.Workspace/SalesLH.Lakehouse/Tables/bronze\"\nfab ls \"Sales.Workspace/SalesLH.Lakehouse/Tables/gold\"\n```\n\n### Get Table Schema\n\n```bash\n# View table schema\nfab table schema \"Sales.Workspace/SalesLH.Lakehouse/Tables/bronze/raw_orders\"\nfab table schema \"Sales.Workspace/SalesLH.Lakehouse/Tables/gold/orders\"\n\n# Save schema to file\nfab table schema \"Sales.Workspace/SalesLH.Lakehouse/Tables/gold/orders\" > orders-schema.json\n```\n\n### Check Table Last Modified\n\n```bash\n# List tables with modification times\nfab ls \"Sales.Workspace/SalesLH.Lakehouse/Tables/gold\" -l\n\n# Get specific table details\nfab get \"Sales.Workspace/SalesLH.Lakehouse/Tables/gold/orders\"\n```\n\n## Working with Reports\n\n### List Reports\n\n```bash\n# List all reports\nfab ls \"Sales.Workspace\" | grep \"Report\"\n\n# Get report details\nfab get \"Sales.Workspace/Sales Dashboard.Report\"\n\n# Get report ID\nfab get \"Sales.Workspace/Sales Dashboard.Report\" -q \"id\"\n```\n\n### Export Report Definition\n\n```bash\n# Get report definition as JSON\nfab get \"Sales.Workspace/Sales Dashboard.Report\" -q \"definition\" > sales-report.json\n\n# Export report to local directory (creates PBIR structure)\nfab export \"Sales.Workspace/Sales Dashboard.Report\" -o ./reports-backup -f\n```\n\n### Get Report Metadata\n\n```bash\n# Get connected semantic model ID\nfab get \"Sales.Workspace/Sales Dashboard.Report\" -q \"properties.datasetId\"\n\n# Get report connection string\nfab get \"Sales.Workspace/Sales Dashboard.Report\" -q \"definition.parts[?path=='definition.pbir'].payload.datasetReference\"\n```\n\n## Download and Re-upload Workflows\n\n### Backup Semantic Model\n\n```bash\n# 1. Get model definition\nfab get \"Sales.Workspace/Sales Model.SemanticModel\" -q \"definition\" > backup-sales-model-$(date +%Y%m%d).json\n\n# 2. Get model metadata\nfab get \"Sales.Workspace/Sales Model.SemanticModel\" > backup-sales-model-metadata-$(date +%Y%m%d).json\n```\n\n### Export and Import Notebook\n\n```bash\n# Export notebook\nfab export \"Sales.Workspace/ETL - Extract.Notebook\" -o ./notebooks-backup\n\n# Import to another workspace (or same workspace with different name)\nfab import \"Dev.Workspace/ETL Extract Copy.Notebook\" -i ./notebooks-backup/ETL\\ -\\ Extract.Notebook\n```\n\n### Copy Items Between Workspaces\n\n```bash\n# Copy semantic model\nfab cp \"Sales.Workspace/Sales Model.SemanticModel\" \"Dev.Workspace\"\n\n# Copy with new name\nfab cp \"Sales.Workspace/Sales Model.SemanticModel\" \"Dev.Workspace/Sales Model Test.SemanticModel\"\n\n# Copy notebook\nfab cp \"Sales.Workspace/ETL - Extract.Notebook\" \"Dev.Workspace\"\n\n# Copy report\nfab cp \"Sales.Workspace/Sales Dashboard.Report\" \"Dev.Workspace\"\n```\n\n## Combined Workflows\n\n### Complete Model Status Check\n\n```bash\n# Check last refresh\nfab api -A powerbi \"groups/a1b2c3d4-e5f6-7890-abcd-ef1234567890/datasets/12345678-abcd-ef12-3456-789abcdef012/refreshes?\\$top=1\"\n\n# Check latest data in model\nfab api -A powerbi \"groups/a1b2c3d4-e5f6-7890-abcd-ef1234567890/datasets/12345678-abcd-ef12-3456-789abcdef012/executeQueries\" -X post -i '{\"queries\":[{\"query\":\"EVALUATE TOPN(1, '\\''Orders'\\'', '\\''Orders'\\''[OrderDate], DESC)\"}],\"serializerSettings\":{\"includeNulls\":true}}'\n\n# Check lakehouse data freshness\nfab ls \"Sales.Workspace/SalesLH.Lakehouse/Tables/gold/orders\" -l\n```\n\n### Check All Notebooks in Workspace\n\n```bash\n# List all notebooks\nfab ls \"Sales.Workspace\" | grep Notebook\n\n# Check execution history for each\nfab job run-list \"Sales.Workspace/ETL - Extract.Notebook\"\nfab job run-list \"Sales.Workspace/ETL - Transform.Notebook\"\n```\n\n### Monitor Lakehouse Data Freshness\n\n```bash\n# Check gold layer tables\nfab ls \"Sales.Workspace/SalesLH.Lakehouse/Tables/gold\" -l\n\n# Check bronze layer tables\nfab ls \"Sales.Workspace/SalesLH.Lakehouse/Tables/bronze\" -l\n\n# Check latest files\nfab ls \"Sales.Workspace/SalesLH.Lakehouse/Files/2025/10\" -l\n```\n\n## Tips and Tricks\n\n### Get IDs for API Calls\n\n```bash\n# Get workspace ID\nfab get \"Sales.Workspace\" -q \"id\"\n\n# Get model ID\nfab get \"Sales.Workspace/Sales Model.SemanticModel\" -q \"id\"\n\n# Get lakehouse ID\nfab get \"Sales.Workspace/SalesLH.Lakehouse\" -q \"id\"\n\n# Then use the IDs directly in API calls\nfab api \"workspaces/a1b2c3d4-e5f6-7890-abcd-ef1234567890/items\"\nfab api -A powerbi \"groups/a1b2c3d4-e5f6-7890-abcd-ef1234567890/datasets/12345678-abcd-ef12-3456-789abcdef012/refreshes\"\n```\n\n### Pipe to jq for Pretty JSON\n\n```bash\n# Pretty print JSON output\nfab get \"Sales.Workspace/Sales Model.SemanticModel\" | jq .\n\n# Extract specific fields\nfab get \"Sales.Workspace/Sales Model.SemanticModel\" | jq '{id: .id, name: .displayName, created: .properties.createdDateTime}'\n\n# Get workspace ID first, then filter arrays\nfab get \"Sales.Workspace\" -q \"id\"\nfab api \"workspaces/a1b2c3d4-e5f6-7890-abcd-ef1234567890/items\" | jq '.value[] | select(.type==\"Notebook\") | .displayName'\n```\n\n### Use with grep for Filtering\n\n```bash\n# Find items by pattern\nfab ls \"Sales.Workspace\" | grep -i \"etl\"\nfab ls \"Sales.Workspace\" | grep -i \"sales\"\n\n# Count items by type\nfab ls \"Sales.Workspace\" | grep -c \"Notebook\"\nfab ls \"Sales.Workspace\" | grep -c \"SemanticModel\"\n```\n\n### Create Aliases for Common Commands\n\n```bash\n# Add to ~/.bashrc or ~/.zshrc\nalias sales-ls='fab ls \"Sales.Workspace\"'\nalias sales-notebooks='fab ls \"Sales.Workspace\" | grep Notebook'\nalias sales-refresh='fab api -A powerbi \"groups/<ws-id>/datasets/<model-id>/refreshes?\\$top=1\"'\n\n# Then use:\nsales-ls\nsales-notebooks\nsales-refresh\n```\n\n## Common Patterns\n\n### Get All Items of a Type\n\n```bash\n# Get workspace ID first\nfab get \"Sales.Workspace\" -q \"id\"\n\n# Get all notebooks\nfab api \"workspaces/a1b2c3d4-e5f6-7890-abcd-ef1234567890/items\" -q \"value[?type=='Notebook']\"\n\n# Get all semantic models\nfab api \"workspaces/a1b2c3d4-e5f6-7890-abcd-ef1234567890/items\" -q \"value[?type=='SemanticModel']\"\n\n# Get all lakehouses\nfab api \"workspaces/a1b2c3d4-e5f6-7890-abcd-ef1234567890/items\" -q \"value[?type=='Lakehouse']\"\n\n# Get all reports\nfab api \"workspaces/a1b2c3d4-e5f6-7890-abcd-ef1234567890/items\" -q \"value[?type=='Report']\"\n```\n\n### Export Entire Workspace\n\n```bash\n# Export all items in workspace\nfab export \"Sales.Workspace\" -o ./sales-workspace-backup -a\n\n# This creates a full backup with all items\n```\n\n### Find Items by Name Pattern\n\n```bash\n# Get workspace ID first\nfab get \"Sales.Workspace\" -q \"id\"\n\n# Find items with \"ETL\" in name\nfab api \"workspaces/a1b2c3d4-e5f6-7890-abcd-ef1234567890/items\" -q \"value[?contains(displayName, 'ETL')]\"\n\n# Find items with \"Sales\" in name\nfab api \"workspaces/a1b2c3d4-e5f6-7890-abcd-ef1234567890/items\" -q \"value[?contains(displayName, 'Sales')]\"\n```\n\n## Next Steps\n\n- See [semantic-models.md](./semantic-models.md) for advanced model operations\n- See [notebooks.md](./notebooks.md) for notebook scheduling and orchestration\n- See [reports.md](./reports.md) for report deployment workflows\n- See [scripts/README.md](../scripts/README.md) for helper scripts\n",
        "skills/fabric-cli/references/reference.md": "# Fabric CLI Command Reference\n\nComprehensive reference for Microsoft Fabric CLI commands, flags, and patterns.\n\n## Table of Contents\n\n- [Item Types](#item-types)\n- [File System Commands](#file-system-commands)\n- [API Commands](#api-commands)\n- [Job Commands](#job-commands)\n- [Table Commands](#table-commands)\n- [Advanced Patterns](#advanced-patterns)\n\n## Item Types\n\nAll 35 supported item types:\n\n| Extension | Description |\n|-----------|-------------|\n| `.Workspace` | Workspaces (containers) |\n| `.SemanticModel` | Power BI datasets/semantic models |\n| `.Report` | Power BI reports |\n| `.Dashboard` | Power BI dashboards |\n| `.PaginatedReport` | Paginated reports |\n| `.Notebook` | Fabric notebooks |\n| `.DataPipeline` | Data pipelines |\n| `.SparkJobDefinition` | Spark job definitions |\n| `.Lakehouse` | Lakehouses |\n| `.Warehouse` | Warehouses |\n| `.SQLDatabase` | SQL databases |\n| `.SQLEndpoint` | SQL endpoints |\n| `.MirroredDatabase` | Mirrored databases |\n| `.MirroredWarehouse` | Mirrored warehouses |\n| `.KQLDatabase` | KQL databases |\n| `.KQLDashboard` | KQL dashboards |\n| `.KQLQueryset` | KQL querysets |\n| `.Eventhouse` | Eventhouses |\n| `.Eventstream` | Event streams |\n| `.Datamart` | Datamarts |\n| `.Environment` | Spark environments |\n| `.Reflex` | Reflex items |\n| `.MLModel` | ML models |\n| `.MLExperiment` | ML experiments |\n| `.GraphQLApi` | GraphQL APIs |\n| `.MountedDataFactory` | Mounted data factories |\n| `.CopyJob` | Copy jobs |\n| `.VariableLibrary` | Variable libraries |\n| `.SparkPool` | Spark pools |\n| `.ManagedIdentity` | Managed identities |\n| `.ManagedPrivateEndpoint` | Managed private endpoints |\n| `.ExternalDataShare` | External data shares |\n| `.Folder` | Folders |\n| `.Capacity` | Capacities |\n| `.Personal` | Personal workspaces |\n\n## File System Commands\n\n### ls (dir) - List Resources\n\n#### Syntax\n\n```bash\nfab ls [path] [-l] [-a]\n```\n\n#### Flags\n\n- `-l` - Long format (detailed)\n- `-a` - Show hidden items\n\n#### Examples\n\n```bash\n# List workspaces\nfab ls\n\n# List items in workspace with details\nfab ls \"Production.Workspace\" -l\n\n# Show hidden items (capacities, connections, domains, gateways)\nfab ls -la\n\n# List lakehouse contents\nfab ls \"Data.Workspace/LH.Lakehouse\"\nfab ls \"Data.Workspace/LH.Lakehouse/Files\"\nfab ls \"Data.Workspace/LH.Lakehouse/Tables/dbo\"\n```\n\n### cd - Change Directory\n\n#### Syntax\n\n```bash\nfab cd <path>\n```\n\n#### Examples\n\n```bash\n# Navigate to workspace\nfab cd \"Production.Workspace\"\n\n# Navigate to item\nfab cd \"/Analytics.Workspace/Sales.SemanticModel\"\n\n# Relative navigation\nfab cd \"../Dev.Workspace\"\n\n# Personal workspace\nfab cd ~\n```\n\n### pwd - Print Working Directory\n\n#### Syntax\n\n```bash\nfab pwd\n```\n\n### exists - Check Existence\n\n#### Syntax\n\n```bash\nfab exists <path>\n```\n\n#### Returns\n\n`* true` or `* false`\n\n#### Examples\n\n```bash\nfab exists \"Production.Workspace\"\nfab exists \"Production.Workspace/Sales.SemanticModel\"\n```\n\n### get - Get Resource Details\n\n#### Syntax\n\n```bash\nfab get <path> [-v] [-q <jmespath>] [-o <output>]\n```\n\n#### Flags\n\n- `-v, --verbose` - Show all properties\n- `-q, --query` - JMESPath query\n- `-o, --output` - Save to file\n\n#### Examples\n\n```bash\n# Get workspace\nfab get \"Production.Workspace\"\n\n# Get item with all properties\nfab get \"Production.Workspace/Sales.Report\" -v\n\n# Query specific property\nfab get \"Production.Workspace\" -q \"id\"\nfab get \"Production.Workspace/Sales.SemanticModel\" -q \"definition.parts[0]\"\n\n# Save to file\nfab get \"Production.Workspace/Sales.SemanticModel\" -o /tmp/model.json\n```\n\n### set - Set Resource Properties\n\n#### Syntax\n\n```bash\nfab set <path> -q <property_path> -i <value>\n```\n\n#### Flags\n\n- `-q, --query` - Property path (JMESPath-style)\n- `-i, --input` - New value (string or JSON)\n\n#### Examples\n\n```bash\n# Update display name\nfab set \"Production.Workspace/Item.Notebook\" -q displayName -i \"New Name\"\n\n# Update description\nfab set \"Production.Workspace\" -q description -i \"Production environment\"\n\n# Update Spark runtime\nfab set \"Production.Workspace\" -q sparkSettings.environment.runtimeVersion -i 1.2\n\n# Assign Spark pool\nfab set \"Production.Workspace\" -q sparkSettings.pool.defaultPool -i '{\"name\": \"Starter Pool\", \"type\": \"Workspace\"}'\n\n# Rebind report to model\nfab set \"Production.Workspace/Report.Report\" -q semanticModelId -i \"<model-id>\"\n```\n\n### mkdir (create) - Create Resources\n\n#### Syntax\n\n```bash\nfab mkdir <path> [-P <param=value>]\n```\n\n#### Flags\n\n- `-P, --params` - Parameters (key=value format)\n\n#### Examples\n\n```bash\n# Create workspace\nfab mkdir \"NewWorkspace.Workspace\"\nfab mkdir \"NewWorkspace.Workspace\" -P capacityname=MyCapacity\nfab mkdir \"NewWorkspace.Workspace\" -P capacityname=none\n\n# Create items\nfab mkdir \"Production.Workspace/NewLakehouse.Lakehouse\"\nfab mkdir \"Production.Workspace/Notebook.Notebook\"\n\n# Create with parameters\nfab mkdir \"Production.Workspace/LH.Lakehouse\" -P enableSchemas=true\nfab mkdir \"Production.Workspace/DW.Warehouse\" -P enableCaseInsensitive=true\n\n# Check supported parameters\nfab mkdir \"Item.Lakehouse\" -P\n```\n\n### cp (copy) - Copy Resources\n\n#### Syntax\n\n```bash\nfab cp <source> <destination>\n```\n\n#### Supported types\n\n`.Notebook`, `.SparkJobDefinition`, `.DataPipeline`, `.Report`, `.SemanticModel`, `.KQLDatabase`, `.KQLDashboard`, `.KQLQueryset`, `.Eventhouse`, `.Eventstream`, `.MirroredDatabase`, `.Reflex`, `.MountedDataFactory`, `.CopyJob`, `.VariableLibrary`\n\n#### Examples\n\n```bash\n# Copy item to workspace (keeps same name)\nfab cp \"Dev.Workspace/Pipeline.DataPipeline\" \"Production.Workspace\"\n\n# Copy with new name\nfab cp \"Dev.Workspace/Pipeline.DataPipeline\" \"Production.Workspace/ProdPipeline.DataPipeline\"\n\n# Copy to folder\nfab cp \"Dev.Workspace/Report.Report\" \"Production.Workspace/Reports.Folder\"\n\n# Copy files to/from lakehouse\nfab cp ./local-data.csv \"Data.Workspace/LH.Lakehouse/Files/data.csv\"\nfab cp \"Data.Workspace/LH.Lakehouse/Files/data.csv\" ~/Downloads/\n```\n\n### mv (move) - Move Resources\n\n#### Syntax\n\n```bash\nfab mv <source> <destination>\n```\n\n#### Examples\n\n```bash\n# Move item to workspace\nfab mv \"Dev.Workspace/Pipeline.DataPipeline\" \"Production.Workspace\"\n\n# Move with rename\nfab mv \"Dev.Workspace/Pipeline.DataPipeline\" \"Production.Workspace/NewPipeline.DataPipeline\"\n\n# Move to folder\nfab mv \"Dev.Workspace/Report.Report\" \"Production.Workspace/Archive.Folder\"\n```\n\n### rm (del) - Delete Resources\n\n#### Syntax\n\n```bash\nfab rm <path> [-f]\n```\n\n#### Flags\n\n- `-f, --force` - Skip confirmation\n\n#### Examples\n\n```bash\n# Delete with confirmation (interactive)\nfab rm \"Dev.Workspace/OldReport.Report\"\n\n# Force delete\nfab rm \"Dev.Workspace/OldLakehouse.Lakehouse\" -f\n\n# Delete workspace and all contents\nfab rm \"OldWorkspace.Workspace\" -f\n```\n\n### export - Export Item Definitions\n\n#### Syntax\n\n```bash\nfab export <item_path> -o <output_path> [-a]\n```\n\n#### Flags\n\n- `-o, --output` - Output directory (local or lakehouse Files)\n- `-a` - Export all items (when exporting workspace)\n\n#### Supported types\n\nSame as `cp` command\n\n#### Examples\n\n```bash\n# Export item to local\nfab export \"Production.Workspace/Sales.SemanticModel\" -o /tmp/exports\n\n# Export all workspace items\nfab export \"Production.Workspace\" -o /tmp/backup -a\n\n# Export to lakehouse\nfab export \"Production.Workspace/Pipeline.DataPipeline\" -o \"Data.Workspace/Archive.Lakehouse/Files/exports\"\n```\n\n### import - Import Item Definitions\n\n#### Syntax\n\n```bash\nfab import <item_path> -i <input_path> [--format <format>]\n```\n\n#### Flags\n\n- `-i, --input` - Input directory or file\n- `--format` - Format override (e.g., `py` for notebooks)\n\n#### Examples\n\n```bash\n# Import item from local\nfab import \"Production.Workspace/Pipeline.DataPipeline\" -i /tmp/exports/Pipeline.DataPipeline\n\n# Import notebook from Python file\nfab import \"Production.Workspace/ETL.Notebook\" -i /tmp/etl_script.py --format py\n\n# Import from lakehouse\nfab import \"Production.Workspace/Report.Report\" -i \"Data.Workspace/Archive.Lakehouse/Files/exports/Report.Report\"\n```\n\n### open - Open in Browser\n\n#### Syntax\n\n```bash\nfab open <path>\n```\n\n#### Examples\n\n```bash\nfab open \"Production.Workspace\"\nfab open \"Production.Workspace/Sales.Report\"\n```\n\n### ln (mklink) - Create Shortcuts\n\n#### Syntax\n\n```bash\nfab ln <source> <destination>\n```\n\n### assign / unassign - Capacity Assignment\n\n#### Syntax\n\n```bash\nfab assign <workspace> -P capacityId=<capacity-id>\nfab unassign <workspace>\n```\n\n### start / stop - Start/Stop Resources\n\n#### Syntax\n\n```bash\nfab start <path> [-f]\nfab stop <path> [-f]\n```\n\n#### Supported\n\n`.MirroredDatabase`\n\n#### Examples\n\n```bash\nfab start \"Data.Workspace/Mirror.MirroredDatabase\" -f\nfab stop \"Data.Workspace/Mirror.MirroredDatabase\" -f\n```\n\n## API Commands\n\n### api - Make API Requests\n\n#### Syntax\n\n```bash\nfab api <endpoint> [-A <audience>] [-X <method>] [-i <input>] [-q <query>] [-P <params>] [-H <headers>] [--show_headers]\n```\n\n#### Flags\n\n- `-A, --audience` - API audience (fabric, powerbi, storage, azure)\n- `-X, --method` - HTTP method (get, post, put, delete, patch)\n- `-i, --input` - Request body (JSON string or file path)\n- `-q, --query` - JMESPath query to filter response\n- `-P, --params` - Query parameters (key=value format)\n- `-H, --headers` - Additional headers (key=value format)\n- `--show_headers` - Include response headers\n\n#### Audiences\n\n| Audience | Base URL | Use For |\n|----------|----------|---------|\n| `fabric` (default) | `https://api.fabric.microsoft.com` | Fabric REST API |\n| `powerbi` | `https://api.powerbi.com` | Power BI REST API, DAX queries |\n| `storage` | `https://*.dfs.fabric.microsoft.com` | OneLake Storage API |\n| `azure` | `https://management.azure.com` | Azure Resource Manager |\n\n#### Examples\n\n#### Fabric API\n\n```bash\n# GET requests\nfab api workspaces\nfab api \"workspaces/<workspace-id>\"\nfab api \"workspaces/<workspace-id>/items\"\nfab api workspaces -q \"value[?type=='Workspace']\"\n\n# POST request with inline JSON\nfab api -X post \"workspaces/<workspace-id>/items\" -i '{\"displayName\": \"New Item\", \"type\": \"Lakehouse\"}'\n\n# POST with file\nfab api -X post \"workspaces/<workspace-id>/lakehouses\" -i /tmp/config.json\n\n# PUT to update\nfab api -X put \"workspaces/<workspace-id>/items/<item-id>\" -i '{\"displayName\": \"Updated\"}'\n\n# DELETE\nfab api -X delete \"workspaces/<workspace-id>/items/<item-id>\"\n\n# Update semantic model definition\nfab api -X post \"workspaces/<workspace-id>/semanticModels/<model-id>/updateDefinition\" -i /tmp/definition.json --show_headers\n```\n\n#### Power BI API\n\n```bash\n# List groups (workspaces)\nfab api -A powerbi groups\n\n# Get datasets in workspace\nfab api -A powerbi \"groups/<workspace-id>/datasets\"\n\n# Execute DAX query\nfab api -A powerbi \"datasets/<model-id>/executeQueries\" -X post -i '{\"queries\": [{\"query\": \"EVALUATE VALUES(Date[Year])\"}]}'\n\n# Refresh dataset\nfab api -A powerbi \"datasets/<model-id>/refreshes\" -X post -i '{}'\n\n# Get refresh history\nfab api -A powerbi \"datasets/<model-id>/refreshes\"\n```\n\n#### OneLake Storage API\n\n```bash\n# List files with parameters\nfab api -A storage \"WorkspaceName.Workspace/LH.Lakehouse/Files\" -P resource=filesystem,recursive=false\n\n# With query string\nfab api -A storage \"WorkspaceName/LH.Lakehouse/Files?resource=filesystem&recursive=false\"\n```\n\n#### Azure Resource Manager\n\n```bash\n# List Fabric capacities\nfab api -A azure \"subscriptions/<subscription-id>/providers/Microsoft.Fabric/capacities?api-version=2023-11-01\"\n\n# Get available SKUs\nfab api -A azure \"subscriptions/<subscription-id>/providers/Microsoft.Fabric/skus?api-version=2023-11-01\"\n```\n\n## Job Commands\n\n### job run - Run Job Synchronously\n\n#### Syntax\n\n```bash\nfab job run <item_path> [--timeout <seconds>] [-P <params>] [-C <config>] [-i <input>]\n```\n\n#### Flags\n\n- `--timeout` - Timeout in seconds\n- `-P, --params` - Job parameters (typed: `name:type=value`)\n- `-C, --config` - Configuration JSON (file or inline)\n- `-i, --input` - Raw JSON input (file or inline)\n\n#### Supported items\n\n`.Notebook`, `.DataPipeline`, `.SparkJobDefinition`, `.Lakehouse` (maintenance)\n\n#### Parameter types\n\n- **Notebook**: `string`, `int`, `float`, `bool`\n- **DataPipeline**: `string`, `int`, `float`, `bool`, `object`, `array`, `secureString`\n\n#### Examples\n\n```bash\n# Run notebook\nfab job run \"Production.Workspace/ETL.Notebook\"\n\n# Run with timeout\nfab job run \"Production.Workspace/LongProcess.Notebook\" --timeout 300\n\n# Run with parameters\nfab job run \"Production.Workspace/ETL.Notebook\" -P date:string=2024-01-01,batch_size:int=1000,debug:bool=false\n\n# Run pipeline with complex parameters\nfab job run \"Production.Workspace/Pipeline.DataPipeline\" -P 'config:object={\"source\":\"s3\",\"batch\":100},ids:array=[1,2,3],secret:secureString=mysecret'\n\n# Run with Spark configuration\nfab job run \"Production.Workspace/ETL.Notebook\" -C '{\"conf\": {\"spark.executor.memory\": \"8g\"}, \"environment\": {\"id\": \"<env-id>\", \"name\": \"ProdEnv\"}}'\n\n# Run with default lakehouse\nfab job run \"Production.Workspace/Process.Notebook\" -C '{\"defaultLakehouse\": {\"name\": \"MainLH\", \"id\": \"<lh-id>\"}}'\n\n# Run with workspace pool\nfab job run \"Production.Workspace/BigData.Notebook\" -C '{\"useStarterPool\": false, \"useWorkspacePool\": \"HighMemoryPool\"}'\n\n# Run with raw JSON\nfab job run \"Production.Workspace/ETL.Notebook\" -i '{\"parameters\": {\"date\": {\"type\": \"string\", \"value\": \"2024-01-01\"}}, \"configuration\": {\"conf\": {\"spark.conf1\": \"value\"}}}'\n```\n\n### job start - Start Job Asynchronously\n\n#### Syntax\n\n```bash\nfab job start <item_path> [-P <params>] [-C <config>] [-i <input>]\n```\n\n#### Examples\n\n```bash\n# Start and return immediately\nfab job start \"Production.Workspace/ETL.Notebook\"\n\n# Start with parameters\nfab job start \"Production.Workspace/Pipeline.DataPipeline\" -P source:string=salesdb,batch:int=500\n```\n\n### job run-list - List Job History\n\n#### Syntax\n\n```bash\nfab job run-list <item_path> [--schedule]\n```\n\n#### Flags\n\n- `--schedule` - Show only scheduled job runs\n\n#### Examples\n\n```bash\n# List all job runs\nfab job run-list \"Production.Workspace/ETL.Notebook\"\n\n# List scheduled runs only\nfab job run-list \"Production.Workspace/ETL.Notebook\" --schedule\n```\n\n### job run-status - Get Job Status\n\n#### Syntax\n\n```bash\nfab job run-status <item_path> --id <job_id> [--schedule]\n```\n\n#### Flags\n\n- `--id` - Job or schedule ID\n- `--schedule` - Check scheduled job status\n\n#### Examples\n\n```bash\n# Check job instance status\nfab job run-status \"Production.Workspace/ETL.Notebook\" --id <job-id>\n\n# Check schedule status\nfab job run-status \"Production.Workspace/Pipeline.DataPipeline\" --id <schedule-id> --schedule\n```\n\n### job run-cancel - Cancel Job\n\n#### Syntax\n\n```bash\nfab job run-cancel <item_path> --id <job_id>\n```\n\n#### Examples\n\n```bash\nfab job run-cancel \"Production.Workspace/ETL.Notebook\" --id <job-id>\n```\n\n### job run-sch - Schedule Job\n\n#### Syntax\n\n```bash\nfab job run-sch <item_path> --type <type> --interval <interval> [--days <days>] --start <datetime> [--end <datetime>] [--enable] [-i <json>]\n```\n\n#### Flags\n\n- `--type` - Schedule type (cron, daily, weekly)\n- `--interval` - Interval (minutes for cron, HH:MM for daily/weekly)\n- `--days` - Days for weekly (Monday,Friday)\n- `--start` - Start datetime (ISO 8601)\n- `--end` - End datetime (ISO 8601)\n- `--enable` - Enable schedule immediately\n- `-i, --input` - Raw JSON schedule configuration\n\n#### Examples\n\n```bash\n# Cron schedule (every 10 minutes)\nfab job run-sch \"Production.Workspace/Pipeline.DataPipeline\" --type cron --interval 10 --start 2024-11-15T09:00:00 --end 2024-12-15T10:00:00 --enable\n\n# Daily schedule\nfab job run-sch \"Production.Workspace/Pipeline.DataPipeline\" --type daily --interval 10:00,16:00 --start 2024-11-15T09:00:00 --end 2024-12-16T10:00:00\n\n# Weekly schedule\nfab job run-sch \"Production.Workspace/Pipeline.DataPipeline\" --type weekly --interval 10:00,16:00 --days Monday,Friday --start 2024-11-15T09:00:00 --end 2024-12-16T10:00:00 --enable\n\n# Custom JSON configuration\nfab job run-sch \"Production.Workspace/Pipeline.DataPipeline\" -i '{\"enabled\": true, \"configuration\": {\"startDateTime\": \"2024-04-28T00:00:00\", \"endDateTime\": \"2024-04-30T23:59:00\", \"localTimeZoneId\": \"Central Standard Time\", \"type\": \"Cron\", \"interval\": 10}}'\n```\n\n### job run-update - Update Job Schedule\n\n#### Syntax\n\n```bash\nfab job run-update <item_path> --id <schedule_id> [--type <type>] [--interval <interval>] [--enable] [--disable] [-i <json>]\n```\n\n#### Examples\n\n```bash\n# Disable schedule\nfab job run-update \"Production.Workspace/Pipeline.DataPipeline\" --id <schedule-id> --disable\n\n# Update frequency\nfab job run-update \"Production.Workspace/Pipeline.DataPipeline\" --id <schedule-id> --type cron --interval 5 --enable\n\n# Update with JSON\nfab job run-update \"Production.Workspace/Pipeline.DataPipeline\" --id <schedule-id> -i '{\"enabled\": false}'\n```\n\n## Table Commands\n\n### table schema - View Table Schema\n\n#### Syntax\n\n```bash\nfab table schema <table_path>\n```\n\n#### Supported items\n\n`.Lakehouse`, `.Warehouse`, `.MirroredDatabase`, `.SQLDatabase`, `.SemanticModel`, `.KQLDatabase`\n\n#### Examples\n\n```bash\nfab table schema \"Data.Workspace/LH.Lakehouse/Tables/dbo/customers\"\nfab table schema \"Analytics.Workspace/DW.Warehouse/Tables/sales/orders\"\n```\n\n### table load - Load Data\n\n#### Syntax\n\n```bash\nfab table load <table_path> --file <file_path> [--mode <mode>] [--format <format>] [--extension <ext>]\n```\n\n#### Flags\n\n- `--file` - Source file or folder path (lakehouse Files location)\n- `--mode` - Load mode (append, overwrite) - default: append\n- `--format` - Format options (e.g., `format=csv,header=true,delimiter=;`)\n- `--extension` - File extension filter\n\n#### Note\n\nNot supported in schema-enabled lakehouses.\n\n#### Examples\n\n```bash\n# Load CSV from folder\nfab table load \"Data.Workspace/LH.Lakehouse/Tables/customers\" --file \"Data.Workspace/LH.Lakehouse/Files/csv/customers\"\n\n# Load specific CSV with append mode\nfab table load \"Data.Workspace/LH.Lakehouse/Tables/sales\" --file \"Data.Workspace/LH.Lakehouse/Files/daily_sales.csv\" --mode append\n\n# Load with custom CSV format\nfab table load \"Data.Workspace/LH.Lakehouse/Tables/products\" --file \"Data.Workspace/LH.Lakehouse/Files/data\" --format \"format=csv,header=false,delimiter=;\"\n\n# Load Parquet files\nfab table load \"Data.Workspace/LH.Lakehouse/Tables/events\" --file \"Data.Workspace/LH.Lakehouse/Files/parquet/events\" --format format=parquet --mode append\n```\n\n### table optimize - Optimize Table\n\n#### Syntax\n\n```bash\nfab table optimize <table_path> [--vorder] [--zorder <columns>]\n```\n\n#### Flags\n\n- `--vorder` - Enable V-Order optimization\n- `--zorder` - Z-Order columns (comma-separated)\n\n#### Note\n\nLakehouse only.\n\n#### Examples\n\n```bash\n# Basic optimization\nfab table optimize \"Data.Workspace/LH.Lakehouse/Tables/transactions\"\n\n# V-Order optimization\nfab table optimize \"Data.Workspace/LH.Lakehouse/Tables/sales\" --vorder\n\n# V-Order + Z-Order\nfab table optimize \"Data.Workspace/LH.Lakehouse/Tables/customers\" --vorder --zorder customer_id,region_id\n```\n\n### table vacuum - Vacuum Table\n\n#### Syntax\n\n```bash\nfab table vacuum <table_path> [--retain_n_hours <hours>]\n```\n\n#### Flags\n\n- `--retain_n_hours` - Retention period in hours (default: 168 = 7 days)\n\n#### Note\n\nLakehouse only.\n\n#### Examples\n\n```bash\n# Vacuum with default retention (7 days)\nfab table vacuum \"Data.Workspace/LH.Lakehouse/Tables/transactions\"\n\n# Vacuum with custom retention (48 hours)\nfab table vacuum \"Data.Workspace/LH.Lakehouse/Tables/temp_data\" --retain_n_hours 48\n```\n\n## Advanced Patterns\n\n### Batch Operations with Shell Scripts\n\n```bash\n#!/bin/bash\n\n# Export all semantic models from workspace\nWORKSPACE=\"Production.Workspace\"\nMODELS=$(fab api workspaces -q \"value[?displayName=='Production'].id | [0]\" | xargs -I {} fab api workspaces/{}/items -q \"value[?type=='SemanticModel'].displayName\")\n\nfor MODEL in $MODELS; do\n  fab export \"$WORKSPACE/$MODEL.SemanticModel\" -o /tmp/exports\ndone\n```\n\n### DAX Query Workflow\n\n```bash\n# 1. Get model ID\nMODEL_ID=$(fab get \"Production.Workspace/Sales.SemanticModel\" -q \"id\")\n\n# 2. Execute DAX query\nfab api -A powerbi \"datasets/$MODEL_ID/executeQueries\" -X post -i '{\n  \"queries\": [{\n    \"query\": \"EVALUATE TOPN(10, Sales, Sales[Amount], DESC)\"\n  }],\n  \"serializerSettings\": {\n    \"includeNulls\": false\n  }\n}'\n\n# 3. Execute multiple queries\nfab api -A powerbi \"datasets/$MODEL_ID/executeQueries\" -X post -i '{\n  \"queries\": [\n    {\"query\": \"EVALUATE VALUES(Date[Year])\"},\n    {\"query\": \"EVALUATE SUMMARIZE(Sales, Date[Year], \\\"Total\\\", SUM(Sales[Amount]))\"}\n  ]\n}'\n```\n\n### Semantic Model Update Workflow\n\n```bash\n# 1. Get current definition\nfab get \"Production.Workspace/Sales.SemanticModel\" -q definition -o /tmp/current-def.json\n\n# 2. Modify definition (edit JSON file)\n# ... edit /tmp/current-def.json ...\n\n# 3. Get workspace and model IDs\nWS_ID=$(fab get \"Production.Workspace\" -q \"id\")\nMODEL_ID=$(fab get \"Production.Workspace/Sales.SemanticModel\" -q \"id\")\n\n# 4. Prepare update request (wrap definition)\ncat > /tmp/update-request.json <<EOF\n{\n  \"definition\": $(cat /tmp/current-def.json)\n}\nEOF\n\n# 5. Update definition\nfab api -X post \"workspaces/$WS_ID/semanticModels/$MODEL_ID/updateDefinition\" -i /tmp/update-request.json --show_headers\n\n# 6. Poll operation status (extract operation ID from Location header)\n# Operation ID is in Location header: .../operations/{operation-id}\nfab api \"operations/<operation-id>\"\n```\n\n### Environment Migration Script\n\n```bash\n#!/bin/bash\n\nSOURCE_WS=\"Dev.Workspace\"\nTARGET_WS=\"Production.Workspace\"\n\n# Export all exportable items\nfab export \"$SOURCE_WS\" -o /tmp/migration -a\n\n# Import items to target workspace\nfor ITEM in /tmp/migration/*.Notebook; do\n  ITEM_NAME=$(basename \"$ITEM\")\n  fab import \"$TARGET_WS/$ITEM_NAME\" -i \"$ITEM\"\ndone\n\nfor ITEM in /tmp/migration/*.DataPipeline; do\n  ITEM_NAME=$(basename \"$ITEM\")\n  fab import \"$TARGET_WS/$ITEM_NAME\" -i \"$ITEM\"\ndone\n```\n\n### Job Monitoring Loop\n\n```bash\n#!/bin/bash\n\n# Start job\nJOB_ID=$(fab job start \"Production.Workspace/ETL.Notebook\" | grep -o '\"id\": \"[^\"]*\"' | cut -d'\"' -f4)\n\n# Poll status every 10 seconds\nwhile true; do\n  STATUS=$(fab job run-status \"Production.Workspace/ETL.Notebook\" --id \"$JOB_ID\" -q \"status\")\n  echo \"Job status: $STATUS\"\n\n  if [[ \"$STATUS\" == \"Completed\" ]] || [[ \"$STATUS\" == \"Failed\" ]]; then\n    break\n  fi\n\n  sleep 10\ndone\n\necho \"Job finished with status: $STATUS\"\n```\n\n### Workspace Inventory\n\n```bash\n#!/bin/bash\n\n# Get all workspaces and their item counts\nWORKSPACES=$(fab api workspaces -q \"value[].{name: displayName, id: id}\")\n\necho \"$WORKSPACES\" | jq -r '.[] | [.name, .id] | @tsv' | while IFS=$'\\t' read -r NAME ID; do\n  ITEM_COUNT=$(fab api \"workspaces/$ID/items\" -q \"value | length\")\n  echo \"$NAME: $ITEM_COUNT items\"\ndone\n```\n\n## JMESPath Quick Reference\n\nCommon JMESPath patterns for `-q` flag:\n\n```bash\n# Get field\n-q \"id\"\n-q \"displayName\"\n\n# Get nested field\n-q \"properties.sqlEndpointProperties\"\n-q \"definition.parts[0]\"\n\n# Filter array\n-q \"value[?type=='Lakehouse']\"\n-q \"value[?contains(name, 'prod')]\"\n-q \"value[?starts_with(displayName, 'Test')]\"\n\n# Get first/last element\n-q \"value[0]\"\n-q \"value[-1]\"\n\n# Pipe operations\n-q \"definition.parts[?path=='model.tmdl'] | [0]\"\n-q \"definition.parts[?path=='definition/tables/Sales.tmdl'].payload | [0]\"\n\n# Projections (select specific fields)\n-q \"value[].{name: displayName, id: id, type: type}\"\n\n# Length/count\n-q \"value | length\"\n\n# Multi-select list\n-q \"value[].[displayName, id, type]\"\n\n# Flatten\n-q \"value[].items[]\"\n\n# Sort\n-q \"sort_by(value, &displayName)\"\n\n# Boolean logic\n-q \"value[?type=='Lakehouse' && contains(displayName, 'prod')]\"\n-q \"value[?type=='Lakehouse' || type=='Warehouse']\"\n\n# Contains\n-q \"contains(value[].type, 'Lakehouse')\"\n```\n\n## Common Error Scenarios\n\n### Authentication Issues\n\n```bash\n# 403 Forbidden - Check authentication\nfab auth login\n\n# 401 Unauthorized - Token expired, re-authenticate\nfab auth login\n\n# Use service principal for automation\nfab auth login -u <client-id> -p <client-secret> --tenant <tenant-id>\n```\n\n### Resource Not Found\n\n```bash\n# 404 Not Found - Check resource exists\nfab exists \"Production.Workspace/Sales.SemanticModel\"\n\n# List available resources\nfab ls \"Production.Workspace\"\n\n# Get resource ID for API calls\nfab get \"Production.Workspace/Sales.SemanticModel\" -q \"id\"\n```\n\n### Job Failures\n\n```bash\n# Check job status\nfab job run-status \"Production.Workspace/ETL.Notebook\" --id <job-id>\n\n# View job history for patterns\nfab job run-list \"Production.Workspace/ETL.Notebook\"\n\n# Run with timeout to prevent hanging\nfab job run \"Production.Workspace/ETL.Notebook\" --timeout 300\n```\n\n### API Errors\n\n```bash\n# 400 Bad Request - Check JSON payload\nfab api -X post \"workspaces/<ws-id>/items\" -i /tmp/payload.json --show_headers\n\n# Debug with headers\nfab api workspaces --show_headers\n\n# Save response to inspect\nfab api workspaces -o /tmp/debug.json\n```\n\n## Performance Tips\n\n1. **Use `ls` instead of `get` for checking existence** - 10-20x faster\n2. **Use `exists` before `get` operations** - Avoids expensive failed gets\n3. **Filter with JMESPath `-q`** - Reduce response size\n4. **Use GUIDs in automation** - More stable than display names\n5. **Batch exports** - Export workspace with `-a` instead of individual items\n6. **Parallel job execution** - Use `job start` + polling for multiple jobs\n7. **Cache workspace/item IDs** - Avoid repeated `get` calls for IDs\n8. **Use appropriate API audience** - Power BI API is faster for dataset queries\n\n## Security Best Practices\n\n1. **Use service principals for automation** - Don't use interactive auth in scripts\n2. **Store credentials securely** - Use environment variables or key vaults\n3. **Use least-privilege access** - Grant minimal required permissions\n4. **Audit API calls** - Log all API operations in production\n5. **Validate inputs** - Sanitize user inputs before passing to API\n6. **Use force flag carefully** - `-f` skips confirmations, easy to delete wrong resources\n",
        "skills/fabric-cli/references/reports.md": "# Report Operations\n\n## Get Report Info\n\n```bash\n# Check exists\nfab exists \"ws.Workspace/Report.Report\"\n\n# Get properties\nfab get \"ws.Workspace/Report.Report\"\n\n# Get ID\nfab get \"ws.Workspace/Report.Report\" -q \"id\"\n```\n\n## Get Report Definition\n\n```bash\n# Full definition\nfab get \"ws.Workspace/Report.Report\" -q \"definition\"\n\n# Save to file\nfab get \"ws.Workspace/Report.Report\" -q \"definition\" -o /tmp/report-def.json\n\n# Specific parts\nfab get \"ws.Workspace/Report.Report\" -q \"definition.parts[?path=='definition/report.json'].payload | [0]\"\n```\n\n## Get Connected Model\n\n```bash\n# Get model reference from definition.pbir\nfab get \"ws.Workspace/Report.Report\" -q \"definition.parts[?contains(path, 'definition.pbir')].payload | [0]\"\n```\n\nOutput shows `byConnection.connectionString` with `semanticmodelid`.\n\n## Export Report\n\n1. Export to local directory:\n\n```bash\nfab export \"ws.Workspace/Report.Report\" -o /tmp/exports -f\n```\n\n2. Creates structure:\n\n```\nReport.Report/\n├── .platform\n├── definition.pbir\n└── definition/\n    ├── report.json\n    ├── version.json\n    └── pages/\n        └── {page-id}/\n            ├── page.json\n            └── visuals/{visual-id}/visual.json\n```\n\n## Import Report\n\n1. Import from local PBIP:\n\n```bash\nfab import \"ws.Workspace/Report.Report\" -i /tmp/exports/Report.Report -f\n```\n\n2. Import with new name:\n\n```bash\nfab import \"ws.Workspace/NewName.Report\" -i /tmp/exports/Report.Report -f\n```\n\n## Copy Report Between Workspaces\n\n```bash\nfab cp \"dev.Workspace/Report.Report\" \"prod.Workspace\" -f\n```\n\n## Create Blank Report\n\n1. Get model ID:\n\n```bash\nfab get \"ws.Workspace/Model.SemanticModel\" -q \"id\"\n```\n\n2. Create report via API:\n\n```bash\nWS_ID=$(fab get \"ws.Workspace\" -q \"id\" | tr -d '\"')\nfab api -X post \"workspaces/$WS_ID/reports\" -i '{\n  \"displayName\": \"New Report\",\n  \"datasetId\": \"<model-id>\"\n}'\n```\n\n## Update Report Properties\n\n```bash\n# Rename\nfab set \"ws.Workspace/Report.Report\" -q displayName -i \"New Name\"\n\n# Update description\nfab set \"ws.Workspace/Report.Report\" -q description -i \"Description text\"\n```\n\n## Rebind to Different Model\n\n1. Get new model ID:\n\n```bash\nfab get \"ws.Workspace/NewModel.SemanticModel\" -q \"id\"\n```\n\n2. Rebind:\n\n```bash\nfab set \"ws.Workspace/Report.Report\" -q semanticModelId -i \"<new-model-id>\"\n```\n\n## Delete Report\n\n```bash\nfab rm \"ws.Workspace/Report.Report\" -f\n```\n\n## List Pages\n\n```bash\nfab get \"ws.Workspace/Report.Report\" -q \"definition.parts[?contains(path, 'page.json')].path\"\n```\n\n## List Visuals\n\n```bash\nfab get \"ws.Workspace/Report.Report\" -q \"definition.parts[?contains(path, '/visuals/')].path\"\n```\n\n## Count Visuals by Type\n\n1. Export visuals:\n\n```bash\nfab get \"ws.Workspace/Report.Report\" -q \"definition.parts[?contains(path,'/visuals/')]\" > /tmp/visuals.json\n```\n\n2. Count by type:\n\n```bash\njq -r '.[] | .payload.visual.visualType' < /tmp/visuals.json | sort | uniq -c | sort -rn\n```\n\n## Extract Fields Used in Report\n\n1. Export visuals (if not done):\n\n```bash\nfab get \"ws.Workspace/Report.Report\" -q \"definition.parts[?contains(path,'/visuals/')]\" > /tmp/visuals.json\n```\n\n2. List unique fields:\n\n```bash\njq -r '[.[] | (.payload.visual.query.queryState // {} | to_entries[] | .value.projections[]? | if .field.Column then \"\\(.field.Column.Expression.SourceRef.Entity).\\(.field.Column.Property)\" elif .field.Measure then \"\\(.field.Measure.Expression.SourceRef.Entity).\\(.field.Measure.Property)\" else empty end)] | unique | sort | .[]' < /tmp/visuals.json\n```\n\n## Validate Fields Against Model\n\n1. Export report:\n\n```bash\nfab export \"ws.Workspace/Report.Report\" -o /tmp/report -f\n```\n\n2. Extract field references:\n\n```bash\nfind /tmp/report -name \"visual.json\" -exec grep -B2 '\"Property\":' {} \\; | \\\n  grep -E '\"Entity\":|\"Property\":' | paste -d' ' - - | \\\n  sed 's/.*\"Entity\": \"\\([^\"]*\\)\".*\"Property\": \"\\([^\"]*\\)\".*/\\1.\\2/' | sort -u\n```\n\n3. Compare against model definition to find missing fields.\n\n## Report Permissions\n\n1. Get IDs:\n\n```bash\nWS_ID=$(fab get \"ws.Workspace\" -q \"id\" | tr -d '\"')\nREPORT_ID=$(fab get \"ws.Workspace/Report.Report\" -q \"id\" | tr -d '\"')\n```\n\n2. List users:\n\n```bash\nfab api -A powerbi \"groups/$WS_ID/reports/$REPORT_ID/users\"\n```\n\n3. Add user:\n\n```bash\nfab api -A powerbi \"groups/$WS_ID/reports/$REPORT_ID/users\" -X post -i '{\n  \"emailAddress\": \"user@domain.com\",\n  \"reportUserAccessRight\": \"View\"\n}'\n```\n\n## Deploy Report (Dev to Prod)\n\n1. Export from dev:\n\n```bash\nfab export \"dev.Workspace/Report.Report\" -o /tmp/deploy -f\n```\n\n2. Import to prod:\n\n```bash\nfab import \"prod.Workspace/Report.Report\" -i /tmp/deploy/Report.Report -f\n```\n\n3. Verify:\n\n```bash\nfab exists \"prod.Workspace/Report.Report\"\n```\n\n## Clone Report with Different Model\n\n1. Export source:\n\n```bash\nfab export \"ws.Workspace/Template.Report\" -o /tmp/clone -f\n```\n\n2. Edit `/tmp/clone/Template.Report/definition.pbir` to update `semanticmodelid`\n\n3. Import as new report:\n\n```bash\nfab import \"ws.Workspace/NewReport.Report\" -i /tmp/clone/Template.Report -f\n```\n\n## Troubleshooting\n\n### Report Not Found\n\n```bash\nfab exists \"ws.Workspace\"\nfab ls \"ws.Workspace\" | grep -i report\n```\n\n### Model Binding Issues\n\n```bash\n# Check current binding\nfab get \"ws.Workspace/Report.Report\" -q \"definition.parts[?contains(path, 'definition.pbir')].payload | [0]\"\n\n# Rebind\nfab set \"ws.Workspace/Report.Report\" -q semanticModelId -i \"<model-id>\"\n```\n\n### Import Fails\n\n```bash\n# Verify structure\nls -R /tmp/exports/Report.Report/\n\n# Check definition is valid JSON\nfab get \"ws.Workspace/Report.Report\" -q \"definition\" | jq . > /dev/null && echo \"Valid\"\n```\n",
        "skills/fabric-cli/references/semantic-models.md": "# Semantic Model Operations\n\nComprehensive guide for working with semantic models (Power BI datasets) using the Fabric CLI.\n\n## Overview\n\nSemantic models in Fabric use TMDL (Tabular Model Definition Language) format for their definitions. This guide covers getting, updating, exporting, and managing semantic models.\n\n## Getting Model Information\n\n### Basic Model Info\n\n```bash\n# Check if model exists\nfab exists \"Production.Workspace/Sales.SemanticModel\"\n\n# Get model properties\nfab get \"Production.Workspace/Sales.SemanticModel\"\n\n# Get model with all details (verbose)\nfab get \"Production.Workspace/Sales.SemanticModel\" -v\n\n# Get only model ID\nfab get \"Production.Workspace/Sales.SemanticModel\" -q \"id\"\n```\n\n### Get Model Definition\n\nThe model definition contains all TMDL parts (tables, measures, relationships, etc.):\n\n```bash\n# Get full definition (all TMDL parts)\nfab get \"Production.Workspace/Sales.SemanticModel\" -q \"definition\"\n\n# Save definition to file\nfab get \"Production.Workspace/Sales.SemanticModel\" -q \"definition\" -o /tmp/model-def.json\n```\n\n### Get Specific TMDL Parts\n\n```bash\n# Get model.tmdl (main model properties)\nfab get \"Production.Workspace/Sales.SemanticModel\" -q \"definition.parts[?path=='model.tmdl'].payload | [0]\"\n\n# Get specific table definition\nfab get \"Production.Workspace/Sales.SemanticModel\" -q \"definition.parts[?path=='definition/tables/Customers.tmdl'].payload | [0]\"\n\n# Get all table definitions\nfab get \"Production.Workspace/Sales.SemanticModel\" -q \"definition.parts[?starts_with(path, 'definition/tables/')]\"\n\n# Get relationships.tmdl\nfab get \"Production.Workspace/Sales.SemanticModel\" -q \"definition.parts[?path=='definition/relationships.tmdl'].payload | [0]\"\n\n# Get functions.tmdl (DAX functions)\nfab get \"Production.Workspace/Sales.SemanticModel\" -q \"definition.parts[?path=='definition/functions.tmdl'].payload | [0]\"\n\n# Get all definition part paths (for reference)\nfab get \"Production.Workspace/Sales.SemanticModel\" -q \"definition.parts[].path\"\n```\n\n## Exporting Models\n\n### Export as PBIP (Power BI Project)\n\nPBIP format is best for local development in Power BI Desktop or Tabular Editor:\n\n```bash\n# Export using the export script\npython3 scripts/export_semantic_model_as_pbip.py \\\n  \"Production.Workspace/Sales.SemanticModel\" -o /tmp/exports\n```\n\n### Export as TMDL\n\nThe export script creates PBIP format which includes TMDL in the definition folder:\n\n```bash\npython3 scripts/export_semantic_model_as_pbip.py \\\n  \"Production.Workspace/Sales.SemanticModel\" -o /tmp/exports\n\n# TMDL files will be in: /tmp/exports/Sales.SemanticModel/definition/\n```\n\n### Export Specific Parts Only\n\n```bash\n# Export just tables\nfab get \"Production.Workspace/Sales.SemanticModel\" -q \"definition.parts[?starts_with(path, 'definition/tables/')]\" -o /tmp/tables.json\n\n# Export just measures (within tables)\nfab get \"Production.Workspace/Sales.SemanticModel\" -q \"definition.parts[?contains(path, '/tables/')]\" | grep -A 20 \"measure\"\n```\n\n## Listing Model Contents\n\n```bash\n# List all items in model (if OneLake enabled)\nfab ls \"Production.Workspace/Sales.SemanticModel\"\n\n# Query model structure via API\nfab api workspaces -q \"value[?displayName=='Production'].id | [0]\" | xargs -I {} \\\n  fab api \"workspaces/{}/items\" -q \"value[?type=='SemanticModel']\"\n```\n\n## Updating Model Definitions\n\n**CRITICAL**: When updating semantic models, you must:\n1. Get the full definition\n2. Modify the specific parts you want to change\n3. Include ALL parts in the update request (modified + unmodified)\n4. Never include `.platform` file\n5. Test immediately\n\n### Update Workflow\n\n```bash\n# 1. Get workspace and model IDs\nWS_ID=$(fab get \"Production.Workspace\" -q \"id\")\nMODEL_ID=$(fab get \"Production.Workspace/Sales.SemanticModel\" -q \"id\")\n\n# 2. Get current definition\nfab get \"Production.Workspace/Sales.SemanticModel\" -q \"definition\" -o /tmp/current-def.json\n\n# 3. Modify definition (edit JSON file or use script)\n# ... modify /tmp/current-def.json ...\n\n# 4. Wrap definition in update request\ncat > /tmp/update-request.json <<EOF\n{\n  \"definition\": $(cat /tmp/current-def.json)\n}\nEOF\n\n# 5. Update via API\nfab api -X post \"workspaces/$WS_ID/semanticModels/$MODEL_ID/updateDefinition\" \\\n  -i /tmp/update-request.json \\\n  --show_headers\n\n# 6. Extract operation ID from Location header and poll status\nOPERATION_ID=\"<extracted-from-Location-header>\"\nfab api \"operations/$OPERATION_ID\"\n```\n\n### Example: Add a Measure\n\n```python\n# Python script to add measure to definition\nimport json\n\nwith open('/tmp/current-def.json', 'r') as f:\n    definition = json.load(f)\n\n# Find the table's TMDL part\nfor part in definition['parts']:\n    if part['path'] == 'definition/tables/Sales.tmdl':\n        # Decode base64 content\n        import base64\n        tmdl_content = base64.b64decode(part['payload']).decode('utf-8')\n\n        # Add measure (simplified - real implementation needs proper TMDL syntax)\n        measure_tmdl = \"\"\"\nmeasure 'Total Revenue' = SUM(Sales[Amount])\n    formatString: #,0.00\n    displayFolder: \"KPIs\"\n\"\"\"\n        tmdl_content += measure_tmdl\n\n        # Re-encode\n        part['payload'] = base64.b64encode(tmdl_content.encode('utf-8')).decode('utf-8')\n\n# Save modified definition\nwith open('/tmp/modified-def.json', 'w') as f:\n    json.dump(definition, f)\n```\n\n## Executing DAX Queries\n\nUse Power BI API to execute DAX queries against semantic models:\n\n```bash\n# Get model ID\nMODEL_ID=$(fab get \"Production.Workspace/Sales.SemanticModel\" -q \"id\")\n\n# Execute simple DAX query\nfab api -A powerbi \"datasets/$MODEL_ID/executeQueries\" -X post -i '{\n  \"queries\": [{\n    \"query\": \"EVALUATE VALUES(Date[Year])\"\n  }]\n}'\n\n# Execute TOPN query\nfab api -A powerbi \"datasets/$MODEL_ID/executeQueries\" -X post -i '{\n  \"queries\": [{\n    \"query\": \"EVALUATE TOPN(10, Sales, Sales[Amount], DESC)\"\n  }]\n}'\n\n# Execute multiple queries\nfab api -A powerbi \"datasets/$MODEL_ID/executeQueries\" -X post -i '{\n  \"queries\": [\n    {\"query\": \"EVALUATE VALUES(Date[Year])\"},\n    {\"query\": \"EVALUATE SUMMARIZE(Sales, Date[Year], \\\"Total\\\", SUM(Sales[Amount]))\"}\n  ],\n  \"serializerSettings\": {\n    \"includeNulls\": false\n  }\n}'\n\n# Execute query with parameters\nfab api -A powerbi \"datasets/$MODEL_ID/executeQueries\" -X post -i '{\n  \"queries\": [{\n    \"query\": \"EVALUATE FILTER(Sales, Sales[Year] = @Year)\",\n    \"parameters\": [\n      {\"name\": \"@Year\", \"value\": \"2024\"}\n    ]\n  }]\n}'\n```\n\n#### Using the DAX query script\n\n```bash\npython3 scripts/execute_dax.py \\\n  --workspace \"Production.Workspace\" \\\n  --model \"Sales.SemanticModel\" \\\n  --query \"EVALUATE TOPN(10, Sales)\" \\\n  --output /tmp/results.json\n```\n\n## Refreshing Models\n\n```bash\n# Get workspace and model IDs\nWS_ID=$(fab get \"Production.Workspace\" -q \"id\")\nMODEL_ID=$(fab get \"Production.Workspace/Sales.SemanticModel\" -q \"id\")\n\n# Trigger full refresh\nfab api -A powerbi \"groups/$WS_ID/datasets/$MODEL_ID/refreshes\" -X post -i '{\"type\":\"Full\"}'\n\n# Check latest refresh status\nfab api -A powerbi \"groups/$WS_ID/datasets/$MODEL_ID/refreshes?\\$top=1\"\n\n# Get refresh history\nfab api -A powerbi \"groups/$WS_ID/datasets/$MODEL_ID/refreshes\"\n\n# Cancel refresh\nREFRESH_ID=\"<refresh-request-id>\"\nfab api -A powerbi \"groups/$WS_ID/datasets/$MODEL_ID/refreshes/$REFRESH_ID\" -X delete\n```\n\n## Model Refresh Schedule\n\n```bash\nMODEL_ID=$(fab get \"Production.Workspace/Sales.SemanticModel\" -q \"id\")\n\n# Get current schedule\nfab api -A powerbi \"datasets/$MODEL_ID/refreshSchedule\"\n\n# Update schedule (daily at 2 AM)\nfab api -A powerbi \"datasets/$MODEL_ID/refreshSchedule\" -X patch -i '{\n  \"enabled\": true,\n  \"days\": [\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\"],\n  \"times\": [\"02:00\"],\n  \"localTimeZoneId\": \"UTC\"\n}'\n\n# Disable schedule\nfab api -A powerbi \"datasets/$MODEL_ID/refreshSchedule\" -X patch -i '{\n  \"enabled\": false\n}'\n```\n\n## Copying Models\n\n```bash\n# Copy semantic model between workspaces (full paths required)\nfab cp \"Dev.Workspace/Sales.SemanticModel\" \"Production.Workspace/Sales.SemanticModel\" -f\n\n# Copy with new name\nfab cp \"Dev.Workspace/Sales.SemanticModel\" \"Production.Workspace/SalesProduction.SemanticModel\" -f\n\n# Note: Both source and destination must include workspace.Workspace/model.SemanticModel\n# This copies the definition, not data or refreshes\n```\n\n## Model Deployment Workflow\n\n### Dev to Production\n\n```bash\n#!/bin/bash\n\nDEV_WS=\"Development.Workspace\"\nPROD_WS=\"Production.Workspace\"\nMODEL_NAME=\"Sales.SemanticModel\"\n\n# 1. Export from dev\nfab export \"$DEV_WS/$MODEL_NAME\" -o /tmp/deployment\n\n# 2. Test locally (optional - requires Power BI Desktop)\n# Open /tmp/deployment/Sales/*.pbip in Power BI Desktop\n\n# 3. Import to production\nfab import \"$PROD_WS/$MODEL_NAME\" -i /tmp/deployment/$MODEL_NAME\n\n# 4. Trigger refresh in production\nPROD_MODEL_ID=$(fab get \"$PROD_WS/$MODEL_NAME\" -q \"id\")\nfab api -A powerbi \"datasets/$PROD_MODEL_ID/refreshes\" -X post -i '{\"type\": \"Full\"}'\n\n# 5. Monitor refresh\nsleep 10\nfab api -A powerbi \"datasets/$PROD_MODEL_ID/refreshes\" -q \"value[0]\"\n```\n\n## Working with Model Metadata\n\n### Update Display Name\n\n```bash\nfab set \"Production.Workspace/Sales.SemanticModel\" -q displayName -i \"Sales Analytics Model\"\n```\n\n### Update Description\n\n```bash\nfab set \"Production.Workspace/Sales.SemanticModel\" -q description -i \"Primary sales analytics semantic model for production reporting\"\n```\n\n## Advanced Patterns\n\n### Extract All Measures\n\n```bash\n# Get all table definitions containing measures\nfab get \"Production.Workspace/Sales.SemanticModel\" -q \"definition.parts[?contains(path, '/tables/')]\" -o /tmp/tables.json\n\n# Process with script to extract measures\npython3 << 'EOF'\nimport json\nimport base64\n\nwith open('/tmp/tables.json', 'r') as f:\n    parts = json.load(f)\n\nmeasures = []\nfor part in parts:\n    if 'tables' in part['path']:\n        content = base64.b64decode(part['payload']).decode('utf-8')\n        # Extract measure definitions (simple regex - real parser needed for production)\n        import re\n        measure_blocks = re.findall(r'measure\\s+[^\\n]+\\s*=.*?(?=\\n\\s*(?:measure|column|$))', content, re.DOTALL)\n        measures.extend(measure_blocks)\n\nfor i, measure in enumerate(measures, 1):\n    print(f\"\\n--- Measure {i} ---\")\n    print(measure)\nEOF\n```\n\n### Compare Models (Diff)\n\n```bash\n# Export both models\nfab get \"Production.Workspace/Sales.SemanticModel\" -q \"definition\" -o /tmp/model1-def.json\nfab get \"Dev.Workspace/Sales.SemanticModel\" -q \"definition\" -o /tmp/model2-def.json\n\n# Use diff tool\ndiff <(jq -S . /tmp/model1-def.json) <(jq -S . /tmp/model2-def.json)\n\n# jq -S sorts keys for consistent comparison\n```\n\n### Backup Model Definition\n\n```bash\n#!/bin/bash\n\nWORKSPACE=\"Production.Workspace\"\nMODEL=\"Sales.SemanticModel\"\nBACKUP_DIR=\"/backups/$(date +%Y%m%d)\"\n\nmkdir -p \"$BACKUP_DIR\"\n\n# Export as multiple formats for redundancy\nfab get \"$WORKSPACE/$MODEL\" -q \"definition\" -o \"$BACKUP_DIR/definition.json\"\n\n# Export as PBIP\npython3 scripts/export_semantic_model_as_pbip.py \\\n  \"$WORKSPACE/$MODEL\" -o \"$BACKUP_DIR/pbip\"\n\n# Save metadata\nfab get \"$WORKSPACE/$MODEL\" -o \"$BACKUP_DIR/metadata.json\"\n\necho \"Backup completed: $BACKUP_DIR\"\n```\n\n## TMDL Structure Reference\n\nA semantic model's TMDL definition consists of these parts:\n\n```\nmodel.tmdl                          # Model properties, culture, compatibility\n.platform                           # Git integration metadata (exclude from updates)\ndefinition/\n├── model.tmdl                     # Alternative location for model properties\n├── database.tmdl                  # Database properties\n├── roles.tmdl                     # Row-level security roles\n├── relationships.tmdl             # Relationships between tables\n├── functions.tmdl                 # DAX user-defined functions\n├── expressions/                   # M queries for data sources\n│   ├── Source1.tmdl\n│   └── Source2.tmdl\n└── tables/                        # Table definitions\n    ├── Customers.tmdl             # Columns, measures, hierarchies\n    ├── Sales.tmdl\n    ├── Products.tmdl\n    └── Date.tmdl\n```\n\n### Common TMDL Parts to Query\n\n```bash\nMODEL=\"Production.Workspace/Sales.SemanticModel\"\n\n# Model properties\nfab get \"$MODEL\" -q \"definition.parts[?path=='model.tmdl'].payload | [0]\"\n\n# Roles and RLS\nfab get \"$MODEL\" -q \"definition.parts[?path=='definition/roles.tmdl'].payload | [0]\"\n\n# Relationships\nfab get \"$MODEL\" -q \"definition.parts[?path=='definition/relationships.tmdl'].payload | [0]\"\n\n# Data source expressions\nfab get \"$MODEL\" -q \"definition.parts[?starts_with(path, 'definition/expressions/')]\"\n\n# All tables\nfab get \"$MODEL\" -q \"definition.parts[?starts_with(path, 'definition/tables/')].path\"\n```\n\n## Troubleshooting\n\n### Model Not Found\n\n```bash\n# Verify workspace exists\nfab exists \"Production.Workspace\"\n\n# List semantic models in workspace\nWS_ID=$(fab get \"Production.Workspace\" -q \"id\")\nfab api \"workspaces/$WS_ID/items\" -q \"value[?type=='SemanticModel']\"\n```\n\n### Update Definition Fails\n\nCommon issues:\n1. **Included `.platform` file**: Never include this in updates\n2. **Missing parts**: Must include ALL parts, not just modified ones\n3. **Invalid TMDL syntax**: Validate TMDL before updating\n4. **Encoding issues**: Ensure base64 encoding is correct\n\n```bash\n# Debug update operation\nfab api \"operations/$OPERATION_ID\" -q \"error\"\n```\n\n### DAX Query Errors\n\n```bash\n# Check model is online\nfab get \"Production.Workspace/Sales.SemanticModel\" -q \"properties\"\n\n# Try simple query first\nMODEL_ID=$(fab get \"Production.Workspace/Sales.SemanticModel\" -q \"id\")\nfab api -A powerbi \"datasets/$MODEL_ID/executeQueries\" -X post -i '{\n  \"queries\": [{\"query\": \"EVALUATE {1}\"}]\n}'\n```\n\n## Storage Mode\n\nCheck table partition mode to determine if model is Direct Lake, Import, or DirectQuery.\n\n```bash\n# Get table definition and check partition mode\nfab get \"ws.Workspace/Model.SemanticModel\" -q \"definition.parts[?contains(path, 'tables/TableName')].payload | [0]\"\n```\n\nOutput shows partition type:\n\n```\n# Direct Lake\npartition TableName = entity\n    mode: directLake\n    source\n        entityName: table_name\n        schemaName: schema\n        expressionSource: DatabaseQuery\n\n# Import\npartition TableName = m\n    mode: import\n    source =\n        let\n            Source = Sql.Database(\"connection\", \"database\"),\n            Data = Source{[Schema=\"schema\",Item=\"table\"]}[Data]\n        in\n            Data\n```\n\n## Workspace Access\n\n```bash\n# Get workspace ID\nfab get \"ws.Workspace\" -q \"id\"\n\n# List users with access\nfab api -A powerbi \"groups/<workspace-id>/users\"\n```\n\nOutput:\n\n```json\n{\n  \"value\": [\n    {\n      \"emailAddress\": \"user@domain.com\",\n      \"groupUserAccessRight\": \"Admin\",\n      \"displayName\": \"User Name\",\n      \"principalType\": \"User\"\n    }\n  ]\n}\n```\n\nAccess rights: `Admin`, `Member`, `Contributor`, `Viewer`\n\n## Find Reports Using a Model\n\nCheck report's `definition.pbir` for `byConnection.semanticmodelid`:\n\n```bash\n# Get model ID\nfab get \"ws.Workspace/Model.SemanticModel\" -q \"id\"\n\n# Check a report's connection\nfab get \"ws.Workspace/Report.Report\" -q \"definition.parts[?contains(path, 'definition.pbir')].payload | [0]\"\n```\n\nOutput:\n\n```json\n{\n  \"datasetReference\": {\n    \"byConnection\": {\n      \"connectionString\": \"...semanticmodelid=bee906a0-255e-...\"\n    }\n  }\n}\n```\n\nTo find all reports using a model, check each report's definition.pbir for matching `semanticmodelid`.\n\n## Performance Tips\n\n1. **Cache model IDs**: Don't repeatedly query for the same ID\n2. **Use JMESPath filtering**: Get only what you need\n3. **Batch DAX queries**: Combine multiple queries in one request\n4. **Export during off-hours**: Large model exports can be slow\n5. **Use Power BI API for queries**: It's optimized for DAX execution\n\n## Security Considerations\n\n1. **Row-Level Security**: Check roles before exposing data\n2. **Credentials in data sources**: Don't commit data source credentials\n3. **Sensitive measures**: Review calculated columns/measures for sensitive logic\n4. **Export restrictions**: Ensure exported models don't contain sensitive data\n\n## Related Scripts\n\n- `scripts/create_direct_lake_model.py` - Create Direct Lake model from lakehouse table\n- `scripts/export_semantic_model_as_pbip.py` - Export model as PBIP\n- `scripts/execute_dax.py` - Execute DAX queries\n",
        "skills/fabric-cli/references/workspaces.md": "# Workspace Operations\n\nComprehensive guide for managing Fabric workspaces using the Fabric CLI.\n\n## Overview\n\nWorkspaces are containers for Fabric items and provide collaboration and security boundaries. This guide covers workspace management, configuration, and operations.\n\n## Listing Workspaces\n\n### List All Workspaces\n\n```bash\n# Simple list\nfab ls\n\n# Detailed list with metadata\nfab ls -l\n\n# List with hidden tenant-level items\nfab ls -la\n\n# Hidden items include: capacities, connections, domains, gateways\n```\n\n### Filter Workspaces\n\n```bash\n# Using API with JMESPath query\nfab api workspaces -q \"value[].{name: displayName, id: id, type: type}\"\n\n# Filter by name pattern\nfab api workspaces -q \"value[?contains(displayName, 'Production')]\"\n\n# Filter by capacity\nfab api workspaces -q \"value[?capacityId=='<capacity-id>']\"\n\n# Get workspace count\nfab api workspaces -q \"value | length\"\n```\n\n## Getting Workspace Information\n\n### Basic Workspace Info\n\n```bash\n# Check if workspace exists\nfab exists \"Production.Workspace\"\n\n# Get workspace details\nfab get \"Production.Workspace\"\n\n# Get specific property\nfab get \"Production.Workspace\" -q \"id\"\nfab get \"Production.Workspace\" -q \"capacityId\"\nfab get \"Production.Workspace\" -q \"description\"\n\n# Get all properties (verbose)\nfab get \"Production.Workspace\" -v\n\n# Save to file\nfab get \"Production.Workspace\" -o /tmp/workspace-info.json\n```\n\n### Get Workspace Configuration\n\n```bash\n# Get Spark settings\nfab get \"Production.Workspace\" -q \"sparkSettings\"\n\n# Get Spark runtime version\nfab get \"Production.Workspace\" -q \"sparkSettings.environment.runtimeVersion\"\n\n# Get default Spark pool\nfab get \"Production.Workspace\" -q \"sparkSettings.pool.defaultPool\"\n```\n\n## Creating Workspaces\n\n### Create with Default Capacity\n\n```bash\n# Use CLI-configured default capacity\nfab mkdir \"NewWorkspace.Workspace\"\n\n# Verify capacity configuration first\nfab api workspaces -q \"value[0].capacityId\"\n```\n\n### Create with Specific Capacity\n\n```bash\n# Assign to specific capacity\nfab mkdir \"Production Workspace.Workspace\" -P capacityname=ProductionCapacity\n\n# Get capacity name from capacity list\nfab ls -la | grep Capacity\n```\n\n### Create without Capacity\n\n```bash\n# Create in shared capacity (not recommended for production)\nfab mkdir \"Dev Workspace.Workspace\" -P capacityname=none\n```\n\n## Listing Workspace Contents\n\n### List Items in Workspace\n\n```bash\n# Simple list\nfab ls \"Production.Workspace\"\n\n# Detailed list with metadata\nfab ls \"Production.Workspace\" -l\n\n# Include hidden items (Spark pools, managed identities, etc.)\nfab ls \"Production.Workspace\" -la\n\n# Hidden workspace items include:\n# - External Data Shares\n# - Managed Identities\n# - Managed Private Endpoints\n# - Spark Pools\n```\n\n### Filter Items by Type\n\n```bash\nWS_ID=$(fab get \"Production.Workspace\" -q \"id\")\n\n# List semantic models only\nfab api \"workspaces/$WS_ID/items\" -q \"value[?type=='SemanticModel']\"\n\n# List reports only\nfab api \"workspaces/$WS_ID/items\" -q \"value[?type=='Report']\"\n\n# List notebooks\nfab api \"workspaces/$WS_ID/items\" -q \"value[?type=='Notebook']\"\n\n# List lakehouses\nfab api \"workspaces/$WS_ID/items\" -q \"value[?type=='Lakehouse']\"\n\n# Count items by type\nfab api \"workspaces/$WS_ID/items\" -q \"value | group_by(@, &type)\"\n```\n\n## Updating Workspaces\n\n### Update Display Name\n\n```bash\nfab set \"OldName.Workspace\" -q displayName -i \"NewName\"\n\n# Note: This changes the display name, not the workspace ID\n```\n\n### Update Description\n\n```bash\nfab set \"Production.Workspace\" -q description -i \"Production environment for enterprise analytics\"\n```\n\n### Configure Spark Settings\n\n```bash\n# Set Spark runtime version\nfab set \"Production.Workspace\" -q sparkSettings.environment.runtimeVersion -i 1.2\n\n# Set starter pool as default\nfab set \"Production.Workspace\" -q sparkSettings.pool.defaultPool -i '{\n  \"name\": \"Starter Pool\",\n  \"type\": \"Workspace\"\n}'\n\n# Set custom workspace pool\nfab set \"Production.Workspace\" -q sparkSettings.pool.defaultPool -i '{\n  \"name\": \"HighMemoryPool\",\n  \"type\": \"Workspace\",\n  \"id\": \"<pool-id>\"\n}'\n```\n\n## Capacity Management\n\n### Assign Workspace to Capacity\n\n```bash\n# Get capacity ID\nCAPACITY_ID=$(fab api -A azure \"subscriptions/<subscription-id>/providers/Microsoft.Fabric/capacities?api-version=2023-11-01\" -q \"value[?name=='MyCapacity'].id | [0]\")\n\n# Assign workspace\nfab assign \"Production.Workspace\" -P capacityId=$CAPACITY_ID\n```\n\n### Unassign from Capacity\n\n```bash\n# Move to shared capacity\nfab unassign \"Dev.Workspace\"\n```\n\n### List Workspaces by Capacity\n\n```bash\n# Get all workspaces\nfab api workspaces -q \"value[] | group_by(@, &capacityId)\"\n\n# List workspaces on specific capacity\nfab api workspaces -q \"value[?capacityId=='<capacity-id>'].displayName\"\n```\n\n## Workspace Migration\n\n### Export Entire Workspace\n\n```bash\n# Export all items\nfab export \"Production.Workspace\" -o /tmp/workspace-backup -a\n\n# This exports all supported item types:\n# - Notebooks\n# - Data Pipelines\n# - Reports\n# - Semantic Models\n# - etc.\n```\n\n### Selective Export\n\n```bash\n#!/bin/bash\n\nWORKSPACE=\"Production.Workspace\"\nOUTPUT_DIR=\"/tmp/migration\"\n\n# Export only semantic models\nWS_ID=$(fab get \"$WORKSPACE\" -q \"id\")\nMODELS=$(fab api \"workspaces/$WS_ID/items\" -q \"value[?type=='SemanticModel'].displayName\")\n\nfor MODEL in $MODELS; do\n  fab export \"$WORKSPACE/$MODEL.SemanticModel\" -o \"$OUTPUT_DIR/models\"\ndone\n\n# Export only reports\nREPORTS=$(fab api \"workspaces/$WS_ID/items\" -q \"value[?type=='Report'].displayName\")\n\nfor REPORT in $REPORTS; do\n  fab export \"$WORKSPACE/$REPORT.Report\" -o \"$OUTPUT_DIR/reports\"\ndone\n```\n\n### Copy Workspace Contents\n\n```bash\n# Copy all items to another workspace (interactive selection)\nfab cp \"Source.Workspace\" \"Target.Workspace\"\n\n# Copy specific items\nfab cp \"Source.Workspace/Model.SemanticModel\" \"Target.Workspace\"\nfab cp \"Source.Workspace/Report.Report\" \"Target.Workspace\"\nfab cp \"Source.Workspace/Notebook.Notebook\" \"Target.Workspace\"\n```\n\n## Deleting Workspaces\n\n### Delete with Confirmation\n\n```bash\n# Interactive confirmation (lists items first)\nfab rm \"OldWorkspace.Workspace\"\n```\n\n### Force Delete\n\n```bash\n# Delete workspace and all contents without confirmation\n# ⚠️ DANGEROUS - Cannot be undone\nfab rm \"TestWorkspace.Workspace\" -f\n```\n\n## Navigation\n\n### Change to Workspace\n\n```bash\n# Navigate to workspace\nfab cd \"Production.Workspace\"\n\n# Verify current location\nfab pwd\n\n# Navigate to personal workspace\nfab cd ~\n```\n\n### Relative Navigation\n\n```bash\n# From workspace to another\nfab cd \"../Dev.Workspace\"\n\n# To parent (tenant level)\nfab cd ..\n```\n\n## Workspace Inventory\n\n### Get Complete Inventory\n\n```bash\n#!/bin/bash\n\nWORKSPACE=\"Production.Workspace\"\nWS_ID=$(fab get \"$WORKSPACE\" -q \"id\")\n\necho \"=== Workspace: $WORKSPACE ===\"\necho\n\n# Get all items\nITEMS=$(fab api \"workspaces/$WS_ID/items\")\n\n# Count by type\necho \"Item Counts:\"\necho \"$ITEMS\" | jq -r '.value | group_by(.type) | map({type: .[0].type, count: length}) | .[] | \"\\(.type): \\(.count)\"'\n\necho\necho \"Total Items: $(echo \"$ITEMS\" | jq '.value | length')\"\n\n# List items\necho\necho \"=== Items ===\"\necho \"$ITEMS\" | jq -r '.value[] | \"\\(.type): \\(.displayName)\"' | sort\n```\n\n### Generate Inventory Report\n\n```bash\n#!/bin/bash\n\nOUTPUT_FILE=\"/tmp/workspace-inventory.csv\"\n\necho \"Workspace,Item Type,Item Name,Created Date,Modified Date\" > \"$OUTPUT_FILE\"\n\n# Get all workspaces\nWORKSPACES=$(fab api workspaces -q \"value[].{name: displayName, id: id}\")\n\necho \"$WORKSPACES\" | jq -r '.[] | [.name, .id] | @tsv' | while IFS=$'\\t' read -r WS_NAME WS_ID; do\n  # Get items in workspace\n  ITEMS=$(fab api \"workspaces/$WS_ID/items\")\n\n  echo \"$ITEMS\" | jq -r --arg ws \"$WS_NAME\" '.value[] | [$ws, .type, .displayName, .createdDate, .lastModifiedDate] | @csv' >> \"$OUTPUT_FILE\"\ndone\n\necho \"Inventory saved to $OUTPUT_FILE\"\n```\n\n## Workspace Permissions\n\n### List Workspace Users\n\n```bash\nWS_ID=$(fab get \"Production.Workspace\" -q \"id\")\n\n# List users with access\nfab api -A powerbi \"groups/$WS_ID/users\"\n```\n\n### Add User to Workspace\n\n```bash\nWS_ID=$(fab get \"Production.Workspace\" -q \"id\")\n\n# Add user as member\nfab api -A powerbi \"groups/$WS_ID/users\" -X post -i '{\n  \"emailAddress\": \"user@company.com\",\n  \"groupUserAccessRight\": \"Member\"\n}'\n\n# Access levels: Admin, Member, Contributor, Viewer\n```\n\n### Remove User from Workspace\n\n```bash\nWS_ID=$(fab get \"Production.Workspace\" -q \"id\")\n\n# Remove user\nfab api -A powerbi \"groups/$WS_ID/users/user@company.com\" -X delete\n```\n\n## Workspace Settings\n\n### Git Integration\n\n```bash\nWS_ID=$(fab get \"Production.Workspace\" -q \"id\")\n\n# Get Git connection status\nfab api \"workspaces/$WS_ID/git/connection\"\n\n# Connect to Git (requires Git integration setup)\nfab api -X post \"workspaces/$WS_ID/git/initializeConnection\" -i '{\n  \"gitProviderDetails\": {\n    \"organizationName\": \"myorg\",\n    \"projectName\": \"fabric-project\",\n    \"repositoryName\": \"production\",\n    \"branchName\": \"main\",\n    \"directoryName\": \"/workspace-content\"\n  }\n}'\n```\n\n## Advanced Workflows\n\n### Clone Workspace\n\n```bash\n#!/bin/bash\n\nSOURCE_WS=\"Template.Workspace\"\nTARGET_WS=\"New Project.Workspace\"\nCAPACITY=\"MyCapacity\"\n\n# 1. Create target workspace\nfab mkdir \"$TARGET_WS\" -P capacityname=$CAPACITY\n\n# 2. Export all items from source\nfab export \"$SOURCE_WS\" -o /tmp/clone -a\n\n# 3. Import items to target\nfor ITEM in /tmp/clone/*; do\n  ITEM_NAME=$(basename \"$ITEM\")\n  fab import \"$TARGET_WS/$ITEM_NAME\" -i \"$ITEM\"\ndone\n\necho \"Workspace cloned successfully\"\n```\n\n### Workspace Comparison\n\n```bash\n#!/bin/bash\n\nWS1=\"Production.Workspace\"\nWS2=\"Development.Workspace\"\n\nWS1_ID=$(fab get \"$WS1\" -q \"id\")\nWS2_ID=$(fab get \"$WS2\" -q \"id\")\n\necho \"=== Comparing Workspaces ===\"\necho\n\necho \"--- $WS1 ---\"\nfab api \"workspaces/$WS1_ID/items\" -q \"value[].{type: type, name: displayName}\" | jq -r '.[] | \"\\(.type): \\(.name)\"' | sort > /tmp/ws1.txt\n\necho \"--- $WS2 ---\"\nfab api \"workspaces/$WS2_ID/items\" -q \"value[].{type: type, name: displayName}\" | jq -r '.[] | \"\\(.type): \\(.name)\"' | sort > /tmp/ws2.txt\n\necho\necho \"=== Differences ===\"\ndiff /tmp/ws1.txt /tmp/ws2.txt\n\nrm /tmp/ws1.txt /tmp/ws2.txt\n```\n\n### Batch Workspace Operations\n\n```bash\n#!/bin/bash\n\n# Update description for all production workspaces\nPROD_WORKSPACES=$(fab api workspaces -q \"value[?contains(displayName, 'Prod')].displayName\")\n\nfor WS in $PROD_WORKSPACES; do\n  echo \"Updating $WS...\"\n  fab set \"$WS.Workspace\" -q description -i \"Production environment - managed by Data Platform team\"\ndone\n```\n\n## Workspace Monitoring\n\n### Monitor Workspace Activity\n\n```bash\nWS_ID=$(fab get \"Production.Workspace\" -q \"id\")\n\n# Get activity events (requires admin access)\nfab api -A powerbi \"admin/activityevents?filter=Workspace%20eq%20'$WS_ID'\"\n```\n\n### Track Workspace Size\n\n```bash\n#!/bin/bash\n\nWORKSPACE=\"Production.Workspace\"\nWS_ID=$(fab get \"$WORKSPACE\" -q \"id\")\n\n# Count items\nITEM_COUNT=$(fab api \"workspaces/$WS_ID/items\" -q \"value | length\")\n\n# Count by type\necho \"=== Workspace: $WORKSPACE ===\"\necho \"Total Items: $ITEM_COUNT\"\necho\n\necho \"Items by Type:\"\nfab api \"workspaces/$WS_ID/items\" -q \"value | group_by(@, &type) | map({type: .[0].type, count: length}) | sort_by(.count) | reverse | .[]\" | jq -r '\"\\(.type): \\(.count)\"'\n```\n\n## Troubleshooting\n\n### Workspace Not Found\n\n```bash\n# List all workspaces to verify name\nfab ls | grep -i \"production\"\n\n# Get by ID directly\nfab api \"workspaces/<workspace-id>\"\n```\n\n### Capacity Issues\n\n```bash\n# Check workspace capacity assignment\nfab get \"Production.Workspace\" -q \"capacityId\"\n\n# List available capacities\nfab ls -la | grep Capacity\n\n# Verify capacity status (via Azure API)\nfab api -A azure \"subscriptions/<subscription-id>/providers/Microsoft.Fabric/capacities?api-version=2023-11-01\" -q \"value[].{name: name, state: properties.state, sku: sku.name}\"\n```\n\n### Permission Errors\n\n```bash\n# Verify your access level\nWS_ID=$(fab get \"Production.Workspace\" -q \"id\")\nfab api -A powerbi \"groups/$WS_ID/users\" | grep \"$(whoami)\"\n\n# Check if you're workspace admin\nfab api -A powerbi \"groups/$WS_ID/users\" -q \"value[?emailAddress=='your@email.com'].groupUserAccessRight\"\n```\n\n## Best Practices\n\n1. **Naming conventions**: Use consistent naming (e.g., \"ProjectName - Environment\")\n2. **Capacity planning**: Assign workspaces to appropriate capacities\n3. **Access control**: Use least-privilege principle for permissions\n4. **Git integration**: Enable for production workspaces\n5. **Regular backups**: Export critical workspaces periodically\n6. **Documentation**: Maintain workspace descriptions\n7. **Monitoring**: Track workspace activity and growth\n8. **Cleanup**: Remove unused workspaces regularly\n\n## Performance Tips\n\n1. **Cache workspace IDs**: Don't repeatedly query for same ID\n2. **Use JMESPath filters**: Get only needed data\n3. **Parallel operations**: Export multiple items concurrently\n4. **Batch updates**: Group similar operations\n5. **Off-peak operations**: Schedule large migrations during low usage\n\n## Security Considerations\n\n1. **Access reviews**: Regularly audit workspace permissions\n2. **Sensitive data**: Use appropriate security labels\n3. **Capacity isolation**: Separate dev/test/prod workspaces\n4. **Git secrets**: Don't commit credentials in Git-integrated workspaces\n5. **Audit logging**: Enable and monitor activity logs\n\n## Related Scripts\n\n- `scripts/download_workspace.py` - Download complete workspace with all items and lakehouse files\n",
        "skills/fabric-cli/scripts/README.md": "# Fabric CLI Utility Scripts\n\nPython scripts extending `fab` CLI with common operations. All scripts use the same path syntax as fab commands.\n\n## Path Syntax\n\nAll scripts use Fabric path format: `Workspace.Workspace/Item.ItemType`\n\n```bash\n# Examples\n\"Sales.Workspace/Model.SemanticModel\"\n\"Production.Workspace/LH.Lakehouse\"\n\"Dev.Workspace/Report.Report\"\n```\n\n## Scripts\n\n### create_direct_lake_model.py\n\nCreate a Direct Lake semantic model from lakehouse tables. This is the recommended approach for querying lakehouse data via DAX.\n\n```bash\npython3 create_direct_lake_model.py \"src.Workspace/LH.Lakehouse\" \"dest.Workspace/Model.SemanticModel\" -t schema.table\npython3 create_direct_lake_model.py \"Sales.Workspace/SalesLH.Lakehouse\" \"Sales.Workspace/Sales Model.SemanticModel\" -t gold.orders\n```\n\nArguments:\n\n- `source` - Source lakehouse: Workspace.Workspace/Lakehouse.Lakehouse\n- `dest` - Destination model: Workspace.Workspace/Model.SemanticModel\n- `-t, --table` - Table in schema.table format (required)\n\n### execute_dax.py\n\nExecute DAX queries against semantic models.\n\n```bash\npython3 execute_dax.py \"ws.Workspace/Model.SemanticModel\" -q \"EVALUATE VALUES('Date'[Year])\"\npython3 execute_dax.py \"Sales.Workspace/Sales Model.SemanticModel\" -q \"EVALUATE TOPN(10, 'Orders')\" --format csv\npython3 execute_dax.py \"ws.Workspace/Model.SemanticModel\" -q \"EVALUATE ROW(\\\"Total\\\", SUM('Sales'[Amount]))\" -o results.json\n```\n\nOptions:\n\n- `-q, --query` - DAX query (required)\n- `-o, --output` - Output file\n- `--format` - Output format: table (default), csv, json\n- `--include-nulls` - Include null values\n\n### export_semantic_model_as_pbip.py\n\nExport semantic model as PBIP (Power BI Project) format.\n\n```bash\npython3 export_semantic_model_as_pbip.py \"ws.Workspace/Model.SemanticModel\" -o ./output\npython3 export_semantic_model_as_pbip.py \"Sales.Workspace/Sales Model.SemanticModel\" -o /tmp/exports\n```\n\nCreates complete PBIP structure with TMDL definition and blank report.\n\n### download_workspace.py\n\nDownload complete workspace with all items and lakehouse files.\n\n```bash\npython3 download_workspace.py \"Sales.Workspace\"\npython3 download_workspace.py \"Production.Workspace\" ./backup\npython3 download_workspace.py \"Dev.Workspace\" --no-lakehouse-files\n```\n\nOptions:\n\n- `output_dir` - Output directory (default: ./workspace_downloads/<name>)\n- `--no-lakehouse-files` - Skip lakehouse file downloads\n\n## Requirements\n\n- Python 3.10+\n- `fab` CLI installed and authenticated\n- For lakehouse file downloads: `azure-storage-file-datalake`, `azure-identity`\n",
        "skills/fabric-cli/servers/win32/Resources/calendar_instructions_and_examples.md": "---\r\nname: 'Calendar Instructions and Examples'\r\ndescription: 'Guidelines for creating Power BI calendar objects'\r\nuriTemplate: 'resource://calendar_instructions_and_examples'\r\n---\r\n# Calendar Column Groups Guide\r\n\r\nThis guide explains how to define calendar column groups in a Power BI date table so time intelligence works as expected and consistently across models.\r\n\r\n## Concepts\r\n\r\n- Calendar Column Groups. Use these when a primary column cleanly represents a standard time unit such as Year, Quarter, Month, Week, or Date. Time units (including \"of year\" variants) are defined by a fixed enumeration; use the exact casing shown below.\r\n- Time-related groups. Use these for relative columns that are time-aware but are not a standard time unit (for example, RelativeMonth with values like \"Current\"/\"Previous\"). They can be used to slice/label time-aware analyses but do not themselves define a standard unit.\r\n- Primary vs. associated columns. When a column maps to a specific unit, make it the primaryColumn for that unit. If column A is sorted by column B (Power BI SortByColumn), then B should be the primaryColumn and A should be an associatedColumn. Optionally add other 1-to-1 associatedColumns for alternate labels (e.g., a long and a short month name).\r\n\r\n## Mapping guidance\r\n\r\n- Prefer a time unit group when the mapping is unambiguous; use the exact unit name from the list below.\r\n- Use a time-related group for relative states (e.g., current/previous/next) that don't represent a standard unit. Time-related groups have unknown time units.\r\n- For textual labels that are sorted by another column, use the sort column as the primary and add the text label as an associatedColumn.\r\n- Add associatedColumns only for strict 1-to-1 label relationships.\r\n- Each calendar definition should use columns from only its host table.\r\n- Build hierarchies that roll up cleanly (Year → Quarter → Month → Date).\r\n- Do not repeat a time unit within the same calendar.\r\n- Columns tagged in a calendar must be tagged to the same time unit (or as a time-related column) across all calendars.\r\n- Do not use the same physical column more than once in the same calendar.\r\n- Associated columns are optional but must be 1-to-1 with the primary.\r\n- Complete vs. Partial units:\r\n  - Complete units uniquely identify a single period and must include the calendar context (e.g., include the year): Year, Quarter, Month, Week, Date.\r\n    - Examples: 2024 (Year), Q3 2024 (Quarter), 2024-01 or \"January 2024\" (Month), 2024-W49 (Week), 2024-01-15 (Date).\r\n  - Partial units are positions within a larger period and are not unique by themselves: QuarterOfYear (1–4), MonthOfYear (1–12 or names), WeekOfYear (1–52/53), DayOfYear (1–365/366). Variants exist for Quarter/Month (e.g., MonthOfQuarter).\r\n    - Use these primarily for labels, slicers, or seasonality—not as keys or for hierarchical rollups.\r\n  - Mapping examples:\r\n    - \"December 2024\" → Month (complete, includes year). \"December\" → MonthOfYear (not unique across years).\r\n    - \"Q3 2023\" → Quarter. \"Q3\" → QuarterOfYear.\r\n    - \"2024-W49\" or \"Week 49 of 2024\" → Week. \"Week 49\" → WeekOfYear.\r\n    - \"15th day of month\" → DayOfMonth. \"15th day of the year\" → DayOfYear.\r\n  - Rules of thumb:\r\n    - For standard hierarchies (Year → Quarter → Month → Date), use complete units at every level.\r\n    - You may associate a partial label with a complete primary (e.g., Month primary: Year Month; associated label: MonthOfYear name) if it is 1-to-1 with the primary.\r\n    - Do not map MonthOfYear to Month, WeekOfYear to Week, or QuarterOfYear to Quarter—these are different concepts.\r\n    - For weeks, prefer ISO Year-Week for complete Week labels. If your organization uses a non-ISO week system, still include the year context and use your defined week-numbering convention.\r\n\r\n## Allowed time units\r\n\r\n```yaml\r\ntimeUnits:\r\n  - id: Unknown\r\n    example: \"IsWeekend\"\r\n    note: \"Used for both season-type and period-type time-related columns. Future enhancements may provide separate categories.\"\r\n  - id: Year\r\n    example: 2022\r\n  - id: Quarter\r\n    example: \"Q3 2022\"\r\n  - id: QuarterOfYear\r\n    example: 4        # 4th quarter of the year\r\n  - id: Month\r\n    example: \"January 2022\"\r\n  - id: MonthOfYear\r\n    example: \"January\"\r\n  - id: MonthOfQuarter\r\n    example: 2        # 2nd month of the quarter\r\n  - id: Week\r\n    example: \"2022-W49\"   # ISO Year-Week or unique year+week label\r\n  - id: WeekOfYear\r\n    example: 49\r\n  - id: WeekOfQuarter\r\n    example: 11\r\n  - id: WeekOfMonth\r\n    example: 3\r\n  - id: Date\r\n    example: \"2022-01-01\"\r\n  - id: DayOfYear\r\n    example: 241\r\n  - id: DayOfQuarter\r\n    example: 71\r\n  - id: DayOfMonth\r\n    example: 23\r\n  - id: DayOfWeek\r\n    example: 4         # e.g., Thursday if 1=Monday\r\n```\r\n\r\n## Example\r\n\r\n```yaml\r\nTables:\r\n  - Name: DimDate\r\n    Columns:\r\n      - Name: Date\r\n        Type: Date\r\n      - Name: Year\r\n        Type: Integer\r\n      - Name: Quarter\r\n        Type: Text\r\n        SortByColumnName: Year Quarter Number\r\n      - Name: Year Quarter\r\n        Type: Text\r\n        SortByColumnName: Year Quarter Number\r\n      - Name: Year Quarter Number\r\n        Type: Integer\r\n      - Name: Month\r\n        Type: Text\r\n        SortByColumnName: Month Number\r\n      - Name: Month Short\r\n        Type: Text\r\n        SortByColumnName: Month Number\r\n      - Name: Month Number\r\n        Type: Integer\r\n      - Name: Year Month\r\n        Type: Text\r\n        SortByColumnName: Year Month Number\r\n      - Name: Year Month Short\r\n        Type: Text\r\n        SortByColumnName: Year Month Number\r\n      - Name: Year Month Number\r\n        Type: Integer\r\n      - Name: Week of Year\r\n        Type: Integer\r\n      - Name: ISO Year-Week\r\n        Type: Text\r\n        SortByColumnName: ISO Year-Week Number\r\n      - Name: ISO Year-Week Number\r\n        Type: Integer\r\n      - Name: Fiscal Year Number\r\n        Type: Integer\r\n      - Name: Fiscal Year Name\r\n        Type: Text\r\n        SortByColumnName: Fiscal Year Number\r\n      - Name: Fiscal Year Month\r\n        Type: Text\r\n        SortByColumnName: Fiscal Year Month Number\r\n      - Name: Fiscal Year Month Number\r\n        Type: Integer\r\n      - Name: Fiscal Month Number of Year\r\n        Type: Integer\r\n      - Name: Fiscal Month Name\r\n        Type: Text\r\n        SortByColumnName: Fiscal Month Number of Year\r\n      - Name: RelativeMonth  # Period-type: represents relative states\r\n        Type: Text\r\n      - Name: Season         # Season-type: represents cyclical concepts\r\n        Type: Text\r\n    Calendars:\r\n      - name: Gregorian Calendar\r\n        calendarColumnGroups:\r\n          - timeUnit: Year\r\n            primaryColumn: Year\r\n          - timeUnit: Quarter\r\n            primaryColumn: Year Quarter Number\r\n            associatedColumns:\r\n              - Year Quarter\r\n          - timeUnit: Month\r\n            primaryColumn: Year Month Number\r\n            associatedColumns:\r\n              - Year Month\r\n              - Year Month Short\r\n          - timeUnit: Week\r\n            primaryColumn: ISO Year-Week Number\r\n            associatedColumns:\r\n              - ISO Year-Week\r\n          - timeUnit: WeekOfYear\r\n            primaryColumn: Week of Year\r\n          - timeUnit: Date\r\n            primaryColumn: Date\r\n      - name: Fiscal Calendar\r\n        calendarColumnGroups:\r\n          - timeUnit: Year\r\n            primaryColumn: Fiscal Year Number\r\n            associatedColumns:\r\n              - Fiscal Year Name\r\n          - timeUnit: Month\r\n            primaryColumn: Fiscal Year Month Number\r\n            associatedColumns:\r\n              - Fiscal Year Month\r\n          - timeUnit: MonthOfYear\r\n            primaryColumn: Fiscal Month Number of Year\r\n            associatedColumns:\r\n              - Fiscal Month Name\r\n        timeRelatedGroups:\r\n          - column: RelativeMonth\r\n          - column: Season\r\n```\r\n",
        "skills/fabric-cli/servers/win32/Resources/dax_query_instructions_and_examples.md": "---\r\nname: 'DAX Query Instructions and Examples'\r\ndescription: 'Guidelines for writing Power BI DAX queries'\r\nuriTemplate: 'resource://dax_query_instructions_and_examples'\r\n---\r\n# DAX Query Language Guide\r\n\r\n## Overview\r\n\r\nDAX (Data Analysis Expressions) is a formula language used in Power BI for creating custom calculations and queries. This guide provides comprehensive instructions and examples for writing valid DAX query expressions.\r\n\r\n## DAX Query Best Practices\r\n\r\nWhen writing DAX queries, follow these recommendations for optimal results:\r\n\r\n* Include comments for clarity (DAX comments use `//` not `--`)\r\n* Always include an ORDER BY clause when returning multiple rows\r\n* Use meaningful variable names to improve readability\r\n* Define measures with fully qualified names in DEFINE blocks\r\n\r\n## DAX Query Syntax Rules\r\n\r\n### Query Structure\r\n\r\n#### DEFINE Block\r\n\r\n* Use DEFINE at the beginning if the query includes VAR, MEASURE, COLUMN, or TABLE definitions\r\n* Only use a single DEFINE block per query\r\n* Separate definitions with new lines (no commas or semicolons)\r\n\r\n#### Measure Definitions\r\n\r\n* When defining: ALWAYS fully qualify the measure name including its host table\r\n  * Example: `DEFINE MEASURE 'TableName'[MeasureName] = ...`\r\n  * The host table must exist in the semantic model\r\n* When using: Refer to the measure by name only, without the table qualifier\r\n  * Example: Use `[MeasureName]` in expressions like `CALCULATE([MeasureName], ...)`\r\n\r\n#### Ordering Results\r\n\r\n* ALWAYS include an ORDER BY clause when EVALUATE returns multiple rows\r\n* Do not use the ORDERBY function to sort the final query result\r\n\r\n### CALCULATE and CALCULATETABLE Filter Rules\r\n\r\nBoolean filters in CALCULATE or CALCULATETABLE have important restrictions:\r\n\r\n* Cannot directly use a measure or another CALCULATE function\r\n  * Solution: Use a variable to store the result, then reference the variable\r\n* Cannot reference columns from two different tables\r\n* When using the IN operator, the table operand must be a table variable, not a table expression\r\n* Do not assign a boolean filter to a VAR definition\r\n\r\n### SUMMARIZECOLUMNS Function\r\n\r\n**Purpose**: Build summary tables with groupby columns and measure-like extension columns\r\n\r\n**Parameter Order** (all optional, but must follow this order if used):\r\n\r\n1. Groupby columns (can be from one or multiple tables)\r\n2. Filters\r\n3. Measures or measure-like calculations\r\n\r\n**Key Rules**:\r\n\r\n* Use SUMMARIZECOLUMNS as the default for building summary tables with measures\r\n* Do not use SUMMARIZECOLUMNS without measure-like extension columns\r\n* Returns only rows where at least one measure value is not BLANK\r\n* Allows ANY number of measure-like calculations of arbitrary complexity\r\n* DO NOT use boolean filters with SUMMARIZECOLUMNS\r\n\r\n**When to Use Alternatives**:\r\n\r\n* If there are no measures or calculations, use SUMMARIZE instead\r\n\r\n### SUMMARIZE Function\r\n\r\n**Allowed Pattern**:\r\n\r\n```dax\r\nSUMMARIZE(<table expression>, <column1>, …, <columnN>)\r\n```\r\n\r\n**Critical Restrictions**:\r\n\r\n* NEVER use SUMMARIZE with measure-like expressions\r\n  * ❌ Incorrect: `SUMMARIZE(<table>, <column>, \"expr1\", <expr1>, …)`\r\n  * ✅ Correct: Use SUMMARIZECOLUMNS for measure calculations\r\n* Use for extracting distinct combinations of columns only\r\n* `VALUES('Table'[Column])` is a shortcut for `SUMMARIZE('Table', 'Table'[Column])`\r\n* When extracting a column from a table variable: `SUMMARIZE(_TableVar, [Column])`\r\n  * Note: `_TableVar[Column]` is not valid syntax\r\n\r\n**When to Use Alternatives**:\r\n\r\n* For measure calculations: Use SUMMARIZECOLUMNS\r\n* For aggregations on table variables: Use GROUPBY\r\n\r\n### GROUPBY Function\r\n\r\n**Purpose**: Perform simple aggregations on table-valued variables at a grouped level\r\n\r\n**Key Rules**:\r\n\r\n* Only use GROUPBY with a table-valued variable as the first argument\r\n* The CURRENTGROUP function is valid ONLY within GROUPBY\r\n* CURRENTGROUP must not be used elsewhere\r\n\r\n### SELECTCOLUMNS Function\r\n\r\n**Purpose**: Project columns while preserving duplicates or renaming columns\r\n\r\n**Key Rules**:\r\n\r\n* Use to preserve duplicate rows (unlike SUMMARIZE which removes them)\r\n* Use to rename columns for clarity\r\n* When renaming columns, subsequent expressions (TOPN, ORDER BY) must use the NEW column names\r\n\r\n**Important**: Include all columns needed for later operations (ORDER BY, FILTER, etc.)\r\n\r\n### Table Expressions and Filters\r\n\r\n* When using table expressions (SELECTCOLUMNS, CALCULATETABLE), include any columns needed later\r\n* Filters applied to one table can propagate across relationships based on filter direction (unidirectional or bidirectional)\r\n\r\n### Set Functions\r\n\r\nWhen using INTERSECT, UNION, or EXCEPT:\r\n\r\n* Both input tables must produce an identical number of columns\r\n\r\n### Time Intelligence Functions\r\n\r\n**DATESINPERIOD Rolling Windows**:\r\n\r\n* The negative period offset must precisely match the number of periods required\r\n* Examples:\r\n  * 12-month window: Use -12 (not -11)\r\n  * 3-month window: Use -3 (not -2)\r\n* This prevents off-by-one errors\r\n\r\n**Maintaining Clear Date Context**:\r\n\r\n* Always establish a valid date context for time intelligence calculations\r\n* Methods:\r\n  * Include groupby columns from the date table, OR\r\n  * Apply filters on date columns\r\n* Without date context, time intelligence functions cannot determine a \"current date\" reference\r\n* When using ROW function with time intelligence, supply external filters through CALCULATETABLE\r\n\r\n## Sample Data Model\r\n\r\nThese examples use a simplified hypothetical data model:\r\n\r\n```yaml\r\nTables:\r\n  - Name: Sales\r\n    Measures:\r\n      - Name: Total Discount\r\n        Type: Decimal\r\n      - Name: Total Amount\r\n        Type: Decimal\r\n      - Name: Total Quantity\r\n        Type: Integer\r\n    Columns:\r\n      - Name: CustomerKey\r\n        Type: Text\r\n      - Name: Order Quantity\r\n        Type: Integer\r\n      - Name: ProductKey\r\n        Type: Text\r\n      - Name: OrderDate\r\n        Type: Date\r\n      - Name: Sales Amount\r\n        Type: Decimal\r\n  - Name: Product\r\n    Measures:\r\n      - Name: Median List Price\r\n        Type: Decimal\r\n    Columns:\r\n      - Name: Category\r\n        Type: Text\r\n        MinValue: Consumer Electronics\r\n        MaxValue: Toys\r\n      - Name: Color\r\n        Type: Text\r\n        MinValue: Beige\r\n        MaxValue: Red\r\n      - Name: List Price\r\n        Description: Retail price of the product\r\n        Type: Decimal\r\n      - Name: Name\r\n        Type: Text\r\n      - Name: ProductKey\r\n        Type: Text\r\n  - Name: Customer\r\n    Columns:\r\n      - Name: CustomerKey\r\n        Type: Text\r\n      - Name: Name\r\n        Type: Text\r\n      - Name: Age\r\n        Type: Integer\r\n  - Name: Calendar\r\n    Columns:\r\n      - Name: Date\r\n        Type: Date\r\n      - Name: Month\r\n        Type: Text\r\n        SortByColumnName: MonthNumberOfYear\r\n      - Name: MonthNumberOfYear\r\n        Type: Integer\r\n      - Name: Year\r\n        Type: Integer\r\nActive Relationships:\r\n  - PK: 'Product'[ProductKey]\r\n    FK: 'Sales'[ProductKey]\r\n    Unidirectional Filter Propagation: \"'Product' filters 'Sales'\"\r\n  - PK: 'Customer'[CustomerKey]\r\n    FK: 'Sales'[CustomerKey]\r\n    Unidirectional Filter Propagation: \"'Customer' filters 'Sales'\"\r\n  - PK: 'Calendar'[Date]\r\n    FK: 'Sales'[OrderDate]\r\n    Unidirectional Filter Propagation: \"'Calendar' filters 'Sales'\"\r\n```\r\n\r\n## DAX Query Examples\r\n\r\nThe following examples demonstrate proper DAX query syntax and best practices using the data model defined above.\r\n\r\n### Example 1: Time Intelligence with Rolling Averages\r\n\r\n**Scenario**: Calculate year-to-date total sales and 14-day moving average for red products.\r\n\r\n```dax\r\n// Year-to-date total sales and 14-day moving average of sales for red products.\r\nEVALUATE\r\n  CALCULATETABLE(\r\n    ROW(\r\n      \"Total Sales Amount YTD\", TOTALYTD([Total Amount], 'Calendar'[Date]),\r\n      \"Total Sales Amount 14-Day MA\", AVERAGEX(DATESINPERIOD('Calendar'[Date], MAX('Calendar'[Date]), -14, DAY), [Total Amount]) // Note that the number_of_intervals parameter must be -14 instead of -13.\r\n    ),\r\n    'Product'[Color] == \"Red\",\r\n    TREATAS({ MAX('Sales'[OrderDate]) }, 'Calendar'[Date]) // Establish a reference date for TI functions TOTALYTD and DATESINPERIOD.\r\n  )\r\n```\r\n\r\n**Key Concepts**:\r\n\r\n* Uses CALCULATETABLE with ROW to establish date context for time intelligence functions\r\n* TREATAS establishes a clear \"current date\" reference for time intelligence\r\n* DATESINPERIOD uses -14 (not -13) for a proper 14-day window\r\n\r\n### Example 2: Multi-Level Aggregation with GROUPBY\r\n\r\n**Scenario**: Calculate average, minimum, and maximum monthly sales quantity by year for Consumer Electronics before 2023.\r\n\r\n```dax\r\nDEFINE\r\n  // Filters for products in Consumer Electronics category\r\n  VAR _Filter1 = TREATAS(\r\n    {\r\n      \"Consumer Electronics\"\r\n    },\r\n    'Product'[Category]\r\n  )\r\n  // Filters to years before 2023\r\n  VAR _Filter2 = FILTER(\r\n    ALL('Calendar'[Year]),\r\n    'Calendar'[Year] < 2023\r\n  )\r\n\r\n// Quantity filtered to Consumer Electronics products for years before 2023, grouped by month\r\n  VAR _SummaryTable = SUMMARIZECOLUMNS(\r\n    'Calendar'[Year],\r\n    'Calendar'[Month],\r\n    // [Month] is a required groupby column.\r\n    // A query always sorts by required groupby columns.\r\n    // [MonthNumberOfYear] is the orderby column for [Month].\r\n    // Also include [MonthNumberOfYear] to be used in the ORDER BY clause.\r\n    'Calendar'[MonthNumberOfYear],\r\n    _Filter1,\r\n    _Filter2,\r\n    \"Monthly Quantity\", [Total Quantity]\r\n  )\r\n// Aggregate the summarized monthly data by year and month to derive average, minimum, and maximum monthly quantities.\r\nEVALUATE\r\n  // GROUPBY function is used to summarize intermediate tables.\r\n  GROUPBY(\r\n    _SummaryTable,\r\n    'Calendar'[Year],\r\n    'Calendar'[Month],\r\n    'Calendar'[MonthNumberOfYear],\r\n    \"Avg Monthly Quantity\",\r\n    AVERAGEX(\r\n      CURRENTGROUP(), // must be used inside GROUPBY function\r\n      [Monthly Quantity]\r\n    ),\r\n    \"Min Monthly Quantity\",\r\n    MINX(\r\n      CURRENTGROUP(), // must be used inside GROUPBY function\r\n      [Monthly Quantity]\r\n    ),\r\n    \"Max Monthly Quantity\",\r\n    MAXX(\r\n      CURRENTGROUP(),  // must be used inside GROUPBY function\r\n      [Monthly Quantity]\r\n    )\r\n  )\r\n  ORDER BY\r\n    'Calendar'[Year] ASC,\r\n    // [MonthNumberOfYear] is the orderby column for [Month].\r\n    // ORDER BY [MonthNumberOfYear] instead of [Month].\r\n    'Calendar'[MonthNumberOfYear] ASC\r\n```\r\n\r\n**Key Concepts**:\r\n\r\n* SUMMARIZECOLUMNS creates initial summary with measures\r\n* GROUPBY performs secondary aggregation on the summary table\r\n* CURRENTGROUP is used exclusively within GROUPBY\r\n* Includes MonthNumberOfYear for proper sorting\r\n\r\n### Example 3: Filtering with Measures Using Variables\r\n\r\n**Scenario**: Find products with total sales over $1 million that are red or black.\r\n\r\n```dax\r\nDEFINE\r\n  // Red or Black product filter\r\n  VAR _Filter = TREATAS(\r\n    {\r\n      \"Red\",\r\n      \"Black\"\r\n    },\r\n    'Product'[Color]\r\n  )\r\n  // Sales of Red or Black products\r\n  VAR _SummaryTable = SUMMARIZECOLUMNS(\r\n    'Product'[Name],\r\n    _Filter,\r\n    \"Total Sales\", [Total Amount]\r\n  )\r\n\r\n// Products with total sales above $1,000,000\r\nEVALUATE\r\n  SELECTCOLUMNS(\r\n    FILTER(\r\n      _SummaryTable,\r\n      [Total Sales] > 1000000\r\n    ),\r\n    'Product'[Name]\r\n  )\r\n  ORDER BY\r\n    'Product'[Name] ASC\r\n```\r\n\r\n**Key Concepts**:\r\n\r\n* TREATAS creates a filter from a list of values\r\n* SUMMARIZECOLUMNS builds summary with measure\r\n* FILTER applied to summary table variable\r\n* SELECTCOLUMNS projects only needed columns\r\n\r\n### Example 4: SUMMARIZE for Distinct Values (No Duplicates)\r\n\r\n**Scenario**: Get unique combinations of color, category, and product key for products sold in 2022.\r\n\r\n```dax\r\n// Product color, category and key for products sold in 2022\r\nEVALUATE\r\n  CALCULATETABLE(\r\n    // SUMMARIZE is used to remove duplicate rows\r\n    SUMMARIZE(\r\n      'Sales',\r\n      'Product'[Color],\r\n      'Product'[Category],\r\n      'Sales'[ProductKey]\r\n    ),\r\n    'Calendar'[Year] == 2022\r\n  )\r\n  ORDER BY\r\n    'Product'[Color] ASC,\r\n    'Product'[Category] ASC,\r\n    'Sales'[ProductKey] ASC\r\n```\r\n\r\n**Key Concepts**:\r\n\r\n* SUMMARIZE removes duplicate rows\r\n* CALCULATETABLE applies year filter\r\n* Related table columns accessed via relationships\r\n\r\n### Example 5: SELECTCOLUMNS for Preserving Duplicates\r\n\r\n**Scenario**: Get color, category, and product key for products sold in 2022, keeping all duplicate rows.\r\n\r\n```dax\r\n// Product color, category and key for products sold in 2022\r\nEVALUATE\r\n  CALCULATETABLE(\r\n    SELECTCOLUMNS(\r\n      'Sales',\r\n      \"Color\",\r\n      RELATED('Product'[Color]),\r\n      \"Category\",\r\n      RELATED('Product'[Category]),\r\n      'Sales'[ProductKey]\r\n    ),\r\n    'Calendar'[Year] == 2022\r\n  )\r\n  ORDER BY\r\n    [Color] ASC,\r\n    [Category] ASC,\r\n    'Sales'[ProductKey] ASC\r\n```\r\n\r\n**Key Concepts**:\r\n\r\n* SELECTCOLUMNS preserves duplicate rows (unlike SUMMARIZE)\r\n* Column renaming requires using new names in ORDER BY\r\n* RELATED accesses columns from related tables\r\n\r\n### Example 6: Finding Products with No Sales\r\n\r\n**Scenario**: Identify products that have never been sold.\r\n\r\n```dax\r\nDEFINE\r\n  // Sale row count\r\n  MEASURE 'Sales'[Row Count] = COUNTROWS()\r\n\r\n// Products with no sales\r\nEVALUATE\r\n  FILTER(\r\n    'Product',\r\n    ISBLANK([Row Count])\r\n  )\r\n  ORDER BY\r\n    'Product'[Name] ASC,\r\n    'Product'[ProductKey] ASC\r\n```\r\n\r\n**Key Concepts**:\r\n\r\n* Defines a measure for row counting\r\n* FILTER with ISBLANK identifies products with no related sales\r\n* Filter context propagates from Product to Sales table\r\n\r\n### Example 7: Using Variables to Store Measure Results\r\n\r\n**Scenario**: Find products with list prices above the median.\r\n\r\n**Solution 1 - Using CALCULATETABLE**:\r\n\r\n```dax\r\nDEFINE\r\n  // Calculate the value of the [Median List Price] measure and store the result in a variable.\r\n  VAR _MedianListPrice = [Median List Price]\r\n\r\n// Products with list price over the median.\r\nEVALUATE\r\n  CALCULATETABLE(\r\n    VALUES('Product'[Name]),\r\n    'Product'[List Price] > _MedianListPrice // boolean filter uses variable instead of measure directly\r\n  )\r\n  ORDER BY\r\n    'Product'[Name] ASC\r\n```\r\n\r\n**Solution 2 - Using FILTER**:\r\n\r\n```dax\r\nDEFINE\r\n  // Calculate the value of the [Median List Price] measure and store the result in a variable.\r\n  VAR _MedianListPrice = [Median List Price]\r\n\r\n// Products with list price over the median.\r\nEVALUATE\r\n  SELECTCOLUMNS(\r\n    FILTER(\r\n      VALUES('Product'),\r\n      'Product'[List Price] > _MedianListPrice // Use variable instead of measure reference to ensure the median list price is calculated across all products\r\n    ),\r\n    'Product'[Name]\r\n  )\r\n  ORDER BY\r\n    'Product'[Name] ASC\r\n```\r\n\r\n**Key Concepts**:\r\n\r\n* Variables store measure results for use in boolean filters\r\n* Cannot use measures directly in CALCULATE boolean filters\r\n* Both approaches yield the same result with different syntax\r\n\r\n### Example 8: Using Table Variables as Filters\r\n\r\n**Scenario**: Find the product with highest demand since 2020 and get its sale dates.\r\n\r\n```dax\r\nDEFINE\r\n  // To make query more readable, a filter can be defined separately.\r\n  VAR _Filter = FILTER(\r\n    ALL('Calendar'[Year]),\r\n    'Calendar'[Year] >= 2020\r\n  )\r\n  // Get the product with the maximum Total Quantity\r\n  VAR _TopProduct = TOPN(\r\n    1,\r\n    SUMMARIZECOLUMNS(\r\n      'Product'[ProductKey],\r\n      _Filter,\r\n      \"Total Quantity\", [Total Quantity]\r\n    ),\r\n    [Total Quantity],\r\n    DESC\r\n  )\r\n\r\n// Name and order date for sales of the top product\r\nEVALUATE\r\n  SELECTCOLUMNS(\r\n    CALCULATETABLE(\r\n      'Sales',\r\n      // Use table-valued variable _TopProduct directly as a filter.\r\n      // No need to extract the 'Product'[ProductKey] first.\r\n      // Calculated column [Total Quantity] has no effect in the filter context.\r\n      _TopProduct\r\n    ),\r\n    \"Product Name\",\r\n    RELATED('Product'[Name]),\r\n    'Sales'[OrderDate]\r\n  )\r\n  ORDER BY\r\n    [Product Name] ASC,\r\n    'Sales'[OrderDate] ASC\r\n```\r\n\r\n**Key Concepts**:\r\n\r\n* TOPN identifies the product with highest total quantity\r\n* Table variables can be used directly as filters in CALCULATETABLE\r\n* Calculated columns in table variables don't affect filter context\r\n\r\n### Example 9: Calculating Averages on Filtered Subsets\r\n\r\n**Scenario**: Calculate the average list price of products sold in 2022.\r\n\r\n```dax\r\nDEFINE\r\n  // Distinct products sold in 2022\r\n  VAR _ProductsSold2022 = CALCULATETABLE(\r\n    SUMMARIZE(\r\n      'Sales',\r\n      'Product'[ProductKey]\r\n    ),\r\n    'Calendar'[Year] == 2022\r\n  )\r\nEVALUATE\r\n  ROW(\r\n    \"Average List Price of Products Sold in 2022\",\r\n    CALCULATE(\r\n      AVERAGE('Product'[List Price]),\r\n      _ProductsSold2022 // Apply table-valued variable as a filter\r\n    )\r\n  )\r\n```\r\n\r\n**Key Concepts**:\r\n\r\n* SUMMARIZE extracts distinct products sold in 2022\r\n* ROW returns a single-row result\r\n* Table variable applied as filter in CALCULATE\r\n\r\n### Example 10: Column Renaming with SELECTCOLUMNS\r\n\r\n**Scenario**: Get products sold and customers, sorted by renamed column names.\r\n\r\n```dax\r\n// Sorted product and customer names for all sales\r\nDEFINE\r\n  VAR _UniqueProductCustomerPairs = SUMMARIZE(\r\n    'Sales',\r\n    'Product'[Name],\r\n    'Customer'[Name]\r\n  )\r\nEVALUATE\r\n  SELECTCOLUMNS(\r\n    _UniqueProductCustomerPairs,\r\n    \"Product Name\", // New name for the 'Product'[Name] column\r\n    'Product'[Name],\r\n    \"Customer Name\", // New name for the 'Customer'[Name] column\r\n    'Customer'[Name]\r\n  )\r\n  // ORDER BY needs to use the renamed column names\r\n  ORDER BY\r\n    [Product Name] ASC, // Use the new column name assigned by SELECTCOLUMNS instead of the original column name 'Product'[Name]\r\n    [Customer Name] ASC // Use the new column name assigned by SELECTCOLUMNS instead of the original column name 'Customer'[Name]\r\n```\r\n\r\n**Key Concepts**:\r\n\r\n* SELECTCOLUMNS renames columns for clarity\r\n* ORDER BY must reference the NEW column names, not original names\r\n* SUMMARIZE removes duplicate product-customer pairs\r\n\r\n### Example 11: Multiple Filters with TREATAS\r\n\r\n**Scenario**: For the three oldest customers, show total discounts by year for the last three years.\r\n\r\n```dax\r\nDEFINE\r\n  VAR _OldestThreeCustomers = TOPN(\r\n    3,\r\n    'Customer',\r\n    'Customer'[Age],\r\n    DESC\r\n  )\r\n  // Determine the last year based on actual sales dates.\r\n  // Avoid using the last year in the 'Calendar' table, as it may include future dates without sales.\r\n  VAR _LastYear = YEAR(MAX('Sales'[OrderDate]))\r\nEVALUATE\r\n  SUMMARIZECOLUMNS(\r\n    'Customer'[Name],\r\n    'Calendar'[Year],\r\n    TREATAS({_LastYear, _LastYear - 1, _LastYear - 2}, 'Calendar'[Year]),\r\n    _OldestThreeCustomers, // Apply table-valued variable as filter.\r\n    \"Total Discount\",\r\n    [Total Discount]\r\n  )\r\n  ORDER BY\r\n    'Customer'[Name] ASC,\r\n    'Calendar'[Year] ASC\r\n```\r\n\r\n**Key Concepts**:\r\n\r\n* TOPN selects top 3 customers by age\r\n* TREATAS creates filter from calculated year values\r\n* Determines last year from actual sales data, not calendar table\r\n\r\n### Example 12: Filtering Aggregated Results\r\n\r\n**Scenario**: For each customer, show products purchased at least three times with purchase count.\r\n\r\n```dax\r\nDEFINE\r\n  MEASURE 'Sales'[Purchase Count] = COUNTROWS()\r\n  VAR _SummaryTable = SUMMARIZECOLUMNS(\r\n    'Customer'[Name],\r\n    'Product'[Name],\r\n    \"Purchase Count\", [Purchase Count]\r\n  )\r\nEVALUATE\r\n  FILTER(_SummaryTable, [Purchase Count] >= 3)\r\n  ORDER BY\r\n    'Customer'[Name] ASC,\r\n    'Product'[Name] ASC\r\n```\r\n\r\n**Key Concepts**:\r\n\r\n* Defines measure for counting purchases\r\n* SUMMARIZECOLUMNS creates summary with measure\r\n* FILTER applied to summary table variable\r\n* Simple and efficient two-step pattern\r\n\r\n### Example 13: Calculated Columns in DEFINE\r\n\r\n**Scenario**: Categorize products by price (above/below median) and show max/min quantities per category.\r\n\r\n```dax\r\nDEFINE\r\n  // define a new column so that it can be used in SUMMARIZECOLUMNS\r\n  COLUMN 'Product'[Price Group] = \r\n    VAR _MedianListPrice = [Median List Price]\r\n    RETURN\r\n    IF(\r\n      'Product'[List Price] > _MedianListPrice,\r\n      \"High Priced\",\r\n      \"Low Priced\"\r\n    )\r\n  MEASURE 'Sales'[Max Quantity] = MAX('Sales'[Order Quantity])\r\n  MEASURE 'Sales'[Min Quantity] = MIN('Sales'[Order Quantity])\r\nEVALUATE\r\n  SUMMARIZECOLUMNS(\r\n    'Product'[Price Group],\r\n    \"Max Quantity\",\r\n    [Max Quantity],\r\n    \"Min Quantity\",\r\n    [Min Quantity]\r\n  )\r\n  ORDER BY 'Product'[Price Group] ASC\r\n```\r\n\r\n**Key Concepts**:\r\n\r\n* COLUMN defines a calculated column in DEFINE block\r\n* Calculated columns can be used as groupby columns in SUMMARIZECOLUMNS\r\n* Multiple measures defined and used in same query\r\n* IF expression for conditional logic\r\n\r\n## Summary\r\n\r\nThis guide covers the essential rules and patterns for writing valid DAX queries. Key takeaways:\r\n\r\n1. **Always use ORDER BY** when returning multiple rows\r\n2. **Store measure results in variables** before using in boolean filters\r\n3. **Choose the right function**: SUMMARIZECOLUMNS for measures, SUMMARIZE for distinct values, GROUPBY for table variables\r\n4. **Establish date context** for time intelligence functions\r\n5. **Use renamed columns** in ORDER BY after SELECTCOLUMNS\r\n6. **Leverage table variables** as filters for cleaner, more maintainable code\r\n\r\nPractice these patterns to write efficient, readable DAX queries that follow best practices.\r\n",
        "skills/fabric-cli/servers/win32/Resources/dax_udf_instructions_and_examples.md": "---\r\nname: 'DAX UDF Instructions and Examples'\r\ndescription: 'Guidelines for creating Power BI DAX user-defined functions (UDFs)'\r\nuriTemplate: 'resource://dax_udf_instructions_and_examples'\r\n---\r\n# DAX User-Defined Functions (UDFs) Guide\r\n\r\n## Overview\r\n\r\nDAX User-Defined Functions (UDFs) allow you to create reusable function definitions in Power BI semantic models. This guide explains the syntax, type system, and best practices for defining UDFs.\r\n\r\n## Basic Syntax\r\n\r\nA UDF definition consists of a function name and a function definition with parameters and a body:\r\n\r\n```yaml\r\nFunctionName: MyFunction\r\nFunctionDefinition: |-\r\n  (param1 [: Type [Scalar Subtype] [Val|Expr]],\r\n   param2 [: Type [Scalar Subtype] [Val|Expr]],\r\n   ...\r\n  ) =>\r\n      <Function body>\r\n```\r\n\r\n## Type System\r\n\r\n### Parameter Types\r\n\r\nDAX UDFs support three main parameter types:\r\n\r\n* **Scalar**: A single value (number, text, date/time, boolean)\r\n* **Table**: A DAX table expression\r\n* **AnyRef**: A direct reference to an existing semantic model object without pre-evaluation\r\n\r\n#### Scalar Subtypes\r\n\r\nWhen using `Scalar` type, you can optionally specify a subtype:\r\n\r\n* `Int64`: Integer values\r\n* `Decimal`: Decimal numbers\r\n* `Double`: Double-precision floating-point numbers\r\n* `String`: Text values\r\n* `DateTime`: Date and time values\r\n* `Boolean`: True/false values\r\n* `Numeric`: Any numeric type (Int64, Decimal, or Double)\r\n* `Variant`: Any scalar type (use when the expression may yield different types)\r\n\r\n**Note**: `BLANK()` is valid for any subtype.\r\n\r\n### AnyRef Type\r\n\r\nUse `AnyRef` when you need a direct reference to a model object rather than its evaluated value. This is useful for functions that need to pass references to functions like CALCULATE, TREATAS, or SAMEPERIODLASTYEAR.\r\n\r\nAllowed reference forms:\r\n\r\n* Column reference: `'Table'[Column]`\r\n* Table reference: `'Table'`\r\n* Measure reference: `[Measure]`\r\n* Calendar reference: `MyCalendar`\r\n\r\n### Parameter Modes\r\n\r\nParameters can be evaluated in two modes:\r\n\r\n* **Val** (value mode - default): The argument expression is evaluated at the call site before entering the function. The resulting value is substituted wherever the parameter is used.\r\n* **Expr** (expression mode): The raw argument expression is substituted into the function body and evaluated in its inner context. Use this when you want the expression to be re-evaluated within inner contexts created by CALCULATE, FILTER, or iteration functions.\r\n\r\n## Example Schema\r\n\r\nThe following examples reference this sample data model:\r\n\r\n```yaml\r\nTables:\r\n  - Name: Sales\r\n    Measures:\r\n      - Name: Total Amount\r\n        Type: Decimal\r\n      - Name: Total Quantity\r\n        Type: Integer\r\n    Columns:\r\n      - Name: CustomerKey\r\n        Type: Text\r\n      - Name: ProductKey\r\n        Type: Text\r\n      - Name: OrderDate\r\n        Type: Date\r\n  - Name: Product\r\n    Columns:\r\n      - Name: ProductKey\r\n        Type: Text\r\n      - Name: Name\r\n        Type: Text\r\n      - Name: Color\r\n        Type: Text\r\n  - Name: Customer\r\n    Columns:\r\n      - Name: CustomerKey\r\n        Type: Text\r\n      - Name: Name\r\n        Type: Text\r\n  - Name: Calendar\r\n    Columns:\r\n      - Name: Date\r\n        Type: Date\r\n      - Name: Month\r\n        Type: Text\r\n        SortByColumnName: MonthNumberOfYear\r\n      - Name: MonthNumberOfYear\r\n        Type: Integer\r\n      - Name: Year\r\n        Type: Integer\r\nActive Relationships:\r\n  - PK: 'Product'[ProductKey]\r\n    FK: 'Sales'[ProductKey]\r\n    Unidirectional Filter Propagation: \"'Product' filters 'Sales'\"\r\n  - PK: 'Customer'[CustomerKey]\r\n    FK: 'Sales'[CustomerKey]\r\n    Unidirectional Filter Propagation: \"'Customer' filters 'Sales'\"\r\n  - PK: 'Calendar'[Date]\r\n    FK: 'Sales'[OrderDate]\r\n    Unidirectional Filter Propagation: \"'Calendar' filters 'Sales'\"\r\n```\r\n\r\n## Examples\r\n\r\n### Example 1: Simple Numeric Calculation\r\n\r\nCalculate the area of a circle from its radius.\r\n\r\n```yaml\r\nFunctionName: CircleArea\r\nFunctionDefinition: |-\r\n    (radius : Scalar Numeric) =>\r\n        PI() * radius * radius\r\n```\r\n\r\n**Usage**: `CircleArea(5)` returns approximately 78.54\r\n\r\n### Example 2: Basic Value Transformation\r\n\r\nDouble an input value.\r\n\r\n```yaml\r\nFunctionName: DoubleValue\r\nFunctionDefinition: |-\r\n    (inputValue : Scalar Numeric Val) =>\r\n        inputValue * 2\r\n```\r\n\r\n**Usage**: `DoubleValue(10)` returns 20\r\n\r\n### Example 3: Working with AnyRef - Statistical Function\r\n\r\nReturns the most frequently occurring value in a column.\r\n\r\n```yaml\r\nFunctionName: Mode\r\nFunctionDefinition: |-\r\n    (tab : AnyRef,\r\n     col : AnyRef\r\n    ) =>\r\n        MINX(\r\n            TOPN(\r\n                1,\r\n                ADDCOLUMNS(\r\n                    VALUES(col),\r\n                    \"Freq\", CALCULATE(COUNTROWS(tab))\r\n                ),\r\n                [Freq], DESC\r\n            ),\r\n            col\r\n        )\r\n```\r\n\r\n**Explanation**: This function uses `AnyRef` for both parameters because it needs to pass the table and column references to DAX functions like VALUES and CALCULATE. The function finds the value that appears most frequently by counting occurrences.\r\n\r\n**Usage**: `Mode('Sales', 'Sales'[ProductKey])`\r\n\r\n### Example 4: Using Expr Mode for Time Intelligence\r\n\r\nEvaluate any scalar expression in the prior year.\r\n\r\n```yaml\r\nFunctionName: PriorYearValue\r\nFunctionDefinition: |-\r\n    (expression : Scalar Variant Expr,\r\n     dateColumn : AnyRef\r\n    ) =>\r\n        CALCULATE(\r\n            expression,\r\n            SAMEPERIODLASTYEAR(dateColumn)\r\n        )\r\n```\r\n\r\n**Explanation**: The `expression` parameter uses `Expr` mode so it's evaluated within the CALCULATE context with the prior year filter applied. The `dateColumn` uses `AnyRef` to pass the column reference to SAMEPERIODLASTYEAR.\r\n\r\n**Usage**: `PriorYearValue([Total Amount], 'Calendar'[Date])`\r\n\r\n### Example 5: Returning a Table Filter\r\n\r\nReturn today's date as a one-row table for filtering.\r\n\r\n```yaml\r\nFunctionName: TodayAsDate\r\nFunctionDefinition: |-\r\n    () =>\r\n        TREATAS(\r\n            { TODAY() },\r\n            'Date'[Date]\r\n        )\r\n```\r\n\r\n**Explanation**: This function demonstrates a UDF that returns a table. It uses TREATAS to convert the single-value table into a filter compatible with the Date table.\r\n\r\n**Usage**: `CALCULATE([Total Amount], TodayAsDate())`\r\n\r\n### Example 6: Table-Returning Function\r\n\r\nReturn a table of the top 3 Products by the [Sales] measure.\r\n\r\n```yaml\r\nFunctionName: Top3ProductsBySales\r\nFunctionDefinition: |-\r\n    () =>\r\n        TOPN(\r\n            3,\r\n            VALUES('Product'[ProductKey]),\r\n            [Sales], DESC\r\n        )\r\n```\r\n\r\n**Explanation**: This parameterless function returns a table containing the top 3 products. It can be used anywhere a table expression is expected.\r\n\r\n**Usage**: `CALCULATE([Total Amount], Top3ProductsBySales())`\r\n\r\n### Example 7: String Manipulation with Table Return\r\n\r\nSplit a text by a delimiter and return a single-column table.\r\n\r\n```yaml\r\nFunctionName: SplitString\r\nFunctionDefinition: |-\r\n    (s : Scalar String,\r\n     delimiter : Scalar String\r\n    ) =>\r\n    VAR str =\r\n        SUBSTITUTE(s, delimiter, \"|\")\r\n    VAR len =\r\n        PATHLENGTH(str)\r\n    RETURN\r\n        SELECTCOLUMNS(\r\n            GENERATESERIES(1, len),\r\n            \"Value\", PATHITEM(str, [Value], TEXT)\r\n        )\r\n```\r\n\r\n**Explanation**: This function uses variables (VAR) and demonstrates how to build complex logic. It converts the delimiter to a path separator, counts the parts, and returns a table with each part as a row.\r\n\r\n**Usage**: `SplitString(\"apple,banana,cherry\", \",\")`\r\n\r\n## Best Practices\r\n\r\n1. **Use appropriate type hints**: Specify types and subtypes to make your functions more robust and self-documenting\r\n2. **Choose the right parameter mode**: Use `Expr` when you need the expression to be evaluated in the function's context, otherwise use `Val` (default)\r\n3. **Use AnyRef for references**: When passing columns, tables, or measures to DAX functions that expect references, use `AnyRef`\r\n4. **Document your functions**: Include clear descriptions of what each function does\r\n5. **Test with edge cases**: Consider BLANK values and empty tables in your function logic\r\n6. **Keep functions focused**: Each function should have a single, well-defined purpose\r\n7. **Use variables**: For complex functions, use VAR to break down logic and improve readability\r\n",
        "skills/fabric-cli/servers/win32/Resources/powerbi_project_instructions.md": "﻿---\r\nname: 'PowerBI Project Instructions'\r\ndescription: 'Instructions for structuring Power BI projects'\r\nuriTemplate: 'resource://powerbi_project_instructions'\r\n---\r\nYou are an expert in Power BI Project (PBIP) file structure.\r\n\r\nIf the `powerbi-modeling-mcp` MCP server is available, **do not create or edit the TMDL files directly**.\r\n\r\n## PBIP structure\r\n\r\n```text\r\nroot/\r\n├── [Name].SemanticModel/\r\n|   ├── /definition # The semantic model definition using TMDL language\r\n|   ├──  definition.pbism # The semantic model definition file\r\n├── [Name].Report/        \r\n|   ├── /definition # The report definition using PBIR format\r\n|   ├──  definition.pbir # The report definition file with a byPath relative reference to the semantic model folder\r\n└── [Name].pbip # A shortcut file to the report folder\r\n```    \r\n\r\n**Example of a definition.pbism file**\r\n\r\nNo modifications are needed—just create the file exactly as shown in the example.\r\n\r\n```json\r\n{\r\n    \"$schema\": \"https://developer.microsoft.com/json-schemas/fabric/item/semanticModel/definitionProperties/1.0.0/schema.json\",\r\n    \"version\": \"4.2\",\r\n    \"settings\": {\r\n        \"qnaEnabled\": true\r\n    }\r\n}\r\n```\r\n\r\n**Example of a definition.pbir file**\r\n\r\nThe `byPath` property should reference the semantic model folder using a relative path like below.\r\n\r\n```json\r\n{\r\n    \"$schema\": \"https://developer.microsoft.com/json-schemas/fabric/item/report/definitionProperties/2.0.0/schema.json\",\r\n    \"version\": \"4.0\",\r\n    \"datasetReference\": {\r\n        \"byPath\": {\r\n            \"path\": \"../{Name of the Semantic Model}.SemanticModel\"\r\n        }\r\n    }\r\n}\r\n```\r\n\r\n**Example of a {Name of the Semantic Model}.pbip file**\r\n\r\n```json\r\n{\r\n    \"$schema\": \"https://developer.microsoft.com/json-schemas/fabric/pbip/pbipProperties/1.0.0/schema.json\",\r\n    \"version\": \"1.0\",\r\n    \"artifacts\": [\r\n        {\r\n        \"report\": {\r\n            \"path\": \"{Name of the Semantic Model}.Report\"\r\n        }\r\n        }\r\n    ],\r\n    \"settings\": {\r\n        \"enableAutoRecovery\": true\r\n    }\r\n}\r\n```\r\n\r\n## Open from PBIP\r\n\r\nWhen asked to open/load the semantic model from a PBIP, you must only load the `[Name].SemanticModel/definition` folder. No other folder is suitable to load from semantic model developer tools.\r\n\r\n## Save to PBIP\r\n\r\nWhen asked to save to a new PBIP folder make sure you create the folder and files from the structure above using the provided examples.\r\n\r\n## Creation of new semantic model\r\n\r\n1. Create a PBIP folder for the semantic model following the structure above\r\n2. Use the `database_operations` tool of the MCP server to `Create` a new database. \r\n3. In the end of the modeling session, serialize as TMDL to the `definition/` folder in the PBIP\r\n"
      },
      "plugins": [
        {
          "name": "fabric-cli-plugin",
          "description": "Fabric CLI context and resources for effective use of the Fabric CLI with coding agents.",
          "source": "./",
          "strict": false,
          "skills": [
            "./skills/fabric-cli"
          ],
          "commands": [
            "./commands"
          ],
          "mcpServers": "./.mcp.json",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add data-goblin/fabric-cli-plugin",
            "/plugin install fabric-cli-plugin@fabric-cli-plugin"
          ]
        }
      ]
    }
  ]
}