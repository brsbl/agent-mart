{
  "author": {
    "id": "doodledood",
    "display_name": "doodledood",
    "avatar_url": "https://avatars.githubusercontent.com/u/888717?v=4"
  },
  "marketplaces": [
    {
      "name": "manifest-dev",
      "version": null,
      "description": "Verification-first manifest workflows for Claude Code",
      "repo_full_name": "doodledood/manifest-dev",
      "repo_url": "https://github.com/doodledood/manifest-dev",
      "repo_description": "Define what you'd accept. Verify until it's done. Stop micromanaging AI implementation. Define acceptance criteria, let the verify-fix loop handle the rest.",
      "signals": {
        "stars": 1,
        "forks": 0,
        "pushed_at": "2026-01-30T15:01:15Z"
      },
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"name\": \"manifest-dev\",\n  \"owner\": {\n    \"name\": \"doodledood\",\n    \"email\": \"doodledood@github.com\"\n  },\n  \"metadata\": {\n    \"description\": \"Verification-first manifest workflows for Claude Code\"\n  },\n  \"plugins\": [\n    { \"name\": \"manifest-dev\", \"source\": \"./claude-plugins/manifest-dev\" }\n  ]\n}\n",
        "README.md": "<p align=\"center\">\n  <picture>\n    <img src=\"assets/logo.png\" alt=\"Manifest Dev Logo\" width=\"120\" style=\"background: transparent;\">\n  </picture>\n</p>\n\n# Manifest-Driven Development\n\nDefine what you'd accept. Verify until it's done.\n\nStop micromanaging AI implementation. Define acceptance criteria, let the verify-fix loop handle the rest.\n\n## Quick Start\n\n```bash\n# Install\nclaude plugins add github.com/doodledood/manifest-dev\nclaude plugins install manifest-dev@manifest-dev-marketplace\n\n# Use\n/define <what you want to build>\n/do <manifest-path>\n```\n\n**Pro tip**: Run `/do` in a fresh session after `/define` completes—or at minimum, `/compact` before starting. The manifest is your external state; the session doesn't need to remember the conversation.\n\n## Contents\n\n- [The Problem](#the-problem)\n- [The Mindset Shift](#the-mindset-shift)\n- [How It Works](#how-it-works)\n- [What /define Produces](#what-define-produces)\n- [Plugin Architecture](#plugin-architecture)\n- [The Benefits](#the-benefits)\n- [Who This Is For](#who-this-is-for)\n\n## The Problem\n\nYou give the agent a task. It generates code. The code looks reasonable. You ship it. Two days later you're debugging something that should have been obvious—or worse, realizing the AI \"finished\" but left critical pieces incomplete.\n\nThis is the vibe coding hangover. We got excited about the speed. We ignored the cleanup cost.\n\nThe tools are getting smarter. Claude, GPT, the latest models—they can genuinely code. But we're throwing them into deep water without defining what \"done\" actually means.\n\n## The Mindset Shift\n\n**Stop thinking about how to make the AI implement correctly. Start defining what would make you accept the output.**\n\nWhen you ask \"how should the LLM do this?\", you end up micromanaging the implementation. You write detailed plans. You specify function names and types. You try to puppeteer the AI through every step. The moment something unexpected happens—the plan breaks down. The AI starts using `any` types, adding `// @ts-ignore` comments, bending reality to satisfy the letter of your instructions while violating the spirit.\n\nWhen you ask \"what would make me accept this?\", you define success criteria. You specify what the output must do, not how it must be built. You encode your quality standards as verifiable acceptance criteria. Then you let the AI figure out the implementation—and you verify whether it hit the bar.\n\nThis is manifest-driven development.\n\nIf you know spec-driven development, this is a cousin—adapted for LLM execution. The manifest is a spec, but ephemeral: it drives one task, then the code is the source of truth. No spec maintenance. No drift problem.\n\n## How It Works\n\nManifest-driven development separates three concerns:\n\n1. **WHAT** needs to be built (deliverables with acceptance criteria)\n2. **RULES** that must always be followed (global invariants)\n3. **HOW** to verify each criterion (automated checks)\n\n```\n/define → Interview → Manifest → /do → Execute → /verify → Fix loop → /done\n```\n\n**Define**: An LLM interviews you to surface what you actually want. Not just what you say you want—your latent criteria. The stuff you'd reject in a PR but wouldn't think to specify upfront.\n\n**Do**: The AI implements toward the acceptance criteria. It has flexibility on the how. It doesn't have flexibility on the what.\n\n**Verify**: Automated checks run against every criterion. Failing checks get specific—they say exactly what's wrong.\n\n**Fix**: The AI fixes what failed. Only what failed. It doesn't restart. It doesn't touch passing criteria.\n\nThe loop continues until everything passes—or until a blocker requires human intervention.\n\n<details>\n<summary><strong>Why This Works (LLM First Principles)</strong></summary>\n\nLLMs aren't general reasoners. They're goal-oriented pattern matchers trained through reinforcement learning. This has implications:\n\n**They're trained on goals, not processes.** RL during training made them fundamentally goal-oriented. Clear acceptance criteria play to their strength. Rigid step-by-step plans fight their nature.\n\n**They can't hold all the nuances.** Neither can you. Some implementation details only surface once you're deep in the code. A rigid plan can't account for unknowns. Acceptance criteria can—because they define the destination, not the path.\n\n**They suffer from context drift.** Long sessions cause \"context rot\"—the model loses track of earlier instructions. Manifest-driven development compensates with external state (the manifest file) and verification that catches drift before it ships.\n\n**They don't know when they're wrong.** LLMs can't express genuine uncertainty. They'll confidently produce broken code. The verify-fix loop doesn't rely on the AI knowing it failed—it relies on automated checks catching failures.\n\nThis isn't a hack around LLM limitations. It's a design that treats those limitations as first principles.\n\n</details>\n\n## What /define Produces\n\nThe interview classifies your task (Code, Document, Blog, Research) and loads task-specific guidance. It probes for latent criteria—things you'd reject in a PR but wouldn't think to specify upfront. A `manifest-verifier` agent validates the manifest for gaps before output.\n\nHere's an example manifest:\n\n````markdown\n# Definition: User Authentication\n\n## 1. Intent & Context\n- **Goal:** Add password-based authentication to existing Express app\n  with JWT sessions. Users can register, log in, and log out.\n- **Mental Model:** Auth is a cross-cutting concern. Security invariants\n  apply globally; endpoint behavior is per-deliverable.\n\n## 2. Approach\n- **Architecture:** Middleware-based auth with JWT stored in httpOnly cookies\n- **Execution Order:** D1 (Model) → D2 (Endpoints) → D3 (Protected Routes)\n- **Risk Areas:**\n  - [R-1] Session fixation if tokens not rotated | Detect: security review\n  - [R-2] Timing attacks on password comparison | Detect: constant-time check\n- **Trade-offs:**\n  - [T-1] Simplicity vs Security → Prefer security (use bcrypt, not md5)\n\n## 3. Global Invariants (The Constitution)\n- [INV-G1] Passwords never stored in plaintext\n  ```yaml\n  verify:\n    method: bash\n    command: \"! grep -r 'password.*=' src/ | grep -v hash | grep -v test\"\n  ```\n- [INV-G2] All auth endpoints rate-limited (max 5 attempts/minute)\n  ```yaml\n  verify:\n    method: subagent\n    agent: general-purpose\n    prompt: \"Verify rate limiting exists on /login and /register endpoints\"\n  ```\n- [INV-G3] JWT secrets from environment, never hardcoded\n  ```yaml\n  verify:\n    method: bash\n    command: \"grep -r 'process.env.JWT' src/auth/\"\n  ```\n\n## 4. Process Guidance (Non-Verifiable)\n- [PG-1] Follow existing error handling patterns in the codebase\n- [PG-2] Use established logging conventions\n\n## 5. Known Assumptions\n- [ASM-1] Express.js already configured | Default: true | Impact if wrong: Add setup step\n- [ASM-2] PostgreSQL available | Default: true | Impact if wrong: Adjust migration\n\n## 6. Deliverables (The Work)\n\n### Deliverable 1: User Model & Migration\n**Acceptance Criteria:**\n- [AC-1.1] User model has id, email, hashedPassword, createdAt\n  ```yaml\n  verify:\n    method: codebase\n    pattern: \"User.*id.*email.*hashedPassword.*createdAt\"\n  ```\n- [AC-1.2] Email has unique constraint\n- [AC-1.3] Migration creates users table with indexes\n\n### Deliverable 2: Auth Endpoints\n**Acceptance Criteria:**\n- [AC-2.1] POST /register creates user, returns 201\n- [AC-2.2] POST /login validates credentials, returns JWT\n- [AC-2.3] Invalid credentials return 401, not 500\n  ```yaml\n  verify:\n    method: subagent\n    agent: code-bugs-reviewer\n    prompt: \"Check auth routes return 401 for auth failures, not 500\"\n  ```\n````\n\n## The Manifest Schema\n\n| Section | Purpose | ID Scheme |\n|---------|---------|-----------|\n| **Intent & Context** | Goal and mental model | — |\n| **Approach** | Architecture, execution order, risks, trade-offs | `R-{N}`, `T-{N}` |\n| **Global Invariants** | Task-level rules (task fails if violated) | `INV-G{N}` |\n| **Process Guidance** | Non-verifiable constraints on how to work | `PG-{N}` |\n| **Known Assumptions** | Low-impact items with defaults | `ASM-{N}` |\n| **Deliverables** | Ordered work items with acceptance criteria | `AC-{D}.{N}` |\n\nApproach section is added for complex tasks with dependencies, risks, or architectural decisions.\n\n## Verification Methods\n\nEvery criterion can have an automated verification method:\n\n| Method | When to Use | Example |\n|--------|-------------|---------|\n| `bash` | Commands with deterministic output | `npm run typecheck && npm run lint` |\n| `codebase` | Code pattern checks | Check file exists, pattern matches |\n| `subagent` | LLM-as-judge for subjective criteria | Code quality, security review |\n| `research` | External information lookup | API compatibility, version checks |\n| `manual` | Human verification required | UI review, deployment checks |\n\n```yaml\n# Bash verification\nverify:\n  method: bash\n  command: \"npm run test -- --coverage\"\n\n# Subagent verification with specialized reviewer\nverify:\n  method: subagent\n  agent: code-maintainability-reviewer\n  prompt: \"Review for DRY violations and coupling issues\"\n\n# Manual verification\nverify:\n  method: manual\n  instructions: \"Verify the login flow works in staging\"\n```\n\n## Plugin Architecture\n\n### Core Skills\n\n| Skill | Type | Description |\n|-------|------|-------------|\n| `/define` | User-invoked | Interview-driven manifest creation. Classifies task type, probes for latent criteria, generates manifest with verification methods. |\n| `/do` | User-invoked | Autonomous execution against manifest. Follows execution order, watches for risks, logs progress for disaster recovery. |\n| `/verify` | Internal | Spawns parallel verifiers for all criteria. Routes to `criteria-checker` agents based on verification method. |\n| `/done` | Internal | Outputs hierarchical completion summary mirroring manifest structure. |\n| `/escalate` | Internal | Structured escalation when blockers require human intervention. Requires evidence: 3+ attempted approaches, failure reasons, hypothesis, resolution options. |\n\n### Specialized Review Agents\n\nBuilt-in agents for quality verification via `subagent` method:\n\n| Agent | Focus |\n|-------|-------|\n| `criteria-checker` | Core verifier—validates single criterion using bash/codebase/subagent/research methods |\n| `manifest-verifier` | Validates manifest completeness during `/define` |\n| `code-bugs-reviewer` | Race conditions, data loss, edge cases, logic errors, resource leaks |\n| `code-maintainability-reviewer` | DRY violations, coupling, cohesion, dead code, consistency |\n| `code-simplicity-reviewer` | Over-engineering, premature optimization, cognitive complexity |\n| `code-testability-reviewer` | Excessive mocking requirements, logic buried in IO, hidden dependencies |\n| `code-coverage-reviewer` | Test coverage gaps in changed code |\n| `type-safety-reviewer` | TypeScript type safety—`any` abuse, invalid states representable, narrowing issues |\n| `docs-reviewer` | Documentation accuracy against code changes |\n| `claude-md-adherence-reviewer` | Compliance with CLAUDE.md project rules |\n\nEach reviewer returns structured output with severity levels (Critical, High, Medium, Low) and specific fix guidance.\n\n### Workflow Enforcement Hooks\n\nHooks enforce workflow integrity—the AI can't skip steps:\n\n| Hook | Event | Purpose |\n|------|-------|---------|\n| `stop_do_hook` | Stop command | Blocks premature stopping. Can't stop without verification passing or proper escalation. |\n| `pretool_escalate_hook` | `/escalate` invocation | Requires `/verify` before `/escalate`. No lazy escalation without attempting verification. |\n| `post_compact_hook` | Session compaction | Restores /do workflow context after compaction. Reminds to re-read manifest and log. |\n| `pretool_verify_hook` | `/verify` invocation | Ensures manifest and log are in context before spawning verifiers. |\n\n**Stop hook decision matrix:**\n- API error → Allow (system failure)\n- No `/do` active → Allow (not in workflow)\n- `/do` + `/done` → Allow (verified complete)\n- `/do` + `/escalate` → Allow (properly escalated)\n- `/do` only → Block (must verify)\n- `/do` + `/verify` with failures → Block (fix first)\n\n### Task-Specific Guidance\n\n`/define` loads conditional guidance based on task classification:\n\n| Task Type | Guidance | Quality Gates |\n|-----------|----------|---------------|\n| **Code** | `tasks/CODING.md` | Bug detection, type safety, maintainability, simplicity, test coverage, testability, CLAUDE.md adherence |\n| **Document** | `tasks/DOCUMENT.md` | Structure completeness, audience fit, clarity, consistency, accuracy |\n| **Blog** | `tasks/BLOG.md` | Blog-specific quality criteria |\n| **Research** | `tasks/RESEARCH.md` | Research-specific criteria (citations, topic coverage) |\n\n## Workflow Diagram\n\n```mermaid\nflowchart TB\n    subgraph define [\"/define\"]\n        A[Task Request] --> B[Interview]\n        B --> C[Generate Manifest]\n        C --> D[manifest-verifier]\n    end\n\n    D --> E[manifest.md]\n\n    subgraph do [\"/do\"]\n        E --> F[Execute Deliverable]\n        F --> G[\"/verify\"]\n        G --> H{Result?}\n        H -->|All pass| I[\"/done\"]\n        H -->|Failures| J[Fix Loop]\n        J --> G\n        H -->|Blocker| K[\"/escalate\"]\n    end\n```\n\n**Hooks enforce:** Can't stop without `/verify` passing or `/escalate`. Can't `/escalate` without running `/verify` first.\n\n## The Benefits\n\n**Your first pass lands closer to done.** Verification catches issues before you see them. The fix loop handles cleanup automatically.\n\n**You can trust the output.** Not because the AI is infallible, but because every acceptance criterion has been verified. You know what was checked.\n\n**You can parallelize.** While one manifest is executing, you can define the next. The define phase is where your judgment matters. The do-verify-fix phase runs on its own.\n\n**You stay connected to your codebase.** The define phase forces involvement—you can't write acceptance criteria without understanding what you want. This combats the atrophy problem where heavy AI assistance means losing touch with your own code.\n\n**Your process compounds.** When a PR passes verification but reviewers still find issues, encode those as new review agents or CLAUDE.md guidelines. Next time, the system catches what you missed.\n\n**It's dead simple to use.** Run `/define`, answer the interview questions, run `/do`, go grab coffee. No prompt engineering. No babysitting.\n\n**Resist the urge to intervene.** It won't nail everything on the first pass—that's expected. The verify-fix loop exists precisely for this. You invested in define; now let the loop run. It rarely gets there in a straight line, but it gets there.\n\n## Who This Is For\n\nExperienced developers frustrated by hype-driven AI coding tools. If you're tired of chasing the latest \"game-changing\" prompt that produces code you spend hours debugging, this offers a grounded alternative.\n\n**Our approach:**\n- Workflows designed around how LLMs actually work, not how we wish they worked\n- Quality over speed—invest upfront, ship with confidence\n- Simple to use, sophisticated under the hood\n\n**Not for:**\n- Cost optimizers (workflows may use more tokens for better results)\n- Speed-first developers (we prioritize quality over raw speed)\n- Hype chasers (we're grounded and realistic)\n\n## Development\n\n```bash\n# Lint, format, typecheck\nruff check --fix claude-plugins/ && black claude-plugins/ && mypy\n\n# Test hooks (run after ANY hook changes)\npytest tests/hooks/ -v\n\n# Test plugin locally\n/plugin marketplace add /path/to/manifest-dev\n/plugin install manifest-dev@manifest-dev-marketplace\n```\n\n## Contributing\n\nSee [CONTRIBUTING.md](./CONTRIBUTING.md) for plugin development guidelines.\n\n## License\n\nMIT\n\n---\n\n*Built by developers who understand LLM limitations—and design around them.*\n\nFollow along: [@aviramk](https://twitter.com/aviramk)\n",
        "claude-plugins/manifest-dev/README.md": "# manifest-dev\n\nManifest-driven workflows separating **what to build** (Deliverables) from **rules to follow** (Global Invariants).\n\n## Overview\n\nA structured approach to task definition and execution:\n\n1. **Approach** (complex tasks) - Validated implementation direction: architecture, execution order, risks, trade-offs\n2. **Global Invariants** - Rules that apply to the ENTIRE task (e.g., \"tests must pass\")\n3. **Deliverables** - Specific items to complete, each with **Acceptance Criteria**\n   - ACs can be positive (\"user can log in\") or negative (\"passwords are hashed\")\n\n## The Manifest Schema\n\n```markdown\n# Definition: [Title]\n\n## 1. Intent & Context\n- **Goal:** [High-level purpose]\n- **Mental Model:** [Key concepts/architecture]\n\n## 2. Approach (Complex Tasks Only)\n*Validated implementation direction.*\n\n- **Architecture:** [High-level HOW - validated direction]\n- **Execution Order:** D1 → D2 → D3 | Rationale: [why]\n- **Risk Areas:**\n  - [R-1] [What could go wrong] | Detect: [how you'd know]\n- **Trade-offs:**\n  - [T-1] [A] vs [B] → Prefer [A] because [reason]\n\n## 3. Global Invariants (The Constitution)\n- [INV-G1] Description | Verify: [method]\n- [INV-G2] Description | Verify: [method]\n\n## 4. Deliverables (The Work)\n\n### Deliverable 1: [Name]\n- **Acceptance Criteria**:\n  - [AC-1.1] Description | Verify: [method]\n  - [AC-1.2] Description | Verify: [method]\n```\n\n## ID Scheme\n\n| Type | Pattern | Purpose | Used By |\n|------|---------|---------|---------|\n| Global Invariant | INV-G{N} | Task-level rules | /verify (verified) |\n| Process Guidance | PG-{N} | Non-verifiable HOW constraints | /do (followed) |\n| Risk Area | R-{N} | Pre-mortem flags | /do (watched) |\n| Trade-off | T-{N} | Decision criteria for adjustment | /do (consulted) |\n| Acceptance Criteria | AC-{D}.{N} | Deliverable completion | /verify (verified) |\n\n## Interview Philosophy\n\n**YOU generate, user validates.** Users have surface-level knowledge. Don't ask open-ended questions - generate candidates from domain knowledge, present concrete options, learn from reactions.\n\n**Phase order** (high info-gain first):\n1. Intent & Context (task type, scope, risk)\n2. Deliverables (what are we building?)\n3. Acceptance Criteria (how do we know each is done?)\n4. Approach (complex tasks: architecture, execution order, risks, trade-offs)\n5. Global Invariants & Process Guidance (auto-detect + generate candidates)\n\n## Skills\n\n### User-Invocable\n\n| Skill | Description |\n|-------|-------------|\n| `/define` | Manifest builder with verification criteria. Converts known requirements into Deliverables + Invariants. Outputs executable manifest. |\n| `/do` | Manifest executor. Iterates through Deliverables satisfying Acceptance Criteria, then verifies all ACs and Global Invariants pass. |\n\n### Task-Specific Guidance\n\n`/define` is domain-agnostic and works for any deliverable type. Task-specific guidance is loaded conditionally:\n\n| Task Type | File | When Loaded |\n|-----------|------|-------------|\n| Code | `skills/define/tasks/CODING.md` | APIs, features, fixes, refactors, tests |\n| Document | `skills/define/tasks/DOCUMENT.md` | Specs, proposals, reports, articles, docs |\n| Blog | `skills/define/tasks/BLOG.md` | Blog posts, content writing |\n| Research | `skills/define/tasks/RESEARCH.md` | Research tasks, analysis, investigation |\n| Other | (none) | Doesn't fit above categories |\n\nThe universal flow (core principles, manifest schema) works without any task file.\n\n### Internal\n\n| Skill | Purpose |\n|-------|---------|\n| `/verify` | Manifest verification runner. Spawns parallel verifiers for Global Invariants and Acceptance Criteria. |\n| `/done` | Completion marker. Outputs hierarchical execution summary showing Global Invariants respected and all Deliverables completed. |\n| `/escalate` | Structured escalation with evidence. Surfaces blocking issues for human decision, referencing the Manifest hierarchy. |\n\n## Agents\n\n### Core Workflow\n\n| Agent | Purpose |\n|-------|---------|\n| `criteria-checker` | Read-only verification agent. Validates a single criterion using commands, codebase analysis, file inspection, reasoning, or web research. Returns structured PASS/FAIL. |\n| `manifest-verifier` | Reviews /define manifests for gaps and outputs actionable continuation steps. Returns specific questions to ask and areas to probe. |\n\n### Code Reviewers\n\nSpecialized review agents spawned in parallel during `/verify`:\n\n| Agent | Focus |\n|-------|-------|\n| `code-bugs-reviewer` | Audits code changes for logical bugs without making modifications |\n| `code-coverage-reviewer` | Verifies code changes have adequate test coverage, reports gaps |\n| `code-maintainability-reviewer` | DRY violations, coupling, cohesion, consistency, dead code, architectural boundaries |\n| `code-simplicity-reviewer` | Unnecessary complexity, over-engineering, cognitive burden |\n| `code-testability-reviewer` | Code that requires excessive mocking, business logic hard to verify in isolation |\n| `type-safety-reviewer` | TypeScript type holes, opportunities to make invalid states unrepresentable |\n| `claude-md-adherence-reviewer` | Verifies code changes comply with CLAUDE.md instructions and project standards |\n| `docs-reviewer` | Audits documentation accuracy against recent code changes |\n\n## Hooks\n\n| Hook | Purpose |\n|------|---------|\n| `stop_do_hook.py` | Enforces verification before stopping |\n| `post_compact_hook.py` | Restores /do workflow context after session compaction |\n| `pretool_verify_hook.py` | Reminds to read manifest/log before verification |\n\n## Workflow\n\n```\n/define \"task\" → Interview → Manifest file\n                    │\n                    ├─ Intent & Context\n                    ├─ Deliverables (with ACs)\n                    ├─ Approach (complex tasks: architecture, order, risks, trade-offs)\n                    └─ Global Invariants & Process Guidance\n                                   ↓\n/do manifest.md → Follow execution order, watch for risks\n                    │\n                    ├─ Risk detected? → Consult trade-offs → Adjust approach\n                    │                   (ACs achievable? Continue : /escalate)\n                    │\n                    └─ For each Deliverable: Satisfy ACs\n                                   ↓\n                  /verify → (failures) → Fix specific criterion → /verify again\n                         ↓\n                  All pass → /done\n                         ↓\n                  (stuck) → /escalate\n```\n\n## Execution Semantics\n\n| Phase | Check | Failure Impact |\n|-------|-------|----------------|\n| After each deliverable | Acceptance Criteria | Deliverable incomplete |\n| Final verification | Global Invariants + all ACs | Must all pass for /done |\n\n## Status\n\nUse when you want quality-focused autonomous execution with clear separation of constraints and deliverables.\n"
      },
      "plugins": [
        {
          "name": "manifest-dev",
          "source": "./claude-plugins/manifest-dev",
          "description": null,
          "categories": [],
          "install_commands": [
            "/plugin marketplace add doodledood/manifest-dev",
            "/plugin install manifest-dev@manifest-dev"
          ]
        }
      ]
    }
  ]
}