{
  "author": {
    "id": "chrisvoncsefalvay",
    "display_name": "Chris von Csefalvay",
    "avatar_url": "https://avatars.githubusercontent.com/u/2670100?u=f96086afdbb09c4b3e6445cd51114612ad8edb10&v=4"
  },
  "marketplaces": [
    {
      "name": "funsloth",
      "version": null,
      "description": "Claude Code skills marketplace for fine-tuning language models with Unsloth",
      "repo_full_name": "chrisvoncsefalvay/funsloth",
      "repo_url": "https://github.com/chrisvoncsefalvay/funsloth",
      "repo_description": "The fun way to manage fine-tuning with Unsloth.",
      "signals": {
        "stars": 4,
        "forks": 0,
        "pushed_at": "2025-12-18T15:13:51Z"
      },
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"name\": \"funsloth\",\n  \"owner\": {\n    \"name\": \"Chris von Csefalvay\",\n    \"email\": \"chris@chrisvoncsefalvay.com\"\n  },\n  \"metadata\": {\n    \"description\": \"Claude Code skills marketplace for fine-tuning language models with Unsloth\",\n    \"version\": \"1.0.0\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"funsloth\",\n      \"source\": \"./\",\n      \"description\": \"Complete workflow for fine-tuning LLMs with Unsloth: dataset validation, notebook generation, training management, and model deployment\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Chris von Csefalvay\",\n        \"email\": \"chris@chrisvoncsefalvay.com\"\n      },\n      \"repository\": \"https://github.com/chrisvoncsefalvay/funsloth\",\n      \"license\": \"MIT\",\n      \"keywords\": [\n        \"unsloth\",\n        \"fine-tuning\",\n        \"llm\",\n        \"lora\",\n        \"qlora\",\n        \"machine-learning\",\n        \"huggingface\",\n        \"training\"\n      ],\n      \"category\": \"machine-learning\",\n      \"strict\": false\n    }\n  ]\n}\n",
        "README.md": "# funsloth: Fully Automated Luxury LLM Fine-Tuning\n\n<div align=\"center\">\n  <img src=\".github/assets/logo.jpeg\" alt=\"funsloth logo\" width=\"600\">\n</div>\n\n<br>\n\nA Claude Code skills marketplace for fine-tuning language models with [Unsloth](https://github.com/unslothai/unsloth). It's premised on the [dorkestration](https://chrisvoncsefalvay.com/posts/dorkestration/) paradigm, enabling seamless orchestration of multiple skills to achieve complex workflows using Claude Code.\n\n## Overview\n\nfunsloth provides six connected skills that guide you through the complete fine-tuning workflow:\n\n```\nfunsloth-check → funsloth-train → [hfjobs|runpod|local] → funsloth-upload\n```\n\n| Skill | Description |\n|-------|-------------|\n| `funsloth-check` | Validate datasets, analyze token counts, calculate Chinchilla optimality |\n| `funsloth-train` | Generate Unsloth training notebooks with sensible defaults or custom config |\n| `funsloth-hfjobs` | Train on Hugging Face Jobs cloud GPUs |\n| `funsloth-runpod` | Train on RunPod GPU instances |\n| `funsloth-local` | Train on your local GPU |\n| `funsloth-upload` | Generate model cards and upload to Hugging Face Hub |\n\n## Installation\n\n```bash\nclaude plugin install funsloth\n```\n\nOr install from source:\n```bash\ngit clone https://github.com/chrisvoncsefalvay/funsloth\ncd funsloth\nclaude plugin install .\n```\n\n## Usage\n\n### Quick start\n\nJust tell Claude what you want to do:\n\n```\n> I want to fine-tune Llama 3.1 8B on my custom dataset\n```\n\nClaude will automatically invoke the appropriate skills.\n\n### Manual skill invocation\n\nYou can also invoke skills directly:\n\n```\n> /funsloth-check mlabonne/FineTome-100k\n> /funsloth-train\n> /funsloth-local\n> /funsloth-upload\n```\n\n## Supported models\n\n| Family | Sizes | Recommended 4-bit |\n|--------|-------|-------------------|\n| Llama 3.x | 1B, 3B, 8B, 70B | `unsloth/llama-3.1-8b-unsloth-bnb-4bit` |\n| Qwen 2.5/3 | 0.5B-72B | `unsloth/Qwen2.5-7B-Instruct-bnb-4bit` |\n| Gemma 2/3 | 2B, 9B, 27B | `unsloth/gemma-2-9b-it-bnb-4bit` |\n| Phi-4 | 14B | `unsloth/Phi-4-bnb-4bit` |\n| Mistral | 7B, 8x7B | `unsloth/mistral-7b-instruct-v0.3-bnb-4bit` |\n| DeepSeek | 7B+ | `unsloth/DeepSeek-R1-Distill-Qwen-7B-bnb-4bit` |\n\n## Supported training techniques\n\n- **SFT** - Supervised Fine-Tuning\n- **[DPO](https://arxiv.org/abs/2305.18290)** - Direct Preference Optimization (Rafailov et al., NeurIPS 2023)\n- **[GRPO](https://arxiv.org/abs/2402.03300)** - Group Relative Policy Optimization (DeepSeekMath, 2024)\n- **[ORPO](https://arxiv.org/abs/2403.07691)** - Odds Ratio Preference Optimization (Hong et al., EMNLP 2024)\n- **[KTO](https://arxiv.org/abs/2402.01306)** - Kahneman-Tversky Optimization (Ethayarajh et al., ICML 2024)\n\n## Data formats\n\n\n\n| Format | Structure | Use Case |\n|--------|-----------|----------|\n| Raw Corpus | `{\"text\": \"...\"}` | Continued pretraining |\n| Alpaca | `{\"instruction\", \"input\", \"output\"}` | Instruction tuning |\n| ShareGPT | `[{\"from\": \"human\", \"value\": \"...\"}]` | Conversations |\n| ChatML | `[{\"role\": \"user\", \"content\": \"...\"}]` | Native chat |\n\nUnsloth supports many [chat templates](https://docs.unsloth.ai/basics/chat-templates) including `llama-3`, `chatml`, `mistral`, `gemma`, `phi-3`, `phi-4`, `qwen-2.5`, `alpaca`, `zephyr` and `vicuna`. Custom templates can be provided as a `(template, eos_token)` tuple.\n\n## Requirements\n\n- Claude Code CLI\n- Python 3.10+\n- CUDA-capable GPU (for local training)\n- Hugging Face account (for dataset/model hosting)\n\n## License\n\nMIT\n\n## Acknowledgments\n\n- [Ben Burtenshaw](https://huggingface.co/blog/hf-skills-training), the OG of Claude Code fine-tuning skills\n- [Unsloth](https://github.com/unslothai/unsloth) - fast LLM fine-tuning\n- [Hugging Face](https://huggingface.co) - model and dataset hosting\n- [TRL](https://github.com/huggingface/trl) - RL for transformers!\n"
      },
      "plugins": [
        {
          "name": "funsloth",
          "source": "./",
          "description": "Complete workflow for fine-tuning LLMs with Unsloth: dataset validation, notebook generation, training management, and model deployment",
          "version": "1.0.0",
          "author": {
            "name": "Chris von Csefalvay",
            "email": "chris@chrisvoncsefalvay.com"
          },
          "repository": "https://github.com/chrisvoncsefalvay/funsloth",
          "license": "MIT",
          "keywords": [
            "unsloth",
            "fine-tuning",
            "llm",
            "lora",
            "qlora",
            "machine-learning",
            "huggingface",
            "training"
          ],
          "category": "machine-learning",
          "strict": false,
          "categories": [
            "fine-tuning",
            "huggingface",
            "llm",
            "lora",
            "machine-learning",
            "qlora",
            "training",
            "unsloth"
          ],
          "install_commands": [
            "/plugin marketplace add chrisvoncsefalvay/funsloth",
            "/plugin install funsloth@funsloth"
          ]
        }
      ]
    }
  ]
}