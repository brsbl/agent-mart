{
  "author": {
    "id": "c-daly",
    "display_name": "Christopher Daly",
    "type": "User",
    "avatar_url": "https://avatars.githubusercontent.com/u/83555576?u=915c74cf27d5161d65d88d1660c3138461e471b4&v=4",
    "url": "https://github.com/c-daly",
    "bio": "I like to make things.",
    "stats": {
      "total_marketplaces": 1,
      "total_plugins": 1,
      "total_commands": 3,
      "total_skills": 10,
      "total_stars": 0,
      "total_forks": 0
    }
  },
  "marketplaces": [
    {
      "name": "fearsidhe-plugins",
      "version": null,
      "description": "Personal plugins for Claude Code",
      "owner_info": {
        "name": "fearsidhe"
      },
      "keywords": [],
      "repo_full_name": "c-daly/agent-swarm",
      "repo_url": "https://github.com/c-daly/agent-swarm",
      "repo_description": "Enforcement system for agent-based workflows in Claude Code",
      "homepage": null,
      "signals": {
        "stars": 0,
        "forks": 0,
        "pushed_at": "2026-01-29T21:09:07Z",
        "created_at": "2026-01-04T21:18:40Z",
        "license": null
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/manifest.json",
          "type": "blob",
          "size": 1014
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 621
        },
        {
          "path": ".claude-plugin/plugin.json",
          "type": "blob",
          "size": 158
        },
        {
          "path": "README.md",
          "type": "blob",
          "size": 1621
        },
        {
          "path": "agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "agents/AGENT_RULES.md",
          "type": "blob",
          "size": 865
        },
        {
          "path": "agents/adversary.md",
          "type": "blob",
          "size": 1713
        },
        {
          "path": "agents/architect.md",
          "type": "blob",
          "size": 867
        },
        {
          "path": "agents/debugger.md",
          "type": "blob",
          "size": 867
        },
        {
          "path": "agents/explorer.md",
          "type": "blob",
          "size": 1126
        },
        {
          "path": "agents/git-agent.md",
          "type": "blob",
          "size": 1189
        },
        {
          "path": "agents/implementer.md",
          "type": "blob",
          "size": 1240
        },
        {
          "path": "agents/researcher.md",
          "type": "blob",
          "size": 769
        },
        {
          "path": "agents/reviewer.md",
          "type": "blob",
          "size": 781
        },
        {
          "path": "commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "commands/implement.md",
          "type": "blob",
          "size": 822
        },
        {
          "path": "commands/orchestrate.md",
          "type": "blob",
          "size": 423
        },
        {
          "path": "commands/spawn.md",
          "type": "blob",
          "size": 256
        },
        {
          "path": "examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/neural-net-viz",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/neural-net-viz/README.md",
          "type": "blob",
          "size": 3900
        },
        {
          "path": "hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "hooks/.context",
          "type": "tree",
          "size": null
        },
        {
          "path": "hooks/.context/EPISODES.md",
          "type": "blob",
          "size": 19
        },
        {
          "path": "hooks/__init__.py",
          "type": "blob",
          "size": 16
        },
        {
          "path": "hooks/auto-orchestrate-hook.sh",
          "type": "blob",
          "size": 386
        },
        {
          "path": "hooks/background-enforcement.py",
          "type": "blob",
          "size": 1385
        },
        {
          "path": "hooks/base-enforcement.py",
          "type": "blob",
          "size": 3535
        },
        {
          "path": "hooks/combined-enforcement.py.backup",
          "type": "blob",
          "size": 24973
        },
        {
          "path": "hooks/combined_enforcement.py",
          "type": "blob",
          "size": 23
        },
        {
          "path": "hooks/context-injection.py",
          "type": "blob",
          "size": 1125
        },
        {
          "path": "hooks/enforce-protocol.sh",
          "type": "blob",
          "size": 364
        },
        {
          "path": "hooks/hook_logging.py",
          "type": "blob",
          "size": 3479
        },
        {
          "path": "hooks/hooks.json",
          "type": "blob",
          "size": 4221
        },
        {
          "path": "hooks/implementer-only-enforcement.py",
          "type": "blob",
          "size": 3409
        },
        {
          "path": "hooks/inject-subagent-briefing.sh",
          "type": "blob",
          "size": 1799
        },
        {
          "path": "hooks/iterate-enforcement.py",
          "type": "blob",
          "size": 5594
        },
        {
          "path": "hooks/max-agents-enforcement.py",
          "type": "blob",
          "size": 1643
        },
        {
          "path": "hooks/monitor_agent.py",
          "type": "blob",
          "size": 11175
        },
        {
          "path": "hooks/native-tool-blocking.py",
          "type": "blob",
          "size": 3526
        },
        {
          "path": "hooks/nonblocking-enforcement.py",
          "type": "blob",
          "size": 1368
        },
        {
          "path": "hooks/parallel-enforcement.py",
          "type": "blob",
          "size": 4613
        },
        {
          "path": "hooks/post-task-tracking.py",
          "type": "blob",
          "size": 9062
        },
        {
          "path": "hooks/post-tool-tracking.py",
          "type": "blob",
          "size": 4946
        },
        {
          "path": "hooks/pre-compacting.py",
          "type": "blob",
          "size": 7825
        },
        {
          "path": "hooks/router-event-hook.py",
          "type": "blob",
          "size": 3185
        },
        {
          "path": "hooks/session-end.py",
          "type": "blob",
          "size": 12405
        },
        {
          "path": "hooks/session-init.py",
          "type": "blob",
          "size": 2958
        },
        {
          "path": "hooks/session-start.py",
          "type": "blob",
          "size": 29978
        },
        {
          "path": "hooks/state-protection.py",
          "type": "blob",
          "size": 3414
        },
        {
          "path": "hooks/subagent-bash-lockdown.py",
          "type": "blob",
          "size": 1664
        },
        {
          "path": "hooks/subagent-briefing.md",
          "type": "blob",
          "size": 5873
        },
        {
          "path": "hooks/subagent-complete.py",
          "type": "blob",
          "size": 10286
        },
        {
          "path": "hooks/subagent-complete.py.bak",
          "type": "blob",
          "size": 5665
        },
        {
          "path": "hooks/subagent-enforcement.py",
          "type": "blob",
          "size": 13536
        },
        {
          "path": "hooks/subagent-mcp-bypass.py",
          "type": "blob",
          "size": 1801
        },
        {
          "path": "hooks/telemetry-posttool.py",
          "type": "blob",
          "size": 8413
        },
        {
          "path": "hooks/telemetry-pretool.py",
          "type": "blob",
          "size": 3535
        },
        {
          "path": "hooks/telemetry-sessionstart.py",
          "type": "blob",
          "size": 2584
        },
        {
          "path": "hooks/test-existence-enforcement.py",
          "type": "blob",
          "size": 4309
        },
        {
          "path": "hooks/transcript-debug.py",
          "type": "blob",
          "size": 783
        },
        {
          "path": "hooks/verification_gates.py",
          "type": "blob",
          "size": 10371
        },
        {
          "path": "hooks/work_seeker.py.obsolete",
          "type": "blob",
          "size": 3678
        },
        {
          "path": "hooks/workflow-enforcement.py",
          "type": "blob",
          "size": 3278
        },
        {
          "path": "hooks/workflow-state-enforcement.py",
          "type": "blob",
          "size": 3400
        },
        {
          "path": "lib",
          "type": "tree",
          "size": null
        },
        {
          "path": "lib/README.md",
          "type": "blob",
          "size": 1876
        },
        {
          "path": "skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/ctx",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/ctx/SKILL.md",
          "type": "blob",
          "size": 1869
        },
        {
          "path": "skills/debug",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/debug/SKILL.md",
          "type": "blob",
          "size": 6235
        },
        {
          "path": "skills/distill",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/distill/SKILL.md",
          "type": "blob",
          "size": 2603
        },
        {
          "path": "skills/implement",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/implement/SKILL.md",
          "type": "blob",
          "size": 2146
        },
        {
          "path": "skills/iterate",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/iterate/SKILL.md",
          "type": "blob",
          "size": 19828
        },
        {
          "path": "skills/poll",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/poll/SKILL.md",
          "type": "blob",
          "size": 2831
        },
        {
          "path": "skills/pr-comment",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/pr-comment/SKILL.md",
          "type": "blob",
          "size": 5617
        },
        {
          "path": "skills/remember",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/remember/SKILL.md",
          "type": "blob",
          "size": 823
        },
        {
          "path": "skills/spawn",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/spawn/SKILL.md",
          "type": "blob",
          "size": 6295
        },
        {
          "path": "skills/verify",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/verify/SKILL.md",
          "type": "blob",
          "size": 1815
        }
      ],
      "files": {
        ".claude-plugin/manifest.json": "{\n  \"name\": \"agent-swarm\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Enforcement system for agent-based workflows\",\n  \"author\": {\n    \"name\": \"fearsidhe\"\n  },\n  \"skills\": [\n    \"orchestrate\",\n    \"spawn\",\n    \"iterate\"\n  ],\n  \"hooks\": {\n    \"preToolUse\": {\n      \"script\": \"../hooks/iterate-enforcement.py\",\n      \"description\": \"Enforces iterate workflow phase restrictions\"\n    },\n    \"postToolUse\": {\n      \"script\": \"../hooks/post-task-tracking.py\",\n      \"description\": \"Track subagent completion, token usage, and auto-detect new plugins\"\n    },\n    \"SessionStart\": {\n      \"script\": \"../hooks/session-start.py\",\n      \"description\": \"Auto-search episodic memory for relevant past conversations\"\n    },\n    \"SessionEnd\": {\n      \"script\": \"../hooks/session-end.py\",\n      \"description\": \"Auto-generate metrics dashboard at session end\"\n    },\n    \"preCompacting\": {\n      \"script\": \"../hooks/pre-compacting.py\",\n      \"description\": \"Auto-write handoff before context compacting to preserve state\"\n    }\n  }\n}\n",
        ".claude-plugin/marketplace.json": "{\n  \"$schema\": \"https://anthropic.com/claude-code/marketplace.schema.json\",\n  \"name\": \"fearsidhe-plugins\",\n  \"description\": \"Personal plugins for Claude Code\",\n  \"owner\": {\n    \"name\": \"fearsidhe\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"agent-swarm\",\n      \"description\": \"Enforcement system for agent-based workflows. Manages phase enforcement, script routing for batch MCP operations, and autopilot approval.\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"fearsidhe\"\n      },\n      \"source\": \"./\",\n      \"category\": \"productivity\",\n      \"homepage\": \"https://github.com/c-daly/agent-swarm\"\n    }\n  ]\n}\n",
        ".claude-plugin/plugin.json": "{\n  \"name\": \"agent-swarm\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Enforcement system for agent-based workflows\",\n  \"author\": {\n    \"name\": \"fearsidhe\"\n  }\n}\n",
        "README.md": "# Agent Swarm\n\nEnforcement system for agent-based workflows in Claude Code.\n\n## Features\n\n- **Phase Enforcement**: During 'implement' phase, blocks direct Edit/Write tools - requires spawning subagents for code changes\n- **Script Routing**: Tracks search tool usage, encourages batch scripts for MCP operations\n- **Autopilot Mode**: Auto-approves all tools when enabled\n\n## Installation\n\n```bash\nclaude plugin install agent-swarm@<your-marketplace>\n```\n\nOr install from GitHub:\n\n```bash\nclaude plugin add https://github.com/fearsidhe/agent-swarm\n```\n\n## Usage\n\n### Session State\n\nThe plugin uses `~/.claude/plugins/agent-swarm/.state/session.json` to track:\n\n```json\n{\n  \"phase\": \"implement\",\n  \"autopilot_override\": false,\n  \"in_subagent\": false,\n  \"search_count\": 0\n}\n```\n\n### Phases\n\n- **intake**: Gathering requirements (no restrictions)\n- **design**: Planning implementation (no restrictions)\n- **implement**: Coding phase (Edit/Write blocked unless via subagent)\n- **verify**: Testing (no restrictions)\n- **review**: Final review (no restrictions)\n\n### Autopilot Mode\n\nEnable autopilot to auto-approve all tools:\n\n```bash\npython3 -c \"\nimport json\nfrom pathlib import Path\nstate_path = Path.home() / '.claude/plugins/agent-swarm/.state/session.json'\nstate = json.loads(state_path.read_text()) if state_path.exists() else {}\nstate['autopilot_override'] = True\nstate_path.write_text(json.dumps(state, indent=2))\nprint('Autopilot enabled')\n\"\n```\n\n## Files\n\n- `hooks/combined-enforcement.py` - Main enforcement hook\n- `.state/session.json` - Runtime state (gitignored)\n- `.claude-plugin/manifest.json` - Plugin manifest\n",
        "agents/AGENT_RULES.md": "# Agent Rules - Non-Negotiable\n\n**READ FIRST:** [CORE_PROTOCOL.md](../CORE_PROTOCOL.md) contains universal rules for all agents.\n\nThis file contains role-specific constraints for agent-swarm agents.\n\n## Output Rules\n\n1. **NO PROSE** - Use bullet points, tables, code blocks\n2. **NO EXPLANATIONS** - Just facts and actions\n3. **NO PLEASANTRIES** - No \"I'll help you\", \"Let me\", \"Great question\"\n4. **NO REPETITION** - Say it once, reference by name after\n5. **MAX LENGTHS**:\n   - Summary: 3 sentences\n   - File reference: path:line only\n   - Code snippet: 10 lines max, use \"...\" for omitted\n   - Error description: 1 line\n\n## Communication Rules\n\n1. **STRUCTURED RETURNS** - Use the output format from your agent spec\n2. **FAIL FAST** - If blocked, report immediately, don't try alternatives\n3. **STAY IN LANE** - Only do what you're tasked. No \"while I'm here...\"\n",
        "agents/adversary.md": "# Adversary Agent\n\n**Model**: sonnet\n\n**READ FIRST:** [CORE_PROTOCOL.md](../CORE_PROTOCOL.md) for tool selection, batch operations, and parallel execution rules.\n\n## Purpose\nAdversarial test quality evaluation. Used for:\n- Identifying test coverage blind spots\n- Writing tests for uncovered code paths\n- Validating test legitimacy via Greptile\n- Routing failures back to implementer\n\n## Behavior\n- Run `pytest --cov --cov-report=json` to collect coverage\n- Use `scripts/adversary_analyze.py` to parse coverage data\n- Query Greptile: \"Should passing tests give confidence? What's missing?\"\n- Write tests targeting identified gaps (same test directory)\n- Submit new tests to Greptile for fairness validation\n- Run tests; route to implementer on failure\n- Loop until Greptile approves coverage\n\n## Scopes\n- `commit`: Files changed in HEAD commit\n- `pr`: Files changed in PR vs base branch\n- `codebase`: Full coverage analysis\n\n## Greptile Queries\n\n**Gap Discovery:**\n> Review tests for [files]. Coverage: [X]%. Uncovered lines: [list].\n> Should passing tests give confidence this code is strong? What's missing?\n\n**Test Validation:**\n> Review these new tests. Are they:\n> 1. Testing real behavior (not trivial)?\n> 2. Fair (not gaming coverage)?\n> 3. Following project test patterns?\n\n## Output Format (REQUIRED)\n\n**Max length:** 2000 characters\n\n```markdown\n## Adversary: [scope]\n\n**Coverage:** [X]% overall | [Y]% for scope\n\n**Gaps Found:**\n- [file:line]: [description]\n\n**Tests Written:**\n- [test_file:line]: [what it tests]\n\n**Greptile Says:** [summary of validation]\n\n**Verdict:** [WEAK|STRENGTHENED|SOLID]\n**Action:** [LOOP|IMPLEMENTER|PROCEED]\n```\n\n**Enforcement:** Responses exceeding limits will be rejected\n",
        "agents/architect.md": "# Architect Agent\n\n**Model**: sonnet\n\n**READ FIRST:** [CORE_PROTOCOL.md](../CORE_PROTOCOL.md) for tool selection, batch operations, and parallel execution rules.\n\n## Purpose\nArchitecture and design decisions. Used for:\n- Planning multi-file changes\n- Evaluating implementation approaches\n- Ensuring consistency with existing patterns\n\n## Behavior\n- Survey existing patterns first (Serena)\n- Propose multiple approaches when appropriate\n- Consider maintainability and testing\n- Reference similar implementations\n\n## Output Format (REQUIRED)\n\n**Max length:** 2500 characters\n\n```markdown\n## Architecture: [Decision]\n\n**Current State:**\n- Relevant existing patterns\n\n**Proposed Approach:**\n- Design with rationale\n\n**Files Affected:**\n- List with change types\n\n**Trade-offs:**\n- Pros and cons considered\n```\n\n**Enforcement:** Responses exceeding limits will be rejected\n",
        "agents/debugger.md": "# Debugger Agent\n\n**Model**: sonnet (needs to reason about code)\n\n## Purpose\nDebug and fix issues. Used when:\n- Tests fail\n- Runtime errors occur\n- Unexpected behavior reported\n\n## Behavior\n- Reproduce the issue\n- Trace the root cause\n- Implement minimal fix\n- Verify fix works\n\n## Token Efficiency\n- Start from error message/stack trace\n- Binary search for root cause\n- Fix only the bug (no refactoring)\n- Return fix + verification\n\n## Process\n1. Understand the symptom\n2. Reproduce (run test or manual steps)\n3. Trace backwards from error\n4. Identify root cause\n5. Implement minimal fix\n6. Verify fix\n7. Check for regressions\n\n## Output Format\n```markdown\n## Debug: [Issue]\n\n**Symptom:** What was observed\n\n**Root Cause:** Why it happened\n\n**Fix:**\n- `file:line` - what changed\n\n**Verification:** How fix was confirmed\n\n**Regression Check:** Other areas tested\n```\n",
        "agents/explorer.md": "# Explorer Agent\n\n**Model**: haiku (fast exploration, many parallel searches)\n\n**READ FIRST:** [CORE_PROTOCOL.md](../CORE_PROTOCOL.md) for tool selection, batch operations, and parallel execution rules.\n\n## Purpose\nCodebase exploration for understanding existing code. Used for:\n- Finding relevant files\n- Understanding patterns in use\n- Locating similar implementations\n- Mapping dependencies\n\n## Behavior\n- Use Glob/Grep efficiently (batch patterns when 3+)\n- Read only relevant sections of files\n- Return file:line references, not full content\n- Summarize patterns found\n\n## Output Format (REQUIRED)\n\n**Max length:** 2000 characters\n**Max file references:** 20\n**Max description per item:** 50 characters\n\n```markdown\n## Exploration: [Query]\n\n**Relevant Files:** (max 20)\n- `path/file.ts:line` - brief description (max 50 chars)\n\n**Patterns Found:** (max 10)\n- Pattern name: where used\n\n**Key Functions/Classes:** (max 15)\n- `functionName` in `file.ts` - what it does (max 50 chars)\n\n**Suggested Starting Points:** (max 5)\n1. file:line - why (max 50 chars)\n```\n\n**Enforcement:** Responses exceeding limits will be rejected\n",
        "agents/git-agent.md": "# Git Agent\n\n**Model**: haiku (straightforward operations)\n\n## Purpose\nVersion control operations. Used for:\n- Staging and committing\n- Branch management\n- PR creation\n- Conflict resolution\n\n## Behavior\n- Follow repository conventions\n- Write clear commit messages\n- Check before destructive operations\n- Report status after operations\n\n## Token Efficiency\n- Use git commands directly (no explanations)\n- Batch related operations\n- Return status, not command output\n\n## Safety\n- NEVER force push to main/master\n- NEVER amend pushed commits without explicit approval\n- NEVER skip hooks without explicit approval\n- Always verify branch before push\n\n## Commit Message Format\n```\n<type>: <description>\n\n<body if needed>\n```\nTypes: feat, fix, refactor, test, docs, chore\n\n**IMPORTANT:**\n- Do NOT add attributions (\"Generated with Claude Code\", \"Co-Authored-By\", etc.)\n- Do NOT add emoji or decorations\n- Keep messages clean and professional\n- Follow existing repository conventions\n\n## Output Format\n```markdown\n## Git: [Operation]\n\n**Actions:**\n- action taken\n\n**Status:**\n- Current branch: X\n- Commits ahead/behind: X\n- Uncommitted changes: yes/no\n\n**Next:** suggested next step (if any)\n```\n",
        "agents/implementer.md": "---\nname: implementer\ntools: Bash(mcp*)\ndescription: Code implementation with quality focus - writing new functionality, modifying existing code, ensuring side-effect safety\nmodel: opus\n---\n\n# Implementer Agent\n\n**READ FIRST:** [CORE_PROTOCOL.md](../CORE_PROTOCOL.md) for tool selection, batch operations, and parallel execution rules.\n\n## Purpose\nCode implementation with quality focus. Used for:\n- Writing new functionality\n- Modifying existing code\n- Ensuring side-effect safety\n- Maintaining code quality\n\n## Behavior\n- Check side-effects before changes (find_referencing_symbols)\n- Write tests for new functionality\n- Follow existing code patterns\n- Use Serena for precise edits\n- In TEST phase: Run pytest AND lint (ruff check .) before reporting success\n\n## Output Format (REQUIRED)\n\n**Max length:** 1500 characters\n**Max file references:** 10\n**Max description per item:** 100 characters\n\n```markdown\n## Implementation: [Task]\n\n**Files Modified:** (max 10)\n- `path/file.ts:line` - what changed (max 100 chars)\n\n**Changes Made:**\n- Brief description per file\n\n**Tests Added:** (if applicable)\n- Test descriptions\n\n**Side Effects Checked:**\n- List of callers verified\n```\n\n**Enforcement:** Responses exceeding limits will be rejected\n",
        "agents/researcher.md": "# Researcher Agent\n\n**Model**: haiku\n\n**READ FIRST:** [CORE_PROTOCOL.md](../CORE_PROTOCOL.md) for tool selection, batch operations, and parallel execution rules.\n\n## Purpose\nResearch and documentation lookup. Used for:\n- Finding how to use libraries\n- Understanding API patterns\n- Gathering context for implementation\n\n## Behavior\n- Use Context7 for library docs (not WebSearch)\n- Use Serena for existing code patterns\n- Return references with key insights\n\n## Output Format (REQUIRED)\n\n**Max length:** 2000 characters\n\n```markdown\n## Research: [Topic]\n\n**Findings:**\n- Key insight with reference\n\n**Recommended Approach:**\n- Implementation guidance\n\n**Relevant Docs:**\n- Links to authoritative sources\n```\n\n**Enforcement:** Responses exceeding limits will be rejected\n",
        "agents/reviewer.md": "# Reviewer Agent\n\n**Model**: sonnet\n\n**READ FIRST:** [CORE_PROTOCOL.md](../CORE_PROTOCOL.md) for tool selection, batch operations, and parallel execution rules.\n\n## Purpose\nCode review and quality checking. Used for:\n- Reviewing changes before commit\n- Checking for common issues\n- Ensuring test coverage\n\n## Behavior\n- Check side-effects (find_referencing_symbols)\n- Verify tests exist for new code\n- Look for security issues\n- Ensure consistency with codebase patterns\n\n## Output Format (REQUIRED)\n\n**Max length:** 1500 characters\n\n```markdown\n## Review: [Changes]\n\n**Issues Found:** (if any)\n- Issue with severity and location\n\n**Suggestions:**\n- Improvement recommendations\n\n**Approved:** [YES/NO with reasons]\n```\n\n**Enforcement:** Responses exceeding limits will be rejected\n",
        "commands/implement.md": "---\ndescription: Start implementer workflow for general tasks (use when iterate is broken or unnecessary)\nargument-hint: [task description]\n---\n\nStart the implementer workflow for general implementation tasks.\n\n**Use this when:**\n- Iterate workflow is broken or unavailable\n- You need to make quick fixes without TDD ceremony\n- General implementation without specialized workflow\n\n## Initialize\n\n```bash\npython3 ~/.claude/plugins/agent-swarm/lib/implementer_workflow.py $ARGUMENTS\n```\n\n## Workflow\n\nSimple: WORK → VERIFY → DONE\n\nAll tools allowed in WORK phase. This is the escape hatch when iterate has issues.\n\n## Commands\n\n- `python3 lib/implementer_workflow.py status` - Check status\n- `python3 lib/implementer_workflow.py advance` - Move to next phase\n- `python3 lib/implementer_workflow.py stop` - Stop workflow\n",
        "commands/orchestrate.md": "---\ndescription: Start orchestrated workflow for complex tasks\nargument-hint: [task description]\n---\n\nUse the orchestrate skill to manage this task: $ARGUMENTS\n\nInitialize the workflow session and follow the orchestrator protocol:\n1. Classify the task complexity\n2. Set up phase tracking\n3. Proceed through phases with checkpoints as configured\n\nIf --autopilot flag is present, enable autopilot mode to bypass checkpoints.\n",
        "commands/spawn.md": "---\ndescription: Reference for spawning subagents correctly\nargument-hint: [agent-type]\n---\n\nUse the spawn skill to understand how to spawn subagents for: $ARGUMENTS\n\nFollow the subagent spawning protocol with correct model selection and prompt structure.\n",
        "examples/neural-net-viz/README.md": "# Neural Network Visualizer\n\nReal-time visualization of neural network training on the MNIST dataset. Watch weights, activations, and gradients update as your network learns to recognize handwritten digits.\n\n## Features\n\n- **Custom Neural Network**: Pure NumPy implementation with configurable architecture\n- **Real-time Visualization**: Canvas-based rendering of network structure with live weight and activation updates\n- **Interactive Controls**: Configure network architecture, training parameters, and control training\n- **MNIST Dataset**: Automatic download and preprocessing of MNIST handwritten digits\n- **WebSocket Updates**: Real-time training metrics and visualization via SocketIO\n- **Edge Sampling**: Intelligent sampling of top 500 weights per layer for large networks\n\n## Installation\n\n1. Install dependencies:\n```bash\npip install -r requirements.txt\n```\n\n## Usage\n\n1. Start the server:\n```bash\npython run.py\n```\n\n2. Open your browser to:\n```\nhttp://localhost:5000\n```\n\n3. Create a network:\n   - Enter hidden layer sizes (e.g., \"128, 64\")\n   - Click \"Create Network\"\n\n4. Start training:\n   - Adjust epochs and batch size\n   - Click \"Start Training\"\n   - Watch the network learn in real-time!\n\n## Architecture Overview\n\n### Backend (`backend/`)\n\n- **neural_net.py**: Custom neural network implementation\n  - `Layer` class with forward/backward passes\n  - `Network` class with training loop\n  - Gradient and activation caching for visualization\n\n- **data_loader.py**: MNIST dataset handling\n  - Automatic download via TensorFlow\n  - Preprocessing and normalization\n  - One-hot encoding\n\n- **trainer.py**: Training orchestration\n  - Batch processing\n  - Real-time metric calculation\n  - SocketIO event emission\n\n- **app.py**: Flask + SocketIO server\n  - REST API for network creation and training control\n  - WebSocket communication for real-time updates\n  - Background thread for training\n\n### Frontend\n\n- **templates/index.html**: Main UI structure\n- **static/css/style.css**: Dark theme styling with gradient accents\n- **static/js/network-viz.js**: Canvas-based network visualization\n  - Dynamic node positioning\n  - Weight-based edge rendering (thickness = magnitude, color = sign)\n  - Activation-based node coloring\n- **static/js/socket-client.js**: WebSocket client for real-time updates\n- **static/js/controls.js**: UI controls and API communication\n\n## Visualization Details\n\n- **Edges**:\n  - Thickness proportional to weight magnitude\n  - Red = positive weights, Blue = negative weights\n  - Opacity based on normalized magnitude\n\n- **Nodes**:\n  - Color intensity reflects activation value\n  - Brighter = higher activation\n\n- **Edge Sampling**:\n  - For layers with >500 connections, only top 500 highest magnitude weights are displayed\n  - Ensures smooth performance even with large networks\n\n## Training Metrics\n\nReal-time display of:\n- Current epoch and batch\n- Training loss (cross-entropy)\n- Training accuracy\n- Network structure visualization\n\n## Technical Stack\n\n- **Backend**: Flask, Flask-SocketIO, NumPy\n- **Frontend**: Vanilla JavaScript, Canvas API, Socket.IO\n- **ML**: Custom NumPy neural network (no PyTorch/TensorFlow for NN)\n- **Data**: TensorFlow (MNIST download only)\n\n## Network Configuration\n\n- **Input Layer**: 784 neurons (28×28 MNIST images)\n- **Hidden Layers**: Configurable (default: 128, 64)\n- **Output Layer**: 10 neurons (digits 0-9)\n- **Activation**: ReLU for hidden layers, Softmax for output\n- **Loss**: Cross-entropy\n- **Optimizer**: Gradient descent with fixed learning rate (0.01)\n\n## Performance\n\n- Real-time updates every 10 batches\n- Visualization runs at ~30 FPS\n- Training on CPU: ~10 seconds per epoch (60,000 samples)\n\n## Future Enhancements\n\n- Gradient flow animation (backward pass visualization)\n- Multiple optimization algorithms (Adam, RMSprop)\n- Convolutional layer support\n- Test set evaluation visualization\n- Export trained model\n",
        "hooks/.context/EPISODES.md": "# Episodes: hooks\n\n",
        "hooks/__init__.py": "# Hooks package\n",
        "hooks/auto-orchestrate-hook.sh": "#!/bin/bash\n# Auto-orchestrate hook for UserPromptSubmit\n# Detects complex tasks and suggests workflow activation\n\n# Read input from stdin\nINPUT=$(cat)\n\n# Extract the prompt\nPROMPT=$(echo \"$INPUT\" | jq -r '.prompt // .message // empty' 2>/dev/null)\n\n# For now, just allow - the CLAUDE.md classification system handles this\necho '{\"hookSpecificOutput\": {\"permissionDecision\": \"allow\"}}'\n",
        "hooks/background-enforcement.py": "#!/usr/bin/env python3\n\"\"\"PreToolUse hook to enforce run_in_background=true for Task tool.\"\"\"\n\nimport json\nimport sys\n\n\ndef main():\n    \"\"\"Enforce run_in_background=true for Task tool calls.\"\"\"\n    input_data = json.loads(sys.stdin.read())\n    tool_name = input_data.get(\"tool_name\", \"\")\n    tool_input = input_data.get(\"tool_input\", {})\n    \n    # Only check Task tool\n    if tool_name != \"Task\":\n        print(json.dumps({\n            \"hookSpecificOutput\": {\n                \"hookEventName\": \"PreToolUse\",\n                \"permissionDecision\": \"allow\"\n            }\n        }))\n        return\n    \n    # Check run_in_background parameter\n    run_in_background = tool_input.get(\"run_in_background\", False)\n    \n    if not run_in_background:\n        print(json.dumps({\n            \"hookSpecificOutput\": {\n                \"hookEventName\": \"PreToolUse\",\n                \"permissionDecision\": \"deny\",\n                \"permissionDecisionReason\": (\n                    \"[BACKGROUND_REQUIRED] Task tool must use run_in_background=true \"\n                    \"for parallel execution. Add run_in_background=true to your Task call.\"\n                )\n            }\n        }))\n        return\n    \n    print(json.dumps({\n        \"hookSpecificOutput\": {\n            \"hookEventName\": \"PreToolUse\",\n            \"permissionDecision\": \"allow\"\n        }\n    }))\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "hooks/base-enforcement.py": "#!/usr/bin/env python3\n\"\"\"Base enforcement hook - blocks editing tools when no workflow is active.\n\nThis is the foundational enforcement that ensures Edit/Write/NotebookEdit\nare blocked unless a workflow (/iterate, /orchestrate) is running.\n\nEach workflow has its own enforcement hook for phase-specific rules.\nThis hook only handles the \"No workflow = no editing\" rule.\n\"\"\"\n\nimport sys\nimport json\nfrom pathlib import Path\n\n# Add lib to path for workflow_client\nlib_dir = Path(__file__).parent.parent / \"lib\"\nsys.path.insert(0, str(lib_dir))\n\ntry:\n    from workflow_client import workflow_is_active\nexcept ImportError:\n    # If workflow_client not available, check if router is running\n    # If not, allow everything (fail-open during bootstrap)\n    def workflow_is_active(workflow_id: str) -> bool:\n        return False\n\n# Tools that require an active workflow (normalized names - no mcp__router__ prefix)\nEDITING_TOOLS = {\n    # Native editing tools\n    \"Edit\", \"Write\", \"NotebookEdit\",\n    # MCP native tools\n    \"native__write_file\",\n    \"native__edit_file\",\n    # Serena text manipulation tools\n    \"serena__create_text_file\",\n    \"serena__replace_content\",\n    \"serena__replace_symbol_body\",\n    \"serena__insert_after_symbol\",\n    \"serena__insert_before_symbol\",\n    # Legacy MCP plugin format (with mcp__plugin prefix)\n    \"mcp__plugin_serena_serena__create_text_file\",\n    \"mcp__plugin_serena_serena__replace_content\",\n    \"mcp__plugin_serena_serena__replace_symbol_body\",\n    \"mcp__plugin_serena_serena__insert_after_symbol\",\n    \"mcp__plugin_serena_serena__insert_before_symbol\",\n}\n\n\ndef is_any_workflow_active() -> bool:\n    \"\"\"Check if any workflow is currently active via state server.\"\"\"\n    # Check known workflows via workflow_client\n    # workflow_client handles connection errors gracefully (returns False)\n    for workflow_id in (\"iterate\", \"orchestrate\", \"pr_comment\", \"debug\", \"implementer\"):\n        if workflow_is_active(workflow_id):\n            return True\n    return False\n\n\ndef allow(reason: str = \"\") -> dict:\n    \"\"\"Return allow decision.\"\"\"\n    result = {\n        \"hookSpecificOutput\": {\n            \"hookEventName\": \"PreToolUse\",\n            \"permissionDecision\": \"allow\"\n        }\n    }\n    if reason:\n        result[\"hookSpecificOutput\"][\"permissionDecisionReason\"] = reason\n    return result\n\n\ndef block(reason: str):\n    \"\"\"Block tool by exiting with code 2 (stderr used as message).\n\n    Exit code 2 is the reliable blocking mechanism - JSON deny responses\n    are sometimes ignored (Claude Code bug #4669).\n    \"\"\"\n    print(reason, file=sys.stderr)\n    sys.exit(2)\n\n\ndef main():\n    \"\"\"Main enforcement logic.\"\"\"\n    try:\n        input_data = json.loads(sys.stdin.read())\n    except json.JSONDecodeError:\n        print(json.dumps(allow()))\n        return\n\n    tool_name = input_data.get(\"tool_name\", \"\")\n\n    # Normalize MCP router prefix (mcp__router__native__bash -> native__bash)\n    if tool_name.startswith(\"mcp__router__\"):\n        tool_name = tool_name[len(\"mcp__router__\"):]\n\n    # Only check editing tools\n    if tool_name not in EDITING_TOOLS:\n        print(json.dumps(allow()))\n        return\n\n    # Block editing tools if no workflow is active\n    if not is_any_workflow_active():\n        block(\n            f\"[NO WORKFLOW] {tool_name} blocked. \"\n            f\"Start /iterate or /orchestrate to edit files.\"\n        )\n\n    # Workflow is active - allow (workflow-specific hooks handle phase rules)\n    print(json.dumps(allow(\"Workflow active\")))\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "hooks/combined-enforcement.py.backup": "#!/usr/bin/env python3\n\"\"\"\nCombined enforcement hook for agent-swarm plugin.\n\nEnforces:\n1. Phase restrictions - tools allowed per phase\n2. Subagent requirements - implement phase requires subagents\n3. Token efficiency - blocks excessive direct tool use\n4. Scope discipline - prevents off-task exploration\n5. Autopilot mode - auto-approves when enabled\n\"\"\"\n\nimport sys\nimport json\nimport re\nfrom datetime import datetime\nfrom pathlib import Path\n\n# Configuration\nSTATE_FILE = Path.home() / \".claude/plugins/agent-swarm/.state/session.json\"\nCONFIG_FILE = Path.home() / \".claude/plugins/agent-swarm/config/workflow.json\"\nLOG_FILE = Path.home() / \".claude/plugins/agent-swarm/.state/activity.log\"\nSTATS_FILE = Path.home() / \".claude/plugins/agent-swarm/.state/stats.json\"\n\ndef log_event(event_type: str, details: str):\n    \"\"\"Append event to activity log.\"\"\"\n    LOG_FILE.parent.mkdir(parents=True, exist_ok=True)\n    timestamp = datetime.now().strftime(\"%H:%M:%S\")\n    with open(LOG_FILE, \"a\") as f:\n        f.write(f\"[{timestamp}] {event_type}: {details}\\n\")\n\ndef update_stats(allowed: bool, reason: str = None, tool_name: str = None):\n    \"\"\"Update usage statistics.\"\"\"\n    stats = {}\n    if STATS_FILE.exists():\n        try:\n            stats = json.loads(STATS_FILE.read_text())\n        except:\n            pass\n\n    if allowed:\n        stats[\"tools_allowed\"] = stats.get(\"tools_allowed\", 0) + 1\n    else:\n        stats[\"tools_blocked\"] = stats.get(\"tools_blocked\", 0) + 1\n        if reason:\n            blocks = stats.get(\"blocks_by_reason\", {})\n            # Extract first word as category\n            category = reason.split(\"]\")[0].replace(\"[\", \"\") if \"]\" in reason else \"other\"\n            blocks[category] = blocks.get(category, 0) + 1\n            stats[\"blocks_by_reason\"] = blocks\n\n    if tool_name == \"Task\":\n        stats[\"subagents_spawned\"] = stats.get(\"subagents_spawned\", 0) + 1\n\n    STATS_FILE.parent.mkdir(parents=True, exist_ok=True)\n    STATS_FILE.write_text(json.dumps(stats, indent=2))\n\n# Tool categories\nWRITE_TOOLS = {\"Edit\", \"Write\", \"NotebookEdit\"}\nSEARCH_TOOLS = {\"Glob\", \"Grep\"}  # Read has its own counter\nRESEARCH_TOOLS = {\"WebSearch\", \"WebFetch\"}\n\n# Model enforcement for subagent spawning\nAGENT_MODEL_MAP = {\n    # Built-in agent types\n    \"Explore\": \"haiku\",\n    \"Plan\": \"sonnet\",\n    \"general-purpose\": \"sonnet\",\n    # agent-swarm specific agents\n    \"agent-swarm:explorer\": \"haiku\",\n    \"agent-swarm:researcher\": \"haiku\",\n    \"agent-swarm:git-agent\": \"haiku\",\n    \"agent-swarm:architect\": \"sonnet\",\n    \"agent-swarm:implementer\": \"sonnet\",\n    \"agent-swarm:reviewer\": \"sonnet\",\n    \"agent-swarm:debugger\": \"sonnet\",\n}\nSUBAGENT_TOOLS = {\"Task\"}\nGIT_TOOLS = {\"Bash\"}  # git commands via bash\n\n# Phase restrictions\nPHASE_ALLOWED_TOOLS = {\n    \"intake\": {\"Read\", \"Glob\", \"Grep\", \"AskUserQuestion\"},\n    \"research\": {\"WebSearch\", \"WebFetch\", \"Read\", \"Task\"},\n    \"explore\": {\"Glob\", \"Grep\", \"Read\", \"Task\"},\n    \"design\": {\"Read\", \"Glob\", \"Grep\", \"Task\", \"AskUserQuestion\"},\n    \"implement\": {\"Task\", \"Read\"},  # Write only via subagent\n    \"review\": {\"Read\", \"Glob\", \"Grep\", \"Bash\", \"Task\"},\n    \"debug\": {\"Read\", \"Glob\", \"Grep\", \"Bash\", \"Edit\", \"Write\", \"Task\"},\n    \"git\": {\"Bash\", \"Read\"},\n    \"\": set(),  # No phase = no restrictions\n}\n\n# Tools always allowed regardless of phase\nALWAYS_ALLOWED = {\"TodoWrite\", \"AskUserQuestion\"}\n\n# Thresholds\nMAX_DIRECT_SEARCHES = 2  # After this, must use scripts\nMAX_FILE_READS = 2  # After this, must use subagent\n\n# MCP tools allowed without script (low-cost single operations)\nMCP_DIRECT_ALLOWED = {\n    \"mcp__plugin_serena_serena__find_symbol\",\n    \"mcp__plugin_serena_serena__get_definition\",\n    \"mcp__plugin_serena_serena__get_symbols_overview\",\n    \"mcp__plugin_serena_serena__search_for_pattern\",  # Smart tool for code understanding\n    \"mcp__context7__resolve-library-id\",\n    \"mcp__context7__query-docs\",\n    \"mcp__memory__read_memory\",\n    \"mcp__memory__search_nodes\",\n    \"mcp__filesystem__read_text_file\",\n}\n\n# MCP tools that require script (high-cost/batch operations)\nMCP_SCRIPT_REQUIRED = {\n    \"mcp__plugin_serena_serena__list_dir\",  # recursive can be huge\n    \"mcp__plugin_serena_serena__search_for_pattern\",\n    \"mcp__plugin_serena_serena__find_referencing_symbols\",\n    \"mcp__filesystem__directory_tree\",\n    \"mcp__filesystem__search_files\",\n}\n\ndef load_json(path: Path) -> dict:\n    \"\"\"Load JSON file safely.\"\"\"\n    if path.exists():\n        try:\n            return json.loads(path.read_text())\n        except (json.JSONDecodeError, IOError):\n            pass\n    return {}\n\ndef save_state(state: dict) -> None:\n    \"\"\"Save session state.\"\"\"\n    STATE_FILE.parent.mkdir(parents=True, exist_ok=True)\n    STATE_FILE.write_text(json.dumps(state, indent=2))\n\ndef allow(reason: str = None) -> dict:\n    \"\"\"Return allow decision.\"\"\"\n    result = {\"hookSpecificOutput\": {\"hookEventName\": \"PreToolUse\", \"permissionDecision\": \"allow\"}}\n    if reason:\n        result[\"hookSpecificOutput\"][\"permissionDecisionReason\"] = reason\n    return result\n\ndef block(reason: str) -> dict:\n    \"\"\"Return block decision.\"\"\"\n    return {\n        \"hookSpecificOutput\": {\n            \"hookEventName\": \"PreToolUse\",\n            \"permissionDecision\": \"deny\",\n            \"permissionDecisionReason\": reason\n        }\n    }\n\ndef check_autopilot(state: dict) -> dict | None:\n    \"\"\"Autopilot mode bypasses all enforcement.\"\"\"\n    if state.get(\"autopilot_override\", False):\n        return allow(\"[AUTOPILOT] Auto-approved\")\n    return None\n\ndef check_phase_restrictions(tool_name: str, state: dict, tool_input: dict = None) -> dict | None:\n    \"\"\"Enforce phase-specific tool restrictions.\"\"\"\n    phase = state.get(\"phase\", \"\")\n\n    # No phase = no restrictions\n    if not phase:\n        return None\n\n    # Always allowed tools\n    if tool_name in ALWAYS_ALLOWED:\n        return None\n\n    # Special case: Bash in intake phase (Python execution only)\n    if phase == \"intake\" and tool_name == \"Bash\" and tool_input:\n        command = tool_input.get(\"command\", \"\").strip()\n\n        # Allow orchestrator phase-transition commands\n        if 'AGENT_PHASE=' in command or '/tmp/phase_' in command:\n            return None  # Allow\n\n        # Allow Python execution patterns\n        python_patterns = [\n            command.startswith(\"python3 -c\"),\n            command.startswith(\"python3 <<\"),\n            \"cat >\" in command and \".py\" in command,  # Creating temp Python script\n            command.startswith(\"python3 /tmp/\") and \".py\" in command,  # Running temp script\n            command.startswith(\"rm /tmp/\") and \".py\" in command,  # Cleanup\n        ]\n\n        if any(python_patterns):\n            return None  # Allow Python-related Bash\n\n        # Block all other Bash commands in intake\n        return block(\n            f\"[PHASE: intake] Bash restricted to Python execution only.\\n\"\n            f\"Allowed patterns:\\n\"\n            f\"  - python3 -c \\\"...\\\"\\n\"\n            f\"  - cat > /tmp/script.py << 'EOF'\\n\"\n            f\"  - python3 /tmp/script.py\\n\"\n            f\"  - rm /tmp/*.py\\n\"\n            f\"For other operations, use allowed tools: Read, Glob, Grep, AskUserQuestion\"\n        )\n\n    # Check phase restrictions\n    allowed_tools = PHASE_ALLOWED_TOOLS.get(phase, set())\n\n    # During implement phase, write tools require subagent\n    if phase == \"implement\" and tool_name in WRITE_TOOLS:\n        return block(\n            f\"[PHASE: {phase}] {tool_name} blocked. \"\n            f\"Use Task tool to spawn implementer subagent. \"\n            f\"Direct edits bypass review and context management.\"\n        )\n\n    # Strict phase enforcement (default True, can disable in config)\n    config = load_json(CONFIG_FILE)\n    if config.get(\"strict_phase_enforcement\", True):\n        if tool_name not in allowed_tools and tool_name not in ALWAYS_ALLOWED:\n            return block(\n                f\"[PHASE: {phase}] {tool_name} not allowed in this phase. \"\n                f\"Allowed: {', '.join(allowed_tools)}\"\n            )\n\n    return None\n\ndef check_token_efficiency(tool_name: str, tool_input: dict, state: dict) -> dict | None:\n    \"\"\"Enforce token-saving measures.\"\"\"\n    from datetime import datetime, timedelta\n\n    # Detect phase changes and reset counters\n    current_phase = state.get(\"phase\", \"\")\n    last_phase = state.get(\"last_phase\", \"\")\n    \n    if current_phase != last_phase and last_phase:\n        # Phase changed, reset counters\n        state[\"search_count\"] = 0\n        state[\"read_count\"] = 0\n        state[\"files_read\"] = []\n        state[\"last_phase\"] = current_phase\n        log_event(\"COUNTER_RESET\", f\"Phase changed from '{last_phase}' to '{current_phase}', counters reset\")\n        save_state(state)\n    elif not last_phase:\n        # Initialize last_phase tracking\n        state[\"last_phase\"] = current_phase\n        save_state(state)\n\n    # Detect new conversation and reset counters\n    # If more than 30 minutes since last tool use, consider it a new conversation\n    last_tool_time = state.get(\"last_tool_time\")\n    current_time = datetime.now().isoformat()\n\n    if last_tool_time:\n        try:\n            last_time = datetime.fromisoformat(last_tool_time)\n            time_since_last = datetime.now() - last_time\n\n            # Reset counters if been idle for 30+ minutes (new conversation)\n            if time_since_last > timedelta(minutes=30):\n                state[\"search_count\"] = 0\n                state[\"read_count\"] = 0\n                state[\"files_read\"] = []\n                log_event(\"COUNTER_RESET\", \"New conversation detected, counters reset\")\n        except (ValueError, TypeError):\n            pass  # Invalid timestamp, ignore\n\n    # Update last tool time\n    state[\"last_tool_time\"] = current_time\n    save_state(state)\n\n    # Track search tool usage\n    if tool_name in SEARCH_TOOLS:\n        count = state.get(\"search_count\", 0) + 1\n        state[\"search_count\"] = count\n        save_state(state)\n\n        if count > MAX_DIRECT_SEARCHES:\n            return block(\n                f\"[TOKEN EFFICIENCY] {count} direct searches used. \"\n                f\"Use a batch script instead:\\n\"\n                f\"```python\\n\"\n                f\"from mcp_bridge import native_glob, native_grep\\n\"\n                f\"# Batch your searches\\n\"\n                f\"```\\n\"\n                f\"Or spawn an Explorer subagent with Task tool.\"\n            )\n\n    # Track file reads and detect duplicates\n    if tool_name == \"Read\":\n        file_path = tool_input.get(\"file_path\", \"\")\n\n        # Track which files have been read\n        files_read = state.get(\"files_read\", [])\n\n        # Check for duplicate\n        if file_path in files_read:\n            return block(\n                f\"[DUPLICATE READ] File already read in this session:\\n\"\n                f\"  {file_path}\\n\\n\"\n                f\"Reading the same file multiple times wastes tokens.\\n\"\n                f\"If you need to re-check: review conversation history.\\n\"\n                f\"If content changed: explain why re-reading is necessary.\"\n            )\n\n        # Track this file\n        files_read.append(file_path)\n        state[\"files_read\"] = files_read\n\n        count = state.get(\"read_count\", 0) + 1\n        state[\"read_count\"] = count\n        save_state(state)\n\n        if count > MAX_FILE_READS:\n            return block(\n                f\"[TOKEN EFFICIENCY] {count} direct file reads. \"\n                f\"Spawn an Explorer subagent to aggregate findings, \"\n                f\"or use a script to read and summarize.\"\n            )\n\n    return None\n\ndef check_scope_discipline(tool_name: str, tool_input: dict, state: dict) -> dict | None:\n    \"\"\"Prevent off-task exploration.\"\"\"\n    phase = state.get(\"phase\", \"\")\n    task_summary = state.get(\"task_summary\", \"\")\n\n    # Only enforce during active phases\n    if not phase or phase in (\"intake\", \"research\", \"explore\"):\n        return None\n\n    # Check if spawning subagent without clear purpose\n    if tool_name == \"Task\":\n        prompt = tool_input.get(\"prompt\", \"\")\n        if len(prompt) < 20:\n            return block(\n                f\"[SCOPE] Subagent prompt too vague. \"\n                f\"Provide clear, specific instructions for the subagent.\"\n            )\n\n    return None\n\ndef check_mcp_script_requirement(tool_name: str, tool_input: dict, state: dict) -> dict | None:\n    \"\"\"Enforce script usage for high-cost MCP operations.\"\"\"\n\n    # Check if this is an MCP tool\n    if not tool_name.startswith(\"mcp__\"):\n        return None\n\n    # Allow low-cost operations directly\n    if tool_name in MCP_DIRECT_ALLOWED:\n        return None\n\n    # Block high-cost operations - require script\n    if tool_name in MCP_SCRIPT_REQUIRED:\n        return block(\n            f\"[MCP SCRIPT] {tool_name} requires batch script.\\n\"\n            f\"This operation can return large results. Use:\\n\"\n            f\"```python\\n\"\n            f\"from mcp_bridge import call_mcp\\n\"\n            f\"result = call_mcp('{tool_name}', {tool_input})\\n\"\n            f\"# Process and summarize result\\n\"\n            f\"```\"\n        )\n\n    # Track repeated MCP calls of same type\n    mcp_counts = state.get(\"mcp_counts\", {})\n    count = mcp_counts.get(tool_name, 0) + 1\n    mcp_counts[tool_name] = count\n    state[\"mcp_counts\"] = mcp_counts\n    save_state(state)\n\n    # Block after 2nd call of same MCP tool\n    if count > 2:\n        return block(\n            f\"[MCP SCRIPT] {tool_name} called {count} times.\\n\"\n            f\"Batch repeated calls into a script.\"\n        )\n\n    return None\n\ndef check_smart_tool_usage(tool_name: str, tool_input: dict, state: dict) -> dict | None:\n    \"\"\"Block dumb methods when smarter alternatives exist.\"\"\"\n\n    # WebSearch for library docs → use Context7\n    if tool_name == \"WebSearch\":\n        query = tool_input.get(\"query\", \"\").lower()\n        doc_indicators = [\"docs\", \"documentation\", \"api\", \"how to\", \"example\",\n                          \"tutorial\", \"guide\", \"reference\", \"usage\"]\n\n        # Common libraries that are definitely in Context7\n        known_libs = [\"react\", \"next\", \"vue\", \"angular\", \"svelte\", \"express\",\n                      \"fastapi\", \"django\", \"flask\", \"prisma\", \"drizzle\",\n                      \"tailwind\", \"typescript\", \"node\", \"deno\", \"bun\"]\n\n        if any(ind in query for ind in doc_indicators):\n            if any(lib in query for lib in known_libs):\n                return block(\n                    f\"[SMART TOOLS] Use Context7 instead of WebSearch for docs:\\n\"\n                    f\"  1. mcp__context7__resolve-library-id\\n\"\n                    f\"  2. mcp__context7__query-docs\\n\"\n                    f\"Context7 has curated, up-to-date docs. WebSearch wastes tokens on noise.\"\n                )\n\n    # Read for code understanding → use Serena\n    if tool_name == \"Read\":\n        file_path = tool_input.get(\"file_path\", \"\")\n        # Code file extensions\n        code_exts = [\".py\", \".ts\", \".js\", \".tsx\", \".jsx\", \".go\", \".rs\", \".java\", \".rb\"]\n\n        if any(file_path.endswith(ext) for ext in code_exts):\n            # Check if this looks like exploration vs targeted read\n            phase = state.get(\"phase\", \"\")\n            read_count = state.get(\"read_count\", 0)\n\n            # First read is usually OK, but suggest Serena after that\n            if read_count >= 2:\n                return block(\n                    f\"[SMART TOOLS] Use Serena instead of Read for code understanding:\\n\"\n                    f\"  mcp__plugin_serena_serena__find_symbol - locate definitions\\n\"\n                    f\"  mcp__plugin_serena_serena__get_definition - get signature + docs\\n\"\n                    f\"  mcp__plugin_serena_serena__find_references - find usages\\n\"\n                    f\"Serena extracts structure. Read dumps entire files into context.\"\n                )\n\n    # Bash for git → suggest gh_wrapper for queries\n    if tool_name == \"Bash\":\n        cmd = tool_input.get(\"command\", \"\")\n\n        # Exempt orchestrator system commands\n        if 'AGENT_PHASE=' in cmd or '/tmp/phase_' in cmd:\n            return None  # Allow\n\n        # CRITICAL: Detect Bash abuse patterns (cat/grep/find)\n        # These should NEVER be done via Bash - proper tools exist\n\n        # cat abuse → use Read or Write tools\n        # Block cat UNLESS it's receiving piped input (e.g., grep | cat)\n        if 'cat' in cmd and not re.search(r'\\|\\s*cat\\s*(?:[|;]|$)', cmd):\n            # Cat reading files\n            if re.search(r'\\bcat\\s+[^\\|<>]', cmd):\n                return block(\n                    f\"[BASH ABUSE] Don't use 'cat' for reading - use Read tool instead\\n\"\n                    f\"❌ Bash: {cmd[:60]}\\n\"\n                    f\"✅ Read: {{'file_path': '<path>'}}\\n\"\n                    f\"Bash cat wastes tokens and bypasses tracking.\"\n                )\n            # Cat with heredocs (writing)\n            if re.search(r'\\bcat\\s*[>]+.*<<|\\bcat\\s*<<', cmd):\n                return block(\n                    f\"[BASH ABUSE] Don't use 'cat' for writing - use Write tool instead\\n\"\n                    f\"❌ Bash: {cmd[:60]}\\n\"\n                    f\"✅ Write: {{'file_path': '<path>', 'content': '...'}}\\n\"\n                    f\"Bash cat wastes tokens and bypasses tracking.\"\n                )\n\n        # grep/rg abuse → use Grep (powered by ripgrep)\n        if re.search(r'\\b(grep|rg|egrep|fgrep)\\s+', cmd):\n            return block(\n                f\"[BASH ABUSE] Don't use grep/rg via Bash - use Grep tool instead\\n\"\n                f\"❌ Bash: {cmd[:60]}\\n\"\n                f\"✅ Grep: {{'pattern': '<regex>', 'path': '.', 'output_mode': 'files_with_matches'}}\\n\"\n                f\"The Grep tool is powered by ripgrep (rg) and has proper output formatting.\"\n            )\n\n        # find abuse → use Glob\n        if re.search(r'\\bfind\\s+', cmd):\n            return block(\n                f\"[BASH ABUSE] Don't use 'find' - use Glob tool instead\\n\"\n                f\"❌ Bash: {cmd[:60]}\\n\"\n                f\"✅ Glob: {{'pattern': '**/*.ext', 'path': '.'}}\\n\"\n                f\"Glob is faster and integrates with tracking.\"\n            )\n\n        # sed/awk for file editing → use Edit\n        if re.search(r'\\b(sed|awk)\\s+', cmd) and not re.search(r'\\|', cmd):\n            return block(\n                f\"[BASH ABUSE] Don't use sed/awk for file editing - use Edit tool\\n\"\n                f\"❌ Bash: {cmd[:60]}\\n\"\n                f\"✅ Edit: {{'file_path': '<path>', 'old_string': '...', 'new_string': '...'}}\\n\"\n                f\"Edit tool is atomic and tracked.\"\n            )\n\n        if cmd.startswith(\"gh \") and not any(x in cmd for x in [\"create\", \"merge\", \"close\", \"edit\"]):\n            # Query commands, not mutating commands\n            if any(x in cmd for x in [\"list\", \"view\", \"status\", \"search\"]):\n                return block(\n                    f\"[SMART TOOLS] Use gh_wrapper.py for summarized output:\\n\"\n                    f\"  python3 ~/.claude/plugins/agent-swarm/scripts/gh_wrapper.py {cmd[3:]}\\n\"\n                    f\"Raw gh output floods context. Wrapper extracts key info only.\"\n                )\n\n    return None\n\ndef check_checkpoint_approval(tool_name: str, tool_input: dict, state: dict) -> dict | None:\n    \"\"\"Enforce checkpoint approval requirement before critical operations.\"\"\"\n    phase = state.get(\"phase\", \"\")\n    if not phase:\n        return None\n    \n    # Load config to check if checkpoint enabled for this phase\n    config = load_json(CONFIG_FILE)\n    checkpoints = config.get(\"checkpoints\", {})\n    \n    if not checkpoints.get(phase, False):\n        return None  # No checkpoint required for this phase\n    \n    # Check if approval has been granted for this phase\n    checkpoint_approvals = state.get(\"checkpoint_approvals\", {})\n    if checkpoint_approvals.get(phase, False):\n        return None  # Approval already granted\n    \n    # Block critical operations that require checkpoint approval\n    if tool_name == \"Bash\":\n        command = tool_input.get(\"command\", \"\")\n        \n        # Block git push operations\n        if re.search(r'\\bgit\\s+push\\b', command):\n            return block(\n                f\"[CHECKPOINT: {phase}] Git push requires approval\\n\"\n                f\"This phase has checkpoint enabled. Get user approval before pushing.\\n\"\n                f\"To approve: Add 'checkpoint_approvals': {{'{phase}': true}} to state\"\n            )\n        \n        # Block git commit operations\n        if re.search(r'\\bgit\\s+commit\\b', command):\n            return block(\n                f\"[CHECKPOINT: {phase}] Git commit requires approval\\n\"\n                f\"This phase has checkpoint enabled. Get user approval before committing.\\n\"\n                f\"To approve: Add 'checkpoint_approvals': {{'{phase}': true}} to state\"\n            )\n    \n    # Block phase transitions\n    if tool_name == \"Bash\" and (\"AGENT_PHASE=\" in tool_input.get(\"command\", \"\") or \n                                 \"/tmp/phase_\" in tool_input.get(\"command\", \"\")):\n        return block(\n            f\"[CHECKPOINT: {phase}] Phase transition requires approval\\n\"\n            f\"Complete checkpoint for '{phase}' phase before transitioning.\\n\"\n            f\"To approve: Add 'checkpoint_approvals': {{'{phase}': true}} to state\"\n        )\n    \n    return None\n\ndef check_git_safety(tool_name: str, tool_input: dict, state: dict) -> dict | None:\n    \"\"\"Prevent dangerous git operations.\"\"\"\n    if tool_name != \"Bash\":\n        return None\n\n    command = tool_input.get(\"command\", \"\")\n\n    # Dangerous patterns\n    dangerous = [\n        \"git push --force\",\n        \"git push -f\",\n        \"git reset --hard\",\n        \"git clean -fd\",\n        \"git checkout .\",  # Discards all changes\n    ]\n\n    for pattern in dangerous:\n        if pattern in command:\n            return block(\n                f\"[GIT SAFETY] Dangerous command blocked: {pattern}\\n\"\n                f\"This operation is destructive. Use explicit approval.\"\n            )\n\n    # Warn about amending\n    if \"git commit --amend\" in command:\n        phase = state.get(\"phase\", \"\")\n        if phase != \"git\":\n            return block(\n                f\"[GIT SAFETY] Amend outside git phase. \"\n                f\"Switch to git phase first, or get explicit approval.\"\n            )\n\n    return None\n\n\n\ndef check_subagent_model(tool_name: str, tool_input: dict, state: dict) -> dict | None:\n    \"\"\"Enforce correct model usage when spawning subagents.\"\"\"\n    if tool_name != \"Task\":\n        return None\n    \n    subagent_type = tool_input.get(\"subagent_type\", \"\")\n    specified_model = tool_input.get(\"model\", \"\")\n    \n    # Skip if not an agent type we track\n    if subagent_type not in AGENT_MODEL_MAP:\n        return None\n    \n    expected_model = AGENT_MODEL_MAP[subagent_type]\n    \n    # If no model specified, warn and suggest\n    if not specified_model:\n        return block(\n            f\"[MODEL] Task missing 'model' parameter.\\n\"\n            f\"  For {subagent_type}, use: model: \\\"{expected_model}\\\"\"\n        )\n    \n    # If wrong model, block with correction\n    if specified_model != expected_model:\n        # Allow downgrade (sonnet agent using haiku for simple task)\n        if expected_model == \"sonnet\" and specified_model == \"haiku\":\n            return None  # Downgrade is OK\n        \n        # Block upgrade or wrong model\n        return block(\n            f\"[MODEL] Wrong model for {subagent_type}.\\n\"\n            f\"  Expected: {expected_model}, got: {specified_model}\\n\"\n            f\"  Downgrades OK (sonnet->haiku), upgrades blocked.\"\n        )\n    \n    return None\n\ndef main():\n    # Read input from stdin\n    try:\n        input_data = json.loads(sys.stdin.read())\n    except json.JSONDecodeError:\n        print(json.dumps(allow()))\n        return\n\n    tool_name = input_data.get(\"tool_name\", \"\")\n    tool_input = input_data.get(\"tool_input\", {})\n\n    # Load session state\n    state = load_json(STATE_FILE)\n\n    # Check autopilot first\n    result = check_autopilot(state)\n    if result:\n        print(json.dumps(result))\n        return\n\n    # Run all enforcement checks\n    checks = [\n        check_phase_restrictions(tool_name, state, tool_input),\n        check_checkpoint_approval(tool_name, tool_input, state),\n        check_mcp_script_requirement(tool_name, tool_input, state),\n        check_smart_tool_usage(tool_name, tool_input, state),\n        check_token_efficiency(tool_name, tool_input, state),\n        check_scope_discipline(tool_name, tool_input, state),\n        check_git_safety(tool_name, tool_input, state),\n        check_subagent_model(tool_name, tool_input, state),\n    ]\n\n    for result in checks:\n        if result:\n            # Log the block\n            msg = result.get(\"hookSpecificOutput\", {}).get(\"permissionDecisionReason\", \"blocked\")\n            log_event(\"BLOCKED\", f\"{tool_name}: {msg[:50]}\")\n            update_stats(allowed=False, reason=msg, tool_name=tool_name)\n            print(json.dumps(result))\n            return\n\n    # Default: allow\n    log_event(\"ALLOWED\", tool_name)\n    update_stats(allowed=True, tool_name=tool_name)\n    print(json.dumps(allow()))\n\nif __name__ == \"__main__\":\n    main()\n",
        "hooks/combined_enforcement.py": "combined-enforcement.py",
        "hooks/context-injection.py": "#!/usr/bin/env python3\n\"\"\"\nContext Injection CLI for Subagent Briefing Hook\n\nCalled by inject-subagent-briefing.sh to fetch agent-specific context.\nThis bridge script allows the bash hook to access Python context resolution.\n\"\"\"\n\nimport sys\nfrom pathlib import Path\n\n# Add context module to path\nsys.path.insert(0, str(Path(__file__).parent.parent))\n\nfrom context.resolver import get_agent_context  # noqa: E402\n\n\ndef main():\n    if len(sys.argv) < 4:\n        print(\"Usage: context-injection.py inject <agent_type> <working_dir>\", file=sys.stderr)\n        sys.exit(1)\n\n    command = sys.argv[1]\n    if command != \"inject\":\n        print(f\"Unknown command: {command}\", file=sys.stderr)\n        sys.exit(1)\n\n    agent_type = sys.argv[2]\n    working_dir = Path(sys.argv[3]).resolve()\n\n    try:\n        context_md = get_agent_context(agent_type, working_dir)\n        if context_md:\n            print(\"# Hierarchical Context\")\n            print()\n            print(context_md)\n        sys.exit(0)\n    except Exception as e:\n        print(f\"Error: {e}\", file=sys.stderr)\n        sys.exit(1)\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "hooks/enforce-protocol.sh": "#!/bin/bash\n# General protocol enforcement hook\n# Validates tool usage against CLAUDE.md guidelines\n\n# Read input from stdin\nINPUT=$(cat)\n\n# Extract tool name\nTOOL_NAME=$(echo \"$INPUT\" | jq -r '.tool_name // empty' 2>/dev/null)\n\n# Allow by default - specific enforcement is in combined-enforcement.py\necho '{\"hookSpecificOutput\": {\"permissionDecision\": \"allow\"}}'\n",
        "hooks/hook_logging.py": "\"\"\"\nShared logging module for agent-swarm hooks.\n\nProvides consistent logging across all hook files with:\n- File-based logging to .state/hooks.log\n- Configurable log levels via LOG_LEVEL env var\n- Structured log format with timestamps and context\n- Graceful degradation if log file isn't writable\n\"\"\"\n\nimport logging\nimport os\nimport sys\nfrom pathlib import Path\nfrom typing import Optional\n\n# Log file location (alongside other state files)\nLOG_DIR = Path.home() / \".claude/plugins/agent-swarm/.state\"\nLOG_FILE = LOG_DIR / \"hooks.log\"\n\n# Default log level, overridable via environment\nDEFAULT_LOG_LEVEL = \"INFO\"\n\n# Module-level logger instance\n_logger: Optional[logging.Logger] = None\n\n\ndef get_logger(name: str = \"agent-swarm.hooks\") -> logging.Logger:\n    \"\"\"\n    Get or create a configured logger instance.\n\n    Args:\n        name: Logger name (typically module name)\n\n    Returns:\n        Configured logger instance\n    \"\"\"\n    global _logger\n\n    if _logger is not None:\n        return _logger\n\n    _logger = logging.getLogger(name)\n\n    # Get log level from environment\n    level_name = os.environ.get(\"AGENT_SWARM_LOG_LEVEL\", DEFAULT_LOG_LEVEL).upper()\n    level = getattr(logging, level_name, logging.INFO)\n    _logger.setLevel(level)\n\n    # Avoid duplicate handlers if called multiple times\n    if _logger.handlers:\n        return _logger\n\n    # Create formatter\n    formatter = logging.Formatter(\n        fmt=\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\",\n        datefmt=\"%Y-%m-%d %H:%M:%S\"\n    )\n\n    # Try to add file handler\n    try:\n        LOG_DIR.mkdir(parents=True, exist_ok=True)\n        file_handler = logging.FileHandler(LOG_FILE, encoding=\"utf-8\")\n        file_handler.setFormatter(formatter)\n        file_handler.setLevel(level)\n        _logger.addHandler(file_handler)\n    except (OSError, IOError) as e:\n        # Can't write to log file - fall back to stderr only\n        sys.stderr.write(f\"[hook_logging] Cannot create log file: {e}\\n\")\n\n    # Always add stderr handler for ERROR and above\n    stderr_handler = logging.StreamHandler(sys.stderr)\n    stderr_handler.setFormatter(formatter)\n    stderr_handler.setLevel(logging.ERROR)\n    _logger.addHandler(stderr_handler)\n\n    return _logger\n\n\ndef log_error(message: str, exc_info: bool = False, **context) -> None:\n    \"\"\"Log an error with optional exception info and context.\"\"\"\n    logger = get_logger()\n    if context:\n        message = f\"{message} | {context}\"\n    logger.error(message, exc_info=exc_info)\n\n\ndef log_warning(message: str, **context) -> None:\n    \"\"\"Log a warning with optional context.\"\"\"\n    logger = get_logger()\n    if context:\n        message = f\"{message} | {context}\"\n    logger.warning(message)\n\n\ndef log_info(message: str, **context) -> None:\n    \"\"\"Log info with optional context.\"\"\"\n    logger = get_logger()\n    if context:\n        message = f\"{message} | {context}\"\n    logger.info(message)\n\n\ndef log_debug(message: str, **context) -> None:\n    \"\"\"Log debug info with optional context.\"\"\"\n    logger = get_logger()\n    if context:\n        message = f\"{message} | {context}\"\n    logger.debug(message)\n\n\nclass HookError(Exception):\n    \"\"\"Base exception for hook errors.\"\"\"\n    pass\n\n\nclass ConfigError(HookError):\n    \"\"\"Configuration file error (missing, malformed, etc.).\"\"\"\n    pass\n\n\nclass StateError(HookError):\n    \"\"\"State file error (corrupted, inaccessible, etc.).\"\"\"\n    pass\n\n\nclass ValidationError(HookError):\n    \"\"\"Input validation error.\"\"\"\n    pass\n",
        "hooks/hooks.json": "{\n  \"description\": \"Agent-swarm enforcement hooks\",\n  \"hooks\": {\n    \"PreToolUse\": [\n      {\n        \"matcher\": \"mcp__router__router__*\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"python3 /home/fearsidhe/.claude/plugins/agent-swarm/hooks/router-event-hook.py\",\n            \"timeout\": 10\n          }\n        ]\n      },\n      {\n        \"matcher\": \"mcp__*\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"python3 /home/fearsidhe/.claude/plugins/agent-swarm/hooks/subagent-mcp-bypass.py\",\n            \"timeout\": 3\n          }\n        ]\n      },\n      {\n        \"matcher\": \"*\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"python3 /home/fearsidhe/.claude/plugins/agent-swarm/hooks/native-tool-blocking.py\",\n            \"timeout\": 3\n          },\n          {\n            \"type\": \"command\",\n            \"command\": \"cd /home/fearsidhe/.claude/plugins/agent-swarm && poetry run python3 hooks/telemetry-pretool.py\",\n            \"timeout\": 2\n          }\n        ]\n      },\n      {\n        \"matcher\": \"Task\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"python3 /home/fearsidhe/.claude/plugins/agent-swarm/hooks/implementer-only-enforcement.py\",\n            \"timeout\": 3\n          },\n          {\n            \"type\": \"command\",\n            \"command\": \"python3 /home/fearsidhe/.claude/plugins/agent-swarm/hooks/background-enforcement.py\",\n            \"timeout\": 3\n          },\n          {\n            \"type\": \"command\",\n            \"command\": \"bash /home/fearsidhe/.claude/plugins/agent-swarm/hooks/inject-subagent-briefing.sh\",\n            \"timeout\": 5\n          }\n        ]\n      }\n    ],\n    \"PostToolUse\": [\n      {\n        \"matcher\": \"*\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"cd /home/fearsidhe/.claude/plugins/agent-swarm && poetry run python3 hooks/telemetry-posttool.py\",\n            \"timeout\": 2\n          },\n          {\n            \"type\": \"command\",\n            \"command\": \"python3 /home/fearsidhe/.claude/plugins/agent-swarm/hooks/post-tool-tracking.py\",\n            \"timeout\": 3\n          }\n        ]\n      },\n      {\n        \"matcher\": \"Task\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"python3 /home/fearsidhe/.claude/plugins/agent-swarm/hooks/post-task-tracking.py\",\n            \"timeout\": 3\n          }\n        ]\n      }\n    ],\n    \"PreCompact\": [\n      {\n        \"matcher\": \"*\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"python3 /home/fearsidhe/.claude/plugins/agent-swarm/hooks/pre-compacting.py\",\n            \"timeout\": 5\n          }\n        ]\n      }\n    ],\n    \"SubagentStart\": [\n      {\n        \"matcher\": \"*\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"python3 /home/fearsidhe/.claude/plugins/agent-swarm/hooks/subagent-enforcement.py\",\n            \"timeout\": 3\n          }\n        ]\n      }\n    ],\n    \"SubagentStop\": [\n      {\n        \"matcher\": \"*\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"python3 /home/fearsidhe/.claude/plugins/agent-swarm/hooks/subagent-complete.py\",\n            \"timeout\": 3\n          }\n        ]\n      }\n    ],\n    \"SessionStart\": [\n      {\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"python3 /home/fearsidhe/.claude/plugins/agent-swarm/hooks/session-start.py\",\n            \"timeout\": 10\n          },\n          {\n            \"type\": \"command\",\n            \"command\": \"python3 /home/fearsidhe/.claude/plugins/agent-swarm/hooks/session-init.py\",\n            \"timeout\": 5\n          },\n          {\n            \"type\": \"command\",\n            \"command\": \"python3 /home/fearsidhe/.claude/plugins/agent-swarm/hooks/telemetry-sessionstart.py\",\n            \"timeout\": 30\n          }\n        ]\n      }\n    ],\n    \"SessionEnd\": [\n      {\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"python3 /home/fearsidhe/.claude/plugins/agent-swarm/hooks/session-end.py\",\n            \"timeout\": 30\n          }\n        ]\n      }\n    ]\n  }\n}\n",
        "hooks/implementer-only-enforcement.py": "#!/usr/bin/env python3\n\"\"\"Implementer-only enforcement hook.\n\nEnforces that during the orchestrate phase of iterate workflow (TDD mode),\nonly agent-swarm:implementer agents can be spawned. This ensures that all\nimplementation work goes through the TDD loop (test_writing → implement → test → review).\n\nSpawning explorers, researchers, etc. from orchestrate phase bypasses TDD discipline.\n\"\"\"\n\nimport json\nimport sys\nfrom pathlib import Path\n\n# Add lib to path for workflow_client\nlib_dir = Path(__file__).parent.parent / \"lib\"\nsys.path.insert(0, str(lib_dir))\n\ntry:\n    from workflow_client import workflow_is_active, workflow_get_state\nexcept ImportError:\n    # Fail-open if workflow_client not available\n    def workflow_is_active(workflow_id: str) -> bool:\n        return False\n    def workflow_get_state(workflow_id: str) -> dict | None:\n        return None\n\n\ndef allow(reason: str = \"\") -> dict:\n    \"\"\"Return allow decision.\"\"\"\n    result = {\n        \"hookSpecificOutput\": {\n            \"hookEventName\": \"PreToolUse\",\n            \"permissionDecision\": \"allow\"\n        }\n    }\n    if reason:\n        result[\"hookSpecificOutput\"][\"permissionDecisionReason\"] = reason\n    return result\n\n\ndef block(reason: str) -> dict:\n    \"\"\"Return block decision.\"\"\"\n    return {\n        \"hookSpecificOutput\": {\n            \"hookEventName\": \"PreToolUse\",\n            \"permissionDecision\": \"deny\",\n            \"permissionDecisionReason\": reason\n        }\n    }\n\n\ndef main():\n    \"\"\"Main enforcement logic.\"\"\"\n    try:\n        input_data = json.loads(sys.stdin.read())\n    except json.JSONDecodeError:\n        # Fail-open on invalid input\n        print(json.dumps(allow()))\n        return\n\n    tool_name = input_data.get(\"tool_name\", \"\")\n    tool_input = input_data.get(\"tool_input\", {})\n\n    # Only check Task tool\n    if tool_name != \"Task\":\n        print(json.dumps(allow()))\n        return\n\n    # Check if iterate workflow is active\n    if not workflow_is_active(\"iterate\"):\n        # Not in iterate workflow - allow any agent type\n        print(json.dumps(allow()))\n        return\n\n    # Get iterate workflow state\n    state = workflow_get_state(\"iterate\")\n    if not state:\n        # Can't get state - fail-open\n        print(json.dumps(allow()))\n        return\n\n    # Check if in orchestrate phase\n    phase = state.get(\"phase\", \"\")\n    if phase != \"orchestrate\":\n        # Not in orchestrate phase - allow any agent type\n        print(json.dumps(allow()))\n        return\n\n    # In iterate workflow + orchestrate phase - only allow implementer agents\n    subagent_type = tool_input.get(\"subagent_type\", \"\")\n    \n    if not subagent_type:\n        # No subagent_type specified - fail-open\n        print(json.dumps(allow()))\n        return\n\n    if subagent_type == \"agent-swarm:implementer\":\n        # Implementer allowed\n        print(json.dumps(allow(\"Implementer agent in orchestrate phase\")))\n        return\n\n    # Non-implementer agent in orchestrate phase - BLOCK\n    print(json.dumps(block(\n        f\"[ITERATE/ORCHESTRATE] Only agent-swarm:implementer agents allowed during \"\n        f\"orchestrate phase of iterate workflow (TDD enforcement). \"\n        f\"Attempted to spawn: {subagent_type}. \"\n        f\"Implementers go through full TDD loop (test_writing → implement → test → review). \"\n        f\"Spawning other agent types bypasses TDD discipline.\"\n    )))\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "hooks/inject-subagent-briefing.sh": "#!/bin/bash\n# Enforces that Task prompts include the subagent briefing\n# Hook: PreToolUse for Task tool\n#\n# Instead of injecting briefing (which doesn't work for Task),\n# this hook ENFORCES that the orchestrator assembled the prompt correctly.\n\n# Read input from stdin\nINPUT=$(cat)\n\n# Extract the prompt from the tool input\nPROMPT=$(echo \"$INPUT\" | jq -r '.tool_input.prompt // empty')\n\nif [ -z \"$PROMPT\" ]; then\n    # No prompt provided - allow (Task tool will handle error)\n    echo '{\"hookSpecificOutput\": {\"hookEventName\": \"PreToolUse\", \"permissionDecision\": \"allow\"}}'\n    exit 0\nfi\n\n# Check if prompt contains the briefing marker\nif echo \"$PROMPT\" | grep -q \"SUBAGENT OPERATING PROTOCOL\"; then\n    # Briefing is present - allow\n    echo '{\"hookSpecificOutput\": {\"hookEventName\": \"PreToolUse\", \"permissionDecision\": \"allow\"}}'\n    exit 0\nfi\n\n# Briefing is missing - deny with instructions\n# NOTE: \"block\" is IGNORED for Task tool, must use \"deny\"\n# NOTE: Must use \"permissionDecisionReason\" not \"message\"\ncat << 'EOF'\n{\"hookSpecificOutput\": {\"hookEventName\": \"PreToolUse\", \"permissionDecision\": \"deny\", \"permissionDecisionReason\": \"[BRIEFING_REQUIRED] Task prompt must include subagent briefing.\\n\\nAssemble the prompt:\\n1. Read: cat ~/.claude/plugins/agent-swarm/hooks/subagent-briefing.md\\n2. Prepend to your task with header: # SUBAGENT OPERATING PROTOCOL\\n3. Add phase restrictions if in iterate workflow\\n4. Re-call Task with assembled prompt\\n\\nSubagent Tools (allowed_tools to include):\\n- Shell: mcp-call pytest, mcp-call ruff, mcp-call git, etc.\\n- Files: mcp-call native__read_file, mcp-call native__write_file\\n- Search: mcp-call native__glob, mcp-call native__grep\\n- Serena: mcp-call serena__find_symbol, etc.\\n\\nSee 'Subagent Prompt Assembly' in iterate skill for details.\"}}\nEOF\n",
        "hooks/iterate-enforcement.py": "#!/usr/bin/env python3\n\"\"\"Iterate workflow enforcement hook.\n\nThis hook enforces phase-based tool restrictions for the /iterate workflow.\nIt ONLY applies when /iterate is active - base-enforcement.py handles the\n\"no workflow = no editing\" rule.\n\nEach workflow owns its own enforcement logic.\n\"\"\"\n\nimport sys\nimport json\nfrom pathlib import Path\n\n# Add lib to path\nlib_dir = Path(__file__).parent.parent / \"lib\"\nsys.path.insert(0, str(lib_dir))\n\ntry:\n    from iterate_workflow import is_tool_allowed, is_active, get_phase\n    from workflow_client import agent_get_state\nexcept ImportError:\n    # If module not available, allow everything (fail-open)\n    def is_tool_allowed(tool_name: str, command: str | None = None) -> tuple[bool, str]:\n        return True, \"\"\n    def is_active() -> bool:\n        return False\n    def get_phase():\n        return None\n    def agent_get_state(agent_id: str) -> dict | None:\n        return None\n\n\ndef is_tool_allowed_for_agent(tool_name: str, agent_id: str, command: str | None = None) -> tuple[bool, str, str]:\n    \"\"\"Check tool permission for a specific agent using its stored phase.\n    \n    Returns (allowed, reason, phase_name).\n    \"\"\"\n    agent_state = agent_get_state(agent_id)\n    if not agent_state:\n        # No agent state found, fall back to global\n        allowed, reason = is_tool_allowed(tool_name, command=command)\n        phase = get_phase()\n        return allowed, reason, phase.value if phase else \"unknown\"\n    \n    # Get agent's phase from stored state\n    phase_name = agent_state.get(\"phase\", \"unknown\")\n    \n    # Import phase model to check restrictions\n    try:\n        from phase_model import get_phase_info, TOOL_CATEGORIES\n        phase_info = get_phase_info(phase_name)\n        if not phase_info:\n            return True, \"\", phase_name\n        \n        # Check if tool is explicitly blocked\n        if tool_name in phase_info.blocked_tools:\n            return False, f\"Tool '{tool_name}' blocked in {phase_name} phase\", phase_name\n        \n        # Check tool category\n        tool_cat = TOOL_CATEGORIES.get(tool_name)\n        if tool_cat and tool_cat not in phase_info.allowed_categories:\n            return False, f\"Tool '{tool_name}' (category: {tool_cat}) not allowed in {phase_name} phase\", phase_name\n        \n        return True, \"\", phase_name\n    except ImportError:\n        return True, \"\", phase_name\n\n\n# Orchestrator cannot use editing tools - reserved for subagents\nORCHESTRATOR_BLOCKED_TOOLS = {\"Edit\", \"Write\", \"NotebookEdit\"}\n\n\ndef allow(reason: str = \"\") -> dict:\n    \"\"\"Return allow decision.\"\"\"\n    result = {\n        \"hookSpecificOutput\": {\n            \"hookEventName\": \"PreToolUse\",\n            \"permissionDecision\": \"allow\"\n        }\n    }\n    if reason:\n        result[\"hookSpecificOutput\"][\"permissionDecisionReason\"] = reason\n    return result\n\n\ndef block(reason: str):\n    \"\"\"Block tool by exiting with code 2 (stderr used as message).\n\n    Exit code 2 is the reliable blocking mechanism - JSON deny responses\n    are sometimes ignored (Claude Code bug #4669).\n    \"\"\"\n    print(reason, file=sys.stderr)\n    sys.exit(2)\n\n\ndef main():\n    \"\"\"Main enforcement logic - only applies when /iterate is active.\"\"\"\n    # Parse input\n    try:\n        input_data = json.loads(sys.stdin.read())\n    except json.JSONDecodeError:\n        print(json.dumps(allow()))\n        return\n\n    # Skip if /iterate is not active - exit silently to let other hooks decide\n    if not is_active():\n        sys.exit(0)\n\n    tool_name = input_data.get(\"tool_name\", \"\")\n    tool_input = input_data.get(\"tool_input\", {})\n\n    # Normalize MCP router prefix (mcp__router__native__bash -> native__bash)\n    if tool_name.startswith(\"mcp__router__\"):\n        tool_name = tool_name[len(\"mcp__router__\"):]\n\n    # Extract command for bash tools (for git/gh blocking)\n    # native__bash is the routed version through MCP router\n    command = tool_input.get(\"command\") if tool_name in (\"Bash\", \"native__bash\") else None\n\n    # Check if this is a subagent with stored state\n    agent_id = input_data.get(\"agentId\")\n\n    # Subagents can only use Bash for mcp-call (their tool interface)\n    # This exception allows implementers to call Serena/Context7 via router\n    if agent_id and tool_name == \"Bash\":\n        cmd = tool_input.get(\"command\", \"\").strip()\n        mcp_call_path = \"/home/fearsidhe/.claude/plugins/agent-swarm/bin/mcp-call\"\n        if cmd.startswith(\"mcp-call \") or cmd.startswith(f\"{mcp_call_path} \"):\n            print(json.dumps(allow(\"Subagent mcp-call allowed\")))\n            return\n        # Block non-mcp-call bash for subagents\n        block(\"[SUBAGENT] Only mcp-call allowed. Use: mcp-call <tool> '<args>'\")\n\n    # Orchestrator cannot use editing tools - reserved for subagents\n    if not agent_id and tool_name in ORCHESTRATOR_BLOCKED_TOOLS:\n        block(f\"[ORCHESTRATOR] Tool '{tool_name}' is reserved for subagents\")\n\n    if agent_id:\n        # Use agent-specific phase enforcement\n        allowed, reason, phase_name = is_tool_allowed_for_agent(tool_name, agent_id, command=command)\n        if not allowed:\n            full_reason = f\"[ITERATE:{phase_name}] {reason}\"\n            block(full_reason)\n    else:\n        # Use global iterate workflow phase enforcement\n        allowed, reason = is_tool_allowed(tool_name, command=command)\n        if not allowed:\n            phase = get_phase()\n            phase_name = phase.value if phase else \"unknown\"\n            full_reason = f\"[ITERATE:{phase_name}] {reason}\"\n            block(full_reason)\n\n    print(json.dumps(allow()))\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "hooks/max-agents-enforcement.py": "#!/usr/bin/env python3\n\"\"\"PreToolUse hook to enforce max_agents limit for Task tool.\"\"\"\n\nimport json\nimport sys\nfrom pathlib import Path\n\n# Add lib to path\nlib_dir = Path(__file__).parent.parent / \"lib\"\nsys.path.insert(0, str(lib_dir))\n\n\ndef main():\n    \"\"\"Main hook logic - enforce max_agents for Task tool.\"\"\"\n    input_data = json.loads(sys.stdin.read())\n    tool_name = input_data.get(\"tool_name\", \"\")\n    \n    # Only check Task tool\n    if tool_name != \"Task\":\n        print(json.dumps({\"hookSpecificOutput\": {\"hookEventName\": \"PreToolUse\", \"permissionDecision\": \"allow\"}}))\n        return\n    \n    # Check if spawning is allowed\n    try:\n        from worker_pool import should_spawn_worker, is_active\n        \n        if not is_active():\n            # Worker pool not active - allow (no enforcement)\n            print(json.dumps({\"hookSpecificOutput\": {\"hookEventName\": \"PreToolUse\", \"permissionDecision\": \"allow\"}}))\n            return\n            \n        if not should_spawn_worker(queue_has_work=True):\n            # At max agents - block\n            print(json.dumps({\n                \"hookSpecificOutput\": {\n                    \"hookEventName\": \"PreToolUse\",\n                    \"permissionDecision\": \"deny\",\n                    \"permissionDecisionReason\": \"[MAX_AGENTS] Cannot spawn more agents. Wait for existing agents to complete.\"\n                }\n            }))\n            return\n            \n    except ImportError:\n        pass  # worker_pool not available - allow\n    \n    print(json.dumps({\"hookSpecificOutput\": {\"hookEventName\": \"PreToolUse\", \"permissionDecision\": \"allow\"}}))\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "hooks/monitor_agent.py": "#!/usr/bin/env python3\n\"\"\"\nMonitor Agent - Contextual enforcement using Haiku API.\n\nProvides lightweight contextual validation for scenarios where regex/rules are insufficient.\nUsed for: commit message validation, classification appropriateness, context understanding.\n\"\"\"\n\nimport os\nimport re\nfrom typing import Dict, Optional, Any\n\ntry:\n    from hook_logging import log_error, log_warning, log_info, log_debug, ConfigError, StateError\nexcept ImportError:\n    # Fallback: define minimal logging functions\n    def log_error(msg, **kw):\n        pass\n    def log_warning(msg, **kw):\n        pass\n    def log_info(msg, **kw):\n        pass\n    def log_debug(msg, **kw):\n        pass\n    class ConfigError(Exception):\n        pass\n    class StateError(Exception):\n        pass\n# Try to import anthropic, gracefully degrade if not available\ntry:\n    import anthropic\n    ANTHROPIC_AVAILABLE = True\nexcept ImportError:\n    ANTHROPIC_AVAILABLE = False\n\n\ndef needs_monitoring(tool_name: str, tool_input: dict, state: dict) -> bool:\n    \"\"\"\n    Decide if this tool invocation needs monitor agent review.\n\n    Criteria for monitoring:\n    - Git commits (validate message content)\n    - First file edit with SIMPLE classification (validate appropriateness)\n    - COMPLEX tasks without workflow invocation\n\n    Returns: True if monitor should be called\n    \"\"\"\n    # Only monitor if API is available\n    if not ANTHROPIC_AVAILABLE:\n        return False\n\n    # Monitor git commits for message violations\n    if tool_name == \"Bash\" and \"git commit\" in tool_input.get(\"command\", \"\"):\n        return True\n\n    # Monitor first file edit to validate SIMPLE classification\n    if tool_name in {\"Write\", \"Edit\", \"mcp__plugin_serena_serena__replace_symbol_body\",\n                     \"mcp__plugin_serena_serena__create_text_file\",\n                     \"mcp__plugin_serena_serena__replace_content\"}:\n        classification = state.get(\"classification_type\")\n        files_edited = state.get(\"files_edited_this_session\", [])\n\n        # Check on first edit if classified as SIMPLE\n        if classification == \"SIMPLE\" and len(files_edited) == 0:\n            return True\n\n    return False\n\n\ndef call_monitor_agent(tool_name: str, tool_input: dict, state: dict) -> Optional[Dict[str, Any]]:\n    \"\"\"\n    Call Haiku API to make contextual enforcement decision.\n\n    Returns: Decision dict with structure:\n        {\n            \"allowed\": bool,\n            \"reason\": str,\n            \"confidence\": float  # 0.0-1.0\n        }\n    Or None if API call fails or is unavailable.\n    \"\"\"\n    if not ANTHROPIC_AVAILABLE:\n        return None\n\n    api_key = os.environ.get(\"ANTHROPIC_API_KEY\")\n    if not api_key:\n        return None\n\n    try:\n        client = anthropic.Anthropic(api_key=api_key)\n\n        # Build context-specific prompt based on tool\n        prompt = _build_monitor_prompt(tool_name, tool_input, state)\n\n        # Call Haiku for fast, cheap decision\n        response = client.messages.create(\n            model=\"claude-3-5-haiku-20241022\",\n            max_tokens=200,\n            temperature=0,\n            messages=[{\n                \"role\": \"user\",\n                \"content\": prompt\n            }]\n        )\n\n        # Parse response\n        decision_text = response.content[0].text.strip()\n        return _parse_decision(decision_text)\n\n    except Exception as e:\n        # Log error but don't block - fail open for availability\n        import sys\n        print(f\"Monitor agent error: {e}\", file=sys.stderr)\n        return None\n\n\ndef _build_monitor_prompt(tool_name: str, tool_input: dict, state: dict) -> str:\n    \"\"\"Build prompt for monitor agent based on context.\"\"\"\n    if tool_input is None:\n        tool_input = {}\n    if state is None:\n        state = {}\n\n    # Git commit validation\n    if tool_name == \"Bash\" and \"git commit\" in tool_input.get(\"command\", \"\"):\n        command = tool_input[\"command\"]\n\n        # Extract commit message\n        message = _extract_commit_message(command)\n\n        return f\"\"\"You are validating a git commit message against project standards.\n\nSTANDARDS (from CLAUDE.md):\n- NEVER add attributions, emoji, or decorations to commit messages\n- Follow existing repository conventions\n- Focus on the \"why\" rather than the \"what\"\n- Keep messages concise (1-2 sentences)\n\nCOMMIT MESSAGE TO VALIDATE:\n{message}\n\nVIOLATIONS TO CHECK:\n1. Attribution text like \"Generated with Claude\" or \"Co-Authored-By: Claude\"\n2. Emoji (any Unicode emoji characters)\n3. Robot emoji 🤖 or similar decorations\n4. Marketing/branding language\n\nRespond EXACTLY in this format:\nALLOWED: yes/no\nREASON: (one sentence explanation)\nCONFIDENCE: (0.0-1.0)\n\nIf the message violates standards, respond: \"ALLOWED: no\"\nIf the message is clean, respond: \"ALLOWED: yes\"\n\"\"\"\n\n    # Classification validation for SIMPLE tasks\n    if tool_name in {\"Write\", \"Edit\"}:\n        file_path = tool_input.get(\"file_path\", \"unknown\")\n        classification = state.get(\"classification_type\", \"unknown\")\n\n        # Get recent conversation context if available\n        # For now, just validate based on state\n\n        return f\"\"\"You are validating task classification for workflow enforcement.\n\nTASK CLASSIFICATION: {classification}\nFILE BEING EDITED: {file_path}\nFILES EDITED SO FAR: {len(state.get(\"files_edited_this_session\", []))}\n\nCLASSIFICATION RULES (from CLAUDE.md):\n- SIMPLE: Single file, <50 lines, clear requirements\n- COMPLEX: Multiple files OR unclear scope OR architectural decisions\n\nRed flags that mean COMPLEX, not SIMPLE:\n- Multiple files need changes\n- Unsure where code should go\n- Requirements have ambiguity\n- Architectural decisions involved\n\nBased on the context, is SIMPLE classification appropriate?\n\nRespond EXACTLY in this format:\nALLOWED: yes/no\nREASON: (one sentence explanation)\nCONFIDENCE: (0.0-1.0)\n\nIf SIMPLE is appropriate, respond: \"ALLOWED: yes\"\nIf should be COMPLEX, respond: \"ALLOWED: no\"\n\"\"\"\n\n    return \"\"\n\n\ndef _extract_commit_message(command: str) -> str:\n    \"\"\"Extract commit message from git command.\"\"\"\n    # Handle heredoc format\n    heredoc_match = re.search(r'<<[\"\\']?EOF[\"\\']?\\s*(.*?)\\s*EOF', command, re.DOTALL)\n    if heredoc_match:\n        return heredoc_match.group(1)\n\n    # Handle -m flag\n    msg_match = re.search(r'-m\\s+[\"\\'](.+?)[\"\\']', command, re.DOTALL)\n    if msg_match:\n        return msg_match.group(1)\n\n    return \"(unable to extract message)\"\n\n\ndef _parse_decision(text: str) -> Optional[Dict[str, Any]]:\n    \"\"\"Parse monitor agent response into decision dict.\"\"\"\n    try:\n        # Extract components using regex\n        allowed_match = re.search(r'ALLOWED:\\s*(yes|no)', text, re.IGNORECASE)\n        reason_match = re.search(r'REASON:\\s*(.+?)(?:\\n|$)', text, re.DOTALL)\n        confidence_match = re.search(r'CONFIDENCE:\\s*([\\d.]+)', text)\n\n        if not allowed_match:\n            return None\n\n        return {\n            \"allowed\": allowed_match.group(1).lower() == \"yes\",\n            \"reason\": reason_match.group(1).strip() if reason_match else \"No reason provided\",\n            \"confidence\": float(confidence_match.group(1)) if confidence_match else 0.5\n        }\n    except Exception:\n        return None\n\n\ndef format_monitor_result(decision: Dict[str, Any]) -> dict:\n    \"\"\"\n    Convert monitor decision to hook result format.\n\n    Args:\n        decision: Dict with \"allowed\", \"reason\", \"confidence\" keys\n\n    Returns: Hook result dict ready to return from pre_tool_use hook\n    \"\"\"\n    if decision[\"allowed\"]:\n        return {\n            \"hookSpecificOutput\": {\n                \"hookEventName\": \"PreToolUse\",\n                \"permissionDecision\": \"allow\",\n                \"permissionDecisionReason\": f\"[MONITOR] Approved: {decision['reason']}\"\n            }\n        }\n    else:\n        return {\n            \"hookSpecificOutput\": {\n                \"hookEventName\": \"PreToolUse\",\n                \"permissionDecision\": \"deny\",\n                \"permissionDecisionReason\": (\n                    f\"[MONITOR AGENT] {decision['reason']}\\n\"\n                    f\"Confidence: {decision['confidence']:.0%}\\n\"\n                    \"\\n\"\n                    \"The monitor agent identified a potential policy violation.\\n\"\n                    \"Please review and correct before proceeding.\"\n                )\n            }\n        }\n\n\ndef detect_batch_need(tool_name: str, tool_input: dict, state: dict, recent_messages: list) -> dict | None:\n    \"\"\"\n    Detect patterns in conversation indicating batch operations needed.\n    \n    Returns block decision if batch operation clearly needed, None otherwise.\n    \"\"\"\n    import re\n    \n    # Only check on first few searches/reads\n    search_count = state.get(\"search_count\", 0)\n    read_count = state.get(\"read_count\", 0)\n    \n    # Only intervene early (before limit hit)\n    if search_count > 2 or read_count > 2:\n        return None\n    \n    # Get last 3 messages (user + assistant turns)\n    recent_text = \" \".join([\n        msg.get(\"content\", \"\") \n        for msg in recent_messages[-3:]\n    ]).lower()\n    \n    # Patterns indicating batch operations\n    batch_indicators = [\n        (r'\\b(\\d+)\\s+(files?|patterns?|searches?)', 'files/patterns'),\n        (r'check\\s+all', 'all checks'),\n        (r'find\\s+all\\s+.*\\s+that', 'find all pattern'),\n        (r'across\\s+(multiple|many)', 'multiple targets'),\n        (r'throughout\\s+the\\s+codebase', 'codebase-wide'),\n        (r'every\\s+\\w+\\s+in', 'iteration pattern'),\n    ]\n    \n    for pattern, description in batch_indicators:\n        match = re.search(pattern, recent_text)\n        if match:\n            # Extract number if present (only if group contains digits)\n            num = None\n            if match.lastindex and match.lastindex >= 1:\n                group_val = match.group(1)\n                if group_val and group_val.isdigit():\n                    num = int(group_val)\n            \n            # If explicit number > 5, or qualitative indicator (\"all\", \"every\", etc.)\n            if num and num > 5:\n                return {\n                    \"allowed\": False,\n                    \"message\": (\n                        f\"[PROACTIVE BLOCK] Detected intent to process {num} items ({description}).\\n\\n\"\n                        f\"REQUIRED: Use batch approach BEFORE starting:\\n\\n\"\n                        f\"✓ OPTION 1: Spawn Explorer subagent\\n\"\n                        f\"  Task(subagent_type='Explore', prompt='...')\\n\\n\"\n                        f\"✓ OPTION 2: Write batch script\\n\"\n                        f\"  Write(file_path='/tmp/batch_search.py', content='''...''')\\n\\n\"\n                        f\"Don't start direct tool calls when you know you'll hit limits.\"\n                    )\n                }\n            elif not num and description in ['all checks', 'find all pattern', 'codebase-wide']:\n                return {\n                    \"allowed\": False,\n                    \"message\": (\n                        f\"[PROACTIVE BLOCK] Detected codebase-wide operation ({description}).\\n\\n\"\n                        f\"Use Explorer subagent for codebase exploration:\\n\"\n                        f\"  Task(subagent_type='Explore', prompt='Find all {description}...')\"\n                    )\n                }\n    \n    return None\n",
        "hooks/native-tool-blocking.py": "#!/usr/bin/env python3\n\"\"\"Block all tools except router-mediated access.\n\nWhitelist approach:\n1. mcp__router__* or mcp__plugin_* tools → allow (going through router)\n2. Bash with mcp-call/mcp prefix → allow (subagent access to router)\n3. Claude Code system/meta tools → allow (infrastructure, not file ops)\n4. Everything else → block\n\nMain agents use mcp__router__* directly.\nSubagents use Bash(mcp-call ...) which routes through the router.\nBoth paths enforce permissions via the router's permissions.yaml.\n\"\"\"\nimport json\nimport sys\n\n\nMCP_CALL_PATH = \"/home/fearsidhe/.claude/plugins/agent-swarm/bin/mcp-call\"\n\n# Claude Code system/meta tools — infrastructure, not file/code operations.\n# These don't need router mediation because they don't touch files or run code.\nSYSTEM_TOOLS = {\n    \"Task\", \"TaskOutput\", \"TaskStop\",\n    \"TaskCreate\", \"TaskGet\", \"TaskUpdate\", \"TaskList\",\n    \"TodoWrite\",\n    \"AskUserQuestion\",\n    \"Skill\",\n    \"KillShell\",\n    \"EnterPlanMode\", \"ExitPlanMode\",\n    \"ToolSearch\",\n}\n\n\ndef allow(reason: str = \"\"):\n    result = {\n        \"hookSpecificOutput\": {\n            \"hookEventName\": \"PreToolUse\",\n            \"permissionDecision\": \"allow\"\n        }\n    }\n    if reason:\n        result[\"hookSpecificOutput\"][\"permissionDecisionReason\"] = reason\n    return result\n\n\ndef block(reason: str):\n    \"\"\"Block tool via exit code 2 (reliable blocking).\"\"\"\n    print(reason, file=sys.stderr)\n    sys.exit(2)\n\n\ndef main():\n    try:\n        input_data = json.loads(sys.stdin.read())\n    except json.JSONDecodeError:\n        print(json.dumps(allow()))\n        return\n\n    tool_name = input_data.get(\"tool_name\", \"\")\n    tool_input = input_data.get(\"tool_input\", {})\n    # Claude Code passes agentId for subagent tool calls\n    agent_id = input_data.get(\"agentId\") or input_data.get(\"agent_id\")\n    is_subagent = bool(agent_id)\n\n    # Subagents: ONLY Bash(mcp-call) allowed — no direct stdio router access\n    if is_subagent:\n        if tool_name == \"Bash\":\n            cmd = tool_input.get(\"command\", \"\").strip()\n            if cmd.startswith(\"mcp\") or cmd.startswith(MCP_CALL_PATH):\n                print(json.dumps(allow(\"Subagent mcp-call\")))\n                return\n        # Allow system tools subagents need (TaskOutput for reporting, etc.)\n        if tool_name in SYSTEM_TOOLS:\n            print(json.dumps(allow(\"Subagent system tool\")))\n            return\n        block(\n            f\"[SUBAGENT BLOCKED] '{tool_name}' not allowed for subagents. \"\n            f\"Use mcp-call via Bash: mcp-call <tool> '<json_args>'\"\n        )\n        return\n\n    # Main agent rules below\n\n    # Rule 1: MCP router/plugin tools → allow (router enforces permissions)\n    if tool_name.startswith(\"mcp__router__\") or tool_name.startswith(\"mcp__plugin_\"):\n        print(json.dumps(allow(\"Router-mediated tool\")))\n        return\n\n    # Rule 2: Bash with mcp prefix → allow (mcp-call access to router)\n    if tool_name == \"Bash\":\n        cmd = tool_input.get(\"command\", \"\").strip()\n        if cmd.startswith(\"mcp\") or cmd.startswith(MCP_CALL_PATH):\n            print(json.dumps(allow(\"mcp-call\")))\n            return\n\n    # Rule 3: Claude Code system tools → allow (not file operations)\n    if tool_name in SYSTEM_TOOLS:\n        print(json.dumps(allow(\"System tool\")))\n        return\n\n    # Rule 4: Everything else → block\n    block(\n        f\"[BLOCKED] '{tool_name}' blocked. \"\n        f\"Use mcp__router__* tools (main agent) or mcp-call via Bash (subagent).\"\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "hooks/nonblocking-enforcement.py": "#!/usr/bin/env python3\n\"\"\"PreToolUse hook to enforce block=false for TaskOutput tool.\"\"\"\n\nimport json\nimport sys\n\n\ndef main():\n    \"\"\"Enforce block=false for TaskOutput tool calls.\"\"\"\n    input_data = json.loads(sys.stdin.read())\n    tool_name = input_data.get(\"tool_name\", \"\")\n    tool_input = input_data.get(\"tool_input\", {})\n    \n    # Only check TaskOutput tool\n    if tool_name != \"TaskOutput\":\n        print(json.dumps({\n            \"hookSpecificOutput\": {\n                \"hookEventName\": \"PreToolUse\",\n                \"permissionDecision\": \"allow\"\n            }\n        }))\n        return\n    \n    # Check block parameter (defaults to True if not specified)\n    block = tool_input.get(\"block\", True)\n    \n    if block:\n        print(json.dumps({\n            \"hookSpecificOutput\": {\n                \"hookEventName\": \"PreToolUse\",\n                \"permissionDecision\": \"deny\",\n                \"permissionDecisionReason\": (\n                    \"[NONBLOCKING_REQUIRED] TaskOutput tool must use block=false \"\n                    \"for parallel agent monitoring. Add block=false to your TaskOutput call.\"\n                )\n            }\n        }))\n        return\n    \n    print(json.dumps({\n        \"hookSpecificOutput\": {\n            \"hookEventName\": \"PreToolUse\",\n            \"permissionDecision\": \"allow\"\n        }\n    }))\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "hooks/parallel-enforcement.py": "#!/usr/bin/env python3\n\"\"\"PreToolUse hook to enforce parallel Task spawning.\n\nDetects when orchestrators spawn Task agents sequentially instead of in parallel.\nWarns on 2+ sequential spawns, blocks on 3rd spawn.\n\"\"\"\n\nimport json\nimport sys\nimport time\nfrom pathlib import Path\n\n# Add lib to path\nlib_dir = Path(__file__).parent.parent / \"lib\"\nsys.path.insert(0, str(lib_dir))\n\ntry:\n    from workflow_client import workflow_get_state, workflow_update\nexcept ImportError:\n    def workflow_get_state(workflow_id: str) -> dict | None:\n        return None\n    def workflow_update(workflow_id: str, updates: dict) -> dict:\n        return {}\n\n\ndef get_enforcement_state() -> dict:\n    \"\"\"Get current parallel enforcement tracking state.\"\"\"\n    session_state = workflow_get_state(\"session\")\n    if not session_state:\n        return {\"recent_spawns\": [], \"last_output_time\": 0}\n    \n    return session_state.get(\"parallel_enforcement_state\", {\n        \"recent_spawns\": [],\n        \"last_output_time\": 0\n    })\n\n\ndef update_enforcement_state(state: dict) -> None:\n    \"\"\"Update parallel enforcement tracking state.\"\"\"\n    try:\n        workflow_update(\"session\", {\"parallel_enforcement_state\": state})\n    except Exception:\n        pass  # Fail silently if state update fails\n\n\ndef clean_old_spawns(spawns: list, current_time: float, window: float = 5.0) -> list:\n    \"\"\"Remove spawns older than the time window.\"\"\"\n    return [s for s in spawns if current_time - s[\"timestamp\"] < window]\n\n\ndef main():\n    \"\"\"Main hook logic - enforce parallel spawning for Task tool.\"\"\"\n    input_data = json.loads(sys.stdin.read())\n    tool_name = input_data.get(\"tool_name\", \"\")\n    tool_input = input_data.get(\"tool_input\", {})\n    \n    # Only check Task tool\n    if tool_name != \"Task\":\n        print(json.dumps({\"hookSpecificOutput\": {\n            \"hookEventName\": \"PreToolUse\",\n            \"permissionDecision\": \"allow\"\n        }}))\n        return\n    \n    current_time = time.time()\n    state = get_enforcement_state()\n    \n    # Clean old spawns (outside 5 second window)\n    state[\"recent_spawns\"] = clean_old_spawns(\n        state[\"recent_spawns\"],\n        current_time,\n        window=5.0\n    )\n    \n    # Count recent spawns\n    recent_count = len(state[\"recent_spawns\"])\n    \n    # Record this spawn\n    task_desc = tool_input.get(\"description\", \"unknown task\")\n    state[\"recent_spawns\"].append({\n        \"timestamp\": current_time,\n        \"task_desc\": task_desc[:50]  # Truncate for storage\n    })\n    \n    # Determine action based on count\n    if recent_count == 0:\n        # First spawn - always allow\n        update_enforcement_state(state)\n        print(json.dumps({\"hookSpecificOutput\": {\n            \"hookEventName\": \"PreToolUse\",\n            \"permissionDecision\": \"allow\"\n        }}))\n        return\n    \n    elif recent_count == 1:\n        # Second spawn within window - WARN\n        update_enforcement_state(state)\n        warning_message = \"\"\"\n## PARALLEL SPAWNING RECOMMENDED\n\nYou are spawning Task agents **sequentially** (one after another).\n\n**Issue:** Sequential spawning is inefficient. Agents run one at a time instead of in parallel.\n\n**Solution:** Spawn all independent tasks in ONE message block:\n\n```python\n# ❌ Sequential (slow)\nTask(description=\"Task A\", ...)  # Wait for response\n# ... then later ...\nTask(description=\"Task B\", ...)  # Wait for response\n\n# ✅ Parallel (fast)\nTask(description=\"Task A\", ...)\nTask(description=\"Task B\", ...)\nTask(description=\"Task C\", ...)\n# All spawn together, run simultaneously\n```\n\n**Current spawns in last 5 seconds:** {}\n\nThis is a warning. Third sequential spawn will be blocked.\n\"\"\".format(recent_count + 1)\n        \n        print(json.dumps({\"hookSpecificOutput\": {\n            \"hookEventName\": \"PreToolUse\",\n            \"permissionDecision\": \"allow\",\n            \"additionalContext\": warning_message\n        }}))\n        return\n    \n    else:\n        # Third or more spawn within window - BLOCK\n        update_enforcement_state(state)\n        print(json.dumps({\"hookSpecificOutput\": {\n            \"hookEventName\": \"PreToolUse\",\n            \"permissionDecision\": \"deny\",\n            \"permissionDecisionReason\": (\n                f\"[PARALLEL_ENFORCEMENT] Cannot spawn {recent_count + 1} Task agents sequentially. \"\n                f\"You have spawned {recent_count} agents in the last 5 seconds. \"\n                \"Please spawn all independent tasks in ONE message block to run them in parallel. \"\n                \"Example: Task(...) Task(...) Task(...) in a single response.\"\n            )\n        }}))\n        return\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "hooks/post-task-tracking.py": "#!/usr/bin/env python3\n\"\"\"\nPost-task hook: Automatically track subagent completion.\n\nRuns after Task tool completes to log token usage and update metrics.\n\"\"\"\n\nimport sys\nimport json\nimport re\nfrom pathlib import Path\nfrom datetime import datetime\n\ntry:\n    from hook_logging import log_error, log_warning, log_info, log_debug, ConfigError, StateError\nexcept ImportError:\n    # Fallback: define minimal logging functions\n    def log_error(msg, **kw):\n        pass\n    def log_warning(msg, **kw):\n        pass\n    def log_info(msg, **kw):\n        pass\n    def log_debug(msg, **kw):\n        pass\n    class ConfigError(Exception):\n        pass\n    class StateError(Exception):\n        pass\n\n# Import workflow client for state tracking\ntry:\n    import sys\n    sys.path.insert(0, str(Path(__file__).parent.parent / \"lib\"))\n    import workflow_client\n    HAS_WORKFLOW_CLIENT = True\nexcept ImportError:\n    HAS_WORKFLOW_CLIENT = False\nSTATE_DIR = Path.home() / \".claude/plugins/agent-swarm/.state\"\n# DISABLED: No longer writing subagent metrics\n# SUBAGENT_METRICS = STATE_DIR / \"subagent_metrics.json\"\nSUBAGENT_METRICS = None\n\ndef load_metrics():\n    \"\"\"Load existing subagent metrics.\"\"\"\n    if SUBAGENT_METRICS.exists():\n        try:\n            return json.loads(SUBAGENT_METRICS.read_text())\n        except Exception as e:\n            log_warning(f\"Caught exception: {e}\")\n\n    # Initialize structure\n    return {}\n\ndef save_metrics(metrics):\n    \"\"\"Save subagent metrics.\"\"\"\n    STATE_DIR.mkdir(parents=True, exist_ok=True)\n    SUBAGENT_METRICS.write_text(json.dumps(metrics, indent=2))\n\ndef extract_agent_info(tool_output):\n    \"\"\"Extract agent ID and type from Task tool output.\"\"\"\n    # Look for agent ID in output\n    # Formats:\n    #   - \"agentId: abc1234\" (async agents)\n    #   - \"Agent abc1234: ...\" (older format)\n    #   - \"task_id: abc1234\" (task output format)\n\n    # Try agentId: format first (most common)\n    agent_id_match = re.search(r'agentId:\\s*([a-f0-9]{7,})', tool_output, re.IGNORECASE)\n    \n    # Fall back to other formats\n    if not agent_id_match:\n        agent_id_match = re.search(r'Agent\\s+([a-f0-9]{7,})', tool_output)\n    if not agent_id_match:\n        agent_id_match = re.search(r'task_id[\"\\s:]+([a-f0-9-]+)', tool_output)\n\n    if not agent_id_match:\n        return None, None\n\n    agent_id = agent_id_match.group(1)\n\n    # Try to find agent type (not in output, will be set from tool_input)\n    type_match = re.search(r'subagent[_\\s]type[\"\\s:]+([a-zA-Z0-9_:-]+)', tool_output, re.IGNORECASE)\n    agent_type = type_match.group(1) if type_match else \"unknown\"\n\n    return agent_id, agent_type\n\ndef track_subagent(agent_id, agent_type, prompt=\"\"):\n    \"\"\"Track a spawned subagent in workflow state.\"\"\"\n    # Update workflow state (survives compaction)\n    if HAS_WORKFLOW_CLIENT:\n        try:\n            if workflow_client.workflow_is_active(\"iterate\"):\n                agents = workflow_client.workflow_get_value(\"iterate\", \"active_agents\") or {}\n                agents[agent_id] = {\n                    \"description\": prompt[:100] if prompt else \"No description\",\n                    \"type\": agent_type,\n                    \"spawned_at\": datetime.now().isoformat()\n                }\n                workflow_client.workflow_update(\"iterate\", {\"active_agents\": agents})\n                log_info(f\"Tracked agent {agent_id} in workflow state\")\n        except Exception as e:\n            log_warning(f\"Failed to update workflow state: {e}\")\n\ndef check_new_plugins():\n    \"\"\"Check for and auto-document new plugins (every 10 tool uses).\"\"\"\n    import subprocess\n    from pathlib import Path\n\n    # Track check count\n    state_file = Path.home() / \".claude/plugins/agent-swarm/.state/plugin_check_count.txt\"\n    state_file.parent.mkdir(parents=True, exist_ok=True)\n\n    try:\n        count = int(state_file.read_text()) if state_file.exists() else 0\n    except (ValueError, IOError) as e:\n        log_debug(f\"Failed to read plugin check count: {e}\")\n        count = 0\n\n    count += 1\n\n    # Check every 10 tools\n    if count % 10 == 0:\n        try:\n            doc_script = Path(__file__).parent.parent / \"scripts/document_plugins.py\"\n            result = subprocess.run(\n                [\"python3\", str(doc_script)],\n                capture_output=True,\n                text=True,\n                timeout=5\n            )\n\n            # If new plugins were documented, return message\n            if \"new plugin\" in result.stdout.lower():\n                state_file.write_text(\"0\")  # Reset counter\n                return result.stdout\n        except Exception as e:\n            log_warning(f\"Caught exception: {e}\")\n\n    state_file.write_text(str(count))\n    return None\n\ndef main():\n    \"\"\"Post-task hook main.\"\"\"\n    try:\n        # Read hook input\n        input_data = json.loads(sys.stdin.read())\n    except Exception:\n        # No input or invalid JSON - allow\n        print(json.dumps({\"hookSpecificOutput\": {}}))\n        return\n\n    tool_name = input_data.get(\"tool_name\", \"\")\n    tool_input = input_data.get(\"tool_input\", {})\n    tool_output_raw = input_data.get(\"tool_response\", {})\n\n    # DEBUG: Log full structure to understand format\n    # DISABLED: Debug logging no longer written to file\n    debug_file = None  # STATE_DIR / \"post_task_debug.log\"\n    try:\n        with open(debug_file, \"a\") as f:\n            f.write(f\"\\n[{datetime.now().isoformat()}] FULL INPUT DUMP\\n\")\n            f.write(f\"  tool_name: {tool_name}\\n\")\n            f.write(f\"  input_data keys: {list(input_data.keys())}\\n\")\n            f.write(f\"  tool_output type: {type(tool_output_raw)}\\n\")\n            f.write(f\"  tool_output repr: {repr(tool_output_raw)[:500]}\\n\")\n            if isinstance(tool_output_raw, dict):\n                f.write(f\"  tool_output keys: {list(tool_output_raw.keys())}\\n\")\n                # Check for common fields\n                for key in ['agentId', 'agent_id', 'task_id', 'id', 'result', 'output']:\n                    if key in tool_output_raw:\n                        f.write(f\"  Found {key}: {tool_output_raw[key]}\\n\")\n    except Exception as e:\n        # Log the exception\n        try:\n            with open(debug_file, \"a\") as f:\n                f.write(f\"  ERROR in debug logging: {e}\\n\")\n        except Exception as e:\n            log_warning(f\"Caught exception: {e}\")\n\n    tool_output = tool_output_raw\n\n    # Track Task tool completions\n    if tool_name == \"Task\":\n        # DEBUG: Log to see if hook is running\n        # DISABLED: Debug logging no longer written to file\n        debug_file = None  # STATE_DIR / \"post_task_debug.log\"\n        try:\n            with open(debug_file, \"a\") as f:\n                f.write(f\"[{datetime.now().isoformat()}] Task tool detected\\n\")\n                f.write(f\"  tool_input keys: {list(tool_input.keys())}\\n\")\n                f.write(f\"  tool_response type: {type(tool_output)} keys: {list(tool_output.keys()) if isinstance(tool_output, dict) else len(tool_output)}\\n\")\n        except Exception as e:\n            log_warning(f\"Caught exception: {e}\")\n\n        # Extract agent info from input and output\n        # subagent_type is in tool_input, agent_id is in tool_output\n        subagent_type = tool_input.get(\"subagent_type\", \"unknown\")\n        \n        # Extract agent_id from tool_response\n        if isinstance(tool_output, dict):\n            # Modern format: dict with 'agentId' field\n            agent_id = tool_output.get('agentId', None)\n        else:\n            # Legacy format: string that needs parsing\n            agent_id, _ = extract_agent_info(str(tool_output))\n\n        # DEBUG: Log extraction result\n        try:\n            with open(debug_file, \"a\") as f:\n                f.write(f\"  agent_id extracted: {agent_id}\\n\")\n                f.write(f\"  subagent_type: {subagent_type}\\n\")\n        except Exception as e:\n            log_warning(f\"Caught exception: {e}\")\n\n        if agent_id:\n            try:\n                prompt = tool_input.get(\"prompt\", \"\")\n                track_subagent(agent_id, subagent_type, prompt)\n                # DEBUG: Confirm tracking\n                try:\n                    with open(debug_file, \"a\") as f:\n                        f.write(\"  ✅ Tracked successfully\\n\")\n                except Exception as e:\n                    log_warning(f\"Caught exception: {e}\")\n            except Exception as e:\n                # Don't fail the hook if tracking fails\n                try:\n                    with open(debug_file, \"a\") as f:\n                        f.write(f\"  ❌ Tracking failed: {e}\\n\")\n                except Exception as e:\n                    log_warning(f\"Caught exception: {e}\")\n\n    # Check for new plugins periodically\n    plugin_msg = None\n    try:\n        plugin_msg = check_new_plugins()\n    except Exception as e:\n        log_warning(f\"Caught exception: {e}\")\n\n    # Return message if new plugins found\n    output = {\"hookSpecificOutput\": {}}\n    if plugin_msg:\n        output[\"hookSpecificOutput\"][\"message\"] = f\"📦 {plugin_msg}\"\n\n    print(json.dumps(output))\n\nif __name__ == \"__main__\":\n    main()\n",
        "hooks/post-tool-tracking.py": "#!/usr/bin/env python3\n\"\"\"\nPost-tool tracking hook for agent-swarm plugin.\n\nTracks:\n1. Subagent spawns (Task tool completions)\n2. Function signature changes (Edit/Write tools)\n\"\"\"\n\nimport sys\nimport json\nimport re\nfrom datetime import datetime\nfrom pathlib import Path\n\ntry:\n    from hook_logging import log_error, log_warning, log_info, log_debug, ConfigError, StateError\nexcept ImportError:\n    # Fallback: define minimal logging functions\n    def log_error(msg, **kw):\n        pass\n    def log_warning(msg, **kw):\n        pass\n    def log_info(msg, **kw):\n        pass\n    def log_debug(msg, **kw):\n        pass\n    class ConfigError(Exception):\n        pass\n    class StateError(Exception):\n        pass\n# Configuration\n# DISABLED: No longer writing subagent metrics to file\n# METRICS_FILE = Path.home() / \".claude/plugins/agent-swarm/.state/subagent_metrics.json\"\nMETRICS_FILE = None\n# DISABLED: No longer writing session state to file\n# STATE_FILE = Path.home() / \".claude/plugins/agent-swarm/.state/session.json\"\nSTATE_FILE = None\n\ndef load_json(path: Path) -> dict:\n    \"\"\"Load JSON file safely.\"\"\"\n    if path is None:\n        return {}\n    if path.exists():\n        try:\n            return json.loads(path.read_text())\n        except (json.JSONDecodeError, IOError) as e:\n            log_warning(f\"Caught exception: {e}\")\n    return {}\n\ndef save_json(path: Path, data: dict) -> None:\n    \"\"\"Save JSON file.\"\"\"\n    if path is None:\n        return  # State file writes disabled\n    path.parent.mkdir(parents=True, exist_ok=True)\n    path.write_text(json.dumps(data, indent=2))\n\ndef extract_agent_id(output: str) -> str | None:\n    \"\"\"Extract agent_id from Task tool output using regex.\"\"\"\n    # Pattern: agent_id: <id> or \"agent_id\": \"<id>\"\n    patterns = [\n        r'agentId[\"\\']?\\s*:\\s*[\"\\']?([a-zA-Z0-9_-]+)',\n        r'Agent ID:\\s*([a-zA-Z0-9_-]+)',\n        r'Spawned:\\s*([a-zA-Z0-9_-]+)',\n    ]\n\n    for pattern in patterns:\n        match = re.search(pattern, output, re.IGNORECASE)\n        if match:\n            return match.group(1)\n\n    return None\n\ndef track_subagent(tool_input: dict, tool_output) -> None:\n    \"\"\"Track subagent spawn from Task tool.\"\"\"\n    # Extract agent_id from output (dict or string)\n    if isinstance(tool_output, dict):\n        agent_id = tool_output.get('agentId', None)\n    else:\n        agent_id = extract_agent_id(str(tool_output))\n\n    if not agent_id:\n        # No agent_id found, skip silently\n        return\n\n    # Load existing metrics\n    metrics = load_json(METRICS_FILE)\n\n    # Get agent type from input\n    agent_type = tool_input.get(\"subagent_type\", \"unknown\")\n\n    # Store metric\n    metrics[agent_id] = {\n        \"spawned_at\": datetime.now().isoformat(),\n        \"agent_type\": agent_type,\n        \"status\": \"running\",\n        \"prompt\": tool_input.get(\"prompt\", \"\")[:100]  # First 100 chars\n    }\n\n    save_json(METRICS_FILE, metrics)\n\ndef detect_signature_change(tool_name: str, tool_input: dict) -> None:\n    \"\"\"Detect function signature changes and store reminder.\"\"\"\n    if tool_name not in {\"Edit\", \"Write\", \"NotebookEdit\"}:\n        return\n\n    # Check for signature-like patterns in the content\n    content = tool_input.get(\"new_string\", \"\") or tool_input.get(\"content\", \"\")\n\n    # Patterns that indicate function/method signatures\n    signature_patterns = [\n        r'def\\s+\\w+\\s*\\([^)]*\\)',  # Python\n        r'function\\s+\\w+\\s*\\([^)]*\\)',  # JavaScript\n        r'(public|private|protected)?\\s*\\w+\\s+\\w+\\s*\\([^)]*\\)',  # Java/TypeScript\n        r'=>\\s*\\(',  # Arrow functions\n    ]\n\n    has_signature = any(re.search(p, content) for p in signature_patterns)\n\n    if not has_signature:\n        return\n\n    # Load state and add reminder\n    state = load_json(STATE_FILE)\n\n    reminders = state.get(\"signature_change_reminders\", [])\n    file_path = tool_input.get(\"file_path\", \"unknown\")\n\n    reminder = {\n        \"file\": file_path,\n        \"timestamp\": datetime.now().isoformat(),\n        \"tool\": tool_name\n    }\n\n    # Avoid duplicates\n    if not any(r[\"file\"] == file_path for r in reminders[-5:]):\n        reminders.append(reminder)\n        # Keep only last 10 reminders\n        state[\"signature_change_reminders\"] = reminders[-10:]\n        save_json(STATE_FILE, state)\n\ndef main():\n    # Read input from stdin\n    try:\n        input_data = json.loads(sys.stdin.read())\n    except json.JSONDecodeError:\n        # Invalid input, return empty response\n        print(json.dumps({}))\n        return\n\n    tool_name = input_data.get(\"tool_name\", \"\")\n    tool_input = input_data.get(\"tool_input\", {})\n    tool_output = input_data.get(\"tool_output\", {})  # Can be dict or string\n\n    # Track subagent spawns\n    if tool_name == \"Task\":\n        track_subagent(tool_input, tool_output)\n\n    # Detect signature changes\n    detect_signature_change(tool_name, tool_input)\n\n    # PostToolUse hooks don't need to return anything\n    print(json.dumps({}))\n\nif __name__ == \"__main__\":\n    main()\n",
        "hooks/pre-compacting.py": "#!/usr/bin/env python3\n\"\"\"\nPre-Compacting Hook - Auto-write handoff before context compression\n\nAutomatically generates a handoff document before conversation compacting\nto preserve important context, decisions, and progress.\n\"\"\"\n\nimport fcntl\nimport json\nimport sys\nfrom datetime import datetime\nfrom pathlib import Path\n\ntry:\n    from hook_logging import log_error, log_warning, log_info, log_debug, ConfigError, StateError\nexcept ImportError:\n    # Fallback: define minimal logging functions\n    def log_error(msg, **kw):\n        pass\n    def log_warning(msg, **kw):\n        pass\n    def log_info(msg, **kw):\n        pass\n    def log_debug(msg, **kw):\n        pass\n    class ConfigError(Exception):\n        pass\n    class StateError(Exception):\n        pass\n\nSTATE_DIR = Path.home() / \".claude/plugins/agent-swarm/.state\"\n\n\ndef detect_handoff_scope(cwd: Path) -> Path:\n    \"\"\"\n    Detect appropriate handoff scope and return path.\n\n    Priority:\n    1. Git repo root (project-scoped) → <repo>/.claude/HANDOFF.md\n    2. Current working directory (directory-scoped) → <cwd>/.context/HANDOFF.md\n    3. Global scratch (fallback) → ~/.claude/docs/scratch/HANDOFF.md\n    \"\"\"\n    # Check for git root by walking up\n    current = cwd.resolve()\n    while current != current.parent:\n        if (current / \".git\").exists():\n            # Found git repo\n            handoff_dir = current / \".claude\"\n            handoff_dir.mkdir(exist_ok=True)\n            return handoff_dir / \"HANDOFF.md\"\n        current = current.parent\n\n    # No git repo, use current directory\n    if cwd.exists() and cwd.is_dir():\n        handoff_dir = cwd / \".context\"\n        handoff_dir.mkdir(exist_ok=True)\n        return handoff_dir / \"HANDOFF.md\"\n\n    # Fallback to global\n    global_dir = Path.home() / \".claude/docs/scratch\"\n    global_dir.mkdir(parents=True, exist_ok=True)\n    return global_dir / \"HANDOFF.md\"\n\ndef extract_session_info():\n    \"\"\"Extract key information from the current session.\"\"\"\n\n    # Try to load session state\n    session_file = STATE_DIR / \"session.json\"\n    if session_file.exists():\n        try:\n            session = json.loads(session_file.read_text())\n            phase = session.get(\"phase\", \"unknown\")\n            task = session.get(\"task_summary\", \"No task specified\")\n        except (json.JSONDecodeError, IOError) as e:\n            log_debug(f\"Failed to load session file: {e}\")\n            phase = \"unknown\"\n            task = \"No task specified\"\n    else:\n        phase = \"unknown\"\n        task = \"No task specified\"\n\n    # Load metrics if available\n    metrics_file = STATE_DIR / \"metrics.json\"\n    metrics = {}\n    if metrics_file.exists():\n        try:\n            metrics = json.loads(metrics_file.read_text())\n        except Exception as e:\n            log_warning(f\"Caught exception: {e}\")\n\n    return {\n        \"phase\": phase,\n        \"task\": task,\n        \"metrics\": metrics,\n        \"timestamp\": datetime.now().isoformat()\n    }\n\ndef generate_handoff(session_info):\n    \"\"\"Generate handoff document content.\"\"\"\n\n    timestamp = datetime.now().strftime(\"%Y-%m-%d %I:%M%p\")\n\n    handoff = f\"\"\"# Session Handoff - Auto-Generated\n\n**Date:** {timestamp}\n**Status:** 🔄 IN PROGRESS - Pre-compacting checkpoint\n\n---\n\n## 📍 Current State\n\n**Phase:** {session_info['phase']}\n**Task:** {session_info['task']}\n\n---\n\n## ⚠️ IMPORTANT: Context is about to be compacted\n\nThis handoff was automatically generated before conversation compacting.\nKey context from the current session will be condensed.\n\n### What to do:\n\n1. **Review current progress** - Check what's been completed\n2. **Note any blockers** - Document issues encountered\n3. **Update task status** - Mark completed items\n4. **Preserve decisions** - Record important choices made\n\n---\n\n## 📊 Session Metrics\n\n\"\"\"\n\n    metrics = session_info.get(\"metrics\", {})\n    if metrics:\n        tools = metrics.get(\"tools_by_type\", {})\n        if tools:\n            handoff += \"**Tool Usage:**\\n\"\n            for tool, count in sorted(tools.items(), key=lambda x: x[1], reverse=True)[:10]:\n                handoff += f\"- {tool}: {count}\\n\"\n            handoff += \"\\n\"\n\n        efficiency = metrics.get(\"efficiency_score\", 0)\n        if efficiency:\n            handoff += f\"**Efficiency Score:** {efficiency:.1f}%\\n\\n\"\n\n    handoff += \"\"\"---\n\n## 📝 Next Steps\n\n**Before resuming:**\n1. Review this handoff\n2. Update with current progress\n3. Continue from last checkpoint\n\n---\n\n## 🔧 Quick Commands\n\n```bash\n# View current state\ncat ~/.claude/plugins/agent-swarm/HANDOFF.md\n\n# Generate metrics\npython3 ~/.claude/plugins/agent-swarm/scripts/charts.py dashboard\n\n# Check session status\ncat ~/.claude/plugins/agent-swarm/.state/session.json\n```\n\n---\n\n**Auto-generated by pre-compacting hook**\n\"\"\"\n\n    return handoff\n\n\nCOMPACTION_STATE_FILE = STATE_DIR / \"compaction_state.json\"\n\n# Flags that should persist across compaction (same conversation)\nPERSISTENT_FLAGS = [\n    \"user_approved_commit\",\n    \"tests_executed\",\n    \"verify_signal_given\",\n    \"phase\",  # Preserve workflow phase\n    \"workflow_invoked\",  # Workflow should persist across compaction\n]\n\n# Flags that must be CLEARED on compaction (depend on message history)\nMESSAGE_DEPENDENT_FLAGS = [\n    \"classification_given\",\n    \"classification_type\",\n    \"edits_this_response\",\n]\n\ndef save_compaction_state():\n    \"\"\"Save persistent flags before compaction so they survive session reset.\"\"\"\n    session_file = STATE_DIR / \"session.json\"\n    \n    if not session_file.exists():\n        return False\n    \n    try:\n        session = json.loads(session_file.read_text())\n        \n        # Extract only persistent flags that are set\n        compaction_state = {\n            \"saved_at\": datetime.now().isoformat(),\n            \"flags\": {}\n        }\n        \n        for flag in PERSISTENT_FLAGS:\n            if session.get(flag):\n                compaction_state[\"flags\"][flag] = session[flag]\n        \n        # Only write if there are flags to preserve\n        if compaction_state[\"flags\"]:\n            COMPACTION_STATE_FILE.write_text(json.dumps(compaction_state, indent=2))\n            return True\n        \n        return False\n    except (IOError, json.JSONDecodeError):\n        return False  # Silent fallback - state save is best-effort\n\ndef main():\n    \"\"\"Pre-compacting hook entry point.\"\"\"\n\n    # Read hook input\n    try:\n        input_data = json.loads(sys.stdin.read())\n    except json.JSONDecodeError:\n        input_data = {}  # Empty input is acceptable\n\n    # Get working directory for scoped handoffs\n    cwd = Path(input_data.get(\"cwd\", \".\")).resolve()\n\n    # Detect appropriate handoff scope\n    handoff_file = detect_handoff_scope(cwd)\n\n    # Save persistent flags before compaction\n    flags_saved = save_compaction_state()\n\n    # Extract session information\n    session_info = extract_session_info()\n\n    # Generate handoff\n    handoff_content = generate_handoff(session_info)\n\n    # Write handoff file with locking to prevent race conditions\n    try:\n        handoff_file.parent.mkdir(parents=True, exist_ok=True)\n        with open(handoff_file, 'w') as f:\n            fcntl.flock(f.fileno(), fcntl.LOCK_EX)\n            f.write(handoff_content)\n            fcntl.flock(f.fileno(), fcntl.LOCK_UN)\n        message = f\"✓ Handoff auto-generated: {handoff_file}\"\n    except Exception as e:\n        message = f\"⚠️ Failed to write handoff: {e}\"\n\n    # Build status message\n    status_parts = [f\"[PRE-COMPACTING] {message}\"]\n    if flags_saved:\n        status_parts.append(\"   ✓ Approval state preserved for session continuity\")\n    status_parts.append(\"   Context will be compacted - review handoff for preserved state\")\n\n    # Return result\n    output = {\n        \"systemMessage\": \"\\n\".join(status_parts)\n    }\n\n    print(json.dumps(output))\n\nif __name__ == \"__main__\":\n    main()\n",
        "hooks/router-event-hook.py": "#!/usr/bin/env python3\n\"\"\"\nHook to intercept router__ pseudo-tools and execute via Python.\n\nThis hook intercepts tool calls to router__request, router__poll, etc.\nand executes them via workflow_client, bypassing MCP permissions.\n\"\"\"\nimport json\nimport os\nimport sys\n\n# Add lib to path\nsys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'lib'))\n\nfrom workflow_client import call_tool, list_tools, generate_correlation_id, _log  # noqa: E402\n\n\ndef main():\n    # Read hook input from stdin\n    hook_input = json.loads(sys.stdin.read())\n\n    tool_name = hook_input.get(\"tool_name\", \"\")\n    tool_input = hook_input.get(\"tool_input\", {})\n\n    # Only handle router__ pseudo-tools\n    if not tool_name.startswith(\"mcp__router__router__\"):\n        # Allow other tools to proceed normally\n        print(json.dumps({\"decision\": \"allow\"}))\n        return\n\n    # Extract the actual pseudo-tool name\n    pseudo_tool = tool_name.replace(\"mcp__router__router__\", \"router__\")\n    _log(\"router-event-hook\", f\"Intercepting {pseudo_tool}\")\n\n    try:\n        if pseudo_tool == \"router__request\":\n            # Async tool request\n            target_tool = tool_input.get(\"tool\", \"\")\n            args = tool_input.get(\"args\", {})\n            correlation_id = generate_correlation_id()\n\n            # Execute the tool call via socket\n            result = call_tool(target_tool, args)\n\n            # Return result with correlation ID\n            response = {\n                \"correlation_id\": correlation_id,\n                \"status\": \"completed\",\n                \"result\": result\n            }\n\n            print(json.dumps({\n                \"decision\": \"block\",\n                \"message\": json.dumps(response)\n            }))\n\n        elif pseudo_tool == \"router__poll\":\n            correlation_id = tool_input.get(\"correlation_id\", \"\")\n            # For now, return empty (true async would check queue)\n            print(json.dumps({\n                \"decision\": \"block\",\n                \"message\": json.dumps({\"correlation_id\": correlation_id, \"status\": \"pending\"})\n            }))\n\n        elif pseudo_tool == \"router__publish\":\n            topic = tool_input.get(\"topic\", \"\")\n            _ = tool_input.get(\"data\", {})\n            correlation_id = generate_correlation_id()\n\n            # Would publish to event bus here\n            print(json.dumps({\n                \"decision\": \"block\",\n                \"message\": json.dumps({\n                    \"status\": \"published\",\n                    \"topic\": topic,\n                    \"correlation_id\": correlation_id\n                })\n            }))\n\n        elif pseudo_tool == \"router__list_tools\":\n            tools = list_tools()\n            print(json.dumps({\n                \"decision\": \"block\",\n                \"message\": json.dumps({\"tools\": tools})\n            }))\n\n        else:\n            # Unknown pseudo-tool, allow it through\n            print(json.dumps({\"decision\": \"allow\"}))\n\n    except Exception as e:\n        _log(\"router-event-hook\", f\"ERROR: {e}\", \"ERROR\")\n        print(json.dumps({\n            \"decision\": \"block\",\n            \"message\": json.dumps({\"error\": str(e)})\n        }))\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "hooks/session-end.py": "#!/usr/bin/env python3\n\"\"\"\nSession End Hook - Auto-generate metrics dashboard & prompt memory capture\n\nAutomatically generates the metrics dashboard at the end of each session\nand prompts the agent to write important learnings to memory.\n\"\"\"\n\nimport json\nimport re\nimport sys\nimport subprocess\nimport signal\nimport glob\nfrom datetime import datetime\nfrom pathlib import Path\n\n# Add plugin root and lib to path for imports\nsys.path.insert(0, str(Path(__file__).parent.parent))\nsys.path.insert(0, str(Path(__file__).parent.parent / \"lib\"))\nsys.path.insert(0, str(Path(__file__).parent.parent / \"context\"))\nfrom lib.stores.compression import compress_old_sessions  # noqa: E402\nfrom memory import EpisodeStore, trigger_distillation  # noqa: E402\n\nSCRIPTS_DIR = Path(__file__).parent.parent / \"scripts\"\nCHARTS_SCRIPT = SCRIPTS_DIR / \"charts.py\"\nSTATE_DIR = Path(__file__).parent.parent / \".state\"\nSESSION_FILE = STATE_DIR / \"session.json\"\n\n\ndef get_projects_dir() -> Path:\n    \"\"\"Return the Claude projects directory path.\"\"\"\n    return Path.home() / \".claude\" / \"projects\"\n\n\ndef get_context_dir() -> Path:\n    \"\"\"Return the .context directory for the plugin.\"\"\"\n    return Path(__file__).parent.parent / \".context\"\n\n\ndef extract_learnings_from_conversation(jsonl_lines: list[str]) -> list[str]:\n    \"\"\"Extract LEARNING: tags from assistant messages in conversation JSONL.\n\n    Args:\n        jsonl_lines: List of JSONL strings, each representing a conversation message\n\n    Returns:\n        List of learning descriptions extracted from assistant messages\n    \"\"\"\n    learnings = []\n    pattern = r\"LEARNING:\\s*(.+?)(?:\\n|$)\"\n\n    for line in jsonl_lines:\n        try:\n            data = json.loads(line)\n            # Only process assistant messages\n            if data.get(\"type\") != \"assistant\":\n                continue\n\n            # Get content from message\n            message = data.get(\"message\", {})\n            content = message.get(\"content\", \"\")\n            if not content:\n                continue\n\n            # Extract LEARNING: tags (case insensitive)\n            matches = re.findall(pattern, content, re.IGNORECASE)\n            learnings.extend(matches)\n        except (json.JSONDecodeError, AttributeError, KeyError):\n            continue\n\n    return learnings\n\n\ndef find_conversation_file(session_id: str) -> Path | None:\n    \"\"\"Find the conversation JSONL file for a given session ID.\n\n    Searches in ~/.claude/projects/*/{session_id}.jsonl\n\n    Args:\n        session_id: The session ID to search for\n\n    Returns:\n        Path to the JSONL file if found, None otherwise\n    \"\"\"\n    projects_dir = get_projects_dir()\n    if not projects_dir.exists():\n        return None\n\n    # Search for session file in any project directory\n    pattern = str(projects_dir / \"*\" / f\"{session_id}.jsonl\")\n    matches = glob.glob(pattern)\n\n    if matches:\n        return Path(matches[0])\n    return None\n\n\ndef log_main_agent_learnings(learnings: list[str], source: str) -> None:\n    \"\"\"Append learnings from main agent to EPISODES.md.\n\n    Creates .context directory and EPISODES.md if they don't exist.\n\n    Args:\n        learnings: List of learning descriptions\n        source: Source identifier (e.g., \"main-agent\")\n    \"\"\"\n    if not learnings:\n        return\n\n    context_dir = get_context_dir()\n    context_dir.mkdir(parents=True, exist_ok=True)\n\n    episodes_file = context_dir / \"EPISODES.md\"\n\n    # Build episode entry\n    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n    entry_lines = [\n        f\"\\n## Session: {timestamp} - Main Agent Learning Capture\",\n        f\"- **Source**: {source}\",\n        \"- **Outcome**: success\",\n        \"- **Learnings**:\",\n    ]\n    for learning in learnings:\n        entry_lines.append(f\"  - {learning}\")\n    entry_lines.append(\"\")\n\n    # Append to file\n    with open(episodes_file, \"a\") as f:\n        f.write(\"\\n\".join(entry_lines))\n\n\ndef capture_main_agent_learnings(session_id: str) -> int:\n    \"\"\"Capture LEARNING: tags from main agent conversation and log to EPISODES.md.\n\n    Args:\n        session_id: The session ID to process\n\n    Returns:\n        Number of learnings captured\n    \"\"\"\n    try:\n        # Find the conversation file\n        conv_file = find_conversation_file(session_id)\n        if not conv_file or not conv_file.exists():\n            return 0\n\n        # Read JSONL lines\n        jsonl_lines = conv_file.read_text().strip().split(\"\\n\")\n\n        # Extract learnings\n        learnings = extract_learnings_from_conversation(jsonl_lines)\n\n        if learnings:\n            log_main_agent_learnings(learnings, f\"main-agent:{session_id[:8]}\")\n\n        return len(learnings)\n    except Exception:\n        return 0  # Don't fail hook on learning capture errors\n\n\ndef generate_dashboard():\n    \"\"\"Generate the metrics dashboard.\"\"\"\n    try:\n        # First capture a snapshot of current metrics\n        snapshot_result = subprocess.run(\n            [\"poetry\", \"run\", \"python\", str(CHARTS_SCRIPT), \"snapshot\"],\n            capture_output=True,\n            text=True,\n            timeout=10,\n        )\n\n        # Log snapshot result (don't fail if it errors)\n        if snapshot_result.returncode != 0:\n            print(\n                f\"\\u26a0\\ufe0f Snapshot capture failed: {snapshot_result.stderr}\", file=sys.stderr\n            )\n\n        # Then generate the full dashboard\n        result = subprocess.run(\n            [\"poetry\", \"run\", \"python\", str(CHARTS_SCRIPT), \"all\"],\n            capture_output=True,\n            text=True,\n            timeout=30,\n        )\n\n        if result.returncode == 0:\n            # Extract dashboard path from output\n            for line in result.stdout.split(\"\\n\"):\n                if \"dashboard.html\" in line and \"file://\" in line:\n                    return {\n                        \"success\": True,\n                        \"path\": line.strip(),\n                        \"message\": \"\\u2705 Metrics dashboard generated automatically\",\n                    }\n\n            return {\"success\": True, \"message\": \"\\u2705 Metrics dashboard generated\"}\n        else:\n            return {\"success\": False, \"error\": result.stderr}\n\n    except subprocess.TimeoutExpired:\n        return {\"success\": False, \"error\": \"Dashboard generation timed out\"}\n    except Exception as e:\n        return {\"success\": False, \"error\": str(e)}\n\n\ndef check_memory_write_needed(input_data):\n    \"\"\"Memory write is ALWAYS required at session end.\"\"\"\n\n    # Memory write is mandatory for all sessions\n    # Even if no code was changed, conversations have context worth preserving\n    if True:  # Always true - memory write always required\n        return {\n            \"needed\": True,\n            \"message\": (\n                \"\\n\\n============================================================\\n\"\n                \"\\ud83d\\udcdd MEMORY CAPTURE REQUIRED\\n\"\n                \"============================================================\\n\"\n                \"Before ending this session, you MUST write learnings to memory.\\n\"\n                \"Even brief conversations contain valuable context.\\n\"\n                \"\\n\"\n                \"Tool: mcp__plugin_serena_serena__write_memory\\n\"\n                \"\\n\"\n                \"What to capture:\\n\"\n                \"  \\u2022 Key decisions made and rationale\\n\"\n                \"  \\u2022 Gotchas/issues encountered and solutions\\n\"\n                \"  \\u2022 Architecture changes or patterns introduced\\n\"\n                \"  \\u2022 Important context for future sessions\\n\"\n                \"  \\u2022 Even simple Q&A if it reveals codebase details\\n\"\n                \"\\n\"\n                \"Example:\\n\"\n                \"  write_memory(\\n\"\n                \"      memory_file_name='<feature>-<date>',\\n\"\n                \"      content='# Session Summary\\\\n\\\\n'\\n\"\n                \"              '<what was done>\\\\n'\\n\"\n                \"              '<key decisions>\\\\n'\\n\"\n                \"              '<gotchas and solutions>'\\n\"\n                \"  )\\n\"\n                \"============================================================\"\n            ),\n        }\n\n    return {\"needed\": False}\n\n\ndef compress_old_session_files():\n    \"\"\"Compress session JSONL files older than 24 hours.\"\"\"\n    sessions_dir = STATE_DIR / \"sessions\"\n    if not sessions_dir.exists():\n        return {\"compressed\": 0}\n\n    try:\n        count = compress_old_sessions(sessions_dir, max_age_hours=24)\n        return {\"compressed\": count}\n    except Exception as e:\n        return {\"error\": str(e)}\n\n\ndef check_and_distill(scope_path: Path, threshold: int = 10, timeout_seconds: int = 5) -> dict:\n    \"\"\"\n    Check episode count and trigger distillation if threshold exceeded.\n\n    Args:\n        scope_path: Path to the project/scope directory\n        threshold: Minimum episode count to trigger distillation (default: 10)\n        timeout_seconds: Maximum time to wait for distillation (default: 5)\n\n    Returns:\n        dict with keys:\n            - distilled: bool - whether distillation was performed\n            - episode_count: int - number of episodes found\n            - pattern_count: int - number of patterns after distillation (if distilled)\n            - error: str - error message if failed (optional)\n    \"\"\"\n    try:\n        store = EpisodeStore(scope_path)\n        episodes = store.get_episodes()\n        episode_count = len(episodes)\n\n        if episode_count < threshold:\n            return {\"distilled\": False, \"episode_count\": episode_count}\n\n        # Set timeout for distillation (Unix only - SIGALRM not available on Windows)\n        use_alarm = hasattr(signal, \"SIGALRM\")\n        old_handler = None\n\n        if use_alarm:\n\n            def timeout_handler(signum, frame):\n                raise TimeoutError(\"Distillation timed out\")\n\n            old_handler = signal.signal(signal.SIGALRM, timeout_handler)\n            signal.alarm(timeout_seconds)\n\n        try:\n            memory = trigger_distillation(scope_path)\n            if use_alarm:\n                signal.alarm(0)  # Cancel alarm\n            return {\n                \"distilled\": True,\n                \"episode_count\": episode_count,\n                \"pattern_count\": len(memory.patterns),\n            }\n        except TimeoutError as e:\n            return {\"distilled\": False, \"episode_count\": episode_count, \"error\": str(e)}\n        finally:\n            if use_alarm and old_handler is not None:\n                signal.signal(signal.SIGALRM, old_handler)\n                signal.alarm(0)\n\n    except Exception as e:\n        return {\"distilled\": False, \"episode_count\": 0, \"error\": str(e)}\n\n\ndef main():\n    \"\"\"Session end hook entry point.\"\"\"\n\n    # Read session data from stdin (if any)\n    try:\n        input_data = json.loads(sys.stdin.read())\n    except json.JSONDecodeError:\n        input_data = {}\n\n    # Generate dashboard\n    result = generate_dashboard()\n\n    # Check if memory write is recommended\n    memory_check = check_memory_write_needed(input_data)\n\n    # Build output message\n    message = (\n        result.get(\"message\", \"\")\n        or f\"\\u26a0\\ufe0f Dashboard generation failed: {result.get('error', 'Unknown error')}\"\n    )\n\n    if result.get(\"path\"):\n        message += f\"\\n   {result['path']}\"\n\n    # Capture LEARNING: tags from main agent conversation\n    session_id = input_data.get(\"sessionId\", \"\")\n    if session_id:\n        learnings_captured = capture_main_agent_learnings(session_id)\n        if learnings_captured > 0:\n            message += f\"\\n\\ud83d\\udcdd Captured {learnings_captured} learning(s) from main agent\"\n\n    # Compress old session files\n    compression_result = compress_old_session_files()\n    if compression_result.get(\"compressed\", 0) > 0:\n        message += (\n            f\"\\n\\ud83d\\udce6 Compressed {compression_result['compressed']} old session file(s)\"\n        )\n\n    # Auto-distillation check\n    distill_result = check_and_distill(Path.cwd())\n    if distill_result.get(\"distilled\"):\n        message += f\"\\n\\ud83d\\udcdd Distilled {distill_result['episode_count']} episodes into {distill_result['pattern_count']} patterns\"\n    elif distill_result.get(\"error\"):\n        message += f\"\\n\\u26a0\\ufe0f Auto-distillation failed: {distill_result['error']}\"\n\n    # Append memory suggestion if needed\n    if memory_check.get(\"needed\"):\n        message += memory_check[\"message\"]\n\n    # Return result\n    output = {\"systemMessage\": message}\n\n    print(json.dumps(output))\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "hooks/session-init.py": "#!/usr/bin/env python3\n\"\"\"Session initialization hook - consolidated session startup.\n\nReplaces: work_seeker.py, session-start.py\nEvent: SessionStart\nReturns: {\"additionalContext\": \"...permission info...\"}\n\nQueries the permission store for active workflow permissions and injects\nthem into the session context so the agent knows its constraints.\n\"\"\"\nimport json\nimport sys\nfrom pathlib import Path\n\n# Add lib to path\nlib_dir = Path(__file__).parent.parent / \"lib\"\nsys.path.insert(0, str(lib_dir))\n\ntry:\n    from permission_query import get_permissions, get_active_workflow_id\nexcept ImportError:\n    # Fallback if permission_query not available\n    def get_active_workflow_id():\n        return None\n    def get_permissions(workflow_id=None):\n        return None\n\n\ndef format_permissions_for_prompt(perms) -> str:\n    \"\"\"Format PermissionStore as human-readable prompt context.\n\n    Args:\n        perms: PermissionStore instance or None\n\n    Returns:\n        Formatted string for injection into prompt context\n    \"\"\"\n    if not perms:\n        return \"No active workflow - standard permissions apply.\"\n\n    lines = [\n        f\"Active workflow: {perms.workflow_type} ({perms.workflow_id})\",\n        f\"Current phase: {perms.phase}\",\n    ]\n\n    if perms.phase_permissions:\n        pp = perms.phase_permissions\n\n        if pp.blocked_tools:\n            # Show first 10 blocked tools to avoid context bloat\n            tools = list(pp.blocked_tools)[:10]\n            if len(pp.blocked_tools) > 10:\n                tools.append(f\"...and {len(pp.blocked_tools) - 10} more\")\n            lines.append(f\"Blocked tools: {', '.join(tools)}\")\n\n        if pp.blocked_commands:\n            cmds = list(pp.blocked_commands)[:5]\n            if len(pp.blocked_commands) > 5:\n                cmds.append(f\"...and {len(pp.blocked_commands) - 5} more\")\n            lines.append(f\"Blocked commands: {', '.join(cmds)}\")\n\n        if pp.allowed_categories:\n            lines.append(f\"Allowed categories: {', '.join(pp.allowed_categories)}\")\n\n        if pp.required_tools:\n            lines.append(f\"Required tools: {', '.join(pp.required_tools)}\")\n\n    if perms.is_subagent:\n        lines.append(\"Running as subagent - restricted permissions apply\")\n\n    return \"\\n\".join(lines)\n\n\ndef main():\n    \"\"\"Main entry point for SessionStart hook.\"\"\"\n    # Read hook input from stdin\n    try:\n        _input_data = json.loads(sys.stdin.read())  # noqa: F841\n    except json.JSONDecodeError:\n        pass  # Input parsed but not used yet\n\n    # Get current permissions from active workflow\n    workflow_id = get_active_workflow_id()\n    perms = get_permissions(workflow_id) if workflow_id else None\n\n    # Format for prompt injection\n    context = format_permissions_for_prompt(perms)\n\n    # Build result - SessionStart uses additionalContext for prompt injection\n    result = {\n        \"additionalContext\": context\n    }\n\n    print(json.dumps(result))\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "hooks/session-start.py": "#!/usr/bin/env python3\n\"\"\"\nSession Start Hook - Auto-search episodic memory & reset counters & inject capabilities\n\nAutomatically searches episodic memory at the start of each session\nto recover relevant context from past conversations.\n\nAlso resets enforcement counters for the new conversation.\n\nRuns inventory.py to inject available MCP servers, skills, and capabilities.\n\"\"\"\n\nimport json\nimport re\nimport sys\nimport subprocess\nimport time\nfrom pathlib import Path\n\n# Add lib and context to path\nplugin_dir = Path(__file__).parent.parent\nlib_dir = plugin_dir / \"lib\"\ncontext_dir = plugin_dir / \"context\"\nsys.path.insert(0, str(lib_dir))\nsys.path.insert(0, str(context_dir))\n\ntry:\n    from hook_logging import log_error, log_warning, log_info, log_debug, ConfigError, StateError\nexcept ImportError:\n    # Fallback: define minimal logging functions\n    def log_error(msg, **kw):\n        pass\n    def log_warning(msg, **kw):\n        pass\n    def log_info(msg, **kw):\n        pass\n    def log_debug(msg, **kw):\n        pass\n    class ConfigError(Exception):\n        pass\n    class StateError(Exception):\n        pass\n\ntry:\n    from workflow_client import workflow_get_state, workflow_set_state, agent_set_state\nexcept ImportError:\n    # Fallback if workflow_client not available\n    def workflow_get_state(workflow_id: str) -> dict | None:\n        return None\n    def workflow_set_state(workflow_id: str, state: dict) -> dict | None:\n        return None\n    def agent_set_state(agent_id: str, state: dict) -> dict | None:\n        return None\n\ntry:\n    from resolver import resolve_context\nexcept ImportError:\n    resolve_context = None\n\ntry:\n    from project_root import find_project_root, find_recent_handoffs\nexcept ImportError:\n    find_project_root = None\n    find_recent_handoffs = None\n\n\n# Maximum age for HANDOFF.md files to be included (in hours)\nHANDOFF_MAX_AGE_HOURS = 48\n\n\ndef load_memory_patterns(scope_path: Path) -> list[dict]:\n    \"\"\"Load patterns from .context/MEMORY.md file.\n    \n    Parses the markdown format and returns structured pattern data.\n    \n    Args:\n        scope_path: Path to the scope directory containing .context/MEMORY.md\n        \n    Returns:\n        List of pattern dicts with keys: content, category, confidence, last_reinforced\n    \"\"\"\n    memory_file = scope_path / \".context\" / \"MEMORY.md\"\n    \n    if not memory_file.exists():\n        return []\n    \n    try:\n        content = memory_file.read_text()\n    except Exception:\n        return []\n    \n    patterns = []\n    current_category = None\n    \n    # Map section headers to category names\n    category_map = {\n        \"patterns observed\": \"pattern\",\n        \"pitfalls discovered\": \"pitfall\",\n        \"preferences inferred\": \"preference\",\n        \"effective approaches\": \"approach\",\n    }\n    \n    content_lines = content.split(\"\\n\")\n    i = 0\n    while i < len(content_lines):\n        line = content_lines[i]\n        \n        # Check for section header\n        if line.startswith(\"## \"):\n            header = line[3:].strip().lower()\n            current_category = category_map.get(header)\n            i += 1\n            continue\n        \n        # Check for pattern entry (starts with \"- \")\n        if line.startswith(\"- \") and current_category:\n            pattern_content = line[2:].strip()\n            \n            # Look for confidence line on next line\n            if i + 1 < len(content_lines):\n                next_line = content_lines[i + 1].strip()\n                confidence_match = re.match(\n                    r\"Confidence:\\s*(high|medium|low)\\s*\\|\\s*Last reinforced:\\s*(\\d{4}-\\d{2}-\\d{2})\",\n                    next_line,\n                    re.IGNORECASE\n                )\n                \n                if confidence_match:\n                    patterns.append({\n                        \"content\": pattern_content,\n                        \"category\": current_category,\n                        \"confidence\": confidence_match.group(1).lower(),\n                        \"last_reinforced\": confidence_match.group(2),\n                    })\n                    i += 2\n                    continue\n        \n        i += 1\n    \n    return patterns\n\n\ndef format_memory_patterns(patterns: list[dict], max_patterns: int = 5) -> str:\n    \"\"\"Format patterns for display in session context.\n    \n    Groups patterns by category and formats them for readable output.\n    \n    Args:\n        patterns: List of pattern dicts from load_memory_patterns\n        max_patterns: Maximum number of patterns to include\n        \n    Returns:\n        Formatted string for display, or empty string if no patterns\n    \"\"\"\n    if not patterns:\n        return \"\"\n    \n    # Group by category\n    by_category: dict[str, list[dict]] = {}\n    for p in patterns:\n        cat = p[\"category\"]\n        if cat not in by_category:\n            by_category[cat] = []\n        by_category[cat].append(p)\n    \n    # Category display names\n    category_names = {\n        \"pattern\": \"Patterns\",\n        \"pitfall\": \"Pitfalls\",\n        \"preference\": \"Preferences\",\n        \"approach\": \"Approaches\",\n    }\n    \n    # Format output, limiting total patterns\n    output_lines = [\"**Learned Patterns from Memory:**\"]\n    pattern_count = 0\n    \n    for cat_key in [\"pitfall\", \"pattern\", \"approach\", \"preference\"]:\n        if cat_key not in by_category:\n            continue\n        \n        cat_patterns = by_category[cat_key]\n        cat_name = category_names.get(cat_key, cat_key.title())\n        \n        for p in cat_patterns:\n            if pattern_count >= max_patterns:\n                break\n            \n            conf = p[\"confidence\"]\n            conf_indicator = \"!\" if conf == \"high\" else \"~\" if conf == \"medium\" else \"?\"\n            output_lines.append(f\"  {conf_indicator} [{cat_name}] {p['content']}\")\n            pattern_count += 1\n        \n        if pattern_count >= max_patterns:\n            break\n    \n    if pattern_count == 0:\n        return \"\"\n    \n    return \"\\n\".join(output_lines)\n\n\ndef _detect_hierarchy_level(path: Path, working_dir: Path, user_dir: Path) -> str:\n    \"\"\"Detect what level of hierarchy this path represents.\"\"\"\n    if path == user_dir:\n        return \"user\"\n    \n    # Check for .git to identify repo root\n    if (path / \".git\").exists():\n        return \"repo\"\n    \n    # Check for workspace (directory containing multiple repos)\n    try:\n        git_children = sum(1 for child in path.iterdir() \n                          if child.is_dir() and (child / \".git\").exists())\n        if git_children >= 2:\n            return \"workspace\"\n    except PermissionError:\n        pass\n    \n    # Default to component for anything else\n    return \"component\"\n\n\ndef load_context_hierarchy(working_dir: Path, user_dir: Path | None = None) -> list[dict]:\n    \"\"\"Load context from all hierarchy levels.\n    \n    Walks up from working_dir to user_dir, loading CONTEXT.md, MEMORY.md,\n    and HANDOFF.md (if < 48 hours old) from each .context/ directory.\n    \n    Args:\n        working_dir: Current working directory\n        user_dir: User's .claude directory (defaults to ~/.claude)\n        \n    Returns:\n        List of context dicts with keys: level, path, content, memory, handoff\n    \"\"\"\n    if user_dir is None:\n        user_dir = Path.home() / \".claude\"\n    \n    hierarchy = []\n    current = working_dir.resolve()\n    filesystem_root = Path(current.anchor)\n    \n    # Track visited to avoid duplicates\n    visited = set()\n    \n    # Walk up from working directory\n    while current != filesystem_root:\n        if current in visited:\n            current = current.parent\n            continue\n        visited.add(current)\n        \n        context_dir = current / \".context\"\n        if context_dir.exists():\n            entry = {\n                \"level\": _detect_hierarchy_level(current, working_dir, user_dir),\n                \"path\": str(current),\n                \"content\": None,\n                \"memory\": None,\n                \"handoff\": None,\n            }\n            \n            # Load CONTEXT.md\n            context_file = context_dir / \"CONTEXT.md\"\n            if context_file.exists():\n                try:\n                    entry[\"content\"] = context_file.read_text()\n                except Exception:\n                    pass\n            \n            # Load MEMORY.md\n            memory_file = context_dir / \"MEMORY.md\"\n            if memory_file.exists():\n                try:\n                    entry[\"memory\"] = memory_file.read_text()\n                except Exception:\n                    pass\n            \n            # Load HANDOFF.md (only if < 48 hours old)\n            handoff_file = context_dir / \"HANDOFF.md\"\n            if handoff_file.exists():\n                try:\n                    mtime = handoff_file.stat().st_mtime\n                    age_hours = (time.time() - mtime) / 3600\n                    if age_hours <= HANDOFF_MAX_AGE_HOURS:\n                        entry[\"handoff\"] = handoff_file.read_text()\n                except Exception:\n                    pass\n            \n            hierarchy.append(entry)\n        \n        current = current.parent\n    \n    # Add user-level context\n    if user_dir not in visited:\n        user_context_dir = user_dir / \".context\"\n        if user_context_dir.exists() or user_dir.exists():\n            entry = {\n                \"level\": \"user\",\n                \"path\": str(user_dir),\n                \"content\": None,\n                \"memory\": None,\n                \"handoff\": None,\n            }\n            \n            # Try .context/CONTEXT.md first, then CONTEXT.md in user_dir\n            for context_path in [user_context_dir / \"CONTEXT.md\", user_dir / \"CONTEXT.md\"]:\n                if context_path.exists():\n                    try:\n                        entry[\"content\"] = context_path.read_text()\n                        break\n                    except Exception:\n                        pass\n            \n            if entry[\"content\"] or entry[\"memory\"] or entry[\"handoff\"]:\n                hierarchy.append(entry)\n            elif user_context_dir.exists():\n                hierarchy.append(entry)\n    \n    # Reverse so general comes first (user -> repo -> component)\n    hierarchy.reverse()\n    \n    return hierarchy\n\n\ndef format_hierarchy_context(hierarchy: list[dict], max_chars: int = 3000) -> str:\n    \"\"\"Format hierarchical context with scope tags.\n    \n    Args:\n        hierarchy: List of context dicts from load_context_hierarchy\n        max_chars: Maximum characters for output\n        \n    Returns:\n        Formatted string with scope-tagged context lines\n    \"\"\"\n    lines = []\n    \n    for ctx in hierarchy:\n        level = ctx.get(\"level\", \"unknown\")\n        path = ctx.get(\"path\", \"\")\n        content = ctx.get(\"content\", \"\")\n        \n        if not content:\n            continue\n        \n        # Build scope tag\n        if level == \"user\":\n            tag = \"[user]\"\n        elif level == \"repo\":\n            # Extract repo name from path\n            repo_name = Path(path).name\n            tag = f\"[repo:{repo_name}]\"\n        elif level == \"workspace\":\n            ws_name = Path(path).name\n            tag = f\"[workspace:{ws_name}]\"\n        elif level == \"component\":\n            comp_name = Path(path).name\n            tag = f\"[component:{comp_name}]\"\n        else:\n            tag = f\"[{level}]\"\n        \n        # Extract key lines from content (first meaningful lines)\n        content_lines = [line.strip() for line in content.split(\"\\n\") \n                        if line.strip() and not line.startswith(\"#\")]\n        \n        # Add up to 3 lines per scope\n        for line in content_lines[:3]:\n            # Truncate long lines\n            if len(line) > 100:\n                line = line[:97] + \"...\"\n            lines.append(f\"{tag} {line}\")\n    \n    result = \"\\n\".join(lines)\n    \n    if len(result) > max_chars:\n        result = result[:max_chars - 3] + \"...\"\n    \n    return result\n\n\ndef load_iterate_state() -> dict:\n    \"\"\"Load iterate workflow state from state server.\"\"\"\n    state = workflow_get_state(\"iterate\")\n    return state if state else {}\n\n\ndef get_session_context(working_dir: Path, max_chars: int = 2000) -> str:\n    \"\"\"Get hierarchical context for session start.\n\n    Args:\n        working_dir: Current working directory\n        max_chars: Maximum characters for context output\n\n    Returns:\n        Formatted context string or empty string on failure\n    \"\"\"\n    if resolve_context is None:\n        return \"\"\n\n    try:\n        ctx = resolve_context(working_dir)\n        if not ctx.layers:\n            return \"\"\n\n        # Get priority sections for session context\n        priority_sections = [\"boundaries\", \"conventions\", \"patterns\", \"pitfalls\"]\n        sections = ctx.get_sections(priority_sections)\n\n        # Format as compact output\n        parts = []\n        for name in priority_sections:\n            content = sections.get(name)\n            if content:\n                # Truncate long sections\n                truncated = content[:400] + \"...\" if len(content) > 400 else content\n                parts.append(f\"**{name.title()}**: {truncated}\")\n\n        result = \"\\n\\n\".join(parts)\n        return result[:max_chars] if len(result) > max_chars else result\n\n    except Exception as e:\n        log_debug(f\"Context resolution failed: {e}\")\n        return \"\"\n\n\ndef reset_enforcement_counters(agent_id: str | None = None):\n    \"\"\"Reset enforcement counters but preserve workflow state for new conversation.\n\n    Args:\n        agent_id: If provided, this is a subagent - inherit phase from orchestrator.\n    \"\"\"\n    # Use absolute path to match pre-compacting.py\n    state_dir = Path.home() / \".claude/plugins/agent-swarm/.state\"\n    # DISABLED: Session state file no longer used\n    # state_file = state_dir / \"session.json\"\n    compaction_state_file = state_dir / \"compaction_state.json\"\n\n    try:\n        # Check for compaction state (preserved across context compaction)\n        compaction_flags = {}\n        if compaction_state_file.exists():\n            try:\n                compaction_data = json.loads(compaction_state_file.read_text())\n                compaction_flags = compaction_data.get(\"flags\", {})\n                # Delete after reading - one-time use\n                compaction_state_file.unlink()\n            except (json.JSONDecodeError, IOError) as e:\n                log_warning(f\"Caught exception: {e}\")\n\n        # Initialize fresh session state for counters\n        # NOTE: blocked_at and mcp_counts are intentionally NOT included\n        # This clears any blocking state from previous sessions\n        state = {\n            \"last_phase\": None,\n            \"last_tool_time\": None,\n            \"signature_change_reminders\": [],\n            \"files_read\": [],\n            \"read_count\": 0,\n            \"files_edited_this_session\": [],\n            \"phase\": None,  # Will be set below if subagent\n            \"search_count\": 0,\n            \"edits_this_response\": 0,\n            \"memory_search_suggested\": 1,\n            \"mcp_counts\": {},  # Reset MCP tool counts\n            \"classification_given\": False,  # Reset classification state\n            \"classification_type\": None,\n            \"workflow_invoked\": False,  # Reset workflow state\n        }\n        # NOTE: blocked_at is NOT set, which clears it\n\n        # Restore flags preserved from compaction\n        state.update(compaction_flags)\n\n        # Set project_root so router can auto-activate Serena for the correct project\n        if find_project_root is not None:\n            try:\n                state[\"project_root\"] = str(find_project_root(Path.cwd()))\n            except Exception:\n                pass\n\n        # If subagent, inherit phase from orchestrator and set per-agent state\n        if agent_id:\n            iterate_state = load_iterate_state()\n            phase = iterate_state.get(\"phase\")\n            if phase:\n                state[\"phase\"] = phase\n            # Store state keyed by agent_id for subagent-specific queries\n            agent_set_state(agent_id, state)\n\n        # Write session state to state server (global session for main agent)\n        workflow_set_state(\"session\", state)\n\n        return True\n    except Exception as e:\n        log_warning(f\"Caught Exception: {e}\")  # Fail silently, not critical\n\n    return False\n\ndef run_inventory():\n    \"\"\"Run inventory.py to discover available capabilities.\"\"\"\n    try:\n        inventory_path = Path(__file__).parent.parent / \"scripts\" / \"inventory.py\"\n        if not inventory_path.exists():\n            return None\n\n        result = subprocess.run(\n            [\"python3\", str(inventory_path), \"all\"],\n            capture_output=True,\n            text=True,\n            timeout=10\n        )\n\n        if result.returncode == 0:\n            return result.stdout\n        return None\n\n    except Exception:\n        return None\n\ndef list_serena_memories():\n    \"\"\"List available Serena memories for the current project.\"\"\"\n    try:\n        memories_dir = Path.home() / \".claude/plugins/agent-swarm/.serena/memories\"\n        if not memories_dir.exists():\n            return []\n        return [f.stem for f in memories_dir.glob(\"*.md\")]\n    except Exception:\n        return []\n\n\ndef find_relevant_memories(query: str, memories: list[str], max_count: int = 3) -> list[str]:\n    \"\"\"Find memories that match query keywords.\n    \n    Args:\n        query: Search query string\n        memories: List of memory filenames (without extension)\n        max_count: Maximum number of memories to return\n        \n    Returns:\n        List of matching memory names, sorted by relevance\n    \"\"\"\n    if not query or not memories:\n        return []\n    \n    # Split query into keywords, normalize\n    keywords = [k.lower() for k in query.split() if len(k) > 2]\n    if not keywords:\n        return []\n    \n    # Score each memory by keyword matches\n    scored = []\n    for memory in memories:\n        memory_lower = memory.lower()\n        score = sum(1 for kw in keywords if kw in memory_lower)\n        if score > 0:\n            scored.append((memory, score))\n    \n    # Sort by score descending, take top N\n    scored.sort(key=lambda x: x[1], reverse=True)\n    return [m[0] for m in scored[:max_count]]\n\n\ndef read_memory_snippets(memory_names: list[str], max_chars_per_memory: int = 500) -> str:\n    \"\"\"Read content snippets from memory files.\n    \n    Args:\n        memory_names: List of memory filenames (without extension)\n        max_chars_per_memory: Maximum characters to read per memory\n        \n    Returns:\n        Formatted string with memory snippets\n    \"\"\"\n    if not memory_names:\n        return \"\"\n    \n    memories_dir = Path.home() / \".claude/plugins/agent-swarm/.serena/memories\"\n    snippets = []\n    \n    for name in memory_names:\n        file_path = memories_dir / f\"{name}.md\"\n        try:\n            if not file_path.exists():\n                continue\n            full_content = file_path.read_text()\n            content = full_content[:max_chars_per_memory]\n            # Add ellipsis if truncated\n            if len(full_content) > max_chars_per_memory:\n                content += \"...\"\n            snippets.append(f\"**{name}**\\n> {content.replace(chr(10), chr(10) + '> ')}\")\n        except Exception:\n            continue\n    \n    return \"\\n\\n\".join(snippets)\n\n\ndef search_episodic_memory(query_terms: str, limit: int = 3) -> list:\n    \"\"\"Search episodic memory using the CLI for past conversation snippets.\n\n    Uses --text mode for speed (no embedding model loading required).\n\n    Args:\n        query_terms: Search query\n        limit: Max number of results (default 3)\n\n    Returns:\n        List of dicts with 'date', 'snippet' keys, or empty list on failure\n    \"\"\"\n    try:\n        # Path to episodic-memory CLI\n        episodic_root = Path.home() / \".claude/plugins/cache/superpowers-marketplace/episodic-memory\"\n\n        if not episodic_root.exists():\n            return []\n\n        # Find the installed version directory\n        version_dirs = [d for d in episodic_root.iterdir() if d.is_dir() and d.name[0].isdigit()]\n        if not version_dirs:\n            return []\n\n        # Use most recent version\n        version_dir = sorted(version_dirs, reverse=True)[0]\n        search_cli = version_dir / \"cli\" / \"search-conversations.js\"\n\n        if not search_cli.exists():\n            return []\n\n        # Run search with text mode (faster, no embedding model)\n        result = subprocess.run(\n            [\"node\", str(search_cli), \"--text\", \"--limit\", str(limit), query_terms],\n            capture_output=True,\n            text=True,\n            timeout=2,  # Must complete in 2 seconds\n            cwd=str(version_dir)\n        )\n\n        if result.returncode != 0:\n            return []\n\n        # Parse output - format is:\n        # Found N relevant conversations:\n        #\n        # 1. [project, DATE] - X% match\n        #    \"snippet...\"\n        #    Lines X-Y in /path/to/file.jsonl\n        results = []\n        lines = result.stdout.strip().split('\\n')\n        i = 0\n        while i < len(lines):\n            line = lines[i]\n            # Look for numbered results: \"1. [project, DATE]...\"\n            if line and line[0].isdigit() and '. [' in line:\n                # Extract date from \"[project, DATE]\"\n                date_match = line.split(', ')\n                date = date_match[-1].split(']')[0] if len(date_match) > 1 else \"unknown\"\n\n                # Next line is the snippet (indented, in quotes)\n                if i + 1 < len(lines):\n                    snippet_line = lines[i + 1].strip()\n                    if snippet_line.startswith('\"'):\n                        snippet = snippet_line.strip('\"')\n                        # Truncate long snippets\n                        if len(snippet) > 150:\n                            snippet = snippet[:147] + \"...\"\n                        results.append({\n                            \"date\": date,\n                            \"snippet\": snippet\n                        })\n            i += 1\n\n        return results[:limit]\n\n    except subprocess.TimeoutExpired:\n        return []\n    except Exception:\n        return []\n\n\ndef suggest_memory_options(query_terms):\n    \"\"\"Search episodic memory and suggest other memory systems.\n\n    Actually searches episodic memory and returns results, with fallback\n    to manual search instructions if the search fails.\n    \"\"\"\n    serena_memories = list_serena_memories()\n\n    # Actually search episodic memory\n    episodic_results = search_episodic_memory(query_terms)\n\n    messages = []\n\n    # Episodic memory results (if found) - show first\n    if episodic_results:\n        results_text = []\n        for i, r in enumerate(episodic_results, 1):\n            results_text.append(f\"   {i}. [{r['date']}] {r['snippet']}\")\n        messages.append(\n            \"Relevant past conversations:\\n\" + \"\\n\".join(results_text) +\n            f\"\\n   For more: mcp__plugin_episodic-memory_episodic-memory__search(query='{query_terms}')\"\n        )\n    else:\n        # Fallback: suggest manual search\n        messages.append(\n            \"Episodic Memory:\\n\"\n            f\"   mcp__plugin_episodic-memory_episodic-memory__search(query='{query_terms}')\"\n        )\n\n    # Auto-read relevant Serena memories\n    if serena_memories:\n        relevant = find_relevant_memories(query_terms, serena_memories, max_count=2)\n        if relevant:\n            snippets = read_memory_snippets(relevant, max_chars_per_memory=500)\n            if snippets:\n                matched_keywords = [k for k in query_terms.lower().split()\n                                   if any(k in m.lower() for m in relevant)]\n                match_info = f\" (matched: {', '.join(matched_keywords[:3])})\" if matched_keywords else \"\"\n                messages.append(\n                    f\"Relevant Memories Found{match_info}:\\n\\n{snippets}\\n\\n\"\n                    f\"For more: mcp__router__serena__read_memory(memory_file_name='...')\"\n                )\n\n        # Still list other available memories\n        other_memories = [m for m in serena_memories if m not in relevant][:3]\n        if other_memories:\n            memory_list = \", \".join(other_memories)\n            messages.append(\n                f\"Other Serena Memories: {memory_list}\"\n            )\n\n    # Knowledge graph (structured facts/relations)\n    messages.append(\n        \"Knowledge Graph:\\n\"\n        \"   mcp__memory__search_nodes(query='<topic>')\"\n    )\n\n    return {\n        \"found\": bool(episodic_results),\n        \"conversations\": episodic_results,\n        \"message\": \"\\n\\n\".join(messages),\n        \"serena_memories\": serena_memories\n    }\n\n\ndef discover_project_handoffs(working_dir: Path | None = None) -> list[Path]:\n    \"\"\"Discover handoff files in the current project.\n    \n    Args:\n        working_dir: Working directory to start project detection from.\n                    Defaults to cwd.\n    \n    Returns:\n        List of handoff file paths, sorted by recency (newest first).\n    \"\"\"\n    if find_project_root is None or find_recent_handoffs is None:\n        return []\n    \n    try:\n        if working_dir is None:\n            working_dir = Path.cwd()\n        \n        project_root = find_project_root(working_dir)\n        return find_recent_handoffs(project_root, max_count=3, max_age_hours=48)\n    except Exception as e:\n        log_debug(f\"Handoff discovery failed: {e}\")\n        return []\n\n\ndef format_handoff_context(handoffs: list[Path], max_chars: int = 1500) -> str:\n    \"\"\"Format handoff files into context message.\n    \n    Args:\n        handoffs: List of handoff file paths\n        max_chars: Maximum characters for output\n        \n    Returns:\n        Formatted context string, or empty string if no handoffs\n    \"\"\"\n    if not handoffs:\n        return \"\"\n    \n    try:\n        # Read the most recent handoff\n        most_recent = handoffs[0]\n        content = most_recent.read_text()\n        \n        # Truncate if needed\n        if len(content) > max_chars - 100:\n            content = content[:max_chars - 100] + \"\\n\\n[truncated...]\"\n        \n        header = f\"**Previous Session Handoff** ({most_recent.name}):\\n\\n\"\n        result = header + content\n        \n        # Mention if there are other handoffs\n        if len(handoffs) > 1:\n            other_names = [h.name for h in handoffs[1:3]]\n            result += f\"\\n\\n_Other recent handoffs: {', '.join(other_names)}_\"\n        \n        return result[:max_chars]\n        \n    except Exception as e:\n        log_debug(f\"Failed to format handoff: {e}\")\n        return \"\"\n\n\ndef main():\n    \"\"\"Session start hook entry point.\"\"\"\n\n    # Read session data from stdin first (need agentId for reset)\n    try:\n        input_data = json.loads(sys.stdin.read())\n    except json.JSONDecodeError:\n        input_data = {}\n\n    # Check if this is a subagent (has agentId)\n    agent_id = input_data.get(\"agentId\")\n\n    # Reset enforcement counters - pass agent_id to inherit phase if subagent\n    reset_enforcement_counters(agent_id)\n\n    # Clean up stale output files (only for main agent, not subagents)\n    cleanup_message = None\n    if not agent_id:\n        try:\n            from output_cleanup import cleanup_stale_outputs\n            result = cleanup_stale_outputs(max_age_hours=48, dry_run=False)\n            if result[\"files_deleted\"] > 0:\n                space_mb = result[\"space_reclaimed\"] / (1024 * 1024)\n                cleanup_message = f\"Cleaned {result['files_deleted']} stale output files ({space_mb:.1f} MB)\"\n        except Exception:\n            pass  # Fail silently - cleanup shouldn't break session start\n\n    # Run inventory to discover capabilities\n    inventory_output = run_inventory()\n\n    # Get any initial context from the session\n    initial_messages = input_data.get(\"messages\", [])\n\n    # Extract potential search terms from first user message\n    query_terms = \"agent-swarm workflow automation\"\n    if initial_messages:\n        first_msg = initial_messages[0].get(\"content\", \"\")\n        # Simple heuristic: use first few words\n        words = first_msg.split()[:5]\n        if words:\n            query_terms = \" \".join(words)\n\n    # Suggest memory options\n    results = suggest_memory_options(query_terms)\n\n    # Build output message\n    messages = []\n\n    # Add hierarchical context with scope tags (only for main agent)\n    if not agent_id:\n        hierarchy = load_context_hierarchy(Path.cwd())\n        if hierarchy:\n            hierarchy_context = format_hierarchy_context(hierarchy)\n            if hierarchy_context:\n                messages.append(f\"**Hierarchical Context:**\\n{hierarchy_context}\")\n        \n        # Also use the resolver-based context for additional detail\n        context_summary = get_session_context(Path.cwd())\n        if context_summary:\n            messages.append(f\"Project Context:\\n{context_summary}\")\n        \n        # Auto-discover project handoffs (only for main agent)\n        handoffs = discover_project_handoffs(Path.cwd())\n        if handoffs:\n            handoff_context = format_handoff_context(handoffs)\n            if handoff_context:\n                messages.append(handoff_context)\n\n    # Load learned patterns from MEMORY.md (only for main agent)\n    if not agent_id:\n        memory_patterns = load_memory_patterns(Path.cwd())\n        if memory_patterns:\n            formatted_patterns = format_memory_patterns(memory_patterns, max_patterns=5)\n            if formatted_patterns:\n                messages.append(formatted_patterns)\n\n    # Add cleanup message if files were cleaned\n    if cleanup_message:\n        messages.append(cleanup_message)\n\n    # Add inventory if available\n    if inventory_output:\n        messages.append(\"Capability Inventory:\\n\" + inventory_output[:1000])  # Limit size\n\n    # Add memory suggestions (always show the message, which now includes auto-read snippets)\n    messages.append(results.get(\"message\", \"\"))\n\n    # Return result with suggestion\n    output = {\n        \"systemMessage\": \"\\n\\n\".join(messages) if messages else \"\"\n    }\n\n    print(json.dumps(output))\n\nif __name__ == \"__main__\":\n    main()\n",
        "hooks/state-protection.py": "#!/usr/bin/env python3\n\"\"\"State file protection hook - blocks edits to .state/ directory.\n\nPrevents accidental or unauthorized modification of workflow state files.\nOnly allows edits when allow_state_edits=true in .state/config.json.\n\"\"\"\n\nimport sys\nimport json\nfrom pathlib import Path\n\nSTATE_DIR = Path.home() / \".claude\" / \"plugins\" / \"agent-swarm\" / \".state\"\nCONFIG_FILE = STATE_DIR / \"config.json\"\n\n# Tools that write files\nWRITE_TOOLS = {\n    \"Write\", \"Edit\", \"NotebookEdit\",\n    \"mcp__router__native__write_file\",\n    \"mcp__router__native__edit_file\",\n    \"mcp__router__serena__create_text_file\",\n    \"mcp__router__serena__replace_content\",\n    \"mcp__router__serena__replace_symbol_body\",\n}\n\n\ndef is_state_edit_allowed() -> bool:\n    \"\"\"Check if state edits are explicitly allowed via config.\"\"\"\n    if not CONFIG_FILE.exists():\n        return False\n    try:\n        with open(CONFIG_FILE) as f:\n            config = json.load(f)\n            return config.get(\"allow_state_edits\", False) is True\n    except (json.JSONDecodeError, IOError):\n        return False\n\n\ndef is_state_path(file_path: str) -> bool:\n    \"\"\"Check if the path targets the .state directory.\"\"\"\n    try:\n        path = Path(file_path).resolve()\n        state_resolved = STATE_DIR.resolve()\n        return str(path).startswith(str(state_resolved))\n    except Exception:\n        return False\n\n\ndef get_file_path_from_input(tool_name: str, tool_input: dict) -> str | None:\n    \"\"\"Extract file path from tool input based on tool type.\"\"\"\n    # Native tools\n    if \"file_path\" in tool_input:\n        return tool_input[\"file_path\"]\n    # Serena tools\n    if \"relative_path\" in tool_input:\n        # Serena uses relative paths from project root\n        return str(STATE_DIR.parent / tool_input[\"relative_path\"])\n    return None\n\n\ndef allow(reason: str = \"\") -> dict:\n    result = {\n        \"hookSpecificOutput\": {\n            \"hookEventName\": \"PreToolUse\",\n            \"permissionDecision\": \"allow\"\n        }\n    }\n    if reason:\n        result[\"hookSpecificOutput\"][\"permissionDecisionReason\"] = reason\n    return result\n\n\ndef block(reason: str) -> dict:\n    return {\n        \"hookSpecificOutput\": {\n            \"hookEventName\": \"PreToolUse\",\n            \"permissionDecision\": \"deny\",\n            \"permissionDecisionReason\": reason\n        }\n    }\n\n\ndef main():\n    try:\n        input_data = json.loads(sys.stdin.read())\n    except json.JSONDecodeError:\n        print(json.dumps(allow()))\n        return\n\n    tool_name = input_data.get(\"tool_name\", \"\")\n    tool_input = input_data.get(\"tool_input\", {})\n\n    # Only check write tools\n    if tool_name not in WRITE_TOOLS:\n        print(json.dumps(allow()))\n        return\n\n    # Get the target file path\n    file_path = get_file_path_from_input(tool_name, tool_input)\n    if not file_path:\n        print(json.dumps(allow()))\n        return\n\n    # Check if targeting state directory\n    if not is_state_path(file_path):\n        print(json.dumps(allow()))\n        return\n\n    # State path targeted - check if explicitly allowed\n    if is_state_edit_allowed():\n        print(json.dumps(allow(\"State edits enabled via config\")))\n        return\n\n    # Block state edits\n    print(json.dumps(block(\n        f\"[STATE PROTECTED] Cannot edit {file_path}. \"\n        f\"State files are protected. Set allow_state_edits=true in .state/config.json to override.\"\n    )))\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "hooks/subagent-bash-lockdown.py": "#!/usr/bin/env python3\n\"\"\"\nLock down Bash in subagents to ONLY allow mcp-call.\n\nSubagents must use mcp-call for all external operations.\nAll other Bash commands are blocked.\n\"\"\"\nimport json\nimport sys\n\nMCP_CALL_PATH = \"/home/fearsidhe/.claude/plugins/agent-swarm/bin/mcp-call\"\n\n\ndef is_mcp_call(command: str) -> bool:\n    \"\"\"Check if command is mcp-call.\"\"\"\n    command = command.strip()\n    return (\n        command.startswith(\"mcp-call \") or\n        command.startswith(f\"{MCP_CALL_PATH} \") or\n        command == \"mcp-call\" or\n        command == MCP_CALL_PATH\n    )\n\n\ndef block(reason: str):\n    \"\"\"Block tool by exiting with code 2 (stderr used as message).\n\n    Exit code 2 is the reliable blocking mechanism - JSON deny responses\n    are sometimes ignored (Claude Code bug #4669).\n    \"\"\"\n    print(reason, file=sys.stderr)\n    sys.exit(2)\n\n\ndef main():\n    try:\n        input_data = json.loads(sys.stdin.read())\n    except json.JSONDecodeError:\n        sys.exit(0)\n\n    tool_name = input_data.get(\"tool_name\", \"\")\n    tool_input = input_data.get(\"tool_input\", {})\n    # Claude Code uses camelCase 'agentId'\n    agent_id = input_data.get(\"agentId\") or input_data.get(\"agent_id\")\n\n    # Only apply to subagent Bash calls\n    if not agent_id:\n        sys.exit(0)\n\n    if tool_name != \"Bash\":\n        sys.exit(0)\n\n    command = tool_input.get(\"command\", \"\")\n\n    if not is_mcp_call(command):\n        # Block - only mcp-call allowed\n        block(\n            \"[SUBAGENT LOCKDOWN] Only mcp-call is allowed in subagents. \"\n            \"Use: mcp-call <tool_name> '<json_args>'\"\n        )\n\n    # Allow mcp-call\n    sys.exit(0)\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "hooks/subagent-briefing.md": "# Subagent Operating Protocol\n\n**You are a subagent spawned by the orchestrator.**\n\n## Tools Available\n\nYou have ONE tool: **Bash**\n\nUse Bash to run `mcp-call` commands:\n\n| Operation | Command |\n|-----------|---------|\n| Run shell commands | `mcp-call git status`, `mcp-call pytest`, `mcp-call ruff check .` |\n| Read files | `mcp-call serena__read_file '{\"relative_path\": \"path/to/file\"}'` |\n| List directory | `mcp-call serena__list_dir '{\"relative_path\": \".\"}'` |\n| Find files | `mcp-call serena__find_file '{\"file_name_pattern\": \"*.py\"}'` |\n| Search code | `mcp-call serena__search_for_pattern '{\"pattern\": \"def main\"}'` |\n| Get symbols | `mcp-call serena__get_symbols_overview '{\"relative_path\": \"file.py\"}'` |\n| Find symbol | `mcp-call serena__find_symbol '{\"name_path_pattern\": \"MyClass\"}'` |\n| Replace content | `mcp-call serena__replace_content '{\"relative_path\": \"file.py\", ...}'` |\n\n### Shell Aliases\n\nThese common commands work directly with mcp-call:\n- `mcp-call pytest tests/` - run tests\n- `mcp-call ruff check .` - lint code\n- `mcp-call mypy src/` - type check\n- `mcp-call git status` - git operations\n- `mcp-call gh pr view` - github CLI\n- `mcp-call python script.py` - run python\n\n### Multi-Repo Git Operations\n\nWhen working in a specific repository (not the plugin directory), use the `--cwd` flag:\n\n```bash\n# Run git commands in a specific repo\nmcp-call --cwd=/home/fearsidhe/projects/LOGOS/logos git status\nmcp-call --cwd=/home/fearsidhe/projects/LOGOS/sophia git log -5\n\n# Run tests in a specific repo\nmcp-call --cwd=/path/to/repo pytest tests/\n\n# Lint a specific repo\nmcp-call --cwd=/path/to/repo ruff check .\n```\n\nThe `--cwd` flag ensures commands run in the correct directory, which is essential for:\n- Git operations (finding the correct .git directory)\n- pytest (finding conftest.py and test discovery)\n- Poetry/venv commands (using the correct environment)\n\n### Serena Tools\n\nFor code intelligence, use `mcp-call serena__<tool>`:\n- `serena__read_file` - read file contents\n- `serena__list_dir` - list directory\n- `serena__find_file` - find files by pattern\n- `serena__search_for_pattern` - grep-like search\n- `serena__get_symbols_overview` - get file symbols\n- `serena__find_symbol` - find symbol by name\n- `serena__replace_content` - replace text in file\n- `serena__replace_symbol_body` - replace entire symbol\n\n## CRITICAL: Token Efficiency Rules\n\n### File Reading Limits\n- **MAX 5 file reads** before you must write a batch script\n- Use write + bash to create and run scripts for multiple files\n\n### Search Limits\n- **MAX 5 searches** before you must batch\n- Use scripts with `from mcp_bridge import native_grep, native_glob`\n- Process results in script, return summary only\n\n### Duplicate Prevention\n- **Track what you've read** - don't read the same file twice\n\n### Script Requirements\nWhen you need to:\n- Read 3+ files → Write a batch script\n- Search 3+ patterns → Write a batch script\n- Process large results → Write a script, output summary only\n\n## Enforcement\n\nThe orchestrator monitors your token usage. Inefficient subagents may be terminated early.\n\n**Stay focused. Be efficient. Complete your task.**\n\n---\n\n## Git Operations\n\nSubagents handle git operations in their REVIEW phase (after tests pass).\n\n### Workflow Phases\n\n1. **IMPLEMENT Phase**: Write tests, implement code, verify tests pass\n2. **REVIEW Phase**: Commit, push, create PR for external review\n3. **Greptile Review**: External service reviews the PR\n4. **Comment Handling**: Orchestrator queues Greptile comments as new tasks\n\n## Git Responsibilities\n\n### Branch Verification (NEVER Create Branches)\n\nThe orchestrator has already created the feature branch for your task group. Your prompt includes the branch name.\n\n1. Verify you're on the correct branch: `mcp-call git branch --show-current`\n2. If not on the right branch, switch: `mcp-call git checkout feature/<task-name>`\n3. **NEVER** run `git checkout -b` — branch creation is the orchestrator's job\n4. Make your changes (tests, implementation)\n5. Stage files: `mcp-call git add <files>`\n\n### In REVIEW Phase (Subagent)\n\nWhen your tests pass, you enter REVIEW phase and handle git operations:\n\n1. Commit: `mcp-call git commit -m \"feat: <description>\"`\n2. Push: `mcp-call git push -u origin feature/<task-name>`\n3. Create PR: `mcp-call gh pr create --title \"...\" --body \"...\"`\n\n### External Review (Greptile)\n\nAfter you create the PR:\n- Greptile (external code review service) automatically reviews it\n- If Greptile leaves comments, the orchestrator picks them up\n- Orchestrator adds comment-fix tasks to the queue\n- New subagents are spawned to address those comments\n\n**Key Point:** You commit and create the PR. Greptile reviews. Orchestrator manages resulting comments.\n\n---\n\n## Memory Integration\n\nYou have access to multiple memory systems. Use them strategically to avoid repeating work.\n\n### Before Starting Your Task\n\n1. **Check Serena memories** (project-specific learnings):\n   - `mcp-call serena__list_memories` to see available memories\n   - `mcp-call serena__read_memory '{\"memory_name\": \"...\"}'` if relevant\n\n### After Completing Your Task\n\nIf you learned something useful, record it:\n\n1. **Patterns discovered** - What approaches worked well?\n2. **Pitfalls found** - What didn't work or caused issues?\n3. **Key decisions** - What choices did you make and why?\n\nUse `mcp-call serena__write_memory '{\"memory_name\": \"...\", \"content\": \"...\"}'` to persist significant learnings.\n\n### When to Use Memory\n\n**DO use memory for:**\n- Architectural patterns that keep recurring\n- Error solutions that took time to figure out\n- Project-specific conventions not in CLAUDE.md\n\n**DON'T use memory for:**\n- One-off implementation details\n- Information already in code comments\n- Temporary state (use handoffs instead)\n\n**Note:** Memory access adds tokens - only search if genuinely relevant to your task.\n",
        "hooks/subagent-complete.py": "#!/usr/bin/env python3\n\"\"\"SubagentStop hook - runs when a subagent completes.\n\nLogs completion and can trigger queue updates.\nDetects and handles failed agents using agent_recovery module.\nNote: This runs in the PARENT context, not inside the subagent.\n\"\"\"\n\nimport json\nimport re\nimport sys\nfrom datetime import datetime\nfrom pathlib import Path\nimport glob\n\n# Add lib to path for workflow_client and agent_recovery\nsys.path.insert(0, str(Path(__file__).parent.parent / \"lib\"))\nfrom workflow_client import (  # noqa: E402\n    agent_set_state,\n    workflow_is_active,\n    workflow_get_value,\n    workflow_update,\n)\nfrom agent_recovery import detect_failed_agent, handle_failed_agent  # noqa: E402\n\nSTATE_DIR = Path.home() / \".claude\" / \"plugins\" / \"agent-swarm\" / \".state\"\nSTATE_FILE = STATE_DIR / \"session.json\"\nSUBAGENT_LOG = STATE_DIR / \"subagent_executions.log\"\n\n\ndef load_state() -> dict:\n    \"\"\"Load session state.\"\"\"\n    if STATE_FILE.exists():\n        return json.loads(STATE_FILE.read_text())\n    return {}\n\n\ndef log_subagent_stop(agent_type: str, session_id: str, phase: str) -> None:\n    \"\"\"Log subagent completion to tracking file.\"\"\"\n    STATE_DIR.mkdir(parents=True, exist_ok=True)\n    with open(SUBAGENT_LOG, \"a\") as f:\n        f.write(f\"{datetime.now().isoformat()} | STOP | agent={agent_type} | session={session_id} | phase={phase}\\n\")\n\n\ndef extract_learnings(output_text: str) -> list[str]:\n    \"\"\"Extract LEARNING: tags from agent output.\n    \n    Looks for lines containing 'LEARNING: description' and returns\n    the description text for each.\n    \n    Returns list of learning descriptions (may be empty).\n    \"\"\"\n    if not output_text:\n        return []\n    \n    pattern = r'LEARNING:\\s*(.+?)(?:\\n|$)'\n    return re.findall(pattern, output_text, re.IGNORECASE)\n\n\ndef log_learnings_to_episodes(learnings: list[str], agent_type: str, task: str) -> None:\n    \"\"\"Append captured learnings to EPISODES.md.\n    \n    Creates .context directory and EPISODES.md if they don't exist.\n    Appends a new episode entry with the learnings.\n    \"\"\"\n    if not learnings:\n        return\n    \n    context_dir = Path(__file__).parent.parent / \".context\"\n    context_dir.mkdir(parents=True, exist_ok=True)\n    \n    episodes_file = context_dir / \"EPISODES.md\"\n    \n    # Build episode entry\n    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n    entry_lines = [\n        f\"\\n## Session: {timestamp} - Learning Capture\",\n        f\"- **Task**: {task[:100]}...\" if len(task) > 100 else f\"- **Task**: {task}\",\n        \"- **Outcome**: success\",\n        f\"- **Agent**: {agent_type}\",\n        \"- **Learnings**:\",\n    ]\n    for learning in learnings:\n        entry_lines.append(f\"  - {learning}\")\n    entry_lines.append(\"\")\n    \n    # Append to file\n    with open(episodes_file, \"a\") as f:\n        f.write(\"\\n\".join(entry_lines))\n\n\ndef extract_agent_completion_info(output_text: str) -> dict:\n    \"\"\"Extract completion info from agent output.\n\n    Looks for JSON block containing agent completion data.\n    Handles both markdown code fences and raw JSON.\n\n    Returns dict with: agent_id, status, summary, files_modified, tests_passed\n    Returns empty dict if parsing fails.\n    \"\"\"\n    if not output_text:\n        return {}\n\n    try:\n        # Try to find JSON in markdown code fence first\n        json_match = re.search(r'```json\\s*\\n(.*?)\\n```', output_text, re.DOTALL)\n        if json_match:\n            json_str = json_match.group(1)\n        else:\n            # Try to find raw JSON object\n            json_match = re.search(r'\\{[^}]*\"agent_id\"[^}]*\\}', output_text, re.DOTALL)\n            if json_match:\n                json_str = json_match.group(0)\n            else:\n                return {}\n\n        # Parse the JSON\n        data = json.loads(json_str)\n\n        # Extract expected fields with defaults\n        return {\n            \"agent_id\": data.get(\"agent_id\", \"\"),\n            \"status\": data.get(\"status\", \"unknown\"),\n            \"summary\": data.get(\"summary\", \"\"),\n            \"files_modified\": data.get(\"files_modified\", []),\n            \"tests_passed\": data.get(\"tests_passed\", None)\n        }\n    except (json.JSONDecodeError, AttributeError, KeyError):\n        # Parsing failed, return empty dict\n        return {}\n\n\ndef find_agent_output_file(session_id: str):\n    \"\"\"Find agent output file for given session ID.\n    \n    Agent output files are in /tmp/claude/.../tasks/{session_id}.output\n    \"\"\"\n    try:\n        # Search for output file matching session ID\n        pattern = f\"/tmp/claude/*/tasks/{session_id}.output\"\n        matches = glob.glob(pattern)\n        if matches:\n            return matches[0]\n        return None\n    except Exception:\n        return None\n\n\ndef capture_learnings_from_output(session_id: str, agent_type: str, task: str = \"\") -> int:\n    \"\"\"Capture LEARNING: tags from agent output and log to EPISODES.md.\n    \n    Args:\n        session_id: The agent session ID\n        agent_type: Type of agent (e.g., 'implementer')\n        task: Task description (optional, used in episode entry)\n    \n    Returns:\n        Number of learnings captured\n    \"\"\"\n    try:\n        output_file = find_agent_output_file(session_id)\n        if not output_file:\n            return 0\n        \n        output_path = Path(output_file)\n        if not output_path.exists():\n            return 0\n        \n        output_text = output_path.read_text()\n        learnings = extract_learnings(output_text)\n        \n        if learnings:\n            task_desc = task if task else f\"{agent_type} subagent task\"\n            log_learnings_to_episodes(learnings, agent_type, task_desc)\n        \n        return len(learnings)\n    except Exception:\n        return 0  # Don't fail hook on learning capture errors\n\n\ndef persist_agent_output(session_id: str, agent_type: str) -> dict:\n    \"\"\"Persist agent completion data to state manager.\n    \n    Extracts completion info from agent output and stores via agent_set_state.\n    Also detects and handles failed agents.\n    Handles errors gracefully - does not fail if parsing/storage fails.\n    \n    Returns:\n        dict with failure_detected (bool) and failure_reason (str) if failed\n    \"\"\"\n    failure_info = {\"failure_detected\": False, \"failure_reason\": \"\"}\n    \n    try:\n        # Find and read agent output file\n        output_file = find_agent_output_file(session_id)\n        if not output_file:\n            return failure_info  # No output file found, skip silently\n        \n        output_path = Path(output_file)\n        if not output_path.exists():\n            return failure_info\n        \n        output_text = output_path.read_text()\n        \n        # Extract completion info from output\n        completion_info = extract_agent_completion_info(output_text)\n        if not completion_info or not completion_info.get(\"agent_id\"):\n            return failure_info  # No valid completion info found\n        \n        # Add timestamp and agent type\n        completion_info[\"timestamp\"] = datetime.now().isoformat()\n        completion_info[\"agent_type\"] = agent_type\n        \n        # Calculate duration if we can find start time\n        # (Would need to be stored somewhere, skipping for now)\n        \n        # Persist to state manager\n        agent_id = completion_info[\"agent_id\"]\n        agent_set_state(agent_id, completion_info)\n        \n        # Detect and handle failed agents\n        if detect_failed_agent(agent_id):\n            reason = f\"Agent output indicates failure (status={completion_info.get('status')}, summary contains error keywords)\"\n            handle_failed_agent(agent_id, reason=reason)\n            failure_info[\"failure_detected\"] = True\n            failure_info[\"failure_reason\"] = reason\n        \n        return failure_info\n        \n    except Exception:\n        # Log error but don't fail the hook\n        # (In production, could use logging module)\n        return failure_info\n\n\ndef update_workflow_state(session_id: str, agent_type: str) -> None:\n    \"\"\"Move agent from active_agents to completed_tasks in workflow state.\"\"\"\n    try:\n        if not workflow_is_active(\"iterate\"):\n            return\n        \n        # Get current active agents\n        active = workflow_get_value(\"iterate\", \"active_agents\") or {}\n        \n        # Remove this agent from active\n        agent_info = active.pop(session_id, None)\n        \n        # Get completed tasks list\n        completed = workflow_get_value(\"iterate\", \"completed_tasks\") or []\n        \n        # Add to completed (use description if available, else session_id)\n        if agent_info:\n            desc = agent_info.get(\"description\", session_id)\n        else:\n            desc = f\"{agent_type}:{session_id}\"\n        completed.append(desc)\n        \n        # Update workflow state\n        workflow_update(\"iterate\", {\n            \"active_agents\": active,\n            \"completed_tasks\": completed\n        })\n    except Exception:\n        pass  # Don't fail hook on workflow state errors\n\n\ndef main():\n    # Read hook input\n    input_data = json.loads(sys.stdin.read())\n\n    # Extract info\n    session_id = input_data.get(\"sessionId\", \"unknown\")[:8]\n    agent_type = input_data.get(\"agentType\", \"unknown\")\n\n    # Get current phase\n    state = load_state()\n    phase = state.get(\"phase\") or state.get(\"iterate_phase\") or \"none\"\n\n    # Persist agent output to state manager and check for failures\n    failure_info = persist_agent_output(session_id, agent_type)\n\n    # Capture LEARNING: tags from agent output and log to EPISODES.md\n    learnings_captured = capture_learnings_from_output(session_id, agent_type)\n\n    # Update workflow state - move from active_agents to completed_tasks\n    update_workflow_state(session_id, agent_type)\n\n    # Build result with failure status if detected\n    message = f\"Subagent {agent_type} completed in phase {phase}\"\n    if learnings_captured > 0:\n        message += f\" [{learnings_captured} learning(s) captured]\"\n    if failure_info[\"failure_detected\"]:\n        message += f\" [FAILED: {failure_info['failure_reason']}]\"\n\n    result = {\n        \"hookSpecificOutput\": {\n            \"hookEventName\": \"SubagentStop\",\n            \"message\": message,\n            \"failure_detected\": failure_info[\"failure_detected\"]\n        }\n    }\n\n    print(json.dumps(result))\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "hooks/subagent-complete.py.bak": "#!/usr/bin/env python3\n\"\"\"SubagentStop hook - runs when a subagent completes.\n\nLogs completion and can trigger queue updates.\nNote: This runs in the PARENT context, not inside the subagent.\n\"\"\"\n\nimport json\nimport re\nimport sys\nfrom datetime import datetime\nfrom pathlib import Path\nimport glob\n\n# Add lib to path for workflow_client\nsys.path.insert(0, str(Path(__file__).parent.parent / \"lib\"))\nfrom workflow_client import agent_set_state\n\nSTATE_DIR = Path.home() / \".claude\" / \"plugins\" / \"agent-swarm\" / \".state\"\nSTATE_FILE = STATE_DIR / \"session.json\"\nSUBAGENT_LOG = STATE_DIR / \"subagent_executions.log\"\n\n\ndef load_state() -> dict:\n    \"\"\"Load session state.\"\"\"\n    if STATE_FILE.exists():\n        return json.loads(STATE_FILE.read_text())\n    return {}\n\n\ndef log_subagent_stop(agent_type: str, session_id: str, phase: str) -> None:\n    \"\"\"Log subagent completion to tracking file.\"\"\"\n    STATE_DIR.mkdir(parents=True, exist_ok=True)\n    with open(SUBAGENT_LOG, \"a\") as f:\n        f.write(f\"{datetime.now().isoformat()} | STOP | agent={agent_type} | session={session_id} | phase={phase}\\n\")\n\n\ndef extract_agent_completion_info(output_text: str) -> dict:\n    \"\"\"Extract completion info from agent output.\n\n    Looks for JSON block containing agent completion data.\n    Handles both markdown code fences and raw JSON.\n\n    Returns dict with: agent_id, status, summary, files_modified, tests_passed\n    Returns empty dict if parsing fails.\n    \"\"\"\n    if not output_text:\n        return {}\n\n    try:\n        # Try to find JSON in markdown code fence first\n        json_match = re.search(r'```json\\s*\\n(.*?)\\n```', output_text, re.DOTALL)\n        if json_match:\n            json_str = json_match.group(1)\n        else:\n            # Try to find raw JSON object\n            json_match = re.search(r'\\{[^}]*\"agent_id\"[^}]*\\}', output_text, re.DOTALL)\n            if json_match:\n                json_str = json_match.group(0)\n            else:\n                return {}\n\n        # Parse the JSON\n        data = json.loads(json_str)\n\n        # Extract expected fields with defaults\n        return {\n            \"agent_id\": data.get(\"agent_id\", \"\"),\n            \"status\": data.get(\"status\", \"unknown\"),\n            \"summary\": data.get(\"summary\", \"\"),\n            \"files_modified\": data.get(\"files_modified\", []),\n            \"tests_passed\": data.get(\"tests_passed\", None)\n        }\n    except (json.JSONDecodeError, AttributeError, KeyError):\n        # Parsing failed, return empty dict\n        return {}\n\n\ndef find_agent_output_file(session_id: str):\n    \"\"\"Find agent output file for given session ID.\n    \n    Agent output files are in /tmp/claude/.../tasks/{session_id}.output\n    \"\"\"\n    try:\n        # Search for output file matching session ID\n        pattern = f\"/tmp/claude/*/tasks/{session_id}.output\"\n        matches = glob.glob(pattern)\n        if matches:\n            return matches[0]\n        return None\n    except Exception:\n        return None\n\n\ndef persist_agent_output(session_id: str, agent_type: str) -> None:\n    \"\"\"Persist agent completion data to state manager.\n    \n    Extracts completion info from agent output and stores via agent_set_state.\n    Handles errors gracefully - does not fail if parsing/storage fails.\n    \"\"\"\n    try:\n        # Find and read agent output file\n        output_file = find_agent_output_file(session_id)\n        if not output_file:\n            return  # No output file found, skip silently\n        \n        output_path = Path(output_file)\n        if not output_path.exists():\n            return\n        \n        output_text = output_path.read_text()\n        \n        # Extract completion info from output\n        completion_info = extract_agent_completion_info(output_text)\n        if not completion_info or not completion_info.get(\"agent_id\"):\n            return  # No valid completion info found\n        \n        # Add timestamp and agent type\n        completion_info[\"timestamp\"] = datetime.now().isoformat()\n        completion_info[\"agent_type\"] = agent_type\n        \n        # Calculate duration if we can find start time\n        # (Would need to be stored somewhere, skipping for now)\n        \n        # Persist to state manager\n        agent_id = completion_info[\"agent_id\"]\n        agent_set_state(agent_id, completion_info)\n        \n    except Exception as e:\n        # Log error but don't fail the hook\n        # (In production, could use logging module)\n        pass\n\n\ndef main():\n    # Read hook input\n    input_data = json.loads(sys.stdin.read())\n\n    # Extract info\n    session_id = input_data.get(\"sessionId\", \"unknown\")[:8]\n    agent_type = input_data.get(\"agentType\", \"unknown\")\n\n    # Get current phase\n    state = load_state()\n    phase = state.get(\"phase\") or state.get(\"iterate_phase\") or \"none\"\n\n    # Persist agent output to state manager\n    persist_agent_output(session_id, agent_type)\n\n    # Log the subagent completion\n    # DISABLED: No longer logging subagent stops to file\n\n    # log_subagent_stop(agent_type, session_id, phase)\n    \n    # Decrement active agent count\n    # DISABLED: Not tracking active agents in file\n\n    # state[\"active_agents\"] = max(0, state.get(\"active_agents\", 0) - 1)\n    # DISABLED: Not creating state directory for session file\n\n    # STATE_FILE.parent.mkdir(parents=True, exist_ok=True)\n    # DISABLED: No longer writing session state to file\n\n    # STATE_FILE.write_text(json.dumps(state, indent=2))\n\n    result = {\n        \"hookSpecificOutput\": {\n            \"hookEventName\": \"SubagentStop\",\n            \"message\": f\"Subagent {agent_type} completed in phase {phase}\"\n        }\n    }\n\n    print(json.dumps(result))\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "hooks/subagent-enforcement.py": "#!/usr/bin/env python3\n\"\"\"SubagentStart enforcement hook.\n\nRuns when a subagent is spawned. Injects workflow context and constraints.\nNote: This runs in the PARENT context, not inside the subagent.\n\nSubagents receive:\n- Their task description\n- Phase constraints (what tools are blocked)\n- TDD workflow instructions\n\nSubagents do NOT:\n- Call start() (would overwrite orchestrator state)\n- Write to iterate.json (orchestrator's state file)\n- Load state from state_manager (they get it injected)\n\"\"\"\n\nimport json\nimport sys\nimport uuid\nfrom pathlib import Path\n\n# Add lib to path for workflow_client and permission_store\nlib_dir = Path(__file__).parent.parent / \"lib\"\nsys.path.insert(0, str(lib_dir))\n\ntry:\n    from workflow_client import workflow_get_state, agent_set_state, workflow_is_active\nexcept ImportError:\n    def workflow_get_state(workflow_id: str) -> dict | None:\n        return None\n    def agent_set_state(agent_id: str, state: dict) -> dict | None:\n        return None\n    def workflow_is_active(workflow_id: str) -> bool:\n        return False\n\ntry:\n    from permission_store import PermissionStore, TOOL_CATEGORIES, get_tool_category\nexcept ImportError:\n    # Fallback if permission_store not available\n    PermissionStore = None\n    TOOL_CATEGORIES = {}\n    def get_tool_category(name):\n        return None\n\n# Telemetry store for agent type registration\n_telemetry_store = None\nSTATE_DIR = Path.home() / \".claude/plugins/agent-swarm/.state\"\n\n\ndef get_telemetry_store():\n    \"\"\"Get or create DuckDBStore for telemetry.\"\"\"\n    global _telemetry_store\n    if _telemetry_store is None:\n        try:\n            from stores.duckdb_store import DuckDBStore\n            STATE_DIR.mkdir(parents=True, exist_ok=True)\n            _telemetry_store = DuckDBStore(data_dir=str(STATE_DIR))\n        except Exception:\n            pass  # Telemetry unavailable\n    return _telemetry_store\n\n\ndef load_session_state() -> dict:\n    \"\"\"Load session state from state server.\"\"\"\n    state = workflow_get_state(\"session\")\n    return state if state else {}\n\n\ndef load_iterate_state() -> dict:\n    \"\"\"Load iterate workflow state from state server.\"\"\"\n    state = workflow_get_state(\"iterate\")\n    return state if state else {}\n\n\ndef get_blocked_tools_for_phase(phase: str, perms_dict: dict | None) -> list[str]:\n    \"\"\"Get blocked tools for a phase using PermissionStore if available.\n\n    Args:\n        phase: Current workflow phase\n        perms_dict: Permissions dict from workflow state (may contain phase_permissions)\n\n    Returns:\n        List of blocked tool names\n    \"\"\"\n    blocked = []\n\n    # Try to use PermissionStore\n    if PermissionStore and perms_dict:\n        store = PermissionStore.from_dict(perms_dict)\n        if store.phase_permissions:\n            blocked.extend(store.phase_permissions.blocked_tools)\n            # Add tools from blocked categories\n            if store.phase_permissions.allowed_categories:\n                for tool_name, category in TOOL_CATEGORIES.items():\n                    if category and category not in store.phase_permissions.allowed_categories:\n                        if tool_name not in blocked:\n                            blocked.append(tool_name)\n        return blocked\n\n    # Fallback to phase_model if PermissionStore not available\n    try:\n        from phase_model import get_phase_info, TOOL_CATEGORIES as PM_CATEGORIES\n        phase_info = get_phase_info(phase)\n        if phase_info:\n            blocked = list(phase_info.blocked_tools)\n            for tool, cat in PM_CATEGORIES.items():\n                if cat and cat not in phase_info.allowed_categories:\n                    if tool not in blocked:\n                        blocked.append(tool)\n    except Exception:\n        pass\n\n    return blocked\n\n\ndef build_tdd_context(agent_id: str, task_desc: str, group: str = \"default\", repo_path: str = \"\") -> str:\n    \"\"\"Build TDD workflow context for implementer subagents.\"\"\"\n    repo_info = f\"\\n**Repo:** {repo_path}\" if repo_path else \"\"\n    return f\"\"\"\n## SUBAGENT WORKFLOW CONTEXT\n\n**Agent ID:** {agent_id}\n**Task:** {task_desc}\n**Group/PR:** {group}{repo_info}\n**Spawned by:** Orchestrator\n\n### Tools Available\n\nYou have ONE tool: **Bash**\n\nUse Bash to run `mcp-call` commands:\n- Shell: `mcp-call pytest`, `mcp-call git status`, `mcp-call ruff check .`\n- Read files: `mcp-call serena__read_file '{{\"relative_path\": \"path/to/file\"}}'`\n- Search: `mcp-call serena__search_for_pattern '{{\"pattern\": \"...\"}}'`\n- List dir: `mcp-call serena__list_dir '{{\"relative_path\": \".\"}}'`\n- Edit: `mcp-call serena__replace_content '{{\"relative_path\": \"...\", \"old\": \"...\", \"new\": \"...\"}}'`\n- Symbols: `mcp-call serena__get_symbols_overview '{{\"relative_path\": \"...\"}}'`\n\n**DO NOT** use Read, Write, Edit, Glob, Grep directly. Use mcp-call via Bash.\n\n### TDD Workflow (Follow This Order)\n\n**YOU CANNOT SKIP PHASES.** You MUST follow this exact sequence:\n\n1. **TEST_WRITING** - Write failing tests first\n2. **IMPLEMENT** - Write code to make tests pass\n3. **TEST** - Run tests, lint, coverage and record results\n4. **REVIEW** - Git workflow (branch/PR/commit/push)\n\nPhase-specific instructions are injected when you enter each phase.\n\n### Important Notes\n\n- **DO NOT** call iterate_workflow.start() - orchestrator manages workflow state\n- **DO NOT** write to iterate.json - that's orchestrator's state file\n- **REPORT** your completion status when done (pass/fail/blocked)\n- Focus only on your assigned task\n\n### Completion\n\nWhen done, return a summary:\n```json\n{{\n  \"agent_id\": \"{agent_id}\",\n  \"status\": \"complete|failed|blocked\",\n  \"summary\": \"What was accomplished\",\n  \"files_modified\": [\"list of files\"],\n  \"tests_passed\": true|false\n}}\n```\n\"\"\"\n\n\ndef build_test_writing_context(agent_id: str, task_desc: str, group: str, repo_path: str = \"\") -> str:\n    \"\"\"Context injected when entering TEST_WRITING phase.\"\"\"\n    cd_cmd = f\"cd {repo_path} && \" if repo_path else \"\"\n    return f\"\"\"\n## TEST_WRITING PHASE\n\n**Goal:** Write failing tests that define expected behavior.\n\n**Steps:**\n1. Read task requirements and existing code patterns\n2. Write test file(s) that will fail (red phase of TDD)\n3. Verify tests fail: `{cd_cmd}pytest <test_file> -x`\n4. When done, advance: `python3 ~/.claude/plugins/agent-swarm/lib/iterate_workflow.py advance`\n\n**DO NOT skip to implementation. Tests MUST exist and FAIL before proceeding.**\n\"\"\"\n\n\ndef build_implement_context(agent_id: str, task_desc: str, group: str, repo_path: str = \"\") -> str:\n    \"\"\"Context injected when entering IMPLEMENT phase.\"\"\"\n    cd_cmd = f\"cd {repo_path} && \" if repo_path else \"\"\n    return f\"\"\"\n## IMPLEMENT PHASE\n\n**Goal:** Write minimal code to make tests pass.\n\n**Steps:**\n1. Check side-effects before changes with `mcp-call serena__find_referencing_symbols '{\"name_path_pattern\": \"...\"}'`\n2. Follow existing code patterns\n3. Run tests frequently: `{cd_cmd}pytest <test_file> -x`\n4. When tests pass, advance: `python3 ~/.claude/plugins/agent-swarm/lib/iterate_workflow.py advance`\n\n**Write only what's needed to make tests pass. No extra features.**\n\"\"\"\n\n\ndef build_test_context(agent_id: str, task_desc: str, group: str, repo_path: str = \"\") -> str:\n    \"\"\"Context injected when entering TEST phase.\"\"\"\n    cd_cmd = f\"cd {repo_path} && \" if repo_path else \"\"\n    return f\"\"\"\n## TEST PHASE\n\n**Goal:** Full verification - tests, lint, coverage. NO EDITING in this phase.\n\n**Steps:**\n1. Run full test suite: `{cd_cmd}pytest`\n2. Run linter: `{cd_cmd}ruff check .`\n3. Check coverage: `{cd_cmd}pytest --cov`\n4. Record results: `python3 ~/.claude/plugins/agent-swarm/lib/iterate_workflow.py test 1 1 1` (args: tests lint coverage, 1=pass 0=fail)\n5. Advance: `python3 ~/.claude/plugins/agent-swarm/lib/iterate_workflow.py advance`\n\n**Kickbacks:** Tests/lint fail → IMPLEMENT | Coverage low → TEST_WRITING\n\"\"\"\n\n\ndef build_review_context(agent_id: str, task_desc: str, group: str, repo_path: str = \"\") -> str:\n    \"\"\"Context injected when entering REVIEW phase.\"\"\"\n    # Build cd command if repo_path provided (multi-repo support)\n    cd_cmd = f\"cd {repo_path} && \" if repo_path else \"\"\n    repo_info = f\"\\n**Repo:** {repo_path}\" if repo_path else \"\"\n    push_repo_arg = f\" --repo-path={repo_path}\" if repo_path else \"\"\n\n    return f\"\"\"\n## REVIEW PHASE\n\n**Goal:** Git workflow - branch, commit, PR, gated push.\n**Group:** {group} | **Branch:** feature/{group}{repo_info}\n\n**Steps:**\n1. Check/create branch: `{cd_cmd}git checkout -b \"feature/{group}\"` or `{cd_cmd}git checkout \"feature/{group}\"`\n2. Create PR if first task: `{cd_cmd}gh pr create --title \"{group}\" --body \"Implementation tasks\" --draft`\n3. Commit: `{cd_cmd}git add -A && git commit -m \"{task_desc}\"`\n4. Gated push: `python3 scripts/iterate_state.py push --pr={group}{push_repo_arg}`\n5. Record and advance: `python3 ~/.claude/plugins/agent-swarm/lib/iterate_workflow.py review 1 && python3 ~/.claude/plugins/agent-swarm/lib/iterate_workflow.py advance`\n\"\"\"\n\n\n# Map phases to their context builders\nPHASE_CONTEXT_BUILDERS = {\n    \"test_writing\": build_test_writing_context,\n    \"implement\": build_implement_context,\n    \"test\": build_test_context,\n    \"review\": build_review_context,\n}\n\n\ndef build_phase_restriction_context(agent_id: str, phase: str, mode: str, blocked_tools: list[str]) -> str:\n    \"\"\"Build phase restriction context for subagents.\"\"\"\n    if not blocked_tools:\n        return \"\"\n\n    mode_suffix = f\" ({mode} mode)\" if mode else \"\"\n    return f\"\"\"\n## PHASE RESTRICTIONS (ENFORCED)\n\n**Agent ID:** {agent_id}\n**Current phase:** {phase}{mode_suffix}\n\n**BLOCKED TOOLS - DO NOT USE:**\n{chr(10).join(f'- {t}' for t in sorted(set(blocked_tools))[:15])}\n\nIf you need a blocked tool, STOP and report to orchestrator.\n\"\"\"\n\n\ndef main():\n    # Read hook input\n    input_data = json.loads(sys.stdin.read())\n    # Extract info - Claude Code uses snake_case for field names\n    session_id = input_data.get(\"session_id\", \"unknown\")[:8]\n    agent_type = input_data.get(\"agent_type\", \"unknown\")\n    task_desc = input_data.get(\"task\", \"implementation task\")\n\n    # Use Claude Code's agentId if provided, otherwise generate one\n    # Claude Code uses camelCase 'agentId'\n    agent_id = input_data.get(\"agentId\") or input_data.get(\"agent_id\") or f\"sub-{uuid.uuid4().hex[:8]}\"\n\n    # Register agent type for telemetry (non-blocking)\n    try:\n        store = get_telemetry_store()\n        if store:\n            store.register_agent_type(agent_id, agent_type)\n    except Exception:\n        pass  # Don't fail the hook if telemetry registration fails\n\n    # Load all state from state server\n    session_state = load_session_state()\n    iterate_state = load_iterate_state()\n\n    # Determine context\n    phase = iterate_state.get(\"phase\") or session_state.get(\"phase\") or \"none\"\n    mode = iterate_state.get(\"mode\", \"\")\n    perms_dict = iterate_state.get(\"permissions\") or session_state.get(\"permissions\")\n\n    # CRITICAL: When iterate workflow is active and spawning from orchestrate phase,\n    # force ALL subagent types to start in test_writing phase (TDD enforcement)\n    if workflow_is_active(\"iterate\") and phase == \"orchestrate\":\n        phase = \"test_writing\"\n\n    # Store subagent state with its phase\n    agent_state = {\n        \"phase\": phase,\n        \"mode\": mode,\n        \"task\": task_desc,\n        \"parent_session\": session_id,\n    }\n    agent_set_state(agent_id, agent_state)\n\n    # Build context to inject based on mode and phase\n    additional_context = []\n    message_suffix = \"\"\n\n    # Early exit if no phase\n    if not phase or phase == \"none\":\n        pass  # No context to inject\n\n    elif mode == \"iterate-tdd\":\n        # In iterate-tdd mode\n        # Extract group and repo info from iterate state or default\n        group = iterate_state.get(\"current_group\") or iterate_state.get(\"pr_id\") or \"default\"\n        repo_path = iterate_state.get(\"current_repo_path\") or \"\"\n\n        if agent_type == \"implementer\" and phase in (\"orchestrate\", \"test_writing\", \"implement\", \"test\", \"review\"):\n            # Subagent spawned by orchestrator for implementation work\n            message_suffix = f\" (iterate-tdd/{phase})\"\n            additional_context.append(build_tdd_context(agent_id, task_desc, group, repo_path))\n\n            # Add phase-specific context if we have a builder for this phase\n            if phase in PHASE_CONTEXT_BUILDERS:\n                phase_builder = PHASE_CONTEXT_BUILDERS[phase]\n                additional_context.append(phase_builder(agent_id, task_desc, group, repo_path))\n        else:\n            # iterate-tdd but not implementer or different phase - apply phase restrictions\n            blocked_tools = get_blocked_tools_for_phase(phase, perms_dict)\n            if blocked_tools:\n                additional_context.append(\n                    build_phase_restriction_context(agent_id, phase, \"iterate-tdd\", blocked_tools)\n                )\n\n    else:\n        # Has phase but not iterate-tdd mode - apply generic restrictions\n        blocked_tools = get_blocked_tools_for_phase(phase, perms_dict)\n        if blocked_tools:\n            additional_context.append(\n                build_phase_restriction_context(agent_id, phase, mode, blocked_tools)\n            )\n\n    result = {\n        \"hookSpecificOutput\": {\n            \"hookEventName\": \"SubagentStart\",\n            \"additionalContext\": \"\\n\".join(additional_context) if additional_context else None,\n            \"message\": f\"Subagent {agent_type} ({agent_id}) started in phase {phase}{message_suffix}\"\n        }\n    }\n\n    print(json.dumps(result))\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "hooks/subagent-mcp-bypass.py": "#!/usr/bin/env python3\n\"\"\"\nSubagent MCP bypass hook.\n\nAuto-approves MCP tool calls from subagents to bypass the \"prompts unavailable\"\npermission denial. This allows subagents to use MCP tools autonomously.\n\nUses the correct hookSpecificOutput format with permissionDecision: \"allow\".\n\"\"\"\nimport json\nimport sys\n\n# Tools that should NOT be auto-approved (security/safety)\nEXCLUDED_TOOLS = {\n    # System-level tools that need explicit permission\n    \"execute_shell_command\",\n    \"bash\",\n    \"browser_run_code\",\n}\n\n\ndef should_auto_approve(tool_name: str) -> bool:\n    \"\"\"Check if this MCP tool should be auto-approved for subagents.\n\n    Returns True for most MCP tools, False for dangerous ones.\n    \"\"\"\n    # Only handle MCP tools\n    if not tool_name.startswith(\"mcp__\"):\n        return False\n\n    # Check exclusions by looking for dangerous tool suffixes\n    for excluded in EXCLUDED_TOOLS:\n        if tool_name.endswith(f\"__{excluded}\"):\n            return False\n\n    return True\n\n\ndef main():\n    try:\n        input_data = json.loads(sys.stdin.read())\n    except json.JSONDecodeError:\n        # Invalid input, allow normal flow\n        sys.exit(0)\n\n    tool_name = input_data.get(\"tool_name\", \"\")\n    agent_id = input_data.get(\"agentId\")\n\n    # Only apply to subagent calls\n    if not agent_id:\n        sys.exit(0)\n\n    # Check if we should auto-approve this tool\n    if not should_auto_approve(tool_name):\n        sys.exit(0)\n\n    # Auto-approve the MCP tool using correct hookSpecificOutput format\n    print(json.dumps({\n        \"hookSpecificOutput\": {\n            \"hookEventName\": \"PreToolUse\",\n            \"permissionDecision\": \"allow\",\n            \"permissionDecisionReason\": f\"Auto-approved for subagent {agent_id}\"\n        }\n    }))\n    sys.exit(0)\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "hooks/telemetry-posttool.py": "#!/usr/bin/env python3\n\"\"\"PostToolUse telemetry hook - completes tracking for ALL tool calls.\n\nPairs with telemetry-pretool.py to record duration and status.\nWrites to DuckDB via TelemetryService (v3 telemetry).\n\nExtracts ACTUAL token usage from transcript when available.\n\"\"\"\n\nimport sys\nimport json\nimport time\nimport os\nfrom datetime import datetime, timezone\nfrom pathlib import Path\n\n# Add project root to path for imports\nsys.path.insert(0, str(Path(__file__).parent.parent))\n\nfrom lib.stores.events import ToolCallEvent  # noqa: F401, E402 - referenced by other hooks\nfrom lib.telemetry_service import TelemetryService  # noqa: E402\n\n# Shared state files\nPENDING_FILE = Path.home() / \".claude/plugins/agent-swarm/.state/telemetry_pending.json\"\nLATEST_FILE = Path.home() / \".claude/plugins/agent-swarm/.state/telemetry_latest.json\"\n\n# Token estimates by tool type (conservative estimates)\nTOKEN_ESTIMATES = {\n    # Native Claude tools\n    \"Read\": 2000,\n    \"Write\": 500,\n    \"Edit\": 800,\n    \"Bash\": 1000,\n    \"Glob\": 500,\n    \"Grep\": 1000,\n    \"Task\": 5000,  # Base, subagent adds more\n    \"WebFetch\": 3000,\n    \"WebSearch\": 2000,\n    \"AskUserQuestion\": 200,\n    \"TodoWrite\": 100,\n    # MCP tools (rough estimates)\n    \"read_file\": 2000,\n    \"find_symbol\": 1500,\n    \"search_for_pattern\": 1500,\n    \"get_symbols_overview\": 1000,\n    \"list_dir\": 500,\n    \"replace_content\": 800,\n    \"execute_shell_command\": 1000,\n}\n\n# Subagent token estimates (much larger)\nSUBAGENT_TOKEN_ESTIMATES = {\n    \"Explore\": 25000,\n    \"Plan\": 30000,\n    \"Implement\": 100000,\n    \"general-purpose\": 50000,\n    \"Bash\": 10000,\n    \"feature-dev:code-explorer\": 40000,\n    \"feature-dev:code-reviewer\": 30000,\n    \"feature-dev:code-architect\": 50000,\n}\n\n\ndef extract_tokens_from_transcript(transcript_path: str) -> dict | None:\n    \"\"\"Extract actual token usage from the conversation transcript.\"\"\"\n    if not transcript_path:\n        return None\n\n    path = Path(transcript_path)\n    if not path.exists():\n        return None\n\n    try:\n        content = path.read_text()\n        lines = content.strip().split('\\n')\n\n        for line in reversed(lines[-20:]):\n            try:\n                entry = json.loads(line)\n                if \"message\" in entry and \"usage\" in entry.get(\"message\", {}):\n                    usage = entry[\"message\"][\"usage\"]\n                    return {\n                        \"input_tokens\": usage.get(\"input_tokens\", 0),\n                        \"output_tokens\": usage.get(\"output_tokens\", 0),\n                        \"cache_read_input_tokens\": usage.get(\"cache_read_input_tokens\", 0),\n                        \"cache_creation_input_tokens\": usage.get(\"cache_creation_input_tokens\", 0),\n                        \"total\": (\n                            usage.get(\"input_tokens\", 0) +\n                            usage.get(\"output_tokens\", 0) +\n                            usage.get(\"cache_read_input_tokens\", 0) +\n                            usage.get(\"cache_creation_input_tokens\", 0)\n                        )\n                    }\n            except json.JSONDecodeError:\n                continue\n        return None\n    except Exception:\n        return None\n\n\ndef load_json(path: Path) -> dict:\n    \"\"\"Load JSON file safely.\"\"\"\n    if path.exists():\n        try:\n            return json.loads(path.read_text())\n        except (json.JSONDecodeError, IOError):\n            pass\n    return {}\n\n\ndef save_json(path: Path, data: dict) -> None:\n    \"\"\"Save JSON file.\"\"\"\n    path.parent.mkdir(parents=True, exist_ok=True)\n    path.write_text(json.dumps(data, indent=2))\n\n\ndef estimate_tokens(tool_name: str, subagent_type: str = \"\") -> int:\n    \"\"\"Estimate tokens for a tool call.\"\"\"\n    if subagent_type:\n        return SUBAGENT_TOKEN_ESTIMATES.get(subagent_type, 50000)\n\n    if tool_name.startswith(\"mcp__\"):\n        parts = tool_name.split(\"__\")\n        base_name = parts[-1] if parts else tool_name\n        return TOKEN_ESTIMATES.get(base_name, 500)\n\n    return TOKEN_ESTIMATES.get(tool_name, 500)\n\n\ndef detect_error(tool_response) -> tuple[bool, str]:\n    \"\"\"Detect if tool output indicates an error.\"\"\"\n    if isinstance(tool_response, dict):\n        if tool_response.get(\"isError\"):\n            return True, str(tool_response.get(\"content\", \"\"))[:200]\n        if \"error\" in tool_response:\n            return True, str(tool_response[\"error\"])[:200]\n    if isinstance(tool_response, str):\n        lower = tool_response.lower()\n        if \"error:\" in lower or \"exception:\" in lower or \"failed:\" in lower:\n            return True, tool_response[:200]\n    return False, \"\"\n\n\ndef get_session_id() -> str:\n    \"\"\"Get current session ID from environment or generate one.\"\"\"\n    return os.environ.get(\"CLAUDE_SESSION_ID\", datetime.now(timezone.utc).strftime(\"%Y%m%d-%H%M%S\"))\n\n\ndef main():\n    try:\n        input_data = json.loads(sys.stdin.read())\n    except json.JSONDecodeError:\n        print(json.dumps({}))\n        return\n\n    tool_name = input_data.get(\"tool_name\", \"\")\n    tool_response = input_data.get(\"tool_response\", {})\n\n    # Find the pending request\n    latest = load_json(LATEST_FILE)\n    request_id = latest.get(tool_name)\n\n    if not request_id:\n        print(json.dumps({}))\n        return\n\n    pending = load_json(PENDING_FILE)\n    request_data = pending.pop(request_id, None)\n    save_json(PENDING_FILE, pending)\n\n    if tool_name in latest:\n        del latest[tool_name]\n        save_json(LATEST_FILE, latest)\n\n    if not request_data:\n        print(json.dumps({}))\n        return\n\n    # Calculate duration\n    start_time = request_data.get(\"start_time\", time.time())\n    duration_ms = int((time.time() - start_time) * 1000)\n\n    # Detect errors\n    is_error, error_msg = detect_error(tool_response)\n\n    # Try to get actual tokens from transcript, fall back to estimate\n    subagent_type = request_data.get(\"subagent_type\", \"\")\n    transcript_path = input_data.get(\"transcript_path\", \"\")\n    actual_tokens = extract_tokens_from_transcript(transcript_path)\n\n    if actual_tokens:\n        input_tokens = actual_tokens[\"input_tokens\"]\n        output_tokens = actual_tokens[\"output_tokens\"]\n        cache_read = actual_tokens[\"cache_read_input_tokens\"]\n        cache_create = actual_tokens[\"cache_creation_input_tokens\"]\n    else:\n        input_tokens = 0\n        output_tokens = 0\n        cache_read = 0\n        cache_create = 0\n\n    backend = request_data.get(\"backend\", \"unknown\")\n    session_id = get_session_id()\n\n    # === Telemetry v3: Write to DuckDB via TelemetryService ===\n    try:\n        state_dir = Path.home() / \".claude/plugins/agent-swarm/.state\"\n        service = TelemetryService(data_dir=str(state_dir))\n        \n        # Summary tracking - measure output size and determine if it would be summarized\n        SUMMARIZATION_THRESHOLD = 2000  # chars - matches SummarizationService\n\n        # Calculate original size from tool output\n        if isinstance(tool_response, dict):\n            output_str = json.dumps(tool_response)\n        else:\n            output_str = str(tool_response) if tool_response else \"\"\n\n        original_size = len(output_str)\n\n        # Determine if this would have been summarized (output exceeds threshold)\n        was_summarized = original_size > SUMMARIZATION_THRESHOLD\n        summary_size = min(original_size, SUMMARIZATION_THRESHOLD) if was_summarized else None\n        \n        event_data = {\n            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n            \"session_id\": session_id,\n            \"agent_id\": os.environ.get(\"CLAUDE_AGENT_ID\", session_id),\n            \"tool\": tool_name,\n            \"backend\": backend,\n            \"duration_ms\": duration_ms,\n            \"status\": \"error\" if is_error else \"success\",\n            \"error_type\": error_msg[:100] if is_error else None,\n            \"input_tokens\": input_tokens,\n            \"output_tokens\": output_tokens,\n            \"cache_read_tokens\": cache_read,\n            \"cache_creation_tokens\": cache_create,\n            \"agent_type\": subagent_type if subagent_type else None,\n            \"workflow_id\": os.environ.get(\"WORKFLOW_ID\"),\n            \"was_summarized\": was_summarized,\n            \"original_size\": original_size,\n            \"summary_size\": summary_size,\n        }\n        service.insert_event(event_data)\n    except Exception:\n        # Don't fail the hook if telemetry writing fails\n        pass\n\n    print(json.dumps({}))\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "hooks/telemetry-pretool.py": "#!/usr/bin/env python3\n\"\"\"PreToolUse telemetry hook - tracks start of ALL tool calls.\n\nWrites pending request to telemetry system for timing tracking.\nWorks with telemetry-posttool.py to complete the entry.\n\"\"\"\n\nimport sys\nimport json\nimport hashlib\nimport time\nfrom pathlib import Path\n\n# Shared state files\nPENDING_FILE = Path.home() / \".claude/plugins/agent-swarm/.state/telemetry_pending.json\"\nTELEMETRY_FILE = Path.home() / \".claude/plugins/agent-swarm/.state/telemetry.json\"\n\n\ndef load_json(path: Path) -> dict:\n    \"\"\"Load JSON file safely.\"\"\"\n    if path.exists():\n        try:\n            return json.loads(path.read_text())\n        except (json.JSONDecodeError, IOError):\n            pass\n    return {}\n\n\ndef save_json(path: Path, data: dict) -> None:\n    \"\"\"Save JSON file.\"\"\"\n    path.parent.mkdir(parents=True, exist_ok=True)\n    path.write_text(json.dumps(data, indent=2))\n\n\ndef generate_request_id(tool_name: str, tool_input: dict) -> str:\n    \"\"\"Generate a unique-ish ID for this request.\"\"\"\n    # Use tool name + hash of input + timestamp\n    content = json.dumps({\"tool\": tool_name, \"input\": tool_input}, sort_keys=True)\n    hash_part = hashlib.sha256(content.encode()).hexdigest()[:8]\n    return f\"req-{int(time.time() * 1000)}-{hash_part}\"\n\n\ndef extract_subagent_context(tool_name: str, tool_input: dict) -> tuple[str, str]:\n    \"\"\"Extract subagent type and task summary if this is a Task tool call.\n    \n    Returns:\n        tuple of (subagent_type, task_summary)\n    \"\"\"\n    if tool_name == \"Task\":\n        subagent_type = tool_input.get(\"subagent_type\", \"unknown\")\n        prompt = tool_input.get(\"prompt\", \"\")\n        task_summary = prompt[:100] if prompt else \"\"\n        return subagent_type, task_summary\n    return \"\", \"\"\n\n\ndef main():\n    try:\n        input_data = json.loads(sys.stdin.read())\n    except json.JSONDecodeError:\n        # Invalid input, skip tracking (exit 0, no output)\n        return\n\n    tool_name = input_data.get(\"tool_name\", \"\")\n    tool_input = input_data.get(\"tool_input\", {})\n\n    # Generate request ID\n    request_id = generate_request_id(tool_name, tool_input)\n\n    # Extract metadata\n    subagent_type, task_summary = extract_subagent_context(tool_name, tool_input)\n\n    # Determine backend (MCP tools have prefixes)\n    if tool_name.startswith(\"mcp__\"):\n        # mcp__plugin_serena_serena__find_symbol -> serena\n        parts = tool_name.split(\"__\")\n        backend = parts[1] if len(parts) > 1 else \"mcp\"\n    else:\n        backend = \"claude-native\"\n\n    # Store pending request\n    pending = load_json(PENDING_FILE)\n    pending[request_id] = {\n        \"tool\": tool_name,\n        \"backend\": backend,\n        \"subagent_type\": subagent_type,\n        \"task_summary\": task_summary,\n        \"start_time\": time.time(),\n        \"input_summary\": str(tool_input)[:200]  # First 200 chars for debugging\n    }\n\n    # Clean old pending requests (older than 5 minutes)\n    cutoff = time.time() - 300\n    pending = {k: v for k, v in pending.items() if v.get(\"start_time\", 0) > cutoff}\n\n    save_json(PENDING_FILE, pending)\n\n    # Store request_id in a way PostToolUse can find it\n    # Use a separate \"latest\" file keyed by tool_name\n    latest_file = Path.home() / \".claude/plugins/agent-swarm/.state/telemetry_latest.json\"\n    latest = load_json(latest_file)\n    latest[tool_name] = request_id\n    save_json(latest_file, latest)\n\n    # Just tracking, don't affect permission decision (exit 0, no output)\n    # Other hooks handle allow/deny decisions\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "hooks/telemetry-sessionstart.py": "#!/usr/bin/env python3\n\"\"\"\nSessionStart hook - Process historical JSONL files incrementally.\n\nOn each session start, processes a batch of unprocessed JSONL files\nto extract actual token usage data into the v2 telemetry structure.\n\nFeatures:\n- Processes max 25 files per session start (fast startup)\n- Tracks progress to avoid re-processing\n- Updates telemetry with actual token data\n\"\"\"\n\nimport json\nimport sys\nfrom pathlib import Path\n\n# Add lib to path\nlib_dir = Path(__file__).parent.parent / \"lib\"\nsys.path.insert(0, str(lib_dir))\n\n# Processing limit per session start (balance speed vs progress)\nMAX_FILES_PER_SESSION = 25\n\n\ndef main():\n    \"\"\"Process a batch of JSONL files on session start.\"\"\"\n    try:\n        # Import extractor (will fail gracefully if module not found)\n        from jsonl_extractor import process_batch, load_progress, find_jsonl_files, is_file_processed\n        \n        # Check how many files need processing\n        progress = load_progress()\n        all_files = find_jsonl_files()\n        unprocessed = [f for f in all_files if not is_file_processed(f, progress)]\n        \n        if not unprocessed:\n            # All files processed, nothing to do\n            print(json.dumps({\n                \"status\": \"complete\",\n                \"message\": f\"All {len(all_files)} JSONL files processed\"\n            }))\n            return\n        \n        # Process a batch quietly\n        stats = process_batch(max_files=MAX_FILES_PER_SESSION, verbose=False)\n        \n        # Output status for hook system\n        result = {\n            \"status\": \"processed\",\n            \"files_processed\": stats[\"processed_this_batch\"],\n            \"tokens_extracted\": stats[\"tokens_extracted\"],\n            \"remaining\": stats[\"remaining\"],\n            \"total_files\": stats[\"total_files\"]\n        }\n        \n        # Add progress message if still processing\n        if stats[\"remaining\"] > 0:\n            pct = ((stats[\"total_files\"] - stats[\"remaining\"]) / stats[\"total_files\"]) * 100\n            result[\"message\"] = f\"JSONL processing: {pct:.0f}% complete ({stats['remaining']} files remaining)\"\n        \n        print(json.dumps(result))\n        \n    except ImportError as e:\n        # Module not found - skip gracefully\n        print(json.dumps({\n            \"status\": \"skipped\",\n            \"reason\": f\"Module not available: {e}\"\n        }))\n    except Exception as e:\n        # Log error but don't fail the session\n        print(json.dumps({\n            \"status\": \"error\",\n            \"error\": str(e)\n        }), file=sys.stderr)\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "hooks/test-existence-enforcement.py": "#!/usr/bin/env python3\n\"\"\"PreToolUse hook to enforce TDD by requiring test files before editing Python files.\"\"\"\n\nimport json\nimport sys\nfrom pathlib import Path\n\n\ndef is_edit_tool(tool_name: str) -> bool:\n    \"\"\"Check if tool is an Edit-type tool, handling MCP router prefixes.\"\"\"\n    # Normalize MCP router prefixes\n    normalized = tool_name\n    if tool_name.startswith(\"mcp__router__\"):\n        # Extract last part: mcp__router__serena__edit_file -> edit_file\n        parts = tool_name.split(\"__\")\n        normalized = parts[-1] if parts else tool_name\n\n    # Check for edit tools\n    return normalized.lower() in (\"edit\", \"edit_file\")\n\n\ndef is_test_file(file_path: Path) -> bool:\n    \"\"\"Check if a file is a test file (test_*.py or *_test.py).\"\"\"\n    name = file_path.name\n    return name.startswith(\"test_\") or name.endswith(\"_test.py\")\n\n\ndef find_test_file(impl_path: Path) -> Path | None:\n    \"\"\"Find corresponding test file for an implementation file.\n\n    Searches for test_<module>.py or <module>_test.py in:\n    1. Same directory\n    2. tests/ directory at same level\n    3. tests/ directory at project root (walking up)\n    \"\"\"\n    module_name = impl_path.stem  # e.g., \"foo\" from \"foo.py\"\n\n    # Possible test file names\n    test_names = [f\"test_{module_name}.py\", f\"{module_name}_test.py\"]\n\n    # Search in same directory\n    for test_name in test_names:\n        test_path = impl_path.parent / test_name\n        if test_path.exists():\n            return test_path\n\n    # Search in sibling tests/ directory\n    for test_name in test_names:\n        test_path = impl_path.parent / \"tests\" / test_name\n        if test_path.exists():\n            return test_path\n\n    # Walk up looking for tests/ directories\n    current = impl_path.parent\n    for _ in range(10):  # Limit search depth\n        parent = current.parent\n        if parent == current:  # Reached root\n            break\n\n        # Check tests/ at this level\n        tests_dir = parent / \"tests\"\n        if tests_dir.is_dir():\n            for test_name in test_names:\n                test_path = tests_dir / test_name\n                if test_path.exists():\n                    return test_path\n\n        current = parent\n\n    return None\n\n\ndef allow(reason: str = \"\") -> dict:\n    \"\"\"Return allow response.\"\"\"\n    return {\n        \"hookSpecificOutput\": {\n            \"permissionDecision\": \"allow\",\n            \"permissionDecisionReason\": reason\n        }\n    }\n\n\ndef deny(reason: str) -> dict:\n    \"\"\"Return deny response.\"\"\"\n    return {\n        \"hookSpecificOutput\": {\n            \"permissionDecision\": \"deny\",\n            \"permissionDecisionReason\": reason\n        }\n    }\n\n\ndef main():\n    \"\"\"Enforce test existence before editing Python implementation files.\"\"\"\n    try:\n        input_data = json.loads(sys.stdin.read())\n    except (json.JSONDecodeError, TypeError):\n        # Malformed input - fail open\n        print(json.dumps(allow(\"Malformed input - allowing\")))\n        return\n\n    tool_name = input_data.get(\"tool_name\", \"\")\n    tool_input = input_data.get(\"tool_input\")\n\n    # Only check Edit-type tools\n    if not is_edit_tool(tool_name):\n        print(json.dumps(allow()))\n        return\n\n    # Handle missing or None tool_input - fail open\n    if not tool_input or not isinstance(tool_input, dict):\n        print(json.dumps(allow(\"No tool input - allowing\")))\n        return\n\n    # Get file path\n    file_path_str = tool_input.get(\"file_path\")\n    if not file_path_str:\n        print(json.dumps(allow(\"No file path - allowing\")))\n        return\n\n    file_path = Path(file_path_str)\n\n    # Only check Python files\n    if file_path.suffix != \".py\":\n        print(json.dumps(allow(\"Not a Python file\")))\n        return\n\n    # Always allow editing test files\n    if is_test_file(file_path):\n        print(json.dumps(allow(\"Editing test file\")))\n        return\n\n    # Check if corresponding test exists\n    test_file = find_test_file(file_path)\n    if test_file:\n        print(json.dumps(allow(f\"Test exists: {test_file.name}\")))\n        return\n\n    # No test file found - deny\n    module_name = file_path.stem\n    print(json.dumps(deny(\n        f\"[TDD] No test file found for {file_path.name}. \"\n        f\"Create tests/test_{module_name}.py first before editing implementation.\"\n    )))\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "hooks/transcript-debug.py": "#!/usr/bin/env python3\n\"\"\"Debug: log ALL PreToolUse input fields.\"\"\"\nimport json\nimport sys\nfrom pathlib import Path\nfrom datetime import datetime\n\nlog = Path.home() / \".claude/plugins/agent-swarm/.state/transcript_debug.log\"\nlog.parent.mkdir(parents=True, exist_ok=True)\n\ndata = json.loads(sys.stdin.read())\n\nwith open(log, \"a\") as f:\n    f.write(f\"\\n{datetime.now().isoformat()} | ALL_KEYS: {sorted(data.keys())}\\n\")\n    f.write(f\"  agentId: {data.get('agentId', 'NOT_PRESENT')}\\n\")\n    f.write(f\"  tool_name: {data.get('tool_name', 'NOT_PRESENT')}\\n\")\n    f.write(f\"  sessionId: {data.get('sessionId', 'NOT_PRESENT')}\\n\")\n    f.write(f\"  transcript_path: {data.get('transcript_path', 'NOT_PRESENT')}\\n\")\n\nprint(json.dumps({\"hookSpecificOutput\": {\"permissionDecision\": \"allow\"}}))\n",
        "hooks/verification_gates.py": "#!/usr/bin/env python3\n\"\"\"\nVerification gates for agent-swarm plugin.\n\nEnforces:\n1. Verification state tracking - block [VERIFY] without actual runs\n2. Tool version validation - warn on pyproject.toml mismatch\n3. Agent spawning enforcement - require Task tool for >1 pending\n4. PR completion composite gate\n\"\"\"\n\nimport json\nimport re\nimport subprocess\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Dict, List, Optional, Tuple\n\n# State file for tracking verification runs\nSTATE_FILE = Path.home() / \".claude/plugins/agent-swarm/.state/verification.json\"\n\n\ndef load_verification_state() -> Dict:\n    \"\"\"Load verification state from file.\"\"\"\n    if STATE_FILE.exists():\n        try:\n            return json.loads(STATE_FILE.read_text())\n        except json.JSONDecodeError:\n            pass\n    return {\n        \"lint_run\": False,\n        \"tests_run\": False,\n        \"format_run\": False,\n        \"last_reset\": datetime.now().isoformat(),\n    }\n\n\ndef save_verification_state(state: Dict) -> None:\n    \"\"\"Save verification state to file.\"\"\"\n    STATE_FILE.parent.mkdir(parents=True, exist_ok=True)\n    STATE_FILE.write_text(json.dumps(state, indent=2))\n\n\ndef reset_verification_state() -> None:\n    \"\"\"Reset verification state for new task.\"\"\"\n    save_verification_state({\n        \"lint_run\": False,\n        \"tests_run\": False,\n        \"format_run\": False,\n        \"last_reset\": datetime.now().isoformat(),\n    })\n\n\ndef on_bash_complete(command: str, exit_code: int) -> None:\n    \"\"\"Update verification state after bash commands complete.\"\"\"\n    state = load_verification_state()\n\n    # Track lint runs\n    if re.search(r'ruff check|black --check|pylint|flake8', command):\n        state[\"lint_run\"] = True\n        state[\"lint_timestamp\"] = datetime.now().isoformat()\n\n    # Track test runs\n    if re.search(r'pytest|python.*-m\\s+pytest', command):\n        state[\"tests_run\"] = True\n        state[\"tests_timestamp\"] = datetime.now().isoformat()\n        state[\"tests_passed\"] = (exit_code == 0)\n\n    # Track format runs (not just --check)\n    if re.search(r'ruff format|black\\s', command) and '--check' not in command:\n        state[\"format_run\"] = True\n        state[\"format_timestamp\"] = datetime.now().isoformat()\n\n    save_verification_state(state)\n\n\ndef check_verify_signal(message: str) -> Optional[str]:\n    \"\"\"\n    Block [VERIFY] or completion claims without actual verification runs.\n\n    Returns error message if blocked, None if allowed.\n    \"\"\"\n    # Patterns that indicate claiming verification complete\n    verify_patterns = [\n        r'\\[VERIFY\\].*✓',\n        r'Complete.*all.*pass',\n        r'all checks pass',\n        r'ready for merge',\n        r'PR.*ready',\n    ]\n\n    is_claiming_complete = any(\n        re.search(pattern, message, re.IGNORECASE)\n        for pattern in verify_patterns\n    )\n\n    if not is_claiming_complete:\n        return None\n\n    state = load_verification_state()\n    missing = []\n\n    if not state.get(\"lint_run\"):\n        missing.append(\"lint (run: ruff check or black --check)\")\n    if not state.get(\"tests_run\"):\n        missing.append(\"tests (run: pytest)\")\n\n    if missing:\n        return (\n            f\"[BLOCKED] Cannot claim verification without running: {missing}\\n\\n\"\n            \"Run the missing checks before declaring complete.\"\n        )\n\n    # Also check if tests actually passed\n    if state.get(\"tests_run\") and not state.get(\"tests_passed\", True):\n        return \"[BLOCKED] Tests failed. Fix before claiming verification complete.\"\n\n    return None\n\n\ndef check_tool_versions(cwd: str) -> Optional[str]:\n    \"\"\"\n    Warn if local tool versions don't match pyproject.toml.\n\n    Returns warning message if mismatch, None if OK.\n    \"\"\"\n    pyproject_path = Path(cwd) / \"pyproject.toml\"\n    if not pyproject_path.exists():\n        return None\n\n    try:\n        import tomli\n        with open(pyproject_path, \"rb\") as f:\n            pyproject = tomli.load(f)\n    except ImportError:\n        # tomli not available, skip check\n        return None\n    except Exception:\n        return None\n\n    warnings = []\n\n    # Get dev dependencies\n    dev_deps = (\n        pyproject.get(\"tool\", {})\n        .get(\"poetry\", {})\n        .get(\"group\", {})\n        .get(\"dev\", {})\n        .get(\"dependencies\", {})\n    )\n\n    # Check black version\n    if \"black\" in dev_deps:\n        expected = dev_deps[\"black\"]\n        try:\n            result = subprocess.run(\n                [\"black\", \"--version\"],\n                capture_output=True,\n                text=True,\n                timeout=5\n            )\n            if result.returncode == 0:\n                actual = result.stdout.split()[1] if result.stdout else \"unknown\"\n                if not _version_matches(actual, expected):\n                    warnings.append(f\"black: local={actual}, pyproject={expected}\")\n        except Exception:\n            pass\n\n    if warnings:\n        return f\"[WARNING] Tool version mismatch: {warnings}\"\n\n    return None\n\n\ndef _version_matches(actual: str, expected: str) -> bool:\n    \"\"\"Check if actual version matches expected constraint.\"\"\"\n    # Handle ^X.Y.Z format\n    if expected.startswith(\"^\"):\n        major_expected = expected[1:].split(\".\")[0]\n        major_actual = actual.split(\".\")[0]\n        return major_expected == major_actual\n\n    # Handle >=X.Y.Z,<A.B.C format\n    if \">=\" in expected and \"<\" in expected:\n        # Just check major version for simplicity\n        parts = re.findall(r'\\d+', expected)\n        if parts:\n            return actual.startswith(parts[0])\n\n    return True  # Default to OK if we can't parse\n\n\ndef check_agent_spawning(todo_list: List[Dict], max_inline: int = 1) -> Optional[str]:\n    \"\"\"\n    Enforce agent spawning when multiple tasks exist.\n\n    Returns error message if blocked, None if allowed.\n    \"\"\"\n    pending = [t for t in todo_list if t.get(\"status\") == \"pending\"]\n    in_progress = [t for t in todo_list if t.get(\"status\") == \"in_progress\"]\n\n    if len(pending) > max_inline and len(in_progress) <= 1:\n        return (\n            f\"[BLOCKED] {len(pending)} pending tasks. \"\n            \"Spawn agents for parallel work using Task tool.\"\n        )\n\n    return None\n\n\ndef update_greptile_state(\n    pr_number: int,\n    repo: str,\n    unaddressed_p0_comments: List[Dict],\n    api_available: bool = True\n) -> None:\n    \"\"\"\n    Update cached Greptile state after MCP tool call.\n\n    Call this after using mcp__plugin_greptile_greptile__list_merge_request_comments\n    to cache the results for the hook gate to use.\n\n    Args:\n        pr_number: The PR number checked\n        repo: Repository in format \"owner/repo\"\n        unaddressed_p0_comments: List of dicts with keys: id, body, path\n        api_available: Whether the API call succeeded\n    \"\"\"\n    state = load_verification_state()\n    state[\"greptile_state\"] = {\n        \"pr_number\": pr_number,\n        \"repo\": repo,\n        \"last_checked\": datetime.now().isoformat(),\n        \"unaddressed_p0_count\": len(unaddressed_p0_comments),\n        \"unaddressed_p0_comments\": unaddressed_p0_comments[:5],  # Limit stored\n        \"api_available\": api_available,\n    }\n    save_verification_state(state)\n\n\ndef clear_greptile_state() -> None:\n    \"\"\"Clear cached Greptile state (e.g., after addressing comments).\"\"\"\n    state = load_verification_state()\n    state.pop(\"greptile_state\", None)\n    save_verification_state(state)\n\n\ndef check_greptile_comments(pr_number: int, repo: str = \"c-daly/apollo\") -> Optional[str]:\n    \"\"\"\n    Check if there are unaddressed P0 Greptile comments on PR.\n\n    Returns error message if unaddressed P0 comments exist, None if OK.\n\n    This function reads from cached state (populated by update_greptile_state()).\n    The MCP tool mcp__plugin_greptile_greptile__list_merge_request_comments\n    cannot be called from hooks - the agent must call it and cache results.\n\n    State structure expected:\n        greptile_state: {\n            \"pr_number\": int,\n            \"repo\": str,\n            \"last_checked\": ISO timestamp,\n            \"unaddressed_p0_count\": int,\n            \"unaddressed_p0_comments\": [{\"id\": str, \"body\": str, \"path\": str}],\n            \"api_available\": bool\n        }\n    \"\"\"\n    state = load_verification_state()\n    greptile = state.get(\"greptile_state\", {})\n\n    # If no cached state, skip gracefully (API may not be available)\n    if not greptile:\n        return None\n\n    # Check if cached state matches current PR\n    cached_pr = greptile.get(\"pr_number\")\n    cached_repo = greptile.get(\"repo\")\n    if cached_pr != pr_number or cached_repo != repo:\n        # Stale cache - skip check but warn\n        return None\n\n    # Check if API was unavailable during last check\n    if not greptile.get(\"api_available\", True):\n        return None\n\n    # Check for unaddressed P0 comments\n    p0_count = greptile.get(\"unaddressed_p0_count\", 0)\n    if p0_count > 0:\n        comments = greptile.get(\"unaddressed_p0_comments\", [])\n        preview = \"\"\n        if comments:\n            first = comments[0]\n            path = first.get(\"path\", \"unknown\")\n            body = first.get(\"body\", \"\")[:100]\n            preview = f\" (e.g., {path}: {body}...)\"\n        return f\"[GREPTILE] {p0_count} unaddressed P0 comment(s) exist{preview}. Address before commit.\"\n\n    return None\n\n\ndef check_pr_ready(pr_number: int, repo: str = \"c-daly/apollo\") -> Tuple[bool, List[str]]:\n    \"\"\"\n    Composite gate for PR readiness.\n\n    Returns (is_ready, list_of_issues).\n    \"\"\"\n    issues = []\n    state = load_verification_state()\n\n    # Check verification state\n    if not state.get(\"lint_run\"):\n        issues.append(\"Lint check not run\")\n    if not state.get(\"tests_run\"):\n        issues.append(\"Tests not run\")\n    if state.get(\"tests_run\") and not state.get(\"tests_passed\", True):\n        issues.append(\"Tests failed\")\n\n    # Check Greptile (if integration available)\n    greptile_msg = check_greptile_comments(pr_number, repo)\n    if greptile_msg:\n        issues.append(greptile_msg)\n\n    return (len(issues) == 0, issues)\n\n\n# Export for use in combined-enforcement.py\n__all__ = [\n    \"load_verification_state\",\n    \"save_verification_state\",\n    \"reset_verification_state\",\n    \"on_bash_complete\",\n    \"check_verify_signal\",\n    \"check_tool_versions\",\n    \"check_agent_spawning\",\n    \"check_greptile_comments\",\n    \"update_greptile_state\",\n    \"clear_greptile_state\",\n    \"check_pr_ready\",\n]\n",
        "hooks/work_seeker.py.obsolete": "#!/usr/bin/env python3\n\"\"\"Work-seeking behavior for agent workflow.\n\nAfter any task completion, proactively identify next work items.\nInject prompts to keep the agent productive.\n\"\"\"\n\nimport json\nfrom pathlib import Path\nfrom typing import List, Dict, Any\n\n# Sources of pending work\nWORK_SOURCES = [\n    \"todo_list\",           # TodoWrite items with status=pending\n    \"greptile_comments\",   # Unaddressed review comments\n    \"ci_failures\",         # Failing CI runs\n    \"agent_outputs\",       # Completed agents needing followup\n    \"pr_items\",            # PRs needing attention\n]\n\ndef find_pending_work() -> List[Dict[str, Any]]:\n    \"\"\"Scan all sources for pending work items.\"\"\"\n    work_items = []\n\n    # Check todo list\n    todo_file = Path.home() / \".claude/todos.json\"\n    if todo_file.exists():\n        try:\n            todos = json.loads(todo_file.read_text())\n            pending = [t for t in todos if t.get(\"status\") == \"pending\"]\n            for t in pending:\n                work_items.append({\n                    \"source\": \"todo_list\",\n                    \"description\": t.get(\"content\"),\n                    \"priority\": \"high\" if \"P0\" in t.get(\"content\", \"\") else \"normal\",\n                })\n        except json.JSONDecodeError:\n            pass\n\n    # Check for completed agent outputs needing review\n    agent_dir = Path(\"/tmp/claude\")\n    if agent_dir.exists():\n        last_check_file = Path.home() / \".claude/.last_work_check\"\n        last_check_time = last_check_file.stat().st_mtime if last_check_file.exists() else 0\n\n        for output_file in agent_dir.glob(\"**/tasks/*.output\"):\n            if output_file.stat().st_mtime > last_check_time:\n                work_items.append({\n                    \"source\": \"agent_output\",\n                    \"description\": f\"Review agent output: {output_file.name}\",\n                    \"priority\": \"normal\",\n                    \"file\": str(output_file),\n                })\n\n    return work_items\n\ndef generate_work_prompt(items: List[Dict[str, Any]]) -> str:\n    \"\"\"Generate a prompt directing agent to next work.\"\"\"\n    if not items:\n        return \"\"\n\n    high_priority = [i for i in items if i.get(\"priority\") == \"high\"]\n    normal = [i for i in items if i.get(\"priority\") != \"high\"]\n\n    prompt_parts = [\"[WORK AVAILABLE]\"]\n\n    if high_priority:\n        prompt_parts.append(f\"\\nHigh priority ({len(high_priority)}):\")\n        for item in high_priority[:3]:\n            prompt_parts.append(f\"  - {item['description']}\")\n\n    if normal:\n        prompt_parts.append(f\"\\nPending ({len(normal)}):\")\n        for item in normal[:5]:\n            prompt_parts.append(f\"  - {item['description']}\")\n\n    prompt_parts.append(\"\\nContinue with next item. Launch parallel agents if multiple independent items exist.\")\n\n    return \"\\n\".join(prompt_parts)\n\ndef check_for_work() -> Dict[str, Any]:\n    \"\"\"Main entry point - check for work and return hook response.\"\"\"\n    items = find_pending_work()\n\n    if items:\n        prompt = generate_work_prompt(items)\n        return {\n            \"hookSpecificOutput\": {\n                \"hookEventName\": \"PostToolUse\",\n                \"additionalContext\": prompt,\n            }\n        }\n\n    return {\n        \"hookSpecificOutput\": {\n            \"hookEventName\": \"PostToolUse\",\n        }\n    }\n\nif __name__ == \"__main__\":\n    try:\n        result = check_for_work()\n        print(json.dumps(result))\n    except Exception as e:\n        # Always output valid JSON even on error\n        print(json.dumps({\n            \"hookSpecificOutput\": {\n                \"hookEventName\": \"PostToolUse\",\n                \"additionalContext\": f\"[work_seeker error: {str(e)}]\"\n            }\n        }))\n",
        "hooks/workflow-enforcement.py": "#!/usr/bin/env python3\n\"\"\"Generic workflow enforcement hook.\n\nDispatches to active workflow's tool restrictions.\nSupports: debug, pr_comment, iterate workflows.\n\nThis hook consolidates enforcement for all workflow types.\nEach workflow registers its own tool restrictions via WorkflowEngine.\n\"\"\"\n\nimport sys\nimport json\nfrom pathlib import Path\n\nlib_dir = Path(__file__).parent.parent / \"lib\"\nsys.path.insert(0, str(lib_dir))\n\ntry:\n    from workflow_client import workflow_get_state\n    from debug_workflow import DebugWorkflow\n    from pr_comment_workflow import PRCommentWorkflow\n    from implementer_workflow import ImplementerWorkflow\n    from iterate_workflow import is_tool_allowed as iterate_is_tool_allowed, is_active as iterate_is_active\nexcept ImportError:\n    # Fail open if modules not available\n    def workflow_get_state(wf_id):\n        return None\n    def iterate_is_active():\n        return False\n    def iterate_is_tool_allowed(tool, command=None):\n        return True, \"\"\n    DebugWorkflow = None\n    PRCommentWorkflow = None\n    ImplementerWorkflow = None\n\n\n# Workflows using base classes (WorkflowEngine)\nBASE_WORKFLOWS = {\n    \"debug\": DebugWorkflow,\n    \"pr_comment\": PRCommentWorkflow,\n    \"implementer\": ImplementerWorkflow,\n}\n\n\ndef allow(reason: str = \"\") -> dict:\n    result = {\n        \"hookSpecificOutput\": {\n            \"hookEventName\": \"PreToolUse\",\n            \"permissionDecision\": \"allow\"\n        }\n    }\n    if reason:\n        result[\"hookSpecificOutput\"][\"permissionDecisionReason\"] = reason\n    return result\n\n\ndef block(reason: str) -> dict:\n    return {\n        \"hookSpecificOutput\": {\n            \"hookEventName\": \"PreToolUse\",\n            \"permissionDecision\": \"deny\",\n            \"permissionDecisionReason\": reason\n        }\n    }\n\n\ndef main():\n    try:\n        input_data = json.loads(sys.stdin.read())\n    except json.JSONDecodeError:\n        print(json.dumps(allow()))\n        return\n\n    tool_name = input_data.get(\"tool_name\", \"\")\n    tool_input = input_data.get(\"tool_input\", {})\n\n    # Normalize MCP prefix\n    if tool_name.startswith(\"mcp__router__\"):\n        tool_name = tool_name[len(\"mcp__router__\"):]\n\n    # Extract file_path for file-based restrictions\n    file_path = tool_input.get(\"file_path\") or tool_input.get(\"path\") or tool_input.get(\"relative_path\")\n\n    # Check each workflow type using base classes\n    for wf_id, wf_class in BASE_WORKFLOWS.items():\n        if wf_class is None:\n            continue\n\n        state = workflow_get_state(wf_id)\n        if state and state.get(\"active\"):\n            wf = wf_class()\n\n            allowed, reason = wf.is_tool_allowed(tool_name, file_path=file_path)\n\n            if not allowed:\n                phase = state.get(\"phase\", \"unknown\")\n                print(json.dumps(block(f\"[{wf_id.upper()}:{phase}] {reason}\")))\n                return\n\n    # Check iterate workflow (uses its own logic, not base classes yet)\n    if iterate_is_active():\n        command = tool_input.get(\"command\") if tool_name in (\"Bash\", \"native__bash\") else None\n        allowed, reason = iterate_is_tool_allowed(tool_name, command=command)\n        if not allowed:\n            print(json.dumps(block(reason)))\n            return\n\n    print(json.dumps(allow()))\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "hooks/workflow-state-enforcement.py": "#!/usr/bin/env python3\n\"\"\"Workflow state modification enforcement hook.\n\nThis hook prevents subagents from modifying workflow state directly.\nOnly the orchestrator (main session without agentId) should control\nworkflow transitions.\n\nBlocked for subagents:\n- workflow_start, workflow_stop, workflow_update\n- workflow_set_state, workflow_set_value\n\nAllowed for subagents:\n- workflow_get_state, workflow_get_value, workflow_is_active (read-only)\n- agent_set_state (only for their own agent state)\n\"\"\"\n\nimport sys\nimport json\n\n# Workflow state modification tools that subagents cannot use\nWORKFLOW_MODIFY_TOOLS = {\n    # Base names\n    \"workflow_start\",\n    \"workflow_stop\",\n    \"workflow_update\",\n    \"workflow_set_state\",\n    \"workflow_set_value\",\n    # With workflow__ prefix (MCP format)\n    \"workflow__workflow_start\",\n    \"workflow__workflow_stop\",\n    \"workflow__workflow_update\",\n    \"workflow__workflow_set_state\",\n    \"workflow__workflow_set_value\",\n}\n\n# Agent state modification tool\nAGENT_SET_STATE_TOOLS = {\n    \"agent_set_state\",\n    \"workflow__agent_set_state\",\n}\n\n\ndef normalize_tool_name(tool_name: str) -> str:\n    \"\"\"Remove mcp__router__ prefix if present.\"\"\"\n    if tool_name.startswith(\"mcp__router__\"):\n        return tool_name[len(\"mcp__router__\"):]\n    return tool_name\n\n\ndef allow(reason: str = \"\") -> dict:\n    \"\"\"Return allow decision.\"\"\"\n    result = {\n        \"hookSpecificOutput\": {\n            \"hookEventName\": \"PreToolUse\",\n            \"permissionDecision\": \"allow\"\n        }\n    }\n    if reason:\n        result[\"hookSpecificOutput\"][\"permissionDecisionReason\"] = reason\n    return result\n\n\ndef block(reason: str) -> dict:\n    \"\"\"Return block decision.\"\"\"\n    return {\n        \"hookSpecificOutput\": {\n            \"hookEventName\": \"PreToolUse\",\n            \"permissionDecision\": \"deny\",\n            \"permissionDecisionReason\": reason\n        }\n    }\n\n\ndef main():\n    \"\"\"Main enforcement logic.\"\"\"\n    try:\n        input_data = json.loads(sys.stdin.read())\n    except json.JSONDecodeError:\n        print(json.dumps(allow()))\n        return\n\n    tool_name = input_data.get(\"tool_name\", \"\")\n    tool_input = input_data.get(\"tool_input\", {})\n    agent_id = input_data.get(\"agentId\")  # Present if this is a subagent\n\n    # Normalize tool name\n    normalized_tool = normalize_tool_name(tool_name)\n\n    # If not a subagent (no agentId), allow everything\n    if not agent_id:\n        print(json.dumps(allow()))\n        return\n\n    # Subagent context - check for blocked tools\n\n    # Block workflow state modification tools\n    if normalized_tool in WORKFLOW_MODIFY_TOOLS:\n        print(json.dumps(block(\n            f\"[SUBAGENT BLOCKED] {normalized_tool}: Subagents cannot modify workflow state. \"\n            f\"Only the orchestrator controls workflow transitions.\"\n        )))\n        return\n\n    # For agent_set_state, only allow if modifying own state\n    if normalized_tool in AGENT_SET_STATE_TOOLS:\n        target_agent_id = tool_input.get(\"agent_id\", \"\")\n        if target_agent_id != agent_id:\n            print(json.dumps(block(\n                f\"[SUBAGENT BLOCKED] {normalized_tool}: Subagent '{agent_id}' cannot modify \"\n                f\"another agent's state ('{target_agent_id}'). Subagents can only modify their own state.\"\n            )))\n            return\n\n    # All other tools are allowed\n    print(json.dumps(allow()))\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "lib/README.md": "# /home/fearsidhe/.claude/plugins/agent-swarm/lib\n\nShared utilities and scripts for Claude Code operations.\n\n## Files\n\n### `mcp_bridge.py`\nProgrammatic access to MCP tools from Python scripts.\n\n**Native helpers (fast, no MCP required):**\n- `native_glob(pattern, path)` - Glob pattern matching\n- `native_grep(pattern, path, ...)` - Ripgrep-based search\n\n**MCP protocol support (for batching MCP tool calls):**\n- `call_mcp(tool_name, arguments)` - Call MCP tools programmatically\n- `close_all_servers()` - Cleanup function\n- `MCPBridge` class - Convenience wrapper\n\n**Example:**\n```python\nimport sys\nsys.path.insert(0, '/home/fearsidhe/.claude/plugins/agent-swarm/lib')\nfrom mcp_bridge import native_glob, native_grep\n\n# Fast local operations\nfiles = native_glob(\"**/*.py\", \"/project\")\nresults = native_grep(\"TODO\", \"/project\", output_mode=\"content\")\n```\n\n## scripts/\n\nCommon batch operation scripts.\n\n### `batch_search.py`\nSearch multiple patterns efficiently.\n```bash\npython3 /home/fearsidhe/.claude/plugins/agent-swarm/lib/scripts/batch_search.py \"TODO\" \"FIXME\" \"XXX\" --path src/\n```\n\n### `batch_glob.py`\nFind files matching multiple patterns.\n```bash\npython3 /home/fearsidhe/.claude/plugins/agent-swarm/lib/scripts/batch_glob.py \"*.py\" \"*.md\" --path /project\n```\n\n## Usage in CLAUDE.md Scripts\n\nWhen CLAUDE.md instructs you to write scripts, use these utilities:\n\n```python\n#!/usr/bin/env python3\nimport sys\nsys.path.insert(0, '/home/fearsidhe/.claude/plugins/agent-swarm/lib')\nfrom mcp_bridge import native_glob, native_grep\n\n# Your batch operation here\nfiles = native_glob(\"**/*.py\", \".\")\nfor f in files:\n    results = native_grep(\"class \", f, output_mode=\"count\")\n    print(f\"{f}: {results['total']} classes\")\n```\n\n## Installation\n\nThis directory should be in your dotclaude git repo:\n```bash\ncd ~/.claude\ngit add lib/\ngit commit -m \"Add lib infrastructure\"\ngit push\n```\n",
        "skills/ctx/SKILL.md": "---\nname: ctx\ndescription: View and manage hierarchical context for the current directory\nuser_invocable: true\n---\n\n# /ctx - Hierarchical Context Viewer\n\nShows the resolved context for the current working directory, aggregated from all levels of the hierarchy.\n\n## Usage\n\n```\n/ctx           # Show resolved context\n/ctx tree      # Show context hierarchy as tree\n/ctx edit      # Edit context at current scope\n```\n\n## What This Shows\n\nContext is resolved by walking up from the current directory:\n\n1. **Feature/Component level** - `.context/CONTEXT.md` in current or parent dirs\n2. **Repo level** - `.claude/CONTEXT.md` at repository root\n3. **Project level** - Context above the repo\n4. **User level** - `~/.claude/CONTEXT.md` for global preferences\n\nEach level can:\n- **Inherit** from parent (default)\n- **Override** specific sections with `@override: section_name`\n- **Ignore** parent sections with `@ignore: section_name`\n\n## Implementation\n\nWhen invoked, run:\n\n```bash\npython3 ~/.claude/plugins/agent-swarm/ctx/resolver.py resolve .\n```\n\nFor tree view:\n```bash\npython3 ~/.claude/plugins/agent-swarm/ctx/resolver.py tree .\n```\n\n## Context File Format\n\n```markdown\n# Context: [Scope Name]\n\n@inherit: true           # Default, inherit from parent\n@override: conventions   # Override this section completely\n@priority: high          # Mark as high priority\n\n## Purpose\nWhat this scope is for.\n\n## Conventions\nCoding standards for this scope.\n\n## Patterns\nCommon patterns used here.\n\n## Pitfalls\nKnown issues to avoid.\n```\n\n## Creating Context\n\nTo add context at current scope:\n\n1. Create `.context/CONTEXT.md` in the directory\n2. Add relevant sections\n3. Use `@override` for sections that shouldn't merge with parent\n\n## Memory vs Context\n\n- **CONTEXT.md** - Static, intentionally written knowledge\n- **MEMORY.md** - Distilled learnings from sessions (see `/distill`)\n",
        "skills/debug/SKILL.md": "---\nname: debug\ndescription: Systematic debugging workflow with root cause verification before fixing\nuser_invocable: true\n---\n\n# Debug Workflow\n\nRoot cause verification workflow: understand the bug, prove the cause, then fix.\n\n## Initialize (REQUIRED FIRST STEP)\n\n```bash\npython3 ~/.claude/plugins/agent-swarm/lib/debug_workflow.py start \"$ARGUMENTS\"\n```\n\nThis creates the workflow state and sets you to the TRIAGE phase.\n\n## Flow\n\n```\nTRIAGE → REPRODUCE → HYPOTHESIZE → [adversary] → PROVE → [adversary] → FIX → [adversary] → VERIFY → PUSH → CHECK_STATUS → DONE\n   ↑                      ↑                         ↑\n   └── can't reproduce ───┴── prediction fails ─────┴── verification fails\n```\n\n## Phases\n\n| Phase | Purpose | Allowed Tools | Blocked | Required Outputs |\n|-------|---------|---------------|---------|------------------|\n| **TRIAGE** | Understand bug context | Read, Glob, Grep, WebSearch, WebFetch | Edit, Write | severity, affected_components, error_artifacts |\n| **REPRODUCE** | Create failing test | Read, Glob, Grep, Edit, Write, Bash | - | failing_test |\n| **HYPOTHESIZE** | Form root cause theory | Read, Glob, Grep | Edit, Write | hypothesis, prediction |\n| **PROVE** | Verify hypothesis | Read, Glob, Grep, Bash | Edit, Write | prediction_confirmed, mechanism_traced, alternative_ruled_out |\n| **FIX** | Implement fix | Read, Glob, Grep, Edit, Write, Bash | - | - |\n| **VERIFY** | Run tests/lint | Read, Glob, Grep, Bash | Edit, Write | tests_pass, lint_pass |\n| **PUSH** | Push changes | Bash | - | - |\n| **CHECK_STATUS** | Verify CI/reviews | Read, Bash | Edit, Write | ci_pass, no new_review_comments |\n| **DONE** | Complete | - | - | - |\n\n## File Restrictions\n\nIn **REPRODUCE** phase, editing is limited to test files:\n- `tests/**`, `*_test.py`, `test_*.py`\n- `conftest.py`, `fixtures/**`, `mocks/**`\n\nThis prevents \"fixing\" the bug before proving you understand it.\n\n## Adversary Gates\n\nThree phases have adversary gates that challenge your work:\n\n1. **HYPOTHESIZE** - Adversary challenges your hypothesis\n   - \"What alternative explanations exist?\"\n   - \"How does your prediction distinguish this from X?\"\n\n2. **PROVE** - Adversary verifies your proof\n   - \"Did the prediction actually confirm the mechanism?\"\n   - \"What other evidence supports this?\"\n\n3. **FIX** - Adversary checks fix matches proof\n   - \"Does this fix address the proven root cause?\"\n   - \"Could this fix work for the wrong reason?\"\n\n## Kickback Logic\n\n| From | To | Trigger |\n|------|-----|---------|\n| REPRODUCE | TRIAGE | Can't reproduce bug |\n| PROVE | HYPOTHESIZE | Prediction not confirmed |\n| VERIFY | PROVE | Tests/lint fail |\n| CHECK_STATUS | PROVE | CI fails or new review comments |\n\nKickbacks force you to re-examine your understanding rather than repeatedly tweaking the fix.\n\n## CLI Commands\n\n```bash\n# Start workflow\npython3 lib/debug_workflow.py start \"bug description\"\n\n# Check status\npython3 lib/debug_workflow.py status\n\n# Get current phase\npython3 lib/debug_workflow.py phase\n\n# Set phase manually (recovery)\npython3 lib/debug_workflow.py set-phase <phase>\n\n# Record triage outputs\npython3 lib/debug_workflow.py triage <severity> \"<components>\" \"<artifacts>\"\n\n# Record hypothesis\npython3 lib/debug_workflow.py hypothesis \"<hypothesis>\" \"<prediction>\"\n\n# Record verification results\npython3 lib/debug_workflow.py verify <tests_pass> <lint_pass>\n# Example: python3 lib/debug_workflow.py verify 1 1\n\n# Advance to next phase\npython3 lib/debug_workflow.py advance\n\n# Stop workflow\npython3 lib/debug_workflow.py stop\n```\n\n## Example Session\n\n```bash\n# 1. Start\npython3 lib/debug_workflow.py start \"Login fails with 500 error\"\n\n# 2. TRIAGE - Read logs, identify affected components\n# Record findings:\npython3 lib/debug_workflow.py triage \"high\" \"auth,session\" \"stacktrace,error_log\"\n\n# 3. REPRODUCE - Write a failing test\n# (Edit only test files in this phase!)\npython3 lib/debug_workflow.py advance\n\n# 4. HYPOTHESIZE - Form theory\npython3 lib/debug_workflow.py hypothesis \\\n  \"Session token validation fails on expired refresh tokens\" \\\n  \"Adding logging at validate_token() will show 'expired' before 500\"\n\n# 5. PROVE - Verify prediction\n# Run the test with logging, confirm prediction\npython3 lib/debug_workflow.py advance\n\n# 6. FIX - Implement the fix\npython3 lib/debug_workflow.py advance\n\n# 7. VERIFY - Run tests\npytest && ruff check .\npython3 lib/debug_workflow.py verify 1 1\npython3 lib/debug_workflow.py advance\n\n# 8. PUSH\ngit add -A && git commit -m \"fix: handle expired refresh tokens\"\ngit push\npython3 lib/debug_workflow.py advance\n\n# 9. CHECK_STATUS - Wait for CI, check reviews\npython3 lib/debug_workflow.py advance\n\n# 10. DONE\n```\n\n## DO NOT\n\n- Skip REPRODUCE (no test = no proof the bug exists)\n- Edit production code in HYPOTHESIZE or PROVE phases\n- Fix before proving the root cause\n- Ignore kickbacks - they exist because your understanding was wrong\n- Bypass adversary gates - they catch flawed reasoning\n\n## Why This Matters\n\nWithout root cause verification:\n- Fixes often address symptoms, not causes\n- Same bug resurfaces in different form\n- Time wasted on trial-and-error fixes\n\nThis workflow forces you to **prove understanding before acting**.\n\n## Permission Awareness\n\nAt task start, check workflow state for active permissions:\n\n1. **Check active workflow**: `get_active_workflow_id()` returns current workflow\n2. **Get permissions**: `get_permissions(workflow_id)` returns PermissionStore\n3. **Verify tool access**: `is_tool_allowed(tool_name, **context)` before operations\n\n**Self-enforcement**: The phase table above shows allowed/blocked tools per phase. Do not attempt blocked operations - they exist to enforce the \"understand before fixing\" discipline.\n\n**Programmatic check** (lib/permission_query.py):\n```python\nfrom permission_query import get_permissions, is_tool_allowed\n\n# Check if Edit is allowed in current phase\nallowed, reason = is_tool_allowed(\"Edit\", file_path=\"src/auth.py\")\nif not allowed:\n    print(f\"Blocked: {reason}\")  # e.g., \"Edit blocked in HYPOTHESIZE phase\"\n```\n\n**File restrictions**: In REPRODUCE phase, editing is limited to test files. The permission system enforces this via file path patterns.\n",
        "skills/distill/SKILL.md": "---\nname: distill\ndescription: Distill session episodes into persistent memory patterns\nuser_invocable: true\n---\n\n# /distill - Memory Distillation\n\nTransforms episodic session logs into refined semantic memory patterns.\n\n## Usage\n\n```\n/distill           # Distill episodes at current scope\n/distill show      # Show current memory without distilling\n/distill episodes  # Show pending episodes awaiting distillation\n```\n\n## How It Works\n\n### Episode Collection\n\nDuring agent work, learnings are logged to `.context/EPISODES.md`:\n\n```markdown\n## Session: 2026-01-10T14:30:00Z\n- **Task**: Fix authentication bug\n- **Outcome**: success\n- **Learnings**:\n  - JWT tokens need refresh handling in middleware\n  - Error messages should include request ID\n```\n\n### Distillation Process\n\nWhen `/distill` runs:\n\n1. **Extract patterns** from each episode's learnings\n2. **Classify** as pattern, pitfall, preference, or approach\n3. **Match** against existing patterns in memory\n4. **Reinforce** matching patterns (increases confidence)\n5. **Add** new patterns with low initial confidence\n6. **Decay** old patterns not recently reinforced\n7. **Prune** patterns below confidence threshold\n\n### Memory Output\n\nResults are saved to `.context/MEMORY.md`:\n\n```markdown\n# Memory: [Scope]\n\n## Patterns Observed\n- JWT tokens need refresh handling\n  Confidence: high | Last reinforced: 2026-01-10\n\n## Pitfalls Discovered\n- Avoid storing tokens in localStorage\n  Confidence: medium | Last reinforced: 2026-01-08\n```\n\n## Implementation\n\nWhen invoked, run:\n\n```bash\npython3 ~/.claude/plugins/agent-swarm/context/memory.py distill .\n```\n\nFor showing memory:\n```bash\npython3 ~/.claude/plugins/agent-swarm/context/memory.py show .\n```\n\nFor pending episodes:\n```bash\npython3 ~/.claude/plugins/agent-swarm/context/memory.py episodes .\n```\n\n## Logging Learnings\n\nAgents can log learnings by including in their output:\n\n```\nLEARNING: [description of pattern, pitfall, or approach]\n```\n\nThese are captured by post-task hooks and added to EPISODES.md.\n\n## Confidence Mechanics\n\n| Confidence | Meaning |\n|------------|---------|\n| 0.0 - 0.2 | Uncertain, may be pruned |\n| 0.2 - 0.4 | Low, needs reinforcement |\n| 0.4 - 0.7 | Medium, established pattern |\n| 0.7 - 0.95 | High, well-validated |\n\n- **Reinforcement**: Each observation increases confidence\n- **Decay**: Patterns not seen in 30+ days lose confidence\n- **Pruning**: Patterns below 0.2 are removed\n\n## Automatic Distillation\n\nDistillation triggers automatically when:\n- Episode count exceeds threshold (default: 10)\n- Session ends (if configured)\n- Manually via `/distill` command\n",
        "skills/implement/SKILL.md": "---\nname: implement\ndescription: Use as default workflow when no specialized workflow (debug, iterate, pr-comment) applies. For general implementation tasks.\nuser_invocable: true\n---\n\n# Implement Workflow\n\nSimple default workflow for general implementation. Use when specialized workflows don't apply.\n\n## Initialize\n\n```bash\npython3 ~/.claude/plugins/agent-swarm/lib/implementer_workflow.py $ARGUMENTS\n```\n\n## Flow\n\n```\nWORK → VERIFY → DONE\n     ↑    |\n     +----+ (if tests/lint fail)\n```\n\n## Phases\n\n| Phase | Purpose | Tools |\n|-------|---------|-------|\n| **work** | Do the task | All tools allowed |\n| **verify** | Run tests and lint | All tools (can fix issues) |\n| **done** | Complete | - |\n\n## When to Use\n\n- Subagent doing implementation work\n- General task without specific workflow\n- Default when no better workflow applies\n\n## CLI\n\n```bash\n# Start\npython3 lib/implementer_workflow.py start \"Add feature X\"\n\n# With custom ID (for parallel agents)\npython3 lib/implementer_workflow.py --id agent-123 start \"Task\"\n\n# Check status\npython3 lib/implementer_workflow.py status\n\n# Record verification\npython3 lib/implementer_workflow.py verify 1 1  # tests=pass, lint=pass\n\n# Advance\npython3 lib/implementer_workflow.py advance\n```\n\n## Exit Conditions\n\n| Condition | Trigger |\n|-----------|---------|\n| `done` | Verification passed |\n| `max_iterations` | 3 kickbacks |\n| `user_stopped` | Manual stop |\n\n## Permission Awareness\n\nAt task start, check workflow state for active permissions:\n\n1. **Check active workflow**: `get_active_workflow_id()` returns current workflow\n2. **Get permissions**: `get_permissions(workflow_id)` returns PermissionStore\n3. **Verify tool access**: `is_tool_allowed(tool_name, **context)` before operations\n\n**Programmatic check** (lib/permission_query.py):\n```python\nfrom permission_query import get_permissions, is_tool_allowed\n\n# Check if Edit is allowed\nallowed, reason = is_tool_allowed(\"Edit\", file_path=\"src/main.py\")\nif not allowed:\n    print(f\"Blocked: {reason}\")\n```\n\n**Self-enforcement**: Even in `work` phase with all tools allowed, respect any file path restrictions passed by the orchestrator.\n",
        "skills/iterate/SKILL.md": "---\nname: iterate\ndescription: Autonomous TDD implementation workflow with phase gates\nuser_invocable: true\n---\n\n# Iterate Workflow\n\nTDD development loop with phase gates. Works autonomously until exit conditions met.\n\n## Initialize (REQUIRED FIRST STEP)\n\n**Run this command immediately to start the workflow:**\n\n```bash\npython3 ~/.claude/plugins/agent-swarm/lib/iterate_workflow.py $ARGUMENTS\n```\n\nThe workflow auto-initializes when invoked. This creates the state file that subagents need for TDD context.\n\n## Flow\n\n**With ORCHESTRATE (main agent coordinates workers):**\n```\nORCHESTRATE ──┬──→ [spawn subagents] ──→ queue empty? ──→ done\n              │                              ↓\n              └──────────── no ←─────────────┘\n```\n\n**Subagents (TDD loop):**\n```\ntest_writing → implement → test → review → done\n      ↑            ↑         |       |\n      |            |         v       v\n      +-- coverage +-- fail -+  issues\n```\n\n## Phases\n\n| Phase | Purpose | Allowed Tools |\n|-------|---------|---------------|\n| **orchestrate** | Main agent coordinates workers | Read, Task, TaskOutput, TodoWrite (NO Edit/Write/Bash!) |\n| **test_writing** | Write tests first (spec) | Read, Glob, Grep, Edit, Write, Bash |\n| **implement** | Make tests pass | Read, Glob, Grep, Edit, Write, Bash |\n| **test** | Run pytest, lint, coverage | Read, Glob, Grep, Bash (no editing!) |\n| **review** | Fix Greptile issues | Read, Glob, Grep, Edit, Write, Bash |\n\n**Git Operations:** Commits happen in the REVIEW phase, not during implementation. Subagents make file changes; the orchestrator commits after all work is verified.\n\n## Orchestrator Role\n\n**When in ORCHESTRATE phase, you NEVER do implementation tasks. Edit/Write/Bash are BLOCKED.**\n\nThe ORCHESTRATE phase enforces the orchestrator role through tool restrictions:\n- ✅ Read, Task, TaskOutput, TodoWrite - coordination tools\n- ❌ Edit, Write, NotebookEdit, Bash - blocked, spawn agents instead\n\n### Rules\n- 1 task → spawn 1 agent\n- 5 tasks → spawn 5 agents in parallel\n- No exceptions - you cannot edit code yourself\n\n### Orchestrator responsibilities (ORCHESTRATE phase)\n- Read specs/queue files\n- Spawn subagents via Task tool\n- Monitor completions via TaskOutput\n- Track progress via TodoWrite\n- Check completion: `is_orchestration_complete()` → queue empty AND no workers\n\n### What the orchestrator does NOT do\n- **Does NOT evaluate agent work quality** - agents are responsible for their own verification\n- **Does NOT run tests** - test/review agents handle that\n- **Does NOT review code** - reviewer agents handle that\n- Just spawns agents, takes output, updates queue\n\nFuture: Autokill feature may terminate long-running or failing agents.\n\n### Insufficient context → Go back to INTAKE\nIf the orchestrator cannot write quality prompts because it lacks context:\n\n**Orchestrator does NOT explore in ORCHESTRATE.** Go back to INTAKE phase.\n\nINTAKE is where orchestrator context gathering happens:\n- Explore codebase\n- Read relevant files\n- Understand scope and patterns\n- Gather requirements\n\n**If you're in ORCHESTRATE and can't write good prompts:**\n```bash\npython3 ~/.claude/plugins/agent-swarm/lib/iterate_workflow.py set-phase intake\n```\n\nThen complete intake properly before returning to orchestrate.\n\n**Signs you rushed intake:**\n- Writing vague prompts like \"implement spec.md\"\n- Not knowing which files to reference\n- Unclear on existing patterns\n- Missing acceptance criteria\n\n### Subagents explore current code\nSubagents are close to the code - they explore their specific area of the **current codebase**:\n- Read related files to understand patterns\n- Grep for similar implementations\n- Check for side effects before changes\n- Find existing code to follow\n\n**NOT web searches.** Subagents understand the code they're changing, not researching external topics.\nWeb/external research happens in RESEARCH phase (orchestrator responsibility).\n\n### Subagent responsibilities (TDD loop phases)\n- Write tests (test_writing phase)\n- Write/modify code (implement phase)\n- Fix review issues (review phase)\n\n## Explore-Before-Prompt Pattern\n\n**When orchestrator has insufficient context for good prompts, use this pattern WITHIN a task.**\n\nUnlike going back to INTAKE (full phase change), this pattern lets the orchestrator spawn an explorer for ONE task, get context, then spawn implementer with enriched prompt.\n\n### When to use exploration\n\nThe orchestrator should spawn an explorer agent BEFORE spawning an implementer when:\n- Task references files/modules not yet read by orchestrator\n- Task description is vague (< 10 words, no file paths)\n- Task uses vague verbs (\"improve\", \"fix\") without specifics\n- Task mentions \"similar to X\" without providing examples\n- No code snippets or file paths in available context\n\n**Detection helper:**\n```python\nfrom exploration_helpers import needs_exploration\n\nif needs_exploration(task, context):\n    # Spawn explorer first, then implementer\n```\n\n### Exploration workflow\n\n```\n1. Detect: needs_exploration(task, context) → True\n2. Explore: Spawn explorer with targeted prompt\n3. Enrich: Add explorer output to context\n4. Implement: Spawn implementer with enriched prompt\n```\n\n**Example:**\n\n```python\n# Bad: vague prompt\nTask(\n    description=\"Fix auth system\",\n    subagent_type=\"agent-swarm:implementer\",\n    prompt=\"Fix the authentication system bugs\"\n)\n\n# Good: explore-first\nfrom exploration_helpers import needs_exploration, detect_exploration_type, format_explorer_prompt\n\ntask = {\"description\": \"Fix auth system\"}\ncontext = {\"files_read\": [], \"code_snippets\": []}\n\nif needs_exploration(task, context):\n    # Spawn explorer\n    exploration_type = detect_exploration_type(task)\n    explorer_prompt = format_explorer_prompt(exploration_type, task)\n\n    explorer_result = Task(\n        description=\"Explore auth system\",\n        subagent_type=\"agent-swarm:explorer\",\n        model=\"haiku\",\n        token_budget=50000,\n        prompt=explorer_prompt\n    )\n\n    # Wait for explorer result, then spawn implementer\n    enriched_context = {**context, \"exploration\": explorer_result}\n\n    Task(\n        description=\"Fix auth token expiration\",\n        subagent_type=\"agent-swarm:implementer\",\n        model=\"opus\",\n        token_budget=100000,\n        prompt=f\"\"\"Fix auth token expiration bug.\n\n**Context from Exploration:**\n{explorer_result}\n\n**Requirements:**\n- Token should expire after 24h\n- Refresh tokens should work\n- Tests must pass\n\nFollow patterns shown in exploration output.\"\"\"\n    )\n```\n\n### Exploration types\n\n| Type | When to Use | What Explorer Returns |\n|------|-------------|----------------------|\n| `file_discovery` | Task mentions files/modules | File paths, key functions, entry points |\n| `pattern_matching` | Task says \"similar to X\" | Existing implementations, patterns, helpers |\n| `dependency_mapping` | Task modifies existing code | Callers, imports, side effects, tests |\n| `context_enrichment` | Task is vague | Directory structure, key files, patterns |\n\n**Prompt templates available in `lib/prompt_templates.py`:**\n```python\nfrom prompt_templates import EXPLORATION_PROMPTS, IMPLEMENTER_PROMPTS\n\n# Get exploration prompt template\nexplorer_prompt = EXPLORATION_PROMPTS[\"file_discovery\"].format(topic=\"authentication\")\n\n# Get implementer prompt template\nimplementer_prompt = IMPLEMENTER_PROMPTS[\"with_exploration\"].format(\n    task_description=\"Fix auth bug\",\n    explorer_output=\"...\",\n    requirements=\"...\",\n    suggested_files=\"...\"\n)\n```\n\n### When NOT to use this pattern\n\n- **Orchestrator phase change needed**: If ALL tasks need context, go back to INTAKE phase\n- **Subagent exploration sufficient**: Let implementers explore their specific area during work\n- **Context already available**: If you have files, patterns, examples already\n\n**Rule of thumb:**\n- Missing context for 1-2 tasks? → Explore-before-prompt pattern\n- Missing context for ALL tasks? → Go back to INTAKE phase\n\n\n\n## Task Queue (Fundamental)\n\n**ALL work flows through the task queue.** This is not optional infrastructure - it's the enforcement mechanism that ensures subagents are spawned.\n\n### Queue Flow\n\n```\nIMPLEMENT phase:\n  Identify work → Add to queue → Spawn agents → Mark done → Push when queue empty\n\nREVIEW phase:\n  Get comments → Add to queue → Spawn agents → Mark done → Push when queue empty\n```\n\n### Queue Operations\n\n| Operation | When | API |\n|-----------|------|-----|\n| Add task | After decomposing work | `workflow_queue.add_task(task)` |\n| Spawn agents | After populating queue | `Task(...)` for each item (up to max parallel) |\n| Mark done | After agent completes | `workflow_queue.mark_done(task_id)` |\n| Check empty | Before push | `workflow_queue.all_complete(pr_id)` |\n\n### Push Triggers\n\nPush happens when:\n- Implementation batch is complete (queue empty after implement phase)\n- Review comments are all addressed (queue empty after review fixes)\n\n**Do NOT push after each task.** Wait for the batch.\n\n### Dynamic Queue Updates\n\nThe queue can grow during execution:\n- New implementation tasks discovered during work\n- New review comments after push\n- Dependencies identified by subagents\n\nThe orchestrator monitors and repopulates as needed.\n\n## Parallel Execution\n\nSpawn up to `config.orchestrate.max_agents` agents in ONE message block to run simultaneously:\n\n```\nTask(description=\"Implement module A\", subagent_type=\"agent-swarm:implementer\", prompt=\"...\")\nTask(description=\"Implement module B\", subagent_type=\"agent-swarm:implementer\", prompt=\"...\")\nTask(description=\"Implement module C\", subagent_type=\"agent-swarm:implementer\", prompt=\"...\")\n```\n\nEven for a single task:\n```\nTask(description=\"Implement module A\", subagent_type=\"agent-swarm:implementer\", prompt=\"...\")\n```\n\n**Non-blocking monitoring:** Use `TaskOutput` with `block=false` to check agent status without waiting. The orchestrator continues working while agents run in background.\n\n```python\n# Good: non-blocking check\nresult = TaskOutput(task_id=agent_id, block=False, timeout=1000)\nif result.status == \"completed\":\n    # Process result\n\n# Bad: blocking wait\nresult = TaskOutput(task_id=agent_id, block=True, timeout=120000)  # DON'T DO THIS\n```\n\n## Subagent Prompting\n\nWhen spawning implementer agents, your prompt must include:\n\n1. **Architectural context** - How the component fits in the system design\n2. **Constraints** - What NOT to do and why (prevent logical-but-wrong fixes)\n3. **Design intent** - The \"why\" behind existing code/restrictions\n4. **TDD instruction** - Agents must write tests FIRST, then implement\n\n**IMPORTANT:** Every implementer prompt MUST include:\n```\n**TDD:** Write tests FIRST that specify the expected behavior, then implement code to make tests pass, then verify all tests pass.\n```\n\nOne agent handles the complete TDD cycle. Do NOT spawn separate agents for test-writing and implementation.\n\n**Example prompt structure:**\n```\n**Context:** [How this component relates to the system architecture]\n\n**Constraint:** Do NOT [specific thing to avoid] because [reason].\n\n**Task:** [Specific work to do]\n\n**TDD:** Write tests FIRST that specify the expected behavior, then implement code to make tests pass, then verify all tests pass.\n\n**Verification:** [How to verify the fix is correct]\n```\n\nWithout architectural context, agents may make fixes that pass tests but break system invariants.\n\n## Kick-back Logic\n\nAfter `test` phase, results determine next phase:\n\n| Result | Next Phase | Why |\n|--------|------------|-----|\n| All pass | review | Ready for code review |\n| Coverage low | test_writing | Need more tests |\n| Tests fail | implement | Fix code |\n| Lint fail | implement | Fix code |\n\nAfter `review` phase:\n\n| Result | Next Phase | Why |\n|--------|------------|-----|\n| Clean | done | Workflow complete |\n| Issues | implement | Fix review comments |\n\n## Usage\n\n```bash\n# Start iterate workflow\n/iterate \"Add user validation feature\"\n\n# With custom max iterations\n/iterate --max 10 \"Refactor auth module\"\n```\n\n## CLI Commands\n\n```bash\n# Start workflow\npython3 ~/.claude/plugins/agent-swarm/lib/iterate_workflow.py start \"task description\" [max_iterations]\n\n# Check status\npython3 ~/.claude/plugins/agent-swarm/lib/iterate_workflow.py status\n\n# Get current phase\npython3 ~/.claude/plugins/agent-swarm/lib/iterate_workflow.py phase\n\n# Advance to next phase\npython3 ~/.claude/plugins/agent-swarm/lib/iterate_workflow.py advance\n\n# Record test results (after running pytest, lint, coverage)\npython3 ~/.claude/plugins/agent-swarm/lib/iterate_workflow.py test <tests> <lint> <coverage>\n# Use 1=pass, 0=fail. Example: test 1 1 0 (tests pass, lint pass, coverage fail)\n\n# Record review status\npython3 ~/.claude/plugins/agent-swarm/lib/iterate_workflow.py review <clean>\n# Use 1=clean, 0=issues\n\n# Stop workflow\npython3 ~/.claude/plugins/agent-swarm/lib/iterate_workflow.py stop\n```\n\n## Test Phase Verification\n\nIn the `test` phase, run these checks:\n\n```bash\n# 1. Run tests\npytest tests/ -v\n\n# 2. Run lint\nruff check .\n\n# 3. Run coverage\npytest --cov=. --cov-report=term-missing\n\n# 4. Record results\npython3 ~/.claude/plugins/agent-swarm/lib/iterate_workflow.py test 1 1 1  # if all pass\npython3 ~/.claude/plugins/agent-swarm/lib/iterate_workflow.py test 1 0 1  # if lint failed\npython3 ~/.claude/plugins/agent-swarm/lib/iterate_workflow.py test 0 1 1  # if tests failed\n\n# 5. Advance phase (kick-back or proceed based on results)\npython3 ~/.claude/plugins/agent-swarm/lib/iterate_workflow.py advance\n```\n\n## PR Completion Tracking\n\nWork is grouped by logical PRs. The orchestrator MUST track completion properly:\n\nAfter each agent completes:\n1. Update task status: `workflow_queue.mark_done(task_id)`\n2. Check if PR ready: `workflow_queue.all_complete(pr_id)`\n3. Only push when ALL tasks for PR are done\n4. Triggers ONE Greptile review per PR, not per task\n\n**Do NOT:**\n- Push after each individual task completes\n- Trigger multiple reviews for the same PR\n\n## Review Phase\n\n1. Verify all PR tasks complete (see PR Completion Tracking)\n2. Push changes to trigger Greptile review (ONE push per PR)\n3. Check for review comments\n4. If issues found: `python3 ~/.claude/plugins/agent-swarm/lib/iterate_workflow.py review 0` then `advance`\n5. If clean: `python3 ~/.claude/plugins/agent-swarm/lib/iterate_workflow.py review 1` then `advance`\n\n## Orchestrator: Monitoring PRs\n\nAfter a subagent completes and pushes:\n\n### 1. Poll for PR\n```bash\ngh pr list --head <branch-name> --json number,url\n```\n\n### 2. Check for Review Comments\n```bash\n# Get PR reviews and inline comments\ngh pr view <pr-number> --json reviews,comments\n\n# Get line-level review comments (more detailed)\ngh api repos/{owner}/{repo}/pulls/{pr}/comments\n```\n\n### 3. Add Comments to Task Queue\nFor each review comment that requires action:\n- Create a new task with the comment content\n- Include file path and line number\n- Spawn an implementer agent to address it\n\n```python\nfrom workflow_queue import WorkflowQueue\n\nqueue = WorkflowQueue()\nfor comment in review_comments:\n    if requires_action(comment):\n        queue.add_task({\n            \"description\": f\"Fix: {comment['body'][:80]}...\",\n            \"file\": comment.get(\"path\"),\n            \"line\": comment.get(\"line\"),\n            \"comment_id\": comment.get(\"id\"),\n            \"full_comment\": comment[\"body\"]\n        })\n```\n\n### 4. Iterate Until Clean\n- Re-push after fixes\n- Re-poll for new comments\n- Continue until review is clean (no actionable comments)\n\n**Example polling loop:**\n```bash\n# After agent completes and pushes\nPR_NUM=$(gh pr list --head feature-branch --json number -q '.[0].number')\n\n# Get review comments\ngh api repos/owner/repo/pulls/$PR_NUM/comments --jq '.[] | {path, line, body}'\n\n# Check if any unresolved\nCOMMENTS=$(gh api repos/owner/repo/pulls/$PR_NUM/comments | jq 'length')\nif [ \"$COMMENTS\" -gt 0 ]; then\n    # Add to queue and spawn agents\n    python3 ~/.claude/plugins/agent-swarm/lib/iterate_workflow.py review 0\nelse\n    python3 ~/.claude/plugins/agent-swarm/lib/iterate_workflow.py review 1\nfi\n```\n\n## Git Workflow\n\n### Branch Creation (Orchestrator Responsibility)\n\nThe **orchestrator** creates feature branches before spawning subagents. This happens during intake/setup, before entering orchestrate phase (where Bash is blocked):\n\n1. Identify task groups and their branch names\n2. Create each branch: `git checkout -b feature/<task-name>`\n3. Return to the base branch (e.g., `main` or `master`)\n4. Include the branch name in every subagent prompt for that group\n\n**Subagents MUST NOT create branches.** They verify they're on the correct branch and switch if needed, but never `checkout -b`.\n\n**Why orchestrator-owned:** Subagents may skip or hallucinate git operations. Branch creation is a critical operation that must be verified, so the orchestrator handles it directly during setup.\n\n### PR Creation\n\nIn the **review phase**, after all implementation work is complete:\n\n1. Stage all changes: `git add <files>`\n2. Commit with descriptive message: `git commit -m \"feat: <description>\"`\n3. Push branch to remote: `git push -u origin feature/<task-name>`\n4. Create PR: `gh pr create --title \"...\" --body \"...\"`\n5. Greptile reviews the PR automatically\n\n**Note:** PR creation happens ONCE per task group, not per subagent. The orchestrator coordinates this after verifying all subagent work is complete.\n\n## Orchestrator Progress Output\n\nThe orchestrator MUST provide informative output during workflow execution:\n\n### After spawning agents:\n```\n[SPAWNED] {count} agent(s) for: {task_summaries}\n```\n\n### After agent completion:\n```\n[COMPLETE] {agent_description}: {brief_result_summary}\n```\n\n### Before phase transitions:\n```\n[PHASE] {current_phase} → {next_phase} | Reason: {reason}\n```\n\n### On task queue updates:\n```\n[QUEUE] {pending_count} pending, {in_progress_count} in progress, {completed_count} completed\n```\n\nThis output helps users understand workflow progress and aids debugging.\n\n## Exit Conditions\n\n| Condition | Trigger |\n|-----------|---------|\n| `orchestration_complete` | Queue empty AND no active workers (ORCHESTRATE mode) |\n| `review_approved` | Review clean, workflow complete (TDD loop) |\n| `max_iterations` | Hit iteration limit (default: 5) |\n| `user_stopped` | Manual `/iterate stop` |\n\n## DO NOT\n\n- Skip phases (workflow enforces order)\n- Use Edit/Write in test phase (blocked by hook)\n- Use Edit/Write/Bash in ORCHESTRATE phase (blocked - spawn agents instead)\n- Ignore kick-back (follow the loop)\n- Bypass test verification\n- Do implementation work yourself (ALWAYS spawn agents)\n- Spawn agents sequentially (use ONE message block for parallel execution)\n- Split TDD across multiple agents (one agent = complete TDD cycle: test → implement → verify)\n- Use blocking TaskOutput calls (orchestrator spawns and monitors, doesn't wait)\n- Spawn agents without informative progress output\n\n## Permission Awareness\n\nAt task start, check workflow state for active permissions:\n\n1. **Check active workflow**: `get_active_workflow_id()` returns current workflow\n2. **Get permissions**: `get_permissions(workflow_id)` returns PermissionStore\n3. **Verify tool access**: `is_tool_allowed(tool_name, **context)` before operations\n\n**Self-enforcement**: Do not attempt blocked operations. The phase table above shows allowed tools per phase - respect these restrictions even if hooks don't catch violations.\n\n**Programmatic check** (lib/permission_query.py):\n```python\nfrom permission_query import get_permissions, is_tool_allowed\n\n# Check if Edit is allowed\nallowed, reason = is_tool_allowed(\"Edit\", file_path=\"src/main.py\")\nif not allowed:\n    print(f\"Blocked: {reason}\")\n```\n\n**Subagent awareness**: When spawning subagents, include current phase in prompt so they know their tool restrictions.\n",
        "skills/poll/SKILL.md": "# Poll At Interval\n\n**User-Invocable:** Yes (`/poll <seconds> <tool> <condition>`)\n\nControlled polling for async operations. Waits for a condition to be met, checking at specified intervals.\n\n## When to Use\n\n- Waiting for Greptile review to complete\n- Waiting for CI/CD status changes\n- Any async operation that needs status polling\n- Avoiding rate limit blocks from repeated tool calls\n\n## Syntax\n\n```\n/poll <interval_seconds> <tool_name> <condition_field> <expected_value>\n```\n\n**Examples:**\n```\n/poll 30 greptile:get_code_review status COMPLETED\n/poll 60 gh:pr_checks conclusion success\n```\n\n## How It Works\n\n1. Executes the specified tool\n2. Checks if condition is met\n3. If not met, waits `interval_seconds`\n4. Repeats until condition met or max iterations (default: 20)\n5. Returns final result\n\n## Parameters\n\n| Parameter | Description | Default |\n|-----------|-------------|---------|\n| `interval` | Seconds between checks | Required |\n| `tool` | MCP tool to call | Required |\n| `condition_field` | JSON path to check | Required |\n| `expected_value` | Value that signals completion | Required |\n| `max_iterations` | Maximum poll attempts | 20 |\n| `tool_params` | Parameters to pass to tool | {} |\n\n## Bypass Hook Limits\n\nThis skill is whitelisted in combined-enforcement.py to avoid the 6-call limit.\nThe hook recognizes polling patterns and allows controlled repetition.\n\n## Implementation\n\nThe skill spawns a background task that:\n1. Calls the tool with provided params\n2. Extracts `condition_field` from response\n3. Compares to `expected_value`\n4. If match: returns result\n5. If no match: sleeps `interval`, repeats\n\n```python\nimport time\nimport json\n\ndef poll_until(tool_name, tool_params, condition_field, expected_value,\n               interval=30, max_iterations=20):\n    \"\"\"Poll a tool until condition is met.\"\"\"\n    for i in range(max_iterations):\n        result = call_mcp_tool(tool_name, tool_params)\n\n        # Extract nested field (e.g., \"codeReview.status\")\n        value = result\n        for key in condition_field.split('.'):\n            value = value.get(key, {})\n\n        if value == expected_value:\n            return {\"success\": True, \"iterations\": i + 1, \"result\": result}\n\n        if i < max_iterations - 1:\n            time.sleep(interval)\n\n    return {\"success\": False, \"iterations\": max_iterations, \"last_result\": result}\n```\n\n## Integration with Hooks\n\nTo whitelist polling in `combined-enforcement.py`, add to `check_mcp_script_requirement`:\n\n```python\n# Whitelist polling operations (controlled repetition)\nPOLLING_TOOLS = {\n    \"mcp__plugin_greptile_greptile__get_code_review\",\n    \"mcp__plugin_greptile_greptile__list_code_reviews\",\n}\n\nif tool_name in POLLING_TOOLS:\n    # Check if this is a poll skill invocation\n    if state.get(\"poll_skill_active\"):\n        return None  # Allow polling\n```\n",
        "skills/pr-comment/SKILL.md": "---\nname: pr-comment\ndescription: PR review comment workflow - understand the concern before fixing\nuser_invocable: true\n---\n\n# PR Comment Workflow\n\nUnderstanding-first workflow for addressing PR review comments.\n\n## Initialize (REQUIRED FIRST STEP)\n\n```bash\npython3 ~/.claude/plugins/agent-swarm/lib/pr_comment_workflow.py start \"<comment>\" <pr_number>\n```\n\nThis creates the workflow state and sets you to the UNDERSTAND phase.\n\n## Flow\n\n```\nUNDERSTAND → [adversary] → FIX → [adversary] → VERIFY → PUSH → CHECK_REVIEWS → DONE\n     ↑                             ↑                              |\n     └──────── new comments ───────┴──── tests fail ──────────────┘\n```\n\n## Phases\n\n| Phase | Purpose | Allowed Tools | Blocked | Required Outputs |\n|-------|---------|---------------|---------|------------------|\n| **UNDERSTAND** | Grasp reviewer's concern | Read, Glob, Grep | Edit, Write, Bash | articulation, current_code_problem |\n| **FIX** | Implement changes | Read, Glob, Grep, Edit, Write, Bash | - | - |\n| **VERIFY** | Run tests/lint | Read, Glob, Grep, Bash | Edit, Write | tests_pass, lint_pass |\n| **PUSH** | Push changes | Bash | - | - |\n| **CHECK_REVIEWS** | Check for new comments | Read, Bash | Edit, Write | no new_comments |\n| **DONE** | Complete | - | - | - |\n\n## Adversary Gates\n\nTwo phases have adversary gates:\n\n1. **UNDERSTAND** - Adversary validates your understanding\n   - \"Can you articulate the reviewer's concern in your own words?\"\n   - \"What specific problem does the current code have?\"\n   - \"Why is the reviewer's suggestion better than what exists?\"\n\n2. **FIX** - Adversary checks fix matches understanding\n   - \"Does this change address the articulated concern?\"\n   - \"Did you change anything beyond what the reviewer asked?\"\n\n## Required Outputs\n\nBefore leaving UNDERSTAND phase, you must provide:\n\n- **articulation** - Restate the reviewer's concern in your own words\n- **current_code_problem** - What specific problem exists in the current code\n\nThis prevents \"just do what they said\" without understanding why.\n\n## Kickback Logic\n\n| From | To | Trigger |\n|------|-----|---------|\n| VERIFY | FIX | Tests or lint fail |\n| CHECK_REVIEWS | UNDERSTAND | New review comments appear |\n\nIf new comments appear after your push, you go back to UNDERSTAND - not FIX. This ensures you don't just keep tweaking without understanding.\n\n## CLI Commands\n\n```bash\n# Start workflow\npython3 lib/pr_comment_workflow.py start \"reviewer comment\" 123\n\n# Check status\npython3 lib/pr_comment_workflow.py status\n\n# Get current phase\npython3 lib/pr_comment_workflow.py phase\n\n# Record understanding\npython3 lib/pr_comment_workflow.py understand \"<articulation>\" \"<problem>\"\n\n# Record verification\npython3 lib/pr_comment_workflow.py verify <tests_pass> <lint_pass>\n\n# Advance to next phase\npython3 lib/pr_comment_workflow.py advance\n\n# Stop workflow\npython3 lib/pr_comment_workflow.py stop\n```\n\n## Example Session\n\n```bash\n# 1. Start with the review comment\npython3 lib/pr_comment_workflow.py start \\\n  \"This function is doing too much. Consider extracting the validation logic.\" \\\n  42\n\n# 2. UNDERSTAND - Read the code, understand the concern\n# Record your understanding:\npython3 lib/pr_comment_workflow.py understand \\\n  \"Reviewer wants separation of concerns - validation mixed with business logic makes testing harder\" \\\n  \"validate_and_process() has 3 responsibilities: input validation, transformation, and persistence\"\n\n# 3. FIX - Extract validation\npython3 lib/pr_comment_workflow.py advance\n# Make the changes...\n\n# 4. VERIFY\npytest && ruff check .\npython3 lib/pr_comment_workflow.py verify 1 1\npython3 lib/pr_comment_workflow.py advance\n\n# 5. PUSH\ngit add -A && git commit -m \"refactor: extract validation from validate_and_process\"\ngit push\npython3 lib/pr_comment_workflow.py advance\n\n# 6. CHECK_REVIEWS - Wait, check for new comments\npython3 lib/pr_comment_workflow.py advance\n\n# 7. DONE (or kickback to UNDERSTAND if new comments)\n```\n\n## DO NOT\n\n- Start editing before articulating the concern\n- Skip the understanding phase because \"it's obvious\"\n- Make changes beyond what the reviewer asked\n- Ignore kickbacks - new comments mean you missed something\n\n## Why This Matters\n\nCommon anti-patterns this prevents:\n\n1. **Cargo cult fixes** - \"Reviewer said X, so I'll do X\" without understanding why\n2. **Scope creep** - Making extra changes \"while I'm here\"\n3. **Iteration churn** - Pushing fixes that don't address the actual concern\n\nThis workflow ensures you **understand before acting** and **stay focused on the specific feedback**.\n\n## Permission Awareness\n\nAt task start, check workflow state for active permissions:\n\n1. **Check active workflow**: `get_active_workflow_id()` returns current workflow\n2. **Get permissions**: `get_permissions(workflow_id)` returns PermissionStore\n3. **Verify tool access**: `is_tool_allowed(tool_name, **context)` before operations\n\n**Self-enforcement**: The phase table above shows allowed/blocked tools per phase. Do not attempt blocked operations - they exist to enforce the \"understand before fixing\" discipline.\n\n**Programmatic check** (lib/permission_query.py):\n```python\nfrom permission_query import get_permissions, is_tool_allowed\n\n# Check if Edit is allowed in current phase\nallowed, reason = is_tool_allowed(\"Edit\", file_path=\"src/auth.py\")\nif not allowed:\n    print(f\"Blocked: {reason}\")  # e.g., \"Edit blocked in UNDERSTAND phase\"\n```\n\n**UNDERSTAND phase**: Edit/Write/Bash are blocked to force articulation of the reviewer's concern before making changes.\n",
        "skills/remember/SKILL.md": "---\nname: remember\ndescription: Save a learning to persistent memory with automatic scope inference\nuser_invocable: true\n---\n\n# /remember - Save Learning\n\nSaves a pattern, preference, or learning to persistent memory.\n\n## Usage\n\n```\n/remember <thing to remember>\n/remember --scope=global <thing>\n/remember --scope=project <thing>\n```\n\n## How It Works\n\n1. Analyzes the content to infer appropriate scope\n2. Saves to the correct .context/MEMORY.md file\n3. Confirms where it was saved\n\n## Scope Inference\n\n- Contains \"always\", \"never\", \"I prefer\" → user level (~/.claude/MEMORY.md)\n- References specific file paths → component level\n- General repo patterns → repo level\n\n## Implementation\n\nWhen invoked, the skill runs:\n```bash\npython3 ~/.claude/plugins/agent-swarm/context/remember.py \"<content>\" [--scope=<scope>]\n```\n",
        "skills/spawn/SKILL.md": "---\nname: spawn\ndescription: How to spawn subagents correctly. Use this reference when you need to delegate work to a specialized agent.\n---\n\n# Spawning Subagents\n\n## Why Subagents\n\n1. **Model efficiency**: Orchestrator uses opus, subagents use cheaper models\n2. **Context isolation**: Subagent work doesn't flood main context\n3. **Parallelization**: Multiple subagents can work simultaneously\n4. **Focus**: Each agent has a specific role with clear constraints\n\n## How to Spawn\n\nUse the Task tool with these parameters:\n\n```json\n{\n  \"description\": \"3-5 word summary\",\n  \"prompt\": \"Detailed instructions for the agent\",\n  \"subagent_type\": \"Explore|Plan|general-purpose\",\n  \"model\": \"haiku|sonnet|opus\"\n}\n```\n\n## Agent Selection\n\n| Need | Agent | Default Model | subagent_type |\n|------|-------|---------------|---------------|\n| Find code/files | explorer | haiku | Explore |\n| Web/doc research | researcher | haiku | general-purpose |\n| Plan implementation | architect | sonnet | Plan |\n| Write code | implementer | sonnet | general-purpose |\n| Review changes | reviewer | sonnet | general-purpose |\n| Fix bugs | debugger | sonnet | general-purpose |\n| Git operations | git-agent | haiku | general-purpose |\n\n## Self-Model-Selection\n\nAgents can downgrade their model when task is simpler than expected:\n\n```\n[In subagent prompt, agent can say:]\n\"This is straightforward - spawning with haiku instead of sonnet\"\n```\n\n**Model selection criteria:**\n\n| Complexity | Indicators | Model |\n|------------|------------|-------|\n| trivial | Single pattern search, one file change, simple command | haiku |\n| simple | Clear logic, existing pattern to follow, <50 lines | haiku or sonnet |\n| medium | Some reasoning needed, multiple considerations | sonnet |\n| complex | Architectural decisions, novel patterns, tricky edge cases | sonnet |\n| very complex | Cross-cutting concerns, security implications | opus (rare, ask orchestrator) |\n\n**Rules:**\n- Can always downgrade (sonnet → haiku)\n- Never upgrade without asking orchestrator\n- Default to cheaper when unsure\n- If haiku struggles, retry with sonnet (not automatic, must be explicit)\n\n**Example:**\n```\nOrchestrator spawns implementer (default: sonnet) to add a button.\nImplementer sees: \"Just adding one onClick handler to existing component.\"\nImplementer says: \"Trivial task, using haiku\" and spawns sub-agent with haiku.\n```\n\n## Prompt Structure\n\nEvery subagent prompt MUST include:\n\n1. **Task**: What exactly to do (one clear objective)\n2. **Scope**: What files/areas to touch\n3. **Output**: What to return (be specific about format)\n4. **Constraints**: What NOT to do\n\n### Example Prompts\n\n**Explorer (finding code):**\n```\nFind all authentication-related code.\n\nScope: src/ directory\nOutput: List of file:line references with one-line descriptions\nConstraints: Don't read file contents, just locate. Use Serena tools.\n```\n\n**Implementer (writing code):**\n```\nAdd input validation to the login form.\n\nScope: src/components/LoginForm.tsx only\nOutput: Summary of changes made\nConstraints: Don't modify other files. Don't add new dependencies.\nFollow existing patterns in the codebase.\n```\n\n**Reviewer (checking code):**\n```\nReview changes to authentication flow.\n\nScope: Files modified in current branch vs main\nOutput: PASS or NEEDS_CHANGES with specific issues\nConstraints: Focus on bugs and security. Skip style issues.\n```\n\n## Parallel Spawning\n\nWhen tasks are independent, spawn multiple in one message:\n\n```\nI'll spawn three subagents in parallel:\n1. Explorer to find auth files\n2. Explorer to find test files\n3. Researcher to get latest JWT best practices\n\n[Three Task tool calls in same message]\n```\n\n## Recursive Spawning\n\nSubagents CAN spawn more subagents when:\n- Task is too large for one agent\n- Multiple independent subtasks discovered\n- Parallelization would help\n\n**Example: Explorer finds large codebase**\n```\nExplorer finds 50 auth-related files. Instead of returning all:\n1. Spawn 5 sub-explorers, each handling 10 files\n2. Each sub-explorer returns summarized findings\n3. Main explorer aggregates into final report\n```\n\n**Model inheritance:**\n- Subagent uses same or cheaper model\n- Never spawn opus from haiku\n- haiku → haiku (OK)\n- sonnet → haiku (OK, for simple subtasks)\n- sonnet → sonnet (OK, for complex subtasks)\n\n**Depth limit:** Max 3 levels deep to prevent runaway spawning\n- Orchestrator (opus) → Agent (sonnet/haiku) → Sub-agent → Sub-sub-agent\n\n## Anti-Patterns\n\n**DON'T:**\n- Spawn subagent for one-liner tasks\n- Use opus model for subagents (reserved for orchestrator)\n- Give vague prompts like \"look around\"\n- Spawn subagent to do what you could do in 2 tool calls\n- Spawn more than 5 parallel subagents (coordination overhead)\n\n**DO:**\n- Batch related work into one subagent\n- Specify exact output format\n- Use cheapest model that can do the job\n- Set clear scope boundaries\n- Let agents spawn sub-agents for large tasks\n\n## Subagent Context\n\nSubagents receive:\n- The prompt you provide\n- Agent rules from AGENT_RULES.md (via hook injection)\n- Access to same tools as you\n\nSubagents do NOT receive:\n- Your conversation history\n- Current phase/state (unless you tell them)\n- Other subagents' results (unless you include them)\n\n## Permission Awareness\n\nWhen spawning subagents within a workflow:\n\n1. **Include phase context**: Tell subagents what phase they're in and what tools are blocked\n2. **Pass workflow ID**: Include workflow ID so subagents can query permissions\n3. **Self-enforcement**: Subagents should check permissions before file operations\n\n**Example prompt with permission context:**\n```\nImplement input validation for login form.\n\n**Workflow Context:**\n- Workflow: iterate (implement phase)\n- Blocked tools: None in this phase\n- File restrictions: Only modify src/components/LoginForm.tsx\n\n**Permission check:** Use `is_tool_allowed(\"Edit\", file_path=path)` if unsure.\n```\n\n**Programmatic check** (lib/permission_query.py):\n```python\nfrom permission_query import get_permissions, is_tool_allowed\n\n# Subagent checks before editing\nallowed, reason = is_tool_allowed(\"Edit\", file_path=\"src/main.py\")\nif not allowed:\n    print(f\"Cannot edit: {reason}\")\n```\n\n**Orchestrator phase**: In ORCHESTRATE phase, Edit/Write/Bash are blocked - you MUST spawn subagents for all implementation work.\n",
        "skills/verify/SKILL.md": "---\nname: verify\ndescription: Run code quality checks (ruff, black, mypy, pytest)\nuser_invocable: true\n---\n\n# /verify - Code Quality Verification\n\nRuns all configured quality checks and reports pass/fail status. Required before commits when enforcement is enabled.\n\n## Usage\n\n```\n/verify              # Run all checks\n/verify --fix        # Run with auto-fix where possible\n/verify lint         # Run only linting (ruff)\n/verify format       # Run only formatting check (black)\n/verify types        # Run only type checking (mypy)\n/verify tests        # Run only tests (pytest)\n```\n\n## Tools\n\n| Tool | Purpose | Auto-fix |\n|------|---------|----------|\n| ruff | Linting + import sorting | Yes (`--fix`) |\n| black | Code formatting | Yes |\n| mypy | Type checking | No |\n| pytest | Test execution | No |\n\n## Implementation\n\nWhen invoked, execute the verify script:\n\n```bash\npython3 ~/.claude/plugins/agent-swarm/scripts/verify.py [args]\n```\n\nThe script will:\n1. Detect project type (Python, Node, etc.)\n2. Run appropriate checks\n3. Set `verify_passed` flag in session state if all pass\n4. Output compliance signal: `[VERIFY] tests: X | types: X | lint: X | format: X`\n\n## Enforcement Integration\n\nWhen verify enforcement is enabled:\n- Git commits are blocked unless verify has passed\n- The `verify_passed` flag resets on any file edit\n- Run `/verify` after making changes, before committing\n\n## Exit Codes\n\n- `0` - All checks passed\n- `1` - One or more checks failed\n- `2` - Configuration error (missing tools)\n\n## Configuration\n\nThe script auto-detects tools. To customize, create `.verify.json`:\n\n```json\n{\n  \"lint\": {\"enabled\": true, \"cmd\": \"ruff check .\"},\n  \"format\": {\"enabled\": true, \"cmd\": \"black --check .\"},\n  \"types\": {\"enabled\": true, \"cmd\": \"mypy .\"},\n  \"tests\": {\"enabled\": true, \"cmd\": \"pytest\"}\n}\n```\n"
      },
      "plugins": [
        {
          "name": "agent-swarm",
          "description": "Enforcement system for agent-based workflows. Manages phase enforcement, script routing for batch MCP operations, and autopilot approval.",
          "version": "1.0.0",
          "author": {
            "name": "fearsidhe"
          },
          "source": "./",
          "category": "productivity",
          "homepage": "https://github.com/c-daly/agent-swarm",
          "categories": [
            "productivity"
          ],
          "install_commands": [
            "/plugin marketplace add c-daly/agent-swarm",
            "/plugin install agent-swarm@fearsidhe-plugins"
          ]
        }
      ]
    }
  ]
}