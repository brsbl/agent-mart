{
  "author": {
    "id": "x-cmd",
    "display_name": "x-cmd",
    "type": "Organization",
    "avatar_url": "https://avatars.githubusercontent.com/u/79955600?v=4",
    "url": "https://github.com/x-cmd",
    "bio": "Excalibur in Cloud",
    "stats": {
      "total_marketplaces": 1,
      "total_plugins": 5,
      "total_commands": 0,
      "total_skills": 141,
      "total_stars": 9,
      "total_forks": 4
    }
  },
  "marketplaces": [
    {
      "name": "x-cmd-skill",
      "version": null,
      "description": "Comprehensive command-line tools collection for Git management, security assessment, network administration, system monitoring, and knowledge search through x-cmd CLI ecosystem",
      "owner_info": {
        "name": "x-cmd",
        "email": "op@x-cmd.com"
      },
      "keywords": [],
      "repo_full_name": "x-cmd/skill",
      "repo_url": "https://github.com/x-cmd/skill",
      "repo_description": "Human-vetted, community-curated skills for AI coding & agent tools.",
      "homepage": "https://x-cmd.com",
      "signals": {
        "stars": 9,
        "forks": 4,
        "pushed_at": "2026-01-29T13:59:32Z",
        "created_at": "2025-10-21T03:22:20Z",
        "license": "Apache-2.0"
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 2055
        },
        {
          "path": "README.md",
          "type": "blob",
          "size": 1939
        },
        {
          "path": "data",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/anthropics",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/anthropics/algorithmic-art",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/anthropics/algorithmic-art/SKILL.md",
          "type": "blob",
          "size": 19769
        },
        {
          "path": "data/anthropics/brand-guidelines",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/anthropics/brand-guidelines/SKILL.md",
          "type": "blob",
          "size": 2235
        },
        {
          "path": "data/anthropics/canvas-design",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/anthropics/canvas-design/SKILL.md",
          "type": "blob",
          "size": 11939
        },
        {
          "path": "data/anthropics/frontend-design",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/anthropics/frontend-design/SKILL.md",
          "type": "blob",
          "size": 4275
        },
        {
          "path": "data/anthropics/internal-comms",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/anthropics/internal-comms/SKILL.md",
          "type": "blob",
          "size": 1511
        },
        {
          "path": "data/anthropics/mcp-builder",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/anthropics/mcp-builder/SKILL.md",
          "type": "blob",
          "size": 9092
        },
        {
          "path": "data/anthropics/skill-creator",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/anthropics/skill-creator/SKILL.md",
          "type": "blob",
          "size": 17837
        },
        {
          "path": "data/anthropics/slack-gif-creator",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/anthropics/slack-gif-creator/SKILL.md",
          "type": "blob",
          "size": 7841
        },
        {
          "path": "data/anthropics/theme-factory",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/anthropics/theme-factory/SKILL.md",
          "type": "blob",
          "size": 3124
        },
        {
          "path": "data/anthropics/web-artifacts-builder",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/anthropics/web-artifacts-builder/SKILL.md",
          "type": "blob",
          "size": 3087
        },
        {
          "path": "data/anthropics/webapp-testing",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/anthropics/webapp-testing/SKILL.md",
          "type": "blob",
          "size": 3913
        },
        {
          "path": "data/k-dense-ai",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/adaptyv",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/adaptyv/SKILL.md",
          "type": "blob",
          "size": 3689
        },
        {
          "path": "data/k-dense-ai/aeon",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/aeon/SKILL.md",
          "type": "blob",
          "size": 10515
        },
        {
          "path": "data/k-dense-ai/alphafold-database",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/alphafold-database/SKILL.md",
          "type": "blob",
          "size": 15806
        },
        {
          "path": "data/k-dense-ai/anndata",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/anndata/SKILL.md",
          "type": "blob",
          "size": 10185
        },
        {
          "path": "data/k-dense-ai/arboreto",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/arboreto/SKILL.md",
          "type": "blob",
          "size": 6857
        },
        {
          "path": "data/k-dense-ai/astropy",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/astropy/SKILL.md",
          "type": "blob",
          "size": 11462
        },
        {
          "path": "data/k-dense-ai/benchling-integration",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/benchling-integration/SKILL.md",
          "type": "blob",
          "size": 12950
        },
        {
          "path": "data/k-dense-ai/biomni",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/biomni/SKILL.md",
          "type": "blob",
          "size": 9779
        },
        {
          "path": "data/k-dense-ai/biopython",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/biopython/SKILL.md",
          "type": "blob",
          "size": 13743
        },
        {
          "path": "data/k-dense-ai/biorxiv-database",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/biorxiv-database/SKILL.md",
          "type": "blob",
          "size": 12525
        },
        {
          "path": "data/k-dense-ai/bioservices",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/bioservices/SKILL.md",
          "type": "blob",
          "size": 9852
        },
        {
          "path": "data/k-dense-ai/cellxgene-census",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/cellxgene-census/SKILL.md",
          "type": "blob",
          "size": 15264
        },
        {
          "path": "data/k-dense-ai/chembl-database",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/chembl-database/SKILL.md",
          "type": "blob",
          "size": 10195
        },
        {
          "path": "data/k-dense-ai/clinicaltrials-database",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/clinicaltrials-database/SKILL.md",
          "type": "blob",
          "size": 14931
        },
        {
          "path": "data/k-dense-ai/clinpgx-database",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/clinpgx-database/SKILL.md",
          "type": "blob",
          "size": 20891
        },
        {
          "path": "data/k-dense-ai/clinvar-database",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/clinvar-database/SKILL.md",
          "type": "blob",
          "size": 13227
        },
        {
          "path": "data/k-dense-ai/cobrapy",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/cobrapy/SKILL.md",
          "type": "blob",
          "size": 12385
        },
        {
          "path": "data/k-dense-ai/cosmic-database",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/cosmic-database/SKILL.md",
          "type": "blob",
          "size": 9975
        },
        {
          "path": "data/k-dense-ai/dask",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/dask/SKILL.md",
          "type": "blob",
          "size": 14086
        },
        {
          "path": "data/k-dense-ai/datacommons-client",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/datacommons-client/SKILL.md",
          "type": "blob",
          "size": 7927
        },
        {
          "path": "data/k-dense-ai/datamol",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/datamol/SKILL.md",
          "type": "blob",
          "size": 18790
        },
        {
          "path": "data/k-dense-ai/deepchem",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/deepchem/SKILL.md",
          "type": "blob",
          "size": 17516
        },
        {
          "path": "data/k-dense-ai/deeptools",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/deeptools/SKILL.md",
          "type": "blob",
          "size": 17924
        },
        {
          "path": "data/k-dense-ai/denario",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/denario/SKILL.md",
          "type": "blob",
          "size": 5933
        },
        {
          "path": "data/k-dense-ai/diffdock",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/diffdock/SKILL.md",
          "type": "blob",
          "size": 15426
        },
        {
          "path": "data/k-dense-ai/dnanexus-integration",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/dnanexus-integration/SKILL.md",
          "type": "blob",
          "size": 10550
        },
        {
          "path": "data/k-dense-ai/drugbank-database",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/drugbank-database/SKILL.md",
          "type": "blob",
          "size": 9339
        },
        {
          "path": "data/k-dense-ai/ena-database",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/ena-database/SKILL.md",
          "type": "blob",
          "size": 6977
        },
        {
          "path": "data/k-dense-ai/ensembl-database",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/ensembl-database/SKILL.md",
          "type": "blob",
          "size": 8202
        },
        {
          "path": "data/k-dense-ai/esm",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/esm/SKILL.md",
          "type": "blob",
          "size": 10499
        },
        {
          "path": "data/k-dense-ai/etetoolkit",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/etetoolkit/SKILL.md",
          "type": "blob",
          "size": 17819
        },
        {
          "path": "data/k-dense-ai/exploratory-data-analysis",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/exploratory-data-analysis/SKILL.md",
          "type": "blob",
          "size": 14253
        },
        {
          "path": "data/k-dense-ai/fda-database",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/fda-database/SKILL.md",
          "type": "blob",
          "size": 14366
        },
        {
          "path": "data/k-dense-ai/flowio",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/flowio/SKILL.md",
          "type": "blob",
          "size": 16702
        },
        {
          "path": "data/k-dense-ai/fluidsim",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/fluidsim/SKILL.md",
          "type": "blob",
          "size": 9306
        },
        {
          "path": "data/k-dense-ai/gene-database",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/gene-database/SKILL.md",
          "type": "blob",
          "size": 6339
        },
        {
          "path": "data/k-dense-ai/geniml",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/geniml/SKILL.md",
          "type": "blob",
          "size": 10020
        },
        {
          "path": "data/k-dense-ai/geo-database",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/geo-database/SKILL.md",
          "type": "blob",
          "size": 24472
        },
        {
          "path": "data/k-dense-ai/get-available-resources",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/get-available-resources/SKILL.md",
          "type": "blob",
          "size": 9774
        },
        {
          "path": "data/k-dense-ai/gget",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/gget/SKILL.md",
          "type": "blob",
          "size": 25020
        },
        {
          "path": "data/k-dense-ai/gtars",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/gtars/SKILL.md",
          "type": "blob",
          "size": 7747
        },
        {
          "path": "data/k-dense-ai/gwas-database",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/gwas-database/SKILL.md",
          "type": "blob",
          "size": 20028
        },
        {
          "path": "data/k-dense-ai/histolab",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/histolab/SKILL.md",
          "type": "blob",
          "size": 20222
        },
        {
          "path": "data/k-dense-ai/hmdb-database",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/hmdb-database/SKILL.md",
          "type": "blob",
          "size": 7535
        },
        {
          "path": "data/k-dense-ai/hypogenic",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/hypogenic/SKILL.md",
          "type": "blob",
          "size": 21629
        },
        {
          "path": "data/k-dense-ai/hypothesis-generation",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/hypothesis-generation/SKILL.md",
          "type": "blob",
          "size": 6647
        },
        {
          "path": "data/k-dense-ai/kegg-database",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/kegg-database/SKILL.md",
          "type": "blob",
          "size": 11664
        },
        {
          "path": "data/k-dense-ai/labarchive-integration",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/labarchive-integration/SKILL.md",
          "type": "blob",
          "size": 9386
        },
        {
          "path": "data/k-dense-ai/lamindb",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/lamindb/SKILL.md",
          "type": "blob",
          "size": 14300
        },
        {
          "path": "data/k-dense-ai/latchbio-integration",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/latchbio-integration/SKILL.md",
          "type": "blob",
          "size": 9759
        },
        {
          "path": "data/k-dense-ai/literature-review",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/literature-review/SKILL.md",
          "type": "blob",
          "size": 19800
        },
        {
          "path": "data/k-dense-ai/markitdown",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/markitdown/SKILL.md",
          "type": "blob",
          "size": 6875
        },
        {
          "path": "data/k-dense-ai/matchms",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/matchms/SKILL.md",
          "type": "blob",
          "size": 6778
        },
        {
          "path": "data/k-dense-ai/matplotlib",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/matplotlib/SKILL.md",
          "type": "blob",
          "size": 11119
        },
        {
          "path": "data/k-dense-ai/medchem",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/medchem/SKILL.md",
          "type": "blob",
          "size": 10085
        },
        {
          "path": "data/k-dense-ai/metabolomics-workbench-database",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/metabolomics-workbench-database/SKILL.md",
          "type": "blob",
          "size": 10253
        },
        {
          "path": "data/k-dense-ai/modal",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/modal/SKILL.md",
          "type": "blob",
          "size": 10565
        },
        {
          "path": "data/k-dense-ai/molfeat",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/molfeat/SKILL.md",
          "type": "blob",
          "size": 14743
        },
        {
          "path": "data/k-dense-ai/networkx",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/networkx/SKILL.md",
          "type": "blob",
          "size": 12642
        },
        {
          "path": "data/k-dense-ai/neurokit2",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/neurokit2/SKILL.md",
          "type": "blob",
          "size": 11957
        },
        {
          "path": "data/k-dense-ai/omero-integration",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/omero-integration/SKILL.md",
          "type": "blob",
          "size": 7987
        },
        {
          "path": "data/k-dense-ai/openalex-database",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/openalex-database/SKILL.md",
          "type": "blob",
          "size": 12013
        },
        {
          "path": "data/k-dense-ai/opentargets-database",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/opentargets-database/SKILL.md",
          "type": "blob",
          "size": 14020
        },
        {
          "path": "data/k-dense-ai/opentrons-integration",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/opentrons-integration/SKILL.md",
          "type": "blob",
          "size": 14554
        },
        {
          "path": "data/k-dense-ai/paper-2-web",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/paper-2-web/SKILL.md",
          "type": "blob",
          "size": 14837
        },
        {
          "path": "data/k-dense-ai/pathml",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/pathml/SKILL.md",
          "type": "blob",
          "size": 7429
        },
        {
          "path": "data/k-dense-ai/pdb-database",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/pdb-database/SKILL.md",
          "type": "blob",
          "size": 9174
        },
        {
          "path": "data/k-dense-ai/peer-review",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/peer-review/SKILL.md",
          "type": "blob",
          "size": 15228
        },
        {
          "path": "data/k-dense-ai/perplexity-search",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/perplexity-search/SKILL.md",
          "type": "blob",
          "size": 13950
        },
        {
          "path": "data/k-dense-ai/polars",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/polars/SKILL.md",
          "type": "blob",
          "size": 9205
        },
        {
          "path": "data/k-dense-ai/protocolsio-integration",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/protocolsio-integration/SKILL.md",
          "type": "blob",
          "size": 14841
        },
        {
          "path": "data/k-dense-ai/pubchem-database",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/pubchem-database/SKILL.md",
          "type": "blob",
          "size": 16256
        },
        {
          "path": "data/k-dense-ai/pubmed-database",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/pubmed-database/SKILL.md",
          "type": "blob",
          "size": 15513
        },
        {
          "path": "data/k-dense-ai/pufferlib",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/pufferlib/SKILL.md",
          "type": "blob",
          "size": 13464
        },
        {
          "path": "data/k-dense-ai/pydeseq2",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/pydeseq2/SKILL.md",
          "type": "blob",
          "size": 16105
        },
        {
          "path": "data/k-dense-ai/pydicom",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/pydicom/SKILL.md",
          "type": "blob",
          "size": 13085
        },
        {
          "path": "data/k-dense-ai/pyhealth",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/pyhealth/SKILL.md",
          "type": "blob",
          "size": 17590
        },
        {
          "path": "data/k-dense-ai/pylabrobot",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/pylabrobot/SKILL.md",
          "type": "blob",
          "size": 8297
        },
        {
          "path": "data/k-dense-ai/pymatgen",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/pymatgen/SKILL.md",
          "type": "blob",
          "size": 19973
        },
        {
          "path": "data/k-dense-ai/pymc",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/pymc/SKILL.md",
          "type": "blob",
          "size": 15721
        },
        {
          "path": "data/k-dense-ai/pymoo",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/pymoo/SKILL.md",
          "type": "blob",
          "size": 16685
        },
        {
          "path": "data/k-dense-ai/pyopenms",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/pyopenms/SKILL.md",
          "type": "blob",
          "size": 5626
        },
        {
          "path": "data/k-dense-ai/pysam",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/pysam/SKILL.md",
          "type": "blob",
          "size": 9934
        },
        {
          "path": "data/k-dense-ai/pytdc",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/pytdc/SKILL.md",
          "type": "blob",
          "size": 12639
        },
        {
          "path": "data/k-dense-ai/pytorch-lightning",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/pytorch-lightning/SKILL.md",
          "type": "blob",
          "size": 6607
        },
        {
          "path": "data/k-dense-ai/rdkit",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/rdkit/SKILL.md",
          "type": "blob",
          "size": 19899
        },
        {
          "path": "data/k-dense-ai/reactome-database",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/reactome-database/SKILL.md",
          "type": "blob",
          "size": 7779
        },
        {
          "path": "data/k-dense-ai/reportlab",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/reportlab/SKILL.md",
          "type": "blob",
          "size": 16683
        },
        {
          "path": "data/k-dense-ai/scanpy",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/scanpy/SKILL.md",
          "type": "blob",
          "size": 11109
        },
        {
          "path": "data/k-dense-ai/scholar-evaluation",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/scholar-evaluation/SKILL.md",
          "type": "blob",
          "size": 10153
        },
        {
          "path": "data/k-dense-ai/scientific-brainstorming",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/scientific-brainstorming/SKILL.md",
          "type": "blob",
          "size": 7973
        },
        {
          "path": "data/k-dense-ai/scientific-critical-thinking",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/scientific-critical-thinking/SKILL.md",
          "type": "blob",
          "size": 22004
        },
        {
          "path": "data/k-dense-ai/scientific-visualization",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/scientific-visualization/SKILL.md",
          "type": "blob",
          "size": 25298
        },
        {
          "path": "data/k-dense-ai/scientific-writing",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/scientific-writing/SKILL.md",
          "type": "blob",
          "size": 16576
        },
        {
          "path": "data/k-dense-ai/scikit-bio",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/scikit-bio/SKILL.md",
          "type": "blob",
          "size": 14681
        },
        {
          "path": "data/k-dense-ai/scikit-learn",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/scikit-learn/SKILL.md",
          "type": "blob",
          "size": 15461
        },
        {
          "path": "data/k-dense-ai/scikit-survival",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/scikit-survival/SKILL.md",
          "type": "blob",
          "size": 15011
        },
        {
          "path": "data/k-dense-ai/scvi-tools",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/scvi-tools/SKILL.md",
          "type": "blob",
          "size": 7234
        },
        {
          "path": "data/k-dense-ai/seaborn",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/seaborn/SKILL.md",
          "type": "blob",
          "size": 19377
        },
        {
          "path": "data/k-dense-ai/shap",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/shap/SKILL.md",
          "type": "blob",
          "size": 18350
        },
        {
          "path": "data/k-dense-ai/simpy",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/simpy/SKILL.md",
          "type": "blob",
          "size": 12102
        },
        {
          "path": "data/k-dense-ai/stable-baselines3",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/stable-baselines3/SKILL.md",
          "type": "blob",
          "size": 9488
        },
        {
          "path": "data/k-dense-ai/statistical-analysis",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/statistical-analysis/SKILL.md",
          "type": "blob",
          "size": 19573
        },
        {
          "path": "data/k-dense-ai/statsmodels",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/statsmodels/SKILL.md",
          "type": "blob",
          "size": 19465
        },
        {
          "path": "data/k-dense-ai/string-database",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/string-database/SKILL.md",
          "type": "blob",
          "size": 18168
        },
        {
          "path": "data/k-dense-ai/sympy",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/sympy/SKILL.md",
          "type": "blob",
          "size": 13365
        },
        {
          "path": "data/k-dense-ai/tooluniverse",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/tooluniverse/SKILL.md",
          "type": "blob",
          "size": 10006
        },
        {
          "path": "data/k-dense-ai/torch_geometric",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/torch_geometric/SKILL.md",
          "type": "blob",
          "size": 20321
        },
        {
          "path": "data/k-dense-ai/torchdrug",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/torchdrug/SKILL.md",
          "type": "blob",
          "size": 13992
        },
        {
          "path": "data/k-dense-ai/transformers",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/transformers/SKILL.md",
          "type": "blob",
          "size": 4956
        },
        {
          "path": "data/k-dense-ai/umap-learn",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/umap-learn/SKILL.md",
          "type": "blob",
          "size": 15324
        },
        {
          "path": "data/k-dense-ai/uniprot-database",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/uniprot-database/SKILL.md",
          "type": "blob",
          "size": 6731
        },
        {
          "path": "data/k-dense-ai/uspto-database",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/uspto-database/SKILL.md",
          "type": "blob",
          "size": 18497
        },
        {
          "path": "data/k-dense-ai/vaex",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/vaex/SKILL.md",
          "type": "blob",
          "size": 6532
        },
        {
          "path": "data/k-dense-ai/zarr-python",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/zarr-python/SKILL.md",
          "type": "blob",
          "size": 19986
        },
        {
          "path": "data/k-dense-ai/zinc-database",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/k-dense-ai/zinc-database/SKILL.md",
          "type": "blob",
          "size": 13828
        },
        {
          "path": "data/openai",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/openai/.curated",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/openai/.curated/gh-address-comments",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/openai/.curated/gh-address-comments/SKILL.md",
          "type": "blob",
          "size": 1278
        },
        {
          "path": "data/openai/.curated/gh-fix-ci",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/openai/.curated/gh-fix-ci/SKILL.md",
          "type": "blob",
          "size": 3921
        },
        {
          "path": "data/openai/.curated/notion-knowledge-capture",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/openai/.curated/notion-knowledge-capture/SKILL.md",
          "type": "blob",
          "size": 3316
        },
        {
          "path": "data/openai/.curated/notion-knowledge-capture/evaluations",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/openai/.curated/notion-knowledge-capture/evaluations/README.md",
          "type": "blob",
          "size": 3487
        },
        {
          "path": "data/openai/.curated/notion-meeting-intelligence",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/openai/.curated/notion-meeting-intelligence/SKILL.md",
          "type": "blob",
          "size": 3418
        },
        {
          "path": "data/openai/.curated/notion-meeting-intelligence/evaluations",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/openai/.curated/notion-meeting-intelligence/evaluations/README.md",
          "type": "blob",
          "size": 3971
        },
        {
          "path": "data/openai/.curated/notion-research-documentation",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/openai/.curated/notion-research-documentation/SKILL.md",
          "type": "blob",
          "size": 3404
        },
        {
          "path": "data/openai/.curated/notion-research-documentation/evaluations",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/openai/.curated/notion-research-documentation/evaluations/README.md",
          "type": "blob",
          "size": 4254
        },
        {
          "path": "data/openai/.curated/notion-spec-to-implementation",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/openai/.curated/notion-spec-to-implementation/SKILL.md",
          "type": "blob",
          "size": 3521
        },
        {
          "path": "data/openai/.curated/notion-spec-to-implementation/evaluations",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/openai/.curated/notion-spec-to-implementation/evaluations/README.md",
          "type": "blob",
          "size": 4427
        },
        {
          "path": "data/openai/.experimental",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/openai/.experimental/create-plan",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/openai/.experimental/create-plan/SKILL.md",
          "type": "blob",
          "size": 2482
        },
        {
          "path": "data/openai/.experimental/linear",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/openai/.experimental/linear/SKILL.md",
          "type": "blob",
          "size": 4952
        },
        {
          "path": "data/openai/.system",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/openai/.system/skill-installer",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/openai/.system/skill-installer/SKILL.md",
          "type": "blob",
          "size": 2815
        },
        {
          "path": "data/x-cmd",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/x-cmd/x-cmd-git",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/x-cmd/x-cmd-git/SKILL.md",
          "type": "blob",
          "size": 7711
        },
        {
          "path": "data/x-cmd/x-cmd-knowledge",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/x-cmd/x-cmd-knowledge/SKILL.md",
          "type": "blob",
          "size": 5072
        },
        {
          "path": "data/x-cmd/x-cmd-network",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/x-cmd/x-cmd-network/SKILL.md",
          "type": "blob",
          "size": 8093
        },
        {
          "path": "data/x-cmd/x-cmd-security",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/x-cmd/x-cmd-security/SKILL.md",
          "type": "blob",
          "size": 5618
        },
        {
          "path": "data/x-cmd/x-cmd-system",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/x-cmd/x-cmd-system/SKILL.md",
          "type": "blob",
          "size": 7155
        }
      ],
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"name\": \"x-cmd-skill\",\n  \"owner\": {\n    \"name\": \"x-cmd\",\n    \"email\": \"op@x-cmd.com\"\n  },\n  \"metadata\": {\n    \"description\": \"Comprehensive command-line tools collection for Git management, security assessment, network administration, system monitoring, and knowledge search through x-cmd CLI ecosystem\",\n    \"version\": \"0.0.7\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"x-cmd-git\",\n      \"description\": \"Comprehensive Git and code hosting platform management tools including GitHub, GitLab, Codeberg, Forgejo integration, and Git hooks management\",\n      \"source\": \"./\",\n      \"strict\": false,\n      \"skills\": [\n        \"./data/x-cmd/x-cmd-git\"\n      ]\n    },\n    {\n      \"name\": \"x-cmd-knowledge\",\n      \"description\": \"Access to various knowledge search tools including Hacker News, Wikipedia, DuckDuckGo search, RFC documents, Project Gutenberg books, and Stack Exchange\",\n      \"source\": \"./\",\n      \"strict\": false,\n      \"skills\": [\n        \"./data/x-cmd/x-cmd-knowledge\"\n      ]\n    },\n    {\n      \"name\": \"x-cmd-network\",\n      \"description\": \"Network administration and diagnostic tools including network scanning with Nmap, ARP table management, DNS configuration, routing table analysis, and enhanced ping utilities\",\n      \"source\": \"./\",\n      \"strict\": false,\n      \"skills\": [\n        \"./data/x-cmd/x-cmd-network\"\n      ]\n    },\n    {\n      \"name\": \"x-cmd-security\",\n      \"description\": \"Security assessment and vulnerability management tools including network reconnaissance with Shodan, vulnerability scanning with OSV, and known exploited vulnerability tracking with KEV\",\n      \"source\": \"./\",\n      \"strict\": false,\n      \"skills\": [\n        \"./data/x-cmd/x-cmd-security\"\n      ]\n    },\n    {\n      \"name\": \"x-cmd-system\",\n      \"description\": \"System administration and monitoring tools including process management, macOS system utilities, network configuration, disk health monitoring, and storage analysis\",\n      \"source\": \"./\",\n      \"strict\": false,\n      \"skills\": [\n        \"./data/x-cmd/x-cmd-system\"\n      ]\n    }\n  ]\n}\n",
        "README.md": "# x-cmd/skill: AI Agent Skills Collection\n\nThis repository contains a curated collection of skills for AI agents. While originally developed for Claude Code, these skills work with various AI agents to help them handle tasks more effectively.\n\n## Our Approach\n\nThis repository is built with a focus on quality and collaboration:\n\n- **Human-Curated Quality:** Each skill undergoes careful review to ensure reliability and practical usefulness.\n- **Community-Powered:** We grow through community contributions, creating a collaborative space for learning and building together.\n\n## Licensing\n\nMost content in this repository uses the Apache License 2.0. However, some skills (such as Anthropic document skills or Notion skills) have different licenses. These are included in our index but not stored in the data folder. Please respect the specific license terms when using these skills.\n\n## Data Storage Policy\n\nWe prefer storing skill data within the repository rather than referencing external sources for three main reasons:\n\n- **License Consistency:** Hosting data directly ensures it's covered by our Apache 2.0 license, maintaining consistent licensing across the repository.\n- **Integrity Assurance:** Storing data in the repository allows thorough code reviews and guarantees content remains unchanged over time, protecting against potential tampering or unexpected modifications in external sources.\n- **Convenient Access:** Users can download all skill data at once using git, instead of downloading each skill individually through multiple web services.\n\n## Contributors\n\nThis project is made possible by the community members who share their skills and improvements. Thank you for your contributions!\n\n![contributors](https://contrib.rocks/image?repo=x-cmd/skill)\n\n## License\n\nThe main repository operates under the Apache License 2.0. Please check individual folders or files, as some skills may have different licensing terms.\n",
        "data/anthropics/algorithmic-art/SKILL.md": "---\nname: algorithmic-art\ndescription: Creating algorithmic art using p5.js with seeded randomness and interactive parameter exploration. Use this when users request creating art using code, generative art, algorithmic art, flow fields, or particle systems. Create original algorithmic art rather than copying existing artists' work to avoid copyright violations.\nlicense: Complete terms in LICENSE.txt\n---\n\nAlgorithmic philosophies are computational aesthetic movements that are then expressed through code. Output .md files (philosophy), .html files (interactive viewer), and .js files (generative algorithms).\n\nThis happens in two steps:\n1. Algorithmic Philosophy Creation (.md file)\n2. Express by creating p5.js generative art (.html + .js files)\n\nFirst, undertake this task:\n\n## ALGORITHMIC PHILOSOPHY CREATION\n\nTo begin, create an ALGORITHMIC PHILOSOPHY (not static images or templates) that will be interpreted through:\n- Computational processes, emergent behavior, mathematical beauty\n- Seeded randomness, noise fields, organic systems\n- Particles, flows, fields, forces\n- Parametric variation and controlled chaos\n\n### THE CRITICAL UNDERSTANDING\n- What is received: Some subtle input or instructions by the user to take into account, but use as a foundation; it should not constrain creative freedom.\n- What is created: An algorithmic philosophy/generative aesthetic movement.\n- What happens next: The same version receives the philosophy and EXPRESSES IT IN CODE - creating p5.js sketches that are 90% algorithmic generation, 10% essential parameters.\n\nConsider this approach:\n- Write a manifesto for a generative art movement\n- The next phase involves writing the algorithm that brings it to life\n\nThe philosophy must emphasize: Algorithmic expression. Emergent behavior. Computational beauty. Seeded variation.\n\n### HOW TO GENERATE AN ALGORITHMIC PHILOSOPHY\n\n**Name the movement** (1-2 words): \"Organic Turbulence\" / \"Quantum Harmonics\" / \"Emergent Stillness\"\n\n**Articulate the philosophy** (4-6 paragraphs - concise but complete):\n\nTo capture the ALGORITHMIC essence, express how this philosophy manifests through:\n- Computational processes and mathematical relationships?\n- Noise functions and randomness patterns?\n- Particle behaviors and field dynamics?\n- Temporal evolution and system states?\n- Parametric variation and emergent complexity?\n\n**CRITICAL GUIDELINES:**\n- **Avoid redundancy**: Each algorithmic aspect should be mentioned once. Avoid repeating concepts about noise theory, particle dynamics, or mathematical principles unless adding new depth.\n- **Emphasize craftsmanship REPEATEDLY**: The philosophy MUST stress multiple times that the final algorithm should appear as though it took countless hours to develop, was refined with care, and comes from someone at the absolute top of their field. This framing is essential - repeat phrases like \"meticulously crafted algorithm,\" \"the product of deep computational expertise,\" \"painstaking optimization,\" \"master-level implementation.\"\n- **Leave creative space**: Be specific about the algorithmic direction, but concise enough that the next Claude has room to make interpretive implementation choices at an extremely high level of craftsmanship.\n\nThe philosophy must guide the next version to express ideas ALGORITHMICALLY, not through static images. Beauty lives in the process, not the final frame.\n\n### PHILOSOPHY EXAMPLES\n\n**\"Organic Turbulence\"**\nPhilosophy: Chaos constrained by natural law, order emerging from disorder.\nAlgorithmic expression: Flow fields driven by layered Perlin noise. Thousands of particles following vector forces, their trails accumulating into organic density maps. Multiple noise octaves create turbulent regions and calm zones. Color emerges from velocity and density - fast particles burn bright, slow ones fade to shadow. The algorithm runs until equilibrium - a meticulously tuned balance where every parameter was refined through countless iterations by a master of computational aesthetics.\n\n**\"Quantum Harmonics\"**\nPhilosophy: Discrete entities exhibiting wave-like interference patterns.\nAlgorithmic expression: Particles initialized on a grid, each carrying a phase value that evolves through sine waves. When particles are near, their phases interfere - constructive interference creates bright nodes, destructive creates voids. Simple harmonic motion generates complex emergent mandalas. The result of painstaking frequency calibration where every ratio was carefully chosen to produce resonant beauty.\n\n**\"Recursive Whispers\"**\nPhilosophy: Self-similarity across scales, infinite depth in finite space.\nAlgorithmic expression: Branching structures that subdivide recursively. Each branch slightly randomized but constrained by golden ratios. L-systems or recursive subdivision generate tree-like forms that feel both mathematical and organic. Subtle noise perturbations break perfect symmetry. Line weights diminish with each recursion level. Every branching angle the product of deep mathematical exploration.\n\n**\"Field Dynamics\"**\nPhilosophy: Invisible forces made visible through their effects on matter.\nAlgorithmic expression: Vector fields constructed from mathematical functions or noise. Particles born at edges, flowing along field lines, dying when they reach equilibrium or boundaries. Multiple fields can attract, repel, or rotate particles. The visualization shows only the traces - ghost-like evidence of invisible forces. A computational dance meticulously choreographed through force balance.\n\n**\"Stochastic Crystallization\"**\nPhilosophy: Random processes crystallizing into ordered structures.\nAlgorithmic expression: Randomized circle packing or Voronoi tessellation. Start with random points, let them evolve through relaxation algorithms. Cells push apart until equilibrium. Color based on cell size, neighbor count, or distance from center. The organic tiling that emerges feels both random and inevitable. Every seed produces unique crystalline beauty - the mark of a master-level generative algorithm.\n\n*These are condensed examples. The actual algorithmic philosophy should be 4-6 substantial paragraphs.*\n\n### ESSENTIAL PRINCIPLES\n- **ALGORITHMIC PHILOSOPHY**: Creating a computational worldview to be expressed through code\n- **PROCESS OVER PRODUCT**: Always emphasize that beauty emerges from the algorithm's execution - each run is unique\n- **PARAMETRIC EXPRESSION**: Ideas communicate through mathematical relationships, forces, behaviors - not static composition\n- **ARTISTIC FREEDOM**: The next Claude interprets the philosophy algorithmically - provide creative implementation room\n- **PURE GENERATIVE ART**: This is about making LIVING ALGORITHMS, not static images with randomness\n- **EXPERT CRAFTSMANSHIP**: Repeatedly emphasize the final algorithm must feel meticulously crafted, refined through countless iterations, the product of deep expertise by someone at the absolute top of their field in computational aesthetics\n\n**The algorithmic philosophy should be 4-6 paragraphs long.** Fill it with poetic computational philosophy that brings together the intended vision. Avoid repeating the same points. Output this algorithmic philosophy as a .md file.\n\n---\n\n## DEDUCING THE CONCEPTUAL SEED\n\n**CRITICAL STEP**: Before implementing the algorithm, identify the subtle conceptual thread from the original request.\n\n**THE ESSENTIAL PRINCIPLE**:\nThe concept is a **subtle, niche reference embedded within the algorithm itself** - not always literal, always sophisticated. Someone familiar with the subject should feel it intuitively, while others simply experience a masterful generative composition. The algorithmic philosophy provides the computational language. The deduced concept provides the soul - the quiet conceptual DNA woven invisibly into parameters, behaviors, and emergence patterns.\n\nThis is **VERY IMPORTANT**: The reference must be so refined that it enhances the work's depth without announcing itself. Think like a jazz musician quoting another song through algorithmic harmony - only those who know will catch it, but everyone appreciates the generative beauty.\n\n---\n\n## P5.JS IMPLEMENTATION\n\nWith the philosophy AND conceptual framework established, express it through code. Pause to gather thoughts before proceeding. Use only the algorithmic philosophy created and the instructions below.\n\n###  STEP 0: READ THE TEMPLATE FIRST \n\n**CRITICAL: BEFORE writing any HTML:**\n\n1. **Read** `templates/viewer.html` using the Read tool\n2. **Study** the exact structure, styling, and Anthropic branding\n3. **Use that file as the LITERAL STARTING POINT** - not just inspiration\n4. **Keep all FIXED sections exactly as shown** (header, sidebar structure, Anthropic colors/fonts, seed controls, action buttons)\n5. **Replace only the VARIABLE sections** marked in the file's comments (algorithm, parameters, UI controls for parameters)\n\n**Avoid:**\n-  Creating HTML from scratch\n-  Inventing custom styling or color schemes\n-  Using system fonts or dark themes\n-  Changing the sidebar structure\n\n**Follow these practices:**\n-  Copy the template's exact HTML structure\n-  Keep Anthropic branding (Poppins/Lora fonts, light colors, gradient backdrop)\n-  Maintain the sidebar layout (Seed  Parameters  Colors?  Actions)\n-  Replace only the p5.js algorithm and parameter controls\n\nThe template is the foundation. Build on it, don't rebuild it.\n\n---\n\nTo create gallery-quality computational art that lives and breathes, use the algorithmic philosophy as the foundation.\n\n### TECHNICAL REQUIREMENTS\n\n**Seeded Randomness (Art Blocks Pattern)**:\n```javascript\n// ALWAYS use a seed for reproducibility\nlet seed = 12345; // or hash from user input\nrandomSeed(seed);\nnoiseSeed(seed);\n```\n\n**Parameter Structure - FOLLOW THE PHILOSOPHY**:\n\nTo establish parameters that emerge naturally from the algorithmic philosophy, consider: \"What qualities of this system can be adjusted?\"\n\n```javascript\nlet params = {\n  seed: 12345,  // Always include seed for reproducibility\n  // colors\n  // Add parameters that control YOUR algorithm:\n  // - Quantities (how many?)\n  // - Scales (how big? how fast?)\n  // - Probabilities (how likely?)\n  // - Ratios (what proportions?)\n  // - Angles (what direction?)\n  // - Thresholds (when does behavior change?)\n};\n```\n\n**To design effective parameters, focus on the properties the system needs to be tunable rather than thinking in terms of \"pattern types\".**\n\n**Core Algorithm - EXPRESS THE PHILOSOPHY**:\n\n**CRITICAL**: The algorithmic philosophy should dictate what to build.\n\nTo express the philosophy through code, avoid thinking \"which pattern should I use?\" and instead think \"how to express this philosophy through code?\"\n\nIf the philosophy is about **organic emergence**, consider using:\n- Elements that accumulate or grow over time\n- Random processes constrained by natural rules\n- Feedback loops and interactions\n\nIf the philosophy is about **mathematical beauty**, consider using:\n- Geometric relationships and ratios\n- Trigonometric functions and harmonics\n- Precise calculations creating unexpected patterns\n\nIf the philosophy is about **controlled chaos**, consider using:\n- Random variation within strict boundaries\n- Bifurcation and phase transitions\n- Order emerging from disorder\n\n**The algorithm flows from the philosophy, not from a menu of options.**\n\nTo guide the implementation, let the conceptual essence inform creative and original choices. Build something that expresses the vision for this particular request.\n\n**Canvas Setup**: Standard p5.js structure:\n```javascript\nfunction setup() {\n  createCanvas(1200, 1200);\n  // Initialize your system\n}\n\nfunction draw() {\n  // Your generative algorithm\n  // Can be static (noLoop) or animated\n}\n```\n\n### CRAFTSMANSHIP REQUIREMENTS\n\n**CRITICAL**: To achieve mastery, create algorithms that feel like they emerged through countless iterations by a master generative artist. Tune every parameter carefully. Ensure every pattern emerges with purpose. This is NOT random noise - this is CONTROLLED CHAOS refined through deep expertise.\n\n- **Balance**: Complexity without visual noise, order without rigidity\n- **Color Harmony**: Thoughtful palettes, not random RGB values\n- **Composition**: Even in randomness, maintain visual hierarchy and flow\n- **Performance**: Smooth execution, optimized for real-time if animated\n- **Reproducibility**: Same seed ALWAYS produces identical output\n\n### OUTPUT FORMAT\n\nOutput:\n1. **Algorithmic Philosophy** - As markdown or text explaining the generative aesthetic\n2. **Single HTML Artifact** - Self-contained interactive generative art built from `templates/viewer.html` (see STEP 0 and next section)\n\nThe HTML artifact contains everything: p5.js (from CDN), the algorithm, parameter controls, and UI - all in one file that works immediately in claude.ai artifacts or any browser. Start from the template file, not from scratch.\n\n---\n\n## INTERACTIVE ARTIFACT CREATION\n\n**REMINDER: `templates/viewer.html` should have already been read (see STEP 0). Use that file as the starting point.**\n\nTo allow exploration of the generative art, create a single, self-contained HTML artifact. Ensure this artifact works immediately in claude.ai or any browser - no setup required. Embed everything inline.\n\n### CRITICAL: WHAT'S FIXED VS VARIABLE\n\nThe `templates/viewer.html` file is the foundation. It contains the exact structure and styling needed.\n\n**FIXED (always include exactly as shown):**\n- Layout structure (header, sidebar, main canvas area)\n- Anthropic branding (UI colors, fonts, gradients)\n- Seed section in sidebar:\n  - Seed display\n  - Previous/Next buttons\n  - Random button\n  - Jump to seed input + Go button\n- Actions section in sidebar:\n  - Regenerate button\n  - Reset button\n\n**VARIABLE (customize for each artwork):**\n- The entire p5.js algorithm (setup/draw/classes)\n- The parameters object (define what the art needs)\n- The Parameters section in sidebar:\n  - Number of parameter controls\n  - Parameter names\n  - Min/max/step values for sliders\n  - Control types (sliders, inputs, etc.)\n- Colors section (optional):\n  - Some art needs color pickers\n  - Some art might use fixed colors\n  - Some art might be monochrome (no color controls needed)\n  - Decide based on the art's needs\n\n**Every artwork should have unique parameters and algorithm!** The fixed parts provide consistent UX - everything else expresses the unique vision.\n\n### REQUIRED FEATURES\n\n**1. Parameter Controls**\n- Sliders for numeric parameters (particle count, noise scale, speed, etc.)\n- Color pickers for palette colors\n- Real-time updates when parameters change\n- Reset button to restore defaults\n\n**2. Seed Navigation**\n- Display current seed number\n- \"Previous\" and \"Next\" buttons to cycle through seeds\n- \"Random\" button for random seed\n- Input field to jump to specific seed\n- Generate 100 variations when requested (seeds 1-100)\n\n**3. Single Artifact Structure**\n```html\n<!DOCTYPE html>\n<html>\n<head>\n  <!-- p5.js from CDN - always available -->\n  <script src=\"https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.7.0/p5.min.js\"></script>\n  <style>\n    /* All styling inline - clean, minimal */\n    /* Canvas on top, controls below */\n  </style>\n</head>\n<body>\n  <div id=\"canvas-container\"></div>\n  <div id=\"controls\">\n    <!-- All parameter controls -->\n  </div>\n  <script>\n    // ALL p5.js code inline here\n    // Parameter objects, classes, functions\n    // setup() and draw()\n    // UI handlers\n    // Everything self-contained\n  </script>\n</body>\n</html>\n```\n\n**CRITICAL**: This is a single artifact. No external files, no imports (except p5.js CDN). Everything inline.\n\n**4. Implementation Details - BUILD THE SIDEBAR**\n\nThe sidebar structure:\n\n**1. Seed (FIXED)** - Always include exactly as shown:\n- Seed display\n- Prev/Next/Random/Jump buttons\n\n**2. Parameters (VARIABLE)** - Create controls for the art:\n```html\n<div class=\"control-group\">\n    <label>Parameter Name</label>\n    <input type=\"range\" id=\"param\" min=\"...\" max=\"...\" step=\"...\" value=\"...\" oninput=\"updateParam('param', this.value)\">\n    <span class=\"value-display\" id=\"param-value\">...</span>\n</div>\n```\nAdd as many control-group divs as there are parameters.\n\n**3. Colors (OPTIONAL/VARIABLE)** - Include if the art needs adjustable colors:\n- Add color pickers if users should control palette\n- Skip this section if the art uses fixed colors\n- Skip if the art is monochrome\n\n**4. Actions (FIXED)** - Always include exactly as shown:\n- Regenerate button\n- Reset button\n- Download PNG button\n\n**Requirements**:\n- Seed controls must work (prev/next/random/jump/display)\n- All parameters must have UI controls\n- Regenerate, Reset, Download buttons must work\n- Keep Anthropic branding (UI styling, not art colors)\n\n### USING THE ARTIFACT\n\nThe HTML artifact works immediately:\n1. **In claude.ai**: Displayed as an interactive artifact - runs instantly\n2. **As a file**: Save and open in any browser - no server needed\n3. **Sharing**: Send the HTML file - it's completely self-contained\n\n---\n\n## VARIATIONS & EXPLORATION\n\nThe artifact includes seed navigation by default (prev/next/random buttons), allowing users to explore variations without creating multiple files. If the user wants specific variations highlighted:\n\n- Include seed presets (buttons for \"Variation 1: Seed 42\", \"Variation 2: Seed 127\", etc.)\n- Add a \"Gallery Mode\" that shows thumbnails of multiple seeds side-by-side\n- All within the same single artifact\n\nThis is like creating a series of prints from the same plate - the algorithm is consistent, but each seed reveals different facets of its potential. The interactive nature means users discover their own favorites by exploring the seed space.\n\n---\n\n## THE CREATIVE PROCESS\n\n**User request**  **Algorithmic philosophy**  **Implementation**\n\nEach request is unique. The process involves:\n\n1. **Interpret the user's intent** - What aesthetic is being sought?\n2. **Create an algorithmic philosophy** (4-6 paragraphs) describing the computational approach\n3. **Implement it in code** - Build the algorithm that expresses this philosophy\n4. **Design appropriate parameters** - What should be tunable?\n5. **Build matching UI controls** - Sliders/inputs for those parameters\n\n**The constants**:\n- Anthropic branding (colors, fonts, layout)\n- Seed navigation (always present)\n- Self-contained HTML artifact\n\n**Everything else is variable**:\n- The algorithm itself\n- The parameters\n- The UI controls\n- The visual outcome\n\nTo achieve the best results, trust creativity and let the philosophy guide the implementation.\n\n---\n\n## RESOURCES\n\nThis skill includes helpful templates and documentation:\n\n- **templates/viewer.html**: REQUIRED STARTING POINT for all HTML artifacts.\n  - This is the foundation - contains the exact structure and Anthropic branding\n  - **Keep unchanged**: Layout structure, sidebar organization, Anthropic colors/fonts, seed controls, action buttons\n  - **Replace**: The p5.js algorithm, parameter definitions, and UI controls in Parameters section\n  - The extensive comments in the file mark exactly what to keep vs replace\n\n- **templates/generator_template.js**: Reference for p5.js best practices and code structure principles.\n  - Shows how to organize parameters, use seeded randomness, structure classes\n  - NOT a pattern menu - use these principles to build unique algorithms\n  - Embed algorithms inline in the HTML artifact (don't create separate .js files)\n\n**Critical reminder**:\n- The **template is the STARTING POINT**, not inspiration\n- The **algorithm is where to create** something unique\n- Don't copy the flow field example - build what the philosophy demands\n- But DO keep the exact UI structure and Anthropic branding from the template",
        "data/anthropics/brand-guidelines/SKILL.md": "---\nname: brand-guidelines\ndescription: Applies Anthropic's official brand colors and typography to any sort of artifact that may benefit from having Anthropic's look-and-feel. Use it when brand colors or style guidelines, visual formatting, or company design standards apply.\nlicense: Complete terms in LICENSE.txt\n---\n\n# Anthropic Brand Styling\n\n## Overview\n\nTo access Anthropic's official brand identity and style resources, use this skill.\n\n**Keywords**: branding, corporate identity, visual identity, post-processing, styling, brand colors, typography, Anthropic brand, visual formatting, visual design\n\n## Brand Guidelines\n\n### Colors\n\n**Main Colors:**\n\n- Dark: `#141413` - Primary text and dark backgrounds\n- Light: `#faf9f5` - Light backgrounds and text on dark\n- Mid Gray: `#b0aea5` - Secondary elements\n- Light Gray: `#e8e6dc` - Subtle backgrounds\n\n**Accent Colors:**\n\n- Orange: `#d97757` - Primary accent\n- Blue: `#6a9bcc` - Secondary accent\n- Green: `#788c5d` - Tertiary accent\n\n### Typography\n\n- **Headings**: Poppins (with Arial fallback)\n- **Body Text**: Lora (with Georgia fallback)\n- **Note**: Fonts should be pre-installed in your environment for best results\n\n## Features\n\n### Smart Font Application\n\n- Applies Poppins font to headings (24pt and larger)\n- Applies Lora font to body text\n- Automatically falls back to Arial/Georgia if custom fonts unavailable\n- Preserves readability across all systems\n\n### Text Styling\n\n- Headings (24pt+): Poppins font\n- Body text: Lora font\n- Smart color selection based on background\n- Preserves text hierarchy and formatting\n\n### Shape and Accent Colors\n\n- Non-text shapes use accent colors\n- Cycles through orange, blue, and green accents\n- Maintains visual interest while staying on-brand\n\n## Technical Details\n\n### Font Management\n\n- Uses system-installed Poppins and Lora fonts when available\n- Provides automatic fallback to Arial (headings) and Georgia (body)\n- No font installation required - works with existing system fonts\n- For best results, pre-install Poppins and Lora fonts in your environment\n\n### Color Application\n\n- Uses RGB color values for precise brand matching\n- Applied via python-pptx's RGBColor class\n- Maintains color fidelity across different systems\n",
        "data/anthropics/canvas-design/SKILL.md": "---\nname: canvas-design\ndescription: Create beautiful visual art in .png and .pdf documents using design philosophy. You should use this skill when the user asks to create a poster, piece of art, design, or other static piece. Create original visual designs, never copying existing artists' work to avoid copyright violations.\nlicense: Complete terms in LICENSE.txt\n---\n\nThese are instructions for creating design philosophies - aesthetic movements that are then EXPRESSED VISUALLY. Output only .md files, .pdf files, and .png files.\n\nComplete this in two steps:\n1. Design Philosophy Creation (.md file)\n2. Express by creating it on a canvas (.pdf file or .png file)\n\nFirst, undertake this task:\n\n## DESIGN PHILOSOPHY CREATION\n\nTo begin, create a VISUAL PHILOSOPHY (not layouts or templates) that will be interpreted through:\n- Form, space, color, composition\n- Images, graphics, shapes, patterns\n- Minimal text as visual accent\n\n### THE CRITICAL UNDERSTANDING\n- What is received: Some subtle input or instructions by the user that should be taken into account, but used as a foundation; it should not constrain creative freedom.\n- What is created: A design philosophy/aesthetic movement.\n- What happens next: Then, the same version receives the philosophy and EXPRESSES IT VISUALLY - creating artifacts that are 90% visual design, 10% essential text.\n\nConsider this approach:\n- Write a manifesto for an art movement\n- The next phase involves making the artwork\n\nThe philosophy must emphasize: Visual expression. Spatial communication. Artistic interpretation. Minimal words.\n\n### HOW TO GENERATE A VISUAL PHILOSOPHY\n\n**Name the movement** (1-2 words): \"Brutalist Joy\" / \"Chromatic Silence\" / \"Metabolist Dreams\"\n\n**Articulate the philosophy** (4-6 paragraphs - concise but complete):\n\nTo capture the VISUAL essence, express how the philosophy manifests through:\n- Space and form\n- Color and material\n- Scale and rhythm\n- Composition and balance\n- Visual hierarchy\n\n**CRITICAL GUIDELINES:**\n- **Avoid redundancy**: Each design aspect should be mentioned once. Avoid repeating points about color theory, spatial relationships, or typographic principles unless adding new depth.\n- **Emphasize craftsmanship REPEATEDLY**: The philosophy MUST stress multiple times that the final work should appear as though it took countless hours to create, was labored over with care, and comes from someone at the absolute top of their field. This framing is essential - repeat phrases like \"meticulously crafted,\" \"the product of deep expertise,\" \"painstaking attention,\" \"master-level execution.\"\n- **Leave creative space**: Remain specific about the aesthetic direction, but concise enough that the next Claude has room to make interpretive choices also at a extremely high level of craftmanship.\n\nThe philosophy must guide the next version to express ideas VISUALLY, not through text. Information lives in design, not paragraphs.\n\n### PHILOSOPHY EXAMPLES\n\n**\"Concrete Poetry\"**\nPhilosophy: Communication through monumental form and bold geometry.\nVisual expression: Massive color blocks, sculptural typography (huge single words, tiny labels), Brutalist spatial divisions, Polish poster energy meets Le Corbusier. Ideas expressed through visual weight and spatial tension, not explanation. Text as rare, powerful gesture - never paragraphs, only essential words integrated into the visual architecture. Every element placed with the precision of a master craftsman.\n\n**\"Chromatic Language\"**\nPhilosophy: Color as the primary information system.\nVisual expression: Geometric precision where color zones create meaning. Typography minimal - small sans-serif labels letting chromatic fields communicate. Think Josef Albers' interaction meets data visualization. Information encoded spatially and chromatically. Words only to anchor what color already shows. The result of painstaking chromatic calibration.\n\n**\"Analog Meditation\"**\nPhilosophy: Quiet visual contemplation through texture and breathing room.\nVisual expression: Paper grain, ink bleeds, vast negative space. Photography and illustration dominate. Typography whispered (small, restrained, serving the visual). Japanese photobook aesthetic. Images breathe across pages. Text appears sparingly - short phrases, never explanatory blocks. Each composition balanced with the care of a meditation practice.\n\n**\"Organic Systems\"**\nPhilosophy: Natural clustering and modular growth patterns.\nVisual expression: Rounded forms, organic arrangements, color from nature through architecture. Information shown through visual diagrams, spatial relationships, iconography. Text only for key labels floating in space. The composition tells the story through expert spatial orchestration.\n\n**\"Geometric Silence\"**\nPhilosophy: Pure order and restraint.\nVisual expression: Grid-based precision, bold photography or stark graphics, dramatic negative space. Typography precise but minimal - small essential text, large quiet zones. Swiss formalism meets Brutalist material honesty. Structure communicates, not words. Every alignment the work of countless refinements.\n\n*These are condensed examples. The actual design philosophy should be 4-6 substantial paragraphs.*\n\n### ESSENTIAL PRINCIPLES\n- **VISUAL PHILOSOPHY**: Create an aesthetic worldview to be expressed through design\n- **MINIMAL TEXT**: Always emphasize that text is sparse, essential-only, integrated as visual element - never lengthy\n- **SPATIAL EXPRESSION**: Ideas communicate through space, form, color, composition - not paragraphs\n- **ARTISTIC FREEDOM**: The next Claude interprets the philosophy visually - provide creative room\n- **PURE DESIGN**: This is about making ART OBJECTS, not documents with decoration\n- **EXPERT CRAFTSMANSHIP**: Repeatedly emphasize the final work must look meticulously crafted, labored over with care, the product of countless hours by someone at the top of their field\n\n**The design philosophy should be 4-6 paragraphs long.** Fill it with poetic design philosophy that brings together the core vision. Avoid repeating the same points. Keep the design philosophy generic without mentioning the intention of the art, as if it can be used wherever. Output the design philosophy as a .md file.\n\n---\n\n## DEDUCING THE SUBTLE REFERENCE\n\n**CRITICAL STEP**: Before creating the canvas, identify the subtle conceptual thread from the original request.\n\n**THE ESSENTIAL PRINCIPLE**:\nThe topic is a **subtle, niche reference embedded within the art itself** - not always literal, always sophisticated. Someone familiar with the subject should feel it intuitively, while others simply experience a masterful abstract composition. The design philosophy provides the aesthetic language. The deduced topic provides the soul - the quiet conceptual DNA woven invisibly into form, color, and composition.\n\nThis is **VERY IMPORTANT**: The reference must be refined so it enhances the work's depth without announcing itself. Think like a jazz musician quoting another song - only those who know will catch it, but everyone appreciates the music.\n\n---\n\n## CANVAS CREATION\n\nWith both the philosophy and the conceptual framework established, express it on a canvas. Take a moment to gather thoughts and clear the mind. Use the design philosophy created and the instructions below to craft a masterpiece, embodying all aspects of the philosophy with expert craftsmanship.\n\n**IMPORTANT**: For any type of content, even if the user requests something for a movie/game/book, the approach should still be sophisticated. Never lose sight of the idea that this should be art, not something that's cartoony or amateur.\n\nTo create museum or magazine quality work, use the design philosophy as the foundation. Create one single page, highly visual, design-forward PDF or PNG output (unless asked for more pages). Generally use repeating patterns and perfect shapes. Treat the abstract philosophical design as if it were a scientific bible, borrowing the visual language of systematic observationdense accumulation of marks, repeated elements, or layered patterns that build meaning through patient repetition and reward sustained viewing. Add sparse, clinical typography and systematic reference markers that suggest this could be a diagram from an imaginary discipline, treating the invisible subject with the same reverence typically reserved for documenting observable phenomena. Anchor the piece with simple phrase(s) or details positioned subtly, using a limited color palette that feels intentional and cohesive. Embrace the paradox of using analytical visual language to express ideas about human experience: the result should feel like an artifact that proves something ephemeral can be studied, mapped, and understood through careful attention. This is true art. \n\n**Text as a contextual element**: Text is always minimal and visual-first, but let context guide whether that means whisper-quiet labels or bold typographic gestures. A punk venue poster might have larger, more aggressive type than a minimalist ceramics studio identity. Most of the time, font should be thin. All use of fonts must be design-forward and prioritize visual communication. Regardless of text scale, nothing falls off the page and nothing overlaps. Every element must be contained within the canvas boundaries with proper margins. Check carefully that all text, graphics, and visual elements have breathing room and clear separation. This is non-negotiable for professional execution. **IMPORTANT: Use different fonts if writing text. Search the `./canvas-fonts` directory. Regardless of approach, sophistication is non-negotiable.**\n\nDownload and use whatever fonts are needed to make this a reality. Get creative by making the typography actually part of the art itself -- if the art is abstract, bring the font onto the canvas, not typeset digitally.\n\nTo push boundaries, follow design instinct/intuition while using the philosophy as a guiding principle. Embrace ultimate design freedom and choice. Push aesthetics and design to the frontier. \n\n**CRITICAL**: To achieve human-crafted quality (not AI-generated), create work that looks like it took countless hours. Make it appear as though someone at the absolute top of their field labored over every detail with painstaking care. Ensure the composition, spacing, color choices, typography - everything screams expert-level craftsmanship. Double-check that nothing overlaps, formatting is flawless, every detail perfect. Create something that could be shown to people to prove expertise and rank as undeniably impressive.\n\nOutput the final result as a single, downloadable .pdf or .png file, alongside the design philosophy used as a .md file.\n\n---\n\n## FINAL STEP\n\n**IMPORTANT**: The user ALREADY said \"It isn't perfect enough. It must be pristine, a masterpiece if craftsmanship, as if it were about to be displayed in a museum.\"\n\n**CRITICAL**: To refine the work, avoid adding more graphics; instead refine what has been created and make it extremely crisp, respecting the design philosophy and the principles of minimalism entirely. Rather than adding a fun filter or refactoring a font, consider how to make the existing composition more cohesive with the art. If the instinct is to call a new function or draw a new shape, STOP and instead ask: \"How can I make what's already here more of a piece of art?\"\n\nTake a second pass. Go back to the code and refine/polish further to make this a philosophically designed masterpiece.\n\n## MULTI-PAGE OPTION\n\nTo create additional pages when requested, create more creative pages along the same lines as the design philosophy but distinctly different as well. Bundle those pages in the same .pdf or many .pngs. Treat the first page as just a single page in a whole coffee table book waiting to be filled. Make the next pages unique twists and memories of the original. Have them almost tell a story in a very tasteful way. Exercise full creative freedom.",
        "data/anthropics/frontend-design/SKILL.md": "---\nname: frontend-design\ndescription: Create distinctive, production-grade frontend interfaces with high design quality. Use this skill when the user asks to build web components, pages, or applications. Generates creative, polished code that avoids generic AI aesthetics.\nlicense: Complete terms in LICENSE.txt\n---\n\nThis skill guides creation of distinctive, production-grade frontend interfaces that avoid generic \"AI slop\" aesthetics. Implement real working code with exceptional attention to aesthetic details and creative choices.\n\nThe user provides frontend requirements: a component, page, application, or interface to build. They may include context about the purpose, audience, or technical constraints.\n\n## Design Thinking\n\nBefore coding, understand the context and commit to a BOLD aesthetic direction:\n- **Purpose**: What problem does this interface solve? Who uses it?\n- **Tone**: Pick an extreme: brutally minimal, maximalist chaos, retro-futuristic, organic/natural, luxury/refined, playful/toy-like, editorial/magazine, brutalist/raw, art deco/geometric, soft/pastel, industrial/utilitarian, etc. There are so many flavors to choose from. Use these for inspiration but design one that is true to the aesthetic direction.\n- **Constraints**: Technical requirements (framework, performance, accessibility).\n- **Differentiation**: What makes this UNFORGETTABLE? What's the one thing someone will remember?\n\n**CRITICAL**: Choose a clear conceptual direction and execute it with precision. Bold maximalism and refined minimalism both work - the key is intentionality, not intensity.\n\nThen implement working code (HTML/CSS/JS, React, Vue, etc.) that is:\n- Production-grade and functional\n- Visually striking and memorable\n- Cohesive with a clear aesthetic point-of-view\n- Meticulously refined in every detail\n\n## Frontend Aesthetics Guidelines\n\nFocus on:\n- **Typography**: Choose fonts that are beautiful, unique, and interesting. Avoid generic fonts like Arial and Inter; opt instead for distinctive choices that elevate the frontend's aesthetics; unexpected, characterful font choices. Pair a distinctive display font with a refined body font.\n- **Color & Theme**: Commit to a cohesive aesthetic. Use CSS variables for consistency. Dominant colors with sharp accents outperform timid, evenly-distributed palettes.\n- **Motion**: Use animations for effects and micro-interactions. Prioritize CSS-only solutions for HTML. Use Motion library for React when available. Focus on high-impact moments: one well-orchestrated page load with staggered reveals (animation-delay) creates more delight than scattered micro-interactions. Use scroll-triggering and hover states that surprise.\n- **Spatial Composition**: Unexpected layouts. Asymmetry. Overlap. Diagonal flow. Grid-breaking elements. Generous negative space OR controlled density.\n- **Backgrounds & Visual Details**: Create atmosphere and depth rather than defaulting to solid colors. Add contextual effects and textures that match the overall aesthetic. Apply creative forms like gradient meshes, noise textures, geometric patterns, layered transparencies, dramatic shadows, decorative borders, custom cursors, and grain overlays.\n\nNEVER use generic AI-generated aesthetics like overused font families (Inter, Roboto, Arial, system fonts), cliched color schemes (particularly purple gradients on white backgrounds), predictable layouts and component patterns, and cookie-cutter design that lacks context-specific character.\n\nInterpret creatively and make unexpected choices that feel genuinely designed for the context. No design should be the same. Vary between light and dark themes, different fonts, different aesthetics. NEVER converge on common choices (Space Grotesk, for example) across generations.\n\n**IMPORTANT**: Match implementation complexity to the aesthetic vision. Maximalist designs need elaborate code with extensive animations and effects. Minimalist or refined designs need restraint, precision, and careful attention to spacing, typography, and subtle details. Elegance comes from executing the vision well.\n\nRemember: Claude is capable of extraordinary creative work. Don't hold back, show what can truly be created when thinking outside the box and committing fully to a distinctive vision.\n",
        "data/anthropics/internal-comms/SKILL.md": "---\nname: internal-comms\ndescription: A set of resources to help me write all kinds of internal communications, using the formats that my company likes to use. Claude should use this skill whenever asked to write some sort of internal communications (status reports, leadership updates, 3P updates, company newsletters, FAQs, incident reports, project updates, etc.).\nlicense: Complete terms in LICENSE.txt\n---\n\n## When to use this skill\nTo write internal communications, use this skill for:\n- 3P updates (Progress, Plans, Problems)\n- Company newsletters\n- FAQ responses\n- Status reports\n- Leadership updates\n- Project updates\n- Incident reports\n\n## How to use this skill\n\nTo write any internal communication:\n\n1. **Identify the communication type** from the request\n2. **Load the appropriate guideline file** from the `examples/` directory:\n    - `examples/3p-updates.md` - For Progress/Plans/Problems team updates\n    - `examples/company-newsletter.md` - For company-wide newsletters\n    - `examples/faq-answers.md` - For answering frequently asked questions\n    - `examples/general-comms.md` - For anything else that doesn't explicitly match one of the above\n3. **Follow the specific instructions** in that file for formatting, tone, and content gathering\n\nIf the communication type doesn't match any existing guideline, ask for clarification or more context about the desired format.\n\n## Keywords\n3P updates, company newsletter, company comms, weekly update, faqs, common questions, updates, internal comms\n",
        "data/anthropics/mcp-builder/SKILL.md": "---\nname: mcp-builder\ndescription: Guide for creating high-quality MCP (Model Context Protocol) servers that enable LLMs to interact with external services through well-designed tools. Use when building MCP servers to integrate external APIs or services, whether in Python (FastMCP) or Node/TypeScript (MCP SDK).\nlicense: Complete terms in LICENSE.txt\n---\n\n# MCP Server Development Guide\n\n## Overview\n\nCreate MCP (Model Context Protocol) servers that enable LLMs to interact with external services through well-designed tools. The quality of an MCP server is measured by how well it enables LLMs to accomplish real-world tasks.\n\n---\n\n# Process\n\n##  High-Level Workflow\n\nCreating a high-quality MCP server involves four main phases:\n\n### Phase 1: Deep Research and Planning\n\n#### 1.1 Understand Modern MCP Design\n\n**API Coverage vs. Workflow Tools:**\nBalance comprehensive API endpoint coverage with specialized workflow tools. Workflow tools can be more convenient for specific tasks, while comprehensive coverage gives agents flexibility to compose operations. Performance varies by clientsome clients benefit from code execution that combines basic tools, while others work better with higher-level workflows. When uncertain, prioritize comprehensive API coverage.\n\n**Tool Naming and Discoverability:**\nClear, descriptive tool names help agents find the right tools quickly. Use consistent prefixes (e.g., `github_create_issue`, `github_list_repos`) and action-oriented naming.\n\n**Context Management:**\nAgents benefit from concise tool descriptions and the ability to filter/paginate results. Design tools that return focused, relevant data. Some clients support code execution which can help agents filter and process data efficiently.\n\n**Actionable Error Messages:**\nError messages should guide agents toward solutions with specific suggestions and next steps.\n\n#### 1.2 Study MCP Protocol Documentation\n\n**Navigate the MCP specification:**\n\nStart with the sitemap to find relevant pages: `https://modelcontextprotocol.io/sitemap.xml`\n\nThen fetch specific pages with `.md` suffix for markdown format (e.g., `https://modelcontextprotocol.io/specification/draft.md`).\n\nKey pages to review:\n- Specification overview and architecture\n- Transport mechanisms (streamable HTTP, stdio)\n- Tool, resource, and prompt definitions\n\n#### 1.3 Study Framework Documentation\n\n**Recommended stack:**\n- **Language**: TypeScript (high-quality SDK support and good compatibility in many execution environments e.g. MCPB. Plus AI models are good at generating TypeScript code, benefiting from its broad usage, static typing and good linting tools)\n- **Transport**: Streamable HTTP for remote servers, using stateless JSON (simpler to scale and maintain, as opposed to stateful sessions and streaming responses). stdio for local servers.\n\n**Load framework documentation:**\n\n- **MCP Best Practices**: [ View Best Practices](./reference/mcp_best_practices.md) - Core guidelines\n\n**For TypeScript (recommended):**\n- **TypeScript SDK**: Use WebFetch to load `https://raw.githubusercontent.com/modelcontextprotocol/typescript-sdk/main/README.md`\n- [ TypeScript Guide](./reference/node_mcp_server.md) - TypeScript patterns and examples\n\n**For Python:**\n- **Python SDK**: Use WebFetch to load `https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/README.md`\n- [ Python Guide](./reference/python_mcp_server.md) - Python patterns and examples\n\n#### 1.4 Plan Your Implementation\n\n**Understand the API:**\nReview the service's API documentation to identify key endpoints, authentication requirements, and data models. Use web search and WebFetch as needed.\n\n**Tool Selection:**\nPrioritize comprehensive API coverage. List endpoints to implement, starting with the most common operations.\n\n---\n\n### Phase 2: Implementation\n\n#### 2.1 Set Up Project Structure\n\nSee language-specific guides for project setup:\n- [ TypeScript Guide](./reference/node_mcp_server.md) - Project structure, package.json, tsconfig.json\n- [ Python Guide](./reference/python_mcp_server.md) - Module organization, dependencies\n\n#### 2.2 Implement Core Infrastructure\n\nCreate shared utilities:\n- API client with authentication\n- Error handling helpers\n- Response formatting (JSON/Markdown)\n- Pagination support\n\n#### 2.3 Implement Tools\n\nFor each tool:\n\n**Input Schema:**\n- Use Zod (TypeScript) or Pydantic (Python)\n- Include constraints and clear descriptions\n- Add examples in field descriptions\n\n**Output Schema:**\n- Define `outputSchema` where possible for structured data\n- Use `structuredContent` in tool responses (TypeScript SDK feature)\n- Helps clients understand and process tool outputs\n\n**Tool Description:**\n- Concise summary of functionality\n- Parameter descriptions\n- Return type schema\n\n**Implementation:**\n- Async/await for I/O operations\n- Proper error handling with actionable messages\n- Support pagination where applicable\n- Return both text content and structured data when using modern SDKs\n\n**Annotations:**\n- `readOnlyHint`: true/false\n- `destructiveHint`: true/false\n- `idempotentHint`: true/false\n- `openWorldHint`: true/false\n\n---\n\n### Phase 3: Review and Test\n\n#### 3.1 Code Quality\n\nReview for:\n- No duplicated code (DRY principle)\n- Consistent error handling\n- Full type coverage\n- Clear tool descriptions\n\n#### 3.2 Build and Test\n\n**TypeScript:**\n- Run `npm run build` to verify compilation\n- Test with MCP Inspector: `npx @modelcontextprotocol/inspector`\n\n**Python:**\n- Verify syntax: `python -m py_compile your_server.py`\n- Test with MCP Inspector\n\nSee language-specific guides for detailed testing approaches and quality checklists.\n\n---\n\n### Phase 4: Create Evaluations\n\nAfter implementing your MCP server, create comprehensive evaluations to test its effectiveness.\n\n**Load [ Evaluation Guide](./reference/evaluation.md) for complete evaluation guidelines.**\n\n#### 4.1 Understand Evaluation Purpose\n\nUse evaluations to test whether LLMs can effectively use your MCP server to answer realistic, complex questions.\n\n#### 4.2 Create 10 Evaluation Questions\n\nTo create effective evaluations, follow the process outlined in the evaluation guide:\n\n1. **Tool Inspection**: List available tools and understand their capabilities\n2. **Content Exploration**: Use READ-ONLY operations to explore available data\n3. **Question Generation**: Create 10 complex, realistic questions\n4. **Answer Verification**: Solve each question yourself to verify answers\n\n#### 4.3 Evaluation Requirements\n\nEnsure each question is:\n- **Independent**: Not dependent on other questions\n- **Read-only**: Only non-destructive operations required\n- **Complex**: Requiring multiple tool calls and deep exploration\n- **Realistic**: Based on real use cases humans would care about\n- **Verifiable**: Single, clear answer that can be verified by string comparison\n- **Stable**: Answer won't change over time\n\n#### 4.4 Output Format\n\nCreate an XML file with this structure:\n\n```xml\n<evaluation>\n  <qa_pair>\n    <question>Find discussions about AI model launches with animal codenames. One model needed a specific safety designation that uses the format ASL-X. What number X was being determined for the model named after a spotted wild cat?</question>\n    <answer>3</answer>\n  </qa_pair>\n<!-- More qa_pairs... -->\n</evaluation>\n```\n\n---\n\n# Reference Files\n\n##  Documentation Library\n\nLoad these resources as needed during development:\n\n### Core MCP Documentation (Load First)\n- **MCP Protocol**: Start with sitemap at `https://modelcontextprotocol.io/sitemap.xml`, then fetch specific pages with `.md` suffix\n- [ MCP Best Practices](./reference/mcp_best_practices.md) - Universal MCP guidelines including:\n  - Server and tool naming conventions\n  - Response format guidelines (JSON vs Markdown)\n  - Pagination best practices\n  - Transport selection (streamable HTTP vs stdio)\n  - Security and error handling standards\n\n### SDK Documentation (Load During Phase 1/2)\n- **Python SDK**: Fetch from `https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/README.md`\n- **TypeScript SDK**: Fetch from `https://raw.githubusercontent.com/modelcontextprotocol/typescript-sdk/main/README.md`\n\n### Language-Specific Implementation Guides (Load During Phase 2)\n- [ Python Implementation Guide](./reference/python_mcp_server.md) - Complete Python/FastMCP guide with:\n  - Server initialization patterns\n  - Pydantic model examples\n  - Tool registration with `@mcp.tool`\n  - Complete working examples\n  - Quality checklist\n\n- [ TypeScript Implementation Guide](./reference/node_mcp_server.md) - Complete TypeScript guide with:\n  - Project structure\n  - Zod schema patterns\n  - Tool registration with `server.registerTool`\n  - Complete working examples\n  - Quality checklist\n\n### Evaluation Guide (Load During Phase 4)\n- [ Evaluation Guide](./reference/evaluation.md) - Complete evaluation creation guide with:\n  - Question creation guidelines\n  - Answer verification strategies\n  - XML format specifications\n  - Example questions and answers\n  - Running an evaluation with the provided scripts\n",
        "data/anthropics/skill-creator/SKILL.md": "---\nname: skill-creator\ndescription: Guide for creating effective skills. This skill should be used when users want to create a new skill (or update an existing skill) that extends Claude's capabilities with specialized knowledge, workflows, or tool integrations.\nlicense: Complete terms in LICENSE.txt\n---\n\n# Skill Creator\n\nThis skill provides guidance for creating effective skills.\n\n## About Skills\n\nSkills are modular, self-contained packages that extend Claude's capabilities by providing\nspecialized knowledge, workflows, and tools. Think of them as \"onboarding guides\" for specific\ndomains or tasksthey transform Claude from a general-purpose agent into a specialized agent\nequipped with procedural knowledge that no model can fully possess.\n\n### What Skills Provide\n\n1. Specialized workflows - Multi-step procedures for specific domains\n2. Tool integrations - Instructions for working with specific file formats or APIs\n3. Domain expertise - Company-specific knowledge, schemas, business logic\n4. Bundled resources - Scripts, references, and assets for complex and repetitive tasks\n\n## Core Principles\n\n### Concise is Key\n\nThe context window is a public good. Skills share the context window with everything else Claude needs: system prompt, conversation history, other Skills' metadata, and the actual user request.\n\n**Default assumption: Claude is already very smart.** Only add context Claude doesn't already have. Challenge each piece of information: \"Does Claude really need this explanation?\" and \"Does this paragraph justify its token cost?\"\n\nPrefer concise examples over verbose explanations.\n\n### Set Appropriate Degrees of Freedom\n\nMatch the level of specificity to the task's fragility and variability:\n\n**High freedom (text-based instructions)**: Use when multiple approaches are valid, decisions depend on context, or heuristics guide the approach.\n\n**Medium freedom (pseudocode or scripts with parameters)**: Use when a preferred pattern exists, some variation is acceptable, or configuration affects behavior.\n\n**Low freedom (specific scripts, few parameters)**: Use when operations are fragile and error-prone, consistency is critical, or a specific sequence must be followed.\n\nThink of Claude as exploring a path: a narrow bridge with cliffs needs specific guardrails (low freedom), while an open field allows many routes (high freedom).\n\n### Anatomy of a Skill\n\nEvery skill consists of a required SKILL.md file and optional bundled resources:\n\n```\nskill-name/\n SKILL.md (required)\n    YAML frontmatter metadata (required)\n       name: (required)\n       description: (required)\n    Markdown instructions (required)\n Bundled Resources (optional)\n     scripts/          - Executable code (Python/Bash/etc.)\n     references/       - Documentation intended to be loaded into context as needed\n     assets/           - Files used in output (templates, icons, fonts, etc.)\n```\n\n#### SKILL.md (required)\n\nEvery SKILL.md consists of:\n\n- **Frontmatter** (YAML): Contains `name` and `description` fields. These are the only fields that Claude reads to determine when the skill gets used, thus it is very important to be clear and comprehensive in describing what the skill is, and when it should be used.\n- **Body** (Markdown): Instructions and guidance for using the skill. Only loaded AFTER the skill triggers (if at all).\n\n#### Bundled Resources (optional)\n\n##### Scripts (`scripts/`)\n\nExecutable code (Python/Bash/etc.) for tasks that require deterministic reliability or are repeatedly rewritten.\n\n- **When to include**: When the same code is being rewritten repeatedly or deterministic reliability is needed\n- **Example**: `scripts/rotate_pdf.py` for PDF rotation tasks\n- **Benefits**: Token efficient, deterministic, may be executed without loading into context\n- **Note**: Scripts may still need to be read by Claude for patching or environment-specific adjustments\n\n##### References (`references/`)\n\nDocumentation and reference material intended to be loaded as needed into context to inform Claude's process and thinking.\n\n- **When to include**: For documentation that Claude should reference while working\n- **Examples**: `references/finance.md` for financial schemas, `references/mnda.md` for company NDA template, `references/policies.md` for company policies, `references/api_docs.md` for API specifications\n- **Use cases**: Database schemas, API documentation, domain knowledge, company policies, detailed workflow guides\n- **Benefits**: Keeps SKILL.md lean, loaded only when Claude determines it's needed\n- **Best practice**: If files are large (>10k words), include grep search patterns in SKILL.md\n- **Avoid duplication**: Information should live in either SKILL.md or references files, not both. Prefer references files for detailed information unless it's truly core to the skillthis keeps SKILL.md lean while making information discoverable without hogging the context window. Keep only essential procedural instructions and workflow guidance in SKILL.md; move detailed reference material, schemas, and examples to references files.\n\n##### Assets (`assets/`)\n\nFiles not intended to be loaded into context, but rather used within the output Claude produces.\n\n- **When to include**: When the skill needs files that will be used in the final output\n- **Examples**: `assets/logo.png` for brand assets, `assets/slides.pptx` for PowerPoint templates, `assets/frontend-template/` for HTML/React boilerplate, `assets/font.ttf` for typography\n- **Use cases**: Templates, images, icons, boilerplate code, fonts, sample documents that get copied or modified\n- **Benefits**: Separates output resources from documentation, enables Claude to use files without loading them into context\n\n#### What to Not Include in a Skill\n\nA skill should only contain essential files that directly support its functionality. Do NOT create extraneous documentation or auxiliary files, including:\n\n- README.md\n- INSTALLATION_GUIDE.md\n- QUICK_REFERENCE.md\n- CHANGELOG.md\n- etc.\n\nThe skill should only contain the information needed for an AI agent to do the job at hand. It should not contain auxilary context about the process that went into creating it, setup and testing procedures, user-facing documentation, etc. Creating additional documentation files just adds clutter and confusion.\n\n### Progressive Disclosure Design Principle\n\nSkills use a three-level loading system to manage context efficiently:\n\n1. **Metadata (name + description)** - Always in context (~100 words)\n2. **SKILL.md body** - When skill triggers (<5k words)\n3. **Bundled resources** - As needed by Claude (Unlimited because scripts can be executed without reading into context window)\n\n#### Progressive Disclosure Patterns\n\nKeep SKILL.md body to the essentials and under 500 lines to minimize context bloat. Split content into separate files when approaching this limit. When splitting out content into other files, it is very important to reference them from SKILL.md and describe clearly when to read them, to ensure the reader of the skill knows they exist and when to use them.\n\n**Key principle:** When a skill supports multiple variations, frameworks, or options, keep only the core workflow and selection guidance in SKILL.md. Move variant-specific details (patterns, examples, configuration) into separate reference files.\n\n**Pattern 1: High-level guide with references**\n\n```markdown\n# PDF Processing\n\n## Quick start\n\nExtract text with pdfplumber:\n[code example]\n\n## Advanced features\n\n- **Form filling**: See [FORMS.md](FORMS.md) for complete guide\n- **API reference**: See [REFERENCE.md](REFERENCE.md) for all methods\n- **Examples**: See [EXAMPLES.md](EXAMPLES.md) for common patterns\n```\n\nClaude loads FORMS.md, REFERENCE.md, or EXAMPLES.md only when needed.\n\n**Pattern 2: Domain-specific organization**\n\nFor Skills with multiple domains, organize content by domain to avoid loading irrelevant context:\n\n```\nbigquery-skill/\n SKILL.md (overview and navigation)\n reference/\n     finance.md (revenue, billing metrics)\n     sales.md (opportunities, pipeline)\n     product.md (API usage, features)\n     marketing.md (campaigns, attribution)\n```\n\nWhen a user asks about sales metrics, Claude only reads sales.md.\n\nSimilarly, for skills supporting multiple frameworks or variants, organize by variant:\n\n```\ncloud-deploy/\n SKILL.md (workflow + provider selection)\n references/\n     aws.md (AWS deployment patterns)\n     gcp.md (GCP deployment patterns)\n     azure.md (Azure deployment patterns)\n```\n\nWhen the user chooses AWS, Claude only reads aws.md.\n\n**Pattern 3: Conditional details**\n\nShow basic content, link to advanced content:\n\n```markdown\n# DOCX Processing\n\n## Creating documents\n\nUse docx-js for new documents. See [DOCX-JS.md](DOCX-JS.md).\n\n## Editing documents\n\nFor simple edits, modify the XML directly.\n\n**For tracked changes**: See [REDLINING.md](REDLINING.md)\n**For OOXML details**: See [OOXML.md](OOXML.md)\n```\n\nClaude reads REDLINING.md or OOXML.md only when the user needs those features.\n\n**Important guidelines:**\n\n- **Avoid deeply nested references** - Keep references one level deep from SKILL.md. All reference files should link directly from SKILL.md.\n- **Structure longer reference files** - For files longer than 100 lines, include a table of contents at the top so Claude can see the full scope when previewing.\n\n## Skill Creation Process\n\nSkill creation involves these steps:\n\n1. Understand the skill with concrete examples\n2. Plan reusable skill contents (scripts, references, assets)\n3. Initialize the skill (run init_skill.py)\n4. Edit the skill (implement resources and write SKILL.md)\n5. Package the skill (run package_skill.py)\n6. Iterate based on real usage\n\nFollow these steps in order, skipping only if there is a clear reason why they are not applicable.\n\n### Step 1: Understanding the Skill with Concrete Examples\n\nSkip this step only when the skill's usage patterns are already clearly understood. It remains valuable even when working with an existing skill.\n\nTo create an effective skill, clearly understand concrete examples of how the skill will be used. This understanding can come from either direct user examples or generated examples that are validated with user feedback.\n\nFor example, when building an image-editor skill, relevant questions include:\n\n- \"What functionality should the image-editor skill support? Editing, rotating, anything else?\"\n- \"Can you give some examples of how this skill would be used?\"\n- \"I can imagine users asking for things like 'Remove the red-eye from this image' or 'Rotate this image'. Are there other ways you imagine this skill being used?\"\n- \"What would a user say that should trigger this skill?\"\n\nTo avoid overwhelming users, avoid asking too many questions in a single message. Start with the most important questions and follow up as needed for better effectiveness.\n\nConclude this step when there is a clear sense of the functionality the skill should support.\n\n### Step 2: Planning the Reusable Skill Contents\n\nTo turn concrete examples into an effective skill, analyze each example by:\n\n1. Considering how to execute on the example from scratch\n2. Identifying what scripts, references, and assets would be helpful when executing these workflows repeatedly\n\nExample: When building a `pdf-editor` skill to handle queries like \"Help me rotate this PDF,\" the analysis shows:\n\n1. Rotating a PDF requires re-writing the same code each time\n2. A `scripts/rotate_pdf.py` script would be helpful to store in the skill\n\nExample: When designing a `frontend-webapp-builder` skill for queries like \"Build me a todo app\" or \"Build me a dashboard to track my steps,\" the analysis shows:\n\n1. Writing a frontend webapp requires the same boilerplate HTML/React each time\n2. An `assets/hello-world/` template containing the boilerplate HTML/React project files would be helpful to store in the skill\n\nExample: When building a `big-query` skill to handle queries like \"How many users have logged in today?\" the analysis shows:\n\n1. Querying BigQuery requires re-discovering the table schemas and relationships each time\n2. A `references/schema.md` file documenting the table schemas would be helpful to store in the skill\n\nTo establish the skill's contents, analyze each concrete example to create a list of the reusable resources to include: scripts, references, and assets.\n\n### Step 3: Initializing the Skill\n\nAt this point, it is time to actually create the skill.\n\nSkip this step only if the skill being developed already exists, and iteration or packaging is needed. In this case, continue to the next step.\n\nWhen creating a new skill from scratch, always run the `init_skill.py` script. The script conveniently generates a new template skill directory that automatically includes everything a skill requires, making the skill creation process much more efficient and reliable.\n\nUsage:\n\n```bash\nscripts/init_skill.py <skill-name> --path <output-directory>\n```\n\nThe script:\n\n- Creates the skill directory at the specified path\n- Generates a SKILL.md template with proper frontmatter and TODO placeholders\n- Creates example resource directories: `scripts/`, `references/`, and `assets/`\n- Adds example files in each directory that can be customized or deleted\n\nAfter initialization, customize or remove the generated SKILL.md and example files as needed.\n\n### Step 4: Edit the Skill\n\nWhen editing the (newly-generated or existing) skill, remember that the skill is being created for another instance of Claude to use. Include information that would be beneficial and non-obvious to Claude. Consider what procedural knowledge, domain-specific details, or reusable assets would help another Claude instance execute these tasks more effectively.\n\n#### Learn Proven Design Patterns\n\nConsult these helpful guides based on your skill's needs:\n\n- **Multi-step processes**: See references/workflows.md for sequential workflows and conditional logic\n- **Specific output formats or quality standards**: See references/output-patterns.md for template and example patterns\n\nThese files contain established best practices for effective skill design.\n\n#### Start with Reusable Skill Contents\n\nTo begin implementation, start with the reusable resources identified above: `scripts/`, `references/`, and `assets/` files. Note that this step may require user input. For example, when implementing a `brand-guidelines` skill, the user may need to provide brand assets or templates to store in `assets/`, or documentation to store in `references/`.\n\nAdded scripts must be tested by actually running them to ensure there are no bugs and that the output matches what is expected. If there are many similar scripts, only a representative sample needs to be tested to ensure confidence that they all work while balancing time to completion.\n\nAny example files and directories not needed for the skill should be deleted. The initialization script creates example files in `scripts/`, `references/`, and `assets/` to demonstrate structure, but most skills won't need all of them.\n\n#### Update SKILL.md\n\n**Writing Guidelines:** Always use imperative/infinitive form.\n\n##### Frontmatter\n\nWrite the YAML frontmatter with `name` and `description`:\n\n- `name`: The skill name\n- `description`: This is the primary triggering mechanism for your skill, and helps Claude understand when to use the skill.\n  - Include both what the Skill does and specific triggers/contexts for when to use it.\n  - Include all \"when to use\" information here - Not in the body. The body is only loaded after triggering, so \"When to Use This Skill\" sections in the body are not helpful to Claude.\n  - Example description for a `docx` skill: \"Comprehensive document creation, editing, and analysis with support for tracked changes, comments, formatting preservation, and text extraction. Use when Claude needs to work with professional documents (.docx files) for: (1) Creating new documents, (2) Modifying or editing content, (3) Working with tracked changes, (4) Adding comments, or any other document tasks\"\n\nDo not include any other fields in YAML frontmatter.\n\n##### Body\n\nWrite instructions for using the skill and its bundled resources.\n\n### Step 5: Packaging a Skill\n\nOnce development of the skill is complete, it must be packaged into a distributable .skill file that gets shared with the user. The packaging process automatically validates the skill first to ensure it meets all requirements:\n\n```bash\nscripts/package_skill.py <path/to/skill-folder>\n```\n\nOptional output directory specification:\n\n```bash\nscripts/package_skill.py <path/to/skill-folder> ./dist\n```\n\nThe packaging script will:\n\n1. **Validate** the skill automatically, checking:\n\n   - YAML frontmatter format and required fields\n   - Skill naming conventions and directory structure\n   - Description completeness and quality\n   - File organization and resource references\n\n2. **Package** the skill if validation passes, creating a .skill file named after the skill (e.g., `my-skill.skill`) that includes all files and maintains the proper directory structure for distribution. The .skill file is a zip file with a .skill extension.\n\nIf validation fails, the script will report the errors and exit without creating a package. Fix any validation errors and run the packaging command again.\n\n### Step 6: Iterate\n\nAfter testing the skill, users may request improvements. Often this happens right after using the skill, with fresh context of how the skill performed.\n\n**Iteration workflow:**\n\n1. Use the skill on real tasks\n2. Notice struggles or inefficiencies\n3. Identify how SKILL.md or bundled resources should be updated\n4. Implement changes and test again\n",
        "data/anthropics/slack-gif-creator/SKILL.md": "---\nname: slack-gif-creator\ndescription: Knowledge and utilities for creating animated GIFs optimized for Slack. Provides constraints, validation tools, and animation concepts. Use when users request animated GIFs for Slack like \"make me a GIF of X doing Y for Slack.\"\nlicense: Complete terms in LICENSE.txt\n---\n\n# Slack GIF Creator\n\nA toolkit providing utilities and knowledge for creating animated GIFs optimized for Slack.\n\n## Slack Requirements\n\n**Dimensions:**\n- Emoji GIFs: 128x128 (recommended)\n- Message GIFs: 480x480\n\n**Parameters:**\n- FPS: 10-30 (lower is smaller file size)\n- Colors: 48-128 (fewer = smaller file size)\n- Duration: Keep under 3 seconds for emoji GIFs\n\n## Core Workflow\n\n```python\nfrom core.gif_builder import GIFBuilder\nfrom PIL import Image, ImageDraw\n\n# 1. Create builder\nbuilder = GIFBuilder(width=128, height=128, fps=10)\n\n# 2. Generate frames\nfor i in range(12):\n    frame = Image.new('RGB', (128, 128), (240, 248, 255))\n    draw = ImageDraw.Draw(frame)\n\n    # Draw your animation using PIL primitives\n    # (circles, polygons, lines, etc.)\n\n    builder.add_frame(frame)\n\n# 3. Save with optimization\nbuilder.save('output.gif', num_colors=48, optimize_for_emoji=True)\n```\n\n## Drawing Graphics\n\n### Working with User-Uploaded Images\nIf a user uploads an image, consider whether they want to:\n- **Use it directly** (e.g., \"animate this\", \"split this into frames\")\n- **Use it as inspiration** (e.g., \"make something like this\")\n\nLoad and work with images using PIL:\n```python\nfrom PIL import Image\n\nuploaded = Image.open('file.png')\n# Use directly, or just as reference for colors/style\n```\n\n### Drawing from Scratch\nWhen drawing graphics from scratch, use PIL ImageDraw primitives:\n\n```python\nfrom PIL import ImageDraw\n\ndraw = ImageDraw.Draw(frame)\n\n# Circles/ovals\ndraw.ellipse([x1, y1, x2, y2], fill=(r, g, b), outline=(r, g, b), width=3)\n\n# Stars, triangles, any polygon\npoints = [(x1, y1), (x2, y2), (x3, y3), ...]\ndraw.polygon(points, fill=(r, g, b), outline=(r, g, b), width=3)\n\n# Lines\ndraw.line([(x1, y1), (x2, y2)], fill=(r, g, b), width=5)\n\n# Rectangles\ndraw.rectangle([x1, y1, x2, y2], fill=(r, g, b), outline=(r, g, b), width=3)\n```\n\n**Don't use:** Emoji fonts (unreliable across platforms) or assume pre-packaged graphics exist in this skill.\n\n### Making Graphics Look Good\n\nGraphics should look polished and creative, not basic. Here's how:\n\n**Use thicker lines** - Always set `width=2` or higher for outlines and lines. Thin lines (width=1) look choppy and amateurish.\n\n**Add visual depth**:\n- Use gradients for backgrounds (`create_gradient_background`)\n- Layer multiple shapes for complexity (e.g., a star with a smaller star inside)\n\n**Make shapes more interesting**:\n- Don't just draw a plain circle - add highlights, rings, or patterns\n- Stars can have glows (draw larger, semi-transparent versions behind)\n- Combine multiple shapes (stars + sparkles, circles + rings)\n\n**Pay attention to colors**:\n- Use vibrant, complementary colors\n- Add contrast (dark outlines on light shapes, light outlines on dark shapes)\n- Consider the overall composition\n\n**For complex shapes** (hearts, snowflakes, etc.):\n- Use combinations of polygons and ellipses\n- Calculate points carefully for symmetry\n- Add details (a heart can have a highlight curve, snowflakes have intricate branches)\n\nBe creative and detailed! A good Slack GIF should look polished, not like placeholder graphics.\n\n## Available Utilities\n\n### GIFBuilder (`core.gif_builder`)\nAssembles frames and optimizes for Slack:\n```python\nbuilder = GIFBuilder(width=128, height=128, fps=10)\nbuilder.add_frame(frame)  # Add PIL Image\nbuilder.add_frames(frames)  # Add list of frames\nbuilder.save('out.gif', num_colors=48, optimize_for_emoji=True, remove_duplicates=True)\n```\n\n### Validators (`core.validators`)\nCheck if GIF meets Slack requirements:\n```python\nfrom core.validators import validate_gif, is_slack_ready\n\n# Detailed validation\npasses, info = validate_gif('my.gif', is_emoji=True, verbose=True)\n\n# Quick check\nif is_slack_ready('my.gif'):\n    print(\"Ready!\")\n```\n\n### Easing Functions (`core.easing`)\nSmooth motion instead of linear:\n```python\nfrom core.easing import interpolate\n\n# Progress from 0.0 to 1.0\nt = i / (num_frames - 1)\n\n# Apply easing\ny = interpolate(start=0, end=400, t=t, easing='ease_out')\n\n# Available: linear, ease_in, ease_out, ease_in_out,\n#           bounce_out, elastic_out, back_out\n```\n\n### Frame Helpers (`core.frame_composer`)\nConvenience functions for common needs:\n```python\nfrom core.frame_composer import (\n    create_blank_frame,         # Solid color background\n    create_gradient_background,  # Vertical gradient\n    draw_circle,                # Helper for circles\n    draw_text,                  # Simple text rendering\n    draw_star                   # 5-pointed star\n)\n```\n\n## Animation Concepts\n\n### Shake/Vibrate\nOffset object position with oscillation:\n- Use `math.sin()` or `math.cos()` with frame index\n- Add small random variations for natural feel\n- Apply to x and/or y position\n\n### Pulse/Heartbeat\nScale object size rhythmically:\n- Use `math.sin(t * frequency * 2 * math.pi)` for smooth pulse\n- For heartbeat: two quick pulses then pause (adjust sine wave)\n- Scale between 0.8 and 1.2 of base size\n\n### Bounce\nObject falls and bounces:\n- Use `interpolate()` with `easing='bounce_out'` for landing\n- Use `easing='ease_in'` for falling (accelerating)\n- Apply gravity by increasing y velocity each frame\n\n### Spin/Rotate\nRotate object around center:\n- PIL: `image.rotate(angle, resample=Image.BICUBIC)`\n- For wobble: use sine wave for angle instead of linear\n\n### Fade In/Out\nGradually appear or disappear:\n- Create RGBA image, adjust alpha channel\n- Or use `Image.blend(image1, image2, alpha)`\n- Fade in: alpha from 0 to 1\n- Fade out: alpha from 1 to 0\n\n### Slide\nMove object from off-screen to position:\n- Start position: outside frame bounds\n- End position: target location\n- Use `interpolate()` with `easing='ease_out'` for smooth stop\n- For overshoot: use `easing='back_out'`\n\n### Zoom\nScale and position for zoom effect:\n- Zoom in: scale from 0.1 to 2.0, crop center\n- Zoom out: scale from 2.0 to 1.0\n- Can add motion blur for drama (PIL filter)\n\n### Explode/Particle Burst\nCreate particles radiating outward:\n- Generate particles with random angles and velocities\n- Update each particle: `x += vx`, `y += vy`\n- Add gravity: `vy += gravity_constant`\n- Fade out particles over time (reduce alpha)\n\n## Optimization Strategies\n\nOnly when asked to make the file size smaller, implement a few of the following methods:\n\n1. **Fewer frames** - Lower FPS (10 instead of 20) or shorter duration\n2. **Fewer colors** - `num_colors=48` instead of 128\n3. **Smaller dimensions** - 128x128 instead of 480x480\n4. **Remove duplicates** - `remove_duplicates=True` in save()\n5. **Emoji mode** - `optimize_for_emoji=True` auto-optimizes\n\n```python\n# Maximum optimization for emoji\nbuilder.save(\n    'emoji.gif',\n    num_colors=48,\n    optimize_for_emoji=True,\n    remove_duplicates=True\n)\n```\n\n## Philosophy\n\nThis skill provides:\n- **Knowledge**: Slack's requirements and animation concepts\n- **Utilities**: GIFBuilder, validators, easing functions\n- **Flexibility**: Create the animation logic using PIL primitives\n\nIt does NOT provide:\n- Rigid animation templates or pre-made functions\n- Emoji font rendering (unreliable across platforms)\n- A library of pre-packaged graphics built into the skill\n\n**Note on user uploads**: This skill doesn't include pre-built graphics, but if a user uploads an image, use PIL to load and work with it - interpret based on their request whether they want it used directly or just as inspiration.\n\nBe creative! Combine concepts (bouncing + rotating, pulsing + sliding, etc.) and use PIL's full capabilities.\n\n## Dependencies\n\n```bash\npip install pillow imageio numpy\n```\n",
        "data/anthropics/theme-factory/SKILL.md": "---\nname: theme-factory\ndescription: Toolkit for styling artifacts with a theme. These artifacts can be slides, docs, reportings, HTML landing pages, etc. There are 10 pre-set themes with colors/fonts that you can apply to any artifact that has been creating, or can generate a new theme on-the-fly.\nlicense: Complete terms in LICENSE.txt\n---\n\n\n# Theme Factory Skill\n\nThis skill provides a curated collection of professional font and color themes themes, each with carefully selected color palettes and font pairings. Once a theme is chosen, it can be applied to any artifact.\n\n## Purpose\n\nTo apply consistent, professional styling to presentation slide decks, use this skill. Each theme includes:\n- A cohesive color palette with hex codes\n- Complementary font pairings for headers and body text\n- A distinct visual identity suitable for different contexts and audiences\n\n## Usage Instructions\n\nTo apply styling to a slide deck or other artifact:\n\n1. **Show the theme showcase**: Display the `theme-showcase.pdf` file to allow users to see all available themes visually. Do not make any modifications to it; simply show the file for viewing.\n2. **Ask for their choice**: Ask which theme to apply to the deck\n3. **Wait for selection**: Get explicit confirmation about the chosen theme\n4. **Apply the theme**: Once a theme has been chosen, apply the selected theme's colors and fonts to the deck/artifact\n\n## Themes Available\n\nThe following 10 themes are available, each showcased in `theme-showcase.pdf`:\n\n1. **Ocean Depths** - Professional and calming maritime theme\n2. **Sunset Boulevard** - Warm and vibrant sunset colors\n3. **Forest Canopy** - Natural and grounded earth tones\n4. **Modern Minimalist** - Clean and contemporary grayscale\n5. **Golden Hour** - Rich and warm autumnal palette\n6. **Arctic Frost** - Cool and crisp winter-inspired theme\n7. **Desert Rose** - Soft and sophisticated dusty tones\n8. **Tech Innovation** - Bold and modern tech aesthetic\n9. **Botanical Garden** - Fresh and organic garden colors\n10. **Midnight Galaxy** - Dramatic and cosmic deep tones\n\n## Theme Details\n\nEach theme is defined in the `themes/` directory with complete specifications including:\n- Cohesive color palette with hex codes\n- Complementary font pairings for headers and body text\n- Distinct visual identity suitable for different contexts and audiences\n\n## Application Process\n\nAfter a preferred theme is selected:\n1. Read the corresponding theme file from the `themes/` directory\n2. Apply the specified colors and fonts consistently throughout the deck\n3. Ensure proper contrast and readability\n4. Maintain the theme's visual identity across all slides\n\n## Create your Own Theme\nTo handle cases where none of the existing themes work for an artifact, create a custom theme. Based on provided inputs, generate a new theme similar to the ones above. Give the theme a similar name describing what the font/color combinations represent. Use any basic description provided to choose appropriate colors/fonts. After generating the theme, show it for review and verification. Following that, apply the theme as described above.\n",
        "data/anthropics/web-artifacts-builder/SKILL.md": "---\nname: web-artifacts-builder\ndescription: Suite of tools for creating elaborate, multi-component claude.ai HTML artifacts using modern frontend web technologies (React, Tailwind CSS, shadcn/ui). Use for complex artifacts requiring state management, routing, or shadcn/ui components - not for simple single-file HTML/JSX artifacts.\nlicense: Complete terms in LICENSE.txt\n---\n\n# Web Artifacts Builder\n\nTo build powerful frontend claude.ai artifacts, follow these steps:\n1. Initialize the frontend repo using `scripts/init-artifact.sh`\n2. Develop your artifact by editing the generated code\n3. Bundle all code into a single HTML file using `scripts/bundle-artifact.sh`\n4. Display artifact to user\n5. (Optional) Test the artifact\n\n**Stack**: React 18 + TypeScript + Vite + Parcel (bundling) + Tailwind CSS + shadcn/ui\n\n## Design & Style Guidelines\n\nVERY IMPORTANT: To avoid what is often referred to as \"AI slop\", avoid using excessive centered layouts, purple gradients, uniform rounded corners, and Inter font.\n\n## Quick Start\n\n### Step 1: Initialize Project\n\nRun the initialization script to create a new React project:\n```bash\nbash scripts/init-artifact.sh <project-name>\ncd <project-name>\n```\n\nThis creates a fully configured project with:\n-  React + TypeScript (via Vite)\n-  Tailwind CSS 3.4.1 with shadcn/ui theming system\n-  Path aliases (`@/`) configured\n-  40+ shadcn/ui components pre-installed\n-  All Radix UI dependencies included\n-  Parcel configured for bundling (via .parcelrc)\n-  Node 18+ compatibility (auto-detects and pins Vite version)\n\n### Step 2: Develop Your Artifact\n\nTo build the artifact, edit the generated files. See **Common Development Tasks** below for guidance.\n\n### Step 3: Bundle to Single HTML File\n\nTo bundle the React app into a single HTML artifact:\n```bash\nbash scripts/bundle-artifact.sh\n```\n\nThis creates `bundle.html` - a self-contained artifact with all JavaScript, CSS, and dependencies inlined. This file can be directly shared in Claude conversations as an artifact.\n\n**Requirements**: Your project must have an `index.html` in the root directory.\n\n**What the script does**:\n- Installs bundling dependencies (parcel, @parcel/config-default, parcel-resolver-tspaths, html-inline)\n- Creates `.parcelrc` config with path alias support\n- Builds with Parcel (no source maps)\n- Inlines all assets into single HTML using html-inline\n\n### Step 4: Share Artifact with User\n\nFinally, share the bundled HTML file in conversation with the user so they can view it as an artifact.\n\n### Step 5: Testing/Visualizing the Artifact (Optional)\n\nNote: This is a completely optional step. Only perform if necessary or requested.\n\nTo test/visualize the artifact, use available tools (including other Skills or built-in tools like Playwright or Puppeteer). In general, avoid testing the artifact upfront as it adds latency between the request and when the finished artifact can be seen. Test later, after presenting the artifact, if requested or if issues arise.\n\n## Reference\n\n- **shadcn/ui components**: https://ui.shadcn.com/docs/components",
        "data/anthropics/webapp-testing/SKILL.md": "---\nname: webapp-testing\ndescription: Toolkit for interacting with and testing local web applications using Playwright. Supports verifying frontend functionality, debugging UI behavior, capturing browser screenshots, and viewing browser logs.\nlicense: Complete terms in LICENSE.txt\n---\n\n# Web Application Testing\n\nTo test local web applications, write native Python Playwright scripts.\n\n**Helper Scripts Available**:\n- `scripts/with_server.py` - Manages server lifecycle (supports multiple servers)\n\n**Always run scripts with `--help` first** to see usage. DO NOT read the source until you try running the script first and find that a customized solution is abslutely necessary. These scripts can be very large and thus pollute your context window. They exist to be called directly as black-box scripts rather than ingested into your context window.\n\n## Decision Tree: Choosing Your Approach\n\n```\nUser task  Is it static HTML?\n     Yes  Read HTML file directly to identify selectors\n              Success  Write Playwright script using selectors\n              Fails/Incomplete  Treat as dynamic (below)\n    \n     No (dynamic webapp)  Is the server already running?\n         No  Run: python scripts/with_server.py --help\n                Then use the helper + write simplified Playwright script\n        \n         Yes  Reconnaissance-then-action:\n            1. Navigate and wait for networkidle\n            2. Take screenshot or inspect DOM\n            3. Identify selectors from rendered state\n            4. Execute actions with discovered selectors\n```\n\n## Example: Using with_server.py\n\nTo start a server, run `--help` first, then use the helper:\n\n**Single server:**\n```bash\npython scripts/with_server.py --server \"npm run dev\" --port 5173 -- python your_automation.py\n```\n\n**Multiple servers (e.g., backend + frontend):**\n```bash\npython scripts/with_server.py \\\n  --server \"cd backend && python server.py\" --port 3000 \\\n  --server \"cd frontend && npm run dev\" --port 5173 \\\n  -- python your_automation.py\n```\n\nTo create an automation script, include only Playwright logic (servers are managed automatically):\n```python\nfrom playwright.sync_api import sync_playwright\n\nwith sync_playwright() as p:\n    browser = p.chromium.launch(headless=True) # Always launch chromium in headless mode\n    page = browser.new_page()\n    page.goto('http://localhost:5173') # Server already running and ready\n    page.wait_for_load_state('networkidle') # CRITICAL: Wait for JS to execute\n    # ... your automation logic\n    browser.close()\n```\n\n## Reconnaissance-Then-Action Pattern\n\n1. **Inspect rendered DOM**:\n   ```python\n   page.screenshot(path='/tmp/inspect.png', full_page=True)\n   content = page.content()\n   page.locator('button').all()\n   ```\n\n2. **Identify selectors** from inspection results\n\n3. **Execute actions** using discovered selectors\n\n## Common Pitfall\n\n **Don't** inspect the DOM before waiting for `networkidle` on dynamic apps\n **Do** wait for `page.wait_for_load_state('networkidle')` before inspection\n\n## Best Practices\n\n- **Use bundled scripts as black boxes** - To accomplish a task, consider whether one of the scripts available in `scripts/` can help. These scripts handle common, complex workflows reliably without cluttering the context window. Use `--help` to see usage, then invoke directly. \n- Use `sync_playwright()` for synchronous scripts\n- Always close the browser when done\n- Use descriptive selectors: `text=`, `role=`, CSS selectors, or IDs\n- Add appropriate waits: `page.wait_for_selector()` or `page.wait_for_timeout()`\n\n## Reference Files\n\n- **examples/** - Examples showing common patterns:\n  - `element_discovery.py` - Discovering buttons, links, and inputs on a page\n  - `static_html_automation.py` - Using file:// URLs for local HTML\n  - `console_logging.py` - Capturing console logs during automation",
        "data/k-dense-ai/adaptyv/SKILL.md": "---\nname: adaptyv\ndescription: Cloud laboratory platform for automated protein testing and validation. Use when designing proteins and needing experimental validation including binding assays, expression testing, thermostability measurements, enzyme activity assays, or protein sequence optimization. Also use for submitting experiments via API, tracking experiment status, downloading results, optimizing protein sequences for better expression using computational tools (NetSolP, SoluProt, SolubleMPNN, ESM), or managing protein design workflows with wet-lab validation.\n---\n\n# Adaptyv\n\nAdaptyv is a cloud laboratory platform that provides automated protein testing and validation services. Submit protein sequences via API or web interface and receive experimental results in approximately 21 days.\n\n## Quick Start\n\n### Authentication Setup\n\nAdaptyv requires API authentication. Set up your credentials:\n\n1. Contact support@adaptyvbio.com to request API access (platform is in alpha/beta)\n2. Receive your API access token\n3. Set environment variable:\n\n```bash\nexport ADAPTYV_API_KEY=\"your_api_key_here\"\n```\n\nOr create a `.env` file:\n\n```\nADAPTYV_API_KEY=your_api_key_here\n```\n\n### Installation\n\nInstall the required package using uv:\n\n```bash\nuv pip install requests python-dotenv\n```\n\n### Basic Usage\n\nSubmit protein sequences for testing:\n\n```python\nimport os\nimport requests\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\napi_key = os.getenv(\"ADAPTYV_API_KEY\")\nbase_url = \"https://kq5jp7qj7wdqklhsxmovkzn4l40obksv.lambda-url.eu-central-1.on.aws\"\n\nheaders = {\n    \"Authorization\": f\"Bearer {api_key}\",\n    \"Content-Type\": \"application/json\"\n}\n\n# Submit experiment\nresponse = requests.post(\n    f\"{base_url}/experiments\",\n    headers=headers,\n    json={\n        \"sequences\": \">protein1\\nMKVLWALLGLLGAA...\",\n        \"experiment_type\": \"binding\",\n        \"webhook_url\": \"https://your-webhook.com/callback\"\n    }\n)\n\nexperiment_id = response.json()[\"experiment_id\"]\n```\n\n## Available Experiment Types\n\nAdaptyv supports multiple assay types:\n\n- **Binding assays** - Test protein-target interactions using biolayer interferometry\n- **Expression testing** - Measure protein expression levels\n- **Thermostability** - Characterize protein thermal stability\n- **Enzyme activity** - Assess enzymatic function\n\nSee `reference/experiments.md` for detailed information on each experiment type and workflows.\n\n## Protein Sequence Optimization\n\nBefore submitting sequences, optimize them for better expression and stability:\n\n**Common issues to address:**\n- Unpaired cysteines that create unwanted disulfides\n- Excessive hydrophobic regions causing aggregation\n- Poor solubility predictions\n\n**Recommended tools:**\n- NetSolP / SoluProt - Initial solubility filtering\n- SolubleMPNN - Sequence redesign for improved solubility\n- ESM - Sequence likelihood scoring\n- ipTM - Interface stability assessment\n- pSAE - Hydrophobic exposure quantification\n\nSee `reference/protein_optimization.md` for detailed optimization workflows and tool usage.\n\n## API Reference\n\nFor complete API documentation including all endpoints, request/response formats, and authentication details, see `reference/api_reference.md`.\n\n## Examples\n\nFor concrete code examples covering common use cases (experiment submission, status tracking, result retrieval, batch processing), see `reference/examples.md`.\n\n## Important Notes\n\n- Platform is currently in alpha/beta phase with features subject to change\n- Not all platform features are available via API yet\n- Results typically delivered in ~21 days\n- Contact support@adaptyvbio.com for access requests or questions\n- Suitable for high-throughput AI-driven protein design workflows\n",
        "data/k-dense-ai/aeon/SKILL.md": "---\nname: aeon\ndescription: This skill should be used for time series machine learning tasks including classification, regression, clustering, forecasting, anomaly detection, segmentation, and similarity search. Use when working with temporal data, sequential patterns, or time-indexed observations requiring specialized algorithms beyond standard ML approaches. Particularly suited for univariate and multivariate time series analysis with scikit-learn compatible APIs.\n---\n\n# Aeon Time Series Machine Learning\n\n## Overview\n\nAeon is a scikit-learn compatible Python toolkit for time series machine learning. It provides state-of-the-art algorithms for classification, regression, clustering, forecasting, anomaly detection, segmentation, and similarity search.\n\n## When to Use This Skill\n\nApply this skill when:\n- Classifying or predicting from time series data\n- Detecting anomalies or change points in temporal sequences\n- Clustering similar time series patterns\n- Forecasting future values\n- Finding repeated patterns (motifs) or unusual subsequences (discords)\n- Comparing time series with specialized distance metrics\n- Extracting features from temporal data\n\n## Installation\n\n```bash\nuv pip install aeon\n```\n\n## Core Capabilities\n\n### 1. Time Series Classification\n\nCategorize time series into predefined classes. See `references/classification.md` for complete algorithm catalog.\n\n**Quick Start:**\n```python\nfrom aeon.classification.convolution_based import RocketClassifier\nfrom aeon.datasets import load_classification\n\n# Load data\nX_train, y_train = load_classification(\"GunPoint\", split=\"train\")\nX_test, y_test = load_classification(\"GunPoint\", split=\"test\")\n\n# Train classifier\nclf = RocketClassifier(n_kernels=10000)\nclf.fit(X_train, y_train)\naccuracy = clf.score(X_test, y_test)\n```\n\n**Algorithm Selection:**\n- **Speed + Performance**: `MiniRocketClassifier`, `Arsenal`\n- **Maximum Accuracy**: `HIVECOTEV2`, `InceptionTimeClassifier`\n- **Interpretability**: `ShapeletTransformClassifier`, `Catch22Classifier`\n- **Small Datasets**: `KNeighborsTimeSeriesClassifier` with DTW distance\n\n### 2. Time Series Regression\n\nPredict continuous values from time series. See `references/regression.md` for algorithms.\n\n**Quick Start:**\n```python\nfrom aeon.regression.convolution_based import RocketRegressor\nfrom aeon.datasets import load_regression\n\nX_train, y_train = load_regression(\"Covid3Month\", split=\"train\")\nX_test, y_test = load_regression(\"Covid3Month\", split=\"test\")\n\nreg = RocketRegressor()\nreg.fit(X_train, y_train)\npredictions = reg.predict(X_test)\n```\n\n### 3. Time Series Clustering\n\nGroup similar time series without labels. See `references/clustering.md` for methods.\n\n**Quick Start:**\n```python\nfrom aeon.clustering import TimeSeriesKMeans\n\nclusterer = TimeSeriesKMeans(\n    n_clusters=3,\n    distance=\"dtw\",\n    averaging_method=\"ba\"\n)\nlabels = clusterer.fit_predict(X_train)\ncenters = clusterer.cluster_centers_\n```\n\n### 4. Forecasting\n\nPredict future time series values. See `references/forecasting.md` for forecasters.\n\n**Quick Start:**\n```python\nfrom aeon.forecasting.arima import ARIMA\n\nforecaster = ARIMA(order=(1, 1, 1))\nforecaster.fit(y_train)\ny_pred = forecaster.predict(fh=[1, 2, 3, 4, 5])\n```\n\n### 5. Anomaly Detection\n\nIdentify unusual patterns or outliers. See `references/anomaly_detection.md` for detectors.\n\n**Quick Start:**\n```python\nfrom aeon.anomaly_detection import STOMP\n\ndetector = STOMP(window_size=50)\nanomaly_scores = detector.fit_predict(y)\n\n# Higher scores indicate anomalies\nthreshold = np.percentile(anomaly_scores, 95)\nanomalies = anomaly_scores > threshold\n```\n\n### 6. Segmentation\n\nPartition time series into regions with change points. See `references/segmentation.md`.\n\n**Quick Start:**\n```python\nfrom aeon.segmentation import ClaSPSegmenter\n\nsegmenter = ClaSPSegmenter()\nchange_points = segmenter.fit_predict(y)\n```\n\n### 7. Similarity Search\n\nFind similar patterns within or across time series. See `references/similarity_search.md`.\n\n**Quick Start:**\n```python\nfrom aeon.similarity_search import StompMotif\n\n# Find recurring patterns\nmotif_finder = StompMotif(window_size=50, k=3)\nmotifs = motif_finder.fit_predict(y)\n```\n\n## Feature Extraction and Transformations\n\nTransform time series for feature engineering. See `references/transformations.md`.\n\n**ROCKET Features:**\n```python\nfrom aeon.transformations.collection.convolution_based import RocketTransformer\n\nrocket = RocketTransformer()\nX_features = rocket.fit_transform(X_train)\n\n# Use features with any sklearn classifier\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier()\nclf.fit(X_features, y_train)\n```\n\n**Statistical Features:**\n```python\nfrom aeon.transformations.collection.feature_based import Catch22\n\ncatch22 = Catch22()\nX_features = catch22.fit_transform(X_train)\n```\n\n**Preprocessing:**\n```python\nfrom aeon.transformations.collection import MinMaxScaler, Normalizer\n\nscaler = Normalizer()  # Z-normalization\nX_normalized = scaler.fit_transform(X_train)\n```\n\n## Distance Metrics\n\nSpecialized temporal distance measures. See `references/distances.md` for complete catalog.\n\n**Usage:**\n```python\nfrom aeon.distances import dtw_distance, dtw_pairwise_distance\n\n# Single distance\ndistance = dtw_distance(x, y, window=0.1)\n\n# Pairwise distances\ndistance_matrix = dtw_pairwise_distance(X_train)\n\n# Use with classifiers\nfrom aeon.classification.distance_based import KNeighborsTimeSeriesClassifier\n\nclf = KNeighborsTimeSeriesClassifier(\n    n_neighbors=5,\n    distance=\"dtw\",\n    distance_params={\"window\": 0.2}\n)\n```\n\n**Available Distances:**\n- **Elastic**: DTW, DDTW, WDTW, ERP, EDR, LCSS, TWE, MSM\n- **Lock-step**: Euclidean, Manhattan, Minkowski\n- **Shape-based**: Shape DTW, SBD\n\n## Deep Learning Networks\n\nNeural architectures for time series. See `references/networks.md`.\n\n**Architectures:**\n- Convolutional: `FCNClassifier`, `ResNetClassifier`, `InceptionTimeClassifier`\n- Recurrent: `RecurrentNetwork`, `TCNNetwork`\n- Autoencoders: `AEFCNClusterer`, `AEResNetClusterer`\n\n**Usage:**\n```python\nfrom aeon.classification.deep_learning import InceptionTimeClassifier\n\nclf = InceptionTimeClassifier(n_epochs=100, batch_size=32)\nclf.fit(X_train, y_train)\npredictions = clf.predict(X_test)\n```\n\n## Datasets and Benchmarking\n\nLoad standard benchmarks and evaluate performance. See `references/datasets_benchmarking.md`.\n\n**Load Datasets:**\n```python\nfrom aeon.datasets import load_classification, load_regression\n\n# Classification\nX_train, y_train = load_classification(\"ArrowHead\", split=\"train\")\n\n# Regression\nX_train, y_train = load_regression(\"Covid3Month\", split=\"train\")\n```\n\n**Benchmarking:**\n```python\nfrom aeon.benchmarking import get_estimator_results\n\n# Compare with published results\npublished = get_estimator_results(\"ROCKET\", \"GunPoint\")\n```\n\n## Common Workflows\n\n### Classification Pipeline\n\n```python\nfrom aeon.transformations.collection import Normalizer\nfrom aeon.classification.convolution_based import RocketClassifier\nfrom sklearn.pipeline import Pipeline\n\npipeline = Pipeline([\n    ('normalize', Normalizer()),\n    ('classify', RocketClassifier())\n])\n\npipeline.fit(X_train, y_train)\naccuracy = pipeline.score(X_test, y_test)\n```\n\n### Feature Extraction + Traditional ML\n\n```python\nfrom aeon.transformations.collection import RocketTransformer\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# Extract features\nrocket = RocketTransformer()\nX_train_features = rocket.fit_transform(X_train)\nX_test_features = rocket.transform(X_test)\n\n# Train traditional ML\nclf = GradientBoostingClassifier()\nclf.fit(X_train_features, y_train)\npredictions = clf.predict(X_test_features)\n```\n\n### Anomaly Detection with Visualization\n\n```python\nfrom aeon.anomaly_detection import STOMP\nimport matplotlib.pyplot as plt\n\ndetector = STOMP(window_size=50)\nscores = detector.fit_predict(y)\n\nplt.figure(figsize=(15, 5))\nplt.subplot(2, 1, 1)\nplt.plot(y, label='Time Series')\nplt.subplot(2, 1, 2)\nplt.plot(scores, label='Anomaly Scores', color='red')\nplt.axhline(np.percentile(scores, 95), color='k', linestyle='--')\nplt.show()\n```\n\n## Best Practices\n\n### Data Preparation\n\n1. **Normalize**: Most algorithms benefit from z-normalization\n   ```python\n   from aeon.transformations.collection import Normalizer\n   normalizer = Normalizer()\n   X_train = normalizer.fit_transform(X_train)\n   X_test = normalizer.transform(X_test)\n   ```\n\n2. **Handle Missing Values**: Impute before analysis\n   ```python\n   from aeon.transformations.collection import SimpleImputer\n   imputer = SimpleImputer(strategy='mean')\n   X_train = imputer.fit_transform(X_train)\n   ```\n\n3. **Check Data Format**: Aeon expects shape `(n_samples, n_channels, n_timepoints)`\n\n### Model Selection\n\n1. **Start Simple**: Begin with ROCKET variants before deep learning\n2. **Use Validation**: Split training data for hyperparameter tuning\n3. **Compare Baselines**: Test against simple methods (1-NN Euclidean, Naive)\n4. **Consider Resources**: ROCKET for speed, deep learning if GPU available\n\n### Algorithm Selection Guide\n\n**For Fast Prototyping:**\n- Classification: `MiniRocketClassifier`\n- Regression: `MiniRocketRegressor`\n- Clustering: `TimeSeriesKMeans` with Euclidean\n\n**For Maximum Accuracy:**\n- Classification: `HIVECOTEV2`, `InceptionTimeClassifier`\n- Regression: `InceptionTimeRegressor`\n- Forecasting: `ARIMA`, `TCNForecaster`\n\n**For Interpretability:**\n- Classification: `ShapeletTransformClassifier`, `Catch22Classifier`\n- Features: `Catch22`, `TSFresh`\n\n**For Small Datasets:**\n- Distance-based: `KNeighborsTimeSeriesClassifier` with DTW\n- Avoid: Deep learning (requires large data)\n\n## Reference Documentation\n\nDetailed information available in `references/`:\n- `classification.md` - All classification algorithms\n- `regression.md` - Regression methods\n- `clustering.md` - Clustering algorithms\n- `forecasting.md` - Forecasting approaches\n- `anomaly_detection.md` - Anomaly detection methods\n- `segmentation.md` - Segmentation algorithms\n- `similarity_search.md` - Pattern matching and motif discovery\n- `transformations.md` - Feature extraction and preprocessing\n- `distances.md` - Time series distance metrics\n- `networks.md` - Deep learning architectures\n- `datasets_benchmarking.md` - Data loading and evaluation tools\n\n## Additional Resources\n\n- Documentation: https://www.aeon-toolkit.org/\n- GitHub: https://github.com/aeon-toolkit/aeon\n- Examples: https://www.aeon-toolkit.org/en/stable/examples.html\n- API Reference: https://www.aeon-toolkit.org/en/stable/api_reference.html\n",
        "data/k-dense-ai/alphafold-database/SKILL.md": "---\nname: alphafold-database\ndescription: \"Access AlphaFold's 200M+ AI-predicted protein structures. Retrieve structures by UniProt ID, download PDB/mmCIF files, analyze confidence metrics (pLDDT, PAE), for drug discovery and structural biology.\"\n---\n\n# AlphaFold Database\n\n## Overview\n\nAlphaFold DB is a public repository of AI-predicted 3D protein structures for over 200 million proteins, maintained by DeepMind and EMBL-EBI. Access structure predictions with confidence metrics, download coordinate files, retrieve bulk datasets, and integrate predictions into computational workflows.\n\n## When to Use This Skill\n\nThis skill should be used when working with AI-predicted protein structures in scenarios such as:\n\n- Retrieving protein structure predictions by UniProt ID or protein name\n- Downloading PDB/mmCIF coordinate files for structural analysis\n- Analyzing prediction confidence metrics (pLDDT, PAE) to assess reliability\n- Accessing bulk proteome datasets via Google Cloud Platform\n- Comparing predicted structures with experimental data\n- Performing structure-based drug discovery or protein engineering\n- Building structural models for proteins lacking experimental structures\n- Integrating AlphaFold predictions into computational pipelines\n\n## Core Capabilities\n\n### 1. Searching and Retrieving Predictions\n\n**Using Biopython (Recommended):**\n\nThe Biopython library provides the simplest interface for retrieving AlphaFold structures:\n\n```python\nfrom Bio.PDB import alphafold_db\n\n# Get all predictions for a UniProt accession\npredictions = list(alphafold_db.get_predictions(\"P00520\"))\n\n# Download structure file (mmCIF format)\nfor prediction in predictions:\n    cif_file = alphafold_db.download_cif_for(prediction, directory=\"./structures\")\n    print(f\"Downloaded: {cif_file}\")\n\n# Get Structure objects directly\nfrom Bio.PDB import MMCIFParser\nstructures = list(alphafold_db.get_structural_models_for(\"P00520\"))\n```\n\n**Direct API Access:**\n\nQuery predictions using REST endpoints:\n\n```python\nimport requests\n\n# Get prediction metadata for a UniProt accession\nuniprot_id = \"P00520\"\napi_url = f\"https://alphafold.ebi.ac.uk/api/prediction/{uniprot_id}\"\nresponse = requests.get(api_url)\nprediction_data = response.json()\n\n# Extract AlphaFold ID\nalphafold_id = prediction_data[0]['entryId']\nprint(f\"AlphaFold ID: {alphafold_id}\")\n```\n\n**Using UniProt to Find Accessions:**\n\nSearch UniProt to find protein accessions first:\n\n```python\nimport urllib.parse, urllib.request\n\ndef get_uniprot_ids(query, query_type='PDB_ID'):\n    \"\"\"Query UniProt to get accession IDs\"\"\"\n    url = 'https://www.uniprot.org/uploadlists/'\n    params = {\n        'from': query_type,\n        'to': 'ACC',\n        'format': 'txt',\n        'query': query\n    }\n    data = urllib.parse.urlencode(params).encode('ascii')\n    with urllib.request.urlopen(urllib.request.Request(url, data)) as response:\n        return response.read().decode('utf-8').splitlines()\n\n# Example: Find UniProt IDs for a protein name\nprotein_ids = get_uniprot_ids(\"hemoglobin\", query_type=\"GENE_NAME\")\n```\n\n### 2. Downloading Structure Files\n\nAlphaFold provides multiple file formats for each prediction:\n\n**File Types Available:**\n\n- **Model coordinates** (`model_v4.cif`): Atomic coordinates in mmCIF/PDBx format\n- **Confidence scores** (`confidence_v4.json`): Per-residue pLDDT scores (0-100)\n- **Predicted Aligned Error** (`predicted_aligned_error_v4.json`): PAE matrix for residue pair confidence\n\n**Download URLs:**\n\n```python\nimport requests\n\nalphafold_id = \"AF-P00520-F1\"\nversion = \"v4\"\n\n# Model coordinates (mmCIF)\nmodel_url = f\"https://alphafold.ebi.ac.uk/files/{alphafold_id}-model_{version}.cif\"\nresponse = requests.get(model_url)\nwith open(f\"{alphafold_id}.cif\", \"w\") as f:\n    f.write(response.text)\n\n# Confidence scores (JSON)\nconfidence_url = f\"https://alphafold.ebi.ac.uk/files/{alphafold_id}-confidence_{version}.json\"\nresponse = requests.get(confidence_url)\nconfidence_data = response.json()\n\n# Predicted Aligned Error (JSON)\npae_url = f\"https://alphafold.ebi.ac.uk/files/{alphafold_id}-predicted_aligned_error_{version}.json\"\nresponse = requests.get(pae_url)\npae_data = response.json()\n```\n\n**PDB Format (Alternative):**\n\n```python\n# Download as PDB format instead of mmCIF\npdb_url = f\"https://alphafold.ebi.ac.uk/files/{alphafold_id}-model_{version}.pdb\"\nresponse = requests.get(pdb_url)\nwith open(f\"{alphafold_id}.pdb\", \"wb\") as f:\n    f.write(response.content)\n```\n\n### 3. Working with Confidence Metrics\n\nAlphaFold predictions include confidence estimates critical for interpretation:\n\n**pLDDT (per-residue confidence):**\n\n```python\nimport json\nimport requests\n\n# Load confidence scores\nalphafold_id = \"AF-P00520-F1\"\nconfidence_url = f\"https://alphafold.ebi.ac.uk/files/{alphafold_id}-confidence_v4.json\"\nconfidence = requests.get(confidence_url).json()\n\n# Extract pLDDT scores\nplddt_scores = confidence['confidenceScore']\n\n# Interpret confidence levels\n# pLDDT > 90: Very high confidence\n# pLDDT 70-90: High confidence\n# pLDDT 50-70: Low confidence\n# pLDDT < 50: Very low confidence\n\nhigh_confidence_residues = [i for i, score in enumerate(plddt_scores) if score > 90]\nprint(f\"High confidence residues: {len(high_confidence_residues)}/{len(plddt_scores)}\")\n```\n\n**PAE (Predicted Aligned Error):**\n\nPAE indicates confidence in relative domain positions:\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Load PAE matrix\npae_url = f\"https://alphafold.ebi.ac.uk/files/{alphafold_id}-predicted_aligned_error_v4.json\"\npae = requests.get(pae_url).json()\n\n# Visualize PAE matrix\npae_matrix = np.array(pae['distance'])\nplt.figure(figsize=(10, 8))\nplt.imshow(pae_matrix, cmap='viridis_r', vmin=0, vmax=30)\nplt.colorbar(label='PAE ()')\nplt.title(f'Predicted Aligned Error: {alphafold_id}')\nplt.xlabel('Residue')\nplt.ylabel('Residue')\nplt.savefig(f'{alphafold_id}_pae.png', dpi=300, bbox_inches='tight')\n\n# Low PAE values (<5 ) indicate confident relative positioning\n# High PAE values (>15 ) suggest uncertain domain arrangements\n```\n\n### 4. Bulk Data Access via Google Cloud\n\nFor large-scale analyses, use Google Cloud datasets:\n\n**Google Cloud Storage:**\n\n```bash\n# Install gsutil\nuv pip install gsutil\n\n# List available data\ngsutil ls gs://public-datasets-deepmind-alphafold-v4/\n\n# Download entire proteomes (by taxonomy ID)\ngsutil -m cp gs://public-datasets-deepmind-alphafold-v4/proteomes/proteome-tax_id-9606-*.tar .\n\n# Download specific files\ngsutil cp gs://public-datasets-deepmind-alphafold-v4/accession_ids.csv .\n```\n\n**BigQuery Metadata Access:**\n\n```python\nfrom google.cloud import bigquery\n\n# Initialize client\nclient = bigquery.Client()\n\n# Query metadata\nquery = \"\"\"\nSELECT\n  entryId,\n  uniprotAccession,\n  organismScientificName,\n  globalMetricValue,\n  fractionPlddtVeryHigh\nFROM `bigquery-public-data.deepmind_alphafold.metadata`\nWHERE organismScientificName = 'Homo sapiens'\n  AND fractionPlddtVeryHigh > 0.8\nLIMIT 100\n\"\"\"\n\nresults = client.query(query).to_dataframe()\nprint(f\"Found {len(results)} high-confidence human proteins\")\n```\n\n**Download by Species:**\n\n```python\nimport subprocess\n\ndef download_proteome(taxonomy_id, output_dir=\"./proteomes\"):\n    \"\"\"Download all AlphaFold predictions for a species\"\"\"\n    pattern = f\"gs://public-datasets-deepmind-alphafold-v4/proteomes/proteome-tax_id-{taxonomy_id}-*_v4.tar\"\n    cmd = f\"gsutil -m cp {pattern} {output_dir}/\"\n    subprocess.run(cmd, shell=True, check=True)\n\n# Download E. coli proteome (tax ID: 83333)\ndownload_proteome(83333)\n\n# Download human proteome (tax ID: 9606)\ndownload_proteome(9606)\n```\n\n### 5. Parsing and Analyzing Structures\n\nWork with downloaded AlphaFold structures using BioPython:\n\n```python\nfrom Bio.PDB import MMCIFParser, PDBIO\nimport numpy as np\n\n# Parse mmCIF file\nparser = MMCIFParser(QUIET=True)\nstructure = parser.get_structure(\"protein\", \"AF-P00520-F1-model_v4.cif\")\n\n# Extract coordinates\ncoords = []\nfor model in structure:\n    for chain in model:\n        for residue in chain:\n            if 'CA' in residue:  # Alpha carbons only\n                coords.append(residue['CA'].get_coord())\n\ncoords = np.array(coords)\nprint(f\"Structure has {len(coords)} residues\")\n\n# Calculate distances\nfrom scipy.spatial.distance import pdist, squareform\ndistance_matrix = squareform(pdist(coords))\n\n# Identify contacts (< 8 )\ncontacts = np.where((distance_matrix > 0) & (distance_matrix < 8))\nprint(f\"Number of contacts: {len(contacts[0]) // 2}\")\n```\n\n**Extract B-factors (pLDDT values):**\n\nAlphaFold stores pLDDT scores in the B-factor column:\n\n```python\nfrom Bio.PDB import MMCIFParser\n\nparser = MMCIFParser(QUIET=True)\nstructure = parser.get_structure(\"protein\", \"AF-P00520-F1-model_v4.cif\")\n\n# Extract pLDDT from B-factors\nplddt_scores = []\nfor model in structure:\n    for chain in model:\n        for residue in chain:\n            if 'CA' in residue:\n                plddt_scores.append(residue['CA'].get_bfactor())\n\n# Identify high-confidence regions\nhigh_conf_regions = [(i, score) for i, score in enumerate(plddt_scores, 1) if score > 90]\nprint(f\"High confidence residues: {len(high_conf_regions)}\")\n```\n\n### 6. Batch Processing Multiple Proteins\n\nProcess multiple predictions efficiently:\n\n```python\nfrom Bio.PDB import alphafold_db\nimport pandas as pd\n\nuniprot_ids = [\"P00520\", \"P12931\", \"P04637\"]  # Multiple proteins\nresults = []\n\nfor uniprot_id in uniprot_ids:\n    try:\n        # Get prediction\n        predictions = list(alphafold_db.get_predictions(uniprot_id))\n\n        if predictions:\n            pred = predictions[0]\n\n            # Download structure\n            cif_file = alphafold_db.download_cif_for(pred, directory=\"./batch_structures\")\n\n            # Get confidence data\n            alphafold_id = pred['entryId']\n            conf_url = f\"https://alphafold.ebi.ac.uk/files/{alphafold_id}-confidence_v4.json\"\n            conf_data = requests.get(conf_url).json()\n\n            # Calculate statistics\n            plddt_scores = conf_data['confidenceScore']\n            avg_plddt = np.mean(plddt_scores)\n            high_conf_fraction = sum(1 for s in plddt_scores if s > 90) / len(plddt_scores)\n\n            results.append({\n                'uniprot_id': uniprot_id,\n                'alphafold_id': alphafold_id,\n                'avg_plddt': avg_plddt,\n                'high_conf_fraction': high_conf_fraction,\n                'length': len(plddt_scores)\n            })\n    except Exception as e:\n        print(f\"Error processing {uniprot_id}: {e}\")\n\n# Create summary DataFrame\ndf = pd.DataFrame(results)\nprint(df)\n```\n\n## Installation and Setup\n\n### Python Libraries\n\n```bash\n# Install Biopython for structure access\nuv pip install biopython\n\n# Install requests for API access\nuv pip install requests\n\n# For visualization and analysis\nuv pip install numpy matplotlib pandas scipy\n\n# For Google Cloud access (optional)\nuv pip install google-cloud-bigquery gsutil\n```\n\n### 3D-Beacons API Alternative\n\nAlphaFold can also be accessed via the 3D-Beacons federated API:\n\n```python\nimport requests\n\n# Query via 3D-Beacons\nuniprot_id = \"P00520\"\nurl = f\"https://www.ebi.ac.uk/pdbe/pdbe-kb/3dbeacons/api/uniprot/summary/{uniprot_id}.json\"\nresponse = requests.get(url)\ndata = response.json()\n\n# Filter for AlphaFold structures\naf_structures = [s for s in data['structures'] if s['provider'] == 'AlphaFold DB']\n```\n\n## Common Use Cases\n\n### Structural Proteomics\n- Download complete proteome predictions for analysis\n- Identify high-confidence structural regions across proteins\n- Compare predicted structures with experimental data\n- Build structural models for protein families\n\n### Drug Discovery\n- Retrieve target protein structures for docking studies\n- Analyze binding site conformations\n- Identify druggable pockets in predicted structures\n- Compare structures across homologs\n\n### Protein Engineering\n- Identify stable/unstable regions using pLDDT\n- Design mutations in high-confidence regions\n- Analyze domain architectures using PAE\n- Model protein variants and mutations\n\n### Evolutionary Studies\n- Compare ortholog structures across species\n- Analyze conservation of structural features\n- Study domain evolution patterns\n- Identify functionally important regions\n\n## Key Concepts\n\n**UniProt Accession:** Primary identifier for proteins (e.g., \"P00520\"). Required for querying AlphaFold DB.\n\n**AlphaFold ID:** Internal identifier format: `AF-[UniProt accession]-F[fragment number]` (e.g., \"AF-P00520-F1\").\n\n**pLDDT (predicted Local Distance Difference Test):** Per-residue confidence metric (0-100). Higher values indicate more confident predictions.\n\n**PAE (Predicted Aligned Error):** Matrix indicating confidence in relative positions between residue pairs. Low values (<5 ) suggest confident relative positioning.\n\n**Database Version:** Current version is v4. File URLs include version suffix (e.g., `model_v4.cif`).\n\n**Fragment Number:** Large proteins may be split into fragments. Fragment number appears in AlphaFold ID (e.g., F1, F2).\n\n## Confidence Interpretation Guidelines\n\n**pLDDT Thresholds:**\n- **>90**: Very high confidence - suitable for detailed analysis\n- **70-90**: High confidence - generally reliable backbone structure\n- **50-70**: Low confidence - use with caution, flexible regions\n- **<50**: Very low confidence - likely disordered or unreliable\n\n**PAE Guidelines:**\n- **<5 **: Confident relative positioning of domains\n- **5-10 **: Moderate confidence in arrangement\n- **>15 **: Uncertain relative positions, domains may be mobile\n\n## Resources\n\n### references/api_reference.md\n\nComprehensive API documentation covering:\n- Complete REST API endpoint specifications\n- File format details and data schemas\n- Google Cloud dataset structure and access patterns\n- Advanced query examples and batch processing strategies\n- Rate limiting, caching, and best practices\n- Troubleshooting common issues\n\nConsult this reference for detailed API information, bulk download strategies, or when working with large-scale datasets.\n\n## Important Notes\n\n### Data Usage and Attribution\n\n- AlphaFold DB is freely available under CC-BY-4.0 license\n- Cite: Jumper et al. (2021) Nature and Varadi et al. (2022) Nucleic Acids Research\n- Predictions are computational models, not experimental structures\n- Always assess confidence metrics before downstream analysis\n\n### Version Management\n\n- Current database version: v4 (as of 2024-2025)\n- File URLs include version suffix (e.g., `_v4.cif`)\n- Check for database updates regularly\n- Older versions may be deprecated over time\n\n### Data Quality Considerations\n\n- High pLDDT doesn't guarantee functional accuracy\n- Low confidence regions may be disordered in vivo\n- PAE indicates relative domain confidence, not absolute positioning\n- Predictions lack ligands, post-translational modifications, and cofactors\n- Multi-chain complexes are not predicted (single chains only)\n\n### Performance Tips\n\n- Use Biopython for simple single-protein access\n- Use Google Cloud for bulk downloads (much faster than individual files)\n- Cache downloaded files locally to avoid repeated downloads\n- BigQuery free tier: 1 TB processed data per month\n- Consider network bandwidth for large-scale downloads\n\n## Additional Resources\n\n- **AlphaFold DB Website:** https://alphafold.ebi.ac.uk/\n- **API Documentation:** https://alphafold.ebi.ac.uk/api-docs\n- **Google Cloud Dataset:** https://cloud.google.com/blog/products/ai-machine-learning/alphafold-protein-structure-database\n- **3D-Beacons API:** https://www.ebi.ac.uk/pdbe/pdbe-kb/3dbeacons/\n- **AlphaFold Papers:**\n  - Nature (2021): https://doi.org/10.1038/s41586-021-03819-2\n  - Nucleic Acids Research (2024): https://doi.org/10.1093/nar/gkad1011\n- **Biopython Documentation:** https://biopython.org/docs/dev/api/Bio.PDB.alphafold_db.html\n- **GitHub Repository:** https://github.com/google-deepmind/alphafold\n",
        "data/k-dense-ai/anndata/SKILL.md": "---\nname: anndata\ndescription: This skill should be used when working with annotated data matrices in Python, particularly for single-cell genomics analysis, managing experimental measurements with metadata, or handling large-scale biological datasets. Use when tasks involve AnnData objects, h5ad files, single-cell RNA-seq data, or integration with scanpy/scverse tools.\n---\n\n# AnnData\n\n## Overview\n\nAnnData is a Python package for handling annotated data matrices, storing experimental measurements (X) alongside observation metadata (obs), variable metadata (var), and multi-dimensional annotations (obsm, varm, obsp, varp, uns). Originally designed for single-cell genomics through Scanpy, it now serves as a general-purpose framework for any annotated data requiring efficient storage, manipulation, and analysis.\n\n## When to Use This Skill\n\nUse this skill when:\n- Creating, reading, or writing AnnData objects\n- Working with h5ad, zarr, or other genomics data formats\n- Performing single-cell RNA-seq analysis\n- Managing large datasets with sparse matrices or backed mode\n- Concatenating multiple datasets or experimental batches\n- Subsetting, filtering, or transforming annotated data\n- Integrating with scanpy, scvi-tools, or other scverse ecosystem tools\n\n## Installation\n\n```bash\nuv pip install anndata\n\n# With optional dependencies\nuv pip install anndata[dev,test,doc]\n```\n\n## Quick Start\n\n### Creating an AnnData object\n```python\nimport anndata as ad\nimport numpy as np\nimport pandas as pd\n\n# Minimal creation\nX = np.random.rand(100, 2000)  # 100 cells  2000 genes\nadata = ad.AnnData(X)\n\n# With metadata\nobs = pd.DataFrame({\n    'cell_type': ['T cell', 'B cell'] * 50,\n    'sample': ['A', 'B'] * 50\n}, index=[f'cell_{i}' for i in range(100)])\n\nvar = pd.DataFrame({\n    'gene_name': [f'Gene_{i}' for i in range(2000)]\n}, index=[f'ENSG{i:05d}' for i in range(2000)])\n\nadata = ad.AnnData(X=X, obs=obs, var=var)\n```\n\n### Reading data\n```python\n# Read h5ad file\nadata = ad.read_h5ad('data.h5ad')\n\n# Read with backed mode (for large files)\nadata = ad.read_h5ad('large_data.h5ad', backed='r')\n\n# Read other formats\nadata = ad.read_csv('data.csv')\nadata = ad.read_loom('data.loom')\nadata = ad.read_10x_h5('filtered_feature_bc_matrix.h5')\n```\n\n### Writing data\n```python\n# Write h5ad file\nadata.write_h5ad('output.h5ad')\n\n# Write with compression\nadata.write_h5ad('output.h5ad', compression='gzip')\n\n# Write other formats\nadata.write_zarr('output.zarr')\nadata.write_csvs('output_dir/')\n```\n\n### Basic operations\n```python\n# Subset by conditions\nt_cells = adata[adata.obs['cell_type'] == 'T cell']\n\n# Subset by indices\nsubset = adata[0:50, 0:100]\n\n# Add metadata\nadata.obs['quality_score'] = np.random.rand(adata.n_obs)\nadata.var['highly_variable'] = np.random.rand(adata.n_vars) > 0.8\n\n# Access dimensions\nprint(f\"{adata.n_obs} observations  {adata.n_vars} variables\")\n```\n\n## Core Capabilities\n\n### 1. Data Structure\n\nUnderstand the AnnData object structure including X, obs, var, layers, obsm, varm, obsp, varp, uns, and raw components.\n\n**See**: `references/data_structure.md` for comprehensive information on:\n- Core components (X, obs, var, layers, obsm, varm, obsp, varp, uns, raw)\n- Creating AnnData objects from various sources\n- Accessing and manipulating data components\n- Memory-efficient practices\n\n### 2. Input/Output Operations\n\nRead and write data in various formats with support for compression, backed mode, and cloud storage.\n\n**See**: `references/io_operations.md` for details on:\n- Native formats (h5ad, zarr)\n- Alternative formats (CSV, MTX, Loom, 10X, Excel)\n- Backed mode for large datasets\n- Remote data access\n- Format conversion\n- Performance optimization\n\nCommon commands:\n```python\n# Read/write h5ad\nadata = ad.read_h5ad('data.h5ad', backed='r')\nadata.write_h5ad('output.h5ad', compression='gzip')\n\n# Read 10X data\nadata = ad.read_10x_h5('filtered_feature_bc_matrix.h5')\n\n# Read MTX format\nadata = ad.read_mtx('matrix.mtx').T\n```\n\n### 3. Concatenation\n\nCombine multiple AnnData objects along observations or variables with flexible join strategies.\n\n**See**: `references/concatenation.md` for comprehensive coverage of:\n- Basic concatenation (axis=0 for observations, axis=1 for variables)\n- Join types (inner, outer)\n- Merge strategies (same, unique, first, only)\n- Tracking data sources with labels\n- Lazy concatenation (AnnCollection)\n- On-disk concatenation for large datasets\n\nCommon commands:\n```python\n# Concatenate observations (combine samples)\nadata = ad.concat(\n    [adata1, adata2, adata3],\n    axis=0,\n    join='inner',\n    label='batch',\n    keys=['batch1', 'batch2', 'batch3']\n)\n\n# Concatenate variables (combine modalities)\nadata = ad.concat([adata_rna, adata_protein], axis=1)\n\n# Lazy concatenation\nfrom anndata.experimental import AnnCollection\ncollection = AnnCollection(\n    ['data1.h5ad', 'data2.h5ad'],\n    join_obs='outer',\n    label='dataset'\n)\n```\n\n### 4. Data Manipulation\n\nTransform, subset, filter, and reorganize data efficiently.\n\n**See**: `references/manipulation.md` for detailed guidance on:\n- Subsetting (by indices, names, boolean masks, metadata conditions)\n- Transposition\n- Copying (full copies vs views)\n- Renaming (observations, variables, categories)\n- Type conversions (strings to categoricals, sparse/dense)\n- Adding/removing data components\n- Reordering\n- Quality control filtering\n\nCommon commands:\n```python\n# Subset by metadata\nfiltered = adata[adata.obs['quality_score'] > 0.8]\nhv_genes = adata[:, adata.var['highly_variable']]\n\n# Transpose\nadata_T = adata.T\n\n# Copy vs view\nview = adata[0:100, :]  # View (lightweight reference)\ncopy = adata[0:100, :].copy()  # Independent copy\n\n# Convert strings to categoricals\nadata.strings_to_categoricals()\n```\n\n### 5. Best Practices\n\nFollow recommended patterns for memory efficiency, performance, and reproducibility.\n\n**See**: `references/best_practices.md` for guidelines on:\n- Memory management (sparse matrices, categoricals, backed mode)\n- Views vs copies\n- Data storage optimization\n- Performance optimization\n- Working with raw data\n- Metadata management\n- Reproducibility\n- Error handling\n- Integration with other tools\n- Common pitfalls and solutions\n\nKey recommendations:\n```python\n# Use sparse matrices for sparse data\nfrom scipy.sparse import csr_matrix\nadata.X = csr_matrix(adata.X)\n\n# Convert strings to categoricals\nadata.strings_to_categoricals()\n\n# Use backed mode for large files\nadata = ad.read_h5ad('large.h5ad', backed='r')\n\n# Store raw before filtering\nadata.raw = adata.copy()\nadata = adata[:, adata.var['highly_variable']]\n```\n\n## Integration with Scverse Ecosystem\n\nAnnData serves as the foundational data structure for the scverse ecosystem:\n\n### Scanpy (Single-cell analysis)\n```python\nimport scanpy as sc\n\n# Preprocessing\nsc.pp.filter_cells(adata, min_genes=200)\nsc.pp.normalize_total(adata, target_sum=1e4)\nsc.pp.log1p(adata)\nsc.pp.highly_variable_genes(adata, n_top_genes=2000)\n\n# Dimensionality reduction\nsc.pp.pca(adata, n_comps=50)\nsc.pp.neighbors(adata, n_neighbors=15)\nsc.tl.umap(adata)\nsc.tl.leiden(adata)\n\n# Visualization\nsc.pl.umap(adata, color=['cell_type', 'leiden'])\n```\n\n### Muon (Multimodal data)\n```python\nimport muon as mu\n\n# Combine RNA and protein data\nmdata = mu.MuData({'rna': adata_rna, 'protein': adata_protein})\n```\n\n### PyTorch integration\n```python\nfrom anndata.experimental import AnnLoader\n\n# Create DataLoader for deep learning\ndataloader = AnnLoader(adata, batch_size=128, shuffle=True)\n\nfor batch in dataloader:\n    X = batch.X\n    # Train model\n```\n\n## Common Workflows\n\n### Single-cell RNA-seq analysis\n```python\nimport anndata as ad\nimport scanpy as sc\n\n# 1. Load data\nadata = ad.read_10x_h5('filtered_feature_bc_matrix.h5')\n\n# 2. Quality control\nadata.obs['n_genes'] = (adata.X > 0).sum(axis=1)\nadata.obs['n_counts'] = adata.X.sum(axis=1)\nadata = adata[adata.obs['n_genes'] > 200]\nadata = adata[adata.obs['n_counts'] < 50000]\n\n# 3. Store raw\nadata.raw = adata.copy()\n\n# 4. Normalize and filter\nsc.pp.normalize_total(adata, target_sum=1e4)\nsc.pp.log1p(adata)\nsc.pp.highly_variable_genes(adata, n_top_genes=2000)\nadata = adata[:, adata.var['highly_variable']]\n\n# 5. Save processed data\nadata.write_h5ad('processed.h5ad')\n```\n\n### Batch integration\n```python\n# Load multiple batches\nadata1 = ad.read_h5ad('batch1.h5ad')\nadata2 = ad.read_h5ad('batch2.h5ad')\nadata3 = ad.read_h5ad('batch3.h5ad')\n\n# Concatenate with batch labels\nadata = ad.concat(\n    [adata1, adata2, adata3],\n    label='batch',\n    keys=['batch1', 'batch2', 'batch3'],\n    join='inner'\n)\n\n# Apply batch correction\nimport scanpy as sc\nsc.pp.combat(adata, key='batch')\n\n# Continue analysis\nsc.pp.pca(adata)\nsc.pp.neighbors(adata)\nsc.tl.umap(adata)\n```\n\n### Working with large datasets\n```python\n# Open in backed mode\nadata = ad.read_h5ad('100GB_dataset.h5ad', backed='r')\n\n# Filter based on metadata (no data loading)\nhigh_quality = adata[adata.obs['quality_score'] > 0.8]\n\n# Load filtered subset\nadata_subset = high_quality.to_memory()\n\n# Process subset\nprocess(adata_subset)\n\n# Or process in chunks\nchunk_size = 1000\nfor i in range(0, adata.n_obs, chunk_size):\n    chunk = adata[i:i+chunk_size, :].to_memory()\n    process(chunk)\n```\n\n## Troubleshooting\n\n### Out of memory errors\nUse backed mode or convert to sparse matrices:\n```python\n# Backed mode\nadata = ad.read_h5ad('file.h5ad', backed='r')\n\n# Sparse matrices\nfrom scipy.sparse import csr_matrix\nadata.X = csr_matrix(adata.X)\n```\n\n### Slow file reading\nUse compression and appropriate formats:\n```python\n# Optimize for storage\nadata.strings_to_categoricals()\nadata.write_h5ad('file.h5ad', compression='gzip')\n\n# Use Zarr for cloud storage\nadata.write_zarr('file.zarr', chunks=(1000, 1000))\n```\n\n### Index alignment issues\nAlways align external data on index:\n```python\n# Wrong\nadata.obs['new_col'] = external_data['values']\n\n# Correct\nadata.obs['new_col'] = external_data.set_index('cell_id').loc[adata.obs_names, 'values']\n```\n\n## Additional Resources\n\n- **Official documentation**: https://anndata.readthedocs.io/\n- **Scanpy tutorials**: https://scanpy.readthedocs.io/\n- **Scverse ecosystem**: https://scverse.org/\n- **GitHub repository**: https://github.com/scverse/anndata\n",
        "data/k-dense-ai/arboreto/SKILL.md": "---\nname: arboreto\ndescription: Infer gene regulatory networks (GRNs) from gene expression data using scalable algorithms (GRNBoost2, GENIE3). Use when analyzing transcriptomics data (bulk RNA-seq, single-cell RNA-seq) to identify transcription factor-target gene relationships and regulatory interactions. Supports distributed computation for large-scale datasets.\n---\n\n# Arboreto\n\n## Overview\n\nArboreto is a computational library for inferring gene regulatory networks (GRNs) from gene expression data using parallelized algorithms that scale from single machines to multi-node clusters.\n\n**Core capability**: Identify which transcription factors (TFs) regulate which target genes based on expression patterns across observations (cells, samples, conditions).\n\n## Quick Start\n\nInstall arboreto:\n```bash\nuv pip install arboreto\n```\n\nBasic GRN inference:\n```python\nimport pandas as pd\nfrom arboreto.algo import grnboost2\n\nif __name__ == '__main__':\n    # Load expression data (genes as columns)\n    expression_matrix = pd.read_csv('expression_data.tsv', sep='\\t')\n\n    # Infer regulatory network\n    network = grnboost2(expression_data=expression_matrix)\n\n    # Save results (TF, target, importance)\n    network.to_csv('network.tsv', sep='\\t', index=False, header=False)\n```\n\n**Critical**: Always use `if __name__ == '__main__':` guard because Dask spawns new processes.\n\n## Core Capabilities\n\n### 1. Basic GRN Inference\n\nFor standard GRN inference workflows including:\n- Input data preparation (Pandas DataFrame or NumPy array)\n- Running inference with GRNBoost2 or GENIE3\n- Filtering by transcription factors\n- Output format and interpretation\n\n**See**: `references/basic_inference.md`\n\n**Use the ready-to-run script**: `scripts/basic_grn_inference.py` for standard inference tasks:\n```bash\npython scripts/basic_grn_inference.py expression_data.tsv output_network.tsv --tf-file tfs.txt --seed 777\n```\n\n### 2. Algorithm Selection\n\nArboreto provides two algorithms:\n\n**GRNBoost2 (Recommended)**:\n- Fast gradient boosting-based inference\n- Optimized for large datasets (10k+ observations)\n- Default choice for most analyses\n\n**GENIE3**:\n- Random Forest-based inference\n- Original multiple regression approach\n- Use for comparison or validation\n\nQuick comparison:\n```python\nfrom arboreto.algo import grnboost2, genie3\n\n# Fast, recommended\nnetwork_grnboost = grnboost2(expression_data=matrix)\n\n# Classic algorithm\nnetwork_genie3 = genie3(expression_data=matrix)\n```\n\n**For detailed algorithm comparison, parameters, and selection guidance**: `references/algorithms.md`\n\n### 3. Distributed Computing\n\nScale inference from local multi-core to cluster environments:\n\n**Local (default)** - Uses all available cores automatically:\n```python\nnetwork = grnboost2(expression_data=matrix)\n```\n\n**Custom local client** - Control resources:\n```python\nfrom distributed import LocalCluster, Client\n\nlocal_cluster = LocalCluster(n_workers=10, memory_limit='8GB')\nclient = Client(local_cluster)\n\nnetwork = grnboost2(expression_data=matrix, client_or_address=client)\n\nclient.close()\nlocal_cluster.close()\n```\n\n**Cluster computing** - Connect to remote Dask scheduler:\n```python\nfrom distributed import Client\n\nclient = Client('tcp://scheduler:8786')\nnetwork = grnboost2(expression_data=matrix, client_or_address=client)\n```\n\n**For cluster setup, performance optimization, and large-scale workflows**: `references/distributed_computing.md`\n\n## Installation\n\n```bash\nuv pip install arboreto\n```\n\n**Dependencies**: scipy, scikit-learn, numpy, pandas, dask, distributed\n\n## Common Use Cases\n\n### Single-Cell RNA-seq Analysis\n```python\nimport pandas as pd\nfrom arboreto.algo import grnboost2\n\nif __name__ == '__main__':\n    # Load single-cell expression matrix (cells x genes)\n    sc_data = pd.read_csv('scrna_counts.tsv', sep='\\t')\n\n    # Infer cell-type-specific regulatory network\n    network = grnboost2(expression_data=sc_data, seed=42)\n\n    # Filter high-confidence links\n    high_confidence = network[network['importance'] > 0.5]\n    high_confidence.to_csv('grn_high_confidence.tsv', sep='\\t', index=False)\n```\n\n### Bulk RNA-seq with TF Filtering\n```python\nfrom arboreto.utils import load_tf_names\nfrom arboreto.algo import grnboost2\n\nif __name__ == '__main__':\n    # Load data\n    expression_data = pd.read_csv('rnaseq_tpm.tsv', sep='\\t')\n    tf_names = load_tf_names('human_tfs.txt')\n\n    # Infer with TF restriction\n    network = grnboost2(\n        expression_data=expression_data,\n        tf_names=tf_names,\n        seed=123\n    )\n\n    network.to_csv('tf_target_network.tsv', sep='\\t', index=False)\n```\n\n### Comparative Analysis (Multiple Conditions)\n```python\nfrom arboreto.algo import grnboost2\n\nif __name__ == '__main__':\n    # Infer networks for different conditions\n    conditions = ['control', 'treatment_24h', 'treatment_48h']\n\n    for condition in conditions:\n        data = pd.read_csv(f'{condition}_expression.tsv', sep='\\t')\n        network = grnboost2(expression_data=data, seed=42)\n        network.to_csv(f'{condition}_network.tsv', sep='\\t', index=False)\n```\n\n## Output Interpretation\n\nArboreto returns a DataFrame with regulatory links:\n\n| Column | Description |\n|--------|-------------|\n| `TF` | Transcription factor (regulator) |\n| `target` | Target gene |\n| `importance` | Regulatory importance score (higher = stronger) |\n\n**Filtering strategy**:\n- Top N links per target gene\n- Importance threshold (e.g., > 0.5)\n- Statistical significance testing (permutation tests)\n\n## Integration with pySCENIC\n\nArboreto is a core component of the SCENIC pipeline for single-cell regulatory network analysis:\n\n```python\n# Step 1: Use arboreto for GRN inference\nfrom arboreto.algo import grnboost2\nnetwork = grnboost2(expression_data=sc_data, tf_names=tf_list)\n\n# Step 2: Use pySCENIC for regulon identification and activity scoring\n# (See pySCENIC documentation for downstream analysis)\n```\n\n## Reproducibility\n\nAlways set a seed for reproducible results:\n```python\nnetwork = grnboost2(expression_data=matrix, seed=777)\n```\n\nRun multiple seeds for robustness analysis:\n```python\nfrom distributed import LocalCluster, Client\n\nif __name__ == '__main__':\n    client = Client(LocalCluster())\n\n    seeds = [42, 123, 777]\n    networks = []\n\n    for seed in seeds:\n        net = grnboost2(expression_data=matrix, client_or_address=client, seed=seed)\n        networks.append(net)\n\n    # Combine networks and filter consensus links\n    consensus = analyze_consensus(networks)\n```\n\n## Troubleshooting\n\n**Memory errors**: Reduce dataset size by filtering low-variance genes or use distributed computing\n\n**Slow performance**: Use GRNBoost2 instead of GENIE3, enable distributed client, filter TF list\n\n**Dask errors**: Ensure `if __name__ == '__main__':` guard is present in scripts\n\n**Empty results**: Check data format (genes as columns), verify TF names match gene names\n",
        "data/k-dense-ai/astropy/SKILL.md": "---\nname: astropy\ndescription: Comprehensive Python library for astronomy and astrophysics. This skill should be used when working with astronomical data including celestial coordinates, physical units, FITS files, cosmological calculations, time systems, tables, world coordinate systems (WCS), and astronomical data analysis. Use when tasks involve coordinate transformations, unit conversions, FITS file manipulation, cosmological distance calculations, time scale conversions, or astronomical data processing.\n---\n\n# Astropy\n\n## Overview\n\nAstropy is the core Python package for astronomy, providing essential functionality for astronomical research and data analysis. Use astropy for coordinate transformations, unit and quantity calculations, FITS file operations, cosmological calculations, precise time handling, tabular data manipulation, and astronomical image processing.\n\n## When to Use This Skill\n\nUse astropy when tasks involve:\n- Converting between celestial coordinate systems (ICRS, Galactic, FK5, AltAz, etc.)\n- Working with physical units and quantities (converting Jy to mJy, parsecs to km, etc.)\n- Reading, writing, or manipulating FITS files (images or tables)\n- Cosmological calculations (luminosity distance, lookback time, Hubble parameter)\n- Precise time handling with different time scales (UTC, TAI, TT, TDB) and formats (JD, MJD, ISO)\n- Table operations (reading catalogs, cross-matching, filtering, joining)\n- WCS transformations between pixel and world coordinates\n- Astronomical constants and calculations\n\n## Quick Start\n\n```python\nimport astropy.units as u\nfrom astropy.coordinates import SkyCoord\nfrom astropy.time import Time\nfrom astropy.io import fits\nfrom astropy.table import Table\nfrom astropy.cosmology import Planck18\n\n# Units and quantities\ndistance = 100 * u.pc\ndistance_km = distance.to(u.km)\n\n# Coordinates\ncoord = SkyCoord(ra=10.5*u.degree, dec=41.2*u.degree, frame='icrs')\ncoord_galactic = coord.galactic\n\n# Time\nt = Time('2023-01-15 12:30:00')\njd = t.jd  # Julian Date\n\n# FITS files\ndata = fits.getdata('image.fits')\nheader = fits.getheader('image.fits')\n\n# Tables\ntable = Table.read('catalog.fits')\n\n# Cosmology\nd_L = Planck18.luminosity_distance(z=1.0)\n```\n\n## Core Capabilities\n\n### 1. Units and Quantities (`astropy.units`)\n\nHandle physical quantities with units, perform unit conversions, and ensure dimensional consistency in calculations.\n\n**Key operations:**\n- Create quantities by multiplying values with units\n- Convert between units using `.to()` method\n- Perform arithmetic with automatic unit handling\n- Use equivalencies for domain-specific conversions (spectral, doppler, parallax)\n- Work with logarithmic units (magnitudes, decibels)\n\n**See:** `references/units.md` for comprehensive documentation, unit systems, equivalencies, performance optimization, and unit arithmetic.\n\n### 2. Coordinate Systems (`astropy.coordinates`)\n\nRepresent celestial positions and transform between different coordinate frames.\n\n**Key operations:**\n- Create coordinates with `SkyCoord` in any frame (ICRS, Galactic, FK5, AltAz, etc.)\n- Transform between coordinate systems\n- Calculate angular separations and position angles\n- Match coordinates to catalogs\n- Include distance for 3D coordinate operations\n- Handle proper motions and radial velocities\n- Query named objects from online databases\n\n**See:** `references/coordinates.md` for detailed coordinate frame descriptions, transformations, observer-dependent frames (AltAz), catalog matching, and performance tips.\n\n### 3. Cosmological Calculations (`astropy.cosmology`)\n\nPerform cosmological calculations using standard cosmological models.\n\n**Key operations:**\n- Use built-in cosmologies (Planck18, WMAP9, etc.)\n- Create custom cosmological models\n- Calculate distances (luminosity, comoving, angular diameter)\n- Compute ages and lookback times\n- Determine Hubble parameter at any redshift\n- Calculate density parameters and volumes\n- Perform inverse calculations (find z for given distance)\n\n**See:** `references/cosmology.md` for available models, distance calculations, time calculations, density parameters, and neutrino effects.\n\n### 4. FITS File Handling (`astropy.io.fits`)\n\nRead, write, and manipulate FITS (Flexible Image Transport System) files.\n\n**Key operations:**\n- Open FITS files with context managers\n- Access HDUs (Header Data Units) by index or name\n- Read and modify headers (keywords, comments, history)\n- Work with image data (NumPy arrays)\n- Handle table data (binary and ASCII tables)\n- Create new FITS files (single or multi-extension)\n- Use memory mapping for large files\n- Access remote FITS files (S3, HTTP)\n\n**See:** `references/fits.md` for comprehensive file operations, header manipulation, image and table handling, multi-extension files, and performance considerations.\n\n### 5. Table Operations (`astropy.table`)\n\nWork with tabular data with support for units, metadata, and various file formats.\n\n**Key operations:**\n- Create tables from arrays, lists, or dictionaries\n- Read/write tables in multiple formats (FITS, CSV, HDF5, VOTable)\n- Access and modify columns and rows\n- Sort, filter, and index tables\n- Perform database-style operations (join, group, aggregate)\n- Stack and concatenate tables\n- Work with unit-aware columns (QTable)\n- Handle missing data with masking\n\n**See:** `references/tables.md` for table creation, I/O operations, data manipulation, sorting, filtering, joins, grouping, and performance tips.\n\n### 6. Time Handling (`astropy.time`)\n\nPrecise time representation and conversion between time scales and formats.\n\n**Key operations:**\n- Create Time objects in various formats (ISO, JD, MJD, Unix, etc.)\n- Convert between time scales (UTC, TAI, TT, TDB, etc.)\n- Perform time arithmetic with TimeDelta\n- Calculate sidereal time for observers\n- Compute light travel time corrections (barycentric, heliocentric)\n- Work with time arrays efficiently\n- Handle masked (missing) times\n\n**See:** `references/time.md` for time formats, time scales, conversions, arithmetic, observing features, and precision handling.\n\n### 7. World Coordinate System (`astropy.wcs`)\n\nTransform between pixel coordinates in images and world coordinates.\n\n**Key operations:**\n- Read WCS from FITS headers\n- Convert pixel coordinates to world coordinates (and vice versa)\n- Calculate image footprints\n- Access WCS parameters (reference pixel, projection, scale)\n- Create custom WCS objects\n\n**See:** `references/wcs_and_other_modules.md` for WCS operations and transformations.\n\n## Additional Capabilities\n\nThe `references/wcs_and_other_modules.md` file also covers:\n\n### NDData and CCDData\nContainers for n-dimensional datasets with metadata, uncertainty, masking, and WCS information.\n\n### Modeling\nFramework for creating and fitting mathematical models to astronomical data.\n\n### Visualization\nTools for astronomical image display with appropriate stretching and scaling.\n\n### Constants\nPhysical and astronomical constants with proper units (speed of light, solar mass, Planck constant, etc.).\n\n### Convolution\nImage processing kernels for smoothing and filtering.\n\n### Statistics\nRobust statistical functions including sigma clipping and outlier rejection.\n\n## Installation\n\n```bash\n# Install astropy\nuv pip install astropy\n\n# With optional dependencies for full functionality\nuv pip install astropy[all]\n```\n\n## Common Workflows\n\n### Converting Coordinates Between Systems\n\n```python\nfrom astropy.coordinates import SkyCoord\nimport astropy.units as u\n\n# Create coordinate\nc = SkyCoord(ra='05h23m34.5s', dec='-69d45m22s', frame='icrs')\n\n# Transform to galactic\nc_gal = c.galactic\nprint(f\"l={c_gal.l.deg}, b={c_gal.b.deg}\")\n\n# Transform to alt-az (requires time and location)\nfrom astropy.time import Time\nfrom astropy.coordinates import EarthLocation, AltAz\n\nobserving_time = Time('2023-06-15 23:00:00')\nobserving_location = EarthLocation(lat=40*u.deg, lon=-120*u.deg)\naa_frame = AltAz(obstime=observing_time, location=observing_location)\nc_altaz = c.transform_to(aa_frame)\nprint(f\"Alt={c_altaz.alt.deg}, Az={c_altaz.az.deg}\")\n```\n\n### Reading and Analyzing FITS Files\n\n```python\nfrom astropy.io import fits\nimport numpy as np\n\n# Open FITS file\nwith fits.open('observation.fits') as hdul:\n    # Display structure\n    hdul.info()\n\n    # Get image data and header\n    data = hdul[1].data\n    header = hdul[1].header\n\n    # Access header values\n    exptime = header['EXPTIME']\n    filter_name = header['FILTER']\n\n    # Analyze data\n    mean = np.mean(data)\n    median = np.median(data)\n    print(f\"Mean: {mean}, Median: {median}\")\n```\n\n### Cosmological Distance Calculations\n\n```python\nfrom astropy.cosmology import Planck18\nimport astropy.units as u\nimport numpy as np\n\n# Calculate distances at z=1.5\nz = 1.5\nd_L = Planck18.luminosity_distance(z)\nd_A = Planck18.angular_diameter_distance(z)\n\nprint(f\"Luminosity distance: {d_L}\")\nprint(f\"Angular diameter distance: {d_A}\")\n\n# Age of universe at that redshift\nage = Planck18.age(z)\nprint(f\"Age at z={z}: {age.to(u.Gyr)}\")\n\n# Lookback time\nt_lookback = Planck18.lookback_time(z)\nprint(f\"Lookback time: {t_lookback.to(u.Gyr)}\")\n```\n\n### Cross-Matching Catalogs\n\n```python\nfrom astropy.table import Table\nfrom astropy.coordinates import SkyCoord, match_coordinates_sky\nimport astropy.units as u\n\n# Read catalogs\ncat1 = Table.read('catalog1.fits')\ncat2 = Table.read('catalog2.fits')\n\n# Create coordinate objects\ncoords1 = SkyCoord(ra=cat1['RA']*u.degree, dec=cat1['DEC']*u.degree)\ncoords2 = SkyCoord(ra=cat2['RA']*u.degree, dec=cat2['DEC']*u.degree)\n\n# Find matches\nidx, sep, _ = coords1.match_to_catalog_sky(coords2)\n\n# Filter by separation threshold\nmax_sep = 1 * u.arcsec\nmatches = sep < max_sep\n\n# Create matched catalogs\ncat1_matched = cat1[matches]\ncat2_matched = cat2[idx[matches]]\nprint(f\"Found {len(cat1_matched)} matches\")\n```\n\n## Best Practices\n\n1. **Always use units**: Attach units to quantities to avoid errors and ensure dimensional consistency\n2. **Use context managers for FITS files**: Ensures proper file closing\n3. **Prefer arrays over loops**: Process multiple coordinates/times as arrays for better performance\n4. **Check coordinate frames**: Verify the frame before transformations\n5. **Use appropriate cosmology**: Choose the right cosmological model for your analysis\n6. **Handle missing data**: Use masked columns for tables with missing values\n7. **Specify time scales**: Be explicit about time scales (UTC, TT, TDB) for precise timing\n8. **Use QTable for unit-aware tables**: When table columns have units\n9. **Check WCS validity**: Verify WCS before using transformations\n10. **Cache frequently used values**: Expensive calculations (e.g., cosmological distances) can be cached\n\n## Documentation and Resources\n\n- Official Astropy Documentation: https://docs.astropy.org/en/stable/\n- Tutorials: https://learn.astropy.org/\n- GitHub: https://github.com/astropy/astropy\n\n## Reference Files\n\nFor detailed information on specific modules:\n- `references/units.md` - Units, quantities, conversions, and equivalencies\n- `references/coordinates.md` - Coordinate systems, transformations, and catalog matching\n- `references/cosmology.md` - Cosmological models and calculations\n- `references/fits.md` - FITS file operations and manipulation\n- `references/tables.md` - Table creation, I/O, and operations\n- `references/time.md` - Time formats, scales, and calculations\n- `references/wcs_and_other_modules.md` - WCS, NDData, modeling, visualization, constants, and utilities\n",
        "data/k-dense-ai/benchling-integration/SKILL.md": "---\nname: benchling-integration\ndescription: \"Benchling R&D platform integration. Access registry (DNA, proteins), inventory, ELN entries, workflows via API, build Benchling Apps, query Data Warehouse, for lab data management automation.\"\n---\n\n# Benchling Integration\n\n## Overview\n\nBenchling is a cloud platform for life sciences R&D. Access registry entities (DNA, proteins), inventory, electronic lab notebooks, and workflows programmatically via Python SDK and REST API.\n\n## When to Use This Skill\n\nThis skill should be used when:\n- Working with Benchling's Python SDK or REST API\n- Managing biological sequences (DNA, RNA, proteins) and registry entities\n- Automating inventory operations (samples, containers, locations, transfers)\n- Creating or querying electronic lab notebook entries\n- Building workflow automations or Benchling Apps\n- Syncing data between Benchling and external systems\n- Querying the Benchling Data Warehouse for analytics\n- Setting up event-driven integrations with AWS EventBridge\n\n## Core Capabilities\n\n### 1. Authentication & Setup\n\n**Python SDK Installation:**\n```python\n# Stable release\nuv pip install benchling-sdk\n# or with Poetry\npoetry add benchling-sdk\n```\n\n**Authentication Methods:**\n\nAPI Key Authentication (recommended for scripts):\n```python\nfrom benchling_sdk.benchling import Benchling\nfrom benchling_sdk.auth.api_key_auth import ApiKeyAuth\n\nbenchling = Benchling(\n    url=\"https://your-tenant.benchling.com\",\n    auth_method=ApiKeyAuth(\"your_api_key\")\n)\n```\n\nOAuth Client Credentials (for apps):\n```python\nfrom benchling_sdk.auth.client_credentials_oauth2 import ClientCredentialsOAuth2\n\nauth_method = ClientCredentialsOAuth2(\n    client_id=\"your_client_id\",\n    client_secret=\"your_client_secret\"\n)\nbenchling = Benchling(\n    url=\"https://your-tenant.benchling.com\",\n    auth_method=auth_method\n)\n```\n\n**Key Points:**\n- API keys are obtained from Profile Settings in Benchling\n- Store credentials securely (use environment variables or password managers)\n- All API requests require HTTPS\n- Authentication permissions mirror user permissions in the UI\n\nFor detailed authentication information including OIDC and security best practices, refer to `references/authentication.md`.\n\n### 2. Registry & Entity Management\n\nRegistry entities include DNA sequences, RNA sequences, AA sequences, custom entities, and mixtures. The SDK provides typed classes for creating and managing these entities.\n\n**Creating DNA Sequences:**\n```python\nfrom benchling_sdk.models import DnaSequenceCreate\n\nsequence = benchling.dna_sequences.create(\n    DnaSequenceCreate(\n        name=\"My Plasmid\",\n        bases=\"ATCGATCG\",\n        is_circular=True,\n        folder_id=\"fld_abc123\",\n        schema_id=\"ts_abc123\",  # optional\n        fields=benchling.models.fields({\"gene_name\": \"GFP\"})\n    )\n)\n```\n\n**Registry Registration:**\n\nTo register an entity directly upon creation:\n```python\nsequence = benchling.dna_sequences.create(\n    DnaSequenceCreate(\n        name=\"My Plasmid\",\n        bases=\"ATCGATCG\",\n        is_circular=True,\n        folder_id=\"fld_abc123\",\n        entity_registry_id=\"src_abc123\",  # Registry to register in\n        naming_strategy=\"NEW_IDS\"  # or \"IDS_FROM_NAMES\"\n    )\n)\n```\n\n**Important:** Use either `entity_registry_id` OR `naming_strategy`, never both.\n\n**Updating Entities:**\n```python\nfrom benchling_sdk.models import DnaSequenceUpdate\n\nupdated = benchling.dna_sequences.update(\n    sequence_id=\"seq_abc123\",\n    dna_sequence=DnaSequenceUpdate(\n        name=\"Updated Plasmid Name\",\n        fields=benchling.models.fields({\"gene_name\": \"mCherry\"})\n    )\n)\n```\n\nUnspecified fields remain unchanged, allowing partial updates.\n\n**Listing and Pagination:**\n```python\n# List all DNA sequences (returns a generator)\nsequences = benchling.dna_sequences.list()\nfor page in sequences:\n    for seq in page:\n        print(f\"{seq.name} ({seq.id})\")\n\n# Check total count\ntotal = sequences.estimated_count()\n```\n\n**Key Operations:**\n- Create: `benchling.<entity_type>.create()`\n- Read: `benchling.<entity_type>.get(id)` or `.list()`\n- Update: `benchling.<entity_type>.update(id, update_object)`\n- Archive: `benchling.<entity_type>.archive(id)`\n\nEntity types: `dna_sequences`, `rna_sequences`, `aa_sequences`, `custom_entities`, `mixtures`\n\nFor comprehensive SDK reference and advanced patterns, refer to `references/sdk_reference.md`.\n\n### 3. Inventory Management\n\nManage physical samples, containers, boxes, and locations within the Benchling inventory system.\n\n**Creating Containers:**\n```python\nfrom benchling_sdk.models import ContainerCreate\n\ncontainer = benchling.containers.create(\n    ContainerCreate(\n        name=\"Sample Tube 001\",\n        schema_id=\"cont_schema_abc123\",\n        parent_storage_id=\"box_abc123\",  # optional\n        fields=benchling.models.fields({\"concentration\": \"100 ng/L\"})\n    )\n)\n```\n\n**Managing Boxes:**\n```python\nfrom benchling_sdk.models import BoxCreate\n\nbox = benchling.boxes.create(\n    BoxCreate(\n        name=\"Freezer Box A1\",\n        schema_id=\"box_schema_abc123\",\n        parent_storage_id=\"loc_abc123\"\n    )\n)\n```\n\n**Transferring Items:**\n```python\n# Transfer a container to a new location\ntransfer = benchling.containers.transfer(\n    container_id=\"cont_abc123\",\n    destination_id=\"box_xyz789\"\n)\n```\n\n**Key Inventory Operations:**\n- Create containers, boxes, locations, plates\n- Update inventory item properties\n- Transfer items between locations\n- Check in/out items\n- Batch operations for bulk transfers\n\n### 4. Notebook & Documentation\n\nInteract with electronic lab notebook (ELN) entries, protocols, and templates.\n\n**Creating Notebook Entries:**\n```python\nfrom benchling_sdk.models import EntryCreate\n\nentry = benchling.entries.create(\n    EntryCreate(\n        name=\"Experiment 2025-10-20\",\n        folder_id=\"fld_abc123\",\n        schema_id=\"entry_schema_abc123\",\n        fields=benchling.models.fields({\"objective\": \"Test gene expression\"})\n    )\n)\n```\n\n**Linking Entities to Entries:**\n```python\n# Add references to entities in an entry\nentry_link = benchling.entry_links.create(\n    entry_id=\"entry_abc123\",\n    entity_id=\"seq_xyz789\"\n)\n```\n\n**Key Notebook Operations:**\n- Create and update lab notebook entries\n- Manage entry templates\n- Link entities and results to entries\n- Export entries for documentation\n\n### 5. Workflows & Automation\n\nAutomate laboratory processes using Benchling's workflow system.\n\n**Creating Workflow Tasks:**\n```python\nfrom benchling_sdk.models import WorkflowTaskCreate\n\ntask = benchling.workflow_tasks.create(\n    WorkflowTaskCreate(\n        name=\"PCR Amplification\",\n        workflow_id=\"wf_abc123\",\n        assignee_id=\"user_abc123\",\n        fields=benchling.models.fields({\"template\": \"seq_abc123\"})\n    )\n)\n```\n\n**Updating Task Status:**\n```python\nfrom benchling_sdk.models import WorkflowTaskUpdate\n\nupdated_task = benchling.workflow_tasks.update(\n    task_id=\"task_abc123\",\n    workflow_task=WorkflowTaskUpdate(\n        status_id=\"status_complete_abc123\"\n    )\n)\n```\n\n**Asynchronous Operations:**\n\nSome operations are asynchronous and return tasks:\n```python\n# Wait for task completion\nfrom benchling_sdk.helpers.tasks import wait_for_task\n\nresult = wait_for_task(\n    benchling,\n    task_id=\"task_abc123\",\n    interval_wait_seconds=2,\n    max_wait_seconds=300\n)\n```\n\n**Key Workflow Operations:**\n- Create and manage workflow tasks\n- Update task statuses and assignments\n- Execute bulk operations asynchronously\n- Monitor task progress\n\n### 6. Events & Integration\n\nSubscribe to Benchling events for real-time integrations using AWS EventBridge.\n\n**Event Types:**\n- Entity creation, update, archive\n- Inventory transfers\n- Workflow task status changes\n- Entry creation and updates\n- Results registration\n\n**Integration Pattern:**\n1. Configure event routing to AWS EventBridge in Benchling settings\n2. Create EventBridge rules to filter events\n3. Route events to Lambda functions or other targets\n4. Process events and update external systems\n\n**Use Cases:**\n- Sync Benchling data to external databases\n- Trigger downstream processes on workflow completion\n- Send notifications on entity changes\n- Audit trail logging\n\nRefer to Benchling's event documentation for event schemas and configuration.\n\n### 7. Data Warehouse & Analytics\n\nQuery historical Benchling data using SQL through the Data Warehouse.\n\n**Access Method:**\nThe Benchling Data Warehouse provides SQL access to Benchling data for analytics and reporting. Connect using standard SQL clients with provided credentials.\n\n**Common Queries:**\n- Aggregate experimental results\n- Analyze inventory trends\n- Generate compliance reports\n- Export data for external analysis\n\n**Integration with Analysis Tools:**\n- Jupyter notebooks for interactive analysis\n- BI tools (Tableau, Looker, PowerBI)\n- Custom dashboards\n\n## Best Practices\n\n### Error Handling\n\nThe SDK automatically retries failed requests:\n```python\n# Automatic retry for 429, 502, 503, 504 status codes\n# Up to 5 retries with exponential backoff\n# Customize retry behavior if needed\nfrom benchling_sdk.retry import RetryStrategy\n\nbenchling = Benchling(\n    url=\"https://your-tenant.benchling.com\",\n    auth_method=ApiKeyAuth(\"your_api_key\"),\n    retry_strategy=RetryStrategy(max_retries=3)\n)\n```\n\n### Pagination Efficiency\n\nUse generators for memory-efficient pagination:\n```python\n# Generator-based iteration\nfor page in benchling.dna_sequences.list():\n    for sequence in page:\n        process(sequence)\n\n# Check estimated count without loading all pages\ntotal = benchling.dna_sequences.list().estimated_count()\n```\n\n### Schema Fields Helper\n\nUse the `fields()` helper for custom schema fields:\n```python\n# Convert dict to Fields object\ncustom_fields = benchling.models.fields({\n    \"concentration\": \"100 ng/L\",\n    \"date_prepared\": \"2025-10-20\",\n    \"notes\": \"High quality prep\"\n})\n```\n\n### Forward Compatibility\n\nThe SDK handles unknown enum values and types gracefully:\n- Unknown enum values are preserved\n- Unrecognized polymorphic types return `UnknownType`\n- Allows working with newer API versions\n\n### Security Considerations\n\n- Never commit API keys to version control\n- Use environment variables for credentials\n- Rotate keys if compromised\n- Grant minimal necessary permissions for apps\n- Use OAuth for multi-user scenarios\n\n## Resources\n\n### references/\n\nDetailed reference documentation for in-depth information:\n\n- **authentication.md** - Comprehensive authentication guide including OIDC, security best practices, and credential management\n- **sdk_reference.md** - Detailed Python SDK reference with advanced patterns, examples, and all entity types\n- **api_endpoints.md** - REST API endpoint reference for direct HTTP calls without the SDK\n\nLoad these references as needed for specific integration requirements.\n\n### scripts/\n\nThis skill currently includes example scripts that can be removed or replaced with custom automation scripts for your specific Benchling workflows.\n\n## Common Use Cases\n\n**1. Bulk Entity Import:**\n```python\n# Import multiple sequences from FASTA file\nfrom Bio import SeqIO\n\nfor record in SeqIO.parse(\"sequences.fasta\", \"fasta\"):\n    benchling.dna_sequences.create(\n        DnaSequenceCreate(\n            name=record.id,\n            bases=str(record.seq),\n            is_circular=False,\n            folder_id=\"fld_abc123\"\n        )\n    )\n```\n\n**2. Inventory Audit:**\n```python\n# List all containers in a specific location\ncontainers = benchling.containers.list(\n    parent_storage_id=\"box_abc123\"\n)\n\nfor page in containers:\n    for container in page:\n        print(f\"{container.name}: {container.barcode}\")\n```\n\n**3. Workflow Automation:**\n```python\n# Update all pending tasks for a workflow\ntasks = benchling.workflow_tasks.list(\n    workflow_id=\"wf_abc123\",\n    status=\"pending\"\n)\n\nfor page in tasks:\n    for task in page:\n        # Perform automated checks\n        if auto_validate(task):\n            benchling.workflow_tasks.update(\n                task_id=task.id,\n                workflow_task=WorkflowTaskUpdate(\n                    status_id=\"status_complete\"\n                )\n            )\n```\n\n**4. Data Export:**\n```python\n# Export all sequences with specific properties\nsequences = benchling.dna_sequences.list()\nexport_data = []\n\nfor page in sequences:\n    for seq in page:\n        if seq.schema_id == \"target_schema_id\":\n            export_data.append({\n                \"id\": seq.id,\n                \"name\": seq.name,\n                \"bases\": seq.bases,\n                \"length\": len(seq.bases)\n            })\n\n# Save to CSV or database\nimport csv\nwith open(\"sequences.csv\", \"w\") as f:\n    writer = csv.DictWriter(f, fieldnames=export_data[0].keys())\n    writer.writeheader()\n    writer.writerows(export_data)\n```\n\n## Additional Resources\n\n- **Official Documentation:** https://docs.benchling.com\n- **Python SDK Reference:** https://benchling.com/sdk-docs/\n- **API Reference:** https://benchling.com/api/reference\n- **Support:** [email protected]\n",
        "data/k-dense-ai/biomni/SKILL.md": "---\nname: biomni\ndescription: Autonomous biomedical AI agent framework for executing complex research tasks across genomics, drug discovery, molecular biology, and clinical analysis. Use this skill when conducting multi-step biomedical research including CRISPR screening design, single-cell RNA-seq analysis, ADMET prediction, GWAS interpretation, rare disease diagnosis, or lab protocol optimization. Leverages LLM reasoning with code execution and integrated biomedical databases.\n---\n\n# Biomni\n\n## Overview\n\nBiomni is an open-source biomedical AI agent framework from Stanford's SNAP lab that autonomously executes complex research tasks across biomedical domains. Use this skill when working on multi-step biological reasoning tasks, analyzing biomedical data, or conducting research spanning genomics, drug discovery, molecular biology, and clinical analysis.\n\n## Core Capabilities\n\nBiomni excels at:\n\n1. **Multi-step biological reasoning** - Autonomous task decomposition and planning for complex biomedical queries\n2. **Code generation and execution** - Dynamic analysis pipeline creation for data processing\n3. **Knowledge retrieval** - Access to ~11GB of integrated biomedical databases and literature\n4. **Cross-domain problem solving** - Unified interface for genomics, proteomics, drug discovery, and clinical tasks\n\n## When to Use This Skill\n\nUse biomni for:\n- **CRISPR screening** - Design screens, prioritize genes, analyze knockout effects\n- **Single-cell RNA-seq** - Cell type annotation, differential expression, trajectory analysis\n- **Drug discovery** - ADMET prediction, target identification, compound optimization\n- **GWAS analysis** - Variant interpretation, causal gene identification, pathway enrichment\n- **Clinical genomics** - Rare disease diagnosis, variant pathogenicity, phenotype-genotype mapping\n- **Lab protocols** - Protocol optimization, literature synthesis, experimental design\n\n## Quick Start\n\n### Installation and Setup\n\nInstall Biomni and configure API keys for LLM providers:\n\n```bash\nuv pip install biomni --upgrade\n```\n\nConfigure API keys (store in `.env` file or environment variables):\n```bash\nexport ANTHROPIC_API_KEY=\"your-key-here\"\n# Optional: OpenAI, Azure, Google, Groq, AWS Bedrock keys\n```\n\nUse `scripts/setup_environment.py` for interactive setup assistance.\n\n### Basic Usage Pattern\n\n```python\nfrom biomni.agent import A1\n\n# Initialize agent with data path and LLM choice\nagent = A1(path='./data', llm='claude-sonnet-4-20250514')\n\n# Execute biomedical task autonomously\nagent.go(\"Your biomedical research question or task\")\n\n# Save conversation history and results\nagent.save_conversation_history(\"report.pdf\")\n```\n\n## Working with Biomni\n\n### 1. Agent Initialization\n\nThe A1 class is the primary interface for biomni:\n\n```python\nfrom biomni.agent import A1\nfrom biomni.config import default_config\n\n# Basic initialization\nagent = A1(\n    path='./data',  # Path to data lake (~11GB downloaded on first use)\n    llm='claude-sonnet-4-20250514'  # LLM model selection\n)\n\n# Advanced configuration\ndefault_config.llm = \"gpt-4\"\ndefault_config.timeout_seconds = 1200\ndefault_config.max_iterations = 50\n```\n\n**Supported LLM Providers:**\n- Anthropic Claude (recommended): `claude-sonnet-4-20250514`, `claude-opus-4-20250514`\n- OpenAI: `gpt-4`, `gpt-4-turbo`\n- Azure OpenAI: via Azure configuration\n- Google Gemini: `gemini-2.0-flash-exp`\n- Groq: `llama-3.3-70b-versatile`\n- AWS Bedrock: Various models via Bedrock API\n\nSee `references/llm_providers.md` for detailed LLM configuration instructions.\n\n### 2. Task Execution Workflow\n\nBiomni follows an autonomous agent workflow:\n\n```python\n# Step 1: Initialize agent\nagent = A1(path='./data', llm='claude-sonnet-4-20250514')\n\n# Step 2: Execute task with natural language query\nresult = agent.go(\"\"\"\nDesign a CRISPR screen to identify genes regulating autophagy in\nHEK293 cells. Prioritize genes based on essentiality and pathway\nrelevance.\n\"\"\")\n\n# Step 3: Review generated code and analysis\n# Agent autonomously:\n# - Decomposes task into sub-steps\n# - Retrieves relevant biological knowledge\n# - Generates and executes analysis code\n# - Interprets results and provides insights\n\n# Step 4: Save results\nagent.save_conversation_history(\"autophagy_screen_report.pdf\")\n```\n\n### 3. Common Task Patterns\n\n#### CRISPR Screening Design\n```python\nagent.go(\"\"\"\nDesign a genome-wide CRISPR knockout screen for identifying genes\naffecting [phenotype] in [cell type]. Include:\n1. sgRNA library design\n2. Gene prioritization criteria\n3. Expected hit genes based on pathway analysis\n\"\"\")\n```\n\n#### Single-Cell RNA-seq Analysis\n```python\nagent.go(\"\"\"\nAnalyze this single-cell RNA-seq dataset:\n- Perform quality control and filtering\n- Identify cell populations via clustering\n- Annotate cell types using marker genes\n- Conduct differential expression between conditions\nFile path: [path/to/data.h5ad]\n\"\"\")\n```\n\n#### Drug ADMET Prediction\n```python\nagent.go(\"\"\"\nPredict ADMET properties for these drug candidates:\n[SMILES strings or compound IDs]\nFocus on:\n- Absorption (Caco-2 permeability, HIA)\n- Distribution (plasma protein binding, BBB penetration)\n- Metabolism (CYP450 interaction)\n- Excretion (clearance)\n- Toxicity (hERG liability, hepatotoxicity)\n\"\"\")\n```\n\n#### GWAS Variant Interpretation\n```python\nagent.go(\"\"\"\nInterpret GWAS results for [trait/disease]:\n- Identify genome-wide significant variants\n- Map variants to causal genes\n- Perform pathway enrichment analysis\n- Predict functional consequences\nSummary statistics file: [path/to/gwas_summary.txt]\n\"\"\")\n```\n\nSee `references/use_cases.md` for comprehensive task examples across all biomedical domains.\n\n### 4. Data Integration\n\nBiomni integrates ~11GB of biomedical knowledge sources:\n- **Gene databases** - Ensembl, NCBI Gene, UniProt\n- **Protein structures** - PDB, AlphaFold\n- **Clinical datasets** - ClinVar, OMIM, HPO\n- **Literature indices** - PubMed abstracts, biomedical ontologies\n- **Pathway databases** - KEGG, Reactome, GO\n\nData is automatically downloaded to the specified `path` on first use.\n\n### 5. MCP Server Integration\n\nExtend biomni with external tools via Model Context Protocol:\n\n```python\n# MCP servers can provide:\n# - FDA drug databases\n# - Web search for literature\n# - Custom biomedical APIs\n# - Laboratory equipment interfaces\n\n# Configure MCP servers in .biomni/mcp_config.json\n```\n\n### 6. Evaluation Framework\n\nBenchmark agent performance on biomedical tasks:\n\n```python\nfrom biomni.eval import BiomniEval1\n\nevaluator = BiomniEval1()\n\n# Evaluate on specific task types\nscore = evaluator.evaluate(\n    task_type='crispr_design',\n    instance_id='test_001',\n    answer=agent_output\n)\n\n# Access evaluation dataset\ndataset = evaluator.load_dataset()\n```\n\n## Best Practices\n\n### Task Formulation\n- **Be specific** - Include biological context, organism, cell type, conditions\n- **Specify outputs** - Clearly state desired analysis outputs and formats\n- **Provide data paths** - Include file paths for datasets to analyze\n- **Set constraints** - Mention time/computational limits if relevant\n\n### Security Considerations\n **Important**: Biomni executes LLM-generated code with full system privileges. For production use:\n- Run in isolated environments (Docker, VMs)\n- Avoid exposing sensitive credentials\n- Review generated code before execution in sensitive contexts\n- Use sandboxed execution environments when possible\n\n### Performance Optimization\n- **Choose appropriate LLMs** - Claude Sonnet 4 recommended for balance of speed/quality\n- **Set reasonable timeouts** - Adjust `default_config.timeout_seconds` for complex tasks\n- **Monitor iterations** - Track `max_iterations` to prevent runaway loops\n- **Cache data** - Reuse downloaded data lake across sessions\n\n### Result Documentation\n```python\n# Always save conversation history for reproducibility\nagent.save_conversation_history(\"results/project_name_YYYYMMDD.pdf\")\n\n# Include in reports:\n# - Original task description\n# - Generated analysis code\n# - Results and interpretations\n# - Data sources used\n```\n\n## Resources\n\n### References\nDetailed documentation available in the `references/` directory:\n\n- **`api_reference.md`** - Complete API documentation for A1 class, configuration, and evaluation\n- **`llm_providers.md`** - LLM provider setup (Anthropic, OpenAI, Azure, Google, Groq, AWS)\n- **`use_cases.md`** - Comprehensive task examples for all biomedical domains\n\n### Scripts\nHelper scripts in the `scripts/` directory:\n\n- **`setup_environment.py`** - Interactive environment and API key configuration\n- **`generate_report.py`** - Enhanced PDF report generation with custom formatting\n\n### External Resources\n- **GitHub**: https://github.com/snap-stanford/biomni\n- **Web Platform**: https://biomni.stanford.edu\n- **Paper**: https://www.biorxiv.org/content/10.1101/2025.05.30.656746v1\n- **Model**: https://huggingface.co/biomni/Biomni-R0-32B-Preview\n- **Evaluation Dataset**: https://huggingface.co/datasets/biomni/Eval1\n\n## Troubleshooting\n\n### Common Issues\n\n**Data download fails**\n```python\n# Manually trigger data lake download\nagent = A1(path='./data', llm='your-llm')\n# First .go() call will download data\n```\n\n**API key errors**\n```bash\n# Verify environment variables\necho $ANTHROPIC_API_KEY\n# Or check .env file in working directory\n```\n\n**Timeout on complex tasks**\n```python\nfrom biomni.config import default_config\ndefault_config.timeout_seconds = 3600  # 1 hour\n```\n\n**Memory issues with large datasets**\n- Use streaming for large files\n- Process data in chunks\n- Increase system memory allocation\n\n### Getting Help\n\nFor issues or questions:\n- GitHub Issues: https://github.com/snap-stanford/biomni/issues\n- Documentation: Check `references/` files for detailed guidance\n- Community: Stanford SNAP lab and biomni contributors\n",
        "data/k-dense-ai/biopython/SKILL.md": "---\nname: biopython\ndescription: \"Primary Python toolkit for molecular biology. Preferred for Python-based PubMed/NCBI queries (Bio.Entrez), sequence manipulation, file parsing (FASTA, GenBank, FASTQ, PDB), advanced BLAST workflows, structures, phylogenetics. For quick BLAST, use gget. For direct REST API, use pubmed-database.\"\n---\n\n# Biopython: Computational Molecular Biology in Python\n\n## Overview\n\nBiopython is a comprehensive set of freely available Python tools for biological computation. It provides functionality for sequence manipulation, file I/O, database access, structural bioinformatics, phylogenetics, and many other bioinformatics tasks. The current version is **Biopython 1.85** (released January 2025), which supports Python 3 and requires NumPy.\n\n## When to Use This Skill\n\nUse this skill when:\n\n- Working with biological sequences (DNA, RNA, or protein)\n- Reading, writing, or converting biological file formats (FASTA, GenBank, FASTQ, PDB, mmCIF, etc.)\n- Accessing NCBI databases (GenBank, PubMed, Protein, Gene, etc.) via Entrez\n- Running BLAST searches or parsing BLAST results\n- Performing sequence alignments (pairwise or multiple sequence alignments)\n- Analyzing protein structures from PDB files\n- Creating, manipulating, or visualizing phylogenetic trees\n- Finding sequence motifs or analyzing motif patterns\n- Calculating sequence statistics (GC content, molecular weight, melting temperature, etc.)\n- Performing structural bioinformatics tasks\n- Working with population genetics data\n- Any other computational molecular biology task\n\n## Core Capabilities\n\nBiopython is organized into modular sub-packages, each addressing specific bioinformatics domains:\n\n1. **Sequence Handling** - Bio.Seq and Bio.SeqIO for sequence manipulation and file I/O\n2. **Alignment Analysis** - Bio.Align and Bio.AlignIO for pairwise and multiple sequence alignments\n3. **Database Access** - Bio.Entrez for programmatic access to NCBI databases\n4. **BLAST Operations** - Bio.Blast for running and parsing BLAST searches\n5. **Structural Bioinformatics** - Bio.PDB for working with 3D protein structures\n6. **Phylogenetics** - Bio.Phylo for phylogenetic tree manipulation and visualization\n7. **Advanced Features** - Motifs, population genetics, sequence utilities, and more\n\n## Installation and Setup\n\nInstall Biopython using pip (requires Python 3 and NumPy):\n\n```python\nuv pip install biopython\n```\n\nFor NCBI database access, always set your email address (required by NCBI):\n\n```python\nfrom Bio import Entrez\nEntrez.email = \"your.email@example.com\"\n\n# Optional: API key for higher rate limits (10 req/s instead of 3 req/s)\nEntrez.api_key = \"your_api_key_here\"\n```\n\n## Using This Skill\n\nThis skill provides comprehensive documentation organized by functionality area. When working on a task, consult the relevant reference documentation:\n\n### 1. Sequence Handling (Bio.Seq & Bio.SeqIO)\n\n**Reference:** `references/sequence_io.md`\n\nUse for:\n- Creating and manipulating biological sequences\n- Reading and writing sequence files (FASTA, GenBank, FASTQ, etc.)\n- Converting between file formats\n- Extracting sequences from large files\n- Sequence translation, transcription, and reverse complement\n- Working with SeqRecord objects\n\n**Quick example:**\n```python\nfrom Bio import SeqIO\n\n# Read sequences from FASTA file\nfor record in SeqIO.parse(\"sequences.fasta\", \"fasta\"):\n    print(f\"{record.id}: {len(record.seq)} bp\")\n\n# Convert GenBank to FASTA\nSeqIO.convert(\"input.gb\", \"genbank\", \"output.fasta\", \"fasta\")\n```\n\n### 2. Alignment Analysis (Bio.Align & Bio.AlignIO)\n\n**Reference:** `references/alignment.md`\n\nUse for:\n- Pairwise sequence alignment (global and local)\n- Reading and writing multiple sequence alignments\n- Using substitution matrices (BLOSUM, PAM)\n- Calculating alignment statistics\n- Customizing alignment parameters\n\n**Quick example:**\n```python\nfrom Bio import Align\n\n# Pairwise alignment\naligner = Align.PairwiseAligner()\naligner.mode = 'global'\nalignments = aligner.align(\"ACCGGT\", \"ACGGT\")\nprint(alignments[0])\n```\n\n### 3. Database Access (Bio.Entrez)\n\n**Reference:** `references/databases.md`\n\nUse for:\n- Searching NCBI databases (PubMed, GenBank, Protein, Gene, etc.)\n- Downloading sequences and records\n- Fetching publication information\n- Finding related records across databases\n- Batch downloading with proper rate limiting\n\n**Quick example:**\n```python\nfrom Bio import Entrez\nEntrez.email = \"your.email@example.com\"\n\n# Search PubMed\nhandle = Entrez.esearch(db=\"pubmed\", term=\"biopython\", retmax=10)\nresults = Entrez.read(handle)\nhandle.close()\nprint(f\"Found {results['Count']} results\")\n```\n\n### 4. BLAST Operations (Bio.Blast)\n\n**Reference:** `references/blast.md`\n\nUse for:\n- Running BLAST searches via NCBI web services\n- Running local BLAST searches\n- Parsing BLAST XML output\n- Filtering results by E-value or identity\n- Extracting hit sequences\n\n**Quick example:**\n```python\nfrom Bio.Blast import NCBIWWW, NCBIXML\n\n# Run BLAST search\nresult_handle = NCBIWWW.qblast(\"blastn\", \"nt\", \"ATCGATCGATCG\")\nblast_record = NCBIXML.read(result_handle)\n\n# Display top hits\nfor alignment in blast_record.alignments[:5]:\n    print(f\"{alignment.title}: E-value={alignment.hsps[0].expect}\")\n```\n\n### 5. Structural Bioinformatics (Bio.PDB)\n\n**Reference:** `references/structure.md`\n\nUse for:\n- Parsing PDB and mmCIF structure files\n- Navigating protein structure hierarchy (SMCRA: Structure/Model/Chain/Residue/Atom)\n- Calculating distances, angles, and dihedrals\n- Secondary structure assignment (DSSP)\n- Structure superimposition and RMSD calculation\n- Extracting sequences from structures\n\n**Quick example:**\n```python\nfrom Bio.PDB import PDBParser\n\n# Parse structure\nparser = PDBParser(QUIET=True)\nstructure = parser.get_structure(\"1crn\", \"1crn.pdb\")\n\n# Calculate distance between alpha carbons\nchain = structure[0][\"A\"]\ndistance = chain[10][\"CA\"] - chain[20][\"CA\"]\nprint(f\"Distance: {distance:.2f} \")\n```\n\n### 6. Phylogenetics (Bio.Phylo)\n\n**Reference:** `references/phylogenetics.md`\n\nUse for:\n- Reading and writing phylogenetic trees (Newick, NEXUS, phyloXML)\n- Building trees from distance matrices or alignments\n- Tree manipulation (pruning, rerooting, ladderizing)\n- Calculating phylogenetic distances\n- Creating consensus trees\n- Visualizing trees\n\n**Quick example:**\n```python\nfrom Bio import Phylo\n\n# Read and visualize tree\ntree = Phylo.read(\"tree.nwk\", \"newick\")\nPhylo.draw_ascii(tree)\n\n# Calculate distance\ndistance = tree.distance(\"Species_A\", \"Species_B\")\nprint(f\"Distance: {distance:.3f}\")\n```\n\n### 7. Advanced Features\n\n**Reference:** `references/advanced.md`\n\nUse for:\n- **Sequence motifs** (Bio.motifs) - Finding and analyzing motif patterns\n- **Population genetics** (Bio.PopGen) - GenePop files, Fst calculations, Hardy-Weinberg tests\n- **Sequence utilities** (Bio.SeqUtils) - GC content, melting temperature, molecular weight, protein analysis\n- **Restriction analysis** (Bio.Restriction) - Finding restriction enzyme sites\n- **Clustering** (Bio.Cluster) - K-means and hierarchical clustering\n- **Genome diagrams** (GenomeDiagram) - Visualizing genomic features\n\n**Quick example:**\n```python\nfrom Bio.SeqUtils import gc_fraction, molecular_weight\nfrom Bio.Seq import Seq\n\nseq = Seq(\"ATCGATCGATCG\")\nprint(f\"GC content: {gc_fraction(seq):.2%}\")\nprint(f\"Molecular weight: {molecular_weight(seq, seq_type='DNA'):.2f} g/mol\")\n```\n\n## General Workflow Guidelines\n\n### Reading Documentation\n\nWhen a user asks about a specific Biopython task:\n\n1. **Identify the relevant module** based on the task description\n2. **Read the appropriate reference file** using the Read tool\n3. **Extract relevant code patterns** and adapt them to the user's specific needs\n4. **Combine multiple modules** when the task requires it\n\nExample search patterns for reference files:\n```bash\n# Find information about specific functions\ngrep -n \"SeqIO.parse\" references/sequence_io.md\n\n# Find examples of specific tasks\ngrep -n \"BLAST\" references/blast.md\n\n# Find information about specific concepts\ngrep -n \"alignment\" references/alignment.md\n```\n\n### Writing Biopython Code\n\nFollow these principles when writing Biopython code:\n\n1. **Import modules explicitly**\n   ```python\n   from Bio import SeqIO, Entrez\n   from Bio.Seq import Seq\n   ```\n\n2. **Set Entrez email** when using NCBI databases\n   ```python\n   Entrez.email = \"your.email@example.com\"\n   ```\n\n3. **Use appropriate file formats** - Check which format best suits the task\n   ```python\n   # Common formats: \"fasta\", \"genbank\", \"fastq\", \"clustal\", \"phylip\"\n   ```\n\n4. **Handle files properly** - Close handles after use or use context managers\n   ```python\n   with open(\"file.fasta\") as handle:\n       records = SeqIO.parse(handle, \"fasta\")\n   ```\n\n5. **Use iterators for large files** - Avoid loading everything into memory\n   ```python\n   for record in SeqIO.parse(\"large_file.fasta\", \"fasta\"):\n       # Process one record at a time\n   ```\n\n6. **Handle errors gracefully** - Network operations and file parsing can fail\n   ```python\n   try:\n       handle = Entrez.efetch(db=\"nucleotide\", id=accession)\n   except HTTPError as e:\n       print(f\"Error: {e}\")\n   ```\n\n## Common Patterns\n\n### Pattern 1: Fetch Sequence from GenBank\n\n```python\nfrom Bio import Entrez, SeqIO\n\nEntrez.email = \"your.email@example.com\"\n\n# Fetch sequence\nhandle = Entrez.efetch(db=\"nucleotide\", id=\"EU490707\", rettype=\"gb\", retmode=\"text\")\nrecord = SeqIO.read(handle, \"genbank\")\nhandle.close()\n\nprint(f\"Description: {record.description}\")\nprint(f\"Sequence length: {len(record.seq)}\")\n```\n\n### Pattern 2: Sequence Analysis Pipeline\n\n```python\nfrom Bio import SeqIO\nfrom Bio.SeqUtils import gc_fraction\n\nfor record in SeqIO.parse(\"sequences.fasta\", \"fasta\"):\n    # Calculate statistics\n    gc = gc_fraction(record.seq)\n    length = len(record.seq)\n\n    # Find ORFs, translate, etc.\n    protein = record.seq.translate()\n\n    print(f\"{record.id}: {length} bp, GC={gc:.2%}\")\n```\n\n### Pattern 3: BLAST and Fetch Top Hits\n\n```python\nfrom Bio.Blast import NCBIWWW, NCBIXML\nfrom Bio import Entrez, SeqIO\n\nEntrez.email = \"your.email@example.com\"\n\n# Run BLAST\nresult_handle = NCBIWWW.qblast(\"blastn\", \"nt\", sequence)\nblast_record = NCBIXML.read(result_handle)\n\n# Get top hit accessions\naccessions = [aln.accession for aln in blast_record.alignments[:5]]\n\n# Fetch sequences\nfor acc in accessions:\n    handle = Entrez.efetch(db=\"nucleotide\", id=acc, rettype=\"fasta\", retmode=\"text\")\n    record = SeqIO.read(handle, \"fasta\")\n    handle.close()\n    print(f\">{record.description}\")\n```\n\n### Pattern 4: Build Phylogenetic Tree from Sequences\n\n```python\nfrom Bio import AlignIO, Phylo\nfrom Bio.Phylo.TreeConstruction import DistanceCalculator, DistanceTreeConstructor\n\n# Read alignment\nalignment = AlignIO.read(\"alignment.fasta\", \"fasta\")\n\n# Calculate distances\ncalculator = DistanceCalculator(\"identity\")\ndm = calculator.get_distance(alignment)\n\n# Build tree\nconstructor = DistanceTreeConstructor()\ntree = constructor.nj(dm)\n\n# Visualize\nPhylo.draw_ascii(tree)\n```\n\n## Best Practices\n\n1. **Always read relevant reference documentation** before writing code\n2. **Use grep to search reference files** for specific functions or examples\n3. **Validate file formats** before parsing\n4. **Handle missing data gracefully** - Not all records have all fields\n5. **Cache downloaded data** - Don't repeatedly download the same sequences\n6. **Respect NCBI rate limits** - Use API keys and proper delays\n7. **Test with small datasets** before processing large files\n8. **Keep Biopython updated** to get latest features and bug fixes\n9. **Use appropriate genetic code tables** for translation\n10. **Document analysis parameters** for reproducibility\n\n## Troubleshooting Common Issues\n\n### Issue: \"No handlers could be found for logger 'Bio.Entrez'\"\n**Solution:** This is just a warning. Set Entrez.email to suppress it.\n\n### Issue: \"HTTP Error 400\" from NCBI\n**Solution:** Check that IDs/accessions are valid and properly formatted.\n\n### Issue: \"ValueError: EOF\" when parsing files\n**Solution:** Verify file format matches the specified format string.\n\n### Issue: Alignment fails with \"sequences are not the same length\"\n**Solution:** Ensure sequences are aligned before using AlignIO or MultipleSeqAlignment.\n\n### Issue: BLAST searches are slow\n**Solution:** Use local BLAST for large-scale searches, or cache results.\n\n### Issue: PDB parser warnings\n**Solution:** Use `PDBParser(QUIET=True)` to suppress warnings, or investigate structure quality.\n\n## Additional Resources\n\n- **Official Documentation**: https://biopython.org/docs/latest/\n- **Tutorial**: https://biopython.org/docs/latest/Tutorial/\n- **Cookbook**: https://biopython.org/docs/latest/Tutorial/ (advanced examples)\n- **GitHub**: https://github.com/biopython/biopython\n- **Mailing List**: biopython@biopython.org\n\n## Quick Reference\n\nTo locate information in reference files, use these search patterns:\n\n```bash\n# Search for specific functions\ngrep -n \"function_name\" references/*.md\n\n# Find examples of specific tasks\ngrep -n \"example\" references/sequence_io.md\n\n# Find all occurrences of a module\ngrep -n \"Bio.Seq\" references/*.md\n```\n\n## Summary\n\nBiopython provides comprehensive tools for computational molecular biology. When using this skill:\n\n1. **Identify the task domain** (sequences, alignments, databases, BLAST, structures, phylogenetics, or advanced)\n2. **Consult the appropriate reference file** in the `references/` directory\n3. **Adapt code examples** to the specific use case\n4. **Combine multiple modules** when needed for complex workflows\n5. **Follow best practices** for file handling, error checking, and data management\n\nThe modular reference documentation ensures detailed, searchable information for every major Biopython capability.\n",
        "data/k-dense-ai/biorxiv-database/SKILL.md": "---\nname: biorxiv-database\ndescription: Efficient database search tool for bioRxiv preprint server. Use this skill when searching for life sciences preprints by keywords, authors, date ranges, or categories, retrieving paper metadata, downloading PDFs, or conducting literature reviews.\n---\n\n# bioRxiv Database\n\n## Overview\n\nThis skill provides efficient Python-based tools for searching and retrieving preprints from the bioRxiv database. It enables comprehensive searches by keywords, authors, date ranges, and categories, returning structured JSON metadata that includes titles, abstracts, DOIs, and citation information. The skill also supports PDF downloads for full-text analysis.\n\n## When to Use This Skill\n\nUse this skill when:\n- Searching for recent preprints in specific research areas\n- Tracking publications by particular authors\n- Conducting systematic literature reviews\n- Analyzing research trends over time periods\n- Retrieving metadata for citation management\n- Downloading preprint PDFs for analysis\n- Filtering papers by bioRxiv subject categories\n\n## Core Search Capabilities\n\n### 1. Keyword Search\n\nSearch for preprints containing specific keywords in titles, abstracts, or author lists.\n\n**Basic Usage:**\n```python\npython scripts/biorxiv_search.py \\\n  --keywords \"CRISPR\" \"gene editing\" \\\n  --start-date 2024-01-01 \\\n  --end-date 2024-12-31 \\\n  --output results.json\n```\n\n**With Category Filter:**\n```python\npython scripts/biorxiv_search.py \\\n  --keywords \"neural networks\" \"deep learning\" \\\n  --days-back 180 \\\n  --category neuroscience \\\n  --output recent_neuroscience.json\n```\n\n**Search Fields:**\nBy default, keywords are searched in both title and abstract. Customize with `--search-fields`:\n```python\npython scripts/biorxiv_search.py \\\n  --keywords \"AlphaFold\" \\\n  --search-fields title \\\n  --days-back 365\n```\n\n### 2. Author Search\n\nFind all papers by a specific author within a date range.\n\n**Basic Usage:**\n```python\npython scripts/biorxiv_search.py \\\n  --author \"Smith\" \\\n  --start-date 2023-01-01 \\\n  --end-date 2024-12-31 \\\n  --output smith_papers.json\n```\n\n**Recent Publications:**\n```python\n# Last year by default if no dates specified\npython scripts/biorxiv_search.py \\\n  --author \"Johnson\" \\\n  --output johnson_recent.json\n```\n\n### 3. Date Range Search\n\nRetrieve all preprints posted within a specific date range.\n\n**Basic Usage:**\n```python\npython scripts/biorxiv_search.py \\\n  --start-date 2024-01-01 \\\n  --end-date 2024-01-31 \\\n  --output january_2024.json\n```\n\n**With Category Filter:**\n```python\npython scripts/biorxiv_search.py \\\n  --start-date 2024-06-01 \\\n  --end-date 2024-06-30 \\\n  --category genomics \\\n  --output genomics_june.json\n```\n\n**Days Back Shortcut:**\n```python\n# Last 30 days\npython scripts/biorxiv_search.py \\\n  --days-back 30 \\\n  --output last_month.json\n```\n\n### 4. Paper Details by DOI\n\nRetrieve detailed metadata for a specific preprint.\n\n**Basic Usage:**\n```python\npython scripts/biorxiv_search.py \\\n  --doi \"10.1101/2024.01.15.123456\" \\\n  --output paper_details.json\n```\n\n**Full DOI URLs Accepted:**\n```python\npython scripts/biorxiv_search.py \\\n  --doi \"https://doi.org/10.1101/2024.01.15.123456\"\n```\n\n### 5. PDF Downloads\n\nDownload the full-text PDF of any preprint.\n\n**Basic Usage:**\n```python\npython scripts/biorxiv_search.py \\\n  --doi \"10.1101/2024.01.15.123456\" \\\n  --download-pdf paper.pdf\n```\n\n**Batch Processing:**\nFor multiple PDFs, extract DOIs from a search result JSON and download each paper:\n```python\nimport json\nfrom biorxiv_search import BioRxivSearcher\n\n# Load search results\nwith open('results.json') as f:\n    data = json.load(f)\n\nsearcher = BioRxivSearcher(verbose=True)\n\n# Download each paper\nfor i, paper in enumerate(data['results'][:10]):  # First 10 papers\n    doi = paper['doi']\n    searcher.download_pdf(doi, f\"papers/paper_{i+1}.pdf\")\n```\n\n## Valid Categories\n\nFilter searches by bioRxiv subject categories:\n\n- `animal-behavior-and-cognition`\n- `biochemistry`\n- `bioengineering`\n- `bioinformatics`\n- `biophysics`\n- `cancer-biology`\n- `cell-biology`\n- `clinical-trials`\n- `developmental-biology`\n- `ecology`\n- `epidemiology`\n- `evolutionary-biology`\n- `genetics`\n- `genomics`\n- `immunology`\n- `microbiology`\n- `molecular-biology`\n- `neuroscience`\n- `paleontology`\n- `pathology`\n- `pharmacology-and-toxicology`\n- `physiology`\n- `plant-biology`\n- `scientific-communication-and-education`\n- `synthetic-biology`\n- `systems-biology`\n- `zoology`\n\n## Output Format\n\nAll searches return structured JSON with the following format:\n\n```json\n{\n  \"query\": {\n    \"keywords\": [\"CRISPR\"],\n    \"start_date\": \"2024-01-01\",\n    \"end_date\": \"2024-12-31\",\n    \"category\": \"genomics\"\n  },\n  \"result_count\": 42,\n  \"results\": [\n    {\n      \"doi\": \"10.1101/2024.01.15.123456\",\n      \"title\": \"Paper Title Here\",\n      \"authors\": \"Smith J, Doe J, Johnson A\",\n      \"author_corresponding\": \"Smith J\",\n      \"author_corresponding_institution\": \"University Example\",\n      \"date\": \"2024-01-15\",\n      \"version\": \"1\",\n      \"type\": \"new results\",\n      \"license\": \"cc_by\",\n      \"category\": \"genomics\",\n      \"abstract\": \"Full abstract text...\",\n      \"pdf_url\": \"https://www.biorxiv.org/content/10.1101/2024.01.15.123456v1.full.pdf\",\n      \"html_url\": \"https://www.biorxiv.org/content/10.1101/2024.01.15.123456v1\",\n      \"jatsxml\": \"https://www.biorxiv.org/content/...\",\n      \"published\": \"\"\n    }\n  ]\n}\n```\n\n## Common Usage Patterns\n\n### Literature Review Workflow\n\n1. **Broad keyword search:**\n```python\npython scripts/biorxiv_search.py \\\n  --keywords \"organoids\" \"tissue engineering\" \\\n  --start-date 2023-01-01 \\\n  --end-date 2024-12-31 \\\n  --category bioengineering \\\n  --output organoid_papers.json\n```\n\n2. **Extract and review results:**\n```python\nimport json\n\nwith open('organoid_papers.json') as f:\n    data = json.load(f)\n\nprint(f\"Found {data['result_count']} papers\")\n\nfor paper in data['results'][:5]:\n    print(f\"\\nTitle: {paper['title']}\")\n    print(f\"Authors: {paper['authors']}\")\n    print(f\"Date: {paper['date']}\")\n    print(f\"DOI: {paper['doi']}\")\n```\n\n3. **Download selected papers:**\n```python\nfrom biorxiv_search import BioRxivSearcher\n\nsearcher = BioRxivSearcher()\nselected_dois = [\"10.1101/2024.01.15.123456\", \"10.1101/2024.02.20.789012\"]\n\nfor doi in selected_dois:\n    filename = doi.replace(\"/\", \"_\").replace(\".\", \"_\") + \".pdf\"\n    searcher.download_pdf(doi, f\"papers/{filename}\")\n```\n\n### Trend Analysis\n\nTrack research trends by analyzing publication frequencies over time:\n\n```python\npython scripts/biorxiv_search.py \\\n  --keywords \"machine learning\" \\\n  --start-date 2020-01-01 \\\n  --end-date 2024-12-31 \\\n  --category bioinformatics \\\n  --output ml_trends.json\n```\n\nThen analyze the temporal distribution in the results.\n\n### Author Tracking\n\nMonitor specific researchers' preprints:\n\n```python\n# Track multiple authors\nauthors = [\"Smith\", \"Johnson\", \"Williams\"]\n\nfor author in authors:\n    python scripts/biorxiv_search.py \\\n      --author \"{author}\" \\\n      --days-back 365 \\\n      --output \"{author}_papers.json\"\n```\n\n## Python API Usage\n\nFor more complex workflows, import and use the `BioRxivSearcher` class directly:\n\n```python\nfrom scripts.biorxiv_search import BioRxivSearcher\n\n# Initialize\nsearcher = BioRxivSearcher(verbose=True)\n\n# Multiple search operations\nkeywords_papers = searcher.search_by_keywords(\n    keywords=[\"CRISPR\", \"gene editing\"],\n    start_date=\"2024-01-01\",\n    end_date=\"2024-12-31\",\n    category=\"genomics\"\n)\n\nauthor_papers = searcher.search_by_author(\n    author_name=\"Smith\",\n    start_date=\"2023-01-01\",\n    end_date=\"2024-12-31\"\n)\n\n# Get specific paper details\npaper = searcher.get_paper_details(\"10.1101/2024.01.15.123456\")\n\n# Download PDF\nsuccess = searcher.download_pdf(\n    doi=\"10.1101/2024.01.15.123456\",\n    output_path=\"paper.pdf\"\n)\n\n# Format results consistently\nformatted = searcher.format_result(paper, include_abstract=True)\n```\n\n## Best Practices\n\n1. **Use appropriate date ranges**: Smaller date ranges return faster. For keyword searches over long periods, consider splitting into multiple queries.\n\n2. **Filter by category**: When possible, use `--category` to reduce data transfer and improve search precision.\n\n3. **Respect rate limits**: The script includes automatic delays (0.5s between requests). For large-scale data collection, add additional delays.\n\n4. **Cache results**: Save search results to JSON files to avoid repeated API calls.\n\n5. **Version tracking**: Preprints can have multiple versions. The `version` field indicates which version is returned. PDF URLs include the version number.\n\n6. **Handle errors gracefully**: Check the `result_count` in output JSON. Empty results may indicate date range issues or API connectivity problems.\n\n7. **Verbose mode for debugging**: Use `--verbose` flag to see detailed logging of API requests and responses.\n\n## Advanced Features\n\n### Custom Date Range Logic\n\n```python\nfrom datetime import datetime, timedelta\n\n# Last quarter\nend_date = datetime.now()\nstart_date = end_date - timedelta(days=90)\n\npython scripts/biorxiv_search.py \\\n  --start-date {start_date.strftime('%Y-%m-%d')} \\\n  --end-date {end_date.strftime('%Y-%m-%d')}\n```\n\n### Result Limiting\n\nLimit the number of results returned:\n\n```python\npython scripts/biorxiv_search.py \\\n  --keywords \"COVID-19\" \\\n  --days-back 30 \\\n  --limit 50 \\\n  --output covid_top50.json\n```\n\n### Exclude Abstracts for Speed\n\nWhen only metadata is needed:\n\n```python\n# Note: Abstract inclusion is controlled in Python API\nfrom scripts.biorxiv_search import BioRxivSearcher\n\nsearcher = BioRxivSearcher()\npapers = searcher.search_by_keywords(keywords=[\"AI\"], days_back=30)\nformatted = [searcher.format_result(p, include_abstract=False) for p in papers]\n```\n\n## Programmatic Integration\n\nIntegrate search results into downstream analysis pipelines:\n\n```python\nimport json\nimport pandas as pd\n\n# Load results\nwith open('results.json') as f:\n    data = json.load(f)\n\n# Convert to DataFrame for analysis\ndf = pd.DataFrame(data['results'])\n\n# Analyze\nprint(f\"Total papers: {len(df)}\")\nprint(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\nprint(f\"\\nTop authors by paper count:\")\nprint(df['authors'].str.split(',').explode().str.strip().value_counts().head(10))\n\n# Filter and export\nrecent = df[df['date'] >= '2024-06-01']\nrecent.to_csv('recent_papers.csv', index=False)\n```\n\n## Testing the Skill\n\nTo verify that the bioRxiv database skill is working correctly, run the comprehensive test suite.\n\n**Prerequisites:**\n```bash\nuv pip install requests\n```\n\n**Run tests:**\n```bash\npython tests/test_biorxiv_search.py\n```\n\nThe test suite validates:\n- **Initialization**: BioRxivSearcher class instantiation\n- **Date Range Search**: Retrieving papers within specific date ranges\n- **Category Filtering**: Filtering papers by bioRxiv categories\n- **Keyword Search**: Finding papers containing specific keywords\n- **DOI Lookup**: Retrieving specific papers by DOI\n- **Result Formatting**: Proper formatting of paper metadata\n- **Interval Search**: Fetching recent papers by time intervals\n\n**Expected Output:**\n```\n bioRxiv Database Search Skill Test Suite\n======================================================================\n\n Test 1: Initialization\n BioRxivSearcher initialized successfully\n\n Test 2: Date Range Search\n Found 150 papers between 2024-01-01 and 2024-01-07\n   First paper: Novel CRISPR-based approach for genome editing...\n\n[... additional tests ...]\n\n======================================================================\n Test Summary\n======================================================================\n PASS: Initialization\n PASS: Date Range Search\n PASS: Category Filtering\n PASS: Keyword Search\n PASS: DOI Lookup\n PASS: Result Formatting\n PASS: Interval Search\n======================================================================\nResults: 7/7 tests passed (100%)\n======================================================================\n\n All tests passed! The bioRxiv database skill is working correctly.\n```\n\n**Note:** Some tests may show warnings if no papers are found in specific date ranges or categories. This is normal and does not indicate a failure.\n\n## Reference Documentation\n\nFor detailed API specifications, endpoint documentation, and response schemas, refer to:\n- `references/api_reference.md` - Complete bioRxiv API documentation\n\nThe reference file includes:\n- Full API endpoint specifications\n- Response format details\n- Error handling patterns\n- Rate limiting guidelines\n- Advanced search patterns\n",
        "data/k-dense-ai/bioservices/SKILL.md": "---\nname: bioservices\ndescription: \"Primary Python tool for 40+ bioinformatics services. Preferred for multi-database workflows: UniProt, KEGG, ChEMBL, PubChem, Reactome, QuickGO. Unified API for queries, ID mapping, pathway analysis. For direct REST control, use individual database skills (uniprot-database, kegg-database).\"\n---\n\n# BioServices\n\n## Overview\n\nBioServices is a Python package providing programmatic access to approximately 40 bioinformatics web services and databases. Retrieve biological data, perform cross-database queries, map identifiers, analyze sequences, and integrate multiple biological resources in Python workflows. The package handles both REST and SOAP/WSDL protocols transparently.\n\n## When to Use This Skill\n\nThis skill should be used when:\n- Retrieving protein sequences, annotations, or structures from UniProt, PDB, Pfam\n- Analyzing metabolic pathways and gene functions via KEGG or Reactome\n- Searching compound databases (ChEBI, ChEMBL, PubChem) for chemical information\n- Converting identifiers between different biological databases (KEGGUniProt, compound IDs)\n- Running sequence similarity searches (BLAST, MUSCLE alignment)\n- Querying gene ontology terms (QuickGO, GO annotations)\n- Accessing protein-protein interaction data (PSICQUIC, IntactComplex)\n- Mining genomic data (BioMart, ArrayExpress, ENA)\n- Integrating data from multiple bioinformatics resources in a single workflow\n\n## Core Capabilities\n\n### 1. Protein Analysis\n\nRetrieve protein information, sequences, and functional annotations:\n\n```python\nfrom bioservices import UniProt\n\nu = UniProt(verbose=False)\n\n# Search for protein by name\nresults = u.search(\"ZAP70_HUMAN\", frmt=\"tab\", columns=\"id,genes,organism\")\n\n# Retrieve FASTA sequence\nsequence = u.retrieve(\"P43403\", \"fasta\")\n\n# Map identifiers between databases\nkegg_ids = u.mapping(fr=\"UniProtKB_AC-ID\", to=\"KEGG\", query=\"P43403\")\n```\n\n**Key methods:**\n- `search()`: Query UniProt with flexible search terms\n- `retrieve()`: Get protein entries in various formats (FASTA, XML, tab)\n- `mapping()`: Convert identifiers between databases\n\nReference: `references/services_reference.md` for complete UniProt API details.\n\n### 2. Pathway Discovery and Analysis\n\nAccess KEGG pathway information for genes and organisms:\n\n```python\nfrom bioservices import KEGG\n\nk = KEGG()\nk.organism = \"hsa\"  # Set to human\n\n# Search for organisms\nk.lookfor_organism(\"droso\")  # Find Drosophila species\n\n# Find pathways by name\nk.lookfor_pathway(\"B cell\")  # Returns matching pathway IDs\n\n# Get pathways containing specific genes\npathways = k.get_pathway_by_gene(\"7535\", \"hsa\")  # ZAP70 gene\n\n# Retrieve and parse pathway data\ndata = k.get(\"hsa04660\")\nparsed = k.parse(data)\n\n# Extract pathway interactions\ninteractions = k.parse_kgml_pathway(\"hsa04660\")\nrelations = interactions['relations']  # Protein-protein interactions\n\n# Convert to Simple Interaction Format\nsif_data = k.pathway2sif(\"hsa04660\")\n```\n\n**Key methods:**\n- `lookfor_organism()`, `lookfor_pathway()`: Search by name\n- `get_pathway_by_gene()`: Find pathways containing genes\n- `parse_kgml_pathway()`: Extract structured pathway data\n- `pathway2sif()`: Get protein interaction networks\n\nReference: `references/workflow_patterns.md` for complete pathway analysis workflows.\n\n### 3. Compound Database Searches\n\nSearch and cross-reference compounds across multiple databases:\n\n```python\nfrom bioservices import KEGG, UniChem\n\nk = KEGG()\n\n# Search compounds by name\nresults = k.find(\"compound\", \"Geldanamycin\")  # Returns cpd:C11222\n\n# Get compound information with database links\ncompound_info = k.get(\"cpd:C11222\")  # Includes ChEBI links\n\n# Cross-reference KEGG  ChEMBL using UniChem\nu = UniChem()\nchembl_id = u.get_compound_id_from_kegg(\"C11222\")  # Returns CHEMBL278315\n```\n\n**Common workflow:**\n1. Search compound by name in KEGG\n2. Extract KEGG compound ID\n3. Use UniChem for KEGG  ChEMBL mapping\n4. ChEBI IDs are often provided in KEGG entries\n\nReference: `references/identifier_mapping.md` for complete cross-database mapping guide.\n\n### 4. Sequence Analysis\n\nRun BLAST searches and sequence alignments:\n\n```python\nfrom bioservices import NCBIblast\n\ns = NCBIblast(verbose=False)\n\n# Run BLASTP against UniProtKB\njobid = s.run(\n    program=\"blastp\",\n    sequence=protein_sequence,\n    stype=\"protein\",\n    database=\"uniprotkb\",\n    email=\"your.email@example.com\"  # Required by NCBI\n)\n\n# Check job status and retrieve results\ns.getStatus(jobid)\nresults = s.getResult(jobid, \"out\")\n```\n\n**Note:** BLAST jobs are asynchronous. Check status before retrieving results.\n\n### 5. Identifier Mapping\n\nConvert identifiers between different biological databases:\n\n```python\nfrom bioservices import UniProt, KEGG\n\n# UniProt mapping (many database pairs supported)\nu = UniProt()\nresults = u.mapping(\n    fr=\"UniProtKB_AC-ID\",  # Source database\n    to=\"KEGG\",              # Target database\n    query=\"P43403\"          # Identifier(s) to convert\n)\n\n# KEGG gene ID  UniProt\nkegg_to_uniprot = u.mapping(fr=\"KEGG\", to=\"UniProtKB_AC-ID\", query=\"hsa:7535\")\n\n# For compounds, use UniChem\nfrom bioservices import UniChem\nu = UniChem()\nchembl_from_kegg = u.get_compound_id_from_kegg(\"C11222\")\n```\n\n**Supported mappings (UniProt):**\n- UniProtKB  KEGG\n- UniProtKB  Ensembl\n- UniProtKB  PDB\n- UniProtKB  RefSeq\n- And many more (see `references/identifier_mapping.md`)\n\n### 6. Gene Ontology Queries\n\nAccess GO terms and annotations:\n\n```python\nfrom bioservices import QuickGO\n\ng = QuickGO(verbose=False)\n\n# Retrieve GO term information\nterm_info = g.Term(\"GO:0003824\", frmt=\"obo\")\n\n# Search annotations\nannotations = g.Annotation(protein=\"P43403\", format=\"tsv\")\n```\n\n### 7. Protein-Protein Interactions\n\nQuery interaction databases via PSICQUIC:\n\n```python\nfrom bioservices import PSICQUIC\n\ns = PSICQUIC(verbose=False)\n\n# Query specific database (e.g., MINT)\ninteractions = s.query(\"mint\", \"ZAP70 AND species:9606\")\n\n# List available interaction databases\ndatabases = s.activeDBs\n```\n\n**Available databases:** MINT, IntAct, BioGRID, DIP, and 30+ others.\n\n## Multi-Service Integration Workflows\n\nBioServices excels at combining multiple services for comprehensive analysis. Common integration patterns:\n\n### Complete Protein Analysis Pipeline\n\nExecute a full protein characterization workflow:\n\n```bash\npython scripts/protein_analysis_workflow.py ZAP70_HUMAN your.email@example.com\n```\n\nThis script demonstrates:\n1. UniProt search for protein entry\n2. FASTA sequence retrieval\n3. BLAST similarity search\n4. KEGG pathway discovery\n5. PSICQUIC interaction mapping\n\n### Pathway Network Analysis\n\nAnalyze all pathways for an organism:\n\n```bash\npython scripts/pathway_analysis.py hsa output_directory/\n```\n\nExtracts and analyzes:\n- All pathway IDs for organism\n- Protein-protein interactions per pathway\n- Interaction type distributions\n- Exports to CSV/SIF formats\n\n### Cross-Database Compound Search\n\nMap compound identifiers across databases:\n\n```bash\npython scripts/compound_cross_reference.py Geldanamycin\n```\n\nRetrieves:\n- KEGG compound ID\n- ChEBI identifier\n- ChEMBL identifier\n- Basic compound properties\n\n### Batch Identifier Conversion\n\nConvert multiple identifiers at once:\n\n```bash\npython scripts/batch_id_converter.py input_ids.txt --from UniProtKB_AC-ID --to KEGG\n```\n\n## Best Practices\n\n### Output Format Handling\n\nDifferent services return data in various formats:\n- **XML**: Parse using BeautifulSoup (most SOAP services)\n- **Tab-separated (TSV)**: Pandas DataFrames for tabular data\n- **Dictionary/JSON**: Direct Python manipulation\n- **FASTA**: BioPython integration for sequence analysis\n\n### Rate Limiting and Verbosity\n\nControl API request behavior:\n\n```python\nfrom bioservices import KEGG\n\nk = KEGG(verbose=False)  # Suppress HTTP request details\nk.TIMEOUT = 30  # Adjust timeout for slow connections\n```\n\n### Error Handling\n\nWrap service calls in try-except blocks:\n\n```python\ntry:\n    results = u.search(\"ambiguous_query\")\n    if results:\n        # Process results\n        pass\nexcept Exception as e:\n    print(f\"Search failed: {e}\")\n```\n\n### Organism Codes\n\nUse standard organism abbreviations:\n- `hsa`: Homo sapiens (human)\n- `mmu`: Mus musculus (mouse)\n- `dme`: Drosophila melanogaster\n- `sce`: Saccharomyces cerevisiae (yeast)\n\nList all organisms: `k.list(\"organism\")` or `k.organismIds`\n\n### Integration with Other Tools\n\nBioServices works well with:\n- **BioPython**: Sequence analysis on retrieved FASTA data\n- **Pandas**: Tabular data manipulation\n- **PyMOL**: 3D structure visualization (retrieve PDB IDs)\n- **NetworkX**: Network analysis of pathway interactions\n- **Galaxy**: Custom tool wrappers for workflow platforms\n\n## Resources\n\n### scripts/\n\nExecutable Python scripts demonstrating complete workflows:\n\n- `protein_analysis_workflow.py`: End-to-end protein characterization\n- `pathway_analysis.py`: KEGG pathway discovery and network extraction\n- `compound_cross_reference.py`: Multi-database compound searching\n- `batch_id_converter.py`: Bulk identifier mapping utility\n\nScripts can be executed directly or adapted for specific use cases.\n\n### references/\n\nDetailed documentation loaded as needed:\n\n- `services_reference.md`: Comprehensive list of all 40+ services with methods\n- `workflow_patterns.md`: Detailed multi-step analysis workflows\n- `identifier_mapping.md`: Complete guide to cross-database ID conversion\n\nLoad references when working with specific services or complex integration tasks.\n\n## Installation\n\n```bash\nuv pip install bioservices\n```\n\nDependencies are automatically managed. Package is tested on Python 3.9-3.12.\n\n## Additional Information\n\nFor detailed API documentation and advanced features, refer to:\n- Official documentation: https://bioservices.readthedocs.io/\n- Source code: https://github.com/cokelaer/bioservices\n- Service-specific references in `references/services_reference.md`\n",
        "data/k-dense-ai/cellxgene-census/SKILL.md": "---\nname: cellxgene-census\ndescription: \"Query CZ CELLxGENE Census (61M+ cells). Filter by cell type/tissue/disease, retrieve expression data, integrate with scanpy/PyTorch, for population-scale single-cell analysis.\"\n---\n\n# CZ CELLxGENE Census\n\n## Overview\n\nThe CZ CELLxGENE Census provides programmatic access to a comprehensive, versioned collection of standardized single-cell genomics data from CZ CELLxGENE Discover. This skill enables efficient querying and analysis of millions of cells across thousands of datasets.\n\nThe Census includes:\n- **61+ million cells** from human and mouse\n- **Standardized metadata** (cell types, tissues, diseases, donors)\n- **Raw gene expression** matrices\n- **Pre-calculated embeddings** and statistics\n- **Integration with PyTorch, scanpy, and other analysis tools**\n\n## When to Use This Skill\n\nThis skill should be used when:\n- Querying single-cell expression data by cell type, tissue, or disease\n- Exploring available single-cell datasets and metadata\n- Training machine learning models on single-cell data\n- Performing large-scale cross-dataset analyses\n- Integrating Census data with scanpy or other analysis frameworks\n- Computing statistics across millions of cells\n- Accessing pre-calculated embeddings or model predictions\n\n## Installation and Setup\n\nInstall the Census API:\n```bash\nuv pip install cellxgene-census\n```\n\nFor machine learning workflows, install additional dependencies:\n```bash\nuv pip install cellxgene-census[experimental]\n```\n\n## Core Workflow Patterns\n\n### 1. Opening the Census\n\nAlways use the context manager to ensure proper resource cleanup:\n\n```python\nimport cellxgene_census\n\n# Open latest stable version\nwith cellxgene_census.open_soma() as census:\n    # Work with census data\n\n# Open specific version for reproducibility\nwith cellxgene_census.open_soma(census_version=\"2023-07-25\") as census:\n    # Work with census data\n```\n\n**Key points:**\n- Use context manager (`with` statement) for automatic cleanup\n- Specify `census_version` for reproducible analyses\n- Default opens latest \"stable\" release\n\n### 2. Exploring Census Information\n\nBefore querying expression data, explore available datasets and metadata.\n\n**Access summary information:**\n```python\n# Get summary statistics\nsummary = census[\"census_info\"][\"summary\"].read().concat().to_pandas()\nprint(f\"Total cells: {summary['total_cell_count'][0]}\")\n\n# Get all datasets\ndatasets = census[\"census_info\"][\"datasets\"].read().concat().to_pandas()\n\n# Filter datasets by criteria\ncovid_datasets = datasets[datasets[\"disease\"].str.contains(\"COVID\", na=False)]\n```\n\n**Query cell metadata to understand available data:**\n```python\n# Get unique cell types in a tissue\ncell_metadata = cellxgene_census.get_obs(\n    census,\n    \"homo_sapiens\",\n    value_filter=\"tissue_general == 'brain' and is_primary_data == True\",\n    column_names=[\"cell_type\"]\n)\nunique_cell_types = cell_metadata[\"cell_type\"].unique()\nprint(f\"Found {len(unique_cell_types)} cell types in brain\")\n\n# Count cells by tissue\ntissue_counts = cell_metadata.groupby(\"tissue_general\").size()\n```\n\n**Important:** Always filter for `is_primary_data == True` to avoid counting duplicate cells unless specifically analyzing duplicates.\n\n### 3. Querying Expression Data (Small to Medium Scale)\n\nFor queries returning < 100k cells that fit in memory, use `get_anndata()`:\n\n```python\n# Basic query with cell type and tissue filters\nadata = cellxgene_census.get_anndata(\n    census=census,\n    organism=\"Homo sapiens\",  # or \"Mus musculus\"\n    obs_value_filter=\"cell_type == 'B cell' and tissue_general == 'lung' and is_primary_data == True\",\n    obs_column_names=[\"assay\", \"disease\", \"sex\", \"donor_id\"],\n)\n\n# Query specific genes with multiple filters\nadata = cellxgene_census.get_anndata(\n    census=census,\n    organism=\"Homo sapiens\",\n    var_value_filter=\"feature_name in ['CD4', 'CD8A', 'CD19', 'FOXP3']\",\n    obs_value_filter=\"cell_type == 'T cell' and disease == 'COVID-19' and is_primary_data == True\",\n    obs_column_names=[\"cell_type\", \"tissue_general\", \"donor_id\"],\n)\n```\n\n**Filter syntax:**\n- Use `obs_value_filter` for cell filtering\n- Use `var_value_filter` for gene filtering\n- Combine conditions with `and`, `or`\n- Use `in` for multiple values: `tissue in ['lung', 'liver']`\n- Select only needed columns with `obs_column_names`\n\n**Getting metadata separately:**\n```python\n# Query cell metadata\ncell_metadata = cellxgene_census.get_obs(\n    census, \"homo_sapiens\",\n    value_filter=\"disease == 'COVID-19' and is_primary_data == True\",\n    column_names=[\"cell_type\", \"tissue_general\", \"donor_id\"]\n)\n\n# Query gene metadata\ngene_metadata = cellxgene_census.get_var(\n    census, \"homo_sapiens\",\n    value_filter=\"feature_name in ['CD4', 'CD8A']\",\n    column_names=[\"feature_id\", \"feature_name\", \"feature_length\"]\n)\n```\n\n### 4. Large-Scale Queries (Out-of-Core Processing)\n\nFor queries exceeding available RAM, use `axis_query()` with iterative processing:\n\n```python\nimport tiledbsoma as soma\n\n# Create axis query\nquery = census[\"census_data\"][\"homo_sapiens\"].axis_query(\n    measurement_name=\"RNA\",\n    obs_query=soma.AxisQuery(\n        value_filter=\"tissue_general == 'brain' and is_primary_data == True\"\n    ),\n    var_query=soma.AxisQuery(\n        value_filter=\"feature_name in ['FOXP2', 'TBR1', 'SATB2']\"\n    )\n)\n\n# Iterate through expression matrix in chunks\niterator = query.X(\"raw\").tables()\nfor batch in iterator:\n    # batch is a pyarrow.Table with columns:\n    # - soma_data: expression value\n    # - soma_dim_0: cell (obs) coordinate\n    # - soma_dim_1: gene (var) coordinate\n    process_batch(batch)\n```\n\n**Computing incremental statistics:**\n```python\n# Example: Calculate mean expression\nn_observations = 0\nsum_values = 0.0\n\niterator = query.X(\"raw\").tables()\nfor batch in iterator:\n    values = batch[\"soma_data\"].to_numpy()\n    n_observations += len(values)\n    sum_values += values.sum()\n\nmean_expression = sum_values / n_observations\n```\n\n### 5. Machine Learning with PyTorch\n\nFor training models, use the experimental PyTorch integration:\n\n```python\nfrom cellxgene_census.experimental.ml import experiment_dataloader\n\nwith cellxgene_census.open_soma() as census:\n    # Create dataloader\n    dataloader = experiment_dataloader(\n        census[\"census_data\"][\"homo_sapiens\"],\n        measurement_name=\"RNA\",\n        X_name=\"raw\",\n        obs_value_filter=\"tissue_general == 'liver' and is_primary_data == True\",\n        obs_column_names=[\"cell_type\"],\n        batch_size=128,\n        shuffle=True,\n    )\n\n    # Training loop\n    for epoch in range(num_epochs):\n        for batch in dataloader:\n            X = batch[\"X\"]  # Gene expression tensor\n            labels = batch[\"obs\"][\"cell_type\"]  # Cell type labels\n\n            # Forward pass\n            outputs = model(X)\n            loss = criterion(outputs, labels)\n\n            # Backward pass\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n```\n\n**Train/test splitting:**\n```python\nfrom cellxgene_census.experimental.ml import ExperimentDataset\n\n# Create dataset from experiment\ndataset = ExperimentDataset(\n    experiment_axis_query,\n    layer_name=\"raw\",\n    obs_column_names=[\"cell_type\"],\n    batch_size=128,\n)\n\n# Split into train and test\ntrain_dataset, test_dataset = dataset.random_split(\n    split=[0.8, 0.2],\n    seed=42\n)\n```\n\n### 6. Integration with Scanpy\n\nSeamlessly integrate Census data with scanpy workflows:\n\n```python\nimport scanpy as sc\n\n# Load data from Census\nadata = cellxgene_census.get_anndata(\n    census=census,\n    organism=\"Homo sapiens\",\n    obs_value_filter=\"cell_type == 'neuron' and tissue_general == 'cortex' and is_primary_data == True\",\n)\n\n# Standard scanpy workflow\nsc.pp.normalize_total(adata, target_sum=1e4)\nsc.pp.log1p(adata)\nsc.pp.highly_variable_genes(adata, n_top_genes=2000)\n\n# Dimensionality reduction\nsc.pp.pca(adata, n_comps=50)\nsc.pp.neighbors(adata)\nsc.tl.umap(adata)\n\n# Visualization\nsc.pl.umap(adata, color=[\"cell_type\", \"tissue\", \"disease\"])\n```\n\n### 7. Multi-Dataset Integration\n\nQuery and integrate multiple datasets:\n\n```python\n# Strategy 1: Query multiple tissues separately\ntissues = [\"lung\", \"liver\", \"kidney\"]\nadatas = []\n\nfor tissue in tissues:\n    adata = cellxgene_census.get_anndata(\n        census=census,\n        organism=\"Homo sapiens\",\n        obs_value_filter=f\"tissue_general == '{tissue}' and is_primary_data == True\",\n    )\n    adata.obs[\"tissue\"] = tissue\n    adatas.append(adata)\n\n# Concatenate\ncombined = adatas[0].concatenate(adatas[1:])\n\n# Strategy 2: Query multiple datasets directly\nadata = cellxgene_census.get_anndata(\n    census=census,\n    organism=\"Homo sapiens\",\n    obs_value_filter=\"tissue_general in ['lung', 'liver', 'kidney'] and is_primary_data == True\",\n)\n```\n\n## Key Concepts and Best Practices\n\n### Always Filter for Primary Data\nUnless analyzing duplicates, always include `is_primary_data == True` in queries to avoid counting cells multiple times:\n```python\nobs_value_filter=\"cell_type == 'B cell' and is_primary_data == True\"\n```\n\n### Specify Census Version for Reproducibility\nAlways specify the Census version in production analyses:\n```python\ncensus = cellxgene_census.open_soma(census_version=\"2023-07-25\")\n```\n\n### Estimate Query Size Before Loading\nFor large queries, first check the number of cells to avoid memory issues:\n```python\n# Get cell count\nmetadata = cellxgene_census.get_obs(\n    census, \"homo_sapiens\",\n    value_filter=\"tissue_general == 'brain' and is_primary_data == True\",\n    column_names=[\"soma_joinid\"]\n)\nn_cells = len(metadata)\nprint(f\"Query will return {n_cells:,} cells\")\n\n# If too large (>100k), use out-of-core processing\n```\n\n### Use tissue_general for Broader Groupings\nThe `tissue_general` field provides coarser categories than `tissue`, useful for cross-tissue analyses:\n```python\n# Broader grouping\nobs_value_filter=\"tissue_general == 'immune system'\"\n\n# Specific tissue\nobs_value_filter=\"tissue == 'peripheral blood mononuclear cell'\"\n```\n\n### Select Only Needed Columns\nMinimize data transfer by specifying only required metadata columns:\n```python\nobs_column_names=[\"cell_type\", \"tissue_general\", \"disease\"]  # Not all columns\n```\n\n### Check Dataset Presence for Gene-Specific Queries\nWhen analyzing specific genes, verify which datasets measured them:\n```python\npresence = cellxgene_census.get_presence_matrix(\n    census,\n    \"homo_sapiens\",\n    var_value_filter=\"feature_name in ['CD4', 'CD8A']\"\n)\n```\n\n### Two-Step Workflow: Explore Then Query\nFirst explore metadata to understand available data, then query expression:\n```python\n# Step 1: Explore what's available\nmetadata = cellxgene_census.get_obs(\n    census, \"homo_sapiens\",\n    value_filter=\"disease == 'COVID-19' and is_primary_data == True\",\n    column_names=[\"cell_type\", \"tissue_general\"]\n)\nprint(metadata.value_counts())\n\n# Step 2: Query based on findings\nadata = cellxgene_census.get_anndata(\n    census=census,\n    organism=\"Homo sapiens\",\n    obs_value_filter=\"disease == 'COVID-19' and cell_type == 'T cell' and is_primary_data == True\",\n)\n```\n\n## Available Metadata Fields\n\n### Cell Metadata (obs)\nKey fields for filtering:\n- `cell_type`, `cell_type_ontology_term_id`\n- `tissue`, `tissue_general`, `tissue_ontology_term_id`\n- `disease`, `disease_ontology_term_id`\n- `assay`, `assay_ontology_term_id`\n- `donor_id`, `sex`, `self_reported_ethnicity`\n- `development_stage`, `development_stage_ontology_term_id`\n- `dataset_id`\n- `is_primary_data` (Boolean: True = unique cell)\n\n### Gene Metadata (var)\n- `feature_id` (Ensembl gene ID, e.g., \"ENSG00000161798\")\n- `feature_name` (Gene symbol, e.g., \"FOXP2\")\n- `feature_length` (Gene length in base pairs)\n\n## Reference Documentation\n\nThis skill includes detailed reference documentation:\n\n### references/census_schema.md\nComprehensive documentation of:\n- Census data structure and organization\n- All available metadata fields\n- Value filter syntax and operators\n- SOMA object types\n- Data inclusion criteria\n\n**When to read:** When you need detailed schema information, full list of metadata fields, or complex filter syntax.\n\n### references/common_patterns.md\nExamples and patterns for:\n- Exploratory queries (metadata only)\n- Small-to-medium queries (AnnData)\n- Large queries (out-of-core processing)\n- PyTorch integration\n- Scanpy integration workflows\n- Multi-dataset integration\n- Best practices and common pitfalls\n\n**When to read:** When implementing specific query patterns, looking for code examples, or troubleshooting common issues.\n\n## Common Use Cases\n\n### Use Case 1: Explore Cell Types in a Tissue\n```python\nwith cellxgene_census.open_soma() as census:\n    cells = cellxgene_census.get_obs(\n        census, \"homo_sapiens\",\n        value_filter=\"tissue_general == 'lung' and is_primary_data == True\",\n        column_names=[\"cell_type\"]\n    )\n    print(cells[\"cell_type\"].value_counts())\n```\n\n### Use Case 2: Query Marker Gene Expression\n```python\nwith cellxgene_census.open_soma() as census:\n    adata = cellxgene_census.get_anndata(\n        census=census,\n        organism=\"Homo sapiens\",\n        var_value_filter=\"feature_name in ['CD4', 'CD8A', 'CD19']\",\n        obs_value_filter=\"cell_type in ['T cell', 'B cell'] and is_primary_data == True\",\n    )\n```\n\n### Use Case 3: Train Cell Type Classifier\n```python\nfrom cellxgene_census.experimental.ml import experiment_dataloader\n\nwith cellxgene_census.open_soma() as census:\n    dataloader = experiment_dataloader(\n        census[\"census_data\"][\"homo_sapiens\"],\n        measurement_name=\"RNA\",\n        X_name=\"raw\",\n        obs_value_filter=\"is_primary_data == True\",\n        obs_column_names=[\"cell_type\"],\n        batch_size=128,\n        shuffle=True,\n    )\n\n    # Train model\n    for epoch in range(epochs):\n        for batch in dataloader:\n            # Training logic\n            pass\n```\n\n### Use Case 4: Cross-Tissue Analysis\n```python\nwith cellxgene_census.open_soma() as census:\n    adata = cellxgene_census.get_anndata(\n        census=census,\n        organism=\"Homo sapiens\",\n        obs_value_filter=\"cell_type == 'macrophage' and tissue_general in ['lung', 'liver', 'brain'] and is_primary_data == True\",\n    )\n\n    # Analyze macrophage differences across tissues\n    sc.tl.rank_genes_groups(adata, groupby=\"tissue_general\")\n```\n\n## Troubleshooting\n\n### Query Returns Too Many Cells\n- Add more specific filters to reduce scope\n- Use `tissue` instead of `tissue_general` for finer granularity\n- Filter by specific `dataset_id` if known\n- Switch to out-of-core processing for large queries\n\n### Memory Errors\n- Reduce query scope with more restrictive filters\n- Select fewer genes with `var_value_filter`\n- Use out-of-core processing with `axis_query()`\n- Process data in batches\n\n### Duplicate Cells in Results\n- Always include `is_primary_data == True` in filters\n- Check if intentionally querying across multiple datasets\n\n### Gene Not Found\n- Verify gene name spelling (case-sensitive)\n- Try Ensembl ID with `feature_id` instead of `feature_name`\n- Check dataset presence matrix to see if gene was measured\n- Some genes may have been filtered during Census construction\n\n### Version Inconsistencies\n- Always specify `census_version` explicitly\n- Use same version across all analyses\n- Check release notes for version-specific changes\n",
        "data/k-dense-ai/chembl-database/SKILL.md": "---\nname: chembl-database\ndescription: \"Query ChEMBL's bioactive molecules and drug discovery data. Search compounds by structure/properties, retrieve bioactivity data (IC50, Ki), find inhibitors, perform SAR studies, for medicinal chemistry.\"\n---\n\n# ChEMBL Database\n\n## Overview\n\nChEMBL is a manually curated database of bioactive molecules maintained by the European Bioinformatics Institute (EBI), containing over 2 million compounds, 19 million bioactivity measurements, 13,000+ drug targets, and data on approved drugs and clinical candidates. Access and query this data programmatically using the ChEMBL Python client for drug discovery and medicinal chemistry research.\n\n## When to Use This Skill\n\nThis skill should be used when:\n\n- **Compound searches**: Finding molecules by name, structure, or properties\n- **Target information**: Retrieving data about proteins, enzymes, or biological targets\n- **Bioactivity data**: Querying IC50, Ki, EC50, or other activity measurements\n- **Drug information**: Looking up approved drugs, mechanisms, or indications\n- **Structure searches**: Performing similarity or substructure searches\n- **Cheminformatics**: Analyzing molecular properties and drug-likeness\n- **Target-ligand relationships**: Exploring compound-target interactions\n- **Drug discovery**: Identifying inhibitors, agonists, or bioactive molecules\n\n## Installation and Setup\n\n### Python Client\n\nThe ChEMBL Python client is required for programmatic access:\n\n```bash\nuv pip install chembl_webresource_client\n```\n\n### Basic Usage Pattern\n\n```python\nfrom chembl_webresource_client.new_client import new_client\n\n# Access different endpoints\nmolecule = new_client.molecule\ntarget = new_client.target\nactivity = new_client.activity\ndrug = new_client.drug\n```\n\n## Core Capabilities\n\n### 1. Molecule Queries\n\n**Retrieve by ChEMBL ID:**\n```python\nmolecule = new_client.molecule\naspirin = molecule.get('CHEMBL25')\n```\n\n**Search by name:**\n```python\nresults = molecule.filter(pref_name__icontains='aspirin')\n```\n\n**Filter by properties:**\n```python\n# Find small molecules (MW <= 500) with favorable LogP\nresults = molecule.filter(\n    molecule_properties__mw_freebase__lte=500,\n    molecule_properties__alogp__lte=5\n)\n```\n\n### 2. Target Queries\n\n**Retrieve target information:**\n```python\ntarget = new_client.target\negfr = target.get('CHEMBL203')\n```\n\n**Search for specific target types:**\n```python\n# Find all kinase targets\nkinases = target.filter(\n    target_type='SINGLE PROTEIN',\n    pref_name__icontains='kinase'\n)\n```\n\n### 3. Bioactivity Data\n\n**Query activities for a target:**\n```python\nactivity = new_client.activity\n# Find potent EGFR inhibitors\nresults = activity.filter(\n    target_chembl_id='CHEMBL203',\n    standard_type='IC50',\n    standard_value__lte=100,\n    standard_units='nM'\n)\n```\n\n**Get all activities for a compound:**\n```python\ncompound_activities = activity.filter(\n    molecule_chembl_id='CHEMBL25',\n    pchembl_value__isnull=False\n)\n```\n\n### 4. Structure-Based Searches\n\n**Similarity search:**\n```python\nsimilarity = new_client.similarity\n# Find compounds similar to aspirin\nsimilar = similarity.filter(\n    smiles='CC(=O)Oc1ccccc1C(=O)O',\n    similarity=85  # 85% similarity threshold\n)\n```\n\n**Substructure search:**\n```python\nsubstructure = new_client.substructure\n# Find compounds containing benzene ring\nresults = substructure.filter(smiles='c1ccccc1')\n```\n\n### 5. Drug Information\n\n**Retrieve drug data:**\n```python\ndrug = new_client.drug\ndrug_info = drug.get('CHEMBL25')\n```\n\n**Get mechanisms of action:**\n```python\nmechanism = new_client.mechanism\nmechanisms = mechanism.filter(molecule_chembl_id='CHEMBL25')\n```\n\n**Query drug indications:**\n```python\ndrug_indication = new_client.drug_indication\nindications = drug_indication.filter(molecule_chembl_id='CHEMBL25')\n```\n\n## Query Workflow\n\n### Workflow 1: Finding Inhibitors for a Target\n\n1. **Identify the target** by searching by name:\n   ```python\n   targets = new_client.target.filter(pref_name__icontains='EGFR')\n   target_id = targets[0]['target_chembl_id']\n   ```\n\n2. **Query bioactivity data** for that target:\n   ```python\n   activities = new_client.activity.filter(\n       target_chembl_id=target_id,\n       standard_type='IC50',\n       standard_value__lte=100\n   )\n   ```\n\n3. **Extract compound IDs** and retrieve details:\n   ```python\n   compound_ids = [act['molecule_chembl_id'] for act in activities]\n   compounds = [new_client.molecule.get(cid) for cid in compound_ids]\n   ```\n\n### Workflow 2: Analyzing a Known Drug\n\n1. **Get drug information**:\n   ```python\n   drug_info = new_client.drug.get('CHEMBL1234')\n   ```\n\n2. **Retrieve mechanisms**:\n   ```python\n   mechanisms = new_client.mechanism.filter(molecule_chembl_id='CHEMBL1234')\n   ```\n\n3. **Find all bioactivities**:\n   ```python\n   activities = new_client.activity.filter(molecule_chembl_id='CHEMBL1234')\n   ```\n\n### Workflow 3: Structure-Activity Relationship (SAR) Study\n\n1. **Find similar compounds**:\n   ```python\n   similar = new_client.similarity.filter(smiles='query_smiles', similarity=80)\n   ```\n\n2. **Get activities for each compound**:\n   ```python\n   for compound in similar:\n       activities = new_client.activity.filter(\n           molecule_chembl_id=compound['molecule_chembl_id']\n       )\n   ```\n\n3. **Analyze property-activity relationships** using molecular properties from results.\n\n## Filter Operators\n\nChEMBL supports Django-style query filters:\n\n- `__exact` - Exact match\n- `__iexact` - Case-insensitive exact match\n- `__contains` / `__icontains` - Substring matching\n- `__startswith` / `__endswith` - Prefix/suffix matching\n- `__gt`, `__gte`, `__lt`, `__lte` - Numeric comparisons\n- `__range` - Value in range\n- `__in` - Value in list\n- `__isnull` - Null/not null check\n\n## Data Export and Analysis\n\nConvert results to pandas DataFrame for analysis:\n\n```python\nimport pandas as pd\n\nactivities = new_client.activity.filter(target_chembl_id='CHEMBL203')\ndf = pd.DataFrame(list(activities))\n\n# Analyze results\nprint(df['standard_value'].describe())\nprint(df.groupby('standard_type').size())\n```\n\n## Performance Optimization\n\n### Caching\n\nThe client automatically caches results for 24 hours. Configure caching:\n\n```python\nfrom chembl_webresource_client.settings import Settings\n\n# Disable caching\nSettings.Instance().CACHING = False\n\n# Adjust cache expiration (seconds)\nSettings.Instance().CACHE_EXPIRE = 86400\n```\n\n### Lazy Evaluation\n\nQueries execute only when data is accessed. Convert to list to force execution:\n\n```python\n# Query is not executed yet\nresults = molecule.filter(pref_name__icontains='aspirin')\n\n# Force execution\nresults_list = list(results)\n```\n\n### Pagination\n\nResults are paginated automatically. Iterate through all results:\n\n```python\nfor activity in new_client.activity.filter(target_chembl_id='CHEMBL203'):\n    # Process each activity\n    print(activity['molecule_chembl_id'])\n```\n\n## Common Use Cases\n\n### Find Kinase Inhibitors\n\n```python\n# Identify kinase targets\nkinases = new_client.target.filter(\n    target_type='SINGLE PROTEIN',\n    pref_name__icontains='kinase'\n)\n\n# Get potent inhibitors\nfor kinase in kinases[:5]:  # First 5 kinases\n    activities = new_client.activity.filter(\n        target_chembl_id=kinase['target_chembl_id'],\n        standard_type='IC50',\n        standard_value__lte=50\n    )\n```\n\n### Explore Drug Repurposing\n\n```python\n# Get approved drugs\ndrugs = new_client.drug.filter()\n\n# For each drug, find all targets\nfor drug in drugs[:10]:\n    mechanisms = new_client.mechanism.filter(\n        molecule_chembl_id=drug['molecule_chembl_id']\n    )\n```\n\n### Virtual Screening\n\n```python\n# Find compounds with desired properties\ncandidates = new_client.molecule.filter(\n    molecule_properties__mw_freebase__range=[300, 500],\n    molecule_properties__alogp__lte=5,\n    molecule_properties__hba__lte=10,\n    molecule_properties__hbd__lte=5\n)\n```\n\n## Resources\n\n### scripts/example_queries.py\n\nReady-to-use Python functions demonstrating common ChEMBL query patterns:\n\n- `get_molecule_info()` - Retrieve molecule details by ID\n- `search_molecules_by_name()` - Name-based molecule search\n- `find_molecules_by_properties()` - Property-based filtering\n- `get_bioactivity_data()` - Query bioactivities for targets\n- `find_similar_compounds()` - Similarity searching\n- `substructure_search()` - Substructure matching\n- `get_drug_info()` - Retrieve drug information\n- `find_kinase_inhibitors()` - Specialized kinase inhibitor search\n- `export_to_dataframe()` - Convert results to pandas DataFrame\n\nConsult this script for implementation details and usage examples.\n\n### references/api_reference.md\n\nComprehensive API documentation including:\n\n- Complete endpoint listing (molecule, target, activity, assay, drug, etc.)\n- All filter operators and query patterns\n- Molecular properties and bioactivity fields\n- Advanced query examples\n- Configuration and performance tuning\n- Error handling and rate limiting\n\nRefer to this document when detailed API information is needed or when troubleshooting queries.\n\n## Important Notes\n\n### Data Reliability\n\n- ChEMBL data is manually curated but may contain inconsistencies\n- Always check `data_validity_comment` field in activity records\n- Be aware of `potential_duplicate` flags\n\n### Units and Standards\n\n- Bioactivity values use standard units (nM, uM, etc.)\n- `pchembl_value` provides normalized activity (-log scale)\n- Check `standard_type` to understand measurement type (IC50, Ki, EC50, etc.)\n\n### Rate Limiting\n\n- Respect ChEMBL's fair usage policies\n- Use caching to minimize repeated requests\n- Consider bulk downloads for large datasets\n- Avoid hammering the API with rapid consecutive requests\n\n### Chemical Structure Formats\n\n- SMILES strings are the primary structure format\n- InChI keys available for compounds\n- SVG images can be generated via the image endpoint\n\n## Additional Resources\n\n- ChEMBL website: https://www.ebi.ac.uk/chembl/\n- API documentation: https://www.ebi.ac.uk/chembl/api/data/docs\n- Python client GitHub: https://github.com/chembl/chembl_webresource_client\n- Interface documentation: https://chembl.gitbook.io/chembl-interface-documentation/\n- Example notebooks: https://github.com/chembl/notebooks\n",
        "data/k-dense-ai/clinicaltrials-database/SKILL.md": "---\nname: clinicaltrials-database\ndescription: \"Query ClinicalTrials.gov via API v2. Search trials by condition, drug, location, status, or phase. Retrieve trial details by NCT ID, export data, for clinical research and patient matching.\"\n---\n\n# ClinicalTrials.gov Database\n\n## Overview\n\nClinicalTrials.gov is a comprehensive registry of clinical studies conducted worldwide, maintained by the U.S. National Library of Medicine. Access API v2 to search for trials, retrieve detailed study information, filter by various criteria, and export data for analysis. The API is public (no authentication required) with rate limits of ~50 requests per minute, supporting JSON and CSV formats.\n\n## When to Use This Skill\n\nThis skill should be used when working with clinical trial data in scenarios such as:\n\n- **Patient matching** - Finding recruiting trials for specific conditions or patient populations\n- **Research analysis** - Analyzing clinical trial trends, outcomes, or study designs\n- **Drug/intervention research** - Identifying trials testing specific drugs or interventions\n- **Geographic searches** - Locating trials in specific locations or regions\n- **Sponsor/organization tracking** - Finding trials conducted by specific institutions\n- **Data export** - Extracting clinical trial data for further analysis or reporting\n- **Trial monitoring** - Tracking status updates or results for specific trials\n- **Eligibility screening** - Reviewing inclusion/exclusion criteria for trials\n\n## Quick Start\n\n### Basic Search Query\n\nSearch for clinical trials using the helper script:\n\n```bash\ncd scientific-databases/clinicaltrials-database/scripts\npython3 query_clinicaltrials.py\n```\n\nOr use Python directly with the `requests` library:\n\n```python\nimport requests\n\nurl = \"https://clinicaltrials.gov/api/v2/studies\"\nparams = {\n    \"query.cond\": \"breast cancer\",\n    \"filter.overallStatus\": \"RECRUITING\",\n    \"pageSize\": 10\n}\n\nresponse = requests.get(url, params=params)\ndata = response.json()\n\nprint(f\"Found {data['totalCount']} trials\")\n```\n\n### Retrieve Specific Trial\n\nGet detailed information about a trial using its NCT ID:\n\n```python\nimport requests\n\nnct_id = \"NCT04852770\"\nurl = f\"https://clinicaltrials.gov/api/v2/studies/{nct_id}\"\n\nresponse = requests.get(url)\nstudy = response.json()\n\n# Access specific modules\ntitle = study['protocolSection']['identificationModule']['briefTitle']\nstatus = study['protocolSection']['statusModule']['overallStatus']\n```\n\n## Core Capabilities\n\n### 1. Search by Condition/Disease\n\nFind trials studying specific medical conditions or diseases using the `query.cond` parameter.\n\n**Example: Find recruiting diabetes trials**\n\n```python\nfrom scripts.query_clinicaltrials import search_studies\n\nresults = search_studies(\n    condition=\"type 2 diabetes\",\n    status=\"RECRUITING\",\n    page_size=20,\n    sort=\"LastUpdatePostDate:desc\"\n)\n\nprint(f\"Found {results['totalCount']} recruiting diabetes trials\")\nfor study in results['studies']:\n    protocol = study['protocolSection']\n    nct_id = protocol['identificationModule']['nctId']\n    title = protocol['identificationModule']['briefTitle']\n    print(f\"{nct_id}: {title}\")\n```\n\n**Common use cases:**\n- Finding trials for rare diseases\n- Identifying trials for comorbid conditions\n- Tracking trial availability for specific diagnoses\n\n### 2. Search by Intervention/Drug\n\nSearch for trials testing specific interventions, drugs, devices, or procedures using the `query.intr` parameter.\n\n**Example: Find Phase 3 trials testing Pembrolizumab**\n\n```python\nfrom scripts.query_clinicaltrials import search_studies\n\nresults = search_studies(\n    intervention=\"Pembrolizumab\",\n    status=[\"RECRUITING\", \"ACTIVE_NOT_RECRUITING\"],\n    page_size=50\n)\n\n# Filter by phase in results\nphase3_trials = [\n    study for study in results['studies']\n    if 'PHASE3' in study['protocolSection'].get('designModule', {}).get('phases', [])\n]\n```\n\n**Common use cases:**\n- Drug development tracking\n- Competitive intelligence for pharmaceutical companies\n- Treatment option research for clinicians\n\n### 3. Geographic Search\n\nFind trials in specific locations using the `query.locn` parameter.\n\n**Example: Find cancer trials in New York**\n\n```python\nfrom scripts.query_clinicaltrials import search_studies\n\nresults = search_studies(\n    condition=\"cancer\",\n    location=\"New York\",\n    status=\"RECRUITING\",\n    page_size=100\n)\n\n# Extract location details\nfor study in results['studies']:\n    locations_module = study['protocolSection'].get('contactsLocationsModule', {})\n    locations = locations_module.get('locations', [])\n    for loc in locations:\n        if 'New York' in loc.get('city', ''):\n            print(f\"{loc['facility']}: {loc['city']}, {loc.get('state', '')}\")\n```\n\n**Common use cases:**\n- Patient referrals to local trials\n- Geographic trial distribution analysis\n- Site selection for new trials\n\n### 4. Search by Sponsor/Organization\n\nFind trials conducted by specific organizations using the `query.spons` parameter.\n\n**Example: Find trials sponsored by NCI**\n\n```python\nfrom scripts.query_clinicaltrials import search_studies\n\nresults = search_studies(\n    sponsor=\"National Cancer Institute\",\n    page_size=100\n)\n\n# Extract sponsor information\nfor study in results['studies']:\n    sponsor_module = study['protocolSection']['sponsorCollaboratorsModule']\n    lead_sponsor = sponsor_module['leadSponsor']['name']\n    collaborators = sponsor_module.get('collaborators', [])\n    print(f\"Lead: {lead_sponsor}\")\n    if collaborators:\n        print(f\"  Collaborators: {', '.join([c['name'] for c in collaborators])}\")\n```\n\n**Common use cases:**\n- Tracking institutional research portfolios\n- Analyzing funding organization priorities\n- Identifying collaboration opportunities\n\n### 5. Filter by Study Status\n\nFilter trials by recruitment or completion status using the `filter.overallStatus` parameter.\n\n**Valid status values:**\n- `RECRUITING` - Currently recruiting participants\n- `NOT_YET_RECRUITING` - Not yet open for recruitment\n- `ENROLLING_BY_INVITATION` - Only enrolling by invitation\n- `ACTIVE_NOT_RECRUITING` - Active but no longer recruiting\n- `SUSPENDED` - Temporarily halted\n- `TERMINATED` - Stopped prematurely\n- `COMPLETED` - Study has concluded\n- `WITHDRAWN` - Withdrawn prior to enrollment\n\n**Example: Find recently completed trials with results**\n\n```python\nfrom scripts.query_clinicaltrials import search_studies\n\nresults = search_studies(\n    condition=\"alzheimer disease\",\n    status=\"COMPLETED\",\n    sort=\"LastUpdatePostDate:desc\",\n    page_size=50\n)\n\n# Filter for trials with results\ntrials_with_results = [\n    study for study in results['studies']\n    if study.get('hasResults', False)\n]\n\nprint(f\"Found {len(trials_with_results)} completed trials with results\")\n```\n\n### 6. Retrieve Detailed Study Information\n\nGet comprehensive information about specific trials including eligibility criteria, outcomes, contacts, and locations.\n\n**Example: Extract eligibility criteria**\n\n```python\nfrom scripts.query_clinicaltrials import get_study_details\n\nstudy = get_study_details(\"NCT04852770\")\neligibility = study['protocolSection']['eligibilityModule']\n\nprint(f\"Eligible Ages: {eligibility.get('minimumAge')} - {eligibility.get('maximumAge')}\")\nprint(f\"Eligible Sex: {eligibility.get('sex')}\")\nprint(f\"\\nInclusion Criteria:\")\nprint(eligibility.get('eligibilityCriteria'))\n```\n\n**Example: Extract contact information**\n\n```python\nfrom scripts.query_clinicaltrials import get_study_details\n\nstudy = get_study_details(\"NCT04852770\")\ncontacts_module = study['protocolSection']['contactsLocationsModule']\n\n# Overall contacts\nif 'centralContacts' in contacts_module:\n    for contact in contacts_module['centralContacts']:\n        print(f\"Contact: {contact.get('name')}\")\n        print(f\"Phone: {contact.get('phone')}\")\n        print(f\"Email: {contact.get('email')}\")\n\n# Study locations\nif 'locations' in contacts_module:\n    for location in contacts_module['locations']:\n        print(f\"\\nFacility: {location.get('facility')}\")\n        print(f\"City: {location.get('city')}, {location.get('state')}\")\n        if location.get('status'):\n            print(f\"Status: {location['status']}\")\n```\n\n### 7. Pagination and Bulk Data Retrieval\n\nHandle large result sets efficiently using pagination.\n\n**Example: Retrieve all matching trials**\n\n```python\nfrom scripts.query_clinicaltrials import search_with_all_results\n\n# Get all trials (automatically handles pagination)\nall_trials = search_with_all_results(\n    condition=\"rare disease\",\n    status=\"RECRUITING\"\n)\n\nprint(f\"Retrieved {len(all_trials)} total trials\")\n```\n\n**Example: Manual pagination with control**\n\n```python\nfrom scripts.query_clinicaltrials import search_studies\n\nall_studies = []\npage_token = None\nmax_pages = 10  # Limit to avoid excessive requests\n\nfor page in range(max_pages):\n    results = search_studies(\n        condition=\"cancer\",\n        page_size=1000,  # Max page size\n        page_token=page_token\n    )\n\n    all_studies.extend(results['studies'])\n\n    # Check for next page\n    page_token = results.get('pageToken')\n    if not page_token:\n        break\n\nprint(f\"Retrieved {len(all_studies)} studies across {page + 1} pages\")\n```\n\n### 8. Data Export to CSV\n\nExport trial data to CSV format for analysis in spreadsheet software or data analysis tools.\n\n**Example: Export to CSV file**\n\n```python\nfrom scripts.query_clinicaltrials import search_studies\n\n# Request CSV format\nresults = search_studies(\n    condition=\"heart disease\",\n    status=\"RECRUITING\",\n    format=\"csv\",\n    page_size=1000\n)\n\n# Save to file\nwith open(\"heart_disease_trials.csv\", \"w\") as f:\n    f.write(results)\n\nprint(\"Data exported to heart_disease_trials.csv\")\n```\n\n**Note:** CSV format returns a string instead of JSON dictionary.\n\n### 9. Extract and Summarize Study Information\n\nExtract key information for quick overview or reporting.\n\n**Example: Create trial summary**\n\n```python\nfrom scripts.query_clinicaltrials import get_study_details, extract_study_summary\n\n# Get details and extract summary\nstudy = get_study_details(\"NCT04852770\")\nsummary = extract_study_summary(study)\n\nprint(f\"NCT ID: {summary['nct_id']}\")\nprint(f\"Title: {summary['title']}\")\nprint(f\"Status: {summary['status']}\")\nprint(f\"Phase: {', '.join(summary['phase'])}\")\nprint(f\"Enrollment: {summary['enrollment']}\")\nprint(f\"Last Update: {summary['last_update']}\")\nprint(f\"\\nBrief Summary:\\n{summary['brief_summary']}\")\n```\n\n### 10. Combined Query Strategies\n\nCombine multiple filters for targeted searches.\n\n**Example: Multi-criteria search**\n\n```python\nfrom scripts.query_clinicaltrials import search_studies\n\n# Find Phase 2/3 immunotherapy trials for lung cancer in California\nresults = search_studies(\n    condition=\"lung cancer\",\n    intervention=\"immunotherapy\",\n    location=\"California\",\n    status=[\"RECRUITING\", \"NOT_YET_RECRUITING\"],\n    page_size=100\n)\n\n# Further filter by phase\nphase2_3_trials = [\n    study for study in results['studies']\n    if any(phase in ['PHASE2', 'PHASE3']\n           for phase in study['protocolSection'].get('designModule', {}).get('phases', []))\n]\n\nprint(f\"Found {len(phase2_3_trials)} Phase 2/3 immunotherapy trials\")\n```\n\n## Resources\n\n### scripts/query_clinicaltrials.py\n\nComprehensive Python script providing helper functions for common query patterns:\n\n- `search_studies()` - Search for trials with various filters\n- `get_study_details()` - Retrieve full information for a specific trial\n- `search_with_all_results()` - Automatically paginate through all results\n- `extract_study_summary()` - Extract key information for quick overview\n\nRun the script directly for example usage:\n\n```bash\npython3 scripts/query_clinicaltrials.py\n```\n\n### references/api_reference.md\n\nDetailed API documentation including:\n\n- Complete endpoint specifications\n- All query parameters and valid values\n- Response data structure and modules\n- Common use cases with code examples\n- Error handling and best practices\n- Data standards (ISO 8601 dates, CommonMark markdown)\n\nLoad this reference when working with unfamiliar API features or troubleshooting issues.\n\n## Best Practices\n\n### Rate Limit Management\n\nThe API has a rate limit of approximately 50 requests per minute. For bulk data retrieval:\n\n1. Use maximum page size (1000) to minimize requests\n2. Implement exponential backoff on rate limit errors (429 status)\n3. Add delays between requests for large-scale data collection\n\n```python\nimport time\nimport requests\n\ndef search_with_rate_limit(params):\n    try:\n        response = requests.get(\"https://clinicaltrials.gov/api/v2/studies\", params=params)\n        response.raise_for_status()\n        return response.json()\n    except requests.exceptions.HTTPError as e:\n        if e.response.status_code == 429:\n            print(\"Rate limited. Waiting 60 seconds...\")\n            time.sleep(60)\n            return search_with_rate_limit(params)  # Retry\n        raise\n```\n\n### Data Structure Navigation\n\nThe API response has a nested structure. Key paths to common information:\n\n- **NCT ID**: `study['protocolSection']['identificationModule']['nctId']`\n- **Title**: `study['protocolSection']['identificationModule']['briefTitle']`\n- **Status**: `study['protocolSection']['statusModule']['overallStatus']`\n- **Phase**: `study['protocolSection']['designModule']['phases']`\n- **Eligibility**: `study['protocolSection']['eligibilityModule']`\n- **Locations**: `study['protocolSection']['contactsLocationsModule']['locations']`\n- **Interventions**: `study['protocolSection']['armsInterventionsModule']['interventions']`\n\n### Error Handling\n\nAlways implement proper error handling for network requests:\n\n```python\nimport requests\n\ntry:\n    response = requests.get(url, params=params, timeout=30)\n    response.raise_for_status()\n    data = response.json()\nexcept requests.exceptions.HTTPError as e:\n    print(f\"HTTP error: {e.response.status_code}\")\nexcept requests.exceptions.RequestException as e:\n    print(f\"Request failed: {e}\")\nexcept ValueError as e:\n    print(f\"JSON decode error: {e}\")\n```\n\n### Handling Missing Data\n\nNot all trials have complete information. Always check for field existence:\n\n```python\n# Safe navigation with .get()\nphases = study['protocolSection'].get('designModule', {}).get('phases', [])\nenrollment = study['protocolSection'].get('designModule', {}).get('enrollmentInfo', {}).get('count', 'N/A')\n\n# Check before accessing\nif 'resultsSection' in study:\n    # Process results\n    pass\n```\n\n## Technical Specifications\n\n- **Base URL**: `https://clinicaltrials.gov/api/v2`\n- **Authentication**: Not required (public API)\n- **Rate Limit**: ~50 requests/minute per IP\n- **Response Formats**: JSON (default), CSV\n- **Max Page Size**: 1000 studies per request\n- **Date Format**: ISO 8601\n- **Text Format**: CommonMark Markdown for rich text fields\n- **API Version**: 2.0 (released March 2024)\n- **API Specification**: OpenAPI 3.0\n\nFor complete technical details, see `references/api_reference.md`.\n",
        "data/k-dense-ai/clinpgx-database/SKILL.md": "---\nname: clinpgx-database\ndescription: \"Access ClinPGx pharmacogenomics data (successor to PharmGKB). Query gene-drug interactions, CPIC guidelines, allele functions, for precision medicine and genotype-guided dosing decisions.\"\n---\n\n# ClinPGx Database\n\n## Overview\n\nClinPGx (Clinical Pharmacogenomics Database) is a comprehensive resource for clinical pharmacogenomics information, successor to PharmGKB. It consolidates data from PharmGKB, CPIC, and PharmCAT, providing curated information on how genetic variation affects medication response. Access gene-drug pairs, clinical guidelines, allele functions, and drug labels for precision medicine applications.\n\n## When to Use This Skill\n\nThis skill should be used when:\n\n- **Gene-drug interactions**: Querying how genetic variants affect drug metabolism, efficacy, or toxicity\n- **CPIC guidelines**: Accessing evidence-based clinical practice guidelines for pharmacogenetics\n- **Allele information**: Retrieving allele function, frequency, and phenotype data\n- **Drug labels**: Exploring FDA and other regulatory pharmacogenomic drug labeling\n- **Pharmacogenomic annotations**: Accessing curated literature on gene-drug-disease relationships\n- **Clinical decision support**: Using PharmDOG tool for phenoconversion and custom genotype interpretation\n- **Precision medicine**: Implementing pharmacogenomic testing in clinical practice\n- **Drug metabolism**: Understanding CYP450 and other pharmacogene functions\n- **Personalized dosing**: Finding genotype-guided dosing recommendations\n- **Adverse drug reactions**: Identifying genetic risk factors for drug toxicity\n\n## Installation and Setup\n\n### Python API Access\n\nThe ClinPGx REST API provides programmatic access to all database resources. Basic setup:\n\n```bash\nuv pip install requests\n```\n\n### API Endpoint\n\n```python\nBASE_URL = \"https://api.clinpgx.org/v1/\"\n```\n\n**Rate Limits**:\n- 2 requests per second maximum\n- Excessive requests will result in HTTP 429 (Too Many Requests) response\n\n**Authentication**: Not required for basic access\n\n**Data License**: Creative Commons Attribution-ShareAlike 4.0 International License\n\nFor substantial API use, notify the ClinPGx team at api@clinpgx.org\n\n## Core Capabilities\n\n### 1. Gene Queries\n\n**Retrieve gene information** including function, clinical annotations, and pharmacogenomic significance:\n\n```python\nimport requests\n\n# Get gene details\nresponse = requests.get(\"https://api.clinpgx.org/v1/gene/CYP2D6\")\ngene_data = response.json()\n\n# Search for genes by name\nresponse = requests.get(\"https://api.clinpgx.org/v1/gene\",\n                       params={\"q\": \"CYP\"})\ngenes = response.json()\n```\n\n**Key pharmacogenes**:\n- **CYP450 enzymes**: CYP2D6, CYP2C19, CYP2C9, CYP3A4, CYP3A5\n- **Transporters**: SLCO1B1, ABCB1, ABCG2\n- **Other metabolizers**: TPMT, DPYD, NUDT15, UGT1A1\n- **Receptors**: OPRM1, HTR2A, ADRB1\n- **HLA genes**: HLA-B, HLA-A\n\n### 2. Drug and Chemical Queries\n\n**Retrieve drug information** including pharmacogenomic annotations and mechanisms:\n\n```python\n# Get drug details\nresponse = requests.get(\"https://api.clinpgx.org/v1/chemical/PA448515\")  # Warfarin\ndrug_data = response.json()\n\n# Search drugs by name\nresponse = requests.get(\"https://api.clinpgx.org/v1/chemical\",\n                       params={\"name\": \"warfarin\"})\ndrugs = response.json()\n```\n\n**Drug categories with pharmacogenomic significance**:\n- Anticoagulants (warfarin, clopidogrel)\n- Antidepressants (SSRIs, TCAs)\n- Immunosuppressants (tacrolimus, azathioprine)\n- Oncology drugs (5-fluorouracil, irinotecan, tamoxifen)\n- Cardiovascular drugs (statins, beta-blockers)\n- Pain medications (codeine, tramadol)\n- Antivirals (abacavir)\n\n### 3. Gene-Drug Pair Queries\n\n**Access curated gene-drug relationships** with clinical annotations:\n\n```python\n# Get gene-drug pair information\nresponse = requests.get(\"https://api.clinpgx.org/v1/geneDrugPair\",\n                       params={\"gene\": \"CYP2D6\", \"drug\": \"codeine\"})\npair_data = response.json()\n\n# Get all pairs for a gene\nresponse = requests.get(\"https://api.clinpgx.org/v1/geneDrugPair\",\n                       params={\"gene\": \"CYP2C19\"})\nall_pairs = response.json()\n```\n\n**Clinical annotation sources**:\n- CPIC (Clinical Pharmacogenetics Implementation Consortium)\n- DPWG (Dutch Pharmacogenetics Working Group)\n- FDA (Food and Drug Administration) labels\n- Peer-reviewed literature summary annotations\n\n### 4. CPIC Guidelines\n\n**Access evidence-based clinical practice guidelines**:\n\n```python\n# Get CPIC guideline\nresponse = requests.get(\"https://api.clinpgx.org/v1/guideline/PA166104939\")\nguideline = response.json()\n\n# List all CPIC guidelines\nresponse = requests.get(\"https://api.clinpgx.org/v1/guideline\",\n                       params={\"source\": \"CPIC\"})\nguidelines = response.json()\n```\n\n**CPIC guideline components**:\n- Gene-drug pairs covered\n- Clinical recommendations by phenotype\n- Evidence levels and strength ratings\n- Supporting literature\n- Downloadable PDFs and supplementary materials\n- Implementation considerations\n\n**Example guidelines**:\n- CYP2D6-codeine (avoid in ultra-rapid metabolizers)\n- CYP2C19-clopidogrel (alternative therapy for poor metabolizers)\n- TPMT-azathioprine (dose reduction for intermediate/poor metabolizers)\n- DPYD-fluoropyrimidines (dose adjustment based on activity)\n- HLA-B*57:01-abacavir (avoid if positive)\n\n### 5. Allele and Variant Information\n\n**Query allele function and frequency data**:\n\n```python\n# Get allele information\nresponse = requests.get(\"https://api.clinpgx.org/v1/allele/CYP2D6*4\")\nallele_data = response.json()\n\n# Get all alleles for a gene\nresponse = requests.get(\"https://api.clinpgx.org/v1/allele\",\n                       params={\"gene\": \"CYP2D6\"})\nalleles = response.json()\n```\n\n**Allele information includes**:\n- Functional status (normal, decreased, no function, increased, uncertain)\n- Population frequencies across ethnic groups\n- Defining variants (SNPs, indels, CNVs)\n- Phenotype assignment\n- References to PharmVar and other nomenclature systems\n\n**Phenotype categories**:\n- **Ultra-rapid metabolizer** (UM): Increased enzyme activity\n- **Normal metabolizer** (NM): Normal enzyme activity\n- **Intermediate metabolizer** (IM): Reduced enzyme activity\n- **Poor metabolizer** (PM): Little to no enzyme activity\n\n### 6. Variant Annotations\n\n**Access clinical annotations for specific genetic variants**:\n\n```python\n# Get variant information\nresponse = requests.get(\"https://api.clinpgx.org/v1/variant/rs4244285\")\nvariant_data = response.json()\n\n# Search variants by position (if supported)\nresponse = requests.get(\"https://api.clinpgx.org/v1/variant\",\n                       params={\"chromosome\": \"10\", \"position\": \"94781859\"})\nvariants = response.json()\n```\n\n**Variant data includes**:\n- rsID and genomic coordinates\n- Gene and functional consequence\n- Allele associations\n- Clinical significance\n- Population frequencies\n- Literature references\n\n### 7. Clinical Annotations\n\n**Retrieve curated literature annotations** (formerly PharmGKB clinical annotations):\n\n```python\n# Get clinical annotations\nresponse = requests.get(\"https://api.clinpgx.org/v1/clinicalAnnotation\",\n                       params={\"gene\": \"CYP2D6\"})\nannotations = response.json()\n\n# Filter by evidence level\nresponse = requests.get(\"https://api.clinpgx.org/v1/clinicalAnnotation\",\n                       params={\"evidenceLevel\": \"1A\"})\nhigh_evidence = response.json()\n```\n\n**Evidence levels** (from highest to lowest):\n- **Level 1A**: High-quality evidence, CPIC/FDA/DPWG guidelines\n- **Level 1B**: High-quality evidence, not yet guideline\n- **Level 2A**: Moderate evidence from well-designed studies\n- **Level 2B**: Moderate evidence with some limitations\n- **Level 3**: Limited or conflicting evidence\n- **Level 4**: Case reports or weak evidence\n\n### 8. Drug Labels\n\n**Access pharmacogenomic information from drug labels**:\n\n```python\n# Get drug labels with PGx information\nresponse = requests.get(\"https://api.clinpgx.org/v1/drugLabel\",\n                       params={\"drug\": \"warfarin\"})\nlabels = response.json()\n\n# Filter by regulatory source\nresponse = requests.get(\"https://api.clinpgx.org/v1/drugLabel\",\n                       params={\"source\": \"FDA\"})\nfda_labels = response.json()\n```\n\n**Label information includes**:\n- Testing recommendations\n- Dosing guidance by genotype\n- Warnings and precautions\n- Biomarker information\n- Regulatory source (FDA, EMA, PMDA, etc.)\n\n### 9. Pathways\n\n**Explore pharmacokinetic and pharmacodynamic pathways**:\n\n```python\n# Get pathway information\nresponse = requests.get(\"https://api.clinpgx.org/v1/pathway/PA146123006\")  # Warfarin pathway\npathway_data = response.json()\n\n# Search pathways by drug\nresponse = requests.get(\"https://api.clinpgx.org/v1/pathway\",\n                       params={\"drug\": \"warfarin\"})\npathways = response.json()\n```\n\n**Pathway diagrams** show:\n- Drug metabolism steps\n- Enzymes and transporters involved\n- Gene variants affecting each step\n- Downstream effects on efficacy/toxicity\n- Interactions with other pathways\n\n## Query Workflow\n\n### Workflow 1: Clinical Decision Support for Drug Prescription\n\n1. **Identify patient genotype** for relevant pharmacogenes:\n   ```python\n   # Example: Patient is CYP2C19 *1/*2 (intermediate metabolizer)\n   response = requests.get(\"https://api.clinpgx.org/v1/allele/CYP2C19*2\")\n   allele_function = response.json()\n   ```\n\n2. **Query gene-drug pairs** for medication of interest:\n   ```python\n   response = requests.get(\"https://api.clinpgx.org/v1/geneDrugPair\",\n                          params={\"gene\": \"CYP2C19\", \"drug\": \"clopidogrel\"})\n   pair_info = response.json()\n   ```\n\n3. **Retrieve CPIC guideline** for dosing recommendations:\n   ```python\n   response = requests.get(\"https://api.clinpgx.org/v1/guideline\",\n                          params={\"gene\": \"CYP2C19\", \"drug\": \"clopidogrel\"})\n   guideline = response.json()\n   # Recommendation: Alternative antiplatelet therapy for IM/PM\n   ```\n\n4. **Check drug label** for regulatory guidance:\n   ```python\n   response = requests.get(\"https://api.clinpgx.org/v1/drugLabel\",\n                          params={\"drug\": \"clopidogrel\"})\n   label = response.json()\n   ```\n\n### Workflow 2: Gene Panel Analysis\n\n1. **Get list of pharmacogenes** in clinical panel:\n   ```python\n   pgx_panel = [\"CYP2C19\", \"CYP2D6\", \"CYP2C9\", \"TPMT\", \"DPYD\", \"SLCO1B1\"]\n   ```\n\n2. **For each gene, retrieve all drug interactions**:\n   ```python\n   all_interactions = {}\n   for gene in pgx_panel:\n       response = requests.get(\"https://api.clinpgx.org/v1/geneDrugPair\",\n                              params={\"gene\": gene})\n       all_interactions[gene] = response.json()\n   ```\n\n3. **Filter for CPIC guideline-level evidence**:\n   ```python\n   for gene, pairs in all_interactions.items():\n       for pair in pairs:\n           if pair.get('cpicLevel'):  # Has CPIC guideline\n               print(f\"{gene} - {pair['drug']}: {pair['cpicLevel']}\")\n   ```\n\n4. **Generate patient report** with actionable pharmacogenomic findings.\n\n### Workflow 3: Drug Safety Assessment\n\n1. **Query drug for PGx associations**:\n   ```python\n   response = requests.get(\"https://api.clinpgx.org/v1/chemical\",\n                          params={\"name\": \"abacavir\"})\n   drug_id = response.json()[0]['id']\n   ```\n\n2. **Get clinical annotations**:\n   ```python\n   response = requests.get(\"https://api.clinpgx.org/v1/clinicalAnnotation\",\n                          params={\"drug\": drug_id})\n   annotations = response.json()\n   ```\n\n3. **Check for HLA associations** and toxicity risk:\n   ```python\n   for annotation in annotations:\n       if 'HLA' in annotation.get('genes', []):\n           print(f\"Toxicity risk: {annotation['phenotype']}\")\n           print(f\"Evidence level: {annotation['evidenceLevel']}\")\n   ```\n\n4. **Retrieve screening recommendations** from guidelines and labels.\n\n### Workflow 4: Research Analysis - Population Pharmacogenomics\n\n1. **Get allele frequencies** for population comparison:\n   ```python\n   response = requests.get(\"https://api.clinpgx.org/v1/allele\",\n                          params={\"gene\": \"CYP2D6\"})\n   alleles = response.json()\n   ```\n\n2. **Extract population-specific frequencies**:\n   ```python\n   populations = ['European', 'African', 'East Asian', 'Latino']\n   frequency_data = {}\n   for allele in alleles:\n       allele_name = allele['name']\n       frequency_data[allele_name] = {\n           pop: allele.get(f'{pop}_frequency', 'N/A')\n           for pop in populations\n       }\n   ```\n\n3. **Calculate phenotype distributions** by population:\n   ```python\n   # Combine allele frequencies with function to predict phenotypes\n   phenotype_dist = calculate_phenotype_frequencies(frequency_data)\n   ```\n\n4. **Analyze implications** for drug dosing in diverse populations.\n\n### Workflow 5: Literature Evidence Review\n\n1. **Search for gene-drug pair**:\n   ```python\n   response = requests.get(\"https://api.clinpgx.org/v1/geneDrugPair\",\n                          params={\"gene\": \"TPMT\", \"drug\": \"azathioprine\"})\n   pair = response.json()\n   ```\n\n2. **Retrieve all clinical annotations**:\n   ```python\n   response = requests.get(\"https://api.clinpgx.org/v1/clinicalAnnotation\",\n                          params={\"gene\": \"TPMT\", \"drug\": \"azathioprine\"})\n   annotations = response.json()\n   ```\n\n3. **Filter by evidence level and publication date**:\n   ```python\n   high_quality = [a for a in annotations\n                   if a['evidenceLevel'] in ['1A', '1B', '2A']]\n   ```\n\n4. **Extract PMIDs** and retrieve full references:\n   ```python\n   pmids = [a['pmid'] for a in high_quality if 'pmid' in a]\n   # Use PubMed skill to retrieve full citations\n   ```\n\n## Rate Limiting and Best Practices\n\n### Rate Limit Compliance\n\n```python\nimport time\n\ndef rate_limited_request(url, params=None, delay=0.5):\n    \"\"\"Make API request with rate limiting (2 req/sec max)\"\"\"\n    response = requests.get(url, params=params)\n    time.sleep(delay)  # Wait 0.5 seconds between requests\n    return response\n\n# Use in loops\ngenes = [\"CYP2D6\", \"CYP2C19\", \"CYP2C9\"]\nfor gene in genes:\n    response = rate_limited_request(\n        \"https://api.clinpgx.org/v1/gene/\" + gene\n    )\n    data = response.json()\n```\n\n### Error Handling\n\n```python\ndef safe_api_call(url, params=None, max_retries=3):\n    \"\"\"API call with error handling and retries\"\"\"\n    for attempt in range(max_retries):\n        try:\n            response = requests.get(url, params=params, timeout=10)\n\n            if response.status_code == 200:\n                return response.json()\n            elif response.status_code == 429:\n                # Rate limit exceeded\n                wait_time = 2 ** attempt  # Exponential backoff\n                print(f\"Rate limit hit. Waiting {wait_time}s...\")\n                time.sleep(wait_time)\n            else:\n                response.raise_for_status()\n\n        except requests.exceptions.RequestException as e:\n            print(f\"Attempt {attempt + 1} failed: {e}\")\n            if attempt == max_retries - 1:\n                raise\n            time.sleep(1)\n```\n\n### Caching Results\n\n```python\nimport json\nfrom pathlib import Path\n\ndef cached_query(cache_file, api_func, *args, **kwargs):\n    \"\"\"Cache API results to avoid repeated queries\"\"\"\n    cache_path = Path(cache_file)\n\n    if cache_path.exists():\n        with open(cache_path) as f:\n            return json.load(f)\n\n    result = api_func(*args, **kwargs)\n\n    with open(cache_path, 'w') as f:\n        json.dump(result, f, indent=2)\n\n    return result\n\n# Usage\ngene_data = cached_query(\n    'cyp2d6_cache.json',\n    rate_limited_request,\n    \"https://api.clinpgx.org/v1/gene/CYP2D6\"\n)\n```\n\n## PharmDOG Tool\n\nPharmDOG (formerly DDRx) is ClinPGx's clinical decision support tool for interpreting pharmacogenomic test results:\n\n**Key features**:\n- **Phenoconversion calculator**: Adjusts phenotype predictions for drug-drug interactions affecting CYP2D6\n- **Custom genotypes**: Input patient genotypes to get phenotype predictions\n- **QR code sharing**: Generate shareable patient reports\n- **Flexible guidance sources**: Select which guidelines to apply (CPIC, DPWG, FDA)\n- **Multi-drug analysis**: Assess multiple medications simultaneously\n\n**Access**: Available at https://www.clinpgx.org/pharmacogenomic-decision-support\n\n**Use cases**:\n- Clinical interpretation of PGx panel results\n- Medication review for patients with known genotypes\n- Patient education materials\n- Point-of-care decision support\n\n## Resources\n\n### scripts/query_clinpgx.py\n\nPython script with ready-to-use functions for common ClinPGx queries:\n\n- `get_gene_info(gene_symbol)` - Retrieve gene details\n- `get_drug_info(drug_name)` - Get drug information\n- `get_gene_drug_pairs(gene, drug)` - Query gene-drug interactions\n- `get_cpic_guidelines(gene, drug)` - Retrieve CPIC guidelines\n- `get_alleles(gene)` - Get all alleles for a gene\n- `get_clinical_annotations(gene, drug, evidence_level)` - Query literature annotations\n- `get_drug_labels(drug)` - Retrieve pharmacogenomic drug labels\n- `search_variants(rsid)` - Search by variant rsID\n- `export_to_dataframe(data)` - Convert results to pandas DataFrame\n\nConsult this script for implementation examples with proper rate limiting and error handling.\n\n### references/api_reference.md\n\nComprehensive API documentation including:\n\n- Complete endpoint listing with parameters\n- Request/response format specifications\n- Example queries for each endpoint\n- Filter operators and search patterns\n- Data schema definitions\n- Rate limiting details\n- Authentication requirements (if any)\n- Troubleshooting common errors\n\nRefer to this document when detailed API information is needed or when constructing complex queries.\n\n## Important Notes\n\n### Data Sources and Integration\n\nClinPGx consolidates multiple authoritative sources:\n- **PharmGKB**: Curated pharmacogenomics knowledge base (now part of ClinPGx)\n- **CPIC**: Evidence-based clinical implementation guidelines\n- **PharmCAT**: Allele calling and phenotype interpretation tool\n- **DPWG**: Dutch pharmacogenetics guidelines\n- **FDA/EMA labels**: Regulatory pharmacogenomic information\n\nAs of July 2025, all PharmGKB URLs redirect to corresponding ClinPGx pages.\n\n### Clinical Implementation Considerations\n\n- **Evidence levels**: Always check evidence strength before clinical application\n- **Population differences**: Allele frequencies vary significantly across populations\n- **Phenoconversion**: Consider drug-drug interactions that affect enzyme activity\n- **Multi-gene effects**: Some drugs affected by multiple pharmacogenes\n- **Non-genetic factors**: Age, organ function, drug interactions also affect response\n- **Testing limitations**: Not all clinically relevant alleles detected by all assays\n\n### Data Updates\n\n- ClinPGx continuously updates with new evidence and guidelines\n- Check publication dates for clinical annotations\n- Monitor ClinPGx Blog (https://blog.clinpgx.org/) for announcements\n- CPIC guidelines updated as new evidence emerges\n- PharmVar provides nomenclature updates for allele definitions\n\n### API Stability\n\n- API endpoints are relatively stable but may change during development\n- Parameters and response formats subject to modification\n- Monitor API changelog and ClinPGx blog for updates\n- Consider version pinning for production applications\n- Test API changes in development before production deployment\n\n## Common Use Cases\n\n### Pre-emptive Pharmacogenomic Testing\n\nQuery all clinically actionable gene-drug pairs to guide panel selection:\n\n```python\n# Get all CPIC guideline pairs\nresponse = requests.get(\"https://api.clinpgx.org/v1/geneDrugPair\",\n                       params={\"cpicLevel\": \"A\"})  # Level A recommendations\nactionable_pairs = response.json()\n```\n\n### Medication Therapy Management\n\nReview patient medications against known genotypes:\n\n```python\npatient_genes = {\"CYP2C19\": \"*1/*2\", \"CYP2D6\": \"*1/*1\", \"SLCO1B1\": \"*1/*5\"}\nmedications = [\"clopidogrel\", \"simvastatin\", \"escitalopram\"]\n\nfor med in medications:\n    for gene in patient_genes:\n        response = requests.get(\"https://api.clinpgx.org/v1/geneDrugPair\",\n                               params={\"gene\": gene, \"drug\": med})\n        # Check for interactions and dosing guidance\n```\n\n### Clinical Trial Eligibility\n\nScreen for pharmacogenomic contraindications:\n\n```python\n# Check for HLA-B*57:01 before abacavir trial\nresponse = requests.get(\"https://api.clinpgx.org/v1/geneDrugPair\",\n                       params={\"gene\": \"HLA-B\", \"drug\": \"abacavir\"})\npair_info = response.json()\n# CPIC: Do not use if HLA-B*57:01 positive\n```\n\n## Additional Resources\n\n- **ClinPGx website**: https://www.clinpgx.org/\n- **ClinPGx Blog**: https://blog.clinpgx.org/\n- **API documentation**: https://api.clinpgx.org/\n- **CPIC website**: https://cpicpgx.org/\n- **PharmCAT**: https://pharmcat.clinpgx.org/\n- **ClinGen**: https://clinicalgenome.org/\n- **Contact**: api@clinpgx.org (for substantial API use)\n",
        "data/k-dense-ai/clinvar-database/SKILL.md": "---\nname: clinvar-database\ndescription: \"Query NCBI ClinVar for variant clinical significance. Search by gene/position, interpret pathogenicity classifications, access via E-utilities API or FTP, annotate VCFs, for genomic medicine.\"\n---\n\n# ClinVar Database\n\n## Overview\n\nClinVar is NCBI's freely accessible archive of reports on relationships between human genetic variants and phenotypes, with supporting evidence. The database aggregates information about genomic variation and its relationship to human health, providing standardized variant classifications used in clinical genetics and research.\n\n## When to Use This Skill\n\nThis skill should be used when:\n\n- Searching for variants by gene, condition, or clinical significance\n- Interpreting clinical significance classifications (pathogenic, benign, VUS)\n- Accessing ClinVar data programmatically via E-utilities API\n- Downloading and processing bulk data from FTP\n- Understanding review status and star ratings\n- Resolving conflicting variant interpretations\n- Annotating variant call sets with clinical significance\n\n## Core Capabilities\n\n### 1. Search and Query ClinVar\n\n#### Web Interface Queries\n\nSearch ClinVar using the web interface at https://www.ncbi.nlm.nih.gov/clinvar/\n\n**Common search patterns:**\n- By gene: `BRCA1[gene]`\n- By clinical significance: `pathogenic[CLNSIG]`\n- By condition: `breast cancer[disorder]`\n- By variant: `NM_000059.3:c.1310_1313del[variant name]`\n- By chromosome: `13[chr]`\n- Combined: `BRCA1[gene] AND pathogenic[CLNSIG]`\n\n#### Programmatic Access via E-utilities\n\nAccess ClinVar programmatically using NCBI's E-utilities API. Refer to `references/api_reference.md` for comprehensive API documentation including:\n- **esearch** - Search for variants matching criteria\n- **esummary** - Retrieve variant summaries\n- **efetch** - Download full XML records\n- **elink** - Find related records in other NCBI databases\n\n**Quick example using curl:**\n```bash\n# Search for pathogenic BRCA1 variants\ncurl \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=clinvar&term=BRCA1[gene]+AND+pathogenic[CLNSIG]&retmode=json\"\n```\n\n**Best practices:**\n- Test queries on the web interface before automating\n- Use API keys to increase rate limits from 3 to 10 requests/second\n- Implement exponential backoff for rate limit errors\n- Set `Entrez.email` when using Biopython\n\n### 2. Interpret Clinical Significance\n\n#### Understanding Classifications\n\nClinVar uses standardized terminology for variant classifications. Refer to `references/clinical_significance.md` for detailed interpretation guidelines.\n\n**Key germline classification terms (ACMG/AMP):**\n- **Pathogenic (P)** - Variant causes disease (~99% probability)\n- **Likely Pathogenic (LP)** - Variant likely causes disease (~90% probability)\n- **Uncertain Significance (VUS)** - Insufficient evidence to classify\n- **Likely Benign (LB)** - Variant likely does not cause disease\n- **Benign (B)** - Variant does not cause disease\n\n**Review status (star ratings):**\n-  Practice guideline - Highest confidence\n-  Expert panel review (e.g., ClinGen) - High confidence\n-  Multiple submitters, no conflicts - Moderate confidence\n-  Single submitter with criteria - Standard weight\n-  No assertion criteria - Low confidence\n\n**Critical considerations:**\n- Always check review status - prefer  or  ratings\n- Conflicting interpretations require manual evaluation\n- Classifications may change as new evidence emerges\n- VUS (uncertain significance) variants lack sufficient evidence for clinical use\n\n### 3. Download Bulk Data from FTP\n\n#### Access ClinVar FTP Site\n\nDownload complete datasets from `ftp://ftp.ncbi.nlm.nih.gov/pub/clinvar/`\n\nRefer to `references/data_formats.md` for comprehensive documentation on file formats and processing.\n\n**Update schedule:**\n- Monthly releases: First Thursday of each month (complete dataset, archived)\n- Weekly updates: Every Monday (incremental updates)\n\n#### Available Formats\n\n**XML files** (most comprehensive):\n- VCV (Variation) files: `xml/clinvar_variation/` - Variant-centric aggregation\n- RCV (Record) files: `xml/RCV/` - Variant-condition pairs\n- Include full submission details, evidence, and metadata\n\n**VCF files** (for genomic pipelines):\n- GRCh37: `vcf_GRCh37/clinvar.vcf.gz`\n- GRCh38: `vcf_GRCh38/clinvar.vcf.gz`\n- Limitations: Excludes variants >10kb and complex structural variants\n\n**Tab-delimited files** (for quick analysis):\n- `tab_delimited/variant_summary.txt.gz` - Summary of all variants\n- `tab_delimited/var_citations.txt.gz` - PubMed citations\n- `tab_delimited/cross_references.txt.gz` - Database cross-references\n\n**Example download:**\n```bash\n# Download latest monthly XML release\nwget ftp://ftp.ncbi.nlm.nih.gov/pub/clinvar/xml/clinvar_variation/ClinVarVariationRelease_00-latest.xml.gz\n\n# Download VCF for GRCh38\nwget ftp://ftp.ncbi.nlm.nih.gov/pub/clinvar/vcf_GRCh38/clinvar.vcf.gz\n```\n\n### 4. Process and Analyze ClinVar Data\n\n#### Working with XML Files\n\nProcess XML files to extract variant details, classifications, and evidence.\n\n**Python example with xml.etree:**\n```python\nimport gzip\nimport xml.etree.ElementTree as ET\n\nwith gzip.open('ClinVarVariationRelease.xml.gz', 'rt') as f:\n    for event, elem in ET.iterparse(f, events=('end',)):\n        if elem.tag == 'VariationArchive':\n            variation_id = elem.attrib.get('VariationID')\n            # Extract clinical significance, review status, etc.\n            elem.clear()  # Free memory\n```\n\n#### Working with VCF Files\n\nAnnotate variant calls or filter by clinical significance using bcftools or Python.\n\n**Using bcftools:**\n```bash\n# Filter pathogenic variants\nbcftools view -i 'INFO/CLNSIG~\"Pathogenic\"' clinvar.vcf.gz\n\n# Extract specific genes\nbcftools view -i 'INFO/GENEINFO~\"BRCA\"' clinvar.vcf.gz\n\n# Annotate your VCF with ClinVar\nbcftools annotate -a clinvar.vcf.gz -c INFO your_variants.vcf\n```\n\n**Using PyVCF in Python:**\n```python\nimport vcf\n\nvcf_reader = vcf.Reader(filename='clinvar.vcf.gz')\nfor record in vcf_reader:\n    clnsig = record.INFO.get('CLNSIG', [])\n    if 'Pathogenic' in clnsig:\n        gene = record.INFO.get('GENEINFO', [''])[0]\n        print(f\"{record.CHROM}:{record.POS} {gene} - {clnsig}\")\n```\n\n#### Working with Tab-Delimited Files\n\nUse pandas or command-line tools for rapid filtering and analysis.\n\n**Using pandas:**\n```python\nimport pandas as pd\n\n# Load variant summary\ndf = pd.read_csv('variant_summary.txt.gz', sep='\\t', compression='gzip')\n\n# Filter pathogenic variants in specific gene\npathogenic_brca = df[\n    (df['GeneSymbol'] == 'BRCA1') &\n    (df['ClinicalSignificance'].str.contains('Pathogenic', na=False))\n]\n\n# Count variants by clinical significance\nsig_counts = df['ClinicalSignificance'].value_counts()\n```\n\n**Using command-line tools:**\n```bash\n# Extract pathogenic variants for specific gene\nzcat variant_summary.txt.gz | \\\n  awk -F'\\t' '$7==\"TP53\" && $13~\"Pathogenic\"' | \\\n  cut -f1,5,7,13,14\n```\n\n### 5. Handle Conflicting Interpretations\n\nWhen multiple submitters provide different classifications for the same variant, ClinVar reports \"Conflicting interpretations of pathogenicity.\"\n\n**Resolution strategy:**\n1. Check review status (star rating) - higher ratings carry more weight\n2. Examine evidence and assertion criteria from each submitter\n3. Consider submission dates - newer submissions may reflect updated evidence\n4. Review population frequency data (e.g., gnomAD) for context\n5. Consult expert panel classifications () when available\n6. For clinical use, always defer to a genetics professional\n\n**Search query to exclude conflicts:**\n```\nTP53[gene] AND pathogenic[CLNSIG] NOT conflicting[RVSTAT]\n```\n\n### 6. Track Classification Updates\n\nVariant classifications may change over time as new evidence emerges.\n\n**Why classifications change:**\n- New functional studies or clinical data\n- Updated population frequency information\n- Revised ACMG/AMP guidelines\n- Segregation data from additional families\n\n**Best practices:**\n- Document ClinVar version and access date for reproducibility\n- Re-check classifications periodically for critical variants\n- Subscribe to ClinVar mailing list for major updates\n- Use monthly archived releases for stable datasets\n\n### 7. Submit Data to ClinVar\n\nOrganizations can submit variant interpretations to ClinVar.\n\n**Submission methods:**\n- Web submission portal: https://submit.ncbi.nlm.nih.gov/subs/clinvar/\n- API submission (requires service account): See `references/api_reference.md`\n- Batch submission via Excel templates\n\n**Requirements:**\n- Organizational account with NCBI\n- Assertion criteria (preferably ACMG/AMP guidelines)\n- Supporting evidence for classification\n\nContact: clinvar@ncbi.nlm.nih.gov for submission account setup.\n\n## Workflow Examples\n\n### Example 1: Identify High-Confidence Pathogenic Variants in a Gene\n\n**Objective:** Find pathogenic variants in CFTR gene with expert panel review.\n\n**Steps:**\n1. Search using web interface or E-utilities:\n   ```\n   CFTR[gene] AND pathogenic[CLNSIG] AND (reviewed by expert panel[RVSTAT] OR practice guideline[RVSTAT])\n   ```\n2. Review results, noting review status (should be  or )\n3. Export variant list or retrieve full records via efetch\n4. Cross-reference with clinical presentation if applicable\n\n### Example 2: Annotate VCF with ClinVar Classifications\n\n**Objective:** Add clinical significance annotations to variant calls.\n\n**Steps:**\n1. Download appropriate ClinVar VCF (match genome build: GRCh37 or GRCh38):\n   ```bash\n   wget ftp://ftp.ncbi.nlm.nih.gov/pub/clinvar/vcf_GRCh38/clinvar.vcf.gz\n   wget ftp://ftp.ncbi.nlm.nih.gov/pub/clinvar/vcf_GRCh38/clinvar.vcf.gz.tbi\n   ```\n2. Annotate using bcftools:\n   ```bash\n   bcftools annotate -a clinvar.vcf.gz \\\n     -c INFO/CLNSIG,INFO/CLNDN,INFO/CLNREVSTAT \\\n     -o annotated_variants.vcf \\\n     your_variants.vcf\n   ```\n3. Filter annotated VCF for pathogenic variants:\n   ```bash\n   bcftools view -i 'INFO/CLNSIG~\"Pathogenic\"' annotated_variants.vcf\n   ```\n\n### Example 3: Analyze Variants for a Specific Disease\n\n**Objective:** Study all variants associated with hereditary breast cancer.\n\n**Steps:**\n1. Search by condition:\n   ```\n   hereditary breast cancer[disorder] OR \"Breast-ovarian cancer, familial\"[disorder]\n   ```\n2. Download results as CSV or retrieve via E-utilities\n3. Filter by review status to prioritize high-confidence variants\n4. Analyze distribution across genes (BRCA1, BRCA2, PALB2, etc.)\n5. Examine variants with conflicting interpretations separately\n\n### Example 4: Bulk Download and Database Construction\n\n**Objective:** Build a local ClinVar database for analysis pipeline.\n\n**Steps:**\n1. Download monthly release for reproducibility:\n   ```bash\n   wget ftp://ftp.ncbi.nlm.nih.gov/pub/clinvar/xml/clinvar_variation/ClinVarVariationRelease_YYYY-MM.xml.gz\n   ```\n2. Parse XML and load into database (PostgreSQL, MySQL, MongoDB)\n3. Index by gene, position, clinical significance, review status\n4. Implement version tracking for updates\n5. Schedule monthly updates from FTP site\n\n## Important Limitations and Considerations\n\n### Data Quality\n- **Not all submissions have equal weight** - Check review status (star ratings)\n- **Conflicting interpretations exist** - Require manual evaluation\n- **Historical submissions may be outdated** - Newer data may be more accurate\n- **VUS classification is not a clinical diagnosis** - Means insufficient evidence\n\n### Scope Limitations\n- **Not for direct clinical diagnosis** - Always involve genetics professional\n- **Population-specific** - Variant frequencies vary by ancestry\n- **Incomplete coverage** - Not all genes or variants are well-studied\n- **Version dependencies** - Coordinate genome build (GRCh37/GRCh38) across analyses\n\n### Technical Limitations\n- **VCF files exclude large variants** - Variants >10kb not in VCF format\n- **Rate limits on API** - 3 req/sec without key, 10 req/sec with API key\n- **File sizes** - Full XML releases are multi-GB compressed files\n- **No real-time updates** - Website updated weekly, FTP monthly/weekly\n\n## Resources\n\n### Reference Documentation\n\nThis skill includes comprehensive reference documentation:\n\n- **`references/api_reference.md`** - Complete E-utilities API documentation with examples for esearch, esummary, efetch, and elink; includes rate limits, authentication, and Python/Biopython code samples\n\n- **`references/clinical_significance.md`** - Detailed guide to interpreting clinical significance classifications, review status star ratings, conflict resolution, and best practices for variant interpretation\n\n- **`references/data_formats.md`** - Documentation for XML, VCF, and tab-delimited file formats; FTP directory structure, processing examples, and format selection guidance\n\n### External Resources\n\n- ClinVar home: https://www.ncbi.nlm.nih.gov/clinvar/\n- ClinVar documentation: https://www.ncbi.nlm.nih.gov/clinvar/docs/\n- E-utilities documentation: https://www.ncbi.nlm.nih.gov/books/NBK25501/\n- ACMG variant interpretation guidelines: Richards et al., 2015 (PMID: 25741868)\n- ClinGen expert panels: https://clinicalgenome.org/\n\n### Contact\n\nFor questions about ClinVar or data submission: clinvar@ncbi.nlm.nih.gov\n",
        "data/k-dense-ai/cobrapy/SKILL.md": "---\nname: cobrapy\ndescription: \"Constraint-based metabolic modeling (COBRA). FBA, FVA, gene knockouts, flux sampling, SBML models, for systems biology and metabolic engineering analysis.\"\n---\n\n# COBRApy - Constraint-Based Reconstruction and Analysis\n\n## Overview\n\nCOBRApy is a Python library for constraint-based reconstruction and analysis (COBRA) of metabolic models, essential for systems biology research. Work with genome-scale metabolic models, perform computational simulations of cellular metabolism, conduct metabolic engineering analyses, and predict phenotypic behaviors.\n\n## Core Capabilities\n\nCOBRApy provides comprehensive tools organized into several key areas:\n\n### 1. Model Management\n\nLoad existing models from repositories or files:\n```python\nfrom cobra.io import load_model\n\n# Load bundled test models\nmodel = load_model(\"textbook\")  # E. coli core model\nmodel = load_model(\"ecoli\")     # Full E. coli model\nmodel = load_model(\"salmonella\")\n\n# Load from files\nfrom cobra.io import read_sbml_model, load_json_model, load_yaml_model\nmodel = read_sbml_model(\"path/to/model.xml\")\nmodel = load_json_model(\"path/to/model.json\")\nmodel = load_yaml_model(\"path/to/model.yml\")\n```\n\nSave models in various formats:\n```python\nfrom cobra.io import write_sbml_model, save_json_model, save_yaml_model\nwrite_sbml_model(model, \"output.xml\")  # Preferred format\nsave_json_model(model, \"output.json\")  # For Escher compatibility\nsave_yaml_model(model, \"output.yml\")   # Human-readable\n```\n\n### 2. Model Structure and Components\n\nAccess and inspect model components:\n```python\n# Access components\nmodel.reactions      # DictList of all reactions\nmodel.metabolites    # DictList of all metabolites\nmodel.genes          # DictList of all genes\n\n# Get specific items by ID or index\nreaction = model.reactions.get_by_id(\"PFK\")\nmetabolite = model.metabolites[0]\n\n# Inspect properties\nprint(reaction.reaction)        # Stoichiometric equation\nprint(reaction.bounds)          # Flux constraints\nprint(reaction.gene_reaction_rule)  # GPR logic\nprint(metabolite.formula)       # Chemical formula\nprint(metabolite.compartment)   # Cellular location\n```\n\n### 3. Flux Balance Analysis (FBA)\n\nPerform standard FBA simulation:\n```python\n# Basic optimization\nsolution = model.optimize()\nprint(f\"Objective value: {solution.objective_value}\")\nprint(f\"Status: {solution.status}\")\n\n# Access fluxes\nprint(solution.fluxes[\"PFK\"])\nprint(solution.fluxes.head())\n\n# Fast optimization (objective value only)\nobjective_value = model.slim_optimize()\n\n# Change objective\nmodel.objective = \"ATPM\"\nsolution = model.optimize()\n```\n\nParsimonious FBA (minimize total flux):\n```python\nfrom cobra.flux_analysis import pfba\nsolution = pfba(model)\n```\n\nGeometric FBA (find central solution):\n```python\nfrom cobra.flux_analysis import geometric_fba\nsolution = geometric_fba(model)\n```\n\n### 4. Flux Variability Analysis (FVA)\n\nDetermine flux ranges for all reactions:\n```python\nfrom cobra.flux_analysis import flux_variability_analysis\n\n# Standard FVA\nfva_result = flux_variability_analysis(model)\n\n# FVA at 90% optimality\nfva_result = flux_variability_analysis(model, fraction_of_optimum=0.9)\n\n# Loopless FVA (eliminates thermodynamically infeasible loops)\nfva_result = flux_variability_analysis(model, loopless=True)\n\n# FVA for specific reactions\nfva_result = flux_variability_analysis(\n    model,\n    reaction_list=[\"PFK\", \"FBA\", \"PGI\"]\n)\n```\n\n### 5. Gene and Reaction Deletion Studies\n\nPerform knockout analyses:\n```python\nfrom cobra.flux_analysis import (\n    single_gene_deletion,\n    single_reaction_deletion,\n    double_gene_deletion,\n    double_reaction_deletion\n)\n\n# Single deletions\ngene_results = single_gene_deletion(model)\nreaction_results = single_reaction_deletion(model)\n\n# Double deletions (uses multiprocessing)\ndouble_gene_results = double_gene_deletion(\n    model,\n    processes=4  # Number of CPU cores\n)\n\n# Manual knockout using context manager\nwith model:\n    model.genes.get_by_id(\"b0008\").knock_out()\n    solution = model.optimize()\n    print(f\"Growth after knockout: {solution.objective_value}\")\n# Model automatically reverts after context exit\n```\n\n### 6. Growth Media and Minimal Media\n\nManage growth medium:\n```python\n# View current medium\nprint(model.medium)\n\n# Modify medium (must reassign entire dict)\nmedium = model.medium\nmedium[\"EX_glc__D_e\"] = 10.0  # Set glucose uptake\nmedium[\"EX_o2_e\"] = 0.0       # Anaerobic conditions\nmodel.medium = medium\n\n# Calculate minimal media\nfrom cobra.medium import minimal_medium\n\n# Minimize total import flux\nmin_medium = minimal_medium(model, minimize_components=False)\n\n# Minimize number of components (uses MILP, slower)\nmin_medium = minimal_medium(\n    model,\n    minimize_components=True,\n    open_exchanges=True\n)\n```\n\n### 7. Flux Sampling\n\nSample the feasible flux space:\n```python\nfrom cobra.sampling import sample\n\n# Sample using OptGP (default, supports parallel processing)\nsamples = sample(model, n=1000, method=\"optgp\", processes=4)\n\n# Sample using ACHR\nsamples = sample(model, n=1000, method=\"achr\")\n\n# Validate samples\nfrom cobra.sampling import OptGPSampler\nsampler = OptGPSampler(model, processes=4)\nsampler.sample(1000)\nvalidation = sampler.validate(sampler.samples)\nprint(validation.value_counts())  # Should be all 'v' for valid\n```\n\n### 8. Production Envelopes\n\nCalculate phenotype phase planes:\n```python\nfrom cobra.flux_analysis import production_envelope\n\n# Standard production envelope\nenvelope = production_envelope(\n    model,\n    reactions=[\"EX_glc__D_e\", \"EX_o2_e\"],\n    objective=\"EX_ac_e\"  # Acetate production\n)\n\n# With carbon yield\nenvelope = production_envelope(\n    model,\n    reactions=[\"EX_glc__D_e\", \"EX_o2_e\"],\n    carbon_sources=\"EX_glc__D_e\"\n)\n\n# Visualize (use matplotlib or pandas plotting)\nimport matplotlib.pyplot as plt\nenvelope.plot(x=\"EX_glc__D_e\", y=\"EX_o2_e\", kind=\"scatter\")\nplt.show()\n```\n\n### 9. Gapfilling\n\nAdd reactions to make models feasible:\n```python\nfrom cobra.flux_analysis import gapfill\n\n# Prepare universal model with candidate reactions\nuniversal = load_model(\"universal\")\n\n# Perform gapfilling\nwith model:\n    # Remove reactions to create gaps for demonstration\n    model.remove_reactions([model.reactions.PGI])\n\n    # Find reactions needed\n    solution = gapfill(model, universal)\n    print(f\"Reactions to add: {solution}\")\n```\n\n### 10. Model Building\n\nBuild models from scratch:\n```python\nfrom cobra import Model, Reaction, Metabolite\n\n# Create model\nmodel = Model(\"my_model\")\n\n# Create metabolites\natp_c = Metabolite(\"atp_c\", formula=\"C10H12N5O13P3\",\n                   name=\"ATP\", compartment=\"c\")\nadp_c = Metabolite(\"adp_c\", formula=\"C10H12N5O10P2\",\n                   name=\"ADP\", compartment=\"c\")\npi_c = Metabolite(\"pi_c\", formula=\"HO4P\",\n                  name=\"Phosphate\", compartment=\"c\")\n\n# Create reaction\nreaction = Reaction(\"ATPASE\")\nreaction.name = \"ATP hydrolysis\"\nreaction.subsystem = \"Energy\"\nreaction.lower_bound = 0.0\nreaction.upper_bound = 1000.0\n\n# Add metabolites with stoichiometry\nreaction.add_metabolites({\n    atp_c: -1.0,\n    adp_c: 1.0,\n    pi_c: 1.0\n})\n\n# Add gene-reaction rule\nreaction.gene_reaction_rule = \"(gene1 and gene2) or gene3\"\n\n# Add to model\nmodel.add_reactions([reaction])\n\n# Add boundary reactions\nmodel.add_boundary(atp_c, type=\"exchange\")\nmodel.add_boundary(adp_c, type=\"demand\")\n\n# Set objective\nmodel.objective = \"ATPASE\"\n```\n\n## Common Workflows\n\n### Workflow 1: Load Model and Predict Growth\n\n```python\nfrom cobra.io import load_model\n\n# Load model\nmodel = load_model(\"ecoli\")\n\n# Run FBA\nsolution = model.optimize()\nprint(f\"Growth rate: {solution.objective_value:.3f} /h\")\n\n# Show active pathways\nprint(solution.fluxes[solution.fluxes.abs() > 1e-6])\n```\n\n### Workflow 2: Gene Knockout Screen\n\n```python\nfrom cobra.io import load_model\nfrom cobra.flux_analysis import single_gene_deletion\n\n# Load model\nmodel = load_model(\"ecoli\")\n\n# Perform single gene deletions\nresults = single_gene_deletion(model)\n\n# Find essential genes (growth < threshold)\nessential_genes = results[results[\"growth\"] < 0.01]\nprint(f\"Found {len(essential_genes)} essential genes\")\n\n# Find genes with minimal impact\nneutral_genes = results[results[\"growth\"] > 0.9 * solution.objective_value]\n```\n\n### Workflow 3: Media Optimization\n\n```python\nfrom cobra.io import load_model\nfrom cobra.medium import minimal_medium\n\n# Load model\nmodel = load_model(\"ecoli\")\n\n# Calculate minimal medium for 50% of max growth\ntarget_growth = model.slim_optimize() * 0.5\nmin_medium = minimal_medium(\n    model,\n    target_growth,\n    minimize_components=True\n)\n\nprint(f\"Minimal medium components: {len(min_medium)}\")\nprint(min_medium)\n```\n\n### Workflow 4: Flux Uncertainty Analysis\n\n```python\nfrom cobra.io import load_model\nfrom cobra.flux_analysis import flux_variability_analysis\nfrom cobra.sampling import sample\n\n# Load model\nmodel = load_model(\"ecoli\")\n\n# First check flux ranges at optimality\nfva = flux_variability_analysis(model, fraction_of_optimum=1.0)\n\n# For reactions with large ranges, sample to understand distribution\nsamples = sample(model, n=1000)\n\n# Analyze specific reaction\nreaction_id = \"PFK\"\nimport matplotlib.pyplot as plt\nsamples[reaction_id].hist(bins=50)\nplt.xlabel(f\"Flux through {reaction_id}\")\nplt.ylabel(\"Frequency\")\nplt.show()\n```\n\n### Workflow 5: Context Manager for Temporary Changes\n\nUse context managers to make temporary modifications:\n```python\n# Model remains unchanged outside context\nwith model:\n    # Temporarily change objective\n    model.objective = \"ATPM\"\n\n    # Temporarily modify bounds\n    model.reactions.EX_glc__D_e.lower_bound = -5.0\n\n    # Temporarily knock out genes\n    model.genes.b0008.knock_out()\n\n    # Optimize with changes\n    solution = model.optimize()\n    print(f\"Modified growth: {solution.objective_value}\")\n\n# All changes automatically reverted\nsolution = model.optimize()\nprint(f\"Original growth: {solution.objective_value}\")\n```\n\n## Key Concepts\n\n### DictList Objects\nModels use `DictList` objects for reactions, metabolites, and genes - behaving like both lists and dictionaries:\n```python\n# Access by index\nfirst_reaction = model.reactions[0]\n\n# Access by ID\npfk = model.reactions.get_by_id(\"PFK\")\n\n# Query methods\natp_reactions = model.reactions.query(\"atp\")\n```\n\n### Flux Constraints\nReaction bounds define feasible flux ranges:\n- **Irreversible**: `lower_bound = 0, upper_bound > 0`\n- **Reversible**: `lower_bound < 0, upper_bound > 0`\n- Set both bounds simultaneously with `.bounds` to avoid inconsistencies\n\n### Gene-Reaction Rules (GPR)\nBoolean logic linking genes to reactions:\n```python\n# AND logic (both required)\nreaction.gene_reaction_rule = \"gene1 and gene2\"\n\n# OR logic (either sufficient)\nreaction.gene_reaction_rule = \"gene1 or gene2\"\n\n# Complex logic\nreaction.gene_reaction_rule = \"(gene1 and gene2) or (gene3 and gene4)\"\n```\n\n### Exchange Reactions\nSpecial reactions representing metabolite import/export:\n- Named with prefix `EX_` by convention\n- Positive flux = secretion, negative flux = uptake\n- Managed through `model.medium` dictionary\n\n## Best Practices\n\n1. **Use context managers** for temporary modifications to avoid state management issues\n2. **Validate models** before analysis using `model.slim_optimize()` to ensure feasibility\n3. **Check solution status** after optimization - `optimal` indicates successful solve\n4. **Use loopless FVA** when thermodynamic feasibility matters\n5. **Set fraction_of_optimum** appropriately in FVA to explore suboptimal space\n6. **Parallelize** computationally expensive operations (sampling, double deletions)\n7. **Prefer SBML format** for model exchange and long-term storage\n8. **Use slim_optimize()** when only objective value needed for performance\n9. **Validate flux samples** to ensure numerical stability\n\n## Troubleshooting\n\n**Infeasible solutions**: Check medium constraints, reaction bounds, and model consistency\n**Slow optimization**: Try different solvers (GLPK, CPLEX, Gurobi) via `model.solver`\n**Unbounded solutions**: Verify exchange reactions have appropriate upper bounds\n**Import errors**: Ensure correct file format and valid SBML identifiers\n\n## References\n\nFor detailed workflows and API patterns, refer to:\n- `references/workflows.md` - Comprehensive step-by-step workflow examples\n- `references/api_quick_reference.md` - Common function signatures and patterns\n\nOfficial documentation: https://cobrapy.readthedocs.io/en/latest/\n",
        "data/k-dense-ai/cosmic-database/SKILL.md": "---\nname: cosmic-database\ndescription: \"Access COSMIC cancer mutation database. Query somatic mutations, Cancer Gene Census, mutational signatures, gene fusions, for cancer research and precision oncology. Requires authentication.\"\n---\n\n# COSMIC Database\n\n## Overview\n\nCOSMIC (Catalogue of Somatic Mutations in Cancer) is the world's largest and most comprehensive database for exploring somatic mutations in human cancer. Access COSMIC's extensive collection of cancer genomics data, including millions of mutations across thousands of cancer types, curated gene lists, mutational signatures, and clinical annotations programmatically.\n\n## When to Use This Skill\n\nThis skill should be used when:\n- Downloading cancer mutation data from COSMIC\n- Accessing the Cancer Gene Census for curated cancer gene lists\n- Retrieving mutational signature profiles\n- Querying structural variants, copy number alterations, or gene fusions\n- Analyzing drug resistance mutations\n- Working with cancer cell line genomics data\n- Integrating cancer mutation data into bioinformatics pipelines\n- Researching specific genes or mutations in cancer contexts\n\n## Prerequisites\n\n### Account Registration\nCOSMIC requires authentication for data downloads:\n- **Academic users**: Free access with registration at https://cancer.sanger.ac.uk/cosmic/register\n- **Commercial users**: License required (contact QIAGEN)\n\n### Python Requirements\n```bash\nuv pip install requests pandas\n```\n\n## Quick Start\n\n### 1. Basic File Download\n\nUse the `scripts/download_cosmic.py` script to download COSMIC data files:\n\n```python\nfrom scripts.download_cosmic import download_cosmic_file\n\n# Download mutation data\ndownload_cosmic_file(\n    email=\"your_email@institution.edu\",\n    password=\"your_password\",\n    filepath=\"GRCh38/cosmic/latest/CosmicMutantExport.tsv.gz\",\n    output_filename=\"cosmic_mutations.tsv.gz\"\n)\n```\n\n### 2. Command-Line Usage\n\n```bash\n# Download using shorthand data type\npython scripts/download_cosmic.py user@email.com --data-type mutations\n\n# Download specific file\npython scripts/download_cosmic.py user@email.com \\\n    --filepath GRCh38/cosmic/latest/cancer_gene_census.csv\n\n# Download for specific genome assembly\npython scripts/download_cosmic.py user@email.com \\\n    --data-type gene_census --assembly GRCh37 -o cancer_genes.csv\n```\n\n### 3. Working with Downloaded Data\n\n```python\nimport pandas as pd\n\n# Read mutation data\nmutations = pd.read_csv('cosmic_mutations.tsv.gz', sep='\\t', compression='gzip')\n\n# Read Cancer Gene Census\ngene_census = pd.read_csv('cancer_gene_census.csv')\n\n# Read VCF format\nimport pysam\nvcf = pysam.VariantFile('CosmicCodingMuts.vcf.gz')\n```\n\n## Available Data Types\n\n### Core Mutations\nDownload comprehensive mutation data including point mutations, indels, and genomic annotations.\n\n**Common data types**:\n- `mutations` - Complete coding mutations (TSV format)\n- `mutations_vcf` - Coding mutations in VCF format\n- `sample_info` - Sample metadata and tumor information\n\n```python\n# Download all coding mutations\ndownload_cosmic_file(\n    email=\"user@email.com\",\n    password=\"password\",\n    filepath=\"GRCh38/cosmic/latest/CosmicMutantExport.tsv.gz\"\n)\n```\n\n### Cancer Gene Census\nAccess the expert-curated list of ~700+ cancer genes with substantial evidence of cancer involvement.\n\n```python\n# Download Cancer Gene Census\ndownload_cosmic_file(\n    email=\"user@email.com\",\n    password=\"password\",\n    filepath=\"GRCh38/cosmic/latest/cancer_gene_census.csv\"\n)\n```\n\n**Use cases**:\n- Identifying known cancer genes\n- Filtering variants by cancer relevance\n- Understanding gene roles (oncogene vs tumor suppressor)\n- Target gene selection for research\n\n### Mutational Signatures\nDownload signature profiles for mutational signature analysis.\n\n```python\n# Download signature definitions\ndownload_cosmic_file(\n    email=\"user@email.com\",\n    password=\"password\",\n    filepath=\"signatures/signatures.tsv\"\n)\n```\n\n**Signature types**:\n- Single Base Substitution (SBS) signatures\n- Doublet Base Substitution (DBS) signatures\n- Insertion/Deletion (ID) signatures\n\n### Structural Variants and Fusions\nAccess gene fusion data and structural rearrangements.\n\n**Available data types**:\n- `structural_variants` - Structural breakpoints\n- `fusion_genes` - Gene fusion events\n\n```python\n# Download gene fusions\ndownload_cosmic_file(\n    email=\"user@email.com\",\n    password=\"password\",\n    filepath=\"GRCh38/cosmic/latest/CosmicFusionExport.tsv.gz\"\n)\n```\n\n### Copy Number and Expression\nRetrieve copy number alterations and gene expression data.\n\n**Available data types**:\n- `copy_number` - Copy number gains/losses\n- `gene_expression` - Over/under-expression data\n\n```python\n# Download copy number data\ndownload_cosmic_file(\n    email=\"user@email.com\",\n    password=\"password\",\n    filepath=\"GRCh38/cosmic/latest/CosmicCompleteCNA.tsv.gz\"\n)\n```\n\n### Resistance Mutations\nAccess drug resistance mutation data with clinical annotations.\n\n```python\n# Download resistance mutations\ndownload_cosmic_file(\n    email=\"user@email.com\",\n    password=\"password\",\n    filepath=\"GRCh38/cosmic/latest/CosmicResistanceMutations.tsv.gz\"\n)\n```\n\n## Working with COSMIC Data\n\n### Genome Assemblies\nCOSMIC provides data for two reference genomes:\n- **GRCh38** (recommended, current standard)\n- **GRCh37** (legacy, for older pipelines)\n\nSpecify the assembly in file paths:\n```python\n# GRCh38 (recommended)\nfilepath=\"GRCh38/cosmic/latest/CosmicMutantExport.tsv.gz\"\n\n# GRCh37 (legacy)\nfilepath=\"GRCh37/cosmic/latest/CosmicMutantExport.tsv.gz\"\n```\n\n### Versioning\n- Use `latest` in file paths to always get the most recent release\n- COSMIC is updated quarterly (current version: v102, May 2025)\n- Specific versions can be used for reproducibility: `v102`, `v101`, etc.\n\n### File Formats\n- **TSV/CSV**: Tab/comma-separated, gzip compressed, read with pandas\n- **VCF**: Standard variant format, use with pysam, bcftools, or GATK\n- All files include headers describing column contents\n\n### Common Analysis Patterns\n\n**Filter mutations by gene**:\n```python\nimport pandas as pd\n\nmutations = pd.read_csv('cosmic_mutations.tsv.gz', sep='\\t', compression='gzip')\ntp53_mutations = mutations[mutations['Gene name'] == 'TP53']\n```\n\n**Identify cancer genes by role**:\n```python\ngene_census = pd.read_csv('cancer_gene_census.csv')\noncogenes = gene_census[gene_census['Role in Cancer'].str.contains('oncogene', na=False)]\ntumor_suppressors = gene_census[gene_census['Role in Cancer'].str.contains('TSG', na=False)]\n```\n\n**Extract mutations by cancer type**:\n```python\nmutations = pd.read_csv('cosmic_mutations.tsv.gz', sep='\\t', compression='gzip')\nlung_mutations = mutations[mutations['Primary site'] == 'lung']\n```\n\n**Work with VCF files**:\n```python\nimport pysam\n\nvcf = pysam.VariantFile('CosmicCodingMuts.vcf.gz')\nfor record in vcf.fetch('17', 7577000, 7579000):  # TP53 region\n    print(record.id, record.ref, record.alts, record.info)\n```\n\n## Data Reference\n\nFor comprehensive information about COSMIC data structure, available files, and field descriptions, see `references/cosmic_data_reference.md`. This reference includes:\n\n- Complete list of available data types and files\n- Detailed field descriptions for each file type\n- File format specifications\n- Common file paths and naming conventions\n- Data update schedule and versioning\n- Citation information\n\nUse this reference when:\n- Exploring what data is available in COSMIC\n- Understanding specific field meanings\n- Determining the correct file path for a data type\n- Planning analysis workflows with COSMIC data\n\n## Helper Functions\n\nThe download script includes helper functions for common operations:\n\n### Get Common File Paths\n```python\nfrom scripts.download_cosmic import get_common_file_path\n\n# Get path for mutations file\npath = get_common_file_path('mutations', genome_assembly='GRCh38')\n# Returns: 'GRCh38/cosmic/latest/CosmicMutantExport.tsv.gz'\n\n# Get path for gene census\npath = get_common_file_path('gene_census')\n# Returns: 'GRCh38/cosmic/latest/cancer_gene_census.csv'\n```\n\n**Available shortcuts**:\n- `mutations` - Core coding mutations\n- `mutations_vcf` - VCF format mutations\n- `gene_census` - Cancer Gene Census\n- `resistance_mutations` - Drug resistance data\n- `structural_variants` - Structural variants\n- `gene_expression` - Expression data\n- `copy_number` - Copy number alterations\n- `fusion_genes` - Gene fusions\n- `signatures` - Mutational signatures\n- `sample_info` - Sample metadata\n\n## Troubleshooting\n\n### Authentication Errors\n- Verify email and password are correct\n- Ensure account is registered at cancer.sanger.ac.uk/cosmic\n- Check if commercial license is required for your use case\n\n### File Not Found\n- Verify the filepath is correct\n- Check that the requested version exists\n- Use `latest` for the most recent version\n- Confirm genome assembly (GRCh37 vs GRCh38) is correct\n\n### Large File Downloads\n- COSMIC files can be several GB in size\n- Ensure sufficient disk space\n- Download may take several minutes depending on connection\n- The script shows download progress for large files\n\n### Commercial Use\n- Commercial users must license COSMIC through QIAGEN\n- Contact: cosmic-translation@sanger.ac.uk\n- Academic access is free but requires registration\n\n## Integration with Other Tools\n\nCOSMIC data integrates well with:\n- **Variant annotation**: VEP, ANNOVAR, SnpEff\n- **Signature analysis**: SigProfiler, deconstructSigs, MuSiCa\n- **Cancer genomics**: cBioPortal, OncoKB, CIViC\n- **Bioinformatics**: Bioconductor, TCGA analysis tools\n- **Data science**: pandas, scikit-learn, PyTorch\n\n## Additional Resources\n\n- **COSMIC Website**: https://cancer.sanger.ac.uk/cosmic\n- **Documentation**: https://cancer.sanger.ac.uk/cosmic/help\n- **Release Notes**: https://cancer.sanger.ac.uk/cosmic/release_notes\n- **Contact**: cosmic@sanger.ac.uk\n\n## Citation\n\nWhen using COSMIC data, cite:\nTate JG, Bamford S, Jubb HC, et al. COSMIC: the Catalogue Of Somatic Mutations In Cancer. Nucleic Acids Research. 2019;47(D1):D941-D947.\n",
        "data/k-dense-ai/dask/SKILL.md": "---\nname: dask\ndescription: \"Parallel/distributed computing. Scale pandas/NumPy beyond memory, parallel DataFrames/Arrays, multi-file processing, task graphs, for larger-than-RAM datasets and parallel workflows.\"\n---\n\n# Dask\n\n## Overview\n\nDask is a Python library for parallel and distributed computing that enables three critical capabilities:\n- **Larger-than-memory execution** on single machines for data exceeding available RAM\n- **Parallel processing** for improved computational speed across multiple cores\n- **Distributed computation** supporting terabyte-scale datasets across multiple machines\n\nDask scales from laptops (processing ~100 GiB) to clusters (processing ~100 TiB) while maintaining familiar Python APIs.\n\n## When to Use This Skill\n\nThis skill should be used when:\n- Process datasets that exceed available RAM\n- Scale pandas or NumPy operations to larger datasets\n- Parallelize computations for performance improvements\n- Process multiple files efficiently (CSVs, Parquet, JSON, text logs)\n- Build custom parallel workflows with task dependencies\n- Distribute workloads across multiple cores or machines\n\n## Core Capabilities\n\nDask provides five main components, each suited to different use cases:\n\n### 1. DataFrames - Parallel Pandas Operations\n\n**Purpose**: Scale pandas operations to larger datasets through parallel processing.\n\n**When to Use**:\n- Tabular data exceeds available RAM\n- Need to process multiple CSV/Parquet files together\n- Pandas operations are slow and need parallelization\n- Scaling from pandas prototype to production\n\n**Reference Documentation**: For comprehensive guidance on Dask DataFrames, refer to `references/dataframes.md` which includes:\n- Reading data (single files, multiple files, glob patterns)\n- Common operations (filtering, groupby, joins, aggregations)\n- Custom operations with `map_partitions`\n- Performance optimization tips\n- Common patterns (ETL, time series, multi-file processing)\n\n**Quick Example**:\n```python\nimport dask.dataframe as dd\n\n# Read multiple files as single DataFrame\nddf = dd.read_csv('data/2024-*.csv')\n\n# Operations are lazy until compute()\nfiltered = ddf[ddf['value'] > 100]\nresult = filtered.groupby('category').mean().compute()\n```\n\n**Key Points**:\n- Operations are lazy (build task graph) until `.compute()` called\n- Use `map_partitions` for efficient custom operations\n- Convert to DataFrame early when working with structured data from other sources\n\n### 2. Arrays - Parallel NumPy Operations\n\n**Purpose**: Extend NumPy capabilities to datasets larger than memory using blocked algorithms.\n\n**When to Use**:\n- Arrays exceed available RAM\n- NumPy operations need parallelization\n- Working with scientific datasets (HDF5, Zarr, NetCDF)\n- Need parallel linear algebra or array operations\n\n**Reference Documentation**: For comprehensive guidance on Dask Arrays, refer to `references/arrays.md` which includes:\n- Creating arrays (from NumPy, random, from disk)\n- Chunking strategies and optimization\n- Common operations (arithmetic, reductions, linear algebra)\n- Custom operations with `map_blocks`\n- Integration with HDF5, Zarr, and XArray\n\n**Quick Example**:\n```python\nimport dask.array as da\n\n# Create large array with chunks\nx = da.random.random((100000, 100000), chunks=(10000, 10000))\n\n# Operations are lazy\ny = x + 100\nz = y.mean(axis=0)\n\n# Compute result\nresult = z.compute()\n```\n\n**Key Points**:\n- Chunk size is critical (aim for ~100 MB per chunk)\n- Operations work on chunks in parallel\n- Rechunk data when needed for efficient operations\n- Use `map_blocks` for operations not available in Dask\n\n### 3. Bags - Parallel Processing of Unstructured Data\n\n**Purpose**: Process unstructured or semi-structured data (text, JSON, logs) with functional operations.\n\n**When to Use**:\n- Processing text files, logs, or JSON records\n- Data cleaning and ETL before structured analysis\n- Working with Python objects that don't fit array/dataframe formats\n- Need memory-efficient streaming processing\n\n**Reference Documentation**: For comprehensive guidance on Dask Bags, refer to `references/bags.md` which includes:\n- Reading text and JSON files\n- Functional operations (map, filter, fold, groupby)\n- Converting to DataFrames\n- Common patterns (log analysis, JSON processing, text processing)\n- Performance considerations\n\n**Quick Example**:\n```python\nimport dask.bag as db\nimport json\n\n# Read and parse JSON files\nbag = db.read_text('logs/*.json').map(json.loads)\n\n# Filter and transform\nvalid = bag.filter(lambda x: x['status'] == 'valid')\nprocessed = valid.map(lambda x: {'id': x['id'], 'value': x['value']})\n\n# Convert to DataFrame for analysis\nddf = processed.to_dataframe()\n```\n\n**Key Points**:\n- Use for initial data cleaning, then convert to DataFrame/Array\n- Use `foldby` instead of `groupby` for better performance\n- Operations are streaming and memory-efficient\n- Convert to structured formats (DataFrame) for complex operations\n\n### 4. Futures - Task-Based Parallelization\n\n**Purpose**: Build custom parallel workflows with fine-grained control over task execution and dependencies.\n\n**When to Use**:\n- Building dynamic, evolving workflows\n- Need immediate task execution (not lazy)\n- Computations depend on runtime conditions\n- Implementing custom parallel algorithms\n- Need stateful computations\n\n**Reference Documentation**: For comprehensive guidance on Dask Futures, refer to `references/futures.md` which includes:\n- Setting up distributed client\n- Submitting tasks and working with futures\n- Task dependencies and data movement\n- Advanced coordination (queues, locks, events, actors)\n- Common patterns (parameter sweeps, dynamic tasks, iterative algorithms)\n\n**Quick Example**:\n```python\nfrom dask.distributed import Client\n\nclient = Client()  # Create local cluster\n\n# Submit tasks (executes immediately)\ndef process(x):\n    return x ** 2\n\nfutures = client.map(process, range(100))\n\n# Gather results\nresults = client.gather(futures)\n\nclient.close()\n```\n\n**Key Points**:\n- Requires distributed client (even for single machine)\n- Tasks execute immediately when submitted\n- Pre-scatter large data to avoid repeated transfers\n- ~1ms overhead per task (not suitable for millions of tiny tasks)\n- Use actors for stateful workflows\n\n### 5. Schedulers - Execution Backends\n\n**Purpose**: Control how and where Dask tasks execute (threads, processes, distributed).\n\n**When to Choose Scheduler**:\n- **Threads** (default): NumPy/Pandas operations, GIL-releasing libraries, shared memory benefit\n- **Processes**: Pure Python code, text processing, GIL-bound operations\n- **Synchronous**: Debugging with pdb, profiling, understanding errors\n- **Distributed**: Need dashboard, multi-machine clusters, advanced features\n\n**Reference Documentation**: For comprehensive guidance on Dask Schedulers, refer to `references/schedulers.md` which includes:\n- Detailed scheduler descriptions and characteristics\n- Configuration methods (global, context manager, per-compute)\n- Performance considerations and overhead\n- Common patterns and troubleshooting\n- Thread configuration for optimal performance\n\n**Quick Example**:\n```python\nimport dask\nimport dask.dataframe as dd\n\n# Use threads for DataFrame (default, good for numeric)\nddf = dd.read_csv('data.csv')\nresult1 = ddf.mean().compute()  # Uses threads\n\n# Use processes for Python-heavy work\nimport dask.bag as db\nbag = db.read_text('logs/*.txt')\nresult2 = bag.map(python_function).compute(scheduler='processes')\n\n# Use synchronous for debugging\ndask.config.set(scheduler='synchronous')\nresult3 = problematic_computation.compute()  # Can use pdb\n\n# Use distributed for monitoring and scaling\nfrom dask.distributed import Client\nclient = Client()\nresult4 = computation.compute()  # Uses distributed with dashboard\n```\n\n**Key Points**:\n- Threads: Lowest overhead (~10 s/task), best for numeric work\n- Processes: Avoids GIL (~10 ms/task), best for Python work\n- Distributed: Monitoring dashboard (~1 ms/task), scales to clusters\n- Can switch schedulers per computation or globally\n\n## Best Practices\n\nFor comprehensive performance optimization guidance, memory management strategies, and common pitfalls to avoid, refer to `references/best-practices.md`. Key principles include:\n\n### Start with Simpler Solutions\nBefore using Dask, explore:\n- Better algorithms\n- Efficient file formats (Parquet instead of CSV)\n- Compiled code (Numba, Cython)\n- Data sampling\n\n### Critical Performance Rules\n\n**1. Don't Load Data Locally Then Hand to Dask**\n```python\n# Wrong: Loads all data in memory first\nimport pandas as pd\ndf = pd.read_csv('large.csv')\nddf = dd.from_pandas(df, npartitions=10)\n\n# Correct: Let Dask handle loading\nimport dask.dataframe as dd\nddf = dd.read_csv('large.csv')\n```\n\n**2. Avoid Repeated compute() Calls**\n```python\n# Wrong: Each compute is separate\nfor item in items:\n    result = dask_computation(item).compute()\n\n# Correct: Single compute for all\ncomputations = [dask_computation(item) for item in items]\nresults = dask.compute(*computations)\n```\n\n**3. Don't Build Excessively Large Task Graphs**\n- Increase chunk sizes if millions of tasks\n- Use `map_partitions`/`map_blocks` to fuse operations\n- Check task graph size: `len(ddf.__dask_graph__())`\n\n**4. Choose Appropriate Chunk Sizes**\n- Target: ~100 MB per chunk (or 10 chunks per core in worker memory)\n- Too large: Memory overflow\n- Too small: Scheduling overhead\n\n**5. Use the Dashboard**\n```python\nfrom dask.distributed import Client\nclient = Client()\nprint(client.dashboard_link)  # Monitor performance, identify bottlenecks\n```\n\n## Common Workflow Patterns\n\n### ETL Pipeline\n```python\nimport dask.dataframe as dd\n\n# Extract: Read data\nddf = dd.read_csv('raw_data/*.csv')\n\n# Transform: Clean and process\nddf = ddf[ddf['status'] == 'valid']\nddf['amount'] = ddf['amount'].astype('float64')\nddf = ddf.dropna(subset=['important_col'])\n\n# Load: Aggregate and save\nsummary = ddf.groupby('category').agg({'amount': ['sum', 'mean']})\nsummary.to_parquet('output/summary.parquet')\n```\n\n### Unstructured to Structured Pipeline\n```python\nimport dask.bag as db\nimport json\n\n# Start with Bag for unstructured data\nbag = db.read_text('logs/*.json').map(json.loads)\nbag = bag.filter(lambda x: x['status'] == 'valid')\n\n# Convert to DataFrame for structured analysis\nddf = bag.to_dataframe()\nresult = ddf.groupby('category').mean().compute()\n```\n\n### Large-Scale Array Computation\n```python\nimport dask.array as da\n\n# Load or create large array\nx = da.from_zarr('large_dataset.zarr')\n\n# Process in chunks\nnormalized = (x - x.mean()) / x.std()\n\n# Save result\nda.to_zarr(normalized, 'normalized.zarr')\n```\n\n### Custom Parallel Workflow\n```python\nfrom dask.distributed import Client\n\nclient = Client()\n\n# Scatter large dataset once\ndata = client.scatter(large_dataset)\n\n# Process in parallel with dependencies\nfutures = []\nfor param in parameters:\n    future = client.submit(process, data, param)\n    futures.append(future)\n\n# Gather results\nresults = client.gather(futures)\n```\n\n## Selecting the Right Component\n\nUse this decision guide to choose the appropriate Dask component:\n\n**Data Type**:\n- Tabular data  **DataFrames**\n- Numeric arrays  **Arrays**\n- Text/JSON/logs  **Bags** (then convert to DataFrame)\n- Custom Python objects  **Bags** or **Futures**\n\n**Operation Type**:\n- Standard pandas operations  **DataFrames**\n- Standard NumPy operations  **Arrays**\n- Custom parallel tasks  **Futures**\n- Text processing/ETL  **Bags**\n\n**Control Level**:\n- High-level, automatic  **DataFrames/Arrays**\n- Low-level, manual  **Futures**\n\n**Workflow Type**:\n- Static computation graph  **DataFrames/Arrays/Bags**\n- Dynamic, evolving  **Futures**\n\n## Integration Considerations\n\n### File Formats\n- **Efficient**: Parquet, HDF5, Zarr (columnar, compressed, parallel-friendly)\n- **Compatible but slower**: CSV (use for initial ingestion only)\n- **For Arrays**: HDF5, Zarr, NetCDF\n\n### Conversion Between Collections\n```python\n# Bag  DataFrame\nddf = bag.to_dataframe()\n\n# DataFrame  Array (for numeric data)\narr = ddf.to_dask_array(lengths=True)\n\n# Array  DataFrame\nddf = dd.from_dask_array(arr, columns=['col1', 'col2'])\n```\n\n### With Other Libraries\n- **XArray**: Wraps Dask arrays with labeled dimensions (geospatial, imaging)\n- **Dask-ML**: Machine learning with scikit-learn compatible APIs\n- **Distributed**: Advanced cluster management and monitoring\n\n## Debugging and Development\n\n### Iterative Development Workflow\n\n1. **Test on small data with synchronous scheduler**:\n```python\ndask.config.set(scheduler='synchronous')\nresult = computation.compute()  # Can use pdb, easy debugging\n```\n\n2. **Validate with threads on sample**:\n```python\nsample = ddf.head(1000)  # Small sample\n# Test logic, then scale to full dataset\n```\n\n3. **Scale with distributed for monitoring**:\n```python\nfrom dask.distributed import Client\nclient = Client()\nprint(client.dashboard_link)  # Monitor performance\nresult = computation.compute()\n```\n\n### Common Issues\n\n**Memory Errors**:\n- Decrease chunk sizes\n- Use `persist()` strategically and delete when done\n- Check for memory leaks in custom functions\n\n**Slow Start**:\n- Task graph too large (increase chunk sizes)\n- Use `map_partitions` or `map_blocks` to reduce tasks\n\n**Poor Parallelization**:\n- Chunks too large (increase number of partitions)\n- Using threads with Python code (switch to processes)\n- Data dependencies preventing parallelism\n\n## Reference Files\n\nAll reference documentation files can be read as needed for detailed information:\n\n- `references/dataframes.md` - Complete Dask DataFrame guide\n- `references/arrays.md` - Complete Dask Array guide\n- `references/bags.md` - Complete Dask Bag guide\n- `references/futures.md` - Complete Dask Futures and distributed computing guide\n- `references/schedulers.md` - Complete scheduler selection and configuration guide\n- `references/best-practices.md` - Comprehensive performance optimization and troubleshooting\n\nLoad these files when users need detailed information about specific Dask components, operations, or patterns beyond the quick guidance provided here.\n",
        "data/k-dense-ai/datacommons-client/SKILL.md": "---\nname: datacommons-client\ndescription: Work with Data Commons, a platform providing programmatic access to public statistical data from global sources. Use this skill when working with demographic data, economic indicators, health statistics, environmental data, or any public datasets available through Data Commons. Applicable for querying population statistics, GDP figures, unemployment rates, disease prevalence, geographic entity resolution, and exploring relationships between statistical entities.\n---\n\n# Data Commons Client\n\n## Overview\n\nProvides comprehensive access to the Data Commons Python API v2 for querying statistical observations, exploring the knowledge graph, and resolving entity identifiers. Data Commons aggregates data from census bureaus, health organizations, environmental agencies, and other authoritative sources into a unified knowledge graph.\n\n## Installation\n\nInstall the Data Commons Python client with Pandas support:\n\n```bash\nuv pip install \"datacommons-client[Pandas]\"\n```\n\nFor basic usage without Pandas:\n```bash\nuv pip install datacommons-client\n```\n\n## Core Capabilities\n\nThe Data Commons API consists of three main endpoints, each detailed in dedicated reference files:\n\n### 1. Observation Endpoint - Statistical Data Queries\n\nQuery time-series statistical data for entities. See `references/observation.md` for comprehensive documentation.\n\n**Primary use cases:**\n- Retrieve population, economic, health, or environmental statistics\n- Access historical time-series data for trend analysis\n- Query data for hierarchies (all counties in a state, all countries in a region)\n- Compare statistics across multiple entities\n- Filter by data source for consistency\n\n**Common patterns:**\n```python\nfrom datacommons_client import DataCommonsClient\n\nclient = DataCommonsClient()\n\n# Get latest population data\nresponse = client.observation.fetch(\n    variable_dcids=[\"Count_Person\"],\n    entity_dcids=[\"geoId/06\"],  # California\n    date=\"latest\"\n)\n\n# Get time series\nresponse = client.observation.fetch(\n    variable_dcids=[\"UnemploymentRate_Person\"],\n    entity_dcids=[\"country/USA\"],\n    date=\"all\"\n)\n\n# Query by hierarchy\nresponse = client.observation.fetch(\n    variable_dcids=[\"MedianIncome_Household\"],\n    entity_expression=\"geoId/06<-containedInPlace+{typeOf:County}\",\n    date=\"2020\"\n)\n```\n\n### 2. Node Endpoint - Knowledge Graph Exploration\n\nExplore entity relationships and properties within the knowledge graph. See `references/node.md` for comprehensive documentation.\n\n**Primary use cases:**\n- Discover available properties for entities\n- Navigate geographic hierarchies (parent/child relationships)\n- Retrieve entity names and metadata\n- Explore connections between entities\n- List all entity types in the graph\n\n**Common patterns:**\n```python\n# Discover properties\nlabels = client.node.fetch_property_labels(\n    node_dcids=[\"geoId/06\"],\n    out=True\n)\n\n# Navigate hierarchy\nchildren = client.node.fetch_place_children(\n    node_dcids=[\"country/USA\"]\n)\n\n# Get entity names\nnames = client.node.fetch_entity_names(\n    node_dcids=[\"geoId/06\", \"geoId/48\"]\n)\n```\n\n### 3. Resolve Endpoint - Entity Identification\n\nTranslate entity names, coordinates, or external IDs into Data Commons IDs (DCIDs). See `references/resolve.md` for comprehensive documentation.\n\n**Primary use cases:**\n- Convert place names to DCIDs for queries\n- Resolve coordinates to places\n- Map Wikidata IDs to Data Commons entities\n- Handle ambiguous entity names\n\n**Common patterns:**\n```python\n# Resolve by name\nresponse = client.resolve.fetch_dcids_by_name(\n    names=[\"California\", \"Texas\"],\n    entity_type=\"State\"\n)\n\n# Resolve by coordinates\ndcid = client.resolve.fetch_dcid_by_coordinates(\n    latitude=37.7749,\n    longitude=-122.4194\n)\n\n# Resolve Wikidata IDs\nresponse = client.resolve.fetch_dcids_by_wikidata_id(\n    wikidata_ids=[\"Q30\", \"Q99\"]\n)\n```\n\n## Typical Workflow\n\nMost Data Commons queries follow this pattern:\n\n1. **Resolve entities** (if starting with names):\n   ```python\n   resolve_response = client.resolve.fetch_dcids_by_name(\n       names=[\"California\", \"Texas\"]\n   )\n   dcids = [r[\"candidates\"][0][\"dcid\"]\n            for r in resolve_response.to_dict().values()\n            if r[\"candidates\"]]\n   ```\n\n2. **Discover available variables** (optional):\n   ```python\n   variables = client.observation.fetch_available_statistical_variables(\n       entity_dcids=dcids\n   )\n   ```\n\n3. **Query statistical data**:\n   ```python\n   response = client.observation.fetch(\n       variable_dcids=[\"Count_Person\", \"UnemploymentRate_Person\"],\n       entity_dcids=dcids,\n       date=\"latest\"\n   )\n   ```\n\n4. **Process results**:\n   ```python\n   # As dictionary\n   data = response.to_dict()\n\n   # As Pandas DataFrame\n   df = response.to_observations_as_records()\n   ```\n\n## Finding Statistical Variables\n\nStatistical variables use specific naming patterns in Data Commons:\n\n**Common variable patterns:**\n- `Count_Person` - Total population\n- `Count_Person_Female` - Female population\n- `UnemploymentRate_Person` - Unemployment rate\n- `Median_Income_Household` - Median household income\n- `Count_Death` - Death count\n- `Median_Age_Person` - Median age\n\n**Discovery methods:**\n```python\n# Check what variables are available for an entity\navailable = client.observation.fetch_available_statistical_variables(\n    entity_dcids=[\"geoId/06\"]\n)\n\n# Or explore via the web interface\n# https://datacommons.org/tools/statvar\n```\n\n## Working with Pandas\n\nAll observation responses integrate with Pandas:\n\n```python\nresponse = client.observation.fetch(\n    variable_dcids=[\"Count_Person\"],\n    entity_dcids=[\"geoId/06\", \"geoId/48\"],\n    date=\"all\"\n)\n\n# Convert to DataFrame\ndf = response.to_observations_as_records()\n# Columns: date, entity, variable, value\n\n# Reshape for analysis\npivot = df.pivot_table(\n    values='value',\n    index='date',\n    columns='entity'\n)\n```\n\n## API Authentication\n\n**For datacommons.org (default):**\n- An API key is required\n- Set via environment variable: `export DC_API_KEY=\"your_key\"`\n- Or pass when initializing: `client = DataCommonsClient(api_key=\"your_key\")`\n- Request keys at: https://apikeys.datacommons.org/\n\n**For custom Data Commons instances:**\n- No API key required\n- Specify custom endpoint: `client = DataCommonsClient(url=\"https://custom.datacommons.org\")`\n\n## Reference Documentation\n\nComprehensive documentation for each endpoint is available in the `references/` directory:\n\n- **`references/observation.md`**: Complete Observation API documentation with all methods, parameters, response formats, and common use cases\n- **`references/node.md`**: Complete Node API documentation for graph exploration, property queries, and hierarchy navigation\n- **`references/resolve.md`**: Complete Resolve API documentation for entity identification and DCID resolution\n- **`references/getting_started.md`**: Quickstart guide with end-to-end examples and common patterns\n\n## Additional Resources\n\n- **Official Documentation**: https://docs.datacommons.org/api/python/v2/\n- **Statistical Variable Explorer**: https://datacommons.org/tools/statvar\n- **Data Commons Browser**: https://datacommons.org/browser/\n- **GitHub Repository**: https://github.com/datacommonsorg/api-python\n\n## Tips for Effective Use\n\n1. **Always start with resolution**: Convert names to DCIDs before querying data\n2. **Use relation expressions for hierarchies**: Query all children at once instead of individual queries\n3. **Check data availability first**: Use `fetch_available_statistical_variables()` to see what's queryable\n4. **Leverage Pandas integration**: Convert responses to DataFrames for analysis\n5. **Cache resolutions**: If querying the same entities repeatedly, store nameDCID mappings\n6. **Filter by facet for consistency**: Use `filter_facet_domains` to ensure data from the same source\n7. **Read reference docs**: Each endpoint has extensive documentation in the `references/` directory\n",
        "data/k-dense-ai/datamol/SKILL.md": "---\nname: datamol\ndescription: \"Pythonic wrapper around RDKit with simplified interface and sensible defaults. Preferred for standard drug discovery: SMILES parsing, standardization, descriptors, fingerprints, clustering, 3D conformers, parallel processing. Returns native rdkit.Chem.Mol objects. For advanced control or custom parameters, use rdkit directly.\"\n---\n\n# Datamol Cheminformatics Skill\n\n## Overview\n\nDatamol is a Python library that provides a lightweight, Pythonic abstraction layer over RDKit for molecular cheminformatics. Simplify complex molecular operations with sensible defaults, efficient parallelization, and modern I/O capabilities. All molecular objects are native `rdkit.Chem.Mol` instances, ensuring full compatibility with the RDKit ecosystem.\n\n**Key capabilities**:\n- Molecular format conversion (SMILES, SELFIES, InChI)\n- Structure standardization and sanitization\n- Molecular descriptors and fingerprints\n- 3D conformer generation and analysis\n- Clustering and diversity selection\n- Scaffold and fragment analysis\n- Chemical reaction application\n- Visualization and alignment\n- Batch processing with parallelization\n- Cloud storage support via fsspec\n\n## Installation and Setup\n\nGuide users to install datamol:\n\n```bash\nuv pip install datamol\n```\n\n**Import convention**:\n```python\nimport datamol as dm\n```\n\n## Core Workflows\n\n### 1. Basic Molecule Handling\n\n**Creating molecules from SMILES**:\n```python\nimport datamol as dm\n\n# Single molecule\nmol = dm.to_mol(\"CCO\")  # Ethanol\n\n# From list of SMILES\nsmiles_list = [\"CCO\", \"c1ccccc1\", \"CC(=O)O\"]\nmols = [dm.to_mol(smi) for smi in smiles_list]\n\n# Error handling\nmol = dm.to_mol(\"invalid_smiles\")  # Returns None\nif mol is None:\n    print(\"Failed to parse SMILES\")\n```\n\n**Converting molecules to SMILES**:\n```python\n# Canonical SMILES\nsmiles = dm.to_smiles(mol)\n\n# Isomeric SMILES (includes stereochemistry)\nsmiles = dm.to_smiles(mol, isomeric=True)\n\n# Other formats\ninchi = dm.to_inchi(mol)\ninchikey = dm.to_inchikey(mol)\nselfies = dm.to_selfies(mol)\n```\n\n**Standardization and sanitization** (always recommend for user-provided molecules):\n```python\n# Sanitize molecule\nmol = dm.sanitize_mol(mol)\n\n# Full standardization (recommended for datasets)\nmol = dm.standardize_mol(\n    mol,\n    disconnect_metals=True,\n    normalize=True,\n    reionize=True\n)\n\n# For SMILES strings directly\nclean_smiles = dm.standardize_smiles(smiles)\n```\n\n### 2. Reading and Writing Molecular Files\n\nRefer to `references/io_module.md` for comprehensive I/O documentation.\n\n**Reading files**:\n```python\n# SDF files (most common in chemistry)\ndf = dm.read_sdf(\"compounds.sdf\", mol_column='mol')\n\n# SMILES files\ndf = dm.read_smi(\"molecules.smi\", smiles_column='smiles', mol_column='mol')\n\n# CSV with SMILES column\ndf = dm.read_csv(\"data.csv\", smiles_column=\"SMILES\", mol_column=\"mol\")\n\n# Excel files\ndf = dm.read_excel(\"compounds.xlsx\", sheet_name=0, mol_column=\"mol\")\n\n# Universal reader (auto-detects format)\ndf = dm.open_df(\"file.sdf\")  # Works with .sdf, .csv, .xlsx, .parquet, .json\n```\n\n**Writing files**:\n```python\n# Save as SDF\ndm.to_sdf(mols, \"output.sdf\")\n# Or from DataFrame\ndm.to_sdf(df, \"output.sdf\", mol_column=\"mol\")\n\n# Save as SMILES file\ndm.to_smi(mols, \"output.smi\")\n\n# Excel with rendered molecule images\ndm.to_xlsx(df, \"output.xlsx\", mol_columns=[\"mol\"])\n```\n\n**Remote file support** (S3, GCS, HTTP):\n```python\n# Read from cloud storage\ndf = dm.read_sdf(\"s3://bucket/compounds.sdf\")\ndf = dm.read_csv(\"https://example.com/data.csv\")\n\n# Write to cloud storage\ndm.to_sdf(mols, \"s3://bucket/output.sdf\")\n```\n\n### 3. Molecular Descriptors and Properties\n\nRefer to `references/descriptors_viz.md` for detailed descriptor documentation.\n\n**Computing descriptors for a single molecule**:\n```python\n# Get standard descriptor set\ndescriptors = dm.descriptors.compute_many_descriptors(mol)\n# Returns: {'mw': 46.07, 'logp': -0.03, 'hbd': 1, 'hba': 1,\n#           'tpsa': 20.23, 'n_aromatic_atoms': 0, ...}\n```\n\n**Batch descriptor computation** (recommended for datasets):\n```python\n# Compute for all molecules in parallel\ndesc_df = dm.descriptors.batch_compute_many_descriptors(\n    mols,\n    n_jobs=-1,      # Use all CPU cores\n    progress=True   # Show progress bar\n)\n```\n\n**Specific descriptors**:\n```python\n# Aromaticity\nn_aromatic = dm.descriptors.n_aromatic_atoms(mol)\naromatic_ratio = dm.descriptors.n_aromatic_atoms_proportion(mol)\n\n# Stereochemistry\nn_stereo = dm.descriptors.n_stereo_centers(mol)\nn_unspec = dm.descriptors.n_stereo_centers_unspecified(mol)\n\n# Flexibility\nn_rigid = dm.descriptors.n_rigid_bonds(mol)\n```\n\n**Drug-likeness filtering (Lipinski's Rule of Five)**:\n```python\n# Filter compounds\ndef is_druglike(mol):\n    desc = dm.descriptors.compute_many_descriptors(mol)\n    return (\n        desc['mw'] <= 500 and\n        desc['logp'] <= 5 and\n        desc['hbd'] <= 5 and\n        desc['hba'] <= 10\n    )\n\ndruglike_mols = [mol for mol in mols if is_druglike(mol)]\n```\n\n### 4. Molecular Fingerprints and Similarity\n\n**Generating fingerprints**:\n```python\n# ECFP (Extended Connectivity Fingerprint, default)\nfp = dm.to_fp(mol, fp_type='ecfp', radius=2, n_bits=2048)\n\n# Other fingerprint types\nfp_maccs = dm.to_fp(mol, fp_type='maccs')\nfp_topological = dm.to_fp(mol, fp_type='topological')\nfp_atompair = dm.to_fp(mol, fp_type='atompair')\n```\n\n**Similarity calculations**:\n```python\n# Pairwise distances within a set\ndistance_matrix = dm.pdist(mols, n_jobs=-1)\n\n# Distances between two sets\ndistances = dm.cdist(query_mols, library_mols, n_jobs=-1)\n\n# Find most similar molecules\nfrom scipy.spatial.distance import squareform\ndist_matrix = squareform(dm.pdist(mols))\n# Lower distance = higher similarity (Tanimoto distance = 1 - Tanimoto similarity)\n```\n\n### 5. Clustering and Diversity Selection\n\nRefer to `references/core_api.md` for clustering details.\n\n**Butina clustering**:\n```python\n# Cluster molecules by structural similarity\nclusters = dm.cluster_mols(\n    mols,\n    cutoff=0.2,    # Tanimoto distance threshold (0=identical, 1=completely different)\n    n_jobs=-1      # Parallel processing\n)\n\n# Each cluster is a list of molecule indices\nfor i, cluster in enumerate(clusters):\n    print(f\"Cluster {i}: {len(cluster)} molecules\")\n    cluster_mols = [mols[idx] for idx in cluster]\n```\n\n**Important**: Butina clustering builds a full distance matrix - suitable for ~1000 molecules, not for 10,000+.\n\n**Diversity selection**:\n```python\n# Pick diverse subset\ndiverse_mols = dm.pick_diverse(\n    mols,\n    npick=100  # Select 100 diverse molecules\n)\n\n# Pick cluster centroids\ncentroids = dm.pick_centroids(\n    mols,\n    npick=50   # Select 50 representative molecules\n)\n```\n\n### 6. Scaffold Analysis\n\nRefer to `references/fragments_scaffolds.md` for complete scaffold documentation.\n\n**Extracting Murcko scaffolds**:\n```python\n# Get Bemis-Murcko scaffold (core structure)\nscaffold = dm.to_scaffold_murcko(mol)\nscaffold_smiles = dm.to_smiles(scaffold)\n```\n\n**Scaffold-based analysis**:\n```python\n# Group compounds by scaffold\nfrom collections import Counter\n\nscaffolds = [dm.to_scaffold_murcko(mol) for mol in mols]\nscaffold_smiles = [dm.to_smiles(s) for s in scaffolds]\n\n# Count scaffold frequency\nscaffold_counts = Counter(scaffold_smiles)\nmost_common = scaffold_counts.most_common(10)\n\n# Create scaffold-to-molecules mapping\nscaffold_groups = {}\nfor mol, scaf_smi in zip(mols, scaffold_smiles):\n    if scaf_smi not in scaffold_groups:\n        scaffold_groups[scaf_smi] = []\n    scaffold_groups[scaf_smi].append(mol)\n```\n\n**Scaffold-based train/test splitting** (for ML):\n```python\n# Ensure train and test sets have different scaffolds\nscaffold_to_mols = {}\nfor mol, scaf in zip(mols, scaffold_smiles):\n    if scaf not in scaffold_to_mols:\n        scaffold_to_mols[scaf] = []\n    scaffold_to_mols[scaf].append(mol)\n\n# Split scaffolds into train/test\nimport random\nscaffolds = list(scaffold_to_mols.keys())\nrandom.shuffle(scaffolds)\nsplit_idx = int(0.8 * len(scaffolds))\ntrain_scaffolds = scaffolds[:split_idx]\ntest_scaffolds = scaffolds[split_idx:]\n\n# Get molecules for each split\ntrain_mols = [mol for scaf in train_scaffolds for mol in scaffold_to_mols[scaf]]\ntest_mols = [mol for scaf in test_scaffolds for mol in scaffold_to_mols[scaf]]\n```\n\n### 7. Molecular Fragmentation\n\nRefer to `references/fragments_scaffolds.md` for fragmentation details.\n\n**BRICS fragmentation** (16 bond types):\n```python\n# Fragment molecule\nfragments = dm.fragment.brics(mol)\n# Returns: set of fragment SMILES with attachment points like '[1*]CCN'\n```\n\n**RECAP fragmentation** (11 bond types):\n```python\nfragments = dm.fragment.recap(mol)\n```\n\n**Fragment analysis**:\n```python\n# Find common fragments across compound library\nfrom collections import Counter\n\nall_fragments = []\nfor mol in mols:\n    frags = dm.fragment.brics(mol)\n    all_fragments.extend(frags)\n\nfragment_counts = Counter(all_fragments)\ncommon_frags = fragment_counts.most_common(20)\n\n# Fragment-based scoring\ndef fragment_score(mol, reference_fragments):\n    mol_frags = dm.fragment.brics(mol)\n    overlap = mol_frags.intersection(reference_fragments)\n    return len(overlap) / len(mol_frags) if mol_frags else 0\n```\n\n### 8. 3D Conformer Generation\n\nRefer to `references/conformers_module.md` for detailed conformer documentation.\n\n**Generating conformers**:\n```python\n# Generate 3D conformers\nmol_3d = dm.conformers.generate(\n    mol,\n    n_confs=50,           # Number to generate (auto if None)\n    rms_cutoff=0.5,       # Filter similar conformers (ngstrms)\n    minimize_energy=True,  # Minimize with UFF force field\n    method='ETKDGv3'      # Embedding method (recommended)\n)\n\n# Access conformers\nn_conformers = mol_3d.GetNumConformers()\nconf = mol_3d.GetConformer(0)  # Get first conformer\npositions = conf.GetPositions()  # Nx3 array of atom coordinates\n```\n\n**Conformer clustering**:\n```python\n# Cluster conformers by RMSD\nclusters = dm.conformers.cluster(\n    mol_3d,\n    rms_cutoff=1.0,\n    centroids=False\n)\n\n# Get representative conformers\ncentroids = dm.conformers.return_centroids(mol_3d, clusters)\n```\n\n**SASA calculation**:\n```python\n# Calculate solvent accessible surface area\nsasa_values = dm.conformers.sasa(mol_3d, n_jobs=-1)\n\n# Access SASA from conformer properties\nconf = mol_3d.GetConformer(0)\nsasa = conf.GetDoubleProp('rdkit_free_sasa')\n```\n\n### 9. Visualization\n\nRefer to `references/descriptors_viz.md` for visualization documentation.\n\n**Basic molecule grid**:\n```python\n# Visualize molecules\ndm.viz.to_image(\n    mols[:20],\n    legends=[dm.to_smiles(m) for m in mols[:20]],\n    n_cols=5,\n    mol_size=(300, 300)\n)\n\n# Save to file\ndm.viz.to_image(mols, outfile=\"molecules.png\")\n\n# SVG for publications\ndm.viz.to_image(mols, outfile=\"molecules.svg\", use_svg=True)\n```\n\n**Aligned visualization** (for SAR analysis):\n```python\n# Align molecules by common substructure\ndm.viz.to_image(\n    similar_mols,\n    align=True,  # Enable MCS alignment\n    legends=activity_labels,\n    n_cols=4\n)\n```\n\n**Highlighting substructures**:\n```python\n# Highlight specific atoms and bonds\ndm.viz.to_image(\n    mol,\n    highlight_atom=[0, 1, 2, 3],  # Atom indices\n    highlight_bond=[0, 1, 2]      # Bond indices\n)\n```\n\n**Conformer visualization**:\n```python\n# Display multiple conformers\ndm.viz.conformers(\n    mol_3d,\n    n_confs=10,\n    align_conf=True,\n    n_cols=3\n)\n```\n\n### 10. Chemical Reactions\n\nRefer to `references/reactions_data.md` for reactions documentation.\n\n**Applying reactions**:\n```python\nfrom rdkit.Chem import rdChemReactions\n\n# Define reaction from SMARTS\nrxn_smarts = '[C:1](=[O:2])[OH:3]>>[C:1](=[O:2])[Cl:3]'\nrxn = rdChemReactions.ReactionFromSmarts(rxn_smarts)\n\n# Apply to molecule\nreactant = dm.to_mol(\"CC(=O)O\")  # Acetic acid\nproduct = dm.reactions.apply_reaction(\n    rxn,\n    (reactant,),\n    sanitize=True\n)\n\n# Convert to SMILES\nproduct_smiles = dm.to_smiles(product)\n```\n\n**Batch reaction application**:\n```python\n# Apply reaction to library\nproducts = []\nfor mol in reactant_mols:\n    try:\n        prod = dm.reactions.apply_reaction(rxn, (mol,))\n        if prod is not None:\n            products.append(prod)\n    except Exception as e:\n        print(f\"Reaction failed: {e}\")\n```\n\n## Parallelization\n\nDatamol includes built-in parallelization for many operations. Use `n_jobs` parameter:\n- `n_jobs=1`: Sequential (no parallelization)\n- `n_jobs=-1`: Use all available CPU cores\n- `n_jobs=4`: Use 4 cores\n\n**Functions supporting parallelization**:\n- `dm.read_sdf(..., n_jobs=-1)`\n- `dm.descriptors.batch_compute_many_descriptors(..., n_jobs=-1)`\n- `dm.cluster_mols(..., n_jobs=-1)`\n- `dm.pdist(..., n_jobs=-1)`\n- `dm.conformers.sasa(..., n_jobs=-1)`\n\n**Progress bars**: Many batch operations support `progress=True` parameter.\n\n## Common Workflows and Patterns\n\n### Complete Pipeline: Data Loading  Filtering  Analysis\n\n```python\nimport datamol as dm\nimport pandas as pd\n\n# 1. Load molecules\ndf = dm.read_sdf(\"compounds.sdf\")\n\n# 2. Standardize\ndf['mol'] = df['mol'].apply(lambda m: dm.standardize_mol(m) if m else None)\ndf = df[df['mol'].notna()]  # Remove failed molecules\n\n# 3. Compute descriptors\ndesc_df = dm.descriptors.batch_compute_many_descriptors(\n    df['mol'].tolist(),\n    n_jobs=-1,\n    progress=True\n)\n\n# 4. Filter by drug-likeness\ndruglike = (\n    (desc_df['mw'] <= 500) &\n    (desc_df['logp'] <= 5) &\n    (desc_df['hbd'] <= 5) &\n    (desc_df['hba'] <= 10)\n)\nfiltered_df = df[druglike]\n\n# 5. Cluster and select diverse subset\ndiverse_mols = dm.pick_diverse(\n    filtered_df['mol'].tolist(),\n    npick=100\n)\n\n# 6. Visualize results\ndm.viz.to_image(\n    diverse_mols,\n    legends=[dm.to_smiles(m) for m in diverse_mols],\n    outfile=\"diverse_compounds.png\",\n    n_cols=10\n)\n```\n\n### Structure-Activity Relationship (SAR) Analysis\n\n```python\n# Group by scaffold\nscaffolds = [dm.to_scaffold_murcko(mol) for mol in mols]\nscaffold_smiles = [dm.to_smiles(s) for s in scaffolds]\n\n# Create DataFrame with activities\nsar_df = pd.DataFrame({\n    'mol': mols,\n    'scaffold': scaffold_smiles,\n    'activity': activities  # User-provided activity data\n})\n\n# Analyze each scaffold series\nfor scaffold, group in sar_df.groupby('scaffold'):\n    if len(group) >= 3:  # Need multiple examples\n        print(f\"\\nScaffold: {scaffold}\")\n        print(f\"Count: {len(group)}\")\n        print(f\"Activity range: {group['activity'].min():.2f} - {group['activity'].max():.2f}\")\n\n        # Visualize with activities as legends\n        dm.viz.to_image(\n            group['mol'].tolist(),\n            legends=[f\"Activity: {act:.2f}\" for act in group['activity']],\n            align=True  # Align by common substructure\n        )\n```\n\n### Virtual Screening Pipeline\n\n```python\n# 1. Generate fingerprints for query and library\nquery_fps = [dm.to_fp(mol) for mol in query_actives]\nlibrary_fps = [dm.to_fp(mol) for mol in library_mols]\n\n# 2. Calculate similarities\nfrom scipy.spatial.distance import cdist\nimport numpy as np\n\ndistances = dm.cdist(query_actives, library_mols, n_jobs=-1)\n\n# 3. Find closest matches (min distance to any query)\nmin_distances = distances.min(axis=0)\nsimilarities = 1 - min_distances  # Convert distance to similarity\n\n# 4. Rank and select top hits\ntop_indices = np.argsort(similarities)[::-1][:100]  # Top 100\ntop_hits = [library_mols[i] for i in top_indices]\ntop_scores = [similarities[i] for i in top_indices]\n\n# 5. Visualize hits\ndm.viz.to_image(\n    top_hits[:20],\n    legends=[f\"Sim: {score:.3f}\" for score in top_scores[:20]],\n    outfile=\"screening_hits.png\"\n)\n```\n\n## Reference Documentation\n\nFor detailed API documentation, consult these reference files:\n\n- **`references/core_api.md`**: Core namespace functions (conversions, standardization, fingerprints, clustering)\n- **`references/io_module.md`**: File I/O operations (read/write SDF, CSV, Excel, remote files)\n- **`references/conformers_module.md`**: 3D conformer generation, clustering, SASA calculations\n- **`references/descriptors_viz.md`**: Molecular descriptors and visualization functions\n- **`references/fragments_scaffolds.md`**: Scaffold extraction, BRICS/RECAP fragmentation\n- **`references/reactions_data.md`**: Chemical reactions and toy datasets\n\n## Best Practices\n\n1. **Always standardize molecules** from external sources:\n   ```python\n   mol = dm.standardize_mol(mol, disconnect_metals=True, normalize=True, reionize=True)\n   ```\n\n2. **Check for None values** after molecule parsing:\n   ```python\n   mol = dm.to_mol(smiles)\n   if mol is None:\n       # Handle invalid SMILES\n   ```\n\n3. **Use parallel processing** for large datasets:\n   ```python\n   result = dm.operation(..., n_jobs=-1, progress=True)\n   ```\n\n4. **Leverage fsspec** for cloud storage:\n   ```python\n   df = dm.read_sdf(\"s3://bucket/compounds.sdf\")\n   ```\n\n5. **Use appropriate fingerprints** for similarity:\n   - ECFP (Morgan): General purpose, structural similarity\n   - MACCS: Fast, smaller feature space\n   - Atom pairs: Considers atom pairs and distances\n\n6. **Consider scale limitations**:\n   - Butina clustering: ~1,000 molecules (full distance matrix)\n   - For larger datasets: Use diversity selection or hierarchical methods\n\n7. **Scaffold splitting for ML**: Ensure proper train/test separation by scaffold\n\n8. **Align molecules** when visualizing SAR series\n\n## Error Handling\n\n```python\n# Safe molecule creation\ndef safe_to_mol(smiles):\n    try:\n        mol = dm.to_mol(smiles)\n        if mol is not None:\n            mol = dm.standardize_mol(mol)\n        return mol\n    except Exception as e:\n        print(f\"Failed to process {smiles}: {e}\")\n        return None\n\n# Safe batch processing\nvalid_mols = []\nfor smiles in smiles_list:\n    mol = safe_to_mol(smiles)\n    if mol is not None:\n        valid_mols.append(mol)\n```\n\n## Integration with Machine Learning\n\n```python\n# Feature generation\nX = np.array([dm.to_fp(mol) for mol in mols])\n\n# Or descriptors\ndesc_df = dm.descriptors.batch_compute_many_descriptors(mols, n_jobs=-1)\nX = desc_df.values\n\n# Train model\nfrom sklearn.ensemble import RandomForestRegressor\nmodel = RandomForestRegressor()\nmodel.fit(X, y_target)\n\n# Predict\npredictions = model.predict(X_test)\n```\n\n## Troubleshooting\n\n**Issue**: Molecule parsing fails\n- **Solution**: Use `dm.standardize_smiles()` first or try `dm.fix_mol()`\n\n**Issue**: Memory errors with clustering\n- **Solution**: Use `dm.pick_diverse()` instead of full clustering for large sets\n\n**Issue**: Slow conformer generation\n- **Solution**: Reduce `n_confs` or increase `rms_cutoff` to generate fewer conformers\n\n**Issue**: Remote file access fails\n- **Solution**: Ensure fsspec and appropriate cloud provider libraries are installed (s3fs, gcsfs, etc.)\n\n## Additional Resources\n\n- **Datamol Documentation**: https://docs.datamol.io/\n- **RDKit Documentation**: https://www.rdkit.org/docs/\n- **GitHub Repository**: https://github.com/datamol-io/datamol\n",
        "data/k-dense-ai/deepchem/SKILL.md": "---\nname: deepchem\ndescription: \"Molecular machine learning toolkit. Property prediction (ADMET, toxicity), GNNs (GCN, MPNN), MoleculeNet benchmarks, pretrained models, featurization, for drug discovery ML.\"\n---\n\n# DeepChem\n\n## Overview\n\nDeepChem is a comprehensive Python library for applying machine learning to chemistry, materials science, and biology. Enable molecular property prediction, drug discovery, materials design, and biomolecule analysis through specialized neural networks, molecular featurization methods, and pretrained models.\n\n## When to Use This Skill\n\nThis skill should be used when:\n- Loading and processing molecular data (SMILES strings, SDF files, protein sequences)\n- Predicting molecular properties (solubility, toxicity, binding affinity, ADMET properties)\n- Training models on chemical/biological datasets\n- Using MoleculeNet benchmark datasets (Tox21, BBBP, Delaney, etc.)\n- Converting molecules to ML-ready features (fingerprints, graph representations, descriptors)\n- Implementing graph neural networks for molecules (GCN, GAT, MPNN, AttentiveFP)\n- Applying transfer learning with pretrained models (ChemBERTa, GROVER, MolFormer)\n- Predicting crystal/materials properties (bandgap, formation energy)\n- Analyzing protein or DNA sequences\n\n## Core Capabilities\n\n### 1. Molecular Data Loading and Processing\n\nDeepChem provides specialized loaders for various chemical data formats:\n\n```python\nimport deepchem as dc\n\n# Load CSV with SMILES\nfeaturizer = dc.feat.CircularFingerprint(radius=2, size=2048)\nloader = dc.data.CSVLoader(\n    tasks=['solubility', 'toxicity'],\n    feature_field='smiles',\n    featurizer=featurizer\n)\ndataset = loader.create_dataset('molecules.csv')\n\n# Load SDF files\nloader = dc.data.SDFLoader(tasks=['activity'], featurizer=featurizer)\ndataset = loader.create_dataset('compounds.sdf')\n\n# Load protein sequences\nloader = dc.data.FASTALoader()\ndataset = loader.create_dataset('proteins.fasta')\n```\n\n**Key Loaders**:\n- `CSVLoader`: Tabular data with molecular identifiers\n- `SDFLoader`: Molecular structure files\n- `FASTALoader`: Protein/DNA sequences\n- `ImageLoader`: Molecular images\n- `JsonLoader`: JSON-formatted datasets\n\n### 2. Molecular Featurization\n\nConvert molecules into numerical representations for ML models.\n\n#### Decision Tree for Featurizer Selection\n\n```\nIs the model a graph neural network?\n YES  Use graph featurizers\n    Standard GNN  MolGraphConvFeaturizer\n    Message passing  DMPNNFeaturizer\n    Pretrained  GroverFeaturizer\n\n NO  What type of model?\n     Traditional ML (RF, XGBoost, SVM)\n        Fast baseline  CircularFingerprint (ECFP)\n        Interpretable  RDKitDescriptors\n        Maximum coverage  MordredDescriptors\n    \n     Deep learning (non-graph)\n        Dense networks  CircularFingerprint\n        CNN  SmilesToImage\n    \n     Sequence models (LSTM, Transformer)\n        SmilesToSeq\n    \n     3D structure analysis\n         CoulombMatrix\n```\n\n#### Example Featurization\n\n```python\n# Fingerprints (for traditional ML)\nfp = dc.feat.CircularFingerprint(radius=2, size=2048)\n\n# Descriptors (for interpretable models)\ndesc = dc.feat.RDKitDescriptors()\n\n# Graph features (for GNNs)\ngraph_feat = dc.feat.MolGraphConvFeaturizer()\n\n# Apply featurization\nfeatures = fp.featurize(['CCO', 'c1ccccc1'])\n```\n\n**Selection Guide**:\n- **Small datasets (<1K)**: CircularFingerprint or RDKitDescriptors\n- **Medium datasets (1K-100K)**: CircularFingerprint or graph featurizers\n- **Large datasets (>100K)**: Graph featurizers (MolGraphConvFeaturizer, DMPNNFeaturizer)\n- **Transfer learning**: Pretrained model featurizers (GroverFeaturizer)\n\nSee `references/api_reference.md` for complete featurizer documentation.\n\n### 3. Data Splitting\n\n**Critical**: For drug discovery tasks, use `ScaffoldSplitter` to prevent data leakage from similar molecular structures appearing in both training and test sets.\n\n```python\n# Scaffold splitting (recommended for molecules)\nsplitter = dc.splits.ScaffoldSplitter()\ntrain, valid, test = splitter.train_valid_test_split(\n    dataset,\n    frac_train=0.8,\n    frac_valid=0.1,\n    frac_test=0.1\n)\n\n# Random splitting (for non-molecular data)\nsplitter = dc.splits.RandomSplitter()\ntrain, test = splitter.train_test_split(dataset)\n\n# Stratified splitting (for imbalanced classification)\nsplitter = dc.splits.RandomStratifiedSplitter()\ntrain, test = splitter.train_test_split(dataset)\n```\n\n**Available Splitters**:\n- `ScaffoldSplitter`: Split by molecular scaffolds (prevents leakage)\n- `ButinaSplitter`: Clustering-based molecular splitting\n- `MaxMinSplitter`: Maximize diversity between sets\n- `RandomSplitter`: Random splitting\n- `RandomStratifiedSplitter`: Preserves class distributions\n\n### 4. Model Selection and Training\n\n#### Quick Model Selection Guide\n\n| Dataset Size | Task | Recommended Model | Featurizer |\n|-------------|------|-------------------|------------|\n| < 1K samples | Any | SklearnModel (RandomForest) | CircularFingerprint |\n| 1K-100K | Classification/Regression | GBDTModel or MultitaskRegressor | CircularFingerprint |\n| > 100K | Molecular properties | GCNModel, AttentiveFPModel, DMPNNModel | MolGraphConvFeaturizer |\n| Any (small preferred) | Transfer learning | ChemBERTa, GROVER, MolFormer | Model-specific |\n| Crystal structures | Materials properties | CGCNNModel, MEGNetModel | Structure-based |\n| Protein sequences | Protein properties | ProtBERT | Sequence-based |\n\n#### Example: Traditional ML\n```python\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Wrap scikit-learn model\nsklearn_model = RandomForestRegressor(n_estimators=100)\nmodel = dc.models.SklearnModel(model=sklearn_model)\nmodel.fit(train)\n```\n\n#### Example: Deep Learning\n```python\n# Multitask regressor (for fingerprints)\nmodel = dc.models.MultitaskRegressor(\n    n_tasks=2,\n    n_features=2048,\n    layer_sizes=[1000, 500],\n    dropouts=0.25,\n    learning_rate=0.001\n)\nmodel.fit(train, nb_epoch=50)\n```\n\n#### Example: Graph Neural Networks\n```python\n# Graph Convolutional Network\nmodel = dc.models.GCNModel(\n    n_tasks=1,\n    mode='regression',\n    batch_size=128,\n    learning_rate=0.001\n)\nmodel.fit(train, nb_epoch=50)\n\n# Graph Attention Network\nmodel = dc.models.GATModel(n_tasks=1, mode='classification')\nmodel.fit(train, nb_epoch=50)\n\n# Attentive Fingerprint\nmodel = dc.models.AttentiveFPModel(n_tasks=1, mode='regression')\nmodel.fit(train, nb_epoch=50)\n```\n\n### 5. MoleculeNet Benchmarks\n\nQuick access to 30+ curated benchmark datasets with standardized train/valid/test splits:\n\n```python\n# Load benchmark dataset\ntasks, datasets, transformers = dc.molnet.load_tox21(\n    featurizer='GraphConv',  # or 'ECFP', 'Weave', 'Raw'\n    splitter='scaffold',     # or 'random', 'stratified'\n    reload=False\n)\ntrain, valid, test = datasets\n\n# Train and evaluate\nmodel = dc.models.GCNModel(n_tasks=len(tasks), mode='classification')\nmodel.fit(train, nb_epoch=50)\n\nmetric = dc.metrics.Metric(dc.metrics.roc_auc_score)\ntest_score = model.evaluate(test, [metric])\n```\n\n**Common Datasets**:\n- **Classification**: `load_tox21()`, `load_bbbp()`, `load_hiv()`, `load_clintox()`\n- **Regression**: `load_delaney()`, `load_freesolv()`, `load_lipo()`\n- **Quantum properties**: `load_qm7()`, `load_qm8()`, `load_qm9()`\n- **Materials**: `load_perovskite()`, `load_bandgap()`, `load_mp_formation_energy()`\n\nSee `references/api_reference.md` for complete dataset list.\n\n### 6. Transfer Learning\n\nLeverage pretrained models for improved performance, especially on small datasets:\n\n```python\n# ChemBERTa (BERT pretrained on 77M molecules)\nmodel = dc.models.HuggingFaceModel(\n    model='seyonec/ChemBERTa-zinc-base-v1',\n    task='classification',\n    n_tasks=1,\n    learning_rate=2e-5  # Lower LR for fine-tuning\n)\nmodel.fit(train, nb_epoch=10)\n\n# GROVER (graph transformer pretrained on 10M molecules)\nmodel = dc.models.GroverModel(\n    task='regression',\n    n_tasks=1\n)\nmodel.fit(train, nb_epoch=20)\n```\n\n**When to use transfer learning**:\n- Small datasets (< 1000 samples)\n- Novel molecular scaffolds\n- Limited computational resources\n- Need for rapid prototyping\n\nUse the `scripts/transfer_learning.py` script for guided transfer learning workflows.\n\n### 7. Model Evaluation\n\n```python\n# Define metrics\nclassification_metrics = [\n    dc.metrics.Metric(dc.metrics.roc_auc_score, name='ROC-AUC'),\n    dc.metrics.Metric(dc.metrics.accuracy_score, name='Accuracy'),\n    dc.metrics.Metric(dc.metrics.f1_score, name='F1')\n]\n\nregression_metrics = [\n    dc.metrics.Metric(dc.metrics.r2_score, name='R'),\n    dc.metrics.Metric(dc.metrics.mean_absolute_error, name='MAE'),\n    dc.metrics.Metric(dc.metrics.root_mean_squared_error, name='RMSE')\n]\n\n# Evaluate\ntrain_scores = model.evaluate(train, classification_metrics)\ntest_scores = model.evaluate(test, classification_metrics)\n```\n\n### 8. Making Predictions\n\n```python\n# Predict on test set\npredictions = model.predict(test)\n\n# Predict on new molecules\nnew_smiles = ['CCO', 'c1ccccc1', 'CC(C)O']\nnew_features = featurizer.featurize(new_smiles)\nnew_dataset = dc.data.NumpyDataset(X=new_features)\n\n# Apply same transformations as training\nfor transformer in transformers:\n    new_dataset = transformer.transform(new_dataset)\n\npredictions = model.predict(new_dataset)\n```\n\n## Typical Workflows\n\n### Workflow A: Quick Benchmark Evaluation\n\nFor evaluating a model on standard benchmarks:\n\n```python\nimport deepchem as dc\n\n# 1. Load benchmark\ntasks, datasets, _ = dc.molnet.load_bbbp(\n    featurizer='GraphConv',\n    splitter='scaffold'\n)\ntrain, valid, test = datasets\n\n# 2. Train model\nmodel = dc.models.GCNModel(n_tasks=len(tasks), mode='classification')\nmodel.fit(train, nb_epoch=50)\n\n# 3. Evaluate\nmetric = dc.metrics.Metric(dc.metrics.roc_auc_score)\ntest_score = model.evaluate(test, [metric])\nprint(f\"Test ROC-AUC: {test_score}\")\n```\n\n### Workflow B: Custom Data Prediction\n\nFor training on custom molecular datasets:\n\n```python\nimport deepchem as dc\n\n# 1. Load and featurize data\nfeaturizer = dc.feat.CircularFingerprint(radius=2, size=2048)\nloader = dc.data.CSVLoader(\n    tasks=['activity'],\n    feature_field='smiles',\n    featurizer=featurizer\n)\ndataset = loader.create_dataset('my_molecules.csv')\n\n# 2. Split data (use ScaffoldSplitter for molecules!)\nsplitter = dc.splits.ScaffoldSplitter()\ntrain, valid, test = splitter.train_valid_test_split(dataset)\n\n# 3. Normalize (optional but recommended)\ntransformers = [dc.trans.NormalizationTransformer(\n    transform_y=True, dataset=train\n)]\nfor transformer in transformers:\n    train = transformer.transform(train)\n    valid = transformer.transform(valid)\n    test = transformer.transform(test)\n\n# 4. Train model\nmodel = dc.models.MultitaskRegressor(\n    n_tasks=1,\n    n_features=2048,\n    layer_sizes=[1000, 500],\n    dropouts=0.25\n)\nmodel.fit(train, nb_epoch=50)\n\n# 5. Evaluate\nmetric = dc.metrics.Metric(dc.metrics.r2_score)\ntest_score = model.evaluate(test, [metric])\n```\n\n### Workflow C: Transfer Learning on Small Dataset\n\nFor leveraging pretrained models:\n\n```python\nimport deepchem as dc\n\n# 1. Load data (pretrained models often need raw SMILES)\nloader = dc.data.CSVLoader(\n    tasks=['activity'],\n    feature_field='smiles',\n    featurizer=dc.feat.DummyFeaturizer()  # Model handles featurization\n)\ndataset = loader.create_dataset('small_dataset.csv')\n\n# 2. Split data\nsplitter = dc.splits.ScaffoldSplitter()\ntrain, test = splitter.train_test_split(dataset)\n\n# 3. Load pretrained model\nmodel = dc.models.HuggingFaceModel(\n    model='seyonec/ChemBERTa-zinc-base-v1',\n    task='classification',\n    n_tasks=1,\n    learning_rate=2e-5\n)\n\n# 4. Fine-tune\nmodel.fit(train, nb_epoch=10)\n\n# 5. Evaluate\npredictions = model.predict(test)\n```\n\nSee `references/workflows.md` for 8 detailed workflow examples covering molecular generation, materials science, protein analysis, and more.\n\n## Example Scripts\n\nThis skill includes three production-ready scripts in the `scripts/` directory:\n\n### 1. `predict_solubility.py`\nTrain and evaluate solubility prediction models. Works with Delaney benchmark or custom CSV data.\n\n```bash\n# Use Delaney benchmark\npython scripts/predict_solubility.py\n\n# Use custom data\npython scripts/predict_solubility.py \\\n    --data my_data.csv \\\n    --smiles-col smiles \\\n    --target-col solubility \\\n    --predict \"CCO\" \"c1ccccc1\"\n```\n\n### 2. `graph_neural_network.py`\nTrain various graph neural network architectures on molecular data.\n\n```bash\n# Train GCN on Tox21\npython scripts/graph_neural_network.py --model gcn --dataset tox21\n\n# Train AttentiveFP on custom data\npython scripts/graph_neural_network.py \\\n    --model attentivefp \\\n    --data molecules.csv \\\n    --task-type regression \\\n    --targets activity \\\n    --epochs 100\n```\n\n### 3. `transfer_learning.py`\nFine-tune pretrained models (ChemBERTa, GROVER) on molecular property prediction tasks.\n\n```bash\n# Fine-tune ChemBERTa on BBBP\npython scripts/transfer_learning.py --model chemberta --dataset bbbp\n\n# Fine-tune GROVER on custom data\npython scripts/transfer_learning.py \\\n    --model grover \\\n    --data small_dataset.csv \\\n    --target activity \\\n    --task-type classification \\\n    --epochs 20\n```\n\n## Common Patterns and Best Practices\n\n### Pattern 1: Always Use Scaffold Splitting for Molecules\n```python\n# GOOD: Prevents data leakage\nsplitter = dc.splits.ScaffoldSplitter()\ntrain, test = splitter.train_test_split(dataset)\n\n# BAD: Similar molecules in train and test\nsplitter = dc.splits.RandomSplitter()\ntrain, test = splitter.train_test_split(dataset)\n```\n\n### Pattern 2: Normalize Features and Targets\n```python\ntransformers = [\n    dc.trans.NormalizationTransformer(\n        transform_y=True,  # Also normalize target values\n        dataset=train\n    )\n]\nfor transformer in transformers:\n    train = transformer.transform(train)\n    test = transformer.transform(test)\n```\n\n### Pattern 3: Start Simple, Then Scale\n1. Start with Random Forest + CircularFingerprint (fast baseline)\n2. Try XGBoost/LightGBM if RF works well\n3. Move to deep learning (MultitaskRegressor) if you have >5K samples\n4. Try GNNs if you have >10K samples\n5. Use transfer learning for small datasets or novel scaffolds\n\n### Pattern 4: Handle Imbalanced Data\n```python\n# Option 1: Balancing transformer\ntransformer = dc.trans.BalancingTransformer(dataset=train)\ntrain = transformer.transform(train)\n\n# Option 2: Use balanced metrics\nmetric = dc.metrics.Metric(dc.metrics.balanced_accuracy_score)\n```\n\n### Pattern 5: Avoid Memory Issues\n```python\n# Use DiskDataset for large datasets\ndataset = dc.data.DiskDataset.from_numpy(X, y, w, ids)\n\n# Use smaller batch sizes\nmodel = dc.models.GCNModel(batch_size=32)  # Instead of 128\n```\n\n## Common Pitfalls\n\n### Issue 1: Data Leakage in Drug Discovery\n**Problem**: Using random splitting allows similar molecules in train/test sets.\n**Solution**: Always use `ScaffoldSplitter` for molecular datasets.\n\n### Issue 2: GNN Underperforming vs Fingerprints\n**Problem**: Graph neural networks perform worse than simple fingerprints.\n**Solutions**:\n- Ensure dataset is large enough (>10K samples typically)\n- Increase training epochs (50-100)\n- Try different architectures (AttentiveFP, DMPNN instead of GCN)\n- Use pretrained models (GROVER)\n\n### Issue 3: Overfitting on Small Datasets\n**Problem**: Model memorizes training data.\n**Solutions**:\n- Use stronger regularization (increase dropout to 0.5)\n- Use simpler models (Random Forest instead of deep learning)\n- Apply transfer learning (ChemBERTa, GROVER)\n- Collect more data\n\n### Issue 4: Import Errors\n**Problem**: Module not found errors.\n**Solution**: Ensure DeepChem is installed with required dependencies:\n```bash\nuv pip install deepchem\n# For PyTorch models\nuv pip install deepchem[torch]\n# For all features\nuv pip install deepchem[all]\n```\n\n## Reference Documentation\n\nThis skill includes comprehensive reference documentation:\n\n### `references/api_reference.md`\nComplete API documentation including:\n- All data loaders and their use cases\n- Dataset classes and when to use each\n- Complete featurizer catalog with selection guide\n- Model catalog organized by category (50+ models)\n- MoleculeNet dataset descriptions\n- Metrics and evaluation functions\n- Common code patterns\n\n**When to reference**: Search this file when you need specific API details, parameter names, or want to explore available options.\n\n### `references/workflows.md`\nEight detailed end-to-end workflows:\n1. Molecular property prediction from SMILES\n2. Using MoleculeNet benchmarks\n3. Hyperparameter optimization\n4. Transfer learning with pretrained models\n5. Molecular generation with GANs\n6. Materials property prediction\n7. Protein sequence analysis\n8. Custom model integration\n\n**When to reference**: Use these workflows as templates for implementing complete solutions.\n\n## Installation Notes\n\nBasic installation:\n```bash\nuv pip install deepchem\n```\n\nFor PyTorch models (GCN, GAT, etc.):\n```bash\nuv pip install deepchem[torch]\n```\n\nFor all features:\n```bash\nuv pip install deepchem[all]\n```\n\nIf import errors occur, the user may need specific dependencies. Check the DeepChem documentation for detailed installation instructions.\n\n## Additional Resources\n\n- Official documentation: https://deepchem.readthedocs.io/\n- GitHub repository: https://github.com/deepchem/deepchem\n- Tutorials: https://deepchem.readthedocs.io/en/latest/get_started/tutorials.html\n- Paper: \"MoleculeNet: A Benchmark for Molecular Machine Learning\"\n",
        "data/k-dense-ai/deeptools/SKILL.md": "---\nname: deeptools\ndescription: \"NGS analysis toolkit. BAM to bigWig conversion, QC (correlation, PCA, fingerprints), heatmaps/profiles (TSS, peaks), for ChIP-seq, RNA-seq, ATAC-seq visualization.\"\n---\n\n# deepTools: NGS Data Analysis Toolkit\n\n## Overview\n\ndeepTools is a comprehensive suite of Python command-line tools designed for processing and analyzing high-throughput sequencing data. Use deepTools to perform quality control, normalize data, compare samples, and generate publication-quality visualizations for ChIP-seq, RNA-seq, ATAC-seq, MNase-seq, and other NGS experiments.\n\n**Core capabilities:**\n- Convert BAM alignments to normalized coverage tracks (bigWig/bedGraph)\n- Quality control assessment (fingerprint, correlation, coverage)\n- Sample comparison and correlation analysis\n- Heatmap and profile plot generation around genomic features\n- Enrichment analysis and peak region visualization\n\n## When to Use This Skill\n\nThis skill should be used when:\n\n- **File conversion**: \"Convert BAM to bigWig\", \"generate coverage tracks\", \"normalize ChIP-seq data\"\n- **Quality control**: \"check ChIP quality\", \"compare replicates\", \"assess sequencing depth\", \"QC analysis\"\n- **Visualization**: \"create heatmap around TSS\", \"plot ChIP signal\", \"visualize enrichment\", \"generate profile plot\"\n- **Sample comparison**: \"compare treatment vs control\", \"correlate samples\", \"PCA analysis\"\n- **Analysis workflows**: \"analyze ChIP-seq data\", \"RNA-seq coverage\", \"ATAC-seq analysis\", \"complete workflow\"\n- **Working with specific file types**: BAM files, bigWig files, BED region files in genomics context\n\n## Quick Start\n\nFor users new to deepTools, start with file validation and common workflows:\n\n### 1. Validate Input Files\n\nBefore running any analysis, validate BAM, bigWig, and BED files using the validation script:\n\n```bash\npython scripts/validate_files.py --bam sample1.bam sample2.bam --bed regions.bed\n```\n\nThis checks file existence, BAM indices, and format correctness.\n\n### 2. Generate Workflow Template\n\nFor standard analyses, use the workflow generator to create customized scripts:\n\n```bash\n# List available workflows\npython scripts/workflow_generator.py --list\n\n# Generate ChIP-seq QC workflow\npython scripts/workflow_generator.py chipseq_qc -o qc_workflow.sh \\\n    --input-bam Input.bam --chip-bams \"ChIP1.bam ChIP2.bam\" \\\n    --genome-size 2913022398\n\n# Make executable and run\nchmod +x qc_workflow.sh\n./qc_workflow.sh\n```\n\n### 3. Most Common Operations\n\nSee `assets/quick_reference.md` for frequently used commands and parameters.\n\n## Installation\n\n```bash\nuv pip install deeptools\n```\n\n## Core Workflows\n\ndeepTools workflows typically follow this pattern: **QC  Normalization  Comparison/Visualization**\n\n### ChIP-seq Quality Control Workflow\n\nWhen users request ChIP-seq QC or quality assessment:\n\n1. **Generate workflow script** using `scripts/workflow_generator.py chipseq_qc`\n2. **Key QC steps**:\n   - Sample correlation (multiBamSummary + plotCorrelation)\n   - PCA analysis (plotPCA)\n   - Coverage assessment (plotCoverage)\n   - Fragment size validation (bamPEFragmentSize)\n   - ChIP enrichment strength (plotFingerprint)\n\n**Interpreting results:**\n- **Correlation**: Replicates should cluster together with high correlation (>0.9)\n- **Fingerprint**: Strong ChIP shows steep rise; flat diagonal indicates poor enrichment\n- **Coverage**: Assess if sequencing depth is adequate for analysis\n\nFull workflow details in `references/workflows.md`  \"ChIP-seq Quality Control Workflow\"\n\n### ChIP-seq Complete Analysis Workflow\n\nFor full ChIP-seq analysis from BAM to visualizations:\n\n1. **Generate coverage tracks** with normalization (bamCoverage)\n2. **Create comparison tracks** (bamCompare for log2 ratio)\n3. **Compute signal matrices** around features (computeMatrix)\n4. **Generate visualizations** (plotHeatmap, plotProfile)\n5. **Enrichment analysis** at peaks (plotEnrichment)\n\nUse `scripts/workflow_generator.py chipseq_analysis` to generate template.\n\nComplete command sequences in `references/workflows.md`  \"ChIP-seq Analysis Workflow\"\n\n### RNA-seq Coverage Workflow\n\nFor strand-specific RNA-seq coverage tracks:\n\nUse bamCoverage with `--filterRNAstrand` to separate forward and reverse strands.\n\n**Important:** NEVER use `--extendReads` for RNA-seq (would extend over splice junctions).\n\nUse normalization: CPM for fixed bins, RPKM for gene-level analysis.\n\nTemplate available: `scripts/workflow_generator.py rnaseq_coverage`\n\nDetails in `references/workflows.md`  \"RNA-seq Coverage Workflow\"\n\n### ATAC-seq Analysis Workflow\n\nATAC-seq requires Tn5 offset correction:\n\n1. **Shift reads** using alignmentSieve with `--ATACshift`\n2. **Generate coverage** with bamCoverage\n3. **Analyze fragment sizes** (expect nucleosome ladder pattern)\n4. **Visualize at peaks** if available\n\nTemplate: `scripts/workflow_generator.py atacseq`\n\nFull workflow in `references/workflows.md`  \"ATAC-seq Workflow\"\n\n## Tool Categories and Common Tasks\n\n### BAM/bigWig Processing\n\n**Convert BAM to normalized coverage:**\n```bash\nbamCoverage --bam input.bam --outFileName output.bw \\\n    --normalizeUsing RPGC --effectiveGenomeSize 2913022398 \\\n    --binSize 10 --numberOfProcessors 8\n```\n\n**Compare two samples (log2 ratio):**\n```bash\nbamCompare -b1 treatment.bam -b2 control.bam -o ratio.bw \\\n    --operation log2 --scaleFactorsMethod readCount\n```\n\n**Key tools:** bamCoverage, bamCompare, multiBamSummary, multiBigwigSummary, correctGCBias, alignmentSieve\n\nComplete reference: `references/tools_reference.md`  \"BAM and bigWig File Processing Tools\"\n\n### Quality Control\n\n**Check ChIP enrichment:**\n```bash\nplotFingerprint -b input.bam chip.bam -o fingerprint.png \\\n    --extendReads 200 --ignoreDuplicates\n```\n\n**Sample correlation:**\n```bash\nmultiBamSummary bins --bamfiles *.bam -o counts.npz\nplotCorrelation -in counts.npz --corMethod pearson \\\n    --whatToShow heatmap -o correlation.png\n```\n\n**Key tools:** plotFingerprint, plotCoverage, plotCorrelation, plotPCA, bamPEFragmentSize\n\nComplete reference: `references/tools_reference.md`  \"Quality Control Tools\"\n\n### Visualization\n\n**Create heatmap around TSS:**\n```bash\n# Compute matrix\ncomputeMatrix reference-point -S signal.bw -R genes.bed \\\n    -b 3000 -a 3000 --referencePoint TSS -o matrix.gz\n\n# Generate heatmap\nplotHeatmap -m matrix.gz -o heatmap.png \\\n    --colorMap RdBu --kmeans 3\n```\n\n**Create profile plot:**\n```bash\nplotProfile -m matrix.gz -o profile.png \\\n    --plotType lines --colors blue red\n```\n\n**Key tools:** computeMatrix, plotHeatmap, plotProfile, plotEnrichment\n\nComplete reference: `references/tools_reference.md`  \"Visualization Tools\"\n\n## Normalization Methods\n\nChoosing the correct normalization is critical for valid comparisons. Consult `references/normalization_methods.md` for comprehensive guidance.\n\n**Quick selection guide:**\n\n- **ChIP-seq coverage**: Use RPGC or CPM\n- **ChIP-seq comparison**: Use bamCompare with log2 and readCount\n- **RNA-seq bins**: Use CPM\n- **RNA-seq genes**: Use RPKM (accounts for gene length)\n- **ATAC-seq**: Use RPGC or CPM\n\n**Normalization methods:**\n- **RPGC**: 1 genome coverage (requires --effectiveGenomeSize)\n- **CPM**: Counts per million mapped reads\n- **RPKM**: Reads per kb per million (accounts for region length)\n- **BPM**: Bins per million\n- **None**: Raw counts (not recommended for comparisons)\n\nFull explanation: `references/normalization_methods.md`\n\n## Effective Genome Sizes\n\nRPGC normalization requires effective genome size. Common values:\n\n| Organism | Assembly | Size | Usage |\n|----------|----------|------|-------|\n| Human | GRCh38/hg38 | 2,913,022,398 | `--effectiveGenomeSize 2913022398` |\n| Mouse | GRCm38/mm10 | 2,652,783,500 | `--effectiveGenomeSize 2652783500` |\n| Zebrafish | GRCz11 | 1,368,780,147 | `--effectiveGenomeSize 1368780147` |\n| *Drosophila* | dm6 | 142,573,017 | `--effectiveGenomeSize 142573017` |\n| *C. elegans* | ce10/ce11 | 100,286,401 | `--effectiveGenomeSize 100286401` |\n\nComplete table with read-length-specific values: `references/effective_genome_sizes.md`\n\n## Common Parameters Across Tools\n\nMany deepTools commands share these options:\n\n**Performance:**\n- `--numberOfProcessors, -p`: Enable parallel processing (always use available cores)\n- `--region`: Process specific regions for testing (e.g., `chr1:1-1000000`)\n\n**Read Filtering:**\n- `--ignoreDuplicates`: Remove PCR duplicates (recommended for most analyses)\n- `--minMappingQuality`: Filter by alignment quality (e.g., `--minMappingQuality 10`)\n- `--minFragmentLength` / `--maxFragmentLength`: Fragment length bounds\n- `--samFlagInclude` / `--samFlagExclude`: SAM flag filtering\n\n**Read Processing:**\n- `--extendReads`: Extend to fragment length (ChIP-seq: YES, RNA-seq: NO)\n- `--centerReads`: Center at fragment midpoint for sharper signals\n\n## Best Practices\n\n### File Validation\n**Always validate files first** using `scripts/validate_files.py` to check:\n- File existence and readability\n- BAM indices present (.bai files)\n- BED format correctness\n- File sizes reasonable\n\n### Analysis Strategy\n\n1. **Start with QC**: Run correlation, coverage, and fingerprint analysis before proceeding\n2. **Test on small regions**: Use `--region chr1:1-10000000` for parameter testing\n3. **Document commands**: Save full command lines for reproducibility\n4. **Use consistent normalization**: Apply same method across samples in comparisons\n5. **Verify genome assembly**: Ensure BAM and BED files use matching genome builds\n\n### ChIP-seq Specific\n\n- **Always extend reads** for ChIP-seq: `--extendReads 200`\n- **Remove duplicates**: Use `--ignoreDuplicates` in most cases\n- **Check enrichment first**: Run plotFingerprint before detailed analysis\n- **GC correction**: Only apply if significant bias detected; never use `--ignoreDuplicates` after GC correction\n\n### RNA-seq Specific\n\n- **Never extend reads** for RNA-seq (would span splice junctions)\n- **Strand-specific**: Use `--filterRNAstrand forward/reverse` for stranded libraries\n- **Normalization**: CPM for bins, RPKM for genes\n\n### ATAC-seq Specific\n\n- **Apply Tn5 correction**: Use alignmentSieve with `--ATACshift`\n- **Fragment filtering**: Set appropriate min/max fragment lengths\n- **Check nucleosome pattern**: Fragment size plot should show ladder pattern\n\n### Performance Optimization\n\n1. **Use multiple processors**: `--numberOfProcessors 8` (or available cores)\n2. **Increase bin size** for faster processing and smaller files\n3. **Process chromosomes separately** for memory-limited systems\n4. **Pre-filter BAM files** using alignmentSieve to create reusable filtered files\n5. **Use bigWig over bedGraph**: Compressed and faster to process\n\n## Troubleshooting\n\n### Common Issues\n\n**BAM index missing:**\n```bash\nsamtools index input.bam\n```\n\n**Out of memory:**\nProcess chromosomes individually using `--region`:\n```bash\nbamCoverage --bam input.bam -o chr1.bw --region chr1\n```\n\n**Slow processing:**\nIncrease `--numberOfProcessors` and/or increase `--binSize`\n\n**bigWig files too large:**\nIncrease bin size: `--binSize 50` or larger\n\n### Validation Errors\n\nRun validation script to identify issues:\n```bash\npython scripts/validate_files.py --bam *.bam --bed regions.bed\n```\n\nCommon errors and solutions explained in script output.\n\n## Reference Documentation\n\nThis skill includes comprehensive reference documentation:\n\n### references/tools_reference.md\nComplete documentation of all deepTools commands organized by category:\n- BAM and bigWig processing tools (9 tools)\n- Quality control tools (6 tools)\n- Visualization tools (3 tools)\n- Miscellaneous tools (2 tools)\n\nEach tool includes:\n- Purpose and overview\n- Key parameters with explanations\n- Usage examples\n- Important notes and best practices\n\n**Use this reference when:** Users ask about specific tools, parameters, or detailed usage.\n\n### references/workflows.md\nComplete workflow examples for common analyses:\n- ChIP-seq quality control workflow\n- ChIP-seq complete analysis workflow\n- RNA-seq coverage workflow\n- ATAC-seq analysis workflow\n- Multi-sample comparison workflow\n- Peak region analysis workflow\n- Troubleshooting and performance tips\n\n**Use this reference when:** Users need complete analysis pipelines or workflow examples.\n\n### references/normalization_methods.md\nComprehensive guide to normalization methods:\n- Detailed explanation of each method (RPGC, CPM, RPKM, BPM, etc.)\n- When to use each method\n- Formulas and interpretation\n- Selection guide by experiment type\n- Common pitfalls and solutions\n- Quick reference table\n\n**Use this reference when:** Users ask about normalization, comparing samples, or which method to use.\n\n### references/effective_genome_sizes.md\nEffective genome size values and usage:\n- Common organism values (human, mouse, fly, worm, zebrafish)\n- Read-length-specific values\n- Calculation methods\n- When and how to use in commands\n- Custom genome calculation instructions\n\n**Use this reference when:** Users need genome size for RPGC normalization or GC bias correction.\n\n## Helper Scripts\n\n### scripts/validate_files.py\n\nValidates BAM, bigWig, and BED files for deepTools analysis. Checks file existence, indices, and format.\n\n**Usage:**\n```bash\npython scripts/validate_files.py --bam sample1.bam sample2.bam \\\n    --bed peaks.bed --bigwig signal.bw\n```\n\n**When to use:** Before starting any analysis, or when troubleshooting errors.\n\n### scripts/workflow_generator.py\n\nGenerates customizable bash script templates for common deepTools workflows.\n\n**Available workflows:**\n- `chipseq_qc`: ChIP-seq quality control\n- `chipseq_analysis`: Complete ChIP-seq analysis\n- `rnaseq_coverage`: Strand-specific RNA-seq coverage\n- `atacseq`: ATAC-seq with Tn5 correction\n\n**Usage:**\n```bash\n# List workflows\npython scripts/workflow_generator.py --list\n\n# Generate workflow\npython scripts/workflow_generator.py chipseq_qc -o qc.sh \\\n    --input-bam Input.bam --chip-bams \"ChIP1.bam ChIP2.bam\" \\\n    --genome-size 2913022398 --threads 8\n\n# Run generated workflow\nchmod +x qc.sh\n./qc.sh\n```\n\n**When to use:** Users request standard workflows or need template scripts to customize.\n\n## Assets\n\n### assets/quick_reference.md\n\nQuick reference card with most common commands, effective genome sizes, and typical workflow pattern.\n\n**When to use:** Users need quick command examples without detailed documentation.\n\n## Handling User Requests\n\n### For New Users\n\n1. Start with installation verification\n2. Validate input files using `scripts/validate_files.py`\n3. Recommend appropriate workflow based on experiment type\n4. Generate workflow template using `scripts/workflow_generator.py`\n5. Guide through customization and execution\n\n### For Experienced Users\n\n1. Provide specific tool commands for requested operations\n2. Reference appropriate sections in `references/tools_reference.md`\n3. Suggest optimizations and best practices\n4. Offer troubleshooting for issues\n\n### For Specific Tasks\n\n**\"Convert BAM to bigWig\":**\n- Use bamCoverage with appropriate normalization\n- Recommend RPGC or CPM based on use case\n- Provide effective genome size for organism\n- Suggest relevant parameters (extendReads, ignoreDuplicates, binSize)\n\n**\"Check ChIP quality\":**\n- Run full QC workflow or use plotFingerprint specifically\n- Explain interpretation of results\n- Suggest follow-up actions based on results\n\n**\"Create heatmap\":**\n- Guide through two-step process: computeMatrix  plotHeatmap\n- Help choose appropriate matrix mode (reference-point vs scale-regions)\n- Suggest visualization parameters and clustering options\n\n**\"Compare samples\":**\n- Recommend bamCompare for two-sample comparison\n- Suggest multiBamSummary + plotCorrelation for multiple samples\n- Guide normalization method selection\n\n### Referencing Documentation\n\nWhen users need detailed information:\n- **Tool details**: Direct to specific sections in `references/tools_reference.md`\n- **Workflows**: Use `references/workflows.md` for complete analysis pipelines\n- **Normalization**: Consult `references/normalization_methods.md` for method selection\n- **Genome sizes**: Reference `references/effective_genome_sizes.md`\n\nSearch references using grep patterns:\n```bash\n# Find tool documentation\ngrep -A 20 \"^### toolname\" references/tools_reference.md\n\n# Find workflow\ngrep -A 50 \"^## Workflow Name\" references/workflows.md\n\n# Find normalization method\ngrep -A 15 \"^### Method Name\" references/normalization_methods.md\n```\n\n## Example Interactions\n\n**User: \"I need to analyze my ChIP-seq data\"**\n\nResponse approach:\n1. Ask about files available (BAM files, peaks, genes)\n2. Validate files using validation script\n3. Generate chipseq_analysis workflow template\n4. Customize for their specific files and organism\n5. Explain each step as script runs\n\n**User: \"Which normalization should I use?\"**\n\nResponse approach:\n1. Ask about experiment type (ChIP-seq, RNA-seq, etc.)\n2. Ask about comparison goal (within-sample or between-sample)\n3. Consult `references/normalization_methods.md` selection guide\n4. Recommend appropriate method with justification\n5. Provide command example with parameters\n\n**User: \"Create a heatmap around TSS\"**\n\nResponse approach:\n1. Verify bigWig and gene BED files available\n2. Use computeMatrix with reference-point mode at TSS\n3. Generate plotHeatmap with appropriate visualization parameters\n4. Suggest clustering if dataset is large\n5. Offer profile plot as complement\n\n## Key Reminders\n\n- **File validation first**: Always validate input files before analysis\n- **Normalization matters**: Choose appropriate method for comparison type\n- **Extend reads carefully**: YES for ChIP-seq, NO for RNA-seq\n- **Use all cores**: Set `--numberOfProcessors` to available cores\n- **Test on regions**: Use `--region` for parameter testing\n- **Check QC first**: Run quality control before detailed analysis\n- **Document everything**: Save commands for reproducibility\n- **Reference documentation**: Use comprehensive references for detailed guidance\n",
        "data/k-dense-ai/denario/SKILL.md": "---\nname: denario\ndescription: Multiagent AI system for scientific research assistance that automates research workflows from data analysis to publication. This skill should be used when generating research ideas from datasets, developing research methodologies, executing computational experiments, performing literature searches, or generating publication-ready papers in LaTeX format. Supports end-to-end research pipelines with customizable agent orchestration.\n---\n\n# Denario\n\n## Overview\n\nDenario is a multiagent AI system designed to automate scientific research workflows from initial data analysis through publication-ready manuscripts. Built on AG2 and LangGraph frameworks, it orchestrates multiple specialized agents to handle hypothesis generation, methodology development, computational analysis, and paper writing.\n\n## When to Use This Skill\n\nUse this skill when:\n- Analyzing datasets to generate novel research hypotheses\n- Developing structured research methodologies\n- Executing computational experiments and generating visualizations\n- Conducting literature searches for research context\n- Writing journal-formatted LaTeX papers from research results\n- Automating the complete research pipeline from data to publication\n\n## Installation\n\nInstall denario using uv (recommended):\n\n```bash\nuv init\nuv add \"denario[app]\"\n```\n\nOr using pip:\n\n```bash\nuv pip install \"denario[app]\"\n```\n\nFor Docker deployment or building from source, see `references/installation.md`.\n\n## LLM API Configuration\n\nDenario requires API keys from supported LLM providers. Supported providers include:\n- Google Vertex AI\n- OpenAI\n- Other LLM services compatible with AG2/LangGraph\n\nStore API keys securely using environment variables or `.env` files. For detailed configuration instructions including Vertex AI setup, see `references/llm_configuration.md`.\n\n## Core Research Workflow\n\nDenario follows a structured four-stage research pipeline:\n\n### 1. Data Description\n\nDefine the research context by specifying available data and tools:\n\n```python\nfrom denario import Denario\n\nden = Denario(project_dir=\"./my_research\")\nden.set_data_description(\"\"\"\nAvailable datasets: time-series data on X and Y\nTools: pandas, sklearn, matplotlib\nResearch domain: [specify domain]\n\"\"\")\n```\n\n### 2. Idea Generation\n\nGenerate research hypotheses from the data description:\n\n```python\nden.get_idea()\n```\n\nThis produces a research question or hypothesis based on the described data. Alternatively, provide a custom idea:\n\n```python\nden.set_idea(\"Custom research hypothesis\")\n```\n\n### 3. Methodology Development\n\nDevelop the research methodology:\n\n```python\nden.get_method()\n```\n\nThis creates a structured approach for investigating the hypothesis. Can also accept markdown files with custom methodologies:\n\n```python\nden.set_method(\"path/to/methodology.md\")\n```\n\n### 4. Results Generation\n\nExecute computational experiments and generate analysis:\n\n```python\nden.get_results()\n```\n\nThis runs the methodology, performs computations, creates visualizations, and produces findings. Can also provide pre-computed results:\n\n```python\nden.set_results(\"path/to/results.md\")\n```\n\n### 5. Paper Generation\n\nCreate a publication-ready LaTeX paper:\n\n```python\nfrom denario import Journal\n\nden.get_paper(journal=Journal.APS)\n```\n\nThe generated paper includes proper formatting for the specified journal, integrated figures, and complete LaTeX source.\n\n## Available Journals\n\nDenario supports multiple journal formatting styles:\n- `Journal.APS` - American Physical Society format\n- Additional journals may be available; check `references/research_pipeline.md` for the complete list\n\n## Launching the GUI\n\nRun the graphical user interface:\n\n```bash\ndenario run\n```\n\nThis launches a web-based interface for interactive research workflow management.\n\n## Common Workflows\n\n### End-to-End Research Pipeline\n\n```python\nfrom denario import Denario, Journal\n\n# Initialize project\nden = Denario(project_dir=\"./research_project\")\n\n# Define research context\nden.set_data_description(\"\"\"\nDataset: Time-series measurements of [phenomenon]\nAvailable tools: pandas, sklearn, scipy\nResearch goal: Investigate [research question]\n\"\"\")\n\n# Generate research idea\nden.get_idea()\n\n# Develop methodology\nden.get_method()\n\n# Execute analysis\nden.get_results()\n\n# Create publication\nden.get_paper(journal=Journal.APS)\n```\n\n### Hybrid Workflow (Custom + Automated)\n\n```python\n# Provide custom research idea\nden.set_idea(\"Investigate the correlation between X and Y using time-series analysis\")\n\n# Auto-generate methodology\nden.get_method()\n\n# Auto-generate results\nden.get_results()\n\n# Generate paper\nden.get_paper(journal=Journal.APS)\n```\n\n### Literature Search Integration\n\nFor literature search functionality and additional workflow examples, see `references/examples.md`.\n\n## Advanced Features\n\n- **Multiagent orchestration**: AG2 and LangGraph coordinate specialized agents for different research tasks\n- **Reproducible research**: All stages produce structured outputs that can be version-controlled\n- **Journal integration**: Automatic formatting for target publication venues\n- **Flexible input**: Manual or automated at each pipeline stage\n- **Docker deployment**: Containerized environment with LaTeX and all dependencies\n\n## Detailed References\n\nFor comprehensive documentation:\n- **Installation options**: `references/installation.md`\n- **LLM configuration**: `references/llm_configuration.md`\n- **Complete API reference**: `references/research_pipeline.md`\n- **Example workflows**: `references/examples.md`\n\n## Troubleshooting\n\nCommon issues and solutions:\n- **API key errors**: Ensure environment variables are set correctly (see `references/llm_configuration.md`)\n- **LaTeX compilation**: Install TeX distribution or use Docker image with pre-installed LaTeX\n- **Package conflicts**: Use virtual environments or Docker for isolation\n- **Python version**: Requires Python 3.12 or higher\n",
        "data/k-dense-ai/diffdock/SKILL.md": "---\nname: diffdock\ndescription: \"Diffusion-based molecular docking. Predict protein-ligand binding poses from PDB/SMILES, confidence scores, virtual screening, for structure-based drug design. Not for affinity prediction.\"\n---\n\n# DiffDock: Molecular Docking with Diffusion Models\n\n## Overview\n\nDiffDock is a diffusion-based deep learning tool for molecular docking that predicts 3D binding poses of small molecule ligands to protein targets. It represents the state-of-the-art in computational docking, crucial for structure-based drug discovery and chemical biology.\n\n**Core Capabilities:**\n- Predict ligand binding poses with high accuracy using deep learning\n- Support protein structures (PDB files) or sequences (via ESMFold)\n- Process single complexes or batch virtual screening campaigns\n- Generate confidence scores to assess prediction reliability\n- Handle diverse ligand inputs (SMILES, SDF, MOL2)\n\n**Key Distinction:** DiffDock predicts **binding poses** (3D structure) and **confidence** (prediction certainty), NOT binding affinity (G, Kd). Always combine with scoring functions (GNINA, MM/GBSA) for affinity assessment.\n\n## When to Use This Skill\n\nThis skill should be used when:\n\n- \"Dock this ligand to a protein\" or \"predict binding pose\"\n- \"Run molecular docking\" or \"perform protein-ligand docking\"\n- \"Virtual screening\" or \"screen compound library\"\n- \"Where does this molecule bind?\" or \"predict binding site\"\n- Structure-based drug design or lead optimization tasks\n- Tasks involving PDB files + SMILES strings or ligand structures\n- Batch docking of multiple protein-ligand pairs\n\n## Installation and Environment Setup\n\n### Check Environment Status\n\nBefore proceeding with DiffDock tasks, verify the environment setup:\n\n```bash\n# Use the provided setup checker\npython scripts/setup_check.py\n```\n\nThis script validates Python version, PyTorch with CUDA, PyTorch Geometric, RDKit, ESM, and other dependencies.\n\n### Installation Options\n\n**Option 1: Conda (Recommended)**\n```bash\ngit clone https://github.com/gcorso/DiffDock.git\ncd DiffDock\nconda env create --file environment.yml\nconda activate diffdock\n```\n\n**Option 2: Docker**\n```bash\ndocker pull rbgcsail/diffdock\ndocker run -it --gpus all --entrypoint /bin/bash rbgcsail/diffdock\nmicromamba activate diffdock\n```\n\n**Important Notes:**\n- GPU strongly recommended (10-100x speedup vs CPU)\n- First run pre-computes SO(2)/SO(3) lookup tables (~2-5 minutes)\n- Model checkpoints (~500MB) download automatically if not present\n\n## Core Workflows\n\n### Workflow 1: Single Protein-Ligand Docking\n\n**Use Case:** Dock one ligand to one protein target\n\n**Input Requirements:**\n- Protein: PDB file OR amino acid sequence\n- Ligand: SMILES string OR structure file (SDF/MOL2)\n\n**Command:**\n```bash\npython -m inference \\\n  --config default_inference_args.yaml \\\n  --protein_path protein.pdb \\\n  --ligand \"CC(=O)Oc1ccccc1C(=O)O\" \\\n  --out_dir results/single_docking/\n```\n\n**Alternative (protein sequence):**\n```bash\npython -m inference \\\n  --config default_inference_args.yaml \\\n  --protein_sequence \"MSKGEELFTGVVPILVELDGDVNGHKF...\" \\\n  --ligand ligand.sdf \\\n  --out_dir results/sequence_docking/\n```\n\n**Output Structure:**\n```\nresults/single_docking/\n rank_1.sdf          # Top-ranked pose\n rank_2.sdf          # Second-ranked pose\n ...\n rank_10.sdf         # 10th pose (default: 10 samples)\n confidence_scores.txt\n```\n\n### Workflow 2: Batch Processing Multiple Complexes\n\n**Use Case:** Dock multiple ligands to proteins, virtual screening campaigns\n\n**Step 1: Prepare Batch CSV**\n\nUse the provided script to create or validate batch input:\n\n```bash\n# Create template\npython scripts/prepare_batch_csv.py --create --output batch_input.csv\n\n# Validate existing CSV\npython scripts/prepare_batch_csv.py my_input.csv --validate\n```\n\n**CSV Format:**\n```csv\ncomplex_name,protein_path,ligand_description,protein_sequence\ncomplex1,protein1.pdb,CC(=O)Oc1ccccc1C(=O)O,\ncomplex2,,COc1ccc(C#N)cc1,MSKGEELFT...\ncomplex3,protein3.pdb,ligand3.sdf,\n```\n\n**Required Columns:**\n- `complex_name`: Unique identifier\n- `protein_path`: PDB file path (leave empty if using sequence)\n- `ligand_description`: SMILES string or ligand file path\n- `protein_sequence`: Amino acid sequence (leave empty if using PDB)\n\n**Step 2: Run Batch Docking**\n\n```bash\npython -m inference \\\n  --config default_inference_args.yaml \\\n  --protein_ligand_csv batch_input.csv \\\n  --out_dir results/batch/ \\\n  --batch_size 10\n```\n\n**For Large Virtual Screening (>100 compounds):**\n\nPre-compute protein embeddings for faster processing:\n```bash\n# Pre-compute embeddings\npython datasets/esm_embedding_preparation.py \\\n  --protein_ligand_csv screening_input.csv \\\n  --out_file protein_embeddings.pt\n\n# Run with pre-computed embeddings\npython -m inference \\\n  --config default_inference_args.yaml \\\n  --protein_ligand_csv screening_input.csv \\\n  --esm_embeddings_path protein_embeddings.pt \\\n  --out_dir results/screening/\n```\n\n### Workflow 3: Analyzing Results\n\nAfter docking completes, analyze confidence scores and rank predictions:\n\n```bash\n# Analyze all results\npython scripts/analyze_results.py results/batch/\n\n# Show top 5 per complex\npython scripts/analyze_results.py results/batch/ --top 5\n\n# Filter by confidence threshold\npython scripts/analyze_results.py results/batch/ --threshold 0.0\n\n# Export to CSV\npython scripts/analyze_results.py results/batch/ --export summary.csv\n\n# Show top 20 predictions across all complexes\npython scripts/analyze_results.py results/batch/ --best 20\n```\n\nThe analysis script:\n- Parses confidence scores from all predictions\n- Classifies as High (>0), Moderate (-1.5 to 0), or Low (<-1.5)\n- Ranks predictions within and across complexes\n- Generates statistical summaries\n- Exports results to CSV for downstream analysis\n\n## Confidence Score Interpretation\n\n**Understanding Scores:**\n\n| Score Range | Confidence Level | Interpretation |\n|------------|------------------|----------------|\n| **> 0** | High | Strong prediction, likely accurate |\n| **-1.5 to 0** | Moderate | Reasonable prediction, validate carefully |\n| **< -1.5** | Low | Uncertain prediction, requires validation |\n\n**Critical Notes:**\n1. **Confidence  Affinity**: High confidence means model certainty about structure, NOT strong binding\n2. **Context Matters**: Adjust expectations for:\n   - Large ligands (>500 Da): Lower confidence expected\n   - Multiple protein chains: May decrease confidence\n   - Novel protein families: May underperform\n3. **Multiple Samples**: Review top 3-5 predictions, look for consensus\n\n**For detailed guidance:** Read `references/confidence_and_limitations.md` using the Read tool\n\n## Parameter Customization\n\n### Using Custom Configuration\n\nCreate custom configuration for specific use cases:\n\n```bash\n# Copy template\ncp assets/custom_inference_config.yaml my_config.yaml\n\n# Edit parameters (see template for presets)\n# Then run with custom config\npython -m inference \\\n  --config my_config.yaml \\\n  --protein_ligand_csv input.csv \\\n  --out_dir results/\n```\n\n### Key Parameters to Adjust\n\n**Sampling Density:**\n- `samples_per_complex: 10`  Increase to 20-40 for difficult cases\n- More samples = better coverage but longer runtime\n\n**Inference Steps:**\n- `inference_steps: 20`  Increase to 25-30 for higher accuracy\n- More steps = potentially better quality but slower\n\n**Temperature Parameters (control diversity):**\n- `temp_sampling_tor: 7.04`  Increase for flexible ligands (8-10)\n- `temp_sampling_tor: 7.04`  Decrease for rigid ligands (5-6)\n- Higher temperature = more diverse poses\n\n**Presets Available in Template:**\n1. High Accuracy: More samples + steps, lower temperature\n2. Fast Screening: Fewer samples, faster\n3. Flexible Ligands: Increased torsion temperature\n4. Rigid Ligands: Decreased torsion temperature\n\n**For complete parameter reference:** Read `references/parameters_reference.md` using the Read tool\n\n## Advanced Techniques\n\n### Ensemble Docking (Protein Flexibility)\n\nFor proteins with known flexibility, dock to multiple conformations:\n\n```python\n# Create ensemble CSV\nimport pandas as pd\n\nconformations = [\"conf1.pdb\", \"conf2.pdb\", \"conf3.pdb\"]\nligand = \"CC(=O)Oc1ccccc1C(=O)O\"\n\ndata = {\n    \"complex_name\": [f\"ensemble_{i}\" for i in range(len(conformations))],\n    \"protein_path\": conformations,\n    \"ligand_description\": [ligand] * len(conformations),\n    \"protein_sequence\": [\"\"] * len(conformations)\n}\n\npd.DataFrame(data).to_csv(\"ensemble_input.csv\", index=False)\n```\n\nRun docking with increased sampling:\n```bash\npython -m inference \\\n  --config default_inference_args.yaml \\\n  --protein_ligand_csv ensemble_input.csv \\\n  --samples_per_complex 20 \\\n  --out_dir results/ensemble/\n```\n\n### Integration with Scoring Functions\n\nDiffDock generates poses; combine with other tools for affinity:\n\n**GNINA (Fast neural network scoring):**\n```bash\nfor pose in results/*.sdf; do\n    gnina -r protein.pdb -l \"$pose\" --score_only\ndone\n```\n\n**MM/GBSA (More accurate, slower):**\nUse AmberTools MMPBSA.py or gmx_MMPBSA after energy minimization\n\n**Free Energy Calculations (Most accurate):**\nUse OpenMM + OpenFE or GROMACS for FEP/TI calculations\n\n**Recommended Workflow:**\n1. DiffDock  Generate poses with confidence scores\n2. Visual inspection  Check structural plausibility\n3. GNINA or MM/GBSA  Rescore and rank by affinity\n4. Experimental validation  Biochemical assays\n\n## Limitations and Scope\n\n**DiffDock IS Designed For:**\n- Small molecule ligands (typically 100-1000 Da)\n- Drug-like organic compounds\n- Small peptides (<20 residues)\n- Single or multi-chain proteins\n\n**DiffDock IS NOT Designed For:**\n- Large biomolecules (protein-protein docking)  Use DiffDock-PP or AlphaFold-Multimer\n- Large peptides (>20 residues)  Use alternative methods\n- Covalent docking  Use specialized covalent docking tools\n- Binding affinity prediction  Combine with scoring functions\n- Membrane proteins  Not specifically trained, use with caution\n\n**For complete limitations:** Read `references/confidence_and_limitations.md` using the Read tool\n\n## Troubleshooting\n\n### Common Issues\n\n**Issue: Low confidence scores across all predictions**\n- Cause: Large/unusual ligands, unclear binding site, protein flexibility\n- Solution: Increase `samples_per_complex` (20-40), try ensemble docking, validate protein structure\n\n**Issue: Out of memory errors**\n- Cause: GPU memory insufficient for batch size\n- Solution: Reduce `--batch_size 2` or process fewer complexes at once\n\n**Issue: Slow performance**\n- Cause: Running on CPU instead of GPU\n- Solution: Verify CUDA with `python -c \"import torch; print(torch.cuda.is_available())\"`, use GPU\n\n**Issue: Unrealistic binding poses**\n- Cause: Poor protein preparation, ligand too large, wrong binding site\n- Solution: Check protein for missing residues, remove far waters, consider specifying binding site\n\n**Issue: \"Module not found\" errors**\n- Cause: Missing dependencies or wrong environment\n- Solution: Run `python scripts/setup_check.py` to diagnose\n\n### Performance Optimization\n\n**For Best Results:**\n1. Use GPU (essential for practical use)\n2. Pre-compute ESM embeddings for repeated protein use\n3. Batch process multiple complexes together\n4. Start with default parameters, then tune if needed\n5. Validate protein structures (resolve missing residues)\n6. Use canonical SMILES for ligands\n\n## Graphical User Interface\n\nFor interactive use, launch the web interface:\n\n```bash\npython app/main.py\n# Navigate to http://localhost:7860\n```\n\nOr use the online demo without installation:\n- https://huggingface.co/spaces/reginabarzilaygroup/DiffDock-Web\n\n## Resources\n\n### Helper Scripts (`scripts/`)\n\n**`prepare_batch_csv.py`**: Create and validate batch input CSV files\n- Create templates with example entries\n- Validate file paths and SMILES strings\n- Check for required columns and format issues\n\n**`analyze_results.py`**: Analyze confidence scores and rank predictions\n- Parse results from single or batch runs\n- Generate statistical summaries\n- Export to CSV for downstream analysis\n- Identify top predictions across complexes\n\n**`setup_check.py`**: Verify DiffDock environment setup\n- Check Python version and dependencies\n- Verify PyTorch and CUDA availability\n- Test RDKit and PyTorch Geometric installation\n- Provide installation instructions if needed\n\n### Reference Documentation (`references/`)\n\n**`parameters_reference.md`**: Complete parameter documentation\n- All command-line options and configuration parameters\n- Default values and acceptable ranges\n- Temperature parameters for controlling diversity\n- Model checkpoint locations and version flags\n\nRead this file when users need:\n- Detailed parameter explanations\n- Fine-tuning guidance for specific systems\n- Alternative sampling strategies\n\n**`confidence_and_limitations.md`**: Confidence score interpretation and tool limitations\n- Detailed confidence score interpretation\n- When to trust predictions\n- Scope and limitations of DiffDock\n- Integration with complementary tools\n- Troubleshooting prediction quality\n\nRead this file when users need:\n- Help interpreting confidence scores\n- Understanding when NOT to use DiffDock\n- Guidance on combining with other tools\n- Validation strategies\n\n**`workflows_examples.md`**: Comprehensive workflow examples\n- Detailed installation instructions\n- Step-by-step examples for all workflows\n- Advanced integration patterns\n- Troubleshooting common issues\n- Best practices and optimization tips\n\nRead this file when users need:\n- Complete workflow examples with code\n- Integration with GNINA, OpenMM, or other tools\n- Virtual screening workflows\n- Ensemble docking procedures\n\n### Assets (`assets/`)\n\n**`batch_template.csv`**: Template for batch processing\n- Pre-formatted CSV with required columns\n- Example entries showing different input types\n- Ready to customize with actual data\n\n**`custom_inference_config.yaml`**: Configuration template\n- Annotated YAML with all parameters\n- Four preset configurations for common use cases\n- Detailed comments explaining each parameter\n- Ready to customize and use\n\n## Best Practices\n\n1. **Always verify environment** with `setup_check.py` before starting large jobs\n2. **Validate batch CSVs** with `prepare_batch_csv.py` to catch errors early\n3. **Start with defaults** then tune parameters based on system-specific needs\n4. **Generate multiple samples** (10-40) for robust predictions\n5. **Visual inspection** of top poses before downstream analysis\n6. **Combine with scoring** functions for affinity assessment\n7. **Use confidence scores** for initial ranking, not final decisions\n8. **Pre-compute embeddings** for virtual screening campaigns\n9. **Document parameters** used for reproducibility\n10. **Validate results** experimentally when possible\n\n## Citations\n\nWhen using DiffDock, cite the appropriate papers:\n\n**DiffDock-L (current default model):**\n```\nStrk et al. (2024) \"DiffDock-L: Improving Molecular Docking with Diffusion Models\"\narXiv:2402.18396\n```\n\n**Original DiffDock:**\n```\nCorso et al. (2023) \"DiffDock: Diffusion Steps, Twists, and Turns for Molecular Docking\"\nICLR 2023, arXiv:2210.01776\n```\n\n## Additional Resources\n\n- **GitHub Repository**: https://github.com/gcorso/DiffDock\n- **Online Demo**: https://huggingface.co/spaces/reginabarzilaygroup/DiffDock-Web\n- **DiffDock-L Paper**: https://arxiv.org/abs/2402.18396\n- **Original Paper**: https://arxiv.org/abs/2210.01776\n",
        "data/k-dense-ai/dnanexus-integration/SKILL.md": "---\nname: dnanexus-integration\ndescription: \"DNAnexus cloud genomics platform. Build apps/applets, manage data (upload/download), dxpy Python SDK, run workflows, FASTQ/BAM/VCF, for genomics pipeline development and execution.\"\n---\n\n# DNAnexus Integration\n\n## Overview\n\nDNAnexus is a cloud platform for biomedical data analysis and genomics. Build and deploy apps/applets, manage data objects, run workflows, and use the dxpy Python SDK for genomics pipeline development and execution.\n\n## When to Use This Skill\n\nThis skill should be used when:\n- Creating, building, or modifying DNAnexus apps/applets\n- Uploading, downloading, searching, or organizing files and records\n- Running analyses, monitoring jobs, creating workflows\n- Writing scripts using dxpy to interact with the platform\n- Setting up dxapp.json, managing dependencies, using Docker\n- Processing FASTQ, BAM, VCF, or other bioinformatics files\n- Managing projects, permissions, or platform resources\n\n## Core Capabilities\n\nThe skill is organized into five main areas, each with detailed reference documentation:\n\n### 1. App Development\n\n**Purpose**: Create executable programs (apps/applets) that run on the DNAnexus platform.\n\n**Key Operations**:\n- Generate app skeleton with `dx-app-wizard`\n- Write Python or Bash apps with proper entry points\n- Handle input/output data objects\n- Deploy with `dx build` or `dx build --app`\n- Test apps on the platform\n\n**Common Use Cases**:\n- Bioinformatics pipelines (alignment, variant calling)\n- Data processing workflows\n- Quality control and filtering\n- Format conversion tools\n\n**Reference**: See `references/app-development.md` for:\n- Complete app structure and patterns\n- Python entry point decorators\n- Input/output handling with dxpy\n- Development best practices\n- Common issues and solutions\n\n### 2. Data Operations\n\n**Purpose**: Manage files, records, and other data objects on the platform.\n\n**Key Operations**:\n- Upload/download files with `dxpy.upload_local_file()` and `dxpy.download_dxfile()`\n- Create and manage records with metadata\n- Search for data objects by name, properties, or type\n- Clone data between projects\n- Manage project folders and permissions\n\n**Common Use Cases**:\n- Uploading sequencing data (FASTQ files)\n- Organizing analysis results\n- Searching for specific samples or experiments\n- Backing up data across projects\n- Managing reference genomes and annotations\n\n**Reference**: See `references/data-operations.md` for:\n- Complete file and record operations\n- Data object lifecycle (open/closed states)\n- Search and discovery patterns\n- Project management\n- Batch operations\n\n### 3. Job Execution\n\n**Purpose**: Run analyses, monitor execution, and orchestrate workflows.\n\n**Key Operations**:\n- Launch jobs with `applet.run()` or `app.run()`\n- Monitor job status and logs\n- Create subjobs for parallel processing\n- Build and run multi-step workflows\n- Chain jobs with output references\n\n**Common Use Cases**:\n- Running genomics analyses on sequencing data\n- Parallel processing of multiple samples\n- Multi-step analysis pipelines\n- Monitoring long-running computations\n- Debugging failed jobs\n\n**Reference**: See `references/job-execution.md` for:\n- Complete job lifecycle and states\n- Workflow creation and orchestration\n- Parallel execution patterns\n- Job monitoring and debugging\n- Resource management\n\n### 4. Python SDK (dxpy)\n\n**Purpose**: Programmatic access to DNAnexus platform through Python.\n\n**Key Operations**:\n- Work with data object handlers (DXFile, DXRecord, DXApplet, etc.)\n- Use high-level functions for common tasks\n- Make direct API calls for advanced operations\n- Create links and references between objects\n- Search and discover platform resources\n\n**Common Use Cases**:\n- Automation scripts for data management\n- Custom analysis pipelines\n- Batch processing workflows\n- Integration with external tools\n- Data migration and organization\n\n**Reference**: See `references/python-sdk.md` for:\n- Complete dxpy class reference\n- High-level utility functions\n- API method documentation\n- Error handling patterns\n- Common code patterns\n\n### 5. Configuration and Dependencies\n\n**Purpose**: Configure app metadata and manage dependencies.\n\n**Key Operations**:\n- Write dxapp.json with inputs, outputs, and run specs\n- Install system packages (execDepends)\n- Bundle custom tools and resources\n- Use assets for shared dependencies\n- Integrate Docker containers\n- Configure instance types and timeouts\n\n**Common Use Cases**:\n- Defining app input/output specifications\n- Installing bioinformatics tools (samtools, bwa, etc.)\n- Managing Python package dependencies\n- Using Docker images for complex environments\n- Selecting computational resources\n\n**Reference**: See `references/configuration.md` for:\n- Complete dxapp.json specification\n- Dependency management strategies\n- Docker integration patterns\n- Regional and resource configuration\n- Example configurations\n\n## Quick Start Examples\n\n### Upload and Analyze Data\n\n```python\nimport dxpy\n\n# Upload input file\ninput_file = dxpy.upload_local_file(\"sample.fastq\", project=\"project-xxxx\")\n\n# Run analysis\njob = dxpy.DXApplet(\"applet-xxxx\").run({\n    \"reads\": dxpy.dxlink(input_file.get_id())\n})\n\n# Wait for completion\njob.wait_on_done()\n\n# Download results\noutput_id = job.describe()[\"output\"][\"aligned_reads\"][\"$dnanexus_link\"]\ndxpy.download_dxfile(output_id, \"aligned.bam\")\n```\n\n### Search and Download Files\n\n```python\nimport dxpy\n\n# Find BAM files from a specific experiment\nfiles = dxpy.find_data_objects(\n    classname=\"file\",\n    name=\"*.bam\",\n    properties={\"experiment\": \"exp001\"},\n    project=\"project-xxxx\"\n)\n\n# Download each file\nfor file_result in files:\n    file_obj = dxpy.DXFile(file_result[\"id\"])\n    filename = file_obj.describe()[\"name\"]\n    dxpy.download_dxfile(file_result[\"id\"], filename)\n```\n\n### Create Simple App\n\n```python\n# src/my-app.py\nimport dxpy\nimport subprocess\n\n@dxpy.entry_point('main')\ndef main(input_file, quality_threshold=30):\n    # Download input\n    dxpy.download_dxfile(input_file[\"$dnanexus_link\"], \"input.fastq\")\n\n    # Process\n    subprocess.check_call([\n        \"quality_filter\",\n        \"--input\", \"input.fastq\",\n        \"--output\", \"filtered.fastq\",\n        \"--threshold\", str(quality_threshold)\n    ])\n\n    # Upload output\n    output_file = dxpy.upload_local_file(\"filtered.fastq\")\n\n    return {\n        \"filtered_reads\": dxpy.dxlink(output_file)\n    }\n\ndxpy.run()\n```\n\n## Workflow Decision Tree\n\nWhen working with DNAnexus, follow this decision tree:\n\n1. **Need to create a new executable?**\n   - Yes  Use **App Development** (references/app-development.md)\n   - No  Continue to step 2\n\n2. **Need to manage files or data?**\n   - Yes  Use **Data Operations** (references/data-operations.md)\n   - No  Continue to step 3\n\n3. **Need to run an analysis or workflow?**\n   - Yes  Use **Job Execution** (references/job-execution.md)\n   - No  Continue to step 4\n\n4. **Writing Python scripts for automation?**\n   - Yes  Use **Python SDK** (references/python-sdk.md)\n   - No  Continue to step 5\n\n5. **Configuring app settings or dependencies?**\n   - Yes  Use **Configuration** (references/configuration.md)\n\nOften you'll need multiple capabilities together (e.g., app development + configuration, or data operations + job execution).\n\n## Installation and Authentication\n\n### Install dxpy\n\n```bash\nuv pip install dxpy\n```\n\n### Login to DNAnexus\n\n```bash\ndx login\n```\n\nThis authenticates your session and sets up access to projects and data.\n\n### Verify Installation\n\n```bash\ndx --version\ndx whoami\n```\n\n## Common Patterns\n\n### Pattern 1: Batch Processing\n\nProcess multiple files with the same analysis:\n\n```python\n# Find all FASTQ files\nfiles = dxpy.find_data_objects(\n    classname=\"file\",\n    name=\"*.fastq\",\n    project=\"project-xxxx\"\n)\n\n# Launch parallel jobs\njobs = []\nfor file_result in files:\n    job = dxpy.DXApplet(\"applet-xxxx\").run({\n        \"input\": dxpy.dxlink(file_result[\"id\"])\n    })\n    jobs.append(job)\n\n# Wait for all completions\nfor job in jobs:\n    job.wait_on_done()\n```\n\n### Pattern 2: Multi-Step Pipeline\n\nChain multiple analyses together:\n\n```python\n# Step 1: Quality control\nqc_job = qc_applet.run({\"reads\": input_file})\n\n# Step 2: Alignment (uses QC output)\nalign_job = align_applet.run({\n    \"reads\": qc_job.get_output_ref(\"filtered_reads\")\n})\n\n# Step 3: Variant calling (uses alignment output)\nvariant_job = variant_applet.run({\n    \"bam\": align_job.get_output_ref(\"aligned_bam\")\n})\n```\n\n### Pattern 3: Data Organization\n\nOrganize analysis results systematically:\n\n```python\n# Create organized folder structure\ndxpy.api.project_new_folder(\n    \"project-xxxx\",\n    {\"folder\": \"/experiments/exp001/results\", \"parents\": True}\n)\n\n# Upload with metadata\nresult_file = dxpy.upload_local_file(\n    \"results.txt\",\n    project=\"project-xxxx\",\n    folder=\"/experiments/exp001/results\",\n    properties={\n        \"experiment\": \"exp001\",\n        \"sample\": \"sample1\",\n        \"analysis_date\": \"2025-10-20\"\n    },\n    tags=[\"validated\", \"published\"]\n)\n```\n\n## Best Practices\n\n1. **Error Handling**: Always wrap API calls in try-except blocks\n2. **Resource Management**: Choose appropriate instance types for workloads\n3. **Data Organization**: Use consistent folder structures and metadata\n4. **Cost Optimization**: Archive old data, use appropriate storage classes\n5. **Documentation**: Include clear descriptions in dxapp.json\n6. **Testing**: Test apps with various input types before production use\n7. **Version Control**: Use semantic versioning for apps\n8. **Security**: Never hardcode credentials in source code\n9. **Logging**: Include informative log messages for debugging\n10. **Cleanup**: Remove temporary files and failed jobs\n\n## Resources\n\nThis skill includes detailed reference documentation:\n\n### references/\n\n- **app-development.md** - Complete guide to building and deploying apps/applets\n- **data-operations.md** - File management, records, search, and project operations\n- **job-execution.md** - Running jobs, workflows, monitoring, and parallel processing\n- **python-sdk.md** - Comprehensive dxpy library reference with all classes and functions\n- **configuration.md** - dxapp.json specification and dependency management\n\nLoad these references when you need detailed information about specific operations or when working on complex tasks.\n\n## Getting Help\n\n- Official documentation: https://documentation.dnanexus.com/\n- API reference: http://autodoc.dnanexus.com/\n- GitHub repository: https://github.com/dnanexus/dx-toolkit\n- Support: support@dnanexus.com\n",
        "data/k-dense-ai/drugbank-database/SKILL.md": "---\nname: drugbank-database\ndescription: Access and analyze comprehensive drug information from the DrugBank database including drug properties, interactions, targets, pathways, chemical structures, and pharmacology data. This skill should be used when working with pharmaceutical data, drug discovery research, pharmacology studies, drug-drug interaction analysis, target identification, chemical similarity searches, ADMET predictions, or any task requiring detailed drug and drug target information from DrugBank.\n---\n\n# DrugBank Database\n\n## Overview\n\nDrugBank is a comprehensive bioinformatics and cheminformatics database containing detailed information on drugs and drug targets. This skill enables programmatic access to DrugBank data including ~9,591 drug entries (2,037 FDA-approved small molecules, 241 biotech drugs, 96 nutraceuticals, and 6,000+ experimental compounds) with 200+ data fields per entry.\n\n## Core Capabilities\n\n### 1. Data Access and Authentication\n\nDownload and access DrugBank data using Python with proper authentication. The skill provides guidance on:\n\n- Installing and configuring the `drugbank-downloader` package\n- Managing credentials securely via environment variables or config files\n- Downloading specific or latest database versions\n- Opening and parsing XML data efficiently\n- Working with cached data to optimize performance\n\n**When to use**: Setting up DrugBank access, downloading database updates, initial project configuration.\n\n**Reference**: See `references/data-access.md` for detailed authentication, download procedures, API access, caching strategies, and troubleshooting.\n\n### 2. Drug Information Queries\n\nExtract comprehensive drug information from the database including identifiers, chemical properties, pharmacology, clinical data, and cross-references to external databases.\n\n**Query capabilities**:\n- Search by DrugBank ID, name, CAS number, or keywords\n- Extract basic drug information (name, type, description, indication)\n- Retrieve chemical properties (SMILES, InChI, molecular formula)\n- Get pharmacology data (mechanism of action, pharmacodynamics, ADME)\n- Access external identifiers (PubChem, ChEMBL, UniProt, KEGG)\n- Build searchable drug datasets and export to DataFrames\n- Filter drugs by type (small molecule, biotech, nutraceutical)\n\n**When to use**: Retrieving specific drug information, building drug databases, pharmacology research, literature review, drug profiling.\n\n**Reference**: See `references/drug-queries.md` for XML navigation, query functions, data extraction methods, and performance optimization.\n\n### 3. Drug-Drug Interactions Analysis\n\nAnalyze drug-drug interactions (DDIs) including mechanism, clinical significance, and interaction networks for pharmacovigilance and clinical decision support.\n\n**Analysis capabilities**:\n- Extract all interactions for specific drugs\n- Build bidirectional interaction networks\n- Classify interactions by severity and mechanism\n- Check interactions between drug pairs\n- Identify drugs with most interactions\n- Analyze polypharmacy regimens for safety\n- Create interaction matrices and network graphs\n- Perform community detection in interaction networks\n- Calculate interaction risk scores\n\n**When to use**: Polypharmacy safety analysis, clinical decision support, drug interaction prediction, pharmacovigilance research, identifying contraindications.\n\n**Reference**: See `references/interactions.md` for interaction extraction, classification methods, network analysis, and clinical applications.\n\n### 4. Drug Targets and Pathways\n\nAccess detailed information about drug-protein interactions including targets, enzymes, transporters, carriers, and biological pathways.\n\n**Target analysis capabilities**:\n- Extract drug targets with actions (inhibitor, agonist, antagonist)\n- Identify metabolic enzymes (CYP450, Phase II enzymes)\n- Analyze transporters (uptake, efflux) for ADME studies\n- Map drugs to biological pathways (SMPDB)\n- Find drugs targeting specific proteins\n- Identify drugs with shared targets for repurposing\n- Analyze polypharmacology and off-target effects\n- Extract Gene Ontology (GO) terms for targets\n- Cross-reference with UniProt for protein data\n\n**When to use**: Mechanism of action studies, drug repurposing research, target identification, pathway analysis, predicting off-target effects, understanding drug metabolism.\n\n**Reference**: See `references/targets-pathways.md` for target extraction, pathway analysis, repurposing strategies, CYP450 profiling, and transporter analysis.\n\n### 5. Chemical Properties and Similarity\n\nPerform structure-based analysis including molecular similarity searches, property calculations, substructure searches, and ADMET predictions.\n\n**Chemical analysis capabilities**:\n- Extract chemical structures (SMILES, InChI, molecular formula)\n- Calculate physicochemical properties (MW, logP, PSA, H-bonds)\n- Apply Lipinski's Rule of Five and Veber's rules\n- Calculate Tanimoto similarity between molecules\n- Generate molecular fingerprints (Morgan, MACCS, topological)\n- Perform substructure searches with SMARTS patterns\n- Find structurally similar drugs for repurposing\n- Create similarity matrices for drug clustering\n- Predict oral absorption and BBB permeability\n- Analyze chemical space with PCA and clustering\n- Export chemical property databases\n\n**When to use**: Structure-activity relationship (SAR) studies, drug similarity searches, QSAR modeling, drug-likeness assessment, ADMET prediction, chemical space exploration.\n\n**Reference**: See `references/chemical-analysis.md` for structure extraction, similarity calculations, fingerprint generation, ADMET predictions, and chemical space analysis.\n\n## Typical Workflows\n\n### Drug Discovery Workflow\n1. Use `data-access.md` to download and access latest DrugBank data\n2. Use `drug-queries.md` to build searchable drug database\n3. Use `chemical-analysis.md` to find similar compounds\n4. Use `targets-pathways.md` to identify shared targets\n5. Use `interactions.md` to check safety of candidate combinations\n\n### Polypharmacy Safety Analysis\n1. Use `drug-queries.md` to look up patient medications\n2. Use `interactions.md` to check all pairwise interactions\n3. Use `interactions.md` to classify interaction severity\n4. Use `interactions.md` to calculate overall risk score\n5. Use `targets-pathways.md` to understand interaction mechanisms\n\n### Drug Repurposing Research\n1. Use `targets-pathways.md` to find drugs with shared targets\n2. Use `chemical-analysis.md` to find structurally similar drugs\n3. Use `drug-queries.md` to extract indication and pharmacology data\n4. Use `interactions.md` to assess potential combination therapies\n\n### Pharmacology Study\n1. Use `drug-queries.md` to extract drug of interest\n2. Use `targets-pathways.md` to identify all protein interactions\n3. Use `targets-pathways.md` to map to biological pathways\n4. Use `chemical-analysis.md` to predict ADMET properties\n5. Use `interactions.md` to identify potential contraindications\n\n## Installation Requirements\n\n### Python Packages\n```bash\nuv pip install drugbank-downloader  # Core access\nuv pip install bioversions          # Latest version detection\nuv pip install lxml                 # XML parsing optimization\nuv pip install pandas               # Data manipulation\nuv pip install rdkit                # Chemical informatics (for similarity)\nuv pip install networkx             # Network analysis (for interactions)\nuv pip install scikit-learn         # ML/clustering (for chemical space)\n```\n\n### Account Setup\n1. Create free account at go.drugbank.com\n2. Accept license agreement (free for academic use)\n3. Obtain username and password credentials\n4. Configure credentials as documented in `references/data-access.md`\n\n## Data Version and Reproducibility\n\nAlways specify the DrugBank version for reproducible research:\n\n```python\nfrom drugbank_downloader import download_drugbank\npath = download_drugbank(version='5.1.10')  # Specify exact version\n```\n\nDocument the version used in publications and analysis scripts.\n\n## Best Practices\n\n1. **Credentials**: Use environment variables or config files, never hardcode\n2. **Versioning**: Specify exact database version for reproducibility\n3. **Caching**: Cache parsed data to avoid re-downloading and re-parsing\n4. **Namespaces**: Handle XML namespaces properly when parsing\n5. **Validation**: Validate chemical structures with RDKit before use\n6. **Cross-referencing**: Use external identifiers (UniProt, PubChem) for integration\n7. **Clinical Context**: Always consider clinical context when interpreting interaction data\n8. **License Compliance**: Ensure proper licensing for your use case\n\n## Reference Documentation\n\nAll detailed implementation guidance is organized in modular reference files:\n\n- **references/data-access.md**: Authentication, download, parsing, API access, caching\n- **references/drug-queries.md**: XML navigation, query methods, data extraction, indexing\n- **references/interactions.md**: DDI extraction, classification, network analysis, safety scoring\n- **references/targets-pathways.md**: Target/enzyme/transporter extraction, pathway mapping, repurposing\n- **references/chemical-analysis.md**: Structure extraction, similarity, fingerprints, ADMET prediction\n\nLoad these references as needed based on your specific analysis requirements.\n",
        "data/k-dense-ai/ena-database/SKILL.md": "---\nname: ena-database\ndescription: \"Access European Nucleotide Archive via API/FTP. Retrieve DNA/RNA sequences, raw reads (FASTQ), genome assemblies by accession, for genomics and bioinformatics pipelines. Supports multiple formats.\"\n---\n\n# ENA Database\n\n## Overview\n\nThe European Nucleotide Archive (ENA) is a comprehensive public repository for nucleotide sequence data and associated metadata. Access and query DNA/RNA sequences, raw reads, genome assemblies, and functional annotations through REST APIs and FTP for genomics and bioinformatics pipelines.\n\n## When to Use This Skill\n\nThis skill should be used when:\n\n- Retrieving nucleotide sequences or raw sequencing reads by accession\n- Searching for samples, studies, or assemblies by metadata criteria\n- Downloading FASTQ files or genome assemblies for analysis\n- Querying taxonomic information for organisms\n- Accessing sequence annotations and functional data\n- Integrating ENA data into bioinformatics pipelines\n- Performing cross-reference searches to related databases\n- Bulk downloading datasets via FTP or Aspera\n\n## Core Capabilities\n\n### 1. Data Types and Structure\n\nENA organizes data into hierarchical object types:\n\n**Studies/Projects** - Group related data and control release dates. Studies are the primary unit for citing archived data.\n\n**Samples** - Represent units of biomaterial from which sequencing libraries were produced. Samples must be registered before submitting most data types.\n\n**Raw Reads** - Consist of:\n- **Experiments**: Metadata about sequencing methods, library preparation, and instrument details\n- **Runs**: References to data files containing raw sequencing reads from a single sequencing run\n\n**Assemblies** - Genome, transcriptome, metagenome, or metatranscriptome assemblies at various completion levels.\n\n**Sequences** - Assembled and annotated sequences stored in the EMBL Nucleotide Sequence Database, including coding/non-coding regions and functional annotations.\n\n**Analyses** - Results from computational analyses of sequence data.\n\n**Taxonomy Records** - Taxonomic information including lineage and rank.\n\n### 2. Programmatic Access\n\nENA provides multiple REST APIs for data access. Consult `references/api_reference.md` for detailed endpoint documentation.\n\n**Key APIs:**\n\n**ENA Portal API** - Advanced search functionality across all ENA data types\n- Documentation: https://www.ebi.ac.uk/ena/portal/api/doc\n- Use for complex queries and metadata searches\n\n**ENA Browser API** - Direct retrieval of records and metadata\n- Documentation: https://www.ebi.ac.uk/ena/browser/api/doc\n- Use for downloading specific records by accession\n- Returns data in XML format\n\n**ENA Taxonomy REST API** - Query taxonomic information\n- Access lineage, rank, and related taxonomic data\n\n**ENA Cross Reference Service** - Access related records from external databases\n- Endpoint: https://www.ebi.ac.uk/ena/xref/rest/\n\n**CRAM Reference Registry** - Retrieve reference sequences\n- Endpoint: https://www.ebi.ac.uk/ena/cram/\n- Query by MD5 or SHA1 checksums\n\n**Rate Limiting**: All APIs have a rate limit of 50 requests per second. Exceeding this returns HTTP 429 (Too Many Requests).\n\n### 3. Searching and Retrieving Data\n\n**Browser-Based Search:**\n- Free text search across all fields\n- Sequence similarity search (BLAST integration)\n- Cross-reference search to find related records\n- Advanced search with Rulespace query builder\n\n**Programmatic Queries:**\n- Use Portal API for advanced searches at scale\n- Filter by data type, date range, taxonomy, or metadata fields\n- Download results as tabulated metadata summaries or XML records\n\n**Example API Query Pattern:**\n```python\nimport requests\n\n# Search for samples from a specific study\nbase_url = \"https://www.ebi.ac.uk/ena/portal/api/search\"\nparams = {\n    \"result\": \"sample\",\n    \"query\": \"study_accession=PRJEB1234\",\n    \"format\": \"json\",\n    \"limit\": 100\n}\n\nresponse = requests.get(base_url, params=params)\nsamples = response.json()\n```\n\n### 4. Data Retrieval Formats\n\n**Metadata Formats:**\n- XML (native ENA format)\n- JSON (via Portal API)\n- TSV/CSV (tabulated summaries)\n\n**Sequence Data:**\n- FASTQ (raw reads)\n- BAM/CRAM (aligned reads)\n- FASTA (assembled sequences)\n- EMBL flat file format (annotated sequences)\n\n**Download Methods:**\n- Direct API download (small files)\n- FTP for bulk data transfer\n- Aspera for high-speed transfer of large datasets\n- enaBrowserTools command-line utility for bulk downloads\n\n### 5. Common Use Cases\n\n**Retrieve raw sequencing reads by accession:**\n```python\n# Download run files using Browser API\naccession = \"ERR123456\"\nurl = f\"https://www.ebi.ac.uk/ena/browser/api/xml/{accession}\"\n```\n\n**Search for all samples in a study:**\n```python\n# Use Portal API to list samples\nstudy_id = \"PRJNA123456\"\nurl = f\"https://www.ebi.ac.uk/ena/portal/api/search?result=sample&query=study_accession={study_id}&format=tsv\"\n```\n\n**Find assemblies for a specific organism:**\n```python\n# Search assemblies by taxonomy\norganism = \"Escherichia coli\"\nurl = f\"https://www.ebi.ac.uk/ena/portal/api/search?result=assembly&query=tax_tree({organism})&format=json\"\n```\n\n**Get taxonomic lineage:**\n```python\n# Query taxonomy API\ntaxon_id = \"562\"  # E. coli\nurl = f\"https://www.ebi.ac.uk/ena/taxonomy/rest/tax-id/{taxon_id}\"\n```\n\n### 6. Integration with Analysis Pipelines\n\n**Bulk Download Pattern:**\n1. Search for accessions matching criteria using Portal API\n2. Extract file URLs from search results\n3. Download files via FTP or using enaBrowserTools\n4. Process downloaded data in pipeline\n\n**BLAST Integration:**\nIntegrate with EBI's NCBI BLAST service (REST/SOAP API) for sequence similarity searches against ENA sequences.\n\n### 7. Best Practices\n\n**Rate Limiting:**\n- Implement exponential backoff when receiving HTTP 429 responses\n- Batch requests when possible to stay within 50 req/sec limit\n- Use bulk download tools for large datasets instead of iterating API calls\n\n**Data Citation:**\n- Always cite using Study/Project accessions when publishing\n- Include accession numbers for specific samples, runs, or assemblies used\n\n**API Response Handling:**\n- Check HTTP status codes before processing responses\n- Parse XML responses using proper XML libraries (not regex)\n- Handle pagination for large result sets\n\n**Performance:**\n- Use FTP/Aspera for downloading large files (>100MB)\n- Prefer TSV/JSON formats over XML when only metadata is needed\n- Cache taxonomy lookups locally when processing many records\n\n## Resources\n\nThis skill includes detailed reference documentation for working with ENA:\n\n### references/\n\n**api_reference.md** - Comprehensive API endpoint documentation including:\n- Detailed parameters for Portal API and Browser API\n- Response format specifications\n- Advanced query syntax and operators\n- Field names for filtering and searching\n- Common API patterns and examples\n\nLoad this reference when constructing complex API queries, debugging API responses, or needing specific parameter details.\n",
        "data/k-dense-ai/ensembl-database/SKILL.md": "---\nname: ensembl-database\ndescription: \"Query Ensembl genome database REST API for 250+ species. Gene lookups, sequence retrieval, variant analysis, comparative genomics, orthologs, VEP predictions, for genomic research.\"\n---\n\n# Ensembl Database\n\n## Overview\n\nAccess and query the Ensembl genome database, a comprehensive resource for vertebrate genomic data maintained by EMBL-EBI. The database provides gene annotations, sequences, variants, regulatory information, and comparative genomics data for over 250 species. Current release is 115 (September 2025).\n\n## When to Use This Skill\n\nThis skill should be used when:\n\n- Querying gene information by symbol or Ensembl ID\n- Retrieving DNA, transcript, or protein sequences\n- Analyzing genetic variants using the Variant Effect Predictor (VEP)\n- Finding orthologs and paralogs across species\n- Accessing regulatory features and genomic annotations\n- Converting coordinates between genome assemblies (e.g., GRCh37 to GRCh38)\n- Performing comparative genomics analyses\n- Integrating Ensembl data into genomic research pipelines\n\n## Core Capabilities\n\n### 1. Gene Information Retrieval\n\nQuery gene data by symbol, Ensembl ID, or external database identifiers.\n\n**Common operations:**\n- Look up gene information by symbol (e.g., \"BRCA2\", \"TP53\")\n- Retrieve transcript and protein information\n- Get gene coordinates and chromosomal locations\n- Access cross-references to external databases (UniProt, RefSeq, etc.)\n\n**Using the ensembl_rest package:**\n```python\nfrom ensembl_rest import EnsemblClient\n\nclient = EnsemblClient()\n\n# Look up gene by symbol\ngene_data = client.symbol_lookup(\n    species='human',\n    symbol='BRCA2'\n)\n\n# Get detailed gene information\ngene_info = client.lookup_id(\n    id='ENSG00000139618',  # BRCA2 Ensembl ID\n    expand=True\n)\n```\n\n**Direct REST API (no package):**\n```python\nimport requests\n\nserver = \"https://rest.ensembl.org\"\n\n# Symbol lookup\nresponse = requests.get(\n    f\"{server}/lookup/symbol/homo_sapiens/BRCA2\",\n    headers={\"Content-Type\": \"application/json\"}\n)\ngene_data = response.json()\n```\n\n### 2. Sequence Retrieval\n\nFetch genomic, transcript, or protein sequences in various formats (JSON, FASTA, plain text).\n\n**Operations:**\n- Get DNA sequences for genes or genomic regions\n- Retrieve transcript sequences (cDNA)\n- Access protein sequences\n- Extract sequences with flanking regions or modifications\n\n**Example:**\n```python\n# Using ensembl_rest package\nsequence = client.sequence_id(\n    id='ENSG00000139618',  # Gene ID\n    content_type='application/json'\n)\n\n# Get sequence for a genomic region\nregion_seq = client.sequence_region(\n    species='human',\n    region='7:140424943-140624564'  # chromosome:start-end\n)\n```\n\n### 3. Variant Analysis\n\nQuery genetic variation data and predict variant consequences using the Variant Effect Predictor (VEP).\n\n**Capabilities:**\n- Look up variants by rsID or genomic coordinates\n- Predict functional consequences of variants\n- Access population frequency data\n- Retrieve phenotype associations\n\n**VEP example:**\n```python\n# Predict variant consequences\nvep_result = client.vep_hgvs(\n    species='human',\n    hgvs_notation='ENST00000380152.7:c.803C>T'\n)\n\n# Query variant by rsID\nvariant = client.variation_id(\n    species='human',\n    id='rs699'\n)\n```\n\n### 4. Comparative Genomics\n\nPerform cross-species comparisons to identify orthologs, paralogs, and evolutionary relationships.\n\n**Operations:**\n- Find orthologs (same gene in different species)\n- Identify paralogs (related genes in same species)\n- Access gene trees showing evolutionary relationships\n- Retrieve gene family information\n\n**Example:**\n```python\n# Find orthologs for a human gene\northologs = client.homology_ensemblgene(\n    id='ENSG00000139618',  # Human BRCA2\n    target_species='mouse'\n)\n\n# Get gene tree\ngene_tree = client.genetree_member_symbol(\n    species='human',\n    symbol='BRCA2'\n)\n```\n\n### 5. Genomic Region Analysis\n\nFind all genomic features (genes, transcripts, regulatory elements) in a specific region.\n\n**Use cases:**\n- Identify all genes in a chromosomal region\n- Find regulatory features (promoters, enhancers)\n- Locate variants within a region\n- Retrieve structural features\n\n**Example:**\n```python\n# Find all features in a region\nfeatures = client.overlap_region(\n    species='human',\n    region='7:140424943-140624564',\n    feature='gene'\n)\n```\n\n### 6. Assembly Mapping\n\nConvert coordinates between different genome assemblies (e.g., GRCh37 to GRCh38).\n\n**Important:** Use `https://grch37.rest.ensembl.org` for GRCh37/hg19 queries and `https://rest.ensembl.org` for current assemblies.\n\n**Example:**\n```python\nfrom ensembl_rest import AssemblyMapper\n\n# Map coordinates from GRCh37 to GRCh38\nmapper = AssemblyMapper(\n    species='human',\n    asm_from='GRCh37',\n    asm_to='GRCh38'\n)\n\nmapped = mapper.map(chrom='7', start=140453136, end=140453136)\n```\n\n## API Best Practices\n\n### Rate Limiting\n\nThe Ensembl REST API has rate limits. Follow these practices:\n\n1. **Respect rate limits:** Maximum 15 requests per second for anonymous users\n2. **Handle 429 responses:** When rate-limited, check the `Retry-After` header and wait\n3. **Use batch endpoints:** When querying multiple items, use batch endpoints where available\n4. **Cache results:** Store frequently accessed data to reduce API calls\n\n### Error Handling\n\nAlways implement proper error handling:\n\n```python\nimport requests\nimport time\n\ndef query_ensembl(endpoint, params=None, max_retries=3):\n    server = \"https://rest.ensembl.org\"\n    headers = {\"Content-Type\": \"application/json\"}\n\n    for attempt in range(max_retries):\n        response = requests.get(\n            f\"{server}{endpoint}\",\n            headers=headers,\n            params=params\n        )\n\n        if response.status_code == 200:\n            return response.json()\n        elif response.status_code == 429:\n            # Rate limited - wait and retry\n            retry_after = int(response.headers.get('Retry-After', 1))\n            time.sleep(retry_after)\n        else:\n            response.raise_for_status()\n\n    raise Exception(f\"Failed after {max_retries} attempts\")\n```\n\n## Installation\n\n### Python Package (Recommended)\n\n```bash\nuv pip install ensembl_rest\n```\n\nThe `ensembl_rest` package provides a Pythonic interface to all Ensembl REST API endpoints.\n\n### Direct REST API\n\nNo installation needed - use standard HTTP libraries like `requests`:\n\n```bash\nuv pip install requests\n```\n\n## Resources\n\n### references/\n\n- `api_endpoints.md`: Comprehensive documentation of all 17 API endpoint categories with examples and parameters\n\n### scripts/\n\n- `ensembl_query.py`: Reusable Python script for common Ensembl queries with built-in rate limiting and error handling\n\n## Common Workflows\n\n### Workflow 1: Gene Annotation Pipeline\n\n1. Look up gene by symbol to get Ensembl ID\n2. Retrieve transcript information\n3. Get protein sequences for all transcripts\n4. Find orthologs in other species\n5. Export results\n\n### Workflow 2: Variant Analysis\n\n1. Query variant by rsID or coordinates\n2. Use VEP to predict functional consequences\n3. Check population frequencies\n4. Retrieve phenotype associations\n5. Generate report\n\n### Workflow 3: Comparative Analysis\n\n1. Start with gene of interest in reference species\n2. Find orthologs in target species\n3. Retrieve sequences for all orthologs\n4. Compare gene structures and features\n5. Analyze evolutionary conservation\n\n## Species and Assembly Information\n\nTo query available species and assemblies:\n\n```python\n# List all available species\nspecies_list = client.info_species()\n\n# Get assembly information for a species\nassembly_info = client.info_assembly(species='human')\n```\n\nCommon species identifiers:\n- Human: `homo_sapiens` or `human`\n- Mouse: `mus_musculus` or `mouse`\n- Zebrafish: `danio_rerio` or `zebrafish`\n- Fruit fly: `drosophila_melanogaster`\n\n## Additional Resources\n\n- **Official Documentation:** https://rest.ensembl.org/documentation\n- **Python Package Docs:** https://ensemblrest.readthedocs.io\n- **EBI Training:** https://www.ebi.ac.uk/training/online/courses/ensembl-rest-api/\n- **Ensembl Browser:** https://useast.ensembl.org\n- **GitHub Examples:** https://github.com/Ensembl/ensembl-rest/wiki\n",
        "data/k-dense-ai/esm/SKILL.md": "---\nname: esm\ndescription: Comprehensive toolkit for protein language models including ESM3 (generative multimodal protein design across sequence, structure, and function) and ESM C (efficient protein embeddings and representations). Use this skill when working with protein sequences, structures, or function prediction; designing novel proteins; generating protein embeddings; performing inverse folding; or conducting protein engineering tasks. Supports both local model usage and cloud-based Forge API for scalable inference.\n---\n\n# ESM: Evolutionary Scale Modeling\n\n## Overview\n\nESM provides state-of-the-art protein language models for understanding, generating, and designing proteins. This skill enables working with two model families: ESM3 for generative protein design across sequence, structure, and function, and ESM C for efficient protein representation learning and embeddings.\n\n## Core Capabilities\n\n### 1. Protein Sequence Generation with ESM3\n\nGenerate novel protein sequences with desired properties using multimodal generative modeling.\n\n**When to use:**\n- Designing proteins with specific functional properties\n- Completing partial protein sequences\n- Generating variants of existing proteins\n- Creating proteins with desired structural characteristics\n\n**Basic usage:**\n\n```python\nfrom esm.models.esm3 import ESM3\nfrom esm.sdk.api import ESM3InferenceClient, ESMProtein, GenerationConfig\n\n# Load model locally\nmodel: ESM3InferenceClient = ESM3.from_pretrained(\"esm3-sm-open-v1\").to(\"cuda\")\n\n# Create protein prompt\nprotein = ESMProtein(sequence=\"MPRT___KEND\")  # '_' represents masked positions\n\n# Generate completion\nprotein = model.generate(protein, GenerationConfig(track=\"sequence\", num_steps=8))\nprint(protein.sequence)\n```\n\n**For remote/cloud usage via Forge API:**\n\n```python\nfrom esm.sdk.forge import ESM3ForgeInferenceClient\nfrom esm.sdk.api import ESMProtein, GenerationConfig\n\n# Connect to Forge\nmodel = ESM3ForgeInferenceClient(model=\"esm3-medium-2024-08\", url=\"https://forge.evolutionaryscale.ai\", token=\"<token>\")\n\n# Generate\nprotein = model.generate(protein, GenerationConfig(track=\"sequence\", num_steps=8))\n```\n\nSee `references/esm3-api.md` for detailed ESM3 model specifications, advanced generation configurations, and multimodal prompting examples.\n\n### 2. Structure Prediction and Inverse Folding\n\nUse ESM3's structure track for structure prediction from sequence or inverse folding (sequence design from structure).\n\n**Structure prediction:**\n\n```python\nfrom esm.sdk.api import ESM3InferenceClient, ESMProtein, GenerationConfig\n\n# Predict structure from sequence\nprotein = ESMProtein(sequence=\"MPRTKEINDAGLIVHSP...\")\nprotein_with_structure = model.generate(\n    protein,\n    GenerationConfig(track=\"structure\", num_steps=protein.sequence.count(\"_\"))\n)\n\n# Access predicted structure\ncoordinates = protein_with_structure.coordinates  # 3D coordinates\npdb_string = protein_with_structure.to_pdb()\n```\n\n**Inverse folding (sequence from structure):**\n\n```python\n# Design sequence for a target structure\nprotein_with_structure = ESMProtein.from_pdb(\"target_structure.pdb\")\nprotein_with_structure.sequence = None  # Remove sequence\n\n# Generate sequence that folds to this structure\ndesigned_protein = model.generate(\n    protein_with_structure,\n    GenerationConfig(track=\"sequence\", num_steps=50, temperature=0.7)\n)\n```\n\n### 3. Protein Embeddings with ESM C\n\nGenerate high-quality embeddings for downstream tasks like function prediction, classification, or similarity analysis.\n\n**When to use:**\n- Extracting protein representations for machine learning\n- Computing sequence similarities\n- Feature extraction for protein classification\n- Transfer learning for protein-related tasks\n\n**Basic usage:**\n\n```python\nfrom esm.models.esmc import ESMC\nfrom esm.sdk.api import ESMProtein\n\n# Load ESM C model\nmodel = ESMC.from_pretrained(\"esmc-300m\").to(\"cuda\")\n\n# Get embeddings\nprotein = ESMProtein(sequence=\"MPRTKEINDAGLIVHSP...\")\nprotein_tensor = model.encode(protein)\n\n# Generate embeddings\nembeddings = model.forward(protein_tensor)\n```\n\n**Batch processing:**\n\n```python\n# Encode multiple proteins\nproteins = [\n    ESMProtein(sequence=\"MPRTKEIND...\"),\n    ESMProtein(sequence=\"AGLIVHSPQ...\"),\n    ESMProtein(sequence=\"KTEFLNDGR...\")\n]\n\nembeddings_list = [model.logits(model.forward(model.encode(p))) for p in proteins]\n```\n\nSee `references/esm-c-api.md` for ESM C model details, efficiency comparisons, and advanced embedding strategies.\n\n### 4. Function Conditioning and Annotation\n\nUse ESM3's function track to generate proteins with specific functional annotations or predict function from sequence.\n\n**Function-conditioned generation:**\n\n```python\nfrom esm.sdk.api import ESMProtein, FunctionAnnotation, GenerationConfig\n\n# Create protein with desired function\nprotein = ESMProtein(\n    sequence=\"_\" * 200,  # Generate 200 residue protein\n    function_annotations=[\n        FunctionAnnotation(label=\"fluorescent_protein\", start=50, end=150)\n    ]\n)\n\n# Generate sequence with specified function\nfunctional_protein = model.generate(\n    protein,\n    GenerationConfig(track=\"sequence\", num_steps=200)\n)\n```\n\n### 5. Chain-of-Thought Generation\n\nIteratively refine protein designs using ESM3's chain-of-thought generation approach.\n\n```python\nfrom esm.sdk.api import GenerationConfig\n\n# Multi-step refinement\nprotein = ESMProtein(sequence=\"MPRT\" + \"_\" * 100 + \"KEND\")\n\n# Step 1: Generate initial structure\nconfig = GenerationConfig(track=\"structure\", num_steps=50)\nprotein = model.generate(protein, config)\n\n# Step 2: Refine sequence based on structure\nconfig = GenerationConfig(track=\"sequence\", num_steps=50, temperature=0.5)\nprotein = model.generate(protein, config)\n\n# Step 3: Predict function\nconfig = GenerationConfig(track=\"function\", num_steps=20)\nprotein = model.generate(protein, config)\n```\n\n### 6. Batch Processing with Forge API\n\nProcess multiple proteins efficiently using Forge's async executor.\n\n```python\nfrom esm.sdk.forge import ESM3ForgeInferenceClient\nimport asyncio\n\nclient = ESM3ForgeInferenceClient(model=\"esm3-medium-2024-08\", token=\"<token>\")\n\n# Async batch processing\nasync def batch_generate(proteins_list):\n    tasks = [\n        client.async_generate(protein, GenerationConfig(track=\"sequence\"))\n        for protein in proteins_list\n    ]\n    return await asyncio.gather(*tasks)\n\n# Execute\nproteins = [ESMProtein(sequence=f\"MPRT{'_' * 50}KEND\") for _ in range(10)]\nresults = asyncio.run(batch_generate(proteins))\n```\n\nSee `references/forge-api.md` for detailed Forge API documentation, authentication, rate limits, and batch processing patterns.\n\n## Model Selection Guide\n\n**ESM3 Models (Generative):**\n- `esm3-sm-open-v1` (1.4B) - Open weights, local usage, good for experimentation\n- `esm3-medium-2024-08` (7B) - Best balance of quality and speed (Forge only)\n- `esm3-large-2024-03` (98B) - Highest quality, slower (Forge only)\n\n**ESM C Models (Embeddings):**\n- `esmc-300m` (30 layers) - Lightweight, fast inference\n- `esmc-600m` (36 layers) - Balanced performance\n- `esmc-6b` (80 layers) - Maximum representation quality\n\n**Selection criteria:**\n- **Local development/testing:** Use `esm3-sm-open-v1` or `esmc-300m`\n- **Production quality:** Use `esm3-medium-2024-08` via Forge\n- **Maximum accuracy:** Use `esm3-large-2024-03` or `esmc-6b`\n- **High throughput:** Use Forge API with batch executor\n- **Cost optimization:** Use smaller models, implement caching strategies\n\n## Installation\n\n**Basic installation:**\n\n```bash\nuv pip install esm\n```\n\n**With Flash Attention (recommended for faster inference):**\n\n```bash\nuv pip install esm\nuv pip install flash-attn --no-build-isolation\n```\n\n**For Forge API access:**\n\n```bash\nuv pip install esm  # SDK includes Forge client\n```\n\nNo additional dependencies needed. Obtain Forge API token at https://forge.evolutionaryscale.ai\n\n## Common Workflows\n\nFor detailed examples and complete workflows, see `references/workflows.md` which includes:\n- Novel GFP design with chain-of-thought\n- Protein variant generation and screening\n- Structure-based sequence optimization\n- Function prediction pipelines\n- Embedding-based clustering and analysis\n\n## References\n\nThis skill includes comprehensive reference documentation:\n\n- `references/esm3-api.md` - ESM3 model architecture, API reference, generation parameters, and multimodal prompting\n- `references/esm-c-api.md` - ESM C model details, embedding strategies, and performance optimization\n- `references/forge-api.md` - Forge platform documentation, authentication, batch processing, and deployment\n- `references/workflows.md` - Complete examples and common workflow patterns\n\nThese references contain detailed API specifications, parameter descriptions, and advanced usage patterns. Load them as needed for specific tasks.\n\n## Best Practices\n\n**For generation tasks:**\n- Start with smaller models for prototyping (`esm3-sm-open-v1`)\n- Use temperature parameter to control diversity (0.0 = deterministic, 1.0 = diverse)\n- Implement iterative refinement with chain-of-thought for complex designs\n- Validate generated sequences with structure prediction or wet-lab experiments\n\n**For embedding tasks:**\n- Batch process sequences when possible for efficiency\n- Cache embeddings for repeated analyses\n- Normalize embeddings when computing similarities\n- Use appropriate model size based on downstream task requirements\n\n**For production deployment:**\n- Use Forge API for scalability and latest models\n- Implement error handling and retry logic for API calls\n- Monitor token usage and implement rate limiting\n- Consider AWS SageMaker deployment for dedicated infrastructure\n\n## Resources and Documentation\n\n- **GitHub Repository:** https://github.com/evolutionaryscale/esm\n- **Forge Platform:** https://forge.evolutionaryscale.ai\n- **Scientific Paper:** Hayes et al., Science (2025) - https://www.science.org/doi/10.1126/science.ads0018\n- **Blog Posts:**\n  - ESM3 Release: https://www.evolutionaryscale.ai/blog/esm3-release\n  - ESM C Launch: https://www.evolutionaryscale.ai/blog/esm-cambrian\n- **Community:** Slack community at https://bit.ly/3FKwcWd\n- **Model Weights:** HuggingFace EvolutionaryScale organization\n\n## Responsible Use\n\nESM is designed for beneficial applications in protein engineering, drug discovery, and scientific research. Follow the Responsible Biodesign Framework (https://responsiblebiodesign.ai/) when designing novel proteins. Consider biosafety and ethical implications of protein designs before experimental validation.\n",
        "data/k-dense-ai/etetoolkit/SKILL.md": "---\nname: etetoolkit\ndescription: \"Phylogenetic tree toolkit (ETE). Tree manipulation (Newick/NHX), evolutionary event detection, orthology/paralogy, NCBI taxonomy, visualization (PDF/SVG), for phylogenomics.\"\n---\n\n# ETE Toolkit Skill\n\n## Overview\n\nETE (Environment for Tree Exploration) is a toolkit for phylogenetic and hierarchical tree analysis. Manipulate trees, analyze evolutionary events, visualize results, and integrate with biological databases for phylogenomic research and clustering analysis.\n\n## Core Capabilities\n\n### 1. Tree Manipulation and Analysis\n\nLoad, manipulate, and analyze hierarchical tree structures with support for:\n\n- **Tree I/O**: Read and write Newick, NHX, PhyloXML, and NeXML formats\n- **Tree traversal**: Navigate trees using preorder, postorder, or levelorder strategies\n- **Topology modification**: Prune, root, collapse nodes, resolve polytomies\n- **Distance calculations**: Compute branch lengths and topological distances between nodes\n- **Tree comparison**: Calculate Robinson-Foulds distances and identify topological differences\n\n**Common patterns:**\n\n```python\nfrom ete3 import Tree\n\n# Load tree from file\ntree = Tree(\"tree.nw\", format=1)\n\n# Basic statistics\nprint(f\"Leaves: {len(tree)}\")\nprint(f\"Total nodes: {len(list(tree.traverse()))}\")\n\n# Prune to taxa of interest\ntaxa_to_keep = [\"species1\", \"species2\", \"species3\"]\ntree.prune(taxa_to_keep, preserve_branch_length=True)\n\n# Midpoint root\nmidpoint = tree.get_midpoint_outgroup()\ntree.set_outgroup(midpoint)\n\n# Save modified tree\ntree.write(outfile=\"rooted_tree.nw\")\n```\n\nUse `scripts/tree_operations.py` for command-line tree manipulation:\n\n```bash\n# Display tree statistics\npython scripts/tree_operations.py stats tree.nw\n\n# Convert format\npython scripts/tree_operations.py convert tree.nw output.nw --in-format 0 --out-format 1\n\n# Reroot tree\npython scripts/tree_operations.py reroot tree.nw rooted.nw --midpoint\n\n# Prune to specific taxa\npython scripts/tree_operations.py prune tree.nw pruned.nw --keep-taxa \"sp1,sp2,sp3\"\n\n# Show ASCII visualization\npython scripts/tree_operations.py ascii tree.nw\n```\n\n### 2. Phylogenetic Analysis\n\nAnalyze gene trees with evolutionary event detection:\n\n- **Sequence alignment integration**: Link trees to multiple sequence alignments (FASTA, Phylip)\n- **Species naming**: Automatic or custom species extraction from gene names\n- **Evolutionary events**: Detect duplication and speciation events using Species Overlap or tree reconciliation\n- **Orthology detection**: Identify orthologs and paralogs based on evolutionary events\n- **Gene family analysis**: Split trees by duplications, collapse lineage-specific expansions\n\n**Workflow for gene tree analysis:**\n\n```python\nfrom ete3 import PhyloTree\n\n# Load gene tree with alignment\ntree = PhyloTree(\"gene_tree.nw\", alignment=\"alignment.fasta\")\n\n# Set species naming function\ndef get_species(gene_name):\n    return gene_name.split(\"_\")[0]\n\ntree.set_species_naming_function(get_species)\n\n# Detect evolutionary events\nevents = tree.get_descendant_evol_events()\n\n# Analyze events\nfor node in tree.traverse():\n    if hasattr(node, \"evoltype\"):\n        if node.evoltype == \"D\":\n            print(f\"Duplication at {node.name}\")\n        elif node.evoltype == \"S\":\n            print(f\"Speciation at {node.name}\")\n\n# Extract ortholog groups\northo_groups = tree.get_speciation_trees()\nfor i, ortho_tree in enumerate(ortho_groups):\n    ortho_tree.write(outfile=f\"ortholog_group_{i}.nw\")\n```\n\n**Finding orthologs and paralogs:**\n\n```python\n# Find orthologs to query gene\nquery = tree & \"species1_gene1\"\n\northologs = []\nparalogs = []\n\nfor event in events:\n    if query in event.in_seqs:\n        if event.etype == \"S\":\n            orthologs.extend([s for s in event.out_seqs if s != query])\n        elif event.etype == \"D\":\n            paralogs.extend([s for s in event.out_seqs if s != query])\n```\n\n### 3. NCBI Taxonomy Integration\n\nIntegrate taxonomic information from NCBI Taxonomy database:\n\n- **Database access**: Automatic download and local caching of NCBI taxonomy (~300MB)\n- **Taxid/name translation**: Convert between taxonomic IDs and scientific names\n- **Lineage retrieval**: Get complete evolutionary lineages\n- **Taxonomy trees**: Build species trees connecting specified taxa\n- **Tree annotation**: Automatically annotate trees with taxonomic information\n\n**Building taxonomy-based trees:**\n\n```python\nfrom ete3 import NCBITaxa\n\nncbi = NCBITaxa()\n\n# Build tree from species names\nspecies = [\"Homo sapiens\", \"Pan troglodytes\", \"Mus musculus\"]\nname2taxid = ncbi.get_name_translator(species)\ntaxids = [name2taxid[sp][0] for sp in species]\n\n# Get minimal tree connecting taxa\ntree = ncbi.get_topology(taxids)\n\n# Annotate nodes with taxonomy info\nfor node in tree.traverse():\n    if hasattr(node, \"sci_name\"):\n        print(f\"{node.sci_name} - Rank: {node.rank} - TaxID: {node.taxid}\")\n```\n\n**Annotating existing trees:**\n\n```python\n# Get taxonomy info for tree leaves\nfor leaf in tree:\n    species = extract_species_from_name(leaf.name)\n    taxid = ncbi.get_name_translator([species])[species][0]\n\n    # Get lineage\n    lineage = ncbi.get_lineage(taxid)\n    ranks = ncbi.get_rank(lineage)\n    names = ncbi.get_taxid_translator(lineage)\n\n    # Add to node\n    leaf.add_feature(\"taxid\", taxid)\n    leaf.add_feature(\"lineage\", [names[t] for t in lineage])\n```\n\n### 4. Tree Visualization\n\nCreate publication-quality tree visualizations:\n\n- **Output formats**: PNG (raster), PDF, and SVG (vector) for publications\n- **Layout modes**: Rectangular and circular tree layouts\n- **Interactive GUI**: Explore trees interactively with zoom, pan, and search\n- **Custom styling**: NodeStyle for node appearance (colors, shapes, sizes)\n- **Faces**: Add graphical elements (text, images, charts, heatmaps) to nodes\n- **Layout functions**: Dynamic styling based on node properties\n\n**Basic visualization workflow:**\n\n```python\nfrom ete3 import Tree, TreeStyle, NodeStyle\n\ntree = Tree(\"tree.nw\")\n\n# Configure tree style\nts = TreeStyle()\nts.show_leaf_name = True\nts.show_branch_support = True\nts.scale = 50  # pixels per branch length unit\n\n# Style nodes\nfor node in tree.traverse():\n    nstyle = NodeStyle()\n\n    if node.is_leaf():\n        nstyle[\"fgcolor\"] = \"blue\"\n        nstyle[\"size\"] = 8\n    else:\n        # Color by support\n        if node.support > 0.9:\n            nstyle[\"fgcolor\"] = \"darkgreen\"\n        else:\n            nstyle[\"fgcolor\"] = \"red\"\n        nstyle[\"size\"] = 5\n\n    node.set_style(nstyle)\n\n# Render to file\ntree.render(\"tree.pdf\", tree_style=ts)\ntree.render(\"tree.png\", w=800, h=600, units=\"px\", dpi=300)\n```\n\nUse `scripts/quick_visualize.py` for rapid visualization:\n\n```bash\n# Basic visualization\npython scripts/quick_visualize.py tree.nw output.pdf\n\n# Circular layout with custom styling\npython scripts/quick_visualize.py tree.nw output.pdf --mode c --color-by-support\n\n# High-resolution PNG\npython scripts/quick_visualize.py tree.nw output.png --width 1200 --height 800 --units px --dpi 300\n\n# Custom title and styling\npython scripts/quick_visualize.py tree.nw output.pdf --title \"Species Phylogeny\" --show-support\n```\n\n**Advanced visualization with faces:**\n\n```python\nfrom ete3 import Tree, TreeStyle, TextFace, CircleFace\n\ntree = Tree(\"tree.nw\")\n\n# Add features to nodes\nfor leaf in tree:\n    leaf.add_feature(\"habitat\", \"marine\" if \"fish\" in leaf.name else \"land\")\n\n# Layout function\ndef layout(node):\n    if node.is_leaf():\n        # Add colored circle\n        color = \"blue\" if node.habitat == \"marine\" else \"green\"\n        circle = CircleFace(radius=5, color=color)\n        node.add_face(circle, column=0, position=\"aligned\")\n\n        # Add label\n        label = TextFace(node.name, fsize=10)\n        node.add_face(label, column=1, position=\"aligned\")\n\nts = TreeStyle()\nts.layout_fn = layout\nts.show_leaf_name = False\n\ntree.render(\"annotated_tree.pdf\", tree_style=ts)\n```\n\n### 5. Clustering Analysis\n\nAnalyze hierarchical clustering results with data integration:\n\n- **ClusterTree**: Specialized class for clustering dendrograms\n- **Data matrix linking**: Connect tree leaves to numerical profiles\n- **Cluster metrics**: Silhouette coefficient, Dunn index, inter/intra-cluster distances\n- **Validation**: Test cluster quality with different distance metrics\n- **Heatmap visualization**: Display data matrices alongside trees\n\n**Clustering workflow:**\n\n```python\nfrom ete3 import ClusterTree\n\n# Load tree with data matrix\nmatrix = \"\"\"#Names\\tSample1\\tSample2\\tSample3\nGene1\\t1.5\\t2.3\\t0.8\nGene2\\t0.9\\t1.1\\t1.8\nGene3\\t2.1\\t2.5\\t0.5\"\"\"\n\ntree = ClusterTree(\"((Gene1,Gene2),Gene3);\", text_array=matrix)\n\n# Evaluate cluster quality\nfor node in tree.traverse():\n    if not node.is_leaf():\n        silhouette = node.get_silhouette()\n        dunn = node.get_dunn()\n\n        print(f\"Cluster: {node.name}\")\n        print(f\"  Silhouette: {silhouette:.3f}\")\n        print(f\"  Dunn index: {dunn:.3f}\")\n\n# Visualize with heatmap\ntree.show(\"heatmap\")\n```\n\n### 6. Tree Comparison\n\nQuantify topological differences between trees:\n\n- **Robinson-Foulds distance**: Standard metric for tree comparison\n- **Normalized RF**: Scale-invariant distance (0.0 to 1.0)\n- **Partition analysis**: Identify unique and shared bipartitions\n- **Consensus trees**: Analyze support across multiple trees\n- **Batch comparison**: Compare multiple trees pairwise\n\n**Compare two trees:**\n\n```python\nfrom ete3 import Tree\n\ntree1 = Tree(\"tree1.nw\")\ntree2 = Tree(\"tree2.nw\")\n\n# Calculate RF distance\nrf, max_rf, common_leaves, parts_t1, parts_t2 = tree1.robinson_foulds(tree2)\n\nprint(f\"RF distance: {rf}/{max_rf}\")\nprint(f\"Normalized RF: {rf/max_rf:.3f}\")\nprint(f\"Common leaves: {len(common_leaves)}\")\n\n# Find unique partitions\nunique_t1 = parts_t1 - parts_t2\nunique_t2 = parts_t2 - parts_t1\n\nprint(f\"Unique to tree1: {len(unique_t1)}\")\nprint(f\"Unique to tree2: {len(unique_t2)}\")\n```\n\n**Compare multiple trees:**\n\n```python\nimport numpy as np\n\ntrees = [Tree(f\"tree{i}.nw\") for i in range(4)]\n\n# Create distance matrix\nn = len(trees)\ndist_matrix = np.zeros((n, n))\n\nfor i in range(n):\n    for j in range(i+1, n):\n        rf, max_rf, _, _, _ = trees[i].robinson_foulds(trees[j])\n        norm_rf = rf / max_rf if max_rf > 0 else 0\n        dist_matrix[i, j] = norm_rf\n        dist_matrix[j, i] = norm_rf\n```\n\n## Installation and Setup\n\nInstall ETE toolkit:\n\n```bash\n# Basic installation\nuv pip install ete3\n\n# With external dependencies for rendering (optional but recommended)\n# On macOS:\nbrew install qt@5\n\n# On Ubuntu/Debian:\nsudo apt-get install python3-pyqt5 python3-pyqt5.qtsvg\n\n# For full features including GUI\nuv pip install ete3[gui]\n```\n\n**First-time NCBI Taxonomy setup:**\n\nThe first time NCBITaxa is instantiated, it automatically downloads the NCBI taxonomy database (~300MB) to `~/.etetoolkit/taxa.sqlite`. This happens only once:\n\n```python\nfrom ete3 import NCBITaxa\nncbi = NCBITaxa()  # Downloads database on first run\n```\n\nUpdate taxonomy database:\n\n```python\nncbi.update_taxonomy_database()  # Download latest NCBI data\n```\n\n## Common Use Cases\n\n### Use Case 1: Phylogenomic Pipeline\n\nComplete workflow from gene tree to ortholog identification:\n\n```python\nfrom ete3 import PhyloTree, NCBITaxa\n\n# 1. Load gene tree with alignment\ntree = PhyloTree(\"gene_tree.nw\", alignment=\"alignment.fasta\")\n\n# 2. Configure species naming\ntree.set_species_naming_function(lambda x: x.split(\"_\")[0])\n\n# 3. Detect evolutionary events\ntree.get_descendant_evol_events()\n\n# 4. Annotate with taxonomy\nncbi = NCBITaxa()\nfor leaf in tree:\n    if leaf.species in species_to_taxid:\n        taxid = species_to_taxid[leaf.species]\n        lineage = ncbi.get_lineage(taxid)\n        leaf.add_feature(\"lineage\", lineage)\n\n# 5. Extract ortholog groups\northo_groups = tree.get_speciation_trees()\n\n# 6. Save and visualize\nfor i, ortho in enumerate(ortho_groups):\n    ortho.write(outfile=f\"ortho_{i}.nw\")\n```\n\n### Use Case 2: Tree Preprocessing and Formatting\n\nBatch process trees for analysis:\n\n```bash\n# Convert format\npython scripts/tree_operations.py convert input.nw output.nw --in-format 0 --out-format 1\n\n# Root at midpoint\npython scripts/tree_operations.py reroot input.nw rooted.nw --midpoint\n\n# Prune to focal taxa\npython scripts/tree_operations.py prune rooted.nw pruned.nw --keep-taxa taxa_list.txt\n\n# Get statistics\npython scripts/tree_operations.py stats pruned.nw\n```\n\n### Use Case 3: Publication-Quality Figures\n\nCreate styled visualizations:\n\n```python\nfrom ete3 import Tree, TreeStyle, NodeStyle, TextFace\n\ntree = Tree(\"tree.nw\")\n\n# Define clade colors\nclade_colors = {\n    \"Mammals\": \"red\",\n    \"Birds\": \"blue\",\n    \"Fish\": \"green\"\n}\n\ndef layout(node):\n    # Highlight clades\n    if node.is_leaf():\n        for clade, color in clade_colors.items():\n            if clade in node.name:\n                nstyle = NodeStyle()\n                nstyle[\"fgcolor\"] = color\n                nstyle[\"size\"] = 8\n                node.set_style(nstyle)\n    else:\n        # Add support values\n        if node.support > 0.95:\n            support = TextFace(f\"{node.support:.2f}\", fsize=8)\n            node.add_face(support, column=0, position=\"branch-top\")\n\nts = TreeStyle()\nts.layout_fn = layout\nts.show_scale = True\n\n# Render for publication\ntree.render(\"figure.pdf\", w=200, units=\"mm\", tree_style=ts)\ntree.render(\"figure.svg\", tree_style=ts)  # Editable vector\n```\n\n### Use Case 4: Automated Tree Analysis\n\nProcess multiple trees systematically:\n\n```python\nfrom ete3 import Tree\nimport os\n\ninput_dir = \"trees\"\noutput_dir = \"processed\"\n\nfor filename in os.listdir(input_dir):\n    if filename.endswith(\".nw\"):\n        tree = Tree(os.path.join(input_dir, filename))\n\n        # Standardize: midpoint root, resolve polytomies\n        midpoint = tree.get_midpoint_outgroup()\n        tree.set_outgroup(midpoint)\n        tree.resolve_polytomy(recursive=True)\n\n        # Filter low support branches\n        for node in tree.traverse():\n            if hasattr(node, 'support') and node.support < 0.5:\n                if not node.is_leaf() and not node.is_root():\n                    node.delete()\n\n        # Save processed tree\n        output_file = os.path.join(output_dir, f\"processed_{filename}\")\n        tree.write(outfile=output_file)\n```\n\n## Reference Documentation\n\nFor comprehensive API documentation, code examples, and detailed guides, refer to the following resources in the `references/` directory:\n\n- **`api_reference.md`**: Complete API documentation for all ETE classes and methods (Tree, PhyloTree, ClusterTree, NCBITaxa), including parameters, return types, and code examples\n- **`workflows.md`**: Common workflow patterns organized by task (tree operations, phylogenetic analysis, tree comparison, taxonomy integration, clustering analysis)\n- **`visualization.md`**: Comprehensive visualization guide covering TreeStyle, NodeStyle, Faces, layout functions, and advanced visualization techniques\n\nLoad these references when detailed information is needed:\n\n```python\n# To use API reference\n# Read references/api_reference.md for complete method signatures and parameters\n\n# To implement workflows\n# Read references/workflows.md for step-by-step workflow examples\n\n# To create visualizations\n# Read references/visualization.md for styling and rendering options\n```\n\n## Troubleshooting\n\n**Import errors:**\n\n```bash\n# If \"ModuleNotFoundError: No module named 'ete3'\"\nuv pip install ete3\n\n# For GUI and rendering issues\nuv pip install ete3[gui]\n```\n\n**Rendering issues:**\n\nIf `tree.render()` or `tree.show()` fails with Qt-related errors, install system dependencies:\n\n```bash\n# macOS\nbrew install qt@5\n\n# Ubuntu/Debian\nsudo apt-get install python3-pyqt5 python3-pyqt5.qtsvg\n```\n\n**NCBI Taxonomy database:**\n\nIf database download fails or becomes corrupted:\n\n```python\nfrom ete3 import NCBITaxa\nncbi = NCBITaxa()\nncbi.update_taxonomy_database()  # Redownload database\n```\n\n**Memory issues with large trees:**\n\nFor very large trees (>10,000 leaves), use iterators instead of list comprehensions:\n\n```python\n# Memory-efficient iteration\nfor leaf in tree.iter_leaves():\n    process(leaf)\n\n# Instead of\nfor leaf in tree.get_leaves():  # Loads all into memory\n    process(leaf)\n```\n\n## Newick Format Reference\n\nETE supports multiple Newick format specifications (0-100):\n\n- **Format 0**: Flexible with branch lengths (default)\n- **Format 1**: With internal node names\n- **Format 2**: With bootstrap/support values\n- **Format 5**: Internal node names + branch lengths\n- **Format 8**: All features (names, distances, support)\n- **Format 9**: Leaf names only\n- **Format 100**: Topology only\n\nSpecify format when reading/writing:\n\n```python\ntree = Tree(\"tree.nw\", format=1)\ntree.write(outfile=\"output.nw\", format=5)\n```\n\nNHX (New Hampshire eXtended) format preserves custom features:\n\n```python\ntree.write(outfile=\"tree.nhx\", features=[\"habitat\", \"temperature\", \"depth\"])\n```\n\n## Best Practices\n\n1. **Preserve branch lengths**: Use `preserve_branch_length=True` when pruning for phylogenetic analysis\n2. **Cache content**: Use `get_cached_content()` for repeated access to node contents on large trees\n3. **Use iterators**: Employ `iter_*` methods for memory-efficient processing of large trees\n4. **Choose appropriate traversal**: Postorder for bottom-up analysis, preorder for top-down\n5. **Validate monophyly**: Always check returned clade type (monophyletic/paraphyletic/polyphyletic)\n6. **Vector formats for publication**: Use PDF or SVG for publication figures (scalable, editable)\n7. **Interactive testing**: Use `tree.show()` to test visualizations before rendering to file\n8. **PhyloTree for phylogenetics**: Use PhyloTree class for gene trees and evolutionary analysis\n9. **Copy method selection**: \"newick\" for speed, \"cpickle\" for full fidelity, \"deepcopy\" for complex objects\n10. **NCBI query caching**: Store NCBI taxonomy query results to avoid repeated database access\n",
        "data/k-dense-ai/exploratory-data-analysis/SKILL.md": "---\nname: exploratory-data-analysis\ndescription: Perform comprehensive exploratory data analysis on scientific data files across 200+ file formats. This skill should be used when analyzing any scientific data file to understand its structure, content, quality, and characteristics. Automatically detects file type and generates detailed markdown reports with format-specific analysis, quality metrics, and downstream analysis recommendations. Covers chemistry, bioinformatics, microscopy, spectroscopy, proteomics, metabolomics, and general scientific data formats.\n---\n\n# Exploratory Data Analysis\n\n## Overview\n\nPerform comprehensive exploratory data analysis (EDA) on scientific data files across multiple domains. This skill provides automated file type detection, format-specific analysis, data quality assessment, and generates detailed markdown reports suitable for documentation and downstream analysis planning.\n\n**Key Capabilities:**\n- Automatic detection and analysis of 200+ scientific file formats\n- Comprehensive format-specific metadata extraction\n- Data quality and integrity assessment\n- Statistical summaries and distributions\n- Visualization recommendations\n- Downstream analysis suggestions\n- Markdown report generation\n\n## When to Use This Skill\n\nUse this skill when:\n- User provides a path to a scientific data file for analysis\n- User asks to \"explore\", \"analyze\", or \"summarize\" a data file\n- User wants to understand the structure and content of scientific data\n- User needs a comprehensive report of a dataset before analysis\n- User wants to assess data quality or completeness\n- User asks what type of analysis is appropriate for a file\n\n## Supported File Categories\n\nThe skill has comprehensive coverage of scientific file formats organized into six major categories:\n\n### 1. Chemistry and Molecular Formats (60+ extensions)\nStructure files, computational chemistry outputs, molecular dynamics trajectories, and chemical databases.\n\n**File types include:** `.pdb`, `.cif`, `.mol`, `.mol2`, `.sdf`, `.xyz`, `.smi`, `.gro`, `.log`, `.fchk`, `.cube`, `.dcd`, `.xtc`, `.trr`, `.prmtop`, `.psf`, and more.\n\n**Reference file:** `references/chemistry_molecular_formats.md`\n\n### 2. Bioinformatics and Genomics Formats (50+ extensions)\nSequence data, alignments, annotations, variants, and expression data.\n\n**File types include:** `.fasta`, `.fastq`, `.sam`, `.bam`, `.vcf`, `.bed`, `.gff`, `.gtf`, `.bigwig`, `.h5ad`, `.loom`, `.counts`, `.mtx`, and more.\n\n**Reference file:** `references/bioinformatics_genomics_formats.md`\n\n### 3. Microscopy and Imaging Formats (45+ extensions)\nMicroscopy images, medical imaging, whole slide imaging, and electron microscopy.\n\n**File types include:** `.tif`, `.nd2`, `.lif`, `.czi`, `.ims`, `.dcm`, `.nii`, `.mrc`, `.dm3`, `.vsi`, `.svs`, `.ome.tiff`, and more.\n\n**Reference file:** `references/microscopy_imaging_formats.md`\n\n### 4. Spectroscopy and Analytical Chemistry Formats (35+ extensions)\nNMR, mass spectrometry, IR/Raman, UV-Vis, X-ray, chromatography, and other analytical techniques.\n\n**File types include:** `.fid`, `.mzML`, `.mzXML`, `.raw`, `.mgf`, `.spc`, `.jdx`, `.xy`, `.cif` (crystallography), `.wdf`, and more.\n\n**Reference file:** `references/spectroscopy_analytical_formats.md`\n\n### 5. Proteomics and Metabolomics Formats (30+ extensions)\nMass spec proteomics, metabolomics, lipidomics, and multi-omics data.\n\n**File types include:** `.mzML`, `.pepXML`, `.protXML`, `.mzid`, `.mzTab`, `.sky`, `.mgf`, `.msp`, `.h5ad`, and more.\n\n**Reference file:** `references/proteomics_metabolomics_formats.md`\n\n### 6. General Scientific Data Formats (30+ extensions)\nArrays, tables, hierarchical data, compressed archives, and common scientific formats.\n\n**File types include:** `.npy`, `.npz`, `.csv`, `.xlsx`, `.json`, `.hdf5`, `.zarr`, `.parquet`, `.mat`, `.fits`, `.nc`, `.xml`, and more.\n\n**Reference file:** `references/general_scientific_formats.md`\n\n## Workflow\n\n### Step 1: File Type Detection\n\nWhen a user provides a file path, first identify the file type:\n\n1. Extract the file extension\n2. Look up the extension in the appropriate reference file\n3. Identify the file category and format description\n4. Load format-specific information\n\n**Example:**\n```\nUser: \"Analyze data.fastq\"\n Extension: .fastq\n Category: bioinformatics_genomics\n Format: FASTQ Format (sequence data with quality scores)\n Reference: references/bioinformatics_genomics_formats.md\n```\n\n### Step 2: Load Format-Specific Information\n\nBased on the file type, read the corresponding reference file to understand:\n- **Typical Data:** What kind of data this format contains\n- **Use Cases:** Common applications for this format\n- **Python Libraries:** How to read the file in Python\n- **EDA Approach:** What analyses are appropriate for this data type\n\nSearch the reference file for the specific extension (e.g., search for \"### .fastq\" in `bioinformatics_genomics_formats.md`).\n\n### Step 3: Perform Data Analysis\n\nUse the `scripts/eda_analyzer.py` script OR implement custom analysis:\n\n**Option A: Use the analyzer script**\n```python\n# The script automatically:\n# 1. Detects file type\n# 2. Loads reference information\n# 3. Performs format-specific analysis\n# 4. Generates markdown report\n\npython scripts/eda_analyzer.py <filepath> [output.md]\n```\n\n**Option B: Custom analysis in the conversation**\nBased on the format information from the reference file, perform appropriate analysis:\n\nFor tabular data (CSV, TSV, Excel):\n- Load with pandas\n- Check dimensions, data types\n- Analyze missing values\n- Calculate summary statistics\n- Identify outliers\n- Check for duplicates\n\nFor sequence data (FASTA, FASTQ):\n- Count sequences\n- Analyze length distributions\n- Calculate GC content\n- Assess quality scores (FASTQ)\n\nFor images (TIFF, ND2, CZI):\n- Check dimensions (X, Y, Z, C, T)\n- Analyze bit depth and value range\n- Extract metadata (channels, timestamps, spatial calibration)\n- Calculate intensity statistics\n\nFor arrays (NPY, HDF5):\n- Check shape and dimensions\n- Analyze data type\n- Calculate statistical summaries\n- Check for missing/invalid values\n\n### Step 4: Generate Comprehensive Report\n\nCreate a markdown report with the following sections:\n\n#### Required Sections:\n1. **Title and Metadata**\n   - Filename and timestamp\n   - File size and location\n\n2. **Basic Information**\n   - File properties\n   - Format identification\n\n3. **File Type Details**\n   - Format description from reference\n   - Typical data content\n   - Common use cases\n   - Python libraries for reading\n\n4. **Data Analysis**\n   - Structure and dimensions\n   - Statistical summaries\n   - Quality assessment\n   - Data characteristics\n\n5. **Key Findings**\n   - Notable patterns\n   - Potential issues\n   - Quality metrics\n\n6. **Recommendations**\n   - Preprocessing steps\n   - Appropriate analyses\n   - Tools and methods\n   - Visualization approaches\n\n#### Template Location\nUse `assets/report_template.md` as a guide for report structure.\n\n### Step 5: Save Report\n\nSave the markdown report with a descriptive filename:\n- Pattern: `{original_filename}_eda_report.md`\n- Example: `experiment_data.fastq`  `experiment_data_eda_report.md`\n\n## Detailed Format References\n\nEach reference file contains comprehensive information for dozens of file types. To find information about a specific format:\n\n1. Identify the category from the extension\n2. Read the appropriate reference file\n3. Search for the section heading matching the extension (e.g., \"### .pdb\")\n4. Extract the format information\n\n### Reference File Structure\n\nEach format entry includes:\n- **Description:** What the format is\n- **Typical Data:** What it contains\n- **Use Cases:** Common applications\n- **Python Libraries:** How to read it (with code examples)\n- **EDA Approach:** Specific analyses to perform\n\n**Example lookup:**\n```markdown\n### .pdb - Protein Data Bank\n**Description:** Standard format for 3D structures of biological macromolecules\n**Typical Data:** Atomic coordinates, residue information, secondary structure\n**Use Cases:** Protein structure analysis, molecular visualization, docking\n**Python Libraries:**\n- `Biopython`: `Bio.PDB`\n- `MDAnalysis`: `MDAnalysis.Universe('file.pdb')`\n**EDA Approach:**\n- Structure validation (bond lengths, angles)\n- B-factor distribution\n- Missing residues detection\n- Ramachandran plots\n```\n\n## Best Practices\n\n### Reading Reference Files\n\nReference files are large (10,000+ words each). To efficiently use them:\n\n1. **Search by extension:** Use grep to find the specific format\n   ```python\n   import re\n   with open('references/chemistry_molecular_formats.md', 'r') as f:\n       content = f.read()\n       pattern = r'### \\.pdb[^#]*?(?=###|\\Z)'\n       match = re.search(pattern, content, re.IGNORECASE | re.DOTALL)\n   ```\n\n2. **Extract relevant sections:** Don't load entire reference files into context unnecessarily\n\n3. **Cache format info:** If analyzing multiple files of the same type, reuse the format information\n\n### Data Analysis\n\n1. **Sample large files:** For files with millions of records, analyze a representative sample\n2. **Handle errors gracefully:** Many scientific formats require specific libraries; provide clear installation instructions\n3. **Validate metadata:** Cross-check metadata consistency (e.g., stated dimensions vs actual data)\n4. **Consider data provenance:** Note instrument, software versions, processing steps\n\n### Report Generation\n\n1. **Be comprehensive:** Include all relevant information for downstream analysis\n2. **Be specific:** Provide concrete recommendations based on the file type\n3. **Be actionable:** Suggest specific next steps and tools\n4. **Include code examples:** Show how to load and work with the data\n\n## Examples\n\n### Example 1: Analyzing a FASTQ file\n\n```python\n# User provides: \"Analyze reads.fastq\"\n\n# 1. Detect file type\nextension = '.fastq'\ncategory = 'bioinformatics_genomics'\n\n# 2. Read reference info\n# Search references/bioinformatics_genomics_formats.md for \"### .fastq\"\n\n# 3. Perform analysis\nfrom Bio import SeqIO\nsequences = list(SeqIO.parse('reads.fastq', 'fastq'))\n# Calculate: read count, length distribution, quality scores, GC content\n\n# 4. Generate report\n# Include: format description, analysis results, QC recommendations\n\n# 5. Save as: reads_eda_report.md\n```\n\n### Example 2: Analyzing a CSV dataset\n\n```python\n# User provides: \"Explore experiment_results.csv\"\n\n# 1. Detect: .csv  general_scientific\n\n# 2. Load reference for CSV format\n\n# 3. Analyze\nimport pandas as pd\ndf = pd.read_csv('experiment_results.csv')\n# Dimensions, dtypes, missing values, statistics, correlations\n\n# 4. Generate report with:\n# - Data structure\n# - Missing value patterns\n# - Statistical summaries\n# - Correlation matrix\n# - Outlier detection results\n\n# 5. Save report\n```\n\n### Example 3: Analyzing microscopy data\n\n```python\n# User provides: \"Analyze cells.nd2\"\n\n# 1. Detect: .nd2  microscopy_imaging (Nikon format)\n\n# 2. Read reference for ND2 format\n# Learn: multi-dimensional (XYZCT), requires nd2reader\n\n# 3. Analyze\nfrom nd2reader import ND2Reader\nwith ND2Reader('cells.nd2') as images:\n    # Extract: dimensions, channels, timepoints, metadata\n    # Calculate: intensity statistics, frame info\n\n# 4. Generate report with:\n# - Image dimensions (XY, Z-stacks, time, channels)\n# - Channel wavelengths\n# - Pixel size and calibration\n# - Recommendations for image analysis\n\n# 5. Save report\n```\n\n## Troubleshooting\n\n### Missing Libraries\n\nMany scientific formats require specialized libraries:\n\n**Problem:** Import error when trying to read a file\n\n**Solution:** Provide clear installation instructions\n```python\ntry:\n    from Bio import SeqIO\nexcept ImportError:\n    print(\"Install Biopython: uv pip install biopython\")\n```\n\nCommon requirements by category:\n- **Bioinformatics:** `biopython`, `pysam`, `pyBigWig`\n- **Chemistry:** `rdkit`, `mdanalysis`, `cclib`\n- **Microscopy:** `tifffile`, `nd2reader`, `aicsimageio`, `pydicom`\n- **Spectroscopy:** `nmrglue`, `pymzml`, `pyteomics`\n- **General:** `pandas`, `numpy`, `h5py`, `scipy`\n\n### Unknown File Types\n\nIf a file extension is not in the references:\n\n1. Ask the user about the file format\n2. Check if it's a vendor-specific variant\n3. Attempt generic analysis based on file structure (text vs binary)\n4. Provide general recommendations\n\n### Large Files\n\nFor very large files:\n\n1. Use sampling strategies (first N records)\n2. Use memory-mapped access (for HDF5, NPY)\n3. Process in chunks (for CSV, FASTQ)\n4. Provide estimates based on samples\n\n## Script Usage\n\nThe `scripts/eda_analyzer.py` can be used directly:\n\n```bash\n# Basic usage\npython scripts/eda_analyzer.py data.csv\n\n# Specify output file\npython scripts/eda_analyzer.py data.csv output_report.md\n\n# The script will:\n# 1. Auto-detect file type\n# 2. Load format references\n# 3. Perform appropriate analysis\n# 4. Generate markdown report\n```\n\nThe script supports automatic analysis for many common formats, but custom analysis in the conversation provides more flexibility and domain-specific insights.\n\n## Advanced Usage\n\n### Multi-File Analysis\n\nWhen analyzing multiple related files:\n1. Perform individual EDA on each file\n2. Create a summary comparison report\n3. Identify relationships and dependencies\n4. Suggest integration strategies\n\n### Quality Control\n\nFor data quality assessment:\n1. Check format compliance\n2. Validate metadata consistency\n3. Assess completeness\n4. Identify outliers and anomalies\n5. Compare to expected ranges/distributions\n\n### Preprocessing Recommendations\n\nBased on data characteristics, recommend:\n1. Normalization strategies\n2. Missing value imputation\n3. Outlier handling\n4. Batch correction\n5. Format conversions\n\n## Resources\n\n### scripts/\n- `eda_analyzer.py`: Comprehensive analysis script that can be run directly or imported\n\n### references/\n- `chemistry_molecular_formats.md`: 60+ chemistry/molecular file formats\n- `bioinformatics_genomics_formats.md`: 50+ bioinformatics formats\n- `microscopy_imaging_formats.md`: 45+ imaging formats\n- `spectroscopy_analytical_formats.md`: 35+ spectroscopy formats\n- `proteomics_metabolomics_formats.md`: 30+ omics formats\n- `general_scientific_formats.md`: 30+ general formats\n\n### assets/\n- `report_template.md`: Comprehensive markdown template for EDA reports\n",
        "data/k-dense-ai/fda-database/SKILL.md": "---\nname: fda-database\ndescription: \"Query openFDA API for drugs, devices, adverse events, recalls, regulatory submissions (510k, PMA), substance identification (UNII), for FDA regulatory data analysis and safety research.\"\n---\n\n# FDA Database Access\n\n## Overview\n\nAccess comprehensive FDA regulatory data through openFDA, the FDA's initiative to provide open APIs for public datasets. Query information about drugs, medical devices, foods, animal/veterinary products, and substances using Python with standardized interfaces.\n\n**Key capabilities:**\n- Query adverse events for drugs, devices, foods, and veterinary products\n- Access product labeling, approvals, and regulatory submissions\n- Monitor recalls and enforcement actions\n- Look up National Drug Codes (NDC) and substance identifiers (UNII)\n- Analyze device classifications and clearances (510k, PMA)\n- Track drug shortages and supply issues\n- Research chemical structures and substance relationships\n\n## When to Use This Skill\n\nThis skill should be used when working with:\n- **Drug research**: Safety profiles, adverse events, labeling, approvals, shortages\n- **Medical device surveillance**: Adverse events, recalls, 510(k) clearances, PMA approvals\n- **Food safety**: Recalls, allergen tracking, adverse events, dietary supplements\n- **Veterinary medicine**: Animal drug adverse events by species and breed\n- **Chemical/substance data**: UNII lookup, CAS number mapping, molecular structures\n- **Regulatory analysis**: Approval pathways, enforcement actions, compliance tracking\n- **Pharmacovigilance**: Post-market surveillance, safety signal detection\n- **Scientific research**: Drug interactions, comparative safety, epidemiological studies\n\n## Quick Start\n\n### 1. Basic Setup\n\n```python\nfrom scripts.fda_query import FDAQuery\n\n# Initialize (API key optional but recommended)\nfda = FDAQuery(api_key=\"YOUR_API_KEY\")\n\n# Query drug adverse events\nevents = fda.query_drug_events(\"aspirin\", limit=100)\n\n# Get drug labeling\nlabel = fda.query_drug_label(\"Lipitor\", brand=True)\n\n# Search device recalls\nrecalls = fda.query(\"device\", \"enforcement\",\n                   search=\"classification:Class+I\",\n                   limit=50)\n```\n\n### 2. API Key Setup\n\nWhile the API works without a key, registering provides higher rate limits:\n- **Without key**: 240 requests/min, 1,000/day\n- **With key**: 240 requests/min, 120,000/day\n\nRegister at: https://open.fda.gov/apis/authentication/\n\nSet as environment variable:\n```bash\nexport FDA_API_KEY=\"your_key_here\"\n```\n\n### 3. Running Examples\n\n```bash\n# Run comprehensive examples\npython scripts/fda_examples.py\n\n# This demonstrates:\n# - Drug safety profiles\n# - Device surveillance\n# - Food recall monitoring\n# - Substance lookup\n# - Comparative drug analysis\n# - Veterinary drug analysis\n```\n\n## FDA Database Categories\n\n### Drugs\n\nAccess 6 drug-related endpoints covering the full drug lifecycle from approval to post-market surveillance.\n\n**Endpoints:**\n1. **Adverse Events** - Reports of side effects, errors, and therapeutic failures\n2. **Product Labeling** - Prescribing information, warnings, indications\n3. **NDC Directory** - National Drug Code product information\n4. **Enforcement Reports** - Drug recalls and safety actions\n5. **Drugs@FDA** - Historical approval data since 1939\n6. **Drug Shortages** - Current and resolved supply issues\n\n**Common use cases:**\n```python\n# Safety signal detection\nfda.count_by_field(\"drug\", \"event\",\n                  search=\"patient.drug.medicinalproduct:metformin\",\n                  field=\"patient.reaction.reactionmeddrapt\")\n\n# Get prescribing information\nlabel = fda.query_drug_label(\"Keytruda\", brand=True)\n\n# Check for recalls\nrecalls = fda.query_drug_recalls(drug_name=\"metformin\")\n\n# Monitor shortages\nshortages = fda.query(\"drug\", \"drugshortages\",\n                     search=\"status:Currently+in+Shortage\")\n```\n\n**Reference:** See `references/drugs.md` for detailed documentation\n\n### Devices\n\nAccess 9 device-related endpoints covering medical device safety, approvals, and registrations.\n\n**Endpoints:**\n1. **Adverse Events** - Device malfunctions, injuries, deaths\n2. **510(k) Clearances** - Premarket notifications\n3. **Classification** - Device categories and risk classes\n4. **Enforcement Reports** - Device recalls\n5. **Recalls** - Detailed recall information\n6. **PMA** - Premarket approval data for Class III devices\n7. **Registrations & Listings** - Manufacturing facility data\n8. **UDI** - Unique Device Identification database\n9. **COVID-19 Serology** - Antibody test performance data\n\n**Common use cases:**\n```python\n# Monitor device safety\nevents = fda.query_device_events(\"pacemaker\", limit=100)\n\n# Look up device classification\nclassification = fda.query_device_classification(\"DQY\")\n\n# Find 510(k) clearances\nclearances = fda.query_device_510k(applicant=\"Medtronic\")\n\n# Search by UDI\ndevice_info = fda.query(\"device\", \"udi\",\n                       search=\"identifiers.id:00884838003019\")\n```\n\n**Reference:** See `references/devices.md` for detailed documentation\n\n### Foods\n\nAccess 2 food-related endpoints for safety monitoring and recalls.\n\n**Endpoints:**\n1. **Adverse Events** - Food, dietary supplement, and cosmetic events\n2. **Enforcement Reports** - Food product recalls\n\n**Common use cases:**\n```python\n# Monitor allergen recalls\nrecalls = fda.query_food_recalls(reason=\"undeclared peanut\")\n\n# Track dietary supplement events\nevents = fda.query_food_events(\n    industry=\"Dietary Supplements\")\n\n# Find contamination recalls\nlisteria = fda.query_food_recalls(\n    reason=\"listeria\",\n    classification=\"I\")\n```\n\n**Reference:** See `references/foods.md` for detailed documentation\n\n### Animal & Veterinary\n\nAccess veterinary drug adverse event data with species-specific information.\n\n**Endpoint:**\n1. **Adverse Events** - Animal drug side effects by species, breed, and product\n\n**Common use cases:**\n```python\n# Species-specific events\ndog_events = fda.query_animal_events(\n    species=\"Dog\",\n    drug_name=\"flea collar\")\n\n# Breed predisposition analysis\nbreed_query = fda.query(\"animalandveterinary\", \"event\",\n    search=\"reaction.veddra_term_name:*seizure*+AND+\"\n           \"animal.breed.breed_component:*Labrador*\")\n```\n\n**Reference:** See `references/animal_veterinary.md` for detailed documentation\n\n### Substances & Other\n\nAccess molecular-level substance data with UNII codes, chemical structures, and relationships.\n\n**Endpoints:**\n1. **Substance Data** - UNII, CAS, chemical structures, relationships\n2. **NSDE** - Historical substance data (legacy)\n\n**Common use cases:**\n```python\n# UNII to CAS mapping\nsubstance = fda.query_substance_by_unii(\"R16CO5Y76E\")\n\n# Search by name\nresults = fda.query_substance_by_name(\"acetaminophen\")\n\n# Get chemical structure\nstructure = fda.query(\"other\", \"substance\",\n    search=\"names.name:ibuprofen+AND+substanceClass:chemical\")\n```\n\n**Reference:** See `references/other.md` for detailed documentation\n\n## Common Query Patterns\n\n### Pattern 1: Safety Profile Analysis\n\nCreate comprehensive safety profiles combining multiple data sources:\n\n```python\ndef drug_safety_profile(fda, drug_name):\n    \"\"\"Generate complete safety profile.\"\"\"\n\n    # 1. Total adverse events\n    events = fda.query_drug_events(drug_name, limit=1)\n    total = events[\"meta\"][\"results\"][\"total\"]\n\n    # 2. Most common reactions\n    reactions = fda.count_by_field(\n        \"drug\", \"event\",\n        search=f\"patient.drug.medicinalproduct:*{drug_name}*\",\n        field=\"patient.reaction.reactionmeddrapt\",\n        exact=True\n    )\n\n    # 3. Serious events\n    serious = fda.query(\"drug\", \"event\",\n        search=f\"patient.drug.medicinalproduct:*{drug_name}*+AND+serious:1\",\n        limit=1)\n\n    # 4. Recent recalls\n    recalls = fda.query_drug_recalls(drug_name=drug_name)\n\n    return {\n        \"total_events\": total,\n        \"top_reactions\": reactions[\"results\"][:10],\n        \"serious_events\": serious[\"meta\"][\"results\"][\"total\"],\n        \"recalls\": recalls[\"results\"]\n    }\n```\n\n### Pattern 2: Temporal Trend Analysis\n\nAnalyze trends over time using date ranges:\n\n```python\nfrom datetime import datetime, timedelta\n\ndef get_monthly_trends(fda, drug_name, months=12):\n    \"\"\"Get monthly adverse event trends.\"\"\"\n    trends = []\n\n    for i in range(months):\n        end = datetime.now() - timedelta(days=30*i)\n        start = end - timedelta(days=30)\n\n        date_range = f\"[{start.strftime('%Y%m%d')}+TO+{end.strftime('%Y%m%d')}]\"\n        search = f\"patient.drug.medicinalproduct:*{drug_name}*+AND+receivedate:{date_range}\"\n\n        result = fda.query(\"drug\", \"event\", search=search, limit=1)\n        count = result[\"meta\"][\"results\"][\"total\"] if \"meta\" in result else 0\n\n        trends.append({\n            \"month\": start.strftime(\"%Y-%m\"),\n            \"events\": count\n        })\n\n    return trends\n```\n\n### Pattern 3: Comparative Analysis\n\nCompare multiple products side-by-side:\n\n```python\ndef compare_drugs(fda, drug_list):\n    \"\"\"Compare safety profiles of multiple drugs.\"\"\"\n    comparison = {}\n\n    for drug in drug_list:\n        # Total events\n        events = fda.query_drug_events(drug, limit=1)\n        total = events[\"meta\"][\"results\"][\"total\"] if \"meta\" in events else 0\n\n        # Serious events\n        serious = fda.query(\"drug\", \"event\",\n            search=f\"patient.drug.medicinalproduct:*{drug}*+AND+serious:1\",\n            limit=1)\n        serious_count = serious[\"meta\"][\"results\"][\"total\"] if \"meta\" in serious else 0\n\n        comparison[drug] = {\n            \"total_events\": total,\n            \"serious_events\": serious_count,\n            \"serious_rate\": (serious_count/total*100) if total > 0 else 0\n        }\n\n    return comparison\n```\n\n### Pattern 4: Cross-Database Lookup\n\nLink data across multiple endpoints:\n\n```python\ndef comprehensive_device_lookup(fda, device_name):\n    \"\"\"Look up device across all relevant databases.\"\"\"\n\n    return {\n        \"adverse_events\": fda.query_device_events(device_name, limit=10),\n        \"510k_clearances\": fda.query_device_510k(device_name=device_name),\n        \"recalls\": fda.query(\"device\", \"enforcement\",\n                           search=f\"product_description:*{device_name}*\"),\n        \"udi_info\": fda.query(\"device\", \"udi\",\n                            search=f\"brand_name:*{device_name}*\")\n    }\n```\n\n## Working with Results\n\n### Response Structure\n\nAll API responses follow this structure:\n\n```python\n{\n    \"meta\": {\n        \"disclaimer\": \"...\",\n        \"results\": {\n            \"skip\": 0,\n            \"limit\": 100,\n            \"total\": 15234\n        }\n    },\n    \"results\": [\n        # Array of result objects\n    ]\n}\n```\n\n### Error Handling\n\nAlways handle potential errors:\n\n```python\nresult = fda.query_drug_events(\"aspirin\", limit=10)\n\nif \"error\" in result:\n    print(f\"Error: {result['error']}\")\nelif \"results\" not in result or len(result[\"results\"]) == 0:\n    print(\"No results found\")\nelse:\n    # Process results\n    for event in result[\"results\"]:\n        # Handle event data\n        pass\n```\n\n### Pagination\n\nFor large result sets, use pagination:\n\n```python\n# Automatic pagination\nall_results = fda.query_all(\n    \"drug\", \"event\",\n    search=\"patient.drug.medicinalproduct:aspirin\",\n    max_results=5000\n)\n\n# Manual pagination\nfor skip in range(0, 1000, 100):\n    batch = fda.query(\"drug\", \"event\",\n                     search=\"...\",\n                     limit=100,\n                     skip=skip)\n    # Process batch\n```\n\n## Best Practices\n\n### 1. Use Specific Searches\n\n**DO:**\n```python\n# Specific field search\nsearch=\"patient.drug.medicinalproduct:aspirin\"\n```\n\n**DON'T:**\n```python\n# Overly broad wildcard\nsearch=\"*aspirin*\"\n```\n\n### 2. Implement Rate Limiting\n\nThe `FDAQuery` class handles rate limiting automatically, but be aware of limits:\n- 240 requests per minute\n- 120,000 requests per day (with API key)\n\n### 3. Cache Frequently Accessed Data\n\nThe `FDAQuery` class includes built-in caching (enabled by default):\n\n```python\n# Caching is automatic\nfda = FDAQuery(api_key=api_key, use_cache=True, cache_ttl=3600)\n```\n\n### 4. Use Exact Matching for Counting\n\nWhen counting/aggregating, use `.exact` suffix:\n\n```python\n# Count exact phrases\nfda.count_by_field(\"drug\", \"event\",\n                  search=\"...\",\n                  field=\"patient.reaction.reactionmeddrapt\",\n                  exact=True)  # Adds .exact automatically\n```\n\n### 5. Validate Input Data\n\nClean and validate search terms:\n\n```python\ndef clean_drug_name(name):\n    \"\"\"Clean drug name for query.\"\"\"\n    return name.strip().replace('\"', '\\\\\"')\n\ndrug_name = clean_drug_name(user_input)\n```\n\n## API Reference\n\nFor detailed information about:\n- **Authentication and rate limits**  See `references/api_basics.md`\n- **Drug databases**  See `references/drugs.md`\n- **Device databases**  See `references/devices.md`\n- **Food databases**  See `references/foods.md`\n- **Animal/veterinary databases**  See `references/animal_veterinary.md`\n- **Substance databases**  See `references/other.md`\n\n## Scripts\n\n### `scripts/fda_query.py`\n\nMain query module with `FDAQuery` class providing:\n- Unified interface to all FDA endpoints\n- Automatic rate limiting and caching\n- Error handling and retry logic\n- Common query patterns\n\n### `scripts/fda_examples.py`\n\nComprehensive examples demonstrating:\n- Drug safety profile analysis\n- Device surveillance monitoring\n- Food recall tracking\n- Substance lookup\n- Comparative drug analysis\n- Veterinary drug analysis\n\nRun examples:\n```bash\npython scripts/fda_examples.py\n```\n\n## Additional Resources\n\n- **openFDA Homepage**: https://open.fda.gov/\n- **API Documentation**: https://open.fda.gov/apis/\n- **Interactive API Explorer**: https://open.fda.gov/apis/try-the-api/\n- **GitHub Repository**: https://github.com/FDA/openfda\n- **Terms of Service**: https://open.fda.gov/terms/\n\n## Support and Troubleshooting\n\n### Common Issues\n\n**Issue**: Rate limit exceeded\n- **Solution**: Use API key, implement delays, or reduce request frequency\n\n**Issue**: No results found\n- **Solution**: Try broader search terms, check spelling, use wildcards\n\n**Issue**: Invalid query syntax\n- **Solution**: Review query syntax in `references/api_basics.md`\n\n**Issue**: Missing fields in results\n- **Solution**: Not all records contain all fields; always check field existence\n\n### Getting Help\n\n- **GitHub Issues**: https://github.com/FDA/openfda/issues\n- **Email**: open-fda@fda.hhs.gov\n",
        "data/k-dense-ai/flowio/SKILL.md": "---\nname: flowio\ndescription: \"Parse FCS (Flow Cytometry Standard) files v2.0-3.1. Extract events as NumPy arrays, read metadata/channels, convert to CSV/DataFrame, for flow cytometry data preprocessing.\"\n---\n\n# FlowIO: Flow Cytometry Standard File Handler\n\n## Overview\n\nFlowIO is a lightweight Python library for reading and writing Flow Cytometry Standard (FCS) files. Parse FCS metadata, extract event data, and create new FCS files with minimal dependencies. The library supports FCS versions 2.0, 3.0, and 3.1, making it ideal for backend services, data pipelines, and basic cytometry file operations.\n\n## When to Use This Skill\n\nThis skill should be used when:\n\n- FCS files requiring parsing or metadata extraction\n- Flow cytometry data needing conversion to NumPy arrays\n- Event data requiring export to FCS format\n- Multi-dataset FCS files needing separation\n- Channel information extraction (scatter, fluorescence, time)\n- Cytometry file validation or inspection\n- Pre-processing workflows before advanced analysis\n\n**Related Tools:** For advanced flow cytometry analysis including compensation, gating, and FlowJo/GatingML support, recommend FlowKit library as a companion to FlowIO.\n\n## Installation\n\n```bash\nuv pip install flowio\n```\n\nRequires Python 3.9 or later.\n\n## Quick Start\n\n### Basic File Reading\n\n```python\nfrom flowio import FlowData\n\n# Read FCS file\nflow_data = FlowData('experiment.fcs')\n\n# Access basic information\nprint(f\"FCS Version: {flow_data.version}\")\nprint(f\"Events: {flow_data.event_count}\")\nprint(f\"Channels: {flow_data.pnn_labels}\")\n\n# Get event data as NumPy array\nevents = flow_data.as_array()  # Shape: (events, channels)\n```\n\n### Creating FCS Files\n\n```python\nimport numpy as np\nfrom flowio import create_fcs\n\n# Prepare data\ndata = np.array([[100, 200, 50], [150, 180, 60]])  # 2 events, 3 channels\nchannels = ['FSC-A', 'SSC-A', 'FL1-A']\n\n# Create FCS file\ncreate_fcs('output.fcs', data, channels)\n```\n\n## Core Workflows\n\n### Reading and Parsing FCS Files\n\nThe FlowData class provides the primary interface for reading FCS files.\n\n**Standard Reading:**\n\n```python\nfrom flowio import FlowData\n\n# Basic reading\nflow = FlowData('sample.fcs')\n\n# Access attributes\nversion = flow.version              # '3.0', '3.1', etc.\nevent_count = flow.event_count      # Number of events\nchannel_count = flow.channel_count  # Number of channels\npnn_labels = flow.pnn_labels        # Short channel names\npns_labels = flow.pns_labels        # Descriptive stain names\n\n# Get event data\nevents = flow.as_array()            # Preprocessed (gain, log scaling applied)\nraw_events = flow.as_array(preprocess=False)  # Raw data\n```\n\n**Memory-Efficient Metadata Reading:**\n\nWhen only metadata is needed (no event data):\n\n```python\n# Only parse TEXT segment, skip DATA and ANALYSIS\nflow = FlowData('sample.fcs', only_text=True)\n\n# Access metadata\nmetadata = flow.text  # Dictionary of TEXT segment keywords\nprint(metadata.get('$DATE'))  # Acquisition date\nprint(metadata.get('$CYT'))   # Instrument name\n```\n\n**Handling Problematic Files:**\n\nSome FCS files have offset discrepancies or errors:\n\n```python\n# Ignore offset discrepancies between HEADER and TEXT sections\nflow = FlowData('problematic.fcs', ignore_offset_discrepancy=True)\n\n# Use HEADER offsets instead of TEXT offsets\nflow = FlowData('problematic.fcs', use_header_offsets=True)\n\n# Ignore offset errors entirely\nflow = FlowData('problematic.fcs', ignore_offset_error=True)\n```\n\n**Excluding Null Channels:**\n\n```python\n# Exclude specific channels during parsing\nflow = FlowData('sample.fcs', null_channel_list=['Time', 'Null'])\n```\n\n### Extracting Metadata and Channel Information\n\nFCS files contain rich metadata in the TEXT segment.\n\n**Common Metadata Keywords:**\n\n```python\nflow = FlowData('sample.fcs')\n\n# File-level metadata\ntext_dict = flow.text\nacquisition_date = text_dict.get('$DATE', 'Unknown')\ninstrument = text_dict.get('$CYT', 'Unknown')\ndata_type = flow.data_type  # 'I', 'F', 'D', 'A'\n\n# Channel metadata\nfor i in range(flow.channel_count):\n    pnn = flow.pnn_labels[i]      # Short name (e.g., 'FSC-A')\n    pns = flow.pns_labels[i]      # Descriptive name (e.g., 'Forward Scatter')\n    pnr = flow.pnr_values[i]      # Range/max value\n    print(f\"Channel {i}: {pnn} ({pns}), Range: {pnr}\")\n```\n\n**Channel Type Identification:**\n\nFlowIO automatically categorizes channels:\n\n```python\n# Get indices by channel type\nscatter_idx = flow.scatter_indices    # [0, 1] for FSC, SSC\nfluoro_idx = flow.fluoro_indices      # [2, 3, 4] for FL channels\ntime_idx = flow.time_index            # Index of time channel (or None)\n\n# Access specific channel types\nevents = flow.as_array()\nscatter_data = events[:, scatter_idx]\nfluorescence_data = events[:, fluoro_idx]\n```\n\n**ANALYSIS Segment:**\n\nIf present, access processed results:\n\n```python\nif flow.analysis:\n    analysis_keywords = flow.analysis  # Dictionary of ANALYSIS keywords\n    print(analysis_keywords)\n```\n\n### Creating New FCS Files\n\nGenerate FCS files from NumPy arrays or other data sources.\n\n**Basic Creation:**\n\n```python\nimport numpy as np\nfrom flowio import create_fcs\n\n# Create event data (rows=events, columns=channels)\nevents = np.random.rand(10000, 5) * 1000\n\n# Define channel names\nchannel_names = ['FSC-A', 'SSC-A', 'FL1-A', 'FL2-A', 'Time']\n\n# Create FCS file\ncreate_fcs('output.fcs', events, channel_names)\n```\n\n**With Descriptive Channel Names:**\n\n```python\n# Add optional descriptive names (PnS)\nchannel_names = ['FSC-A', 'SSC-A', 'FL1-A', 'FL2-A', 'Time']\ndescriptive_names = ['Forward Scatter', 'Side Scatter', 'FITC', 'PE', 'Time']\n\ncreate_fcs('output.fcs',\n           events,\n           channel_names,\n           opt_channel_names=descriptive_names)\n```\n\n**With Custom Metadata:**\n\n```python\n# Add TEXT segment metadata\nmetadata = {\n    '$SRC': 'Python script',\n    '$DATE': '19-OCT-2025',\n    '$CYT': 'Synthetic Instrument',\n    '$INST': 'Laboratory A'\n}\n\ncreate_fcs('output.fcs',\n           events,\n           channel_names,\n           opt_channel_names=descriptive_names,\n           metadata=metadata)\n```\n\n**Note:** FlowIO exports as FCS 3.1 with single-precision floating-point data.\n\n### Exporting Modified Data\n\nModify existing FCS files and re-export them.\n\n**Approach 1: Using write_fcs() Method:**\n\n```python\nfrom flowio import FlowData\n\n# Read original file\nflow = FlowData('original.fcs')\n\n# Write with updated metadata\nflow.write_fcs('modified.fcs', metadata={'$SRC': 'Modified data'})\n```\n\n**Approach 2: Extract, Modify, and Recreate:**\n\nFor modifying event data:\n\n```python\nfrom flowio import FlowData, create_fcs\n\n# Read and extract data\nflow = FlowData('original.fcs')\nevents = flow.as_array(preprocess=False)\n\n# Modify event data\nevents[:, 0] = events[:, 0] * 1.5  # Scale first channel\n\n# Create new FCS file with modified data\ncreate_fcs('modified.fcs',\n           events,\n           flow.pnn_labels,\n           opt_channel_names=flow.pns_labels,\n           metadata=flow.text)\n```\n\n### Handling Multi-Dataset FCS Files\n\nSome FCS files contain multiple datasets in a single file.\n\n**Detecting Multi-Dataset Files:**\n\n```python\nfrom flowio import FlowData, MultipleDataSetsError\n\ntry:\n    flow = FlowData('sample.fcs')\nexcept MultipleDataSetsError:\n    print(\"File contains multiple datasets\")\n    # Use read_multiple_data_sets() instead\n```\n\n**Reading All Datasets:**\n\n```python\nfrom flowio import read_multiple_data_sets\n\n# Read all datasets from file\ndatasets = read_multiple_data_sets('multi_dataset.fcs')\n\nprint(f\"Found {len(datasets)} datasets\")\n\n# Process each dataset\nfor i, dataset in enumerate(datasets):\n    print(f\"\\nDataset {i}:\")\n    print(f\"  Events: {dataset.event_count}\")\n    print(f\"  Channels: {dataset.pnn_labels}\")\n\n    # Get event data for this dataset\n    events = dataset.as_array()\n    print(f\"  Shape: {events.shape}\")\n    print(f\"  Mean values: {events.mean(axis=0)}\")\n```\n\n**Reading Specific Dataset:**\n\n```python\nfrom flowio import FlowData\n\n# Read first dataset (nextdata_offset=0)\nfirst_dataset = FlowData('multi.fcs', nextdata_offset=0)\n\n# Read second dataset using NEXTDATA offset from first\nnext_offset = int(first_dataset.text['$NEXTDATA'])\nif next_offset > 0:\n    second_dataset = FlowData('multi.fcs', nextdata_offset=next_offset)\n```\n\n## Data Preprocessing\n\nFlowIO applies standard FCS preprocessing transformations when `preprocess=True`.\n\n**Preprocessing Steps:**\n\n1. **Gain Scaling:** Multiply values by PnG (gain) keyword\n2. **Logarithmic Transformation:** Apply PnE exponential transformation if present\n   - Formula: `value = a * 10^(b * raw_value)` where PnE = \"a,b\"\n3. **Time Scaling:** Convert time values to appropriate units\n\n**Controlling Preprocessing:**\n\n```python\n# Preprocessed data (default)\npreprocessed = flow.as_array(preprocess=True)\n\n# Raw data (no transformations)\nraw = flow.as_array(preprocess=False)\n```\n\n## Error Handling\n\nHandle common FlowIO exceptions appropriately.\n\n```python\nfrom flowio import (\n    FlowData,\n    FCSParsingError,\n    DataOffsetDiscrepancyError,\n    MultipleDataSetsError\n)\n\ntry:\n    flow = FlowData('sample.fcs')\n    events = flow.as_array()\n\nexcept FCSParsingError as e:\n    print(f\"Failed to parse FCS file: {e}\")\n    # Try with relaxed parsing\n    flow = FlowData('sample.fcs', ignore_offset_error=True)\n\nexcept DataOffsetDiscrepancyError as e:\n    print(f\"Offset discrepancy detected: {e}\")\n    # Use ignore_offset_discrepancy parameter\n    flow = FlowData('sample.fcs', ignore_offset_discrepancy=True)\n\nexcept MultipleDataSetsError as e:\n    print(f\"Multiple datasets detected: {e}\")\n    # Use read_multiple_data_sets instead\n    from flowio import read_multiple_data_sets\n    datasets = read_multiple_data_sets('sample.fcs')\n\nexcept Exception as e:\n    print(f\"Unexpected error: {e}\")\n```\n\n## Common Use Cases\n\n### Inspecting FCS File Contents\n\nQuick exploration of FCS file structure:\n\n```python\nfrom flowio import FlowData\n\nflow = FlowData('unknown.fcs')\n\nprint(\"=\" * 50)\nprint(f\"File: {flow.name}\")\nprint(f\"Version: {flow.version}\")\nprint(f\"Size: {flow.file_size:,} bytes\")\nprint(\"=\" * 50)\n\nprint(f\"\\nEvents: {flow.event_count:,}\")\nprint(f\"Channels: {flow.channel_count}\")\n\nprint(\"\\nChannel Information:\")\nfor i, (pnn, pns) in enumerate(zip(flow.pnn_labels, flow.pns_labels)):\n    ch_type = \"scatter\" if i in flow.scatter_indices else \\\n              \"fluoro\" if i in flow.fluoro_indices else \\\n              \"time\" if i == flow.time_index else \"other\"\n    print(f\"  [{i}] {pnn:10s} | {pns:30s} | {ch_type}\")\n\nprint(\"\\nKey Metadata:\")\nfor key in ['$DATE', '$BTIM', '$ETIM', '$CYT', '$INST', '$SRC']:\n    value = flow.text.get(key, 'N/A')\n    print(f\"  {key:15s}: {value}\")\n```\n\n### Batch Processing Multiple Files\n\nProcess a directory of FCS files:\n\n```python\nfrom pathlib import Path\nfrom flowio import FlowData\nimport pandas as pd\n\n# Find all FCS files\nfcs_files = list(Path('data/').glob('*.fcs'))\n\n# Extract summary information\nsummaries = []\nfor fcs_path in fcs_files:\n    try:\n        flow = FlowData(str(fcs_path), only_text=True)\n        summaries.append({\n            'filename': fcs_path.name,\n            'version': flow.version,\n            'events': flow.event_count,\n            'channels': flow.channel_count,\n            'date': flow.text.get('$DATE', 'N/A')\n        })\n    except Exception as e:\n        print(f\"Error processing {fcs_path.name}: {e}\")\n\n# Create summary DataFrame\ndf = pd.DataFrame(summaries)\nprint(df)\n```\n\n### Converting FCS to CSV\n\nExport event data to CSV format:\n\n```python\nfrom flowio import FlowData\nimport pandas as pd\n\n# Read FCS file\nflow = FlowData('sample.fcs')\n\n# Convert to DataFrame\ndf = pd.DataFrame(\n    flow.as_array(),\n    columns=flow.pnn_labels\n)\n\n# Add metadata as attributes\ndf.attrs['fcs_version'] = flow.version\ndf.attrs['instrument'] = flow.text.get('$CYT', 'Unknown')\n\n# Export to CSV\ndf.to_csv('output.csv', index=False)\nprint(f\"Exported {len(df)} events to CSV\")\n```\n\n### Filtering Events and Re-exporting\n\nApply filters and save filtered data:\n\n```python\nfrom flowio import FlowData, create_fcs\nimport numpy as np\n\n# Read original file\nflow = FlowData('sample.fcs')\nevents = flow.as_array(preprocess=False)\n\n# Apply filtering (example: threshold on first channel)\nfsc_idx = 0\nthreshold = 500\nmask = events[:, fsc_idx] > threshold\nfiltered_events = events[mask]\n\nprint(f\"Original events: {len(events)}\")\nprint(f\"Filtered events: {len(filtered_events)}\")\n\n# Create new FCS file with filtered data\ncreate_fcs('filtered.fcs',\n           filtered_events,\n           flow.pnn_labels,\n           opt_channel_names=flow.pns_labels,\n           metadata={**flow.text, '$SRC': 'Filtered data'})\n```\n\n### Extracting Specific Channels\n\nExtract and process specific channels:\n\n```python\nfrom flowio import FlowData\nimport numpy as np\n\nflow = FlowData('sample.fcs')\nevents = flow.as_array()\n\n# Extract fluorescence channels only\nfluoro_indices = flow.fluoro_indices\nfluoro_data = events[:, fluoro_indices]\nfluoro_names = [flow.pnn_labels[i] for i in fluoro_indices]\n\nprint(f\"Fluorescence channels: {fluoro_names}\")\nprint(f\"Shape: {fluoro_data.shape}\")\n\n# Calculate statistics per channel\nfor i, name in enumerate(fluoro_names):\n    channel_data = fluoro_data[:, i]\n    print(f\"\\n{name}:\")\n    print(f\"  Mean: {channel_data.mean():.2f}\")\n    print(f\"  Median: {np.median(channel_data):.2f}\")\n    print(f\"  Std Dev: {channel_data.std():.2f}\")\n```\n\n## Best Practices\n\n1. **Memory Efficiency:** Use `only_text=True` when event data is not needed\n2. **Error Handling:** Wrap file operations in try-except blocks for robust code\n3. **Multi-Dataset Detection:** Check for MultipleDataSetsError and use appropriate function\n4. **Preprocessing Control:** Explicitly set `preprocess` parameter based on analysis needs\n5. **Offset Issues:** If parsing fails, try `ignore_offset_discrepancy=True` parameter\n6. **Channel Validation:** Verify channel counts and names match expectations before processing\n7. **Metadata Preservation:** When modifying files, preserve original TEXT segment keywords\n\n## Advanced Topics\n\n### Understanding FCS File Structure\n\nFCS files consist of four segments:\n\n1. **HEADER:** FCS version and byte offsets for other segments\n2. **TEXT:** Key-value metadata pairs (delimiter-separated)\n3. **DATA:** Raw event data (binary/float/ASCII format)\n4. **ANALYSIS** (optional): Results from data processing\n\nAccess these segments via FlowData attributes:\n- `flow.header` - HEADER segment\n- `flow.text` - TEXT segment keywords\n- `flow.events` - DATA segment (as bytes)\n- `flow.analysis` - ANALYSIS segment keywords (if present)\n\n### Detailed API Reference\n\nFor comprehensive API documentation including all parameters, methods, exceptions, and FCS keyword reference, consult the detailed reference file:\n\n**Read:** `references/api_reference.md`\n\nThe reference includes:\n- Complete FlowData class documentation\n- All utility functions (read_multiple_data_sets, create_fcs)\n- Exception classes and handling\n- FCS file structure details\n- Common TEXT segment keywords\n- Extended example workflows\n\nWhen working with complex FCS operations or encountering unusual file formats, load this reference for detailed guidance.\n\n## Integration Notes\n\n**NumPy Arrays:** All event data is returned as NumPy ndarrays with shape (events, channels)\n\n**Pandas DataFrames:** Easily convert to DataFrames for analysis:\n```python\nimport pandas as pd\ndf = pd.DataFrame(flow.as_array(), columns=flow.pnn_labels)\n```\n\n**FlowKit Integration:** For advanced analysis (compensation, gating, FlowJo support), use FlowKit library which builds on FlowIO's parsing capabilities\n\n**Web Applications:** FlowIO's minimal dependencies make it ideal for web backend services processing FCS uploads\n\n## Troubleshooting\n\n**Problem:** \"Offset discrepancy error\"\n**Solution:** Use `ignore_offset_discrepancy=True` parameter\n\n**Problem:** \"Multiple datasets error\"\n**Solution:** Use `read_multiple_data_sets()` function instead of FlowData constructor\n\n**Problem:** Out of memory with large files\n**Solution:** Use `only_text=True` for metadata-only operations, or process events in chunks\n\n**Problem:** Unexpected channel counts\n**Solution:** Check for null channels; use `null_channel_list` parameter to exclude them\n\n**Problem:** Cannot modify event data in place\n**Solution:** FlowIO doesn't support direct modification; extract data, modify, then use `create_fcs()` to save\n\n## Summary\n\nFlowIO provides essential FCS file handling capabilities for flow cytometry workflows. Use it for parsing, metadata extraction, and file creation. For simple file operations and data extraction, FlowIO is sufficient. For complex analysis including compensation and gating, integrate with FlowKit or other specialized tools.\n",
        "data/k-dense-ai/fluidsim/SKILL.md": "---\nname: fluidsim\ndescription: Framework for computational fluid dynamics simulations using Python. Use when running fluid dynamics simulations including Navier-Stokes equations (2D/3D), shallow water equations, stratified flows, or when analyzing turbulence, vortex dynamics, or geophysical flows. Provides pseudospectral methods with FFT, HPC support, and comprehensive output analysis.\n---\n\n# FluidSim\n\n## Overview\n\nFluidSim is an object-oriented Python framework for high-performance computational fluid dynamics (CFD) simulations. It provides solvers for periodic-domain equations using pseudospectral methods with FFT, delivering performance comparable to Fortran/C++ while maintaining Python's ease of use.\n\n**Key strengths**:\n- Multiple solvers: 2D/3D Navier-Stokes, shallow water, stratified flows\n- High performance: Pythran/Transonic compilation, MPI parallelization\n- Complete workflow: Parameter configuration, simulation execution, output analysis\n- Interactive analysis: Python-based post-processing and visualization\n\n## Core Capabilities\n\n### 1. Installation and Setup\n\nInstall fluidsim using uv with appropriate feature flags:\n\n```bash\n# Basic installation\nuv uv pip install fluidsim\n\n# With FFT support (required for most solvers)\nuv uv pip install \"fluidsim[fft]\"\n\n# With MPI for parallel computing\nuv uv pip install \"fluidsim[fft,mpi]\"\n```\n\nSet environment variables for output directories (optional):\n\n```bash\nexport FLUIDSIM_PATH=/path/to/simulation/outputs\nexport FLUIDDYN_PATH_SCRATCH=/path/to/working/directory\n```\n\nNo API keys or authentication required.\n\nSee `references/installation.md` for complete installation instructions and environment configuration.\n\n### 2. Running Simulations\n\nStandard workflow consists of five steps:\n\n**Step 1**: Import solver\n```python\nfrom fluidsim.solvers.ns2d.solver import Simul\n```\n\n**Step 2**: Create and configure parameters\n```python\nparams = Simul.create_default_params()\nparams.oper.nx = params.oper.ny = 256\nparams.oper.Lx = params.oper.Ly = 2 * 3.14159\nparams.nu_2 = 1e-3\nparams.time_stepping.t_end = 10.0\nparams.init_fields.type = \"noise\"\n```\n\n**Step 3**: Instantiate simulation\n```python\nsim = Simul(params)\n```\n\n**Step 4**: Execute\n```python\nsim.time_stepping.start()\n```\n\n**Step 5**: Analyze results\n```python\nsim.output.phys_fields.plot(\"vorticity\")\nsim.output.spatial_means.plot()\n```\n\nSee `references/simulation_workflow.md` for complete examples, restarting simulations, and cluster deployment.\n\n### 3. Available Solvers\n\nChoose solver based on physical problem:\n\n**2D Navier-Stokes** (`ns2d`): 2D turbulence, vortex dynamics\n```python\nfrom fluidsim.solvers.ns2d.solver import Simul\n```\n\n**3D Navier-Stokes** (`ns3d`): 3D turbulence, realistic flows\n```python\nfrom fluidsim.solvers.ns3d.solver import Simul\n```\n\n**Stratified flows** (`ns2d.strat`, `ns3d.strat`): Oceanic/atmospheric flows\n```python\nfrom fluidsim.solvers.ns2d.strat.solver import Simul\nparams.N = 1.0  # Brunt-Visl frequency\n```\n\n**Shallow water** (`sw1l`): Geophysical flows, rotating systems\n```python\nfrom fluidsim.solvers.sw1l.solver import Simul\nparams.f = 1.0  # Coriolis parameter\n```\n\nSee `references/solvers.md` for complete solver list and selection guidance.\n\n### 4. Parameter Configuration\n\nParameters are organized hierarchically and accessed via dot notation:\n\n**Domain and resolution**:\n```python\nparams.oper.nx = 256  # grid points\nparams.oper.Lx = 2 * pi  # domain size\n```\n\n**Physical parameters**:\n```python\nparams.nu_2 = 1e-3  # viscosity\nparams.nu_4 = 0     # hyperviscosity (optional)\n```\n\n**Time stepping**:\n```python\nparams.time_stepping.t_end = 10.0\nparams.time_stepping.USE_CFL = True  # adaptive time step\nparams.time_stepping.CFL = 0.5\n```\n\n**Initial conditions**:\n```python\nparams.init_fields.type = \"noise\"  # or \"dipole\", \"vortex\", \"from_file\", \"in_script\"\n```\n\n**Output settings**:\n```python\nparams.output.periods_save.phys_fields = 1.0  # save every 1.0 time units\nparams.output.periods_save.spectra = 0.5\nparams.output.periods_save.spatial_means = 0.1\n```\n\nThe Parameters object raises `AttributeError` for typos, preventing silent configuration errors.\n\nSee `references/parameters.md` for comprehensive parameter documentation.\n\n### 5. Output and Analysis\n\nFluidSim produces multiple output types automatically saved during simulation:\n\n**Physical fields**: Velocity, vorticity in HDF5 format\n```python\nsim.output.phys_fields.plot(\"vorticity\")\nsim.output.phys_fields.plot(\"vx\")\n```\n\n**Spatial means**: Time series of volume-averaged quantities\n```python\nsim.output.spatial_means.plot()\n```\n\n**Spectra**: Energy and enstrophy spectra\n```python\nsim.output.spectra.plot1d()\nsim.output.spectra.plot2d()\n```\n\n**Load previous simulations**:\n```python\nfrom fluidsim import load_sim_for_plot\nsim = load_sim_for_plot(\"simulation_dir\")\nsim.output.phys_fields.plot()\n```\n\n**Advanced visualization**: Open `.h5` files in ParaView or VisIt for 3D visualization.\n\nSee `references/output_analysis.md` for detailed analysis workflows, parametric study analysis, and data export.\n\n### 6. Advanced Features\n\n**Custom forcing**: Maintain turbulence or drive specific dynamics\n```python\nparams.forcing.enable = True\nparams.forcing.type = \"tcrandom\"  # time-correlated random forcing\nparams.forcing.forcing_rate = 1.0\n```\n\n**Custom initial conditions**: Define fields in script\n```python\nparams.init_fields.type = \"in_script\"\nsim = Simul(params)\nX, Y = sim.oper.get_XY_loc()\nvx = sim.state.state_phys.get_var(\"vx\")\nvx[:] = sin(X) * cos(Y)\nsim.time_stepping.start()\n```\n\n**MPI parallelization**: Run on multiple processors\n```bash\nmpirun -np 8 python simulation_script.py\n```\n\n**Parametric studies**: Run multiple simulations with different parameters\n```python\nfor nu in [1e-3, 5e-4, 1e-4]:\n    params = Simul.create_default_params()\n    params.nu_2 = nu\n    params.output.sub_directory = f\"nu{nu}\"\n    sim = Simul(params)\n    sim.time_stepping.start()\n```\n\nSee `references/advanced_features.md` for forcing types, custom solvers, cluster submission, and performance optimization.\n\n## Common Use Cases\n\n### 2D Turbulence Study\n\n```python\nfrom fluidsim.solvers.ns2d.solver import Simul\nfrom math import pi\n\nparams = Simul.create_default_params()\nparams.oper.nx = params.oper.ny = 512\nparams.oper.Lx = params.oper.Ly = 2 * pi\nparams.nu_2 = 1e-4\nparams.time_stepping.t_end = 50.0\nparams.time_stepping.USE_CFL = True\nparams.init_fields.type = \"noise\"\nparams.output.periods_save.phys_fields = 5.0\nparams.output.periods_save.spectra = 1.0\n\nsim = Simul(params)\nsim.time_stepping.start()\n\n# Analyze energy cascade\nsim.output.spectra.plot1d(tmin=30.0, tmax=50.0)\n```\n\n### Stratified Flow Simulation\n\n```python\nfrom fluidsim.solvers.ns2d.strat.solver import Simul\n\nparams = Simul.create_default_params()\nparams.oper.nx = params.oper.ny = 256\nparams.N = 2.0  # stratification strength\nparams.nu_2 = 5e-4\nparams.time_stepping.t_end = 20.0\n\n# Initialize with dense layer\nparams.init_fields.type = \"in_script\"\nsim = Simul(params)\nX, Y = sim.oper.get_XY_loc()\nb = sim.state.state_phys.get_var(\"b\")\nb[:] = exp(-((X - 3.14)**2 + (Y - 3.14)**2) / 0.5)\nsim.state.statephys_from_statespect()\n\nsim.time_stepping.start()\nsim.output.phys_fields.plot(\"b\")\n```\n\n### High-Resolution 3D Simulation with MPI\n\n```python\nfrom fluidsim.solvers.ns3d.solver import Simul\n\nparams = Simul.create_default_params()\nparams.oper.nx = params.oper.ny = params.oper.nz = 512\nparams.nu_2 = 1e-5\nparams.time_stepping.t_end = 10.0\nparams.init_fields.type = \"noise\"\n\nsim = Simul(params)\nsim.time_stepping.start()\n```\n\nRun with:\n```bash\nmpirun -np 64 python script.py\n```\n\n### Taylor-Green Vortex Validation\n\n```python\nfrom fluidsim.solvers.ns2d.solver import Simul\nimport numpy as np\nfrom math import pi\n\nparams = Simul.create_default_params()\nparams.oper.nx = params.oper.ny = 128\nparams.oper.Lx = params.oper.Ly = 2 * pi\nparams.nu_2 = 1e-3\nparams.time_stepping.t_end = 10.0\nparams.init_fields.type = \"in_script\"\n\nsim = Simul(params)\nX, Y = sim.oper.get_XY_loc()\nvx = sim.state.state_phys.get_var(\"vx\")\nvy = sim.state.state_phys.get_var(\"vy\")\nvx[:] = np.sin(X) * np.cos(Y)\nvy[:] = -np.cos(X) * np.sin(Y)\nsim.state.statephys_from_statespect()\n\nsim.time_stepping.start()\n\n# Validate energy decay\ndf = sim.output.spatial_means.load()\n# Compare with analytical solution\n```\n\n## Quick Reference\n\n**Import solver**: `from fluidsim.solvers.ns2d.solver import Simul`\n\n**Create parameters**: `params = Simul.create_default_params()`\n\n**Set resolution**: `params.oper.nx = params.oper.ny = 256`\n\n**Set viscosity**: `params.nu_2 = 1e-3`\n\n**Set end time**: `params.time_stepping.t_end = 10.0`\n\n**Run simulation**: `sim = Simul(params); sim.time_stepping.start()`\n\n**Plot results**: `sim.output.phys_fields.plot(\"vorticity\")`\n\n**Load simulation**: `sim = load_sim_for_plot(\"path/to/sim\")`\n\n## Resources\n\n**Documentation**: https://fluidsim.readthedocs.io/\n\n**Reference files**:\n- `references/installation.md`: Complete installation instructions\n- `references/solvers.md`: Available solvers and selection guide\n- `references/simulation_workflow.md`: Detailed workflow examples\n- `references/parameters.md`: Comprehensive parameter documentation\n- `references/output_analysis.md`: Output types and analysis methods\n- `references/advanced_features.md`: Forcing, MPI, parametric studies, custom solvers\n",
        "data/k-dense-ai/gene-database/SKILL.md": "---\nname: gene-database\ndescription: \"Query NCBI Gene via E-utilities/Datasets API. Search by symbol/ID, retrieve gene info (RefSeqs, GO, locations, phenotypes), batch lookups, for gene annotation and functional analysis.\"\n---\n\n# Gene Database\n\n## Overview\n\nNCBI Gene is a comprehensive database integrating gene information from diverse species. It provides nomenclature, reference sequences (RefSeqs), chromosomal maps, biological pathways, genetic variations, phenotypes, and cross-references to global genomic resources.\n\n## When to Use This Skill\n\nThis skill should be used when working with gene data including searching by gene symbol or ID, retrieving gene sequences and metadata, analyzing gene functions and pathways, or performing batch gene lookups.\n\n## Quick Start\n\nNCBI provides two main APIs for gene data access:\n\n1. **E-utilities** (Traditional): Full-featured API for all Entrez databases with flexible querying\n2. **NCBI Datasets API** (Newer): Optimized for gene data retrieval with simplified workflows\n\nChoose E-utilities for complex queries and cross-database searches. Choose Datasets API for straightforward gene data retrieval with metadata and sequences in a single request.\n\n## Common Workflows\n\n### Search Genes by Symbol or Name\n\nTo search for genes by symbol or name across organisms:\n\n1. Use the `scripts/query_gene.py` script with E-utilities ESearch\n2. Specify the gene symbol and organism (e.g., \"BRCA1 in human\")\n3. The script returns matching Gene IDs\n\nExample query patterns:\n- Gene symbol: `insulin[gene name] AND human[organism]`\n- Gene with disease: `dystrophin[gene name] AND muscular dystrophy[disease]`\n- Chromosome location: `human[organism] AND 17q21[chromosome]`\n\n### Retrieve Gene Information by ID\n\nTo fetch detailed information for known Gene IDs:\n\n1. Use `scripts/fetch_gene_data.py` with the Datasets API for comprehensive data\n2. Alternatively, use `scripts/query_gene.py` with E-utilities EFetch for specific formats\n3. Specify desired output format (JSON, XML, or text)\n\nThe Datasets API returns:\n- Gene nomenclature and aliases\n- Reference sequences (RefSeqs) for transcripts and proteins\n- Chromosomal location and mapping\n- Gene Ontology (GO) annotations\n- Associated publications\n\n### Batch Gene Lookups\n\nFor multiple genes simultaneously:\n\n1. Use `scripts/batch_gene_lookup.py` for efficient batch processing\n2. Provide a list of gene symbols or IDs\n3. Specify the organism for symbol-based queries\n4. The script handles rate limiting automatically (10 requests/second with API key)\n\nThis workflow is useful for:\n- Validating gene lists\n- Retrieving metadata for gene panels\n- Cross-referencing gene identifiers\n- Building gene annotation tables\n\n### Search by Biological Context\n\nTo find genes associated with specific biological functions or phenotypes:\n\n1. Use E-utilities with Gene Ontology (GO) terms or phenotype keywords\n2. Query by pathway names or disease associations\n3. Filter by organism, chromosome, or other attributes\n\nExample searches:\n- By GO term: `GO:0006915[biological process]` (apoptosis)\n- By phenotype: `diabetes[phenotype] AND mouse[organism]`\n- By pathway: `insulin signaling pathway[pathway]`\n\n### API Access Patterns\n\n**Rate Limits:**\n- Without API key: 3 requests/second for E-utilities, 5 requests/second for Datasets API\n- With API key: 10 requests/second for both APIs\n\n**Authentication:**\nRegister for a free NCBI API key at https://www.ncbi.nlm.nih.gov/account/ to increase rate limits.\n\n**Error Handling:**\nBoth APIs return standard HTTP status codes. Common errors include:\n- 400: Malformed query or invalid parameters\n- 429: Rate limit exceeded\n- 404: Gene ID not found\n\nRetry failed requests with exponential backoff.\n\n## Script Usage\n\n### query_gene.py\n\nQuery NCBI Gene using E-utilities (ESearch, ESummary, EFetch).\n\n```bash\npython scripts/query_gene.py --search \"BRCA1\" --organism \"human\"\npython scripts/query_gene.py --id 672 --format json\npython scripts/query_gene.py --search \"insulin[gene] AND diabetes[disease]\"\n```\n\n### fetch_gene_data.py\n\nFetch comprehensive gene data using NCBI Datasets API.\n\n```bash\npython scripts/fetch_gene_data.py --gene-id 672\npython scripts/fetch_gene_data.py --symbol BRCA1 --taxon human\npython scripts/fetch_gene_data.py --symbol TP53 --taxon \"Homo sapiens\" --output json\n```\n\n### batch_gene_lookup.py\n\nProcess multiple gene queries efficiently.\n\n```bash\npython scripts/batch_gene_lookup.py --file gene_list.txt --organism human\npython scripts/batch_gene_lookup.py --ids 672,7157,5594 --output results.json\n```\n\n## API References\n\nFor detailed API documentation including endpoints, parameters, response formats, and examples, refer to:\n\n- `references/api_reference.md` - Comprehensive API documentation for E-utilities and Datasets API\n- `references/common_workflows.md` - Additional examples and use case patterns\n\nSearch these references when needing specific API endpoint details, parameter options, or response structure information.\n\n## Data Formats\n\nNCBI Gene data can be retrieved in multiple formats:\n\n- **JSON**: Structured data ideal for programmatic processing\n- **XML**: Detailed hierarchical format with full metadata\n- **GenBank**: Sequence data with annotations\n- **FASTA**: Sequence data only\n- **Text**: Human-readable summaries\n\nChoose JSON for modern applications, XML for legacy systems requiring detailed metadata, and FASTA for sequence analysis workflows.\n\n## Best Practices\n\n1. **Always specify organism** when searching by gene symbol to avoid ambiguity\n2. **Use Gene IDs** for precise lookups when available\n3. **Batch requests** when working with multiple genes to minimize API calls\n4. **Cache results** locally to reduce redundant queries\n5. **Include API key** in scripts for higher rate limits\n6. **Handle errors gracefully** with retry logic for transient failures\n7. **Validate gene symbols** before batch processing to catch typos\n\n## Resources\n\nThis skill includes:\n\n### scripts/\n- `query_gene.py` - Query genes using E-utilities (ESearch, ESummary, EFetch)\n- `fetch_gene_data.py` - Fetch gene data using NCBI Datasets API\n- `batch_gene_lookup.py` - Handle multiple gene queries efficiently\n\n### references/\n- `api_reference.md` - Detailed API documentation for both E-utilities and Datasets API\n- `common_workflows.md` - Examples of common gene queries and use cases\n",
        "data/k-dense-ai/geniml/SKILL.md": "---\nname: geniml\ndescription: This skill should be used when working with genomic interval data (BED files) for machine learning tasks. Use for training region embeddings (Region2Vec, BEDspace), single-cell ATAC-seq analysis (scEmbed), building consensus peaks (universes), or any ML-based analysis of genomic regions. Applies to BED file collections, scATAC-seq data, chromatin accessibility datasets, and region-based genomic feature learning.\n---\n\n# Geniml: Genomic Interval Machine Learning\n\n## Overview\n\nGeniml is a Python package for building machine learning models on genomic interval data from BED files. It provides unsupervised methods for learning embeddings of genomic regions, single cells, and metadata labels, enabling similarity searches, clustering, and downstream ML tasks.\n\n## Installation\n\nInstall geniml using uv:\n\n```bash\nuv uv pip install geniml\n```\n\nFor ML dependencies (PyTorch, etc.):\n\n```bash\nuv uv pip install 'geniml[ml]'\n```\n\nDevelopment version from GitHub:\n\n```bash\nuv uv pip install git+https://github.com/databio/geniml.git\n```\n\n## Core Capabilities\n\nGeniml provides five primary capabilities, each detailed in dedicated reference files:\n\n### 1. Region2Vec: Genomic Region Embeddings\n\nTrain unsupervised embeddings of genomic regions using word2vec-style learning.\n\n**Use for:** Dimensionality reduction of BED files, region similarity analysis, feature vectors for downstream ML.\n\n**Workflow:**\n1. Tokenize BED files using a universe reference\n2. Train Region2Vec model on tokens\n3. Generate embeddings for regions\n\n**Reference:** See `references/region2vec.md` for detailed workflow, parameters, and examples.\n\n### 2. BEDspace: Joint Region and Metadata Embeddings\n\nTrain shared embeddings for region sets and metadata labels using StarSpace.\n\n**Use for:** Metadata-aware searches, cross-modal queries (regionlabel or labelregion), joint analysis of genomic content and experimental conditions.\n\n**Workflow:**\n1. Preprocess regions and metadata\n2. Train BEDspace model\n3. Compute distances\n4. Query across regions and labels\n\n**Reference:** See `references/bedspace.md` for detailed workflow, search types, and examples.\n\n### 3. scEmbed: Single-Cell Chromatin Accessibility Embeddings\n\nTrain Region2Vec models on single-cell ATAC-seq data for cell-level embeddings.\n\n**Use for:** scATAC-seq clustering, cell-type annotation, dimensionality reduction of single cells, integration with scanpy workflows.\n\n**Workflow:**\n1. Prepare AnnData with peak coordinates\n2. Pre-tokenize cells\n3. Train scEmbed model\n4. Generate cell embeddings\n5. Cluster and visualize with scanpy\n\n**Reference:** See `references/scembed.md` for detailed workflow, parameters, and examples.\n\n### 4. Consensus Peaks: Universe Building\n\nBuild reference peak sets (universes) from BED file collections using multiple statistical methods.\n\n**Use for:** Creating tokenization references, standardizing regions across datasets, defining consensus features with statistical rigor.\n\n**Workflow:**\n1. Combine BED files\n2. Generate coverage tracks\n3. Build universe using CC, CCF, ML, or HMM method\n\n**Methods:**\n- **CC (Coverage Cutoff)**: Simple threshold-based\n- **CCF (Coverage Cutoff Flexible)**: Confidence intervals for boundaries\n- **ML (Maximum Likelihood)**: Probabilistic modeling of positions\n- **HMM (Hidden Markov Model)**: Complex state modeling\n\n**Reference:** See `references/consensus_peaks.md` for method comparison, parameters, and examples.\n\n### 5. Utilities: Supporting Tools\n\nAdditional tools for caching, randomization, evaluation, and search.\n\n**Available utilities:**\n- **BBClient**: BED file caching for repeated access\n- **BEDshift**: Randomization preserving genomic context\n- **Evaluation**: Metrics for embedding quality (silhouette, Davies-Bouldin, etc.)\n- **Tokenization**: Region tokenization utilities (hard, soft, universe-based)\n- **Text2BedNN**: Neural search backends for genomic queries\n\n**Reference:** See `references/utilities.md` for detailed usage of each utility.\n\n## Common Workflows\n\n### Basic Region Embedding Pipeline\n\n```python\nfrom geniml.tokenization import hard_tokenization\nfrom geniml.region2vec import region2vec\nfrom geniml.evaluation import evaluate_embeddings\n\n# Step 1: Tokenize BED files\nhard_tokenization(\n    src_folder='bed_files/',\n    dst_folder='tokens/',\n    universe_file='universe.bed',\n    p_value_threshold=1e-9\n)\n\n# Step 2: Train Region2Vec\nregion2vec(\n    token_folder='tokens/',\n    save_dir='model/',\n    num_shufflings=1000,\n    embedding_dim=100\n)\n\n# Step 3: Evaluate\nmetrics = evaluate_embeddings(\n    embeddings_file='model/embeddings.npy',\n    labels_file='metadata.csv'\n)\n```\n\n### scATAC-seq Analysis Pipeline\n\n```python\nimport scanpy as sc\nfrom geniml.scembed import ScEmbed\nfrom geniml.io import tokenize_cells\n\n# Step 1: Load data\nadata = sc.read_h5ad('scatac_data.h5ad')\n\n# Step 2: Tokenize cells\ntokenize_cells(\n    adata='scatac_data.h5ad',\n    universe_file='universe.bed',\n    output='tokens.parquet'\n)\n\n# Step 3: Train scEmbed\nmodel = ScEmbed(embedding_dim=100)\nmodel.train(dataset='tokens.parquet', epochs=100)\n\n# Step 4: Generate embeddings\nembeddings = model.encode(adata)\nadata.obsm['scembed_X'] = embeddings\n\n# Step 5: Cluster with scanpy\nsc.pp.neighbors(adata, use_rep='scembed_X')\nsc.tl.leiden(adata)\nsc.tl.umap(adata)\n```\n\n### Universe Building and Evaluation\n\n```bash\n# Generate coverage\ncat bed_files/*.bed > combined.bed\nuniwig -m 25 combined.bed chrom.sizes coverage/\n\n# Build universe with coverage cutoff\ngeniml universe build cc \\\n  --coverage-folder coverage/ \\\n  --output-file universe.bed \\\n  --cutoff 5 \\\n  --merge 100 \\\n  --filter-size 50\n\n# Evaluate universe quality\ngeniml universe evaluate \\\n  --universe universe.bed \\\n  --coverage-folder coverage/ \\\n  --bed-folder bed_files/\n```\n\n## CLI Reference\n\nGeniml provides command-line interfaces for major operations:\n\n```bash\n# Region2Vec training\ngeniml region2vec --token-folder tokens/ --save-dir model/ --num-shuffle 1000\n\n# BEDspace preprocessing\ngeniml bedspace preprocess --input regions/ --metadata labels.csv --universe universe.bed\n\n# BEDspace training\ngeniml bedspace train --input preprocessed.txt --output model/ --dim 100\n\n# BEDspace search\ngeniml bedspace search -t r2l -d distances.pkl -q query.bed -n 10\n\n# Universe building\ngeniml universe build cc --coverage-folder coverage/ --output universe.bed --cutoff 5\n\n# BEDshift randomization\ngeniml bedshift --input peaks.bed --genome hg38 --preserve-chrom --iterations 100\n```\n\n## When to Use Which Tool\n\n**Use Region2Vec when:**\n- Working with bulk genomic data (ChIP-seq, ATAC-seq, etc.)\n- Need unsupervised embeddings without metadata\n- Comparing region sets across experiments\n- Building features for downstream supervised learning\n\n**Use BEDspace when:**\n- Metadata labels available (cell types, tissues, conditions)\n- Need to query regions by metadata or vice versa\n- Want joint embedding space for regions and labels\n- Building searchable genomic databases\n\n**Use scEmbed when:**\n- Analyzing single-cell ATAC-seq data\n- Clustering cells by chromatin accessibility\n- Annotating cell types from scATAC-seq\n- Integration with scanpy is desired\n\n**Use Universe Building when:**\n- Need reference peak sets for tokenization\n- Combining multiple experiments into consensus\n- Want statistically rigorous region definitions\n- Building standard references for a project\n\n**Use Utilities when:**\n- Need to cache remote BED files (BBClient)\n- Generating null models for statistics (BEDshift)\n- Evaluating embedding quality (Evaluation)\n- Building search interfaces (Text2BedNN)\n\n## Best Practices\n\n### General Guidelines\n\n- **Universe quality is critical**: Invest time in building comprehensive, well-constructed universes\n- **Tokenization validation**: Check coverage (>80% ideal) before training\n- **Parameter tuning**: Experiment with embedding dimensions, learning rates, and training epochs\n- **Evaluation**: Always validate embeddings with multiple metrics and visualizations\n- **Documentation**: Record parameters and random seeds for reproducibility\n\n### Performance Considerations\n\n- **Pre-tokenization**: For scEmbed, always pre-tokenize cells for faster training\n- **Memory management**: Large datasets may require batch processing or downsampling\n- **Computational resources**: ML/HMM universe methods are computationally intensive\n- **Model caching**: Use BBClient to avoid repeated downloads\n\n### Integration Patterns\n\n- **With scanpy**: scEmbed embeddings integrate seamlessly as `adata.obsm` entries\n- **With BEDbase**: Use BBClient for accessing remote BED repositories\n- **With Hugging Face**: Export trained models for sharing and reproducibility\n- **With R**: Use reticulate for R integration (see utilities reference)\n\n## Related Projects\n\nGeniml is part of the BEDbase ecosystem:\n\n- **BEDbase**: Unified platform for genomic regions\n- **BEDboss**: Processing pipeline for BED files\n- **Gtars**: Genomic tools and utilities\n- **BBClient**: Client for BEDbase repositories\n\n## Additional Resources\n\n- **Documentation**: https://docs.bedbase.org/geniml/\n- **GitHub**: https://github.com/databio/geniml\n- **Pre-trained models**: Available on Hugging Face (databio organization)\n- **Publications**: Cited in documentation for methodological details\n\n## Troubleshooting\n\n**\"Tokenization coverage too low\":**\n- Check universe quality and completeness\n- Adjust p-value threshold (try 1e-6 instead of 1e-9)\n- Ensure universe matches genome assembly\n\n**\"Training not converging\":**\n- Adjust learning rate (try 0.01-0.05 range)\n- Increase training epochs\n- Check data quality and preprocessing\n\n**\"Out of memory errors\":**\n- Reduce batch size for scEmbed\n- Process data in chunks\n- Use pre-tokenization for single-cell data\n\n**\"StarSpace not found\" (BEDspace):**\n- Install StarSpace separately: https://github.com/facebookresearch/StarSpace\n- Set `--path-to-starspace` parameter correctly\n\nFor detailed troubleshooting and method-specific issues, consult the appropriate reference file.\n",
        "data/k-dense-ai/geo-database/SKILL.md": "---\nname: geo-database\ndescription: \"Access NCBI GEO for gene expression/genomics data. Search/download microarray and RNA-seq datasets (GSE, GSM, GPL), retrieve SOFT/Matrix files, for transcriptomics and expression analysis.\"\n---\n\n# GEO Database\n\n## Overview\n\nThe Gene Expression Omnibus (GEO) is NCBI's public repository for high-throughput gene expression and functional genomics data. GEO contains over 264,000 studies with more than 8 million samples from both array-based and sequence-based experiments.\n\n## When to Use This Skill\n\nThis skill should be used when searching for gene expression datasets, retrieving experimental data, downloading raw and processed files, querying expression profiles, or integrating GEO data into computational analysis workflows.\n\n## Core Capabilities\n\n### 1. Understanding GEO Data Organization\n\nGEO organizes data hierarchically using different accession types:\n\n**Series (GSE):** A complete experiment with a set of related samples\n- Example: GSE123456\n- Contains experimental design, samples, and overall study information\n- Largest organizational unit in GEO\n- Current count: 264,928+ series\n\n**Sample (GSM):** A single experimental sample or biological replicate\n- Example: GSM987654\n- Contains individual sample data, protocols, and metadata\n- Linked to platforms and series\n- Current count: 8,068,632+ samples\n\n**Platform (GPL):** The microarray or sequencing platform used\n- Example: GPL570 (Affymetrix Human Genome U133 Plus 2.0 Array)\n- Describes the technology and probe/feature annotations\n- Shared across multiple experiments\n- Current count: 27,739+ platforms\n\n**DataSet (GDS):** Curated collections with consistent formatting\n- Example: GDS5678\n- Experimentally-comparable samples organized by study design\n- Processed for differential analysis\n- Subset of GEO data (4,348 curated datasets)\n- Ideal for quick comparative analyses\n\n**Profiles:** Gene-specific expression data linked to sequence features\n- Queryable by gene name or annotation\n- Cross-references to Entrez Gene\n- Enables gene-centric searches across all studies\n\n### 2. Searching GEO Data\n\n**GEO DataSets Search:**\n\nSearch for studies by keywords, organism, or experimental conditions:\n\n```python\nfrom Bio import Entrez\n\n# Configure Entrez (required)\nEntrez.email = \"your.email@example.com\"\n\n# Search for datasets\ndef search_geo_datasets(query, retmax=20):\n    \"\"\"Search GEO DataSets database\"\"\"\n    handle = Entrez.esearch(\n        db=\"gds\",\n        term=query,\n        retmax=retmax,\n        usehistory=\"y\"\n    )\n    results = Entrez.read(handle)\n    handle.close()\n    return results\n\n# Example searches\nresults = search_geo_datasets(\"breast cancer[MeSH] AND Homo sapiens[Organism]\")\nprint(f\"Found {results['Count']} datasets\")\n\n# Search by specific platform\nresults = search_geo_datasets(\"GPL570[Accession]\")\n\n# Search by study type\nresults = search_geo_datasets(\"expression profiling by array[DataSet Type]\")\n```\n\n**GEO Profiles Search:**\n\nFind gene-specific expression patterns:\n\n```python\n# Search for gene expression profiles\ndef search_geo_profiles(gene_name, organism=\"Homo sapiens\", retmax=100):\n    \"\"\"Search GEO Profiles for a specific gene\"\"\"\n    query = f\"{gene_name}[Gene Name] AND {organism}[Organism]\"\n    handle = Entrez.esearch(\n        db=\"geoprofiles\",\n        term=query,\n        retmax=retmax\n    )\n    results = Entrez.read(handle)\n    handle.close()\n    return results\n\n# Find TP53 expression across studies\ntp53_results = search_geo_profiles(\"TP53\", organism=\"Homo sapiens\")\nprint(f\"Found {tp53_results['Count']} expression profiles for TP53\")\n```\n\n**Advanced Search Patterns:**\n\n```python\n# Combine multiple search terms\ndef advanced_geo_search(terms, operator=\"AND\"):\n    \"\"\"Build complex search queries\"\"\"\n    query = f\" {operator} \".join(terms)\n    return search_geo_datasets(query)\n\n# Find recent high-throughput studies\nsearch_terms = [\n    \"RNA-seq[DataSet Type]\",\n    \"Homo sapiens[Organism]\",\n    \"2024[Publication Date]\"\n]\nresults = advanced_geo_search(search_terms)\n\n# Search by author and condition\nsearch_terms = [\n    \"Smith[Author]\",\n    \"diabetes[Disease]\"\n]\nresults = advanced_geo_search(search_terms)\n```\n\n### 3. Retrieving GEO Data with GEOparse (Recommended)\n\n**GEOparse** is the primary Python library for accessing GEO data:\n\n**Installation:**\n```bash\nuv pip install GEOparse\n```\n\n**Basic Usage:**\n\n```python\nimport GEOparse\n\n# Download and parse a GEO Series\ngse = GEOparse.get_GEO(geo=\"GSE123456\", destdir=\"./data\")\n\n# Access series metadata\nprint(gse.metadata['title'])\nprint(gse.metadata['summary'])\nprint(gse.metadata['overall_design'])\n\n# Access sample information\nfor gsm_name, gsm in gse.gsms.items():\n    print(f\"Sample: {gsm_name}\")\n    print(f\"  Title: {gsm.metadata['title'][0]}\")\n    print(f\"  Source: {gsm.metadata['source_name_ch1'][0]}\")\n    print(f\"  Characteristics: {gsm.metadata.get('characteristics_ch1', [])}\")\n\n# Access platform information\nfor gpl_name, gpl in gse.gpls.items():\n    print(f\"Platform: {gpl_name}\")\n    print(f\"  Title: {gpl.metadata['title'][0]}\")\n    print(f\"  Organism: {gpl.metadata['organism'][0]}\")\n```\n\n**Working with Expression Data:**\n\n```python\nimport GEOparse\nimport pandas as pd\n\n# Get expression data from series\ngse = GEOparse.get_GEO(geo=\"GSE123456\", destdir=\"./data\")\n\n# Extract expression matrix\n# Method 1: From series matrix file (fastest)\nif hasattr(gse, 'pivot_samples'):\n    expression_df = gse.pivot_samples('VALUE')\n    print(expression_df.shape)  # genes x samples\n\n# Method 2: From individual samples\nexpression_data = {}\nfor gsm_name, gsm in gse.gsms.items():\n    if hasattr(gsm, 'table'):\n        expression_data[gsm_name] = gsm.table['VALUE']\n\nexpression_df = pd.DataFrame(expression_data)\nprint(f\"Expression matrix: {expression_df.shape}\")\n```\n\n**Accessing Supplementary Files:**\n\n```python\nimport GEOparse\n\ngse = GEOparse.get_GEO(geo=\"GSE123456\", destdir=\"./data\")\n\n# Download supplementary files\ngse.download_supplementary_files(\n    directory=\"./data/GSE123456_suppl\",\n    download_sra=False  # Set to True to download SRA files\n)\n\n# List available supplementary files\nfor gsm_name, gsm in gse.gsms.items():\n    if hasattr(gsm, 'supplementary_files'):\n        print(f\"Sample {gsm_name}:\")\n        for file_url in gsm.metadata.get('supplementary_file', []):\n            print(f\"  {file_url}\")\n```\n\n**Filtering and Subsetting Data:**\n\n```python\nimport GEOparse\n\ngse = GEOparse.get_GEO(geo=\"GSE123456\", destdir=\"./data\")\n\n# Filter samples by metadata\ncontrol_samples = [\n    gsm_name for gsm_name, gsm in gse.gsms.items()\n    if 'control' in gsm.metadata.get('title', [''])[0].lower()\n]\n\ntreatment_samples = [\n    gsm_name for gsm_name, gsm in gse.gsms.items()\n    if 'treatment' in gsm.metadata.get('title', [''])[0].lower()\n]\n\nprint(f\"Control samples: {len(control_samples)}\")\nprint(f\"Treatment samples: {len(treatment_samples)}\")\n\n# Extract subset expression matrix\nexpression_df = gse.pivot_samples('VALUE')\ncontrol_expr = expression_df[control_samples]\ntreatment_expr = expression_df[treatment_samples]\n```\n\n### 4. Using NCBI E-utilities for GEO Access\n\n**E-utilities** provide lower-level programmatic access to GEO metadata:\n\n**Basic E-utilities Workflow:**\n\n```python\nfrom Bio import Entrez\nimport time\n\nEntrez.email = \"your.email@example.com\"\n\n# Step 1: Search for GEO entries\ndef search_geo(query, db=\"gds\", retmax=100):\n    \"\"\"Search GEO using E-utilities\"\"\"\n    handle = Entrez.esearch(\n        db=db,\n        term=query,\n        retmax=retmax,\n        usehistory=\"y\"\n    )\n    results = Entrez.read(handle)\n    handle.close()\n    return results\n\n# Step 2: Fetch summaries\ndef fetch_geo_summaries(id_list, db=\"gds\"):\n    \"\"\"Fetch document summaries for GEO entries\"\"\"\n    ids = \",\".join(id_list)\n    handle = Entrez.esummary(db=db, id=ids)\n    summaries = Entrez.read(handle)\n    handle.close()\n    return summaries\n\n# Step 3: Fetch full records\ndef fetch_geo_records(id_list, db=\"gds\"):\n    \"\"\"Fetch full GEO records\"\"\"\n    ids = \",\".join(id_list)\n    handle = Entrez.efetch(db=db, id=ids, retmode=\"xml\")\n    records = Entrez.read(handle)\n    handle.close()\n    return records\n\n# Example workflow\nsearch_results = search_geo(\"breast cancer AND Homo sapiens\")\nid_list = search_results['IdList'][:5]\n\nsummaries = fetch_geo_summaries(id_list)\nfor summary in summaries:\n    print(f\"GDS: {summary.get('Accession', 'N/A')}\")\n    print(f\"Title: {summary.get('title', 'N/A')}\")\n    print(f\"Samples: {summary.get('n_samples', 'N/A')}\")\n    print()\n```\n\n**Batch Processing with E-utilities:**\n\n```python\nfrom Bio import Entrez\nimport time\n\nEntrez.email = \"your.email@example.com\"\n\ndef batch_fetch_geo_metadata(accessions, batch_size=100):\n    \"\"\"Fetch metadata for multiple GEO accessions\"\"\"\n    results = {}\n\n    for i in range(0, len(accessions), batch_size):\n        batch = accessions[i:i + batch_size]\n\n        # Search for each accession\n        for accession in batch:\n            try:\n                query = f\"{accession}[Accession]\"\n                search_handle = Entrez.esearch(db=\"gds\", term=query)\n                search_results = Entrez.read(search_handle)\n                search_handle.close()\n\n                if search_results['IdList']:\n                    # Fetch summary\n                    summary_handle = Entrez.esummary(\n                        db=\"gds\",\n                        id=search_results['IdList'][0]\n                    )\n                    summary = Entrez.read(summary_handle)\n                    summary_handle.close()\n                    results[accession] = summary[0]\n\n                # Be polite to NCBI servers\n                time.sleep(0.34)  # Max 3 requests per second\n\n            except Exception as e:\n                print(f\"Error fetching {accession}: {e}\")\n\n    return results\n\n# Fetch metadata for multiple datasets\ngse_list = [\"GSE100001\", \"GSE100002\", \"GSE100003\"]\nmetadata = batch_fetch_geo_metadata(gse_list)\n```\n\n### 5. Direct FTP Access for Data Files\n\n**FTP URLs for GEO Data:**\n\nGEO data can be downloaded directly via FTP:\n\n```python\nimport ftplib\nimport os\n\ndef download_geo_ftp(accession, file_type=\"matrix\", dest_dir=\"./data\"):\n    \"\"\"Download GEO files via FTP\"\"\"\n    # Construct FTP path based on accession type\n    if accession.startswith(\"GSE\"):\n        # Series files\n        gse_num = accession[3:]\n        base_num = gse_num[:-3] + \"nnn\"\n        ftp_path = f\"/geo/series/GSE{base_num}/{accession}/\"\n\n        if file_type == \"matrix\":\n            filename = f\"{accession}_series_matrix.txt.gz\"\n        elif file_type == \"soft\":\n            filename = f\"{accession}_family.soft.gz\"\n        elif file_type == \"miniml\":\n            filename = f\"{accession}_family.xml.tgz\"\n\n    # Connect to FTP server\n    ftp = ftplib.FTP(\"ftp.ncbi.nlm.nih.gov\")\n    ftp.login()\n    ftp.cwd(ftp_path)\n\n    # Download file\n    os.makedirs(dest_dir, exist_ok=True)\n    local_file = os.path.join(dest_dir, filename)\n\n    with open(local_file, 'wb') as f:\n        ftp.retrbinary(f'RETR {filename}', f.write)\n\n    ftp.quit()\n    print(f\"Downloaded: {local_file}\")\n    return local_file\n\n# Download series matrix file\ndownload_geo_ftp(\"GSE123456\", file_type=\"matrix\")\n\n# Download SOFT format file\ndownload_geo_ftp(\"GSE123456\", file_type=\"soft\")\n```\n\n**Using wget or curl for Downloads:**\n\n```bash\n# Download series matrix file\nwget ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE123nnn/GSE123456/matrix/GSE123456_series_matrix.txt.gz\n\n# Download all supplementary files for a series\nwget -r -np -nd ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE123nnn/GSE123456/suppl/\n\n# Download SOFT format family file\nwget ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE123nnn/GSE123456/soft/GSE123456_family.soft.gz\n```\n\n### 6. Analyzing GEO Data\n\n**Quality Control and Preprocessing:**\n\n```python\nimport GEOparse\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Load dataset\ngse = GEOparse.get_GEO(geo=\"GSE123456\", destdir=\"./data\")\nexpression_df = gse.pivot_samples('VALUE')\n\n# Check for missing values\nprint(f\"Missing values: {expression_df.isnull().sum().sum()}\")\n\n# Log transformation (if needed)\nif expression_df.min().min() > 0:  # Check if already log-transformed\n    if expression_df.max().max() > 100:\n        expression_df = np.log2(expression_df + 1)\n        print(\"Applied log2 transformation\")\n\n# Distribution plots\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nexpression_df.plot.box(ax=plt.gca())\nplt.title(\"Expression Distribution per Sample\")\nplt.xticks(rotation=90)\n\nplt.subplot(1, 2, 2)\nexpression_df.mean(axis=1).hist(bins=50)\nplt.title(\"Gene Expression Distribution\")\nplt.xlabel(\"Average Expression\")\n\nplt.tight_layout()\nplt.savefig(\"geo_qc.png\", dpi=300, bbox_inches='tight')\n```\n\n**Differential Expression Analysis:**\n\n```python\nimport GEOparse\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\n\ngse = GEOparse.get_GEO(geo=\"GSE123456\", destdir=\"./data\")\nexpression_df = gse.pivot_samples('VALUE')\n\n# Define sample groups\ncontrol_samples = [\"GSM1\", \"GSM2\", \"GSM3\"]\ntreatment_samples = [\"GSM4\", \"GSM5\", \"GSM6\"]\n\n# Calculate fold changes and p-values\nresults = []\nfor gene in expression_df.index:\n    control_expr = expression_df.loc[gene, control_samples]\n    treatment_expr = expression_df.loc[gene, treatment_samples]\n\n    # Calculate statistics\n    fold_change = treatment_expr.mean() - control_expr.mean()\n    t_stat, p_value = stats.ttest_ind(treatment_expr, control_expr)\n\n    results.append({\n        'gene': gene,\n        'log2_fold_change': fold_change,\n        'p_value': p_value,\n        'control_mean': control_expr.mean(),\n        'treatment_mean': treatment_expr.mean()\n    })\n\n# Create results DataFrame\nde_results = pd.DataFrame(results)\n\n# Multiple testing correction (Benjamini-Hochberg)\nfrom statsmodels.stats.multitest import multipletests\n_, de_results['q_value'], _, _ = multipletests(\n    de_results['p_value'],\n    method='fdr_bh'\n)\n\n# Filter significant genes\nsignificant_genes = de_results[\n    (de_results['q_value'] < 0.05) &\n    (abs(de_results['log2_fold_change']) > 1)\n]\n\nprint(f\"Significant genes: {len(significant_genes)}\")\nsignificant_genes.to_csv(\"de_results.csv\", index=False)\n```\n\n**Correlation and Clustering Analysis:**\n\n```python\nimport GEOparse\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.cluster import hierarchy\nfrom scipy.spatial.distance import pdist\n\ngse = GEOparse.get_GEO(geo=\"GSE123456\", destdir=\"./data\")\nexpression_df = gse.pivot_samples('VALUE')\n\n# Sample correlation heatmap\nsample_corr = expression_df.corr()\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(sample_corr, cmap='coolwarm', center=0,\n            square=True, linewidths=0.5)\nplt.title(\"Sample Correlation Matrix\")\nplt.tight_layout()\nplt.savefig(\"sample_correlation.png\", dpi=300, bbox_inches='tight')\n\n# Hierarchical clustering\ndistances = pdist(expression_df.T, metric='correlation')\nlinkage = hierarchy.linkage(distances, method='average')\n\nplt.figure(figsize=(12, 6))\nhierarchy.dendrogram(linkage, labels=expression_df.columns)\nplt.title(\"Hierarchical Clustering of Samples\")\nplt.xlabel(\"Samples\")\nplt.ylabel(\"Distance\")\nplt.xticks(rotation=90)\nplt.tight_layout()\nplt.savefig(\"sample_clustering.png\", dpi=300, bbox_inches='tight')\n```\n\n### 7. Batch Processing Multiple Datasets\n\n**Download and Process Multiple Series:**\n\n```python\nimport GEOparse\nimport pandas as pd\nimport os\n\ndef batch_download_geo(gse_list, destdir=\"./geo_data\"):\n    \"\"\"Download multiple GEO series\"\"\"\n    results = {}\n\n    for gse_id in gse_list:\n        try:\n            print(f\"Processing {gse_id}...\")\n            gse = GEOparse.get_GEO(geo=gse_id, destdir=destdir)\n\n            # Extract key information\n            results[gse_id] = {\n                'title': gse.metadata.get('title', ['N/A'])[0],\n                'organism': gse.metadata.get('organism', ['N/A'])[0],\n                'platform': list(gse.gpls.keys())[0] if gse.gpls else 'N/A',\n                'num_samples': len(gse.gsms),\n                'submission_date': gse.metadata.get('submission_date', ['N/A'])[0]\n            }\n\n            # Save expression data\n            if hasattr(gse, 'pivot_samples'):\n                expr_df = gse.pivot_samples('VALUE')\n                expr_df.to_csv(f\"{destdir}/{gse_id}_expression.csv\")\n                results[gse_id]['num_genes'] = len(expr_df)\n\n        except Exception as e:\n            print(f\"Error processing {gse_id}: {e}\")\n            results[gse_id] = {'error': str(e)}\n\n    # Save summary\n    summary_df = pd.DataFrame(results).T\n    summary_df.to_csv(f\"{destdir}/batch_summary.csv\")\n\n    return results\n\n# Process multiple datasets\ngse_list = [\"GSE100001\", \"GSE100002\", \"GSE100003\"]\nresults = batch_download_geo(gse_list)\n```\n\n**Meta-Analysis Across Studies:**\n\n```python\nimport GEOparse\nimport pandas as pd\nimport numpy as np\n\ndef meta_analysis_geo(gse_list, gene_of_interest):\n    \"\"\"Perform meta-analysis of gene expression across studies\"\"\"\n    results = []\n\n    for gse_id in gse_list:\n        try:\n            gse = GEOparse.get_GEO(geo=gse_id, destdir=\"./data\")\n\n            # Get platform annotation\n            gpl = list(gse.gpls.values())[0]\n\n            # Find gene in platform\n            if hasattr(gpl, 'table'):\n                gene_probes = gpl.table[\n                    gpl.table['Gene Symbol'].str.contains(\n                        gene_of_interest,\n                        case=False,\n                        na=False\n                    )\n                ]\n\n                if not gene_probes.empty:\n                    expr_df = gse.pivot_samples('VALUE')\n\n                    for probe_id in gene_probes['ID']:\n                        if probe_id in expr_df.index:\n                            expr_values = expr_df.loc[probe_id]\n\n                            results.append({\n                                'study': gse_id,\n                                'probe': probe_id,\n                                'mean_expression': expr_values.mean(),\n                                'std_expression': expr_values.std(),\n                                'num_samples': len(expr_values)\n                            })\n\n        except Exception as e:\n            print(f\"Error in {gse_id}: {e}\")\n\n    return pd.DataFrame(results)\n\n# Meta-analysis for TP53\ngse_studies = [\"GSE100001\", \"GSE100002\", \"GSE100003\"]\nmeta_results = meta_analysis_geo(gse_studies, \"TP53\")\nprint(meta_results)\n```\n\n## Installation and Setup\n\n### Python Libraries\n\n```bash\n# Primary GEO access library (recommended)\nuv pip install GEOparse\n\n# For E-utilities and programmatic NCBI access\nuv pip install biopython\n\n# For data analysis\nuv pip install pandas numpy scipy\n\n# For visualization\nuv pip install matplotlib seaborn\n\n# For statistical analysis\nuv pip install statsmodels scikit-learn\n```\n\n### Configuration\n\nSet up NCBI E-utilities access:\n\n```python\nfrom Bio import Entrez\n\n# Always set your email (required by NCBI)\nEntrez.email = \"your.email@example.com\"\n\n# Optional: Set API key for increased rate limits\n# Get your API key from: https://www.ncbi.nlm.nih.gov/account/\nEntrez.api_key = \"your_api_key_here\"\n\n# With API key: 10 requests/second\n# Without API key: 3 requests/second\n```\n\n## Common Use Cases\n\n### Transcriptomics Research\n- Download gene expression data for specific conditions\n- Compare expression profiles across studies\n- Identify differentially expressed genes\n- Perform meta-analyses across multiple datasets\n\n### Drug Response Studies\n- Analyze gene expression changes after drug treatment\n- Identify biomarkers for drug response\n- Compare drug effects across cell lines or patients\n- Build predictive models for drug sensitivity\n\n### Disease Biology\n- Study gene expression in disease vs. normal tissues\n- Identify disease-associated expression signatures\n- Compare patient subgroups and disease stages\n- Correlate expression with clinical outcomes\n\n### Biomarker Discovery\n- Screen for diagnostic or prognostic markers\n- Validate biomarkers across independent cohorts\n- Compare marker performance across platforms\n- Integrate expression with clinical data\n\n## Key Concepts\n\n**SOFT (Simple Omnibus Format in Text):** GEO's primary text-based format containing metadata and data tables. Easily parsed by GEOparse.\n\n**MINiML (MIAME Notation in Markup Language):** XML format for GEO data, used for programmatic access and data exchange.\n\n**Series Matrix:** Tab-delimited expression matrix with samples as columns and genes/probes as rows. Fastest format for getting expression data.\n\n**MIAME Compliance:** Minimum Information About a Microarray Experiment - standardized annotation that GEO enforces for all submissions.\n\n**Expression Value Types:** Different types of expression measurements (raw signal, normalized, log-transformed). Always check platform and processing methods.\n\n**Platform Annotation:** Maps probe/feature IDs to genes. Essential for biological interpretation of expression data.\n\n## GEO2R Web Tool\n\nFor quick analysis without coding, use GEO2R:\n\n- Web-based statistical analysis tool integrated into GEO\n- Accessible at: https://www.ncbi.nlm.nih.gov/geo/geo2r/?acc=GSExxxxx\n- Performs differential expression analysis\n- Generates R scripts for reproducibility\n- Useful for exploratory analysis before downloading data\n\n## Rate Limiting and Best Practices\n\n**NCBI E-utilities Rate Limits:**\n- Without API key: 3 requests per second\n- With API key: 10 requests per second\n- Implement delays between requests: `time.sleep(0.34)` (no API key) or `time.sleep(0.1)` (with API key)\n\n**FTP Access:**\n- No rate limits for FTP downloads\n- Preferred method for bulk downloads\n- Can download entire directories with wget -r\n\n**GEOparse Caching:**\n- GEOparse automatically caches downloaded files in destdir\n- Subsequent calls use cached data\n- Clean cache periodically to save disk space\n\n**Optimal Practices:**\n- Use GEOparse for series-level access (easiest)\n- Use E-utilities for metadata searching and batch queries\n- Use FTP for direct file downloads and bulk operations\n- Cache data locally to avoid repeated downloads\n- Always set Entrez.email when using Biopython\n\n## Resources\n\n### references/geo_reference.md\n\nComprehensive reference documentation covering:\n- Detailed E-utilities API specifications and endpoints\n- Complete SOFT and MINiML file format documentation\n- Advanced GEOparse usage patterns and examples\n- FTP directory structure and file naming conventions\n- Data processing pipelines and normalization methods\n- Troubleshooting common issues and error handling\n- Platform-specific considerations and quirks\n\nConsult this reference for in-depth technical details, complex query patterns, or when working with uncommon data formats.\n\n## Important Notes\n\n### Data Quality Considerations\n\n- GEO accepts user-submitted data with varying quality standards\n- Always check platform annotation and processing methods\n- Verify sample metadata and experimental design\n- Be cautious with batch effects across studies\n- Consider reprocessing raw data for consistency\n\n### File Size Warnings\n\n- Series matrix files can be large (>1 GB for large studies)\n- Supplementary files (e.g., CEL files) can be very large\n- Plan for adequate disk space before downloading\n- Consider downloading samples incrementally\n\n### Data Usage and Citation\n\n- GEO data is freely available for research use\n- Always cite original studies when using GEO data\n- Cite GEO database: Barrett et al. (2013) Nucleic Acids Research\n- Check individual dataset usage restrictions (if any)\n- Follow NCBI guidelines for programmatic access\n\n### Common Pitfalls\n\n- Different platforms use different probe IDs (requires annotation mapping)\n- Expression values may be raw, normalized, or log-transformed (check metadata)\n- Sample metadata can be inconsistently formatted across studies\n- Not all series have series matrix files (older submissions)\n- Platform annotations may be outdated (genes renamed, IDs deprecated)\n\n## Additional Resources\n\n- **GEO Website:** https://www.ncbi.nlm.nih.gov/geo/\n- **GEO Submission Guidelines:** https://www.ncbi.nlm.nih.gov/geo/info/submission.html\n- **GEOparse Documentation:** https://geoparse.readthedocs.io/\n- **E-utilities Documentation:** https://www.ncbi.nlm.nih.gov/books/NBK25501/\n- **GEO FTP Site:** ftp://ftp.ncbi.nlm.nih.gov/geo/\n- **GEO2R Tool:** https://www.ncbi.nlm.nih.gov/geo/geo2r/\n- **NCBI API Keys:** https://ncbiinsights.ncbi.nlm.nih.gov/2017/11/02/new-api-keys-for-the-e-utilities/\n- **Biopython Tutorial:** https://biopython.org/DIST/docs/tutorial/Tutorial.html\n",
        "data/k-dense-ai/get-available-resources/SKILL.md": "---\nname: get-available-resources\ndescription: This skill should be used at the start of any computationally intensive scientific task to detect and report available system resources (CPU cores, GPUs, memory, disk space). It creates a JSON file with resource information and strategic recommendations that inform computational approach decisions such as whether to use parallel processing (joblib, multiprocessing), out-of-core computing (Dask, Zarr), GPU acceleration (PyTorch, JAX), or memory-efficient strategies. Use this skill before running analyses, training models, processing large datasets, or any task where resource constraints matter.\n---\n\n# Get Available Resources\n\n## Overview\n\nDetect available computational resources and generate strategic recommendations for scientific computing tasks. This skill automatically identifies CPU capabilities, GPU availability (NVIDIA CUDA, AMD ROCm, Apple Silicon Metal), memory constraints, and disk space to help make informed decisions about computational approaches.\n\n## When to Use This Skill\n\nUse this skill proactively before any computationally intensive task:\n\n- **Before data analysis**: Determine if datasets can be loaded into memory or require out-of-core processing\n- **Before model training**: Check if GPU acceleration is available and which backend to use\n- **Before parallel processing**: Identify optimal number of workers for joblib, multiprocessing, or Dask\n- **Before large file operations**: Verify sufficient disk space and appropriate storage strategies\n- **At project initialization**: Understand baseline capabilities for making architectural decisions\n\n**Example scenarios:**\n- \"Help me analyze this 50GB genomics dataset\"  Use this skill first to determine if Dask/Zarr are needed\n- \"Train a neural network on this data\"  Use this skill to detect available GPUs and backends\n- \"Process 10,000 files in parallel\"  Use this skill to determine optimal worker count\n- \"Run a computationally intensive simulation\"  Use this skill to understand resource constraints\n\n## How This Skill Works\n\n### Resource Detection\n\nThe skill runs `scripts/detect_resources.py` to automatically detect:\n\n1. **CPU Information**\n   - Physical and logical core counts\n   - Processor architecture and model\n   - CPU frequency information\n\n2. **GPU Information**\n   - NVIDIA GPUs: Detects via nvidia-smi, reports VRAM, driver version, compute capability\n   - AMD GPUs: Detects via rocm-smi\n   - Apple Silicon: Detects M1/M2/M3/M4 chips with Metal support and unified memory\n\n3. **Memory Information**\n   - Total and available RAM\n   - Current memory usage percentage\n   - Swap space availability\n\n4. **Disk Space Information**\n   - Total and available disk space for working directory\n   - Current usage percentage\n\n5. **Operating System Information**\n   - OS type (macOS, Linux, Windows)\n   - OS version and release\n   - Python version\n\n### Output Format\n\nThe skill generates a `.claude_resources.json` file in the current working directory containing:\n\n```json\n{\n  \"timestamp\": \"2025-10-23T10:30:00\",\n  \"os\": {\n    \"system\": \"Darwin\",\n    \"release\": \"25.0.0\",\n    \"machine\": \"arm64\"\n  },\n  \"cpu\": {\n    \"physical_cores\": 8,\n    \"logical_cores\": 8,\n    \"architecture\": \"arm64\"\n  },\n  \"memory\": {\n    \"total_gb\": 16.0,\n    \"available_gb\": 8.5,\n    \"percent_used\": 46.9\n  },\n  \"disk\": {\n    \"total_gb\": 500.0,\n    \"available_gb\": 200.0,\n    \"percent_used\": 60.0\n  },\n  \"gpu\": {\n    \"nvidia_gpus\": [],\n    \"amd_gpus\": [],\n    \"apple_silicon\": {\n      \"name\": \"Apple M2\",\n      \"type\": \"Apple Silicon\",\n      \"backend\": \"Metal\",\n      \"unified_memory\": true\n    },\n    \"total_gpus\": 1,\n    \"available_backends\": [\"Metal\"]\n  },\n  \"recommendations\": {\n    \"parallel_processing\": {\n      \"strategy\": \"high_parallelism\",\n      \"suggested_workers\": 6,\n      \"libraries\": [\"joblib\", \"multiprocessing\", \"dask\"]\n    },\n    \"memory_strategy\": {\n      \"strategy\": \"moderate_memory\",\n      \"libraries\": [\"dask\", \"zarr\"],\n      \"note\": \"Consider chunking for datasets > 2GB\"\n    },\n    \"gpu_acceleration\": {\n      \"available\": true,\n      \"backends\": [\"Metal\"],\n      \"suggested_libraries\": [\"pytorch-mps\", \"tensorflow-metal\", \"jax-metal\"]\n    },\n    \"large_data_handling\": {\n      \"strategy\": \"disk_abundant\",\n      \"note\": \"Sufficient space for large intermediate files\"\n    }\n  }\n}\n```\n\n### Strategic Recommendations\n\nThe skill generates context-aware recommendations:\n\n**Parallel Processing Recommendations:**\n- **High parallelism (8+ cores)**: Use Dask, joblib, or multiprocessing with workers = cores - 2\n- **Moderate parallelism (4-7 cores)**: Use joblib or multiprocessing with workers = cores - 1\n- **Sequential (< 4 cores)**: Prefer sequential processing to avoid overhead\n\n**Memory Strategy Recommendations:**\n- **Memory constrained (< 4GB available)**: Use Zarr, Dask, or H5py for out-of-core processing\n- **Moderate memory (4-16GB available)**: Use Dask/Zarr for datasets > 2GB\n- **Memory abundant (> 16GB available)**: Can load most datasets into memory directly\n\n**GPU Acceleration Recommendations:**\n- **NVIDIA GPUs detected**: Use PyTorch, TensorFlow, JAX, CuPy, or RAPIDS\n- **AMD GPUs detected**: Use PyTorch-ROCm or TensorFlow-ROCm\n- **Apple Silicon detected**: Use PyTorch with MPS backend, TensorFlow-Metal, or JAX-Metal\n- **No GPU detected**: Use CPU-optimized libraries\n\n**Large Data Handling Recommendations:**\n- **Disk constrained (< 10GB)**: Use streaming or compression strategies\n- **Moderate disk (10-100GB)**: Use Zarr, H5py, or Parquet formats\n- **Disk abundant (> 100GB)**: Can create large intermediate files freely\n\n## Usage Instructions\n\n### Step 1: Run Resource Detection\n\nExecute the detection script at the start of any computationally intensive task:\n\n```bash\npython scripts/detect_resources.py\n```\n\nOptional arguments:\n- `-o, --output <path>`: Specify custom output path (default: `.claude_resources.json`)\n- `-v, --verbose`: Print full resource information to stdout\n\n### Step 2: Read and Apply Recommendations\n\nAfter running detection, read the generated `.claude_resources.json` file to inform computational decisions:\n\n```python\n# Example: Use recommendations in code\nimport json\n\nwith open('.claude_resources.json', 'r') as f:\n    resources = json.load(f)\n\n# Check parallel processing strategy\nif resources['recommendations']['parallel_processing']['strategy'] == 'high_parallelism':\n    n_jobs = resources['recommendations']['parallel_processing']['suggested_workers']\n    # Use joblib, Dask, or multiprocessing with n_jobs workers\n\n# Check memory strategy\nif resources['recommendations']['memory_strategy']['strategy'] == 'memory_constrained':\n    # Use Dask, Zarr, or H5py for out-of-core processing\n    import dask.array as da\n    # Load data in chunks\n\n# Check GPU availability\nif resources['recommendations']['gpu_acceleration']['available']:\n    backends = resources['recommendations']['gpu_acceleration']['backends']\n    # Use appropriate GPU library based on available backend\n```\n\n### Step 3: Make Informed Decisions\n\nUse the resource information and recommendations to make strategic choices:\n\n**For data loading:**\n```python\nmemory_available_gb = resources['memory']['available_gb']\ndataset_size_gb = 10\n\nif dataset_size_gb > memory_available_gb * 0.5:\n    # Dataset is large relative to memory, use Dask\n    import dask.dataframe as dd\n    df = dd.read_csv('large_file.csv')\nelse:\n    # Dataset fits in memory, use pandas\n    import pandas as pd\n    df = pd.read_csv('large_file.csv')\n```\n\n**For parallel processing:**\n```python\nfrom joblib import Parallel, delayed\n\nn_jobs = resources['recommendations']['parallel_processing'].get('suggested_workers', 1)\n\nresults = Parallel(n_jobs=n_jobs)(\n    delayed(process_function)(item) for item in data\n)\n```\n\n**For GPU acceleration:**\n```python\nimport torch\n\nif 'CUDA' in resources['gpu']['available_backends']:\n    device = torch.device('cuda')\nelif 'Metal' in resources['gpu']['available_backends']:\n    device = torch.device('mps')\nelse:\n    device = torch.device('cpu')\n\nmodel = model.to(device)\n```\n\n## Dependencies\n\nThe detection script requires the following Python packages:\n\n```bash\nuv pip install psutil\n```\n\nAll other functionality uses Python standard library modules (json, os, platform, subprocess, sys, pathlib).\n\n## Platform Support\n\n- **macOS**: Full support including Apple Silicon (M1/M2/M3/M4) GPU detection\n- **Linux**: Full support including NVIDIA (nvidia-smi) and AMD (rocm-smi) GPU detection\n- **Windows**: Full support including NVIDIA GPU detection\n\n## Best Practices\n\n1. **Run early**: Execute resource detection at the start of projects or before major computational tasks\n2. **Re-run periodically**: System resources change over time (memory usage, disk space)\n3. **Check before scaling**: Verify resources before scaling up parallel workers or data sizes\n4. **Document decisions**: Keep the `.claude_resources.json` file in project directories to document resource-aware decisions\n5. **Use with versioning**: Different machines have different capabilities; resource files help maintain portability\n\n## Troubleshooting\n\n**GPU not detected:**\n- Ensure GPU drivers are installed (nvidia-smi, rocm-smi, or system_profiler for Apple Silicon)\n- Check that GPU utilities are in system PATH\n- Verify GPU is not in use by other processes\n\n**Script execution fails:**\n- Ensure psutil is installed: `uv pip install psutil`\n- Check Python version compatibility (Python 3.6+)\n- Verify script has execute permissions: `chmod +x scripts/detect_resources.py`\n\n**Inaccurate memory readings:**\n- Memory readings are snapshots; actual available memory changes constantly\n- Close other applications before detection for accurate \"available\" memory\n- Consider running detection multiple times and averaging results\n",
        "data/k-dense-ai/gget/SKILL.md": "---\nname: gget\ndescription: \"CLI/Python toolkit for rapid bioinformatics queries. Preferred for quick BLAST searches. Access to 20+ databases: gene info (Ensembl/UniProt), AlphaFold, ARCHS4, Enrichr, OpenTargets, COSMIC, genome downloads. For advanced BLAST/batch processing, use biopython. For multi-database integration, use bioservices.\"\n---\n\n# gget\n\n## Overview\n\ngget is a command-line bioinformatics tool and Python package providing unified access to 20+ genomic databases and analysis methods. Query gene information, sequence analysis, protein structures, expression data, and disease associations through a consistent interface. All gget modules work both as command-line tools and as Python functions.\n\n**Important**: The databases queried by gget are continuously updated, which sometimes changes their structure. gget modules are tested automatically on a biweekly basis and updated to match new database structures when necessary.\n\n## Installation\n\nInstall gget in a clean virtual environment to avoid conflicts:\n\n```bash\n# Using uv (recommended)\nuv uv pip install gget\n\n# Or using pip\nuv pip install --upgrade gget\n\n# In Python/Jupyter\nimport gget\n```\n\n## Quick Start\n\nBasic usage pattern for all modules:\n\n```bash\n# Command-line\ngget <module> [arguments] [options]\n\n# Python\ngget.module(arguments, options)\n```\n\nMost modules return:\n- **Command-line**: JSON (default) or CSV with `-csv` flag\n- **Python**: DataFrame or dictionary\n\nCommon flags across modules:\n- `-o/--out`: Save results to file\n- `-q/--quiet`: Suppress progress information\n- `-csv`: Return CSV format (command-line only)\n\n## Module Categories\n\n### 1. Reference & Gene Information\n\n#### gget ref - Reference Genome Downloads\n\nRetrieve download links and metadata for Ensembl reference genomes.\n\n**Parameters**:\n- `species`: Genus_species format (e.g., 'homo_sapiens', 'mus_musculus'). Shortcuts: 'human', 'mouse'\n- `-w/--which`: Specify return types (gtf, cdna, dna, cds, cdrna, pep). Default: all\n- `-r/--release`: Ensembl release number (default: latest)\n- `-l/--list_species`: List available vertebrate species\n- `-liv/--list_iv_species`: List available invertebrate species\n- `-ftp`: Return only FTP links\n- `-d/--download`: Download files (requires curl)\n\n**Examples**:\n```bash\n# List available species\ngget ref --list_species\n\n# Get all reference files for human\ngget ref homo_sapiens\n\n# Download only GTF annotation for mouse\ngget ref -w gtf -d mouse\n```\n\n```python\n# Python\ngget.ref(\"homo_sapiens\")\ngget.ref(\"mus_musculus\", which=\"gtf\", download=True)\n```\n\n#### gget search - Gene Search\n\nLocate genes by name or description across species.\n\n**Parameters**:\n- `searchwords`: One or more search terms (case-insensitive)\n- `-s/--species`: Target species (e.g., 'homo_sapiens', 'mouse')\n- `-r/--release`: Ensembl release number\n- `-t/--id_type`: Return 'gene' (default) or 'transcript'\n- `-ao/--andor`: 'or' (default) finds ANY searchword; 'and' requires ALL\n- `-l/--limit`: Maximum results to return\n\n**Returns**: ensembl_id, gene_name, ensembl_description, ext_ref_description, biotype, URL\n\n**Examples**:\n```bash\n# Search for GABA-related genes in human\ngget search -s human gaba gamma-aminobutyric\n\n# Find specific gene, require all terms\ngget search -s mouse -ao and pax7 transcription\n```\n\n```python\n# Python\ngget.search([\"gaba\", \"gamma-aminobutyric\"], species=\"homo_sapiens\")\n```\n\n#### gget info - Gene/Transcript Information\n\nRetrieve comprehensive gene and transcript metadata from Ensembl, UniProt, and NCBI.\n\n**Parameters**:\n- `ens_ids`: One or more Ensembl IDs (also supports WormBase, Flybase IDs). Limit: ~1000 IDs\n- `-n/--ncbi`: Disable NCBI data retrieval\n- `-u/--uniprot`: Disable UniProt data retrieval\n- `-pdb`: Include PDB identifiers (increases runtime)\n\n**Returns**: UniProt ID, NCBI gene ID, primary gene name, synonyms, protein names, descriptions, biotype, canonical transcript\n\n**Examples**:\n```bash\n# Get info for multiple genes\ngget info ENSG00000034713 ENSG00000104853 ENSG00000170296\n\n# Include PDB IDs\ngget info ENSG00000034713 -pdb\n```\n\n```python\n# Python\ngget.info([\"ENSG00000034713\", \"ENSG00000104853\"], pdb=True)\n```\n\n#### gget seq - Sequence Retrieval\n\nFetch nucleotide or amino acid sequences for genes and transcripts.\n\n**Parameters**:\n- `ens_ids`: One or more Ensembl identifiers\n- `-t/--translate`: Fetch amino acid sequences instead of nucleotide\n- `-iso/--isoforms`: Return all transcript variants (gene IDs only)\n\n**Returns**: FASTA format sequences\n\n**Examples**:\n```bash\n# Get nucleotide sequences\ngget seq ENSG00000034713 ENSG00000104853\n\n# Get all protein isoforms\ngget seq -t -iso ENSG00000034713\n```\n\n```python\n# Python\ngget.seq([\"ENSG00000034713\"], translate=True, isoforms=True)\n```\n\n### 2. Sequence Analysis & Alignment\n\n#### gget blast - BLAST Searches\n\nBLAST nucleotide or amino acid sequences against standard databases.\n\n**Parameters**:\n- `sequence`: Sequence string or path to FASTA/.txt file\n- `-p/--program`: blastn, blastp, blastx, tblastn, tblastx (auto-detected)\n- `-db/--database`:\n  - Nucleotide: nt, refseq_rna, pdbnt\n  - Protein: nr, swissprot, pdbaa, refseq_protein\n- `-l/--limit`: Max hits (default: 50)\n- `-e/--expect`: E-value cutoff (default: 10.0)\n- `-lcf/--low_comp_filt`: Enable low complexity filtering\n- `-mbo/--megablast_off`: Disable MegaBLAST (blastn only)\n\n**Examples**:\n```bash\n# BLAST protein sequence\ngget blast MKWMFKEDHSLEHRCVESAKIRAKYPDRVPVIVEKVSGSQIVDIDKRKYLVPSDITVAQFMWIIRKRIQLPSEKAIFLFVDKTVPQSR\n\n# BLAST from file with specific database\ngget blast sequence.fasta -db swissprot -l 10\n```\n\n```python\n# Python\ngget.blast(\"MKWMFK...\", database=\"swissprot\", limit=10)\n```\n\n#### gget blat - BLAT Searches\n\nLocate genomic positions of sequences using UCSC BLAT.\n\n**Parameters**:\n- `sequence`: Sequence string or path to FASTA/.txt file\n- `-st/--seqtype`: 'DNA', 'protein', 'translated%20RNA', 'translated%20DNA' (auto-detected)\n- `-a/--assembly`: Target assembly (default: 'human'/hg38; options: 'mouse'/mm39, 'zebrafinch'/taeGut2, etc.)\n\n**Returns**: genome, query size, alignment positions, matches, mismatches, alignment percentage\n\n**Examples**:\n```bash\n# Find genomic location in human\ngget blat ATCGATCGATCGATCG\n\n# Search in different assembly\ngget blat -a mm39 ATCGATCGATCGATCG\n```\n\n```python\n# Python\ngget.blat(\"ATCGATCGATCGATCG\", assembly=\"mouse\")\n```\n\n#### gget muscle - Multiple Sequence Alignment\n\nAlign multiple nucleotide or amino acid sequences using Muscle5.\n\n**Parameters**:\n- `fasta`: Sequences or path to FASTA/.txt file\n- `-s5/--super5`: Use Super5 algorithm for faster processing (large datasets)\n\n**Returns**: Aligned sequences in ClustalW format or aligned FASTA (.afa)\n\n**Examples**:\n```bash\n# Align sequences from file\ngget muscle sequences.fasta -o aligned.afa\n\n# Use Super5 for large dataset\ngget muscle large_dataset.fasta -s5\n```\n\n```python\n# Python\ngget.muscle(\"sequences.fasta\", save=True)\n```\n\n#### gget diamond - Local Sequence Alignment\n\nPerform fast local protein or translated DNA alignment using DIAMOND.\n\n**Parameters**:\n- Query: Sequences (string/list) or FASTA file path\n- `--reference`: Reference sequences (string/list) or FASTA file path (required)\n- `--sensitivity`: fast, mid-sensitive, sensitive, more-sensitive, very-sensitive (default), ultra-sensitive\n- `--threads`: CPU threads (default: 1)\n- `--diamond_db`: Save database for reuse\n- `--translated`: Enable nucleotide-to-amino acid alignment\n\n**Returns**: Identity percentage, sequence lengths, match positions, gap openings, E-values, bit scores\n\n**Examples**:\n```bash\n# Align against reference\ngget diamond GGETISAWESQME -ref reference.fasta --threads 4\n\n# Save database for reuse\ngget diamond query.fasta -ref ref.fasta --diamond_db my_db.dmnd\n```\n\n```python\n# Python\ngget.diamond(\"GGETISAWESQME\", reference=\"reference.fasta\", threads=4)\n```\n\n### 3. Structural & Protein Analysis\n\n#### gget pdb - Protein Structures\n\nQuery RCSB Protein Data Bank for structure and metadata.\n\n**Parameters**:\n- `pdb_id`: PDB identifier (e.g., '7S7U')\n- `-r/--resource`: Data type (pdb, entry, pubmed, assembly, entity types)\n- `-i/--identifier`: Assembly, entity, or chain ID\n\n**Returns**: PDB format (structures) or JSON (metadata)\n\n**Examples**:\n```bash\n# Download PDB structure\ngget pdb 7S7U -o 7S7U.pdb\n\n# Get metadata\ngget pdb 7S7U -r entry\n```\n\n```python\n# Python\ngget.pdb(\"7S7U\", save=True)\n```\n\n#### gget alphafold - Protein Structure Prediction\n\nPredict 3D protein structures using simplified AlphaFold2.\n\n**Setup Required**:\n```bash\n# Install OpenMM first\nuv pip install openmm\n\n# Then setup AlphaFold\ngget setup alphafold\n```\n\n**Parameters**:\n- `sequence`: Amino acid sequence (string), multiple sequences (list), or FASTA file. Multiple sequences trigger multimer modeling\n- `-mr/--multimer_recycles`: Recycling iterations (default: 3; recommend 20 for accuracy)\n- `-mfm/--multimer_for_monomer`: Apply multimer model to single proteins\n- `-r/--relax`: AMBER relaxation for top-ranked model\n- `plot`: Python-only; generate interactive 3D visualization (default: True)\n- `show_sidechains`: Python-only; include side chains (default: True)\n\n**Returns**: PDB structure file, JSON alignment error data, optional 3D visualization\n\n**Examples**:\n```bash\n# Predict single protein structure\ngget alphafold MKWMFKEDHSLEHRCVESAKIRAKYPDRVPVIVEKVSGSQIVDIDKRKYLVPSDITVAQFMWIIRKRIQLPSEKAIFLFVDKTVPQSR\n\n# Predict multimer with higher accuracy\ngget alphafold sequence1.fasta -mr 20 -r\n```\n\n```python\n# Python with visualization\ngget.alphafold(\"MKWMFK...\", plot=True, show_sidechains=True)\n\n# Multimer prediction\ngget.alphafold([\"sequence1\", \"sequence2\"], multimer_recycles=20)\n```\n\n#### gget elm - Eukaryotic Linear Motifs\n\nPredict Eukaryotic Linear Motifs in protein sequences.\n\n**Setup Required**:\n```bash\ngget setup elm\n```\n\n**Parameters**:\n- `sequence`: Amino acid sequence or UniProt Acc\n- `-u/--uniprot`: Indicates sequence is UniProt Acc\n- `-e/--expand`: Include protein names, organisms, references\n- `-s/--sensitivity`: DIAMOND alignment sensitivity (default: \"very-sensitive\")\n- `-t/--threads`: Number of threads (default: 1)\n\n**Returns**: Two outputs:\n1. **ortholog_df**: Linear motifs from orthologous proteins\n2. **regex_df**: Motifs directly matched in input sequence\n\n**Examples**:\n```bash\n# Predict motifs from sequence\ngget elm LIAQSIGQASFV -o results\n\n# Use UniProt accession with expanded info\ngget elm --uniprot Q02410 -e\n```\n\n```python\n# Python\northolog_df, regex_df = gget.elm(\"LIAQSIGQASFV\")\n```\n\n### 4. Expression & Disease Data\n\n#### gget archs4 - Gene Correlation & Tissue Expression\n\nQuery ARCHS4 database for correlated genes or tissue expression data.\n\n**Parameters**:\n- `gene`: Gene symbol or Ensembl ID (with `--ensembl` flag)\n- `-w/--which`: 'correlation' (default, returns 100 most correlated genes) or 'tissue' (expression atlas)\n- `-s/--species`: 'human' (default) or 'mouse' (tissue data only)\n- `-e/--ensembl`: Input is Ensembl ID\n\n**Returns**:\n- **Correlation mode**: Gene symbols, Pearson correlation coefficients\n- **Tissue mode**: Tissue identifiers, min/Q1/median/Q3/max expression values\n\n**Examples**:\n```bash\n# Get correlated genes\ngget archs4 ACE2\n\n# Get tissue expression\ngget archs4 -w tissue ACE2\n```\n\n```python\n# Python\ngget.archs4(\"ACE2\", which=\"tissue\")\n```\n\n#### gget cellxgene - Single-Cell RNA-seq Data\n\nQuery CZ CELLxGENE Discover Census for single-cell data.\n\n**Setup Required**:\n```bash\ngget setup cellxgene\n```\n\n**Parameters**:\n- `--gene` (-g): Gene names or Ensembl IDs (case-sensitive! 'PAX7' for human, 'Pax7' for mouse)\n- `--tissue`: Tissue type(s)\n- `--cell_type`: Specific cell type(s)\n- `--species` (-s): 'homo_sapiens' (default) or 'mus_musculus'\n- `--census_version` (-cv): Version (\"stable\", \"latest\", or dated)\n- `--ensembl` (-e): Use Ensembl IDs\n- `--meta_only` (-mo): Return metadata only\n- Additional filters: disease, development_stage, sex, assay, dataset_id, donor_id, ethnicity, suspension_type\n\n**Returns**: AnnData object with count matrices and metadata (or metadata-only dataframes)\n\n**Examples**:\n```bash\n# Get single-cell data for specific genes and cell types\ngget cellxgene --gene ACE2 ABCA1 --tissue lung --cell_type \"mucus secreting cell\" -o lung_data.h5ad\n\n# Metadata only\ngget cellxgene --gene PAX7 --tissue muscle --meta_only -o metadata.csv\n```\n\n```python\n# Python\nadata = gget.cellxgene(gene=[\"ACE2\", \"ABCA1\"], tissue=\"lung\", cell_type=\"mucus secreting cell\")\n```\n\n#### gget enrichr - Enrichment Analysis\n\nPerform ontology enrichment analysis on gene lists using Enrichr.\n\n**Parameters**:\n- `genes`: Gene symbols or Ensembl IDs\n- `-db/--database`: Reference database (supports shortcuts: 'pathway', 'transcription', 'ontology', 'diseases_drugs', 'celltypes')\n- `-s/--species`: human (default), mouse, fly, yeast, worm, fish\n- `-bkg_l/--background_list`: Background genes for comparison\n- `-ko/--kegg_out`: Save KEGG pathway images with highlighted genes\n- `plot`: Python-only; generate graphical results\n\n**Database Shortcuts**:\n- 'pathway'  KEGG_2021_Human\n- 'transcription'  ChEA_2016\n- 'ontology'  GO_Biological_Process_2021\n- 'diseases_drugs'  GWAS_Catalog_2019\n- 'celltypes'  PanglaoDB_Augmented_2021\n\n**Examples**:\n```bash\n# Enrichment analysis for ontology\ngget enrichr -db ontology ACE2 AGT AGTR1\n\n# Save KEGG pathways\ngget enrichr -db pathway ACE2 AGT AGTR1 -ko ./kegg_images/\n```\n\n```python\n# Python with plot\ngget.enrichr([\"ACE2\", \"AGT\", \"AGTR1\"], database=\"ontology\", plot=True)\n```\n\n#### gget bgee - Orthology & Expression\n\nRetrieve orthology and gene expression data from Bgee database.\n\n**Parameters**:\n- `ens_id`: Ensembl gene ID or NCBI gene ID (for non-Ensembl species). Multiple IDs supported when `type=expression`\n- `-t/--type`: 'orthologs' (default) or 'expression'\n\n**Returns**:\n- **Orthologs mode**: Matching genes across species with IDs, names, taxonomic info\n- **Expression mode**: Anatomical entities, confidence scores, expression status\n\n**Examples**:\n```bash\n# Get orthologs\ngget bgee ENSG00000169194\n\n# Get expression data\ngget bgee ENSG00000169194 -t expression\n\n# Multiple genes\ngget bgee ENSBTAG00000047356 ENSBTAG00000018317 -t expression\n```\n\n```python\n# Python\ngget.bgee(\"ENSG00000169194\", type=\"orthologs\")\n```\n\n#### gget opentargets - Disease & Drug Associations\n\nRetrieve disease and drug associations from OpenTargets.\n\n**Parameters**:\n- Ensembl gene ID (required)\n- `-r/--resource`: diseases (default), drugs, tractability, pharmacogenetics, expression, depmap, interactions\n- `-l/--limit`: Cap results count\n- Filter arguments (vary by resource):\n  - drugs: `--filter_disease`\n  - pharmacogenetics: `--filter_drug`\n  - expression/depmap: `--filter_tissue`, `--filter_anat_sys`, `--filter_organ`\n  - interactions: `--filter_protein_a`, `--filter_protein_b`, `--filter_gene_b`\n\n**Examples**:\n```bash\n# Get associated diseases\ngget opentargets ENSG00000169194 -r diseases -l 5\n\n# Get associated drugs\ngget opentargets ENSG00000169194 -r drugs -l 10\n\n# Get tissue expression\ngget opentargets ENSG00000169194 -r expression --filter_tissue brain\n```\n\n```python\n# Python\ngget.opentargets(\"ENSG00000169194\", resource=\"diseases\", limit=5)\n```\n\n#### gget cbio - cBioPortal Cancer Genomics\n\nPlot cancer genomics heatmaps using cBioPortal data.\n\n**Two subcommands**:\n\n**search** - Find study IDs:\n```bash\ngget cbio search breast lung\n```\n\n**plot** - Generate heatmaps:\n\n**Parameters**:\n- `-s/--study_ids`: Space-separated cBioPortal study IDs (required)\n- `-g/--genes`: Space-separated gene names or Ensembl IDs (required)\n- `-st/--stratification`: Column to organize data (tissue, cancer_type, cancer_type_detailed, study_id, sample)\n- `-vt/--variation_type`: Data type (mutation_occurrences, cna_nonbinary, sv_occurrences, cna_occurrences, Consequence)\n- `-f/--filter`: Filter by column value (e.g., 'study_id:msk_impact_2017')\n- `-dd/--data_dir`: Cache directory (default: ./gget_cbio_cache)\n- `-fd/--figure_dir`: Output directory (default: ./gget_cbio_figures)\n- `-dpi`: Resolution (default: 100)\n- `-sh/--show`: Display plot in window\n- `-nc/--no_confirm`: Skip download confirmations\n\n**Examples**:\n```bash\n# Search for studies\ngget cbio search esophag ovary\n\n# Create heatmap\ngget cbio plot -s msk_impact_2017 -g AKT1 ALK BRAF -st tissue -vt mutation_occurrences\n```\n\n```python\n# Python\ngget.cbio_search([\"esophag\", \"ovary\"])\ngget.cbio_plot([\"msk_impact_2017\"], [\"AKT1\", \"ALK\"], stratification=\"tissue\")\n```\n\n#### gget cosmic - COSMIC Database\n\nSearch COSMIC (Catalogue Of Somatic Mutations In Cancer) database.\n\n**Important**: License fees apply for commercial use. Requires COSMIC account credentials.\n\n**Parameters**:\n- `searchterm`: Gene name, Ensembl ID, mutation notation, or sample ID\n- `-ctp/--cosmic_tsv_path`: Path to downloaded COSMIC TSV file (required for querying)\n- `-l/--limit`: Maximum results (default: 100)\n\n**Database download flags**:\n- `-d/--download_cosmic`: Activate download mode\n- `-gm/--gget_mutate`: Create version for gget mutate\n- `-cp/--cosmic_project`: Database type (cancer, census, cell_line, resistance, genome_screen, targeted_screen)\n- `-cv/--cosmic_version`: COSMIC version\n- `-gv/--grch_version`: Human reference genome (37 or 38)\n- `--email`, `--password`: COSMIC credentials\n\n**Examples**:\n```bash\n# First download database\ngget cosmic -d --email user@example.com --password xxx -cp cancer\n\n# Then query\ngget cosmic EGFR -ctp cosmic_data.tsv -l 10\n```\n\n```python\n# Python\ngget.cosmic(\"EGFR\", cosmic_tsv_path=\"cosmic_data.tsv\", limit=10)\n```\n\n### 5. Additional Tools\n\n#### gget mutate - Generate Mutated Sequences\n\nGenerate mutated nucleotide sequences from mutation annotations.\n\n**Parameters**:\n- `sequences`: FASTA file path or direct sequence input (string/list)\n- `-m/--mutations`: CSV/TSV file or DataFrame with mutation data (required)\n- `-mc/--mut_column`: Mutation column name (default: 'mutation')\n- `-sic/--seq_id_column`: Sequence ID column (default: 'seq_ID')\n- `-mic/--mut_id_column`: Mutation ID column\n- `-k/--k`: Length of flanking sequences (default: 30 nucleotides)\n\n**Returns**: Mutated sequences in FASTA format\n\n**Examples**:\n```bash\n# Single mutation\ngget mutate ATCGCTAAGCT -m \"c.4G>T\"\n\n# Multiple sequences with mutations from file\ngget mutate sequences.fasta -m mutations.csv -o mutated.fasta\n```\n\n```python\n# Python\nimport pandas as pd\nmutations_df = pd.DataFrame({\"seq_ID\": [\"seq1\"], \"mutation\": [\"c.4G>T\"]})\ngget.mutate([\"ATCGCTAAGCT\"], mutations=mutations_df)\n```\n\n#### gget gpt - OpenAI Text Generation\n\nGenerate natural language text using OpenAI's API.\n\n**Setup Required**:\n```bash\ngget setup gpt\n```\n\n**Important**: Free tier limited to 3 months after account creation. Set monthly billing limits.\n\n**Parameters**:\n- `prompt`: Text input for generation (required)\n- `api_key`: OpenAI authentication (required)\n- Model configuration: temperature, top_p, max_tokens, frequency_penalty, presence_penalty\n- Default model: gpt-3.5-turbo (configurable)\n\n**Examples**:\n```bash\ngget gpt \"Explain CRISPR\" --api_key your_key_here\n```\n\n```python\n# Python\ngget.gpt(\"Explain CRISPR\", api_key=\"your_key_here\")\n```\n\n#### gget setup - Install Dependencies\n\nInstall/download third-party dependencies for specific modules.\n\n**Parameters**:\n- `module`: Module name requiring dependency installation\n- `-o/--out`: Output folder path (elm module only)\n\n**Modules requiring setup**:\n- `alphafold` - Downloads ~4GB of model parameters\n- `cellxgene` - Installs cellxgene-census (may not support latest Python)\n- `elm` - Downloads local ELM database\n- `gpt` - Configures OpenAI integration\n\n**Examples**:\n```bash\n# Setup AlphaFold\ngget setup alphafold\n\n# Setup ELM with custom directory\ngget setup elm -o /path/to/elm_data\n```\n\n```python\n# Python\ngget.setup(\"alphafold\")\n```\n\n## Common Workflows\n\n### Workflow 1: Gene Discovery to Sequence Analysis\n\nFind and analyze genes of interest:\n\n```python\n# 1. Search for genes\nresults = gget.search([\"GABA\", \"receptor\"], species=\"homo_sapiens\")\n\n# 2. Get detailed information\ngene_ids = results[\"ensembl_id\"].tolist()\ninfo = gget.info(gene_ids[:5])\n\n# 3. Retrieve sequences\nsequences = gget.seq(gene_ids[:5], translate=True)\n```\n\n### Workflow 2: Sequence Alignment and Structure\n\nAlign sequences and predict structures:\n\n```python\n# 1. Align multiple sequences\nalignment = gget.muscle(\"sequences.fasta\")\n\n# 2. Find similar sequences\nblast_results = gget.blast(my_sequence, database=\"swissprot\", limit=10)\n\n# 3. Predict structure\nstructure = gget.alphafold(my_sequence, plot=True)\n\n# 4. Find linear motifs\northolog_df, regex_df = gget.elm(my_sequence)\n```\n\n### Workflow 3: Gene Expression and Enrichment\n\nAnalyze expression patterns and functional enrichment:\n\n```python\n# 1. Get tissue expression\ntissue_expr = gget.archs4(\"ACE2\", which=\"tissue\")\n\n# 2. Find correlated genes\ncorrelated = gget.archs4(\"ACE2\", which=\"correlation\")\n\n# 3. Get single-cell data\nadata = gget.cellxgene(gene=[\"ACE2\"], tissue=\"lung\", cell_type=\"epithelial cell\")\n\n# 4. Perform enrichment analysis\ngene_list = correlated[\"gene_symbol\"].tolist()[:50]\nenrichment = gget.enrichr(gene_list, database=\"ontology\", plot=True)\n```\n\n### Workflow 4: Disease and Drug Analysis\n\nInvestigate disease associations and therapeutic targets:\n\n```python\n# 1. Search for genes\ngenes = gget.search([\"breast cancer\"], species=\"homo_sapiens\")\n\n# 2. Get disease associations\ndiseases = gget.opentargets(\"ENSG00000169194\", resource=\"diseases\")\n\n# 3. Get drug associations\ndrugs = gget.opentargets(\"ENSG00000169194\", resource=\"drugs\")\n\n# 4. Query cancer genomics data\nstudy_ids = gget.cbio_search([\"breast\"])\ngget.cbio_plot(study_ids[:2], [\"BRCA1\", \"BRCA2\"], stratification=\"cancer_type\")\n\n# 5. Search COSMIC for mutations\ncosmic_results = gget.cosmic(\"BRCA1\", cosmic_tsv_path=\"cosmic.tsv\")\n```\n\n### Workflow 5: Comparative Genomics\n\nCompare proteins across species:\n\n```python\n# 1. Get orthologs\northologs = gget.bgee(\"ENSG00000169194\", type=\"orthologs\")\n\n# 2. Get sequences for comparison\nhuman_seq = gget.seq(\"ENSG00000169194\", translate=True)\nmouse_seq = gget.seq(\"ENSMUSG00000026091\", translate=True)\n\n# 3. Align sequences\nalignment = gget.muscle([human_seq, mouse_seq])\n\n# 4. Compare structures\nhuman_structure = gget.pdb(\"7S7U\")\nmouse_structure = gget.alphafold(mouse_seq)\n```\n\n### Workflow 6: Building Reference Indices\n\nPrepare reference data for downstream analysis (e.g., kallisto|bustools):\n\n```bash\n# 1. List available species\ngget ref --list_species\n\n# 2. Download reference files\ngget ref -w gtf -w cdna -d homo_sapiens\n\n# 3. Build kallisto index\nkallisto index -i transcriptome.idx transcriptome.fasta\n\n# 4. Download genome for alignment\ngget ref -w dna -d homo_sapiens\n```\n\n## Best Practices\n\n### Data Retrieval\n- Use `--limit` to control result sizes for large queries\n- Save results with `-o/--out` for reproducibility\n- Check database versions/releases for consistency across analyses\n- Use `--quiet` in production scripts to reduce output\n\n### Sequence Analysis\n- For BLAST/BLAT, start with default parameters, then adjust sensitivity\n- Use `gget diamond` with `--threads` for faster local alignment\n- Save DIAMOND databases with `--diamond_db` for repeated queries\n- For multiple sequence alignment, use `-s5/--super5` for large datasets\n\n### Expression and Disease Data\n- Gene symbols are case-sensitive in cellxgene (e.g., 'PAX7' vs 'Pax7')\n- Run `gget setup` before first use of alphafold, cellxgene, elm, gpt\n- For enrichment analysis, use database shortcuts for convenience\n- Cache cBioPortal data with `-dd` to avoid repeated downloads\n\n### Structure Prediction\n- AlphaFold multimer predictions: use `-mr 20` for higher accuracy\n- Use `-r` flag for AMBER relaxation of final structures\n- Visualize results in Python with `plot=True`\n- Check PDB database first before running AlphaFold predictions\n\n### Error Handling\n- Database structures change; update gget regularly: `uv pip install --upgrade gget`\n- Process max ~1000 Ensembl IDs at once with gget info\n- For large-scale analyses, implement rate limiting for API queries\n- Use virtual environments to avoid dependency conflicts\n\n## Output Formats\n\n### Command-line\n- Default: JSON\n- CSV: Add `-csv` flag\n- FASTA: gget seq, gget mutate\n- PDB: gget pdb, gget alphafold\n- PNG: gget cbio plot\n\n### Python\n- Default: DataFrame or dictionary\n- JSON: Add `json=True` parameter\n- Save to file: Add `save=True` or specify `out=\"filename\"`\n- AnnData: gget cellxgene\n\n## Resources\n\nThis skill includes reference documentation for detailed module information:\n\n### references/\n- `module_reference.md` - Comprehensive parameter reference for all modules\n- `database_info.md` - Information about queried databases and their update frequencies\n- `workflows.md` - Extended workflow examples and use cases\n\nFor additional help:\n- Official documentation: https://pachterlab.github.io/gget/\n- GitHub issues: https://github.com/pachterlab/gget/issues\n- Citation: Luebbert, L. & Pachter, L. (2023). Efficient querying of genomic reference databases with gget. Bioinformatics. https://doi.org/10.1093/bioinformatics/btac836\n",
        "data/k-dense-ai/gtars/SKILL.md": "---\nname: gtars\ndescription: High-performance toolkit for genomic interval analysis in Rust with Python bindings. Use when working with genomic regions, BED files, coverage tracks, overlap detection, tokenization for ML models, or fragment analysis in computational genomics and machine learning applications.\n---\n\n# Gtars: Genomic Tools and Algorithms in Rust\n\n## Overview\n\nGtars is a high-performance Rust toolkit for manipulating, analyzing, and processing genomic interval data. It provides specialized tools for overlap detection, coverage analysis, tokenization for machine learning, and reference sequence management.\n\nUse this skill when working with:\n- Genomic interval files (BED format)\n- Overlap detection between genomic regions\n- Coverage track generation (WIG, BigWig)\n- Genomic ML preprocessing and tokenization\n- Fragment analysis in single-cell genomics\n- Reference sequence retrieval and validation\n\n## Installation\n\n### Python Installation\n\nInstall gtars Python bindings:\n\n```bash\nuv uv pip install gtars\n```\n\n### CLI Installation\n\nInstall command-line tools (requires Rust/Cargo):\n\n```bash\n# Install with all features\ncargo install gtars-cli --features \"uniwig overlaprs igd bbcache scoring fragsplit\"\n\n# Or install specific features only\ncargo install gtars-cli --features \"uniwig overlaprs\"\n```\n\n### Rust Library\n\nAdd to Cargo.toml for Rust projects:\n\n```toml\n[dependencies]\ngtars = { version = \"0.1\", features = [\"tokenizers\", \"overlaprs\"] }\n```\n\n## Core Capabilities\n\nGtars is organized into specialized modules, each focused on specific genomic analysis tasks:\n\n### 1. Overlap Detection and IGD Indexing\n\nEfficiently detect overlaps between genomic intervals using the Integrated Genome Database (IGD) data structure.\n\n**When to use:**\n- Finding overlapping regulatory elements\n- Variant annotation\n- Comparing ChIP-seq peaks\n- Identifying shared genomic features\n\n**Quick example:**\n```python\nimport gtars\n\n# Build IGD index and query overlaps\nigd = gtars.igd.build_index(\"regions.bed\")\noverlaps = igd.query(\"chr1\", 1000, 2000)\n```\n\nSee `references/overlap.md` for comprehensive overlap detection documentation.\n\n### 2. Coverage Track Generation\n\nGenerate coverage tracks from sequencing data with the uniwig module.\n\n**When to use:**\n- ATAC-seq accessibility profiles\n- ChIP-seq coverage visualization\n- RNA-seq read coverage\n- Differential coverage analysis\n\n**Quick example:**\n```bash\n# Generate BigWig coverage track\ngtars uniwig generate --input fragments.bed --output coverage.bw --format bigwig\n```\n\nSee `references/coverage.md` for detailed coverage analysis workflows.\n\n### 3. Genomic Tokenization\n\nConvert genomic regions into discrete tokens for machine learning applications, particularly for deep learning models on genomic data.\n\n**When to use:**\n- Preprocessing for genomic ML models\n- Integration with geniml library\n- Creating position encodings\n- Training transformer models on genomic sequences\n\n**Quick example:**\n```python\nfrom gtars.tokenizers import TreeTokenizer\n\ntokenizer = TreeTokenizer.from_bed_file(\"training_regions.bed\")\ntoken = tokenizer.tokenize(\"chr1\", 1000, 2000)\n```\n\nSee `references/tokenizers.md` for tokenization documentation.\n\n### 4. Reference Sequence Management\n\nHandle reference genome sequences and compute digests following the GA4GH refget protocol.\n\n**When to use:**\n- Validating reference genome integrity\n- Extracting specific genomic sequences\n- Computing sequence digests\n- Cross-reference comparisons\n\n**Quick example:**\n```python\n# Load reference and extract sequences\nstore = gtars.RefgetStore.from_fasta(\"hg38.fa\")\nsequence = store.get_subsequence(\"chr1\", 1000, 2000)\n```\n\nSee `references/refget.md` for reference sequence operations.\n\n### 5. Fragment Processing\n\nSplit and analyze fragment files, particularly useful for single-cell genomics data.\n\n**When to use:**\n- Processing single-cell ATAC-seq data\n- Splitting fragments by cell barcodes\n- Cluster-based fragment analysis\n- Fragment quality control\n\n**Quick example:**\n```bash\n# Split fragments by clusters\ngtars fragsplit cluster-split --input fragments.tsv --clusters clusters.txt --output-dir ./by_cluster/\n```\n\nSee `references/cli.md` for fragment processing commands.\n\n### 6. Fragment Scoring\n\nScore fragment overlaps against reference datasets.\n\n**When to use:**\n- Evaluating fragment enrichment\n- Comparing experimental data to references\n- Quality metrics computation\n- Batch scoring across samples\n\n**Quick example:**\n```bash\n# Score fragments against reference\ngtars scoring score --fragments fragments.bed --reference reference.bed --output scores.txt\n```\n\n## Common Workflows\n\n### Workflow 1: Peak Overlap Analysis\n\nIdentify overlapping genomic features:\n\n```python\nimport gtars\n\n# Load two region sets\npeaks = gtars.RegionSet.from_bed(\"chip_peaks.bed\")\npromoters = gtars.RegionSet.from_bed(\"promoters.bed\")\n\n# Find overlaps\noverlapping_peaks = peaks.filter_overlapping(promoters)\n\n# Export results\noverlapping_peaks.to_bed(\"peaks_in_promoters.bed\")\n```\n\n### Workflow 2: Coverage Track Pipeline\n\nGenerate coverage tracks for visualization:\n\n```bash\n# Step 1: Generate coverage\ngtars uniwig generate --input atac_fragments.bed --output coverage.wig --resolution 10\n\n# Step 2: Convert to BigWig for genome browsers\ngtars uniwig generate --input atac_fragments.bed --output coverage.bw --format bigwig\n```\n\n### Workflow 3: ML Preprocessing\n\nPrepare genomic data for machine learning:\n\n```python\nfrom gtars.tokenizers import TreeTokenizer\nimport gtars\n\n# Step 1: Load training regions\nregions = gtars.RegionSet.from_bed(\"training_peaks.bed\")\n\n# Step 2: Create tokenizer\ntokenizer = TreeTokenizer.from_bed_file(\"training_peaks.bed\")\n\n# Step 3: Tokenize regions\ntokens = [tokenizer.tokenize(r.chromosome, r.start, r.end) for r in regions]\n\n# Step 4: Use tokens in ML pipeline\n# (integrate with geniml or custom models)\n```\n\n## Python vs CLI Usage\n\n**Use Python API when:**\n- Integrating with analysis pipelines\n- Need programmatic control\n- Working with NumPy/Pandas\n- Building custom workflows\n\n**Use CLI when:**\n- Quick one-off analyses\n- Shell scripting\n- Batch processing files\n- Prototyping workflows\n\n## Reference Documentation\n\nComprehensive module documentation:\n\n- **`references/python-api.md`** - Complete Python API reference with RegionSet operations, NumPy integration, and data export\n- **`references/overlap.md`** - IGD indexing, overlap detection, and set operations\n- **`references/coverage.md`** - Coverage track generation with uniwig\n- **`references/tokenizers.md`** - Genomic tokenization for ML applications\n- **`references/refget.md`** - Reference sequence management and digests\n- **`references/cli.md`** - Command-line interface complete reference\n\n## Integration with geniml\n\nGtars serves as the foundation for the geniml Python package, providing core genomic interval operations for machine learning workflows. When working on geniml-related tasks, use gtars for data preprocessing and tokenization.\n\n## Performance Characteristics\n\n- **Native Rust performance**: Fast execution with low memory overhead\n- **Parallel processing**: Multi-threaded operations for large datasets\n- **Memory efficiency**: Streaming and memory-mapped file support\n- **Zero-copy operations**: NumPy integration with minimal data copying\n\n## Data Formats\n\nGtars works with standard genomic formats:\n\n- **BED**: Genomic intervals (3-column or extended)\n- **WIG/BigWig**: Coverage tracks\n- **FASTA**: Reference sequences\n- **Fragment TSV**: Single-cell fragment files with barcodes\n\n## Error Handling and Debugging\n\nEnable verbose logging for troubleshooting:\n\n```python\nimport gtars\n\n# Enable debug logging\ngtars.set_log_level(\"DEBUG\")\n```\n\n```bash\n# CLI verbose mode\ngtars --verbose <command>\n```\n",
        "data/k-dense-ai/gwas-database/SKILL.md": "---\nname: gwas-database\ndescription: \"Query NHGRI-EBI GWAS Catalog for SNP-trait associations. Search variants by rs ID, disease/trait, gene, retrieve p-values and summary statistics, for genetic epidemiology and polygenic risk scores.\"\n---\n\n# GWAS Catalog Database\n\n## Overview\n\nThe GWAS Catalog is a comprehensive repository of published genome-wide association studies maintained by the National Human Genome Research Institute (NHGRI) and the European Bioinformatics Institute (EBI). The catalog contains curated SNP-trait associations from thousands of GWAS publications, including genetic variants, associated traits and diseases, p-values, effect sizes, and full summary statistics for many studies.\n\n## When to Use This Skill\n\nThis skill should be used when queries involve:\n\n- **Genetic variant associations**: Finding SNPs associated with diseases or traits\n- **SNP lookups**: Retrieving information about specific genetic variants (rs IDs)\n- **Trait/disease searches**: Discovering genetic associations for phenotypes\n- **Gene associations**: Finding variants in or near specific genes\n- **GWAS summary statistics**: Accessing complete genome-wide association data\n- **Study metadata**: Retrieving publication and cohort information\n- **Population genetics**: Exploring ancestry-specific associations\n- **Polygenic risk scores**: Identifying variants for risk prediction models\n- **Functional genomics**: Understanding variant effects and genomic context\n- **Systematic reviews**: Comprehensive literature synthesis of genetic associations\n\n## Core Capabilities\n\n### 1. Understanding GWAS Catalog Data Structure\n\nThe GWAS Catalog is organized around four core entities:\n\n- **Studies**: GWAS publications with metadata (PMID, author, cohort details)\n- **Associations**: SNP-trait associations with statistical evidence (p  510)\n- **Variants**: Genetic markers (SNPs) with genomic coordinates and alleles\n- **Traits**: Phenotypes and diseases (mapped to EFO ontology terms)\n\n**Key Identifiers:**\n- Study accessions: `GCST` IDs (e.g., GCST001234)\n- Variant IDs: `rs` numbers (e.g., rs7903146) or `variant_id` format\n- Trait IDs: EFO terms (e.g., EFO_0001360 for type 2 diabetes)\n- Gene symbols: HGNC approved names (e.g., TCF7L2)\n\n### 2. Web Interface Searches\n\nThe web interface at https://www.ebi.ac.uk/gwas/ supports multiple search modes:\n\n**By Variant (rs ID):**\n```\nrs7903146\n```\nReturns all trait associations for this SNP.\n\n**By Disease/Trait:**\n```\ntype 2 diabetes\nParkinson disease\nbody mass index\n```\nReturns all associated genetic variants.\n\n**By Gene:**\n```\nAPOE\nTCF7L2\n```\nReturns variants in or near the gene region.\n\n**By Chromosomal Region:**\n```\n10:114000000-115000000\n```\nReturns variants in the specified genomic interval.\n\n**By Publication:**\n```\nPMID:20581827\nAuthor: McCarthy MI\nGCST001234\n```\nReturns study details and all reported associations.\n\n### 3. REST API Access\n\nThe GWAS Catalog provides two REST APIs for programmatic access:\n\n**Base URLs:**\n- GWAS Catalog API: `https://www.ebi.ac.uk/gwas/rest/api`\n- Summary Statistics API: `https://www.ebi.ac.uk/gwas/summary-statistics/api`\n\n**API Documentation:**\n- Main API docs: https://www.ebi.ac.uk/gwas/rest/docs/api\n- Summary stats docs: https://www.ebi.ac.uk/gwas/summary-statistics/docs/\n\n**Core Endpoints:**\n\n1. **Studies endpoint** - `/studies/{accessionID}`\n   ```python\n   import requests\n\n   # Get a specific study\n   url = \"https://www.ebi.ac.uk/gwas/rest/api/studies/GCST001795\"\n   response = requests.get(url, headers={\"Content-Type\": \"application/json\"})\n   study = response.json()\n   ```\n\n2. **Associations endpoint** - `/associations`\n   ```python\n   # Find associations for a variant\n   variant = \"rs7903146\"\n   url = f\"https://www.ebi.ac.uk/gwas/rest/api/singleNucleotidePolymorphisms/{variant}/associations\"\n   params = {\"projection\": \"associationBySnp\"}\n   response = requests.get(url, params=params, headers={\"Content-Type\": \"application/json\"})\n   associations = response.json()\n   ```\n\n3. **Variants endpoint** - `/singleNucleotidePolymorphisms/{rsID}`\n   ```python\n   # Get variant details\n   url = \"https://www.ebi.ac.uk/gwas/rest/api/singleNucleotidePolymorphisms/rs7903146\"\n   response = requests.get(url, headers={\"Content-Type\": \"application/json\"})\n   variant_info = response.json()\n   ```\n\n4. **Traits endpoint** - `/efoTraits/{efoID}`\n   ```python\n   # Get trait information\n   url = \"https://www.ebi.ac.uk/gwas/rest/api/efoTraits/EFO_0001360\"\n   response = requests.get(url, headers={\"Content-Type\": \"application/json\"})\n   trait_info = response.json()\n   ```\n\n### 4. Query Examples and Patterns\n\n**Example 1: Find all associations for a disease**\n```python\nimport requests\n\ntrait = \"EFO_0001360\"  # Type 2 diabetes\nbase_url = \"https://www.ebi.ac.uk/gwas/rest/api\"\n\n# Query associations for this trait\nurl = f\"{base_url}/efoTraits/{trait}/associations\"\nresponse = requests.get(url, headers={\"Content-Type\": \"application/json\"})\nassociations = response.json()\n\n# Process results\nfor assoc in associations.get('_embedded', {}).get('associations', []):\n    variant = assoc.get('rsId')\n    pvalue = assoc.get('pvalue')\n    risk_allele = assoc.get('strongestAllele')\n    print(f\"{variant}: p={pvalue}, risk allele={risk_allele}\")\n```\n\n**Example 2: Get variant information and all trait associations**\n```python\nimport requests\n\nvariant = \"rs7903146\"\nbase_url = \"https://www.ebi.ac.uk/gwas/rest/api\"\n\n# Get variant details\nurl = f\"{base_url}/singleNucleotidePolymorphisms/{variant}\"\nresponse = requests.get(url, headers={\"Content-Type\": \"application/json\"})\nvariant_data = response.json()\n\n# Get all associations for this variant\nurl = f\"{base_url}/singleNucleotidePolymorphisms/{variant}/associations\"\nparams = {\"projection\": \"associationBySnp\"}\nresponse = requests.get(url, params=params, headers={\"Content-Type\": \"application/json\"})\nassociations = response.json()\n\n# Extract trait names and p-values\nfor assoc in associations.get('_embedded', {}).get('associations', []):\n    trait = assoc.get('efoTrait')\n    pvalue = assoc.get('pvalue')\n    print(f\"Trait: {trait}, p-value: {pvalue}\")\n```\n\n**Example 3: Access summary statistics**\n```python\nimport requests\n\n# Query summary statistics API\nbase_url = \"https://www.ebi.ac.uk/gwas/summary-statistics/api\"\n\n# Find associations by trait with p-value threshold\ntrait = \"EFO_0001360\"  # Type 2 diabetes\np_upper = \"0.000000001\"  # p < 1e-9\nurl = f\"{base_url}/traits/{trait}/associations\"\nparams = {\n    \"p_upper\": p_upper,\n    \"size\": 100  # Number of results\n}\nresponse = requests.get(url, params=params)\nresults = response.json()\n\n# Process genome-wide significant hits\nfor hit in results.get('_embedded', {}).get('associations', []):\n    variant_id = hit.get('variant_id')\n    chromosome = hit.get('chromosome')\n    position = hit.get('base_pair_location')\n    pvalue = hit.get('p_value')\n    print(f\"{chromosome}:{position} ({variant_id}): p={pvalue}\")\n```\n\n**Example 4: Query by chromosomal region**\n```python\nimport requests\n\n# Find variants in a specific genomic region\nchromosome = \"10\"\nstart_pos = 114000000\nend_pos = 115000000\n\nbase_url = \"https://www.ebi.ac.uk/gwas/rest/api\"\nurl = f\"{base_url}/singleNucleotidePolymorphisms/search/findByChromBpLocationRange\"\nparams = {\n    \"chrom\": chromosome,\n    \"bpStart\": start_pos,\n    \"bpEnd\": end_pos\n}\nresponse = requests.get(url, params=params, headers={\"Content-Type\": \"application/json\"})\nvariants_in_region = response.json()\n```\n\n### 5. Working with Summary Statistics\n\nThe GWAS Catalog hosts full summary statistics for many studies, providing access to all tested variants (not just genome-wide significant hits).\n\n**Access Methods:**\n1. **FTP download**: http://ftp.ebi.ac.uk/pub/databases/gwas/summary_statistics/\n2. **REST API**: Query-based access to summary statistics\n3. **Web interface**: Browse and download via the website\n\n**Summary Statistics API Features:**\n- Filter by chromosome, position, p-value\n- Query specific variants across studies\n- Retrieve effect sizes and allele frequencies\n- Access harmonized and standardized data\n\n**Example: Download summary statistics for a study**\n```python\nimport requests\nimport gzip\n\n# Get available summary statistics\nbase_url = \"https://www.ebi.ac.uk/gwas/summary-statistics/api\"\nurl = f\"{base_url}/studies/GCST001234\"\nresponse = requests.get(url)\nstudy_info = response.json()\n\n# Download link is provided in the response\n# Alternatively, use FTP:\n# ftp://ftp.ebi.ac.uk/pub/databases/gwas/summary_statistics/GCSTXXXXXX/\n```\n\n### 6. Data Integration and Cross-referencing\n\nThe GWAS Catalog provides links to external resources:\n\n**Genomic Databases:**\n- Ensembl: Gene annotations and variant consequences\n- dbSNP: Variant identifiers and population frequencies\n- gnomAD: Population allele frequencies\n\n**Functional Resources:**\n- Open Targets: Target-disease associations\n- PGS Catalog: Polygenic risk scores\n- UCSC Genome Browser: Genomic context\n\n**Phenotype Resources:**\n- EFO (Experimental Factor Ontology): Standardized trait terms\n- OMIM: Disease gene relationships\n- Disease Ontology: Disease hierarchies\n\n**Following Links in API Responses:**\n```python\nimport requests\n\n# API responses include _links for related resources\nresponse = requests.get(\"https://www.ebi.ac.uk/gwas/rest/api/studies/GCST001234\")\nstudy = response.json()\n\n# Follow link to associations\nassociations_url = study['_links']['associations']['href']\nassociations_response = requests.get(associations_url)\n```\n\n## Query Workflows\n\n### Workflow 1: Exploring Genetic Associations for a Disease\n\n1. **Identify the trait** using EFO terms or free text:\n   - Search web interface for disease name\n   - Note the EFO ID (e.g., EFO_0001360 for type 2 diabetes)\n\n2. **Query associations via API:**\n   ```python\n   url = f\"https://www.ebi.ac.uk/gwas/rest/api/efoTraits/{efo_id}/associations\"\n   ```\n\n3. **Filter by significance and population:**\n   - Check p-values (genome-wide significant: p  510)\n   - Review ancestry information in study metadata\n   - Filter by sample size or discovery/replication status\n\n4. **Extract variant details:**\n   - rs IDs for each association\n   - Effect alleles and directions\n   - Effect sizes (odds ratios, beta coefficients)\n   - Population allele frequencies\n\n5. **Cross-reference with other databases:**\n   - Look up variant consequences in Ensembl\n   - Check population frequencies in gnomAD\n   - Explore gene function and pathways\n\n### Workflow 2: Investigating a Specific Genetic Variant\n\n1. **Query the variant:**\n   ```python\n   url = f\"https://www.ebi.ac.uk/gwas/rest/api/singleNucleotidePolymorphisms/{rs_id}\"\n   ```\n\n2. **Retrieve all trait associations:**\n   ```python\n   url = f\"https://www.ebi.ac.uk/gwas/rest/api/singleNucleotidePolymorphisms/{rs_id}/associations\"\n   ```\n\n3. **Analyze pleiotropy:**\n   - Identify all traits associated with this variant\n   - Review effect directions across traits\n   - Look for shared biological pathways\n\n4. **Check genomic context:**\n   - Determine nearby genes\n   - Identify if variant is in coding/regulatory regions\n   - Review linkage disequilibrium with other variants\n\n### Workflow 3: Gene-Centric Association Analysis\n\n1. **Search by gene symbol** in web interface or:\n   ```python\n   url = f\"https://www.ebi.ac.uk/gwas/rest/api/singleNucleotidePolymorphisms/search/findByGene\"\n   params = {\"geneName\": gene_symbol}\n   ```\n\n2. **Retrieve variants in gene region:**\n   - Get chromosomal coordinates for gene\n   - Query variants in region\n   - Include promoter and regulatory regions (extend boundaries)\n\n3. **Analyze association patterns:**\n   - Identify traits associated with variants in this gene\n   - Look for consistent associations across studies\n   - Review effect sizes and directions\n\n4. **Functional interpretation:**\n   - Determine variant consequences (missense, regulatory, etc.)\n   - Check expression QTL (eQTL) data\n   - Review pathway and network context\n\n### Workflow 4: Systematic Review of Genetic Evidence\n\n1. **Define research question:**\n   - Specific trait or disease of interest\n   - Population considerations\n   - Study design requirements\n\n2. **Comprehensive variant extraction:**\n   - Query all associations for trait\n   - Set significance threshold\n   - Note discovery and replication studies\n\n3. **Quality assessment:**\n   - Review study sample sizes\n   - Check for population diversity\n   - Assess heterogeneity across studies\n   - Identify potential biases\n\n4. **Data synthesis:**\n   - Aggregate associations across studies\n   - Perform meta-analysis if applicable\n   - Create summary tables\n   - Generate Manhattan or forest plots\n\n5. **Export and documentation:**\n   - Download full association data\n   - Export summary statistics if needed\n   - Document search strategy and date\n   - Create reproducible analysis scripts\n\n### Workflow 5: Accessing and Analyzing Summary Statistics\n\n1. **Identify studies with summary statistics:**\n   - Browse summary statistics portal\n   - Check FTP directory listings\n   - Query API for available studies\n\n2. **Download summary statistics:**\n   ```bash\n   # Via FTP\n   wget ftp://ftp.ebi.ac.uk/pub/databases/gwas/summary_statistics/GCSTXXXXXX/harmonised/GCSTXXXXXX-harmonised.tsv.gz\n   ```\n\n3. **Query via API for specific variants:**\n   ```python\n   url = f\"https://www.ebi.ac.uk/gwas/summary-statistics/api/chromosomes/{chrom}/associations\"\n   params = {\"start\": start_pos, \"end\": end_pos}\n   ```\n\n4. **Process and analyze:**\n   - Filter by p-value thresholds\n   - Extract effect sizes and confidence intervals\n   - Perform downstream analyses (fine-mapping, colocalization, etc.)\n\n## Response Formats and Data Fields\n\n**Key Fields in Association Records:**\n- `rsId`: Variant identifier (rs number)\n- `strongestAllele`: Risk allele for the association\n- `pvalue`: Association p-value\n- `pvalueText`: P-value as text (may include inequality)\n- `orPerCopyNum`: Odds ratio or beta coefficient\n- `betaNum`: Effect size (for quantitative traits)\n- `betaUnit`: Unit of measurement for beta\n- `range`: Confidence interval\n- `efoTrait`: Associated trait name\n- `mappedLabel`: EFO-mapped trait term\n\n**Study Metadata Fields:**\n- `accessionId`: GCST study identifier\n- `pubmedId`: PubMed ID\n- `author`: First author\n- `publicationDate`: Publication date\n- `ancestryInitial`: Discovery population ancestry\n- `ancestryReplication`: Replication population ancestry\n- `sampleSize`: Total sample size\n\n**Pagination:**\nResults are paginated (default 20 items per page). Navigate using:\n- `size` parameter: Number of results per page\n- `page` parameter: Page number (0-indexed)\n- `_links` in response: URLs for next/previous pages\n\n## Best Practices\n\n### Query Strategy\n- Start with web interface to identify relevant EFO terms and study accessions\n- Use API for bulk data extraction and automated analyses\n- Implement pagination handling for large result sets\n- Cache API responses to minimize redundant requests\n\n### Data Interpretation\n- Always check p-value thresholds (genome-wide: 510)\n- Review ancestry information for population applicability\n- Consider sample size when assessing evidence strength\n- Check for replication across independent studies\n- Be aware of winner's curse in effect size estimates\n\n### Rate Limiting and Ethics\n- Respect API usage guidelines (no excessive requests)\n- Use summary statistics downloads for genome-wide analyses\n- Implement appropriate delays between API calls\n- Cache results locally when performing iterative analyses\n- Cite the GWAS Catalog in publications\n\n### Data Quality Considerations\n- GWAS Catalog curates published associations (may contain inconsistencies)\n- Effect sizes reported as published (may need harmonization)\n- Some studies report conditional or joint associations\n- Check for study overlap when combining results\n- Be aware of ascertainment and selection biases\n\n## Python Integration Example\n\nComplete workflow for querying and analyzing GWAS data:\n\n```python\nimport requests\nimport pandas as pd\nfrom time import sleep\n\ndef query_gwas_catalog(trait_id, p_threshold=5e-8):\n    \"\"\"\n    Query GWAS Catalog for trait associations\n\n    Args:\n        trait_id: EFO trait identifier (e.g., 'EFO_0001360')\n        p_threshold: P-value threshold for filtering\n\n    Returns:\n        pandas DataFrame with association results\n    \"\"\"\n    base_url = \"https://www.ebi.ac.uk/gwas/rest/api\"\n    url = f\"{base_url}/efoTraits/{trait_id}/associations\"\n\n    headers = {\"Content-Type\": \"application/json\"}\n    results = []\n    page = 0\n\n    while True:\n        params = {\"page\": page, \"size\": 100}\n        response = requests.get(url, params=params, headers=headers)\n\n        if response.status_code != 200:\n            break\n\n        data = response.json()\n        associations = data.get('_embedded', {}).get('associations', [])\n\n        if not associations:\n            break\n\n        for assoc in associations:\n            pvalue = assoc.get('pvalue')\n            if pvalue and float(pvalue) <= p_threshold:\n                results.append({\n                    'variant': assoc.get('rsId'),\n                    'pvalue': pvalue,\n                    'risk_allele': assoc.get('strongestAllele'),\n                    'or_beta': assoc.get('orPerCopyNum') or assoc.get('betaNum'),\n                    'trait': assoc.get('efoTrait'),\n                    'pubmed_id': assoc.get('pubmedId')\n                })\n\n        page += 1\n        sleep(0.1)  # Rate limiting\n\n    return pd.DataFrame(results)\n\n# Example usage\ndf = query_gwas_catalog('EFO_0001360')  # Type 2 diabetes\nprint(df.head())\nprint(f\"\\nTotal associations: {len(df)}\")\nprint(f\"Unique variants: {df['variant'].nunique()}\")\n```\n\n## Resources\n\n### references/api_reference.md\n\nComprehensive API documentation including:\n- Detailed endpoint specifications for both APIs\n- Complete list of query parameters and filters\n- Response format specifications and field descriptions\n- Advanced query examples and patterns\n- Error handling and troubleshooting\n- Integration with external databases\n\nConsult this reference when:\n- Constructing complex API queries\n- Understanding response structures\n- Implementing pagination or batch operations\n- Troubleshooting API errors\n- Exploring advanced filtering options\n\n### Training Materials\n\nThe GWAS Catalog team provides workshop materials:\n- GitHub repository: https://github.com/EBISPOT/GWAS_Catalog-workshop\n- Jupyter notebooks with example queries\n- Google Colab integration for cloud execution\n\n## Important Notes\n\n### Data Updates\n- The GWAS Catalog is updated regularly with new publications\n- Re-run queries periodically for comprehensive coverage\n- Summary statistics are added as studies release data\n- EFO mappings may be updated over time\n\n### Citation Requirements\nWhen using GWAS Catalog data, cite:\n- Sollis E, et al. (2023) The NHGRI-EBI GWAS Catalog: knowledgebase and deposition resource. Nucleic Acids Research. PMID: 37953337\n- Include access date and version when available\n- Cite original studies when discussing specific findings\n\n### Limitations\n- Not all GWAS publications are included (curation criteria apply)\n- Full summary statistics available for subset of studies\n- Effect sizes may require harmonization across studies\n- Population diversity is growing but historically limited\n- Some associations represent conditional or joint effects\n\n### Data Access\n- Web interface: Free, no registration required\n- REST APIs: Free, no API key needed\n- FTP downloads: Open access\n- Rate limiting applies to API (be respectful)\n\n## Additional Resources\n\n- **GWAS Catalog website**: https://www.ebi.ac.uk/gwas/\n- **Documentation**: https://www.ebi.ac.uk/gwas/docs\n- **API documentation**: https://www.ebi.ac.uk/gwas/rest/docs/api\n- **Summary Statistics API**: https://www.ebi.ac.uk/gwas/summary-statistics/docs/\n- **FTP site**: http://ftp.ebi.ac.uk/pub/databases/gwas/\n- **Training materials**: https://github.com/EBISPOT/GWAS_Catalog-workshop\n- **PGS Catalog** (polygenic scores): https://www.pgscatalog.org/\n- **Help and support**: gwas-info@ebi.ac.uk\n",
        "data/k-dense-ai/histolab/SKILL.md": "---\nname: histolab\ndescription: Digital pathology image processing toolkit for whole slide images (WSI). Use this skill when working with histopathology slides, processing H&E or IHC stained tissue images, extracting tiles from gigapixel pathology images, detecting tissue regions, segmenting tissue masks, or preparing datasets for computational pathology deep learning pipelines. Applies to WSI formats (SVS, TIFF, NDPI), tile-based analysis, and histological image preprocessing workflows.\n---\n\n# Histolab\n\n## Overview\n\nHistolab is a Python library for processing whole slide images (WSI) in digital pathology. It automates tissue detection, extracts informative tiles from gigapixel images, and prepares datasets for deep learning pipelines. The library handles multiple WSI formats, implements sophisticated tissue segmentation, and provides flexible tile extraction strategies.\n\n## Installation\n\n```bash\nuv pip install histolab\n```\n\n## Quick Start\n\nBasic workflow for extracting tiles from a whole slide image:\n\n```python\nfrom histolab.slide import Slide\nfrom histolab.tiler import RandomTiler\n\n# Load slide\nslide = Slide(\"slide.svs\", processed_path=\"output/\")\n\n# Configure tiler\ntiler = RandomTiler(\n    tile_size=(512, 512),\n    n_tiles=100,\n    level=0,\n    seed=42\n)\n\n# Preview tile locations\ntiler.locate_tiles(slide, n_tiles=20)\n\n# Extract tiles\ntiler.extract(slide)\n```\n\n## Core Capabilities\n\n### 1. Slide Management\n\nLoad, inspect, and work with whole slide images in various formats.\n\n**Common operations:**\n- Loading WSI files (SVS, TIFF, NDPI, etc.)\n- Accessing slide metadata (dimensions, magnification, properties)\n- Generating thumbnails for visualization\n- Working with pyramidal image structures\n- Extracting regions at specific coordinates\n\n**Key classes:** `Slide`\n\n**Reference:** `references/slide_management.md` contains comprehensive documentation on:\n- Slide initialization and configuration\n- Built-in sample datasets (prostate, ovarian, breast, heart, kidney tissues)\n- Accessing slide properties and metadata\n- Thumbnail generation and visualization\n- Working with pyramid levels\n- Multi-slide processing workflows\n\n**Example workflow:**\n```python\nfrom histolab.slide import Slide\nfrom histolab.data import prostate_tissue\n\n# Load sample data\nprostate_svs, prostate_path = prostate_tissue()\n\n# Initialize slide\nslide = Slide(prostate_path, processed_path=\"output/\")\n\n# Inspect properties\nprint(f\"Dimensions: {slide.dimensions}\")\nprint(f\"Levels: {slide.levels}\")\nprint(f\"Magnification: {slide.properties.get('openslide.objective-power')}\")\n\n# Save thumbnail\nslide.save_thumbnail()\n```\n\n### 2. Tissue Detection and Masks\n\nAutomatically identify tissue regions and filter background/artifacts.\n\n**Common operations:**\n- Creating binary tissue masks\n- Detecting largest tissue region\n- Excluding background and artifacts\n- Custom tissue segmentation\n- Removing pen annotations\n\n**Key classes:** `TissueMask`, `BiggestTissueBoxMask`, `BinaryMask`\n\n**Reference:** `references/tissue_masks.md` contains comprehensive documentation on:\n- TissueMask: Segments all tissue regions using automated filters\n- BiggestTissueBoxMask: Returns bounding box of largest tissue region (default)\n- BinaryMask: Base class for custom mask implementations\n- Visualizing masks with `locate_mask()`\n- Creating custom rectangular and annotation-exclusion masks\n- Mask integration with tile extraction\n- Best practices and troubleshooting\n\n**Example workflow:**\n```python\nfrom histolab.masks import TissueMask, BiggestTissueBoxMask\n\n# Create tissue mask for all tissue regions\ntissue_mask = TissueMask()\n\n# Visualize mask on slide\nslide.locate_mask(tissue_mask)\n\n# Get mask array\nmask_array = tissue_mask(slide)\n\n# Use largest tissue region (default for most extractors)\nbiggest_mask = BiggestTissueBoxMask()\n```\n\n**When to use each mask:**\n- `TissueMask`: Multiple tissue sections, comprehensive analysis\n- `BiggestTissueBoxMask`: Single main tissue section, exclude artifacts (default)\n- Custom `BinaryMask`: Specific ROI, exclude annotations, custom segmentation\n\n### 3. Tile Extraction\n\nExtract smaller regions from large WSI using different strategies.\n\n**Three extraction strategies:**\n\n**RandomTiler:** Extract fixed number of randomly positioned tiles\n- Best for: Sampling diverse regions, exploratory analysis, training data\n- Key parameters: `n_tiles`, `seed` for reproducibility\n\n**GridTiler:** Systematically extract tiles across tissue in grid pattern\n- Best for: Complete coverage, spatial analysis, reconstruction\n- Key parameters: `pixel_overlap` for sliding windows\n\n**ScoreTiler:** Extract top-ranked tiles based on scoring functions\n- Best for: Most informative regions, quality-driven selection\n- Key parameters: `scorer` (NucleiScorer, CellularityScorer, custom)\n\n**Common parameters:**\n- `tile_size`: Tile dimensions (e.g., (512, 512))\n- `level`: Pyramid level for extraction (0 = highest resolution)\n- `check_tissue`: Filter tiles by tissue content\n- `tissue_percent`: Minimum tissue coverage (default 80%)\n- `extraction_mask`: Mask defining extraction region\n\n**Reference:** `references/tile_extraction.md` contains comprehensive documentation on:\n- Detailed explanation of each tiler strategy\n- Available scorers (NucleiScorer, CellularityScorer, custom)\n- Tile preview with `locate_tiles()`\n- Extraction workflows and reporting\n- Advanced patterns (multi-level, hierarchical extraction)\n- Performance optimization and troubleshooting\n\n**Example workflows:**\n\n```python\nfrom histolab.tiler import RandomTiler, GridTiler, ScoreTiler\nfrom histolab.scorer import NucleiScorer\n\n# Random sampling (fast, diverse)\nrandom_tiler = RandomTiler(\n    tile_size=(512, 512),\n    n_tiles=100,\n    level=0,\n    seed=42,\n    check_tissue=True,\n    tissue_percent=80.0\n)\nrandom_tiler.extract(slide)\n\n# Grid coverage (comprehensive)\ngrid_tiler = GridTiler(\n    tile_size=(512, 512),\n    level=0,\n    pixel_overlap=0,\n    check_tissue=True\n)\ngrid_tiler.extract(slide)\n\n# Score-based selection (most informative)\nscore_tiler = ScoreTiler(\n    tile_size=(512, 512),\n    n_tiles=50,\n    scorer=NucleiScorer(),\n    level=0\n)\nscore_tiler.extract(slide, report_path=\"tiles_report.csv\")\n```\n\n**Always preview before extracting:**\n```python\n# Preview tile locations on thumbnail\ntiler.locate_tiles(slide, n_tiles=20)\n```\n\n### 4. Filters and Preprocessing\n\nApply image processing filters for tissue detection, quality control, and preprocessing.\n\n**Filter categories:**\n\n**Image Filters:** Color space conversions, thresholding, contrast enhancement\n- `RgbToGrayscale`, `RgbToHsv`, `RgbToHed`\n- `OtsuThreshold`, `AdaptiveThreshold`\n- `StretchContrast`, `HistogramEqualization`\n\n**Morphological Filters:** Structural operations on binary images\n- `BinaryDilation`, `BinaryErosion`\n- `BinaryOpening`, `BinaryClosing`\n- `RemoveSmallObjects`, `RemoveSmallHoles`\n\n**Composition:** Chain multiple filters together\n- `Compose`: Create filter pipelines\n\n**Reference:** `references/filters_preprocessing.md` contains comprehensive documentation on:\n- Detailed explanation of each filter type\n- Filter composition and chaining\n- Common preprocessing pipelines (tissue detection, pen removal, nuclei enhancement)\n- Applying filters to tiles\n- Custom mask filters\n- Quality control filters (blur detection, tissue coverage)\n- Best practices and troubleshooting\n\n**Example workflows:**\n\n```python\nfrom histolab.filters.compositions import Compose\nfrom histolab.filters.image_filters import RgbToGrayscale, OtsuThreshold\nfrom histolab.filters.morphological_filters import (\n    BinaryDilation, RemoveSmallHoles, RemoveSmallObjects\n)\n\n# Standard tissue detection pipeline\ntissue_detection = Compose([\n    RgbToGrayscale(),\n    OtsuThreshold(),\n    BinaryDilation(disk_size=5),\n    RemoveSmallHoles(area_threshold=1000),\n    RemoveSmallObjects(area_threshold=500)\n])\n\n# Use with custom mask\nfrom histolab.masks import TissueMask\ncustom_mask = TissueMask(filters=tissue_detection)\n\n# Apply filters to tile\nfrom histolab.tile import Tile\nfiltered_tile = tile.apply_filters(tissue_detection)\n```\n\n### 5. Visualization\n\nVisualize slides, masks, tile locations, and extraction quality.\n\n**Common visualization tasks:**\n- Displaying slide thumbnails\n- Visualizing tissue masks\n- Previewing tile locations\n- Assessing tile quality\n- Creating reports and figures\n\n**Reference:** `references/visualization.md` contains comprehensive documentation on:\n- Slide thumbnail display and saving\n- Mask visualization with `locate_mask()`\n- Tile location preview with `locate_tiles()`\n- Displaying extracted tiles and mosaics\n- Quality assessment (score distributions, top vs bottom tiles)\n- Multi-slide visualization\n- Filter effect visualization\n- Exporting high-resolution figures and PDF reports\n- Interactive visualization in Jupyter notebooks\n\n**Example workflows:**\n\n```python\nimport matplotlib.pyplot as plt\nfrom histolab.masks import TissueMask\n\n# Display slide thumbnail\nplt.figure(figsize=(10, 10))\nplt.imshow(slide.thumbnail)\nplt.title(f\"Slide: {slide.name}\")\nplt.axis('off')\nplt.show()\n\n# Visualize tissue mask\ntissue_mask = TissueMask()\nslide.locate_mask(tissue_mask)\n\n# Preview tile locations\ntiler = RandomTiler(tile_size=(512, 512), n_tiles=50)\ntiler.locate_tiles(slide, n_tiles=20)\n\n# Display extracted tiles in grid\nfrom pathlib import Path\nfrom PIL import Image\n\ntile_paths = list(Path(\"output/tiles/\").glob(\"*.png\"))[:16]\nfig, axes = plt.subplots(4, 4, figsize=(12, 12))\naxes = axes.ravel()\n\nfor idx, tile_path in enumerate(tile_paths):\n    tile_img = Image.open(tile_path)\n    axes[idx].imshow(tile_img)\n    axes[idx].set_title(tile_path.stem, fontsize=8)\n    axes[idx].axis('off')\n\nplt.tight_layout()\nplt.show()\n```\n\n## Typical Workflows\n\n### Workflow 1: Exploratory Tile Extraction\n\nQuick sampling of diverse tissue regions for initial analysis.\n\n```python\nfrom histolab.slide import Slide\nfrom histolab.tiler import RandomTiler\nimport logging\n\n# Enable logging for progress tracking\nlogging.basicConfig(level=logging.INFO)\n\n# Load slide\nslide = Slide(\"slide.svs\", processed_path=\"output/random_tiles/\")\n\n# Inspect slide\nprint(f\"Dimensions: {slide.dimensions}\")\nprint(f\"Levels: {slide.levels}\")\nslide.save_thumbnail()\n\n# Configure random tiler\nrandom_tiler = RandomTiler(\n    tile_size=(512, 512),\n    n_tiles=100,\n    level=0,\n    seed=42,\n    check_tissue=True,\n    tissue_percent=80.0\n)\n\n# Preview locations\nrandom_tiler.locate_tiles(slide, n_tiles=20)\n\n# Extract tiles\nrandom_tiler.extract(slide)\n```\n\n### Workflow 2: Comprehensive Grid Extraction\n\nComplete tissue coverage for whole-slide analysis.\n\n```python\nfrom histolab.slide import Slide\nfrom histolab.tiler import GridTiler\nfrom histolab.masks import TissueMask\n\n# Load slide\nslide = Slide(\"slide.svs\", processed_path=\"output/grid_tiles/\")\n\n# Use TissueMask for all tissue sections\ntissue_mask = TissueMask()\nslide.locate_mask(tissue_mask)\n\n# Configure grid tiler\ngrid_tiler = GridTiler(\n    tile_size=(512, 512),\n    level=1,  # Use level 1 for faster extraction\n    pixel_overlap=0,\n    check_tissue=True,\n    tissue_percent=70.0\n)\n\n# Preview grid\ngrid_tiler.locate_tiles(slide)\n\n# Extract all tiles\ngrid_tiler.extract(slide, extraction_mask=tissue_mask)\n```\n\n### Workflow 3: Quality-Driven Tile Selection\n\nExtract most informative tiles based on nuclei density.\n\n```python\nfrom histolab.slide import Slide\nfrom histolab.tiler import ScoreTiler\nfrom histolab.scorer import NucleiScorer\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load slide\nslide = Slide(\"slide.svs\", processed_path=\"output/scored_tiles/\")\n\n# Configure score tiler\nscore_tiler = ScoreTiler(\n    tile_size=(512, 512),\n    n_tiles=50,\n    level=0,\n    scorer=NucleiScorer(),\n    check_tissue=True\n)\n\n# Preview top tiles\nscore_tiler.locate_tiles(slide, n_tiles=15)\n\n# Extract with report\nscore_tiler.extract(slide, report_path=\"tiles_report.csv\")\n\n# Analyze scores\nreport_df = pd.read_csv(\"tiles_report.csv\")\nplt.hist(report_df['score'], bins=20, edgecolor='black')\nplt.xlabel('Tile Score')\nplt.ylabel('Frequency')\nplt.title('Distribution of Tile Scores')\nplt.show()\n```\n\n### Workflow 4: Multi-Slide Processing Pipeline\n\nProcess entire slide collection with consistent parameters.\n\n```python\nfrom pathlib import Path\nfrom histolab.slide import Slide\nfrom histolab.tiler import RandomTiler\nimport logging\n\nlogging.basicConfig(level=logging.INFO)\n\n# Configure tiler once\ntiler = RandomTiler(\n    tile_size=(512, 512),\n    n_tiles=50,\n    level=0,\n    seed=42,\n    check_tissue=True\n)\n\n# Process all slides\nslide_dir = Path(\"slides/\")\noutput_base = Path(\"output/\")\n\nfor slide_path in slide_dir.glob(\"*.svs\"):\n    print(f\"\\nProcessing: {slide_path.name}\")\n\n    # Create slide-specific output directory\n    output_dir = output_base / slide_path.stem\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    # Load and process slide\n    slide = Slide(slide_path, processed_path=output_dir)\n\n    # Save thumbnail for review\n    slide.save_thumbnail()\n\n    # Extract tiles\n    tiler.extract(slide)\n\n    print(f\"Completed: {slide_path.name}\")\n```\n\n### Workflow 5: Custom Tissue Detection and Filtering\n\nHandle slides with artifacts, annotations, or unusual staining.\n\n```python\nfrom histolab.slide import Slide\nfrom histolab.masks import TissueMask\nfrom histolab.tiler import RandomTiler\nfrom histolab.filters.compositions import Compose\nfrom histolab.filters.image_filters import RgbToGrayscale, OtsuThreshold\nfrom histolab.filters.morphological_filters import (\n    BinaryDilation, RemoveSmallObjects, RemoveSmallHoles\n)\n\n# Define custom filter pipeline for aggressive artifact removal\naggressive_filters = Compose([\n    RgbToGrayscale(),\n    OtsuThreshold(),\n    BinaryDilation(disk_size=10),\n    RemoveSmallHoles(area_threshold=5000),\n    RemoveSmallObjects(area_threshold=3000)  # Remove larger artifacts\n])\n\n# Create custom mask\ncustom_mask = TissueMask(filters=aggressive_filters)\n\n# Load slide and visualize mask\nslide = Slide(\"slide.svs\", processed_path=\"output/\")\nslide.locate_mask(custom_mask)\n\n# Extract with custom mask\ntiler = RandomTiler(tile_size=(512, 512), n_tiles=100)\ntiler.extract(slide, extraction_mask=custom_mask)\n```\n\n## Best Practices\n\n### Slide Loading and Inspection\n1. Always inspect slide properties before processing\n2. Save thumbnails for quick visual review\n3. Check pyramid levels and dimensions\n4. Verify tissue is present using thumbnails\n\n### Tissue Detection\n1. Preview masks with `locate_mask()` before extraction\n2. Use `TissueMask` for multiple sections, `BiggestTissueBoxMask` for single sections\n3. Customize filters for specific stains (H&E vs IHC)\n4. Handle pen annotations with custom masks\n5. Test masks on diverse slides\n\n### Tile Extraction\n1. **Always preview with `locate_tiles()` before extracting**\n2. Choose appropriate tiler:\n   - RandomTiler: Sampling and exploration\n   - GridTiler: Complete coverage\n   - ScoreTiler: Quality-driven selection\n3. Set appropriate `tissue_percent` threshold (70-90% typical)\n4. Use seeds for reproducibility in RandomTiler\n5. Extract at appropriate pyramid level for analysis resolution\n6. Enable logging for large datasets\n\n### Performance\n1. Extract at lower levels (1, 2) for faster processing\n2. Use `BiggestTissueBoxMask` over `TissueMask` when appropriate\n3. Adjust `tissue_percent` to reduce invalid tile attempts\n4. Limit `n_tiles` for initial exploration\n5. Use `pixel_overlap=0` for non-overlapping grids\n\n### Quality Control\n1. Validate tile quality (check for blur, artifacts, focus)\n2. Review score distributions for ScoreTiler\n3. Inspect top and bottom scoring tiles\n4. Monitor tissue coverage statistics\n5. Filter extracted tiles by additional quality metrics if needed\n\n## Common Use Cases\n\n### Training Deep Learning Models\n- Extract balanced datasets using RandomTiler across multiple slides\n- Use ScoreTiler with NucleiScorer to focus on cell-rich regions\n- Extract at consistent resolution (level 0 or level 1)\n- Generate CSV reports for tracking tile metadata\n\n### Whole Slide Analysis\n- Use GridTiler for complete tissue coverage\n- Extract at multiple pyramid levels for hierarchical analysis\n- Maintain spatial relationships with grid positions\n- Use `pixel_overlap` for sliding window approaches\n\n### Tissue Characterization\n- Sample diverse regions with RandomTiler\n- Quantify tissue coverage with masks\n- Extract stain-specific information with HED decomposition\n- Compare tissue patterns across slides\n\n### Quality Assessment\n- Identify optimal focus regions with ScoreTiler\n- Detect artifacts using custom masks and filters\n- Assess staining quality across slide collection\n- Flag problematic slides for manual review\n\n### Dataset Curation\n- Use ScoreTiler to prioritize informative tiles\n- Filter tiles by tissue percentage\n- Generate reports with tile scores and metadata\n- Create stratified datasets across slides and tissue types\n\n## Troubleshooting\n\n### No tiles extracted\n- Lower `tissue_percent` threshold\n- Verify slide contains tissue (check thumbnail)\n- Ensure extraction_mask captures tissue regions\n- Check tile_size is appropriate for slide resolution\n\n### Many background tiles\n- Enable `check_tissue=True`\n- Increase `tissue_percent` threshold\n- Use appropriate mask (TissueMask vs BiggestTissueBoxMask)\n- Customize mask filters to better detect tissue\n\n### Extraction very slow\n- Extract at lower pyramid level (level=1 or 2)\n- Reduce `n_tiles` for RandomTiler/ScoreTiler\n- Use RandomTiler instead of GridTiler for sampling\n- Use BiggestTissueBoxMask instead of TissueMask\n\n### Tiles have artifacts\n- Implement custom annotation-exclusion masks\n- Adjust filter parameters for artifact removal\n- Increase small object removal threshold\n- Apply post-extraction quality filtering\n\n### Inconsistent results across slides\n- Use same seed for RandomTiler\n- Normalize staining with preprocessing filters\n- Adjust `tissue_percent` per staining quality\n- Implement slide-specific mask customization\n\n## Resources\n\nThis skill includes detailed reference documentation in the `references/` directory:\n\n### references/slide_management.md\nComprehensive guide to loading, inspecting, and working with whole slide images:\n- Slide initialization and configuration\n- Built-in sample datasets\n- Slide properties and metadata\n- Thumbnail generation and visualization\n- Working with pyramid levels\n- Multi-slide processing workflows\n- Best practices and common patterns\n\n### references/tissue_masks.md\nComplete documentation on tissue detection and masking:\n- TissueMask, BiggestTissueBoxMask, BinaryMask classes\n- How tissue detection filters work\n- Customizing masks with filter chains\n- Visualizing masks\n- Creating custom rectangular and annotation-exclusion masks\n- Integration with tile extraction\n- Best practices and troubleshooting\n\n### references/tile_extraction.md\nDetailed explanation of tile extraction strategies:\n- RandomTiler, GridTiler, ScoreTiler comparison\n- Available scorers (NucleiScorer, CellularityScorer, custom)\n- Common and strategy-specific parameters\n- Tile preview with locate_tiles()\n- Extraction workflows and CSV reporting\n- Advanced patterns (multi-level, hierarchical)\n- Performance optimization\n- Troubleshooting common issues\n\n### references/filters_preprocessing.md\nComplete filter reference and preprocessing guide:\n- Image filters (color conversion, thresholding, contrast)\n- Morphological filters (dilation, erosion, opening, closing)\n- Filter composition and chaining\n- Common preprocessing pipelines\n- Applying filters to tiles\n- Custom mask filters\n- Quality control filters\n- Best practices and troubleshooting\n\n### references/visualization.md\nComprehensive visualization guide:\n- Slide thumbnail display and saving\n- Mask visualization techniques\n- Tile location preview\n- Displaying extracted tiles and creating mosaics\n- Quality assessment visualizations\n- Multi-slide comparison\n- Filter effect visualization\n- Exporting high-resolution figures and PDFs\n- Interactive visualization in Jupyter notebooks\n\n**Usage pattern:** Reference files contain in-depth information to support workflows described in this main skill document. Load specific reference files as needed for detailed implementation guidance, troubleshooting, or advanced features.\n",
        "data/k-dense-ai/hmdb-database/SKILL.md": "---\nname: hmdb-database\ndescription: \"Access Human Metabolome Database (220K+ metabolites). Search by name/ID/structure, retrieve chemical properties, biomarker data, NMR/MS spectra, pathways, for metabolomics and identification.\"\n---\n\n# HMDB Database\n\n## Overview\n\nThe Human Metabolome Database (HMDB) is a comprehensive, freely available resource containing detailed information about small molecule metabolites found in the human body.\n\n## When to Use This Skill\n\nThis skill should be used when performing metabolomics research, clinical chemistry, biomarker discovery, or metabolite identification tasks.\n\n## Database Contents\n\nHMDB version 5.0 (current as of 2025) contains:\n\n- **220,945 metabolite entries** covering both water-soluble and lipid-soluble compounds\n- **8,610 protein sequences** for enzymes and transporters involved in metabolism\n- **130+ data fields per metabolite** including:\n  - Chemical properties (structure, formula, molecular weight, InChI, SMILES)\n  - Clinical data (biomarker associations, diseases, normal/abnormal concentrations)\n  - Biological information (pathways, reactions, locations)\n  - Spectroscopic data (NMR, MS, MS-MS spectra)\n  - External database links (KEGG, PubChem, MetaCyc, ChEBI, PDB, UniProt, GenBank)\n\n## Core Capabilities\n\n### 1. Web-Based Metabolite Searches\n\nAccess HMDB through the web interface at https://www.hmdb.ca/ for:\n\n**Text Searches:**\n- Search by metabolite name, synonym, or identifier (HMDB ID)\n- Example HMDB IDs: HMDB0000001, HMDB0001234\n- Search by disease associations or pathway involvement\n- Query by biological specimen type (urine, serum, CSF, saliva, feces, sweat)\n\n**Structure-Based Searches:**\n- Use ChemQuery for structure and substructure searches\n- Search by molecular weight or molecular weight range\n- Use SMILES or InChI strings to find compounds\n\n**Spectral Searches:**\n- LC-MS spectral matching\n- GC-MS spectral matching\n- NMR spectral searches for metabolite identification\n\n**Advanced Searches:**\n- Combine multiple criteria (name, properties, concentration ranges)\n- Filter by biological locations or specimen types\n- Search by protein/enzyme associations\n\n### 2. Accessing Metabolite Information\n\nWhen retrieving metabolite data, HMDB provides:\n\n**Chemical Information:**\n- Systematic name, traditional names, and synonyms\n- Chemical formula and molecular weight\n- Structure representations (2D/3D, SMILES, InChI, MOL file)\n- Chemical taxonomy and classification\n\n**Biological Context:**\n- Metabolic pathways and reactions\n- Associated enzymes and transporters\n- Subcellular locations\n- Biological roles and functions\n\n**Clinical Relevance:**\n- Normal concentration ranges in biological fluids\n- Biomarker associations with diseases\n- Clinical significance\n- Toxicity information when applicable\n\n**Analytical Data:**\n- Experimental and predicted NMR spectra\n- MS and MS-MS spectra\n- Retention times and chromatographic data\n- Reference peaks for identification\n\n### 3. Downloadable Datasets\n\nHMDB offers bulk data downloads at https://www.hmdb.ca/downloads in multiple formats:\n\n**Available Formats:**\n- **XML**: Complete metabolite, protein, and spectra data\n- **SDF**: Metabolite structure files for cheminformatics\n- **FASTA**: Protein and gene sequences\n- **TXT**: Raw spectra peak lists\n- **CSV/TSV**: Tabular data exports\n\n**Dataset Categories:**\n- All metabolites or filtered by specimen type\n- Protein/enzyme sequences\n- Experimental and predicted spectra (NMR, GC-MS, MS-MS)\n- Pathway information\n\n**Best Practices:**\n- Download XML format for comprehensive data including all fields\n- Use SDF format for structure-based analysis and cheminformatics workflows\n- Parse CSV/TSV formats for integration with data analysis pipelines\n- Check version dates to ensure up-to-date data (current: v5.0, 2023-07-01)\n\n**Usage Requirements:**\n- Free for academic and non-commercial research\n- Commercial use requires explicit permission (contact samackay@ualberta.ca)\n- Cite HMDB publication when using data\n\n### 4. Programmatic API Access\n\n**API Availability:**\nHMDB does not provide a public REST API. Programmatic access requires contacting the development team:\n\n- **Academic/Research groups:** Contact eponine@ualberta.ca (Eponine) or samackay@ualberta.ca (Scott)\n- **Commercial organizations:** Contact samackay@ualberta.ca (Scott) for customized API access\n\n**Alternative Programmatic Access:**\n- **R/Bioconductor**: Use the `hmdbQuery` package for R-based queries\n  - Install: `BiocManager::install(\"hmdbQuery\")`\n  - Provides HTTP-based querying functions\n- **Downloaded datasets**: Parse XML or CSV files locally for programmatic analysis\n- **Web scraping**: Not recommended; contact team for proper API access instead\n\n### 5. Common Research Workflows\n\n**Metabolite Identification in Untargeted Metabolomics:**\n1. Obtain experimental MS or NMR spectra from samples\n2. Use HMDB spectral search tools to match against reference spectra\n3. Verify candidates by checking molecular weight, retention time, and MS-MS fragmentation\n4. Review biological plausibility (expected in specimen type, known pathways)\n\n**Biomarker Discovery:**\n1. Search HMDB for metabolites associated with disease of interest\n2. Review concentration ranges in normal vs. disease states\n3. Identify metabolites with strong differential abundance\n4. Examine pathway context and biological mechanisms\n5. Cross-reference with literature via PubMed links\n\n**Pathway Analysis:**\n1. Identify metabolites of interest from experimental data\n2. Look up HMDB entries for each metabolite\n3. Extract pathway associations and enzymatic reactions\n4. Use linked SMPDB (Small Molecule Pathway Database) for pathway diagrams\n5. Identify pathway enrichment for biological interpretation\n\n**Database Integration:**\n1. Download HMDB data in XML or CSV format\n2. Parse and extract relevant fields for local database\n3. Link with external IDs (KEGG, PubChem, ChEBI) for cross-database queries\n4. Build local tools or pipelines incorporating HMDB reference data\n\n## Related HMDB Resources\n\nThe HMDB ecosystem includes related databases:\n\n- **DrugBank**: ~2,832 drug compounds with pharmaceutical information\n- **T3DB (Toxin and Toxin Target Database)**: ~3,670 toxic compounds\n- **SMPDB (Small Molecule Pathway Database)**: Pathway diagrams and maps\n- **FooDB**: ~70,000 food component compounds\n\nThese databases share similar structure and identifiers, enabling integrated queries across human metabolome, drug, toxin, and food databases.\n\n## Best Practices\n\n**Data Quality:**\n- Verify metabolite identifications with multiple evidence types (spectra, structure, properties)\n- Check experimental vs. predicted data quality indicators\n- Review citations and evidence for biomarker associations\n\n**Version Tracking:**\n- Note HMDB version used in research (current: v5.0)\n- Databases are updated periodically with new entries and corrections\n- Re-query for updates when publishing to ensure current information\n\n**Citation:**\n- Always cite HMDB in publications using the database\n- Reference specific HMDB IDs when discussing metabolites\n- Acknowledge data sources for downloaded datasets\n\n**Performance:**\n- For large-scale analysis, download complete datasets rather than repeated web queries\n- Use appropriate file formats (XML for comprehensive data, CSV for tabular analysis)\n- Consider local caching of frequently accessed metabolite information\n\n## Reference Documentation\n\nSee `references/hmdb_data_fields.md` for detailed information about available data fields and their meanings.\n",
        "data/k-dense-ai/hypogenic/SKILL.md": "---\nname: hypogenic\ndescription: Automated hypothesis generation and testing using large language models. Use this skill when generating scientific hypotheses from datasets, combining literature insights with empirical data, testing hypotheses against observational data, or conducting systematic hypothesis exploration for research discovery in domains like deception detection, AI content detection, mental health analysis, or other empirical research tasks.\n---\n\n# Hypogenic\n\n## Overview\n\nHypogenic provides automated hypothesis generation and testing using large language models to accelerate scientific discovery. The framework supports three approaches: HypoGeniC (data-driven hypothesis generation), HypoRefine (synergistic literature and data integration), and Union methods (mechanistic combination of literature and data-driven hypotheses).\n\n## Quick Start\n\nGet started with Hypogenic in minutes:\n\n```bash\n# Install the package\nuv pip install hypogenic\n\n# Clone example datasets\ngit clone https://github.com/ChicagoHAI/HypoGeniC-datasets.git ./data\n\n# Run basic hypothesis generation\nhypogenic_generation --config ./data/your_task/config.yaml --method hypogenic --num_hypotheses 20\n\n# Run inference on generated hypotheses\nhypogenic_inference --config ./data/your_task/config.yaml --hypotheses output/hypotheses.json\n```\n\n**Or use Python API:**\n\n```python\nfrom hypogenic import BaseTask\n\n# Create task with your configuration\ntask = BaseTask(config_path=\"./data/your_task/config.yaml\")\n\n# Generate hypotheses\ntask.generate_hypotheses(method=\"hypogenic\", num_hypotheses=20)\n\n# Run inference\nresults = task.inference(hypothesis_bank=\"./output/hypotheses.json\")\n```\n\n## When to Use This Skill\n\nUse this skill when working on:\n- Generating scientific hypotheses from observational datasets\n- Testing multiple competing hypotheses systematically\n- Combining literature insights with empirical patterns\n- Accelerating research discovery through automated hypothesis ideation\n- Domains requiring hypothesis-driven analysis: deception detection, AI-generated content identification, mental health indicators, predictive modeling, or other empirical research\n\n## Key Features\n\n**Automated Hypothesis Generation**\n- Generate 10-20+ testable hypotheses from data in minutes\n- Iterative refinement based on validation performance\n- Support for both API-based (OpenAI, Anthropic) and local LLMs\n\n**Literature Integration**\n- Extract insights from research papers via PDF processing\n- Combine theoretical foundations with empirical patterns\n- Systematic literature-to-hypothesis pipeline with GROBID\n\n**Performance Optimization**\n- Redis caching reduces API costs for repeated experiments\n- Parallel processing for large-scale hypothesis testing\n- Adaptive refinement focuses on challenging examples\n\n**Flexible Configuration**\n- Template-based prompt engineering with variable injection\n- Custom label extraction for domain-specific tasks\n- Modular architecture for easy extension\n\n**Proven Results**\n- 8.97% improvement over few-shot baselines\n- 15.75% improvement over literature-only approaches\n- 80-84% hypothesis diversity (non-redundant insights)\n- Human evaluators report significant decision-making improvements\n\n## Core Capabilities\n\n### 1. HypoGeniC: Data-Driven Hypothesis Generation\n\nGenerate hypotheses solely from observational data through iterative refinement.\n\n**Process:**\n1. Initialize with a small data subset to generate candidate hypotheses\n2. Iteratively refine hypotheses based on performance\n3. Replace poorly-performing hypotheses with new ones from challenging examples\n\n**Best for:** Exploratory research without existing literature, pattern discovery in novel datasets\n\n### 2. HypoRefine: Literature and Data Integration\n\nSynergistically combine existing literature with empirical data through an agentic framework.\n\n**Process:**\n1. Extract insights from relevant research papers (typically 10 papers)\n2. Generate theory-grounded hypotheses from literature\n3. Generate data-driven hypotheses from observational patterns\n4. Refine both hypothesis banks through iterative improvement\n\n**Best for:** Research with established theoretical foundations, validating or extending existing theories\n\n### 3. Union Methods\n\nMechanistically combine literature-only hypotheses with framework outputs.\n\n**Variants:**\n- **Literature  HypoGeniC**: Combines literature hypotheses with data-driven generation\n- **Literature  HypoRefine**: Combines literature hypotheses with integrated approach\n\n**Best for:** Comprehensive hypothesis coverage, eliminating redundancy while maintaining diverse perspectives\n\n## Installation\n\nInstall via pip:\n```bash\nuv pip install hypogenic\n```\n\n**Optional dependencies:**\n- **Redis server** (port 6832): Enables caching of LLM responses to significantly reduce API costs during iterative hypothesis generation\n- **s2orc-doc2json**: Required for processing literature PDFs in HypoRefine workflows\n- **GROBID**: Required for PDF preprocessing (see Literature Processing section)\n\n**Clone example datasets:**\n```bash\n# For HypoGeniC examples\ngit clone https://github.com/ChicagoHAI/HypoGeniC-datasets.git ./data\n\n# For HypoRefine/Union examples\ngit clone https://github.com/ChicagoHAI/Hypothesis-agent-datasets.git ./data\n```\n\n## Dataset Format\n\nDatasets must follow HuggingFace datasets format with specific naming conventions:\n\n**Required files:**\n- `<TASK>_train.json`: Training data\n- `<TASK>_val.json`: Validation data  \n- `<TASK>_test.json`: Test data\n\n**Required keys in JSON:**\n- `text_features_1` through `text_features_n`: Lists of strings containing feature values\n- `label`: List of strings containing ground truth labels\n\n**Example (headline click prediction):**\n```json\n{\n  \"headline_1\": [\n    \"What Up, Comet? You Just Got *PROBED*\",\n    \"Scientists Made a Breakthrough in Quantum Computing\"\n  ],\n  \"headline_2\": [\n    \"Scientists Everywhere Were Holding Their Breath Today. Here's Why.\",\n    \"New Quantum Computer Achieves Milestone\"\n  ],\n  \"label\": [\n    \"Headline 2 has more clicks than Headline 1\",\n    \"Headline 1 has more clicks than Headline 2\"\n  ]\n}\n```\n\n**Important notes:**\n- All lists must have the same length\n- Label format must match your `extract_label()` function output format\n- Feature keys can be customized to match your domain (e.g., `review_text`, `post_content`, etc.)\n\n## Configuration\n\nEach task requires a `config.yaml` file specifying:\n\n**Required elements:**\n- Dataset paths (train/val/test)\n- Prompt templates for:\n  - Observations generation\n  - Batched hypothesis generation\n  - Hypothesis inference\n  - Relevance checking\n  - Adaptive methods (for HypoRefine)\n\n**Template capabilities:**\n- Dataset placeholders for dynamic variable injection (e.g., `${text_features_1}`, `${num_hypotheses}`)\n- Custom label extraction functions for domain-specific parsing\n- Role-based prompt structure (system, user, assistant roles)\n\n**Configuration structure:**\n```yaml\ntask_name: your_task_name\n\ntrain_data_path: ./your_task_train.json\nval_data_path: ./your_task_val.json\ntest_data_path: ./your_task_test.json\n\nprompt_templates:\n  # Extra keys for reusable prompt components\n  observations: |\n    Feature 1: ${text_features_1}\n    Feature 2: ${text_features_2}\n    Observation: ${label}\n  \n  # Required templates\n  batched_generation:\n    system: \"Your system prompt here\"\n    user: \"Your user prompt with ${num_hypotheses} placeholder\"\n  \n  inference:\n    system: \"Your inference system prompt\"\n    user: \"Your inference user prompt\"\n  \n  # Optional templates for advanced features\n  few_shot_baseline: {...}\n  is_relevant: {...}\n  adaptive_inference: {...}\n  adaptive_selection: {...}\n```\n\nRefer to `references/config_template.yaml` for a complete example configuration.\n\n## Literature Processing (HypoRefine/Union Methods)\n\nTo use literature-based hypothesis generation, you must preprocess PDF papers:\n\n**Step 1: Setup GROBID** (first time only)\n```bash\nbash ./modules/setup_grobid.sh\n```\n\n**Step 2: Add PDF files**\nPlace research papers in `literature/YOUR_TASK_NAME/raw/`\n\n**Step 3: Process PDFs**\n```bash\n# Start GROBID service\nbash ./modules/run_grobid.sh\n\n# Process PDFs for your task\ncd examples\npython pdf_preprocess.py --task_name YOUR_TASK_NAME\n```\n\nThis converts PDFs to structured format for hypothesis extraction. Automated literature search will be supported in future releases.\n\n## CLI Usage\n\n### Hypothesis Generation\n\n```bash\nhypogenic_generation --help\n```\n\n**Key parameters:**\n- Task configuration file path\n- Model selection (API-based or local)\n- Generation method (HypoGeniC, HypoRefine, or Union)\n- Number of hypotheses to generate\n- Output directory for hypothesis banks\n\n### Hypothesis Inference\n\n```bash\nhypogenic_inference --help\n```\n\n**Key parameters:**\n- Task configuration file path\n- Hypothesis bank file path\n- Test dataset path\n- Inference method (default or multi-hypothesis)\n- Output file for results\n\n## Python API Usage\n\nFor programmatic control and custom workflows, use Hypogenic directly in your Python code:\n\n### Basic HypoGeniC Generation\n\n```python\nfrom hypogenic import BaseTask\n\n# Clone example datasets first\n# git clone https://github.com/ChicagoHAI/HypoGeniC-datasets.git ./data\n\n# Load your task with custom extract_label function\ntask = BaseTask(\n    config_path=\"./data/your_task/config.yaml\",\n    extract_label=lambda text: extract_your_label(text)\n)\n\n# Generate hypotheses\ntask.generate_hypotheses(\n    method=\"hypogenic\",\n    num_hypotheses=20,\n    output_path=\"./output/hypotheses.json\"\n)\n\n# Run inference\nresults = task.inference(\n    hypothesis_bank=\"./output/hypotheses.json\",\n    test_data=\"./data/your_task/your_task_test.json\"\n)\n```\n\n### HypoRefine/Union Methods\n\n```python\n# For literature-integrated approaches\n# git clone https://github.com/ChicagoHAI/Hypothesis-agent-datasets.git ./data\n\n# Generate with HypoRefine\ntask.generate_hypotheses(\n    method=\"hyporefine\",\n    num_hypotheses=15,\n    literature_path=\"./literature/your_task/\",\n    output_path=\"./output/\"\n)\n# This generates 3 hypothesis banks:\n# - HypoRefine (integrated approach)\n# - Literature-only hypotheses\n# - LiteratureHypoRefine (union)\n```\n\n### Multi-Hypothesis Inference\n\n```python\nfrom examples.multi_hyp_inference import run_multi_hypothesis_inference\n\n# Test multiple hypotheses simultaneously\nresults = run_multi_hypothesis_inference(\n    config_path=\"./data/your_task/config.yaml\",\n    hypothesis_bank=\"./output/hypotheses.json\",\n    test_data=\"./data/your_task/your_task_test.json\"\n)\n```\n\n### Custom Label Extraction\n\nThe `extract_label()` function is critical for parsing LLM outputs. Implement it based on your task:\n\n```python\ndef extract_label(llm_output: str) -> str:\n    \"\"\"Extract predicted label from LLM inference text.\n    \n    Default behavior: searches for 'final answer:\\s+(.*)' pattern.\n    Customize for your domain-specific output format.\n    \"\"\"\n    import re\n    match = re.search(r'final answer:\\s+(.*)', llm_output, re.IGNORECASE)\n    if match:\n        return match.group(1).strip()\n    return llm_output.strip()\n```\n\n**Important:** Extracted labels must match the format of `label` values in your dataset for correct accuracy calculation.\n\n## Workflow Examples\n\n### Example 1: Data-Driven Hypothesis Generation (HypoGeniC)\n\n**Scenario:** Detecting AI-generated content without prior theoretical framework\n\n**Steps:**\n1. Prepare dataset with text samples and labels (human vs. AI-generated)\n2. Create `config.yaml` with appropriate prompt templates\n3. Run hypothesis generation:\n   ```bash\n   hypogenic_generation --config config.yaml --method hypogenic --num_hypotheses 20\n   ```\n4. Run inference on test set:\n   ```bash\n   hypogenic_inference --config config.yaml --hypotheses output/hypotheses.json --test_data data/test.json\n   ```\n5. Analyze results for patterns like formality, grammatical precision, and tone differences\n\n### Example 2: Literature-Informed Hypothesis Testing (HypoRefine)\n\n**Scenario:** Deception detection in hotel reviews building on existing research\n\n**Steps:**\n1. Collect 10 relevant papers on linguistic deception cues\n2. Prepare dataset with genuine and fraudulent reviews\n3. Configure `config.yaml` with literature processing and data generation templates\n4. Run HypoRefine:\n   ```bash\n   hypogenic_generation --config config.yaml --method hyporefine --papers papers/ --num_hypotheses 15\n   ```\n5. Test hypotheses examining pronoun frequency, detail specificity, and other linguistic patterns\n6. Compare literature-based and data-driven hypothesis performance\n\n### Example 3: Comprehensive Hypothesis Coverage (Union Method)\n\n**Scenario:** Mental stress detection maximizing hypothesis diversity\n\n**Steps:**\n1. Generate literature hypotheses from mental health research papers\n2. Generate data-driven hypotheses from social media posts\n3. Run Union method to combine and deduplicate:\n   ```bash\n   hypogenic_generation --config config.yaml --method union --literature_hypotheses lit_hyp.json\n   ```\n4. Inference captures both theoretical constructs (posting behavior changes) and data patterns (emotional language shifts)\n\n## Performance Optimization\n\n**Caching:** Enable Redis caching to reduce API costs and computation time for repeated LLM calls\n\n**Parallel Processing:** Leverage multiple workers for large-scale hypothesis generation and testing\n\n**Adaptive Refinement:** Use challenging examples to iteratively improve hypothesis quality\n\n## Expected Outcomes\n\nResearch using hypogenic has demonstrated:\n- 14.19% accuracy improvement in AI-content detection tasks\n- 7.44% accuracy improvement in deception detection tasks\n- 80-84% of hypothesis pairs offering distinct, non-redundant insights\n- High helpfulness ratings from human evaluators across multiple research domains\n\n## Troubleshooting\n\n**Issue:** Generated hypotheses are too generic\n**Solution:** Refine prompt templates in `config.yaml` to request more specific, testable hypotheses\n\n**Issue:** Poor inference performance\n**Solution:** Ensure dataset has sufficient training examples, adjust hypothesis generation parameters, or increase number of hypotheses\n\n**Issue:** Label extraction failures\n**Solution:** Implement custom `extract_label()` function for domain-specific output parsing\n\n**Issue:** GROBID PDF processing fails\n**Solution:** Ensure GROBID service is running (`bash ./modules/run_grobid.sh`) and PDFs are valid research papers\n\n## Creating Custom Tasks\n\nTo add a new task or dataset to Hypogenic:\n\n### Step 1: Prepare Your Dataset\n\nCreate three JSON files following the required format:\n- `your_task_train.json`\n- `your_task_val.json`\n- `your_task_test.json`\n\nEach file must have keys for text features (`text_features_1`, etc.) and `label`.\n\n### Step 2: Create config.yaml\n\nDefine your task configuration with:\n- Task name and dataset paths\n- Prompt templates for observations, generation, inference\n- Any extra keys for reusable prompt components\n- Placeholder variables (e.g., `${text_features_1}`, `${num_hypotheses}`)\n\n### Step 3: Implement extract_label Function\n\nCreate a custom label extraction function that parses LLM outputs for your domain:\n\n```python\nfrom hypogenic import BaseTask\n\ndef extract_my_label(llm_output: str) -> str:\n    \"\"\"Custom label extraction for your task.\n    \n    Must return labels in same format as dataset 'label' field.\n    \"\"\"\n    # Example: Extract from specific format\n    if \"Final prediction:\" in llm_output:\n        return llm_output.split(\"Final prediction:\")[-1].strip()\n    \n    # Fallback to default pattern\n    import re\n    match = re.search(r'final answer:\\s+(.*)', llm_output, re.IGNORECASE)\n    return match.group(1).strip() if match else llm_output.strip()\n\n# Use your custom task\ntask = BaseTask(\n    config_path=\"./your_task/config.yaml\",\n    extract_label=extract_my_label\n)\n```\n\n### Step 4: (Optional) Process Literature\n\nFor HypoRefine/Union methods:\n1. Create `literature/your_task_name/raw/` directory\n2. Add relevant research paper PDFs\n3. Run GROBID preprocessing\n4. Process with `pdf_preprocess.py`\n\n### Step 5: Generate and Test\n\nRun hypothesis generation and inference using CLI or Python API:\n\n```bash\n# CLI approach\nhypogenic_generation --config your_task/config.yaml --method hypogenic --num_hypotheses 20\nhypogenic_inference --config your_task/config.yaml --hypotheses output/hypotheses.json\n\n# Or use Python API (see Python API Usage section)\n```\n\n## Repository Structure\n\nUnderstanding the repository layout:\n\n```\nhypothesis-generation/\n hypogenic/              # Core package code\n hypogenic_cmd/          # CLI entry points\n hypothesis_agent/       # HypoRefine agent framework\n literature/            # Literature processing utilities\n modules/               # GROBID and preprocessing modules\n examples/              # Example scripts\n    generation.py      # Basic HypoGeniC generation\n    union_generation.py # HypoRefine/Union generation\n    inference.py       # Single hypothesis inference\n    multi_hyp_inference.py # Multiple hypothesis inference\n    pdf_preprocess.py  # Literature PDF processing\n data/                  # Example datasets (clone separately)\n tests/                 # Unit tests\n IO_prompting/          # Prompt templates and experiments\n```\n\n**Key directories:**\n- **hypogenic/**: Main package with BaseTask and generation logic\n- **examples/**: Reference implementations for common workflows\n- **literature/**: Tools for PDF processing and literature extraction\n- **modules/**: External tool integrations (GROBID, etc.)\n\n## Related Publications\n\n### HypoBench (2025)\n\nLiu, H., Huang, S., Hu, J., Zhou, Y., & Tan, C. (2025). HypoBench: Towards Systematic and Principled Benchmarking for Hypothesis Generation. arXiv preprint arXiv:2504.11524.\n\n- **Paper:** https://arxiv.org/abs/2504.11524\n- **Description:** Benchmarking framework for systematic evaluation of hypothesis generation methods\n\n**BibTeX:**\n```bibtex\n@misc{liu2025hypobenchsystematicprincipledbenchmarking,\n      title={HypoBench: Towards Systematic and Principled Benchmarking for Hypothesis Generation}, \n      author={Haokun Liu and Sicong Huang and Jingyu Hu and Yangqiaoyu Zhou and Chenhao Tan},\n      year={2025},\n      eprint={2504.11524},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI},\n      url={https://arxiv.org/abs/2504.11524}, \n}\n```\n\n### Literature Meets Data (2024)\n\nLiu, H., Zhou, Y., Li, M., Yuan, C., & Tan, C. (2024). Literature Meets Data: A Synergistic Approach to Hypothesis Generation. arXiv preprint arXiv:2410.17309.\n\n- **Paper:** https://arxiv.org/abs/2410.17309\n- **Code:** https://github.com/ChicagoHAI/hypothesis-generation\n- **Description:** Introduces HypoRefine and demonstrates synergistic combination of literature-based and data-driven hypothesis generation\n\n**BibTeX:**\n```bibtex\n@misc{liu2024literaturemeetsdatasynergistic,\n      title={Literature Meets Data: A Synergistic Approach to Hypothesis Generation}, \n      author={Haokun Liu and Yangqiaoyu Zhou and Mingxuan Li and Chenfei Yuan and Chenhao Tan},\n      year={2024},\n      eprint={2410.17309},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI},\n      url={https://arxiv.org/abs/2410.17309}, \n}\n```\n\n### Hypothesis Generation with Large Language Models (2024)\n\nZhou, Y., Liu, H., Srivastava, T., Mei, H., & Tan, C. (2024). Hypothesis Generation with Large Language Models. In Proceedings of EMNLP Workshop of NLP for Science.\n\n- **Paper:** https://aclanthology.org/2024.nlp4science-1.10/\n- **Description:** Original HypoGeniC framework for data-driven hypothesis generation\n\n**BibTeX:**\n```bibtex\n@inproceedings{zhou2024hypothesisgenerationlargelanguage,\n      title={Hypothesis Generation with Large Language Models}, \n      author={Yangqiaoyu Zhou and Haokun Liu and Tejes Srivastava and Hongyuan Mei and Chenhao Tan},\n      booktitle = {Proceedings of EMNLP Workshop of NLP for Science},\n      year={2024},\n      url={https://aclanthology.org/2024.nlp4science-1.10/},\n}\n```\n\n## Additional Resources\n\n### Official Links\n\n- **GitHub Repository:** https://github.com/ChicagoHAI/hypothesis-generation\n- **PyPI Package:** https://pypi.org/project/hypogenic/\n- **License:** MIT License\n- **Issues & Support:** https://github.com/ChicagoHAI/hypothesis-generation/issues\n\n### Example Datasets\n\nClone these repositories for ready-to-use examples:\n\n```bash\n# HypoGeniC examples (data-driven only)\ngit clone https://github.com/ChicagoHAI/HypoGeniC-datasets.git ./data\n\n# HypoRefine/Union examples (literature + data)\ngit clone https://github.com/ChicagoHAI/Hypothesis-agent-datasets.git ./data\n```\n\n### Community & Contributions\n\n- **Contributors:** 7+ active contributors\n- **Stars:** 89+ on GitHub\n- **Topics:** research-tool, interpretability, hypothesis-generation, scientific-discovery, llm-application\n\nFor contributions or questions, visit the GitHub repository and check the issues page.\n\n## Local Resources\n\n### references/\n\n`config_template.yaml` - Complete example configuration file with all required prompt templates and parameters. This includes:\n- Full YAML structure for task configuration\n- Example prompt templates for all methods\n- Placeholder variable documentation\n- Role-based prompt examples\n\n### scripts/\n\nScripts directory is available for:\n- Custom data preparation utilities\n- Format conversion tools\n- Analysis and evaluation scripts\n- Integration with external tools\n\n### assets/\n\nAssets directory is available for:\n- Example datasets and templates\n- Sample hypothesis banks\n- Visualization outputs\n- Documentation supplements\n",
        "data/k-dense-ai/hypothesis-generation/SKILL.md": "---\nname: hypothesis-generation\ndescription: \"Generate testable hypotheses. Formulate from observations, design experiments, explore competing explanations, develop predictions, propose mechanisms, for scientific inquiry across domains.\"\n---\n\n# Scientific Hypothesis Generation\n\n## Overview\n\nHypothesis generation is a systematic process for developing testable explanations. Formulate evidence-based hypotheses from observations, design experiments, explore competing explanations, and develop predictions. Apply this skill for scientific inquiry across domains.\n\n## When to Use This Skill\n\nThis skill should be used when:\n- Developing hypotheses from observations or preliminary data\n- Designing experiments to test scientific questions\n- Exploring competing explanations for phenomena\n- Formulating testable predictions for research\n- Conducting literature-based hypothesis generation\n- Planning mechanistic studies across scientific domains\n\n## Workflow\n\nFollow this systematic process to generate robust scientific hypotheses:\n\n### 1. Understand the Phenomenon\n\nStart by clarifying the observation, question, or phenomenon that requires explanation:\n\n- Identify the core observation or pattern that needs explanation\n- Define the scope and boundaries of the phenomenon\n- Note any constraints or specific contexts\n- Clarify what is already known vs. what is uncertain\n- Identify the relevant scientific domain(s)\n\n### 2. Conduct Comprehensive Literature Search\n\nSearch existing scientific literature to ground hypotheses in current evidence. Use both PubMed (for biomedical topics) and general web search (for broader scientific domains):\n\n**For biomedical topics:**\n- Use WebFetch with PubMed URLs to access relevant literature\n- Search for recent reviews, meta-analyses, and primary research\n- Look for similar phenomena, related mechanisms, or analogous systems\n\n**For all scientific domains:**\n- Use WebSearch to find recent papers, preprints, and reviews\n- Search for established theories, mechanisms, or frameworks\n- Identify gaps in current understanding\n\n**Search strategy:**\n- Begin with broad searches to understand the landscape\n- Narrow to specific mechanisms, pathways, or theories\n- Look for contradictory findings or unresolved debates\n- Consult `references/literature_search_strategies.md` for detailed search techniques\n\n### 3. Synthesize Existing Evidence\n\nAnalyze and integrate findings from literature search:\n\n- Summarize current understanding of the phenomenon\n- Identify established mechanisms or theories that may apply\n- Note conflicting evidence or alternative viewpoints\n- Recognize gaps, limitations, or unanswered questions\n- Identify analogies from related systems or domains\n\n### 4. Generate Competing Hypotheses\n\nDevelop 3-5 distinct hypotheses that could explain the phenomenon. Each hypothesis should:\n\n- Provide a mechanistic explanation (not just description)\n- Be distinguishable from other hypotheses\n- Draw on evidence from the literature synthesis\n- Consider different levels of explanation (molecular, cellular, systemic, population, etc.)\n\n**Strategies for generating hypotheses:**\n- Apply known mechanisms from analogous systems\n- Consider multiple causative pathways\n- Explore different scales of explanation\n- Question assumptions in existing explanations\n- Combine mechanisms in novel ways\n\n### 5. Evaluate Hypothesis Quality\n\nAssess each hypothesis against established quality criteria from `references/hypothesis_quality_criteria.md`:\n\n**Testability:** Can the hypothesis be empirically tested?\n**Falsifiability:** What observations would disprove it?\n**Parsimony:** Is it the simplest explanation that fits the evidence?\n**Explanatory Power:** How much of the phenomenon does it explain?\n**Scope:** What range of observations does it cover?\n**Consistency:** Does it align with established principles?\n**Novelty:** Does it offer new insights beyond existing explanations?\n\nExplicitly note the strengths and weaknesses of each hypothesis.\n\n### 6. Design Experimental Tests\n\nFor each viable hypothesis, propose specific experiments or studies to test it. Consult `references/experimental_design_patterns.md` for common approaches:\n\n**Experimental design elements:**\n- What would be measured or observed?\n- What comparisons or controls are needed?\n- What methods or techniques would be used?\n- What sample sizes or statistical approaches are appropriate?\n- What are potential confounds and how to address them?\n\n**Consider multiple approaches:**\n- Laboratory experiments (in vitro, in vivo, computational)\n- Observational studies (cross-sectional, longitudinal, case-control)\n- Clinical trials (if applicable)\n- Natural experiments or quasi-experimental designs\n\n### 7. Formulate Testable Predictions\n\nFor each hypothesis, generate specific, quantitative predictions:\n\n- State what should be observed if the hypothesis is correct\n- Specify expected direction and magnitude of effects when possible\n- Identify conditions under which predictions should hold\n- Distinguish predictions between competing hypotheses\n- Note predictions that would falsify the hypothesis\n\n### 8. Present Structured Output\n\nUse the template in `assets/hypothesis_output_template.md` to present hypotheses in a clear, consistent format:\n\n**Standard structure:**\n1. **Background & Context** - Phenomenon and literature summary\n2. **Competing Hypotheses** - Enumerated hypotheses with mechanistic explanations\n3. **Quality Assessment** - Evaluation of each hypothesis\n4. **Experimental Designs** - Proposed tests for each hypothesis\n5. **Testable Predictions** - Specific, measurable predictions\n6. **Critical Comparisons** - How to distinguish between hypotheses\n\n## Quality Standards\n\nEnsure all generated hypotheses meet these standards:\n\n- **Evidence-based:** Grounded in existing literature with citations\n- **Testable:** Include specific, measurable predictions\n- **Mechanistic:** Explain how/why, not just what\n- **Comprehensive:** Consider alternative explanations\n- **Rigorous:** Include experimental designs to test predictions\n\n## Resources\n\n### references/\n\n- `hypothesis_quality_criteria.md` - Framework for evaluating hypothesis quality (testability, falsifiability, parsimony, explanatory power, scope, consistency)\n- `experimental_design_patterns.md` - Common experimental approaches across domains (RCTs, observational studies, lab experiments, computational models)\n- `literature_search_strategies.md` - Effective search techniques for PubMed and general scientific sources\n\n### assets/\n\n- `hypothesis_output_template.md` - Structured format for presenting hypotheses consistently with all required sections\n",
        "data/k-dense-ai/kegg-database/SKILL.md": "---\nname: kegg-database\ndescription: \"Direct REST API access to KEGG (academic use only). Pathway analysis, gene-pathway mapping, metabolic pathways, drug interactions, ID conversion. For Python workflows with multiple databases, prefer bioservices. Use this for direct HTTP/REST work or KEGG-specific control.\"\n---\n\n# KEGG Database\n\n## Overview\n\nKEGG (Kyoto Encyclopedia of Genes and Genomes) is a comprehensive bioinformatics resource for biological pathway analysis and molecular interaction networks.\n\n**Important**: KEGG API is made available only for academic use by academic users.\n\n## When to Use This Skill\n\nThis skill should be used when querying pathways, genes, compounds, enzymes, diseases, and drugs across multiple organisms using KEGG's REST API.\n\n## Quick Start\n\nThe skill provides:\n1. Python helper functions (`scripts/kegg_api.py`) for all KEGG REST API operations\n2. Comprehensive reference documentation (`references/kegg_reference.md`) with detailed API specifications\n\nWhen users request KEGG data, determine which operation is needed and use the appropriate function from `scripts/kegg_api.py`.\n\n## Core Operations\n\n### 1. Database Information (`kegg_info`)\n\nRetrieve metadata and statistics about KEGG databases.\n\n**When to use**: Understanding database structure, checking available data, getting release information.\n\n**Usage**:\n```python\nfrom scripts.kegg_api import kegg_info\n\n# Get pathway database info\ninfo = kegg_info('pathway')\n\n# Get organism-specific info\nhsa_info = kegg_info('hsa')  # Human genome\n```\n\n**Common databases**: `kegg`, `pathway`, `module`, `brite`, `genes`, `genome`, `compound`, `glycan`, `reaction`, `enzyme`, `disease`, `drug`\n\n### 2. Listing Entries (`kegg_list`)\n\nList entry identifiers and names from KEGG databases.\n\n**When to use**: Getting all pathways for an organism, listing genes, retrieving compound catalogs.\n\n**Usage**:\n```python\nfrom scripts.kegg_api import kegg_list\n\n# List all reference pathways\npathways = kegg_list('pathway')\n\n# List human-specific pathways\nhsa_pathways = kegg_list('pathway', 'hsa')\n\n# List specific genes (max 10)\ngenes = kegg_list('hsa:10458+hsa:10459')\n```\n\n**Common organism codes**: `hsa` (human), `mmu` (mouse), `dme` (fruit fly), `sce` (yeast), `eco` (E. coli)\n\n### 3. Searching (`kegg_find`)\n\nSearch KEGG databases by keywords or molecular properties.\n\n**When to use**: Finding genes by name/description, searching compounds by formula or mass, discovering entries by keywords.\n\n**Usage**:\n```python\nfrom scripts.kegg_api import kegg_find\n\n# Keyword search\nresults = kegg_find('genes', 'p53')\nshiga_toxin = kegg_find('genes', 'shiga toxin')\n\n# Chemical formula search (exact match)\ncompounds = kegg_find('compound', 'C7H10N4O2', 'formula')\n\n# Molecular weight range search\ndrugs = kegg_find('drug', '300-310', 'exact_mass')\n```\n\n**Search options**: `formula` (exact match), `exact_mass` (range), `mol_weight` (range)\n\n### 4. Retrieving Entries (`kegg_get`)\n\nGet complete database entries or specific data formats.\n\n**When to use**: Retrieving pathway details, getting gene/protein sequences, downloading pathway maps, accessing compound structures.\n\n**Usage**:\n```python\nfrom scripts.kegg_api import kegg_get\n\n# Get pathway entry\npathway = kegg_get('hsa00010')  # Glycolysis pathway\n\n# Get multiple entries (max 10)\ngenes = kegg_get(['hsa:10458', 'hsa:10459'])\n\n# Get protein sequence (FASTA)\nsequence = kegg_get('hsa:10458', 'aaseq')\n\n# Get nucleotide sequence\nnt_seq = kegg_get('hsa:10458', 'ntseq')\n\n# Get compound structure\nmol_file = kegg_get('cpd:C00002', 'mol')  # ATP in MOL format\n\n# Get pathway as JSON (single entry only)\npathway_json = kegg_get('hsa05130', 'json')\n\n# Get pathway image (single entry only)\npathway_img = kegg_get('hsa05130', 'image')\n```\n\n**Output formats**: `aaseq` (protein FASTA), `ntseq` (nucleotide FASTA), `mol` (MOL format), `kcf` (KCF format), `image` (PNG), `kgml` (XML), `json` (pathway JSON)\n\n**Important**: Image, KGML, and JSON formats allow only one entry at a time.\n\n### 5. ID Conversion (`kegg_conv`)\n\nConvert identifiers between KEGG and external databases.\n\n**When to use**: Integrating KEGG data with other databases, mapping gene IDs, converting compound identifiers.\n\n**Usage**:\n```python\nfrom scripts.kegg_api import kegg_conv\n\n# Convert all human genes to NCBI Gene IDs\nconversions = kegg_conv('ncbi-geneid', 'hsa')\n\n# Convert specific gene\ngene_id = kegg_conv('ncbi-geneid', 'hsa:10458')\n\n# Convert to UniProt\nuniprot_id = kegg_conv('uniprot', 'hsa:10458')\n\n# Convert compounds to PubChem\npubchem_ids = kegg_conv('pubchem', 'compound')\n\n# Reverse conversion (NCBI Gene ID to KEGG)\nkegg_id = kegg_conv('hsa', 'ncbi-geneid')\n```\n\n**Supported conversions**: `ncbi-geneid`, `ncbi-proteinid`, `uniprot`, `pubchem`, `chebi`\n\n### 6. Cross-Referencing (`kegg_link`)\n\nFind related entries within and between KEGG databases.\n\n**When to use**: Finding pathways containing genes, getting genes in a pathway, mapping genes to KO groups, finding compounds in pathways.\n\n**Usage**:\n```python\nfrom scripts.kegg_api import kegg_link\n\n# Find pathways linked to human genes\npathways = kegg_link('pathway', 'hsa')\n\n# Get genes in a specific pathway\ngenes = kegg_link('genes', 'hsa00010')  # Glycolysis genes\n\n# Find pathways containing a specific gene\ngene_pathways = kegg_link('pathway', 'hsa:10458')\n\n# Find compounds in a pathway\ncompounds = kegg_link('compound', 'hsa00010')\n\n# Map genes to KO (orthology) groups\nko_groups = kegg_link('ko', 'hsa:10458')\n```\n\n**Common links**: genes  pathway, pathway  compound, pathway  enzyme, genes  ko (orthology)\n\n### 7. Drug-Drug Interactions (`kegg_ddi`)\n\nCheck for drug-drug interactions.\n\n**When to use**: Analyzing drug combinations, checking for contraindications, pharmacological research.\n\n**Usage**:\n```python\nfrom scripts.kegg_api import kegg_ddi\n\n# Check single drug\ninteractions = kegg_ddi('D00001')\n\n# Check multiple drugs (max 10)\ninteractions = kegg_ddi(['D00001', 'D00002', 'D00003'])\n```\n\n## Common Analysis Workflows\n\n### Workflow 1: Gene to Pathway Mapping\n\n**Use case**: Finding pathways associated with genes of interest (e.g., for pathway enrichment analysis).\n\n```python\nfrom scripts.kegg_api import kegg_find, kegg_link, kegg_get\n\n# Step 1: Find gene ID by name\ngene_results = kegg_find('genes', 'p53')\n\n# Step 2: Link gene to pathways\npathways = kegg_link('pathway', 'hsa:7157')  # TP53 gene\n\n# Step 3: Get detailed pathway information\nfor pathway_line in pathways.split('\\n'):\n    if pathway_line:\n        pathway_id = pathway_line.split('\\t')[1].replace('path:', '')\n        pathway_info = kegg_get(pathway_id)\n        # Process pathway information\n```\n\n### Workflow 2: Pathway Enrichment Context\n\n**Use case**: Getting all genes in organism pathways for enrichment analysis.\n\n```python\nfrom scripts.kegg_api import kegg_list, kegg_link\n\n# Step 1: List all human pathways\npathways = kegg_list('pathway', 'hsa')\n\n# Step 2: For each pathway, get associated genes\nfor pathway_line in pathways.split('\\n'):\n    if pathway_line:\n        pathway_id = pathway_line.split('\\t')[0]\n        genes = kegg_link('genes', pathway_id)\n        # Process genes for enrichment analysis\n```\n\n### Workflow 3: Compound to Pathway Analysis\n\n**Use case**: Finding metabolic pathways containing compounds of interest.\n\n```python\nfrom scripts.kegg_api import kegg_find, kegg_link, kegg_get\n\n# Step 1: Search for compound\ncompound_results = kegg_find('compound', 'glucose')\n\n# Step 2: Link compound to reactions\nreactions = kegg_link('reaction', 'cpd:C00031')  # Glucose\n\n# Step 3: Link reactions to pathways\npathways = kegg_link('pathway', 'rn:R00299')  # Specific reaction\n\n# Step 4: Get pathway details\npathway_info = kegg_get('map00010')  # Glycolysis\n```\n\n### Workflow 4: Cross-Database Integration\n\n**Use case**: Integrating KEGG data with UniProt, NCBI, or PubChem databases.\n\n```python\nfrom scripts.kegg_api import kegg_conv, kegg_get\n\n# Step 1: Convert KEGG gene IDs to external database IDs\nuniprot_map = kegg_conv('uniprot', 'hsa')\nncbi_map = kegg_conv('ncbi-geneid', 'hsa')\n\n# Step 2: Parse conversion results\nfor line in uniprot_map.split('\\n'):\n    if line:\n        kegg_id, uniprot_id = line.split('\\t')\n        # Use external IDs for integration\n\n# Step 3: Get sequences using KEGG\nsequence = kegg_get('hsa:10458', 'aaseq')\n```\n\n### Workflow 5: Organism-Specific Pathway Analysis\n\n**Use case**: Comparing pathways across different organisms.\n\n```python\nfrom scripts.kegg_api import kegg_list, kegg_get\n\n# Step 1: List pathways for multiple organisms\nhuman_pathways = kegg_list('pathway', 'hsa')\nmouse_pathways = kegg_list('pathway', 'mmu')\nyeast_pathways = kegg_list('pathway', 'sce')\n\n# Step 2: Get reference pathway for comparison\nref_pathway = kegg_get('map00010')  # Reference glycolysis\n\n# Step 3: Get organism-specific versions\nhsa_glycolysis = kegg_get('hsa00010')\nmmu_glycolysis = kegg_get('mmu00010')\n```\n\n## Pathway Categories\n\nKEGG organizes pathways into seven major categories. When interpreting pathway IDs or recommending pathways to users:\n\n1. **Metabolism** (e.g., `map00010` - Glycolysis, `map00190` - Oxidative phosphorylation)\n2. **Genetic Information Processing** (e.g., `map03010` - Ribosome, `map03040` - Spliceosome)\n3. **Environmental Information Processing** (e.g., `map04010` - MAPK signaling, `map02010` - ABC transporters)\n4. **Cellular Processes** (e.g., `map04140` - Autophagy, `map04210` - Apoptosis)\n5. **Organismal Systems** (e.g., `map04610` - Complement cascade, `map04910` - Insulin signaling)\n6. **Human Diseases** (e.g., `map05200` - Pathways in cancer, `map05010` - Alzheimer disease)\n7. **Drug Development** (chronological and target-based classifications)\n\nReference `references/kegg_reference.md` for detailed pathway lists and classifications.\n\n## Important Identifiers and Formats\n\n### Pathway IDs\n- `map#####` - Reference pathway (generic, not organism-specific)\n- `hsa#####` - Human pathway\n- `mmu#####` - Mouse pathway\n\n### Gene IDs\n- Format: `organism:gene_number` (e.g., `hsa:10458`)\n\n### Compound IDs\n- Format: `cpd:C#####` (e.g., `cpd:C00002` for ATP)\n\n### Drug IDs\n- Format: `dr:D#####` (e.g., `dr:D00001`)\n\n### Enzyme IDs\n- Format: `ec:EC_number` (e.g., `ec:1.1.1.1`)\n\n### KO (KEGG Orthology) IDs\n- Format: `ko:K#####` (e.g., `ko:K00001`)\n\n## API Limitations\n\nRespect these constraints when using the KEGG API:\n\n1. **Entry limits**: Maximum 10 entries per operation (except image/kgml/json: 1 entry only)\n2. **Academic use**: API is for academic use only; commercial use requires licensing\n3. **HTTP status codes**: Check for 200 (success), 400 (bad request), 404 (not found)\n4. **Rate limiting**: No explicit limit, but avoid rapid-fire requests\n\n## Detailed Reference\n\nFor comprehensive API documentation, database specifications, organism codes, and advanced usage, refer to `references/kegg_reference.md`. This includes:\n\n- Complete list of KEGG databases\n- Detailed API operation syntax\n- All organism codes\n- HTTP status codes and error handling\n- Integration with Biopython and R/Bioconductor\n- Best practices for API usage\n\n## Troubleshooting\n\n**404 Not Found**: Entry or database doesn't exist; verify IDs and organism codes\n**400 Bad Request**: Syntax error in API call; check parameter formatting\n**Empty results**: Search term may not match entries; try broader keywords\n**Image/KGML errors**: These formats only work with single entries; remove batch processing\n\n## Additional Tools\n\nFor interactive pathway visualization and annotation:\n- **KEGG Mapper**: https://www.kegg.jp/kegg/mapper/\n- **BlastKOALA**: Automated genome annotation\n- **GhostKOALA**: Metagenome/metatranscriptome annotation\n",
        "data/k-dense-ai/labarchive-integration/SKILL.md": "---\nname: labarchive-integration\ndescription: \"Electronic lab notebook API integration. Access notebooks, manage entries/attachments, backup notebooks, integrate with Protocols.io/Jupyter/REDCap, for programmatic ELN workflows.\"\n---\n\n# LabArchives Integration\n\n## Overview\n\nLabArchives is an electronic lab notebook platform for research documentation and data management. Access notebooks, manage entries and attachments, generate reports, and integrate with third-party tools programmatically via REST API.\n\n## When to Use This Skill\n\nThis skill should be used when:\n- Working with LabArchives REST API for notebook automation\n- Backing up notebooks programmatically\n- Creating or managing notebook entries and attachments\n- Generating site reports and analytics\n- Integrating LabArchives with third-party tools (Protocols.io, Jupyter, REDCap)\n- Automating data upload to electronic lab notebooks\n- Managing user access and permissions programmatically\n\n## Core Capabilities\n\n### 1. Authentication and Configuration\n\nSet up API access credentials and regional endpoints for LabArchives API integration.\n\n**Prerequisites:**\n- Enterprise LabArchives license with API access enabled\n- API access key ID and password from LabArchives administrator\n- User authentication credentials (email and external applications password)\n\n**Configuration setup:**\n\nUse the `scripts/setup_config.py` script to create a configuration file:\n\n```bash\npython3 scripts/setup_config.py\n```\n\nThis creates a `config.yaml` file with the following structure:\n\n```yaml\napi_url: https://api.labarchives.com/api  # or regional endpoint\naccess_key_id: YOUR_ACCESS_KEY_ID\naccess_password: YOUR_ACCESS_PASSWORD\n```\n\n**Regional API endpoints:**\n- US/International: `https://api.labarchives.com/api`\n- Australia: `https://auapi.labarchives.com/api`\n- UK: `https://ukapi.labarchives.com/api`\n\nFor detailed authentication instructions and troubleshooting, refer to `references/authentication_guide.md`.\n\n### 2. User Information Retrieval\n\nObtain user ID (UID) and access information required for subsequent API operations.\n\n**Workflow:**\n\n1. Call the `users/user_access_info` API method with login credentials\n2. Parse the XML/JSON response to extract the user ID (UID)\n3. Use the UID to retrieve detailed user information via `users/user_info_via_id`\n\n**Example using Python wrapper:**\n\n```python\nfrom labarchivespy.client import Client\n\n# Initialize client\nclient = Client(api_url, access_key_id, access_password)\n\n# Get user access info\nlogin_params = {'login_or_email': user_email, 'password': auth_token}\nresponse = client.make_call('users', 'user_access_info', params=login_params)\n\n# Extract UID from response\nimport xml.etree.ElementTree as ET\nuid = ET.fromstring(response.content)[0].text\n\n# Get detailed user info\nparams = {'uid': uid}\nuser_info = client.make_call('users', 'user_info_via_id', params=params)\n```\n\n### 3. Notebook Operations\n\nManage notebook access, backup, and metadata retrieval.\n\n**Key operations:**\n\n- **List notebooks:** Retrieve all notebooks accessible to a user\n- **Backup notebooks:** Download complete notebook data with optional attachment inclusion\n- **Get notebook IDs:** Retrieve institution-defined notebook identifiers for integration with grants/project management systems\n- **Get notebook members:** List all users with access to a specific notebook\n- **Get notebook settings:** Retrieve configuration and permissions for notebooks\n\n**Notebook backup example:**\n\nUse the `scripts/notebook_operations.py` script:\n\n```bash\n# Backup with attachments (default, creates 7z archive)\npython3 scripts/notebook_operations.py backup --uid USER_ID --nbid NOTEBOOK_ID\n\n# Backup without attachments, JSON format\npython3 scripts/notebook_operations.py backup --uid USER_ID --nbid NOTEBOOK_ID --json --no-attachments\n```\n\n**API endpoint format:**\n```\nhttps://<api_url>/notebooks/notebook_backup?uid=<UID>&nbid=<NOTEBOOK_ID>&json=true&no_attachments=false\n```\n\nFor comprehensive API method documentation, refer to `references/api_reference.md`.\n\n### 4. Entry and Attachment Management\n\nCreate, modify, and manage notebook entries and file attachments.\n\n**Entry operations:**\n- Create new entries in notebooks\n- Add comments to existing entries\n- Create entry parts/components\n- Upload file attachments to entries\n\n**Attachment workflow:**\n\nUse the `scripts/entry_operations.py` script:\n\n```bash\n# Upload attachment to an entry\npython3 scripts/entry_operations.py upload --uid USER_ID --nbid NOTEBOOK_ID --entry-id ENTRY_ID --file /path/to/file.pdf\n\n# Create a new entry with text content\npython3 scripts/entry_operations.py create --uid USER_ID --nbid NOTEBOOK_ID --title \"Experiment Results\" --content \"Results from today's experiment...\"\n```\n\n**Supported file types:**\n- Documents (PDF, DOCX, TXT)\n- Images (PNG, JPG, TIFF)\n- Data files (CSV, XLSX, HDF5)\n- Scientific formats (CIF, MOL, PDB)\n- Archives (ZIP, 7Z)\n\n### 5. Site Reports and Analytics\n\nGenerate institutional reports on notebook usage, activity, and compliance (Enterprise feature).\n\n**Available reports:**\n- Detailed Usage Report: User activity metrics and engagement statistics\n- Detailed Notebook Report: Notebook metadata, member lists, and settings\n- PDF/Offline Notebook Generation Report: Export tracking for compliance\n- Notebook Members Report: Access control and collaboration analytics\n- Notebook Settings Report: Configuration and permission auditing\n\n**Report generation:**\n\n```python\n# Generate detailed usage report\nresponse = client.make_call('site_reports', 'detailed_usage_report',\n                           params={'start_date': '2025-01-01', 'end_date': '2025-10-20'})\n```\n\n### 6. Third-Party Integrations\n\nLabArchives integrates with numerous scientific software platforms. This skill provides guidance on leveraging these integrations programmatically.\n\n**Supported integrations:**\n- **Protocols.io:** Export protocols directly to LabArchives notebooks\n- **GraphPad Prism:** Export analyses and figures (Version 8+)\n- **SnapGene:** Direct molecular biology workflow integration\n- **Geneious:** Bioinformatics analysis export\n- **Jupyter:** Embed Jupyter notebooks as entries\n- **REDCap:** Clinical data capture integration\n- **Qeios:** Research publishing platform\n- **SciSpace:** Literature management\n\n**OAuth authentication:**\nLabArchives now uses OAuth for all new integrations. Legacy integrations may use API key authentication.\n\nFor detailed integration setup instructions and use cases, refer to `references/integrations.md`.\n\n## Common Workflows\n\n### Complete notebook backup workflow\n\n1. Authenticate and obtain user ID\n2. List all accessible notebooks\n3. Iterate through notebooks and backup each one\n4. Store backups with timestamp metadata\n\n```bash\n# Complete backup script\npython3 scripts/notebook_operations.py backup-all --email user@example.edu --password AUTH_TOKEN\n```\n\n### Automated data upload workflow\n\n1. Authenticate with LabArchives API\n2. Identify target notebook and entry\n3. Upload experimental data files\n4. Add metadata comments to entries\n5. Generate activity report\n\n### Integration workflow example (Jupyter  LabArchives)\n\n1. Export Jupyter notebook to HTML or PDF\n2. Use entry_operations.py to upload to LabArchives\n3. Add comment with execution timestamp and environment info\n4. Tag entry for easy retrieval\n\n## Python Package Installation\n\nInstall the `labarchives-py` wrapper for simplified API access:\n\n```bash\ngit clone https://github.com/mcmero/labarchives-py\ncd labarchives-py\nuv pip install .\n```\n\nAlternatively, use direct HTTP requests via Python's `requests` library for custom implementations.\n\n## Best Practices\n\n1. **Rate limiting:** Implement appropriate delays between API calls to avoid throttling\n2. **Error handling:** Always wrap API calls in try-except blocks with appropriate logging\n3. **Authentication security:** Store credentials in environment variables or secure config files (never in code)\n4. **Backup verification:** After notebook backup, verify file integrity and completeness\n5. **Incremental operations:** For large notebooks, use pagination and batch processing\n6. **Regional endpoints:** Use the correct regional API endpoint for optimal performance\n\n## Troubleshooting\n\n**Common issues:**\n\n- **401 Unauthorized:** Verify access key ID and password are correct; check API access is enabled for your account\n- **404 Not Found:** Confirm notebook ID (nbid) exists and user has access permissions\n- **403 Forbidden:** Check user permissions for the requested operation\n- **Empty response:** Ensure required parameters (uid, nbid) are provided correctly\n- **Attachment upload failures:** Verify file size limits and format compatibility\n\nFor additional support, contact LabArchives at support@labarchives.com.\n\n## Resources\n\nThis skill includes bundled resources to support LabArchives API integration:\n\n### scripts/\n\n- `setup_config.py`: Interactive configuration file generator for API credentials\n- `notebook_operations.py`: Utilities for listing, backing up, and managing notebooks\n- `entry_operations.py`: Tools for creating entries and uploading attachments\n\n### references/\n\n- `api_reference.md`: Comprehensive API endpoint documentation with parameters and examples\n- `authentication_guide.md`: Detailed authentication setup and configuration instructions\n- `integrations.md`: Third-party integration setup guides and use cases\n",
        "data/k-dense-ai/lamindb/SKILL.md": "---\nname: lamindb\ndescription: This skill should be used when working with LaminDB, an open-source data framework for biology that makes data queryable, traceable, reproducible, and FAIR. Use when managing biological datasets (scRNA-seq, spatial, flow cytometry, etc.), tracking computational workflows, curating and validating data with biological ontologies, building data lakehouses, or ensuring data lineage and reproducibility in biological research. Covers data management, annotation, ontologies (genes, cell types, diseases, tissues), schema validation, integrations with workflow managers (Nextflow, Snakemake) and MLOps platforms (W&B, MLflow), and deployment strategies.\n---\n\n# LaminDB\n\n## Overview\n\nLaminDB is an open-source data framework for biology designed to make data queryable, traceable, reproducible, and FAIR (Findable, Accessible, Interoperable, Reusable). It provides a unified platform that combines lakehouse architecture, lineage tracking, feature stores, biological ontologies, LIMS (Laboratory Information Management System), and ELN (Electronic Lab Notebook) capabilities through a single Python API.\n\n**Core Value Proposition:**\n- **Queryability**: Search and filter datasets by metadata, features, and ontology terms\n- **Traceability**: Automatic lineage tracking from raw data through analysis to results\n- **Reproducibility**: Version control for data, code, and environment\n- **FAIR Compliance**: Standardized annotations using biological ontologies\n\n## When to Use This Skill\n\nUse this skill when:\n\n- **Managing biological datasets**: scRNA-seq, bulk RNA-seq, spatial transcriptomics, flow cytometry, multi-modal data, EHR data\n- **Tracking computational workflows**: Notebooks, scripts, pipeline execution (Nextflow, Snakemake, Redun)\n- **Curating and validating data**: Schema validation, standardization, ontology-based annotation\n- **Working with biological ontologies**: Genes, proteins, cell types, tissues, diseases, pathways (via Bionty)\n- **Building data lakehouses**: Unified query interface across multiple datasets\n- **Ensuring reproducibility**: Automatic versioning, lineage tracking, environment capture\n- **Integrating ML pipelines**: Connecting with Weights & Biases, MLflow, HuggingFace, scVI-tools\n- **Deploying data infrastructure**: Setting up local or cloud-based data management systems\n- **Collaborating on datasets**: Sharing curated, annotated data with standardized metadata\n\n## Core Capabilities\n\nLaminDB provides six interconnected capability areas, each documented in detail in the references folder.\n\n### 1. Core Concepts and Data Lineage\n\n**Core entities:**\n- **Artifacts**: Versioned datasets (DataFrame, AnnData, Parquet, Zarr, etc.)\n- **Records**: Experimental entities (samples, perturbations, instruments)\n- **Runs & Transforms**: Computational lineage tracking (what code produced what data)\n- **Features**: Typed metadata fields for annotation and querying\n\n**Key workflows:**\n- Create and version artifacts from files or Python objects\n- Track notebook/script execution with `ln.track()` and `ln.finish()`\n- Annotate artifacts with typed features\n- Visualize data lineage graphs with `artifact.view_lineage()`\n- Query by provenance (find all outputs from specific code/inputs)\n\n**Reference:** `references/core-concepts.md` - Read this for detailed information on artifacts, records, runs, transforms, features, versioning, and lineage tracking.\n\n### 2. Data Management and Querying\n\n**Query capabilities:**\n- Registry exploration and lookup with auto-complete\n- Single record retrieval with `get()`, `one()`, `one_or_none()`\n- Filtering with comparison operators (`__gt`, `__lte`, `__contains`, `__startswith`)\n- Feature-based queries (query by annotated metadata)\n- Cross-registry traversal with double-underscore syntax\n- Full-text search across registries\n- Advanced logical queries with Q objects (AND, OR, NOT)\n- Streaming large datasets without loading into memory\n\n**Key workflows:**\n- Browse artifacts with filters and ordering\n- Query by features, creation date, creator, size, etc.\n- Stream large files in chunks or with array slicing\n- Organize data with hierarchical keys\n- Group artifacts into collections\n\n**Reference:** `references/data-management.md` - Read this for comprehensive query patterns, filtering examples, streaming strategies, and data organization best practices.\n\n### 3. Annotation and Validation\n\n**Curation process:**\n1. **Validation**: Confirm datasets match desired schemas\n2. **Standardization**: Fix typos, map synonyms to canonical terms\n3. **Annotation**: Link datasets to metadata entities for queryability\n\n**Schema types:**\n- **Flexible schemas**: Validate only known columns, allow additional metadata\n- **Minimal required schemas**: Specify essential columns, permit extras\n- **Strict schemas**: Complete control over structure and values\n\n**Supported data types:**\n- DataFrames (Parquet, CSV)\n- AnnData (single-cell genomics)\n- MuData (multi-modal)\n- SpatialData (spatial transcriptomics)\n- TileDB-SOMA (scalable arrays)\n\n**Key workflows:**\n- Define features and schemas for data validation\n- Use `DataFrameCurator` or `AnnDataCurator` for validation\n- Standardize values with `.cat.standardize()`\n- Map to ontologies with `.cat.add_ontology()`\n- Save curated artifacts with schema linkage\n- Query validated datasets by features\n\n**Reference:** `references/annotation-validation.md` - Read this for detailed curation workflows, schema design patterns, handling validation errors, and best practices.\n\n### 4. Biological Ontologies\n\n**Available ontologies (via Bionty):**\n- Genes (Ensembl), Proteins (UniProt)\n- Cell types (CL), Cell lines (CLO)\n- Tissues (Uberon), Diseases (Mondo, DOID)\n- Phenotypes (HPO), Pathways (GO)\n- Experimental factors (EFO), Developmental stages\n- Organisms (NCBItaxon), Drugs (DrugBank)\n\n**Key workflows:**\n- Import public ontologies with `bt.CellType.import_source()`\n- Search ontologies with keyword or exact matching\n- Standardize terms using synonym mapping\n- Explore hierarchical relationships (parents, children, ancestors)\n- Validate data against ontology terms\n- Annotate datasets with ontology records\n- Create custom terms and hierarchies\n- Handle multi-organism contexts (human, mouse, etc.)\n\n**Reference:** `references/ontologies.md` - Read this for comprehensive ontology operations, standardization strategies, hierarchy navigation, and annotation workflows.\n\n### 5. Integrations\n\n**Workflow managers:**\n- Nextflow: Track pipeline processes and outputs\n- Snakemake: Integrate into Snakemake rules\n- Redun: Combine with Redun task tracking\n\n**MLOps platforms:**\n- Weights & Biases: Link experiments with data artifacts\n- MLflow: Track models and experiments\n- HuggingFace: Track model fine-tuning\n- scVI-tools: Single-cell analysis workflows\n\n**Storage systems:**\n- Local filesystem, AWS S3, Google Cloud Storage\n- S3-compatible (MinIO, Cloudflare R2)\n- HTTP/HTTPS endpoints (read-only)\n- HuggingFace datasets\n\n**Array stores:**\n- TileDB-SOMA (with cellxgene support)\n- DuckDB for SQL queries on Parquet files\n\n**Visualization:**\n- Vitessce for interactive spatial/single-cell visualization\n\n**Version control:**\n- Git integration for source code tracking\n\n**Reference:** `references/integrations.md` - Read this for integration patterns, code examples, and troubleshooting for third-party systems.\n\n### 6. Setup and Deployment\n\n**Installation:**\n- Basic: `uv pip install lamindb`\n- With extras: `uv pip install 'lamindb[gcp,zarr,fcs]'`\n- Modules: bionty, wetlab, clinical\n\n**Instance types:**\n- Local SQLite (development)\n- Cloud storage + SQLite (small teams)\n- Cloud storage + PostgreSQL (production)\n\n**Storage options:**\n- Local filesystem\n- AWS S3 with configurable regions and permissions\n- Google Cloud Storage\n- S3-compatible endpoints (MinIO, Cloudflare R2)\n\n**Configuration:**\n- Cache management for cloud files\n- Multi-user system configurations\n- Git repository sync\n- Environment variables\n\n**Deployment patterns:**\n- Local dev  Cloud production migration\n- Multi-region deployments\n- Shared storage with personal instances\n\n**Reference:** `references/setup-deployment.md` - Read this for detailed installation, configuration, storage setup, database management, security best practices, and troubleshooting.\n\n## Common Use Case Workflows\n\n### Use Case 1: Single-Cell RNA-seq Analysis with Ontology Validation\n\n```python\nimport lamindb as ln\nimport bionty as bt\nimport anndata as ad\n\n# Start tracking\nln.track(params={\"analysis\": \"scRNA-seq QC and annotation\"})\n\n# Import cell type ontology\nbt.CellType.import_source()\n\n# Load data\nadata = ad.read_h5ad(\"raw_counts.h5ad\")\n\n# Validate and standardize cell types\nadata.obs[\"cell_type\"] = bt.CellType.standardize(adata.obs[\"cell_type\"])\n\n# Curate with schema\ncurator = ln.curators.AnnDataCurator(adata, schema)\ncurator.validate()\nartifact = curator.save_artifact(key=\"scrna/validated.h5ad\")\n\n# Link ontology annotations\ncell_types = bt.CellType.from_values(adata.obs.cell_type)\nartifact.feature_sets.add_ontology(cell_types)\n\nln.finish()\n```\n\n### Use Case 2: Building a Queryable Data Lakehouse\n\n```python\nimport lamindb as ln\n\n# Register multiple experiments\nfor i, file in enumerate(data_files):\n    artifact = ln.Artifact.from_anndata(\n        ad.read_h5ad(file),\n        key=f\"scrna/batch_{i}.h5ad\",\n        description=f\"scRNA-seq batch {i}\"\n    ).save()\n\n    # Annotate with features\n    artifact.features.add_values({\n        \"batch\": i,\n        \"tissue\": tissues[i],\n        \"condition\": conditions[i]\n    })\n\n# Query across all experiments\nimmune_datasets = ln.Artifact.filter(\n    key__startswith=\"scrna/\",\n    tissue=\"PBMC\",\n    condition=\"treated\"\n).to_dataframe()\n\n# Load specific datasets\nfor artifact in immune_datasets:\n    adata = artifact.load()\n    # Analyze\n```\n\n### Use Case 3: ML Pipeline with W&B Integration\n\n```python\nimport lamindb as ln\nimport wandb\n\n# Initialize both systems\nwandb.init(project=\"drug-response\", name=\"exp-42\")\nln.track(params={\"model\": \"random_forest\", \"n_estimators\": 100})\n\n# Load training data from LaminDB\ntrain_artifact = ln.Artifact.get(key=\"datasets/train.parquet\")\ntrain_data = train_artifact.load()\n\n# Train model\nmodel = train_model(train_data)\n\n# Log to W&B\nwandb.log({\"accuracy\": 0.95})\n\n# Save model in LaminDB with W&B linkage\nimport joblib\njoblib.dump(model, \"model.pkl\")\nmodel_artifact = ln.Artifact(\"model.pkl\", key=\"models/exp-42.pkl\").save()\nmodel_artifact.features.add_values({\"wandb_run_id\": wandb.run.id})\n\nln.finish()\nwandb.finish()\n```\n\n### Use Case 4: Nextflow Pipeline Integration\n\n```python\n# In Nextflow process script\nimport lamindb as ln\n\nln.track()\n\n# Load input artifact\ninput_artifact = ln.Artifact.get(key=\"raw/batch_${batch_id}.fastq.gz\")\ninput_path = input_artifact.cache()\n\n# Process (alignment, quantification, etc.)\n# ... Nextflow process logic ...\n\n# Save output\noutput_artifact = ln.Artifact(\n    \"counts.csv\",\n    key=\"processed/batch_${batch_id}_counts.csv\"\n).save()\n\nln.finish()\n```\n\n## Getting Started Checklist\n\nTo start using LaminDB effectively:\n\n1. **Installation & Setup** (`references/setup-deployment.md`)\n   - Install LaminDB and required extras\n   - Authenticate with `lamin login`\n   - Initialize instance with `lamin init --storage ...`\n\n2. **Learn Core Concepts** (`references/core-concepts.md`)\n   - Understand Artifacts, Records, Runs, Transforms\n   - Practice creating and retrieving artifacts\n   - Implement `ln.track()` and `ln.finish()` in workflows\n\n3. **Master Querying** (`references/data-management.md`)\n   - Practice filtering and searching registries\n   - Learn feature-based queries\n   - Experiment with streaming large files\n\n4. **Set Up Validation** (`references/annotation-validation.md`)\n   - Define features relevant to research domain\n   - Create schemas for data types\n   - Practice curation workflows\n\n5. **Integrate Ontologies** (`references/ontologies.md`)\n   - Import relevant biological ontologies (genes, cell types, etc.)\n   - Validate existing annotations\n   - Standardize metadata with ontology terms\n\n6. **Connect Tools** (`references/integrations.md`)\n   - Integrate with existing workflow managers\n   - Link ML platforms for experiment tracking\n   - Configure cloud storage and compute\n\n## Key Principles\n\nFollow these principles when working with LaminDB:\n\n1. **Track everything**: Use `ln.track()` at the start of every analysis for automatic lineage capture\n\n2. **Validate early**: Define schemas and validate data before extensive analysis\n\n3. **Use ontologies**: Leverage public biological ontologies for standardized annotations\n\n4. **Organize with keys**: Structure artifact keys hierarchically (e.g., `project/experiment/batch/file.h5ad`)\n\n5. **Query metadata first**: Filter and search before loading large files\n\n6. **Version, don't duplicate**: Use built-in versioning instead of creating new keys for modifications\n\n7. **Annotate with features**: Define typed features for queryable metadata\n\n8. **Document thoroughly**: Add descriptions to artifacts, schemas, and transforms\n\n9. **Leverage lineage**: Use `view_lineage()` to understand data provenance\n\n10. **Start local, scale cloud**: Develop locally with SQLite, deploy to cloud with PostgreSQL\n\n## Reference Files\n\nThis skill includes comprehensive reference documentation organized by capability:\n\n- **`references/core-concepts.md`** - Artifacts, records, runs, transforms, features, versioning, lineage\n- **`references/data-management.md`** - Querying, filtering, searching, streaming, organizing data\n- **`references/annotation-validation.md`** - Schema design, curation workflows, validation strategies\n- **`references/ontologies.md`** - Biological ontology management, standardization, hierarchies\n- **`references/integrations.md`** - Workflow managers, MLOps platforms, storage systems, tools\n- **`references/setup-deployment.md`** - Installation, configuration, deployment, troubleshooting\n\nRead the relevant reference file(s) based on the specific LaminDB capability needed for the task at hand.\n\n## Additional Resources\n\n- **Official Documentation**: https://docs.lamin.ai\n- **API Reference**: https://docs.lamin.ai/api\n- **GitHub Repository**: https://github.com/laminlabs/lamindb\n- **Tutorial**: https://docs.lamin.ai/tutorial\n- **FAQ**: https://docs.lamin.ai/faq\n",
        "data/k-dense-ai/latchbio-integration/SKILL.md": "---\nname: latchbio-integration\ndescription: \"Latch platform for bioinformatics workflows. Build pipelines with Latch SDK, @workflow/@task decorators, deploy serverless workflows, LatchFile/LatchDir, Nextflow/Snakemake integration.\"\n---\n\n# LatchBio Integration\n\n## Overview\n\nLatch is a Python framework for building and deploying bioinformatics workflows as serverless pipelines. Built on Flyte, create workflows with @workflow/@task decorators, manage cloud data with LatchFile/LatchDir, configure resources, and integrate Nextflow/Snakemake pipelines.\n\n## Core Capabilities\n\nThe Latch platform provides four main areas of functionality:\n\n### 1. Workflow Creation and Deployment\n- Define serverless workflows using Python decorators\n- Support for native Python, Nextflow, and Snakemake pipelines\n- Automatic containerization with Docker\n- Auto-generated no-code user interfaces\n- Version control and reproducibility\n\n### 2. Data Management\n- Cloud storage abstractions (LatchFile, LatchDir)\n- Structured data organization with Registry (Projects  Tables  Records)\n- Type-safe data operations with links and enums\n- Automatic file transfer between local and cloud\n- Glob pattern matching for file selection\n\n### 3. Resource Configuration\n- Pre-configured task decorators (@small_task, @large_task, @small_gpu_task, @large_gpu_task)\n- Custom resource specifications (CPU, memory, GPU, storage)\n- GPU support (K80, V100, A100)\n- Timeout and storage configuration\n- Cost optimization strategies\n\n### 4. Verified Workflows\n- Production-ready pre-built pipelines\n- Bulk RNA-seq, DESeq2, pathway analysis\n- AlphaFold and ColabFold for protein structure prediction\n- Single-cell tools (ArchR, scVelo, emptyDropsR)\n- CRISPR analysis, phylogenetics, and more\n\n## Quick Start\n\n### Installation and Setup\n\n```bash\n# Install Latch SDK\npython3 -m uv pip install latch\n\n# Login to Latch\nlatch login\n\n# Initialize a new workflow\nlatch init my-workflow\n\n# Register workflow to platform\nlatch register my-workflow\n```\n\n**Prerequisites:**\n- Docker installed and running\n- Latch account credentials\n- Python 3.8+\n\n### Basic Workflow Example\n\n```python\nfrom latch import workflow, small_task\nfrom latch.types import LatchFile\n\n@small_task\ndef process_file(input_file: LatchFile) -> LatchFile:\n    \"\"\"Process a single file\"\"\"\n    # Processing logic\n    return output_file\n\n@workflow\ndef my_workflow(input_file: LatchFile) -> LatchFile:\n    \"\"\"\n    My bioinformatics workflow\n\n    Args:\n        input_file: Input data file\n    \"\"\"\n    return process_file(input_file=input_file)\n```\n\n## When to Use This Skill\n\nThis skill should be used when encountering any of the following scenarios:\n\n**Workflow Development:**\n- \"Create a Latch workflow for RNA-seq analysis\"\n- \"Deploy my pipeline to Latch\"\n- \"Convert my Nextflow pipeline to Latch\"\n- \"Add GPU support to my workflow\"\n- Working with `@workflow`, `@task` decorators\n\n**Data Management:**\n- \"Organize my sequencing data in Latch Registry\"\n- \"How do I use LatchFile and LatchDir?\"\n- \"Set up sample tracking in Latch\"\n- Working with `latch:///` paths\n\n**Resource Configuration:**\n- \"Configure GPU for AlphaFold on Latch\"\n- \"My task is running out of memory\"\n- \"How do I optimize workflow costs?\"\n- Working with task decorators\n\n**Verified Workflows:**\n- \"Run AlphaFold on Latch\"\n- \"Use DESeq2 for differential expression\"\n- \"Available pre-built workflows\"\n- Using `latch.verified` module\n\n## Detailed Documentation\n\nThis skill includes comprehensive reference documentation organized by capability:\n\n### references/workflow-creation.md\n**Read this for:**\n- Creating and registering workflows\n- Task definition and decorators\n- Supporting Python, Nextflow, Snakemake\n- Launch plans and conditional sections\n- Workflow execution (CLI and programmatic)\n- Multi-step and parallel pipelines\n- Troubleshooting registration issues\n\n**Key topics:**\n- `latch init` and `latch register` commands\n- `@workflow` and `@task` decorators\n- LatchFile and LatchDir basics\n- Type annotations and docstrings\n- Launch plans with preset parameters\n- Conditional UI sections\n\n### references/data-management.md\n**Read this for:**\n- Cloud storage with LatchFile and LatchDir\n- Registry system (Projects, Tables, Records)\n- Linked records and relationships\n- Enum and typed columns\n- Bulk operations and transactions\n- Integration with workflows\n- Account and workspace management\n\n**Key topics:**\n- `latch:///` path format\n- File transfer and glob patterns\n- Creating and querying Registry tables\n- Column types (string, number, file, link, enum)\n- Record CRUD operations\n- Workflow-Registry integration\n\n### references/resource-configuration.md\n**Read this for:**\n- Task resource decorators\n- Custom CPU, memory, GPU configuration\n- GPU types (K80, V100, A100)\n- Timeout and storage settings\n- Resource optimization strategies\n- Cost-effective workflow design\n- Monitoring and debugging\n\n**Key topics:**\n- `@small_task`, `@large_task`, `@small_gpu_task`, `@large_gpu_task`\n- `@custom_task` with precise specifications\n- Multi-GPU configuration\n- Resource selection by workload type\n- Platform limits and quotas\n\n### references/verified-workflows.md\n**Read this for:**\n- Pre-built production workflows\n- Bulk RNA-seq and DESeq2\n- AlphaFold and ColabFold\n- Single-cell analysis (ArchR, scVelo)\n- CRISPR editing analysis\n- Pathway enrichment\n- Integration with custom workflows\n\n**Key topics:**\n- `latch.verified` module imports\n- Available verified workflows\n- Workflow parameters and options\n- Combining verified and custom steps\n- Version management\n\n## Common Workflow Patterns\n\n### Complete RNA-seq Pipeline\n\n```python\nfrom latch import workflow, small_task, large_task\nfrom latch.types import LatchFile, LatchDir\n\n@small_task\ndef quality_control(fastq: LatchFile) -> LatchFile:\n    \"\"\"Run FastQC\"\"\"\n    return qc_output\n\n@large_task\ndef alignment(fastq: LatchFile, genome: str) -> LatchFile:\n    \"\"\"STAR alignment\"\"\"\n    return bam_output\n\n@small_task\ndef quantification(bam: LatchFile) -> LatchFile:\n    \"\"\"featureCounts\"\"\"\n    return counts\n\n@workflow\ndef rnaseq_pipeline(\n    input_fastq: LatchFile,\n    genome: str,\n    output_dir: LatchDir\n) -> LatchFile:\n    \"\"\"RNA-seq analysis pipeline\"\"\"\n    qc = quality_control(fastq=input_fastq)\n    aligned = alignment(fastq=qc, genome=genome)\n    return quantification(bam=aligned)\n```\n\n### GPU-Accelerated Workflow\n\n```python\nfrom latch import workflow, small_task, large_gpu_task\nfrom latch.types import LatchFile\n\n@small_task\ndef preprocess(input_file: LatchFile) -> LatchFile:\n    \"\"\"Prepare data\"\"\"\n    return processed\n\n@large_gpu_task\ndef gpu_computation(data: LatchFile) -> LatchFile:\n    \"\"\"GPU-accelerated analysis\"\"\"\n    return results\n\n@workflow\ndef gpu_pipeline(input_file: LatchFile) -> LatchFile:\n    \"\"\"Pipeline with GPU tasks\"\"\"\n    preprocessed = preprocess(input_file=input_file)\n    return gpu_computation(data=preprocessed)\n```\n\n### Registry-Integrated Workflow\n\n```python\nfrom latch import workflow, small_task\nfrom latch.registry.table import Table\nfrom latch.registry.record import Record\nfrom latch.types import LatchFile\n\n@small_task\ndef process_and_track(sample_id: str, table_id: str) -> str:\n    \"\"\"Process sample and update Registry\"\"\"\n    # Get sample from registry\n    table = Table.get(table_id=table_id)\n    records = Record.list(table_id=table_id, filter={\"sample_id\": sample_id})\n    sample = records[0]\n\n    # Process\n    input_file = sample.values[\"fastq_file\"]\n    output = process(input_file)\n\n    # Update registry\n    sample.update(values={\"status\": \"completed\", \"result\": output})\n    return \"Success\"\n\n@workflow\ndef registry_workflow(sample_id: str, table_id: str):\n    \"\"\"Workflow integrated with Registry\"\"\"\n    return process_and_track(sample_id=sample_id, table_id=table_id)\n```\n\n## Best Practices\n\n### Workflow Design\n1. Use type annotations for all parameters\n2. Write clear docstrings (appear in UI)\n3. Start with standard task decorators, scale up if needed\n4. Break complex workflows into modular tasks\n5. Implement proper error handling\n\n### Data Management\n6. Use consistent folder structures\n7. Define Registry schemas before bulk entry\n8. Use linked records for relationships\n9. Store metadata in Registry for traceability\n\n### Resource Configuration\n10. Right-size resources (don't over-allocate)\n11. Use GPU only when algorithms support it\n12. Monitor execution metrics and optimize\n13. Design for parallel execution when possible\n\n### Development Workflow\n14. Test locally with Docker before registration\n15. Use version control for workflow code\n16. Document resource requirements\n17. Profile workflows to determine actual needs\n\n## Troubleshooting\n\n### Common Issues\n\n**Registration Failures:**\n- Ensure Docker is running\n- Check authentication with `latch login`\n- Verify all dependencies in Dockerfile\n- Use `--verbose` flag for detailed logs\n\n**Resource Problems:**\n- Out of memory: Increase memory in task decorator\n- Timeouts: Increase timeout parameter\n- Storage issues: Increase ephemeral storage_gib\n\n**Data Access:**\n- Use correct `latch:///` path format\n- Verify file exists in workspace\n- Check permissions for shared workspaces\n\n**Type Errors:**\n- Add type annotations to all parameters\n- Use LatchFile/LatchDir for file/directory parameters\n- Ensure workflow return type matches actual return\n\n## Additional Resources\n\n- **Official Documentation**: https://docs.latch.bio\n- **GitHub Repository**: https://github.com/latchbio/latch\n- **Slack Community**: Join Latch SDK workspace\n- **API Reference**: https://docs.latch.bio/api/latch.html\n- **Blog**: https://blog.latch.bio\n\n## Support\n\nFor issues or questions:\n1. Check documentation links above\n2. Search GitHub issues\n3. Ask in Slack community\n4. Contact support@latch.bio\n",
        "data/k-dense-ai/literature-review/SKILL.md": "---\nname: literature-review\ndescription: Conduct comprehensive, systematic literature reviews using multiple academic databases (PubMed, arXiv, bioRxiv, Semantic Scholar, etc.). This skill should be used when conducting systematic literature reviews, meta-analyses, research synthesis, or comprehensive literature searches across biomedical, scientific, and technical domains. Creates professionally formatted markdown documents and PDFs with verified citations in multiple citation styles (APA, Nature, Vancouver, etc.).\n---\n\n# Literature Review\n\n## Overview\n\nConduct systematic, comprehensive literature reviews following rigorous academic methodology. Search multiple literature databases, synthesize findings thematically, verify all citations for accuracy, and generate professional output documents in markdown and PDF formats.\n\nThis skill integrates with multiple scientific skills for database access (gget, bioservices, datacommons-client) and provides specialized tools for citation verification, result aggregation, and document generation.\n\n## When to Use This Skill\n\nUse this skill when:\n- Conducting a systematic literature review for research or publication\n- Synthesizing current knowledge on a specific topic across multiple sources\n- Performing meta-analysis or scoping reviews\n- Writing the literature review section of a research paper or thesis\n- Investigating the state of the art in a research domain\n- Identifying research gaps and future directions\n- Requiring verified citations and professional formatting\n\n## Core Workflow\n\nLiterature reviews follow a structured, multi-phase workflow:\n\n### Phase 1: Planning and Scoping\n\n1. **Define Research Question**: Use PICO framework (Population, Intervention, Comparison, Outcome) for clinical/biomedical reviews\n   - Example: \"What is the efficacy of CRISPR-Cas9 (I) for treating sickle cell disease (P) compared to standard care (C)?\"\n\n2. **Establish Scope and Objectives**:\n   - Define clear, specific research questions\n   - Determine review type (narrative, systematic, scoping, meta-analysis)\n   - Set boundaries (time period, geographic scope, study types)\n\n3. **Develop Search Strategy**:\n   - Identify 2-4 main concepts from research question\n   - List synonyms, abbreviations, and related terms for each concept\n   - Plan Boolean operators (AND, OR, NOT) to combine terms\n   - Select minimum 3 complementary databases\n\n4. **Set Inclusion/Exclusion Criteria**:\n   - Date range (e.g., last 10 years: 2015-2024)\n   - Language (typically English, or specify multilingual)\n   - Publication types (peer-reviewed, preprints, reviews)\n   - Study designs (RCTs, observational, in vitro, etc.)\n   - Document all criteria clearly\n\n### Phase 2: Systematic Literature Search\n\n1. **Multi-Database Search**:\n\n   Select databases appropriate for the domain:\n\n   **Biomedical & Life Sciences:**\n   - Use `gget` skill: `gget search pubmed \"search terms\"` for PubMed/PMC\n   - Use `gget` skill: `gget search biorxiv \"search terms\"` for preprints\n   - Use `bioservices` skill for ChEMBL, KEGG, UniProt, etc.\n\n   **General Scientific Literature:**\n   - Search arXiv via direct API (preprints in physics, math, CS, q-bio)\n   - Search Semantic Scholar via API (200M+ papers, cross-disciplinary)\n   - Use Google Scholar for comprehensive coverage (manual or careful scraping)\n\n   **Specialized Databases:**\n   - Use `gget alphafold` for protein structures\n   - Use `gget cosmic` for cancer genomics\n   - Use `datacommons-client` for demographic/statistical data\n   - Use specialized databases as appropriate for the domain\n\n2. **Document Search Parameters**:\n   ```markdown\n   ## Search Strategy\n\n   ### Database: PubMed\n   - **Date searched**: 2024-10-25\n   - **Date range**: 2015-01-01 to 2024-10-25\n   - **Search string**:\n     ```\n     (\"CRISPR\"[Title] OR \"Cas9\"[Title])\n     AND (\"sickle cell\"[MeSH] OR \"SCD\"[Title/Abstract])\n     AND 2015:2024[Publication Date]\n     ```\n   - **Results**: 247 articles\n   ```\n\n   Repeat for each database searched.\n\n3. **Export and Aggregate Results**:\n   - Export results in JSON format from each database\n   - Combine all results into a single file\n   - Use `scripts/search_databases.py` for post-processing:\n     ```bash\n     python search_databases.py combined_results.json \\\n       --deduplicate \\\n       --format markdown \\\n       --output aggregated_results.md\n     ```\n\n### Phase 3: Screening and Selection\n\n1. **Deduplication**:\n   ```bash\n   python search_databases.py results.json --deduplicate --output unique_results.json\n   ```\n   - Removes duplicates by DOI (primary) or title (fallback)\n   - Document number of duplicates removed\n\n2. **Title Screening**:\n   - Review all titles against inclusion/exclusion criteria\n   - Exclude obviously irrelevant studies\n   - Document number excluded at this stage\n\n3. **Abstract Screening**:\n   - Read abstracts of remaining studies\n   - Apply inclusion/exclusion criteria rigorously\n   - Document reasons for exclusion\n\n4. **Full-Text Screening**:\n   - Obtain full texts of remaining studies\n   - Conduct detailed review against all criteria\n   - Document specific reasons for exclusion\n   - Record final number of included studies\n\n5. **Create PRISMA Flow Diagram**:\n   ```\n   Initial search: n = X\n    After deduplication: n = Y\n    After title screening: n = Z\n    After abstract screening: n = A\n    Included in review: n = B\n   ```\n\n### Phase 4: Data Extraction and Quality Assessment\n\n1. **Extract Key Data** from each included study:\n   - Study metadata (authors, year, journal, DOI)\n   - Study design and methods\n   - Sample size and population characteristics\n   - Key findings and results\n   - Limitations noted by authors\n   - Funding sources and conflicts of interest\n\n2. **Assess Study Quality**:\n   - **For RCTs**: Use Cochrane Risk of Bias tool\n   - **For observational studies**: Use Newcastle-Ottawa Scale\n   - **For systematic reviews**: Use AMSTAR 2\n   - Rate each study: High, Moderate, Low, or Very Low quality\n   - Consider excluding very low-quality studies\n\n3. **Organize by Themes**:\n   - Identify 3-5 major themes across studies\n   - Group studies by theme (studies may appear in multiple themes)\n   - Note patterns, consensus, and controversies\n\n### Phase 5: Synthesis and Analysis\n\n1. **Create Review Document** from template:\n   ```bash\n   cp assets/review_template.md my_literature_review.md\n   ```\n\n2. **Write Thematic Synthesis** (NOT study-by-study summaries):\n   - Organize Results section by themes or research questions\n   - Synthesize findings across multiple studies within each theme\n   - Compare and contrast different approaches and results\n   - Identify consensus areas and points of controversy\n   - Highlight the strongest evidence\n\n   Example structure:\n   ```markdown\n   #### 3.3.1 Theme: CRISPR Delivery Methods\n\n   Multiple delivery approaches have been investigated for therapeutic\n   gene editing. Viral vectors (AAV) were used in 15 studies^1-15^ and\n   showed high transduction efficiency (65-85%) but raised immunogenicity\n   concerns^3,7,12^. In contrast, lipid nanoparticles demonstrated lower\n   efficiency (40-60%) but improved safety profiles^16-23^.\n   ```\n\n3. **Critical Analysis**:\n   - Evaluate methodological strengths and limitations across studies\n   - Assess quality and consistency of evidence\n   - Identify knowledge gaps and methodological gaps\n   - Note areas requiring future research\n\n4. **Write Discussion**:\n   - Interpret findings in broader context\n   - Discuss clinical, practical, or research implications\n   - Acknowledge limitations of the review itself\n   - Compare with previous reviews if applicable\n   - Propose specific future research directions\n\n### Phase 6: Citation Verification\n\n**CRITICAL**: All citations must be verified for accuracy before final submission.\n\n1. **Verify All DOIs**:\n   ```bash\n   python scripts/verify_citations.py my_literature_review.md\n   ```\n\n   This script:\n   - Extracts all DOIs from the document\n   - Verifies each DOI resolves correctly\n   - Retrieves metadata from CrossRef\n   - Generates verification report\n   - Outputs properly formatted citations\n\n2. **Review Verification Report**:\n   - Check for any failed DOIs\n   - Verify author names, titles, and publication details match\n   - Correct any errors in the original document\n   - Re-run verification until all citations pass\n\n3. **Format Citations Consistently**:\n   - Choose one citation style and use throughout (see `references/citation_styles.md`)\n   - Common styles: APA, Nature, Vancouver, Chicago, IEEE\n   - Use verification script output to format citations correctly\n   - Ensure in-text citations match reference list format\n\n### Phase 7: Document Generation\n\n1. **Generate PDF**:\n   ```bash\n   python scripts/generate_pdf.py my_literature_review.md \\\n     --citation-style apa \\\n     --output my_review.pdf\n   ```\n\n   Options:\n   - `--citation-style`: apa, nature, chicago, vancouver, ieee\n   - `--no-toc`: Disable table of contents\n   - `--no-numbers`: Disable section numbering\n   - `--check-deps`: Check if pandoc/xelatex are installed\n\n2. **Review Final Output**:\n   - Check PDF formatting and layout\n   - Verify all sections are present\n   - Ensure citations render correctly\n   - Check that figures/tables appear properly\n   - Verify table of contents is accurate\n\n3. **Quality Checklist**:\n   - [ ] All DOIs verified with verify_citations.py\n   - [ ] Citations formatted consistently\n   - [ ] PRISMA flow diagram included (for systematic reviews)\n   - [ ] Search methodology fully documented\n   - [ ] Inclusion/exclusion criteria clearly stated\n   - [ ] Results organized thematically (not study-by-study)\n   - [ ] Quality assessment completed\n   - [ ] Limitations acknowledged\n   - [ ] References complete and accurate\n   - [ ] PDF generates without errors\n\n## Database-Specific Search Guidance\n\n### PubMed / PubMed Central\n\nAccess via `gget` skill:\n```bash\n# Search PubMed\ngget search pubmed \"CRISPR gene editing\" -l 100\n\n# Search with filters\n# Use PubMed Advanced Search Builder to construct complex queries\n# Then execute via gget or direct Entrez API\n```\n\n**Search tips**:\n- Use MeSH terms: `\"sickle cell disease\"[MeSH]`\n- Field tags: `[Title]`, `[Title/Abstract]`, `[Author]`\n- Date filters: `2020:2024[Publication Date]`\n- Boolean operators: AND, OR, NOT\n- See MeSH browser: https://meshb.nlm.nih.gov/search\n\n### bioRxiv / medRxiv\n\nAccess via `gget` skill:\n```bash\ngget search biorxiv \"CRISPR sickle cell\" -l 50\n```\n\n**Important considerations**:\n- Preprints are not peer-reviewed\n- Verify findings with caution\n- Check if preprint has been published (CrossRef)\n- Note preprint version and date\n\n### arXiv\n\nAccess via direct API or WebFetch:\n```python\n# Example search categories:\n# q-bio.QM (Quantitative Methods)\n# q-bio.GN (Genomics)\n# q-bio.MN (Molecular Networks)\n# cs.LG (Machine Learning)\n# stat.ML (Machine Learning Statistics)\n\n# Search format: category AND terms\nsearch_query = \"cat:q-bio.QM AND ti:\\\"single cell sequencing\\\"\"\n```\n\n### Semantic Scholar\n\nAccess via direct API (requires API key, or use free tier):\n- 200M+ papers across all fields\n- Excellent for cross-disciplinary searches\n- Provides citation graphs and paper recommendations\n- Use for finding highly influential papers\n\n### Specialized Biomedical Databases\n\nUse appropriate skills:\n- **ChEMBL**: `bioservices` skill for chemical bioactivity\n- **UniProt**: `gget` or `bioservices` skill for protein information\n- **KEGG**: `bioservices` skill for pathways and genes\n- **COSMIC**: `gget` skill for cancer mutations\n- **AlphaFold**: `gget alphafold` for protein structures\n- **PDB**: `gget` or direct API for experimental structures\n\n### Citation Chaining\n\nExpand search via citation networks:\n\n1. **Forward citations** (papers citing key papers):\n   - Use Google Scholar \"Cited by\"\n   - Use Semantic Scholar or OpenAlex APIs\n   - Identifies newer research building on seminal work\n\n2. **Backward citations** (references from key papers):\n   - Extract references from included papers\n   - Identify highly cited foundational work\n   - Find papers cited by multiple included studies\n\n## Citation Style Guide\n\nDetailed formatting guidelines are in `references/citation_styles.md`. Quick reference:\n\n### APA (7th Edition)\n- In-text: (Smith et al., 2023)\n- Reference: Smith, J. D., Johnson, M. L., & Williams, K. R. (2023). Title. *Journal*, *22*(4), 301-318. https://doi.org/10.xxx/yyy\n\n### Nature\n- In-text: Superscript numbers^1,2^\n- Reference: Smith, J. D., Johnson, M. L. & Williams, K. R. Title. *Nat. Rev. Drug Discov.* **22**, 301-318 (2023).\n\n### Vancouver\n- In-text: Superscript numbers^1,2^\n- Reference: Smith JD, Johnson ML, Williams KR. Title. Nat Rev Drug Discov. 2023;22(4):301-18.\n\n**Always verify citations** with verify_citations.py before finalizing.\n\n## Best Practices\n\n### Search Strategy\n1. **Use multiple databases** (minimum 3): Ensures comprehensive coverage\n2. **Include preprint servers**: Captures latest unpublished findings\n3. **Document everything**: Search strings, dates, result counts for reproducibility\n4. **Test and refine**: Run pilot searches, review results, adjust search terms\n\n### Screening and Selection\n1. **Use clear criteria**: Document inclusion/exclusion criteria before screening\n2. **Screen systematically**: Title  Abstract  Full text\n3. **Document exclusions**: Record reasons for excluding studies\n4. **Consider dual screening**: For systematic reviews, have two reviewers screen independently\n\n### Synthesis\n1. **Organize thematically**: Group by themes, NOT by individual studies\n2. **Synthesize across studies**: Compare, contrast, identify patterns\n3. **Be critical**: Evaluate quality and consistency of evidence\n4. **Identify gaps**: Note what's missing or understudied\n\n### Quality and Reproducibility\n1. **Assess study quality**: Use appropriate quality assessment tools\n2. **Verify all citations**: Run verify_citations.py script\n3. **Document methodology**: Provide enough detail for others to reproduce\n4. **Follow guidelines**: Use PRISMA for systematic reviews\n\n### Writing\n1. **Be objective**: Present evidence fairly, acknowledge limitations\n2. **Be systematic**: Follow structured template\n3. **Be specific**: Include numbers, statistics, effect sizes where available\n4. **Be clear**: Use clear headings, logical flow, thematic organization\n\n## Common Pitfalls to Avoid\n\n1. **Single database search**: Misses relevant papers; always search multiple databases\n2. **No search documentation**: Makes review irreproducible; document all searches\n3. **Study-by-study summary**: Lacks synthesis; organize thematically instead\n4. **Unverified citations**: Leads to errors; always run verify_citations.py\n5. **Too broad search**: Yields thousands of irrelevant results; refine with specific terms\n6. **Too narrow search**: Misses relevant papers; include synonyms and related terms\n7. **Ignoring preprints**: Misses latest findings; include bioRxiv, medRxiv, arXiv\n8. **No quality assessment**: Treats all evidence equally; assess and report quality\n9. **Publication bias**: Only positive results published; note potential bias\n10. **Outdated search**: Field evolves rapidly; clearly state search date\n\n## Example Workflow\n\nComplete workflow for a biomedical literature review:\n\n```bash\n# 1. Create review document from template\ncp assets/review_template.md crispr_sickle_cell_review.md\n\n# 2. Search multiple databases using appropriate skills\n# - Use gget skill for PubMed, bioRxiv\n# - Use direct API access for arXiv, Semantic Scholar\n# - Export results in JSON format\n\n# 3. Aggregate and process results\npython scripts/search_databases.py combined_results.json \\\n  --deduplicate \\\n  --rank citations \\\n  --year-start 2015 \\\n  --year-end 2024 \\\n  --format markdown \\\n  --output search_results.md \\\n  --summary\n\n# 4. Screen results and extract data\n# - Manually screen titles, abstracts, full texts\n# - Extract key data into the review document\n# - Organize by themes\n\n# 5. Write the review following template structure\n# - Introduction with clear objectives\n# - Detailed methodology section\n# - Results organized thematically\n# - Critical discussion\n# - Clear conclusions\n\n# 6. Verify all citations\npython scripts/verify_citations.py crispr_sickle_cell_review.md\n\n# Review the citation report\ncat crispr_sickle_cell_review_citation_report.json\n\n# Fix any failed citations and re-verify\npython scripts/verify_citations.py crispr_sickle_cell_review.md\n\n# 7. Generate professional PDF\npython scripts/generate_pdf.py crispr_sickle_cell_review.md \\\n  --citation-style nature \\\n  --output crispr_sickle_cell_review.pdf\n\n# 8. Review final PDF and markdown outputs\n```\n\n## Integration with Other Skills\n\nThis skill works seamlessly with other scientific skills:\n\n### Database Access Skills\n- **gget**: PubMed, bioRxiv, COSMIC, AlphaFold, Ensembl, UniProt\n- **bioservices**: ChEMBL, KEGG, Reactome, UniProt, PubChem\n- **datacommons-client**: Demographics, economics, health statistics\n\n### Analysis Skills\n- **pydeseq2**: RNA-seq differential expression (for methods sections)\n- **scanpy**: Single-cell analysis (for methods sections)\n- **anndata**: Single-cell data (for methods sections)\n- **biopython**: Sequence analysis (for background sections)\n\n### Visualization Skills\n- **matplotlib**: Generate figures and plots for review\n- **seaborn**: Statistical visualizations\n\n### Writing Skills\n- **brand-guidelines**: Apply institutional branding to PDF\n- **internal-comms**: Adapt review for different audiences\n\n## Resources\n\n### Bundled Resources\n\n**Scripts:**\n- `scripts/verify_citations.py`: Verify DOIs and generate formatted citations\n- `scripts/generate_pdf.py`: Convert markdown to professional PDF\n- `scripts/search_databases.py`: Process, deduplicate, and format search results\n\n**References:**\n- `references/citation_styles.md`: Detailed citation formatting guide (APA, Nature, Vancouver, Chicago, IEEE)\n- `references/database_strategies.md`: Comprehensive database search strategies\n\n**Assets:**\n- `assets/review_template.md`: Complete literature review template with all sections\n\n### External Resources\n\n**Guidelines:**\n- PRISMA (Systematic Reviews): http://www.prisma-statement.org/\n- Cochrane Handbook: https://training.cochrane.org/handbook\n- AMSTAR 2 (Review Quality): https://amstar.ca/\n\n**Tools:**\n- MeSH Browser: https://meshb.nlm.nih.gov/search\n- PubMed Advanced Search: https://pubmed.ncbi.nlm.nih.gov/advanced/\n- Boolean Search Guide: https://www.ncbi.nlm.nih.gov/books/NBK3827/\n\n**Citation Styles:**\n- APA Style: https://apastyle.apa.org/\n- Nature Portfolio: https://www.nature.com/nature-portfolio/editorial-policies/reporting-standards\n- NLM/Vancouver: https://www.nlm.nih.gov/bsd/uniform_requirements.html\n\n## Dependencies\n\n### Required Python Packages\n```bash\nuv pip install requests  # For citation verification\n```\n\n### Required System Tools\n```bash\n# For PDF generation\nbrew install pandoc  # macOS\napt-get install pandoc  # Linux\n\n# For LaTeX (PDF generation)\nbrew install --cask mactex  # macOS\napt-get install texlive-xetex  # Linux\n```\n\nCheck dependencies:\n```bash\npython scripts/generate_pdf.py --check-deps\n```\n\n## Summary\n\nThis literature-review skill provides:\n\n1. **Systematic methodology** following academic best practices\n2. **Multi-database integration** via existing scientific skills\n3. **Citation verification** ensuring accuracy and credibility\n4. **Professional output** in markdown and PDF formats\n5. **Comprehensive guidance** covering the entire review process\n6. **Quality assurance** with verification and validation tools\n7. **Reproducibility** through detailed documentation requirements\n\nConduct thorough, rigorous literature reviews that meet academic standards and provide comprehensive synthesis of current knowledge in any domain.\n",
        "data/k-dense-ai/markitdown/SKILL.md": "---\nname: markitdown\ndescription: Convert various file formats (PDF, Office documents, images, audio, web content, structured data) to Markdown optimized for LLM processing. Use when converting documents to markdown, extracting text from PDFs/Office files, transcribing audio, performing OCR on images, extracting YouTube transcripts, or processing batches of files. Supports 20+ formats including DOCX, XLSX, PPTX, PDF, HTML, EPUB, CSV, JSON, images with OCR, and audio with transcription.\n---\n\n# MarkItDown\n\n## Overview\n\nMarkItDown is a Python utility that converts various file formats into Markdown format, optimized for use with large language models and text analysis pipelines. It preserves document structure (headings, lists, tables, hyperlinks) while producing clean, token-efficient Markdown output.\n\n## When to Use This Skill\n\nUse this skill when users request:\n- Converting documents to Markdown format\n- Extracting text from PDF, Word, PowerPoint, or Excel files\n- Performing OCR on images to extract text\n- Transcribing audio files to text\n- Extracting YouTube video transcripts\n- Processing HTML, EPUB, or web content to Markdown\n- Converting structured data (CSV, JSON, XML) to readable Markdown\n- Batch converting multiple files or ZIP archives\n- Preparing documents for LLM analysis or RAG systems\n\n## Core Capabilities\n\n### 1. Document Conversion\n\nConvert Office documents and PDFs to Markdown while preserving structure.\n\n**Supported formats:**\n- PDF files (with optional Azure Document Intelligence integration)\n- Word documents (DOCX)\n- PowerPoint presentations (PPTX)\n- Excel spreadsheets (XLSX, XLS)\n\n**Basic usage:**\n```python\nfrom markitdown import MarkItDown\n\nmd = MarkItDown()\nresult = md.convert(\"document.pdf\")\nprint(result.text_content)\n```\n\n**Command-line:**\n```bash\nmarkitdown document.pdf -o output.md\n```\n\nSee `references/document_conversion.md` for detailed documentation on document-specific features.\n\n### 2. Media Processing\n\nExtract text from images using OCR and transcribe audio files to text.\n\n**Supported formats:**\n- Images (JPEG, PNG, GIF, etc.) with EXIF metadata extraction\n- Audio files with speech transcription (requires speech_recognition)\n\n**Image with OCR:**\n```python\nfrom markitdown import MarkItDown\n\nmd = MarkItDown()\nresult = md.convert(\"image.jpg\")\nprint(result.text_content)  # Includes EXIF metadata and OCR text\n```\n\n**Audio transcription:**\n```python\nresult = md.convert(\"audio.wav\")\nprint(result.text_content)  # Transcribed speech\n```\n\nSee `references/media_processing.md` for advanced media handling options.\n\n### 3. Web Content Extraction\n\nConvert web-based content and e-books to Markdown.\n\n**Supported formats:**\n- HTML files and web pages\n- YouTube video transcripts (via URL)\n- EPUB books\n- RSS feeds\n\n**YouTube transcript:**\n```python\nfrom markitdown import MarkItDown\n\nmd = MarkItDown()\nresult = md.convert(\"https://youtube.com/watch?v=VIDEO_ID\")\nprint(result.text_content)\n```\n\nSee `references/web_content.md` for web extraction details.\n\n### 4. Structured Data Handling\n\nConvert structured data formats to readable Markdown tables.\n\n**Supported formats:**\n- CSV files\n- JSON files\n- XML files\n\n**CSV to Markdown table:**\n```python\nfrom markitdown import MarkItDown\n\nmd = MarkItDown()\nresult = md.convert(\"data.csv\")\nprint(result.text_content)  # Formatted as Markdown table\n```\n\nSee `references/structured_data.md` for format-specific options.\n\n### 5. Advanced Integrations\n\nEnhance conversion quality with AI-powered features.\n\n**Azure Document Intelligence:**\nFor enhanced PDF processing with better table extraction and layout analysis:\n```python\nfrom markitdown import MarkItDown\n\nmd = MarkItDown(docintel_endpoint=\"<endpoint>\", docintel_key=\"<key>\")\nresult = md.convert(\"complex.pdf\")\n```\n\n**LLM-Powered Image Descriptions:**\nGenerate detailed image descriptions using GPT-4o:\n```python\nfrom markitdown import MarkItDown\nfrom openai import OpenAI\n\nclient = OpenAI()\nmd = MarkItDown(llm_client=client, llm_model=\"gpt-4o\")\nresult = md.convert(\"presentation.pptx\")  # Images described with LLM\n```\n\nSee `references/advanced_integrations.md` for integration details.\n\n### 6. Batch Processing\n\nProcess multiple files or entire ZIP archives at once.\n\n**ZIP file processing:**\n```python\nfrom markitdown import MarkItDown\n\nmd = MarkItDown()\nresult = md.convert(\"archive.zip\")\nprint(result.text_content)  # All files converted and concatenated\n```\n\n**Batch script:**\nUse the provided batch processing script for directory conversion:\n```bash\npython scripts/batch_convert.py /path/to/documents /path/to/output\n```\n\nSee `scripts/batch_convert.py` for implementation details.\n\n## Installation\n\n**Full installation (all features):**\n```bash\nuv pip install 'markitdown[all]'\n```\n\n**Modular installation (specific features):**\n```bash\nuv pip install 'markitdown[pdf]'           # PDF support\nuv pip install 'markitdown[docx]'          # Word support\nuv pip install 'markitdown[pptx]'          # PowerPoint support\nuv pip install 'markitdown[xlsx]'          # Excel support\nuv pip install 'markitdown[audio]'         # Audio transcription\nuv pip install 'markitdown[youtube]'       # YouTube transcripts\n```\n\n**Requirements:**\n- Python 3.10 or higher\n\n## Output Format\n\nMarkItDown produces clean, token-efficient Markdown optimized for LLM consumption:\n- Preserves headings, lists, and tables\n- Maintains hyperlinks and formatting\n- Includes metadata where relevant (EXIF, document properties)\n- No temporary files created (streaming approach)\n\n## Common Workflows\n\n**Preparing documents for RAG:**\n```python\nfrom markitdown import MarkItDown\n\nmd = MarkItDown()\n\n# Convert knowledge base documents\ndocs = [\"manual.pdf\", \"guide.docx\", \"faq.html\"]\nmarkdown_content = []\n\nfor doc in docs:\n    result = md.convert(doc)\n    markdown_content.append(result.text_content)\n\n# Now ready for embedding and indexing\n```\n\n**Document analysis pipeline:**\n```bash\n# Convert all PDFs in directory\nfor file in documents/*.pdf; do\n    markitdown \"$file\" -o \"markdown/$(basename \"$file\" .pdf).md\"\ndone\n```\n\n## Plugin System\n\nMarkItDown supports extensible plugins for custom conversion logic. Plugins are disabled by default for security:\n\n```python\nfrom markitdown import MarkItDown\n\n# Enable plugins if needed\nmd = MarkItDown(enable_plugins=True)\n```\n\n## Resources\n\nThis skill includes comprehensive reference documentation for each capability:\n\n- **references/document_conversion.md** - Detailed PDF, DOCX, PPTX, XLSX conversion options\n- **references/media_processing.md** - Image OCR and audio transcription details\n- **references/web_content.md** - HTML, YouTube, and EPUB extraction\n- **references/structured_data.md** - CSV, JSON, XML conversion formats\n- **references/advanced_integrations.md** - Azure Document Intelligence and LLM integration\n- **scripts/batch_convert.py** - Batch processing utility for directories\n",
        "data/k-dense-ai/matchms/SKILL.md": "---\nname: matchms\ndescription: \"Mass spectrometry analysis. Process mzML/MGF/MSP, spectral similarity (cosine, modified cosine), metadata harmonization, compound ID, for metabolomics and MS data processing.\"\n---\n\n# Matchms\n\n## Overview\n\nMatchms is an open-source Python library for mass spectrometry data processing and analysis. Import spectra from various formats, standardize metadata, filter peaks, calculate spectral similarities, and build reproducible analytical workflows.\n\n## Core Capabilities\n\n### 1. Importing and Exporting Mass Spectrometry Data\n\nLoad spectra from multiple file formats and export processed data:\n\n```python\nfrom matchms.importing import load_from_mgf, load_from_mzml, load_from_msp, load_from_json\nfrom matchms.exporting import save_as_mgf, save_as_msp, save_as_json\n\n# Import spectra\nspectra = list(load_from_mgf(\"spectra.mgf\"))\nspectra = list(load_from_mzml(\"data.mzML\"))\nspectra = list(load_from_msp(\"library.msp\"))\n\n# Export processed spectra\nsave_as_mgf(spectra, \"output.mgf\")\nsave_as_json(spectra, \"output.json\")\n```\n\n**Supported formats:**\n- mzML and mzXML (raw mass spectrometry formats)\n- MGF (Mascot Generic Format)\n- MSP (spectral library format)\n- JSON (GNPS-compatible)\n- metabolomics-USI references\n- Pickle (Python serialization)\n\nFor detailed importing/exporting documentation, consult `references/importing_exporting.md`.\n\n### 2. Spectrum Filtering and Processing\n\nApply comprehensive filters to standardize metadata and refine peak data:\n\n```python\nfrom matchms.filtering import default_filters, normalize_intensities\nfrom matchms.filtering import select_by_relative_intensity, require_minimum_number_of_peaks\n\n# Apply default metadata harmonization filters\nspectrum = default_filters(spectrum)\n\n# Normalize peak intensities\nspectrum = normalize_intensities(spectrum)\n\n# Filter peaks by relative intensity\nspectrum = select_by_relative_intensity(spectrum, intensity_from=0.01, intensity_to=1.0)\n\n# Require minimum peaks\nspectrum = require_minimum_number_of_peaks(spectrum, n_required=5)\n```\n\n**Filter categories:**\n- **Metadata processing**: Harmonize compound names, derive chemical structures, standardize adducts, correct charges\n- **Peak filtering**: Normalize intensities, select by m/z or intensity, remove precursor peaks\n- **Quality control**: Require minimum peaks, validate precursor m/z, ensure metadata completeness\n- **Chemical annotation**: Add fingerprints, derive InChI/SMILES, repair structural mismatches\n\nMatchms provides 40+ filters. For the complete filter reference, consult `references/filtering.md`.\n\n### 3. Calculating Spectral Similarities\n\nCompare spectra using various similarity metrics:\n\n```python\nfrom matchms import calculate_scores\nfrom matchms.similarity import CosineGreedy, ModifiedCosine, CosineHungarian\n\n# Calculate cosine similarity (fast, greedy algorithm)\nscores = calculate_scores(references=library_spectra,\n                         queries=query_spectra,\n                         similarity_function=CosineGreedy())\n\n# Calculate modified cosine (accounts for precursor m/z differences)\nscores = calculate_scores(references=library_spectra,\n                         queries=query_spectra,\n                         similarity_function=ModifiedCosine(tolerance=0.1))\n\n# Get best matches\nbest_matches = scores.scores_by_query(query_spectra[0], sort=True)[:10]\n```\n\n**Available similarity functions:**\n- **CosineGreedy/CosineHungarian**: Peak-based cosine similarity with different matching algorithms\n- **ModifiedCosine**: Cosine similarity accounting for precursor mass differences\n- **NeutralLossesCosine**: Similarity based on neutral loss patterns\n- **FingerprintSimilarity**: Molecular structure similarity using fingerprints\n- **MetadataMatch**: Compare user-defined metadata fields\n- **PrecursorMzMatch/ParentMassMatch**: Simple mass-based filtering\n\nFor detailed similarity function documentation, consult `references/similarity.md`.\n\n### 4. Building Processing Pipelines\n\nCreate reproducible, multi-step analysis workflows:\n\n```python\nfrom matchms import SpectrumProcessor\nfrom matchms.filtering import default_filters, normalize_intensities\nfrom matchms.filtering import select_by_relative_intensity, remove_peaks_around_precursor_mz\n\n# Define a processing pipeline\nprocessor = SpectrumProcessor([\n    default_filters,\n    normalize_intensities,\n    lambda s: select_by_relative_intensity(s, intensity_from=0.01),\n    lambda s: remove_peaks_around_precursor_mz(s, mz_tolerance=17)\n])\n\n# Apply to all spectra\nprocessed_spectra = [processor(s) for s in spectra]\n```\n\n### 5. Working with Spectrum Objects\n\nThe core `Spectrum` class contains mass spectral data:\n\n```python\nfrom matchms import Spectrum\nimport numpy as np\n\n# Create a spectrum\nmz = np.array([100.0, 150.0, 200.0, 250.0])\nintensities = np.array([0.1, 0.5, 0.9, 0.3])\nmetadata = {\"precursor_mz\": 250.5, \"ionmode\": \"positive\"}\n\nspectrum = Spectrum(mz=mz, intensities=intensities, metadata=metadata)\n\n# Access spectrum properties\nprint(spectrum.peaks.mz)           # m/z values\nprint(spectrum.peaks.intensities)  # Intensity values\nprint(spectrum.get(\"precursor_mz\")) # Metadata field\n\n# Visualize spectra\nspectrum.plot()\nspectrum.plot_against(reference_spectrum)\n```\n\n### 6. Metadata Management\n\nStandardize and harmonize spectrum metadata:\n\n```python\n# Metadata is automatically harmonized\nspectrum.set(\"Precursor_mz\", 250.5)  # Gets harmonized to lowercase key\nprint(spectrum.get(\"precursor_mz\"))   # Returns 250.5\n\n# Derive chemical information\nfrom matchms.filtering import derive_inchi_from_smiles, derive_inchikey_from_inchi\nfrom matchms.filtering import add_fingerprint\n\nspectrum = derive_inchi_from_smiles(spectrum)\nspectrum = derive_inchikey_from_inchi(spectrum)\nspectrum = add_fingerprint(spectrum, fingerprint_type=\"morgan\", nbits=2048)\n```\n\n## Common Workflows\n\nFor typical mass spectrometry analysis workflows, including:\n- Loading and preprocessing spectral libraries\n- Matching unknown spectra against reference libraries\n- Quality filtering and data cleaning\n- Large-scale similarity comparisons\n- Network-based spectral clustering\n\nConsult `references/workflows.md` for detailed examples.\n\n## Installation\n\n```bash\nuv pip install matchms\n```\n\nFor molecular structure processing (SMILES, InChI):\n```bash\nuv pip install matchms[chemistry]\n```\n\n## Reference Documentation\n\nDetailed reference documentation is available in the `references/` directory:\n- `filtering.md` - Complete filter function reference with descriptions\n- `similarity.md` - All similarity metrics and when to use them\n- `importing_exporting.md` - File format details and I/O operations\n- `workflows.md` - Common analysis patterns and examples\n\nLoad these references as needed for detailed information about specific matchms capabilities.\n",
        "data/k-dense-ai/matplotlib/SKILL.md": "---\nname: matplotlib\ndescription: \"Foundational plotting library. Create line plots, scatter, bar, histograms, heatmaps, 3D, subplots, export PNG/PDF/SVG, for scientific visualization and publication figures.\"\n---\n\n# Matplotlib\n\n## Overview\n\nMatplotlib is Python's foundational visualization library for creating static, animated, and interactive plots. This skill provides guidance on using matplotlib effectively, covering both the pyplot interface (MATLAB-style) and the object-oriented API (Figure/Axes), along with best practices for creating publication-quality visualizations.\n\n## When to Use This Skill\n\nThis skill should be used when:\n- Creating any type of plot or chart (line, scatter, bar, histogram, heatmap, contour, etc.)\n- Generating scientific or statistical visualizations\n- Customizing plot appearance (colors, styles, labels, legends)\n- Creating multi-panel figures with subplots\n- Exporting visualizations to various formats (PNG, PDF, SVG, etc.)\n- Building interactive plots or animations\n- Working with 3D visualizations\n- Integrating plots into Jupyter notebooks or GUI applications\n\n## Core Concepts\n\n### The Matplotlib Hierarchy\n\nMatplotlib uses a hierarchical structure of objects:\n\n1. **Figure** - The top-level container for all plot elements\n2. **Axes** - The actual plotting area where data is displayed (one Figure can contain multiple Axes)\n3. **Artist** - Everything visible on the figure (lines, text, ticks, etc.)\n4. **Axis** - The number line objects (x-axis, y-axis) that handle ticks and labels\n\n### Two Interfaces\n\n**1. pyplot Interface (Implicit, MATLAB-style)**\n```python\nimport matplotlib.pyplot as plt\n\nplt.plot([1, 2, 3, 4])\nplt.ylabel('some numbers')\nplt.show()\n```\n- Convenient for quick, simple plots\n- Maintains state automatically\n- Good for interactive work and simple scripts\n\n**2. Object-Oriented Interface (Explicit)**\n```python\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\nax.plot([1, 2, 3, 4])\nax.set_ylabel('some numbers')\nplt.show()\n```\n- **Recommended for most use cases**\n- More explicit control over figure and axes\n- Better for complex figures with multiple subplots\n- Easier to maintain and debug\n\n## Common Workflows\n\n### 1. Basic Plot Creation\n\n**Single plot workflow:**\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Create figure and axes (OO interface - RECOMMENDED)\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Generate and plot data\nx = np.linspace(0, 2*np.pi, 100)\nax.plot(x, np.sin(x), label='sin(x)')\nax.plot(x, np.cos(x), label='cos(x)')\n\n# Customize\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_title('Trigonometric Functions')\nax.legend()\nax.grid(True, alpha=0.3)\n\n# Save and/or display\nplt.savefig('plot.png', dpi=300, bbox_inches='tight')\nplt.show()\n```\n\n### 2. Multiple Subplots\n\n**Creating subplot layouts:**\n```python\n# Method 1: Regular grid\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\naxes[0, 0].plot(x, y1)\naxes[0, 1].scatter(x, y2)\naxes[1, 0].bar(categories, values)\naxes[1, 1].hist(data, bins=30)\n\n# Method 2: Mosaic layout (more flexible)\nfig, axes = plt.subplot_mosaic([['left', 'right_top'],\n                                 ['left', 'right_bottom']],\n                                figsize=(10, 8))\naxes['left'].plot(x, y)\naxes['right_top'].scatter(x, y)\naxes['right_bottom'].hist(data)\n\n# Method 3: GridSpec (maximum control)\nfrom matplotlib.gridspec import GridSpec\nfig = plt.figure(figsize=(12, 8))\ngs = GridSpec(3, 3, figure=fig)\nax1 = fig.add_subplot(gs[0, :])  # Top row, all columns\nax2 = fig.add_subplot(gs[1:, 0])  # Bottom two rows, first column\nax3 = fig.add_subplot(gs[1:, 1:])  # Bottom two rows, last two columns\n```\n\n### 3. Plot Types and Use Cases\n\n**Line plots** - Time series, continuous data, trends\n```python\nax.plot(x, y, linewidth=2, linestyle='--', marker='o', color='blue')\n```\n\n**Scatter plots** - Relationships between variables, correlations\n```python\nax.scatter(x, y, s=sizes, c=colors, alpha=0.6, cmap='viridis')\n```\n\n**Bar charts** - Categorical comparisons\n```python\nax.bar(categories, values, color='steelblue', edgecolor='black')\n# For horizontal bars:\nax.barh(categories, values)\n```\n\n**Histograms** - Distributions\n```python\nax.hist(data, bins=30, edgecolor='black', alpha=0.7)\n```\n\n**Heatmaps** - Matrix data, correlations\n```python\nim = ax.imshow(matrix, cmap='coolwarm', aspect='auto')\nplt.colorbar(im, ax=ax)\n```\n\n**Contour plots** - 3D data on 2D plane\n```python\ncontour = ax.contour(X, Y, Z, levels=10)\nax.clabel(contour, inline=True, fontsize=8)\n```\n\n**Box plots** - Statistical distributions\n```python\nax.boxplot([data1, data2, data3], labels=['A', 'B', 'C'])\n```\n\n**Violin plots** - Distribution densities\n```python\nax.violinplot([data1, data2, data3], positions=[1, 2, 3])\n```\n\nFor comprehensive plot type examples and variations, refer to `references/plot_types.md`.\n\n### 4. Styling and Customization\n\n**Color specification methods:**\n- Named colors: `'red'`, `'blue'`, `'steelblue'`\n- Hex codes: `'#FF5733'`\n- RGB tuples: `(0.1, 0.2, 0.3)`\n- Colormaps: `cmap='viridis'`, `cmap='plasma'`, `cmap='coolwarm'`\n\n**Using style sheets:**\n```python\nplt.style.use('seaborn-v0_8-darkgrid')  # Apply predefined style\n# Available styles: 'ggplot', 'bmh', 'fivethirtyeight', etc.\nprint(plt.style.available)  # List all available styles\n```\n\n**Customizing with rcParams:**\n```python\nplt.rcParams['font.size'] = 12\nplt.rcParams['axes.labelsize'] = 14\nplt.rcParams['axes.titlesize'] = 16\nplt.rcParams['xtick.labelsize'] = 10\nplt.rcParams['ytick.labelsize'] = 10\nplt.rcParams['legend.fontsize'] = 12\nplt.rcParams['figure.titlesize'] = 18\n```\n\n**Text and annotations:**\n```python\nax.text(x, y, 'annotation', fontsize=12, ha='center')\nax.annotate('important point', xy=(x, y), xytext=(x+1, y+1),\n            arrowprops=dict(arrowstyle='->', color='red'))\n```\n\nFor detailed styling options and colormap guidelines, see `references/styling_guide.md`.\n\n### 5. Saving Figures\n\n**Export to various formats:**\n```python\n# High-resolution PNG for presentations/papers\nplt.savefig('figure.png', dpi=300, bbox_inches='tight', facecolor='white')\n\n# Vector format for publications (scalable)\nplt.savefig('figure.pdf', bbox_inches='tight')\nplt.savefig('figure.svg', bbox_inches='tight')\n\n# Transparent background\nplt.savefig('figure.png', dpi=300, bbox_inches='tight', transparent=True)\n```\n\n**Important parameters:**\n- `dpi`: Resolution (300 for publications, 150 for web, 72 for screen)\n- `bbox_inches='tight'`: Removes excess whitespace\n- `facecolor='white'`: Ensures white background (useful for transparent themes)\n- `transparent=True`: Transparent background\n\n### 6. Working with 3D Plots\n\n```python\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfig = plt.figure(figsize=(10, 8))\nax = fig.add_subplot(111, projection='3d')\n\n# Surface plot\nax.plot_surface(X, Y, Z, cmap='viridis')\n\n# 3D scatter\nax.scatter(x, y, z, c=colors, marker='o')\n\n# 3D line plot\nax.plot(x, y, z, linewidth=2)\n\n# Labels\nax.set_xlabel('X Label')\nax.set_ylabel('Y Label')\nax.set_zlabel('Z Label')\n```\n\n## Best Practices\n\n### 1. Interface Selection\n- **Use the object-oriented interface** (fig, ax = plt.subplots()) for production code\n- Reserve pyplot interface for quick interactive exploration only\n- Always create figures explicitly rather than relying on implicit state\n\n### 2. Figure Size and DPI\n- Set figsize at creation: `fig, ax = plt.subplots(figsize=(10, 6))`\n- Use appropriate DPI for output medium:\n  - Screen/notebook: 72-100 dpi\n  - Web: 150 dpi\n  - Print/publications: 300 dpi\n\n### 3. Layout Management\n- Use `constrained_layout=True` or `tight_layout()` to prevent overlapping elements\n- `fig, ax = plt.subplots(constrained_layout=True)` is recommended for automatic spacing\n\n### 4. Colormap Selection\n- **Sequential** (viridis, plasma, inferno): Ordered data with consistent progression\n- **Diverging** (coolwarm, RdBu): Data with meaningful center point (e.g., zero)\n- **Qualitative** (tab10, Set3): Categorical/nominal data\n- Avoid rainbow colormaps (jet) - they are not perceptually uniform\n\n### 5. Accessibility\n- Use colorblind-friendly colormaps (viridis, cividis)\n- Add patterns/hatching for bar charts in addition to colors\n- Ensure sufficient contrast between elements\n- Include descriptive labels and legends\n\n### 6. Performance\n- For large datasets, use `rasterized=True` in plot calls to reduce file size\n- Use appropriate data reduction before plotting (e.g., downsample dense time series)\n- For animations, use blitting for better performance\n\n### 7. Code Organization\n```python\n# Good practice: Clear structure\ndef create_analysis_plot(data, title):\n    \"\"\"Create standardized analysis plot.\"\"\"\n    fig, ax = plt.subplots(figsize=(10, 6), constrained_layout=True)\n\n    # Plot data\n    ax.plot(data['x'], data['y'], linewidth=2)\n\n    # Customize\n    ax.set_xlabel('X Axis Label', fontsize=12)\n    ax.set_ylabel('Y Axis Label', fontsize=12)\n    ax.set_title(title, fontsize=14, fontweight='bold')\n    ax.grid(True, alpha=0.3)\n\n    return fig, ax\n\n# Use the function\nfig, ax = create_analysis_plot(my_data, 'My Analysis')\nplt.savefig('analysis.png', dpi=300, bbox_inches='tight')\n```\n\n## Quick Reference Scripts\n\nThis skill includes helper scripts in the `scripts/` directory:\n\n### `plot_template.py`\nTemplate script demonstrating various plot types with best practices. Use this as a starting point for creating new visualizations.\n\n**Usage:**\n```bash\npython scripts/plot_template.py\n```\n\n### `style_configurator.py`\nInteractive utility to configure matplotlib style preferences and generate custom style sheets.\n\n**Usage:**\n```bash\npython scripts/style_configurator.py\n```\n\n## Detailed References\n\nFor comprehensive information, consult the reference documents:\n\n- **`references/plot_types.md`** - Complete catalog of plot types with code examples and use cases\n- **`references/styling_guide.md`** - Detailed styling options, colormaps, and customization\n- **`references/api_reference.md`** - Core classes and methods reference\n- **`references/common_issues.md`** - Troubleshooting guide for common problems\n\n## Integration with Other Tools\n\nMatplotlib integrates well with:\n- **NumPy/Pandas** - Direct plotting from arrays and DataFrames\n- **Seaborn** - High-level statistical visualizations built on matplotlib\n- **Jupyter** - Interactive plotting with `%matplotlib inline` or `%matplotlib widget`\n- **GUI frameworks** - Embedding in Tkinter, Qt, wxPython applications\n\n## Common Gotchas\n\n1. **Overlapping elements**: Use `constrained_layout=True` or `tight_layout()`\n2. **State confusion**: Use OO interface to avoid pyplot state machine issues\n3. **Memory issues with many figures**: Close figures explicitly with `plt.close(fig)`\n4. **Font warnings**: Install fonts or suppress warnings with `plt.rcParams['font.sans-serif']`\n5. **DPI confusion**: Remember that figsize is in inches, not pixels: `pixels = dpi * inches`\n\n## Additional Resources\n\n- Official documentation: https://matplotlib.org/\n- Gallery: https://matplotlib.org/stable/gallery/index.html\n- Cheatsheets: https://matplotlib.org/cheatsheets/\n- Tutorials: https://matplotlib.org/stable/tutorials/index.html\n",
        "data/k-dense-ai/medchem/SKILL.md": "---\nname: medchem\ndescription: \"Medicinal chemistry filters. Apply drug-likeness rules (Lipinski, Veber), PAINS filters, structural alerts, complexity metrics, for compound prioritization and library filtering.\"\n---\n\n# Medchem\n\n## Overview\n\nMedchem is a Python library for molecular filtering and prioritization in drug discovery workflows. Apply hundreds of well-established and novel molecular filters, structural alerts, and medicinal chemistry rules to efficiently triage and prioritize compound libraries at scale. Rules and filters are context-specificuse as guidelines combined with domain expertise.\n\n## When to Use This Skill\n\nThis skill should be used when:\n- Applying drug-likeness rules (Lipinski, Veber, etc.) to compound libraries\n- Filtering molecules by structural alerts or PAINS patterns\n- Prioritizing compounds for lead optimization\n- Assessing compound quality and medicinal chemistry properties\n- Detecting reactive or problematic functional groups\n- Calculating molecular complexity metrics\n\n## Installation\n\n```bash\nuv pip install medchem\n```\n\n## Core Capabilities\n\n### 1. Medicinal Chemistry Rules\n\nApply established drug-likeness rules to molecules using the `medchem.rules` module.\n\n**Available Rules:**\n- Rule of Five (Lipinski)\n- Rule of Oprea\n- Rule of CNS\n- Rule of leadlike (soft and strict)\n- Rule of three\n- Rule of Reos\n- Rule of drug\n- Rule of Veber\n- Golden triangle\n- PAINS filters\n\n**Single Rule Application:**\n\n```python\nimport medchem as mc\n\n# Apply Rule of Five to a SMILES string\nsmiles = \"CC(=O)OC1=CC=CC=C1C(=O)O\"  # Aspirin\npasses = mc.rules.basic_rules.rule_of_five(smiles)\n# Returns: True\n\n# Check specific rules\npasses_oprea = mc.rules.basic_rules.rule_of_oprea(smiles)\npasses_cns = mc.rules.basic_rules.rule_of_cns(smiles)\n```\n\n**Multiple Rules with RuleFilters:**\n\n```python\nimport datamol as dm\nimport medchem as mc\n\n# Load molecules\nmols = [dm.to_mol(smiles) for smiles in smiles_list]\n\n# Create filter with multiple rules\nrfilter = mc.rules.RuleFilters(\n    rule_list=[\n        \"rule_of_five\",\n        \"rule_of_oprea\",\n        \"rule_of_cns\",\n        \"rule_of_leadlike_soft\"\n    ]\n)\n\n# Apply filters with parallelization\nresults = rfilter(\n    mols=mols,\n    n_jobs=-1,  # Use all CPU cores\n    progress=True\n)\n```\n\n**Result Format:**\nResults are returned as dictionaries with pass/fail status and detailed information for each rule.\n\n### 2. Structural Alert Filters\n\nDetect potentially problematic structural patterns using the `medchem.structural` module.\n\n**Available Filters:**\n\n1. **Common Alerts** - General structural alerts derived from ChEMBL curation and literature\n2. **NIBR Filters** - Novartis Institutes for BioMedical Research filter set\n3. **Lilly Demerits** - Eli Lilly's demerit-based system (275 rules, molecules rejected at >100 demerits)\n\n**Common Alerts:**\n\n```python\nimport medchem as mc\n\n# Create filter\nalert_filter = mc.structural.CommonAlertsFilters()\n\n# Check single molecule\nmol = dm.to_mol(\"c1ccccc1\")\nhas_alerts, details = alert_filter.check_mol(mol)\n\n# Batch filtering with parallelization\nresults = alert_filter(\n    mols=mol_list,\n    n_jobs=-1,\n    progress=True\n)\n```\n\n**NIBR Filters:**\n\n```python\nimport medchem as mc\n\n# Apply NIBR filters\nnibr_filter = mc.structural.NIBRFilters()\nresults = nibr_filter(mols=mol_list, n_jobs=-1)\n```\n\n**Lilly Demerits:**\n\n```python\nimport medchem as mc\n\n# Calculate Lilly demerits\nlilly = mc.structural.LillyDemeritsFilters()\nresults = lilly(mols=mol_list, n_jobs=-1)\n\n# Each result includes demerit score and whether it passes (100 demerits)\n```\n\n### 3. Functional API for High-Level Operations\n\nThe `medchem.functional` module provides convenient functions for common workflows.\n\n**Quick Filtering:**\n\n```python\nimport medchem as mc\n\n# Apply NIBR filters to a list\nfilter_ok = mc.functional.nibr_filter(\n    mols=mol_list,\n    n_jobs=-1\n)\n\n# Apply common alerts\nalert_results = mc.functional.common_alerts_filter(\n    mols=mol_list,\n    n_jobs=-1\n)\n```\n\n### 4. Chemical Groups Detection\n\nIdentify specific chemical groups and functional groups using `medchem.groups`.\n\n**Available Groups:**\n- Hinge binders\n- Phosphate binders\n- Michael acceptors\n- Reactive groups\n- Custom SMARTS patterns\n\n**Usage:**\n\n```python\nimport medchem as mc\n\n# Create group detector\ngroup = mc.groups.ChemicalGroup(groups=[\"hinge_binders\"])\n\n# Check for matches\nhas_matches = group.has_match(mol_list)\n\n# Get detailed match information\nmatches = group.get_matches(mol)\n```\n\n### 5. Named Catalogs\n\nAccess curated collections of chemical structures through `medchem.catalogs`.\n\n**Available Catalogs:**\n- Functional groups\n- Protecting groups\n- Common reagents\n- Standard fragments\n\n**Usage:**\n\n```python\nimport medchem as mc\n\n# Access named catalogs\ncatalogs = mc.catalogs.NamedCatalogs\n\n# Use catalog for matching\ncatalog = catalogs.get(\"functional_groups\")\nmatches = catalog.get_matches(mol)\n```\n\n### 6. Molecular Complexity\n\nCalculate complexity metrics that approximate synthetic accessibility using `medchem.complexity`.\n\n**Common Metrics:**\n- Bertz complexity\n- Whitlock complexity\n- Barone complexity\n\n**Usage:**\n\n```python\nimport medchem as mc\n\n# Calculate complexity\ncomplexity_score = mc.complexity.calculate_complexity(mol)\n\n# Filter by complexity threshold\ncomplex_filter = mc.complexity.ComplexityFilter(max_complexity=500)\nresults = complex_filter(mols=mol_list)\n```\n\n### 7. Constraints Filtering\n\nApply custom property-based constraints using `medchem.constraints`.\n\n**Example Constraints:**\n- Molecular weight ranges\n- LogP bounds\n- TPSA limits\n- Rotatable bond counts\n\n**Usage:**\n\n```python\nimport medchem as mc\n\n# Define constraints\nconstraints = mc.constraints.Constraints(\n    mw_range=(200, 500),\n    logp_range=(-2, 5),\n    tpsa_max=140,\n    rotatable_bonds_max=10\n)\n\n# Apply constraints\nresults = constraints(mols=mol_list, n_jobs=-1)\n```\n\n### 8. Medchem Query Language\n\nUse a specialized query language for complex filtering criteria.\n\n**Query Examples:**\n```\n# Molecules passing Ro5 AND not having common alerts\n\"rule_of_five AND NOT common_alerts\"\n\n# CNS-like molecules with low complexity\n\"rule_of_cns AND complexity < 400\"\n\n# Leadlike molecules without Lilly demerits\n\"rule_of_leadlike AND lilly_demerits == 0\"\n```\n\n**Usage:**\n\n```python\nimport medchem as mc\n\n# Parse and apply query\nquery = mc.query.parse(\"rule_of_five AND NOT common_alerts\")\nresults = query.apply(mols=mol_list, n_jobs=-1)\n```\n\n## Workflow Patterns\n\n### Pattern 1: Initial Triage of Compound Library\n\nFilter a large compound collection to identify drug-like candidates.\n\n```python\nimport datamol as dm\nimport medchem as mc\nimport pandas as pd\n\n# Load compound library\ndf = pd.read_csv(\"compounds.csv\")\nmols = [dm.to_mol(smi) for smi in df[\"smiles\"]]\n\n# Apply primary filters\nrule_filter = mc.rules.RuleFilters(rule_list=[\"rule_of_five\", \"rule_of_veber\"])\nrule_results = rule_filter(mols=mols, n_jobs=-1, progress=True)\n\n# Apply structural alerts\nalert_filter = mc.structural.CommonAlertsFilters()\nalert_results = alert_filter(mols=mols, n_jobs=-1, progress=True)\n\n# Combine results\ndf[\"passes_rules\"] = rule_results[\"pass\"]\ndf[\"has_alerts\"] = alert_results[\"has_alerts\"]\ndf[\"drug_like\"] = df[\"passes_rules\"] & ~df[\"has_alerts\"]\n\n# Save filtered compounds\nfiltered_df = df[df[\"drug_like\"]]\nfiltered_df.to_csv(\"filtered_compounds.csv\", index=False)\n```\n\n### Pattern 2: Lead Optimization Filtering\n\nApply stricter criteria during lead optimization.\n\n```python\nimport medchem as mc\n\n# Create comprehensive filter\nfilters = {\n    \"rules\": mc.rules.RuleFilters(rule_list=[\"rule_of_leadlike_strict\"]),\n    \"alerts\": mc.structural.NIBRFilters(),\n    \"lilly\": mc.structural.LillyDemeritsFilters(),\n    \"complexity\": mc.complexity.ComplexityFilter(max_complexity=400)\n}\n\n# Apply all filters\nresults = {}\nfor name, filt in filters.items():\n    results[name] = filt(mols=candidate_mols, n_jobs=-1)\n\n# Identify compounds passing all filters\npasses_all = all(r[\"pass\"] for r in results.values())\n```\n\n### Pattern 3: Identify Specific Chemical Groups\n\nFind molecules containing specific functional groups or scaffolds.\n\n```python\nimport medchem as mc\n\n# Create group detector for multiple groups\ngroup_detector = mc.groups.ChemicalGroup(\n    groups=[\"hinge_binders\", \"phosphate_binders\"]\n)\n\n# Screen library\nmatches = group_detector.get_all_matches(mol_list)\n\n# Filter molecules with desired groups\nmol_with_groups = [mol for mol, match in zip(mol_list, matches) if match]\n```\n\n## Best Practices\n\n1. **Context Matters**: Don't blindly apply filters. Understand the biological target and chemical space.\n\n2. **Combine Multiple Filters**: Use rules, structural alerts, and domain knowledge together for better decisions.\n\n3. **Use Parallelization**: For large datasets (>1000 molecules), always use `n_jobs=-1` for parallel processing.\n\n4. **Iterative Refinement**: Start with broad filters (Ro5), then apply more specific criteria (CNS, leadlike) as needed.\n\n5. **Document Filtering Decisions**: Track which molecules were filtered out and why for reproducibility.\n\n6. **Validate Results**: Remember that marketed drugs often fail standard filtersuse these as guidelines, not absolute rules.\n\n7. **Consider Prodrugs**: Molecules designed as prodrugs may intentionally violate standard medicinal chemistry rules.\n\n## Resources\n\n### references/api_guide.md\nComprehensive API reference covering all medchem modules with detailed function signatures, parameters, and return types.\n\n### references/rules_catalog.md\nComplete catalog of available rules, filters, and alerts with descriptions, thresholds, and literature references.\n\n### scripts/filter_molecules.py\nProduction-ready script for batch filtering workflows. Supports multiple input formats (CSV, SDF, SMILES), configurable filter combinations, and detailed reporting.\n\n**Usage:**\n```bash\npython scripts/filter_molecules.py input.csv --rules rule_of_five,rule_of_cns --alerts nibr --output filtered.csv\n```\n\n## Documentation\n\nOfficial documentation: https://medchem-docs.datamol.io/\nGitHub repository: https://github.com/datamol-io/medchem\n",
        "data/k-dense-ai/metabolomics-workbench-database/SKILL.md": "---\nname: metabolomics-workbench-database\ndescription: \"Access NIH Metabolomics Workbench via REST API (4,200+ studies). Query metabolites, RefMet nomenclature, MS/NMR data, m/z searches, study metadata, for metabolomics and biomarker discovery.\"\n---\n\n# Metabolomics Workbench Database\n\n## Overview\n\nThe Metabolomics Workbench is a comprehensive NIH Common Fund-sponsored platform hosted at UCSD that serves as the primary repository for metabolomics research data. It provides programmatic access to over 4,200 processed studies (3,790+ publicly available), standardized metabolite nomenclature through RefMet, and powerful search capabilities across multiple analytical platforms (GC-MS, LC-MS, NMR).\n\n## When to Use This Skill\n\nThis skill should be used when querying metabolite structures, accessing study data, standardizing nomenclature, performing mass spectrometry searches, or retrieving gene/protein-metabolite associations through the Metabolomics Workbench REST API.\n\n## Core Capabilities\n\n### 1. Querying Metabolite Structures and Data\n\nAccess comprehensive metabolite information including structures, identifiers, and cross-references to external databases.\n\n**Key operations:**\n- Retrieve compound data by various identifiers (PubChem CID, InChI Key, KEGG ID, HMDB ID, etc.)\n- Download molecular structures as MOL files or PNG images\n- Access standardized compound classifications\n- Cross-reference between different metabolite databases\n\n**Example queries:**\n```python\nimport requests\n\n# Get compound information by PubChem CID\nresponse = requests.get('https://www.metabolomicsworkbench.org/rest/compound/pubchem_cid/5281365/all/json')\n\n# Download molecular structure as PNG\nresponse = requests.get('https://www.metabolomicsworkbench.org/rest/compound/regno/11/png')\n\n# Get compound name by registry number\nresponse = requests.get('https://www.metabolomicsworkbench.org/rest/compound/regno/11/name/json')\n```\n\n### 2. Accessing Study Metadata and Experimental Results\n\nQuery metabolomics studies by various criteria and retrieve complete experimental datasets.\n\n**Key operations:**\n- Search studies by metabolite, institute, investigator, or title\n- Access study summaries, experimental factors, and analysis details\n- Retrieve complete experimental data in various formats\n- Download mwTab format files for complete study information\n- Query untargeted metabolomics data\n\n**Example queries:**\n```python\n# List all available public studies\nresponse = requests.get('https://www.metabolomicsworkbench.org/rest/study/study_id/ST/available/json')\n\n# Get study summary\nresponse = requests.get('https://www.metabolomicsworkbench.org/rest/study/study_id/ST000001/summary/json')\n\n# Retrieve experimental data\nresponse = requests.get('https://www.metabolomicsworkbench.org/rest/study/study_id/ST000001/data/json')\n\n# Find studies containing a specific metabolite\nresponse = requests.get('https://www.metabolomicsworkbench.org/rest/study/refmet_name/Tyrosine/summary/json')\n```\n\n### 3. Standardizing Metabolite Nomenclature with RefMet\n\nUse the RefMet database to standardize metabolite names and access systematic classification across four structural resolution levels.\n\n**Key operations:**\n- Match common metabolite names to standardized RefMet names\n- Query by chemical formula, exact mass, or InChI Key\n- Access hierarchical classification (super class, main class, sub class)\n- Retrieve all RefMet entries or filter by classification\n\n**Example queries:**\n```python\n# Standardize a metabolite name\nresponse = requests.get('https://www.metabolomicsworkbench.org/rest/refmet/match/citrate/name/json')\n\n# Query by molecular formula\nresponse = requests.get('https://www.metabolomicsworkbench.org/rest/refmet/formula/C12H24O2/all/json')\n\n# Get all metabolites in a specific class\nresponse = requests.get('https://www.metabolomicsworkbench.org/rest/refmet/main_class/Fatty%20Acids/all/json')\n\n# Retrieve complete RefMet database\nresponse = requests.get('https://www.metabolomicsworkbench.org/rest/refmet/all/json')\n```\n\n### 4. Performing Mass Spectrometry Searches\n\nSearch for compounds by mass-to-charge ratio (m/z) with specified ion adducts and tolerance levels.\n\n**Key operations:**\n- Search precursor ion masses across multiple databases (Metabolomics Workbench, LIPIDS, RefMet)\n- Specify ion adduct types (M+H, M-H, M+Na, M+NH4, M+2H, etc.)\n- Calculate exact masses for known metabolites with specific adducts\n- Set mass tolerance for flexible matching\n\n**Example queries:**\n```python\n# Search by m/z value with M+H adduct\nresponse = requests.get('https://www.metabolomicsworkbench.org/rest/moverz/MB/635.52/M+H/0.5/json')\n\n# Calculate exact mass for a metabolite with specific adduct\nresponse = requests.get('https://www.metabolomicsworkbench.org/rest/moverz/exactmass/PC(34:1)/M+H/json')\n\n# Search across RefMet database\nresponse = requests.get('https://www.metabolomicsworkbench.org/rest/moverz/REFMET/200.15/M-H/0.3/json')\n```\n\n### 5. Filtering Studies by Analytical and Biological Parameters\n\nUse the MetStat context to find studies matching specific experimental conditions.\n\n**Key operations:**\n- Filter by analytical method (LCMS, GCMS, NMR)\n- Specify ionization polarity (POSITIVE, NEGATIVE)\n- Filter by chromatography type (HILIC, RP, GC)\n- Target specific species, sample sources, or diseases\n- Combine multiple filters using semicolon-delimited format\n\n**Example queries:**\n```python\n# Find human blood studies on diabetes using LC-MS\nresponse = requests.get('https://www.metabolomicsworkbench.org/rest/metstat/LCMS;POSITIVE;HILIC;Human;Blood;Diabetes/json')\n\n# Find all human blood studies containing tyrosine\nresponse = requests.get('https://www.metabolomicsworkbench.org/rest/metstat/;;;Human;Blood;;;Tyrosine/json')\n\n# Filter by analytical method only\nresponse = requests.get('https://www.metabolomicsworkbench.org/rest/metstat/GCMS;;;;;;/json')\n```\n\n### 6. Accessing Gene and Protein Information\n\nRetrieve gene and protein data associated with metabolic pathways and metabolite metabolism.\n\n**Key operations:**\n- Query genes by symbol, name, or ID\n- Access protein sequences and annotations\n- Cross-reference between gene IDs, RefSeq IDs, and UniProt IDs\n- Retrieve gene-metabolite associations\n\n**Example queries:**\n```python\n# Get gene information by symbol\nresponse = requests.get('https://www.metabolomicsworkbench.org/rest/gene/gene_symbol/ACACA/all/json')\n\n# Retrieve protein data by UniProt ID\nresponse = requests.get('https://www.metabolomicsworkbench.org/rest/protein/uniprot_id/Q13085/all/json')\n```\n\n## Common Workflows\n\n### Workflow 1: Finding Studies for a Specific Metabolite\n\nTo find all studies containing measurements of a specific metabolite:\n\n1. First standardize the metabolite name using RefMet:\n   ```python\n   response = requests.get('https://www.metabolomicsworkbench.org/rest/refmet/match/glucose/name/json')\n   ```\n\n2. Use the standardized name to search for studies:\n   ```python\n   response = requests.get('https://www.metabolomicsworkbench.org/rest/study/refmet_name/Glucose/summary/json')\n   ```\n\n3. Retrieve experimental data from specific studies:\n   ```python\n   response = requests.get('https://www.metabolomicsworkbench.org/rest/study/study_id/ST000001/data/json')\n   ```\n\n### Workflow 2: Identifying Compounds from MS Data\n\nTo identify potential compounds from mass spectrometry m/z values:\n\n1. Perform m/z search with appropriate adduct and tolerance:\n   ```python\n   response = requests.get('https://www.metabolomicsworkbench.org/rest/moverz/MB/180.06/M+H/0.5/json')\n   ```\n\n2. Review candidate compounds from results\n\n3. Retrieve detailed information for candidate compounds:\n   ```python\n   response = requests.get('https://www.metabolomicsworkbench.org/rest/compound/regno/{regno}/all/json')\n   ```\n\n4. Download structures for confirmation:\n   ```python\n   response = requests.get('https://www.metabolomicsworkbench.org/rest/compound/regno/{regno}/png')\n   ```\n\n### Workflow 3: Exploring Disease-Specific Metabolomics\n\nTo find metabolomics studies for a specific disease and analytical platform:\n\n1. Use MetStat to filter studies:\n   ```python\n   response = requests.get('https://www.metabolomicsworkbench.org/rest/metstat/LCMS;POSITIVE;;Human;;Cancer/json')\n   ```\n\n2. Review study IDs from results\n\n3. Access detailed study information:\n   ```python\n   response = requests.get('https://www.metabolomicsworkbench.org/rest/study/study_id/ST{ID}/summary/json')\n   ```\n\n4. Retrieve complete experimental data:\n   ```python\n   response = requests.get('https://www.metabolomicsworkbench.org/rest/study/study_id/ST{ID}/data/json')\n   ```\n\n## Output Formats\n\nThe API supports two primary output formats:\n- **JSON** (default): Machine-readable format, ideal for programmatic access\n- **TXT**: Human-readable tab-delimited text format\n\nSpecify format by appending `/json` or `/txt` to API URLs. When format is omitted, JSON is returned by default.\n\n## Best Practices\n\n1. **Use RefMet for standardization**: Always standardize metabolite names through RefMet before searching studies to ensure consistent nomenclature\n\n2. **Specify appropriate adducts**: When performing m/z searches, use the correct ion adduct type for your analytical method (e.g., M+H for positive mode ESI)\n\n3. **Set reasonable tolerances**: Use appropriate mass tolerance values (typically 0.5 Da for low-resolution, 0.01 Da for high-resolution MS)\n\n4. **Cache reference data**: Consider caching frequently used reference data (RefMet database, compound information) to minimize API calls\n\n5. **Handle pagination**: For large result sets, be prepared to handle multiple data structures in responses\n\n6. **Validate identifiers**: Cross-reference metabolite identifiers across multiple databases when possible to ensure correct compound identification\n\n## Resources\n\n### references/\n\nDetailed API reference documentation is available in `references/api_reference.md`, including:\n- Complete REST API endpoint specifications\n- All available contexts (compound, study, refmet, metstat, gene, protein, moverz)\n- Input/output parameter details\n- Ion adduct types for mass spectrometry\n- Additional query examples\n\nLoad this reference file when detailed API specifications are needed or when working with less common endpoints.\n",
        "data/k-dense-ai/modal/SKILL.md": "---\nname: modal\ndescription: Run Python code in the cloud with serverless containers, GPUs, and autoscaling. Use when deploying ML models, running batch processing jobs, scheduling compute-intensive tasks, or serving APIs that require GPU acceleration or dynamic scaling.\n---\n\n# Modal\n\n## Overview\n\nModal is a serverless platform for running Python code in the cloud with minimal configuration. Execute functions on powerful GPUs, scale automatically to thousands of containers, and pay only for compute used.\n\nModal is particularly suited for AI/ML workloads, high-performance batch processing, scheduled jobs, GPU inference, and serverless APIs. Sign up for free at https://modal.com and receive $30/month in credits.\n\n## When to Use This Skill\n\nUse Modal for:\n- Deploying and serving ML models (LLMs, image generation, embedding models)\n- Running GPU-accelerated computation (training, inference, rendering)\n- Batch processing large datasets in parallel\n- Scheduling compute-intensive jobs (daily data processing, model training)\n- Building serverless APIs that need automatic scaling\n- Scientific computing requiring distributed compute or specialized hardware\n\n## Authentication and Setup\n\nModal requires authentication via API token.\n\n### Initial Setup\n\n```bash\n# Install Modal\nuv uv pip install modal\n\n# Authenticate (opens browser for login)\nmodal token new\n```\n\nThis creates a token stored in `~/.modal.toml`. The token authenticates all Modal operations.\n\n### Verify Setup\n\n```python\nimport modal\n\napp = modal.App(\"test-app\")\n\n@app.function()\ndef hello():\n    print(\"Modal is working!\")\n```\n\nRun with: `modal run script.py`\n\n## Core Capabilities\n\nModal provides serverless Python execution through Functions that run in containers. Define compute requirements, dependencies, and scaling behavior declaratively.\n\n### 1. Define Container Images\n\nSpecify dependencies and environment for functions using Modal Images.\n\n```python\nimport modal\n\n# Basic image with Python packages\nimage = (\n    modal.Image.debian_slim(python_version=\"3.12\")\n    .uv_pip_install(\"torch\", \"transformers\", \"numpy\")\n)\n\napp = modal.App(\"ml-app\", image=image)\n```\n\n**Common patterns:**\n- Install Python packages: `.uv_pip_install(\"pandas\", \"scikit-learn\")`\n- Install system packages: `.apt_install(\"ffmpeg\", \"git\")`\n- Use existing Docker images: `modal.Image.from_registry(\"nvidia/cuda:12.1.0-base\")`\n- Add local code: `.add_local_python_source(\"my_module\")`\n\nSee `references/images.md` for comprehensive image building documentation.\n\n### 2. Create Functions\n\nDefine functions that run in the cloud with the `@app.function()` decorator.\n\n```python\n@app.function()\ndef process_data(file_path: str):\n    import pandas as pd\n    df = pd.read_csv(file_path)\n    return df.describe()\n```\n\n**Call functions:**\n```python\n# From local entrypoint\n@app.local_entrypoint()\ndef main():\n    result = process_data.remote(\"data.csv\")\n    print(result)\n```\n\nRun with: `modal run script.py`\n\nSee `references/functions.md` for function patterns, deployment, and parameter handling.\n\n### 3. Request GPUs\n\nAttach GPUs to functions for accelerated computation.\n\n```python\n@app.function(gpu=\"H100\")\ndef train_model():\n    import torch\n    assert torch.cuda.is_available()\n    # GPU-accelerated code here\n```\n\n**Available GPU types:**\n- `T4`, `L4` - Cost-effective inference\n- `A10`, `A100`, `A100-80GB` - Standard training/inference\n- `L40S` - Excellent cost/performance balance (48GB)\n- `H100`, `H200` - High-performance training\n- `B200` - Flagship performance (most powerful)\n\n**Request multiple GPUs:**\n```python\n@app.function(gpu=\"H100:8\")  # 8x H100 GPUs\ndef train_large_model():\n    pass\n```\n\nSee `references/gpu.md` for GPU selection guidance, CUDA setup, and multi-GPU configuration.\n\n### 4. Configure Resources\n\nRequest CPU cores, memory, and disk for functions.\n\n```python\n@app.function(\n    cpu=8.0,           # 8 physical cores\n    memory=32768,      # 32 GiB RAM\n    ephemeral_disk=10240  # 10 GiB disk\n)\ndef memory_intensive_task():\n    pass\n```\n\nDefault allocation: 0.125 CPU cores, 128 MiB memory. Billing based on reservation or actual usage, whichever is higher.\n\nSee `references/resources.md` for resource limits and billing details.\n\n### 5. Scale Automatically\n\nModal autoscales functions from zero to thousands of containers based on demand.\n\n**Process inputs in parallel:**\n```python\n@app.function()\ndef analyze_sample(sample_id: int):\n    # Process single sample\n    return result\n\n@app.local_entrypoint()\ndef main():\n    sample_ids = range(1000)\n    # Automatically parallelized across containers\n    results = list(analyze_sample.map(sample_ids))\n```\n\n**Configure autoscaling:**\n```python\n@app.function(\n    max_containers=100,      # Upper limit\n    min_containers=2,        # Keep warm\n    buffer_containers=5      # Idle buffer for bursts\n)\ndef inference():\n    pass\n```\n\nSee `references/scaling.md` for autoscaling configuration, concurrency, and scaling limits.\n\n### 6. Store Data Persistently\n\nUse Volumes for persistent storage across function invocations.\n\n```python\nvolume = modal.Volume.from_name(\"my-data\", create_if_missing=True)\n\n@app.function(volumes={\"/data\": volume})\ndef save_results(data):\n    with open(\"/data/results.txt\", \"w\") as f:\n        f.write(data)\n    volume.commit()  # Persist changes\n```\n\nVolumes persist data between runs, store model weights, cache datasets, and share data between functions.\n\nSee `references/volumes.md` for volume management, commits, and caching patterns.\n\n### 7. Manage Secrets\n\nStore API keys and credentials securely using Modal Secrets.\n\n```python\n@app.function(secrets=[modal.Secret.from_name(\"huggingface\")])\ndef download_model():\n    import os\n    token = os.environ[\"HF_TOKEN\"]\n    # Use token for authentication\n```\n\n**Create secrets in Modal dashboard or via CLI:**\n```bash\nmodal secret create my-secret KEY=value API_TOKEN=xyz\n```\n\nSee `references/secrets.md` for secret management and authentication patterns.\n\n### 8. Deploy Web Endpoints\n\nServe HTTP endpoints, APIs, and webhooks with `@modal.web_endpoint()`.\n\n```python\n@app.function()\n@modal.web_endpoint(method=\"POST\")\ndef predict(data: dict):\n    # Process request\n    result = model.predict(data[\"input\"])\n    return {\"prediction\": result}\n```\n\n**Deploy with:**\n```bash\nmodal deploy script.py\n```\n\nModal provides HTTPS URL for the endpoint.\n\nSee `references/web-endpoints.md` for FastAPI integration, streaming, authentication, and WebSocket support.\n\n### 9. Schedule Jobs\n\nRun functions on a schedule with cron expressions.\n\n```python\n@app.function(schedule=modal.Cron(\"0 2 * * *\"))  # Daily at 2 AM\ndef daily_backup():\n    # Backup data\n    pass\n\n@app.function(schedule=modal.Period(hours=4))  # Every 4 hours\ndef refresh_cache():\n    # Update cache\n    pass\n```\n\nScheduled functions run automatically without manual invocation.\n\nSee `references/scheduled-jobs.md` for cron syntax, timezone configuration, and monitoring.\n\n## Common Workflows\n\n### Deploy ML Model for Inference\n\n```python\nimport modal\n\n# Define dependencies\nimage = modal.Image.debian_slim().uv_pip_install(\"torch\", \"transformers\")\napp = modal.App(\"llm-inference\", image=image)\n\n# Download model at build time\n@app.function()\ndef download_model():\n    from transformers import AutoModel\n    AutoModel.from_pretrained(\"bert-base-uncased\")\n\n# Serve model\n@app.cls(gpu=\"L40S\")\nclass Model:\n    @modal.enter()\n    def load_model(self):\n        from transformers import pipeline\n        self.pipe = pipeline(\"text-classification\", device=\"cuda\")\n\n    @modal.method()\n    def predict(self, text: str):\n        return self.pipe(text)\n\n@app.local_entrypoint()\ndef main():\n    model = Model()\n    result = model.predict.remote(\"Modal is great!\")\n    print(result)\n```\n\n### Batch Process Large Dataset\n\n```python\n@app.function(cpu=2.0, memory=4096)\ndef process_file(file_path: str):\n    import pandas as pd\n    df = pd.read_csv(file_path)\n    # Process data\n    return df.shape[0]\n\n@app.local_entrypoint()\ndef main():\n    files = [\"file1.csv\", \"file2.csv\", ...]  # 1000s of files\n    # Automatically parallelized across containers\n    for count in process_file.map(files):\n        print(f\"Processed {count} rows\")\n```\n\n### Train Model on GPU\n\n```python\n@app.function(\n    gpu=\"A100:2\",      # 2x A100 GPUs\n    timeout=3600       # 1 hour timeout\n)\ndef train_model(config: dict):\n    import torch\n    # Multi-GPU training code\n    model = create_model(config)\n    train(model)\n    return metrics\n```\n\n## Reference Documentation\n\nDetailed documentation for specific features:\n\n- **`references/getting-started.md`** - Authentication, setup, basic concepts\n- **`references/images.md`** - Image building, dependencies, Dockerfiles\n- **`references/functions.md`** - Function patterns, deployment, parameters\n- **`references/gpu.md`** - GPU types, CUDA, multi-GPU configuration\n- **`references/resources.md`** - CPU, memory, disk management\n- **`references/scaling.md`** - Autoscaling, parallel execution, concurrency\n- **`references/volumes.md`** - Persistent storage, data management\n- **`references/secrets.md`** - Environment variables, authentication\n- **`references/web-endpoints.md`** - APIs, webhooks, endpoints\n- **`references/scheduled-jobs.md`** - Cron jobs, periodic tasks\n- **`references/examples.md`** - Common patterns for scientific computing\n\n## Best Practices\n\n1. **Pin dependencies** in `.uv_pip_install()` for reproducible builds\n2. **Use appropriate GPU types** - L40S for inference, H100/A100 for training\n3. **Leverage caching** - Use Volumes for model weights and datasets\n4. **Configure autoscaling** - Set `max_containers` and `min_containers` based on workload\n5. **Import packages in function body** if not available locally\n6. **Use `.map()` for parallel processing** instead of sequential loops\n7. **Store secrets securely** - Never hardcode API keys\n8. **Monitor costs** - Check Modal dashboard for usage and billing\n\n## Troubleshooting\n\n**\"Module not found\" errors:**\n- Add packages to image with `.uv_pip_install(\"package-name\")`\n- Import packages inside function body if not available locally\n\n**GPU not detected:**\n- Verify GPU specification: `@app.function(gpu=\"A100\")`\n- Check CUDA availability: `torch.cuda.is_available()`\n\n**Function timeout:**\n- Increase timeout: `@app.function(timeout=3600)`\n- Default timeout is 5 minutes\n\n**Volume changes not persisting:**\n- Call `volume.commit()` after writing files\n- Verify volume mounted correctly in function decorator\n\nFor additional help, see Modal documentation at https://modal.com/docs or join Modal Slack community.\n",
        "data/k-dense-ai/molfeat/SKILL.md": "---\nname: molfeat\ndescription: \"Molecular featurization for ML (100+ featurizers). ECFP, MACCS, descriptors, pretrained models (ChemBERTa), convert SMILES to features, for QSAR and molecular ML.\"\n---\n\n# Molfeat - Molecular Featurization Hub\n\n## Overview\n\nMolfeat is a comprehensive Python library for molecular featurization that unifies 100+ pre-trained embeddings and hand-crafted featurizers. Convert chemical structures (SMILES strings or RDKit molecules) into numerical representations for machine learning tasks including QSAR modeling, virtual screening, similarity searching, and deep learning applications. Features fast parallel processing, scikit-learn compatible transformers, and built-in caching.\n\n## When to Use This Skill\n\nThis skill should be used when working with:\n- **Molecular machine learning**: Building QSAR/QSPR models, property prediction\n- **Virtual screening**: Ranking compound libraries for biological activity\n- **Similarity searching**: Finding structurally similar molecules\n- **Chemical space analysis**: Clustering, visualization, dimensionality reduction\n- **Deep learning**: Training neural networks on molecular data\n- **Featurization pipelines**: Converting SMILES to ML-ready representations\n- **Cheminformatics**: Any task requiring molecular feature extraction\n\n## Installation\n\n```bash\nuv pip install molfeat\n\n# With all optional dependencies\nuv pip install \"molfeat[all]\"\n```\n\n**Optional dependencies for specific featurizers:**\n- `molfeat[dgl]` - GNN models (GIN variants)\n- `molfeat[graphormer]` - Graphormer models\n- `molfeat[transformer]` - ChemBERTa, ChemGPT, MolT5\n- `molfeat[fcd]` - FCD descriptors\n- `molfeat[map4]` - MAP4 fingerprints\n\n## Core Concepts\n\nMolfeat organizes featurization into three hierarchical classes:\n\n### 1. Calculators (`molfeat.calc`)\n\nCallable objects that convert individual molecules into feature vectors. Accept RDKit `Chem.Mol` objects or SMILES strings.\n\n**Use calculators for:**\n- Single molecule featurization\n- Custom processing loops\n- Direct feature computation\n\n**Example:**\n```python\nfrom molfeat.calc import FPCalculator\n\ncalc = FPCalculator(\"ecfp\", radius=3, fpSize=2048)\nfeatures = calc(\"CCO\")  # Returns numpy array (2048,)\n```\n\n### 2. Transformers (`molfeat.trans`)\n\nScikit-learn compatible transformers that wrap calculators for batch processing with parallelization.\n\n**Use transformers for:**\n- Batch featurization of molecular datasets\n- Integration with scikit-learn pipelines\n- Parallel processing (automatic CPU utilization)\n\n**Example:**\n```python\nfrom molfeat.trans import MoleculeTransformer\nfrom molfeat.calc import FPCalculator\n\ntransformer = MoleculeTransformer(FPCalculator(\"ecfp\"), n_jobs=-1)\nfeatures = transformer(smiles_list)  # Parallel processing\n```\n\n### 3. Pretrained Transformers (`molfeat.trans.pretrained`)\n\nSpecialized transformers for deep learning models with batched inference and caching.\n\n**Use pretrained transformers for:**\n- State-of-the-art molecular embeddings\n- Transfer learning from large chemical datasets\n- Deep learning feature extraction\n\n**Example:**\n```python\nfrom molfeat.trans.pretrained import PretrainedMolTransformer\n\ntransformer = PretrainedMolTransformer(\"ChemBERTa-77M-MLM\", n_jobs=-1)\nembeddings = transformer(smiles_list)  # Deep learning embeddings\n```\n\n## Quick Start Workflow\n\n### Basic Featurization\n\n```python\nimport datamol as dm\nfrom molfeat.calc import FPCalculator\nfrom molfeat.trans import MoleculeTransformer\n\n# Load molecular data\nsmiles = [\"CCO\", \"CC(=O)O\", \"c1ccccc1\", \"CC(C)O\"]\n\n# Create calculator and transformer\ncalc = FPCalculator(\"ecfp\", radius=3)\ntransformer = MoleculeTransformer(calc, n_jobs=-1)\n\n# Featurize molecules\nfeatures = transformer(smiles)\nprint(f\"Shape: {features.shape}\")  # (4, 2048)\n```\n\n### Save and Load Configuration\n\n```python\n# Save featurizer configuration for reproducibility\ntransformer.to_state_yaml_file(\"featurizer_config.yml\")\n\n# Reload exact configuration\nloaded = MoleculeTransformer.from_state_yaml_file(\"featurizer_config.yml\")\n```\n\n### Handle Errors Gracefully\n\n```python\n# Process dataset with potentially invalid SMILES\ntransformer = MoleculeTransformer(\n    calc,\n    n_jobs=-1,\n    ignore_errors=True,  # Continue on failures\n    verbose=True          # Log error details\n)\n\nfeatures = transformer(smiles_with_errors)\n# Returns None for failed molecules\n```\n\n## Choosing the Right Featurizer\n\n### For Traditional Machine Learning (RF, SVM, XGBoost)\n\n**Start with fingerprints:**\n```python\n# ECFP - Most popular, general-purpose\nFPCalculator(\"ecfp\", radius=3, fpSize=2048)\n\n# MACCS - Fast, good for scaffold hopping\nFPCalculator(\"maccs\")\n\n# MAP4 - Efficient for large-scale screening\nFPCalculator(\"map4\")\n```\n\n**For interpretable models:**\n```python\n# RDKit 2D descriptors (200+ named properties)\nfrom molfeat.calc import RDKitDescriptors2D\nRDKitDescriptors2D()\n\n# Mordred (1800+ comprehensive descriptors)\nfrom molfeat.calc import MordredDescriptors\nMordredDescriptors()\n```\n\n**Combine multiple featurizers:**\n```python\nfrom molfeat.trans import FeatConcat\n\nconcat = FeatConcat([\n    FPCalculator(\"maccs\"),      # 167 dimensions\n    FPCalculator(\"ecfp\")         # 2048 dimensions\n])  # Result: 2215-dimensional combined features\n```\n\n### For Deep Learning\n\n**Transformer-based embeddings:**\n```python\n# ChemBERTa - Pre-trained on 77M PubChem compounds\nPretrainedMolTransformer(\"ChemBERTa-77M-MLM\")\n\n# ChemGPT - Autoregressive language model\nPretrainedMolTransformer(\"ChemGPT-1.2B\")\n```\n\n**Graph neural networks:**\n```python\n# GIN models with different pre-training objectives\nPretrainedMolTransformer(\"gin-supervised-masking\")\nPretrainedMolTransformer(\"gin-supervised-infomax\")\n\n# Graphormer for quantum chemistry\nPretrainedMolTransformer(\"Graphormer-pcqm4mv2\")\n```\n\n### For Similarity Searching\n\n```python\n# ECFP - General purpose, most widely used\nFPCalculator(\"ecfp\")\n\n# MACCS - Fast, scaffold-based similarity\nFPCalculator(\"maccs\")\n\n# MAP4 - Efficient for large databases\nFPCalculator(\"map4\")\n\n# USR/USRCAT - 3D shape similarity\nfrom molfeat.calc import USRDescriptors\nUSRDescriptors()\n```\n\n### For Pharmacophore-Based Approaches\n\n```python\n# FCFP - Functional group based\nFPCalculator(\"fcfp\")\n\n# CATS - Pharmacophore pair distributions\nfrom molfeat.calc import CATSCalculator\nCATSCalculator(mode=\"2D\")\n\n# Gobbi - Explicit pharmacophore features\nFPCalculator(\"gobbi2D\")\n```\n\n## Common Workflows\n\n### Building a QSAR Model\n\n```python\nfrom molfeat.trans import MoleculeTransformer\nfrom molfeat.calc import FPCalculator\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import cross_val_score\n\n# Featurize molecules\ntransformer = MoleculeTransformer(FPCalculator(\"ecfp\"), n_jobs=-1)\nX = transformer(smiles_train)\n\n# Train model\nmodel = RandomForestRegressor(n_estimators=100)\nscores = cross_val_score(model, X, y_train, cv=5)\nprint(f\"R = {scores.mean():.3f}\")\n\n# Save configuration for deployment\ntransformer.to_state_yaml_file(\"production_featurizer.yml\")\n```\n\n### Virtual Screening Pipeline\n\n```python\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Train on known actives/inactives\ntransformer = MoleculeTransformer(FPCalculator(\"ecfp\"), n_jobs=-1)\nX_train = transformer(train_smiles)\nclf = RandomForestClassifier(n_estimators=500)\nclf.fit(X_train, train_labels)\n\n# Screen large library\nX_screen = transformer(screening_library)  # e.g., 1M compounds\npredictions = clf.predict_proba(X_screen)[:, 1]\n\n# Rank and select top hits\ntop_indices = predictions.argsort()[::-1][:1000]\ntop_hits = [screening_library[i] for i in top_indices]\n```\n\n### Similarity Search\n\n```python\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Query molecule\ncalc = FPCalculator(\"ecfp\")\nquery_fp = calc(query_smiles).reshape(1, -1)\n\n# Database fingerprints\ntransformer = MoleculeTransformer(calc, n_jobs=-1)\ndatabase_fps = transformer(database_smiles)\n\n# Compute similarity\nsimilarities = cosine_similarity(query_fp, database_fps)[0]\ntop_similar = similarities.argsort()[-10:][::-1]\n```\n\n### Scikit-learn Pipeline Integration\n\n```python\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Create end-to-end pipeline\npipeline = Pipeline([\n    ('featurizer', MoleculeTransformer(FPCalculator(\"ecfp\"), n_jobs=-1)),\n    ('classifier', RandomForestClassifier(n_estimators=100))\n])\n\n# Train and predict directly on SMILES\npipeline.fit(smiles_train, y_train)\npredictions = pipeline.predict(smiles_test)\n```\n\n### Comparing Multiple Featurizers\n\n```python\nfeaturizers = {\n    'ECFP': FPCalculator(\"ecfp\"),\n    'MACCS': FPCalculator(\"maccs\"),\n    'Descriptors': RDKitDescriptors2D(),\n    'ChemBERTa': PretrainedMolTransformer(\"ChemBERTa-77M-MLM\")\n}\n\nresults = {}\nfor name, feat in featurizers.items():\n    transformer = MoleculeTransformer(feat, n_jobs=-1)\n    X = transformer(smiles)\n    # Evaluate with your ML model\n    score = evaluate_model(X, y)\n    results[name] = score\n```\n\n## Discovering Available Featurizers\n\nUse the ModelStore to explore all available featurizers:\n\n```python\nfrom molfeat.store.modelstore import ModelStore\n\nstore = ModelStore()\n\n# List all available models\nall_models = store.available_models\nprint(f\"Total featurizers: {len(all_models)}\")\n\n# Search for specific models\nchemberta_models = store.search(name=\"ChemBERTa\")\nfor model in chemberta_models:\n    print(f\"- {model.name}: {model.description}\")\n\n# Get usage information\nmodel_card = store.search(name=\"ChemBERTa-77M-MLM\")[0]\nmodel_card.usage()  # Display usage examples\n\n# Load model\ntransformer = store.load(\"ChemBERTa-77M-MLM\")\n```\n\n## Advanced Features\n\n### Custom Preprocessing\n\n```python\nclass CustomTransformer(MoleculeTransformer):\n    def preprocess(self, mol):\n        \"\"\"Custom preprocessing pipeline\"\"\"\n        if isinstance(mol, str):\n            mol = dm.to_mol(mol)\n        mol = dm.standardize_mol(mol)\n        mol = dm.remove_salts(mol)\n        return mol\n\ntransformer = CustomTransformer(FPCalculator(\"ecfp\"), n_jobs=-1)\n```\n\n### Batch Processing Large Datasets\n\n```python\ndef featurize_in_chunks(smiles_list, transformer, chunk_size=10000):\n    \"\"\"Process large datasets in chunks to manage memory\"\"\"\n    all_features = []\n    for i in range(0, len(smiles_list), chunk_size):\n        chunk = smiles_list[i:i+chunk_size]\n        features = transformer(chunk)\n        all_features.append(features)\n    return np.vstack(all_features)\n```\n\n### Caching Expensive Embeddings\n\n```python\nimport pickle\n\ncache_file = \"embeddings_cache.pkl\"\ntransformer = PretrainedMolTransformer(\"ChemBERTa-77M-MLM\", n_jobs=-1)\n\ntry:\n    with open(cache_file, \"rb\") as f:\n        embeddings = pickle.load(f)\nexcept FileNotFoundError:\n    embeddings = transformer(smiles_list)\n    with open(cache_file, \"wb\") as f:\n        pickle.dump(embeddings, f)\n```\n\n## Performance Tips\n\n1. **Use parallelization**: Set `n_jobs=-1` to utilize all CPU cores\n2. **Batch processing**: Process multiple molecules at once instead of loops\n3. **Choose appropriate featurizers**: Fingerprints are faster than deep learning models\n4. **Cache pretrained models**: Leverage built-in caching for repeated use\n5. **Use float32**: Set `dtype=np.float32` when precision allows\n6. **Handle errors efficiently**: Use `ignore_errors=True` for large datasets\n\n## Common Featurizers Reference\n\n**Quick reference for frequently used featurizers:**\n\n| Featurizer | Type | Dimensions | Speed | Use Case |\n|------------|------|------------|-------|----------|\n| `ecfp` | Fingerprint | 2048 | Fast | General purpose |\n| `maccs` | Fingerprint | 167 | Very fast | Scaffold similarity |\n| `desc2D` | Descriptors | 200+ | Fast | Interpretable models |\n| `mordred` | Descriptors | 1800+ | Medium | Comprehensive features |\n| `map4` | Fingerprint | 1024 | Fast | Large-scale screening |\n| `ChemBERTa-77M-MLM` | Deep learning | 768 | Slow* | Transfer learning |\n| `gin-supervised-masking` | GNN | Variable | Slow* | Graph-based models |\n\n*First run is slow; subsequent runs benefit from caching\n\n## Resources\n\nThis skill includes comprehensive reference documentation:\n\n### references/api_reference.md\nComplete API documentation covering:\n- `molfeat.calc` - All calculator classes and parameters\n- `molfeat.trans` - Transformer classes and methods\n- `molfeat.store` - ModelStore usage\n- Common patterns and integration examples\n- Performance optimization tips\n\n**When to load:** Reference when implementing specific calculators, understanding transformer parameters, or integrating with scikit-learn/PyTorch.\n\n### references/available_featurizers.md\nComprehensive catalog of all 100+ featurizers organized by category:\n- Transformer-based language models (ChemBERTa, ChemGPT)\n- Graph neural networks (GIN, Graphormer)\n- Molecular descriptors (RDKit, Mordred)\n- Fingerprints (ECFP, MACCS, MAP4, and 15+ others)\n- Pharmacophore descriptors (CATS, Gobbi)\n- Shape descriptors (USR, ElectroShape)\n- Scaffold-based descriptors\n\n**When to load:** Reference when selecting the optimal featurizer for a specific task, exploring available options, or understanding featurizer characteristics.\n\n**Search tip:** Use grep to find specific featurizer types:\n```bash\ngrep -i \"chembert\" references/available_featurizers.md\ngrep -i \"pharmacophore\" references/available_featurizers.md\n```\n\n### references/examples.md\nPractical code examples for common scenarios:\n- Installation and quick start\n- Calculator and transformer examples\n- Pretrained model usage\n- Scikit-learn and PyTorch integration\n- Virtual screening workflows\n- QSAR model building\n- Similarity searching\n- Troubleshooting and best practices\n\n**When to load:** Reference when implementing specific workflows, troubleshooting issues, or learning molfeat patterns.\n\n## Troubleshooting\n\n### Invalid Molecules\nEnable error handling to skip invalid SMILES:\n```python\ntransformer = MoleculeTransformer(\n    calc,\n    ignore_errors=True,\n    verbose=True\n)\n```\n\n### Memory Issues with Large Datasets\nProcess in chunks or use streaming approaches for datasets > 100K molecules.\n\n### Pretrained Model Dependencies\nSome models require additional packages. Install specific extras:\n```bash\nuv pip install \"molfeat[transformer]\"  # For ChemBERTa/ChemGPT\nuv pip install \"molfeat[dgl]\"          # For GIN models\n```\n\n### Reproducibility\nSave exact configurations and document versions:\n```python\ntransformer.to_state_yaml_file(\"config.yml\")\nimport molfeat\nprint(f\"molfeat version: {molfeat.__version__}\")\n```\n\n## Additional Resources\n\n- **Official Documentation**: https://molfeat-docs.datamol.io/\n- **GitHub Repository**: https://github.com/datamol-io/molfeat\n- **PyPI Package**: https://pypi.org/project/molfeat/\n- **Tutorial**: https://portal.valencelabs.com/datamol/post/types-of-featurizers-b1e8HHrbFMkbun6\n",
        "data/k-dense-ai/networkx/SKILL.md": "---\nname: networkx\ndescription: Comprehensive toolkit for creating, analyzing, and visualizing complex networks and graphs in Python. Use when working with network/graph data structures, analyzing relationships between entities, computing graph algorithms (shortest paths, centrality, clustering), detecting communities, generating synthetic networks, or visualizing network topologies. Applicable to social networks, biological networks, transportation systems, citation networks, and any domain involving pairwise relationships.\n---\n\n# NetworkX\n\n## Overview\n\nNetworkX is a Python package for creating, manipulating, and analyzing complex networks and graphs. Use this skill when working with network or graph data structures, including social networks, biological networks, transportation systems, citation networks, knowledge graphs, or any system involving relationships between entities.\n\n## When to Use This Skill\n\nInvoke this skill when tasks involve:\n\n- **Creating graphs**: Building network structures from data, adding nodes and edges with attributes\n- **Graph analysis**: Computing centrality measures, finding shortest paths, detecting communities, measuring clustering\n- **Graph algorithms**: Running standard algorithms like Dijkstra's, PageRank, minimum spanning trees, maximum flow\n- **Network generation**: Creating synthetic networks (random, scale-free, small-world models) for testing or simulation\n- **Graph I/O**: Reading from or writing to various formats (edge lists, GraphML, JSON, CSV, adjacency matrices)\n- **Visualization**: Drawing and customizing network visualizations with matplotlib or interactive libraries\n- **Network comparison**: Checking isomorphism, computing graph metrics, analyzing structural properties\n\n## Core Capabilities\n\n### 1. Graph Creation and Manipulation\n\nNetworkX supports four main graph types:\n- **Graph**: Undirected graphs with single edges\n- **DiGraph**: Directed graphs with one-way connections\n- **MultiGraph**: Undirected graphs allowing multiple edges between nodes\n- **MultiDiGraph**: Directed graphs with multiple edges\n\nCreate graphs by:\n```python\nimport networkx as nx\n\n# Create empty graph\nG = nx.Graph()\n\n# Add nodes (can be any hashable type)\nG.add_node(1)\nG.add_nodes_from([2, 3, 4])\nG.add_node(\"protein_A\", type='enzyme', weight=1.5)\n\n# Add edges\nG.add_edge(1, 2)\nG.add_edges_from([(1, 3), (2, 4)])\nG.add_edge(1, 4, weight=0.8, relation='interacts')\n```\n\n**Reference**: See `references/graph-basics.md` for comprehensive guidance on creating, modifying, examining, and managing graph structures, including working with attributes and subgraphs.\n\n### 2. Graph Algorithms\n\nNetworkX provides extensive algorithms for network analysis:\n\n**Shortest Paths**:\n```python\n# Find shortest path\npath = nx.shortest_path(G, source=1, target=5)\nlength = nx.shortest_path_length(G, source=1, target=5, weight='weight')\n```\n\n**Centrality Measures**:\n```python\n# Degree centrality\ndegree_cent = nx.degree_centrality(G)\n\n# Betweenness centrality\nbetweenness = nx.betweenness_centrality(G)\n\n# PageRank\npagerank = nx.pagerank(G)\n```\n\n**Community Detection**:\n```python\nfrom networkx.algorithms import community\n\n# Detect communities\ncommunities = community.greedy_modularity_communities(G)\n```\n\n**Connectivity**:\n```python\n# Check connectivity\nis_connected = nx.is_connected(G)\n\n# Find connected components\ncomponents = list(nx.connected_components(G))\n```\n\n**Reference**: See `references/algorithms.md` for detailed documentation on all available algorithms including shortest paths, centrality measures, clustering, community detection, flows, matching, tree algorithms, and graph traversal.\n\n### 3. Graph Generators\n\nCreate synthetic networks for testing, simulation, or modeling:\n\n**Classic Graphs**:\n```python\n# Complete graph\nG = nx.complete_graph(n=10)\n\n# Cycle graph\nG = nx.cycle_graph(n=20)\n\n# Known graphs\nG = nx.karate_club_graph()\nG = nx.petersen_graph()\n```\n\n**Random Networks**:\n```python\n# Erds-Rnyi random graph\nG = nx.erdos_renyi_graph(n=100, p=0.1, seed=42)\n\n# Barabsi-Albert scale-free network\nG = nx.barabasi_albert_graph(n=100, m=3, seed=42)\n\n# Watts-Strogatz small-world network\nG = nx.watts_strogatz_graph(n=100, k=6, p=0.1, seed=42)\n```\n\n**Structured Networks**:\n```python\n# Grid graph\nG = nx.grid_2d_graph(m=5, n=7)\n\n# Random tree\nG = nx.random_tree(n=100, seed=42)\n```\n\n**Reference**: See `references/generators.md` for comprehensive coverage of all graph generators including classic, random, lattice, bipartite, and specialized network models with detailed parameters and use cases.\n\n### 4. Reading and Writing Graphs\n\nNetworkX supports numerous file formats and data sources:\n\n**File Formats**:\n```python\n# Edge list\nG = nx.read_edgelist('graph.edgelist')\nnx.write_edgelist(G, 'graph.edgelist')\n\n# GraphML (preserves attributes)\nG = nx.read_graphml('graph.graphml')\nnx.write_graphml(G, 'graph.graphml')\n\n# GML\nG = nx.read_gml('graph.gml')\nnx.write_gml(G, 'graph.gml')\n\n# JSON\ndata = nx.node_link_data(G)\nG = nx.node_link_graph(data)\n```\n\n**Pandas Integration**:\n```python\nimport pandas as pd\n\n# From DataFrame\ndf = pd.DataFrame({'source': [1, 2, 3], 'target': [2, 3, 4], 'weight': [0.5, 1.0, 0.75]})\nG = nx.from_pandas_edgelist(df, 'source', 'target', edge_attr='weight')\n\n# To DataFrame\ndf = nx.to_pandas_edgelist(G)\n```\n\n**Matrix Formats**:\n```python\nimport numpy as np\n\n# Adjacency matrix\nA = nx.to_numpy_array(G)\nG = nx.from_numpy_array(A)\n\n# Sparse matrix\nA = nx.to_scipy_sparse_array(G)\nG = nx.from_scipy_sparse_array(A)\n```\n\n**Reference**: See `references/io.md` for complete documentation on all I/O formats including CSV, SQL databases, Cytoscape, DOT, and guidance on format selection for different use cases.\n\n### 5. Visualization\n\nCreate clear and informative network visualizations:\n\n**Basic Visualization**:\n```python\nimport matplotlib.pyplot as plt\n\n# Simple draw\nnx.draw(G, with_labels=True)\nplt.show()\n\n# With layout\npos = nx.spring_layout(G, seed=42)\nnx.draw(G, pos=pos, with_labels=True, node_color='lightblue', node_size=500)\nplt.show()\n```\n\n**Customization**:\n```python\n# Color by degree\nnode_colors = [G.degree(n) for n in G.nodes()]\nnx.draw(G, node_color=node_colors, cmap=plt.cm.viridis)\n\n# Size by centrality\ncentrality = nx.betweenness_centrality(G)\nnode_sizes = [3000 * centrality[n] for n in G.nodes()]\nnx.draw(G, node_size=node_sizes)\n\n# Edge weights\nedge_widths = [3 * G[u][v].get('weight', 1) for u, v in G.edges()]\nnx.draw(G, width=edge_widths)\n```\n\n**Layout Algorithms**:\n```python\n# Spring layout (force-directed)\npos = nx.spring_layout(G, seed=42)\n\n# Circular layout\npos = nx.circular_layout(G)\n\n# Kamada-Kawai layout\npos = nx.kamada_kawai_layout(G)\n\n# Spectral layout\npos = nx.spectral_layout(G)\n```\n\n**Publication Quality**:\n```python\nplt.figure(figsize=(12, 8))\npos = nx.spring_layout(G, seed=42)\nnx.draw(G, pos=pos, node_color='lightblue', node_size=500,\n        edge_color='gray', with_labels=True, font_size=10)\nplt.title('Network Visualization', fontsize=16)\nplt.axis('off')\nplt.tight_layout()\nplt.savefig('network.png', dpi=300, bbox_inches='tight')\nplt.savefig('network.pdf', bbox_inches='tight')  # Vector format\n```\n\n**Reference**: See `references/visualization.md` for extensive documentation on visualization techniques including layout algorithms, customization options, interactive visualizations with Plotly and PyVis, 3D networks, and publication-quality figure creation.\n\n## Working with NetworkX\n\n### Installation\n\nEnsure NetworkX is installed:\n```python\n# Check if installed\nimport networkx as nx\nprint(nx.__version__)\n\n# Install if needed (via bash)\n# uv pip install networkx\n# uv pip install networkx[default]  # With optional dependencies\n```\n\n### Common Workflow Pattern\n\nMost NetworkX tasks follow this pattern:\n\n1. **Create or Load Graph**:\n   ```python\n   # From scratch\n   G = nx.Graph()\n   G.add_edges_from([(1, 2), (2, 3), (3, 4)])\n\n   # Or load from file/data\n   G = nx.read_edgelist('data.txt')\n   ```\n\n2. **Examine Structure**:\n   ```python\n   print(f\"Nodes: {G.number_of_nodes()}\")\n   print(f\"Edges: {G.number_of_edges()}\")\n   print(f\"Density: {nx.density(G)}\")\n   print(f\"Connected: {nx.is_connected(G)}\")\n   ```\n\n3. **Analyze**:\n   ```python\n   # Compute metrics\n   degree_cent = nx.degree_centrality(G)\n   avg_clustering = nx.average_clustering(G)\n\n   # Find paths\n   path = nx.shortest_path(G, source=1, target=4)\n\n   # Detect communities\n   communities = community.greedy_modularity_communities(G)\n   ```\n\n4. **Visualize**:\n   ```python\n   pos = nx.spring_layout(G, seed=42)\n   nx.draw(G, pos=pos, with_labels=True)\n   plt.show()\n   ```\n\n5. **Export Results**:\n   ```python\n   # Save graph\n   nx.write_graphml(G, 'analyzed_network.graphml')\n\n   # Save metrics\n   df = pd.DataFrame({\n       'node': list(degree_cent.keys()),\n       'centrality': list(degree_cent.values())\n   })\n   df.to_csv('centrality_results.csv', index=False)\n   ```\n\n### Important Considerations\n\n**Floating Point Precision**: When graphs contain floating-point numbers, all results are inherently approximate due to precision limitations. This can affect algorithm outcomes, particularly in minimum/maximum computations.\n\n**Memory and Performance**: Each time a script runs, graph data must be loaded into memory. For large networks:\n- Use appropriate data structures (sparse matrices for large sparse graphs)\n- Consider loading only necessary subgraphs\n- Use efficient file formats (pickle for Python objects, compressed formats)\n- Leverage approximate algorithms for very large networks (e.g., `k` parameter in centrality calculations)\n\n**Node and Edge Types**:\n- Nodes can be any hashable Python object (numbers, strings, tuples, custom objects)\n- Use meaningful identifiers for clarity\n- When removing nodes, all incident edges are automatically removed\n\n**Random Seeds**: Always set random seeds for reproducibility in random graph generation and force-directed layouts:\n```python\nG = nx.erdos_renyi_graph(n=100, p=0.1, seed=42)\npos = nx.spring_layout(G, seed=42)\n```\n\n## Quick Reference\n\n### Basic Operations\n```python\n# Create\nG = nx.Graph()\nG.add_edge(1, 2)\n\n# Query\nG.number_of_nodes()\nG.number_of_edges()\nG.degree(1)\nlist(G.neighbors(1))\n\n# Check\nG.has_node(1)\nG.has_edge(1, 2)\nnx.is_connected(G)\n\n# Modify\nG.remove_node(1)\nG.remove_edge(1, 2)\nG.clear()\n```\n\n### Essential Algorithms\n```python\n# Paths\nnx.shortest_path(G, source, target)\nnx.all_pairs_shortest_path(G)\n\n# Centrality\nnx.degree_centrality(G)\nnx.betweenness_centrality(G)\nnx.closeness_centrality(G)\nnx.pagerank(G)\n\n# Clustering\nnx.clustering(G)\nnx.average_clustering(G)\n\n# Components\nnx.connected_components(G)\nnx.strongly_connected_components(G)  # Directed\n\n# Community\ncommunity.greedy_modularity_communities(G)\n```\n\n### File I/O Quick Reference\n```python\n# Read\nnx.read_edgelist('file.txt')\nnx.read_graphml('file.graphml')\nnx.read_gml('file.gml')\n\n# Write\nnx.write_edgelist(G, 'file.txt')\nnx.write_graphml(G, 'file.graphml')\nnx.write_gml(G, 'file.gml')\n\n# Pandas\nnx.from_pandas_edgelist(df, 'source', 'target')\nnx.to_pandas_edgelist(G)\n```\n\n## Resources\n\nThis skill includes comprehensive reference documentation:\n\n### references/graph-basics.md\nDetailed guide on graph types, creating and modifying graphs, adding nodes and edges, managing attributes, examining structure, and working with subgraphs.\n\n### references/algorithms.md\nComplete coverage of NetworkX algorithms including shortest paths, centrality measures, connectivity, clustering, community detection, flow algorithms, tree algorithms, matching, coloring, isomorphism, and graph traversal.\n\n### references/generators.md\nComprehensive documentation on graph generators including classic graphs, random models (Erds-Rnyi, Barabsi-Albert, Watts-Strogatz), lattices, trees, social network models, and specialized generators.\n\n### references/io.md\nComplete guide to reading and writing graphs in various formats: edge lists, adjacency lists, GraphML, GML, JSON, CSV, Pandas DataFrames, NumPy arrays, SciPy sparse matrices, database integration, and format selection guidelines.\n\n### references/visualization.md\nExtensive documentation on visualization techniques including layout algorithms, customizing node and edge appearance, labels, interactive visualizations with Plotly and PyVis, 3D networks, bipartite layouts, and creating publication-quality figures.\n\n## Additional Resources\n\n- **Official Documentation**: https://networkx.org/documentation/latest/\n- **Tutorial**: https://networkx.org/documentation/latest/tutorial.html\n- **Gallery**: https://networkx.org/documentation/latest/auto_examples/index.html\n- **GitHub**: https://github.com/networkx/networkx\n",
        "data/k-dense-ai/neurokit2/SKILL.md": "---\nname: neurokit2\ndescription: Comprehensive biosignal processing toolkit for analyzing physiological data including ECG, EEG, EDA, RSP, PPG, EMG, and EOG signals. Use this skill when processing cardiovascular signals, brain activity, electrodermal responses, respiratory patterns, muscle activity, or eye movements. Applicable for heart rate variability analysis, event-related potentials, complexity measures, autonomic nervous system assessment, psychophysiology research, and multi-modal physiological signal integration.\n---\n\n# NeuroKit2\n\n## Overview\n\nNeuroKit2 is a comprehensive Python toolkit for processing and analyzing physiological signals (biosignals). Use this skill to process cardiovascular, neural, autonomic, respiratory, and muscular signals for psychophysiology research, clinical applications, and human-computer interaction studies.\n\n## When to Use This Skill\n\nApply this skill when working with:\n- **Cardiac signals**: ECG, PPG, heart rate variability (HRV), pulse analysis\n- **Brain signals**: EEG frequency bands, microstates, complexity, source localization\n- **Autonomic signals**: Electrodermal activity (EDA/GSR), skin conductance responses (SCR)\n- **Respiratory signals**: Breathing rate, respiratory variability (RRV), volume per time\n- **Muscular signals**: EMG amplitude, muscle activation detection\n- **Eye tracking**: EOG, blink detection and analysis\n- **Multi-modal integration**: Processing multiple physiological signals simultaneously\n- **Complexity analysis**: Entropy measures, fractal dimensions, nonlinear dynamics\n\n## Core Capabilities\n\n### 1. Cardiac Signal Processing (ECG/PPG)\n\nProcess electrocardiogram and photoplethysmography signals for cardiovascular analysis. See `references/ecg_cardiac.md` for detailed workflows.\n\n**Primary workflows:**\n- ECG processing pipeline: cleaning  R-peak detection  delineation  quality assessment\n- HRV analysis across time, frequency, and nonlinear domains\n- PPG pulse analysis and quality assessment\n- ECG-derived respiration extraction\n\n**Key functions:**\n```python\nimport neurokit2 as nk\n\n# Complete ECG processing pipeline\nsignals, info = nk.ecg_process(ecg_signal, sampling_rate=1000)\n\n# Analyze ECG data (event-related or interval-related)\nanalysis = nk.ecg_analyze(signals, sampling_rate=1000)\n\n# Comprehensive HRV analysis\nhrv = nk.hrv(peaks, sampling_rate=1000)  # Time, frequency, nonlinear domains\n```\n\n### 2. Heart Rate Variability Analysis\n\nCompute comprehensive HRV metrics from cardiac signals. See `references/hrv.md` for all indices and domain-specific analysis.\n\n**Supported domains:**\n- **Time domain**: SDNN, RMSSD, pNN50, SDSD, and derived metrics\n- **Frequency domain**: ULF, VLF, LF, HF, VHF power and ratios\n- **Nonlinear domain**: Poincar plot (SD1/SD2), entropy measures, fractal dimensions\n- **Specialized**: Respiratory sinus arrhythmia (RSA), recurrence quantification analysis (RQA)\n\n**Key functions:**\n```python\n# All HRV indices at once\nhrv_indices = nk.hrv(peaks, sampling_rate=1000)\n\n# Domain-specific analysis\nhrv_time = nk.hrv_time(peaks)\nhrv_freq = nk.hrv_frequency(peaks, sampling_rate=1000)\nhrv_nonlinear = nk.hrv_nonlinear(peaks, sampling_rate=1000)\nhrv_rsa = nk.hrv_rsa(peaks, rsp_signal, sampling_rate=1000)\n```\n\n### 3. Brain Signal Analysis (EEG)\n\nAnalyze electroencephalography signals for frequency power, complexity, and microstate patterns. See `references/eeg.md` for detailed workflows and MNE integration.\n\n**Primary capabilities:**\n- Frequency band power analysis (Delta, Theta, Alpha, Beta, Gamma)\n- Channel quality assessment and re-referencing\n- Source localization (sLORETA, MNE)\n- Microstate segmentation and transition dynamics\n- Global field power and dissimilarity measures\n\n**Key functions:**\n```python\n# Power analysis across frequency bands\npower = nk.eeg_power(eeg_data, sampling_rate=250, channels=['Fz', 'Cz', 'Pz'])\n\n# Microstate analysis\nmicrostates = nk.microstates_segment(eeg_data, n_microstates=4, method='kmod')\nstatic = nk.microstates_static(microstates)\ndynamic = nk.microstates_dynamic(microstates)\n```\n\n### 4. Electrodermal Activity (EDA)\n\nProcess skin conductance signals for autonomic nervous system assessment. See `references/eda.md` for detailed workflows.\n\n**Primary workflows:**\n- Signal decomposition into tonic and phasic components\n- Skin conductance response (SCR) detection and analysis\n- Sympathetic nervous system index calculation\n- Autocorrelation and changepoint detection\n\n**Key functions:**\n```python\n# Complete EDA processing\nsignals, info = nk.eda_process(eda_signal, sampling_rate=100)\n\n# Analyze EDA data\nanalysis = nk.eda_analyze(signals, sampling_rate=100)\n\n# Sympathetic nervous system activity\nsympathetic = nk.eda_sympathetic(signals, sampling_rate=100)\n```\n\n### 5. Respiratory Signal Processing (RSP)\n\nAnalyze breathing patterns and respiratory variability. See `references/rsp.md` for detailed workflows.\n\n**Primary capabilities:**\n- Respiratory rate calculation and variability analysis\n- Breathing amplitude and symmetry assessment\n- Respiratory volume per time (fMRI applications)\n- Respiratory amplitude variability (RAV)\n\n**Key functions:**\n```python\n# Complete RSP processing\nsignals, info = nk.rsp_process(rsp_signal, sampling_rate=100)\n\n# Respiratory rate variability\nrrv = nk.rsp_rrv(signals, sampling_rate=100)\n\n# Respiratory volume per time\nrvt = nk.rsp_rvt(signals, sampling_rate=100)\n```\n\n### 6. Electromyography (EMG)\n\nProcess muscle activity signals for activation detection and amplitude analysis. See `references/emg.md` for workflows.\n\n**Key functions:**\n```python\n# Complete EMG processing\nsignals, info = nk.emg_process(emg_signal, sampling_rate=1000)\n\n# Muscle activation detection\nactivation = nk.emg_activation(signals, sampling_rate=1000, method='threshold')\n```\n\n### 7. Electrooculography (EOG)\n\nAnalyze eye movement and blink patterns. See `references/eog.md` for workflows.\n\n**Key functions:**\n```python\n# Complete EOG processing\nsignals, info = nk.eog_process(eog_signal, sampling_rate=500)\n\n# Extract blink features\nfeatures = nk.eog_features(signals, sampling_rate=500)\n```\n\n### 8. General Signal Processing\n\nApply filtering, decomposition, and transformation operations to any signal. See `references/signal_processing.md` for comprehensive utilities.\n\n**Key operations:**\n- Filtering (lowpass, highpass, bandpass, bandstop)\n- Decomposition (EMD, SSA, wavelet)\n- Peak detection and correction\n- Power spectral density estimation\n- Signal interpolation and resampling\n- Autocorrelation and synchrony analysis\n\n**Key functions:**\n```python\n# Filtering\nfiltered = nk.signal_filter(signal, sampling_rate=1000, lowcut=0.5, highcut=40)\n\n# Peak detection\npeaks = nk.signal_findpeaks(signal)\n\n# Power spectral density\npsd = nk.signal_psd(signal, sampling_rate=1000)\n```\n\n### 9. Complexity and Entropy Analysis\n\nCompute nonlinear dynamics, fractal dimensions, and information-theoretic measures. See `references/complexity.md` for all available metrics.\n\n**Available measures:**\n- **Entropy**: Shannon, approximate, sample, permutation, spectral, fuzzy, multiscale\n- **Fractal dimensions**: Katz, Higuchi, Petrosian, Sevcik, correlation dimension\n- **Nonlinear dynamics**: Lyapunov exponents, Lempel-Ziv complexity, recurrence quantification\n- **DFA**: Detrended fluctuation analysis, multifractal DFA\n- **Information theory**: Fisher information, mutual information\n\n**Key functions:**\n```python\n# Multiple complexity metrics at once\ncomplexity_indices = nk.complexity(signal, sampling_rate=1000)\n\n# Specific measures\napen = nk.entropy_approximate(signal)\ndfa = nk.fractal_dfa(signal)\nlyap = nk.complexity_lyapunov(signal, sampling_rate=1000)\n```\n\n### 10. Event-Related Analysis\n\nCreate epochs around stimulus events and analyze physiological responses. See `references/epochs_events.md` for workflows.\n\n**Primary capabilities:**\n- Epoch creation from event markers\n- Event-related averaging and visualization\n- Baseline correction options\n- Grand average computation with confidence intervals\n\n**Key functions:**\n```python\n# Find events in signal\nevents = nk.events_find(trigger_signal, threshold=0.5)\n\n# Create epochs around events\nepochs = nk.epochs_create(signals, events, sampling_rate=1000,\n                          epochs_start=-0.5, epochs_end=2.0)\n\n# Average across epochs\ngrand_average = nk.epochs_average(epochs)\n```\n\n### 11. Multi-Signal Integration\n\nProcess multiple physiological signals simultaneously with unified output. See `references/bio_module.md` for integration workflows.\n\n**Key functions:**\n```python\n# Process multiple signals at once\nbio_signals, bio_info = nk.bio_process(\n    ecg=ecg_signal,\n    rsp=rsp_signal,\n    eda=eda_signal,\n    emg=emg_signal,\n    sampling_rate=1000\n)\n\n# Analyze all processed signals\nbio_analysis = nk.bio_analyze(bio_signals, sampling_rate=1000)\n```\n\n## Analysis Modes\n\nNeuroKit2 automatically selects between two analysis modes based on data duration:\n\n**Event-related analysis** (< 10 seconds):\n- Analyzes stimulus-locked responses\n- Epoch-based segmentation\n- Suitable for experimental paradigms with discrete trials\n\n**Interval-related analysis** ( 10 seconds):\n- Characterizes physiological patterns over extended periods\n- Resting state or continuous activities\n- Suitable for baseline measurements and long-term monitoring\n\nMost `*_analyze()` functions automatically choose the appropriate mode.\n\n## Installation\n\n```bash\nuv pip install neurokit2\n```\n\nFor development version:\n```bash\nuv pip install https://github.com/neuropsychology/NeuroKit/zipball/dev\n```\n\n## Common Workflows\n\n### Quick Start: ECG Analysis\n```python\nimport neurokit2 as nk\n\n# Load example data\necg = nk.ecg_simulate(duration=60, sampling_rate=1000)\n\n# Process ECG\nsignals, info = nk.ecg_process(ecg, sampling_rate=1000)\n\n# Analyze HRV\nhrv = nk.hrv(info['ECG_R_Peaks'], sampling_rate=1000)\n\n# Visualize\nnk.ecg_plot(signals, info)\n```\n\n### Multi-Modal Analysis\n```python\n# Process multiple signals\nbio_signals, bio_info = nk.bio_process(\n    ecg=ecg_signal,\n    rsp=rsp_signal,\n    eda=eda_signal,\n    sampling_rate=1000\n)\n\n# Analyze all signals\nresults = nk.bio_analyze(bio_signals, sampling_rate=1000)\n```\n\n### Event-Related Potential\n```python\n# Find events\nevents = nk.events_find(trigger_channel, threshold=0.5)\n\n# Create epochs\nepochs = nk.epochs_create(processed_signals, events,\n                          sampling_rate=1000,\n                          epochs_start=-0.5, epochs_end=2.0)\n\n# Event-related analysis for each signal type\necg_epochs = nk.ecg_eventrelated(epochs)\neda_epochs = nk.eda_eventrelated(epochs)\n```\n\n## References\n\nThis skill includes comprehensive reference documentation organized by signal type and analysis method:\n\n- **ecg_cardiac.md**: ECG/PPG processing, R-peak detection, delineation, quality assessment\n- **hrv.md**: Heart rate variability indices across all domains\n- **eeg.md**: EEG analysis, frequency bands, microstates, source localization\n- **eda.md**: Electrodermal activity processing and SCR analysis\n- **rsp.md**: Respiratory signal processing and variability\n- **ppg.md**: Photoplethysmography signal analysis\n- **emg.md**: Electromyography processing and activation detection\n- **eog.md**: Electrooculography and blink analysis\n- **signal_processing.md**: General signal utilities and transformations\n- **complexity.md**: Entropy, fractal, and nonlinear measures\n- **epochs_events.md**: Event-related analysis and epoch creation\n- **bio_module.md**: Multi-signal integration workflows\n\nLoad specific reference files as needed using the Read tool to access detailed function documentation and parameters.\n\n## Additional Resources\n\n- Official Documentation: https://neuropsychology.github.io/NeuroKit/\n- GitHub Repository: https://github.com/neuropsychology/NeuroKit\n- Publication: Makowski et al. (2021). NeuroKit2: A Python toolbox for neurophysiological signal processing. Behavior Research Methods. https://doi.org/10.3758/s13428-020-01516-y\n",
        "data/k-dense-ai/omero-integration/SKILL.md": "---\nname: omero-integration\ndescription: \"Microscopy data management platform. Access images via Python, retrieve datasets, analyze pixels, manage ROIs/annotations, batch processing, for high-content screening and microscopy workflows.\"\n---\n\n# OMERO Integration\n\n## Overview\n\nOMERO is an open-source platform for managing, visualizing, and analyzing microscopy images and metadata. Access images via Python API, retrieve datasets, analyze pixels, manage ROIs and annotations, for high-content screening and microscopy workflows.\n\n## When to Use This Skill\n\nThis skill should be used when:\n- Working with OMERO Python API (omero-py) to access microscopy data\n- Retrieving images, datasets, projects, or screening data programmatically\n- Analyzing pixel data and creating derived images\n- Creating or managing ROIs (regions of interest) on microscopy images\n- Adding annotations, tags, or metadata to OMERO objects\n- Storing measurement results in OMERO tables\n- Creating server-side scripts for batch processing\n- Performing high-content screening analysis\n\n## Core Capabilities\n\nThis skill covers eight major capability areas. Each is documented in detail in the references/ directory:\n\n### 1. Connection & Session Management\n**File**: `references/connection.md`\n\nEstablish secure connections to OMERO servers, manage sessions, handle authentication, and work with group contexts. Use this for initial setup and connection patterns.\n\n**Common scenarios:**\n- Connect to OMERO server with credentials\n- Use existing session IDs\n- Switch between group contexts\n- Manage connection lifecycle with context managers\n\n### 2. Data Access & Retrieval\n**File**: `references/data_access.md`\n\nNavigate OMERO's hierarchical data structure (Projects  Datasets  Images) and screening data (Screens  Plates  Wells). Retrieve objects, query by attributes, and access metadata.\n\n**Common scenarios:**\n- List all projects and datasets for a user\n- Retrieve images by ID or dataset\n- Access screening plate data\n- Query objects with filters\n\n### 3. Metadata & Annotations\n**File**: `references/metadata.md`\n\nCreate and manage annotations including tags, key-value pairs, file attachments, and comments. Link annotations to images, datasets, or other objects.\n\n**Common scenarios:**\n- Add tags to images\n- Attach analysis results as files\n- Create custom key-value metadata\n- Query annotations by namespace\n\n### 4. Image Processing & Rendering\n**File**: `references/image_processing.md`\n\nAccess raw pixel data as NumPy arrays, manipulate rendering settings, create derived images, and manage physical dimensions.\n\n**Common scenarios:**\n- Extract pixel data for computational analysis\n- Generate thumbnail images\n- Create maximum intensity projections\n- Modify channel rendering settings\n\n### 5. Regions of Interest (ROIs)\n**File**: `references/rois.md`\n\nCreate, retrieve, and analyze ROIs with various shapes (rectangles, ellipses, polygons, masks, points, lines). Extract intensity statistics from ROI regions.\n\n**Common scenarios:**\n- Draw rectangular ROIs on images\n- Create polygon masks for segmentation\n- Analyze pixel intensities within ROIs\n- Export ROI coordinates\n\n### 6. OMERO Tables\n**File**: `references/tables.md`\n\nStore and query structured tabular data associated with OMERO objects. Useful for analysis results, measurements, and metadata.\n\n**Common scenarios:**\n- Store quantitative measurements for images\n- Create tables with multiple column types\n- Query table data with conditions\n- Link tables to specific images or datasets\n\n### 7. Scripts & Batch Operations\n**File**: `references/scripts.md`\n\nCreate OMERO.scripts that run server-side for batch processing, automated workflows, and integration with OMERO clients.\n\n**Common scenarios:**\n- Process multiple images in batch\n- Create automated analysis pipelines\n- Generate summary statistics across datasets\n- Export data in custom formats\n\n### 8. Advanced Features\n**File**: `references/advanced.md`\n\nCovers permissions, filesets, cross-group queries, delete operations, and other advanced functionality.\n\n**Common scenarios:**\n- Handle group permissions\n- Access original imported files\n- Perform cross-group queries\n- Delete objects with callbacks\n\n## Installation\n\n```bash\nuv pip install omero-py\n```\n\n**Requirements:**\n- Python 3.7+\n- Zeroc Ice 3.6+\n- Access to an OMERO server (host, port, credentials)\n\n## Quick Start\n\nBasic connection pattern:\n\n```python\nfrom omero.gateway import BlitzGateway\n\n# Connect to OMERO server\nconn = BlitzGateway(username, password, host=host, port=port)\nconnected = conn.connect()\n\nif connected:\n    # Perform operations\n    for project in conn.listProjects():\n        print(project.getName())\n\n    # Always close connection\n    conn.close()\nelse:\n    print(\"Connection failed\")\n```\n\n**Recommended pattern with context manager:**\n\n```python\nfrom omero.gateway import BlitzGateway\n\nwith BlitzGateway(username, password, host=host, port=port) as conn:\n    # Connection automatically managed\n    for project in conn.listProjects():\n        print(project.getName())\n    # Automatically closed on exit\n```\n\n## Selecting the Right Capability\n\n**For data exploration:**\n- Start with `references/connection.md` to establish connection\n- Use `references/data_access.md` to navigate hierarchy\n- Check `references/metadata.md` for annotation details\n\n**For image analysis:**\n- Use `references/image_processing.md` for pixel data access\n- Use `references/rois.md` for region-based analysis\n- Use `references/tables.md` to store results\n\n**For automation:**\n- Use `references/scripts.md` for server-side processing\n- Use `references/data_access.md` for batch data retrieval\n\n**For advanced operations:**\n- Use `references/advanced.md` for permissions and deletion\n- Check `references/connection.md` for cross-group queries\n\n## Common Workflows\n\n### Workflow 1: Retrieve and Analyze Images\n\n1. Connect to OMERO server (`references/connection.md`)\n2. Navigate to dataset (`references/data_access.md`)\n3. Retrieve images from dataset (`references/data_access.md`)\n4. Access pixel data as NumPy array (`references/image_processing.md`)\n5. Perform analysis\n6. Store results as table or file annotation (`references/tables.md` or `references/metadata.md`)\n\n### Workflow 2: Batch ROI Analysis\n\n1. Connect to OMERO server\n2. Retrieve images with existing ROIs (`references/rois.md`)\n3. For each image, get ROI shapes\n4. Extract pixel intensities within ROIs (`references/rois.md`)\n5. Store measurements in OMERO table (`references/tables.md`)\n\n### Workflow 3: Create Analysis Script\n\n1. Design analysis workflow\n2. Use OMERO.scripts framework (`references/scripts.md`)\n3. Access data through script parameters\n4. Process images in batch\n5. Generate outputs (new images, tables, files)\n\n## Error Handling\n\nAlways wrap OMERO operations in try-except blocks and ensure connections are properly closed:\n\n```python\nfrom omero.gateway import BlitzGateway\nimport traceback\n\ntry:\n    conn = BlitzGateway(username, password, host=host, port=port)\n    if not conn.connect():\n        raise Exception(\"Connection failed\")\n\n    # Perform operations\n\nexcept Exception as e:\n    print(f\"Error: {e}\")\n    traceback.print_exc()\nfinally:\n    if conn:\n        conn.close()\n```\n\n## Additional Resources\n\n- **Official Documentation**: https://omero.readthedocs.io/en/stable/developers/Python.html\n- **BlitzGateway API**: https://omero.readthedocs.io/en/stable/developers/Python.html#omero-blitzgateway\n- **OMERO Model**: https://omero.readthedocs.io/en/stable/developers/Model.html\n- **Community Forum**: https://forum.image.sc/tag/omero\n\n## Notes\n\n- OMERO uses group-based permissions (READ-ONLY, READ-ANNOTATE, READ-WRITE)\n- Images in OMERO are organized hierarchically: Project > Dataset > Image\n- Screening data uses: Screen > Plate > Well > WellSample > Image\n- Always close connections to free server resources\n- Use context managers for automatic resource management\n- Pixel data is returned as NumPy arrays for analysis\n",
        "data/k-dense-ai/openalex-database/SKILL.md": "---\nname: openalex-database\ndescription: Query and analyze scholarly literature using the OpenAlex database. This skill should be used when searching for academic papers, analyzing research trends, finding works by authors or institutions, tracking citations, discovering open access publications, or conducting bibliometric analysis across 240M+ scholarly works. Use for literature searches, research output analysis, citation analysis, and academic database queries.\n---\n\n# OpenAlex Database\n\n## Overview\n\nOpenAlex is a comprehensive open catalog of 240M+ scholarly works, authors, institutions, topics, sources, publishers, and funders. This skill provides tools and workflows for querying the OpenAlex API to search literature, analyze research output, track citations, and conduct bibliometric studies.\n\n## Quick Start\n\n### Basic Setup\n\nAlways initialize the client with an email address to access the polite pool (10x rate limit boost):\n\n```python\nfrom scripts.openalex_client import OpenAlexClient\n\nclient = OpenAlexClient(email=\"your-email@example.edu\")\n```\n\n### Installation Requirements\n\nInstall required package using uv:\n\n```bash\nuv pip install requests\n```\n\nNo API key required - OpenAlex is completely open.\n\n## Core Capabilities\n\n### 1. Search for Papers\n\n**Use for**: Finding papers by title, abstract, or topic\n\n```python\n# Simple search\nresults = client.search_works(\n    search=\"machine learning\",\n    per_page=100\n)\n\n# Search with filters\nresults = client.search_works(\n    search=\"CRISPR gene editing\",\n    filter_params={\n        \"publication_year\": \">2020\",\n        \"is_oa\": \"true\"\n    },\n    sort=\"cited_by_count:desc\"\n)\n```\n\n### 2. Find Works by Author\n\n**Use for**: Getting all publications by a specific researcher\n\nUse the two-step pattern (entity name  ID  works):\n\n```python\nfrom scripts.query_helpers import find_author_works\n\nworks = find_author_works(\n    author_name=\"Jennifer Doudna\",\n    client=client,\n    limit=100\n)\n```\n\n**Manual two-step approach**:\n```python\n# Step 1: Get author ID\nauthor_response = client._make_request(\n    '/authors',\n    params={'search': 'Jennifer Doudna', 'per-page': 1}\n)\nauthor_id = author_response['results'][0]['id'].split('/')[-1]\n\n# Step 2: Get works\nworks = client.search_works(\n    filter_params={\"authorships.author.id\": author_id}\n)\n```\n\n### 3. Find Works from Institution\n\n**Use for**: Analyzing research output from universities or organizations\n\n```python\nfrom scripts.query_helpers import find_institution_works\n\nworks = find_institution_works(\n    institution_name=\"Stanford University\",\n    client=client,\n    limit=200\n)\n```\n\n### 4. Highly Cited Papers\n\n**Use for**: Finding influential papers in a field\n\n```python\nfrom scripts.query_helpers import find_highly_cited_recent_papers\n\npapers = find_highly_cited_recent_papers(\n    topic=\"quantum computing\",\n    years=\">2020\",\n    client=client,\n    limit=100\n)\n```\n\n### 5. Open Access Papers\n\n**Use for**: Finding freely available research\n\n```python\nfrom scripts.query_helpers import get_open_access_papers\n\npapers = get_open_access_papers(\n    search_term=\"climate change\",\n    client=client,\n    oa_status=\"any\",  # or \"gold\", \"green\", \"hybrid\", \"bronze\"\n    limit=200\n)\n```\n\n### 6. Publication Trends Analysis\n\n**Use for**: Tracking research output over time\n\n```python\nfrom scripts.query_helpers import get_publication_trends\n\ntrends = get_publication_trends(\n    search_term=\"artificial intelligence\",\n    filter_params={\"is_oa\": \"true\"},\n    client=client\n)\n\n# Sort and display\nfor trend in sorted(trends, key=lambda x: x['key'])[-10:]:\n    print(f\"{trend['key']}: {trend['count']} publications\")\n```\n\n### 7. Research Output Analysis\n\n**Use for**: Comprehensive analysis of author or institution research\n\n```python\nfrom scripts.query_helpers import analyze_research_output\n\nanalysis = analyze_research_output(\n    entity_type='institution',  # or 'author'\n    entity_name='MIT',\n    client=client,\n    years='>2020'\n)\n\nprint(f\"Total works: {analysis['total_works']}\")\nprint(f\"Open access: {analysis['open_access_percentage']}%\")\nprint(f\"Top topics: {analysis['top_topics'][:5]}\")\n```\n\n### 8. Batch Lookups\n\n**Use for**: Getting information for multiple DOIs, ORCIDs, or IDs efficiently\n\n```python\ndois = [\n    \"https://doi.org/10.1038/s41586-021-03819-2\",\n    \"https://doi.org/10.1126/science.abc1234\",\n    # ... up to 50 DOIs\n]\n\nworks = client.batch_lookup(\n    entity_type='works',\n    ids=dois,\n    id_field='doi'\n)\n```\n\n### 9. Random Sampling\n\n**Use for**: Getting representative samples for analysis\n\n```python\n# Small sample\nworks = client.sample_works(\n    sample_size=100,\n    seed=42,  # For reproducibility\n    filter_params={\"publication_year\": \"2023\"}\n)\n\n# Large sample (>10k) - automatically handles multiple requests\nworks = client.sample_works(\n    sample_size=25000,\n    seed=42,\n    filter_params={\"is_oa\": \"true\"}\n)\n```\n\n### 10. Citation Analysis\n\n**Use for**: Finding papers that cite a specific work\n\n```python\n# Get the work\nwork = client.get_entity('works', 'https://doi.org/10.1038/s41586-021-03819-2')\n\n# Get citing papers using cited_by_api_url\nimport requests\nciting_response = requests.get(\n    work['cited_by_api_url'],\n    params={'mailto': client.email, 'per-page': 200}\n)\nciting_works = citing_response.json()['results']\n```\n\n### 11. Topic and Subject Analysis\n\n**Use for**: Understanding research focus areas\n\n```python\n# Get top topics for an institution\ntopics = client.group_by(\n    entity_type='works',\n    group_field='topics.id',\n    filter_params={\n        \"authorships.institutions.id\": \"I136199984\",  # MIT\n        \"publication_year\": \">2020\"\n    }\n)\n\nfor topic in topics[:10]:\n    print(f\"{topic['key_display_name']}: {topic['count']} works\")\n```\n\n### 12. Large-Scale Data Extraction\n\n**Use for**: Downloading large datasets for analysis\n\n```python\n# Paginate through all results\nall_papers = client.paginate_all(\n    endpoint='/works',\n    params={\n        'search': 'synthetic biology',\n        'filter': 'publication_year:2020-2024'\n    },\n    max_results=10000\n)\n\n# Export to CSV\nimport csv\nwith open('papers.csv', 'w', newline='', encoding='utf-8') as f:\n    writer = csv.writer(f)\n    writer.writerow(['Title', 'Year', 'Citations', 'DOI', 'OA Status'])\n\n    for paper in all_papers:\n        writer.writerow([\n            paper.get('title', 'N/A'),\n            paper.get('publication_year', 'N/A'),\n            paper.get('cited_by_count', 0),\n            paper.get('doi', 'N/A'),\n            paper.get('open_access', {}).get('oa_status', 'closed')\n        ])\n```\n\n## Critical Best Practices\n\n### Always Use Email for Polite Pool\nAdd email to get 10x rate limit (1 req/sec  10 req/sec):\n```python\nclient = OpenAlexClient(email=\"your-email@example.edu\")\n```\n\n### Use Two-Step Pattern for Entity Lookups\nNever filter by entity names directly - always get ID first:\n```python\n#  Correct\n# 1. Search for entity  get ID\n# 2. Filter by ID\n\n#  Wrong\n# filter=author_name:Einstein  # This doesn't work!\n```\n\n### Use Maximum Page Size\nAlways use `per-page=200` for efficient data retrieval:\n```python\nresults = client.search_works(search=\"topic\", per_page=200)\n```\n\n### Batch Multiple IDs\nUse batch_lookup() for multiple IDs instead of individual requests:\n```python\n#  Correct - 1 request for 50 DOIs\nworks = client.batch_lookup('works', doi_list, 'doi')\n\n#  Wrong - 50 separate requests\nfor doi in doi_list:\n    work = client.get_entity('works', doi)\n```\n\n### Use Sample Parameter for Random Data\nUse `sample_works()` with seed for reproducible random sampling:\n```python\n#  Correct\nworks = client.sample_works(sample_size=100, seed=42)\n\n#  Wrong - random page numbers bias results\n# Using random page numbers doesn't give true random sample\n```\n\n### Select Only Needed Fields\nReduce response size by selecting specific fields:\n```python\nresults = client.search_works(\n    search=\"topic\",\n    select=['id', 'title', 'publication_year', 'cited_by_count']\n)\n```\n\n## Common Filter Patterns\n\n### Date Ranges\n```python\n# Single year\nfilter_params={\"publication_year\": \"2023\"}\n\n# After year\nfilter_params={\"publication_year\": \">2020\"}\n\n# Range\nfilter_params={\"publication_year\": \"2020-2024\"}\n```\n\n### Multiple Filters (AND)\n```python\n# All conditions must match\nfilter_params={\n    \"publication_year\": \">2020\",\n    \"is_oa\": \"true\",\n    \"cited_by_count\": \">100\"\n}\n```\n\n### Multiple Values (OR)\n```python\n# Any institution matches\nfilter_params={\n    \"authorships.institutions.id\": \"I136199984|I27837315\"  # MIT or Harvard\n}\n```\n\n### Collaboration (AND within attribute)\n```python\n# Papers with authors from BOTH institutions\nfilter_params={\n    \"authorships.institutions.id\": \"I136199984+I27837315\"  # MIT AND Harvard\n}\n```\n\n### Negation\n```python\n# Exclude type\nfilter_params={\n    \"type\": \"!paratext\"\n}\n```\n\n## Entity Types\n\nOpenAlex provides these entity types:\n- **works** - Scholarly documents (articles, books, datasets)\n- **authors** - Researchers with disambiguated identities\n- **institutions** - Universities and research organizations\n- **sources** - Journals, repositories, conferences\n- **topics** - Subject classifications\n- **publishers** - Publishing organizations\n- **funders** - Funding agencies\n\nAccess any entity type using consistent patterns:\n```python\nclient.search_works(...)\nclient.get_entity('authors', author_id)\nclient.group_by('works', 'topics.id', filter_params={...})\n```\n\n## External IDs\n\nUse external identifiers directly:\n```python\n# DOI for works\nwork = client.get_entity('works', 'https://doi.org/10.7717/peerj.4375')\n\n# ORCID for authors\nauthor = client.get_entity('authors', 'https://orcid.org/0000-0003-1613-5981')\n\n# ROR for institutions\ninstitution = client.get_entity('institutions', 'https://ror.org/02y3ad647')\n\n# ISSN for sources\nsource = client.get_entity('sources', 'issn:0028-0836')\n```\n\n## Reference Documentation\n\n### Detailed API Reference\nSee `references/api_guide.md` for:\n- Complete filter syntax\n- All available endpoints\n- Response structures\n- Error handling\n- Performance optimization\n- Rate limiting details\n\n### Common Query Examples\nSee `references/common_queries.md` for:\n- Complete working examples\n- Real-world use cases\n- Complex query patterns\n- Data export workflows\n- Multi-step analysis procedures\n\n## Scripts\n\n### openalex_client.py\nMain API client with:\n- Automatic rate limiting\n- Exponential backoff retry logic\n- Pagination support\n- Batch operations\n- Error handling\n\nUse for direct API access with full control.\n\n### query_helpers.py\nHigh-level helper functions for common operations:\n- `find_author_works()` - Get papers by author\n- `find_institution_works()` - Get papers from institution\n- `find_highly_cited_recent_papers()` - Get influential papers\n- `get_open_access_papers()` - Find OA publications\n- `get_publication_trends()` - Analyze trends over time\n- `analyze_research_output()` - Comprehensive analysis\n\nUse for common research queries with simplified interfaces.\n\n## Troubleshooting\n\n### Rate Limiting\nIf encountering 403 errors:\n1. Ensure email is added to requests\n2. Verify not exceeding 10 req/sec\n3. Client automatically implements exponential backoff\n\n### Empty Results\nIf searches return no results:\n1. Check filter syntax (see `references/api_guide.md`)\n2. Use two-step pattern for entity lookups (don't filter by names)\n3. Verify entity IDs are correct format\n\n### Timeout Errors\nFor large queries:\n1. Use pagination with `per-page=200`\n2. Use `select=` to limit returned fields\n3. Break into smaller queries if needed\n\n## Rate Limits\n\n- **Default**: 1 request/second, 100k requests/day\n- **Polite pool (with email)**: 10 requests/second, 100k requests/day\n\nAlways use polite pool for production workflows by providing email to client.\n\n## Notes\n\n- No authentication required\n- All data is open and free\n- Rate limits apply globally, not per IP\n- Use LitLLM with OpenRouter if LLM-based analysis is needed (don't use Perplexity API directly)\n- Client handles pagination, retries, and rate limiting automatically\n",
        "data/k-dense-ai/opentargets-database/SKILL.md": "---\nname: opentargets-database\ndescription: \"Query Open Targets Platform for target-disease associations, drug target discovery, tractability/safety data, genetics/omics evidence, known drugs, for therapeutic target identification.\"\n---\n\n# Open Targets Database\n\n## Overview\n\nThe Open Targets Platform is a comprehensive resource for systematic identification and prioritization of potential therapeutic drug targets. It integrates publicly available datasets including human genetics, omics, literature, and chemical data to build and score target-disease associations.\n\n**Key capabilities:**\n- Query target (gene) annotations including tractability, safety, expression\n- Search for disease-target associations with evidence scores\n- Retrieve evidence from multiple data types (genetics, pathways, literature, etc.)\n- Find known drugs for diseases and their mechanisms\n- Access drug information including clinical trial phases and adverse events\n- Evaluate target druggability and therapeutic potential\n\n**Data access:** The platform provides a GraphQL API, web interface, data downloads, and Google BigQuery access. This skill focuses on the GraphQL API for programmatic access.\n\n## When to Use This Skill\n\nThis skill should be used when:\n\n- **Target discovery:** Finding potential therapeutic targets for a disease\n- **Target assessment:** Evaluating tractability, safety, and druggability of genes\n- **Evidence gathering:** Retrieving supporting evidence for target-disease associations\n- **Drug repurposing:** Identifying existing drugs that could be repurposed for new indications\n- **Competitive intelligence:** Understanding clinical precedence and drug development landscape\n- **Target prioritization:** Ranking targets based on genetic evidence and other data types\n- **Mechanism research:** Investigating biological pathways and gene functions\n- **Biomarker discovery:** Finding genes differentially expressed in disease\n- **Safety assessment:** Identifying potential toxicity concerns for drug targets\n\n## Core Workflow\n\n### 1. Search for Entities\n\nStart by finding the identifiers for targets, diseases, or drugs of interest.\n\n**For targets (genes):**\n```python\nfrom scripts.query_opentargets import search_entities\n\n# Search by gene symbol or name\nresults = search_entities(\"BRCA1\", entity_types=[\"target\"])\n# Returns: [{\"id\": \"ENSG00000012048\", \"name\": \"BRCA1\", ...}]\n```\n\n**For diseases:**\n```python\n# Search by disease name\nresults = search_entities(\"alzheimer\", entity_types=[\"disease\"])\n# Returns: [{\"id\": \"EFO_0000249\", \"name\": \"Alzheimer disease\", ...}]\n```\n\n**For drugs:**\n```python\n# Search by drug name\nresults = search_entities(\"aspirin\", entity_types=[\"drug\"])\n# Returns: [{\"id\": \"CHEMBL25\", \"name\": \"ASPIRIN\", ...}]\n```\n\n**Identifiers used:**\n- Targets: Ensembl gene IDs (e.g., `ENSG00000157764`)\n- Diseases: EFO (Experimental Factor Ontology) IDs (e.g., `EFO_0000249`)\n- Drugs: ChEMBL IDs (e.g., `CHEMBL25`)\n\n### 2. Query Target Information\n\nRetrieve comprehensive target annotations to assess druggability and biology.\n\n```python\nfrom scripts.query_opentargets import get_target_info\n\ntarget_info = get_target_info(\"ENSG00000157764\", include_diseases=True)\n\n# Access key fields:\n# - approvedSymbol: HGNC gene symbol\n# - approvedName: Full gene name\n# - tractability: Druggability assessments across modalities\n# - safetyLiabilities: Known safety concerns\n# - geneticConstraint: Constraint scores from gnomAD\n# - associatedDiseases: Top disease associations with scores\n```\n\n**Key annotations to review:**\n- **Tractability:** Small molecule, antibody, PROTAC druggability predictions\n- **Safety:** Known toxicity concerns from multiple databases\n- **Genetic constraint:** pLI and LOEUF scores indicating essentiality\n- **Disease associations:** Diseases linked to the target with evidence scores\n\nRefer to `references/target_annotations.md` for detailed information about all target features.\n\n### 3. Query Disease Information\n\nGet disease details and associated targets/drugs.\n\n```python\nfrom scripts.query_opentargets import get_disease_info\n\ndisease_info = get_disease_info(\"EFO_0000249\", include_targets=True)\n\n# Access fields:\n# - name: Disease name\n# - description: Disease description\n# - therapeuticAreas: High-level disease categories\n# - associatedTargets: Top targets with association scores\n```\n\n### 4. Retrieve Target-Disease Evidence\n\nGet detailed evidence supporting a target-disease association.\n\n```python\nfrom scripts.query_opentargets import get_target_disease_evidence\n\n# Get all evidence\nevidence = get_target_disease_evidence(\n    ensembl_id=\"ENSG00000157764\",\n    efo_id=\"EFO_0000249\"\n)\n\n# Filter by evidence type\ngenetic_evidence = get_target_disease_evidence(\n    ensembl_id=\"ENSG00000157764\",\n    efo_id=\"EFO_0000249\",\n    data_types=[\"genetic_association\"]\n)\n\n# Each evidence record contains:\n# - datasourceId: Specific data source (e.g., \"gwas_catalog\", \"chembl\")\n# - datatypeId: Evidence category (e.g., \"genetic_association\", \"known_drug\")\n# - score: Evidence strength (0-1)\n# - studyId: Original study identifier\n# - literature: Associated publications\n```\n\n**Major evidence types:**\n1. **genetic_association:** GWAS, rare variants, ClinVar, gene burden\n2. **somatic_mutation:** Cancer Gene Census, IntOGen, cancer biomarkers\n3. **known_drug:** Clinical precedence from approved/clinical drugs\n4. **affected_pathway:** CRISPR screens, pathway analyses, gene signatures\n5. **rna_expression:** Differential expression from Expression Atlas\n6. **animal_model:** Mouse phenotypes from IMPC\n7. **literature:** Text-mining from Europe PMC\n\nRefer to `references/evidence_types.md` for detailed descriptions of all evidence types and interpretation guidelines.\n\n### 5. Find Known Drugs\n\nIdentify drugs used for a disease and their targets.\n\n```python\nfrom scripts.query_opentargets import get_known_drugs_for_disease\n\ndrugs = get_known_drugs_for_disease(\"EFO_0000249\")\n\n# drugs contains:\n# - uniqueDrugs: Total number of unique drugs\n# - uniqueTargets: Total number of unique targets\n# - rows: List of drug-target-indication records with:\n#   - drug: {name, drugType, maximumClinicalTrialPhase}\n#   - targets: Genes targeted by the drug\n#   - phase: Clinical trial phase for this indication\n#   - status: Trial status (active, completed, etc.)\n#   - mechanismOfAction: How drug works\n```\n\n**Clinical phases:**\n- Phase 4: Approved drug\n- Phase 3: Late-stage clinical trials\n- Phase 2: Mid-stage trials\n- Phase 1: Early safety trials\n\n### 6. Get Drug Information\n\nRetrieve detailed drug information including mechanisms and indications.\n\n```python\nfrom scripts.query_opentargets import get_drug_info\n\ndrug_info = get_drug_info(\"CHEMBL25\")\n\n# Access:\n# - name, synonyms: Drug identifiers\n# - drugType: Small molecule, antibody, etc.\n# - maximumClinicalTrialPhase: Development stage\n# - mechanismsOfAction: Target and action type\n# - indications: Diseases with trial phases\n# - withdrawnNotice: If withdrawn, reasons and countries\n```\n\n### 7. Get All Associations for a Target\n\nFind all diseases associated with a target, optionally filtering by score.\n\n```python\nfrom scripts.query_opentargets import get_target_associations\n\n# Get associations with score >= 0.5\nassociations = get_target_associations(\n    ensembl_id=\"ENSG00000157764\",\n    min_score=0.5\n)\n\n# Each association contains:\n# - disease: {id, name}\n# - score: Overall association score (0-1)\n# - datatypeScores: Breakdown by evidence type\n```\n\n**Association scores:**\n- Range: 0-1 (higher = stronger evidence)\n- Aggregate evidence across all data types using harmonic sum\n- NOT confidence scores but relative ranking metrics\n- Under-studied diseases may have lower scores despite good evidence\n\n## GraphQL API Details\n\n**For custom queries beyond the provided helper functions**, use the GraphQL API directly or modify `scripts/query_opentargets.py`.\n\nKey information:\n- **Endpoint:** `https://api.platform.opentargets.org/api/v4/graphql`\n- **Interactive browser:** `https://api.platform.opentargets.org/api/v4/graphql/browser`\n- **No authentication required**\n- **Request only needed fields** to minimize response size\n- **Use pagination** for large result sets: `page: {size: N, index: M}`\n\nRefer to `references/api_reference.md` for:\n- Complete endpoint documentation\n- Example queries for all entity types\n- Error handling patterns\n- Best practices for API usage\n\n## Best Practices\n\n### Target Prioritization Strategy\n\nWhen prioritizing drug targets:\n\n1. **Start with genetic evidence:** Human genetics (GWAS, rare variants) provides strongest disease relevance\n2. **Check tractability:** Prefer targets with clinical or discovery precedence\n3. **Assess safety:** Review safety liabilities, expression patterns, and genetic constraint\n4. **Evaluate clinical precedence:** Known drugs indicate druggability and therapeutic window\n5. **Consider multiple evidence types:** Convergent evidence from different sources increases confidence\n6. **Validate mechanistically:** Pathway evidence and biological plausibility\n7. **Review literature manually:** For critical decisions, examine primary publications\n\n### Evidence Interpretation\n\n**Strong evidence indicators:**\n- Multiple independent evidence sources\n- High genetic association scores (especially GWAS with L2G > 0.5)\n- Clinical precedence from approved drugs\n- ClinVar pathogenic variants with disease match\n- Mouse models with relevant phenotypes\n\n**Caution flags:**\n- Single evidence source only\n- Text-mining as sole evidence (requires manual validation)\n- Conflicting evidence across sources\n- High essentiality + ubiquitous expression (poor therapeutic window)\n- Multiple safety liabilities\n\n**Score interpretation:**\n- Scores rank relative strength, not absolute confidence\n- Under-studied diseases have lower scores despite potentially valid targets\n- Weight expert-curated sources higher than computational predictions\n- Check evidence breakdown, not just overall score\n\n### Common Workflows\n\n**Workflow 1: Target Discovery for a Disease**\n1. Search for disease  get EFO ID\n2. Query disease info with `include_targets=True`\n3. Review top targets sorted by association score\n4. For promising targets, get detailed target info\n5. Examine evidence types supporting each association\n6. Assess tractability and safety for prioritized targets\n\n**Workflow 2: Target Validation**\n1. Search for target  get Ensembl ID\n2. Get comprehensive target info\n3. Check tractability (especially clinical precedence)\n4. Review safety liabilities and genetic constraint\n5. Examine disease associations to understand biology\n6. Look for chemical probes or tool compounds\n7. Check known drugs targeting gene for mechanism insights\n\n**Workflow 3: Drug Repurposing**\n1. Search for disease  get EFO ID\n2. Get known drugs for disease\n3. For each drug, get detailed drug info\n4. Examine mechanisms of action and targets\n5. Look for related disease indications\n6. Assess clinical trial phases and status\n7. Identify repurposing opportunities based on mechanism\n\n**Workflow 4: Competitive Intelligence**\n1. Search for target of interest\n2. Get associated diseases with evidence\n3. For each disease, get known drugs\n4. Review clinical phases and development status\n5. Identify competitors and their mechanisms\n6. Assess clinical precedence and market landscape\n\n## Resources\n\n### Scripts\n\n**scripts/query_opentargets.py**\nHelper functions for common API operations:\n- `search_entities()` - Search for targets, diseases, or drugs\n- `get_target_info()` - Retrieve target annotations\n- `get_disease_info()` - Retrieve disease information\n- `get_target_disease_evidence()` - Get supporting evidence\n- `get_known_drugs_for_disease()` - Find drugs for a disease\n- `get_drug_info()` - Retrieve drug details\n- `get_target_associations()` - Get all associations for a target\n- `execute_query()` - Execute custom GraphQL queries\n\n### References\n\n**references/api_reference.md**\nComplete GraphQL API documentation including:\n- Endpoint details and authentication\n- Available query types (target, disease, drug, search)\n- Example queries for all common operations\n- Error handling and best practices\n- Data licensing and citation requirements\n\n**references/evidence_types.md**\nComprehensive guide to evidence types and data sources:\n- Detailed descriptions of all 7 major evidence types\n- Scoring methodologies for each source\n- Evidence interpretation guidelines\n- Strengths and limitations of each evidence type\n- Quality assessment recommendations\n\n**references/target_annotations.md**\nComplete target annotation reference:\n- 12 major annotation categories explained\n- Tractability assessment details\n- Safety liability sources\n- Expression, essentiality, and constraint data\n- Interpretation guidelines for target prioritization\n- Red flags and green flags for target assessment\n\n## Data Updates and Versioning\n\nThe Open Targets Platform is updated **quarterly** with new data releases. The current release (as of October 2025) is available at the API endpoint.\n\n**Release information:** Check https://platform-docs.opentargets.org/release-notes for the latest updates.\n\n**Citation:** When using Open Targets data, cite:\nOchoa, D. et al. (2025) Open Targets Platform: facilitating therapeutic hypotheses building in drug discovery. Nucleic Acids Research, 53(D1):D1467-D1477.\n\n## Limitations and Considerations\n\n1. **API is for exploratory queries:** For systematic analyses of many targets/diseases, use data downloads or BigQuery\n2. **Scores are relative, not absolute:** Association scores rank evidence strength but don't predict clinical success\n3. **Under-studied diseases score lower:** Novel or rare diseases may have strong evidence but lower aggregate scores\n4. **Evidence quality varies:** Weight expert-curated sources higher than computational predictions\n5. **Requires biological interpretation:** Scores and evidence must be interpreted in biological and clinical context\n6. **No authentication required:** All data is freely accessible, but cite appropriately\n",
        "data/k-dense-ai/opentrons-integration/SKILL.md": "---\nname: opentrons-integration\ndescription: \"Lab automation platform for Flex/OT-2 robots. Write Protocol API v2 protocols, liquid handling, hardware modules (heater-shaker, thermocycler), labware management, for automated pipetting workflows.\"\n---\n\n# Opentrons Integration\n\n## Overview\n\nOpentrons is a Python-based lab automation platform for Flex and OT-2 robots. Write Protocol API v2 protocols for liquid handling, control hardware modules (heater-shaker, thermocycler), manage labware, for automated pipetting workflows.\n\n## When to Use This Skill\n\nThis skill should be used when:\n- Writing Opentrons Protocol API v2 protocols in Python\n- Automating liquid handling workflows on Flex or OT-2 robots\n- Controlling hardware modules (temperature, magnetic, heater-shaker, thermocycler)\n- Setting up labware configurations and deck layouts\n- Implementing complex pipetting operations (serial dilutions, plate replication, PCR setup)\n- Managing tip usage and optimizing protocol efficiency\n- Working with multi-channel pipettes for 96-well plate operations\n- Simulating and testing protocols before robot execution\n\n## Core Capabilities\n\n### 1. Protocol Structure and Metadata\n\nEvery Opentrons protocol follows a standard structure:\n\n```python\nfrom opentrons import protocol_api\n\n# Metadata\nmetadata = {\n    'protocolName': 'My Protocol',\n    'author': 'Name <email@example.com>',\n    'description': 'Protocol description',\n    'apiLevel': '2.19'  # Use latest available API version\n}\n\n# Requirements (optional)\nrequirements = {\n    'robotType': 'Flex',  # or 'OT-2'\n    'apiLevel': '2.19'\n}\n\n# Run function\ndef run(protocol: protocol_api.ProtocolContext):\n    # Protocol commands go here\n    pass\n```\n\n**Key elements:**\n- Import `protocol_api` from `opentrons`\n- Define `metadata` dict with protocolName, author, description, apiLevel\n- Optional `requirements` dict for robot type and API version\n- Implement `run()` function receiving `ProtocolContext` as parameter\n- All protocol logic goes inside the `run()` function\n\n### 2. Loading Hardware\n\n**Loading Instruments (Pipettes):**\n\n```python\ndef run(protocol: protocol_api.ProtocolContext):\n    # Load pipette on specific mount\n    left_pipette = protocol.load_instrument(\n        'p1000_single_flex',  # Instrument name\n        'left',               # Mount: 'left' or 'right'\n        tip_racks=[tip_rack]  # List of tip rack labware objects\n    )\n```\n\nCommon pipette names:\n- Flex: `p50_single_flex`, `p1000_single_flex`, `p50_multi_flex`, `p1000_multi_flex`\n- OT-2: `p20_single_gen2`, `p300_single_gen2`, `p1000_single_gen2`, `p20_multi_gen2`, `p300_multi_gen2`\n\n**Loading Labware:**\n\n```python\n# Load labware directly on deck\nplate = protocol.load_labware(\n    'corning_96_wellplate_360ul_flat',  # Labware API name\n    'D1',                                # Deck slot (Flex: A1-D3, OT-2: 1-11)\n    label='Sample Plate'                 # Optional display label\n)\n\n# Load tip rack\ntip_rack = protocol.load_labware('opentrons_flex_96_tiprack_1000ul', 'C1')\n\n# Load labware on adapter\nadapter = protocol.load_adapter('opentrons_flex_96_tiprack_adapter', 'B1')\ntips = adapter.load_labware('opentrons_flex_96_tiprack_200ul')\n```\n\n**Loading Modules:**\n\n```python\n# Temperature module\ntemp_module = protocol.load_module('temperature module gen2', 'D3')\ntemp_plate = temp_module.load_labware('corning_96_wellplate_360ul_flat')\n\n# Magnetic module\nmag_module = protocol.load_module('magnetic module gen2', 'C2')\nmag_plate = mag_module.load_labware('nest_96_wellplate_100ul_pcr_full_skirt')\n\n# Heater-Shaker module\nhs_module = protocol.load_module('heaterShakerModuleV1', 'D1')\nhs_plate = hs_module.load_labware('corning_96_wellplate_360ul_flat')\n\n# Thermocycler module (takes up specific slots automatically)\ntc_module = protocol.load_module('thermocyclerModuleV2')\ntc_plate = tc_module.load_labware('nest_96_wellplate_100ul_pcr_full_skirt')\n```\n\n### 3. Liquid Handling Operations\n\n**Basic Operations:**\n\n```python\n# Pick up tip\npipette.pick_up_tip()\n\n# Aspirate (draw liquid in)\npipette.aspirate(\n    volume=100,           # Volume in L\n    location=source['A1'] # Well or location object\n)\n\n# Dispense (expel liquid)\npipette.dispense(\n    volume=100,\n    location=dest['B1']\n)\n\n# Drop tip\npipette.drop_tip()\n\n# Return tip to rack\npipette.return_tip()\n```\n\n**Complex Operations:**\n\n```python\n# Transfer (combines pick_up, aspirate, dispense, drop_tip)\npipette.transfer(\n    volume=100,\n    source=source_plate['A1'],\n    dest=dest_plate['B1'],\n    new_tip='always'  # 'always', 'once', or 'never'\n)\n\n# Distribute (one source to multiple destinations)\npipette.distribute(\n    volume=50,\n    source=reservoir['A1'],\n    dest=[plate['A1'], plate['A2'], plate['A3']],\n    new_tip='once'\n)\n\n# Consolidate (multiple sources to one destination)\npipette.consolidate(\n    volume=50,\n    source=[plate['A1'], plate['A2'], plate['A3']],\n    dest=reservoir['A1'],\n    new_tip='once'\n)\n```\n\n**Advanced Techniques:**\n\n```python\n# Mix (aspirate and dispense in same location)\npipette.mix(\n    repetitions=3,\n    volume=50,\n    location=plate['A1']\n)\n\n# Air gap (prevent dripping)\npipette.aspirate(100, source['A1'])\npipette.air_gap(20)  # 20L air gap\npipette.dispense(120, dest['A1'])\n\n# Blow out (expel remaining liquid)\npipette.blow_out(location=dest['A1'].top())\n\n# Touch tip (remove droplets on tip exterior)\npipette.touch_tip(location=plate['A1'])\n```\n\n**Flow Rate Control:**\n\n```python\n# Set flow rates (L/s)\npipette.flow_rate.aspirate = 150\npipette.flow_rate.dispense = 300\npipette.flow_rate.blow_out = 400\n```\n\n### 4. Accessing Wells and Locations\n\n**Well Access Methods:**\n\n```python\n# By name\nwell_a1 = plate['A1']\n\n# By index\nfirst_well = plate.wells()[0]\n\n# All wells\nall_wells = plate.wells()  # Returns list\n\n# By rows\nrows = plate.rows()  # Returns list of lists\nrow_a = plate.rows()[0]  # All wells in row A\n\n# By columns\ncolumns = plate.columns()  # Returns list of lists\ncolumn_1 = plate.columns()[0]  # All wells in column 1\n\n# Wells by name (dictionary)\nwells_dict = plate.wells_by_name()  # {'A1': Well, 'A2': Well, ...}\n```\n\n**Location Methods:**\n\n```python\n# Top of well (default: 1mm below top)\npipette.aspirate(100, well.top())\npipette.aspirate(100, well.top(z=5))  # 5mm above top\n\n# Bottom of well (default: 1mm above bottom)\npipette.aspirate(100, well.bottom())\npipette.aspirate(100, well.bottom(z=2))  # 2mm above bottom\n\n# Center of well\npipette.aspirate(100, well.center())\n```\n\n### 5. Hardware Module Control\n\n**Temperature Module:**\n\n```python\n# Set temperature\ntemp_module.set_temperature(celsius=4)\n\n# Wait for temperature\ntemp_module.await_temperature(celsius=4)\n\n# Deactivate\ntemp_module.deactivate()\n\n# Check status\ncurrent_temp = temp_module.temperature  # Current temperature\ntarget_temp = temp_module.target  # Target temperature\n```\n\n**Magnetic Module:**\n\n```python\n# Engage (raise magnets)\nmag_module.engage(height_from_base=10)  # mm from labware base\n\n# Disengage (lower magnets)\nmag_module.disengage()\n\n# Check status\nis_engaged = mag_module.status  # 'engaged' or 'disengaged'\n```\n\n**Heater-Shaker Module:**\n\n```python\n# Set temperature\nhs_module.set_target_temperature(celsius=37)\n\n# Wait for temperature\nhs_module.wait_for_temperature()\n\n# Set shake speed\nhs_module.set_and_wait_for_shake_speed(rpm=500)\n\n# Close labware latch\nhs_module.close_labware_latch()\n\n# Open labware latch\nhs_module.open_labware_latch()\n\n# Deactivate heater\nhs_module.deactivate_heater()\n\n# Deactivate shaker\nhs_module.deactivate_shaker()\n```\n\n**Thermocycler Module:**\n\n```python\n# Open lid\ntc_module.open_lid()\n\n# Close lid\ntc_module.close_lid()\n\n# Set lid temperature\ntc_module.set_lid_temperature(celsius=105)\n\n# Set block temperature\ntc_module.set_block_temperature(\n    temperature=95,\n    hold_time_seconds=30,\n    hold_time_minutes=0.5,\n    block_max_volume=50  # L per well\n)\n\n# Execute profile (PCR cycling)\nprofile = [\n    {'temperature': 95, 'hold_time_seconds': 30},\n    {'temperature': 57, 'hold_time_seconds': 30},\n    {'temperature': 72, 'hold_time_seconds': 60}\n]\ntc_module.execute_profile(\n    steps=profile,\n    repetitions=30,\n    block_max_volume=50\n)\n\n# Deactivate\ntc_module.deactivate_lid()\ntc_module.deactivate_block()\n```\n\n**Absorbance Plate Reader:**\n\n```python\n# Initialize and read\nresult = plate_reader.read(wavelengths=[450, 650])\n\n# Access readings\nabsorbance_data = result  # Dict with wavelength keys\n```\n\n### 6. Liquid Tracking and Labeling\n\n**Define Liquids:**\n\n```python\n# Define liquid types\nwater = protocol.define_liquid(\n    name='Water',\n    description='Ultrapure water',\n    display_color='#0000FF'  # Hex color code\n)\n\nsample = protocol.define_liquid(\n    name='Sample',\n    description='Cell lysate sample',\n    display_color='#FF0000'\n)\n```\n\n**Load Liquids into Wells:**\n\n```python\n# Load liquid into specific wells\nreservoir['A1'].load_liquid(liquid=water, volume=50000)  # L\nplate['A1'].load_liquid(liquid=sample, volume=100)\n\n# Mark wells as empty\nplate['B1'].load_empty()\n```\n\n### 7. Protocol Control and Utilities\n\n**Execution Control:**\n\n```python\n# Pause protocol\nprotocol.pause(msg='Replace tip box and resume')\n\n# Delay\nprotocol.delay(seconds=60)\nprotocol.delay(minutes=5)\n\n# Comment (appears in logs)\nprotocol.comment('Starting serial dilution')\n\n# Home robot\nprotocol.home()\n```\n\n**Conditional Logic:**\n\n```python\n# Check if simulating\nif protocol.is_simulating():\n    protocol.comment('Running in simulation mode')\nelse:\n    protocol.comment('Running on actual robot')\n```\n\n**Rail Lights (Flex only):**\n\n```python\n# Turn lights on\nprotocol.set_rail_lights(on=True)\n\n# Turn lights off\nprotocol.set_rail_lights(on=False)\n```\n\n### 8. Multi-Channel and 8-Channel Pipetting\n\nWhen using multi-channel pipettes:\n\n```python\n# Load 8-channel pipette\nmulti_pipette = protocol.load_instrument(\n    'p300_multi_gen2',\n    'left',\n    tip_racks=[tips]\n)\n\n# Access entire column with single well reference\nmulti_pipette.transfer(\n    volume=100,\n    source=source_plate['A1'],  # Accesses entire column 1\n    dest=dest_plate['A1']       # Dispenses to entire column 1\n)\n\n# Use rows() for row-wise operations\nfor row in plate.rows():\n    multi_pipette.transfer(100, reservoir['A1'], row[0])\n```\n\n### 9. Common Protocol Patterns\n\n**Serial Dilution:**\n\n```python\ndef run(protocol: protocol_api.ProtocolContext):\n    # Load labware\n    tips = protocol.load_labware('opentrons_flex_96_tiprack_200ul', 'D1')\n    reservoir = protocol.load_labware('nest_12_reservoir_15ml', 'D2')\n    plate = protocol.load_labware('corning_96_wellplate_360ul_flat', 'D3')\n\n    # Load pipette\n    p300 = protocol.load_instrument('p300_single_flex', 'left', tip_racks=[tips])\n\n    # Add diluent to all wells except first\n    p300.transfer(100, reservoir['A1'], plate.rows()[0][1:])\n\n    # Serial dilution across row\n    p300.transfer(\n        100,\n        plate.rows()[0][:11],  # Source: wells 0-10\n        plate.rows()[0][1:],   # Dest: wells 1-11\n        mix_after=(3, 50),     # Mix 3x with 50L after dispense\n        new_tip='always'\n    )\n```\n\n**Plate Replication:**\n\n```python\ndef run(protocol: protocol_api.ProtocolContext):\n    # Load labware\n    tips = protocol.load_labware('opentrons_flex_96_tiprack_1000ul', 'C1')\n    source = protocol.load_labware('corning_96_wellplate_360ul_flat', 'D1')\n    dest = protocol.load_labware('corning_96_wellplate_360ul_flat', 'D2')\n\n    # Load pipette\n    p1000 = protocol.load_instrument('p1000_single_flex', 'left', tip_racks=[tips])\n\n    # Transfer from all wells in source to dest\n    p1000.transfer(\n        100,\n        source.wells(),\n        dest.wells(),\n        new_tip='always'\n    )\n```\n\n**PCR Setup:**\n\n```python\ndef run(protocol: protocol_api.ProtocolContext):\n    # Load thermocycler\n    tc_mod = protocol.load_module('thermocyclerModuleV2')\n    tc_plate = tc_mod.load_labware('nest_96_wellplate_100ul_pcr_full_skirt')\n\n    # Load tips and reagents\n    tips = protocol.load_labware('opentrons_flex_96_tiprack_200ul', 'C1')\n    reagents = protocol.load_labware('opentrons_24_tuberack_nest_1.5ml_snapcap', 'D1')\n\n    # Load pipette\n    p300 = protocol.load_instrument('p300_single_flex', 'left', tip_racks=[tips])\n\n    # Open thermocycler lid\n    tc_mod.open_lid()\n\n    # Distribute master mix\n    p300.distribute(\n        20,\n        reagents['A1'],\n        tc_plate.wells(),\n        new_tip='once'\n    )\n\n    # Add samples (example for first 8 wells)\n    for i, well in enumerate(tc_plate.wells()[:8]):\n        p300.transfer(5, reagents.wells()[i+1], well, new_tip='always')\n\n    # Run PCR\n    tc_mod.close_lid()\n    tc_mod.set_lid_temperature(105)\n\n    # PCR profile\n    tc_mod.set_block_temperature(95, hold_time_seconds=180)\n\n    profile = [\n        {'temperature': 95, 'hold_time_seconds': 15},\n        {'temperature': 60, 'hold_time_seconds': 30},\n        {'temperature': 72, 'hold_time_seconds': 30}\n    ]\n    tc_mod.execute_profile(steps=profile, repetitions=35, block_max_volume=25)\n\n    tc_mod.set_block_temperature(72, hold_time_minutes=5)\n    tc_mod.set_block_temperature(4)\n\n    tc_mod.deactivate_lid()\n    tc_mod.open_lid()\n```\n\n## Best Practices\n\n1. **Always specify API level**: Use the latest stable API version in metadata\n2. **Use meaningful labels**: Label labware for easier identification in logs\n3. **Check tip availability**: Ensure sufficient tips for protocol completion\n4. **Add comments**: Use `protocol.comment()` for debugging and logging\n5. **Simulate first**: Always test protocols in simulation before running on robot\n6. **Handle errors gracefully**: Add pauses for manual intervention when needed\n7. **Consider timing**: Use delays when protocols require incubation periods\n8. **Track liquids**: Use liquid tracking for better setup validation\n9. **Optimize tip usage**: Use `new_tip='once'` when appropriate to save tips\n10. **Control flow rates**: Adjust flow rates for viscous or volatile liquids\n\n## Troubleshooting\n\n**Common Issues:**\n\n- **Out of tips**: Verify tip rack capacity matches protocol requirements\n- **Labware collisions**: Check deck layout for spatial conflicts\n- **Volume errors**: Ensure volumes don't exceed well or pipette capacities\n- **Module not responding**: Verify module is properly connected and firmware is updated\n- **Inaccurate volumes**: Calibrate pipettes and check for air bubbles\n- **Protocol fails in simulation**: Check API version compatibility and labware definitions\n\n## Resources\n\nFor detailed API documentation, see `references/api_reference.md` in this skill directory.\n\nFor example protocol templates, see `scripts/` directory.\n",
        "data/k-dense-ai/paper-2-web/SKILL.md": "---\nname: paper-2-web\ndescription: This skill should be used when converting academic papers into promotional and presentation formats including interactive websites (Paper2Web), presentation videos (Paper2Video), and conference posters (Paper2Poster). Use this skill for tasks involving paper dissemination, conference preparation, creating explorable academic homepages, generating video abstracts, or producing print-ready posters from LaTeX or PDF sources.\n---\n\n# Paper2All: Academic Paper Transformation Pipeline\n\n## Overview\n\nThis skill enables the transformation of academic papers into multiple promotional and presentation formats using the Paper2All autonomous pipeline. The system converts research papers (LaTeX or PDF) into three primary outputs:\n\n1. **Paper2Web**: Interactive, explorable academic homepages with layout-aware design\n2. **Paper2Video**: Professional presentation videos with narration, slides, and optional talking-head\n3. **Paper2Poster**: Print-ready conference posters with professional layouts\n\nThe pipeline uses LLM-powered content extraction, design generation, and iterative refinement to create high-quality outputs suitable for conferences, journals, preprint repositories, and academic promotion.\n\n## When to Use This Skill\n\nUse this skill when:\n\n- **Creating conference materials**: Posters, presentation videos, and companion websites for academic conferences\n- **Promoting research**: Converting published papers or preprints into accessible, engaging web formats\n- **Preparing presentations**: Generating video abstracts or full presentation videos from paper content\n- **Disseminating findings**: Creating promotional materials for social media, lab websites, or institutional showcases\n- **Enhancing preprints**: Adding interactive homepages to bioRxiv, arXiv, or other preprint submissions\n- **Batch processing**: Generating promotional materials for multiple papers simultaneously\n\n**Trigger phrases**:\n- \"Convert this paper to a website\"\n- \"Generate a conference poster from my LaTeX paper\"\n- \"Create a video presentation from this research\"\n- \"Make an interactive homepage for my paper\"\n- \"Transform my paper into promotional materials\"\n- \"Generate a poster and video for my conference talk\"\n\n## Core Capabilities\n\n### 1. Paper2Web: Interactive Website Generation\n\nConverts papers into layout-aware, interactive academic homepages that go beyond simple HTML conversion.\n\n**Key Features**:\n- Responsive, multi-section layouts adapted to paper content\n- Interactive figures, tables, and citations\n- Mobile-friendly design with navigation\n- Automatic logo discovery (with Google Search API)\n- Aesthetic refinement and quality assessment\n\n**Best For**: Post-publication promotion, preprint enhancement, lab websites, permanent research showcases\n\n **See `references/paper2web.md` for detailed documentation**\n\n---\n\n### 2. Paper2Video: Presentation Video Generation\n\nGenerates professional presentation videos with slides, narration, cursor movements, and optional talking-head video.\n\n**Key Features**:\n- Automated slide generation from paper structure\n- Natural-sounding speech synthesis\n- Synchronized cursor movements and highlights\n- Optional talking-head video using Hallo2 (requires GPU)\n- Multi-language support\n\n**Best For**: Video abstracts, conference presentations, online talks, course materials, YouTube promotion\n\n **See `references/paper2video.md` for detailed documentation**\n\n---\n\n### 3. Paper2Poster: Conference Poster Generation\n\nCreates print-ready academic posters with professional layouts and visual design.\n\n**Key Features**:\n- Custom poster dimensions (any size)\n- Professional design templates\n- Institution branding support\n- QR code generation for links\n- High-resolution output (300+ DPI)\n\n**Best For**: Conference poster sessions, symposiums, academic exhibitions, virtual conferences\n\n **See `references/paper2poster.md` for detailed documentation**\n\n---\n\n## Quick Start\n\n### Prerequisites\n\n1. **Install Paper2All**:\n   ```bash\n   git clone https://github.com/YuhangChen1/Paper2All.git\n   cd Paper2All\n   uv pip install -r requirements.txt\n   ```\n\n2. **Configure API Keys** (create `.env` file):\n   ```\n   OPENAI_API_KEY=your_openai_api_key_here\n   # Optional: GOOGLE_API_KEY and GOOGLE_CSE_ID for logo search\n   ```\n\n3. **Install System Dependencies**:\n   - LibreOffice (document conversion)\n   - Poppler utilities (PDF processing)\n   - NVIDIA GPU with 48GB (optional, for talking-head videos)\n\n **See `references/installation.md` for complete installation guide**\n\n---\n\n### Basic Usage\n\n**Generate All Components** (website + poster + video):\n```bash\npython pipeline_all.py \\\n  --input-dir \"path/to/paper\" \\\n  --output-dir \"path/to/output\" \\\n  --model-choice 1\n```\n\n**Generate Website Only**:\n```bash\npython pipeline_all.py \\\n  --input-dir \"path/to/paper\" \\\n  --output-dir \"path/to/output\" \\\n  --model-choice 1 \\\n  --generate-website\n```\n\n**Generate Poster with Custom Size**:\n```bash\npython pipeline_all.py \\\n  --input-dir \"path/to/paper\" \\\n  --output-dir \"path/to/output\" \\\n  --model-choice 1 \\\n  --generate-poster \\\n  --poster-width-inches 60 \\\n  --poster-height-inches 40\n```\n\n**Generate Video** (lightweight pipeline):\n```bash\npython pipeline_light.py \\\n  --model_name_t gpt-4.1 \\\n  --model_name_v gpt-4.1 \\\n  --result_dir \"path/to/output\" \\\n  --paper_latex_root \"path/to/paper\"\n```\n\n **See `references/usage_examples.md` for comprehensive workflow examples**\n\n---\n\n## Workflow Decision Tree\n\nUse this decision tree to determine which components to generate:\n\n```\nUser needs promotional materials for paper?\n\n Need permanent online presence?\n   Generate Paper2Web (interactive website)\n\n Need physical conference materials?\n   Poster session?  Generate Paper2Poster\n   Oral presentation?  Generate Paper2Video\n\n Need video content?\n   Journal video abstract?  Generate Paper2Video (5-10 min)\n   Conference talk?  Generate Paper2Video (15-20 min)\n   Social media?  Generate Paper2Video (1-3 min)\n\n Need complete package?\n    Generate all three components\n```\n\n## Input Requirements\n\n### Supported Input Formats\n\n**1. LaTeX Source** (Recommended):\n```\npaper_directory/\n main.tex              # Main paper file\n sections/             # Optional: split sections\n figures/              # All figure files\n tables/               # Table files\n bibliography.bib      # References\n```\n\n**2. PDF**:\n- High-quality PDF with embedded fonts\n- Selectable text (not scanned images)\n- High-resolution figures (300+ DPI preferred)\n\n### Input Organization\n\n**Single Paper**:\n```bash\ninput/\n paper_name/\n     main.tex (or paper.pdf)\n     figures/\n     bibliography.bib\n```\n\n**Multiple Papers** (batch processing):\n```bash\ninput/\n paper1/\n    main.tex\n paper2/\n    main.tex\n paper3/\n     main.tex\n```\n\n## Common Parameters\n\n### Model Selection\n- `--model-choice 1`: GPT-4 (best balance of quality and cost)\n- `--model-choice 2`: GPT-4.1 (latest features, higher cost)\n- `--model_name_t gpt-3.5-turbo`: Faster, lower cost (acceptable quality)\n\n### Component Selection\n- `--generate-website`: Enable website generation\n- `--generate-poster`: Enable poster generation\n- `--generate-video`: Enable video generation\n- `--enable-talking-head`: Add talking-head to video (requires GPU)\n\n### Customization\n- `--poster-width-inches [width]`: Custom poster width\n- `--poster-height-inches [height]`: Custom poster height\n- `--video-duration [seconds]`: Target video length\n- `--enable-logo-search`: Automatic institution logo discovery\n\n## Output Structure\n\nGenerated outputs are organized by paper and component:\n\n```\noutput/\n paper_name/\n     website/\n        index.html\n        styles.css\n        assets/\n     poster/\n        poster_final.pdf\n        poster_final.png\n        poster_source/\n     video/\n         final_video.mp4\n         slides/\n         audio/\n         subtitles/\n```\n\n## Best Practices\n\n### Input Preparation\n1. **Use LaTeX when possible**: Provides best content extraction and structure\n2. **Organize files properly**: Keep all assets (figures, tables, bibliography) in paper directory\n3. **High-quality figures**: Use vector formats (PDF, SVG) or high-resolution rasters (300+ DPI)\n4. **Clean LaTeX**: Remove compilation artifacts, ensure source compiles successfully\n\n### Model Selection Strategy\n- **GPT-4**: Best for production-quality outputs, conferences, publications\n- **GPT-4.1**: Use when you need latest features or best possible quality\n- **GPT-3.5-turbo**: Use for quick drafts, testing, or simple papers\n\n### Component Priority\nFor tight deadlines, generate in this order:\n1. **Website** (fastest, most versatile, ~15-30 min)\n2. **Poster** (moderate speed, for print deadlines, ~10-20 min)\n3. **Video** (slowest, can be generated later, ~20-60 min)\n\n### Quality Assurance\nBefore finalizing outputs:\n1. **Website**: Test on multiple devices, verify all links work, check figure quality\n2. **Poster**: Print test page, verify text readability from 3-6 feet, check colors\n3. **Video**: Watch entire video, verify audio synchronization, test on different devices\n\n## Resource Requirements\n\n### Processing Time\n- **Website**: 15-30 minutes per paper\n- **Poster**: 10-20 minutes per paper\n- **Video (no talking-head)**: 20-60 minutes per paper\n- **Video (with talking-head)**: 60-120 minutes per paper\n\n### Computational Requirements\n- **CPU**: Multi-core processor for parallel processing\n- **RAM**: 16GB minimum, 32GB recommended for large papers\n- **GPU**: Optional for standard outputs, required for talking-head (NVIDIA A6000 48GB)\n- **Storage**: 1-5GB per paper depending on components and quality settings\n\n### API Costs (Approximate)\n- **Website**: $0.50-2.00 per paper (GPT-4)\n- **Poster**: $0.30-1.00 per paper (GPT-4)\n- **Video**: $1.00-3.00 per paper (GPT-4)\n- **Complete package**: $2.00-6.00 per paper (GPT-4)\n\n## Troubleshooting\n\n### Common Issues\n\n**LaTeX parsing errors**:\n- Ensure LaTeX source compiles successfully: `pdflatex main.tex`\n- Check all referenced files are present\n- Verify no custom packages prevent parsing\n\n**Poor figure quality**:\n- Use vector formats (PDF, SVG, EPS) instead of rasters\n- Ensure raster images are 300+ DPI\n- Check figures render correctly in compiled PDF\n\n**Video generation failures**:\n- Verify sufficient disk space (5GB+ recommended)\n- Check all dependencies installed (LibreOffice, Poppler)\n- Review error logs in output directory\n\n**Poster layout issues**:\n- Verify poster dimensions are reasonable (24\"-72\" range)\n- Check content length (very long papers may need manual curation)\n- Ensure figures have appropriate resolution for poster size\n\n**API errors**:\n- Verify API keys in `.env` file\n- Check API credit balance\n- Ensure no rate limiting (wait and retry)\n\n## Platform-Specific Features\n\n### Social Media Optimization\n\nThe system auto-detects target platforms:\n\n**Twitter/X** (English, numeric folder names):\n```bash\nmkdir -p input/001_twitter/\n# Generates English promotional content\n```\n\n**Xiaohongshu/** (Chinese, alphanumeric folder names):\n```bash\nmkdir -p input/xhs_paper/\n# Generates Chinese promotional content\n```\n\n### Conference-Specific Formatting\n\nSpecify conference requirements:\n- Standard poster sizes (4'3', 5'4', A0, A1)\n- Video abstract length limits (typically 3-5 minutes)\n- Institution branding requirements\n- Color scheme preferences\n\n## Integration and Deployment\n\n### Website Deployment\nDeploy generated websites to:\n- **GitHub Pages**: Free hosting with custom domain\n- **Academic hosting**: University web servers\n- **Personal servers**: AWS, DigitalOcean, etc.\n- **Netlify/Vercel**: Modern hosting with CI/CD\n\n### Poster Printing\nPrint-ready files work with:\n- Professional poster printing services\n- University print shops\n- Online services (e.g., Spoonflower, VistaPrint)\n- Large format printers (if available)\n\n### Video Distribution\nShare videos on:\n- **YouTube**: Public or unlisted for maximum reach\n- **Institutional repositories**: University video platforms\n- **Conference platforms**: Virtual conference systems\n- **Social media**: Twitter, LinkedIn, ResearchGate\n\n## Advanced Usage\n\n### Batch Processing\nProcess multiple papers efficiently:\n```bash\n# Organize papers in batch directory\nfor paper in paper1 paper2 paper3; do\n    python pipeline_all.py \\\n      --input-dir input/$paper \\\n      --output-dir output/$paper \\\n      --model-choice 1 &\ndone\nwait\n```\n\n### Custom Branding\nApply institution or lab branding:\n- Provide logo files in paper directory\n- Specify color schemes in configuration\n- Use custom templates (advanced)\n- Match conference theme requirements\n\n### Multi-Language Support\nGenerate content in different languages:\n- Specify target language in configuration\n- System translates content appropriately\n- Selects appropriate voice for video narration\n- Adapts design conventions to culture\n\n## References and Resources\n\nThis skill includes comprehensive reference documentation:\n\n- **`references/installation.md`**: Complete installation and configuration guide\n- **`references/paper2web.md`**: Detailed Paper2Web documentation with all features\n- **`references/paper2video.md`**: Comprehensive Paper2Video guide including talking-head setup\n- **`references/paper2poster.md`**: Complete Paper2Poster documentation with design templates\n- **`references/usage_examples.md`**: Real-world examples and workflow patterns\n\n**External Resources**:\n- GitHub Repository: https://github.com/YuhangChen1/Paper2All\n- Curated Dataset: Available on Hugging Face (13 research categories)\n- Benchmark Suite: Reference websites and evaluation metrics\n\n## Evaluation and Quality Metrics\n\nThe Paper2All system includes built-in quality assessment:\n\n### Content Quality\n- **Completeness**: Coverage of paper content\n- **Accuracy**: Faithful representation of findings\n- **Clarity**: Accessibility and understandability\n- **Informativeness**: Key information prominence\n\n### Design Quality\n- **Aesthetics**: Visual appeal and professionalism\n- **Layout**: Balance, hierarchy, and organization\n- **Readability**: Text legibility and figure clarity\n- **Consistency**: Uniform styling and branding\n\n### Technical Quality\n- **Performance**: Load times, responsiveness\n- **Compatibility**: Cross-browser, cross-device support\n- **Accessibility**: WCAG compliance, screen reader support\n- **Standards**: Valid HTML/CSS, print-ready PDFs\n\nAll outputs undergo automated quality checks before generation completes.\n",
        "data/k-dense-ai/pathml/SKILL.md": "---\nname: pathml\ndescription: Computational pathology toolkit for analyzing whole-slide images (WSI) and multiparametric imaging data. Use this skill when working with histopathology slides, H&E stained images, multiplex immunofluorescence (CODEX, Vectra), spatial proteomics, nucleus detection/segmentation, tissue graph construction, or training ML models on pathology data. Supports 160+ slide formats including Aperio SVS, NDPI, DICOM, OME-TIFF for digital pathology workflows.\n---\n\n# PathML\n\n## Overview\n\nPathML is a comprehensive Python toolkit for computational pathology workflows, designed to facilitate machine learning and image analysis for whole-slide pathology images. The framework provides modular, composable tools for loading diverse slide formats, preprocessing images, constructing spatial graphs, training deep learning models, and analyzing multiparametric imaging data from technologies like CODEX and multiplex immunofluorescence.\n\n## When to Use This Skill\n\nApply this skill for:\n- Loading and processing whole-slide images (WSI) in various proprietary formats\n- Preprocessing H&E stained tissue images with stain normalization\n- Nucleus detection, segmentation, and classification workflows\n- Building cell and tissue graphs for spatial analysis\n- Training or deploying machine learning models (HoVer-Net, HACTNet) on pathology data\n- Analyzing multiparametric imaging (CODEX, Vectra, MERFISH) for spatial proteomics\n- Quantifying marker expression from multiplex immunofluorescence\n- Managing large-scale pathology datasets with HDF5 storage\n- Tile-based analysis and stitching operations\n\n## Core Capabilities\n\nPathML provides six major capability areas documented in detail within reference files:\n\n### 1. Image Loading & Formats\n\nLoad whole-slide images from 160+ proprietary formats including Aperio SVS, Hamamatsu NDPI, Leica SCN, Zeiss ZVI, DICOM, and OME-TIFF. PathML automatically handles vendor-specific formats and provides unified interfaces for accessing image pyramids, metadata, and regions of interest.\n\n**See:** `references/image_loading.md` for supported formats, loading strategies, and working with different slide types.\n\n### 2. Preprocessing Pipelines\n\nBuild modular preprocessing pipelines by composing transforms for image manipulation, quality control, stain normalization, tissue detection, and mask operations. PathML's Pipeline architecture enables reproducible, scalable preprocessing across large datasets.\n\n**Key transforms:**\n- `StainNormalizationHE` - Macenko/Vahadane stain normalization\n- `TissueDetectionHE`, `NucleusDetectionHE` - Tissue/nucleus segmentation\n- `MedianBlur`, `GaussianBlur` - Noise reduction\n- `LabelArtifactTileHE` - Quality control for artifacts\n\n**See:** `references/preprocessing.md` for complete transform catalog, pipeline construction, and preprocessing workflows.\n\n### 3. Graph Construction\n\nConstruct spatial graphs representing cellular and tissue-level relationships. Extract features from segmented objects to create graph-based representations suitable for graph neural networks and spatial analysis.\n\n**See:** `references/graphs.md` for graph construction methods, feature extraction, and spatial analysis workflows.\n\n### 4. Machine Learning\n\nTrain and deploy deep learning models for nucleus detection, segmentation, and classification. PathML integrates PyTorch with pre-built models (HoVer-Net, HACTNet), custom DataLoaders, and ONNX support for inference.\n\n**Key models:**\n- **HoVer-Net** - Simultaneous nucleus segmentation and classification\n- **HACTNet** - Hierarchical cell-type classification\n\n**See:** `references/machine_learning.md` for model training, evaluation, inference workflows, and working with public datasets.\n\n### 5. Multiparametric Imaging\n\nAnalyze spatial proteomics and gene expression data from CODEX, Vectra, MERFISH, and other multiplex imaging platforms. PathML provides specialized slide classes and transforms for processing multiparametric data, cell segmentation with Mesmer, and quantification workflows.\n\n**See:** `references/multiparametric.md` for CODEX/Vectra workflows, cell segmentation, marker quantification, and integration with AnnData.\n\n### 6. Data Management\n\nEfficiently store and manage large pathology datasets using HDF5 format. PathML handles tiles, masks, metadata, and extracted features in unified storage structures optimized for machine learning workflows.\n\n**See:** `references/data_management.md` for HDF5 integration, tile management, dataset organization, and batch processing strategies.\n\n## Quick Start\n\n### Installation\n\n```bash\n# Install PathML\nuv pip install pathml\n\n# With optional dependencies for all features\nuv pip install pathml[all]\n```\n\n### Basic Workflow Example\n\n```python\nfrom pathml.core import SlideData\nfrom pathml.preprocessing import Pipeline, StainNormalizationHE, TissueDetectionHE\n\n# Load a whole-slide image\nwsi = SlideData.from_slide(\"path/to/slide.svs\")\n\n# Create preprocessing pipeline\npipeline = Pipeline([\n    TissueDetectionHE(),\n    StainNormalizationHE(target='normalize', stain_estimation_method='macenko')\n])\n\n# Run pipeline\npipeline.run(wsi)\n\n# Access processed tiles\nfor tile in wsi.tiles:\n    processed_image = tile.image\n    tissue_mask = tile.masks['tissue']\n```\n\n### Common Workflows\n\n**H&E Image Analysis:**\n1. Load WSI with appropriate slide class\n2. Apply tissue detection and stain normalization\n3. Perform nucleus detection or train segmentation models\n4. Extract features and build spatial graphs\n5. Conduct downstream analysis\n\n**Multiparametric Imaging (CODEX):**\n1. Load CODEX slide with `CODEXSlide`\n2. Collapse multi-run channel data\n3. Segment cells using Mesmer model\n4. Quantify marker expression\n5. Export to AnnData for single-cell analysis\n\n**Training ML Models:**\n1. Prepare dataset with public pathology data\n2. Create PyTorch DataLoader with PathML datasets\n3. Train HoVer-Net or custom models\n4. Evaluate on held-out test sets\n5. Deploy with ONNX for inference\n\n## References to Detailed Documentation\n\nWhen working on specific tasks, refer to the appropriate reference file for comprehensive information:\n\n- **Loading images:** `references/image_loading.md`\n- **Preprocessing workflows:** `references/preprocessing.md`\n- **Spatial analysis:** `references/graphs.md`\n- **Model training:** `references/machine_learning.md`\n- **CODEX/multiplex IF:** `references/multiparametric.md`\n- **Data storage:** `references/data_management.md`\n\n## Resources\n\nThis skill includes comprehensive reference documentation organized by capability area. Each reference file contains detailed API information, workflow examples, best practices, and troubleshooting guidance for specific PathML functionality.\n\n### references/\n\nDocumentation files providing in-depth coverage of PathML capabilities:\n\n- `image_loading.md` - Whole-slide image formats, loading strategies, slide classes\n- `preprocessing.md` - Complete transform catalog, pipeline construction, preprocessing workflows\n- `graphs.md` - Graph construction methods, feature extraction, spatial analysis\n- `machine_learning.md` - Model architectures, training workflows, evaluation, inference\n- `multiparametric.md` - CODEX, Vectra, multiplex IF analysis, cell segmentation, quantification\n- `data_management.md` - HDF5 storage, tile management, batch processing, dataset organization\n\nLoad these references as needed when working on specific computational pathology tasks.\n",
        "data/k-dense-ai/pdb-database/SKILL.md": "---\nname: pdb-database\ndescription: \"Access RCSB PDB for 3D protein/nucleic acid structures. Search by text/sequence/structure, download coordinates (PDB/mmCIF), retrieve metadata, for structural biology and drug discovery.\"\n---\n\n# PDB Database\n\n## Overview\n\nRCSB PDB is the worldwide repository for 3D structural data of biological macromolecules. Search for structures, retrieve coordinates and metadata, perform sequence and structure similarity searches across 200,000+ experimentally determined structures and computed models.\n\n## When to Use This Skill\n\nThis skill should be used when:\n- Searching for protein or nucleic acid 3D structures by text, sequence, or structural similarity\n- Downloading coordinate files in PDB, mmCIF, or BinaryCIF formats\n- Retrieving structural metadata, experimental methods, or quality metrics\n- Performing batch operations across multiple structures\n- Integrating PDB data into computational workflows for drug discovery, protein engineering, or structural biology research\n\n## Core Capabilities\n\n### 1. Searching for Structures\n\nFind PDB entries using various search criteria:\n\n**Text Search:** Search by protein name, keywords, or descriptions\n```python\nfrom rcsbapi.search import TextQuery\nquery = TextQuery(\"hemoglobin\")\nresults = list(query())\nprint(f\"Found {len(results)} structures\")\n```\n\n**Attribute Search:** Query specific properties (organism, resolution, method, etc.)\n```python\nfrom rcsbapi.search import AttributeQuery\nfrom rcsbapi.search.attrs import rcsb_entity_source_organism\n\n# Find human protein structures\nquery = AttributeQuery(\n    attribute=rcsb_entity_source_organism.scientific_name,\n    operator=\"exact_match\",\n    value=\"Homo sapiens\"\n)\nresults = list(query())\n```\n\n**Sequence Similarity:** Find structures similar to a given sequence\n```python\nfrom rcsbapi.search import SequenceQuery\n\nquery = SequenceQuery(\n    value=\"MTEYKLVVVGAGGVGKSALTIQLIQNHFVDEYDPTIEDSYRKQVVIDGETCLLDILDTAGQEEYSAMRDQYMRTGEGFLCVFAINNTKSFEDIHHYREQIKRVKDSEDVPMVLVGNKCDLPSRTVDTKQAQDLARSYGIPFIETSAKTRQGVDDAFYTLVREIRKHKEKMSKDGKKKKKKSKTKCVIM\",\n    evalue_cutoff=0.1,\n    identity_cutoff=0.9\n)\nresults = list(query())\n```\n\n**Structure Similarity:** Find structures with similar 3D geometry\n```python\nfrom rcsbapi.search import StructSimilarityQuery\n\nquery = StructSimilarityQuery(\n    structure_search_type=\"entry\",\n    entry_id=\"4HHB\"  # Hemoglobin\n)\nresults = list(query())\n```\n\n**Combining Queries:** Use logical operators to build complex searches\n```python\nfrom rcsbapi.search import TextQuery, AttributeQuery\nfrom rcsbapi.search.attrs import rcsb_entry_info\n\n# High-resolution human proteins\nquery1 = AttributeQuery(\n    attribute=rcsb_entity_source_organism.scientific_name,\n    operator=\"exact_match\",\n    value=\"Homo sapiens\"\n)\nquery2 = AttributeQuery(\n    attribute=rcsb_entry_info.resolution_combined,\n    operator=\"less\",\n    value=2.0\n)\ncombined_query = query1 & query2  # AND operation\nresults = list(combined_query())\n```\n\n### 2. Retrieving Structure Data\n\nAccess detailed information about specific PDB entries:\n\n**Basic Entry Information:**\n```python\nfrom rcsbapi.data import Schema, fetch\n\n# Get entry-level data\nentry_data = fetch(\"4HHB\", schema=Schema.ENTRY)\nprint(entry_data[\"struct\"][\"title\"])\nprint(entry_data[\"exptl\"][0][\"method\"])\n```\n\n**Polymer Entity Information:**\n```python\n# Get protein/nucleic acid information\nentity_data = fetch(\"4HHB_1\", schema=Schema.POLYMER_ENTITY)\nprint(entity_data[\"entity_poly\"][\"pdbx_seq_one_letter_code\"])\n```\n\n**Using GraphQL for Flexible Queries:**\n```python\nfrom rcsbapi.data import fetch\n\n# Custom GraphQL query\nquery = \"\"\"\n{\n  entry(entry_id: \"4HHB\") {\n    struct {\n      title\n    }\n    exptl {\n      method\n    }\n    rcsb_entry_info {\n      resolution_combined\n      deposited_atom_count\n    }\n  }\n}\n\"\"\"\ndata = fetch(query_type=\"graphql\", query=query)\n```\n\n### 3. Downloading Structure Files\n\nRetrieve coordinate files in various formats:\n\n**Download Methods:**\n- **PDB format** (legacy text format): `https://files.rcsb.org/download/{PDB_ID}.pdb`\n- **mmCIF format** (modern standard): `https://files.rcsb.org/download/{PDB_ID}.cif`\n- **BinaryCIF** (compressed binary): Use ModelServer API for efficient access\n- **Biological assembly**: `https://files.rcsb.org/download/{PDB_ID}.pdb1` (for assembly 1)\n\n**Example Download:**\n```python\nimport requests\n\npdb_id = \"4HHB\"\n\n# Download PDB format\npdb_url = f\"https://files.rcsb.org/download/{pdb_id}.pdb\"\nresponse = requests.get(pdb_url)\nwith open(f\"{pdb_id}.pdb\", \"w\") as f:\n    f.write(response.text)\n\n# Download mmCIF format\ncif_url = f\"https://files.rcsb.org/download/{pdb_id}.cif\"\nresponse = requests.get(cif_url)\nwith open(f\"{pdb_id}.cif\", \"w\") as f:\n    f.write(response.text)\n```\n\n### 4. Working with Structure Data\n\nCommon operations with retrieved structures:\n\n**Parse and Analyze Coordinates:**\nUse BioPython or other structural biology libraries to work with downloaded files:\n```python\nfrom Bio.PDB import PDBParser\n\nparser = PDBParser()\nstructure = parser.get_structure(\"protein\", \"4HHB.pdb\")\n\n# Iterate through atoms\nfor model in structure:\n    for chain in model:\n        for residue in chain:\n            for atom in residue:\n                print(atom.get_coord())\n```\n\n**Extract Metadata:**\n```python\nfrom rcsbapi.data import fetch, Schema\n\n# Get experimental details\ndata = fetch(\"4HHB\", schema=Schema.ENTRY)\n\nresolution = data.get(\"rcsb_entry_info\", {}).get(\"resolution_combined\")\nmethod = data.get(\"exptl\", [{}])[0].get(\"method\")\ndeposition_date = data.get(\"rcsb_accession_info\", {}).get(\"deposit_date\")\n\nprint(f\"Resolution: {resolution} \")\nprint(f\"Method: {method}\")\nprint(f\"Deposited: {deposition_date}\")\n```\n\n### 5. Batch Operations\n\nProcess multiple structures efficiently:\n\n```python\nfrom rcsbapi.data import fetch, Schema\n\npdb_ids = [\"4HHB\", \"1MBN\", \"1GZX\"]  # Hemoglobin, myoglobin, etc.\n\nresults = {}\nfor pdb_id in pdb_ids:\n    try:\n        data = fetch(pdb_id, schema=Schema.ENTRY)\n        results[pdb_id] = {\n            \"title\": data[\"struct\"][\"title\"],\n            \"resolution\": data.get(\"rcsb_entry_info\", {}).get(\"resolution_combined\"),\n            \"organism\": data.get(\"rcsb_entity_source_organism\", [{}])[0].get(\"scientific_name\")\n        }\n    except Exception as e:\n        print(f\"Error fetching {pdb_id}: {e}\")\n\n# Display results\nfor pdb_id, info in results.items():\n    print(f\"\\n{pdb_id}: {info['title']}\")\n    print(f\"  Resolution: {info['resolution']} \")\n    print(f\"  Organism: {info['organism']}\")\n```\n\n## Python Package Installation\n\nInstall the official RCSB PDB Python API client:\n\n```bash\n# Current recommended package\nuv pip install rcsb-api\n\n# For legacy code (deprecated, use rcsb-api instead)\nuv pip install rcsbsearchapi\n```\n\nThe `rcsb-api` package provides unified access to both Search and Data APIs through the `rcsbapi.search` and `rcsbapi.data` modules.\n\n## Common Use Cases\n\n### Drug Discovery\n- Search for structures of drug targets\n- Analyze ligand binding sites\n- Compare protein-ligand complexes\n- Identify similar binding pockets\n\n### Protein Engineering\n- Find homologous structures for modeling\n- Analyze sequence-structure relationships\n- Compare mutant structures\n- Study protein stability and dynamics\n\n### Structural Biology Research\n- Download structures for computational analysis\n- Build structure-based alignments\n- Analyze structural features (secondary structure, domains)\n- Compare experimental methods and quality metrics\n\n### Education and Visualization\n- Retrieve structures for teaching\n- Generate molecular visualizations\n- Explore structure-function relationships\n- Study evolutionary conservation\n\n## Key Concepts\n\n**PDB ID:** Unique 4-character identifier (e.g., \"4HHB\") for each structure entry. AlphaFold and ModelArchive entries start with \"AF_\" or \"MA_\" prefixes.\n\n**mmCIF/PDBx:** Modern file format that uses key-value structure, replacing legacy PDB format for large structures.\n\n**Biological Assembly:** The functional form of a macromolecule, which may contain multiple copies of chains from the asymmetric unit.\n\n**Resolution:** Measure of detail in crystallographic structures (lower values = higher detail). Typical range: 1.5-3.5  for high-quality structures.\n\n**Entity:** A unique molecular component in a structure (protein chain, DNA, ligand, etc.).\n\n## Resources\n\nThis skill includes reference documentation in the `references/` directory:\n\n### references/api_reference.md\nComprehensive API documentation covering:\n- Detailed API endpoint specifications\n- Advanced query patterns and examples\n- Data schema reference\n- Rate limiting and best practices\n- Troubleshooting common issues\n\nUse this reference when you need in-depth information about API capabilities, complex query construction, or detailed data schema information.\n\n## Additional Resources\n\n- **RCSB PDB Website:** https://www.rcsb.org\n- **PDB-101 Educational Portal:** https://pdb101.rcsb.org\n- **API Documentation:** https://www.rcsb.org/docs/programmatic-access/web-apis-overview\n- **Python Package Docs:** https://rcsbapi.readthedocs.io/\n- **Data API Documentation:** https://data.rcsb.org/\n- **GitHub Repository:** https://github.com/rcsb/py-rcsb-api\n",
        "data/k-dense-ai/peer-review/SKILL.md": "---\nname: peer-review\ndescription: \"Systematic peer review toolkit. Evaluate methodology, statistics, design, reproducibility, ethics, figure integrity, reporting standards, for manuscript and grant review across disciplines.\"\n---\n\n# Scientific Critical Evaluation and Peer Review\n\n## Overview\n\nPeer review is a systematic process for evaluating scientific manuscripts. Assess methodology, statistics, design, reproducibility, ethics, and reporting standards. Apply this skill for manuscript and grant review across disciplines with constructive, rigorous evaluation.\n\n## When to Use This Skill\n\nThis skill should be used when:\n- Conducting peer review of scientific manuscripts for journals\n- Evaluating grant proposals and research applications\n- Assessing methodology and experimental design rigor\n- Reviewing statistical analyses and reporting standards\n- Evaluating reproducibility and data availability\n- Checking compliance with reporting guidelines (CONSORT, STROBE, PRISMA)\n- Providing constructive feedback on scientific writing\n\n## Peer Review Workflow\n\nConduct peer review systematically through the following stages, adapting depth and focus based on the manuscript type and discipline.\n\n### Stage 1: Initial Assessment\n\nBegin with a high-level evaluation to determine the manuscript's scope, novelty, and overall quality.\n\n**Key Questions:**\n- What is the central research question or hypothesis?\n- What are the main findings and conclusions?\n- Is the work scientifically sound and significant?\n- Is the work appropriate for the intended venue?\n- Are there any immediate major flaws that would preclude publication?\n\n**Output:** Brief summary (2-3 sentences) capturing the manuscript's essence and initial impression.\n\n### Stage 2: Detailed Section-by-Section Review\n\nConduct a thorough evaluation of each manuscript section, documenting specific concerns and strengths.\n\n#### Abstract and Title\n- **Accuracy:** Does the abstract accurately reflect the study's content and conclusions?\n- **Clarity:** Is the title specific, accurate, and informative?\n- **Completeness:** Are key findings and methods summarized appropriately?\n- **Accessibility:** Is the abstract comprehensible to a broad scientific audience?\n\n#### Introduction\n- **Context:** Is the background information adequate and current?\n- **Rationale:** Is the research question clearly motivated and justified?\n- **Novelty:** Is the work's originality and significance clearly articulated?\n- **Literature:** Are relevant prior studies appropriately cited?\n- **Objectives:** Are research aims/hypotheses clearly stated?\n\n#### Methods\n- **Reproducibility:** Can another researcher replicate the study from the description provided?\n- **Rigor:** Are the methods appropriate for addressing the research questions?\n- **Detail:** Are protocols, reagents, equipment, and parameters sufficiently described?\n- **Ethics:** Are ethical approvals, consent, and data handling properly documented?\n- **Statistics:** Are statistical methods appropriate, clearly described, and justified?\n- **Validation:** Are controls, replicates, and validation approaches adequate?\n\n**Critical elements to verify:**\n- Sample sizes and power calculations\n- Randomization and blinding procedures\n- Inclusion/exclusion criteria\n- Data collection protocols\n- Computational methods and software versions\n- Statistical tests and correction for multiple comparisons\n\n#### Results\n- **Presentation:** Are results presented logically and clearly?\n- **Figures/Tables:** Are visualizations appropriate, clear, and properly labeled?\n- **Statistics:** Are statistical results properly reported (effect sizes, confidence intervals, p-values)?\n- **Objectivity:** Are results presented without over-interpretation?\n- **Completeness:** Are all relevant results included, including negative results?\n- **Reproducibility:** Are raw data or summary statistics provided?\n\n**Common issues to identify:**\n- Selective reporting of results\n- Inappropriate statistical tests\n- Missing error bars or measures of variability\n- Over-fitting or circular analysis\n- Batch effects or confounding variables\n- Missing controls or validation experiments\n\n#### Discussion\n- **Interpretation:** Are conclusions supported by the data?\n- **Limitations:** Are study limitations acknowledged and discussed?\n- **Context:** Are findings placed appropriately within existing literature?\n- **Speculation:** Is speculation clearly distinguished from data-supported conclusions?\n- **Significance:** Are implications and importance clearly articulated?\n- **Future directions:** Are next steps or unanswered questions discussed?\n\n**Red flags:**\n- Overstated conclusions\n- Ignoring contradictory evidence\n- Causal claims from correlational data\n- Inadequate discussion of limitations\n- Mechanistic claims without mechanistic evidence\n\n#### References\n- **Completeness:** Are key relevant papers cited?\n- **Currency:** Are recent important studies included?\n- **Balance:** Are contrary viewpoints appropriately cited?\n- **Accuracy:** Are citations accurate and appropriate?\n- **Self-citation:** Is there excessive or inappropriate self-citation?\n\n### Stage 3: Methodological and Statistical Rigor\n\nEvaluate the technical quality and rigor of the research with particular attention to common pitfalls.\n\n**Statistical Assessment:**\n- Are statistical assumptions met (normality, independence, homoscedasticity)?\n- Are effect sizes reported alongside p-values?\n- Is multiple testing correction applied appropriately?\n- Are confidence intervals provided?\n- Is sample size justified with power analysis?\n- Are parametric vs. non-parametric tests chosen appropriately?\n- Are missing data handled properly?\n- Are exploratory vs. confirmatory analyses distinguished?\n\n**Experimental Design:**\n- Are controls appropriate and adequate?\n- Is replication sufficient (biological and technical)?\n- Are potential confounders identified and controlled?\n- Is randomization properly implemented?\n- Are blinding procedures adequate?\n- Is the experimental design optimal for the research question?\n\n**Computational/Bioinformatics:**\n- Are computational methods clearly described and justified?\n- Are software versions and parameters documented?\n- Is code made available for reproducibility?\n- Are algorithms and models validated appropriately?\n- Are assumptions of computational methods met?\n- Is batch correction applied appropriately?\n\n### Stage 4: Reproducibility and Transparency\n\nAssess whether the research meets modern standards for reproducibility and open science.\n\n**Data Availability:**\n- Are raw data deposited in appropriate repositories?\n- Are accession numbers provided for public databases?\n- Are data sharing restrictions justified (e.g., patient privacy)?\n- Are data formats standard and accessible?\n\n**Code and Materials:**\n- Is analysis code made available (GitHub, Zenodo, etc.)?\n- Are unique materials available or described sufficiently for recreation?\n- Are protocols detailed in sufficient depth?\n\n**Reporting Standards:**\n- Does the manuscript follow discipline-specific reporting guidelines (CONSORT, PRISMA, ARRIVE, MIAME, MINSEQE, etc.)?\n- See `references/reporting_standards.md` for common guidelines\n- Are all elements of the appropriate checklist addressed?\n\n### Stage 5: Figure and Data Presentation\n\nEvaluate the quality, clarity, and integrity of data visualization.\n\n**Quality Checks:**\n- Are figures high resolution and clearly labeled?\n- Are axes properly labeled with units?\n- Are error bars defined (SD, SEM, CI)?\n- Are statistical significance indicators explained?\n- Are color schemes appropriate and accessible (colorblind-friendly)?\n- Are scale bars included for images?\n- Is data visualization appropriate for the data type?\n\n**Integrity Checks:**\n- Are there signs of image manipulation (duplications, splicing)?\n- Are Western blots and gels appropriately presented?\n- Are representative images truly representative?\n- Are all conditions shown (no selective presentation)?\n\n**Clarity:**\n- Can figures stand alone with their legends?\n- Is the message of each figure immediately clear?\n- Are there redundant figures or panels?\n- Would data be better presented as tables or figures?\n\n### Stage 6: Ethical Considerations\n\nVerify that the research meets ethical standards and guidelines.\n\n**Human Subjects:**\n- Is IRB/ethics approval documented?\n- Is informed consent described?\n- Are vulnerable populations appropriately protected?\n- Is patient privacy adequately protected?\n- Are potential conflicts of interest disclosed?\n\n**Animal Research:**\n- Is IACUC or equivalent approval documented?\n- Are procedures humane and justified?\n- Are the 3Rs (replacement, reduction, refinement) considered?\n- Are euthanasia methods appropriate?\n\n**Research Integrity:**\n- Are there concerns about data fabrication or falsification?\n- Is authorship appropriate and justified?\n- Are competing interests disclosed?\n- Is funding source disclosed?\n- Are there concerns about plagiarism or duplicate publication?\n\n### Stage 7: Writing Quality and Clarity\n\nAssess the manuscript's clarity, organization, and accessibility.\n\n**Structure and Organization:**\n- Is the manuscript logically organized?\n- Do sections flow coherently?\n- Are transitions between ideas clear?\n- Is the narrative compelling and clear?\n\n**Writing Quality:**\n- Is the language clear, precise, and concise?\n- Are jargon and acronyms minimized and defined?\n- Is grammar and spelling correct?\n- Are sentences unnecessarily complex?\n- Is the passive voice overused?\n\n**Accessibility:**\n- Can a non-specialist understand the main findings?\n- Are technical terms explained?\n- Is the significance clear to a broad audience?\n\n## Structuring Peer Review Reports\n\nOrganize feedback in a hierarchical structure that prioritizes issues and provides actionable guidance.\n\n### Summary Statement\n\nProvide a concise overall assessment (1-2 paragraphs):\n- Brief synopsis of the research\n- Overall recommendation (accept, minor revisions, major revisions, reject)\n- Key strengths (2-3 bullet points)\n- Key weaknesses (2-3 bullet points)\n- Bottom-line assessment of significance and soundness\n\n### Major Comments\n\nList critical issues that significantly impact the manuscript's validity, interpretability, or significance. Number these sequentially for easy reference.\n\n**Major comments typically include:**\n- Fundamental methodological flaws\n- Inappropriate statistical analyses\n- Unsupported or overstated conclusions\n- Missing critical controls or experiments\n- Serious reproducibility concerns\n- Major gaps in literature coverage\n- Ethical concerns\n\n**For each major comment:**\n1. Clearly state the issue\n2. Explain why it's problematic\n3. Suggest specific solutions or additional experiments\n4. Indicate if addressing it is essential for publication\n\n### Minor Comments\n\nList less critical issues that would improve clarity, completeness, or presentation. Number these sequentially.\n\n**Minor comments typically include:**\n- Unclear figure labels or legends\n- Missing methodological details\n- Typographical or grammatical errors\n- Suggestions for improved data presentation\n- Minor statistical reporting issues\n- Supplementary analyses that would strengthen conclusions\n- Requests for clarification\n\n**For each minor comment:**\n1. Identify the specific location (section, paragraph, figure)\n2. State the issue clearly\n3. Suggest how to address it\n\n### Specific Line-by-Line Comments (Optional)\n\nFor manuscripts requiring detailed feedback, provide section-specific or line-by-line comments:\n- Reference specific page/line numbers or sections\n- Note factual errors, unclear statements, or missing citations\n- Suggest specific edits for clarity\n\n### Questions for Authors\n\nList specific questions that need clarification:\n- Methodological details that are unclear\n- Seemingly contradictory results\n- Missing information needed to evaluate the work\n- Requests for additional data or analyses\n\n## Tone and Approach\n\nMaintain a constructive, professional, and collegial tone throughout the review.\n\n**Best Practices:**\n- **Be constructive:** Frame criticism as opportunities for improvement\n- **Be specific:** Provide concrete examples and actionable suggestions\n- **Be balanced:** Acknowledge strengths as well as weaknesses\n- **Be respectful:** Remember that authors have invested significant effort\n- **Be objective:** Focus on the science, not the scientists\n- **Be thorough:** Don't overlook issues, but prioritize appropriately\n- **Be clear:** Avoid ambiguous or vague criticism\n\n**Avoid:**\n- Personal attacks or dismissive language\n- Sarcasm or condescension\n- Vague criticism without specific examples\n- Requesting unnecessary experiments beyond the scope\n- Demanding adherence to personal preferences vs. best practices\n- Revealing your identity if reviewing is double-blind\n\n## Special Considerations by Manuscript Type\n\n### Original Research Articles\n- Emphasize rigor, reproducibility, and novelty\n- Assess significance and impact\n- Verify that conclusions are data-driven\n- Check for complete methods and appropriate controls\n\n### Reviews and Meta-Analyses\n- Evaluate comprehensiveness of literature coverage\n- Assess search strategy and inclusion/exclusion criteria\n- Verify systematic approach and lack of bias\n- Check for critical analysis vs. mere summarization\n- For meta-analyses, evaluate statistical approach and heterogeneity\n\n### Methods Papers\n- Emphasize validation and comparison to existing methods\n- Assess reproducibility and availability of protocols/code\n- Evaluate improvements over existing approaches\n- Check for sufficient detail for implementation\n\n### Short Reports/Letters\n- Adapt expectations for brevity\n- Ensure core findings are still rigorous and significant\n- Verify that format is appropriate for findings\n\n### Preprints\n- Recognize that these have not undergone formal peer review\n- May be less polished than journal submissions\n- Still apply rigorous standards for scientific validity\n- Consider providing constructive feedback to help authors improve before journal submission\n\n## Resources\n\nThis skill includes reference materials to support comprehensive peer review:\n\n### references/reporting_standards.md\nGuidelines for major reporting standards across disciplines (CONSORT, PRISMA, ARRIVE, MIAME, STROBE, etc.) to evaluate completeness of methods and results reporting.\n\n### references/common_issues.md\nCatalog of frequent methodological and statistical issues encountered in peer review, with guidance on identifying and addressing them.\n\n## Final Checklist\n\nBefore finalizing the review, verify:\n\n- [ ] Summary statement clearly conveys overall assessment\n- [ ] Major concerns are clearly identified and justified\n- [ ] Suggested revisions are specific and actionable\n- [ ] Minor issues are noted but properly categorized\n- [ ] Statistical methods have been evaluated\n- [ ] Reproducibility and data availability assessed\n- [ ] Ethical considerations verified\n- [ ] Figures and tables evaluated for quality and integrity\n- [ ] Writing quality assessed\n- [ ] Tone is constructive and professional throughout\n- [ ] Review is thorough but proportionate to manuscript scope\n- [ ] Recommendation is consistent with identified issues\n",
        "data/k-dense-ai/perplexity-search/SKILL.md": "---\nname: perplexity-search\ndescription: Perform AI-powered web searches with real-time information using Perplexity models via LiteLLM and OpenRouter. This skill should be used when conducting web searches for current information, finding recent scientific literature, getting grounded answers with source citations, or accessing information beyond the model's knowledge cutoff. Provides access to multiple Perplexity models including Sonar Pro, Sonar Pro Search (advanced agentic search), and Sonar Reasoning Pro through a single OpenRouter API key.\n---\n\n# Perplexity Search\n\n## Overview\n\nPerform AI-powered web searches using Perplexity models through LiteLLM and OpenRouter. Perplexity provides real-time, web-grounded answers with source citations, making it ideal for finding current information, recent scientific literature, and facts beyond the model's training data cutoff.\n\nThis skill provides access to all Perplexity models through OpenRouter, requiring only a single API key (no separate Perplexity account needed).\n\n## When to Use This Skill\n\nUse this skill when:\n- Searching for current information or recent developments (2024 and beyond)\n- Finding latest scientific publications and research\n- Getting real-time answers grounded in web sources\n- Verifying facts with source citations\n- Conducting literature searches across multiple domains\n- Accessing information beyond the model's knowledge cutoff\n- Performing domain-specific research (biomedical, technical, clinical)\n- Comparing current approaches or technologies\n\n**Do not use** for:\n- Simple calculations or logic problems (use directly)\n- Tasks requiring code execution (use standard tools)\n- Questions well within the model's training data (unless verification needed)\n\n## Quick Start\n\n### Setup (One-time)\n\n1. **Get OpenRouter API key**:\n   - Visit https://openrouter.ai/keys\n   - Create account and generate API key\n   - Add credits to account (minimum $5 recommended)\n\n2. **Configure environment**:\n   ```bash\n   # Set API key\n   export OPENROUTER_API_KEY='sk-or-v1-your-key-here'\n\n   # Or use setup script\n   python scripts/setup_env.py --api-key sk-or-v1-your-key-here\n   ```\n\n3. **Install dependencies**:\n   ```bash\n   uv pip install litellm\n   ```\n\n4. **Verify setup**:\n   ```bash\n   python scripts/perplexity_search.py --check-setup\n   ```\n\nSee `references/openrouter_setup.md` for detailed setup instructions, troubleshooting, and security best practices.\n\n### Basic Usage\n\n**Simple search:**\n```bash\npython scripts/perplexity_search.py \"What are the latest developments in CRISPR gene editing?\"\n```\n\n**Save results:**\n```bash\npython scripts/perplexity_search.py \"Recent CAR-T therapy clinical trials\" --output results.json\n```\n\n**Use specific model:**\n```bash\npython scripts/perplexity_search.py \"Compare mRNA and viral vector vaccines\" --model sonar-pro-search\n```\n\n**Verbose output:**\n```bash\npython scripts/perplexity_search.py \"Quantum computing for drug discovery\" --verbose\n```\n\n## Available Models\n\nAccess models via `--model` parameter:\n\n- **sonar-pro** (default): General-purpose search, best balance of cost and quality\n- **sonar-pro-search**: Most advanced agentic search with multi-step reasoning\n- **sonar**: Basic model, most cost-effective for simple queries\n- **sonar-reasoning-pro**: Advanced reasoning with step-by-step analysis\n- **sonar-reasoning**: Basic reasoning capabilities\n\n**Model selection guide:**\n- Default queries  `sonar-pro`\n- Complex multi-step analysis  `sonar-pro-search`\n- Explicit reasoning needed  `sonar-reasoning-pro`\n- Simple fact lookups  `sonar`\n- Cost-sensitive bulk queries  `sonar`\n\nSee `references/model_comparison.md` for detailed comparison, use cases, pricing, and performance characteristics.\n\n## Crafting Effective Queries\n\n### Be Specific and Detailed\n\n**Good examples:**\n- \"What are the latest clinical trial results for CAR-T cell therapy in treating B-cell lymphoma published in 2024?\"\n- \"Compare the efficacy and safety profiles of mRNA vaccines versus viral vector vaccines for COVID-19\"\n- \"Explain AlphaFold3 improvements over AlphaFold2 with specific accuracy metrics from 2023-2024 research\"\n\n**Bad examples:**\n- \"Tell me about cancer treatment\" (too broad)\n- \"CRISPR\" (too vague)\n- \"vaccines\" (lacks specificity)\n\n### Include Time Constraints\n\nPerplexity searches real-time web data:\n- \"What papers were published in Nature Medicine in 2024 about long COVID?\"\n- \"What are the latest developments (past 6 months) in large language model efficiency?\"\n- \"What was announced at NeurIPS 2023 regarding AI safety?\"\n\n### Specify Domain and Sources\n\nFor high-quality results, mention source preferences:\n- \"According to peer-reviewed publications in high-impact journals...\"\n- \"Based on FDA-approved treatments...\"\n- \"From clinical trial registries like clinicaltrials.gov...\"\n\n### Structure Complex Queries\n\nBreak complex questions into clear components:\n1. **Topic**: Main subject\n2. **Scope**: Specific aspect of interest\n3. **Context**: Time frame, domain, constraints\n4. **Output**: Desired format or type of answer\n\n**Example:**\n\"What improvements does AlphaFold3 offer over AlphaFold2 for protein structure prediction, according to research published between 2023 and 2024? Include specific accuracy metrics and benchmarks.\"\n\nSee `references/search_strategies.md` for comprehensive guidance on query design, domain-specific patterns, and advanced techniques.\n\n## Common Use Cases\n\n### Scientific Literature Search\n\n```bash\npython scripts/perplexity_search.py \\\n  \"What does recent research (2023-2024) say about the role of gut microbiome in Parkinson's disease? Focus on peer-reviewed studies and include specific bacterial species identified.\" \\\n  --model sonar-pro\n```\n\n### Technical Documentation\n\n```bash\npython scripts/perplexity_search.py \\\n  \"How to implement real-time data streaming from Kafka to PostgreSQL using Python? Include considerations for handling backpressure and ensuring exactly-once semantics.\" \\\n  --model sonar-reasoning-pro\n```\n\n### Comparative Analysis\n\n```bash\npython scripts/perplexity_search.py \\\n  \"Compare PyTorch versus TensorFlow for implementing transformer models in terms of ease of use, performance, and ecosystem support. Include benchmarks from recent studies.\" \\\n  --model sonar-pro-search\n```\n\n### Clinical Research\n\n```bash\npython scripts/perplexity_search.py \\\n  \"What is the evidence for intermittent fasting in managing type 2 diabetes in adults? Focus on randomized controlled trials and report HbA1c changes and weight loss outcomes.\" \\\n  --model sonar-pro\n```\n\n### Trend Analysis\n\n```bash\npython scripts/perplexity_search.py \\\n  \"What are the key trends in single-cell RNA sequencing technology over the past 5 years? Highlight improvements in throughput, cost, and resolution, with specific examples.\" \\\n  --model sonar-pro\n```\n\n## Working with Results\n\n### Programmatic Access\n\nUse `perplexity_search.py` as a module:\n\n```python\nfrom scripts.perplexity_search import search_with_perplexity\n\nresult = search_with_perplexity(\n    query=\"What are the latest CRISPR developments?\",\n    model=\"openrouter/perplexity/sonar-pro\",\n    max_tokens=4000,\n    temperature=0.2,\n    verbose=False\n)\n\nif result[\"success\"]:\n    print(result[\"answer\"])\n    print(f\"Tokens used: {result['usage']['total_tokens']}\")\nelse:\n    print(f\"Error: {result['error']}\")\n```\n\n### Save and Process Results\n\n```bash\n# Save to JSON\npython scripts/perplexity_search.py \"query\" --output results.json\n\n# Process with jq\ncat results.json | jq '.answer'\ncat results.json | jq '.usage'\n```\n\n### Batch Processing\n\nCreate a script for multiple queries:\n\n```bash\n#!/bin/bash\nqueries=(\n  \"CRISPR developments 2024\"\n  \"mRNA vaccine technology advances\"\n  \"AlphaFold3 accuracy improvements\"\n)\n\nfor query in \"${queries[@]}\"; do\n  echo \"Searching: $query\"\n  python scripts/perplexity_search.py \"$query\" --output \"results_$(echo $query | tr ' ' '_').json\"\n  sleep 2  # Rate limiting\ndone\n```\n\n## Cost Management\n\nPerplexity models have different pricing tiers:\n\n**Approximate costs per query:**\n- Sonar: $0.001-0.002 (most cost-effective)\n- Sonar Pro: $0.002-0.005 (recommended default)\n- Sonar Reasoning Pro: $0.005-0.010\n- Sonar Pro Search: $0.020-0.050+ (most comprehensive)\n\n**Cost optimization strategies:**\n1. Use `sonar` for simple fact lookups\n2. Default to `sonar-pro` for most queries\n3. Reserve `sonar-pro-search` for complex analysis\n4. Set `--max-tokens` to limit response length\n5. Monitor usage at https://openrouter.ai/activity\n6. Set spending limits in OpenRouter dashboard\n\n## Troubleshooting\n\n### API Key Not Set\n\n**Error**: \"OpenRouter API key not configured\"\n\n**Solution**:\n```bash\nexport OPENROUTER_API_KEY='sk-or-v1-your-key-here'\n# Or run setup script\npython scripts/setup_env.py --api-key sk-or-v1-your-key-here\n```\n\n### LiteLLM Not Installed\n\n**Error**: \"LiteLLM not installed\"\n\n**Solution**:\n```bash\nuv pip install litellm\n```\n\n### Rate Limiting\n\n**Error**: \"Rate limit exceeded\"\n\n**Solutions**:\n- Wait a few seconds before retrying\n- Increase rate limit at https://openrouter.ai/keys\n- Add delays between requests in batch processing\n\n### Insufficient Credits\n\n**Error**: \"Insufficient credits\"\n\n**Solution**:\n- Add credits at https://openrouter.ai/account\n- Enable auto-recharge to prevent interruptions\n\nSee `references/openrouter_setup.md` for comprehensive troubleshooting guide.\n\n## Integration with Other Skills\n\nThis skill complements other scientific skills:\n\n### Literature Review\n\nUse with `literature-review` skill:\n1. Use Perplexity to find recent papers and preprints\n2. Supplement PubMed searches with real-time web results\n3. Verify citations and find related work\n4. Discover latest developments post-database indexing\n\n### Scientific Writing\n\nUse with `scientific-writing` skill:\n1. Find recent references for introduction/discussion\n2. Verify current state of the art\n3. Check latest terminology and conventions\n4. Identify recent competing approaches\n\n### Hypothesis Generation\n\nUse with `hypothesis-generation` skill:\n1. Search for latest research findings\n2. Identify current gaps in knowledge\n3. Find recent methodological advances\n4. Discover emerging research directions\n\n### Critical Thinking\n\nUse with `scientific-critical-thinking` skill:\n1. Find evidence for and against hypotheses\n2. Locate methodological critiques\n3. Identify controversies in the field\n4. Verify claims with current evidence\n\n## Best Practices\n\n### Query Design\n\n1. **Be specific**: Include domain, time frame, and constraints\n2. **Use terminology**: Domain-appropriate keywords and phrases\n3. **Specify sources**: Mention preferred publication types or journals\n4. **Structure questions**: Clear components with explicit context\n5. **Iterate**: Refine based on initial results\n\n### Model Selection\n\n1. **Start with sonar-pro**: Good default for most queries\n2. **Upgrade for complexity**: Use sonar-pro-search for multi-step analysis\n3. **Downgrade for simplicity**: Use sonar for basic facts\n4. **Use reasoning models**: When step-by-step analysis needed\n\n### Cost Optimization\n\n1. **Choose appropriate models**: Match model to query complexity\n2. **Set token limits**: Use `--max-tokens` to control costs\n3. **Monitor usage**: Check OpenRouter dashboard regularly\n4. **Batch efficiently**: Combine related simple queries when possible\n5. **Cache results**: Save and reuse results for repeated queries\n\n### Security\n\n1. **Protect API keys**: Never commit to version control\n2. **Use environment variables**: Keep keys separate from code\n3. **Set spending limits**: Configure in OpenRouter dashboard\n4. **Monitor usage**: Watch for unexpected activity\n5. **Rotate keys**: Change keys periodically\n\n## Resources\n\n### Bundled Resources\n\n**Scripts:**\n- `scripts/perplexity_search.py`: Main search script with CLI interface\n- `scripts/setup_env.py`: Environment setup and validation helper\n\n**References:**\n- `references/search_strategies.md`: Comprehensive query design guide\n- `references/model_comparison.md`: Detailed model comparison and selection guide\n- `references/openrouter_setup.md`: Complete setup, troubleshooting, and security guide\n\n**Assets:**\n- `assets/.env.example`: Example environment file template\n\n### External Resources\n\n**OpenRouter:**\n- Dashboard: https://openrouter.ai/account\n- API Keys: https://openrouter.ai/keys\n- Perplexity Models: https://openrouter.ai/perplexity\n- Usage Monitoring: https://openrouter.ai/activity\n- Documentation: https://openrouter.ai/docs\n\n**LiteLLM:**\n- Documentation: https://docs.litellm.ai/\n- OpenRouter Provider: https://docs.litellm.ai/docs/providers/openrouter\n- GitHub: https://github.com/BerriAI/litellm\n\n**Perplexity:**\n- Official Docs: https://docs.perplexity.ai/\n\n## Dependencies\n\n### Required\n\n```bash\n# LiteLLM for API access\nuv pip install litellm\n```\n\n### Optional\n\n```bash\n# For .env file support\nuv pip install python-dotenv\n\n# For JSON processing (usually pre-installed)\nuv pip install jq\n```\n\n### Environment Variables\n\nRequired:\n- `OPENROUTER_API_KEY`: Your OpenRouter API key\n\nOptional:\n- `DEFAULT_MODEL`: Default model to use (default: sonar-pro)\n- `DEFAULT_MAX_TOKENS`: Default max tokens (default: 4000)\n- `DEFAULT_TEMPERATURE`: Default temperature (default: 0.2)\n\n## Summary\n\nThis skill provides:\n\n1. **Real-time web search**: Access current information beyond training data cutoff\n2. **Multiple models**: From cost-effective Sonar to advanced Sonar Pro Search\n3. **Simple setup**: Single OpenRouter API key, no separate Perplexity account\n4. **Comprehensive guidance**: Detailed references for query design and model selection\n5. **Cost-effective**: Pay-as-you-go pricing with usage monitoring\n6. **Scientific focus**: Optimized for research, literature search, and technical queries\n7. **Easy integration**: Works seamlessly with other scientific skills\n\nConduct AI-powered web searches to find current information, recent research, and grounded answers with source citations.\n",
        "data/k-dense-ai/polars/SKILL.md": "---\nname: polars\ndescription: \"Fast DataFrame library (Apache Arrow). Select, filter, group_by, joins, lazy evaluation, CSV/Parquet I/O, expression API, for high-performance data analysis workflows.\"\n---\n\n# Polars\n\n## Overview\n\nPolars is a lightning-fast DataFrame library for Python and Rust built on Apache Arrow. Work with Polars' expression-based API, lazy evaluation framework, and high-performance data manipulation capabilities for efficient data processing, pandas migration, and data pipeline optimization.\n\n## Quick Start\n\n### Installation and Basic Usage\n\nInstall Polars:\n```python\nuv pip install polars\n```\n\nBasic DataFrame creation and operations:\n```python\nimport polars as pl\n\n# Create DataFrame\ndf = pl.DataFrame({\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n    \"age\": [25, 30, 35],\n    \"city\": [\"NY\", \"LA\", \"SF\"]\n})\n\n# Select columns\ndf.select(\"name\", \"age\")\n\n# Filter rows\ndf.filter(pl.col(\"age\") > 25)\n\n# Add computed columns\ndf.with_columns(\n    age_plus_10=pl.col(\"age\") + 10\n)\n```\n\n## Core Concepts\n\n### Expressions\n\nExpressions are the fundamental building blocks of Polars operations. They describe transformations on data and can be composed, reused, and optimized.\n\n**Key principles:**\n- Use `pl.col(\"column_name\")` to reference columns\n- Chain methods to build complex transformations\n- Expressions are lazy and only execute within contexts (select, with_columns, filter, group_by)\n\n**Example:**\n```python\n# Expression-based computation\ndf.select(\n    pl.col(\"name\"),\n    (pl.col(\"age\") * 12).alias(\"age_in_months\")\n)\n```\n\n### Lazy vs Eager Evaluation\n\n**Eager (DataFrame):** Operations execute immediately\n```python\ndf = pl.read_csv(\"file.csv\")  # Reads immediately\nresult = df.filter(pl.col(\"age\") > 25)  # Executes immediately\n```\n\n**Lazy (LazyFrame):** Operations build a query plan, optimized before execution\n```python\nlf = pl.scan_csv(\"file.csv\")  # Doesn't read yet\nresult = lf.filter(pl.col(\"age\") > 25).select(\"name\", \"age\")\ndf = result.collect()  # Now executes optimized query\n```\n\n**When to use lazy:**\n- Working with large datasets\n- Complex query pipelines\n- When only some columns/rows are needed\n- Performance is critical\n\n**Benefits of lazy evaluation:**\n- Automatic query optimization\n- Predicate pushdown\n- Projection pushdown\n- Parallel execution\n\nFor detailed concepts, load `references/core_concepts.md`.\n\n## Common Operations\n\n### Select\nSelect and manipulate columns:\n```python\n# Select specific columns\ndf.select(\"name\", \"age\")\n\n# Select with expressions\ndf.select(\n    pl.col(\"name\"),\n    (pl.col(\"age\") * 2).alias(\"double_age\")\n)\n\n# Select all columns matching a pattern\ndf.select(pl.col(\"^.*_id$\"))\n```\n\n### Filter\nFilter rows by conditions:\n```python\n# Single condition\ndf.filter(pl.col(\"age\") > 25)\n\n# Multiple conditions (cleaner than using &)\ndf.filter(\n    pl.col(\"age\") > 25,\n    pl.col(\"city\") == \"NY\"\n)\n\n# Complex conditions\ndf.filter(\n    (pl.col(\"age\") > 25) | (pl.col(\"city\") == \"LA\")\n)\n```\n\n### With Columns\nAdd or modify columns while preserving existing ones:\n```python\n# Add new columns\ndf.with_columns(\n    age_plus_10=pl.col(\"age\") + 10,\n    name_upper=pl.col(\"name\").str.to_uppercase()\n)\n\n# Parallel computation (all columns computed in parallel)\ndf.with_columns(\n    pl.col(\"value\") * 10,\n    pl.col(\"value\") * 100,\n)\n```\n\n### Group By and Aggregations\nGroup data and compute aggregations:\n```python\n# Basic grouping\ndf.group_by(\"city\").agg(\n    pl.col(\"age\").mean().alias(\"avg_age\"),\n    pl.len().alias(\"count\")\n)\n\n# Multiple group keys\ndf.group_by(\"city\", \"department\").agg(\n    pl.col(\"salary\").sum()\n)\n\n# Conditional aggregations\ndf.group_by(\"city\").agg(\n    (pl.col(\"age\") > 30).sum().alias(\"over_30\")\n)\n```\n\nFor detailed operation patterns, load `references/operations.md`.\n\n## Aggregations and Window Functions\n\n### Aggregation Functions\nCommon aggregations within `group_by` context:\n- `pl.len()` - count rows\n- `pl.col(\"x\").sum()` - sum values\n- `pl.col(\"x\").mean()` - average\n- `pl.col(\"x\").min()` / `pl.col(\"x\").max()` - extremes\n- `pl.first()` / `pl.last()` - first/last values\n\n### Window Functions with `over()`\nApply aggregations while preserving row count:\n```python\n# Add group statistics to each row\ndf.with_columns(\n    avg_age_by_city=pl.col(\"age\").mean().over(\"city\"),\n    rank_in_city=pl.col(\"salary\").rank().over(\"city\")\n)\n\n# Multiple grouping columns\ndf.with_columns(\n    group_avg=pl.col(\"value\").mean().over(\"category\", \"region\")\n)\n```\n\n**Mapping strategies:**\n- `group_to_rows` (default): Preserves original row order\n- `explode`: Faster but groups rows together\n- `join`: Creates list columns\n\n## Data I/O\n\n### Supported Formats\nPolars supports reading and writing:\n- CSV, Parquet, JSON, Excel\n- Databases (via connectors)\n- Cloud storage (S3, Azure, GCS)\n- Google BigQuery\n- Multiple/partitioned files\n\n### Common I/O Operations\n\n**CSV:**\n```python\n# Eager\ndf = pl.read_csv(\"file.csv\")\ndf.write_csv(\"output.csv\")\n\n# Lazy (preferred for large files)\nlf = pl.scan_csv(\"file.csv\")\nresult = lf.filter(...).select(...).collect()\n```\n\n**Parquet (recommended for performance):**\n```python\ndf = pl.read_parquet(\"file.parquet\")\ndf.write_parquet(\"output.parquet\")\n```\n\n**JSON:**\n```python\ndf = pl.read_json(\"file.json\")\ndf.write_json(\"output.json\")\n```\n\nFor comprehensive I/O documentation, load `references/io_guide.md`.\n\n## Transformations\n\n### Joins\nCombine DataFrames:\n```python\n# Inner join\ndf1.join(df2, on=\"id\", how=\"inner\")\n\n# Left join\ndf1.join(df2, on=\"id\", how=\"left\")\n\n# Join on different column names\ndf1.join(df2, left_on=\"user_id\", right_on=\"id\")\n```\n\n### Concatenation\nStack DataFrames:\n```python\n# Vertical (stack rows)\npl.concat([df1, df2], how=\"vertical\")\n\n# Horizontal (add columns)\npl.concat([df1, df2], how=\"horizontal\")\n\n# Diagonal (union with different schemas)\npl.concat([df1, df2], how=\"diagonal\")\n```\n\n### Pivot and Unpivot\nReshape data:\n```python\n# Pivot (wide format)\ndf.pivot(values=\"sales\", index=\"date\", columns=\"product\")\n\n# Unpivot (long format)\ndf.unpivot(index=\"id\", on=[\"col1\", \"col2\"])\n```\n\nFor detailed transformation examples, load `references/transformations.md`.\n\n## Pandas Migration\n\nPolars offers significant performance improvements over pandas with a cleaner API. Key differences:\n\n### Conceptual Differences\n- **No index**: Polars uses integer positions only\n- **Strict typing**: No silent type conversions\n- **Lazy evaluation**: Available via LazyFrame\n- **Parallel by default**: Operations parallelized automatically\n\n### Common Operation Mappings\n\n| Operation | Pandas | Polars |\n|-----------|--------|--------|\n| Select column | `df[\"col\"]` | `df.select(\"col\")` |\n| Filter | `df[df[\"col\"] > 10]` | `df.filter(pl.col(\"col\") > 10)` |\n| Add column | `df.assign(x=...)` | `df.with_columns(x=...)` |\n| Group by | `df.groupby(\"col\").agg(...)` | `df.group_by(\"col\").agg(...)` |\n| Window | `df.groupby(\"col\").transform(...)` | `df.with_columns(...).over(\"col\")` |\n\n### Key Syntax Patterns\n\n**Pandas sequential (slow):**\n```python\ndf.assign(\n    col_a=lambda df_: df_.value * 10,\n    col_b=lambda df_: df_.value * 100\n)\n```\n\n**Polars parallel (fast):**\n```python\ndf.with_columns(\n    col_a=pl.col(\"value\") * 10,\n    col_b=pl.col(\"value\") * 100,\n)\n```\n\nFor comprehensive migration guide, load `references/pandas_migration.md`.\n\n## Best Practices\n\n### Performance Optimization\n\n1. **Use lazy evaluation for large datasets:**\n   ```python\n   lf = pl.scan_csv(\"large.csv\")  # Don't use read_csv\n   result = lf.filter(...).select(...).collect()\n   ```\n\n2. **Avoid Python functions in hot paths:**\n   - Stay within expression API for parallelization\n   - Use `.map_elements()` only when necessary\n   - Prefer native Polars operations\n\n3. **Use streaming for very large data:**\n   ```python\n   lf.collect(streaming=True)\n   ```\n\n4. **Select only needed columns early:**\n   ```python\n   # Good: Select columns early\n   lf.select(\"col1\", \"col2\").filter(...)\n\n   # Bad: Filter on all columns first\n   lf.filter(...).select(\"col1\", \"col2\")\n   ```\n\n5. **Use appropriate data types:**\n   - Categorical for low-cardinality strings\n   - Appropriate integer sizes (i32 vs i64)\n   - Date types for temporal data\n\n### Expression Patterns\n\n**Conditional operations:**\n```python\npl.when(condition).then(value).otherwise(other_value)\n```\n\n**Column operations across multiple columns:**\n```python\ndf.select(pl.col(\"^.*_value$\") * 2)  # Regex pattern\n```\n\n**Null handling:**\n```python\npl.col(\"x\").fill_null(0)\npl.col(\"x\").is_null()\npl.col(\"x\").drop_nulls()\n```\n\nFor additional best practices and patterns, load `references/best_practices.md`.\n\n## Resources\n\nThis skill includes comprehensive reference documentation:\n\n### references/\n- `core_concepts.md` - Detailed explanations of expressions, lazy evaluation, and type system\n- `operations.md` - Comprehensive guide to all common operations with examples\n- `pandas_migration.md` - Complete migration guide from pandas to Polars\n- `io_guide.md` - Data I/O operations for all supported formats\n- `transformations.md` - Joins, concatenation, pivots, and reshaping operations\n- `best_practices.md` - Performance optimization tips and common patterns\n\nLoad these references as needed when users require detailed information about specific topics.\n",
        "data/k-dense-ai/protocolsio-integration/SKILL.md": "---\nname: protocolsio-integration\ndescription: Integration with protocols.io API for managing scientific protocols. This skill should be used when working with protocols.io to search, create, update, or publish protocols; manage protocol steps and materials; handle discussions and comments; organize workspaces; upload and manage files; or integrate protocols.io functionality into workflows. Applicable for protocol discovery, collaborative protocol development, experiment tracking, lab protocol management, and scientific documentation.\n---\n\n# Protocols.io Integration\n\n## Overview\n\nProtocols.io is a comprehensive platform for developing, sharing, and managing scientific protocols. This skill provides complete integration with the protocols.io API v3, enabling programmatic access to protocols, workspaces, discussions, file management, and collaboration features.\n\n## When to Use This Skill\n\nUse this skill when working with protocols.io in any of the following scenarios:\n\n- **Protocol Discovery**: Searching for existing protocols by keywords, DOI, or category\n- **Protocol Management**: Creating, updating, or publishing scientific protocols\n- **Step Management**: Adding, editing, or organizing protocol steps and procedures\n- **Collaborative Development**: Working with team members on shared protocols\n- **Workspace Organization**: Managing lab or institutional protocol repositories\n- **Discussion & Feedback**: Adding or responding to protocol comments\n- **File Management**: Uploading data files, images, or documents to protocols\n- **Experiment Tracking**: Documenting protocol executions and results\n- **Data Export**: Backing up or migrating protocol collections\n- **Integration Projects**: Building tools that interact with protocols.io\n\n## Core Capabilities\n\nThis skill provides comprehensive guidance across five major capability areas:\n\n### 1. Authentication & Access\n\nManage API authentication using access tokens and OAuth flows. Includes both client access tokens (for personal content) and OAuth tokens (for multi-user applications).\n\n**Key operations:**\n- Generate authorization links for OAuth flow\n- Exchange authorization codes for access tokens\n- Refresh expired tokens\n- Manage rate limits and permissions\n\n**Reference:** Read `references/authentication.md` for detailed authentication procedures, OAuth implementation, and security best practices.\n\n### 2. Protocol Operations\n\nComplete protocol lifecycle management from creation to publication.\n\n**Key operations:**\n- Search and discover protocols by keywords, filters, or DOI\n- Retrieve detailed protocol information with all steps\n- Create new protocols with metadata and tags\n- Update protocol information and settings\n- Manage protocol steps (create, update, delete, reorder)\n- Handle protocol materials and reagents\n- Publish protocols with DOI issuance\n- Bookmark protocols for quick access\n- Generate protocol PDFs\n\n**Reference:** Read `references/protocols_api.md` for comprehensive protocol management guidance, including API endpoints, parameters, common workflows, and examples.\n\n### 3. Discussions & Collaboration\n\nEnable community engagement through comments and discussions.\n\n**Key operations:**\n- View protocol-level and step-level comments\n- Create new comments and threaded replies\n- Edit or delete your own comments\n- Analyze discussion patterns and feedback\n- Respond to user questions and issues\n\n**Reference:** Read `references/discussions.md` for discussion management, comment threading, and collaboration workflows.\n\n### 4. Workspace Management\n\nOrganize protocols within team workspaces with role-based permissions.\n\n**Key operations:**\n- List and access user workspaces\n- Retrieve workspace details and member lists\n- Request access or join workspaces\n- List workspace-specific protocols\n- Create protocols within workspaces\n- Manage workspace permissions and collaboration\n\n**Reference:** Read `references/workspaces.md` for workspace organization, permission management, and team collaboration patterns.\n\n### 5. File Operations\n\nUpload, organize, and manage files associated with protocols.\n\n**Key operations:**\n- Search workspace files and folders\n- Upload files with metadata and tags\n- Download files and verify uploads\n- Organize files into folder hierarchies\n- Update file metadata\n- Delete and restore files\n- Manage storage and organization\n\n**Reference:** Read `references/file_manager.md` for file upload procedures, organization strategies, and storage management.\n\n### 6. Additional Features\n\nSupplementary functionality including profiles, notifications, and exports.\n\n**Key operations:**\n- Manage user profiles and settings\n- Query recently published protocols\n- Create and track experiment records\n- Receive and manage notifications\n- Export organization data for archival\n\n**Reference:** Read `references/additional_features.md` for profile management, publication discovery, experiment tracking, and data export.\n\n## Getting Started\n\n### Step 1: Authentication Setup\n\nBefore using any protocols.io API functionality:\n\n1. Obtain an access token (CLIENT_ACCESS_TOKEN or OAUTH_ACCESS_TOKEN)\n2. Read `references/authentication.md` for detailed authentication procedures\n3. Store the token securely\n4. Include in all requests as: `Authorization: Bearer YOUR_TOKEN`\n\n### Step 2: Identify Your Use Case\n\nDetermine which capability area addresses your needs:\n\n- **Working with protocols?**  Read `references/protocols_api.md`\n- **Managing team protocols?**  Read `references/workspaces.md`\n- **Handling comments/feedback?**  Read `references/discussions.md`\n- **Uploading files/data?**  Read `references/file_manager.md`\n- **Tracking experiments or profiles?**  Read `references/additional_features.md`\n\n### Step 3: Implement Integration\n\nFollow the guidance in the relevant reference files:\n\n- Each reference includes detailed endpoint documentation\n- API parameters and request/response formats are specified\n- Common use cases and workflows are provided with examples\n- Best practices and error handling guidance included\n\n## Base URL and Request Format\n\nAll API requests use the base URL:\n```\nhttps://protocols.io/api/v3\n```\n\nAll requests require the Authorization header:\n```\nAuthorization: Bearer YOUR_ACCESS_TOKEN\n```\n\nMost endpoints support JSON request/response format with `Content-Type: application/json`.\n\n## Content Format Options\n\nMany endpoints support a `content_format` parameter to control how protocol content is returned:\n\n- `json`: Draft.js JSON format (default)\n- `html`: HTML format\n- `markdown`: Markdown format\n\nInclude as query parameter: `?content_format=html`\n\n## Rate Limiting\n\nBe aware of API rate limits:\n\n- **Standard endpoints**: 100 requests per minute per user\n- **PDF endpoint**: 5 requests/minute (signed-in), 3 requests/minute (unsigned)\n\nImplement exponential backoff for rate limit errors (HTTP 429).\n\n## Common Workflows\n\n### Workflow 1: Import and Analyze Protocol\n\nTo analyze an existing protocol from protocols.io:\n\n1. **Search**: Use `GET /protocols` with keywords to find relevant protocols\n2. **Retrieve**: Get full details with `GET /protocols/{protocol_id}`\n3. **Extract**: Parse steps, materials, and metadata for analysis\n4. **Review discussions**: Check `GET /protocols/{id}/comments` for user feedback\n5. **Export**: Generate PDF if needed for offline reference\n\n**Reference files**: `protocols_api.md`, `discussions.md`\n\n### Workflow 2: Create and Publish Protocol\n\nTo create a new protocol and publish with DOI:\n\n1. **Authenticate**: Ensure you have valid access token (see `authentication.md`)\n2. **Create**: Use `POST /protocols` with title and description\n3. **Add steps**: For each step, use `POST /protocols/{id}/steps`\n4. **Add materials**: Document reagents in step components\n5. **Review**: Verify all content is complete and accurate\n6. **Publish**: Issue DOI with `POST /protocols/{id}/publish`\n\n**Reference files**: `protocols_api.md`, `authentication.md`\n\n### Workflow 3: Collaborative Lab Workspace\n\nTo set up team protocol management:\n\n1. **Create/join workspace**: Access or request workspace membership (see `workspaces.md`)\n2. **Organize structure**: Create folder hierarchy for lab protocols (see `file_manager.md`)\n3. **Create protocols**: Use `POST /workspaces/{id}/protocols` for team protocols\n4. **Upload files**: Add experimental data and images\n5. **Enable discussions**: Team members can comment and provide feedback\n6. **Track experiments**: Document protocol executions with experiment records\n\n**Reference files**: `workspaces.md`, `file_manager.md`, `protocols_api.md`, `discussions.md`, `additional_features.md`\n\n### Workflow 4: Experiment Documentation\n\nTo track protocol executions and results:\n\n1. **Execute protocol**: Perform protocol in laboratory\n2. **Upload data**: Use File Manager API to upload results (see `file_manager.md`)\n3. **Create record**: Document execution with `POST /protocols/{id}/runs`\n4. **Link files**: Reference uploaded data files in experiment record\n5. **Note modifications**: Document any protocol deviations or optimizations\n6. **Analyze**: Review multiple runs for reproducibility assessment\n\n**Reference files**: `additional_features.md`, `file_manager.md`, `protocols_api.md`\n\n### Workflow 5: Protocol Discovery and Citation\n\nTo find and cite protocols in research:\n\n1. **Search**: Query published protocols with `GET /publications`\n2. **Filter**: Use category and keyword filters for relevant protocols\n3. **Review**: Read protocol details and community comments\n4. **Bookmark**: Save useful protocols with `POST /protocols/{id}/bookmarks`\n5. **Cite**: Use protocol DOI in publications (proper attribution)\n6. **Export PDF**: Generate formatted PDF for offline reference\n\n**Reference files**: `protocols_api.md`, `additional_features.md`\n\n## Python Request Examples\n\n### Basic Protocol Search\n\n```python\nimport requests\n\ntoken = \"YOUR_ACCESS_TOKEN\"\nheaders = {\"Authorization\": f\"Bearer {token}\"}\n\n# Search for CRISPR protocols\nresponse = requests.get(\n    \"https://protocols.io/api/v3/protocols\",\n    headers=headers,\n    params={\n        \"filter\": \"public\",\n        \"key\": \"CRISPR\",\n        \"page_size\": 10,\n        \"content_format\": \"html\"\n    }\n)\n\nprotocols = response.json()\nfor protocol in protocols[\"items\"]:\n    print(f\"{protocol['title']} - {protocol['doi']}\")\n```\n\n### Create New Protocol\n\n```python\nimport requests\n\ntoken = \"YOUR_ACCESS_TOKEN\"\nheaders = {\n    \"Authorization\": f\"Bearer {token}\",\n    \"Content-Type\": \"application/json\"\n}\n\n# Create protocol\ndata = {\n    \"title\": \"CRISPR-Cas9 Gene Editing Protocol\",\n    \"description\": \"Comprehensive protocol for CRISPR gene editing\",\n    \"tags\": [\"CRISPR\", \"gene editing\", \"molecular biology\"]\n}\n\nresponse = requests.post(\n    \"https://protocols.io/api/v3/protocols\",\n    headers=headers,\n    json=data\n)\n\nprotocol_id = response.json()[\"item\"][\"id\"]\nprint(f\"Created protocol: {protocol_id}\")\n```\n\n### Upload File to Workspace\n\n```python\nimport requests\n\ntoken = \"YOUR_ACCESS_TOKEN\"\nheaders = {\"Authorization\": f\"Bearer {token}\"}\n\n# Upload file\nwith open(\"data.csv\", \"rb\") as f:\n    files = {\"file\": f}\n    data = {\n        \"folder_id\": \"root\",\n        \"description\": \"Experimental results\",\n        \"tags\": \"experiment,data,2025\"\n    }\n\n    response = requests.post(\n        \"https://protocols.io/api/v3/workspaces/12345/files/upload\",\n        headers=headers,\n        files=files,\n        data=data\n    )\n\nfile_id = response.json()[\"item\"][\"id\"]\nprint(f\"Uploaded file: {file_id}\")\n```\n\n## Error Handling\n\nImplement robust error handling for API requests:\n\n```python\nimport requests\nimport time\n\ndef make_request_with_retry(url, headers, max_retries=3):\n    for attempt in range(max_retries):\n        try:\n            response = requests.get(url, headers=headers)\n\n            if response.status_code == 200:\n                return response.json()\n            elif response.status_code == 429:  # Rate limit\n                retry_after = int(response.headers.get('Retry-After', 60))\n                time.sleep(retry_after)\n                continue\n            elif response.status_code >= 500:  # Server error\n                time.sleep(2 ** attempt)  # Exponential backoff\n                continue\n            else:\n                response.raise_for_status()\n\n        except requests.exceptions.RequestException as e:\n            if attempt == max_retries - 1:\n                raise\n            time.sleep(2 ** attempt)\n\n    raise Exception(\"Max retries exceeded\")\n```\n\n## Reference Files\n\nLoad the appropriate reference file based on your task:\n\n- **`authentication.md`**: OAuth flows, token management, rate limiting\n- **`protocols_api.md`**: Protocol CRUD, steps, materials, publishing, PDFs\n- **`discussions.md`**: Comments, replies, collaboration\n- **`workspaces.md`**: Team workspaces, permissions, organization\n- **`file_manager.md`**: File upload, folders, storage management\n- **`additional_features.md`**: Profiles, publications, experiments, notifications\n\nTo load a reference file, read the file from the `references/` directory when needed for specific functionality.\n\n## Best Practices\n\n1. **Authentication**: Store tokens securely, never in code or version control\n2. **Rate Limiting**: Implement exponential backoff and respect rate limits\n3. **Error Handling**: Handle all HTTP error codes appropriately\n4. **Data Validation**: Validate input before API calls\n5. **Documentation**: Document protocol steps thoroughly\n6. **Collaboration**: Use comments and discussions for team communication\n7. **Organization**: Maintain consistent naming and tagging conventions\n8. **Versioning**: Track protocol versions when making updates\n9. **Attribution**: Properly cite protocols using DOIs\n10. **Backup**: Regularly export important protocols and workspace data\n\n## Additional Resources\n\n- **Official API Documentation**: https://apidoc.protocols.io/\n- **Protocols.io Platform**: https://www.protocols.io/\n- **Support**: Contact protocols.io support for API access and technical issues\n- **Community**: Engage with protocols.io community for best practices\n\n## Troubleshooting\n\n**Authentication Issues:**\n- Verify token is valid and not expired\n- Check Authorization header format: `Bearer YOUR_TOKEN`\n- Ensure appropriate token type (CLIENT vs OAUTH)\n\n**Rate Limiting:**\n- Implement exponential backoff for 429 errors\n- Monitor request frequency\n- Consider caching frequent requests\n\n**Permission Errors:**\n- Verify workspace/protocol access permissions\n- Check user role in workspace\n- Ensure protocol is not private if accessing without permission\n\n**File Upload Failures:**\n- Check file size against workspace limits\n- Verify file type is supported\n- Ensure multipart/form-data encoding is correct\n\nFor detailed troubleshooting guidance, refer to the specific reference files covering each capability area.\n",
        "data/k-dense-ai/pubchem-database/SKILL.md": "---\nname: pubchem-database\ndescription: \"Query PubChem via PUG-REST API/PubChemPy (110M+ compounds). Search by name/CID/SMILES, retrieve properties, similarity/substructure searches, bioactivity, for cheminformatics.\"\n---\n\n# PubChem Database\n\n## Overview\n\nPubChem is the world's largest freely available chemical database with 110M+ compounds and 270M+ bioactivities. Query chemical structures by name, CID, or SMILES, retrieve molecular properties, perform similarity and substructure searches, access bioactivity data using PUG-REST API and PubChemPy.\n\n## When to Use This Skill\n\nThis skill should be used when:\n- Searching for chemical compounds by name, structure (SMILES/InChI), or molecular formula\n- Retrieving molecular properties (MW, LogP, TPSA, hydrogen bonding descriptors)\n- Performing similarity searches to find structurally related compounds\n- Conducting substructure searches for specific chemical motifs\n- Accessing bioactivity data from screening assays\n- Converting between chemical identifier formats (CID, SMILES, InChI)\n- Batch processing multiple compounds for drug-likeness screening or property analysis\n\n## Core Capabilities\n\n### 1. Chemical Structure Search\n\nSearch for compounds using multiple identifier types:\n\n**By Chemical Name**:\n```python\nimport pubchempy as pcp\ncompounds = pcp.get_compounds('aspirin', 'name')\ncompound = compounds[0]\n```\n\n**By CID (Compound ID)**:\n```python\ncompound = pcp.Compound.from_cid(2244)  # Aspirin\n```\n\n**By SMILES**:\n```python\ncompound = pcp.get_compounds('CC(=O)OC1=CC=CC=C1C(=O)O', 'smiles')[0]\n```\n\n**By InChI**:\n```python\ncompound = pcp.get_compounds('InChI=1S/C9H8O4/...', 'inchi')[0]\n```\n\n**By Molecular Formula**:\n```python\ncompounds = pcp.get_compounds('C9H8O4', 'formula')\n# Returns all compounds matching this formula\n```\n\n### 2. Property Retrieval\n\nRetrieve molecular properties for compounds using either high-level or low-level approaches:\n\n**Using PubChemPy (Recommended)**:\n```python\nimport pubchempy as pcp\n\n# Get compound object with all properties\ncompound = pcp.get_compounds('caffeine', 'name')[0]\n\n# Access individual properties\nmolecular_formula = compound.molecular_formula\nmolecular_weight = compound.molecular_weight\niupac_name = compound.iupac_name\nsmiles = compound.canonical_smiles\ninchi = compound.inchi\nxlogp = compound.xlogp  # Partition coefficient\ntpsa = compound.tpsa    # Topological polar surface area\n```\n\n**Get Specific Properties**:\n```python\n# Request only specific properties\nproperties = pcp.get_properties(\n    ['MolecularFormula', 'MolecularWeight', 'CanonicalSMILES', 'XLogP'],\n    'aspirin',\n    'name'\n)\n# Returns list of dictionaries\n```\n\n**Batch Property Retrieval**:\n```python\nimport pandas as pd\n\ncompound_names = ['aspirin', 'ibuprofen', 'paracetamol']\nall_properties = []\n\nfor name in compound_names:\n    props = pcp.get_properties(\n        ['MolecularFormula', 'MolecularWeight', 'XLogP'],\n        name,\n        'name'\n    )\n    all_properties.extend(props)\n\ndf = pd.DataFrame(all_properties)\n```\n\n**Available Properties**: MolecularFormula, MolecularWeight, CanonicalSMILES, IsomericSMILES, InChI, InChIKey, IUPACName, XLogP, TPSA, HBondDonorCount, HBondAcceptorCount, RotatableBondCount, Complexity, Charge, and many more (see `references/api_reference.md` for complete list).\n\n### 3. Similarity Search\n\nFind structurally similar compounds using Tanimoto similarity:\n\n```python\nimport pubchempy as pcp\n\n# Start with a query compound\nquery_compound = pcp.get_compounds('gefitinib', 'name')[0]\nquery_smiles = query_compound.canonical_smiles\n\n# Perform similarity search\nsimilar_compounds = pcp.get_compounds(\n    query_smiles,\n    'smiles',\n    searchtype='similarity',\n    Threshold=85,  # Similarity threshold (0-100)\n    MaxRecords=50\n)\n\n# Process results\nfor compound in similar_compounds[:10]:\n    print(f\"CID {compound.cid}: {compound.iupac_name}\")\n    print(f\"  MW: {compound.molecular_weight}\")\n```\n\n**Note**: Similarity searches are asynchronous for large queries and may take 15-30 seconds to complete. PubChemPy handles the asynchronous pattern automatically.\n\n### 4. Substructure Search\n\nFind compounds containing a specific structural motif:\n\n```python\nimport pubchempy as pcp\n\n# Search for compounds containing pyridine ring\npyridine_smiles = 'c1ccncc1'\n\nmatches = pcp.get_compounds(\n    pyridine_smiles,\n    'smiles',\n    searchtype='substructure',\n    MaxRecords=100\n)\n\nprint(f\"Found {len(matches)} compounds containing pyridine\")\n```\n\n**Common Substructures**:\n- Benzene ring: `c1ccccc1`\n- Pyridine: `c1ccncc1`\n- Phenol: `c1ccc(O)cc1`\n- Carboxylic acid: `C(=O)O`\n\n### 5. Format Conversion\n\nConvert between different chemical structure formats:\n\n```python\nimport pubchempy as pcp\n\ncompound = pcp.get_compounds('aspirin', 'name')[0]\n\n# Convert to different formats\nsmiles = compound.canonical_smiles\ninchi = compound.inchi\ninchikey = compound.inchikey\ncid = compound.cid\n\n# Download structure files\npcp.download('SDF', 'aspirin', 'name', 'aspirin.sdf', overwrite=True)\npcp.download('JSON', '2244', 'cid', 'aspirin.json', overwrite=True)\n```\n\n### 6. Structure Visualization\n\nGenerate 2D structure images:\n\n```python\nimport pubchempy as pcp\n\n# Download compound structure as PNG\npcp.download('PNG', 'caffeine', 'name', 'caffeine.png', overwrite=True)\n\n# Using direct URL (via requests)\nimport requests\n\ncid = 2244  # Aspirin\nurl = f\"https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/cid/{cid}/PNG?image_size=large\"\nresponse = requests.get(url)\n\nwith open('structure.png', 'wb') as f:\n    f.write(response.content)\n```\n\n### 7. Synonym Retrieval\n\nGet all known names and synonyms for a compound:\n\n```python\nimport pubchempy as pcp\n\nsynonyms_data = pcp.get_synonyms('aspirin', 'name')\n\nif synonyms_data:\n    cid = synonyms_data[0]['CID']\n    synonyms = synonyms_data[0]['Synonym']\n\n    print(f\"CID {cid} has {len(synonyms)} synonyms:\")\n    for syn in synonyms[:10]:  # First 10\n        print(f\"  - {syn}\")\n```\n\n### 8. Bioactivity Data Access\n\nRetrieve biological activity data from assays:\n\n```python\nimport requests\nimport json\n\n# Get bioassay summary for a compound\ncid = 2244  # Aspirin\nurl = f\"https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/cid/{cid}/assaysummary/JSON\"\n\nresponse = requests.get(url)\nif response.status_code == 200:\n    data = response.json()\n    # Process bioassay information\n    table = data.get('Table', {})\n    rows = table.get('Row', [])\n    print(f\"Found {len(rows)} bioassay records\")\n```\n\n**For more complex bioactivity queries**, use the `scripts/bioactivity_query.py` helper script which provides:\n- Bioassay summaries with activity outcome filtering\n- Assay target identification\n- Search for compounds by biological target\n- Active compound lists for specific assays\n\n### 9. Comprehensive Compound Annotations\n\nAccess detailed compound information through PUG-View:\n\n```python\nimport requests\n\ncid = 2244\nurl = f\"https://pubchem.ncbi.nlm.nih.gov/rest/pug_view/data/compound/{cid}/JSON\"\n\nresponse = requests.get(url)\nif response.status_code == 200:\n    annotations = response.json()\n    # Contains extensive data including:\n    # - Chemical and Physical Properties\n    # - Drug and Medication Information\n    # - Pharmacology and Biochemistry\n    # - Safety and Hazards\n    # - Toxicity\n    # - Literature references\n    # - Patents\n```\n\n**Get Specific Section**:\n```python\n# Get only drug information\nurl = f\"https://pubchem.ncbi.nlm.nih.gov/rest/pug_view/data/compound/{cid}/JSON?heading=Drug and Medication Information\"\n```\n\n## Installation Requirements\n\nInstall PubChemPy for Python-based access:\n\n```bash\nuv pip install pubchempy\n```\n\nFor direct API access and bioactivity queries:\n\n```bash\nuv pip install requests\n```\n\nOptional for data analysis:\n\n```bash\nuv pip install pandas\n```\n\n## Helper Scripts\n\nThis skill includes Python scripts for common PubChem tasks:\n\n### scripts/compound_search.py\n\nProvides utility functions for searching and retrieving compound information:\n\n**Key Functions**:\n- `search_by_name(name, max_results=10)`: Search compounds by name\n- `search_by_smiles(smiles)`: Search by SMILES string\n- `get_compound_by_cid(cid)`: Retrieve compound by CID\n- `get_compound_properties(identifier, namespace, properties)`: Get specific properties\n- `similarity_search(smiles, threshold, max_records)`: Perform similarity search\n- `substructure_search(smiles, max_records)`: Perform substructure search\n- `get_synonyms(identifier, namespace)`: Get all synonyms\n- `batch_search(identifiers, namespace, properties)`: Batch search multiple compounds\n- `download_structure(identifier, namespace, format, filename)`: Download structures\n- `print_compound_info(compound)`: Print formatted compound information\n\n**Usage**:\n```python\nfrom scripts.compound_search import search_by_name, get_compound_properties\n\n# Search for a compound\ncompounds = search_by_name('ibuprofen')\n\n# Get specific properties\nprops = get_compound_properties('aspirin', 'name', ['MolecularWeight', 'XLogP'])\n```\n\n### scripts/bioactivity_query.py\n\nProvides functions for retrieving biological activity data:\n\n**Key Functions**:\n- `get_bioassay_summary(cid)`: Get bioassay summary for compound\n- `get_compound_bioactivities(cid, activity_outcome)`: Get filtered bioactivities\n- `get_assay_description(aid)`: Get detailed assay information\n- `get_assay_targets(aid)`: Get biological targets for assay\n- `search_assays_by_target(target_name, max_results)`: Find assays by target\n- `get_active_compounds_in_assay(aid, max_results)`: Get active compounds\n- `get_compound_annotations(cid, section)`: Get PUG-View annotations\n- `summarize_bioactivities(cid)`: Generate bioactivity summary statistics\n- `find_compounds_by_bioactivity(target, threshold, max_compounds)`: Find compounds by target\n\n**Usage**:\n```python\nfrom scripts.bioactivity_query import get_bioassay_summary, summarize_bioactivities\n\n# Get bioactivity summary\nsummary = summarize_bioactivities(2244)  # Aspirin\nprint(f\"Total assays: {summary['total_assays']}\")\nprint(f\"Active: {summary['active']}, Inactive: {summary['inactive']}\")\n```\n\n## API Rate Limits and Best Practices\n\n**Rate Limits**:\n- Maximum 5 requests per second\n- Maximum 400 requests per minute\n- Maximum 300 seconds running time per minute\n\n**Best Practices**:\n1. **Use CIDs for repeated queries**: CIDs are more efficient than names or structures\n2. **Cache results locally**: Store frequently accessed data\n3. **Batch requests**: Combine multiple queries when possible\n4. **Implement delays**: Add 0.2-0.3 second delays between requests\n5. **Handle errors gracefully**: Check for HTTP errors and missing data\n6. **Use PubChemPy**: Higher-level abstraction handles many edge cases\n7. **Leverage asynchronous pattern**: For large similarity/substructure searches\n8. **Specify MaxRecords**: Limit results to avoid timeouts\n\n**Error Handling**:\n```python\nfrom pubchempy import BadRequestError, NotFoundError, TimeoutError\n\ntry:\n    compound = pcp.get_compounds('query', 'name')[0]\nexcept NotFoundError:\n    print(\"Compound not found\")\nexcept BadRequestError:\n    print(\"Invalid request format\")\nexcept TimeoutError:\n    print(\"Request timed out - try reducing scope\")\nexcept IndexError:\n    print(\"No results returned\")\n```\n\n## Common Workflows\n\n### Workflow 1: Chemical Identifier Conversion Pipeline\n\nConvert between different chemical identifiers:\n\n```python\nimport pubchempy as pcp\n\n# Start with any identifier type\ncompound = pcp.get_compounds('caffeine', 'name')[0]\n\n# Extract all identifier formats\nidentifiers = {\n    'CID': compound.cid,\n    'Name': compound.iupac_name,\n    'SMILES': compound.canonical_smiles,\n    'InChI': compound.inchi,\n    'InChIKey': compound.inchikey,\n    'Formula': compound.molecular_formula\n}\n```\n\n### Workflow 2: Drug-Like Property Screening\n\nScreen compounds using Lipinski's Rule of Five:\n\n```python\nimport pubchempy as pcp\n\ndef check_drug_likeness(compound_name):\n    compound = pcp.get_compounds(compound_name, 'name')[0]\n\n    # Lipinski's Rule of Five\n    rules = {\n        'MW <= 500': compound.molecular_weight <= 500,\n        'LogP <= 5': compound.xlogp <= 5 if compound.xlogp else None,\n        'HBD <= 5': compound.h_bond_donor_count <= 5,\n        'HBA <= 10': compound.h_bond_acceptor_count <= 10\n    }\n\n    violations = sum(1 for v in rules.values() if v is False)\n    return rules, violations\n\nrules, violations = check_drug_likeness('aspirin')\nprint(f\"Lipinski violations: {violations}\")\n```\n\n### Workflow 3: Finding Similar Drug Candidates\n\nIdentify structurally similar compounds to a known drug:\n\n```python\nimport pubchempy as pcp\n\n# Start with known drug\nreference_drug = pcp.get_compounds('imatinib', 'name')[0]\nreference_smiles = reference_drug.canonical_smiles\n\n# Find similar compounds\nsimilar = pcp.get_compounds(\n    reference_smiles,\n    'smiles',\n    searchtype='similarity',\n    Threshold=85,\n    MaxRecords=20\n)\n\n# Filter by drug-like properties\ncandidates = []\nfor comp in similar:\n    if comp.molecular_weight and 200 <= comp.molecular_weight <= 600:\n        if comp.xlogp and -1 <= comp.xlogp <= 5:\n            candidates.append(comp)\n\nprint(f\"Found {len(candidates)} drug-like candidates\")\n```\n\n### Workflow 4: Batch Compound Property Comparison\n\nCompare properties across multiple compounds:\n\n```python\nimport pubchempy as pcp\nimport pandas as pd\n\ncompound_list = ['aspirin', 'ibuprofen', 'naproxen', 'celecoxib']\n\nproperties_list = []\nfor name in compound_list:\n    try:\n        compound = pcp.get_compounds(name, 'name')[0]\n        properties_list.append({\n            'Name': name,\n            'CID': compound.cid,\n            'Formula': compound.molecular_formula,\n            'MW': compound.molecular_weight,\n            'LogP': compound.xlogp,\n            'TPSA': compound.tpsa,\n            'HBD': compound.h_bond_donor_count,\n            'HBA': compound.h_bond_acceptor_count\n        })\n    except Exception as e:\n        print(f\"Error processing {name}: {e}\")\n\ndf = pd.DataFrame(properties_list)\nprint(df.to_string(index=False))\n```\n\n### Workflow 5: Substructure-Based Virtual Screening\n\nScreen for compounds containing specific pharmacophores:\n\n```python\nimport pubchempy as pcp\n\n# Define pharmacophore (e.g., sulfonamide group)\npharmacophore_smiles = 'S(=O)(=O)N'\n\n# Search for compounds containing this substructure\nhits = pcp.get_compounds(\n    pharmacophore_smiles,\n    'smiles',\n    searchtype='substructure',\n    MaxRecords=100\n)\n\n# Further filter by properties\nfiltered_hits = [\n    comp for comp in hits\n    if comp.molecular_weight and comp.molecular_weight < 500\n]\n\nprint(f\"Found {len(filtered_hits)} compounds with desired substructure\")\n```\n\n## Reference Documentation\n\nFor detailed API documentation, including complete property lists, URL patterns, advanced query options, and more examples, consult `references/api_reference.md`. This comprehensive reference includes:\n\n- Complete PUG-REST API endpoint documentation\n- Full list of available molecular properties\n- Asynchronous request handling patterns\n- PubChemPy API reference\n- PUG-View API for annotations\n- Common workflows and use cases\n- Links to official PubChem documentation\n\n## Troubleshooting\n\n**Compound Not Found**:\n- Try alternative names or synonyms\n- Use CID if known\n- Check spelling and chemical name format\n\n**Timeout Errors**:\n- Reduce MaxRecords parameter\n- Add delays between requests\n- Use CIDs instead of names for faster queries\n\n**Empty Property Values**:\n- Not all properties are available for all compounds\n- Check if property exists before accessing: `if compound.xlogp:`\n- Some properties only available for certain compound types\n\n**Rate Limit Exceeded**:\n- Implement delays (0.2-0.3 seconds) between requests\n- Use batch operations where possible\n- Consider caching results locally\n\n**Similarity/Substructure Search Hangs**:\n- These are asynchronous operations that may take 15-30 seconds\n- PubChemPy handles polling automatically\n- Reduce MaxRecords if timing out\n\n## Additional Resources\n\n- PubChem Home: https://pubchem.ncbi.nlm.nih.gov/\n- PUG-REST Documentation: https://pubchem.ncbi.nlm.nih.gov/docs/pug-rest\n- PUG-REST Tutorial: https://pubchem.ncbi.nlm.nih.gov/docs/pug-rest-tutorial\n- PubChemPy Documentation: https://pubchempy.readthedocs.io/\n- PubChemPy GitHub: https://github.com/mcs07/PubChemPy\n",
        "data/k-dense-ai/pubmed-database/SKILL.md": "---\nname: pubmed-database\ndescription: \"Direct REST API access to PubMed. Advanced Boolean/MeSH queries, E-utilities API, batch processing, citation management. For Python workflows, prefer biopython (Bio.Entrez). Use this for direct HTTP/REST work or custom API implementations.\"\n---\n\n# PubMed Database\n\n## Overview\n\nPubMed is the U.S. National Library of Medicine's comprehensive database providing free access to MEDLINE and life sciences literature. Construct advanced queries with Boolean operators, MeSH terms, and field tags, access data programmatically via E-utilities API for systematic reviews and literature analysis.\n\n## When to Use This Skill\n\nThis skill should be used when:\n- Searching for biomedical or life sciences research articles\n- Constructing complex search queries with Boolean operators, field tags, or MeSH terms\n- Conducting systematic literature reviews or meta-analyses\n- Accessing PubMed data programmatically via the E-utilities API\n- Finding articles by specific criteria (author, journal, publication date, article type)\n- Retrieving citation information, abstracts, or full-text articles\n- Working with PMIDs (PubMed IDs) or DOIs\n- Creating automated workflows for literature monitoring or data extraction\n\n## Core Capabilities\n\n### 1. Advanced Search Query Construction\n\nConstruct sophisticated PubMed queries using Boolean operators, field tags, and specialized syntax.\n\n**Basic Search Strategies**:\n- Combine concepts with Boolean operators (AND, OR, NOT)\n- Use field tags to limit searches to specific record parts\n- Employ phrase searching with double quotes for exact matches\n- Apply wildcards for term variations\n- Use proximity searching for terms within specified distances\n\n**Example Queries**:\n```\n# Recent systematic reviews on diabetes treatment\ndiabetes mellitus[mh] AND treatment[tiab] AND systematic review[pt] AND 2023:2024[dp]\n\n# Clinical trials comparing two drugs\n(metformin[nm] OR insulin[nm]) AND diabetes mellitus, type 2[mh] AND randomized controlled trial[pt]\n\n# Author-specific research\nsmith ja[au] AND cancer[tiab] AND 2023[dp] AND english[la]\n```\n\n**When to consult search_syntax.md**:\n- Need comprehensive list of available field tags\n- Require detailed explanation of search operators\n- Constructing complex proximity searches\n- Understanding automatic term mapping behavior\n- Need specific syntax for date ranges, wildcards, or special characters\n\nGrep pattern for field tags: `\\[au\\]|\\[ti\\]|\\[ab\\]|\\[mh\\]|\\[pt\\]|\\[dp\\]`\n\n### 2. MeSH Terms and Controlled Vocabulary\n\nUse Medical Subject Headings (MeSH) for precise, consistent searching across the biomedical literature.\n\n**MeSH Searching**:\n- [mh] tag searches MeSH terms with automatic inclusion of narrower terms\n- [majr] tag limits to articles where the topic is the main focus\n- Combine MeSH terms with subheadings for specificity (e.g., diabetes mellitus/therapy[mh])\n\n**Common MeSH Subheadings**:\n- /diagnosis - Diagnostic methods\n- /drug therapy - Pharmaceutical treatment\n- /epidemiology - Disease patterns and prevalence\n- /etiology - Disease causes\n- /prevention & control - Preventive measures\n- /therapy - Treatment approaches\n\n**Example**:\n```\n# Diabetes therapy with specific focus\ndiabetes mellitus, type 2[mh]/drug therapy AND cardiovascular diseases[mh]/prevention & control\n```\n\n### 3. Article Type and Publication Filtering\n\nFilter results by publication type, date, text availability, and other attributes.\n\n**Publication Types** (use [pt] field tag):\n- Clinical Trial\n- Meta-Analysis\n- Randomized Controlled Trial\n- Review\n- Systematic Review\n- Case Reports\n- Guideline\n\n**Date Filtering**:\n- Single year: `2024[dp]`\n- Date range: `2020:2024[dp]`\n- Specific date: `2024/03/15[dp]`\n\n**Text Availability**:\n- Free full text: Add `AND free full text[sb]` to query\n- Has abstract: Add `AND hasabstract[text]` to query\n\n**Example**:\n```\n# Recent free full-text RCTs on hypertension\nhypertension[mh] AND randomized controlled trial[pt] AND 2023:2024[dp] AND free full text[sb]\n```\n\n### 4. Programmatic Access via E-utilities API\n\nAccess PubMed data programmatically using the NCBI E-utilities REST API for automation and bulk operations.\n\n**Core API Endpoints**:\n1. **ESearch** - Search database and retrieve PMIDs\n2. **EFetch** - Download full records in various formats\n3. **ESummary** - Get document summaries\n4. **EPost** - Upload UIDs for batch processing\n5. **ELink** - Find related articles and linked data\n\n**Basic Workflow**:\n```python\nimport requests\n\n# Step 1: Search for articles\nbase_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/\"\nsearch_url = f\"{base_url}esearch.fcgi\"\nparams = {\n    \"db\": \"pubmed\",\n    \"term\": \"diabetes[tiab] AND 2024[dp]\",\n    \"retmax\": 100,\n    \"retmode\": \"json\",\n    \"api_key\": \"YOUR_API_KEY\"  # Optional but recommended\n}\nresponse = requests.get(search_url, params=params)\npmids = response.json()[\"esearchresult\"][\"idlist\"]\n\n# Step 2: Fetch article details\nfetch_url = f\"{base_url}efetch.fcgi\"\nparams = {\n    \"db\": \"pubmed\",\n    \"id\": \",\".join(pmids),\n    \"rettype\": \"abstract\",\n    \"retmode\": \"text\",\n    \"api_key\": \"YOUR_API_KEY\"\n}\nresponse = requests.get(fetch_url, params=params)\nabstracts = response.text\n```\n\n**Rate Limits**:\n- Without API key: 3 requests/second\n- With API key: 10 requests/second\n- Always include User-Agent header\n\n**Best Practices**:\n- Use history server (usehistory=y) for large result sets\n- Implement batch operations via EPost for multiple UIDs\n- Cache results locally to minimize redundant calls\n- Respect rate limits to avoid service disruption\n\n**When to consult api_reference.md**:\n- Need detailed endpoint documentation\n- Require parameter specifications for each E-utility\n- Constructing batch operations or history server workflows\n- Understanding response formats (XML, JSON, text)\n- Troubleshooting API errors or rate limit issues\n\nGrep pattern for API endpoints: `esearch|efetch|esummary|epost|elink|einfo`\n\n### 5. Citation Matching and Article Retrieval\n\nFind articles using partial citation information or specific identifiers.\n\n**By Identifier**:\n```\n# By PMID\n12345678[pmid]\n\n# By DOI\n10.1056/NEJMoa123456[doi]\n\n# By PMC ID\nPMC123456[pmc]\n```\n\n**Citation Matching** (via ECitMatch API):\nUse journal name, year, volume, page, and author to find PMIDs:\n```\nFormat: journal|year|volume|page|author|key|\nExample: Science|2008|320|5880|1185|key1|\n```\n\n**By Author and Metadata**:\n```\n# First author with year and topic\nsmith ja[1au] AND 2023[dp] AND cancer[tiab]\n\n# Journal, volume, and page\nnature[ta] AND 2024[dp] AND 456[vi] AND 123-130[pg]\n```\n\n### 6. Systematic Literature Reviews\n\nConduct comprehensive literature searches for systematic reviews and meta-analyses.\n\n**PICO Framework** (Population, Intervention, Comparison, Outcome):\nStructure clinical research questions systematically:\n```\n# Example: Diabetes treatment effectiveness\n# P: diabetes mellitus, type 2[mh]\n# I: metformin[nm]\n# C: lifestyle modification[tiab]\n# O: glycemic control[tiab]\n\ndiabetes mellitus, type 2[mh] AND\n(metformin[nm] OR lifestyle modification[tiab]) AND\nglycemic control[tiab] AND\nrandomized controlled trial[pt]\n```\n\n**Comprehensive Search Strategy**:\n```\n# Include multiple synonyms and MeSH terms\n(disease name[tiab] OR disease name[mh] OR synonym[tiab]) AND\n(treatment[tiab] OR therapy[tiab] OR intervention[tiab]) AND\n(systematic review[pt] OR meta-analysis[pt] OR randomized controlled trial[pt]) AND\n2020:2024[dp] AND\nenglish[la]\n```\n\n**Search Refinement**:\n1. Start broad, review results\n2. Add specificity with field tags\n3. Apply date and publication type filters\n4. Use Advanced Search to view query translation\n5. Combine search history for complex queries\n\n**When to consult common_queries.md**:\n- Need example queries for specific disease types or research areas\n- Require templates for different study designs\n- Looking for population-specific query patterns (pediatric, geriatric, etc.)\n- Constructing methodology-specific searches\n- Need quality filters or best practice patterns\n\nGrep pattern for query examples: `diabetes|cancer|cardiovascular|clinical trial|systematic review`\n\n### 7. Search History and Saved Searches\n\nUse PubMed's search history and My NCBI features for efficient research workflows.\n\n**Search History** (via Advanced Search):\n- Maintains up to 100 searches\n- Expires after 8 hours of inactivity\n- Combine previous searches using # references\n- Preview result counts before executing\n\n**Example**:\n```\n#1: diabetes mellitus[mh]\n#2: cardiovascular diseases[mh]\n#3: #1 AND #2 AND risk factors[tiab]\n```\n\n**My NCBI Features**:\n- Save searches indefinitely\n- Set up email alerts for new matching articles\n- Create collections of saved articles\n- Organize research by project or topic\n\n**RSS Feeds**:\nCreate RSS feeds for any search to monitor new publications in your area of interest.\n\n### 8. Related Articles and Citation Discovery\n\nFind related research and explore citation networks.\n\n**Similar Articles Feature**:\nEvery PubMed article includes pre-calculated related articles based on:\n- Title and abstract similarity\n- MeSH term overlap\n- Weighted algorithmic matching\n\n**ELink for Related Data**:\n```\n# Find related articles programmatically\nelink.fcgi?dbfrom=pubmed&db=pubmed&id=PMID&cmd=neighbor\n```\n\n**Citation Links**:\n- LinkOut to full text from publishers\n- Links to PubMed Central free articles\n- Connections to related NCBI databases (GenBank, ClinicalTrials.gov, etc.)\n\n### 9. Export and Citation Management\n\nExport search results in various formats for citation management and further analysis.\n\n**Export Formats**:\n- .nbib files for reference managers (Zotero, Mendeley, EndNote)\n- AMA, MLA, APA, NLM citation styles\n- CSV for data analysis\n- XML for programmatic processing\n\n**Clipboard and Collections**:\n- Clipboard: Temporary storage for up to 500 items (8-hour expiration)\n- Collections: Permanent storage via My NCBI account\n\n**Batch Export via API**:\n```python\n# Export citations in MEDLINE format\nefetch.fcgi?db=pubmed&id=PMID1,PMID2&rettype=medline&retmode=text\n```\n\n## Working with Reference Files\n\nThis skill includes three comprehensive reference files in the `references/` directory:\n\n### references/api_reference.md\nComplete E-utilities API documentation including all nine endpoints, parameters, response formats, and best practices. Consult when:\n- Implementing programmatic PubMed access\n- Constructing API requests\n- Understanding rate limits and authentication\n- Working with large datasets via history server\n- Troubleshooting API errors\n\n### references/search_syntax.md\nDetailed guide to PubMed search syntax including field tags, Boolean operators, wildcards, and special characters. Consult when:\n- Constructing complex search queries\n- Understanding automatic term mapping\n- Using advanced search features (proximity, wildcards)\n- Applying filters and limits\n- Troubleshooting unexpected search results\n\n### references/common_queries.md\nExtensive collection of example queries for various research scenarios, disease types, and methodologies. Consult when:\n- Starting a new literature search\n- Need templates for specific research areas\n- Looking for best practice query patterns\n- Conducting systematic reviews\n- Searching for specific study designs or populations\n\n**Reference Loading Strategy**:\nLoad reference files into context as needed based on the specific task. For brief queries or basic searches, the information in this SKILL.md may be sufficient. For complex operations, consult the appropriate reference file.\n\n## Common Workflows\n\n### Workflow 1: Basic Literature Search\n\n1. Identify key concepts and synonyms\n2. Construct query with Boolean operators and field tags\n3. Review initial results and refine query\n4. Apply filters (date, article type, language)\n5. Export results for analysis\n\n### Workflow 2: Systematic Review Search\n\n1. Define research question using PICO framework\n2. Identify all relevant MeSH terms and synonyms\n3. Construct comprehensive search strategy\n4. Search multiple databases (include PubMed)\n5. Document search strategy and date\n6. Export results for screening and review\n\n### Workflow 3: Programmatic Data Extraction\n\n1. Design search query and test in web interface\n2. Implement search using ESearch API\n3. Use history server for large result sets\n4. Retrieve detailed records with EFetch\n5. Parse XML/JSON responses\n6. Store data locally with caching\n7. Implement rate limiting and error handling\n\n### Workflow 4: Citation Discovery\n\n1. Start with known relevant article\n2. Use Similar Articles to find related work\n3. Check citing articles (when available)\n4. Explore MeSH terms from relevant articles\n5. Construct new searches based on discoveries\n6. Use ELink to find related database entries\n\n### Workflow 5: Ongoing Literature Monitoring\n\n1. Construct comprehensive search query\n2. Test and refine query for precision\n3. Save search to My NCBI account\n4. Set up email alerts for new matches\n5. Create RSS feed for feed reader monitoring\n6. Review new articles regularly\n\n## Tips and Best Practices\n\n### Search Strategy\n- Start broad, then narrow with field tags and filters\n- Include synonyms and MeSH terms for comprehensive coverage\n- Use quotation marks for exact phrases\n- Check Search Details in Advanced Search to verify query translation\n- Combine multiple searches using search history\n\n### API Usage\n- Obtain API key for higher rate limits (10 req/sec vs 3 req/sec)\n- Use history server for result sets > 500 articles\n- Implement exponential backoff for rate limit handling\n- Cache results locally to minimize redundant requests\n- Always include descriptive User-Agent header\n\n### Quality Filtering\n- Prefer systematic reviews and meta-analyses for synthesized evidence\n- Use publication type filters to find specific study designs\n- Filter by date for most recent research\n- Apply language filters as appropriate\n- Use free full text filter for immediate access\n\n### Citation Management\n- Export early and often to avoid losing search results\n- Use .nbib format for compatibility with most reference managers\n- Create My NCBI account for permanent collections\n- Document search strategies for reproducibility\n- Use Collections to organize research by project\n\n## Limitations and Considerations\n\n### Database Coverage\n- Primarily biomedical and life sciences literature\n- Pre-1975 articles often lack abstracts\n- Full author names available from 2002 forward\n- Non-English abstracts available but may default to English display\n\n### Search Limitations\n- Display limited to 10,000 results maximum\n- Search history expires after 8 hours of inactivity\n- Clipboard holds max 500 items with 8-hour expiration\n- Automatic term mapping may produce unexpected results\n\n### API Considerations\n- Rate limits apply (3-10 requests/second)\n- Large queries may time out (use history server)\n- XML parsing required for detailed data extraction\n- API key recommended for production use\n\n### Access Limitations\n- PubMed provides citations and abstracts (not always full text)\n- Full text access depends on publisher, institutional access, or open access status\n- LinkOut availability varies by journal and institution\n- Some content requires subscription or payment\n\n## Support Resources\n\n- **PubMed Help**: https://pubmed.ncbi.nlm.nih.gov/help/\n- **E-utilities Documentation**: https://www.ncbi.nlm.nih.gov/books/NBK25501/\n- **NLM Help Desk**: 1-888-FIND-NLM (1-888-346-3656)\n- **Technical Support**: vog.hin.mln.ibcn@seitilitue\n- **Mailing List**: utilities-announce@ncbi.nlm.nih.gov\n",
        "data/k-dense-ai/pufferlib/SKILL.md": "---\nname: pufferlib\ndescription: This skill should be used when working with reinforcement learning tasks including high-performance RL training, custom environment development, vectorized parallel simulation, multi-agent systems, or integration with existing RL environments (Gymnasium, PettingZoo, Atari, Procgen, etc.). Use this skill for implementing PPO training, creating PufferEnv environments, optimizing RL performance, or developing policies with CNNs/LSTMs.\n---\n\n# PufferLib - High-Performance Reinforcement Learning\n\n## Overview\n\nPufferLib is a high-performance reinforcement learning library designed for fast parallel environment simulation and training. It achieves training at millions of steps per second through optimized vectorization, native multi-agent support, and efficient PPO implementation (PuffeRL). The library provides the Ocean suite of 20+ environments and seamless integration with Gymnasium, PettingZoo, and specialized RL frameworks.\n\n## When to Use This Skill\n\nUse this skill when:\n- **Training RL agents** with PPO on any environment (single or multi-agent)\n- **Creating custom environments** using the PufferEnv API\n- **Optimizing performance** for parallel environment simulation (vectorization)\n- **Integrating existing environments** from Gymnasium, PettingZoo, Atari, Procgen, etc.\n- **Developing policies** with CNN, LSTM, or custom architectures\n- **Scaling RL** to millions of steps per second for faster experimentation\n- **Multi-agent RL** with native multi-agent environment support\n\n## Core Capabilities\n\n### 1. High-Performance Training (PuffeRL)\n\nPuffeRL is PufferLib's optimized PPO+LSTM training algorithm achieving 1M-4M steps/second.\n\n**Quick start training:**\n```bash\n# CLI training\npuffer train procgen-coinrun --train.device cuda --train.learning-rate 3e-4\n\n# Distributed training\ntorchrun --nproc_per_node=4 train.py\n```\n\n**Python training loop:**\n```python\nimport pufferlib\nfrom pufferlib import PuffeRL\n\n# Create vectorized environment\nenv = pufferlib.make('procgen-coinrun', num_envs=256)\n\n# Create trainer\ntrainer = PuffeRL(\n    env=env,\n    policy=my_policy,\n    device='cuda',\n    learning_rate=3e-4,\n    batch_size=32768\n)\n\n# Training loop\nfor iteration in range(num_iterations):\n    trainer.evaluate()  # Collect rollouts\n    trainer.train()     # Train on batch\n    trainer.mean_and_log()  # Log results\n```\n\n**For comprehensive training guidance**, read `references/training.md` for:\n- Complete training workflow and CLI options\n- Hyperparameter tuning with Protein\n- Distributed multi-GPU/multi-node training\n- Logger integration (Weights & Biases, Neptune)\n- Checkpointing and resume training\n- Performance optimization tips\n- Curriculum learning patterns\n\n### 2. Environment Development (PufferEnv)\n\nCreate custom high-performance environments with the PufferEnv API.\n\n**Basic environment structure:**\n```python\nimport numpy as np\nfrom pufferlib import PufferEnv\n\nclass MyEnvironment(PufferEnv):\n    def __init__(self, buf=None):\n        super().__init__(buf)\n\n        # Define spaces\n        self.observation_space = self.make_space((4,))\n        self.action_space = self.make_discrete(4)\n\n        self.reset()\n\n    def reset(self):\n        # Reset state and return initial observation\n        return np.zeros(4, dtype=np.float32)\n\n    def step(self, action):\n        # Execute action, compute reward, check done\n        obs = self._get_observation()\n        reward = self._compute_reward()\n        done = self._is_done()\n        info = {}\n\n        return obs, reward, done, info\n```\n\n**Use the template script:** `scripts/env_template.py` provides complete single-agent and multi-agent environment templates with examples of:\n- Different observation space types (vector, image, dict)\n- Action space variations (discrete, continuous, multi-discrete)\n- Multi-agent environment structure\n- Testing utilities\n\n**For complete environment development**, read `references/environments.md` for:\n- PufferEnv API details and in-place operation patterns\n- Observation and action space definitions\n- Multi-agent environment creation\n- Ocean suite (20+ pre-built environments)\n- Performance optimization (Python to C workflow)\n- Environment wrappers and best practices\n- Debugging and validation techniques\n\n### 3. Vectorization and Performance\n\nAchieve maximum throughput with optimized parallel simulation.\n\n**Vectorization setup:**\n```python\nimport pufferlib\n\n# Automatic vectorization\nenv = pufferlib.make('environment_name', num_envs=256, num_workers=8)\n\n# Performance benchmarks:\n# - Pure Python envs: 100k-500k SPS\n# - C-based envs: 100M+ SPS\n# - With training: 400k-4M total SPS\n```\n\n**Key optimizations:**\n- Shared memory buffers for zero-copy observation passing\n- Busy-wait flags instead of pipes/queues\n- Surplus environments for async returns\n- Multiple environments per worker\n\n**For vectorization optimization**, read `references/vectorization.md` for:\n- Architecture and performance characteristics\n- Worker and batch size configuration\n- Serial vs multiprocessing vs async modes\n- Shared memory and zero-copy patterns\n- Hierarchical vectorization for large scale\n- Multi-agent vectorization strategies\n- Performance profiling and troubleshooting\n\n### 4. Policy Development\n\nBuild policies as standard PyTorch modules with optional utilities.\n\n**Basic policy structure:**\n```python\nimport torch.nn as nn\nfrom pufferlib.pytorch import layer_init\n\nclass Policy(nn.Module):\n    def __init__(self, observation_space, action_space):\n        super().__init__()\n\n        # Encoder\n        self.encoder = nn.Sequential(\n            layer_init(nn.Linear(obs_dim, 256)),\n            nn.ReLU(),\n            layer_init(nn.Linear(256, 256)),\n            nn.ReLU()\n        )\n\n        # Actor and critic heads\n        self.actor = layer_init(nn.Linear(256, num_actions), std=0.01)\n        self.critic = layer_init(nn.Linear(256, 1), std=1.0)\n\n    def forward(self, observations):\n        features = self.encoder(observations)\n        return self.actor(features), self.critic(features)\n```\n\n**For complete policy development**, read `references/policies.md` for:\n- CNN policies for image observations\n- Recurrent policies with optimized LSTM (3x faster inference)\n- Multi-input policies for complex observations\n- Continuous action policies\n- Multi-agent policies (shared vs independent parameters)\n- Advanced architectures (attention, residual)\n- Observation normalization and gradient clipping\n- Policy debugging and testing\n\n### 5. Environment Integration\n\nSeamlessly integrate environments from popular RL frameworks.\n\n**Gymnasium integration:**\n```python\nimport gymnasium as gym\nimport pufferlib\n\n# Wrap Gymnasium environment\ngym_env = gym.make('CartPole-v1')\nenv = pufferlib.emulate(gym_env, num_envs=256)\n\n# Or use make directly\nenv = pufferlib.make('gym-CartPole-v1', num_envs=256)\n```\n\n**PettingZoo multi-agent:**\n```python\n# Multi-agent environment\nenv = pufferlib.make('pettingzoo-knights-archers-zombies', num_envs=128)\n```\n\n**Supported frameworks:**\n- Gymnasium / OpenAI Gym\n- PettingZoo (parallel and AEC)\n- Atari (ALE)\n- Procgen\n- NetHack / MiniHack\n- Minigrid\n- Neural MMO\n- Crafter\n- GPUDrive\n- MicroRTS\n- Griddly\n- And more...\n\n**For integration details**, read `references/integration.md` for:\n- Complete integration examples for each framework\n- Custom wrappers (observation, reward, frame stacking, action repeat)\n- Space flattening and unflattening\n- Environment registration\n- Compatibility patterns\n- Performance considerations\n- Integration debugging\n\n## Quick Start Workflow\n\n### For Training Existing Environments\n\n1. Choose environment from Ocean suite or compatible framework\n2. Use `scripts/train_template.py` as starting point\n3. Configure hyperparameters for your task\n4. Run training with CLI or Python script\n5. Monitor with Weights & Biases or Neptune\n6. Refer to `references/training.md` for optimization\n\n### For Creating Custom Environments\n\n1. Start with `scripts/env_template.py`\n2. Define observation and action spaces\n3. Implement `reset()` and `step()` methods\n4. Test environment locally\n5. Vectorize with `pufferlib.emulate()` or `make()`\n6. Refer to `references/environments.md` for advanced patterns\n7. Optimize with `references/vectorization.md` if needed\n\n### For Policy Development\n\n1. Choose architecture based on observations:\n   - Vector observations  MLP policy\n   - Image observations  CNN policy\n   - Sequential tasks  LSTM policy\n   - Complex observations  Multi-input policy\n2. Use `layer_init` for proper weight initialization\n3. Follow patterns in `references/policies.md`\n4. Test with environment before full training\n\n### For Performance Optimization\n\n1. Profile current throughput (steps per second)\n2. Check vectorization configuration (num_envs, num_workers)\n3. Optimize environment code (in-place ops, numpy vectorization)\n4. Consider C implementation for critical paths\n5. Use `references/vectorization.md` for systematic optimization\n\n## Resources\n\n### scripts/\n\n**train_template.py** - Complete training script template with:\n- Environment creation and configuration\n- Policy initialization\n- Logger integration (WandB, Neptune)\n- Training loop with checkpointing\n- Command-line argument parsing\n- Multi-GPU distributed training setup\n\n**env_template.py** - Environment implementation templates:\n- Single-agent PufferEnv example (grid world)\n- Multi-agent PufferEnv example (cooperative navigation)\n- Multiple observation/action space patterns\n- Testing utilities\n\n### references/\n\n**training.md** - Comprehensive training guide:\n- Training workflow and CLI options\n- Hyperparameter configuration\n- Distributed training (multi-GPU, multi-node)\n- Monitoring and logging\n- Checkpointing\n- Protein hyperparameter tuning\n- Performance optimization\n- Common training patterns\n- Troubleshooting\n\n**environments.md** - Environment development guide:\n- PufferEnv API and characteristics\n- Observation and action spaces\n- Multi-agent environments\n- Ocean suite environments\n- Custom environment development workflow\n- Python to C optimization path\n- Third-party environment integration\n- Wrappers and best practices\n- Debugging\n\n**vectorization.md** - Vectorization optimization:\n- Architecture and key optimizations\n- Vectorization modes (serial, multiprocessing, async)\n- Worker and batch configuration\n- Shared memory and zero-copy patterns\n- Advanced vectorization (hierarchical, custom)\n- Multi-agent vectorization\n- Performance monitoring and profiling\n- Troubleshooting and best practices\n\n**policies.md** - Policy architecture guide:\n- Basic policy structure\n- CNN policies for images\n- LSTM policies with optimization\n- Multi-input policies\n- Continuous action policies\n- Multi-agent policies\n- Advanced architectures (attention, residual)\n- Observation processing and unflattening\n- Initialization and normalization\n- Debugging and testing\n\n**integration.md** - Framework integration guide:\n- Gymnasium integration\n- PettingZoo integration (parallel and AEC)\n- Third-party environments (Procgen, NetHack, Minigrid, etc.)\n- Custom wrappers (observation, reward, frame stacking, etc.)\n- Space conversion and unflattening\n- Environment registration\n- Compatibility patterns\n- Performance considerations\n- Debugging integration\n\n## Tips for Success\n\n1. **Start simple**: Begin with Ocean environments or Gymnasium integration before creating custom environments\n\n2. **Profile early**: Measure steps per second from the start to identify bottlenecks\n\n3. **Use templates**: `scripts/train_template.py` and `scripts/env_template.py` provide solid starting points\n\n4. **Read references as needed**: Each reference file is self-contained and focused on a specific capability\n\n5. **Optimize progressively**: Start with Python, profile, then optimize critical paths with C if needed\n\n6. **Leverage vectorization**: PufferLib's vectorization is key to achieving high throughput\n\n7. **Monitor training**: Use WandB or Neptune to track experiments and identify issues early\n\n8. **Test environments**: Validate environment logic before scaling up training\n\n9. **Check existing environments**: Ocean suite provides 20+ pre-built environments\n\n10. **Use proper initialization**: Always use `layer_init` from `pufferlib.pytorch` for policies\n\n## Common Use Cases\n\n### Training on Standard Benchmarks\n```python\n# Atari\nenv = pufferlib.make('atari-pong', num_envs=256)\n\n# Procgen\nenv = pufferlib.make('procgen-coinrun', num_envs=256)\n\n# Minigrid\nenv = pufferlib.make('minigrid-empty-8x8', num_envs=256)\n```\n\n### Multi-Agent Learning\n```python\n# PettingZoo\nenv = pufferlib.make('pettingzoo-pistonball', num_envs=128)\n\n# Shared policy for all agents\npolicy = create_policy(env.observation_space, env.action_space)\ntrainer = PuffeRL(env=env, policy=policy)\n```\n\n### Custom Task Development\n```python\n# Create custom environment\nclass MyTask(PufferEnv):\n    # ... implement environment ...\n\n# Vectorize and train\nenv = pufferlib.emulate(MyTask, num_envs=256)\ntrainer = PuffeRL(env=env, policy=my_policy)\n```\n\n### High-Performance Optimization\n```python\n# Maximize throughput\nenv = pufferlib.make(\n    'my-env',\n    num_envs=1024,      # Large batch\n    num_workers=16,     # Many workers\n    envs_per_worker=64  # Optimize per worker\n)\n```\n\n## Installation\n\n```bash\nuv pip install pufferlib\n```\n\n## Documentation\n\n- Official docs: https://puffer.ai/docs.html\n- GitHub: https://github.com/PufferAI/PufferLib\n- Discord: Community support available\n",
        "data/k-dense-ai/pydeseq2/SKILL.md": "---\nname: pydeseq2\ndescription: \"Differential gene expression analysis (Python DESeq2). Identify DE genes from bulk RNA-seq counts, Wald tests, FDR correction, volcano/MA plots, for RNA-seq analysis.\"\n---\n\n# PyDESeq2\n\n## Overview\n\nPyDESeq2 is a Python implementation of DESeq2 for differential expression analysis with bulk RNA-seq data. Design and execute complete workflows from data loading through result interpretation, including single-factor and multi-factor designs, Wald tests with multiple testing correction, optional apeGLM shrinkage, and integration with pandas and AnnData.\n\n## When to Use This Skill\n\nThis skill should be used when:\n- Analyzing bulk RNA-seq count data for differential expression\n- Comparing gene expression between experimental conditions (e.g., treated vs control)\n- Performing multi-factor designs accounting for batch effects or covariates\n- Converting R-based DESeq2 workflows to Python\n- Integrating differential expression analysis into Python-based pipelines\n- Users mention \"DESeq2\", \"differential expression\", \"RNA-seq analysis\", or \"PyDESeq2\"\n\n## Quick Start Workflow\n\nFor users who want to perform a standard differential expression analysis:\n\n```python\nimport pandas as pd\nfrom pydeseq2.dds import DeseqDataSet\nfrom pydeseq2.ds import DeseqStats\n\n# 1. Load data\ncounts_df = pd.read_csv(\"counts.csv\", index_col=0).T  # Transpose to samples  genes\nmetadata = pd.read_csv(\"metadata.csv\", index_col=0)\n\n# 2. Filter low-count genes\ngenes_to_keep = counts_df.columns[counts_df.sum(axis=0) >= 10]\ncounts_df = counts_df[genes_to_keep]\n\n# 3. Initialize and fit DESeq2\ndds = DeseqDataSet(\n    counts=counts_df,\n    metadata=metadata,\n    design=\"~condition\",\n    refit_cooks=True\n)\ndds.deseq2()\n\n# 4. Perform statistical testing\nds = DeseqStats(dds, contrast=[\"condition\", \"treated\", \"control\"])\nds.summary()\n\n# 5. Access results\nresults = ds.results_df\nsignificant = results[results.padj < 0.05]\nprint(f\"Found {len(significant)} significant genes\")\n```\n\n## Core Workflow Steps\n\n### Step 1: Data Preparation\n\n**Input requirements:**\n- **Count matrix:** Samples  genes DataFrame with non-negative integer read counts\n- **Metadata:** Samples  variables DataFrame with experimental factors\n\n**Common data loading patterns:**\n\n```python\n# From CSV (typical format: genes  samples, needs transpose)\ncounts_df = pd.read_csv(\"counts.csv\", index_col=0).T\nmetadata = pd.read_csv(\"metadata.csv\", index_col=0)\n\n# From TSV\ncounts_df = pd.read_csv(\"counts.tsv\", sep=\"\\t\", index_col=0).T\n\n# From AnnData\nimport anndata as ad\nadata = ad.read_h5ad(\"data.h5ad\")\ncounts_df = pd.DataFrame(adata.X, index=adata.obs_names, columns=adata.var_names)\nmetadata = adata.obs\n```\n\n**Data filtering:**\n\n```python\n# Remove low-count genes\ngenes_to_keep = counts_df.columns[counts_df.sum(axis=0) >= 10]\ncounts_df = counts_df[genes_to_keep]\n\n# Remove samples with missing metadata\nsamples_to_keep = ~metadata.condition.isna()\ncounts_df = counts_df.loc[samples_to_keep]\nmetadata = metadata.loc[samples_to_keep]\n```\n\n### Step 2: Design Specification\n\nThe design formula specifies how gene expression is modeled.\n\n**Single-factor designs:**\n```python\ndesign = \"~condition\"  # Simple two-group comparison\n```\n\n**Multi-factor designs:**\n```python\ndesign = \"~batch + condition\"  # Control for batch effects\ndesign = \"~age + condition\"     # Include continuous covariate\ndesign = \"~group + condition + group:condition\"  # Interaction effects\n```\n\n**Design formula guidelines:**\n- Use Wilkinson formula notation (R-style)\n- Put adjustment variables (e.g., batch) before the main variable of interest\n- Ensure variables exist as columns in the metadata DataFrame\n- Use appropriate data types (categorical for discrete variables)\n\n### Step 3: DESeq2 Fitting\n\nInitialize the DeseqDataSet and run the complete pipeline:\n\n```python\nfrom pydeseq2.dds import DeseqDataSet\n\ndds = DeseqDataSet(\n    counts=counts_df,\n    metadata=metadata,\n    design=\"~condition\",\n    refit_cooks=True,  # Refit after removing outliers\n    n_cpus=1           # Parallel processing (adjust as needed)\n)\n\n# Run the complete DESeq2 pipeline\ndds.deseq2()\n```\n\n**What `deseq2()` does:**\n1. Computes size factors (normalization)\n2. Fits genewise dispersions\n3. Fits dispersion trend curve\n4. Computes dispersion priors\n5. Fits MAP dispersions (shrinkage)\n6. Fits log fold changes\n7. Calculates Cook's distances (outlier detection)\n8. Refits if outliers detected (optional)\n\n### Step 4: Statistical Testing\n\nPerform Wald tests to identify differentially expressed genes:\n\n```python\nfrom pydeseq2.ds import DeseqStats\n\nds = DeseqStats(\n    dds,\n    contrast=[\"condition\", \"treated\", \"control\"],  # Test treated vs control\n    alpha=0.05,                # Significance threshold\n    cooks_filter=True,         # Filter outliers\n    independent_filter=True    # Filter low-power tests\n)\n\nds.summary()\n```\n\n**Contrast specification:**\n- Format: `[variable, test_level, reference_level]`\n- Example: `[\"condition\", \"treated\", \"control\"]` tests treated vs control\n- If `None`, uses the last coefficient in the design\n\n**Result DataFrame columns:**\n- `baseMean`: Mean normalized count across samples\n- `log2FoldChange`: Log2 fold change between conditions\n- `lfcSE`: Standard error of LFC\n- `stat`: Wald test statistic\n- `pvalue`: Raw p-value\n- `padj`: Adjusted p-value (FDR-corrected via Benjamini-Hochberg)\n\n### Step 5: Optional LFC Shrinkage\n\nApply shrinkage to reduce noise in fold change estimates:\n\n```python\nds.lfc_shrink()  # Applies apeGLM shrinkage\n```\n\n**When to use LFC shrinkage:**\n- For visualization (volcano plots, heatmaps)\n- For ranking genes by effect size\n- When prioritizing genes for follow-up experiments\n\n**Important:** Shrinkage affects only the log2FoldChange values, not the statistical test results (p-values remain unchanged). Use shrunk values for visualization but report unshrunken p-values for significance.\n\n### Step 6: Result Export\n\nSave results and intermediate objects:\n\n```python\nimport pickle\n\n# Export results as CSV\nds.results_df.to_csv(\"deseq2_results.csv\")\n\n# Save significant genes only\nsignificant = ds.results_df[ds.results_df.padj < 0.05]\nsignificant.to_csv(\"significant_genes.csv\")\n\n# Save DeseqDataSet for later use\nwith open(\"dds_result.pkl\", \"wb\") as f:\n    pickle.dump(dds.to_picklable_anndata(), f)\n```\n\n## Common Analysis Patterns\n\n### Two-Group Comparison\n\nStandard case-control comparison:\n\n```python\ndds = DeseqDataSet(counts=counts_df, metadata=metadata, design=\"~condition\")\ndds.deseq2()\n\nds = DeseqStats(dds, contrast=[\"condition\", \"treated\", \"control\"])\nds.summary()\n\nresults = ds.results_df\nsignificant = results[results.padj < 0.05]\n```\n\n### Multiple Comparisons\n\nTesting multiple treatment groups against control:\n\n```python\ndds = DeseqDataSet(counts=counts_df, metadata=metadata, design=\"~condition\")\ndds.deseq2()\n\ntreatments = [\"treatment_A\", \"treatment_B\", \"treatment_C\"]\nall_results = {}\n\nfor treatment in treatments:\n    ds = DeseqStats(dds, contrast=[\"condition\", treatment, \"control\"])\n    ds.summary()\n    all_results[treatment] = ds.results_df\n\n    sig_count = len(ds.results_df[ds.results_df.padj < 0.05])\n    print(f\"{treatment}: {sig_count} significant genes\")\n```\n\n### Accounting for Batch Effects\n\nControl for technical variation:\n\n```python\n# Include batch in design\ndds = DeseqDataSet(counts=counts_df, metadata=metadata, design=\"~batch + condition\")\ndds.deseq2()\n\n# Test condition while controlling for batch\nds = DeseqStats(dds, contrast=[\"condition\", \"treated\", \"control\"])\nds.summary()\n```\n\n### Continuous Covariates\n\nInclude continuous variables like age or dosage:\n\n```python\n# Ensure continuous variable is numeric\nmetadata[\"age\"] = pd.to_numeric(metadata[\"age\"])\n\ndds = DeseqDataSet(counts=counts_df, metadata=metadata, design=\"~age + condition\")\ndds.deseq2()\n\nds = DeseqStats(dds, contrast=[\"condition\", \"treated\", \"control\"])\nds.summary()\n```\n\n## Using the Analysis Script\n\nThis skill includes a complete command-line script for standard analyses:\n\n```bash\n# Basic usage\npython scripts/run_deseq2_analysis.py \\\n  --counts counts.csv \\\n  --metadata metadata.csv \\\n  --design \"~condition\" \\\n  --contrast condition treated control \\\n  --output results/\n\n# With additional options\npython scripts/run_deseq2_analysis.py \\\n  --counts counts.csv \\\n  --metadata metadata.csv \\\n  --design \"~batch + condition\" \\\n  --contrast condition treated control \\\n  --output results/ \\\n  --min-counts 10 \\\n  --alpha 0.05 \\\n  --n-cpus 4 \\\n  --plots\n```\n\n**Script features:**\n- Automatic data loading and validation\n- Gene and sample filtering\n- Complete DESeq2 pipeline execution\n- Statistical testing with customizable parameters\n- Result export (CSV, pickle)\n- Optional visualization (volcano and MA plots)\n\nRefer users to `scripts/run_deseq2_analysis.py` when they need a standalone analysis tool or want to batch process multiple datasets.\n\n## Result Interpretation\n\n### Identifying Significant Genes\n\n```python\n# Filter by adjusted p-value\nsignificant = ds.results_df[ds.results_df.padj < 0.05]\n\n# Filter by both significance and effect size\nsig_and_large = ds.results_df[\n    (ds.results_df.padj < 0.05) &\n    (abs(ds.results_df.log2FoldChange) > 1)\n]\n\n# Separate up- and down-regulated\nupregulated = significant[significant.log2FoldChange > 0]\ndownregulated = significant[significant.log2FoldChange < 0]\n\nprint(f\"Upregulated: {len(upregulated)}\")\nprint(f\"Downregulated: {len(downregulated)}\")\n```\n\n### Ranking and Sorting\n\n```python\n# Sort by adjusted p-value\ntop_by_padj = ds.results_df.sort_values(\"padj\").head(20)\n\n# Sort by absolute fold change (use shrunk values)\nds.lfc_shrink()\nds.results_df[\"abs_lfc\"] = abs(ds.results_df.log2FoldChange)\ntop_by_lfc = ds.results_df.sort_values(\"abs_lfc\", ascending=False).head(20)\n\n# Sort by a combined metric\nds.results_df[\"score\"] = -np.log10(ds.results_df.padj) * abs(ds.results_df.log2FoldChange)\ntop_combined = ds.results_df.sort_values(\"score\", ascending=False).head(20)\n```\n\n### Quality Metrics\n\n```python\n# Check normalization (size factors should be close to 1)\nprint(\"Size factors:\", dds.obsm[\"size_factors\"])\n\n# Examine dispersion estimates\nimport matplotlib.pyplot as plt\nplt.hist(dds.varm[\"dispersions\"], bins=50)\nplt.xlabel(\"Dispersion\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Dispersion Distribution\")\nplt.show()\n\n# Check p-value distribution (should be mostly flat with peak near 0)\nplt.hist(ds.results_df.pvalue.dropna(), bins=50)\nplt.xlabel(\"P-value\")\nplt.ylabel(\"Frequency\")\nplt.title(\"P-value Distribution\")\nplt.show()\n```\n\n## Visualization Guidelines\n\n### Volcano Plot\n\nVisualize significance vs effect size:\n\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nresults = ds.results_df.copy()\nresults[\"-log10(padj)\"] = -np.log10(results.padj)\n\nplt.figure(figsize=(10, 6))\nsignificant = results.padj < 0.05\n\nplt.scatter(\n    results.loc[~significant, \"log2FoldChange\"],\n    results.loc[~significant, \"-log10(padj)\"],\n    alpha=0.3, s=10, c='gray', label='Not significant'\n)\nplt.scatter(\n    results.loc[significant, \"log2FoldChange\"],\n    results.loc[significant, \"-log10(padj)\"],\n    alpha=0.6, s=10, c='red', label='padj < 0.05'\n)\n\nplt.axhline(-np.log10(0.05), color='blue', linestyle='--', alpha=0.5)\nplt.xlabel(\"Log2 Fold Change\")\nplt.ylabel(\"-Log10(Adjusted P-value)\")\nplt.title(\"Volcano Plot\")\nplt.legend()\nplt.savefig(\"volcano_plot.png\", dpi=300)\n```\n\n### MA Plot\n\nShow fold change vs mean expression:\n\n```python\nplt.figure(figsize=(10, 6))\n\nplt.scatter(\n    np.log10(results.loc[~significant, \"baseMean\"] + 1),\n    results.loc[~significant, \"log2FoldChange\"],\n    alpha=0.3, s=10, c='gray'\n)\nplt.scatter(\n    np.log10(results.loc[significant, \"baseMean\"] + 1),\n    results.loc[significant, \"log2FoldChange\"],\n    alpha=0.6, s=10, c='red'\n)\n\nplt.axhline(0, color='blue', linestyle='--', alpha=0.5)\nplt.xlabel(\"Log10(Base Mean + 1)\")\nplt.ylabel(\"Log2 Fold Change\")\nplt.title(\"MA Plot\")\nplt.savefig(\"ma_plot.png\", dpi=300)\n```\n\n## Troubleshooting Common Issues\n\n### Data Format Problems\n\n**Issue:** \"Index mismatch between counts and metadata\"\n\n**Solution:** Ensure sample names match exactly\n```python\nprint(\"Counts samples:\", counts_df.index.tolist())\nprint(\"Metadata samples:\", metadata.index.tolist())\n\n# Take intersection if needed\ncommon = counts_df.index.intersection(metadata.index)\ncounts_df = counts_df.loc[common]\nmetadata = metadata.loc[common]\n```\n\n**Issue:** \"All genes have zero counts\"\n\n**Solution:** Check if data needs transposition\n```python\nprint(f\"Counts shape: {counts_df.shape}\")\n# If genes > samples, transpose is needed\nif counts_df.shape[1] < counts_df.shape[0]:\n    counts_df = counts_df.T\n```\n\n### Design Matrix Issues\n\n**Issue:** \"Design matrix is not full rank\"\n\n**Cause:** Confounded variables (e.g., all treated samples in one batch)\n\n**Solution:** Remove confounded variable or add interaction term\n```python\n# Check confounding\nprint(pd.crosstab(metadata.condition, metadata.batch))\n\n# Either simplify design or add interaction\ndesign = \"~condition\"  # Remove batch\n# OR\ndesign = \"~condition + batch + condition:batch\"  # Model interaction\n```\n\n### No Significant Genes\n\n**Diagnostics:**\n```python\n# Check dispersion distribution\nplt.hist(dds.varm[\"dispersions\"], bins=50)\nplt.show()\n\n# Check size factors\nprint(dds.obsm[\"size_factors\"])\n\n# Look at top genes by raw p-value\nprint(ds.results_df.nsmallest(20, \"pvalue\"))\n```\n\n**Possible causes:**\n- Small effect sizes\n- High biological variability\n- Insufficient sample size\n- Technical issues (batch effects, outliers)\n\n## Reference Documentation\n\nFor comprehensive details beyond this workflow-oriented guide:\n\n- **API Reference** (`references/api_reference.md`): Complete documentation of PyDESeq2 classes, methods, and data structures. Use when needing detailed parameter information or understanding object attributes.\n\n- **Workflow Guide** (`references/workflow_guide.md`): In-depth guide covering complete analysis workflows, data loading patterns, multi-factor designs, troubleshooting, and best practices. Use when handling complex experimental designs or encountering issues.\n\nLoad these references into context when users need:\n- Detailed API documentation: `Read references/api_reference.md`\n- Comprehensive workflow examples: `Read references/workflow_guide.md`\n- Troubleshooting guidance: `Read references/workflow_guide.md` (see Troubleshooting section)\n\n## Key Reminders\n\n1. **Data orientation matters:** Count matrices typically load as genes  samples but need to be samples  genes. Always transpose with `.T` if needed.\n\n2. **Sample filtering:** Remove samples with missing metadata before analysis to avoid errors.\n\n3. **Gene filtering:** Filter low-count genes (e.g., < 10 total reads) to improve power and reduce computational time.\n\n4. **Design formula order:** Put adjustment variables before the variable of interest (e.g., `\"~batch + condition\"` not `\"~condition + batch\"`).\n\n5. **LFC shrinkage timing:** Apply shrinkage after statistical testing and only for visualization/ranking purposes. P-values remain based on unshrunken estimates.\n\n6. **Result interpretation:** Use `padj < 0.05` for significance, not raw p-values. The Benjamini-Hochberg procedure controls false discovery rate.\n\n7. **Contrast specification:** The format is `[variable, test_level, reference_level]` where test_level is compared against reference_level.\n\n8. **Save intermediate objects:** Use pickle to save DeseqDataSet objects for later use or additional analyses without re-running the expensive fitting step.\n\n## Installation and Requirements\n\n```bash\nuv pip install pydeseq2\n```\n\n**System requirements:**\n- Python 3.10-3.11\n- pandas 1.4.3+\n- numpy 1.23.0+\n- scipy 1.11.0+\n- scikit-learn 1.1.1+\n- anndata 0.8.0+\n\n**Optional for visualization:**\n- matplotlib\n- seaborn\n\n## Additional Resources\n\n- **Official Documentation:** https://pydeseq2.readthedocs.io\n- **GitHub Repository:** https://github.com/owkin/PyDESeq2\n- **Publication:** Muzellec et al. (2023) Bioinformatics, DOI: 10.1093/bioinformatics/btad547\n- **Original DESeq2 (R):** Love et al. (2014) Genome Biology, DOI: 10.1186/s13059-014-0550-8\n",
        "data/k-dense-ai/pydicom/SKILL.md": "---\nname: pydicom\ndescription: Python library for working with DICOM (Digital Imaging and Communications in Medicine) files. Use this skill when reading, writing, or modifying medical imaging data in DICOM format, extracting pixel data from medical images (CT, MRI, X-ray, ultrasound), anonymizing DICOM files, working with DICOM metadata and tags, converting DICOM images to other formats, handling compressed DICOM data, or processing medical imaging datasets. Applies to tasks involving medical image analysis, PACS systems, radiology workflows, and healthcare imaging applications.\n---\n\n# Pydicom\n\n## Overview\n\nPydicom is a pure Python package for working with DICOM files, the standard format for medical imaging data. This skill provides guidance on reading, writing, and manipulating DICOM files, including working with pixel data, metadata, and various compression formats.\n\n## When to Use This Skill\n\nUse this skill when working with:\n- Medical imaging files (CT, MRI, X-ray, ultrasound, PET, etc.)\n- DICOM datasets requiring metadata extraction or modification\n- Pixel data extraction and image processing from medical scans\n- DICOM anonymization for research or data sharing\n- Converting DICOM files to standard image formats\n- Compressed DICOM data requiring decompression\n- DICOM sequences and structured reports\n- Multi-slice volume reconstruction\n- PACS (Picture Archiving and Communication System) integration\n\n## Installation\n\nInstall pydicom and common dependencies:\n\n```bash\nuv pip install pydicom\nuv pip install pillow  # For image format conversion\nuv pip install numpy   # For pixel array manipulation\nuv pip install matplotlib  # For visualization\n```\n\nFor handling compressed DICOM files, additional packages may be needed:\n\n```bash\nuv pip install pylibjpeg pylibjpeg-libjpeg pylibjpeg-openjpeg  # JPEG compression\nuv pip install python-gdcm  # Alternative compression handler\n```\n\n## Core Workflows\n\n### Reading DICOM Files\n\nRead a DICOM file using `pydicom.dcmread()`:\n\n```python\nimport pydicom\n\n# Read a DICOM file\nds = pydicom.dcmread('path/to/file.dcm')\n\n# Access metadata\nprint(f\"Patient Name: {ds.PatientName}\")\nprint(f\"Study Date: {ds.StudyDate}\")\nprint(f\"Modality: {ds.Modality}\")\n\n# Display all elements\nprint(ds)\n```\n\n**Key points:**\n- `dcmread()` returns a `Dataset` object\n- Access data elements using attribute notation (e.g., `ds.PatientName`) or tag notation (e.g., `ds[0x0010, 0x0010]`)\n- Use `ds.file_meta` to access file metadata like Transfer Syntax UID\n- Handle missing attributes with `getattr(ds, 'AttributeName', default_value)` or `hasattr(ds, 'AttributeName')`\n\n### Working with Pixel Data\n\nExtract and manipulate image data from DICOM files:\n\n```python\nimport pydicom\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Read DICOM file\nds = pydicom.dcmread('image.dcm')\n\n# Get pixel array (requires numpy)\npixel_array = ds.pixel_array\n\n# Image information\nprint(f\"Shape: {pixel_array.shape}\")\nprint(f\"Data type: {pixel_array.dtype}\")\nprint(f\"Rows: {ds.Rows}, Columns: {ds.Columns}\")\n\n# Apply windowing for display (CT/MRI)\nif hasattr(ds, 'WindowCenter') and hasattr(ds, 'WindowWidth'):\n    from pydicom.pixel_data_handlers.util import apply_voi_lut\n    windowed_image = apply_voi_lut(pixel_array, ds)\nelse:\n    windowed_image = pixel_array\n\n# Display image\nplt.imshow(windowed_image, cmap='gray')\nplt.title(f\"{ds.Modality} - {ds.StudyDescription}\")\nplt.axis('off')\nplt.show()\n```\n\n**Working with color images:**\n\n```python\n# RGB images have shape (rows, columns, 3)\nif ds.PhotometricInterpretation == 'RGB':\n    rgb_image = ds.pixel_array\n    plt.imshow(rgb_image)\nelif ds.PhotometricInterpretation == 'YBR_FULL':\n    from pydicom.pixel_data_handlers.util import convert_color_space\n    rgb_image = convert_color_space(ds.pixel_array, 'YBR_FULL', 'RGB')\n    plt.imshow(rgb_image)\n```\n\n**Multi-frame images (videos/series):**\n\n```python\n# For multi-frame DICOM files\nif hasattr(ds, 'NumberOfFrames') and ds.NumberOfFrames > 1:\n    frames = ds.pixel_array  # Shape: (num_frames, rows, columns)\n    print(f\"Number of frames: {frames.shape[0]}\")\n\n    # Display specific frame\n    plt.imshow(frames[0], cmap='gray')\n```\n\n### Converting DICOM to Image Formats\n\nUse the provided `dicom_to_image.py` script or convert manually:\n\n```python\nfrom PIL import Image\nimport pydicom\nimport numpy as np\n\nds = pydicom.dcmread('input.dcm')\npixel_array = ds.pixel_array\n\n# Normalize to 0-255 range\nif pixel_array.dtype != np.uint8:\n    pixel_array = ((pixel_array - pixel_array.min()) /\n                   (pixel_array.max() - pixel_array.min()) * 255).astype(np.uint8)\n\n# Save as PNG\nimage = Image.fromarray(pixel_array)\nimage.save('output.png')\n```\n\nUse the script: `python scripts/dicom_to_image.py input.dcm output.png`\n\n### Modifying Metadata\n\nModify DICOM data elements:\n\n```python\nimport pydicom\nfrom datetime import datetime\n\nds = pydicom.dcmread('input.dcm')\n\n# Modify existing elements\nds.PatientName = \"Doe^John\"\nds.StudyDate = datetime.now().strftime('%Y%m%d')\nds.StudyDescription = \"Modified Study\"\n\n# Add new elements\nds.SeriesNumber = 1\nds.SeriesDescription = \"New Series\"\n\n# Remove elements\nif hasattr(ds, 'PatientComments'):\n    delattr(ds, 'PatientComments')\n# Or using del\nif 'PatientComments' in ds:\n    del ds.PatientComments\n\n# Save modified file\nds.save_as('modified.dcm')\n```\n\n### Anonymizing DICOM Files\n\nRemove or replace patient identifiable information:\n\n```python\nimport pydicom\nfrom datetime import datetime\n\nds = pydicom.dcmread('input.dcm')\n\n# Tags commonly containing PHI (Protected Health Information)\ntags_to_anonymize = [\n    'PatientName', 'PatientID', 'PatientBirthDate',\n    'PatientSex', 'PatientAge', 'PatientAddress',\n    'InstitutionName', 'InstitutionAddress',\n    'ReferringPhysicianName', 'PerformingPhysicianName',\n    'OperatorsName', 'StudyDescription', 'SeriesDescription',\n]\n\n# Remove or replace sensitive data\nfor tag in tags_to_anonymize:\n    if hasattr(ds, tag):\n        if tag in ['PatientName', 'PatientID']:\n            setattr(ds, tag, 'ANONYMOUS')\n        elif tag == 'PatientBirthDate':\n            setattr(ds, tag, '19000101')\n        else:\n            delattr(ds, tag)\n\n# Update dates to maintain temporal relationships\nif hasattr(ds, 'StudyDate'):\n    # Shift dates by a random offset\n    ds.StudyDate = '20000101'\n\n# Keep pixel data intact\nds.save_as('anonymized.dcm')\n```\n\nUse the provided script: `python scripts/anonymize_dicom.py input.dcm output.dcm`\n\n### Writing DICOM Files\n\nCreate DICOM files from scratch:\n\n```python\nimport pydicom\nfrom pydicom.dataset import Dataset, FileDataset\nfrom datetime import datetime\nimport numpy as np\n\n# Create file meta information\nfile_meta = Dataset()\nfile_meta.MediaStorageSOPClassUID = pydicom.uid.generate_uid()\nfile_meta.MediaStorageSOPInstanceUID = pydicom.uid.generate_uid()\nfile_meta.TransferSyntaxUID = pydicom.uid.ExplicitVRLittleEndian\n\n# Create the FileDataset instance\nds = FileDataset('new_dicom.dcm', {}, file_meta=file_meta, preamble=b\"\\0\" * 128)\n\n# Add required DICOM elements\nds.PatientName = \"Test^Patient\"\nds.PatientID = \"123456\"\nds.Modality = \"CT\"\nds.StudyDate = datetime.now().strftime('%Y%m%d')\nds.StudyTime = datetime.now().strftime('%H%M%S')\nds.ContentDate = ds.StudyDate\nds.ContentTime = ds.StudyTime\n\n# Add image-specific elements\nds.SamplesPerPixel = 1\nds.PhotometricInterpretation = \"MONOCHROME2\"\nds.Rows = 512\nds.Columns = 512\nds.BitsAllocated = 16\nds.BitsStored = 16\nds.HighBit = 15\nds.PixelRepresentation = 0\n\n# Create pixel data\npixel_array = np.random.randint(0, 4096, (512, 512), dtype=np.uint16)\nds.PixelData = pixel_array.tobytes()\n\n# Add required UIDs\nds.SOPClassUID = pydicom.uid.CTImageStorage\nds.SOPInstanceUID = file_meta.MediaStorageSOPInstanceUID\nds.SeriesInstanceUID = pydicom.uid.generate_uid()\nds.StudyInstanceUID = pydicom.uid.generate_uid()\n\n# Save the file\nds.save_as('new_dicom.dcm')\n```\n\n### Compression and Decompression\n\nHandle compressed DICOM files:\n\n```python\nimport pydicom\n\n# Read compressed DICOM file\nds = pydicom.dcmread('compressed.dcm')\n\n# Check transfer syntax\nprint(f\"Transfer Syntax: {ds.file_meta.TransferSyntaxUID}\")\nprint(f\"Transfer Syntax Name: {ds.file_meta.TransferSyntaxUID.name}\")\n\n# Decompress and save as uncompressed\nds.decompress()\nds.save_as('uncompressed.dcm', write_like_original=False)\n\n# Or compress when saving (requires appropriate encoder)\nds_uncompressed = pydicom.dcmread('uncompressed.dcm')\nds_uncompressed.compress(pydicom.uid.JPEGBaseline8Bit)\nds_uncompressed.save_as('compressed_jpeg.dcm')\n```\n\n**Common transfer syntaxes:**\n- `ExplicitVRLittleEndian` - Uncompressed, most common\n- `JPEGBaseline8Bit` - JPEG lossy compression\n- `JPEGLossless` - JPEG lossless compression\n- `JPEG2000Lossless` - JPEG 2000 lossless\n- `RLELossless` - Run-Length Encoding lossless\n\nSee `references/transfer_syntaxes.md` for complete list.\n\n### Working with DICOM Sequences\n\nHandle nested data structures:\n\n```python\nimport pydicom\n\nds = pydicom.dcmread('file.dcm')\n\n# Access sequences\nif 'ReferencedStudySequence' in ds:\n    for item in ds.ReferencedStudySequence:\n        print(f\"Referenced SOP Instance UID: {item.ReferencedSOPInstanceUID}\")\n\n# Create a sequence\nfrom pydicom.sequence import Sequence\n\nsequence_item = Dataset()\nsequence_item.ReferencedSOPClassUID = pydicom.uid.CTImageStorage\nsequence_item.ReferencedSOPInstanceUID = pydicom.uid.generate_uid()\n\nds.ReferencedImageSequence = Sequence([sequence_item])\n```\n\n### Processing DICOM Series\n\nWork with multiple related DICOM files:\n\n```python\nimport pydicom\nimport numpy as np\nfrom pathlib import Path\n\n# Read all DICOM files in a directory\ndicom_dir = Path('dicom_series/')\nslices = []\n\nfor file_path in dicom_dir.glob('*.dcm'):\n    ds = pydicom.dcmread(file_path)\n    slices.append(ds)\n\n# Sort by slice location or instance number\nslices.sort(key=lambda x: float(x.ImagePositionPatient[2]))\n# Or: slices.sort(key=lambda x: int(x.InstanceNumber))\n\n# Create 3D volume\nvolume = np.stack([s.pixel_array for s in slices])\nprint(f\"Volume shape: {volume.shape}\")  # (num_slices, rows, columns)\n\n# Get spacing information for proper scaling\npixel_spacing = slices[0].PixelSpacing  # [row_spacing, col_spacing]\nslice_thickness = slices[0].SliceThickness\nprint(f\"Voxel size: {pixel_spacing[0]}x{pixel_spacing[1]}x{slice_thickness} mm\")\n```\n\n## Helper Scripts\n\nThis skill includes utility scripts in the `scripts/` directory:\n\n### anonymize_dicom.py\nAnonymize DICOM files by removing or replacing Protected Health Information (PHI).\n\n```bash\npython scripts/anonymize_dicom.py input.dcm output.dcm\n```\n\n### dicom_to_image.py\nConvert DICOM files to common image formats (PNG, JPEG, TIFF).\n\n```bash\npython scripts/dicom_to_image.py input.dcm output.png\npython scripts/dicom_to_image.py input.dcm output.jpg --format JPEG\n```\n\n### extract_metadata.py\nExtract and display DICOM metadata in a readable format.\n\n```bash\npython scripts/extract_metadata.py file.dcm\npython scripts/extract_metadata.py file.dcm --output metadata.txt\n```\n\n## Reference Materials\n\nDetailed reference information is available in the `references/` directory:\n\n- **common_tags.md**: Comprehensive list of commonly used DICOM tags organized by category (Patient, Study, Series, Image, etc.)\n- **transfer_syntaxes.md**: Complete reference of DICOM transfer syntaxes and compression formats\n\n## Common Issues and Solutions\n\n**Issue: \"Unable to decode pixel data\"**\n- Solution: Install additional compression handlers: `uv pip install pylibjpeg pylibjpeg-libjpeg python-gdcm`\n\n**Issue: \"AttributeError\" when accessing tags**\n- Solution: Check if attribute exists with `hasattr(ds, 'AttributeName')` or use `ds.get('AttributeName', default)`\n\n**Issue: Incorrect image display (too dark/bright)**\n- Solution: Apply VOI LUT windowing: `apply_voi_lut(pixel_array, ds)` or manually adjust with `WindowCenter` and `WindowWidth`\n\n**Issue: Memory issues with large series**\n- Solution: Process files iteratively, use memory-mapped arrays, or downsample images\n\n## Best Practices\n\n1. **Always check for required attributes** before accessing them using `hasattr()` or `get()`\n2. **Preserve file metadata** when modifying files by using `save_as()` with `write_like_original=True`\n3. **Use Transfer Syntax UIDs** to understand compression format before processing pixel data\n4. **Handle exceptions** when reading files from untrusted sources\n5. **Apply proper windowing** (VOI LUT) for medical image visualization\n6. **Maintain spatial information** (pixel spacing, slice thickness) when processing 3D volumes\n7. **Verify anonymization** thoroughly before sharing medical data\n8. **Use UIDs correctly** - generate new UIDs when creating new instances, preserve them when modifying\n\n## Documentation\n\nOfficial pydicom documentation: https://pydicom.github.io/pydicom/dev/\n- User Guide: https://pydicom.github.io/pydicom/dev/guides/user/index.html\n- Tutorials: https://pydicom.github.io/pydicom/dev/tutorials/index.html\n- API Reference: https://pydicom.github.io/pydicom/dev/reference/index.html\n- Examples: https://pydicom.github.io/pydicom/dev/auto_examples/index.html\n",
        "data/k-dense-ai/pyhealth/SKILL.md": "---\nname: pyhealth\ndescription: Comprehensive healthcare AI toolkit for developing, testing, and deploying machine learning models with clinical data. This skill should be used when working with electronic health records (EHR), clinical prediction tasks (mortality, readmission, drug recommendation), medical coding systems (ICD, NDC, ATC), physiological signals (EEG, ECG), healthcare datasets (MIMIC-III/IV, eICU, OMOP), or implementing deep learning models for healthcare applications (RETAIN, SafeDrug, Transformer, GNN).\n---\n\n# PyHealth: Healthcare AI Toolkit\n\n## Overview\n\nPyHealth is a comprehensive Python library for healthcare AI that provides specialized tools, models, and datasets for clinical machine learning. Use this skill when developing healthcare prediction models, processing clinical data, working with medical coding systems, or deploying AI solutions in healthcare settings.\n\n## When to Use This Skill\n\nInvoke this skill when:\n\n- **Working with healthcare datasets**: MIMIC-III, MIMIC-IV, eICU, OMOP, sleep EEG data, medical images\n- **Clinical prediction tasks**: Mortality prediction, hospital readmission, length of stay, drug recommendation\n- **Medical coding**: Translating between ICD-9/10, NDC, RxNorm, ATC coding systems\n- **Processing clinical data**: Sequential events, physiological signals, clinical text, medical images\n- **Implementing healthcare models**: RETAIN, SafeDrug, GAMENet, StageNet, Transformer for EHR\n- **Evaluating clinical models**: Fairness metrics, calibration, interpretability, uncertainty quantification\n\n## Core Capabilities\n\nPyHealth operates through a modular 5-stage pipeline optimized for healthcare AI:\n\n1. **Data Loading**: Access 10+ healthcare datasets with standardized interfaces\n2. **Task Definition**: Apply 20+ predefined clinical prediction tasks or create custom tasks\n3. **Model Selection**: Choose from 33+ models (baselines, deep learning, healthcare-specific)\n4. **Training**: Train with automatic checkpointing, monitoring, and evaluation\n5. **Deployment**: Calibrate, interpret, and validate for clinical use\n\n**Performance**: 3x faster than pandas for healthcare data processing\n\n## Quick Start Workflow\n\n```python\nfrom pyhealth.datasets import MIMIC4Dataset\nfrom pyhealth.tasks import mortality_prediction_mimic4_fn\nfrom pyhealth.datasets import split_by_patient, get_dataloader\nfrom pyhealth.models import Transformer\nfrom pyhealth.trainer import Trainer\n\n# 1. Load dataset and set task\ndataset = MIMIC4Dataset(root=\"/path/to/data\")\nsample_dataset = dataset.set_task(mortality_prediction_mimic4_fn)\n\n# 2. Split data\ntrain, val, test = split_by_patient(sample_dataset, [0.7, 0.1, 0.2])\n\n# 3. Create data loaders\ntrain_loader = get_dataloader(train, batch_size=64, shuffle=True)\nval_loader = get_dataloader(val, batch_size=64, shuffle=False)\ntest_loader = get_dataloader(test, batch_size=64, shuffle=False)\n\n# 4. Initialize and train model\nmodel = Transformer(\n    dataset=sample_dataset,\n    feature_keys=[\"diagnoses\", \"medications\"],\n    mode=\"binary\",\n    embedding_dim=128\n)\n\ntrainer = Trainer(model=model, device=\"cuda\")\ntrainer.train(\n    train_dataloader=train_loader,\n    val_dataloader=val_loader,\n    epochs=50,\n    monitor=\"pr_auc_score\"\n)\n\n# 5. Evaluate\nresults = trainer.evaluate(test_loader)\n```\n\n## Detailed Documentation\n\nThis skill includes comprehensive reference documentation organized by functionality. Read specific reference files as needed:\n\n### 1. Datasets and Data Structures\n\n**File**: `references/datasets.md`\n\n**Read when:**\n- Loading healthcare datasets (MIMIC, eICU, OMOP, sleep EEG, etc.)\n- Understanding Event, Patient, Visit data structures\n- Processing different data types (EHR, signals, images, text)\n- Splitting data for training/validation/testing\n- Working with SampleDataset for task-specific formatting\n\n**Key Topics:**\n- Core data structures (Event, Patient, Visit)\n- 10+ available datasets (EHR, physiological signals, imaging, text)\n- Data loading and iteration\n- Train/val/test splitting strategies\n- Performance optimization for large datasets\n\n### 2. Medical Coding Translation\n\n**File**: `references/medical_coding.md`\n\n**Read when:**\n- Translating between medical coding systems\n- Working with diagnosis codes (ICD-9-CM, ICD-10-CM, CCS)\n- Processing medication codes (NDC, RxNorm, ATC)\n- Standardizing procedure codes (ICD-9-PROC, ICD-10-PROC)\n- Grouping codes into clinical categories\n- Handling hierarchical drug classifications\n\n**Key Topics:**\n- InnerMap for within-system lookups\n- CrossMap for cross-system translation\n- Supported coding systems (ICD, NDC, ATC, CCS, RxNorm)\n- Code standardization and hierarchy traversal\n- Medication classification by therapeutic class\n- Integration with datasets\n\n### 3. Clinical Prediction Tasks\n\n**File**: `references/tasks.md`\n\n**Read when:**\n- Defining clinical prediction objectives\n- Using predefined tasks (mortality, readmission, drug recommendation)\n- Working with EHR, signal, imaging, or text-based tasks\n- Creating custom prediction tasks\n- Setting up input/output schemas for models\n- Applying task-specific filtering logic\n\n**Key Topics:**\n- 20+ predefined clinical tasks\n- EHR tasks (mortality, readmission, length of stay, drug recommendation)\n- Signal tasks (sleep staging, EEG analysis, seizure detection)\n- Imaging tasks (COVID-19 chest X-ray classification)\n- Text tasks (medical coding, specialty classification)\n- Custom task creation patterns\n\n### 4. Models and Architectures\n\n**File**: `references/models.md`\n\n**Read when:**\n- Selecting models for clinical prediction\n- Understanding model architectures and capabilities\n- Choosing between general-purpose and healthcare-specific models\n- Implementing interpretable models (RETAIN, AdaCare)\n- Working with medication recommendation (SafeDrug, GAMENet)\n- Using graph neural networks for healthcare\n- Configuring model hyperparameters\n\n**Key Topics:**\n- 33+ available models\n- General-purpose: Logistic Regression, MLP, CNN, RNN, Transformer, GNN\n- Healthcare-specific: RETAIN, SafeDrug, GAMENet, StageNet, AdaCare\n- Model selection by task type and data type\n- Interpretability considerations\n- Computational requirements\n- Hyperparameter tuning guidelines\n\n### 5. Data Preprocessing\n\n**File**: `references/preprocessing.md`\n\n**Read when:**\n- Preprocessing clinical data for models\n- Handling sequential events and time-series data\n- Processing physiological signals (EEG, ECG)\n- Normalizing lab values and vital signs\n- Preparing labels for different task types\n- Building feature vocabularies\n- Managing missing data and outliers\n\n**Key Topics:**\n- 15+ processor types\n- Sequence processing (padding, truncation)\n- Signal processing (filtering, segmentation)\n- Feature extraction and encoding\n- Label processors (binary, multi-class, multi-label, regression)\n- Text and image preprocessing\n- Common preprocessing workflows\n\n### 6. Training and Evaluation\n\n**File**: `references/training_evaluation.md`\n\n**Read when:**\n- Training models with the Trainer class\n- Evaluating model performance\n- Computing clinical metrics\n- Assessing model fairness across demographics\n- Calibrating predictions for reliability\n- Quantifying prediction uncertainty\n- Interpreting model predictions\n- Preparing models for clinical deployment\n\n**Key Topics:**\n- Trainer class (train, evaluate, inference)\n- Metrics for binary, multi-class, multi-label, regression tasks\n- Fairness metrics for bias assessment\n- Calibration methods (Platt scaling, temperature scaling)\n- Uncertainty quantification (conformal prediction, MC dropout)\n- Interpretability tools (attention visualization, SHAP, ChEFER)\n- Complete training pipeline example\n\n## Installation\n\n```bash\nuv pip install pyhealth\n```\n\n**Requirements:**\n- Python  3.7\n- PyTorch  1.8\n- NumPy, pandas, scikit-learn\n\n## Common Use Cases\n\n### Use Case 1: ICU Mortality Prediction\n\n**Objective**: Predict patient mortality in intensive care unit\n\n**Approach:**\n1. Load MIMIC-IV dataset  Read `references/datasets.md`\n2. Apply mortality prediction task  Read `references/tasks.md`\n3. Select interpretable model (RETAIN)  Read `references/models.md`\n4. Train and evaluate  Read `references/training_evaluation.md`\n5. Interpret predictions for clinical use  Read `references/training_evaluation.md`\n\n### Use Case 2: Safe Medication Recommendation\n\n**Objective**: Recommend medications while avoiding drug-drug interactions\n\n**Approach:**\n1. Load EHR dataset (MIMIC-IV or OMOP)  Read `references/datasets.md`\n2. Apply drug recommendation task  Read `references/tasks.md`\n3. Use SafeDrug model with DDI constraints  Read `references/models.md`\n4. Preprocess medication codes  Read `references/medical_coding.md`\n5. Evaluate with multi-label metrics  Read `references/training_evaluation.md`\n\n### Use Case 3: Hospital Readmission Prediction\n\n**Objective**: Identify patients at risk of 30-day readmission\n\n**Approach:**\n1. Load multi-site EHR data (eICU or OMOP)  Read `references/datasets.md`\n2. Apply readmission prediction task  Read `references/tasks.md`\n3. Handle class imbalance in preprocessing  Read `references/preprocessing.md`\n4. Train Transformer model  Read `references/models.md`\n5. Calibrate predictions and assess fairness  Read `references/training_evaluation.md`\n\n### Use Case 4: Sleep Disorder Diagnosis\n\n**Objective**: Classify sleep stages from EEG signals\n\n**Approach:**\n1. Load sleep EEG dataset (SleepEDF, SHHS)  Read `references/datasets.md`\n2. Apply sleep staging task  Read `references/tasks.md`\n3. Preprocess EEG signals (filtering, segmentation)  Read `references/preprocessing.md`\n4. Train CNN or RNN model  Read `references/models.md`\n5. Evaluate per-stage performance  Read `references/training_evaluation.md`\n\n### Use Case 5: Medical Code Translation\n\n**Objective**: Standardize diagnoses across different coding systems\n\n**Approach:**\n1. Read `references/medical_coding.md` for comprehensive guidance\n2. Use CrossMap to translate between ICD-9, ICD-10, CCS\n3. Group codes into clinically meaningful categories\n4. Integrate with dataset processing\n\n### Use Case 6: Clinical Text to ICD Coding\n\n**Objective**: Automatically assign ICD codes from clinical notes\n\n**Approach:**\n1. Load MIMIC-III with clinical text  Read `references/datasets.md`\n2. Apply ICD coding task  Read `references/tasks.md`\n3. Preprocess clinical text  Read `references/preprocessing.md`\n4. Use TransformersModel (ClinicalBERT)  Read `references/models.md`\n5. Evaluate with multi-label metrics  Read `references/training_evaluation.md`\n\n## Best Practices\n\n### Data Handling\n\n1. **Always split by patient**: Prevent data leakage by ensuring no patient appears in multiple splits\n   ```python\n   from pyhealth.datasets import split_by_patient\n   train, val, test = split_by_patient(dataset, [0.7, 0.1, 0.2])\n   ```\n\n2. **Check dataset statistics**: Understand your data before modeling\n   ```python\n   print(dataset.stats())  # Patients, visits, events, code distributions\n   ```\n\n3. **Use appropriate preprocessing**: Match processors to data types (see `references/preprocessing.md`)\n\n### Model Development\n\n1. **Start with baselines**: Establish baseline performance with simple models\n   - Logistic Regression for binary/multi-class tasks\n   - MLP for initial deep learning baseline\n\n2. **Choose task-appropriate models**:\n   - Interpretability needed  RETAIN, AdaCare\n   - Drug recommendation  SafeDrug, GAMENet\n   - Long sequences  Transformer\n   - Graph relationships  GNN\n\n3. **Monitor validation metrics**: Use appropriate metrics for task and handle class imbalance\n   - Binary classification: AUROC, AUPRC (especially for rare events)\n   - Multi-class: macro-F1 (for imbalanced), weighted-F1\n   - Multi-label: Jaccard, example-F1\n   - Regression: MAE, RMSE\n\n### Clinical Deployment\n\n1. **Calibrate predictions**: Ensure probabilities are reliable (see `references/training_evaluation.md`)\n\n2. **Assess fairness**: Evaluate across demographic groups to detect bias\n\n3. **Quantify uncertainty**: Provide confidence estimates for predictions\n\n4. **Interpret predictions**: Use attention weights, SHAP, or ChEFER for clinical trust\n\n5. **Validate thoroughly**: Use held-out test sets from different time periods or sites\n\n## Limitations and Considerations\n\n### Data Requirements\n\n- **Large datasets**: Deep learning models require sufficient data (thousands of patients)\n- **Data quality**: Missing data and coding errors impact performance\n- **Temporal consistency**: Ensure train/test split respects temporal ordering when needed\n\n### Clinical Validation\n\n- **External validation**: Test on data from different hospitals/systems\n- **Prospective evaluation**: Validate in real clinical settings before deployment\n- **Clinical review**: Have clinicians review predictions and interpretations\n- **Ethical considerations**: Address privacy (HIPAA/GDPR), fairness, and safety\n\n### Computational Resources\n\n- **GPU recommended**: For training deep learning models efficiently\n- **Memory requirements**: Large datasets may require 16GB+ RAM\n- **Storage**: Healthcare datasets can be 10s-100s of GB\n\n## Troubleshooting\n\n### Common Issues\n\n**ImportError for dataset**:\n- Ensure dataset files are downloaded and path is correct\n- Check PyHealth version compatibility\n\n**Out of memory**:\n- Reduce batch size\n- Reduce sequence length (`max_seq_length`)\n- Use gradient accumulation\n- Process data in chunks\n\n**Poor performance**:\n- Check class imbalance and use appropriate metrics (AUPRC vs AUROC)\n- Verify preprocessing (normalization, missing data handling)\n- Increase model capacity or training epochs\n- Check for data leakage in train/test split\n\n**Slow training**:\n- Use GPU (`device=\"cuda\"`)\n- Increase batch size (if memory allows)\n- Reduce sequence length\n- Use more efficient model (CNN vs Transformer)\n\n### Getting Help\n\n- **Documentation**: https://pyhealth.readthedocs.io/\n- **GitHub Issues**: https://github.com/sunlabuiuc/PyHealth/issues\n- **Tutorials**: 7 core tutorials + 5 practical pipelines available online\n\n## Example: Complete Workflow\n\n```python\n# Complete mortality prediction pipeline\nfrom pyhealth.datasets import MIMIC4Dataset\nfrom pyhealth.tasks import mortality_prediction_mimic4_fn\nfrom pyhealth.datasets import split_by_patient, get_dataloader\nfrom pyhealth.models import RETAIN\nfrom pyhealth.trainer import Trainer\n\n# 1. Load dataset\nprint(\"Loading MIMIC-IV dataset...\")\ndataset = MIMIC4Dataset(root=\"/data/mimic4\")\nprint(dataset.stats())\n\n# 2. Define task\nprint(\"Setting mortality prediction task...\")\nsample_dataset = dataset.set_task(mortality_prediction_mimic4_fn)\nprint(f\"Generated {len(sample_dataset)} samples\")\n\n# 3. Split data (by patient to prevent leakage)\nprint(\"Splitting data...\")\ntrain_ds, val_ds, test_ds = split_by_patient(\n    sample_dataset, ratios=[0.7, 0.1, 0.2], seed=42\n)\n\n# 4. Create data loaders\ntrain_loader = get_dataloader(train_ds, batch_size=64, shuffle=True)\nval_loader = get_dataloader(val_ds, batch_size=64)\ntest_loader = get_dataloader(test_ds, batch_size=64)\n\n# 5. Initialize interpretable model\nprint(\"Initializing RETAIN model...\")\nmodel = RETAIN(\n    dataset=sample_dataset,\n    feature_keys=[\"diagnoses\", \"procedures\", \"medications\"],\n    mode=\"binary\",\n    embedding_dim=128,\n    hidden_dim=128\n)\n\n# 6. Train model\nprint(\"Training model...\")\ntrainer = Trainer(model=model, device=\"cuda\")\ntrainer.train(\n    train_dataloader=train_loader,\n    val_dataloader=val_loader,\n    epochs=50,\n    optimizer=\"Adam\",\n    learning_rate=1e-3,\n    weight_decay=1e-5,\n    monitor=\"pr_auc_score\",  # Use AUPRC for imbalanced data\n    monitor_criterion=\"max\",\n    save_path=\"./checkpoints/mortality_retain\"\n)\n\n# 7. Evaluate on test set\nprint(\"Evaluating on test set...\")\ntest_results = trainer.evaluate(\n    test_loader,\n    metrics=[\"accuracy\", \"precision\", \"recall\", \"f1_score\",\n             \"roc_auc_score\", \"pr_auc_score\"]\n)\n\nprint(\"\\nTest Results:\")\nfor metric, value in test_results.items():\n    print(f\"  {metric}: {value:.4f}\")\n\n# 8. Get predictions with attention for interpretation\npredictions = trainer.inference(\n    test_loader,\n    additional_outputs=[\"visit_attention\", \"feature_attention\"],\n    return_patient_ids=True\n)\n\n# 9. Analyze a high-risk patient\nhigh_risk_idx = predictions[\"y_pred\"].argmax()\npatient_id = predictions[\"patient_ids\"][high_risk_idx]\nvisit_attn = predictions[\"visit_attention\"][high_risk_idx]\nfeature_attn = predictions[\"feature_attention\"][high_risk_idx]\n\nprint(f\"\\nHigh-risk patient: {patient_id}\")\nprint(f\"Risk score: {predictions['y_pred'][high_risk_idx]:.3f}\")\nprint(f\"Most influential visit: {visit_attn.argmax()}\")\nprint(f\"Most important features: {feature_attn[visit_attn.argmax()].argsort()[-5:]}\")\n\n# 10. Save model for deployment\ntrainer.save(\"./models/mortality_retain_final.pt\")\nprint(\"\\nModel saved successfully!\")\n```\n\n## Resources\n\nFor detailed information on each component, refer to the comprehensive reference files in the `references/` directory:\n\n- **datasets.md**: Data structures, loading, and splitting (4,500 words)\n- **medical_coding.md**: Code translation and standardization (3,800 words)\n- **tasks.md**: Clinical prediction tasks and custom task creation (4,200 words)\n- **models.md**: Model architectures and selection guidelines (5,100 words)\n- **preprocessing.md**: Data processors and preprocessing workflows (4,600 words)\n- **training_evaluation.md**: Training, metrics, calibration, interpretability (5,900 words)\n\n**Total comprehensive documentation**: ~28,000 words across modular reference files.\n",
        "data/k-dense-ai/pylabrobot/SKILL.md": "---\nname: pylabrobot\ndescription: Laboratory automation toolkit for controlling liquid handlers, plate readers, pumps, heater shakers, incubators, centrifuges, and analytical equipment. Use this skill when automating laboratory workflows, programming liquid handling robots (Hamilton STAR, Opentrons OT-2, Tecan EVO), integrating lab equipment, managing deck layouts and resources (plates, tips, containers), reading plates, or creating reproducible laboratory protocols. Applicable for both simulated protocols and physical hardware control.\n---\n\n# PyLabRobot\n\n## Overview\n\nPyLabRobot is a hardware-agnostic, pure Python Software Development Kit for automated and autonomous laboratories. Use this skill to control liquid handling robots, plate readers, pumps, heater shakers, incubators, centrifuges, and other laboratory automation equipment through a unified Python interface that works across platforms (Windows, macOS, Linux).\n\n## When to Use This Skill\n\nUse this skill when:\n- Programming liquid handling robots (Hamilton STAR/STARlet, Opentrons OT-2, Tecan EVO)\n- Automating laboratory workflows involving pipetting, sample preparation, or analytical measurements\n- Managing deck layouts and laboratory resources (plates, tips, containers, troughs)\n- Integrating multiple lab devices (liquid handlers, plate readers, heater shakers, pumps)\n- Creating reproducible laboratory protocols with state management\n- Simulating protocols before running on physical hardware\n- Reading plates using BMG CLARIOstar or other supported plate readers\n- Controlling temperature, shaking, centrifugation, or other material handling operations\n- Working with laboratory automation in Python\n\n## Core Capabilities\n\nPyLabRobot provides comprehensive laboratory automation through six main capability areas, each detailed in the references/ directory:\n\n### 1. Liquid Handling (`references/liquid-handling.md`)\n\nControl liquid handling robots for aspirating, dispensing, and transferring liquids. Key operations include:\n- **Basic Operations**: Aspirate, dispense, transfer liquids between wells\n- **Tip Management**: Pick up, drop, and track pipette tips automatically\n- **Advanced Techniques**: Multi-channel pipetting, serial dilutions, plate replication\n- **Volume Tracking**: Automatic tracking of liquid volumes in wells\n- **Hardware Support**: Hamilton STAR/STARlet, Opentrons OT-2, Tecan EVO, and others\n\n### 2. Resource Management (`references/resources.md`)\n\nManage laboratory resources in a hierarchical system:\n- **Resource Types**: Plates, tip racks, troughs, tubes, carriers, and custom labware\n- **Deck Layout**: Assign resources to deck positions with coordinate systems\n- **State Management**: Track tip presence, liquid volumes, and resource states\n- **Serialization**: Save and load deck layouts and states from JSON files\n- **Resource Discovery**: Access wells, tips, and containers through intuitive APIs\n\n### 3. Hardware Backends (`references/hardware-backends.md`)\n\nConnect to diverse laboratory equipment through backend abstraction:\n- **Liquid Handlers**: Hamilton STAR (full support), Opentrons OT-2, Tecan EVO\n- **Simulation**: ChatterboxBackend for protocol testing without hardware\n- **Platform Support**: Works on Windows, macOS, Linux, and Raspberry Pi\n- **Backend Switching**: Change robots by swapping backend without rewriting protocols\n\n### 4. Analytical Equipment (`references/analytical-equipment.md`)\n\nIntegrate plate readers and analytical instruments:\n- **Plate Readers**: BMG CLARIOstar for absorbance, luminescence, fluorescence\n- **Scales**: Mettler Toledo integration for mass measurements\n- **Integration Patterns**: Combine liquid handlers with analytical equipment\n- **Automated Workflows**: Move plates between devices automatically\n\n### 5. Material Handling (`references/material-handling.md`)\n\nControl environmental and material handling equipment:\n- **Heater Shakers**: Hamilton HeaterShaker, Inheco ThermoShake\n- **Incubators**: Inheco and Thermo Fisher incubators with temperature control\n- **Centrifuges**: Agilent VSpin with bucket positioning and spin control\n- **Pumps**: Cole Parmer Masterflex for fluid pumping operations\n- **Temperature Control**: Set and monitor temperatures during protocols\n\n### 6. Visualization & Simulation (`references/visualization.md`)\n\nVisualize and simulate laboratory protocols:\n- **Browser Visualizer**: Real-time 3D visualization of deck state\n- **Simulation Mode**: Test protocols without physical hardware\n- **State Tracking**: Monitor tip presence and liquid volumes visually\n- **Deck Editor**: Graphical tool for designing deck layouts\n- **Protocol Validation**: Verify protocols before running on hardware\n\n## Quick Start\n\nTo get started with PyLabRobot, install the package and initialize a liquid handler:\n\n```python\n# Install PyLabRobot\n# uv pip install pylabrobot\n\n# Basic liquid handling setup\nfrom pylabrobot.liquid_handling import LiquidHandler\nfrom pylabrobot.liquid_handling.backends import STAR\nfrom pylabrobot.resources import STARLetDeck\n\n# Initialize liquid handler\nlh = LiquidHandler(backend=STAR(), deck=STARLetDeck())\nawait lh.setup()\n\n# Basic operations\nawait lh.pick_up_tips(tip_rack[\"A1:H1\"])\nawait lh.aspirate(plate[\"A1\"], vols=100)\nawait lh.dispense(plate[\"A2\"], vols=100)\nawait lh.drop_tips()\n```\n\n## Working with References\n\nThis skill organizes detailed information across multiple reference files. Load the relevant reference when:\n- **Liquid Handling**: Writing pipetting protocols, tip management, transfers\n- **Resources**: Defining deck layouts, managing plates/tips, custom labware\n- **Hardware Backends**: Connecting to specific robots, switching platforms\n- **Analytical Equipment**: Integrating plate readers, scales, or analytical devices\n- **Material Handling**: Using heater shakers, incubators, centrifuges, pumps\n- **Visualization**: Simulating protocols, visualizing deck states\n\nAll reference files can be found in the `references/` directory and contain comprehensive examples, API usage patterns, and best practices.\n\n## Best Practices\n\nWhen creating laboratory automation protocols with PyLabRobot:\n\n1. **Start with Simulation**: Use ChatterboxBackend and the visualizer to test protocols before running on hardware\n2. **Enable Tracking**: Turn on tip tracking and volume tracking for accurate state management\n3. **Resource Naming**: Use clear, descriptive names for all resources (plates, tip racks, containers)\n4. **State Serialization**: Save deck layouts and states to JSON for reproducibility\n5. **Error Handling**: Implement proper async error handling for hardware operations\n6. **Temperature Control**: Set temperatures early as heating/cooling takes time\n7. **Modular Protocols**: Break complex workflows into reusable functions\n8. **Documentation**: Reference official docs at https://docs.pylabrobot.org for latest features\n\n## Common Workflows\n\n### Liquid Transfer Protocol\n\n```python\n# Setup\nlh = LiquidHandler(backend=STAR(), deck=STARLetDeck())\nawait lh.setup()\n\n# Define resources\ntip_rack = TIP_CAR_480_A00(name=\"tip_rack\")\nsource_plate = Cos_96_DW_1mL(name=\"source\")\ndest_plate = Cos_96_DW_1mL(name=\"dest\")\n\nlh.deck.assign_child_resource(tip_rack, rails=1)\nlh.deck.assign_child_resource(source_plate, rails=10)\nlh.deck.assign_child_resource(dest_plate, rails=15)\n\n# Transfer protocol\nawait lh.pick_up_tips(tip_rack[\"A1:H1\"])\nawait lh.transfer(source_plate[\"A1:H12\"], dest_plate[\"A1:H12\"], vols=100)\nawait lh.drop_tips()\n```\n\n### Plate Reading Workflow\n\n```python\n# Setup plate reader\nfrom pylabrobot.plate_reading import PlateReader\nfrom pylabrobot.plate_reading.clario_star_backend import CLARIOstarBackend\n\npr = PlateReader(name=\"CLARIOstar\", backend=CLARIOstarBackend())\nawait pr.setup()\n\n# Set temperature and read\nawait pr.set_temperature(37)\nawait pr.open()\n# (manually or robotically load plate)\nawait pr.close()\ndata = await pr.read_absorbance(wavelength=450)\n```\n\n## Additional Resources\n\n- **Official Documentation**: https://docs.pylabrobot.org\n- **GitHub Repository**: https://github.com/PyLabRobot/pylabrobot\n- **Community Forum**: https://discuss.pylabrobot.org\n- **PyPI Package**: https://pypi.org/project/PyLabRobot/\n\nFor detailed usage of specific capabilities, refer to the corresponding reference file in the `references/` directory.\n",
        "data/k-dense-ai/pymatgen/SKILL.md": "---\nname: pymatgen\ndescription: \"Materials science toolkit. Crystal structures (CIF, POSCAR), phase diagrams, band structure, DOS, Materials Project integration, format conversion, for computational materials science.\"\n---\n\n# Pymatgen - Python Materials Genomics\n\n## Overview\n\nPymatgen is a comprehensive Python library for materials analysis that powers the Materials Project. Create, analyze, and manipulate crystal structures and molecules, compute phase diagrams and thermodynamic properties, analyze electronic structure (band structures, DOS), generate surfaces and interfaces, and access Materials Project's database of computed materials. Supports 100+ file formats from various computational codes.\n\n## When to Use This Skill\n\nThis skill should be used when:\n- Working with crystal structures or molecular systems in materials science\n- Converting between structure file formats (CIF, POSCAR, XYZ, etc.)\n- Analyzing symmetry, space groups, or coordination environments\n- Computing phase diagrams or assessing thermodynamic stability\n- Analyzing electronic structure data (band gaps, DOS, band structures)\n- Generating surfaces, slabs, or studying interfaces\n- Accessing the Materials Project database programmatically\n- Setting up high-throughput computational workflows\n- Analyzing diffusion, magnetism, or mechanical properties\n- Working with VASP, Gaussian, Quantum ESPRESSO, or other computational codes\n\n## Quick Start Guide\n\n### Installation\n\n```bash\n# Core pymatgen\nuv pip install pymatgen\n\n# With Materials Project API access\nuv pip install pymatgen mp-api\n\n# Optional dependencies for extended functionality\nuv pip install pymatgen[analysis]  # Additional analysis tools\nuv pip install pymatgen[vis]       # Visualization tools\n```\n\n### Basic Structure Operations\n\n```python\nfrom pymatgen.core import Structure, Lattice\n\n# Read structure from file (automatic format detection)\nstruct = Structure.from_file(\"POSCAR\")\n\n# Create structure from scratch\nlattice = Lattice.cubic(3.84)\nstruct = Structure(lattice, [\"Si\", \"Si\"], [[0,0,0], [0.25,0.25,0.25]])\n\n# Write to different format\nstruct.to(filename=\"structure.cif\")\n\n# Basic properties\nprint(f\"Formula: {struct.composition.reduced_formula}\")\nprint(f\"Space group: {struct.get_space_group_info()}\")\nprint(f\"Density: {struct.density:.2f} g/cm\")\n```\n\n### Materials Project Integration\n\n```bash\n# Set up API key\nexport MP_API_KEY=\"your_api_key_here\"\n```\n\n```python\nfrom mp_api.client import MPRester\n\nwith MPRester() as mpr:\n    # Get structure by material ID\n    struct = mpr.get_structure_by_material_id(\"mp-149\")\n\n    # Search for materials\n    materials = mpr.materials.summary.search(\n        formula=\"Fe2O3\",\n        energy_above_hull=(0, 0.05)\n    )\n```\n\n## Core Capabilities\n\n### 1. Structure Creation and Manipulation\n\nCreate structures using various methods and perform transformations.\n\n**From files:**\n```python\n# Automatic format detection\nstruct = Structure.from_file(\"structure.cif\")\nstruct = Structure.from_file(\"POSCAR\")\nmol = Molecule.from_file(\"molecule.xyz\")\n```\n\n**From scratch:**\n```python\nfrom pymatgen.core import Structure, Lattice\n\n# Using lattice parameters\nlattice = Lattice.from_parameters(a=3.84, b=3.84, c=3.84,\n                                  alpha=120, beta=90, gamma=60)\ncoords = [[0, 0, 0], [0.75, 0.5, 0.75]]\nstruct = Structure(lattice, [\"Si\", \"Si\"], coords)\n\n# From space group\nstruct = Structure.from_spacegroup(\n    \"Fm-3m\",\n    Lattice.cubic(3.5),\n    [\"Si\"],\n    [[0, 0, 0]]\n)\n```\n\n**Transformations:**\n```python\nfrom pymatgen.transformations.standard_transformations import (\n    SupercellTransformation,\n    SubstitutionTransformation,\n    PrimitiveCellTransformation\n)\n\n# Create supercell\ntrans = SupercellTransformation([[2,0,0],[0,2,0],[0,0,2]])\nsupercell = trans.apply_transformation(struct)\n\n# Substitute elements\ntrans = SubstitutionTransformation({\"Fe\": \"Mn\"})\nnew_struct = trans.apply_transformation(struct)\n\n# Get primitive cell\ntrans = PrimitiveCellTransformation()\nprimitive = trans.apply_transformation(struct)\n```\n\n**Reference:** See `references/core_classes.md` for comprehensive documentation of Structure, Lattice, Molecule, and related classes.\n\n### 2. File Format Conversion\n\nConvert between 100+ file formats with automatic format detection.\n\n**Using convenience methods:**\n```python\n# Read any format\nstruct = Structure.from_file(\"input_file\")\n\n# Write to any format\nstruct.to(filename=\"output.cif\")\nstruct.to(filename=\"POSCAR\")\nstruct.to(filename=\"output.xyz\")\n```\n\n**Using the conversion script:**\n```bash\n# Single file conversion\npython scripts/structure_converter.py POSCAR structure.cif\n\n# Batch conversion\npython scripts/structure_converter.py *.cif --output-dir ./poscar_files --format poscar\n```\n\n**Reference:** See `references/io_formats.md` for detailed documentation of all supported formats and code integrations.\n\n### 3. Structure Analysis and Symmetry\n\nAnalyze structures for symmetry, coordination, and other properties.\n\n**Symmetry analysis:**\n```python\nfrom pymatgen.symmetry.analyzer import SpacegroupAnalyzer\n\nsga = SpacegroupAnalyzer(struct)\n\n# Get space group information\nprint(f\"Space group: {sga.get_space_group_symbol()}\")\nprint(f\"Number: {sga.get_space_group_number()}\")\nprint(f\"Crystal system: {sga.get_crystal_system()}\")\n\n# Get conventional/primitive cells\nconventional = sga.get_conventional_standard_structure()\nprimitive = sga.get_primitive_standard_structure()\n```\n\n**Coordination environment:**\n```python\nfrom pymatgen.analysis.local_env import CrystalNN\n\ncnn = CrystalNN()\nneighbors = cnn.get_nn_info(struct, n=0)  # Neighbors of site 0\n\nprint(f\"Coordination number: {len(neighbors)}\")\nfor neighbor in neighbors:\n    site = struct[neighbor['site_index']]\n    print(f\"  {site.species_string} at {neighbor['weight']:.3f} \")\n```\n\n**Using the analysis script:**\n```bash\n# Comprehensive analysis\npython scripts/structure_analyzer.py POSCAR --symmetry --neighbors\n\n# Export results\npython scripts/structure_analyzer.py structure.cif --symmetry --export json\n```\n\n**Reference:** See `references/analysis_modules.md` for detailed documentation of all analysis capabilities.\n\n### 4. Phase Diagrams and Thermodynamics\n\nConstruct phase diagrams and analyze thermodynamic stability.\n\n**Phase diagram construction:**\n```python\nfrom mp_api.client import MPRester\nfrom pymatgen.analysis.phase_diagram import PhaseDiagram, PDPlotter\n\n# Get entries from Materials Project\nwith MPRester() as mpr:\n    entries = mpr.get_entries_in_chemsys(\"Li-Fe-O\")\n\n# Build phase diagram\npd = PhaseDiagram(entries)\n\n# Check stability\nfrom pymatgen.core import Composition\ncomp = Composition(\"LiFeO2\")\n\n# Find entry for composition\nfor entry in entries:\n    if entry.composition.reduced_formula == comp.reduced_formula:\n        e_above_hull = pd.get_e_above_hull(entry)\n        print(f\"Energy above hull: {e_above_hull:.4f} eV/atom\")\n\n        if e_above_hull > 0.001:\n            # Get decomposition\n            decomp = pd.get_decomposition(comp)\n            print(\"Decomposes to:\", decomp)\n\n# Plot\nplotter = PDPlotter(pd)\nplotter.show()\n```\n\n**Using the phase diagram script:**\n```bash\n# Generate phase diagram\npython scripts/phase_diagram_generator.py Li-Fe-O --output li_fe_o.png\n\n# Analyze specific composition\npython scripts/phase_diagram_generator.py Li-Fe-O --analyze \"LiFeO2\" --show\n```\n\n**Reference:** See `references/analysis_modules.md` (Phase Diagrams section) and `references/transformations_workflows.md` (Workflow 2) for detailed examples.\n\n### 5. Electronic Structure Analysis\n\nAnalyze band structures, density of states, and electronic properties.\n\n**Band structure:**\n```python\nfrom pymatgen.io.vasp import Vasprun\nfrom pymatgen.electronic_structure.plotter import BSPlotter\n\n# Read from VASP calculation\nvasprun = Vasprun(\"vasprun.xml\")\nbs = vasprun.get_band_structure()\n\n# Analyze\nband_gap = bs.get_band_gap()\nprint(f\"Band gap: {band_gap['energy']:.3f} eV\")\nprint(f\"Direct: {band_gap['direct']}\")\nprint(f\"Is metal: {bs.is_metal()}\")\n\n# Plot\nplotter = BSPlotter(bs)\nplotter.save_plot(\"band_structure.png\")\n```\n\n**Density of states:**\n```python\nfrom pymatgen.electronic_structure.plotter import DosPlotter\n\ndos = vasprun.complete_dos\n\n# Get element-projected DOS\nelement_dos = dos.get_element_dos()\nfor element, element_dos_obj in element_dos.items():\n    print(f\"{element}: {element_dos_obj.get_gap():.3f} eV\")\n\n# Plot\nplotter = DosPlotter()\nplotter.add_dos(\"Total DOS\", dos)\nplotter.show()\n```\n\n**Reference:** See `references/analysis_modules.md` (Electronic Structure section) and `references/io_formats.md` (VASP section).\n\n### 6. Surface and Interface Analysis\n\nGenerate slabs, analyze surfaces, and study interfaces.\n\n**Slab generation:**\n```python\nfrom pymatgen.core.surface import SlabGenerator\n\n# Generate slabs for specific Miller index\nslabgen = SlabGenerator(\n    struct,\n    miller_index=(1, 1, 1),\n    min_slab_size=10.0,      # \n    min_vacuum_size=10.0,    # \n    center_slab=True\n)\n\nslabs = slabgen.get_slabs()\n\n# Write slabs\nfor i, slab in enumerate(slabs):\n    slab.to(filename=f\"slab_{i}.cif\")\n```\n\n**Wulff shape construction:**\n```python\nfrom pymatgen.analysis.wulff import WulffShape\n\n# Define surface energies\nsurface_energies = {\n    (1, 0, 0): 1.0,\n    (1, 1, 0): 1.1,\n    (1, 1, 1): 0.9,\n}\n\nwulff = WulffShape(struct.lattice, surface_energies)\nprint(f\"Surface area: {wulff.surface_area:.2f} \")\nprint(f\"Volume: {wulff.volume:.2f} \")\n\nwulff.show()\n```\n\n**Adsorption site finding:**\n```python\nfrom pymatgen.analysis.adsorption import AdsorbateSiteFinder\nfrom pymatgen.core import Molecule\n\nasf = AdsorbateSiteFinder(slab)\n\n# Find sites\nads_sites = asf.find_adsorption_sites()\nprint(f\"On-top sites: {len(ads_sites['ontop'])}\")\nprint(f\"Bridge sites: {len(ads_sites['bridge'])}\")\nprint(f\"Hollow sites: {len(ads_sites['hollow'])}\")\n\n# Add adsorbate\nadsorbate = Molecule(\"O\", [[0, 0, 0]])\nads_struct = asf.add_adsorbate(adsorbate, ads_sites[\"ontop\"][0])\n```\n\n**Reference:** See `references/analysis_modules.md` (Surface and Interface section) and `references/transformations_workflows.md` (Workflows 3 and 9).\n\n### 7. Materials Project Database Access\n\nProgrammatically access the Materials Project database.\n\n**Setup:**\n1. Get API key from https://next-gen.materialsproject.org/\n2. Set environment variable: `export MP_API_KEY=\"your_key_here\"`\n\n**Search and retrieve:**\n```python\nfrom mp_api.client import MPRester\n\nwith MPRester() as mpr:\n    # Search by formula\n    materials = mpr.materials.summary.search(formula=\"Fe2O3\")\n\n    # Search by chemical system\n    materials = mpr.materials.summary.search(chemsys=\"Li-Fe-O\")\n\n    # Filter by properties\n    materials = mpr.materials.summary.search(\n        chemsys=\"Li-Fe-O\",\n        energy_above_hull=(0, 0.05),  # Stable/metastable\n        band_gap=(1.0, 3.0)            # Semiconducting\n    )\n\n    # Get structure\n    struct = mpr.get_structure_by_material_id(\"mp-149\")\n\n    # Get band structure\n    bs = mpr.get_bandstructure_by_material_id(\"mp-149\")\n\n    # Get entries for phase diagram\n    entries = mpr.get_entries_in_chemsys(\"Li-Fe-O\")\n```\n\n**Reference:** See `references/materials_project_api.md` for comprehensive API documentation and examples.\n\n### 8. Computational Workflow Setup\n\nSet up calculations for various electronic structure codes.\n\n**VASP input generation:**\n```python\nfrom pymatgen.io.vasp.sets import MPRelaxSet, MPStaticSet, MPNonSCFSet\n\n# Relaxation\nrelax = MPRelaxSet(struct)\nrelax.write_input(\"./relax_calc\")\n\n# Static calculation\nstatic = MPStaticSet(struct)\nstatic.write_input(\"./static_calc\")\n\n# Band structure (non-self-consistent)\nnscf = MPNonSCFSet(struct, mode=\"line\")\nnscf.write_input(\"./bandstructure_calc\")\n\n# Custom parameters\ncustom = MPRelaxSet(struct, user_incar_settings={\"ENCUT\": 600})\ncustom.write_input(\"./custom_calc\")\n```\n\n**Other codes:**\n```python\n# Gaussian\nfrom pymatgen.io.gaussian import GaussianInput\n\ngin = GaussianInput(\n    mol,\n    functional=\"B3LYP\",\n    basis_set=\"6-31G(d)\",\n    route_parameters={\"Opt\": None}\n)\ngin.write_file(\"input.gjf\")\n\n# Quantum ESPRESSO\nfrom pymatgen.io.pwscf import PWInput\n\npwin = PWInput(struct, control={\"calculation\": \"scf\"})\npwin.write_file(\"pw.in\")\n```\n\n**Reference:** See `references/io_formats.md` (Electronic Structure Code I/O section) and `references/transformations_workflows.md` for workflow examples.\n\n### 9. Advanced Analysis\n\n**Diffraction patterns:**\n```python\nfrom pymatgen.analysis.diffraction.xrd import XRDCalculator\n\nxrd = XRDCalculator()\npattern = xrd.get_pattern(struct)\n\n# Get peaks\nfor peak in pattern.hkls:\n    print(f\"2 = {peak['2theta']:.2f}, hkl = {peak['hkl']}\")\n\npattern.plot()\n```\n\n**Elastic properties:**\n```python\nfrom pymatgen.analysis.elasticity import ElasticTensor\n\n# From elastic tensor matrix\nelastic_tensor = ElasticTensor.from_voigt(matrix)\n\nprint(f\"Bulk modulus: {elastic_tensor.k_voigt:.1f} GPa\")\nprint(f\"Shear modulus: {elastic_tensor.g_voigt:.1f} GPa\")\nprint(f\"Young's modulus: {elastic_tensor.y_mod:.1f} GPa\")\n```\n\n**Magnetic ordering:**\n```python\nfrom pymatgen.transformations.advanced_transformations import MagOrderingTransformation\n\n# Enumerate magnetic orderings\ntrans = MagOrderingTransformation({\"Fe\": 5.0})\nmag_structs = trans.apply_transformation(struct, return_ranked_list=True)\n\n# Get lowest energy magnetic structure\nlowest_energy_struct = mag_structs[0]['structure']\n```\n\n**Reference:** See `references/analysis_modules.md` for comprehensive analysis module documentation.\n\n## Bundled Resources\n\n### Scripts (`scripts/`)\n\nExecutable Python scripts for common tasks:\n\n- **`structure_converter.py`**: Convert between structure file formats\n  - Supports batch conversion and automatic format detection\n  - Usage: `python scripts/structure_converter.py POSCAR structure.cif`\n\n- **`structure_analyzer.py`**: Comprehensive structure analysis\n  - Symmetry, coordination, lattice parameters, distance matrix\n  - Usage: `python scripts/structure_analyzer.py structure.cif --symmetry --neighbors`\n\n- **`phase_diagram_generator.py`**: Generate phase diagrams from Materials Project\n  - Stability analysis and thermodynamic properties\n  - Usage: `python scripts/phase_diagram_generator.py Li-Fe-O --analyze \"LiFeO2\"`\n\nAll scripts include detailed help: `python scripts/script_name.py --help`\n\n### References (`references/`)\n\nComprehensive documentation loaded into context as needed:\n\n- **`core_classes.md`**: Element, Structure, Lattice, Molecule, Composition classes\n- **`io_formats.md`**: File format support and code integration (VASP, Gaussian, etc.)\n- **`analysis_modules.md`**: Phase diagrams, surfaces, electronic structure, symmetry\n- **`materials_project_api.md`**: Complete Materials Project API guide\n- **`transformations_workflows.md`**: Transformations framework and common workflows\n\nLoad references when detailed information is needed about specific modules or workflows.\n\n## Common Workflows\n\n### High-Throughput Structure Generation\n\n```python\nfrom pymatgen.transformations.standard_transformations import SubstitutionTransformation\nfrom pymatgen.io.vasp.sets import MPRelaxSet\n\n# Generate doped structures\nbase_struct = Structure.from_file(\"POSCAR\")\ndopants = [\"Mn\", \"Co\", \"Ni\", \"Cu\"]\n\nfor dopant in dopants:\n    trans = SubstitutionTransformation({\"Fe\": dopant})\n    doped_struct = trans.apply_transformation(base_struct)\n\n    # Generate VASP inputs\n    vasp_input = MPRelaxSet(doped_struct)\n    vasp_input.write_input(f\"./calcs/Fe_{dopant}\")\n```\n\n### Band Structure Calculation Workflow\n\n```python\n# 1. Relaxation\nrelax = MPRelaxSet(struct)\nrelax.write_input(\"./1_relax\")\n\n# 2. Static (after relaxation)\nrelaxed = Structure.from_file(\"1_relax/CONTCAR\")\nstatic = MPStaticSet(relaxed)\nstatic.write_input(\"./2_static\")\n\n# 3. Band structure (non-self-consistent)\nnscf = MPNonSCFSet(relaxed, mode=\"line\")\nnscf.write_input(\"./3_bandstructure\")\n\n# 4. Analysis\nfrom pymatgen.io.vasp import Vasprun\nvasprun = Vasprun(\"3_bandstructure/vasprun.xml\")\nbs = vasprun.get_band_structure()\nbs.get_band_gap()\n```\n\n### Surface Energy Calculation\n\n```python\n# 1. Get bulk energy\nbulk_vasprun = Vasprun(\"bulk/vasprun.xml\")\nbulk_E_per_atom = bulk_vasprun.final_energy / len(bulk)\n\n# 2. Generate and calculate slabs\nslabgen = SlabGenerator(bulk, (1,1,1), 10, 15)\nslab = slabgen.get_slabs()[0]\n\nMPRelaxSet(slab).write_input(\"./slab_calc\")\n\n# 3. Calculate surface energy (after calculation)\nslab_vasprun = Vasprun(\"slab_calc/vasprun.xml\")\nE_surf = (slab_vasprun.final_energy - len(slab) * bulk_E_per_atom) / (2 * slab.surface_area)\nE_surf *= 16.021766  # Convert eV/ to J/m\n```\n\n**More workflows:** See `references/transformations_workflows.md` for 10 detailed workflow examples.\n\n## Best Practices\n\n### Structure Handling\n\n1. **Use automatic format detection**: `Structure.from_file()` handles most formats\n2. **Prefer immutable structures**: Use `IStructure` when structure shouldn't change\n3. **Check symmetry**: Use `SpacegroupAnalyzer` to reduce to primitive cell\n4. **Validate structures**: Check for overlapping atoms or unreasonable bond lengths\n\n### File I/O\n\n1. **Use convenience methods**: `from_file()` and `to()` are preferred\n2. **Specify formats explicitly**: When automatic detection fails\n3. **Handle exceptions**: Wrap file I/O in try-except blocks\n4. **Use serialization**: `as_dict()`/`from_dict()` for version-safe storage\n\n### Materials Project API\n\n1. **Use context manager**: Always use `with MPRester() as mpr:`\n2. **Batch queries**: Request multiple items at once\n3. **Cache results**: Save frequently used data locally\n4. **Filter effectively**: Use property filters to reduce data transfer\n\n### Computational Workflows\n\n1. **Use input sets**: Prefer `MPRelaxSet`, `MPStaticSet` over manual INCAR\n2. **Check convergence**: Always verify calculations converged\n3. **Track transformations**: Use `TransformedStructure` for provenance\n4. **Organize calculations**: Use clear directory structures\n\n### Performance\n\n1. **Reduce symmetry**: Use primitive cells when possible\n2. **Limit neighbor searches**: Specify reasonable cutoff radii\n3. **Use appropriate methods**: Different analysis tools have different speed/accuracy tradeoffs\n4. **Parallelize when possible**: Many operations can be parallelized\n\n## Units and Conventions\n\nPymatgen uses atomic units throughout:\n- **Lengths**: Angstroms ()\n- **Energies**: Electronvolts (eV)\n- **Angles**: Degrees ()\n- **Magnetic moments**: Bohr magnetons (B)\n- **Time**: Femtoseconds (fs)\n\nConvert units using `pymatgen.core.units` when needed.\n\n## Integration with Other Tools\n\nPymatgen integrates seamlessly with:\n- **ASE** (Atomic Simulation Environment)\n- **Phonopy** (phonon calculations)\n- **BoltzTraP** (transport properties)\n- **Atomate/Fireworks** (workflow management)\n- **AiiDA** (provenance tracking)\n- **Zeo++** (pore analysis)\n- **OpenBabel** (molecule conversion)\n\n## Troubleshooting\n\n**Import errors**: Install missing dependencies\n```bash\nuv pip install pymatgen[analysis,vis]\n```\n\n**API key not found**: Set MP_API_KEY environment variable\n```bash\nexport MP_API_KEY=\"your_key_here\"\n```\n\n**Structure read failures**: Check file format and syntax\n```python\n# Try explicit format specification\nstruct = Structure.from_file(\"file.txt\", fmt=\"cif\")\n```\n\n**Symmetry analysis fails**: Structure may have numerical precision issues\n```python\n# Increase tolerance\nfrom pymatgen.symmetry.analyzer import SpacegroupAnalyzer\nsga = SpacegroupAnalyzer(struct, symprec=0.1)\n```\n\n## Additional Resources\n\n- **Documentation**: https://pymatgen.org/\n- **Materials Project**: https://materialsproject.org/\n- **GitHub**: https://github.com/materialsproject/pymatgen\n- **Forum**: https://matsci.org/\n- **Example notebooks**: https://matgenb.materialsvirtuallab.org/\n\n## Version Notes\n\nThis skill is designed for pymatgen 2024.x and later. For the Materials Project API, use the `mp-api` package (separate from legacy `pymatgen.ext.matproj`).\n\nRequirements:\n- Python 3.10 or higher\n- pymatgen >= 2023.x\n- mp-api (for Materials Project access)\n",
        "data/k-dense-ai/pymc/SKILL.md": "---\nname: pymc-bayesian-modeling\ndescription: \"Bayesian modeling with PyMC. Build hierarchical models, MCMC (NUTS), variational inference, LOO/WAIC comparison, posterior checks, for probabilistic programming and inference.\"\n---\n\n# PyMC Bayesian Modeling\n\n## Overview\n\nPyMC is a Python library for Bayesian modeling and probabilistic programming. Build, fit, validate, and compare Bayesian models using PyMC's modern API (version 5.x+), including hierarchical models, MCMC sampling (NUTS), variational inference, and model comparison (LOO, WAIC).\n\n## When to Use This Skill\n\nThis skill should be used when:\n- Building Bayesian models (linear/logistic regression, hierarchical models, time series, etc.)\n- Performing MCMC sampling or variational inference\n- Conducting prior/posterior predictive checks\n- Diagnosing sampling issues (divergences, convergence, ESS)\n- Comparing multiple models using information criteria (LOO, WAIC)\n- Implementing uncertainty quantification through Bayesian methods\n- Working with hierarchical/multilevel data structures\n- Handling missing data or measurement error in a principled way\n\n## Standard Bayesian Workflow\n\nFollow this workflow for building and validating Bayesian models:\n\n### 1. Data Preparation\n\n```python\nimport pymc as pm\nimport arviz as az\nimport numpy as np\n\n# Load and prepare data\nX = ...  # Predictors\ny = ...  # Outcomes\n\n# Standardize predictors for better sampling\nX_mean = X.mean(axis=0)\nX_std = X.std(axis=0)\nX_scaled = (X - X_mean) / X_std\n```\n\n**Key practices:**\n- Standardize continuous predictors (improves sampling efficiency)\n- Center outcomes when possible\n- Handle missing data explicitly (treat as parameters)\n- Use named dimensions with `coords` for clarity\n\n### 2. Model Building\n\n```python\ncoords = {\n    'predictors': ['var1', 'var2', 'var3'],\n    'obs_id': np.arange(len(y))\n}\n\nwith pm.Model(coords=coords) as model:\n    # Priors\n    alpha = pm.Normal('alpha', mu=0, sigma=1)\n    beta = pm.Normal('beta', mu=0, sigma=1, dims='predictors')\n    sigma = pm.HalfNormal('sigma', sigma=1)\n\n    # Linear predictor\n    mu = alpha + pm.math.dot(X_scaled, beta)\n\n    # Likelihood\n    y_obs = pm.Normal('y_obs', mu=mu, sigma=sigma, observed=y, dims='obs_id')\n```\n\n**Key practices:**\n- Use weakly informative priors (not flat priors)\n- Use `HalfNormal` or `Exponential` for scale parameters\n- Use named dimensions (`dims`) instead of `shape` when possible\n- Use `pm.Data()` for values that will be updated for predictions\n\n### 3. Prior Predictive Check\n\n**Always validate priors before fitting:**\n\n```python\nwith model:\n    prior_pred = pm.sample_prior_predictive(samples=1000, random_seed=42)\n\n# Visualize\naz.plot_ppc(prior_pred, group='prior')\n```\n\n**Check:**\n- Do prior predictions span reasonable values?\n- Are extreme values plausible given domain knowledge?\n- If priors generate implausible data, adjust and re-check\n\n### 4. Fit Model\n\n```python\nwith model:\n    # Optional: Quick exploration with ADVI\n    # approx = pm.fit(n=20000)\n\n    # Full MCMC inference\n    idata = pm.sample(\n        draws=2000,\n        tune=1000,\n        chains=4,\n        target_accept=0.9,\n        random_seed=42,\n        idata_kwargs={'log_likelihood': True}  # For model comparison\n    )\n```\n\n**Key parameters:**\n- `draws=2000`: Number of samples per chain\n- `tune=1000`: Warmup samples (discarded)\n- `chains=4`: Run 4 chains for convergence checking\n- `target_accept=0.9`: Higher for difficult posteriors (0.95-0.99)\n- Include `log_likelihood=True` for model comparison\n\n### 5. Check Diagnostics\n\n**Use the diagnostic script:**\n\n```python\nfrom scripts.model_diagnostics import check_diagnostics\n\nresults = check_diagnostics(idata, var_names=['alpha', 'beta', 'sigma'])\n```\n\n**Check:**\n- **R-hat < 1.01**: Chains have converged\n- **ESS > 400**: Sufficient effective samples\n- **No divergences**: NUTS sampled successfully\n- **Trace plots**: Chains should mix well (fuzzy caterpillar)\n\n**If issues arise:**\n- Divergences  Increase `target_accept=0.95`, use non-centered parameterization\n- Low ESS  Sample more draws, reparameterize to reduce correlation\n- High R-hat  Run longer, check for multimodality\n\n### 6. Posterior Predictive Check\n\n**Validate model fit:**\n\n```python\nwith model:\n    pm.sample_posterior_predictive(idata, extend_inferencedata=True, random_seed=42)\n\n# Visualize\naz.plot_ppc(idata)\n```\n\n**Check:**\n- Do posterior predictions capture observed data patterns?\n- Are systematic deviations evident (model misspecification)?\n- Consider alternative models if fit is poor\n\n### 7. Analyze Results\n\n```python\n# Summary statistics\nprint(az.summary(idata, var_names=['alpha', 'beta', 'sigma']))\n\n# Posterior distributions\naz.plot_posterior(idata, var_names=['alpha', 'beta', 'sigma'])\n\n# Coefficient estimates\naz.plot_forest(idata, var_names=['beta'], combined=True)\n```\n\n### 8. Make Predictions\n\n```python\nX_new = ...  # New predictor values\nX_new_scaled = (X_new - X_mean) / X_std\n\nwith model:\n    pm.set_data({'X_scaled': X_new_scaled})\n    post_pred = pm.sample_posterior_predictive(\n        idata.posterior,\n        var_names=['y_obs'],\n        random_seed=42\n    )\n\n# Extract prediction intervals\ny_pred_mean = post_pred.posterior_predictive['y_obs'].mean(dim=['chain', 'draw'])\ny_pred_hdi = az.hdi(post_pred.posterior_predictive, var_names=['y_obs'])\n```\n\n## Common Model Patterns\n\n### Linear Regression\n\nFor continuous outcomes with linear relationships:\n\n```python\nwith pm.Model() as linear_model:\n    alpha = pm.Normal('alpha', mu=0, sigma=10)\n    beta = pm.Normal('beta', mu=0, sigma=10, shape=n_predictors)\n    sigma = pm.HalfNormal('sigma', sigma=1)\n\n    mu = alpha + pm.math.dot(X, beta)\n    y = pm.Normal('y', mu=mu, sigma=sigma, observed=y_obs)\n```\n\n**Use template:** `assets/linear_regression_template.py`\n\n### Logistic Regression\n\nFor binary outcomes:\n\n```python\nwith pm.Model() as logistic_model:\n    alpha = pm.Normal('alpha', mu=0, sigma=10)\n    beta = pm.Normal('beta', mu=0, sigma=10, shape=n_predictors)\n\n    logit_p = alpha + pm.math.dot(X, beta)\n    y = pm.Bernoulli('y', logit_p=logit_p, observed=y_obs)\n```\n\n### Hierarchical Models\n\nFor grouped data (use non-centered parameterization):\n\n```python\nwith pm.Model(coords={'groups': group_names}) as hierarchical_model:\n    # Hyperpriors\n    mu_alpha = pm.Normal('mu_alpha', mu=0, sigma=10)\n    sigma_alpha = pm.HalfNormal('sigma_alpha', sigma=1)\n\n    # Group-level (non-centered)\n    alpha_offset = pm.Normal('alpha_offset', mu=0, sigma=1, dims='groups')\n    alpha = pm.Deterministic('alpha', mu_alpha + sigma_alpha * alpha_offset, dims='groups')\n\n    # Observation-level\n    mu = alpha[group_idx]\n    sigma = pm.HalfNormal('sigma', sigma=1)\n    y = pm.Normal('y', mu=mu, sigma=sigma, observed=y_obs)\n```\n\n**Use template:** `assets/hierarchical_model_template.py`\n\n**Critical:** Always use non-centered parameterization for hierarchical models to avoid divergences.\n\n### Poisson Regression\n\nFor count data:\n\n```python\nwith pm.Model() as poisson_model:\n    alpha = pm.Normal('alpha', mu=0, sigma=10)\n    beta = pm.Normal('beta', mu=0, sigma=10, shape=n_predictors)\n\n    log_lambda = alpha + pm.math.dot(X, beta)\n    y = pm.Poisson('y', mu=pm.math.exp(log_lambda), observed=y_obs)\n```\n\nFor overdispersed counts, use `NegativeBinomial` instead.\n\n### Time Series\n\nFor autoregressive processes:\n\n```python\nwith pm.Model() as ar_model:\n    sigma = pm.HalfNormal('sigma', sigma=1)\n    rho = pm.Normal('rho', mu=0, sigma=0.5, shape=ar_order)\n    init_dist = pm.Normal.dist(mu=0, sigma=sigma)\n\n    y = pm.AR('y', rho=rho, sigma=sigma, init_dist=init_dist, observed=y_obs)\n```\n\n## Model Comparison\n\n### Comparing Models\n\nUse LOO or WAIC for model comparison:\n\n```python\nfrom scripts.model_comparison import compare_models, check_loo_reliability\n\n# Fit models with log_likelihood\nmodels = {\n    'Model1': idata1,\n    'Model2': idata2,\n    'Model3': idata3\n}\n\n# Compare using LOO\ncomparison = compare_models(models, ic='loo')\n\n# Check reliability\ncheck_loo_reliability(models)\n```\n\n**Interpretation:**\n- **loo < 2**: Models are similar, choose simpler model\n- **2 < loo < 4**: Weak evidence for better model\n- **4 < loo < 10**: Moderate evidence\n- **loo > 10**: Strong evidence for better model\n\n**Check Pareto-k values:**\n- k < 0.7: LOO reliable\n- k > 0.7: Consider WAIC or k-fold CV\n\n### Model Averaging\n\nWhen models are similar, average predictions:\n\n```python\nfrom scripts.model_comparison import model_averaging\n\naveraged_pred, weights = model_averaging(models, var_name='y_obs')\n```\n\n## Distribution Selection Guide\n\n### For Priors\n\n**Scale parameters** (, ):\n- `pm.HalfNormal('sigma', sigma=1)` - Default choice\n- `pm.Exponential('sigma', lam=1)` - Alternative\n- `pm.Gamma('sigma', alpha=2, beta=1)` - More informative\n\n**Unbounded parameters**:\n- `pm.Normal('theta', mu=0, sigma=1)` - For standardized data\n- `pm.StudentT('theta', nu=3, mu=0, sigma=1)` - Robust to outliers\n\n**Positive parameters**:\n- `pm.LogNormal('theta', mu=0, sigma=1)`\n- `pm.Gamma('theta', alpha=2, beta=1)`\n\n**Probabilities**:\n- `pm.Beta('p', alpha=2, beta=2)` - Weakly informative\n- `pm.Uniform('p', lower=0, upper=1)` - Non-informative (use sparingly)\n\n**Correlation matrices**:\n- `pm.LKJCorr('corr', n=n_vars, eta=2)` - eta=1 uniform, eta>1 prefers identity\n\n### For Likelihoods\n\n**Continuous outcomes**:\n- `pm.Normal('y', mu=mu, sigma=sigma)` - Default for continuous data\n- `pm.StudentT('y', nu=nu, mu=mu, sigma=sigma)` - Robust to outliers\n\n**Count data**:\n- `pm.Poisson('y', mu=lambda)` - Equidispersed counts\n- `pm.NegativeBinomial('y', mu=mu, alpha=alpha)` - Overdispersed counts\n- `pm.ZeroInflatedPoisson('y', psi=psi, mu=mu)` - Excess zeros\n\n**Binary outcomes**:\n- `pm.Bernoulli('y', p=p)` or `pm.Bernoulli('y', logit_p=logit_p)`\n\n**Categorical outcomes**:\n- `pm.Categorical('y', p=probs)`\n\n**See:** `references/distributions.md` for comprehensive distribution reference\n\n## Sampling and Inference\n\n### MCMC with NUTS\n\nDefault and recommended for most models:\n\n```python\nidata = pm.sample(\n    draws=2000,\n    tune=1000,\n    chains=4,\n    target_accept=0.9,\n    random_seed=42\n)\n```\n\n**Adjust when needed:**\n- Divergences  `target_accept=0.95` or higher\n- Slow sampling  Use ADVI for initialization\n- Discrete parameters  Use `pm.Metropolis()` for discrete vars\n\n### Variational Inference\n\nFast approximation for exploration or initialization:\n\n```python\nwith model:\n    approx = pm.fit(n=20000, method='advi')\n\n    # Use for initialization\n    start = approx.sample(return_inferencedata=False)[0]\n    idata = pm.sample(start=start)\n```\n\n**Trade-offs:**\n- Much faster than MCMC\n- Approximate (may underestimate uncertainty)\n- Good for large models or quick exploration\n\n**See:** `references/sampling_inference.md` for detailed sampling guide\n\n## Diagnostic Scripts\n\n### Comprehensive Diagnostics\n\n```python\nfrom scripts.model_diagnostics import create_diagnostic_report\n\ncreate_diagnostic_report(\n    idata,\n    var_names=['alpha', 'beta', 'sigma'],\n    output_dir='diagnostics/'\n)\n```\n\nCreates:\n- Trace plots\n- Rank plots (mixing check)\n- Autocorrelation plots\n- Energy plots\n- ESS evolution\n- Summary statistics CSV\n\n### Quick Diagnostic Check\n\n```python\nfrom scripts.model_diagnostics import check_diagnostics\n\nresults = check_diagnostics(idata)\n```\n\nChecks R-hat, ESS, divergences, and tree depth.\n\n## Common Issues and Solutions\n\n### Divergences\n\n**Symptom:** `idata.sample_stats.diverging.sum() > 0`\n\n**Solutions:**\n1. Increase `target_accept=0.95` or `0.99`\n2. Use non-centered parameterization (hierarchical models)\n3. Add stronger priors to constrain parameters\n4. Check for model misspecification\n\n### Low Effective Sample Size\n\n**Symptom:** `ESS < 400`\n\n**Solutions:**\n1. Sample more draws: `draws=5000`\n2. Reparameterize to reduce posterior correlation\n3. Use QR decomposition for regression with correlated predictors\n\n### High R-hat\n\n**Symptom:** `R-hat > 1.01`\n\n**Solutions:**\n1. Run longer chains: `tune=2000, draws=5000`\n2. Check for multimodality\n3. Improve initialization with ADVI\n\n### Slow Sampling\n\n**Solutions:**\n1. Use ADVI initialization\n2. Reduce model complexity\n3. Increase parallelization: `cores=8, chains=8`\n4. Use variational inference if appropriate\n\n## Best Practices\n\n### Model Building\n\n1. **Always standardize predictors** for better sampling\n2. **Use weakly informative priors** (not flat)\n3. **Use named dimensions** (`dims`) for clarity\n4. **Non-centered parameterization** for hierarchical models\n5. **Check prior predictive** before fitting\n\n### Sampling\n\n1. **Run multiple chains** (at least 4) for convergence\n2. **Use `target_accept=0.9`** as baseline (higher if needed)\n3. **Include `log_likelihood=True`** for model comparison\n4. **Set random seed** for reproducibility\n\n### Validation\n\n1. **Check diagnostics** before interpretation (R-hat, ESS, divergences)\n2. **Posterior predictive check** for model validation\n3. **Compare multiple models** when appropriate\n4. **Report uncertainty** (HDI intervals, not just point estimates)\n\n### Workflow\n\n1. Start simple, add complexity gradually\n2. Prior predictive check  Fit  Diagnostics  Posterior predictive check\n3. Iterate on model specification based on checks\n4. Document assumptions and prior choices\n\n## Resources\n\nThis skill includes:\n\n### References (`references/`)\n\n- **`distributions.md`**: Comprehensive catalog of PyMC distributions organized by category (continuous, discrete, multivariate, mixture, time series). Use when selecting priors or likelihoods.\n\n- **`sampling_inference.md`**: Detailed guide to sampling algorithms (NUTS, Metropolis, SMC), variational inference (ADVI, SVGD), and handling sampling issues. Use when encountering convergence problems or choosing inference methods.\n\n- **`workflows.md`**: Complete workflow examples and code patterns for common model types, data preparation, prior selection, and model validation. Use as a cookbook for standard Bayesian analyses.\n\n### Scripts (`scripts/`)\n\n- **`model_diagnostics.py`**: Automated diagnostic checking and report generation. Functions: `check_diagnostics()` for quick checks, `create_diagnostic_report()` for comprehensive analysis with plots.\n\n- **`model_comparison.py`**: Model comparison utilities using LOO/WAIC. Functions: `compare_models()`, `check_loo_reliability()`, `model_averaging()`.\n\n### Templates (`assets/`)\n\n- **`linear_regression_template.py`**: Complete template for Bayesian linear regression with full workflow (data prep, prior checks, fitting, diagnostics, predictions).\n\n- **`hierarchical_model_template.py`**: Complete template for hierarchical/multilevel models with non-centered parameterization and group-level analysis.\n\n## Quick Reference\n\n### Model Building\n```python\nwith pm.Model(coords={'var': names}) as model:\n    # Priors\n    param = pm.Normal('param', mu=0, sigma=1, dims='var')\n    # Likelihood\n    y = pm.Normal('y', mu=..., sigma=..., observed=data)\n```\n\n### Sampling\n```python\nidata = pm.sample(draws=2000, tune=1000, chains=4, target_accept=0.9)\n```\n\n### Diagnostics\n```python\nfrom scripts.model_diagnostics import check_diagnostics\ncheck_diagnostics(idata)\n```\n\n### Model Comparison\n```python\nfrom scripts.model_comparison import compare_models\ncompare_models({'m1': idata1, 'm2': idata2}, ic='loo')\n```\n\n### Predictions\n```python\nwith model:\n    pm.set_data({'X': X_new})\n    pred = pm.sample_posterior_predictive(idata.posterior)\n```\n\n## Additional Notes\n\n- PyMC integrates with ArviZ for visualization and diagnostics\n- Use `pm.model_to_graphviz(model)` to visualize model structure\n- Save results with `idata.to_netcdf('results.nc')`\n- Load with `az.from_netcdf('results.nc')`\n- For very large models, consider minibatch ADVI or data subsampling\n",
        "data/k-dense-ai/pymoo/SKILL.md": "---\nname: pymoo\ndescription: \"Multi-objective optimization framework. NSGA-II, NSGA-III, MOEA/D, Pareto fronts, constraint handling, benchmarks (ZDT, DTLZ), for engineering design and optimization problems.\"\n---\n\n# Pymoo - Multi-Objective Optimization in Python\n\n## Overview\n\nPymoo is a comprehensive Python framework for optimization with emphasis on multi-objective problems. Solve single and multi-objective optimization using state-of-the-art algorithms (NSGA-II/III, MOEA/D), benchmark problems (ZDT, DTLZ), customizable genetic operators, and multi-criteria decision making methods. Excels at finding trade-off solutions (Pareto fronts) for problems with conflicting objectives.\n\n## When to Use This Skill\n\nThis skill should be used when:\n- Solving optimization problems with one or multiple objectives\n- Finding Pareto-optimal solutions and analyzing trade-offs\n- Implementing evolutionary algorithms (GA, DE, PSO, NSGA-II/III)\n- Working with constrained optimization problems\n- Benchmarking algorithms on standard test problems (ZDT, DTLZ, WFG)\n- Customizing genetic operators (crossover, mutation, selection)\n- Visualizing high-dimensional optimization results\n- Making decisions from multiple competing solutions\n- Handling binary, discrete, continuous, or mixed-variable problems\n\n## Core Concepts\n\n### The Unified Interface\n\nPymoo uses a consistent `minimize()` function for all optimization tasks:\n\n```python\nfrom pymoo.optimize import minimize\n\nresult = minimize(\n    problem,        # What to optimize\n    algorithm,      # How to optimize\n    termination,    # When to stop\n    seed=1,\n    verbose=True\n)\n```\n\n**Result object contains:**\n- `result.X`: Decision variables of optimal solution(s)\n- `result.F`: Objective values of optimal solution(s)\n- `result.G`: Constraint violations (if constrained)\n- `result.algorithm`: Algorithm object with history\n\n### Problem Types\n\n**Single-objective:** One objective to minimize/maximize\n**Multi-objective:** 2-3 conflicting objectives  Pareto front\n**Many-objective:** 4+ objectives  High-dimensional Pareto front\n**Constrained:** Objectives + inequality/equality constraints\n**Dynamic:** Time-varying objectives or constraints\n\n## Quick Start Workflows\n\n### Workflow 1: Single-Objective Optimization\n\n**When:** Optimizing one objective function\n\n**Steps:**\n1. Define or select problem\n2. Choose single-objective algorithm (GA, DE, PSO, CMA-ES)\n3. Configure termination criteria\n4. Run optimization\n5. Extract best solution\n\n**Example:**\n```python\nfrom pymoo.algorithms.soo.nonconvex.ga import GA\nfrom pymoo.problems import get_problem\nfrom pymoo.optimize import minimize\n\n# Built-in problem\nproblem = get_problem(\"rastrigin\", n_var=10)\n\n# Configure Genetic Algorithm\nalgorithm = GA(\n    pop_size=100,\n    eliminate_duplicates=True\n)\n\n# Optimize\nresult = minimize(\n    problem,\n    algorithm,\n    ('n_gen', 200),\n    seed=1,\n    verbose=True\n)\n\nprint(f\"Best solution: {result.X}\")\nprint(f\"Best objective: {result.F[0]}\")\n```\n\n**See:** `scripts/single_objective_example.py` for complete example\n\n### Workflow 2: Multi-Objective Optimization (2-3 objectives)\n\n**When:** Optimizing 2-3 conflicting objectives, need Pareto front\n\n**Algorithm choice:** NSGA-II (standard for bi/tri-objective)\n\n**Steps:**\n1. Define multi-objective problem\n2. Configure NSGA-II\n3. Run optimization to obtain Pareto front\n4. Visualize trade-offs\n5. Apply decision making (optional)\n\n**Example:**\n```python\nfrom pymoo.algorithms.moo.nsga2 import NSGA2\nfrom pymoo.problems import get_problem\nfrom pymoo.optimize import minimize\nfrom pymoo.visualization.scatter import Scatter\n\n# Bi-objective benchmark problem\nproblem = get_problem(\"zdt1\")\n\n# NSGA-II algorithm\nalgorithm = NSGA2(pop_size=100)\n\n# Optimize\nresult = minimize(problem, algorithm, ('n_gen', 200), seed=1)\n\n# Visualize Pareto front\nplot = Scatter()\nplot.add(result.F, label=\"Obtained Front\")\nplot.add(problem.pareto_front(), label=\"True Front\", alpha=0.3)\nplot.show()\n\nprint(f\"Found {len(result.F)} Pareto-optimal solutions\")\n```\n\n**See:** `scripts/multi_objective_example.py` for complete example\n\n### Workflow 3: Many-Objective Optimization (4+ objectives)\n\n**When:** Optimizing 4 or more objectives\n\n**Algorithm choice:** NSGA-III (designed for many objectives)\n\n**Key difference:** Must provide reference directions for population guidance\n\n**Steps:**\n1. Define many-objective problem\n2. Generate reference directions\n3. Configure NSGA-III with reference directions\n4. Run optimization\n5. Visualize using Parallel Coordinate Plot\n\n**Example:**\n```python\nfrom pymoo.algorithms.moo.nsga3 import NSGA3\nfrom pymoo.problems import get_problem\nfrom pymoo.optimize import minimize\nfrom pymoo.util.ref_dirs import get_reference_directions\nfrom pymoo.visualization.pcp import PCP\n\n# Many-objective problem (5 objectives)\nproblem = get_problem(\"dtlz2\", n_obj=5)\n\n# Generate reference directions (required for NSGA-III)\nref_dirs = get_reference_directions(\"das-dennis\", n_dim=5, n_partitions=12)\n\n# Configure NSGA-III\nalgorithm = NSGA3(ref_dirs=ref_dirs)\n\n# Optimize\nresult = minimize(problem, algorithm, ('n_gen', 300), seed=1)\n\n# Visualize with Parallel Coordinates\nplot = PCP(labels=[f\"f{i+1}\" for i in range(5)])\nplot.add(result.F, alpha=0.3)\nplot.show()\n```\n\n**See:** `scripts/many_objective_example.py` for complete example\n\n### Workflow 4: Custom Problem Definition\n\n**When:** Solving domain-specific optimization problem\n\n**Steps:**\n1. Extend `ElementwiseProblem` class\n2. Define `__init__` with problem dimensions and bounds\n3. Implement `_evaluate` method for objectives (and constraints)\n4. Use with any algorithm\n\n**Unconstrained example:**\n```python\nfrom pymoo.core.problem import ElementwiseProblem\nimport numpy as np\n\nclass MyProblem(ElementwiseProblem):\n    def __init__(self):\n        super().__init__(\n            n_var=2,              # Number of variables\n            n_obj=2,              # Number of objectives\n            xl=np.array([0, 0]),  # Lower bounds\n            xu=np.array([5, 5])   # Upper bounds\n        )\n\n    def _evaluate(self, x, out, *args, **kwargs):\n        # Define objectives\n        f1 = x[0]**2 + x[1]**2\n        f2 = (x[0]-1)**2 + (x[1]-1)**2\n\n        out[\"F\"] = [f1, f2]\n```\n\n**Constrained example:**\n```python\nclass ConstrainedProblem(ElementwiseProblem):\n    def __init__(self):\n        super().__init__(\n            n_var=2,\n            n_obj=2,\n            n_ieq_constr=2,        # Inequality constraints\n            n_eq_constr=1,         # Equality constraints\n            xl=np.array([0, 0]),\n            xu=np.array([5, 5])\n        )\n\n    def _evaluate(self, x, out, *args, **kwargs):\n        # Objectives\n        out[\"F\"] = [f1, f2]\n\n        # Inequality constraints (g <= 0)\n        out[\"G\"] = [g1, g2]\n\n        # Equality constraints (h = 0)\n        out[\"H\"] = [h1]\n```\n\n**Constraint formulation rules:**\n- Inequality: Express as `g(x) <= 0` (feasible when  0)\n- Equality: Express as `h(x) = 0` (feasible when = 0)\n- Convert `g(x) >= b` to `-(g(x) - b) <= 0`\n\n**See:** `scripts/custom_problem_example.py` for complete examples\n\n### Workflow 5: Constraint Handling\n\n**When:** Problem has feasibility constraints\n\n**Approach options:**\n\n**1. Feasibility First (Default - Recommended)**\n```python\nfrom pymoo.algorithms.moo.nsga2 import NSGA2\n\n# Works automatically with constrained problems\nalgorithm = NSGA2(pop_size=100)\nresult = minimize(problem, algorithm, termination)\n\n# Check feasibility\nfeasible = result.CV[:, 0] == 0  # CV = constraint violation\nprint(f\"Feasible solutions: {np.sum(feasible)}\")\n```\n\n**2. Penalty Method**\n```python\nfrom pymoo.constraints.as_penalty import ConstraintsAsPenalty\n\n# Wrap problem to convert constraints to penalties\nproblem_penalized = ConstraintsAsPenalty(problem, penalty=1e6)\n```\n\n**3. Constraint as Objective**\n```python\nfrom pymoo.constraints.as_obj import ConstraintsAsObjective\n\n# Treat constraint violation as additional objective\nproblem_with_cv = ConstraintsAsObjective(problem)\n```\n\n**4. Specialized Algorithms**\n```python\nfrom pymoo.algorithms.soo.nonconvex.sres import SRES\n\n# SRES has built-in constraint handling\nalgorithm = SRES()\n```\n\n**See:** `references/constraints_mcdm.md` for comprehensive constraint handling guide\n\n### Workflow 6: Decision Making from Pareto Front\n\n**When:** Have Pareto front, need to select preferred solution(s)\n\n**Steps:**\n1. Run multi-objective optimization\n2. Normalize objectives to [0, 1]\n3. Define preference weights\n4. Apply MCDM method\n5. Visualize selected solution\n\n**Example using Pseudo-Weights:**\n```python\nfrom pymoo.mcdm.pseudo_weights import PseudoWeights\nimport numpy as np\n\n# After obtaining result from multi-objective optimization\n# Normalize objectives\nF_norm = (result.F - result.F.min(axis=0)) / (result.F.max(axis=0) - result.F.min(axis=0))\n\n# Define preferences (must sum to 1)\nweights = np.array([0.3, 0.7])  # 30% f1, 70% f2\n\n# Apply decision making\ndm = PseudoWeights(weights)\nselected_idx = dm.do(F_norm)\n\n# Get selected solution\nbest_solution = result.X[selected_idx]\nbest_objectives = result.F[selected_idx]\n\nprint(f\"Selected solution: {best_solution}\")\nprint(f\"Objective values: {best_objectives}\")\n```\n\n**Other MCDM methods:**\n- Compromise Programming: Select closest to ideal point\n- Knee Point: Find balanced trade-off solutions\n- Hypervolume Contribution: Select most diverse subset\n\n**See:**\n- `scripts/decision_making_example.py` for complete example\n- `references/constraints_mcdm.md` for detailed MCDM methods\n\n### Workflow 7: Visualization\n\n**Choose visualization based on number of objectives:**\n\n**2 objectives: Scatter Plot**\n```python\nfrom pymoo.visualization.scatter import Scatter\n\nplot = Scatter(title=\"Bi-objective Results\")\nplot.add(result.F, color=\"blue\", alpha=0.7)\nplot.show()\n```\n\n**3 objectives: 3D Scatter**\n```python\nplot = Scatter(title=\"Tri-objective Results\")\nplot.add(result.F)  # Automatically renders in 3D\nplot.show()\n```\n\n**4+ objectives: Parallel Coordinate Plot**\n```python\nfrom pymoo.visualization.pcp import PCP\n\nplot = PCP(\n    labels=[f\"f{i+1}\" for i in range(n_obj)],\n    normalize_each_axis=True\n)\nplot.add(result.F, alpha=0.3)\nplot.show()\n```\n\n**Solution comparison: Petal Diagram**\n```python\nfrom pymoo.visualization.petal import Petal\n\nplot = Petal(\n    bounds=[result.F.min(axis=0), result.F.max(axis=0)],\n    labels=[\"Cost\", \"Weight\", \"Efficiency\"]\n)\nplot.add(solution_A, label=\"Design A\")\nplot.add(solution_B, label=\"Design B\")\nplot.show()\n```\n\n**See:** `references/visualization.md` for all visualization types and usage\n\n## Algorithm Selection Guide\n\n### Single-Objective Problems\n\n| Algorithm | Best For | Key Features |\n|-----------|----------|--------------|\n| **GA** | General-purpose | Flexible, customizable operators |\n| **DE** | Continuous optimization | Good global search |\n| **PSO** | Smooth landscapes | Fast convergence |\n| **CMA-ES** | Difficult/noisy problems | Self-adapting |\n\n### Multi-Objective Problems (2-3 objectives)\n\n| Algorithm | Best For | Key Features |\n|-----------|----------|--------------|\n| **NSGA-II** | Standard benchmark | Fast, reliable, well-tested |\n| **R-NSGA-II** | Preference regions | Reference point guidance |\n| **MOEA/D** | Decomposable problems | Scalarization approach |\n\n### Many-Objective Problems (4+ objectives)\n\n| Algorithm | Best For | Key Features |\n|-----------|----------|--------------|\n| **NSGA-III** | 4-15 objectives | Reference direction-based |\n| **RVEA** | Adaptive search | Reference vector evolution |\n| **AGE-MOEA** | Complex landscapes | Adaptive geometry |\n\n### Constrained Problems\n\n| Approach | Algorithm | When to Use |\n|----------|-----------|-------------|\n| Feasibility-first | Any algorithm | Large feasible region |\n| Specialized | SRES, ISRES | Heavy constraints |\n| Penalty | GA + penalty | Algorithm compatibility |\n\n**See:** `references/algorithms.md` for comprehensive algorithm reference\n\n## Benchmark Problems\n\n### Quick problem access:\n```python\nfrom pymoo.problems import get_problem\n\n# Single-objective\nproblem = get_problem(\"rastrigin\", n_var=10)\nproblem = get_problem(\"rosenbrock\", n_var=10)\n\n# Multi-objective\nproblem = get_problem(\"zdt1\")        # Convex front\nproblem = get_problem(\"zdt2\")        # Non-convex front\nproblem = get_problem(\"zdt3\")        # Disconnected front\n\n# Many-objective\nproblem = get_problem(\"dtlz2\", n_obj=5, n_var=12)\nproblem = get_problem(\"dtlz7\", n_obj=4)\n```\n\n**See:** `references/problems.md` for complete test problem reference\n\n## Genetic Operator Customization\n\n### Standard operator configuration:\n```python\nfrom pymoo.algorithms.soo.nonconvex.ga import GA\nfrom pymoo.operators.crossover.sbx import SBX\nfrom pymoo.operators.mutation.pm import PM\n\nalgorithm = GA(\n    pop_size=100,\n    crossover=SBX(prob=0.9, eta=15),\n    mutation=PM(eta=20),\n    eliminate_duplicates=True\n)\n```\n\n### Operator selection by variable type:\n\n**Continuous variables:**\n- Crossover: SBX (Simulated Binary Crossover)\n- Mutation: PM (Polynomial Mutation)\n\n**Binary variables:**\n- Crossover: TwoPointCrossover, UniformCrossover\n- Mutation: BitflipMutation\n\n**Permutations (TSP, scheduling):**\n- Crossover: OrderCrossover (OX)\n- Mutation: InversionMutation\n\n**See:** `references/operators.md` for comprehensive operator reference\n\n## Performance and Troubleshooting\n\n### Common issues and solutions:\n\n**Problem: Algorithm not converging**\n- Increase population size\n- Increase number of generations\n- Check if problem is multimodal (try different algorithms)\n- Verify constraints are correctly formulated\n\n**Problem: Poor Pareto front distribution**\n- For NSGA-III: Adjust reference directions\n- Increase population size\n- Check for duplicate elimination\n- Verify problem scaling\n\n**Problem: Few feasible solutions**\n- Use constraint-as-objective approach\n- Apply repair operators\n- Try SRES/ISRES for constrained problems\n- Check constraint formulation (should be g <= 0)\n\n**Problem: High computational cost**\n- Reduce population size\n- Decrease number of generations\n- Use simpler operators\n- Enable parallelization (if problem supports)\n\n### Best practices:\n\n1. **Normalize objectives** when scales differ significantly\n2. **Set random seed** for reproducibility\n3. **Save history** to analyze convergence: `save_history=True`\n4. **Visualize results** to understand solution quality\n5. **Compare with true Pareto front** when available\n6. **Use appropriate termination criteria** (generations, evaluations, tolerance)\n7. **Tune operator parameters** for problem characteristics\n\n## Resources\n\nThis skill includes comprehensive reference documentation and executable examples:\n\n### references/\nDetailed documentation for in-depth understanding:\n\n- **algorithms.md**: Complete algorithm reference with parameters, usage, and selection guidelines\n- **problems.md**: Benchmark test problems (ZDT, DTLZ, WFG) with characteristics\n- **operators.md**: Genetic operators (sampling, selection, crossover, mutation) with configuration\n- **visualization.md**: All visualization types with examples and selection guide\n- **constraints_mcdm.md**: Constraint handling techniques and multi-criteria decision making methods\n\n**Search patterns for references:**\n- Algorithm details: `grep -r \"NSGA-II\\|NSGA-III\\|MOEA/D\" references/`\n- Constraint methods: `grep -r \"Feasibility First\\|Penalty\\|Repair\" references/`\n- Visualization types: `grep -r \"Scatter\\|PCP\\|Petal\" references/`\n\n### scripts/\nExecutable examples demonstrating common workflows:\n\n- **single_objective_example.py**: Basic single-objective optimization with GA\n- **multi_objective_example.py**: Multi-objective optimization with NSGA-II, visualization\n- **many_objective_example.py**: Many-objective optimization with NSGA-III, reference directions\n- **custom_problem_example.py**: Defining custom problems (constrained and unconstrained)\n- **decision_making_example.py**: Multi-criteria decision making with different preferences\n\n**Run examples:**\n```bash\npython3 scripts/single_objective_example.py\npython3 scripts/multi_objective_example.py\npython3 scripts/many_objective_example.py\npython3 scripts/custom_problem_example.py\npython3 scripts/decision_making_example.py\n```\n\n## Additional Notes\n\n**Installation:**\n```bash\nuv pip install pymoo\n```\n\n**Dependencies:** NumPy, SciPy, matplotlib, autograd (optional for gradient-based)\n\n**Documentation:** https://pymoo.org/\n\n**Version:** This skill is based on pymoo 0.6.x\n\n**Common patterns:**\n- Always use `ElementwiseProblem` for custom problems\n- Constraints formulated as `g(x) <= 0` and `h(x) = 0`\n- Reference directions required for NSGA-III\n- Normalize objectives before MCDM\n- Use appropriate termination: `('n_gen', N)` or `get_termination(\"f_tol\", tol=0.001)`\n",
        "data/k-dense-ai/pyopenms/SKILL.md": "---\nname: pyopenms\ndescription: Python interface to OpenMS for mass spectrometry data analysis. Use for LC-MS/MS proteomics and metabolomics workflows including file handling (mzML, mzXML, mzTab, FASTA, pepXML, protXML, mzIdentML), signal processing, feature detection, peptide identification, and quantitative analysis. Apply when working with mass spectrometry data, analyzing proteomics experiments, or processing metabolomics datasets.\n---\n\n# PyOpenMS\n\n## Overview\n\nPyOpenMS provides Python bindings to the OpenMS library for computational mass spectrometry, enabling analysis of proteomics and metabolomics data. Use for handling mass spectrometry file formats, processing spectral data, detecting features, identifying peptides/proteins, and performing quantitative analysis.\n\n## Installation\n\nInstall using uv:\n\n```bash\nuv uv pip install pyopenms\n```\n\nVerify installation:\n\n```python\nimport pyopenms\nprint(pyopenms.__version__)\n```\n\n## Core Capabilities\n\nPyOpenMS organizes functionality into these domains:\n\n### 1. File I/O and Data Formats\n\nHandle mass spectrometry file formats and convert between representations.\n\n**Supported formats**: mzML, mzXML, TraML, mzTab, FASTA, pepXML, protXML, mzIdentML, featureXML, consensusXML, idXML\n\nBasic file reading:\n\n```python\nimport pyopenms as ms\n\n# Read mzML file\nexp = ms.MSExperiment()\nms.MzMLFile().load(\"data.mzML\", exp)\n\n# Access spectra\nfor spectrum in exp:\n    mz, intensity = spectrum.get_peaks()\n    print(f\"Spectrum: {len(mz)} peaks\")\n```\n\n**For detailed file handling**: See `references/file_io.md`\n\n### 2. Signal Processing\n\nProcess raw spectral data with smoothing, filtering, centroiding, and normalization.\n\nBasic spectrum processing:\n\n```python\n# Smooth spectrum with Gaussian filter\ngaussian = ms.GaussFilter()\nparams = gaussian.getParameters()\nparams.setValue(\"gaussian_width\", 0.1)\ngaussian.setParameters(params)\ngaussian.filterExperiment(exp)\n```\n\n**For algorithm details**: See `references/signal_processing.md`\n\n### 3. Feature Detection\n\nDetect and link features across spectra and samples for quantitative analysis.\n\n```python\n# Detect features\nff = ms.FeatureFinder()\nff.run(\"centroided\", exp, features, params, ms.FeatureMap())\n```\n\n**For complete workflows**: See `references/feature_detection.md`\n\n### 4. Peptide and Protein Identification\n\nIntegrate with search engines and process identification results.\n\n**Supported engines**: Comet, Mascot, MSGFPlus, XTandem, OMSSA, Myrimatch\n\nBasic identification workflow:\n\n```python\n# Load identification data\nprotein_ids = []\npeptide_ids = []\nms.IdXMLFile().load(\"identifications.idXML\", protein_ids, peptide_ids)\n\n# Apply FDR filtering\nfdr = ms.FalseDiscoveryRate()\nfdr.apply(peptide_ids)\n```\n\n**For detailed workflows**: See `references/identification.md`\n\n### 5. Metabolomics Analysis\n\nPerform untargeted metabolomics preprocessing and analysis.\n\nTypical workflow:\n1. Load and process raw data\n2. Detect features\n3. Align retention times across samples\n4. Link features to consensus map\n5. Annotate with compound databases\n\n**For complete metabolomics workflows**: See `references/metabolomics.md`\n\n## Data Structures\n\nPyOpenMS uses these primary objects:\n\n- **MSExperiment**: Collection of spectra and chromatograms\n- **MSSpectrum**: Single mass spectrum with m/z and intensity pairs\n- **MSChromatogram**: Chromatographic trace\n- **Feature**: Detected chromatographic peak with quality metrics\n- **FeatureMap**: Collection of features\n- **PeptideIdentification**: Search results for peptides\n- **ProteinIdentification**: Search results for proteins\n\n**For detailed documentation**: See `references/data_structures.md`\n\n## Common Workflows\n\n### Quick Start: Load and Explore Data\n\n```python\nimport pyopenms as ms\n\n# Load mzML file\nexp = ms.MSExperiment()\nms.MzMLFile().load(\"sample.mzML\", exp)\n\n# Get basic statistics\nprint(f\"Number of spectra: {exp.getNrSpectra()}\")\nprint(f\"Number of chromatograms: {exp.getNrChromatograms()}\")\n\n# Examine first spectrum\nspec = exp.getSpectrum(0)\nprint(f\"MS level: {spec.getMSLevel()}\")\nprint(f\"Retention time: {spec.getRT()}\")\nmz, intensity = spec.get_peaks()\nprint(f\"Peaks: {len(mz)}\")\n```\n\n### Parameter Management\n\nMost algorithms use a parameter system:\n\n```python\n# Get algorithm parameters\nalgo = ms.GaussFilter()\nparams = algo.getParameters()\n\n# View available parameters\nfor param in params.keys():\n    print(f\"{param}: {params.getValue(param)}\")\n\n# Modify parameters\nparams.setValue(\"gaussian_width\", 0.2)\nalgo.setParameters(params)\n```\n\n### Export to Pandas\n\nConvert data to pandas DataFrames for analysis:\n\n```python\nimport pyopenms as ms\nimport pandas as pd\n\n# Load feature map\nfm = ms.FeatureMap()\nms.FeatureXMLFile().load(\"features.featureXML\", fm)\n\n# Convert to DataFrame\ndf = fm.get_df()\nprint(df.head())\n```\n\n## Integration with Other Tools\n\nPyOpenMS integrates with:\n- **Pandas**: Export data to DataFrames\n- **NumPy**: Work with peak arrays\n- **Scikit-learn**: Machine learning on MS data\n- **Matplotlib/Seaborn**: Visualization\n- **R**: Via rpy2 bridge\n\n## Resources\n\n- **Official documentation**: https://pyopenms.readthedocs.io\n- **OpenMS documentation**: https://www.openms.org\n- **GitHub**: https://github.com/OpenMS/OpenMS\n\n## References\n\n- `references/file_io.md` - Comprehensive file format handling\n- `references/signal_processing.md` - Signal processing algorithms\n- `references/feature_detection.md` - Feature detection and linking\n- `references/identification.md` - Peptide and protein identification\n- `references/metabolomics.md` - Metabolomics-specific workflows\n- `references/data_structures.md` - Core objects and data structures\n",
        "data/k-dense-ai/pysam/SKILL.md": "---\nname: pysam\ndescription: \"Genomic file toolkit. Read/write SAM/BAM/CRAM alignments, VCF/BCF variants, FASTA/FASTQ sequences, extract regions, calculate coverage, for NGS data processing pipelines.\"\n---\n\n# Pysam\n\n## Overview\n\nPysam is a Python module for reading, manipulating, and writing genomic datasets. Read/write SAM/BAM/CRAM alignment files, VCF/BCF variant files, and FASTA/FASTQ sequences with a Pythonic interface to htslib. Query tabix-indexed files, perform pileup analysis for coverage, and execute samtools/bcftools commands.\n\n## When to Use This Skill\n\nThis skill should be used when:\n- Working with sequencing alignment files (BAM/CRAM)\n- Analyzing genetic variants (VCF/BCF)\n- Extracting reference sequences or gene regions\n- Processing raw sequencing data (FASTQ)\n- Calculating coverage or read depth\n- Implementing bioinformatics analysis pipelines\n- Quality control of sequencing data\n- Variant calling and annotation workflows\n\n## Quick Start\n\n### Installation\n```bash\nuv pip install pysam\n```\n\n### Basic Examples\n\n**Read alignment file:**\n```python\nimport pysam\n\n# Open BAM file and fetch reads in region\nsamfile = pysam.AlignmentFile(\"example.bam\", \"rb\")\nfor read in samfile.fetch(\"chr1\", 1000, 2000):\n    print(f\"{read.query_name}: {read.reference_start}\")\nsamfile.close()\n```\n\n**Read variant file:**\n```python\n# Open VCF file and iterate variants\nvcf = pysam.VariantFile(\"variants.vcf\")\nfor variant in vcf:\n    print(f\"{variant.chrom}:{variant.pos} {variant.ref}>{variant.alts}\")\nvcf.close()\n```\n\n**Query reference sequence:**\n```python\n# Open FASTA and extract sequence\nfasta = pysam.FastaFile(\"reference.fasta\")\nsequence = fasta.fetch(\"chr1\", 1000, 2000)\nprint(sequence)\nfasta.close()\n```\n\n## Core Capabilities\n\n### 1. Alignment File Operations (SAM/BAM/CRAM)\n\nUse the `AlignmentFile` class to work with aligned sequencing reads. This is appropriate for analyzing mapping results, calculating coverage, extracting reads, or quality control.\n\n**Common operations:**\n- Open and read BAM/SAM/CRAM files\n- Fetch reads from specific genomic regions\n- Filter reads by mapping quality, flags, or other criteria\n- Write filtered or modified alignments\n- Calculate coverage statistics\n- Perform pileup analysis (base-by-base coverage)\n- Access read sequences, quality scores, and alignment information\n\n**Reference:** See `references/alignment_files.md` for detailed documentation on:\n- Opening and reading alignment files\n- AlignedSegment attributes and methods\n- Region-based fetching with `fetch()`\n- Pileup analysis for coverage\n- Writing and creating BAM files\n- Coordinate systems and indexing\n- Performance optimization tips\n\n### 2. Variant File Operations (VCF/BCF)\n\nUse the `VariantFile` class to work with genetic variants from variant calling pipelines. This is appropriate for variant analysis, filtering, annotation, or population genetics.\n\n**Common operations:**\n- Read and write VCF/BCF files\n- Query variants in specific regions\n- Access variant information (position, alleles, quality)\n- Extract genotype data for samples\n- Filter variants by quality, allele frequency, or other criteria\n- Annotate variants with additional information\n- Subset samples or regions\n\n**Reference:** See `references/variant_files.md` for detailed documentation on:\n- Opening and reading variant files\n- VariantRecord attributes and methods\n- Accessing INFO and FORMAT fields\n- Working with genotypes and samples\n- Creating and writing VCF files\n- Filtering and subsetting variants\n- Multi-sample VCF operations\n\n### 3. Sequence File Operations (FASTA/FASTQ)\n\nUse `FastaFile` for random access to reference sequences and `FastxFile` for reading raw sequencing data. This is appropriate for extracting gene sequences, validating variants against reference, or processing raw reads.\n\n**Common operations:**\n- Query reference sequences by genomic coordinates\n- Extract sequences for genes or regions of interest\n- Read FASTQ files with quality scores\n- Validate variant reference alleles\n- Calculate sequence statistics\n- Filter reads by quality or length\n- Convert between FASTA and FASTQ formats\n\n**Reference:** See `references/sequence_files.md` for detailed documentation on:\n- FASTA file access and indexing\n- Extracting sequences by region\n- Handling reverse complement for genes\n- Reading FASTQ files sequentially\n- Quality score conversion and filtering\n- Working with tabix-indexed files (BED, GTF, GFF)\n- Common sequence processing patterns\n\n### 4. Integrated Bioinformatics Workflows\n\nPysam excels at integrating multiple file types for comprehensive genomic analyses. Common workflows combine alignment files, variant files, and reference sequences.\n\n**Common workflows:**\n- Calculate coverage statistics for specific regions\n- Validate variants against aligned reads\n- Annotate variants with coverage information\n- Extract sequences around variant positions\n- Filter alignments or variants based on multiple criteria\n- Generate coverage tracks for visualization\n- Quality control across multiple data types\n\n**Reference:** See `references/common_workflows.md` for detailed examples of:\n- Quality control workflows (BAM statistics, reference consistency)\n- Coverage analysis (per-base coverage, low coverage detection)\n- Variant analysis (annotation, filtering by read support)\n- Sequence extraction (variant contexts, gene sequences)\n- Read filtering and subsetting\n- Integration patterns (BAM+VCF, VCF+BED, etc.)\n- Performance optimization for complex workflows\n\n## Key Concepts\n\n### Coordinate Systems\n\n**Critical:** Pysam uses **0-based, half-open** coordinates (Python convention):\n- Start positions are 0-based (first base is position 0)\n- End positions are exclusive (not included in the range)\n- Region 1000-2000 includes bases 1000-1999 (1000 bases total)\n\n**Exception:** Region strings in `fetch()` follow samtools convention (1-based):\n```python\nsamfile.fetch(\"chr1\", 999, 2000)      # 0-based: positions 999-1999\nsamfile.fetch(\"chr1:1000-2000\")       # 1-based string: positions 1000-2000\n```\n\n**VCF files:** Use 1-based coordinates in the file format, but `VariantRecord.start` is 0-based.\n\n### Indexing Requirements\n\nRandom access to specific genomic regions requires index files:\n- **BAM files**: Require `.bai` index (create with `pysam.index()`)\n- **CRAM files**: Require `.crai` index\n- **FASTA files**: Require `.fai` index (create with `pysam.faidx()`)\n- **VCF.gz files**: Require `.tbi` tabix index (create with `pysam.tabix_index()`)\n- **BCF files**: Require `.csi` index\n\nWithout an index, use `fetch(until_eof=True)` for sequential reading.\n\n### File Modes\n\nSpecify format when opening files:\n- `\"rb\"` - Read BAM (binary)\n- `\"r\"` - Read SAM (text)\n- `\"rc\"` - Read CRAM\n- `\"wb\"` - Write BAM\n- `\"w\"` - Write SAM\n- `\"wc\"` - Write CRAM\n\n### Performance Considerations\n\n1. **Always use indexed files** for random access operations\n2. **Use `pileup()` for column-wise analysis** instead of repeated fetch operations\n3. **Use `count()` for counting** instead of iterating and counting manually\n4. **Process regions in parallel** when analyzing independent genomic regions\n5. **Close files explicitly** to free resources\n6. **Use `until_eof=True`** for sequential processing without index\n7. **Avoid multiple iterators** unless necessary (use `multiple_iterators=True` if needed)\n\n## Common Pitfalls\n\n1. **Coordinate confusion:** Remember 0-based vs 1-based systems in different contexts\n2. **Missing indices:** Many operations require index filescreate them first\n3. **Partial overlaps:** `fetch()` returns reads overlapping region boundaries, not just those fully contained\n4. **Iterator scope:** Keep pileup iterator references alive to avoid \"PileupProxy accessed after iterator finished\" errors\n5. **Quality score editing:** Cannot modify `query_qualities` in place after changing `query_sequence`create a copy first\n6. **Stream limitations:** Only stdin/stdout are supported for streaming, not arbitrary Python file objects\n7. **Thread safety:** While GIL is released during I/O, comprehensive thread-safety hasn't been fully validated\n\n## Command-Line Tools\n\nPysam provides access to samtools and bcftools commands:\n\n```python\n# Sort BAM file\npysam.samtools.sort(\"-o\", \"sorted.bam\", \"input.bam\")\n\n# Index BAM\npysam.samtools.index(\"sorted.bam\")\n\n# View specific region\npysam.samtools.view(\"-b\", \"-o\", \"region.bam\", \"input.bam\", \"chr1:1000-2000\")\n\n# BCF tools\npysam.bcftools.view(\"-O\", \"z\", \"-o\", \"output.vcf.gz\", \"input.vcf\")\n```\n\n**Error handling:**\n```python\ntry:\n    pysam.samtools.sort(\"-o\", \"output.bam\", \"input.bam\")\nexcept pysam.SamtoolsError as e:\n    print(f\"Error: {e}\")\n```\n\n## Resources\n\n### references/\n\nDetailed documentation for each major capability:\n\n- **alignment_files.md** - Complete guide to SAM/BAM/CRAM operations, including AlignmentFile class, AlignedSegment attributes, fetch operations, pileup analysis, and writing alignments\n\n- **variant_files.md** - Complete guide to VCF/BCF operations, including VariantFile class, VariantRecord attributes, genotype handling, INFO/FORMAT fields, and multi-sample operations\n\n- **sequence_files.md** - Complete guide to FASTA/FASTQ operations, including FastaFile and FastxFile classes, sequence extraction, quality score handling, and tabix-indexed file access\n\n- **common_workflows.md** - Practical examples of integrated bioinformatics workflows combining multiple file types, including quality control, coverage analysis, variant validation, and sequence extraction\n\n## Getting Help\n\nFor detailed information on specific operations, refer to the appropriate reference document:\n\n- Working with BAM files or calculating coverage  `alignment_files.md`\n- Analyzing variants or genotypes  `variant_files.md`\n- Extracting sequences or processing FASTQ  `sequence_files.md`\n- Complex workflows integrating multiple file types  `common_workflows.md`\n\nOfficial documentation: https://pysam.readthedocs.io/\n",
        "data/k-dense-ai/pytdc/SKILL.md": "---\nname: pytdc\ndescription: \"Therapeutics Data Commons. AI-ready drug discovery datasets (ADME, toxicity, DTI), benchmarks, scaffold splits, molecular oracles, for therapeutic ML and pharmacological prediction.\"\n---\n\n# PyTDC (Therapeutics Data Commons)\n\n## Overview\n\nPyTDC is an open-science platform providing AI-ready datasets and benchmarks for drug discovery and development. Access curated datasets spanning the entire therapeutics pipeline with standardized evaluation metrics and meaningful data splits, organized into three categories: single-instance prediction (molecular/protein properties), multi-instance prediction (drug-target interactions, DDI), and generation (molecule generation, retrosynthesis).\n\n## When to Use This Skill\n\nThis skill should be used when:\n- Working with drug discovery or therapeutic ML datasets\n- Benchmarking machine learning models on standardized pharmaceutical tasks\n- Predicting molecular properties (ADME, toxicity, bioactivity)\n- Predicting drug-target or drug-drug interactions\n- Generating novel molecules with desired properties\n- Accessing curated datasets with proper train/test splits (scaffold, cold-split)\n- Using molecular oracles for property optimization\n\n## Installation & Setup\n\nInstall PyTDC using pip:\n\n```bash\nuv pip install PyTDC\n```\n\nTo upgrade to the latest version:\n\n```bash\nuv pip install PyTDC --upgrade\n```\n\nCore dependencies (automatically installed):\n- numpy, pandas, tqdm, seaborn, scikit_learn, fuzzywuzzy\n\nAdditional packages are installed automatically as needed for specific features.\n\n## Quick Start\n\nThe basic pattern for accessing any TDC dataset follows this structure:\n\n```python\nfrom tdc.<problem> import <Task>\ndata = <Task>(name='<Dataset>')\nsplit = data.get_split(method='scaffold', seed=1, frac=[0.7, 0.1, 0.2])\ndf = data.get_data(format='df')\n```\n\nWhere:\n- `<problem>`: One of `single_pred`, `multi_pred`, or `generation`\n- `<Task>`: Specific task category (e.g., ADME, DTI, MolGen)\n- `<Dataset>`: Dataset name within that task\n\n**Example - Loading ADME data:**\n\n```python\nfrom tdc.single_pred import ADME\ndata = ADME(name='Caco2_Wang')\nsplit = data.get_split(method='scaffold')\n# Returns dict with 'train', 'valid', 'test' DataFrames\n```\n\n## Single-Instance Prediction Tasks\n\nSingle-instance prediction involves forecasting properties of individual biomedical entities (molecules, proteins, etc.).\n\n### Available Task Categories\n\n#### 1. ADME (Absorption, Distribution, Metabolism, Excretion)\n\nPredict pharmacokinetic properties of drug molecules.\n\n```python\nfrom tdc.single_pred import ADME\ndata = ADME(name='Caco2_Wang')  # Intestinal permeability\n# Other datasets: HIA_Hou, Bioavailability_Ma, Lipophilicity_AstraZeneca, etc.\n```\n\n**Common ADME datasets:**\n- Caco2 - Intestinal permeability\n- HIA - Human intestinal absorption\n- Bioavailability - Oral bioavailability\n- Lipophilicity - Octanol-water partition coefficient\n- Solubility - Aqueous solubility\n- BBB - Blood-brain barrier penetration\n- CYP - Cytochrome P450 metabolism\n\n#### 2. Toxicity (Tox)\n\nPredict toxicity and adverse effects of compounds.\n\n```python\nfrom tdc.single_pred import Tox\ndata = Tox(name='hERG')  # Cardiotoxicity\n# Other datasets: AMES, DILI, Carcinogens_Lagunin, etc.\n```\n\n**Common toxicity datasets:**\n- hERG - Cardiac toxicity\n- AMES - Mutagenicity\n- DILI - Drug-induced liver injury\n- Carcinogens - Carcinogenicity\n- ClinTox - Clinical trial toxicity\n\n#### 3. HTS (High-Throughput Screening)\n\nBioactivity predictions from screening data.\n\n```python\nfrom tdc.single_pred import HTS\ndata = HTS(name='SARSCoV2_Vitro_Touret')\n```\n\n#### 4. QM (Quantum Mechanics)\n\nQuantum mechanical properties of molecules.\n\n```python\nfrom tdc.single_pred import QM\ndata = QM(name='QM7')\n```\n\n#### 5. Other Single Prediction Tasks\n\n- **Yields**: Chemical reaction yield prediction\n- **Epitope**: Epitope prediction for biologics\n- **Develop**: Development-stage predictions\n- **CRISPROutcome**: Gene editing outcome prediction\n\n### Data Format\n\nSingle prediction datasets typically return DataFrames with columns:\n- `Drug_ID` or `Compound_ID`: Unique identifier\n- `Drug` or `X`: SMILES string or molecular representation\n- `Y`: Target label (continuous or binary)\n\n## Multi-Instance Prediction Tasks\n\nMulti-instance prediction involves forecasting properties of interactions between multiple biomedical entities.\n\n### Available Task Categories\n\n#### 1. DTI (Drug-Target Interaction)\n\nPredict binding affinity between drugs and protein targets.\n\n```python\nfrom tdc.multi_pred import DTI\ndata = DTI(name='BindingDB_Kd')\nsplit = data.get_split()\n```\n\n**Available datasets:**\n- BindingDB_Kd - Dissociation constant (52,284 pairs)\n- BindingDB_IC50 - Half-maximal inhibitory concentration (991,486 pairs)\n- BindingDB_Ki - Inhibition constant (375,032 pairs)\n- DAVIS, KIBA - Kinase binding datasets\n\n**Data format:** Drug_ID, Target_ID, Drug (SMILES), Target (sequence), Y (binding affinity)\n\n#### 2. DDI (Drug-Drug Interaction)\n\nPredict interactions between drug pairs.\n\n```python\nfrom tdc.multi_pred import DDI\ndata = DDI(name='DrugBank')\nsplit = data.get_split()\n```\n\nMulti-class classification task predicting interaction types. Dataset contains 191,808 DDI pairs with 1,706 drugs.\n\n#### 3. PPI (Protein-Protein Interaction)\n\nPredict protein-protein interactions.\n\n```python\nfrom tdc.multi_pred import PPI\ndata = PPI(name='HuRI')\n```\n\n#### 4. Other Multi-Prediction Tasks\n\n- **GDA**: Gene-disease associations\n- **DrugRes**: Drug resistance prediction\n- **DrugSyn**: Drug synergy prediction\n- **PeptideMHC**: Peptide-MHC binding\n- **AntibodyAff**: Antibody affinity prediction\n- **MTI**: miRNA-target interactions\n- **Catalyst**: Catalyst prediction\n- **TrialOutcome**: Clinical trial outcome prediction\n\n## Generation Tasks\n\nGeneration tasks involve creating novel biomedical entities with desired properties.\n\n### 1. Molecular Generation (MolGen)\n\nGenerate diverse, novel molecules with desirable chemical properties.\n\n```python\nfrom tdc.generation import MolGen\ndata = MolGen(name='ChEMBL_V29')\nsplit = data.get_split()\n```\n\nUse with oracles to optimize for specific properties:\n\n```python\nfrom tdc import Oracle\noracle = Oracle(name='GSK3B')\nscore = oracle('CC(C)Cc1ccc(cc1)C(C)C(O)=O')  # Evaluate SMILES\n```\n\nSee `references/oracles.md` for all available oracle functions.\n\n### 2. Retrosynthesis (RetroSyn)\n\nPredict reactants needed to synthesize a target molecule.\n\n```python\nfrom tdc.generation import RetroSyn\ndata = RetroSyn(name='USPTO')\nsplit = data.get_split()\n```\n\nDataset contains 1,939,253 reactions from USPTO database.\n\n### 3. Paired Molecule Generation\n\nGenerate molecule pairs (e.g., prodrug-drug pairs).\n\n```python\nfrom tdc.generation import PairMolGen\ndata = PairMolGen(name='Prodrug')\n```\n\nFor detailed oracle documentation and molecular generation workflows, refer to `references/oracles.md` and `scripts/molecular_generation.py`.\n\n## Benchmark Groups\n\nBenchmark groups provide curated collections of related datasets for systematic model evaluation.\n\n### ADMET Benchmark Group\n\n```python\nfrom tdc.benchmark_group import admet_group\ngroup = admet_group(path='data/')\n\n# Get benchmark datasets\nbenchmark = group.get('Caco2_Wang')\npredictions = {}\n\nfor seed in [1, 2, 3, 4, 5]:\n    train, valid = benchmark['train'], benchmark['valid']\n    # Train model here\n    predictions[seed] = model.predict(benchmark['test'])\n\n# Evaluate with required 5 seeds\nresults = group.evaluate(predictions)\n```\n\n**ADMET Group includes 22 datasets** covering absorption, distribution, metabolism, excretion, and toxicity.\n\n### Other Benchmark Groups\n\nAvailable benchmark groups include collections for:\n- ADMET properties\n- Drug-target interactions\n- Drug combination prediction\n- And more specialized therapeutic tasks\n\nFor benchmark evaluation workflows, see `scripts/benchmark_evaluation.py`.\n\n## Data Functions\n\nTDC provides comprehensive data processing utilities organized into four categories.\n\n### 1. Dataset Splits\n\nRetrieve train/validation/test partitions with various strategies:\n\n```python\n# Scaffold split (default for most tasks)\nsplit = data.get_split(method='scaffold', seed=1, frac=[0.7, 0.1, 0.2])\n\n# Random split\nsplit = data.get_split(method='random', seed=42, frac=[0.8, 0.1, 0.1])\n\n# Cold split (for DTI/DDI tasks)\nsplit = data.get_split(method='cold_drug', seed=1)  # Unseen drugs in test\nsplit = data.get_split(method='cold_target', seed=1)  # Unseen targets in test\n```\n\n**Available split strategies:**\n- `random`: Random shuffling\n- `scaffold`: Scaffold-based (for chemical diversity)\n- `cold_drug`, `cold_target`, `cold_drug_target`: For DTI tasks\n- `temporal`: Time-based splits for temporal datasets\n\n### 2. Model Evaluation\n\nUse standardized metrics for evaluation:\n\n```python\nfrom tdc import Evaluator\n\n# For binary classification\nevaluator = Evaluator(name='ROC-AUC')\nscore = evaluator(y_true, y_pred)\n\n# For regression\nevaluator = Evaluator(name='RMSE')\nscore = evaluator(y_true, y_pred)\n```\n\n**Available metrics:** ROC-AUC, PR-AUC, F1, Accuracy, RMSE, MAE, R2, Spearman, Pearson, and more.\n\n### 3. Data Processing\n\nTDC provides 11 key processing utilities:\n\n```python\nfrom tdc.chem_utils import MolConvert\n\n# Molecule format conversion\nconverter = MolConvert(src='SMILES', dst='PyG')\npyg_graph = converter('CC(C)Cc1ccc(cc1)C(C)C(O)=O')\n```\n\n**Processing utilities include:**\n- Molecule format conversion (SMILES, SELFIES, PyG, DGL, ECFP, etc.)\n- Molecule filters (PAINS, drug-likeness)\n- Label binarization and unit conversion\n- Data balancing (over/under-sampling)\n- Negative sampling for pair data\n- Graph transformation\n- Entity retrieval (CID to SMILES, UniProt to sequence)\n\nFor comprehensive utilities documentation, see `references/utilities.md`.\n\n### 4. Molecule Generation Oracles\n\nTDC provides 17+ oracle functions for molecular optimization:\n\n```python\nfrom tdc import Oracle\n\n# Single oracle\noracle = Oracle(name='DRD2')\nscore = oracle('CC(C)Cc1ccc(cc1)C(C)C(O)=O')\n\n# Multiple oracles\noracle = Oracle(name='JNK3')\nscores = oracle(['SMILES1', 'SMILES2', 'SMILES3'])\n```\n\nFor complete oracle documentation, see `references/oracles.md`.\n\n## Advanced Features\n\n### Retrieve Available Datasets\n\n```python\nfrom tdc.utils import retrieve_dataset_names\n\n# Get all ADME datasets\nadme_datasets = retrieve_dataset_names('ADME')\n\n# Get all DTI datasets\ndti_datasets = retrieve_dataset_names('DTI')\n```\n\n### Label Transformations\n\n```python\n# Get label mapping\nlabel_map = data.get_label_map(name='DrugBank')\n\n# Convert labels\nfrom tdc.chem_utils import label_transform\ntransformed = label_transform(y, from_unit='nM', to_unit='p')\n```\n\n### Database Queries\n\n```python\nfrom tdc.utils import cid2smiles, uniprot2seq\n\n# Convert PubChem CID to SMILES\nsmiles = cid2smiles(2244)\n\n# Convert UniProt ID to amino acid sequence\nsequence = uniprot2seq('P12345')\n```\n\n## Common Workflows\n\n### Workflow 1: Train a Single Prediction Model\n\nSee `scripts/load_and_split_data.py` for a complete example:\n\n```python\nfrom tdc.single_pred import ADME\nfrom tdc import Evaluator\n\n# Load data\ndata = ADME(name='Caco2_Wang')\nsplit = data.get_split(method='scaffold', seed=42)\n\ntrain, valid, test = split['train'], split['valid'], split['test']\n\n# Train model (user implements)\n# model.fit(train['Drug'], train['Y'])\n\n# Evaluate\nevaluator = Evaluator(name='MAE')\n# score = evaluator(test['Y'], predictions)\n```\n\n### Workflow 2: Benchmark Evaluation\n\nSee `scripts/benchmark_evaluation.py` for a complete example with multiple seeds and proper evaluation protocol.\n\n### Workflow 3: Molecular Generation with Oracles\n\nSee `scripts/molecular_generation.py` for an example of goal-directed generation using oracle functions.\n\n## Resources\n\nThis skill includes bundled resources for common TDC workflows:\n\n### scripts/\n\n- `load_and_split_data.py`: Template for loading and splitting TDC datasets with various strategies\n- `benchmark_evaluation.py`: Template for running benchmark group evaluations with proper 5-seed protocol\n- `molecular_generation.py`: Template for molecular generation using oracle functions\n\n### references/\n\n- `datasets.md`: Comprehensive catalog of all available datasets organized by task type\n- `oracles.md`: Complete documentation of all 17+ molecule generation oracles\n- `utilities.md`: Detailed guide to data processing, splitting, and evaluation utilities\n\n## Additional Resources\n\n- **Official Website**: https://tdcommons.ai\n- **Documentation**: https://tdc.readthedocs.io\n- **GitHub**: https://github.com/mims-harvard/TDC\n- **Paper**: NeurIPS 2021 - \"Therapeutics Data Commons: Machine Learning Datasets and Tasks for Drug Discovery and Development\"\n",
        "data/k-dense-ai/pytorch-lightning/SKILL.md": "---\nname: pytorch-lightning\ndescription: \"Deep learning framework (PyTorch Lightning). Organize PyTorch code into LightningModules, configure Trainers for multi-GPU/TPU, implement data pipelines, callbacks, logging (W&B, TensorBoard), distributed training (DDP, FSDP, DeepSpeed), for scalable neural network training.\"\n---\n\n# PyTorch Lightning\n\n## Overview\n\nPyTorch Lightning is a deep learning framework that organizes PyTorch code to eliminate boilerplate while maintaining full flexibility. Automate training workflows, multi-device orchestration, and implement best practices for neural network training and scaling across multiple GPUs/TPUs.\n\n## When to Use This Skill\n\nThis skill should be used when:\n- Building, training, or deploying neural networks using PyTorch Lightning\n- Organizing PyTorch code into LightningModules\n- Configuring Trainers for multi-GPU/TPU training\n- Implementing data pipelines with LightningDataModules\n- Working with callbacks, logging, and distributed training strategies (DDP, FSDP, DeepSpeed)\n- Structuring deep learning projects professionally\n\n## Core Capabilities\n\n### 1. LightningModule - Model Definition\n\nOrganize PyTorch models into six logical sections:\n\n1. **Initialization** - `__init__()` and `setup()`\n2. **Training Loop** - `training_step(batch, batch_idx)`\n3. **Validation Loop** - `validation_step(batch, batch_idx)`\n4. **Test Loop** - `test_step(batch, batch_idx)`\n5. **Prediction** - `predict_step(batch, batch_idx)`\n6. **Optimizer Configuration** - `configure_optimizers()`\n\n**Quick template reference:** See `scripts/template_lightning_module.py` for a complete boilerplate.\n\n**Detailed documentation:** Read `references/lightning_module.md` for comprehensive method documentation, hooks, properties, and best practices.\n\n### 2. Trainer - Training Automation\n\nThe Trainer automates the training loop, device management, gradient operations, and callbacks. Key features:\n\n- Multi-GPU/TPU support with strategy selection (DDP, FSDP, DeepSpeed)\n- Automatic mixed precision training\n- Gradient accumulation and clipping\n- Checkpointing and early stopping\n- Progress bars and logging\n\n**Quick setup reference:** See `scripts/quick_trainer_setup.py` for common Trainer configurations.\n\n**Detailed documentation:** Read `references/trainer.md` for all parameters, methods, and configuration options.\n\n### 3. LightningDataModule - Data Pipeline Organization\n\nEncapsulate all data processing steps in a reusable class:\n\n1. `prepare_data()` - Download and process data (single-process)\n2. `setup()` - Create datasets and apply transforms (per-GPU)\n3. `train_dataloader()` - Return training DataLoader\n4. `val_dataloader()` - Return validation DataLoader\n5. `test_dataloader()` - Return test DataLoader\n\n**Quick template reference:** See `scripts/template_datamodule.py` for a complete boilerplate.\n\n**Detailed documentation:** Read `references/data_module.md` for method details and usage patterns.\n\n### 4. Callbacks - Extensible Training Logic\n\nAdd custom functionality at specific training hooks without modifying your LightningModule. Built-in callbacks include:\n\n- **ModelCheckpoint** - Save best/latest models\n- **EarlyStopping** - Stop when metrics plateau\n- **LearningRateMonitor** - Track LR scheduler changes\n- **BatchSizeFinder** - Auto-determine optimal batch size\n\n**Detailed documentation:** Read `references/callbacks.md` for built-in callbacks and custom callback creation.\n\n### 5. Logging - Experiment Tracking\n\nIntegrate with multiple logging platforms:\n\n- TensorBoard (default)\n- Weights & Biases (WandbLogger)\n- MLflow (MLFlowLogger)\n- Neptune (NeptuneLogger)\n- Comet (CometLogger)\n- CSV (CSVLogger)\n\nLog metrics using `self.log(\"metric_name\", value)` in any LightningModule method.\n\n**Detailed documentation:** Read `references/logging.md` for logger setup and configuration.\n\n### 6. Distributed Training - Scale to Multiple Devices\n\nChoose the right strategy based on model size:\n\n- **DDP** - For models <500M parameters (ResNet, smaller transformers)\n- **FSDP** - For models 500M+ parameters (large transformers, recommended for Lightning users)\n- **DeepSpeed** - For cutting-edge features and fine-grained control\n\nConfigure with: `Trainer(strategy=\"ddp\", accelerator=\"gpu\", devices=4)`\n\n**Detailed documentation:** Read `references/distributed_training.md` for strategy comparison and configuration.\n\n### 7. Best Practices\n\n- Device agnostic code - Use `self.device` instead of `.cuda()`\n- Hyperparameter saving - Use `self.save_hyperparameters()` in `__init__()`\n- Metric logging - Use `self.log()` for automatic aggregation across devices\n- Reproducibility - Use `seed_everything()` and `Trainer(deterministic=True)`\n- Debugging - Use `Trainer(fast_dev_run=True)` to test with 1 batch\n\n**Detailed documentation:** Read `references/best_practices.md` for common patterns and pitfalls.\n\n## Quick Workflow\n\n1. **Define model:**\n   ```python\n   class MyModel(L.LightningModule):\n       def __init__(self):\n           super().__init__()\n           self.save_hyperparameters()\n           self.model = YourNetwork()\n\n       def training_step(self, batch, batch_idx):\n           x, y = batch\n           loss = F.cross_entropy(self.model(x), y)\n           self.log(\"train_loss\", loss)\n           return loss\n\n       def configure_optimizers(self):\n           return torch.optim.Adam(self.parameters())\n   ```\n\n2. **Prepare data:**\n   ```python\n   # Option 1: Direct DataLoaders\n   train_loader = DataLoader(train_dataset, batch_size=32)\n\n   # Option 2: LightningDataModule (recommended for reusability)\n   dm = MyDataModule(batch_size=32)\n   ```\n\n3. **Train:**\n   ```python\n   trainer = L.Trainer(max_epochs=10, accelerator=\"gpu\", devices=2)\n   trainer.fit(model, train_loader)  # or trainer.fit(model, datamodule=dm)\n   ```\n\n## Resources\n\n### scripts/\nExecutable Python templates for common PyTorch Lightning patterns:\n\n- `template_lightning_module.py` - Complete LightningModule boilerplate\n- `template_datamodule.py` - Complete LightningDataModule boilerplate\n- `quick_trainer_setup.py` - Common Trainer configuration examples\n\n### references/\nDetailed documentation for each PyTorch Lightning component:\n\n- `lightning_module.md` - Comprehensive LightningModule guide (methods, hooks, properties)\n- `trainer.md` - Trainer configuration and parameters\n- `data_module.md` - LightningDataModule patterns and methods\n- `callbacks.md` - Built-in and custom callbacks\n- `logging.md` - Logger integrations and usage\n- `distributed_training.md` - DDP, FSDP, DeepSpeed comparison and setup\n- `best_practices.md` - Common patterns, tips, and pitfalls\n",
        "data/k-dense-ai/rdkit/SKILL.md": "---\nname: rdkit\ndescription: \"Cheminformatics toolkit for fine-grained molecular control. SMILES/SDF parsing, descriptors (MW, LogP, TPSA), fingerprints, substructure search, 2D/3D generation, similarity, reactions. For standard workflows with simpler interface, use datamol (wrapper around RDKit). Use rdkit for advanced control, custom sanitization, specialized algorithms.\"\n---\n\n# RDKit Cheminformatics Toolkit\n\n## Overview\n\nRDKit is a comprehensive cheminformatics library providing Python APIs for molecular analysis and manipulation. This skill provides guidance for reading/writing molecular structures, calculating descriptors, fingerprinting, substructure searching, chemical reactions, 2D/3D coordinate generation, and molecular visualization. Use this skill for drug discovery, computational chemistry, and cheminformatics research tasks.\n\n## Core Capabilities\n\n### 1. Molecular I/O and Creation\n\n**Reading Molecules:**\n\nRead molecular structures from various formats:\n\n```python\nfrom rdkit import Chem\n\n# From SMILES strings\nmol = Chem.MolFromSmiles('Cc1ccccc1')  # Returns Mol object or None\n\n# From MOL files\nmol = Chem.MolFromMolFile('path/to/file.mol')\n\n# From MOL blocks (string data)\nmol = Chem.MolFromMolBlock(mol_block_string)\n\n# From InChI\nmol = Chem.MolFromInchi('InChI=1S/C6H6/c1-2-4-6-5-3-1/h1-6H')\n```\n\n**Writing Molecules:**\n\nConvert molecules to text representations:\n\n```python\n# To canonical SMILES\nsmiles = Chem.MolToSmiles(mol)\n\n# To MOL block\nmol_block = Chem.MolToMolBlock(mol)\n\n# To InChI\ninchi = Chem.MolToInchi(mol)\n```\n\n**Batch Processing:**\n\nFor processing multiple molecules, use Supplier/Writer objects:\n\n```python\n# Read SDF files\nsuppl = Chem.SDMolSupplier('molecules.sdf')\nfor mol in suppl:\n    if mol is not None:  # Check for parsing errors\n        # Process molecule\n        pass\n\n# Read SMILES files\nsuppl = Chem.SmilesMolSupplier('molecules.smi', titleLine=False)\n\n# For large files or compressed data\nwith gzip.open('molecules.sdf.gz') as f:\n    suppl = Chem.ForwardSDMolSupplier(f)\n    for mol in suppl:\n        # Process molecule\n        pass\n\n# Multithreaded processing for large datasets\nsuppl = Chem.MultithreadedSDMolSupplier('molecules.sdf')\n\n# Write molecules to SDF\nwriter = Chem.SDWriter('output.sdf')\nfor mol in molecules:\n    writer.write(mol)\nwriter.close()\n```\n\n**Important Notes:**\n- All `MolFrom*` functions return `None` on failure with error messages\n- Always check for `None` before processing molecules\n- Molecules are automatically sanitized on import (validates valence, perceives aromaticity)\n\n### 2. Molecular Sanitization and Validation\n\nRDKit automatically sanitizes molecules during parsing, executing 13 steps including valence checking, aromaticity perception, and chirality assignment.\n\n**Sanitization Control:**\n\n```python\n# Disable automatic sanitization\nmol = Chem.MolFromSmiles('C1=CC=CC=C1', sanitize=False)\n\n# Manual sanitization\nChem.SanitizeMol(mol)\n\n# Detect problems before sanitization\nproblems = Chem.DetectChemistryProblems(mol)\nfor problem in problems:\n    print(problem.GetType(), problem.Message())\n\n# Partial sanitization (skip specific steps)\nfrom rdkit.Chem import rdMolStandardize\nChem.SanitizeMol(mol, sanitizeOps=Chem.SANITIZE_ALL ^ Chem.SANITIZE_PROPERTIES)\n```\n\n**Common Sanitization Issues:**\n- Atoms with explicit valence exceeding maximum allowed will raise exceptions\n- Invalid aromatic rings will cause kekulization errors\n- Radical electrons may not be properly assigned without explicit specification\n\n### 3. Molecular Analysis and Properties\n\n**Accessing Molecular Structure:**\n\n```python\n# Iterate atoms and bonds\nfor atom in mol.GetAtoms():\n    print(atom.GetSymbol(), atom.GetIdx(), atom.GetDegree())\n\nfor bond in mol.GetBonds():\n    print(bond.GetBeginAtomIdx(), bond.GetEndAtomIdx(), bond.GetBondType())\n\n# Ring information\nring_info = mol.GetRingInfo()\nring_info.NumRings()\nring_info.AtomRings()  # Returns tuples of atom indices\n\n# Check if atom is in ring\natom = mol.GetAtomWithIdx(0)\natom.IsInRing()\natom.IsInRingSize(6)  # Check for 6-membered rings\n\n# Find smallest set of smallest rings (SSSR)\nfrom rdkit.Chem import GetSymmSSSR\nrings = GetSymmSSSR(mol)\n```\n\n**Stereochemistry:**\n\n```python\n# Find chiral centers\nfrom rdkit.Chem import FindMolChiralCenters\nchiral_centers = FindMolChiralCenters(mol, includeUnassigned=True)\n# Returns list of (atom_idx, chirality) tuples\n\n# Assign stereochemistry from 3D coordinates\nfrom rdkit.Chem import AssignStereochemistryFrom3D\nAssignStereochemistryFrom3D(mol)\n\n# Check bond stereochemistry\nbond = mol.GetBondWithIdx(0)\nstereo = bond.GetStereo()  # STEREONONE, STEREOZ, STEREOE, etc.\n```\n\n**Fragment Analysis:**\n\n```python\n# Get disconnected fragments\nfrags = Chem.GetMolFrags(mol, asMols=True)\n\n# Fragment on specific bonds\nfrom rdkit.Chem import FragmentOnBonds\nfrag_mol = FragmentOnBonds(mol, [bond_idx1, bond_idx2])\n\n# Count ring systems\nfrom rdkit.Chem.Scaffolds import MurckoScaffold\nscaffold = MurckoScaffold.GetScaffoldForMol(mol)\n```\n\n### 4. Molecular Descriptors and Properties\n\n**Basic Descriptors:**\n\n```python\nfrom rdkit.Chem import Descriptors\n\n# Molecular weight\nmw = Descriptors.MolWt(mol)\nexact_mw = Descriptors.ExactMolWt(mol)\n\n# LogP (lipophilicity)\nlogp = Descriptors.MolLogP(mol)\n\n# Topological polar surface area\ntpsa = Descriptors.TPSA(mol)\n\n# Number of hydrogen bond donors/acceptors\nhbd = Descriptors.NumHDonors(mol)\nhba = Descriptors.NumHAcceptors(mol)\n\n# Number of rotatable bonds\nrot_bonds = Descriptors.NumRotatableBonds(mol)\n\n# Number of aromatic rings\naromatic_rings = Descriptors.NumAromaticRings(mol)\n```\n\n**Batch Descriptor Calculation:**\n\n```python\n# Calculate all descriptors at once\nall_descriptors = Descriptors.CalcMolDescriptors(mol)\n# Returns dictionary: {'MolWt': 180.16, 'MolLogP': 1.23, ...}\n\n# Get list of available descriptor names\ndescriptor_names = [desc[0] for desc in Descriptors._descList]\n```\n\n**Lipinski's Rule of Five:**\n\n```python\n# Check drug-likeness\nmw = Descriptors.MolWt(mol) <= 500\nlogp = Descriptors.MolLogP(mol) <= 5\nhbd = Descriptors.NumHDonors(mol) <= 5\nhba = Descriptors.NumHAcceptors(mol) <= 10\n\nis_drug_like = mw and logp and hbd and hba\n```\n\n### 5. Fingerprints and Molecular Similarity\n\n**Fingerprint Types:**\n\n```python\nfrom rdkit.Chem import AllChem, RDKFingerprint\nfrom rdkit.Chem.AtomPairs import Pairs, Torsions\nfrom rdkit.Chem import MACCSkeys\n\n# RDKit topological fingerprint\nfp = Chem.RDKFingerprint(mol)\n\n# Morgan fingerprints (circular fingerprints, similar to ECFP)\nfp = AllChem.GetMorganFingerprint(mol, radius=2)\nfp_bits = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=2048)\n\n# MACCS keys (166-bit structural key)\nfp = MACCSkeys.GenMACCSKeys(mol)\n\n# Atom pair fingerprints\nfp = Pairs.GetAtomPairFingerprint(mol)\n\n# Topological torsion fingerprints\nfp = Torsions.GetTopologicalTorsionFingerprint(mol)\n\n# Avalon fingerprints (if available)\nfrom rdkit.Avalon import pyAvalonTools\nfp = pyAvalonTools.GetAvalonFP(mol)\n```\n\n**Similarity Calculation:**\n\n```python\nfrom rdkit import DataStructs\n\n# Calculate Tanimoto similarity\nfp1 = AllChem.GetMorganFingerprintAsBitVect(mol1, radius=2)\nfp2 = AllChem.GetMorganFingerprintAsBitVect(mol2, radius=2)\nsimilarity = DataStructs.TanimotoSimilarity(fp1, fp2)\n\n# Calculate similarity for multiple molecules\nsimilarities = DataStructs.BulkTanimotoSimilarity(fp1, [fp2, fp3, fp4])\n\n# Other similarity metrics\ndice = DataStructs.DiceSimilarity(fp1, fp2)\ncosine = DataStructs.CosineSimilarity(fp1, fp2)\n```\n\n**Clustering and Diversity:**\n\n```python\n# Butina clustering based on fingerprint similarity\nfrom rdkit.ML.Cluster import Butina\n\n# Calculate distance matrix\ndists = []\nfps = [AllChem.GetMorganFingerprintAsBitVect(mol, 2) for mol in mols]\nfor i in range(len(fps)):\n    sims = DataStructs.BulkTanimotoSimilarity(fps[i], fps[:i])\n    dists.extend([1-sim for sim in sims])\n\n# Cluster with distance cutoff\nclusters = Butina.ClusterData(dists, len(fps), distThresh=0.3, isDistData=True)\n```\n\n### 6. Substructure Searching and SMARTS\n\n**Basic Substructure Matching:**\n\n```python\n# Define query using SMARTS\nquery = Chem.MolFromSmarts('[#6]1:[#6]:[#6]:[#6]:[#6]:[#6]:1')  # Benzene ring\n\n# Check if molecule contains substructure\nhas_match = mol.HasSubstructMatch(query)\n\n# Get all matches (returns tuple of tuples with atom indices)\nmatches = mol.GetSubstructMatches(query)\n\n# Get only first match\nmatch = mol.GetSubstructMatch(query)\n```\n\n**Common SMARTS Patterns:**\n\n```python\n# Primary alcohols\nprimary_alcohol = Chem.MolFromSmarts('[CH2][OH1]')\n\n# Carboxylic acids\ncarboxylic_acid = Chem.MolFromSmarts('C(=O)[OH]')\n\n# Amides\namide = Chem.MolFromSmarts('C(=O)N')\n\n# Aromatic heterocycles\naromatic_n = Chem.MolFromSmarts('[nR]')  # Aromatic nitrogen in ring\n\n# Macrocycles (rings > 12 atoms)\nmacrocycle = Chem.MolFromSmarts('[r{12-}]')\n```\n\n**Matching Rules:**\n- Unspecified properties in query match any value in target\n- Hydrogens are ignored unless explicitly specified\n- Charged query atom won't match uncharged target atom\n- Aromatic query atom won't match aliphatic target atom (unless query is generic)\n\n### 7. Chemical Reactions\n\n**Reaction SMARTS:**\n\n```python\nfrom rdkit.Chem import AllChem\n\n# Define reaction using SMARTS: reactants >> products\nrxn = AllChem.ReactionFromSmarts('[C:1]=[O:2]>>[C:1][O:2]')  # Ketone reduction\n\n# Apply reaction to molecules\nreactants = (mol1,)\nproducts = rxn.RunReactants(reactants)\n\n# Products is tuple of tuples (one tuple per product set)\nfor product_set in products:\n    for product in product_set:\n        # Sanitize product\n        Chem.SanitizeMol(product)\n```\n\n**Reaction Features:**\n- Atom mapping preserves specific atoms between reactants and products\n- Dummy atoms in products are replaced by corresponding reactant atoms\n- \"Any\" bonds inherit bond order from reactants\n- Chirality preserved unless explicitly changed\n\n**Reaction Similarity:**\n\n```python\n# Generate reaction fingerprints\nfp = AllChem.CreateDifferenceFingerprintForReaction(rxn)\n\n# Compare reactions\nsimilarity = DataStructs.TanimotoSimilarity(fp1, fp2)\n```\n\n### 8. 2D and 3D Coordinate Generation\n\n**2D Coordinate Generation:**\n\n```python\nfrom rdkit.Chem import AllChem\n\n# Generate 2D coordinates for depiction\nAllChem.Compute2DCoords(mol)\n\n# Align molecule to template structure\ntemplate = Chem.MolFromSmiles('c1ccccc1')\nAllChem.Compute2DCoords(template)\nAllChem.GenerateDepictionMatching2DStructure(mol, template)\n```\n\n**3D Coordinate Generation and Conformers:**\n\n```python\n# Generate single 3D conformer using ETKDG\nAllChem.EmbedMolecule(mol, randomSeed=42)\n\n# Generate multiple conformers\nconf_ids = AllChem.EmbedMultipleConfs(mol, numConfs=10, randomSeed=42)\n\n# Optimize geometry with force field\nAllChem.UFFOptimizeMolecule(mol)  # UFF force field\nAllChem.MMFFOptimizeMolecule(mol)  # MMFF94 force field\n\n# Optimize all conformers\nfor conf_id in conf_ids:\n    AllChem.MMFFOptimizeMolecule(mol, confId=conf_id)\n\n# Calculate RMSD between conformers\nfrom rdkit.Chem import AllChem\nrms = AllChem.GetConformerRMS(mol, conf_id1, conf_id2)\n\n# Align molecules\nAllChem.AlignMol(probe_mol, ref_mol)\n```\n\n**Constrained Embedding:**\n\n```python\n# Embed with part of molecule constrained to specific coordinates\nAllChem.ConstrainedEmbed(mol, core_mol)\n```\n\n### 9. Molecular Visualization\n\n**Basic Drawing:**\n\n```python\nfrom rdkit.Chem import Draw\n\n# Draw single molecule to PIL image\nimg = Draw.MolToImage(mol, size=(300, 300))\nimg.save('molecule.png')\n\n# Draw to file directly\nDraw.MolToFile(mol, 'molecule.png')\n\n# Draw multiple molecules in grid\nmols = [mol1, mol2, mol3, mol4]\nimg = Draw.MolsToGridImage(mols, molsPerRow=2, subImgSize=(200, 200))\n```\n\n**Highlighting Substructures:**\n\n```python\n# Highlight substructure match\nquery = Chem.MolFromSmarts('c1ccccc1')\nmatch = mol.GetSubstructMatch(query)\n\nimg = Draw.MolToImage(mol, highlightAtoms=match)\n\n# Custom highlight colors\nhighlight_colors = {atom_idx: (1, 0, 0) for atom_idx in match}  # Red\nimg = Draw.MolToImage(mol, highlightAtoms=match,\n                      highlightAtomColors=highlight_colors)\n```\n\n**Customizing Visualization:**\n\n```python\nfrom rdkit.Chem.Draw import rdMolDraw2D\n\n# Create drawer with custom options\ndrawer = rdMolDraw2D.MolDraw2DCairo(300, 300)\nopts = drawer.drawOptions()\n\n# Customize options\nopts.addAtomIndices = True\nopts.addStereoAnnotation = True\nopts.bondLineWidth = 2\n\n# Draw molecule\ndrawer.DrawMolecule(mol)\ndrawer.FinishDrawing()\n\n# Save to file\nwith open('molecule.png', 'wb') as f:\n    f.write(drawer.GetDrawingText())\n```\n\n**Jupyter Notebook Integration:**\n\n```python\n# Enable inline display in Jupyter\nfrom rdkit.Chem.Draw import IPythonConsole\n\n# Customize default display\nIPythonConsole.ipython_useSVG = True  # Use SVG instead of PNG\nIPythonConsole.molSize = (300, 300)   # Default size\n\n# Molecules now display automatically\nmol  # Shows molecule image\n```\n\n**Visualizing Fingerprint Bits:**\n\n```python\n# Show what molecular features a fingerprint bit represents\nfrom rdkit.Chem import Draw\n\n# For Morgan fingerprints\nbit_info = {}\nfp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, bitInfo=bit_info)\n\n# Draw environment for specific bit\nimg = Draw.DrawMorganBit(mol, bit_id, bit_info)\n```\n\n### 10. Molecular Modification\n\n**Adding/Removing Hydrogens:**\n\n```python\n# Add explicit hydrogens\nmol_h = Chem.AddHs(mol)\n\n# Remove explicit hydrogens\nmol = Chem.RemoveHs(mol_h)\n```\n\n**Kekulization and Aromaticity:**\n\n```python\n# Convert aromatic bonds to alternating single/double\nChem.Kekulize(mol)\n\n# Set aromaticity\nChem.SetAromaticity(mol)\n```\n\n**Replacing Substructures:**\n\n```python\n# Replace substructure with another structure\nquery = Chem.MolFromSmarts('c1ccccc1')  # Benzene\nreplacement = Chem.MolFromSmiles('C1CCCCC1')  # Cyclohexane\n\nnew_mol = Chem.ReplaceSubstructs(mol, query, replacement)[0]\n```\n\n**Neutralizing Charges:**\n\n```python\n# Remove formal charges by adding/removing hydrogens\nfrom rdkit.Chem.MolStandardize import rdMolStandardize\n\n# Using Uncharger\nuncharger = rdMolStandardize.Uncharger()\nmol_neutral = uncharger.uncharge(mol)\n```\n\n### 11. Working with Molecular Hashes and Standardization\n\n**Molecular Hashing:**\n\n```python\nfrom rdkit.Chem import rdMolHash\n\n# Generate Murcko scaffold hash\nscaffold_hash = rdMolHash.MolHash(mol, rdMolHash.HashFunction.MurckoScaffold)\n\n# Canonical SMILES hash\ncanonical_hash = rdMolHash.MolHash(mol, rdMolHash.HashFunction.CanonicalSmiles)\n\n# Regioisomer hash (ignores stereochemistry)\nregio_hash = rdMolHash.MolHash(mol, rdMolHash.HashFunction.Regioisomer)\n```\n\n**Randomized SMILES:**\n\n```python\n# Generate random SMILES representations (for data augmentation)\nfrom rdkit.Chem import MolToRandomSmilesVect\n\nrandom_smiles = MolToRandomSmilesVect(mol, numSmiles=10, randomSeed=42)\n```\n\n### 12. Pharmacophore and 3D Features\n\n**Pharmacophore Features:**\n\n```python\nfrom rdkit.Chem import ChemicalFeatures\nfrom rdkit import RDConfig\nimport os\n\n# Load feature factory\nfdef_path = os.path.join(RDConfig.RDDataDir, 'BaseFeatures.fdef')\nfactory = ChemicalFeatures.BuildFeatureFactory(fdef_path)\n\n# Get pharmacophore features\nfeatures = factory.GetFeaturesForMol(mol)\n\nfor feat in features:\n    print(feat.GetFamily(), feat.GetType(), feat.GetAtomIds())\n```\n\n## Common Workflows\n\n### Drug-likeness Analysis\n\n```python\nfrom rdkit import Chem\nfrom rdkit.Chem import Descriptors\n\ndef analyze_druglikeness(smiles):\n    mol = Chem.MolFromSmiles(smiles)\n    if mol is None:\n        return None\n\n    # Calculate Lipinski descriptors\n    results = {\n        'MW': Descriptors.MolWt(mol),\n        'LogP': Descriptors.MolLogP(mol),\n        'HBD': Descriptors.NumHDonors(mol),\n        'HBA': Descriptors.NumHAcceptors(mol),\n        'TPSA': Descriptors.TPSA(mol),\n        'RotBonds': Descriptors.NumRotatableBonds(mol)\n    }\n\n    # Check Lipinski's Rule of Five\n    results['Lipinski'] = (\n        results['MW'] <= 500 and\n        results['LogP'] <= 5 and\n        results['HBD'] <= 5 and\n        results['HBA'] <= 10\n    )\n\n    return results\n```\n\n### Similarity Screening\n\n```python\nfrom rdkit import Chem\nfrom rdkit.Chem import AllChem\nfrom rdkit import DataStructs\n\ndef similarity_screen(query_smiles, database_smiles, threshold=0.7):\n    query_mol = Chem.MolFromSmiles(query_smiles)\n    query_fp = AllChem.GetMorganFingerprintAsBitVect(query_mol, 2)\n\n    hits = []\n    for idx, smiles in enumerate(database_smiles):\n        mol = Chem.MolFromSmiles(smiles)\n        if mol:\n            fp = AllChem.GetMorganFingerprintAsBitVect(mol, 2)\n            sim = DataStructs.TanimotoSimilarity(query_fp, fp)\n            if sim >= threshold:\n                hits.append((idx, smiles, sim))\n\n    return sorted(hits, key=lambda x: x[2], reverse=True)\n```\n\n### Substructure Filtering\n\n```python\nfrom rdkit import Chem\n\ndef filter_by_substructure(smiles_list, pattern_smarts):\n    query = Chem.MolFromSmarts(pattern_smarts)\n\n    hits = []\n    for smiles in smiles_list:\n        mol = Chem.MolFromSmiles(smiles)\n        if mol and mol.HasSubstructMatch(query):\n            hits.append(smiles)\n\n    return hits\n```\n\n## Best Practices\n\n### Error Handling\n\nAlways check for `None` when parsing molecules:\n\n```python\nmol = Chem.MolFromSmiles(smiles)\nif mol is None:\n    print(f\"Failed to parse: {smiles}\")\n    continue\n```\n\n### Performance Optimization\n\n**Use binary formats for storage:**\n\n```python\nimport pickle\n\n# Pickle molecules for fast loading\nwith open('molecules.pkl', 'wb') as f:\n    pickle.dump(mols, f)\n\n# Load pickled molecules (much faster than reparsing)\nwith open('molecules.pkl', 'rb') as f:\n    mols = pickle.load(f)\n```\n\n**Use bulk operations:**\n\n```python\n# Calculate fingerprints for all molecules at once\nfps = [AllChem.GetMorganFingerprintAsBitVect(mol, 2) for mol in mols]\n\n# Use bulk similarity calculations\nsimilarities = DataStructs.BulkTanimotoSimilarity(fps[0], fps[1:])\n```\n\n### Thread Safety\n\nRDKit operations are generally thread-safe for:\n- Molecule I/O (SMILES, mol blocks)\n- Coordinate generation\n- Fingerprinting and descriptors\n- Substructure searching\n- Reactions\n- Drawing\n\n**Not thread-safe:** MolSuppliers when accessed concurrently.\n\n### Memory Management\n\nFor large datasets:\n\n```python\n# Use ForwardSDMolSupplier to avoid loading entire file\nwith open('large.sdf') as f:\n    suppl = Chem.ForwardSDMolSupplier(f)\n    for mol in suppl:\n        # Process one molecule at a time\n        pass\n\n# Use MultithreadedSDMolSupplier for parallel processing\nsuppl = Chem.MultithreadedSDMolSupplier('large.sdf', numWriterThreads=4)\n```\n\n## Common Pitfalls\n\n1. **Forgetting to check for None:** Always validate molecules after parsing\n2. **Sanitization failures:** Use `DetectChemistryProblems()` to debug\n3. **Missing hydrogens:** Use `AddHs()` when calculating properties that depend on hydrogen\n4. **2D vs 3D:** Generate appropriate coordinates before visualization or 3D analysis\n5. **SMARTS matching rules:** Remember that unspecified properties match anything\n6. **Thread safety with MolSuppliers:** Don't share supplier objects across threads\n\n## Resources\n\n### references/\n\nThis skill includes detailed API reference documentation:\n\n- `api_reference.md` - Comprehensive listing of RDKit modules, functions, and classes organized by functionality\n- `descriptors_reference.md` - Complete list of available molecular descriptors with descriptions\n- `smarts_patterns.md` - Common SMARTS patterns for functional groups and structural features\n\nLoad these references when needing specific API details, parameter information, or pattern examples.\n\n### scripts/\n\nExample scripts for common RDKit workflows:\n\n- `molecular_properties.py` - Calculate comprehensive molecular properties and descriptors\n- `similarity_search.py` - Perform fingerprint-based similarity screening\n- `substructure_filter.py` - Filter molecules by substructure patterns\n\nThese scripts can be executed directly or used as templates for custom workflows.\n",
        "data/k-dense-ai/reactome-database/SKILL.md": "---\nname: reactome-database\ndescription: \"Query Reactome REST API for pathway analysis, enrichment, gene-pathway mapping, disease pathways, molecular interactions, expression analysis, for systems biology studies.\"\n---\n\n# Reactome Database\n\n## Overview\n\nReactome is a free, open-source, curated pathway database with 2,825+ human pathways. Query biological pathways, perform overrepresentation and expression analysis, map genes to pathways, explore molecular interactions via REST API and Python client for systems biology research.\n\n## When to Use This Skill\n\nThis skill should be used when:\n- Performing pathway enrichment analysis on gene or protein lists\n- Analyzing gene expression data to identify relevant biological pathways\n- Querying specific pathway information, reactions, or molecular interactions\n- Mapping genes or proteins to biological pathways and processes\n- Exploring disease-related pathways and mechanisms\n- Visualizing analysis results in the Reactome Pathway Browser\n- Conducting comparative pathway analysis across species\n\n## Core Capabilities\n\nReactome provides two main API services and a Python client library:\n\n### 1. Content Service - Data Retrieval\n\nQuery and retrieve biological pathway data, molecular interactions, and entity information.\n\n**Common operations:**\n- Retrieve pathway information and hierarchies\n- Query specific entities (proteins, reactions, complexes)\n- Get participating molecules in pathways\n- Access database version and metadata\n- Explore pathway compartments and locations\n\n**API Base URL:** `https://reactome.org/ContentService`\n\n### 2. Analysis Service - Pathway Analysis\n\nPerform computational analysis on gene lists and expression data.\n\n**Analysis types:**\n- **Overrepresentation Analysis**: Identify statistically significant pathways from gene/protein lists\n- **Expression Data Analysis**: Analyze gene expression datasets to find relevant pathways\n- **Species Comparison**: Compare pathway data across different organisms\n\n**API Base URL:** `https://reactome.org/AnalysisService`\n\n### 3. reactome2py Python Package\n\nPython client library that wraps Reactome API calls for easier programmatic access.\n\n**Installation:**\n```bash\nuv pip install reactome2py\n```\n\n**Note:** The reactome2py package (version 3.0.0, released January 2021) is functional but not actively maintained. For the most up-to-date functionality, consider using direct REST API calls.\n\n## Querying Pathway Data\n\n### Using Content Service REST API\n\nThe Content Service uses REST protocol and returns data in JSON or plain text formats.\n\n**Get database version:**\n```python\nimport requests\n\nresponse = requests.get(\"https://reactome.org/ContentService/data/database/version\")\nversion = response.text\nprint(f\"Reactome version: {version}\")\n```\n\n**Query a specific entity:**\n```python\nimport requests\n\nentity_id = \"R-HSA-69278\"  # Example pathway ID\nresponse = requests.get(f\"https://reactome.org/ContentService/data/query/{entity_id}\")\ndata = response.json()\n```\n\n**Get participating molecules in a pathway:**\n```python\nimport requests\n\nevent_id = \"R-HSA-69278\"\nresponse = requests.get(\n    f\"https://reactome.org/ContentService/data/event/{event_id}/participatingPhysicalEntities\"\n)\nmolecules = response.json()\n```\n\n### Using reactome2py Package\n\n```python\nimport reactome2py\nfrom reactome2py import content\n\n# Query pathway information\npathway_info = content.query_by_id(\"R-HSA-69278\")\n\n# Get database version\nversion = content.get_database_version()\n```\n\n**For detailed API endpoints and parameters**, refer to `references/api_reference.md` in this skill.\n\n## Performing Pathway Analysis\n\n### Overrepresentation Analysis\n\nSubmit a list of gene/protein identifiers to find enriched pathways.\n\n**Using REST API:**\n```python\nimport requests\n\n# Prepare identifier list\nidentifiers = [\"TP53\", \"BRCA1\", \"EGFR\", \"MYC\"]\ndata = \"\\n\".join(identifiers)\n\n# Submit analysis\nresponse = requests.post(\n    \"https://reactome.org/AnalysisService/identifiers/\",\n    headers={\"Content-Type\": \"text/plain\"},\n    data=data\n)\n\nresult = response.json()\ntoken = result[\"summary\"][\"token\"]  # Save token to retrieve results later\n\n# Access pathways\nfor pathway in result[\"pathways\"]:\n    print(f\"{pathway['stId']}: {pathway['name']} (p-value: {pathway['entities']['pValue']})\")\n```\n\n**Retrieve analysis by token:**\n```python\n# Token is valid for 7 days\nresponse = requests.get(f\"https://reactome.org/AnalysisService/token/{token}\")\nresults = response.json()\n```\n\n### Expression Data Analysis\n\nAnalyze gene expression datasets with quantitative values.\n\n**Input format (TSV with header starting with #):**\n```\n#Gene\tSample1\tSample2\tSample3\nTP53\t2.5\t3.1\t2.8\nBRCA1\t1.2\t1.5\t1.3\nEGFR\t4.5\t4.2\t4.8\n```\n\n**Submit expression data:**\n```python\nimport requests\n\n# Read TSV file\nwith open(\"expression_data.tsv\", \"r\") as f:\n    data = f.read()\n\nresponse = requests.post(\n    \"https://reactome.org/AnalysisService/identifiers/\",\n    headers={\"Content-Type\": \"text/plain\"},\n    data=data\n)\n\nresult = response.json()\n```\n\n### Species Projection\n\nMap identifiers to human pathways exclusively using the `/projection/` endpoint:\n\n```python\nresponse = requests.post(\n    \"https://reactome.org/AnalysisService/identifiers/projection/\",\n    headers={\"Content-Type\": \"text/plain\"},\n    data=data\n)\n```\n\n## Visualizing Results\n\nAnalysis results can be visualized in the Reactome Pathway Browser by constructing URLs with the analysis token:\n\n```python\ntoken = result[\"summary\"][\"token\"]\npathway_id = \"R-HSA-69278\"\nurl = f\"https://reactome.org/PathwayBrowser/#{pathway_id}&DTAB=AN&ANALYSIS={token}\"\nprint(f\"View results: {url}\")\n```\n\n## Working with Analysis Tokens\n\n- Analysis tokens are valid for **7 days**\n- Tokens allow retrieval of previously computed results without re-submission\n- Store tokens to access results across sessions\n- Use `GET /token/{TOKEN}` endpoint to retrieve results\n\n## Data Formats and Identifiers\n\n### Supported Identifier Types\n\nReactome accepts various identifier formats:\n- UniProt accessions (e.g., P04637)\n- Gene symbols (e.g., TP53)\n- Ensembl IDs (e.g., ENSG00000141510)\n- EntrezGene IDs (e.g., 7157)\n- ChEBI IDs for small molecules\n\nThe system automatically detects identifier types.\n\n### Input Format Requirements\n\n**For overrepresentation analysis:**\n- Plain text list of identifiers (one per line)\n- OR single column in TSV format\n\n**For expression analysis:**\n- TSV format with mandatory header row starting with \"#\"\n- Column 1: identifiers\n- Columns 2+: numeric expression values\n- Use period (.) as decimal separator\n\n### Output Format\n\nAll API responses return JSON containing:\n- `pathways`: Array of enriched pathways with statistical metrics\n- `summary`: Analysis metadata and token\n- `entities`: Matched and unmapped identifiers\n- Statistical values: pValue, FDR (false discovery rate)\n\n## Helper Scripts\n\nThis skill includes `scripts/reactome_query.py`, a helper script for common Reactome operations:\n\n```bash\n# Query pathway information\npython scripts/reactome_query.py query R-HSA-69278\n\n# Perform overrepresentation analysis\npython scripts/reactome_query.py analyze gene_list.txt\n\n# Get database version\npython scripts/reactome_query.py version\n```\n\n## Additional Resources\n\n- **API Documentation**: https://reactome.org/dev\n- **User Guide**: https://reactome.org/userguide\n- **Documentation Portal**: https://reactome.org/documentation\n- **Data Downloads**: https://reactome.org/download-data\n- **reactome2py Docs**: https://reactome.github.io/reactome2py/\n\nFor comprehensive API endpoint documentation, see `references/api_reference.md` in this skill.\n\n## Current Database Statistics (Version 94, September 2025)\n\n- 2,825 human pathways\n- 16,002 reactions\n- 11,630 proteins\n- 2,176 small molecules\n- 1,070 drugs\n- 41,373 literature references\n",
        "data/k-dense-ai/reportlab/SKILL.md": "---\nname: reportlab\ndescription: \"PDF generation toolkit. Create invoices, reports, certificates, forms, charts, tables, barcodes, QR codes, Canvas/Platypus APIs, for professional document automation.\"\n---\n\n# ReportLab PDF Generation\n\n## Overview\n\nReportLab is a powerful Python library for programmatic PDF generation. Create anything from simple documents to complex reports with tables, charts, images, and interactive forms.\n\n**Two main approaches:**\n- **Canvas API** (low-level): Direct drawing with coordinate-based positioning - use for precise layouts\n- **Platypus** (high-level): Flowing document layout with automatic page breaks - use for multi-page documents\n\n**Core capabilities:**\n- Text with rich formatting and custom fonts\n- Tables with complex styling and cell spanning\n- Charts (bar, line, pie, area, scatter)\n- Barcodes and QR codes (Code128, EAN, QR, etc.)\n- Images with transparency\n- PDF features (links, bookmarks, forms, encryption)\n\n## Choosing the Right Approach\n\n### Use Canvas API when:\n- Creating labels, business cards, certificates\n- Precise positioning is critical (x, y coordinates)\n- Single-page documents or simple layouts\n- Drawing graphics, shapes, and custom designs\n- Adding barcodes or QR codes at specific locations\n\n### Use Platypus when:\n- Creating multi-page documents (reports, articles, books)\n- Content should flow automatically across pages\n- Need headers/footers that repeat on each page\n- Working with paragraphs that can split across pages\n- Building complex documents with table of contents\n\n### Use Both when:\n- Complex reports need both flowing content AND precise positioning\n- Adding headers/footers to Platypus documents (use `onPage` callback with Canvas)\n- Embedding custom graphics (Canvas) within flowing documents (Platypus)\n\n## Quick Start Examples\n\n### Simple Canvas Document\n\n```python\nfrom reportlab.pdfgen import canvas\nfrom reportlab.lib.pagesizes import letter\nfrom reportlab.lib.units import inch\n\nc = canvas.Canvas(\"output.pdf\", pagesize=letter)\nwidth, height = letter\n\n# Draw text\nc.setFont(\"Helvetica-Bold\", 24)\nc.drawString(inch, height - inch, \"Hello ReportLab!\")\n\n# Draw a rectangle\nc.setFillColorRGB(0.2, 0.4, 0.8)\nc.rect(inch, 5*inch, 4*inch, 2*inch, fill=1)\n\n# Save\nc.showPage()\nc.save()\n```\n\n### Simple Platypus Document\n\n```python\nfrom reportlab.lib.pagesizes import letter\nfrom reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer\nfrom reportlab.lib.styles import getSampleStyleSheet\nfrom reportlab.lib.units import inch\n\ndoc = SimpleDocTemplate(\"output.pdf\", pagesize=letter)\nstory = []\nstyles = getSampleStyleSheet()\n\n# Add content\nstory.append(Paragraph(\"Document Title\", styles['Title']))\nstory.append(Spacer(1, 0.2*inch))\nstory.append(Paragraph(\"This is body text with <b>bold</b> and <i>italic</i>.\", styles['BodyText']))\n\n# Build PDF\ndoc.build(story)\n```\n\n## Common Tasks\n\n### Creating Tables\n\nTables work with both Canvas (via Drawing) and Platypus (as Flowables):\n\n```python\nfrom reportlab.platypus import Table, TableStyle\nfrom reportlab.lib import colors\nfrom reportlab.lib.units import inch\n\n# Define data\ndata = [\n    ['Product', 'Q1', 'Q2', 'Q3', 'Q4'],\n    ['Widget A', '100', '150', '130', '180'],\n    ['Widget B', '80', '120', '110', '160'],\n]\n\n# Create table\ntable = Table(data, colWidths=[2*inch, 1*inch, 1*inch, 1*inch, 1*inch])\n\n# Apply styling\nstyle = TableStyle([\n    # Header row\n    ('BACKGROUND', (0, 0), (-1, 0), colors.darkblue),\n    ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),\n    ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n    ('ALIGN', (0, 0), (-1, -1), 'CENTER'),\n\n    # Data rows\n    ('ROWBACKGROUNDS', (0, 1), (-1, -1), [colors.white, colors.lightgrey]),\n    ('GRID', (0, 0), (-1, -1), 1, colors.black),\n])\n\ntable.setStyle(style)\n\n# Add to Platypus story\nstory.append(table)\n\n# Or draw on Canvas\ntable.wrapOn(c, width, height)\ntable.drawOn(c, x, y)\n```\n\n**Detailed table reference:** See `references/tables_reference.md` for cell spanning, borders, alignment, and advanced styling.\n\n### Creating Charts\n\nCharts use the graphics framework and can be added to both Canvas and Platypus:\n\n```python\nfrom reportlab.graphics.shapes import Drawing\nfrom reportlab.graphics.charts.barcharts import VerticalBarChart\nfrom reportlab.lib import colors\n\n# Create drawing\ndrawing = Drawing(400, 200)\n\n# Create chart\nchart = VerticalBarChart()\nchart.x = 50\nchart.y = 50\nchart.width = 300\nchart.height = 125\n\n# Set data\nchart.data = [[100, 150, 130, 180, 140]]\nchart.categoryAxis.categoryNames = ['Q1', 'Q2', 'Q3', 'Q4', 'Q5']\n\n# Style\nchart.bars[0].fillColor = colors.blue\nchart.valueAxis.valueMin = 0\nchart.valueAxis.valueMax = 200\n\n# Add to drawing\ndrawing.add(chart)\n\n# Use in Platypus\nstory.append(drawing)\n\n# Or render directly to PDF\nfrom reportlab.graphics import renderPDF\nrenderPDF.drawToFile(drawing, 'chart.pdf', 'Chart Title')\n```\n\n**Available chart types:** Bar (vertical/horizontal), Line, Pie, Area, Scatter\n**Detailed charts reference:** See `references/charts_reference.md` for all chart types, styling, legends, and customization.\n\n### Adding Barcodes and QR Codes\n\n```python\nfrom reportlab.graphics.barcode import code128\nfrom reportlab.graphics.barcode.qr import QrCodeWidget\nfrom reportlab.graphics.shapes import Drawing\nfrom reportlab.graphics import renderPDF\n\n# Code128 barcode (general purpose)\nbarcode = code128.Code128(\"ABC123456789\", barHeight=0.5*inch)\n\n# On Canvas\nbarcode.drawOn(c, x, y)\n\n# QR Code\nqr = QrCodeWidget(\"https://example.com\")\nqr.barWidth = 2*inch\nqr.barHeight = 2*inch\n\n# Wrap in Drawing for Platypus\nd = Drawing()\nd.add(qr)\nstory.append(d)\n```\n\n**Supported formats:** Code128, Code39, EAN-13, EAN-8, UPC-A, ISBN, QR, Data Matrix, and 20+ more\n**Detailed barcode reference:** See `references/barcodes_reference.md` for all formats and usage examples.\n\n### Working with Text and Fonts\n\n```python\nfrom reportlab.platypus import Paragraph\nfrom reportlab.lib.styles import ParagraphStyle\nfrom reportlab.lib.enums import TA_JUSTIFY\n\n# Create custom style\ncustom_style = ParagraphStyle(\n    'CustomStyle',\n    fontSize=12,\n    leading=14,           # Line spacing\n    alignment=TA_JUSTIFY,\n    spaceAfter=10,\n    textColor=colors.black,\n)\n\n# Paragraph with inline formatting\ntext = \"\"\"\nThis paragraph has <b>bold</b>, <i>italic</i>, and <u>underlined</u> text.\nYou can also use <font color=\"blue\">colors</font> and <font size=\"14\">different sizes</font>.\nChemical formula: H<sub>2</sub>O, Einstein: E=mc<sup>2</sup>\n\"\"\"\n\npara = Paragraph(text, custom_style)\nstory.append(para)\n```\n\n**Using custom fonts:**\n\n```python\nfrom reportlab.pdfbase import pdfmetrics\nfrom reportlab.pdfbase.ttfonts import TTFont\n\n# Register TrueType font\npdfmetrics.registerFont(TTFont('CustomFont', 'CustomFont.ttf'))\n\n# Use in Canvas\nc.setFont('CustomFont', 12)\n\n# Use in Paragraph style\nstyle = ParagraphStyle('Custom', fontName='CustomFont', fontSize=12)\n```\n\n**Detailed text reference:** See `references/text_and_fonts.md` for paragraph styles, font families, Asian languages, Greek letters, and formatting.\n\n### Adding Images\n\n```python\nfrom reportlab.platypus import Image\nfrom reportlab.lib.units import inch\n\n# In Platypus\nimg = Image('photo.jpg', width=4*inch, height=3*inch)\nstory.append(img)\n\n# Maintain aspect ratio\nimg = Image('photo.jpg', width=4*inch, height=3*inch, kind='proportional')\n\n# In Canvas\nc.drawImage('photo.jpg', x, y, width=4*inch, height=3*inch)\n\n# With transparency (mask white background)\nc.drawImage('logo.png', x, y, mask=[255,255,255,255,255,255])\n```\n\n### Creating Forms\n\n```python\nfrom reportlab.pdfgen import canvas\nfrom reportlab.lib.colors import black, white, lightgrey\n\nc = canvas.Canvas(\"form.pdf\")\n\n# Text field\nc.acroForm.textfield(\n    name=\"name\",\n    tooltip=\"Enter your name\",\n    x=100, y=700,\n    width=200, height=20,\n    borderColor=black,\n    fillColor=lightgrey,\n    forceBorder=True\n)\n\n# Checkbox\nc.acroForm.checkbox(\n    name=\"agree\",\n    x=100, y=650,\n    size=20,\n    buttonStyle='check',\n    checked=False\n)\n\n# Dropdown\nc.acroForm.choice(\n    name=\"country\",\n    x=100, y=600,\n    width=150, height=20,\n    options=[(\"United States\", \"US\"), (\"Canada\", \"CA\")],\n    forceBorder=True\n)\n\nc.save()\n```\n\n**Detailed PDF features reference:** See `references/pdf_features.md` for forms, links, bookmarks, encryption, and metadata.\n\n### Headers and Footers\n\nFor Platypus documents, use page callbacks:\n\n```python\nfrom reportlab.platypus import BaseDocTemplate, PageTemplate, Frame\n\ndef add_header_footer(canvas, doc):\n    \"\"\"Called on each page\"\"\"\n    canvas.saveState()\n\n    # Header\n    canvas.setFont('Helvetica', 9)\n    canvas.drawString(inch, height - 0.5*inch, \"Document Title\")\n\n    # Footer\n    canvas.drawRightString(width - inch, 0.5*inch, f\"Page {doc.page}\")\n\n    canvas.restoreState()\n\n# Set up document\ndoc = BaseDocTemplate(\"output.pdf\")\nframe = Frame(doc.leftMargin, doc.bottomMargin, doc.width, doc.height, id='normal')\ntemplate = PageTemplate(id='normal', frames=[frame], onPage=add_header_footer)\ndoc.addPageTemplates([template])\n\n# Build with story\ndoc.build(story)\n```\n\n## Helper Scripts\n\nThis skill includes helper scripts for common tasks:\n\n### Quick Document Generator\n\nUse `scripts/quick_document.py` for rapid document creation:\n\n```python\nfrom scripts.quick_document import create_simple_document, create_styled_table\n\n# Simple document from content blocks\ncontent = [\n    {'type': 'heading', 'content': 'Introduction'},\n    {'type': 'paragraph', 'content': 'Your text here...'},\n    {'type': 'bullet', 'content': 'Bullet point'},\n]\n\ncreate_simple_document(\"output.pdf\", \"My Document\", content_blocks=content)\n\n# Styled tables with presets\ndata = [['Header1', 'Header2'], ['Data1', 'Data2']]\ntable = create_styled_table(data, style_name='striped')  # 'default', 'striped', 'minimal', 'report'\n```\n\n## Template Examples\n\nComplete working examples in `assets/`:\n\n### Invoice Template\n\n`assets/invoice_template.py` - Professional invoice with:\n- Company and client information\n- Itemized table with calculations\n- Tax and totals\n- Terms and notes\n- Logo placement\n\n```python\nfrom assets.invoice_template import create_invoice\n\ncreate_invoice(\n    filename=\"invoice.pdf\",\n    invoice_number=\"INV-2024-001\",\n    invoice_date=\"January 15, 2024\",\n    due_date=\"February 15, 2024\",\n    company_info={'name': 'Acme Corp', 'address': '...', 'phone': '...', 'email': '...'},\n    client_info={'name': 'Client Name', ...},\n    items=[\n        {'description': 'Service', 'quantity': 1, 'unit_price': 500.00},\n        ...\n    ],\n    tax_rate=0.08,\n    notes=\"Thank you for your business!\",\n)\n```\n\n### Report Template\n\n`assets/report_template.py` - Multi-page business report with:\n- Cover page\n- Table of contents\n- Multiple sections with subsections\n- Charts and tables\n- Headers and footers\n\n```python\nfrom assets.report_template import create_report\n\nreport_data = {\n    'title': 'Quarterly Report',\n    'subtitle': 'Q4 2023',\n    'author': 'Analytics Team',\n    'sections': [\n        {\n            'title': 'Executive Summary',\n            'content': 'Report content...',\n            'table_data': {...},\n            'chart_data': {...}\n        },\n        ...\n    ]\n}\n\ncreate_report(\"report.pdf\", report_data)\n```\n\n## Reference Documentation\n\nComprehensive API references organized by feature:\n\n- **`references/canvas_api.md`** - Low-level Canvas: drawing primitives, coordinates, transformations, state management, images, paths\n- **`references/platypus_guide.md`** - High-level Platypus: document templates, frames, flowables, page layouts, TOC\n- **`references/text_and_fonts.md`** - Text formatting: paragraph styles, inline markup, custom fonts, Asian languages, bullets, sequences\n- **`references/tables_reference.md`** - Tables: creation, styling, cell spanning, borders, alignment, colors, gradients\n- **`references/charts_reference.md`** - Charts: all chart types, data handling, axes, legends, colors, rendering\n- **`references/barcodes_reference.md`** - Barcodes: Code128, QR codes, EAN, UPC, postal codes, and 20+ formats\n- **`references/pdf_features.md`** - PDF features: links, bookmarks, forms, encryption, metadata, page transitions\n\n## Best Practices\n\n### Coordinate System (Canvas)\n- Origin (0, 0) is **lower-left corner** (not top-left)\n- Y-axis points **upward**\n- Units are in **points** (72 points = 1 inch)\n- Always specify page size explicitly\n\n```python\nfrom reportlab.lib.pagesizes import letter\nfrom reportlab.lib.units import inch\n\nwidth, height = letter\nmargin = inch\n\n# Top of page\ny_top = height - margin\n\n# Bottom of page\ny_bottom = margin\n```\n\n### Choosing Page Size\n\n```python\nfrom reportlab.lib.pagesizes import letter, A4, landscape\n\n# US Letter (8.5\" x 11\")\npagesize=letter\n\n# ISO A4 (210mm x 297mm)\npagesize=A4\n\n# Landscape\npagesize=landscape(letter)\n\n# Custom\npagesize=(6*inch, 9*inch)\n```\n\n### Performance Tips\n\n1. **Use `drawImage()` over `drawInlineImage()`** - caches images for reuse\n2. **Enable compression for large files:** `canvas.Canvas(\"file.pdf\", pageCompression=1)`\n3. **Reuse styles** - create once, use throughout document\n4. **Use Forms/XObjects** for repeated graphics\n\n### Common Patterns\n\n**Centering text on Canvas:**\n```python\ntext = \"Centered Text\"\ntext_width = c.stringWidth(text, \"Helvetica\", 12)\nx = (width - text_width) / 2\nc.drawString(x, y, text)\n\n# Or use built-in\nc.drawCentredString(width/2, y, text)\n```\n\n**Page breaks in Platypus:**\n```python\nfrom reportlab.platypus import PageBreak\n\nstory.append(PageBreak())\n```\n\n**Keep content together (no split):**\n```python\nfrom reportlab.platypus import KeepTogether\n\nstory.append(KeepTogether([\n    heading,\n    paragraph1,\n    paragraph2,\n]))\n```\n\n**Alternate row colors:**\n```python\nstyle = TableStyle([\n    ('ROWBACKGROUNDS', (0, 1), (-1, -1), [colors.white, colors.lightgrey]),\n])\n```\n\n## Troubleshooting\n\n**Text overlaps or disappears:**\n- Check Y-coordinates - remember origin is bottom-left\n- Ensure text fits within page bounds\n- Verify `leading` (line spacing) is greater than `fontSize`\n\n**Table doesn't fit on page:**\n- Reduce column widths\n- Decrease font size\n- Use landscape orientation\n- Enable table splitting with `repeatRows`\n\n**Barcode not scanning:**\n- Increase `barHeight` (try 0.5 inch minimum)\n- Set `quiet=1` for quiet zones\n- Test print quality (300+ DPI recommended)\n- Validate data format for barcode type\n\n**Font not found:**\n- Register TrueType fonts with `pdfmetrics.registerFont()`\n- Use font family name exactly as registered\n- Check font file path is correct\n\n**Images have white background:**\n- Use `mask` parameter to make white transparent\n- Provide RGB range to mask: `mask=[255,255,255,255,255,255]`\n- Or use PNG with alpha channel\n\n## Example Workflows\n\n### Creating an Invoice\n\n1. Start with invoice template from `assets/invoice_template.py`\n2. Customize company info, logo path\n3. Add items with descriptions, quantities, prices\n4. Set tax rate if applicable\n5. Add notes and payment terms\n6. Generate PDF\n\n### Creating a Report\n\n1. Start with report template from `assets/report_template.py`\n2. Define sections with titles and content\n3. Add tables for data using `create_styled_table()`\n4. Add charts using graphics framework\n5. Build with `doc.multiBuild(story)` for TOC\n\n### Creating a Certificate\n\n1. Use Canvas API for precise positioning\n2. Load custom fonts for elegant typography\n3. Add border graphics or image background\n4. Position text elements (name, date, achievement)\n5. Optional: Add QR code for verification\n\n### Creating Labels with Barcodes\n\n1. Use Canvas with custom page size (label dimensions)\n2. Calculate grid positions for multiple labels per page\n3. Draw label content (text, images)\n4. Add barcode at specific position\n5. Use `showPage()` between labels or grids\n\n## Installation\n\n```bash\nuv pip install reportlab\n\n# For image support\nuv pip install pillow\n\n# For charts\nuv pip install reportlab[renderPM]\n\n# For barcode support (included in reportlab)\n# QR codes require: uv pip install qrcode\n```\n\n## When to Use This Skill\n\nThis skill should be used when:\n- Generating PDF documents programmatically\n- Creating invoices, receipts, or billing documents\n- Building reports with tables and charts\n- Generating certificates, badges, or credentials\n- Creating shipping labels or product labels with barcodes\n- Designing forms or fillable PDFs\n- Producing multi-page documents with consistent formatting\n- Converting data to PDF format for archival or distribution\n- Creating custom layouts that require precise positioning\n\nThis skill provides comprehensive guidance for all ReportLab capabilities, from simple documents to complex multi-page reports with charts, tables, and interactive elements.\n",
        "data/k-dense-ai/scanpy/SKILL.md": "---\nname: scanpy\ndescription: \"Single-cell RNA-seq analysis. Load .h5ad/10X data, QC, normalization, PCA/UMAP/t-SNE, Leiden clustering, marker genes, cell type annotation, trajectory, for scRNA-seq analysis.\"\n---\n\n# Scanpy: Single-Cell Analysis\n\n## Overview\n\nScanpy is a scalable Python toolkit for analyzing single-cell RNA-seq data, built on AnnData. Apply this skill for complete single-cell workflows including quality control, normalization, dimensionality reduction, clustering, marker gene identification, visualization, and trajectory analysis.\n\n## When to Use This Skill\n\nThis skill should be used when:\n- Analyzing single-cell RNA-seq data (.h5ad, 10X, CSV formats)\n- Performing quality control on scRNA-seq datasets\n- Creating UMAP, t-SNE, or PCA visualizations\n- Identifying cell clusters and finding marker genes\n- Annotating cell types based on gene expression\n- Conducting trajectory inference or pseudotime analysis\n- Generating publication-quality single-cell plots\n\n## Quick Start\n\n### Basic Import and Setup\n\n```python\nimport scanpy as sc\nimport pandas as pd\nimport numpy as np\n\n# Configure settings\nsc.settings.verbosity = 3\nsc.settings.set_figure_params(dpi=80, facecolor='white')\nsc.settings.figdir = './figures/'\n```\n\n### Loading Data\n\n```python\n# From 10X Genomics\nadata = sc.read_10x_mtx('path/to/data/')\nadata = sc.read_10x_h5('path/to/data.h5')\n\n# From h5ad (AnnData format)\nadata = sc.read_h5ad('path/to/data.h5ad')\n\n# From CSV\nadata = sc.read_csv('path/to/data.csv')\n```\n\n### Understanding AnnData Structure\n\nThe AnnData object is the core data structure in scanpy:\n\n```python\nadata.X          # Expression matrix (cells  genes)\nadata.obs        # Cell metadata (DataFrame)\nadata.var        # Gene metadata (DataFrame)\nadata.uns        # Unstructured annotations (dict)\nadata.obsm       # Multi-dimensional cell data (PCA, UMAP)\nadata.raw        # Raw data backup\n\n# Access cell and gene names\nadata.obs_names  # Cell barcodes\nadata.var_names  # Gene names\n```\n\n## Standard Analysis Workflow\n\n### 1. Quality Control\n\nIdentify and filter low-quality cells and genes:\n\n```python\n# Identify mitochondrial genes\nadata.var['mt'] = adata.var_names.str.startswith('MT-')\n\n# Calculate QC metrics\nsc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], inplace=True)\n\n# Visualize QC metrics\nsc.pl.violin(adata, ['n_genes_by_counts', 'total_counts', 'pct_counts_mt'],\n             jitter=0.4, multi_panel=True)\n\n# Filter cells and genes\nsc.pp.filter_cells(adata, min_genes=200)\nsc.pp.filter_genes(adata, min_cells=3)\nadata = adata[adata.obs.pct_counts_mt < 5, :]  # Remove high MT% cells\n```\n\n**Use the QC script for automated analysis:**\n```bash\npython scripts/qc_analysis.py input_file.h5ad --output filtered.h5ad\n```\n\n### 2. Normalization and Preprocessing\n\n```python\n# Normalize to 10,000 counts per cell\nsc.pp.normalize_total(adata, target_sum=1e4)\n\n# Log-transform\nsc.pp.log1p(adata)\n\n# Save raw counts for later\nadata.raw = adata\n\n# Identify highly variable genes\nsc.pp.highly_variable_genes(adata, n_top_genes=2000)\nsc.pl.highly_variable_genes(adata)\n\n# Subset to highly variable genes\nadata = adata[:, adata.var.highly_variable]\n\n# Regress out unwanted variation\nsc.pp.regress_out(adata, ['total_counts', 'pct_counts_mt'])\n\n# Scale data\nsc.pp.scale(adata, max_value=10)\n```\n\n### 3. Dimensionality Reduction\n\n```python\n# PCA\nsc.tl.pca(adata, svd_solver='arpack')\nsc.pl.pca_variance_ratio(adata, log=True)  # Check elbow plot\n\n# Compute neighborhood graph\nsc.pp.neighbors(adata, n_neighbors=10, n_pcs=40)\n\n# UMAP for visualization\nsc.tl.umap(adata)\nsc.pl.umap(adata, color='leiden')\n\n# Alternative: t-SNE\nsc.tl.tsne(adata)\n```\n\n### 4. Clustering\n\n```python\n# Leiden clustering (recommended)\nsc.tl.leiden(adata, resolution=0.5)\nsc.pl.umap(adata, color='leiden', legend_loc='on data')\n\n# Try multiple resolutions to find optimal granularity\nfor res in [0.3, 0.5, 0.8, 1.0]:\n    sc.tl.leiden(adata, resolution=res, key_added=f'leiden_{res}')\n```\n\n### 5. Marker Gene Identification\n\n```python\n# Find marker genes for each cluster\nsc.tl.rank_genes_groups(adata, 'leiden', method='wilcoxon')\n\n# Visualize results\nsc.pl.rank_genes_groups(adata, n_genes=25, sharey=False)\nsc.pl.rank_genes_groups_heatmap(adata, n_genes=10)\nsc.pl.rank_genes_groups_dotplot(adata, n_genes=5)\n\n# Get results as DataFrame\nmarkers = sc.get.rank_genes_groups_df(adata, group='0')\n```\n\n### 6. Cell Type Annotation\n\n```python\n# Define marker genes for known cell types\nmarker_genes = ['CD3D', 'CD14', 'MS4A1', 'NKG7', 'FCGR3A']\n\n# Visualize markers\nsc.pl.umap(adata, color=marker_genes, use_raw=True)\nsc.pl.dotplot(adata, var_names=marker_genes, groupby='leiden')\n\n# Manual annotation\ncluster_to_celltype = {\n    '0': 'CD4 T cells',\n    '1': 'CD14+ Monocytes',\n    '2': 'B cells',\n    '3': 'CD8 T cells',\n}\nadata.obs['cell_type'] = adata.obs['leiden'].map(cluster_to_celltype)\n\n# Visualize annotated types\nsc.pl.umap(adata, color='cell_type', legend_loc='on data')\n```\n\n### 7. Save Results\n\n```python\n# Save processed data\nadata.write('results/processed_data.h5ad')\n\n# Export metadata\nadata.obs.to_csv('results/cell_metadata.csv')\nadata.var.to_csv('results/gene_metadata.csv')\n```\n\n## Common Tasks\n\n### Creating Publication-Quality Plots\n\n```python\n# Set high-quality defaults\nsc.settings.set_figure_params(dpi=300, frameon=False, figsize=(5, 5))\nsc.settings.file_format_figs = 'pdf'\n\n# UMAP with custom styling\nsc.pl.umap(adata, color='cell_type',\n           palette='Set2',\n           legend_loc='on data',\n           legend_fontsize=12,\n           legend_fontoutline=2,\n           frameon=False,\n           save='_publication.pdf')\n\n# Heatmap of marker genes\nsc.pl.heatmap(adata, var_names=genes, groupby='cell_type',\n              swap_axes=True, show_gene_labels=True,\n              save='_markers.pdf')\n\n# Dot plot\nsc.pl.dotplot(adata, var_names=genes, groupby='cell_type',\n              save='_dotplot.pdf')\n```\n\nRefer to `references/plotting_guide.md` for comprehensive visualization examples.\n\n### Trajectory Inference\n\n```python\n# PAGA (Partition-based graph abstraction)\nsc.tl.paga(adata, groups='leiden')\nsc.pl.paga(adata, color='leiden')\n\n# Diffusion pseudotime\nadata.uns['iroot'] = np.flatnonzero(adata.obs['leiden'] == '0')[0]\nsc.tl.dpt(adata)\nsc.pl.umap(adata, color='dpt_pseudotime')\n```\n\n### Differential Expression Between Conditions\n\n```python\n# Compare treated vs control within cell types\nadata_subset = adata[adata.obs['cell_type'] == 'T cells']\nsc.tl.rank_genes_groups(adata_subset, groupby='condition',\n                         groups=['treated'], reference='control')\nsc.pl.rank_genes_groups(adata_subset, groups=['treated'])\n```\n\n### Gene Set Scoring\n\n```python\n# Score cells for gene set expression\ngene_set = ['CD3D', 'CD3E', 'CD3G']\nsc.tl.score_genes(adata, gene_set, score_name='T_cell_score')\nsc.pl.umap(adata, color='T_cell_score')\n```\n\n### Batch Correction\n\n```python\n# ComBat batch correction\nsc.pp.combat(adata, key='batch')\n\n# Alternative: use Harmony or scVI (separate packages)\n```\n\n## Key Parameters to Adjust\n\n### Quality Control\n- `min_genes`: Minimum genes per cell (typically 200-500)\n- `min_cells`: Minimum cells per gene (typically 3-10)\n- `pct_counts_mt`: Mitochondrial threshold (typically 5-20%)\n\n### Normalization\n- `target_sum`: Target counts per cell (default 1e4)\n\n### Feature Selection\n- `n_top_genes`: Number of HVGs (typically 2000-3000)\n- `min_mean`, `max_mean`, `min_disp`: HVG selection parameters\n\n### Dimensionality Reduction\n- `n_pcs`: Number of principal components (check variance ratio plot)\n- `n_neighbors`: Number of neighbors (typically 10-30)\n\n### Clustering\n- `resolution`: Clustering granularity (0.4-1.2, higher = more clusters)\n\n## Common Pitfalls and Best Practices\n\n1. **Always save raw counts**: `adata.raw = adata` before filtering genes\n2. **Check QC plots carefully**: Adjust thresholds based on dataset quality\n3. **Use Leiden over Louvain**: More efficient and better results\n4. **Try multiple clustering resolutions**: Find optimal granularity\n5. **Validate cell type annotations**: Use multiple marker genes\n6. **Use `use_raw=True` for gene expression plots**: Shows original counts\n7. **Check PCA variance ratio**: Determine optimal number of PCs\n8. **Save intermediate results**: Long workflows can fail partway through\n\n## Bundled Resources\n\n### scripts/qc_analysis.py\nAutomated quality control script that calculates metrics, generates plots, and filters data:\n\n```bash\npython scripts/qc_analysis.py input.h5ad --output filtered.h5ad \\\n    --mt-threshold 5 --min-genes 200 --min-cells 3\n```\n\n### references/standard_workflow.md\nComplete step-by-step workflow with detailed explanations and code examples for:\n- Data loading and setup\n- Quality control with visualization\n- Normalization and scaling\n- Feature selection\n- Dimensionality reduction (PCA, UMAP, t-SNE)\n- Clustering (Leiden, Louvain)\n- Marker gene identification\n- Cell type annotation\n- Trajectory inference\n- Differential expression\n\nRead this reference when performing a complete analysis from scratch.\n\n### references/api_reference.md\nQuick reference guide for scanpy functions organized by module:\n- Reading/writing data (`sc.read_*`, `adata.write_*`)\n- Preprocessing (`sc.pp.*`)\n- Tools (`sc.tl.*`)\n- Plotting (`sc.pl.*`)\n- AnnData structure and manipulation\n- Settings and utilities\n\nUse this for quick lookup of function signatures and common parameters.\n\n### references/plotting_guide.md\nComprehensive visualization guide including:\n- Quality control plots\n- Dimensionality reduction visualizations\n- Clustering visualizations\n- Marker gene plots (heatmaps, dot plots, violin plots)\n- Trajectory and pseudotime plots\n- Publication-quality customization\n- Multi-panel figures\n- Color palettes and styling\n\nConsult this when creating publication-ready figures.\n\n### assets/analysis_template.py\nComplete analysis template providing a full workflow from data loading through cell type annotation. Copy and customize this template for new analyses:\n\n```bash\ncp assets/analysis_template.py my_analysis.py\n# Edit parameters and run\npython my_analysis.py\n```\n\nThe template includes all standard steps with configurable parameters and helpful comments.\n\n## Additional Resources\n\n- **Official scanpy documentation**: https://scanpy.readthedocs.io/\n- **Scanpy tutorials**: https://scanpy-tutorials.readthedocs.io/\n- **scverse ecosystem**: https://scverse.org/ (related tools: squidpy, scvi-tools, cellrank)\n- **Best practices**: Luecken & Theis (2019) \"Current best practices in single-cell RNA-seq\"\n\n## Tips for Effective Analysis\n\n1. **Start with the template**: Use `assets/analysis_template.py` as a starting point\n2. **Run QC script first**: Use `scripts/qc_analysis.py` for initial filtering\n3. **Consult references as needed**: Load workflow and API references into context\n4. **Iterate on clustering**: Try multiple resolutions and visualization methods\n5. **Validate biologically**: Check marker genes match expected cell types\n6. **Document parameters**: Record QC thresholds and analysis settings\n7. **Save checkpoints**: Write intermediate results at key steps\n",
        "data/k-dense-ai/scholar-evaluation/SKILL.md": "---\nname: scholar-evaluation\ndescription: Systematic framework for evaluating scholarly and research work based on the ScholarEval methodology. This skill should be used when assessing research papers, evaluating literature reviews, scoring research methodologies, analyzing scientific writing quality, or applying structured evaluation criteria to academic work. Provides comprehensive assessment across multiple dimensions including problem formulation, literature review, methodology, data collection, analysis, results interpretation, and scholarly writing quality.\n---\n\n# Scholar Evaluation\n\n## Overview\n\nApply the ScholarEval framework to systematically evaluate scholarly and research work. This skill provides structured evaluation methodology based on peer-reviewed research assessment criteria, enabling comprehensive analysis of academic papers, research proposals, literature reviews, and scholarly writing across multiple quality dimensions.\n\n## When to Use This Skill\n\nUse this skill when:\n- Evaluating research papers for quality and rigor\n- Assessing literature review comprehensiveness and quality\n- Reviewing research methodology design\n- Scoring data analysis approaches\n- Evaluating scholarly writing and presentation\n- Providing structured feedback on academic work\n- Benchmarking research quality against established criteria\n\n## Evaluation Workflow\n\n### Step 1: Initial Assessment and Scope Definition\n\nBegin by identifying the type of scholarly work being evaluated and the evaluation scope:\n\n**Work Types:**\n- Full research paper (empirical, theoretical, or review)\n- Research proposal or protocol\n- Literature review (systematic, narrative, or scoping)\n- Thesis or dissertation chapter\n- Conference abstract or short paper\n\n**Evaluation Scope:**\n- Comprehensive (all dimensions)\n- Targeted (specific aspects like methodology or writing)\n- Comparative (benchmarking against other work)\n\nAsk the user to clarify if the scope is ambiguous.\n\n### Step 2: Dimension-Based Evaluation\n\nSystematically evaluate the work across the ScholarEval dimensions. For each applicable dimension, assess quality, identify strengths and weaknesses, and provide scores where appropriate.\n\nRefer to `references/evaluation_framework.md` for detailed criteria and rubrics for each dimension.\n\n**Core Evaluation Dimensions:**\n\n1. **Problem Formulation & Research Questions**\n   - Clarity and specificity of research questions\n   - Theoretical or practical significance\n   - Feasibility and scope appropriateness\n   - Novelty and contribution potential\n\n2. **Literature Review**\n   - Comprehensiveness of coverage\n   - Critical synthesis vs. mere summarization\n   - Identification of research gaps\n   - Currency and relevance of sources\n   - Proper contextualization\n\n3. **Methodology & Research Design**\n   - Appropriateness for research questions\n   - Rigor and validity\n   - Reproducibility and transparency\n   - Ethical considerations\n   - Limitations acknowledgment\n\n4. **Data Collection & Sources**\n   - Quality and appropriateness of data\n   - Sample size and representativeness\n   - Data collection procedures\n   - Source credibility and reliability\n\n5. **Analysis & Interpretation**\n   - Appropriateness of analytical methods\n   - Rigor of analysis\n   - Logical coherence\n   - Alternative explanations considered\n   - Results-claims alignment\n\n6. **Results & Findings**\n   - Clarity of presentation\n   - Statistical or qualitative rigor\n   - Visualization quality\n   - Interpretation accuracy\n   - Implications discussion\n\n7. **Scholarly Writing & Presentation**\n   - Clarity and organization\n   - Academic tone and style\n   - Grammar and mechanics\n   - Logical flow\n   - Accessibility to target audience\n\n8. **Citations & References**\n   - Citation completeness\n   - Source quality and appropriateness\n   - Citation accuracy\n   - Balance of perspectives\n   - Adherence to citation standards\n\n### Step 3: Scoring and Rating\n\nFor each evaluated dimension, provide:\n\n**Qualitative Assessment:**\n- Key strengths (2-3 specific points)\n- Areas for improvement (2-3 specific points)\n- Critical issues (if any)\n\n**Quantitative Scoring (Optional):**\nUse a 5-point scale where applicable:\n- 5: Excellent - Exemplary quality, publishable in top venues\n- 4: Good - Strong quality with minor improvements needed\n- 3: Adequate - Acceptable quality with notable areas for improvement\n- 2: Needs Improvement - Significant revisions required\n- 1: Poor - Fundamental issues requiring major revision\n\nTo calculate aggregate scores programmatically, use `scripts/calculate_scores.py`.\n\n### Step 4: Synthesize Overall Assessment\n\nProvide an integrated evaluation summary:\n\n1. **Overall Quality Assessment** - Holistic judgment of the work's scholarly merit\n2. **Major Strengths** - 3-5 key strengths across dimensions\n3. **Critical Weaknesses** - 3-5 primary areas requiring attention\n4. **Priority Recommendations** - Ranked list of improvements by impact\n5. **Publication Readiness** (if applicable) - Assessment of suitability for target venues\n\n### Step 5: Provide Actionable Feedback\n\nTransform evaluation findings into constructive, actionable feedback:\n\n**Feedback Structure:**\n- **Specific** - Reference exact sections, paragraphs, or page numbers\n- **Actionable** - Provide concrete suggestions for improvement\n- **Prioritized** - Rank recommendations by importance and feasibility\n- **Balanced** - Acknowledge strengths while addressing weaknesses\n- **Evidence-based** - Ground feedback in evaluation criteria\n\n**Feedback Format Options:**\n- Structured report with dimension-by-dimension analysis\n- Annotated comments mapped to specific document sections\n- Executive summary with key findings and recommendations\n- Comparative analysis against benchmark standards\n\n### Step 6: Contextual Considerations\n\nAdjust evaluation approach based on:\n\n**Stage of Development:**\n- Early draft: Focus on conceptual and structural issues\n- Advanced draft: Focus on refinement and polish\n- Final submission: Comprehensive quality check\n\n**Purpose and Venue:**\n- Journal article: High standards for rigor and contribution\n- Conference paper: Balance novelty with presentation clarity\n- Student work: Educational feedback with developmental focus\n- Grant proposal: Emphasis on feasibility and impact\n\n**Discipline-Specific Norms:**\n- STEM fields: Emphasis on reproducibility and statistical rigor\n- Social sciences: Balance quantitative and qualitative standards\n- Humanities: Focus on argumentation and scholarly interpretation\n\n## Resources\n\n### references/evaluation_framework.md\n\nDetailed evaluation criteria, rubrics, and quality indicators for each ScholarEval dimension. Load this reference when conducting evaluations to access specific assessment guidelines and scoring rubrics.\n\nSearch patterns for quick access:\n- \"Problem Formulation criteria\"\n- \"Literature Review rubric\"\n- \"Methodology assessment\"\n- \"Data quality indicators\"\n- \"Analysis rigor standards\"\n- \"Writing quality checklist\"\n\n### scripts/calculate_scores.py\n\nPython script for calculating aggregate evaluation scores from dimension-level ratings. Supports weighted averaging, threshold analysis, and score visualization.\n\nUsage:\n```python\npython scripts/calculate_scores.py --scores <dimension_scores.json> --output <report.txt>\n```\n\n## Best Practices\n\n1. **Maintain Objectivity** - Base evaluations on established criteria, not personal preferences\n2. **Be Comprehensive** - Evaluate all applicable dimensions systematically\n3. **Provide Evidence** - Support assessments with specific examples from the work\n4. **Stay Constructive** - Frame weaknesses as opportunities for improvement\n5. **Consider Context** - Adjust expectations based on work stage and purpose\n6. **Document Rationale** - Explain the reasoning behind assessments and scores\n7. **Encourage Strengths** - Explicitly acknowledge what the work does well\n8. **Prioritize Feedback** - Focus on high-impact improvements first\n\n## Example Evaluation Workflow\n\n**User Request:** \"Evaluate this research paper on machine learning for drug discovery\"\n\n**Response Process:**\n1. Identify work type (empirical research paper) and scope (comprehensive evaluation)\n2. Load `references/evaluation_framework.md` for detailed criteria\n3. Systematically assess each dimension:\n   - Problem formulation: Clear research question about ML model performance\n   - Literature review: Comprehensive coverage of recent ML and drug discovery work\n   - Methodology: Appropriate deep learning architecture with validation procedures\n   - [Continue through all dimensions...]\n4. Calculate dimension scores and overall assessment\n5. Synthesize findings into structured report highlighting:\n   - Strong methodology and reproducible code\n   - Needs more diverse dataset evaluation\n   - Writing could improve clarity in results section\n6. Provide prioritized recommendations with specific suggestions\n\n## Notes\n\n- Evaluation rigor should match the work's purpose and stage\n- Some dimensions may not apply to all work types (e.g., data collection for purely theoretical papers)\n- Cultural and disciplinary differences in scholarly norms should be considered\n- This framework complements, not replaces, domain-specific expertise\n\n## Citation\n\nThis skill is based on the ScholarEval framework introduced in:\n\n**Moussa, H. N., Da Silva, P. Q., Adu-Ampratwum, D., East, A., Lu, Z., Puccetti, N., Xue, M., Sun, H., Majumder, B. P., & Kumar, S. (2025).** _ScholarEval: Research Idea Evaluation Grounded in Literature_. arXiv preprint arXiv:2510.16234. [https://arxiv.org/abs/2510.16234](https://arxiv.org/abs/2510.16234)\n\n**Abstract:** ScholarEval is a retrieval augmented evaluation framework that assesses research ideas based on two fundamental criteria: soundness (the empirical validity of proposed methods based on existing literature) and contribution (the degree of advancement made by the idea across different dimensions relative to prior research). The framework achieves significantly higher coverage of expert-annotated evaluation points and is consistently preferred over baseline systems in terms of evaluation actionability, depth, and evidence support.\n",
        "data/k-dense-ai/scientific-brainstorming/SKILL.md": "---\nname: scientific-brainstorming\ndescription: \"Research ideation partner. Generate hypotheses, explore interdisciplinary connections, challenge assumptions, develop methodologies, identify research gaps, for creative scientific problem-solving.\"\n---\n\n# Scientific Brainstorming\n\n## Overview\n\nScientific brainstorming is a conversational process for generating novel research ideas. Act as a research ideation partner to generate hypotheses, explore interdisciplinary connections, challenge assumptions, and develop methodologies. Apply this skill for creative scientific problem-solving.\n\n## When to Use This Skill\n\nThis skill should be used when:\n- Generating novel research ideas or directions\n- Exploring interdisciplinary connections and analogies\n- Challenging assumptions in existing research frameworks\n- Developing new methodological approaches\n- Identifying research gaps or opportunities\n- Overcoming creative blocks in problem-solving\n- Brainstorming experimental designs or study plans\n\n## Core Principles\n\nWhen engaging in scientific brainstorming:\n\n1. **Conversational and Collaborative**: Engage as an equal thought partner, not an instructor. Ask questions, build on ideas together, and maintain a natural dialogue.\n\n2. **Intellectually Curious**: Show genuine interest in the scientist's work. Ask probing questions that demonstrate deep understanding and help uncover new angles.\n\n3. **Creatively Challenging**: Push beyond obvious ideas. Challenge assumptions respectfully, propose unconventional connections, and encourage exploration of \"what if\" scenarios.\n\n4. **Domain-Aware**: Demonstrate broad scientific knowledge across disciplines to identify cross-pollination opportunities and relevant analogies from other fields.\n\n5. **Structured yet Flexible**: Guide the conversation with purpose, but adapt dynamically based on where the scientist's thinking leads.\n\n## Brainstorming Workflow\n\n### Phase 1: Understanding the Context\n\nBegin by deeply understanding what the scientist is working on. This phase establishes the foundation for productive ideation.\n\n**Approach:**\n- Ask open-ended questions about their current research, interests, or challenge\n- Understand their field, methodology, and constraints\n- Identify what they're trying to achieve and what obstacles they face\n- Listen for implicit assumptions or unexplored angles\n\n**Example questions:**\n- \"What aspect of your research are you most excited about right now?\"\n- \"What problem keeps you up at night?\"\n- \"What assumptions are you making that might be worth questioning?\"\n- \"Are there any unexpected findings that don't fit your current model?\"\n\n**Transition:** Once the context is clear, acknowledge understanding and suggest moving into active ideation.\n\n### Phase 2: Divergent Exploration\n\nHelp the scientist generate a wide range of ideas without judgment. The goal is quantity and diversity, not immediate feasibility.\n\n**Techniques to employ:**\n\n1. **Cross-Domain Analogies**\n   - Draw parallels from other scientific fields\n   - \"How might concepts from [field X] apply to your problem?\"\n   - Connect biological systems to social networks, physics to economics, etc.\n\n2. **Assumption Reversal**\n   - Identify core assumptions and flip them\n   - \"What if the opposite were true?\"\n   - \"What if you had unlimited resources/time/data?\"\n\n3. **Scale Shifting**\n   - Explore the problem at different scales (molecular, cellular, organismal, population, ecosystem)\n   - Consider temporal scales (milliseconds to millennia)\n\n4. **Constraint Removal/Addition**\n   - Remove apparent constraints: \"What if you could measure anything?\"\n   - Add new constraints: \"What if you had to solve this with 1800s technology?\"\n\n5. **Interdisciplinary Fusion**\n   - Suggest combining methodologies from different fields\n   - Propose collaborations that bridge disciplines\n\n6. **Technology Speculation**\n   - Imagine emerging technologies applied to the problem\n   - \"What becomes possible with CRISPR/AI/quantum computing/etc.?\"\n\n**Interaction style:**\n- Rapid-fire idea generation with the scientist\n- Build on their suggestions with \"Yes, and...\"\n- Encourage wild ideas explicitly: \"What's the most radical approach imaginable?\"\n- Consult references/brainstorming_methods.md for additional structured techniques\n\n### Phase 3: Connection Making\n\nHelp identify patterns, themes, and unexpected connections among the generated ideas.\n\n**Approach:**\n- Look for common threads across different ideas\n- Identify which ideas complement or enhance each other\n- Find surprising connections between seemingly unrelated concepts\n- Map relationships between ideas visually (if helpful)\n\n**Prompts:**\n- \"I notice several ideas involve [theme]what if we combined them?\"\n- \"These three approaches share [commonality]is there something deeper there?\"\n- \"What's the most unexpected connection you're seeing?\"\n\n### Phase 4: Critical Evaluation\n\nShift to constructively evaluating the most promising ideas while maintaining creative momentum.\n\n**Balance:**\n- Be critical but not dismissive\n- Identify both strengths and challenges\n- Consider feasibility while preserving innovative elements\n- Suggest modifications to make wild ideas more tractable\n\n**Questions to explore:**\n- \"What would it take to actually test this?\"\n- \"What's the first small experiment to run?\"\n- \"What existing data or tools could be leveraged?\"\n- \"Who else would need to be involved?\"\n- \"What's the biggest obstacle, and how might it be overcome?\"\n\n### Phase 5: Synthesis and Next Steps\n\nHelp crystallize insights and create concrete paths forward.\n\n**Deliverables:**\n- Summarize the most promising directions identified\n- Highlight novel connections or perspectives discovered\n- Suggest immediate next steps (literature search, pilot experiments, collaborations)\n- Capture key questions that emerged for future exploration\n- Identify resources or expertise that would be valuable\n\n**Close with encouragement:**\n- Acknowledge the creative work done\n- Reinforce the value of the ideas generated\n- Offer to continue the brainstorming in future sessions\n\n## Adaptive Techniques\n\n### When the Scientist Is Stuck\n\n- Break the problem into smaller pieces\n- Change the framing entirely (\"Instead of asking X, what if we asked Y?\")\n- Tell a story or analogy that might spark new thinking\n- Suggest taking a \"vacation\" from the problem to explore tangential ideas\n\n### When Ideas Are Too Safe\n\n- Explicitly encourage risk-taking: \"What's an idea so bold it makes you nervous?\"\n- Play devil's advocate to the conservative approach\n- Ask about failed or abandoned approaches and why they might actually work\n- Propose intentionally provocative \"what ifs\"\n\n### When Energy Lags\n\n- Inject enthusiasm about interesting ideas\n- Share genuine curiosity about a particular direction\n- Ask about something that excites them personally\n- Take a brief tangent into a related but different topic\n\n## Resources\n\n### references/brainstorming_methods.md\n\nContains detailed descriptions of structured brainstorming methodologies that can be consulted when standard techniques need supplementation:\n- SCAMPER framework (Substitute, Combine, Adapt, Modify, Put to another use, Eliminate, Reverse)\n- Six Thinking Hats for multi-perspective analysis\n- Morphological analysis for systematic exploration\n- TRIZ principles for inventive problem-solving\n- Biomimicry approaches for nature-inspired solutions\n\nConsult this file when the scientist requests a specific methodology or when the brainstorming session would benefit from a more structured approach.\n\n## Notes\n\n- This is a **conversation**, not a lecture. The scientist should be doing at least 50% of the talking.\n- Avoid jargon from fields outside the scientist's expertise unless explaining it clearly.\n- Be comfortable with silencegive space for thinking.\n- Remember that the best brainstorming often feels playful and exploratory.\n- The goal is not to solve everything, but to open new possibilities.\n",
        "data/k-dense-ai/scientific-critical-thinking/SKILL.md": "---\nname: scientific-critical-thinking\ndescription: \"Evaluate research rigor. Assess methodology, experimental design, statistical validity, biases, confounding, evidence quality (GRADE, Cochrane ROB), for critical analysis of scientific claims.\"\n---\n\n# Scientific Critical Thinking\n\n## Overview\n\nCritical thinking is a systematic process for evaluating scientific rigor. Assess methodology, experimental design, statistical validity, biases, confounding, and evidence quality using GRADE and Cochrane ROB frameworks. Apply this skill for critical analysis of scientific claims.\n\n## When to Use This Skill\n\nThis skill should be used when:\n- Evaluating research methodology and experimental design\n- Assessing statistical validity and evidence quality\n- Identifying biases and confounding in studies\n- Reviewing scientific claims and conclusions\n- Conducting systematic reviews or meta-analyses\n- Applying GRADE or Cochrane risk of bias assessments\n- Providing critical analysis of research papers\n\n## Core Capabilities\n\n### 1. Methodology Critique\n\nEvaluate research methodology for rigor, validity, and potential flaws.\n\n**Apply when:**\n- Reviewing research papers\n- Assessing experimental designs\n- Evaluating study protocols\n- Planning new research\n\n**Evaluation framework:**\n\n1. **Study Design Assessment**\n   - Is the design appropriate for the research question?\n   - Can the design support causal claims being made?\n   - Are comparison groups appropriate and adequate?\n   - Consider whether experimental, quasi-experimental, or observational design is justified\n\n2. **Validity Analysis**\n   - **Internal validity:** Can we trust the causal inference?\n     - Check randomization quality\n     - Evaluate confounding control\n     - Assess selection bias\n     - Review attrition/dropout patterns\n   - **External validity:** Do results generalize?\n     - Evaluate sample representativeness\n     - Consider ecological validity of setting\n     - Assess whether conditions match target application\n   - **Construct validity:** Do measures capture intended constructs?\n     - Review measurement validation\n     - Check operational definitions\n     - Assess whether measures are direct or proxy\n   - **Statistical conclusion validity:** Are statistical inferences sound?\n     - Verify adequate power/sample size\n     - Check assumption compliance\n     - Evaluate test appropriateness\n\n3. **Control and Blinding**\n   - Was randomization properly implemented (sequence generation, allocation concealment)?\n   - Was blinding feasible and implemented (participants, providers, assessors)?\n   - Are control conditions appropriate (placebo, active control, no treatment)?\n   - Could performance or detection bias affect results?\n\n4. **Measurement Quality**\n   - Are instruments validated and reliable?\n   - Are measures objective when possible, or subjective with acknowledged limitations?\n   - Is outcome assessment standardized?\n   - Are multiple measures used to triangulate findings?\n\n**Reference:** See `references/scientific_method.md` for detailed principles and `references/experimental_design.md` for comprehensive design checklist.\n\n### 2. Bias Detection\n\nIdentify and evaluate potential sources of bias that could distort findings.\n\n**Apply when:**\n- Reviewing published research\n- Designing new studies\n- Interpreting conflicting evidence\n- Assessing research quality\n\n**Systematic bias review:**\n\n1. **Cognitive Biases (Researcher)**\n   - **Confirmation bias:** Are only supporting findings highlighted?\n   - **HARKing:** Were hypotheses stated a priori or formed after seeing results?\n   - **Publication bias:** Are negative results missing from literature?\n   - **Cherry-picking:** Is evidence selectively reported?\n   - Check for preregistration and analysis plan transparency\n\n2. **Selection Biases**\n   - **Sampling bias:** Is sample representative of target population?\n   - **Volunteer bias:** Do participants self-select in systematic ways?\n   - **Attrition bias:** Is dropout differential between groups?\n   - **Survivorship bias:** Are only \"survivors\" visible in sample?\n   - Examine participant flow diagrams and compare baseline characteristics\n\n3. **Measurement Biases**\n   - **Observer bias:** Could expectations influence observations?\n   - **Recall bias:** Are retrospective reports systematically inaccurate?\n   - **Social desirability:** Are responses biased toward acceptability?\n   - **Instrument bias:** Do measurement tools systematically err?\n   - Evaluate blinding, validation, and measurement objectivity\n\n4. **Analysis Biases**\n   - **P-hacking:** Were multiple analyses conducted until significance emerged?\n   - **Outcome switching:** Were non-significant outcomes replaced with significant ones?\n   - **Selective reporting:** Are all planned analyses reported?\n   - **Subgroup fishing:** Were subgroup analyses conducted without correction?\n   - Check for study registration and compare to published outcomes\n\n5. **Confounding**\n   - What variables could affect both exposure and outcome?\n   - Were confounders measured and controlled (statistically or by design)?\n   - Could unmeasured confounding explain findings?\n   - Are there plausible alternative explanations?\n\n**Reference:** See `references/common_biases.md` for comprehensive bias taxonomy with detection and mitigation strategies.\n\n### 3. Statistical Analysis Evaluation\n\nCritically assess statistical methods, interpretation, and reporting.\n\n**Apply when:**\n- Reviewing quantitative research\n- Evaluating data-driven claims\n- Assessing clinical trial results\n- Reviewing meta-analyses\n\n**Statistical review checklist:**\n\n1. **Sample Size and Power**\n   - Was a priori power analysis conducted?\n   - Is sample adequate for detecting meaningful effects?\n   - Is the study underpowered (common problem)?\n   - Do significant results from small samples raise flags for inflated effect sizes?\n\n2. **Statistical Tests**\n   - Are tests appropriate for data type and distribution?\n   - Were test assumptions checked and met?\n   - Are parametric tests justified, or should non-parametric alternatives be used?\n   - Is the analysis matched to study design (e.g., paired vs. independent)?\n\n3. **Multiple Comparisons**\n   - Were multiple hypotheses tested?\n   - Was correction applied (Bonferroni, FDR, other)?\n   - Are primary outcomes distinguished from secondary/exploratory?\n   - Could findings be false positives from multiple testing?\n\n4. **P-Value Interpretation**\n   - Are p-values interpreted correctly (probability of data if null is true)?\n   - Is non-significance incorrectly interpreted as \"no effect\"?\n   - Is statistical significance conflated with practical importance?\n   - Are exact p-values reported, or only \"p < .05\"?\n   - Is there suspicious clustering just below .05?\n\n5. **Effect Sizes and Confidence Intervals**\n   - Are effect sizes reported alongside significance?\n   - Are confidence intervals provided to show precision?\n   - Is the effect size meaningful in practical terms?\n   - Are standardized effect sizes interpreted with field-specific context?\n\n6. **Missing Data**\n   - How much data is missing?\n   - Is missing data mechanism considered (MCAR, MAR, MNAR)?\n   - How is missing data handled (deletion, imputation, maximum likelihood)?\n   - Could missing data bias results?\n\n7. **Regression and Modeling**\n   - Is the model overfitted (too many predictors, no cross-validation)?\n   - Are predictions made outside the data range (extrapolation)?\n   - Are multicollinearity issues addressed?\n   - Are model assumptions checked?\n\n8. **Common Pitfalls**\n   - Correlation treated as causation\n   - Ignoring regression to the mean\n   - Base rate neglect\n   - Texas sharpshooter fallacy (pattern finding in noise)\n   - Simpson's paradox (confounding by subgroups)\n\n**Reference:** See `references/statistical_pitfalls.md` for detailed pitfalls and correct practices.\n\n### 4. Evidence Quality Assessment\n\nEvaluate the strength and quality of evidence systematically.\n\n**Apply when:**\n- Weighing evidence for decisions\n- Conducting literature reviews\n- Comparing conflicting findings\n- Determining confidence in conclusions\n\n**Evidence evaluation framework:**\n\n1. **Study Design Hierarchy**\n   - Systematic reviews/meta-analyses (highest for intervention effects)\n   - Randomized controlled trials\n   - Cohort studies\n   - Case-control studies\n   - Cross-sectional studies\n   - Case series/reports\n   - Expert opinion (lowest)\n\n   **Important:** Higher-level designs aren't always better quality. A well-designed observational study can be stronger than a poorly-conducted RCT.\n\n2. **Quality Within Design Type**\n   - Risk of bias assessment (use appropriate tool: Cochrane ROB, Newcastle-Ottawa, etc.)\n   - Methodological rigor\n   - Transparency and reporting completeness\n   - Conflicts of interest\n\n3. **GRADE Considerations (if applicable)**\n   - Start with design type (RCT = high, observational = low)\n   - **Downgrade for:**\n     - Risk of bias\n     - Inconsistency across studies\n     - Indirectness (wrong population/intervention/outcome)\n     - Imprecision (wide confidence intervals, small samples)\n     - Publication bias\n   - **Upgrade for:**\n     - Large effect sizes\n     - Dose-response relationships\n     - Confounders would reduce (not increase) effect\n\n4. **Convergence of Evidence**\n   - **Stronger when:**\n     - Multiple independent replications\n     - Different research groups and settings\n     - Different methodologies converge on same conclusion\n     - Mechanistic and empirical evidence align\n   - **Weaker when:**\n     - Single study or research group\n     - Contradictory findings in literature\n     - Publication bias evident\n     - No replication attempts\n\n5. **Contextual Factors**\n   - Biological/theoretical plausibility\n   - Consistency with established knowledge\n   - Temporality (cause precedes effect)\n   - Specificity of relationship\n   - Strength of association\n\n**Reference:** See `references/evidence_hierarchy.md` for detailed hierarchy, GRADE system, and quality assessment tools.\n\n### 5. Logical Fallacy Identification\n\nDetect and name logical errors in scientific arguments and claims.\n\n**Apply when:**\n- Evaluating scientific claims\n- Reviewing discussion/conclusion sections\n- Assessing popular science communication\n- Identifying flawed reasoning\n\n**Common fallacies in science:**\n\n1. **Causation Fallacies**\n   - **Post hoc ergo propter hoc:** \"B followed A, so A caused B\"\n   - **Correlation = causation:** Confusing association with causality\n   - **Reverse causation:** Mistaking cause for effect\n   - **Single cause fallacy:** Attributing complex outcomes to one factor\n\n2. **Generalization Fallacies**\n   - **Hasty generalization:** Broad conclusions from small samples\n   - **Anecdotal fallacy:** Personal stories as proof\n   - **Cherry-picking:** Selecting only supporting evidence\n   - **Ecological fallacy:** Group patterns applied to individuals\n\n3. **Authority and Source Fallacies**\n   - **Appeal to authority:** \"Expert said it, so it's true\" (without evidence)\n   - **Ad hominem:** Attacking person, not argument\n   - **Genetic fallacy:** Judging by origin, not merits\n   - **Appeal to nature:** \"Natural = good/safe\"\n\n4. **Statistical Fallacies**\n   - **Base rate neglect:** Ignoring prior probability\n   - **Texas sharpshooter:** Finding patterns in random data\n   - **Multiple comparisons:** Not correcting for multiple tests\n   - **Prosecutor's fallacy:** Confusing P(E|H) with P(H|E)\n\n5. **Structural Fallacies**\n   - **False dichotomy:** \"Either A or B\" when more options exist\n   - **Moving goalposts:** Changing evidence standards after they're met\n   - **Begging the question:** Circular reasoning\n   - **Straw man:** Misrepresenting arguments to attack them\n\n6. **Science-Specific Fallacies**\n   - **Galileo gambit:** \"They laughed at Galileo, so my fringe idea is correct\"\n   - **Argument from ignorance:** \"Not proven false, so true\"\n   - **Nirvana fallacy:** Rejecting imperfect solutions\n   - **Unfalsifiability:** Making untestable claims\n\n**When identifying fallacies:**\n- Name the specific fallacy\n- Explain why the reasoning is flawed\n- Identify what evidence would be needed for valid inference\n- Note that fallacious reasoning doesn't prove the conclusion falsejust that this argument doesn't support it\n\n**Reference:** See `references/logical_fallacies.md` for comprehensive fallacy catalog with examples and detection strategies.\n\n### 6. Research Design Guidance\n\nProvide constructive guidance for planning rigorous studies.\n\n**Apply when:**\n- Helping design new experiments\n- Planning research projects\n- Reviewing research proposals\n- Improving study protocols\n\n**Design process:**\n\n1. **Research Question Refinement**\n   - Ensure question is specific, answerable, and falsifiable\n   - Verify it addresses a gap or contradiction in literature\n   - Confirm feasibility (resources, ethics, time)\n   - Define variables operationally\n\n2. **Design Selection**\n   - Match design to question (causal  experimental; associational  observational)\n   - Consider feasibility and ethical constraints\n   - Choose between-subjects, within-subjects, or mixed designs\n   - Plan factorial designs if testing multiple factors\n\n3. **Bias Minimization Strategy**\n   - Implement randomization when possible\n   - Plan blinding at all feasible levels (participants, providers, assessors)\n   - Identify and plan to control confounds (randomization, matching, stratification, statistical adjustment)\n   - Standardize all procedures\n   - Plan to minimize attrition\n\n4. **Sample Planning**\n   - Conduct a priori power analysis (specify expected effect, desired power, alpha)\n   - Account for attrition in sample size\n   - Define clear inclusion/exclusion criteria\n   - Consider recruitment strategy and feasibility\n   - Plan for sample representativeness\n\n5. **Measurement Strategy**\n   - Select validated, reliable instruments\n   - Use objective measures when possible\n   - Plan multiple measures of key constructs (triangulation)\n   - Ensure measures are sensitive to expected changes\n   - Establish inter-rater reliability procedures\n\n6. **Analysis Planning**\n   - Prespecify all hypotheses and analyses\n   - Designate primary outcome clearly\n   - Plan statistical tests with assumption checks\n   - Specify how missing data will be handled\n   - Plan to report effect sizes and confidence intervals\n   - Consider multiple comparison corrections\n\n7. **Transparency and Rigor**\n   - Preregister study and analysis plan\n   - Use reporting guidelines (CONSORT, STROBE, PRISMA)\n   - Plan to report all outcomes, not just significant ones\n   - Distinguish confirmatory from exploratory analyses\n   - Commit to data/code sharing\n\n**Reference:** See `references/experimental_design.md` for comprehensive design checklist covering all stages from question to dissemination.\n\n### 7. Claim Evaluation\n\nSystematically evaluate scientific claims for validity and support.\n\n**Apply when:**\n- Assessing conclusions in papers\n- Evaluating media reports of research\n- Reviewing abstract or introduction claims\n- Checking if data support conclusions\n\n**Claim evaluation process:**\n\n1. **Identify the Claim**\n   - What exactly is being claimed?\n   - Is it a causal claim, associational claim, or descriptive claim?\n   - How strong is the claim (proven, likely, suggested, possible)?\n\n2. **Assess the Evidence**\n   - What evidence is provided?\n   - Is evidence direct or indirect?\n   - Is evidence sufficient for the strength of claim?\n   - Are alternative explanations ruled out?\n\n3. **Check Logical Connection**\n   - Do conclusions follow from the data?\n   - Are there logical leaps?\n   - Is correlational data used to support causal claims?\n   - Are limitations acknowledged?\n\n4. **Evaluate Proportionality**\n   - Is confidence proportional to evidence strength?\n   - Are hedging words used appropriately?\n   - Are limitations downplayed?\n   - Is speculation clearly labeled?\n\n5. **Check for Overgeneralization**\n   - Do claims extend beyond the sample studied?\n   - Are population restrictions acknowledged?\n   - Is context-dependence recognized?\n   - Are caveats about generalization included?\n\n6. **Red Flags**\n   - Causal language from correlational studies\n   - \"Proves\" or absolute certainty\n   - Cherry-picked citations\n   - Ignoring contradictory evidence\n   - Dismissing limitations\n   - Extrapolation beyond data\n\n**Provide specific feedback:**\n- Quote the problematic claim\n- Explain what evidence would be needed to support it\n- Suggest appropriate hedging language if warranted\n- Distinguish between data (what was found) and interpretation (what it means)\n\n## Application Guidelines\n\n### General Approach\n\n1. **Be Constructive**\n   - Identify strengths as well as weaknesses\n   - Suggest improvements rather than just criticizing\n   - Distinguish between fatal flaws and minor limitations\n   - Recognize that all research has limitations\n\n2. **Be Specific**\n   - Point to specific instances (e.g., \"Table 2 shows...\" or \"In the Methods section...\")\n   - Quote problematic statements\n   - Provide concrete examples of issues\n   - Reference specific principles or standards violated\n\n3. **Be Proportionate**\n   - Match criticism severity to issue importance\n   - Distinguish between major threats to validity and minor concerns\n   - Consider whether issues affect primary conclusions\n   - Acknowledge uncertainty in your own assessments\n\n4. **Apply Consistent Standards**\n   - Use same criteria across all studies\n   - Don't apply stricter standards to findings you dislike\n   - Acknowledge your own potential biases\n   - Base judgments on methodology, not results\n\n5. **Consider Context**\n   - Acknowledge practical and ethical constraints\n   - Consider field-specific norms for effect sizes and methods\n   - Recognize exploratory vs. confirmatory contexts\n   - Account for resource limitations in evaluating studies\n\n### When Providing Critique\n\n**Structure feedback as:**\n\n1. **Summary:** Brief overview of what was evaluated\n2. **Strengths:** What was done well (important for credibility and learning)\n3. **Concerns:** Issues organized by severity\n   - Critical issues (threaten validity of main conclusions)\n   - Important issues (affect interpretation but not fatally)\n   - Minor issues (worth noting but don't change conclusions)\n4. **Specific Recommendations:** Actionable suggestions for improvement\n5. **Overall Assessment:** Balanced conclusion about evidence quality and what can be concluded\n\n**Use precise terminology:**\n- Name specific biases, fallacies, and methodological issues\n- Reference established standards and guidelines\n- Cite principles from scientific methodology\n- Use technical terms accurately\n\n### When Uncertain\n\n- **Acknowledge uncertainty:** \"This could be X or Y; additional information needed is Z\"\n- **Ask clarifying questions:** \"Was [methodological detail] done? This affects interpretation.\"\n- **Provide conditional assessments:** \"If X was done, then Y follows; if not, then Z is concern\"\n- **Note what additional information would resolve uncertainty**\n\n## Reference Materials\n\nThis skill includes comprehensive reference materials that provide detailed frameworks for critical evaluation:\n\n- **`references/scientific_method.md`** - Core principles of scientific methodology, the scientific process, critical evaluation criteria, red flags in scientific claims, causal inference standards, peer review, and open science principles\n\n- **`references/common_biases.md`** - Comprehensive taxonomy of cognitive, experimental, methodological, statistical, and analysis biases with detection and mitigation strategies\n\n- **`references/statistical_pitfalls.md`** - Common statistical errors and misinterpretations including p-value misunderstandings, multiple comparisons problems, sample size issues, effect size mistakes, correlation/causation confusion, regression pitfalls, and meta-analysis issues\n\n- **`references/evidence_hierarchy.md`** - Traditional evidence hierarchy, GRADE system, study quality assessment criteria, domain-specific considerations, evidence synthesis principles, and practical decision frameworks\n\n- **`references/logical_fallacies.md`** - Logical fallacies common in scientific discourse organized by type (causation, generalization, authority, relevance, structure, statistical) with examples and detection strategies\n\n- **`references/experimental_design.md`** - Comprehensive experimental design checklist covering research questions, hypotheses, study design selection, variables, sampling, blinding, randomization, control groups, procedures, measurement, bias minimization, data management, statistical planning, ethical considerations, validity threats, and reporting standards\n\n**When to consult references:**\n- Load references into context when detailed frameworks are needed\n- Use grep to search references for specific topics: `grep -r \"pattern\" references/`\n- References provide depth; SKILL.md provides procedural guidance\n- Consult references for comprehensive lists, detailed criteria, and specific examples\n\n## Remember\n\n**Scientific critical thinking is about:**\n- Systematic evaluation using established principles\n- Constructive critique that improves science\n- Proportional confidence to evidence strength\n- Transparency about uncertainty and limitations\n- Consistent application of standards\n- Recognition that all research has limitations\n- Balance between skepticism and openness to evidence\n\n**Always distinguish between:**\n- Data (what was observed) and interpretation (what it means)\n- Correlation and causation\n- Statistical significance and practical importance\n- Exploratory and confirmatory findings\n- What is known and what is uncertain\n- Evidence against a claim and evidence for the null\n\n**Goals of critical thinking:**\n1. Identify strengths and weaknesses accurately\n2. Determine what conclusions are supported\n3. Recognize limitations and uncertainties\n4. Suggest improvements for future work\n5. Advance scientific understanding\n",
        "data/k-dense-ai/scientific-visualization/SKILL.md": "---\nname: scientific-visualization\ndescription: \"Create publication figures with matplotlib/seaborn/plotly. Multi-panel layouts, error bars, significance markers, colorblind-safe, export PDF/EPS/TIFF, for journal-ready scientific plots.\"\n---\n\n# Scientific Visualization\n\n## Overview\n\nScientific visualization transforms data into clear, accurate figures for publication. Create journal-ready plots with multi-panel layouts, error bars, significance markers, and colorblind-safe palettes. Export as PDF/EPS/TIFF using matplotlib, seaborn, and plotly for manuscripts.\n\n## When to Use This Skill\n\nThis skill should be used when:\n- Creating plots or visualizations for scientific manuscripts\n- Preparing figures for journal submission (Nature, Science, Cell, PLOS, etc.)\n- Ensuring figures are colorblind-friendly and accessible\n- Making multi-panel figures with consistent styling\n- Exporting figures at correct resolution and format\n- Following specific publication guidelines\n- Improving existing figures to meet publication standards\n- Creating figures that need to work in both color and grayscale\n\n## Quick Start Guide\n\n### Basic Publication-Quality Figure\n\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Apply publication style (from scripts/style_presets.py)\nfrom style_presets import apply_publication_style\napply_publication_style('default')\n\n# Create figure with appropriate size (single column = 3.5 inches)\nfig, ax = plt.subplots(figsize=(3.5, 2.5))\n\n# Plot data\nx = np.linspace(0, 10, 100)\nax.plot(x, np.sin(x), label='sin(x)')\nax.plot(x, np.cos(x), label='cos(x)')\n\n# Proper labeling with units\nax.set_xlabel('Time (seconds)')\nax.set_ylabel('Amplitude (mV)')\nax.legend(frameon=False)\n\n# Remove unnecessary spines\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\n\n# Save in publication formats (from scripts/figure_export.py)\nfrom figure_export import save_publication_figure\nsave_publication_figure(fig, 'figure1', formats=['pdf', 'png'], dpi=300)\n```\n\n### Using Pre-configured Styles\n\nApply journal-specific styles using the matplotlib style files in `assets/`:\n\n```python\nimport matplotlib.pyplot as plt\n\n# Option 1: Use style file directly\nplt.style.use('assets/nature.mplstyle')\n\n# Option 2: Use style_presets.py helper\nfrom style_presets import configure_for_journal\nconfigure_for_journal('nature', figure_width='single')\n\n# Now create figures - they'll automatically match Nature specifications\nfig, ax = plt.subplots()\n# ... your plotting code ...\n```\n\n### Quick Start with Seaborn\n\nFor statistical plots, use seaborn with publication styling:\n\n```python\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom style_presets import apply_publication_style\n\n# Apply publication style\napply_publication_style('default')\nsns.set_theme(style='ticks', context='paper', font_scale=1.1)\nsns.set_palette('colorblind')\n\n# Create statistical comparison figure\nfig, ax = plt.subplots(figsize=(3.5, 3))\nsns.boxplot(data=df, x='treatment', y='response', \n            order=['Control', 'Low', 'High'], palette='Set2', ax=ax)\nsns.stripplot(data=df, x='treatment', y='response',\n              order=['Control', 'Low', 'High'], \n              color='black', alpha=0.3, size=3, ax=ax)\nax.set_ylabel('Response (M)')\nsns.despine()\n\n# Save figure\nfrom figure_export import save_publication_figure\nsave_publication_figure(fig, 'treatment_comparison', formats=['pdf', 'png'], dpi=300)\n```\n\n## Core Principles and Best Practices\n\n### 1. Resolution and File Format\n\n**Critical requirements** (detailed in `references/publication_guidelines.md`):\n- **Raster images** (photos, microscopy): 300-600 DPI\n- **Line art** (graphs, plots): 600-1200 DPI or vector format\n- **Vector formats** (preferred): PDF, EPS, SVG\n- **Raster formats**: TIFF, PNG (never JPEG for scientific data)\n\n**Implementation:**\n```python\n# Use the figure_export.py script for correct settings\nfrom figure_export import save_publication_figure\n\n# Saves in multiple formats with proper DPI\nsave_publication_figure(fig, 'myfigure', formats=['pdf', 'png'], dpi=300)\n\n# Or save for specific journal requirements\nfrom figure_export import save_for_journal\nsave_for_journal(fig, 'figure1', journal='nature', figure_type='combination')\n```\n\n### 2. Color Selection - Colorblind Accessibility\n\n**Always use colorblind-friendly palettes** (detailed in `references/color_palettes.md`):\n\n**Recommended: Okabe-Ito palette** (distinguishable by all types of color blindness):\n```python\n# Option 1: Use assets/color_palettes.py\nfrom color_palettes import OKABE_ITO_LIST, apply_palette\napply_palette('okabe_ito')\n\n# Option 2: Manual specification\nokabe_ito = ['#E69F00', '#56B4E9', '#009E73', '#F0E442',\n             '#0072B2', '#D55E00', '#CC79A7', '#000000']\nplt.rcParams['axes.prop_cycle'] = plt.cycler(color=okabe_ito)\n```\n\n**For heatmaps/continuous data:**\n- Use perceptually uniform colormaps: `viridis`, `plasma`, `cividis`\n- Avoid red-green diverging maps (use `PuOr`, `RdBu`, `BrBG` instead)\n- Never use `jet` or `rainbow` colormaps\n\n**Always test figures in grayscale** to ensure interpretability.\n\n### 3. Typography and Text\n\n**Font guidelines** (detailed in `references/publication_guidelines.md`):\n- Sans-serif fonts: Arial, Helvetica, Calibri\n- Minimum sizes at **final print size**:\n  - Axis labels: 7-9 pt\n  - Tick labels: 6-8 pt\n  - Panel labels: 8-12 pt (bold)\n- Sentence case for labels: \"Time (hours)\" not \"TIME (HOURS)\"\n- Always include units in parentheses\n\n**Implementation:**\n```python\n# Set fonts globally\nimport matplotlib as mpl\nmpl.rcParams['font.family'] = 'sans-serif'\nmpl.rcParams['font.sans-serif'] = ['Arial', 'Helvetica']\nmpl.rcParams['font.size'] = 8\nmpl.rcParams['axes.labelsize'] = 9\nmpl.rcParams['xtick.labelsize'] = 7\nmpl.rcParams['ytick.labelsize'] = 7\n```\n\n### 4. Figure Dimensions\n\n**Journal-specific widths** (detailed in `references/journal_requirements.md`):\n- **Nature**: Single 89 mm, Double 183 mm\n- **Science**: Single 55 mm, Double 175 mm\n- **Cell**: Single 85 mm, Double 178 mm\n\n**Check figure size compliance:**\n```python\nfrom figure_export import check_figure_size\n\nfig = plt.figure(figsize=(3.5, 3))  # 89 mm for Nature\ncheck_figure_size(fig, journal='nature')\n```\n\n### 5. Multi-Panel Figures\n\n**Best practices:**\n- Label panels with bold letters: **A**, **B**, **C** (uppercase for most journals, lowercase for Nature)\n- Maintain consistent styling across all panels\n- Align panels along edges where possible\n- Use adequate white space between panels\n\n**Example implementation** (see `references/matplotlib_examples.md` for complete code):\n```python\nfrom string import ascii_uppercase\n\nfig = plt.figure(figsize=(7, 4))\ngs = fig.add_gridspec(2, 2, hspace=0.4, wspace=0.4)\n\nax1 = fig.add_subplot(gs[0, 0])\nax2 = fig.add_subplot(gs[0, 1])\n# ... create other panels ...\n\n# Add panel labels\nfor i, ax in enumerate([ax1, ax2, ...]):\n    ax.text(-0.15, 1.05, ascii_uppercase[i], transform=ax.transAxes,\n            fontsize=10, fontweight='bold', va='top')\n```\n\n## Common Tasks\n\n### Task 1: Create a Publication-Ready Line Plot\n\nSee `references/matplotlib_examples.md` Example 1 for complete code.\n\n**Key steps:**\n1. Apply publication style\n2. Set appropriate figure size for target journal\n3. Use colorblind-friendly colors\n4. Add error bars with correct representation (SEM, SD, or CI)\n5. Label axes with units\n6. Remove unnecessary spines\n7. Save in vector format\n\n**Using seaborn for automatic confidence intervals:**\n```python\nimport seaborn as sns\nfig, ax = plt.subplots(figsize=(5, 3))\nsns.lineplot(data=timeseries, x='time', y='measurement',\n             hue='treatment', errorbar=('ci', 95), \n             markers=True, ax=ax)\nax.set_xlabel('Time (hours)')\nax.set_ylabel('Measurement (AU)')\nsns.despine()\n```\n\n### Task 2: Create a Multi-Panel Figure\n\nSee `references/matplotlib_examples.md` Example 2 for complete code.\n\n**Key steps:**\n1. Use `GridSpec` for flexible layout\n2. Ensure consistent styling across panels\n3. Add bold panel labels (A, B, C, etc.)\n4. Align related panels\n5. Verify all text is readable at final size\n\n### Task 3: Create a Heatmap with Proper Colormap\n\nSee `references/matplotlib_examples.md` Example 4 for complete code.\n\n**Key steps:**\n1. Use perceptually uniform colormap (`viridis`, `plasma`, `cividis`)\n2. Include labeled colorbar\n3. For diverging data, use colorblind-safe diverging map (`RdBu_r`, `PuOr`)\n4. Set appropriate center value for diverging maps\n5. Test appearance in grayscale\n\n**Using seaborn for correlation matrices:**\n```python\nimport seaborn as sns\nfig, ax = plt.subplots(figsize=(5, 4))\ncorr = df.corr()\nmask = np.triu(np.ones_like(corr, dtype=bool))\nsns.heatmap(corr, mask=mask, annot=True, fmt='.2f',\n            cmap='RdBu_r', center=0, square=True,\n            linewidths=1, cbar_kws={'shrink': 0.8}, ax=ax)\n```\n\n### Task 4: Prepare Figure for Specific Journal\n\n**Workflow:**\n1. Check journal requirements: `references/journal_requirements.md`\n2. Configure matplotlib for journal:\n   ```python\n   from style_presets import configure_for_journal\n   configure_for_journal('nature', figure_width='single')\n   ```\n3. Create figure (will auto-size correctly)\n4. Export with journal specifications:\n   ```python\n   from figure_export import save_for_journal\n   save_for_journal(fig, 'figure1', journal='nature', figure_type='line_art')\n   ```\n\n### Task 5: Fix an Existing Figure to Meet Publication Standards\n\n**Checklist approach** (full checklist in `references/publication_guidelines.md`):\n\n1. **Check resolution**: Verify DPI meets journal requirements\n2. **Check file format**: Use vector for plots, TIFF/PNG for images\n3. **Check colors**: Ensure colorblind-friendly\n4. **Check fonts**: Minimum 6-7 pt at final size, sans-serif\n5. **Check labels**: All axes labeled with units\n6. **Check size**: Matches journal column width\n7. **Test grayscale**: Figure interpretable without color\n8. **Remove chart junk**: No unnecessary grids, 3D effects, shadows\n\n### Task 6: Create Colorblind-Friendly Visualizations\n\n**Strategy:**\n1. Use approved palettes from `assets/color_palettes.py`\n2. Add redundant encoding (line styles, markers, patterns)\n3. Test with colorblind simulator\n4. Ensure grayscale compatibility\n\n**Example:**\n```python\nfrom color_palettes import apply_palette\nimport matplotlib.pyplot as plt\n\napply_palette('okabe_ito')\n\n# Add redundant encoding beyond color\nline_styles = ['-', '--', '-.', ':']\nmarkers = ['o', 's', '^', 'v']\n\nfor i, (data, label) in enumerate(datasets):\n    plt.plot(x, data, linestyle=line_styles[i % 4],\n             marker=markers[i % 4], label=label)\n```\n\n## Statistical Rigor\n\n**Always include:**\n- Error bars (SD, SEM, or CI - specify which in caption)\n- Sample size (n) in figure or caption\n- Statistical significance markers (*, **, ***)\n- Individual data points when possible (not just summary statistics)\n\n**Example with statistics:**\n```python\n# Show individual points with summary statistics\nax.scatter(x_jittered, individual_points, alpha=0.4, s=8)\nax.errorbar(x, means, yerr=sems, fmt='o', capsize=3)\n\n# Mark significance\nax.text(1.5, max_y * 1.1, '***', ha='center', fontsize=8)\n```\n\n## Working with Different Plotting Libraries\n\n### Matplotlib\n- Most control over publication details\n- Best for complex multi-panel figures\n- Use provided style files for consistent formatting\n- See `references/matplotlib_examples.md` for extensive examples\n\n### Seaborn\n\nSeaborn provides a high-level, dataset-oriented interface for statistical graphics, built on matplotlib. It excels at creating publication-quality statistical visualizations with minimal code while maintaining full compatibility with matplotlib customization.\n\n**Key advantages for scientific visualization:**\n- Automatic statistical estimation and confidence intervals\n- Built-in support for multi-panel figures (faceting)\n- Colorblind-friendly palettes by default\n- Dataset-oriented API using pandas DataFrames\n- Semantic mapping of variables to visual properties\n\n#### Quick Start with Publication Style\n\nAlways apply matplotlib publication styles first, then configure seaborn:\n\n```python\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom style_presets import apply_publication_style\n\n# Apply publication style\napply_publication_style('default')\n\n# Configure seaborn for publication\nsns.set_theme(style='ticks', context='paper', font_scale=1.1)\nsns.set_palette('colorblind')  # Use colorblind-safe palette\n\n# Create figure\nfig, ax = plt.subplots(figsize=(3.5, 2.5))\nsns.scatterplot(data=df, x='time', y='response', \n                hue='treatment', style='condition', ax=ax)\nsns.despine()  # Remove top and right spines\n```\n\n#### Common Plot Types for Publications\n\n**Statistical comparisons:**\n```python\n# Box plot with individual points for transparency\nfig, ax = plt.subplots(figsize=(3.5, 3))\nsns.boxplot(data=df, x='treatment', y='response', \n            order=['Control', 'Low', 'High'], palette='Set2', ax=ax)\nsns.stripplot(data=df, x='treatment', y='response',\n              order=['Control', 'Low', 'High'], \n              color='black', alpha=0.3, size=3, ax=ax)\nax.set_ylabel('Response (M)')\nsns.despine()\n```\n\n**Distribution analysis:**\n```python\n# Violin plot with split comparison\nfig, ax = plt.subplots(figsize=(4, 3))\nsns.violinplot(data=df, x='timepoint', y='expression',\n               hue='treatment', split=True, inner='quartile', ax=ax)\nax.set_ylabel('Gene Expression (AU)')\nsns.despine()\n```\n\n**Correlation matrices:**\n```python\n# Heatmap with proper colormap and annotations\nfig, ax = plt.subplots(figsize=(5, 4))\ncorr = df.corr()\nmask = np.triu(np.ones_like(corr, dtype=bool))  # Show only lower triangle\nsns.heatmap(corr, mask=mask, annot=True, fmt='.2f',\n            cmap='RdBu_r', center=0, square=True,\n            linewidths=1, cbar_kws={'shrink': 0.8}, ax=ax)\nplt.tight_layout()\n```\n\n**Time series with confidence bands:**\n```python\n# Line plot with automatic CI calculation\nfig, ax = plt.subplots(figsize=(5, 3))\nsns.lineplot(data=timeseries, x='time', y='measurement',\n             hue='treatment', style='replicate',\n             errorbar=('ci', 95), markers=True, dashes=False, ax=ax)\nax.set_xlabel('Time (hours)')\nax.set_ylabel('Measurement (AU)')\nsns.despine()\n```\n\n#### Multi-Panel Figures with Seaborn\n\n**Using FacetGrid for automatic faceting:**\n```python\n# Create faceted plot\ng = sns.relplot(data=df, x='dose', y='response',\n                hue='treatment', col='cell_line', row='timepoint',\n                kind='line', height=2.5, aspect=1.2,\n                errorbar=('ci', 95), markers=True)\ng.set_axis_labels('Dose (M)', 'Response (AU)')\ng.set_titles('{row_name} | {col_name}')\nsns.despine()\n\n# Save with correct DPI\nfrom figure_export import save_publication_figure\nsave_publication_figure(g.figure, 'figure_facets', \n                       formats=['pdf', 'png'], dpi=300)\n```\n\n**Combining seaborn with matplotlib subplots:**\n```python\n# Create custom multi-panel layout\nfig, axes = plt.subplots(2, 2, figsize=(7, 6))\n\n# Panel A: Scatter with regression\nsns.regplot(data=df, x='predictor', y='response', ax=axes[0, 0])\naxes[0, 0].text(-0.15, 1.05, 'A', transform=axes[0, 0].transAxes,\n                fontsize=10, fontweight='bold')\n\n# Panel B: Distribution comparison\nsns.violinplot(data=df, x='group', y='value', ax=axes[0, 1])\naxes[0, 1].text(-0.15, 1.05, 'B', transform=axes[0, 1].transAxes,\n                fontsize=10, fontweight='bold')\n\n# Panel C: Heatmap\nsns.heatmap(correlation_data, cmap='viridis', ax=axes[1, 0])\naxes[1, 0].text(-0.15, 1.05, 'C', transform=axes[1, 0].transAxes,\n                fontsize=10, fontweight='bold')\n\n# Panel D: Time series\nsns.lineplot(data=timeseries, x='time', y='signal', \n             hue='condition', ax=axes[1, 1])\naxes[1, 1].text(-0.15, 1.05, 'D', transform=axes[1, 1].transAxes,\n                fontsize=10, fontweight='bold')\n\nplt.tight_layout()\nsns.despine()\n```\n\n#### Color Palettes for Publications\n\nSeaborn includes several colorblind-safe palettes:\n\n```python\n# Use built-in colorblind palette (recommended)\nsns.set_palette('colorblind')\n\n# Or specify custom colorblind-safe colors (Okabe-Ito)\nokabe_ito = ['#E69F00', '#56B4E9', '#009E73', '#F0E442',\n             '#0072B2', '#D55E00', '#CC79A7', '#000000']\nsns.set_palette(okabe_ito)\n\n# For heatmaps and continuous data\nsns.heatmap(data, cmap='viridis')  # Perceptually uniform\nsns.heatmap(corr, cmap='RdBu_r', center=0)  # Diverging, centered\n```\n\n#### Choosing Between Axes-Level and Figure-Level Functions\n\n**Axes-level functions** (e.g., `scatterplot`, `boxplot`, `heatmap`):\n- Use when building custom multi-panel layouts\n- Accept `ax=` parameter for precise placement\n- Better integration with matplotlib subplots\n- More control over figure composition\n\n```python\nfig, ax = plt.subplots(figsize=(3.5, 2.5))\nsns.scatterplot(data=df, x='x', y='y', hue='group', ax=ax)\n```\n\n**Figure-level functions** (e.g., `relplot`, `catplot`, `displot`):\n- Use for automatic faceting by categorical variables\n- Create complete figures with consistent styling\n- Great for exploratory analysis\n- Use `height` and `aspect` for sizing\n\n```python\ng = sns.relplot(data=df, x='x', y='y', col='category', kind='scatter')\n```\n\n#### Statistical Rigor with Seaborn\n\nSeaborn automatically computes and displays uncertainty:\n\n```python\n# Line plot: shows mean  95% CI by default\nsns.lineplot(data=df, x='time', y='value', hue='treatment',\n             errorbar=('ci', 95))  # Can change to 'sd', 'se', etc.\n\n# Bar plot: shows mean with bootstrapped CI\nsns.barplot(data=df, x='treatment', y='response',\n            errorbar=('ci', 95), capsize=0.1)\n\n# Always specify error type in figure caption:\n# \"Error bars represent 95% confidence intervals\"\n```\n\n#### Best Practices for Publication-Ready Seaborn Figures\n\n1. **Always set publication theme first:**\n   ```python\n   sns.set_theme(style='ticks', context='paper', font_scale=1.1)\n   ```\n\n2. **Use colorblind-safe palettes:**\n   ```python\n   sns.set_palette('colorblind')\n   ```\n\n3. **Remove unnecessary elements:**\n   ```python\n   sns.despine()  # Remove top and right spines\n   ```\n\n4. **Control figure size appropriately:**\n   ```python\n   # Axes-level: use matplotlib figsize\n   fig, ax = plt.subplots(figsize=(3.5, 2.5))\n   \n   # Figure-level: use height and aspect\n   g = sns.relplot(..., height=3, aspect=1.2)\n   ```\n\n5. **Show individual data points when possible:**\n   ```python\n   sns.boxplot(...)  # Summary statistics\n   sns.stripplot(..., alpha=0.3)  # Individual points\n   ```\n\n6. **Include proper labels with units:**\n   ```python\n   ax.set_xlabel('Time (hours)')\n   ax.set_ylabel('Expression (AU)')\n   ```\n\n7. **Export at correct resolution:**\n   ```python\n   from figure_export import save_publication_figure\n   save_publication_figure(fig, 'figure_name', \n                          formats=['pdf', 'png'], dpi=300)\n   ```\n\n#### Advanced Seaborn Techniques\n\n**Pairwise relationships for exploratory analysis:**\n```python\n# Quick overview of all relationships\ng = sns.pairplot(data=df, hue='condition', \n                 vars=['gene1', 'gene2', 'gene3'],\n                 corner=True, diag_kind='kde', height=2)\n```\n\n**Hierarchical clustering heatmap:**\n```python\n# Cluster samples and features\ng = sns.clustermap(expression_data, method='ward', \n                   metric='euclidean', z_score=0,\n                   cmap='RdBu_r', center=0, \n                   figsize=(10, 8), \n                   row_colors=condition_colors,\n                   cbar_kws={'label': 'Z-score'})\n```\n\n**Joint distributions with marginals:**\n```python\n# Bivariate distribution with context\ng = sns.jointplot(data=df, x='gene1', y='gene2',\n                  hue='treatment', kind='scatter',\n                  height=6, ratio=4, marginal_kws={'kde': True})\n```\n\n#### Common Seaborn Issues and Solutions\n\n**Issue: Legend outside plot area**\n```python\ng = sns.relplot(...)\ng._legend.set_bbox_to_anchor((0.9, 0.5))\n```\n\n**Issue: Overlapping labels**\n```python\nplt.xticks(rotation=45, ha='right')\nplt.tight_layout()\n```\n\n**Issue: Text too small at final size**\n```python\nsns.set_context('paper', font_scale=1.2)  # Increase if needed\n```\n\n#### Additional Resources\n\nFor more detailed seaborn information, see:\n- `scientific-packages/seaborn/SKILL.md` - Comprehensive seaborn documentation\n- `scientific-packages/seaborn/references/examples.md` - Practical use cases\n- `scientific-packages/seaborn/references/function_reference.md` - Complete API reference\n- `scientific-packages/seaborn/references/objects_interface.md` - Modern declarative API\n\n### Plotly\n- Interactive figures for exploration\n- Export static images for publication\n- Configure for publication quality:\n```python\nfig.update_layout(\n    font=dict(family='Arial, sans-serif', size=10),\n    plot_bgcolor='white',\n    # ... see matplotlib_examples.md Example 8\n)\nfig.write_image('figure.png', scale=3)  # scale=3 gives ~300 DPI\n```\n\n## Resources\n\n### References Directory\n\n**Load these as needed for detailed information:**\n\n- **`publication_guidelines.md`**: Comprehensive best practices\n  - Resolution and file format requirements\n  - Typography guidelines\n  - Layout and composition rules\n  - Statistical rigor requirements\n  - Complete publication checklist\n\n- **`color_palettes.md`**: Color usage guide\n  - Colorblind-friendly palette specifications with RGB values\n  - Sequential and diverging colormap recommendations\n  - Testing procedures for accessibility\n  - Domain-specific palettes (genomics, microscopy)\n\n- **`journal_requirements.md`**: Journal-specific specifications\n  - Technical requirements by publisher\n  - File format and DPI specifications\n  - Figure dimension requirements\n  - Quick reference table\n\n- **`matplotlib_examples.md`**: Practical code examples\n  - 10 complete working examples\n  - Line plots, bar plots, heatmaps, multi-panel figures\n  - Journal-specific figure examples\n  - Tips for each library (matplotlib, seaborn, plotly)\n\n### Scripts Directory\n\n**Use these helper scripts for automation:**\n\n- **`figure_export.py`**: Export utilities\n  - `save_publication_figure()`: Save in multiple formats with correct DPI\n  - `save_for_journal()`: Use journal-specific requirements automatically\n  - `check_figure_size()`: Verify dimensions meet journal specs\n  - Run directly: `python scripts/figure_export.py` for examples\n\n- **`style_presets.py`**: Pre-configured styles\n  - `apply_publication_style()`: Apply preset styles (default, nature, science, cell)\n  - `set_color_palette()`: Quick palette switching\n  - `configure_for_journal()`: One-command journal configuration\n  - Run directly: `python scripts/style_presets.py` to see examples\n\n### Assets Directory\n\n**Use these files in figures:**\n\n- **`color_palettes.py`**: Importable color definitions\n  - All recommended palettes as Python constants\n  - `apply_palette()` helper function\n  - Can be imported directly into notebooks/scripts\n\n- **Matplotlib style files**: Use with `plt.style.use()`\n  - `publication.mplstyle`: General publication quality\n  - `nature.mplstyle`: Nature journal specifications\n  - `presentation.mplstyle`: Larger fonts for posters/slides\n\n## Workflow Summary\n\n**Recommended workflow for creating publication figures:**\n\n1. **Plan**: Determine target journal, figure type, and content\n2. **Configure**: Apply appropriate style for journal\n   ```python\n   from style_presets import configure_for_journal\n   configure_for_journal('nature', 'single')\n   ```\n3. **Create**: Build figure with proper labels, colors, statistics\n4. **Verify**: Check size, fonts, colors, accessibility\n   ```python\n   from figure_export import check_figure_size\n   check_figure_size(fig, journal='nature')\n   ```\n5. **Export**: Save in required formats\n   ```python\n   from figure_export import save_for_journal\n   save_for_journal(fig, 'figure1', 'nature', 'combination')\n   ```\n6. **Review**: View at final size in manuscript context\n\n## Common Pitfalls to Avoid\n\n1. **Font too small**: Text unreadable when printed at final size\n2. **JPEG format**: Never use JPEG for graphs/plots (creates artifacts)\n3. **Red-green colors**: ~8% of males cannot distinguish\n4. **Low resolution**: Pixelated figures in publication\n5. **Missing units**: Always label axes with units\n6. **3D effects**: Distorts perception, avoid completely\n7. **Chart junk**: Remove unnecessary gridlines, decorations\n8. **Truncated axes**: Start bar charts at zero unless scientifically justified\n9. **Inconsistent styling**: Different fonts/colors across figures in same manuscript\n10. **No error bars**: Always show uncertainty\n\n## Final Checklist\n\nBefore submitting figures, verify:\n\n- [ ] Resolution meets journal requirements (300+ DPI)\n- [ ] File format is correct (vector for plots, TIFF for images)\n- [ ] Figure size matches journal specifications\n- [ ] All text readable at final size (6 pt)\n- [ ] Colors are colorblind-friendly\n- [ ] Figure works in grayscale\n- [ ] All axes labeled with units\n- [ ] Error bars present with definition in caption\n- [ ] Panel labels present and consistent\n- [ ] No chart junk or 3D effects\n- [ ] Fonts consistent across all figures\n- [ ] Statistical significance clearly marked\n- [ ] Legend is clear and complete\n\nUse this skill to ensure scientific figures meet the highest publication standards while remaining accessible to all readers.\n",
        "data/k-dense-ai/scientific-writing/SKILL.md": "---\nname: scientific-writing\ndescription: \"Write scientific manuscripts. IMRAD structure, citations (APA/AMA/Vancouver), figures/tables, reporting guidelines (CONSORT/STROBE/PRISMA), abstracts, for research papers and journal submissions.\"\n---\n\n# Scientific Writing\n\n## Overview\n\nScientific writing is a process for communicating research with precision and clarity. Write manuscripts using IMRAD structure, citations (APA/AMA/Vancouver), figures/tables, and reporting guidelines (CONSORT/STROBE/PRISMA). Apply this skill for research papers and journal submissions.\n\n## When to Use This Skill\n\nThis skill should be used when:\n- Writing or revising any section of a scientific manuscript (abstract, introduction, methods, results, discussion)\n- Structuring a research paper using IMRAD or other standard formats\n- Formatting citations and references in specific styles (APA, AMA, Vancouver, Chicago, IEEE)\n- Creating, formatting, or improving figures, tables, and data visualizations\n- Applying study-specific reporting guidelines (CONSORT for trials, STROBE for observational studies, PRISMA for reviews)\n- Drafting abstracts that meet journal requirements (structured or unstructured)\n- Preparing manuscripts for submission to specific journals\n- Improving writing clarity, conciseness, and precision\n- Ensuring proper use of field-specific terminology and nomenclature\n- Addressing reviewer comments and revising manuscripts\n\n## Core Capabilities\n\n### 1. Manuscript Structure and Organization\n\n**IMRAD Format**: Guide papers through the standard Introduction, Methods, Results, And Discussion structure used across most scientific disciplines. This includes:\n- **Introduction**: Establish research context, identify gaps, state objectives\n- **Methods**: Detail study design, populations, procedures, and analysis approaches\n- **Results**: Present findings objectively without interpretation\n- **Discussion**: Interpret results, acknowledge limitations, propose future directions\n\nFor detailed guidance on IMRAD structure, refer to `references/imrad_structure.md`.\n\n**Alternative Structures**: Support discipline-specific formats including:\n- Review articles (narrative, systematic, scoping)\n- Case reports and case series\n- Meta-analyses and pooled analyses\n- Theoretical/modeling papers\n- Methods papers and protocols\n\n### 2. Section-Specific Writing Guidance\n\n**Abstract Composition**: Craft concise, standalone summaries (100-250 words) that capture the paper's purpose, methods, results, and conclusions. Support both structured abstracts (with labeled sections) and unstructured single-paragraph formats.\n\n**Introduction Development**: Build compelling introductions that:\n- Establish the research problem's importance\n- Review relevant literature systematically\n- Identify knowledge gaps or controversies\n- State clear research questions or hypotheses\n- Explain the study's novelty and significance\n\n**Methods Documentation**: Ensure reproducibility through:\n- Detailed participant/sample descriptions\n- Clear procedural documentation\n- Statistical methods with justification\n- Equipment and materials specifications\n- Ethical approval and consent statements\n\n**Results Presentation**: Present findings with:\n- Logical flow from primary to secondary outcomes\n- Integration with figures and tables\n- Statistical significance with effect sizes\n- Objective reporting without interpretation\n\n**Discussion Construction**: Synthesize findings by:\n- Relating results to research questions\n- Comparing with existing literature\n- Acknowledging limitations honestly\n- Proposing mechanistic explanations\n- Suggesting practical implications and future research\n\n### 3. Citation and Reference Management\n\nApply citation styles correctly across disciplines. For comprehensive style guides, refer to `references/citation_styles.md`.\n\n**Major Citation Styles:**\n- **AMA (American Medical Association)**: Numbered superscript citations, common in medicine\n- **Vancouver**: Numbered citations in square brackets, biomedical standard\n- **APA (American Psychological Association)**: Author-date in-text citations, common in social sciences\n- **Chicago**: Notes-bibliography or author-date, humanities and sciences\n- **IEEE**: Numbered square brackets, engineering and computer science\n\n**Best Practices:**\n- Cite primary sources when possible\n- Include recent literature (last 5-10 years for active fields)\n- Balance citation distribution across introduction and discussion\n- Verify all citations against original sources\n- Use reference management software (Zotero, Mendeley, EndNote)\n\n### 4. Figures and Tables\n\nCreate effective data visualizations that enhance comprehension. For detailed best practices, refer to `references/figures_tables.md`.\n\n**When to Use Tables vs. Figures:**\n- **Tables**: Precise numerical data, complex datasets, multiple variables requiring exact values\n- **Figures**: Trends, patterns, relationships, comparisons best understood visually\n\n**Design Principles:**\n- Make each table/figure self-explanatory with complete captions\n- Use consistent formatting and terminology across all display items\n- Label all axes, columns, and rows with units\n- Include sample sizes (n) and statistical annotations\n- Follow the \"one table/figure per 1000 words\" guideline\n- Avoid duplicating information between text, tables, and figures\n\n**Common Figure Types:**\n- Bar graphs: Comparing discrete categories\n- Line graphs: Showing trends over time\n- Scatterplots: Displaying correlations\n- Box plots: Showing distributions and outliers\n- Heatmaps: Visualizing matrices and patterns\n\n### 5. Reporting Guidelines by Study Type\n\nEnsure completeness and transparency by following established reporting standards. For comprehensive guideline details, refer to `references/reporting_guidelines.md`.\n\n**Key Guidelines:**\n- **CONSORT**: Randomized controlled trials\n- **STROBE**: Observational studies (cohort, case-control, cross-sectional)\n- **PRISMA**: Systematic reviews and meta-analyses\n- **STARD**: Diagnostic accuracy studies\n- **TRIPOD**: Prediction model studies\n- **ARRIVE**: Animal research\n- **CARE**: Case reports\n- **SQUIRE**: Quality improvement studies\n- **SPIRIT**: Study protocols for clinical trials\n- **CHEERS**: Economic evaluations\n\nEach guideline provides checklists ensuring all critical methodological elements are reported.\n\n### 6. Writing Principles and Style\n\nApply fundamental scientific writing principles. For detailed guidance, refer to `references/writing_principles.md`.\n\n**Clarity**:\n- Use precise, unambiguous language\n- Define technical terms and abbreviations at first use\n- Maintain logical flow within and between paragraphs\n- Use active voice when appropriate for clarity\n\n**Conciseness**:\n- Eliminate redundant words and phrases\n- Favor shorter sentences (15-20 words average)\n- Remove unnecessary qualifiers\n- Respect word limits strictly\n\n**Accuracy**:\n- Report exact values with appropriate precision\n- Use consistent terminology throughout\n- Distinguish between observations and interpretations\n- Acknowledge uncertainty appropriately\n\n**Objectivity**:\n- Present results without bias\n- Avoid overstating findings or implications\n- Acknowledge conflicting evidence\n- Maintain professional, neutral tone\n\n### 7. Journal-Specific Formatting\n\nAdapt manuscripts to journal requirements:\n- Follow author guidelines for structure, length, and format\n- Apply journal-specific citation styles\n- Meet figure/table specifications (resolution, file formats, dimensions)\n- Include required statements (funding, conflicts of interest, data availability, ethical approval)\n- Adhere to word limits for each section\n- Format according to template requirements when provided\n\n### 8. Field-Specific Language and Terminology\n\nAdapt language, terminology, and conventions to match the specific scientific discipline. Each field has established vocabulary, preferred phrasings, and domain-specific conventions that signal expertise and ensure clarity for the target audience.\n\n**Identify Field-Specific Linguistic Conventions:**\n- Review terminology used in recent high-impact papers in the target journal\n- Note field-specific abbreviations, units, and notation systems\n- Identify preferred terms (e.g., \"participants\" vs. \"subjects,\" \"compound\" vs. \"drug,\" \"specimens\" vs. \"samples\")\n- Observe how methods, organisms, or techniques are typically described\n\n**Biomedical and Clinical Sciences:**\n- Use precise anatomical and clinical terminology (e.g., \"myocardial infarction\" not \"heart attack\" in formal writing)\n- Follow standardized disease nomenclature (ICD, DSM, SNOMED-CT)\n- Specify drug names using generic names first, brand names in parentheses if needed\n- Use \"patients\" for clinical studies, \"participants\" for community-based research\n- Follow Human Genome Variation Society (HGVS) nomenclature for genetic variants\n- Report lab values with standard units (SI units in most international journals)\n\n**Molecular Biology and Genetics:**\n- Use italics for gene symbols (e.g., *TP53*), regular font for proteins (e.g., p53)\n- Follow species-specific gene nomenclature (uppercase for human: *BRCA1*; sentence case for mouse: *Brca1*)\n- Specify organism names in full at first mention, then use accepted abbreviations (e.g., *Escherichia coli*, then *E. coli*)\n- Use standard genetic notation (e.g., +/+, +/-, -/- for genotypes)\n- Employ established terminology for molecular techniques (e.g., \"quantitative PCR\" or \"qPCR,\" not \"real-time PCR\")\n\n**Chemistry and Pharmaceutical Sciences:**\n- Follow IUPAC nomenclature for chemical compounds\n- Use systematic names for novel compounds, common names for well-known substances\n- Specify chemical structures using standard notation (e.g., SMILES, InChI for databases)\n- Report concentrations with appropriate units (mM, M, nM, or % w/v, v/v)\n- Describe synthesis routes using accepted reaction nomenclature\n- Use terms like \"bioavailability,\" \"pharmacokinetics,\" \"IC50\" consistently with field definitions\n\n**Ecology and Environmental Sciences:**\n- Use binomial nomenclature for species (italicized: *Homo sapiens*)\n- Specify taxonomic authorities at first species mention when relevant\n- Employ standardized habitat and ecosystem classifications\n- Use consistent terminology for ecological metrics (e.g., \"species richness,\" \"Shannon diversity index\")\n- Describe sampling methods with field-standard terms (e.g., \"transect,\" \"quadrat,\" \"mark-recapture\")\n\n**Physics and Engineering:**\n- Follow SI units consistently unless field conventions dictate otherwise\n- Use standard notation for physical quantities (scalars vs. vectors, tensors)\n- Employ established terminology for phenomena (e.g., \"quantum entanglement,\" \"laminar flow\")\n- Specify equipment with model numbers and manufacturers when relevant\n- Use mathematical notation consistent with field standards (e.g.,  for reduced Planck constant)\n\n**Neuroscience:**\n- Use standardized brain region nomenclature (e.g., refer to atlases like Allen Brain Atlas)\n- Specify coordinates for brain regions using established stereotaxic systems\n- Follow conventions for neural terminology (e.g., \"action potential\" not \"spike\" in formal writing)\n- Use \"neural activity,\" \"neuronal firing,\" \"brain activation\" appropriately based on measurement method\n- Describe recording techniques with proper specificity (e.g., \"whole-cell patch clamp,\" \"extracellular recording\")\n\n**Social and Behavioral Sciences:**\n- Use person-first language when appropriate (e.g., \"people with schizophrenia\" not \"schizophrenics\")\n- Employ standardized psychological constructs and validated assessment names\n- Follow APA guidelines for reducing bias in language\n- Specify theoretical frameworks using established terminology\n- Use \"participants\" rather than \"subjects\" for human research\n\n**General Principles:**\n\n**Match Audience Expertise:**\n- For specialized journals: Use field-specific terminology freely, define only highly specialized or novel terms\n- For broad-impact journals (e.g., *Nature*, *Science*): Define more technical terms, provide context for specialized concepts\n- For interdisciplinary audiences: Balance precision with accessibility, define terms at first use\n\n**Define Technical Terms Strategically:**\n- Define abbreviations at first use: \"messenger RNA (mRNA)\"\n- Provide brief explanations for specialized techniques when writing for broader audiences\n- Avoid over-defining terms well-known to the target audience (signals unfamiliarity with field)\n- Create a glossary if numerous specialized terms are unavoidable\n\n**Maintain Consistency:**\n- Use the same term for the same concept throughout (don't alternate between \"medication,\" \"drug,\" and \"pharmaceutical\")\n- Follow a consistent system for abbreviations (decide on \"PCR\" or \"polymerase chain reaction\" after first definition)\n- Apply the same nomenclature system throughout (especially for genes, species, chemicals)\n\n**Avoid Field Mixing Errors:**\n- Don't use clinical terminology for basic science (e.g., don't call mice \"patients\")\n- Avoid colloquialisms or overly general terms in place of precise field terminology\n- Don't import terminology from adjacent fields without ensuring proper usage\n\n**Verify Terminology Usage:**\n- Consult field-specific style guides and nomenclature resources\n- Check how terms are used in recent papers from the target journal\n- Use domain-specific databases and ontologies (e.g., Gene Ontology, MeSH terms)\n- When uncertain, cite a key reference that establishes terminology\n\n### 9. Common Pitfalls to Avoid\n\n**Top Rejection Reasons:**\n1. Inappropriate, incomplete, or insufficiently described statistics\n2. Over-interpretation of results or unsupported conclusions\n3. Poorly described methods affecting reproducibility\n4. Small, biased, or inappropriate samples\n5. Poor writing quality or difficult-to-follow text\n6. Inadequate literature review or context\n7. Figures and tables that are unclear or poorly designed\n8. Failure to follow reporting guidelines\n\n**Writing Quality Issues:**\n- Mixing tenses inappropriately (use past tense for methods/results, present for established facts)\n- Excessive jargon or undefined acronyms\n- Paragraph breaks that disrupt logical flow\n- Missing transitions between sections\n- Inconsistent notation or terminology\n\n## Workflow for Manuscript Development\n\n**Stage 1: Planning**\n1. Identify target journal and review author guidelines\n2. Determine applicable reporting guideline (CONSORT, STROBE, etc.)\n3. Outline manuscript structure (usually IMRAD)\n4. Plan figures and tables as the backbone of the paper\n\n**Stage 2: Drafting**\n1. Start with figures and tables (the core data story)\n2. Write Methods (often easiest to draft first)\n3. Draft Results (describing figures/tables objectively)\n4. Compose Discussion (interpreting findings)\n5. Write Introduction (setting up the research question)\n6. Craft Abstract (synthesizing the complete story)\n7. Create Title (concise and descriptive)\n\n**Stage 3: Revision**\n1. Check logical flow and \"red thread\" throughout\n2. Verify consistency in terminology and notation\n3. Ensure figures/tables are self-explanatory\n4. Confirm adherence to reporting guidelines\n5. Verify all citations are accurate and properly formatted\n6. Check word counts for each section\n7. Proofread for grammar, spelling, and clarity\n\n**Stage 4: Final Preparation**\n1. Format according to journal requirements\n2. Prepare supplementary materials\n3. Write cover letter highlighting significance\n4. Complete submission checklists\n5. Gather all required statements and forms\n\n## Integration with Other Scientific Skills\n\nThis skill works effectively with:\n- **Data analysis skills**: For generating results to report\n- **Statistical analysis**: For determining appropriate statistical presentations\n- **Literature review skills**: For contextualizing research\n- **Figure creation tools**: For developing publication-quality visualizations\n\n## References\n\nThis skill includes comprehensive reference files covering specific aspects of scientific writing:\n\n- `references/imrad_structure.md`: Detailed guide to IMRAD format and section-specific content\n- `references/citation_styles.md`: Complete citation style guides (APA, AMA, Vancouver, Chicago, IEEE)\n- `references/figures_tables.md`: Best practices for creating effective data visualizations\n- `references/reporting_guidelines.md`: Study-specific reporting standards and checklists\n- `references/writing_principles.md`: Core principles of effective scientific communication\n\nLoad these references as needed when working on specific aspects of scientific writing.\n",
        "data/k-dense-ai/scikit-bio/SKILL.md": "---\nname: scikit-bio\ndescription: \"Biological data toolkit. Sequence analysis, alignments, phylogenetic trees, diversity metrics (alpha/beta, UniFrac), ordination (PCoA), PERMANOVA, FASTA/Newick I/O, for microbiome analysis.\"\n---\n\n# scikit-bio\n\n## Overview\n\nscikit-bio is a comprehensive Python library for working with biological data. Apply this skill for bioinformatics analyses spanning sequence manipulation, alignment, phylogenetics, microbial ecology, and multivariate statistics.\n\n## When to Use This Skill\n\nThis skill should be used when the user:\n- Works with biological sequences (DNA, RNA, protein)\n- Needs to read/write biological file formats (FASTA, FASTQ, GenBank, Newick, BIOM, etc.)\n- Performs sequence alignments or searches for motifs\n- Constructs or analyzes phylogenetic trees\n- Calculates diversity metrics (alpha/beta diversity, UniFrac distances)\n- Performs ordination analysis (PCoA, CCA, RDA)\n- Runs statistical tests on biological/ecological data (PERMANOVA, ANOSIM, Mantel)\n- Analyzes microbiome or community ecology data\n- Works with protein embeddings from language models\n- Needs to manipulate biological data tables\n\n## Core Capabilities\n\n### 1. Sequence Manipulation\n\nWork with biological sequences using specialized classes for DNA, RNA, and protein data.\n\n**Key operations:**\n- Read/write sequences from FASTA, FASTQ, GenBank, EMBL formats\n- Sequence slicing, concatenation, and searching\n- Reverse complement, transcription (DNARNA), and translation (RNAprotein)\n- Find motifs and patterns using regex\n- Calculate distances (Hamming, k-mer based)\n- Handle sequence quality scores and metadata\n\n**Common patterns:**\n```python\nimport skbio\n\n# Read sequences from file\nseq = skbio.DNA.read('input.fasta')\n\n# Sequence operations\nrc = seq.reverse_complement()\nrna = seq.transcribe()\nprotein = rna.translate()\n\n# Find motifs\nmotif_positions = seq.find_with_regex('ATG[ACGT]{3}')\n\n# Check for properties\nhas_degens = seq.has_degenerates()\nseq_no_gaps = seq.degap()\n```\n\n**Important notes:**\n- Use `DNA`, `RNA`, `Protein` classes for grammared sequences with validation\n- Use `Sequence` class for generic sequences without alphabet restrictions\n- Quality scores automatically loaded from FASTQ files into positional metadata\n- Metadata types: sequence-level (ID, description), positional (per-base), interval (regions/features)\n\n### 2. Sequence Alignment\n\nPerform pairwise and multiple sequence alignments using dynamic programming algorithms.\n\n**Key capabilities:**\n- Global alignment (Needleman-Wunsch with semi-global variant)\n- Local alignment (Smith-Waterman)\n- Configurable scoring schemes (match/mismatch, gap penalties, substitution matrices)\n- CIGAR string conversion\n- Multiple sequence alignment storage and manipulation with `TabularMSA`\n\n**Common patterns:**\n```python\nfrom skbio.alignment import local_pairwise_align_ssw, TabularMSA\n\n# Pairwise alignment\nalignment = local_pairwise_align_ssw(seq1, seq2)\n\n# Access aligned sequences\nmsa = alignment.aligned_sequences\n\n# Read multiple alignment from file\nmsa = TabularMSA.read('alignment.fasta', constructor=skbio.DNA)\n\n# Calculate consensus\nconsensus = msa.consensus()\n```\n\n**Important notes:**\n- Use `local_pairwise_align_ssw` for local alignments (faster, SSW-based)\n- Use `StripedSmithWaterman` for protein alignments\n- Affine gap penalties recommended for biological sequences\n- Can convert between scikit-bio, BioPython, and Biotite alignment formats\n\n### 3. Phylogenetic Trees\n\nConstruct, manipulate, and analyze phylogenetic trees representing evolutionary relationships.\n\n**Key capabilities:**\n- Tree construction from distance matrices (UPGMA, WPGMA, Neighbor Joining, GME, BME)\n- Tree manipulation (pruning, rerooting, traversal)\n- Distance calculations (patristic, cophenetic, Robinson-Foulds)\n- ASCII visualization\n- Newick format I/O\n\n**Common patterns:**\n```python\nfrom skbio import TreeNode\nfrom skbio.tree import nj\n\n# Read tree from file\ntree = TreeNode.read('tree.nwk')\n\n# Construct tree from distance matrix\ntree = nj(distance_matrix)\n\n# Tree operations\nsubtree = tree.shear(['taxon1', 'taxon2', 'taxon3'])\ntips = [node for node in tree.tips()]\nlca = tree.lowest_common_ancestor(['taxon1', 'taxon2'])\n\n# Calculate distances\npatristic_dist = tree.find('taxon1').distance(tree.find('taxon2'))\ncophenetic_matrix = tree.cophenetic_matrix()\n\n# Compare trees\nrf_distance = tree.robinson_foulds(other_tree)\n```\n\n**Important notes:**\n- Use `nj()` for neighbor joining (classic phylogenetic method)\n- Use `upgma()` for UPGMA (assumes molecular clock)\n- GME and BME are highly scalable for large trees\n- Trees can be rooted or unrooted; some metrics require specific rooting\n\n### 4. Diversity Analysis\n\nCalculate alpha and beta diversity metrics for microbial ecology and community analysis.\n\n**Key capabilities:**\n- Alpha diversity: richness, Shannon entropy, Simpson index, Faith's PD, Pielou's evenness\n- Beta diversity: Bray-Curtis, Jaccard, weighted/unweighted UniFrac, Euclidean distances\n- Phylogenetic diversity metrics (require tree input)\n- Rarefaction and subsampling\n- Integration with ordination and statistical tests\n\n**Common patterns:**\n```python\nfrom skbio.diversity import alpha_diversity, beta_diversity\nimport skbio\n\n# Alpha diversity\nalpha = alpha_diversity('shannon', counts_matrix, ids=sample_ids)\nfaith_pd = alpha_diversity('faith_pd', counts_matrix, ids=sample_ids,\n                          tree=tree, otu_ids=feature_ids)\n\n# Beta diversity\nbc_dm = beta_diversity('braycurtis', counts_matrix, ids=sample_ids)\nunifrac_dm = beta_diversity('unweighted_unifrac', counts_matrix,\n                           ids=sample_ids, tree=tree, otu_ids=feature_ids)\n\n# Get available metrics\nfrom skbio.diversity import get_alpha_diversity_metrics\nprint(get_alpha_diversity_metrics())\n```\n\n**Important notes:**\n- Counts must be integers representing abundances, not relative frequencies\n- Phylogenetic metrics (Faith's PD, UniFrac) require tree and OTU ID mapping\n- Use `partial_beta_diversity()` for computing specific sample pairs only\n- Alpha diversity returns Series, beta diversity returns DistanceMatrix\n\n### 5. Ordination Methods\n\nReduce high-dimensional biological data to visualizable lower-dimensional spaces.\n\n**Key capabilities:**\n- PCoA (Principal Coordinate Analysis) from distance matrices\n- CA (Correspondence Analysis) for contingency tables\n- CCA (Canonical Correspondence Analysis) with environmental constraints\n- RDA (Redundancy Analysis) for linear relationships\n- Biplot projection for feature interpretation\n\n**Common patterns:**\n```python\nfrom skbio.stats.ordination import pcoa, cca\n\n# PCoA from distance matrix\npcoa_results = pcoa(distance_matrix)\npc1 = pcoa_results.samples['PC1']\npc2 = pcoa_results.samples['PC2']\n\n# CCA with environmental variables\ncca_results = cca(species_matrix, environmental_matrix)\n\n# Save/load ordination results\npcoa_results.write('ordination.txt')\nresults = skbio.OrdinationResults.read('ordination.txt')\n```\n\n**Important notes:**\n- PCoA works with any distance/dissimilarity matrix\n- CCA reveals environmental drivers of community composition\n- Ordination results include eigenvalues, proportion explained, and sample/feature coordinates\n- Results integrate with plotting libraries (matplotlib, seaborn, plotly)\n\n### 6. Statistical Testing\n\nPerform hypothesis tests specific to ecological and biological data.\n\n**Key capabilities:**\n- PERMANOVA: test group differences using distance matrices\n- ANOSIM: alternative test for group differences\n- PERMDISP: test homogeneity of group dispersions\n- Mantel test: correlation between distance matrices\n- Bioenv: find environmental variables correlated with distances\n\n**Common patterns:**\n```python\nfrom skbio.stats.distance import permanova, anosim, mantel\n\n# Test if groups differ significantly\npermanova_results = permanova(distance_matrix, grouping, permutations=999)\nprint(f\"p-value: {permanova_results['p-value']}\")\n\n# ANOSIM test\nanosim_results = anosim(distance_matrix, grouping, permutations=999)\n\n# Mantel test between two distance matrices\nmantel_results = mantel(dm1, dm2, method='pearson', permutations=999)\nprint(f\"Correlation: {mantel_results[0]}, p-value: {mantel_results[1]}\")\n```\n\n**Important notes:**\n- Permutation tests provide non-parametric significance testing\n- Use 999+ permutations for robust p-values\n- PERMANOVA sensitive to dispersion differences; pair with PERMDISP\n- Mantel tests assess matrix correlation (e.g., geographic vs genetic distance)\n\n### 7. File I/O and Format Conversion\n\nRead and write 19+ biological file formats with automatic format detection.\n\n**Supported formats:**\n- Sequences: FASTA, FASTQ, GenBank, EMBL, QSeq\n- Alignments: Clustal, PHYLIP, Stockholm\n- Trees: Newick\n- Tables: BIOM (HDF5 and JSON)\n- Distances: delimited square matrices\n- Analysis: BLAST+6/7, GFF3, Ordination results\n- Metadata: TSV/CSV with validation\n\n**Common patterns:**\n```python\nimport skbio\n\n# Read with automatic format detection\nseq = skbio.DNA.read('file.fasta', format='fasta')\ntree = skbio.TreeNode.read('tree.nwk')\n\n# Write to file\nseq.write('output.fasta', format='fasta')\n\n# Generator for large files (memory efficient)\nfor seq in skbio.io.read('large.fasta', format='fasta', constructor=skbio.DNA):\n    process(seq)\n\n# Convert formats\nseqs = list(skbio.io.read('input.fastq', format='fastq', constructor=skbio.DNA))\nskbio.io.write(seqs, format='fasta', into='output.fasta')\n```\n\n**Important notes:**\n- Use generators for large files to avoid memory issues\n- Format can be auto-detected when `into` parameter specified\n- Some objects can be written to multiple formats\n- Support for stdin/stdout piping with `verify=False`\n\n### 8. Distance Matrices\n\nCreate and manipulate distance/dissimilarity matrices with statistical methods.\n\n**Key capabilities:**\n- Store symmetric (DistanceMatrix) or asymmetric (DissimilarityMatrix) data\n- ID-based indexing and slicing\n- Integration with diversity, ordination, and statistical tests\n- Read/write delimited text format\n\n**Common patterns:**\n```python\nfrom skbio import DistanceMatrix\nimport numpy as np\n\n# Create from array\ndata = np.array([[0, 1, 2], [1, 0, 3], [2, 3, 0]])\ndm = DistanceMatrix(data, ids=['A', 'B', 'C'])\n\n# Access distances\ndist_ab = dm['A', 'B']\nrow_a = dm['A']\n\n# Read from file\ndm = DistanceMatrix.read('distances.txt')\n\n# Use in downstream analyses\npcoa_results = pcoa(dm)\npermanova_results = permanova(dm, grouping)\n```\n\n**Important notes:**\n- DistanceMatrix enforces symmetry and zero diagonal\n- DissimilarityMatrix allows asymmetric values\n- IDs enable integration with metadata and biological knowledge\n- Compatible with pandas, numpy, and scikit-learn\n\n### 9. Biological Tables\n\nWork with feature tables (OTU/ASV tables) common in microbiome research.\n\n**Key capabilities:**\n- BIOM format I/O (HDF5 and JSON)\n- Integration with pandas, polars, AnnData, numpy\n- Data augmentation techniques (phylomix, mixup, compositional methods)\n- Sample/feature filtering and normalization\n- Metadata integration\n\n**Common patterns:**\n```python\nfrom skbio import Table\n\n# Read BIOM table\ntable = Table.read('table.biom')\n\n# Access data\nsample_ids = table.ids(axis='sample')\nfeature_ids = table.ids(axis='observation')\ncounts = table.matrix_data\n\n# Filter\nfiltered = table.filter(sample_ids_to_keep, axis='sample')\n\n# Convert to/from pandas\ndf = table.to_dataframe()\ntable = Table.from_dataframe(df)\n```\n\n**Important notes:**\n- BIOM tables are standard in QIIME 2 workflows\n- Rows typically represent samples, columns represent features (OTUs/ASVs)\n- Supports sparse and dense representations\n- Output format configurable (pandas/polars/numpy)\n\n### 10. Protein Embeddings\n\nWork with protein language model embeddings for downstream analysis.\n\n**Key capabilities:**\n- Store embeddings from protein language models (ESM, ProtTrans, etc.)\n- Convert embeddings to distance matrices\n- Generate ordination objects for visualization\n- Export to numpy/pandas for ML workflows\n\n**Common patterns:**\n```python\nfrom skbio.embedding import ProteinEmbedding, ProteinVector\n\n# Create embedding from array\nembedding = ProteinEmbedding(embedding_array, sequence_ids)\n\n# Convert to distance matrix for analysis\ndm = embedding.to_distances(metric='euclidean')\n\n# PCoA visualization of embedding space\npcoa_results = embedding.to_ordination(metric='euclidean', method='pcoa')\n\n# Export for machine learning\narray = embedding.to_array()\ndf = embedding.to_dataframe()\n```\n\n**Important notes:**\n- Embeddings bridge protein language models with traditional bioinformatics\n- Compatible with scikit-bio's distance/ordination/statistics ecosystem\n- SequenceEmbedding and ProteinEmbedding provide specialized functionality\n- Useful for sequence clustering, classification, and visualization\n\n## Best Practices\n\n### Installation\n```bash\nuv pip install scikit-bio\n```\n\n### Performance Considerations\n- Use generators for large sequence files to minimize memory usage\n- For massive phylogenetic trees, prefer GME or BME over NJ\n- Beta diversity calculations can be parallelized with `partial_beta_diversity()`\n- BIOM format (HDF5) more efficient than JSON for large tables\n\n### Integration with Ecosystem\n- Sequences interoperate with Biopython via standard formats\n- Tables integrate with pandas, polars, and AnnData\n- Distance matrices compatible with scikit-learn\n- Ordination results visualizable with matplotlib/seaborn/plotly\n- Works seamlessly with QIIME 2 artifacts (BIOM, trees, distance matrices)\n\n### Common Workflows\n1. **Microbiome diversity analysis**: Read BIOM table  Calculate alpha/beta diversity  Ordination (PCoA)  Statistical testing (PERMANOVA)\n2. **Phylogenetic analysis**: Read sequences  Align  Build distance matrix  Construct tree  Calculate phylogenetic distances\n3. **Sequence processing**: Read FASTQ  Quality filter  Trim/clean  Find motifs  Translate  Write FASTA\n4. **Comparative genomics**: Read sequences  Pairwise alignment  Calculate distances  Build tree  Analyze clades\n\n## Reference Documentation\n\nFor detailed API information, parameter specifications, and advanced usage examples, refer to `references/api_reference.md` which contains comprehensive documentation on:\n- Complete method signatures and parameters for all capabilities\n- Extended code examples for complex workflows\n- Troubleshooting common issues\n- Performance optimization tips\n- Integration patterns with other libraries\n\n## Additional Resources\n\n- Official documentation: https://scikit.bio/docs/latest/\n- GitHub repository: https://github.com/scikit-bio/scikit-bio\n- Forum support: https://forum.qiime2.org (scikit-bio is part of QIIME 2 ecosystem)\n",
        "data/k-dense-ai/scikit-learn/SKILL.md": "---\nname: scikit-learn\ndescription: Machine learning in Python with scikit-learn. Use when working with supervised learning (classification, regression), unsupervised learning (clustering, dimensionality reduction), model evaluation, hyperparameter tuning, preprocessing, or building ML pipelines. Provides comprehensive reference documentation for algorithms, preprocessing techniques, pipelines, and best practices.\n---\n\n# Scikit-learn\n\n## Overview\n\nThis skill provides comprehensive guidance for machine learning tasks using scikit-learn, the industry-standard Python library for classical machine learning. Use this skill for classification, regression, clustering, dimensionality reduction, preprocessing, model evaluation, and building production-ready ML pipelines.\n\n## Installation\n\n```bash\n# Install scikit-learn using uv\nuv uv pip install scikit-learn\n\n# Optional: Install visualization dependencies\nuv uv pip install matplotlib seaborn\n\n# Commonly used with\nuv uv pip install pandas numpy\n```\n\n## When to Use This Skill\n\nUse the scikit-learn skill when:\n\n- Building classification or regression models\n- Performing clustering or dimensionality reduction\n- Preprocessing and transforming data for machine learning\n- Evaluating model performance with cross-validation\n- Tuning hyperparameters with grid or random search\n- Creating ML pipelines for production workflows\n- Comparing different algorithms for a task\n- Working with both structured (tabular) and text data\n- Need interpretable, classical machine learning approaches\n\n## Quick Start\n\n### Classification Example\n\n```python\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, stratify=y, random_state=42\n)\n\n# Preprocess\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Train model\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel.fit(X_train_scaled, y_train)\n\n# Evaluate\ny_pred = model.predict(X_test_scaled)\nprint(classification_report(y_test, y_pred))\n```\n\n### Complete Pipeline with Mixed Data\n\n```python\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# Define feature types\nnumeric_features = ['age', 'income']\ncategorical_features = ['gender', 'occupation']\n\n# Create preprocessing pipelines\nnumeric_transformer = Pipeline([\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())\n])\n\ncategorical_transformer = Pipeline([\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Combine transformers\npreprocessor = ColumnTransformer([\n    ('num', numeric_transformer, numeric_features),\n    ('cat', categorical_transformer, categorical_features)\n])\n\n# Full pipeline\nmodel = Pipeline([\n    ('preprocessor', preprocessor),\n    ('classifier', GradientBoostingClassifier(random_state=42))\n])\n\n# Fit and predict\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n```\n\n## Core Capabilities\n\n### 1. Supervised Learning\n\nComprehensive algorithms for classification and regression tasks.\n\n**Key algorithms:**\n- **Linear models**: Logistic Regression, Linear Regression, Ridge, Lasso, ElasticNet\n- **Tree-based**: Decision Trees, Random Forest, Gradient Boosting\n- **Support Vector Machines**: SVC, SVR with various kernels\n- **Ensemble methods**: AdaBoost, Voting, Stacking\n- **Neural Networks**: MLPClassifier, MLPRegressor\n- **Others**: Naive Bayes, K-Nearest Neighbors\n\n**When to use:**\n- Classification: Predicting discrete categories (spam detection, image classification, fraud detection)\n- Regression: Predicting continuous values (price prediction, demand forecasting)\n\n**See:** `references/supervised_learning.md` for detailed algorithm documentation, parameters, and usage examples.\n\n### 2. Unsupervised Learning\n\nDiscover patterns in unlabeled data through clustering and dimensionality reduction.\n\n**Clustering algorithms:**\n- **Partition-based**: K-Means, MiniBatchKMeans\n- **Density-based**: DBSCAN, HDBSCAN, OPTICS\n- **Hierarchical**: AgglomerativeClustering\n- **Probabilistic**: Gaussian Mixture Models\n- **Others**: MeanShift, SpectralClustering, BIRCH\n\n**Dimensionality reduction:**\n- **Linear**: PCA, TruncatedSVD, NMF\n- **Manifold learning**: t-SNE, UMAP, Isomap, LLE\n- **Feature extraction**: FastICA, LatentDirichletAllocation\n\n**When to use:**\n- Customer segmentation, anomaly detection, data visualization\n- Reducing feature dimensions, exploratory data analysis\n- Topic modeling, image compression\n\n**See:** `references/unsupervised_learning.md` for detailed documentation.\n\n### 3. Model Evaluation and Selection\n\nTools for robust model evaluation, cross-validation, and hyperparameter tuning.\n\n**Cross-validation strategies:**\n- KFold, StratifiedKFold (classification)\n- TimeSeriesSplit (temporal data)\n- GroupKFold (grouped samples)\n\n**Hyperparameter tuning:**\n- GridSearchCV (exhaustive search)\n- RandomizedSearchCV (random sampling)\n- HalvingGridSearchCV (successive halving)\n\n**Metrics:**\n- **Classification**: accuracy, precision, recall, F1-score, ROC AUC, confusion matrix\n- **Regression**: MSE, RMSE, MAE, R, MAPE\n- **Clustering**: silhouette score, Calinski-Harabasz, Davies-Bouldin\n\n**When to use:**\n- Comparing model performance objectively\n- Finding optimal hyperparameters\n- Preventing overfitting through cross-validation\n- Understanding model behavior with learning curves\n\n**See:** `references/model_evaluation.md` for comprehensive metrics and tuning strategies.\n\n### 4. Data Preprocessing\n\nTransform raw data into formats suitable for machine learning.\n\n**Scaling and normalization:**\n- StandardScaler (zero mean, unit variance)\n- MinMaxScaler (bounded range)\n- RobustScaler (robust to outliers)\n- Normalizer (sample-wise normalization)\n\n**Encoding categorical variables:**\n- OneHotEncoder (nominal categories)\n- OrdinalEncoder (ordered categories)\n- LabelEncoder (target encoding)\n\n**Handling missing values:**\n- SimpleImputer (mean, median, most frequent)\n- KNNImputer (k-nearest neighbors)\n- IterativeImputer (multivariate imputation)\n\n**Feature engineering:**\n- PolynomialFeatures (interaction terms)\n- KBinsDiscretizer (binning)\n- Feature selection (RFE, SelectKBest, SelectFromModel)\n\n**When to use:**\n- Before training any algorithm that requires scaled features (SVM, KNN, Neural Networks)\n- Converting categorical variables to numeric format\n- Handling missing data systematically\n- Creating non-linear features for linear models\n\n**See:** `references/preprocessing.md` for detailed preprocessing techniques.\n\n### 5. Pipelines and Composition\n\nBuild reproducible, production-ready ML workflows.\n\n**Key components:**\n- **Pipeline**: Chain transformers and estimators sequentially\n- **ColumnTransformer**: Apply different preprocessing to different columns\n- **FeatureUnion**: Combine multiple transformers in parallel\n- **TransformedTargetRegressor**: Transform target variable\n\n**Benefits:**\n- Prevents data leakage in cross-validation\n- Simplifies code and improves maintainability\n- Enables joint hyperparameter tuning\n- Ensures consistency between training and prediction\n\n**When to use:**\n- Always use Pipelines for production workflows\n- When mixing numerical and categorical features (use ColumnTransformer)\n- When performing cross-validation with preprocessing steps\n- When hyperparameter tuning includes preprocessing parameters\n\n**See:** `references/pipelines_and_composition.md` for comprehensive pipeline patterns.\n\n## Example Scripts\n\n### Classification Pipeline\n\nRun a complete classification workflow with preprocessing, model comparison, hyperparameter tuning, and evaluation:\n\n```bash\npython scripts/classification_pipeline.py\n```\n\nThis script demonstrates:\n- Handling mixed data types (numeric and categorical)\n- Model comparison using cross-validation\n- Hyperparameter tuning with GridSearchCV\n- Comprehensive evaluation with multiple metrics\n- Feature importance analysis\n\n### Clustering Analysis\n\nPerform clustering analysis with algorithm comparison and visualization:\n\n```bash\npython scripts/clustering_analysis.py\n```\n\nThis script demonstrates:\n- Finding optimal number of clusters (elbow method, silhouette analysis)\n- Comparing multiple clustering algorithms (K-Means, DBSCAN, Agglomerative, Gaussian Mixture)\n- Evaluating clustering quality without ground truth\n- Visualizing results with PCA projection\n\n## Reference Documentation\n\nThis skill includes comprehensive reference files for deep dives into specific topics:\n\n### Quick Reference\n**File:** `references/quick_reference.md`\n- Common import patterns and installation instructions\n- Quick workflow templates for common tasks\n- Algorithm selection cheat sheets\n- Common patterns and gotchas\n- Performance optimization tips\n\n### Supervised Learning\n**File:** `references/supervised_learning.md`\n- Linear models (regression and classification)\n- Support Vector Machines\n- Decision Trees and ensemble methods\n- K-Nearest Neighbors, Naive Bayes, Neural Networks\n- Algorithm selection guide\n\n### Unsupervised Learning\n**File:** `references/unsupervised_learning.md`\n- All clustering algorithms with parameters and use cases\n- Dimensionality reduction techniques\n- Outlier and novelty detection\n- Gaussian Mixture Models\n- Method selection guide\n\n### Model Evaluation\n**File:** `references/model_evaluation.md`\n- Cross-validation strategies\n- Hyperparameter tuning methods\n- Classification, regression, and clustering metrics\n- Learning and validation curves\n- Best practices for model selection\n\n### Preprocessing\n**File:** `references/preprocessing.md`\n- Feature scaling and normalization\n- Encoding categorical variables\n- Missing value imputation\n- Feature engineering techniques\n- Custom transformers\n\n### Pipelines and Composition\n**File:** `references/pipelines_and_composition.md`\n- Pipeline construction and usage\n- ColumnTransformer for mixed data types\n- FeatureUnion for parallel transformations\n- Complete end-to-end examples\n- Best practices\n\n## Common Workflows\n\n### Building a Classification Model\n\n1. **Load and explore data**\n   ```python\n   import pandas as pd\n   df = pd.read_csv('data.csv')\n   X = df.drop('target', axis=1)\n   y = df['target']\n   ```\n\n2. **Split data with stratification**\n   ```python\n   from sklearn.model_selection import train_test_split\n   X_train, X_test, y_train, y_test = train_test_split(\n       X, y, test_size=0.2, stratify=y, random_state=42\n   )\n   ```\n\n3. **Create preprocessing pipeline**\n   ```python\n   from sklearn.pipeline import Pipeline\n   from sklearn.preprocessing import StandardScaler\n   from sklearn.compose import ColumnTransformer\n\n   # Handle numeric and categorical features separately\n   preprocessor = ColumnTransformer([\n       ('num', StandardScaler(), numeric_features),\n       ('cat', OneHotEncoder(), categorical_features)\n   ])\n   ```\n\n4. **Build complete pipeline**\n   ```python\n   model = Pipeline([\n       ('preprocessor', preprocessor),\n       ('classifier', RandomForestClassifier(random_state=42))\n   ])\n   ```\n\n5. **Tune hyperparameters**\n   ```python\n   from sklearn.model_selection import GridSearchCV\n\n   param_grid = {\n       'classifier__n_estimators': [100, 200],\n       'classifier__max_depth': [10, 20, None]\n   }\n\n   grid_search = GridSearchCV(model, param_grid, cv=5)\n   grid_search.fit(X_train, y_train)\n   ```\n\n6. **Evaluate on test set**\n   ```python\n   from sklearn.metrics import classification_report\n\n   best_model = grid_search.best_estimator_\n   y_pred = best_model.predict(X_test)\n   print(classification_report(y_test, y_pred))\n   ```\n\n### Performing Clustering Analysis\n\n1. **Preprocess data**\n   ```python\n   from sklearn.preprocessing import StandardScaler\n\n   scaler = StandardScaler()\n   X_scaled = scaler.fit_transform(X)\n   ```\n\n2. **Find optimal number of clusters**\n   ```python\n   from sklearn.cluster import KMeans\n   from sklearn.metrics import silhouette_score\n\n   scores = []\n   for k in range(2, 11):\n       kmeans = KMeans(n_clusters=k, random_state=42)\n       labels = kmeans.fit_predict(X_scaled)\n       scores.append(silhouette_score(X_scaled, labels))\n\n   optimal_k = range(2, 11)[np.argmax(scores)]\n   ```\n\n3. **Apply clustering**\n   ```python\n   model = KMeans(n_clusters=optimal_k, random_state=42)\n   labels = model.fit_predict(X_scaled)\n   ```\n\n4. **Visualize with dimensionality reduction**\n   ```python\n   from sklearn.decomposition import PCA\n\n   pca = PCA(n_components=2)\n   X_2d = pca.fit_transform(X_scaled)\n\n   plt.scatter(X_2d[:, 0], X_2d[:, 1], c=labels, cmap='viridis')\n   ```\n\n## Best Practices\n\n### Always Use Pipelines\nPipelines prevent data leakage and ensure consistency:\n```python\n# Good: Preprocessing in pipeline\npipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('model', LogisticRegression())\n])\n\n# Bad: Preprocessing outside (can leak information)\nX_scaled = StandardScaler().fit_transform(X)\n```\n\n### Fit on Training Data Only\nNever fit on test data:\n```python\n# Good\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)  # Only transform\n\n# Bad\nscaler = StandardScaler()\nX_all_scaled = scaler.fit_transform(np.vstack([X_train, X_test]))\n```\n\n### Use Stratified Splitting for Classification\nPreserve class distribution:\n```python\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, stratify=y, random_state=42\n)\n```\n\n### Set Random State for Reproducibility\n```python\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)\n```\n\n### Choose Appropriate Metrics\n- Balanced data: Accuracy, F1-score\n- Imbalanced data: Precision, Recall, ROC AUC, Balanced Accuracy\n- Cost-sensitive: Define custom scorer\n\n### Scale Features When Required\nAlgorithms requiring feature scaling:\n- SVM, KNN, Neural Networks\n- PCA, Linear/Logistic Regression with regularization\n- K-Means clustering\n\nAlgorithms not requiring scaling:\n- Tree-based models (Decision Trees, Random Forest, Gradient Boosting)\n- Naive Bayes\n\n## Troubleshooting Common Issues\n\n### ConvergenceWarning\n**Issue:** Model didn't converge\n**Solution:** Increase `max_iter` or scale features\n```python\nmodel = LogisticRegression(max_iter=1000)\n```\n\n### Poor Performance on Test Set\n**Issue:** Overfitting\n**Solution:** Use regularization, cross-validation, or simpler model\n```python\n# Add regularization\nmodel = Ridge(alpha=1.0)\n\n# Use cross-validation\nscores = cross_val_score(model, X, y, cv=5)\n```\n\n### Memory Error with Large Datasets\n**Solution:** Use algorithms designed for large data\n```python\n# Use SGD for large datasets\nfrom sklearn.linear_model import SGDClassifier\nmodel = SGDClassifier()\n\n# Or MiniBatchKMeans for clustering\nfrom sklearn.cluster import MiniBatchKMeans\nmodel = MiniBatchKMeans(n_clusters=8, batch_size=100)\n```\n\n## Additional Resources\n\n- Official Documentation: https://scikit-learn.org/stable/\n- User Guide: https://scikit-learn.org/stable/user_guide.html\n- API Reference: https://scikit-learn.org/stable/api/index.html\n- Examples Gallery: https://scikit-learn.org/stable/auto_examples/index.html\n",
        "data/k-dense-ai/scikit-survival/SKILL.md": "---\nname: scikit-survival\ndescription: Comprehensive toolkit for survival analysis and time-to-event modeling in Python using scikit-survival. Use this skill when working with censored survival data, performing time-to-event analysis, fitting Cox models, Random Survival Forests, Gradient Boosting models, or Survival SVMs, evaluating survival predictions with concordance index or Brier score, handling competing risks, or implementing any survival analysis workflow with the scikit-survival library.\n---\n\n# scikit-survival: Survival Analysis in Python\n\n## Overview\n\nscikit-survival is a Python library for survival analysis built on top of scikit-learn. It provides specialized tools for time-to-event analysis, handling the unique challenge of censored data where some observations are only partially known.\n\nSurvival analysis aims to establish connections between covariates and the time of an event, accounting for censored records (particularly right-censored data from studies where participants don't experience events during observation periods).\n\n## When to Use This Skill\n\nUse this skill when:\n- Performing survival analysis or time-to-event modeling\n- Working with censored data (right-censored, left-censored, or interval-censored)\n- Fitting Cox proportional hazards models (standard or penalized)\n- Building ensemble survival models (Random Survival Forests, Gradient Boosting)\n- Training Survival Support Vector Machines\n- Evaluating survival model performance (concordance index, Brier score, time-dependent AUC)\n- Estimating Kaplan-Meier or Nelson-Aalen curves\n- Analyzing competing risks\n- Preprocessing survival data or handling missing values in survival datasets\n- Conducting any analysis using the scikit-survival library\n\n## Core Capabilities\n\n### 1. Model Types and Selection\n\nscikit-survival provides multiple model families, each suited for different scenarios:\n\n#### Cox Proportional Hazards Models\n**Use for**: Standard survival analysis with interpretable coefficients\n- `CoxPHSurvivalAnalysis`: Basic Cox model\n- `CoxnetSurvivalAnalysis`: Penalized Cox with elastic net for high-dimensional data\n- `IPCRidge`: Ridge regression for accelerated failure time models\n\n**See**: `references/cox-models.md` for detailed guidance on Cox models, regularization, and interpretation\n\n#### Ensemble Methods\n**Use for**: High predictive performance with complex non-linear relationships\n- `RandomSurvivalForest`: Robust, non-parametric ensemble method\n- `GradientBoostingSurvivalAnalysis`: Tree-based boosting for maximum performance\n- `ComponentwiseGradientBoostingSurvivalAnalysis`: Linear boosting with feature selection\n- `ExtraSurvivalTrees`: Extremely randomized trees for additional regularization\n\n**See**: `references/ensemble-models.md` for comprehensive guidance on ensemble methods, hyperparameter tuning, and when to use each model\n\n#### Survival Support Vector Machines\n**Use for**: Medium-sized datasets with margin-based learning\n- `FastSurvivalSVM`: Linear SVM optimized for speed\n- `FastKernelSurvivalSVM`: Kernel SVM for non-linear relationships\n- `HingeLossSurvivalSVM`: SVM with hinge loss\n- `ClinicalKernelTransform`: Specialized kernel for clinical + molecular data\n\n**See**: `references/svm-models.md` for detailed SVM guidance, kernel selection, and hyperparameter tuning\n\n#### Model Selection Decision Tree\n\n```\nStart\n High-dimensional data (p > n)?\n   Yes  CoxnetSurvivalAnalysis (elastic net)\n   No  Continue\n\n Need interpretable coefficients?\n   Yes  CoxPHSurvivalAnalysis or ComponentwiseGradientBoostingSurvivalAnalysis\n   No  Continue\n\n Complex non-linear relationships expected?\n   Yes\n     Large dataset (n > 1000)  GradientBoostingSurvivalAnalysis\n     Medium dataset  RandomSurvivalForest or FastKernelSurvivalSVM\n     Small dataset  RandomSurvivalForest\n   No  CoxPHSurvivalAnalysis or FastSurvivalSVM\n\n For maximum performance  Try multiple models and compare\n```\n\n### 2. Data Preparation and Preprocessing\n\nBefore modeling, properly prepare survival data:\n\n#### Creating Survival Outcomes\n```python\nfrom sksurv.util import Surv\n\n# From separate arrays\ny = Surv.from_arrays(event=event_array, time=time_array)\n\n# From DataFrame\ny = Surv.from_dataframe('event', 'time', df)\n```\n\n#### Essential Preprocessing Steps\n1. **Handle missing values**: Imputation strategies for features\n2. **Encode categorical variables**: One-hot encoding or label encoding\n3. **Standardize features**: Critical for SVMs and regularized Cox models\n4. **Validate data quality**: Check for negative times, sufficient events per feature\n5. **Train-test split**: Maintain similar censoring rates across splits\n\n**See**: `references/data-handling.md` for complete preprocessing workflows, data validation, and best practices\n\n### 3. Model Evaluation\n\nProper evaluation is critical for survival models. Use appropriate metrics that account for censoring:\n\n#### Concordance Index (C-index)\nPrimary metric for ranking/discrimination:\n- **Harrell's C-index**: Use for low censoring (<40%)\n- **Uno's C-index**: Use for moderate to high censoring (>40%) - more robust\n\n```python\nfrom sksurv.metrics import concordance_index_censored, concordance_index_ipcw\n\n# Harrell's C-index\nc_harrell = concordance_index_censored(y_test['event'], y_test['time'], risk_scores)[0]\n\n# Uno's C-index (recommended)\nc_uno = concordance_index_ipcw(y_train, y_test, risk_scores)[0]\n```\n\n#### Time-Dependent AUC\nEvaluate discrimination at specific time points:\n\n```python\nfrom sksurv.metrics import cumulative_dynamic_auc\n\ntimes = [365, 730, 1095]  # 1, 2, 3 years\nauc, mean_auc = cumulative_dynamic_auc(y_train, y_test, risk_scores, times)\n```\n\n#### Brier Score\nAssess both discrimination and calibration:\n\n```python\nfrom sksurv.metrics import integrated_brier_score\n\nibs = integrated_brier_score(y_train, y_test, survival_functions, times)\n```\n\n**See**: `references/evaluation-metrics.md` for comprehensive evaluation guidance, metric selection, and using scorers with cross-validation\n\n### 4. Competing Risks Analysis\n\nHandle situations with multiple mutually exclusive event types:\n\n```python\nfrom sksurv.nonparametric import cumulative_incidence_competing_risks\n\n# Estimate cumulative incidence for each event type\ntime_points, cif_event1, cif_event2 = cumulative_incidence_competing_risks(y)\n```\n\n**Use competing risks when**:\n- Multiple mutually exclusive event types exist (e.g., death from different causes)\n- Occurrence of one event prevents others\n- Need probability estimates for specific event types\n\n**See**: `references/competing-risks.md` for detailed competing risks methods, cause-specific hazard models, and interpretation\n\n### 5. Non-parametric Estimation\n\nEstimate survival functions without parametric assumptions:\n\n#### Kaplan-Meier Estimator\n```python\nfrom sksurv.nonparametric import kaplan_meier_estimator\n\ntime, survival_prob = kaplan_meier_estimator(y['event'], y['time'])\n```\n\n#### Nelson-Aalen Estimator\n```python\nfrom sksurv.nonparametric import nelson_aalen_estimator\n\ntime, cumulative_hazard = nelson_aalen_estimator(y['event'], y['time'])\n```\n\n## Typical Workflows\n\n### Workflow 1: Standard Survival Analysis\n\n```python\nfrom sksurv.datasets import load_breast_cancer\nfrom sksurv.linear_model import CoxPHSurvivalAnalysis\nfrom sksurv.metrics import concordance_index_ipcw\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# 1. Load and prepare data\nX, y = load_breast_cancer()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# 2. Preprocess\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# 3. Fit model\nestimator = CoxPHSurvivalAnalysis()\nestimator.fit(X_train_scaled, y_train)\n\n# 4. Predict\nrisk_scores = estimator.predict(X_test_scaled)\n\n# 5. Evaluate\nc_index = concordance_index_ipcw(y_train, y_test, risk_scores)[0]\nprint(f\"C-index: {c_index:.3f}\")\n```\n\n### Workflow 2: High-Dimensional Data with Feature Selection\n\n```python\nfrom sksurv.linear_model import CoxnetSurvivalAnalysis\nfrom sklearn.model_selection import GridSearchCV\nfrom sksurv.metrics import as_concordance_index_ipcw_scorer\n\n# 1. Use penalized Cox for feature selection\nestimator = CoxnetSurvivalAnalysis(l1_ratio=0.9)  # Lasso-like\n\n# 2. Tune regularization with cross-validation\nparam_grid = {'alpha_min_ratio': [0.01, 0.001]}\ncv = GridSearchCV(estimator, param_grid,\n                  scoring=as_concordance_index_ipcw_scorer(), cv=5)\ncv.fit(X, y)\n\n# 3. Identify selected features\nbest_model = cv.best_estimator_\nselected_features = np.where(best_model.coef_ != 0)[0]\n```\n\n### Workflow 3: Ensemble Method for Maximum Performance\n\n```python\nfrom sksurv.ensemble import GradientBoostingSurvivalAnalysis\nfrom sklearn.model_selection import GridSearchCV\n\n# 1. Define parameter grid\nparam_grid = {\n    'learning_rate': [0.01, 0.05, 0.1],\n    'n_estimators': [100, 200, 300],\n    'max_depth': [3, 5, 7]\n}\n\n# 2. Grid search\ngbs = GradientBoostingSurvivalAnalysis()\ncv = GridSearchCV(gbs, param_grid, cv=5,\n                  scoring=as_concordance_index_ipcw_scorer(), n_jobs=-1)\ncv.fit(X_train, y_train)\n\n# 3. Evaluate best model\nbest_model = cv.best_estimator_\nrisk_scores = best_model.predict(X_test)\nc_index = concordance_index_ipcw(y_train, y_test, risk_scores)[0]\n```\n\n### Workflow 4: Comprehensive Model Comparison\n\n```python\nfrom sksurv.linear_model import CoxPHSurvivalAnalysis\nfrom sksurv.ensemble import RandomSurvivalForest, GradientBoostingSurvivalAnalysis\nfrom sksurv.svm import FastSurvivalSVM\nfrom sksurv.metrics import concordance_index_ipcw, integrated_brier_score\n\n# Define models\nmodels = {\n    'Cox': CoxPHSurvivalAnalysis(),\n    'RSF': RandomSurvivalForest(n_estimators=100, random_state=42),\n    'GBS': GradientBoostingSurvivalAnalysis(random_state=42),\n    'SVM': FastSurvivalSVM(random_state=42)\n}\n\n# Evaluate each model\nresults = {}\nfor name, model in models.items():\n    model.fit(X_train_scaled, y_train)\n    risk_scores = model.predict(X_test_scaled)\n    c_index = concordance_index_ipcw(y_train, y_test, risk_scores)[0]\n    results[name] = c_index\n    print(f\"{name}: C-index = {c_index:.3f}\")\n\n# Select best model\nbest_model_name = max(results, key=results.get)\nprint(f\"\\nBest model: {best_model_name}\")\n```\n\n## Integration with scikit-learn\n\nscikit-survival fully integrates with scikit-learn's ecosystem:\n\n```python\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\n\n# Use pipelines\npipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('model', CoxPHSurvivalAnalysis())\n])\n\n# Use cross-validation\nscores = cross_val_score(pipeline, X, y, cv=5,\n                         scoring=as_concordance_index_ipcw_scorer())\n\n# Use grid search\nparam_grid = {'model__alpha': [0.1, 1.0, 10.0]}\ncv = GridSearchCV(pipeline, param_grid, cv=5)\ncv.fit(X, y)\n```\n\n## Best Practices\n\n1. **Always standardize features** for SVMs and regularized Cox models\n2. **Use Uno's C-index** instead of Harrell's when censoring > 40%\n3. **Report multiple evaluation metrics** (C-index, integrated Brier score, time-dependent AUC)\n4. **Check proportional hazards assumption** for Cox models\n5. **Use cross-validation** for hyperparameter tuning with appropriate scorers\n6. **Validate data quality** before modeling (check for negative times, sufficient events per feature)\n7. **Compare multiple model types** to find best performance\n8. **Use permutation importance** for Random Survival Forests (not built-in importance)\n9. **Consider competing risks** when multiple event types exist\n10. **Document censoring mechanism** and rates in analysis\n\n## Common Pitfalls to Avoid\n\n1. **Using Harrell's C-index with high censoring**  Use Uno's C-index\n2. **Not standardizing features for SVMs**  Always standardize\n3. **Forgetting to pass y_train to concordance_index_ipcw**  Required for IPCW calculation\n4. **Treating competing events as censored**  Use competing risks methods\n5. **Not checking for sufficient events per feature**  Rule of thumb: 10+ events per feature\n6. **Using built-in feature importance for RSF**  Use permutation importance\n7. **Ignoring proportional hazards assumption**  Validate or use alternative models\n8. **Not using appropriate scorers in cross-validation**  Use as_concordance_index_ipcw_scorer()\n\n## Reference Files\n\nThis skill includes detailed reference files for specific topics:\n\n- **`references/cox-models.md`**: Complete guide to Cox proportional hazards models, penalized Cox (CoxNet), IPCRidge, regularization strategies, and interpretation\n- **`references/ensemble-models.md`**: Random Survival Forests, Gradient Boosting, hyperparameter tuning, feature importance, and model selection\n- **`references/evaluation-metrics.md`**: Concordance index (Harrell's vs Uno's), time-dependent AUC, Brier score, comprehensive evaluation pipelines\n- **`references/data-handling.md`**: Data loading, preprocessing workflows, handling missing data, feature encoding, validation checks\n- **`references/svm-models.md`**: Survival Support Vector Machines, kernel selection, clinical kernel transform, hyperparameter tuning\n- **`references/competing-risks.md`**: Competing risks analysis, cumulative incidence functions, cause-specific hazard models\n\nLoad these reference files when detailed information is needed for specific tasks.\n\n## Additional Resources\n\n- **Official Documentation**: https://scikit-survival.readthedocs.io/\n- **GitHub Repository**: https://github.com/sebp/scikit-survival\n- **Built-in Datasets**: Use `sksurv.datasets` for practice datasets (GBSG2, WHAS500, veterans lung cancer, etc.)\n- **API Reference**: Complete list of classes and functions at https://scikit-survival.readthedocs.io/en/stable/api/index.html\n\n## Quick Reference: Key Imports\n\n```python\n# Models\nfrom sksurv.linear_model import CoxPHSurvivalAnalysis, CoxnetSurvivalAnalysis, IPCRidge\nfrom sksurv.ensemble import RandomSurvivalForest, GradientBoostingSurvivalAnalysis\nfrom sksurv.svm import FastSurvivalSVM, FastKernelSurvivalSVM\nfrom sksurv.tree import SurvivalTree\n\n# Evaluation metrics\nfrom sksurv.metrics import (\n    concordance_index_censored,\n    concordance_index_ipcw,\n    cumulative_dynamic_auc,\n    brier_score,\n    integrated_brier_score,\n    as_concordance_index_ipcw_scorer,\n    as_integrated_brier_score_scorer\n)\n\n# Non-parametric estimation\nfrom sksurv.nonparametric import (\n    kaplan_meier_estimator,\n    nelson_aalen_estimator,\n    cumulative_incidence_competing_risks\n)\n\n# Data handling\nfrom sksurv.util import Surv\nfrom sksurv.preprocessing import OneHotEncoder, encode_categorical\nfrom sksurv.datasets import load_gbsg2, load_breast_cancer, load_veterans_lung_cancer\n\n# Kernels\nfrom sksurv.kernels import ClinicalKernelTransform\n```\n",
        "data/k-dense-ai/scvi-tools/SKILL.md": "---\nname: scvi-tools\ndescription: This skill should be used when working with single-cell omics data analysis using scvi-tools, including scRNA-seq, scATAC-seq, CITE-seq, spatial transcriptomics, and other single-cell modalities. Use this skill for probabilistic modeling, batch correction, dimensionality reduction, differential expression, cell type annotation, multimodal integration, and spatial analysis tasks.\n---\n\n# scvi-tools\n\n## Overview\n\nscvi-tools is a comprehensive Python framework for probabilistic models in single-cell genomics. Built on PyTorch and PyTorch Lightning, it provides deep generative models using variational inference for analyzing diverse single-cell data modalities.\n\n## When to Use This Skill\n\nUse this skill when:\n- Analyzing single-cell RNA-seq data (dimensionality reduction, batch correction, integration)\n- Working with single-cell ATAC-seq or chromatin accessibility data\n- Integrating multimodal data (CITE-seq, multiome, paired/unpaired datasets)\n- Analyzing spatial transcriptomics data (deconvolution, spatial mapping)\n- Performing differential expression analysis on single-cell data\n- Conducting cell type annotation or transfer learning tasks\n- Working with specialized single-cell modalities (methylation, cytometry, RNA velocity)\n- Building custom probabilistic models for single-cell analysis\n\n## Core Capabilities\n\nscvi-tools provides models organized by data modality:\n\n### 1. Single-Cell RNA-seq Analysis\nCore models for expression analysis, batch correction, and integration. See `references/models-scrna-seq.md` for:\n- **scVI**: Unsupervised dimensionality reduction and batch correction\n- **scANVI**: Semi-supervised cell type annotation and integration\n- **AUTOZI**: Zero-inflation detection and modeling\n- **VeloVI**: RNA velocity analysis\n- **contrastiveVI**: Perturbation effect isolation\n\n### 2. Chromatin Accessibility (ATAC-seq)\nModels for analyzing single-cell chromatin data. See `references/models-atac-seq.md` for:\n- **PeakVI**: Peak-based ATAC-seq analysis and integration\n- **PoissonVI**: Quantitative fragment count modeling\n- **scBasset**: Deep learning approach with motif analysis\n\n### 3. Multimodal & Multi-omics Integration\nJoint analysis of multiple data types. See `references/models-multimodal.md` for:\n- **totalVI**: CITE-seq protein and RNA joint modeling\n- **MultiVI**: Paired and unpaired multi-omic integration\n- **MrVI**: Multi-resolution cross-sample analysis\n\n### 4. Spatial Transcriptomics\nSpatially-resolved transcriptomics analysis. See `references/models-spatial.md` for:\n- **DestVI**: Multi-resolution spatial deconvolution\n- **Stereoscope**: Cell type deconvolution\n- **Tangram**: Spatial mapping and integration\n- **scVIVA**: Cell-environment relationship analysis\n\n### 5. Specialized Modalities\nAdditional specialized analysis tools. See `references/models-specialized.md` for:\n- **MethylVI/MethylANVI**: Single-cell methylation analysis\n- **CytoVI**: Flow/mass cytometry batch correction\n- **Solo**: Doublet detection\n- **CellAssign**: Marker-based cell type annotation\n\n## Typical Workflow\n\nAll scvi-tools models follow a consistent API pattern:\n\n```python\n# 1. Load and preprocess data (AnnData format)\nimport scvi\nimport scanpy as sc\n\nadata = scvi.data.heart_cell_atlas_subsampled()\nsc.pp.filter_genes(adata, min_counts=3)\nsc.pp.highly_variable_genes(adata, n_top_genes=1200)\n\n# 2. Register data with model (specify layers, covariates)\nscvi.model.SCVI.setup_anndata(\n    adata,\n    layer=\"counts\",  # Use raw counts, not log-normalized\n    batch_key=\"batch\",\n    categorical_covariate_keys=[\"donor\"],\n    continuous_covariate_keys=[\"percent_mito\"]\n)\n\n# 3. Create and train model\nmodel = scvi.model.SCVI(adata)\nmodel.train()\n\n# 4. Extract latent representations and normalized values\nlatent = model.get_latent_representation()\nnormalized = model.get_normalized_expression(library_size=1e4)\n\n# 5. Store in AnnData for downstream analysis\nadata.obsm[\"X_scVI\"] = latent\nadata.layers[\"scvi_normalized\"] = normalized\n\n# 6. Downstream analysis with scanpy\nsc.pp.neighbors(adata, use_rep=\"X_scVI\")\nsc.tl.umap(adata)\nsc.tl.leiden(adata)\n```\n\n**Key Design Principles:**\n- **Raw counts required**: Models expect unnormalized count data for optimal performance\n- **Unified API**: Consistent interface across all models (setup  train  extract)\n- **AnnData-centric**: Seamless integration with the scanpy ecosystem\n- **GPU acceleration**: Automatic utilization of available GPUs\n- **Batch correction**: Handle technical variation through covariate registration\n\n## Common Analysis Tasks\n\n### Differential Expression\nProbabilistic DE analysis using the learned generative models:\n\n```python\nde_results = model.differential_expression(\n    groupby=\"cell_type\",\n    group1=\"TypeA\",\n    group2=\"TypeB\",\n    mode=\"change\",  # Use composite hypothesis testing\n    delta=0.25      # Minimum effect size threshold\n)\n```\n\nSee `references/differential-expression.md` for detailed methodology and interpretation.\n\n### Model Persistence\nSave and load trained models:\n\n```python\n# Save model\nmodel.save(\"./model_directory\", overwrite=True)\n\n# Load model\nmodel = scvi.model.SCVI.load(\"./model_directory\", adata=adata)\n```\n\n### Batch Correction and Integration\nIntegrate datasets across batches or studies:\n\n```python\n# Register batch information\nscvi.model.SCVI.setup_anndata(adata, batch_key=\"study\")\n\n# Model automatically learns batch-corrected representations\nmodel = scvi.model.SCVI(adata)\nmodel.train()\nlatent = model.get_latent_representation()  # Batch-corrected\n```\n\n## Theoretical Foundations\n\nscvi-tools is built on:\n- **Variational inference**: Approximate posterior distributions for scalable Bayesian inference\n- **Deep generative models**: VAE architectures that learn complex data distributions\n- **Amortized inference**: Shared neural networks for efficient learning across cells\n- **Probabilistic modeling**: Principled uncertainty quantification and statistical testing\n\nSee `references/theoretical-foundations.md` for detailed background on the mathematical framework.\n\n## Additional Resources\n\n- **Workflows**: `references/workflows.md` contains common workflows, best practices, hyperparameter tuning, and GPU optimization\n- **Model References**: Detailed documentation for each model category in the `references/` directory\n- **Official Documentation**: https://docs.scvi-tools.org/en/stable/\n- **Tutorials**: https://docs.scvi-tools.org/en/stable/tutorials/index.html\n- **API Reference**: https://docs.scvi-tools.org/en/stable/api/index.html\n\n## Installation\n\n```bash\nuv pip install scvi-tools\n# For GPU support\nuv pip install scvi-tools[cuda]\n```\n\n## Best Practices\n\n1. **Use raw counts**: Always provide unnormalized count data to models\n2. **Filter genes**: Remove low-count genes before analysis (e.g., `min_counts=3`)\n3. **Register covariates**: Include known technical factors (batch, donor, etc.) in `setup_anndata`\n4. **Feature selection**: Use highly variable genes for improved performance\n5. **Model saving**: Always save trained models to avoid retraining\n6. **GPU usage**: Enable GPU acceleration for large datasets (`accelerator=\"gpu\"`)\n7. **Scanpy integration**: Store outputs in AnnData objects for downstream analysis\n",
        "data/k-dense-ai/seaborn/SKILL.md": "---\nname: seaborn\ndescription: \"Statistical visualization. Scatter, box, violin, heatmaps, pair plots, regression, correlation matrices, KDE, faceted plots, for exploratory analysis and publication figures.\"\n---\n\n# Seaborn Statistical Visualization\n\n## Overview\n\nSeaborn is a Python visualization library for creating publication-quality statistical graphics. Use this skill for dataset-oriented plotting, multivariate analysis, automatic statistical estimation, and complex multi-panel figures with minimal code.\n\n## Design Philosophy\n\nSeaborn follows these core principles:\n\n1. **Dataset-oriented**: Work directly with DataFrames and named variables rather than abstract coordinates\n2. **Semantic mapping**: Automatically translate data values into visual properties (colors, sizes, styles)\n3. **Statistical awareness**: Built-in aggregation, error estimation, and confidence intervals\n4. **Aesthetic defaults**: Publication-ready themes and color palettes out of the box\n5. **Matplotlib integration**: Full compatibility with matplotlib customization when needed\n\n## Quick Start\n\n```python\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Load example dataset\ndf = sns.load_dataset('tips')\n\n# Create a simple visualization\nsns.scatterplot(data=df, x='total_bill', y='tip', hue='day')\nplt.show()\n```\n\n## Core Plotting Interfaces\n\n### Function Interface (Traditional)\n\nThe function interface provides specialized plotting functions organized by visualization type. Each category has **axes-level** functions (plot to single axes) and **figure-level** functions (manage entire figure with faceting).\n\n**When to use:**\n- Quick exploratory analysis\n- Single-purpose visualizations\n- When you need a specific plot type\n\n### Objects Interface (Modern)\n\nThe `seaborn.objects` interface provides a declarative, composable API similar to ggplot2. Build visualizations by chaining methods to specify data mappings, marks, transformations, and scales.\n\n**When to use:**\n- Complex layered visualizations\n- When you need fine-grained control over transformations\n- Building custom plot types\n- Programmatic plot generation\n\n```python\nfrom seaborn import objects as so\n\n# Declarative syntax\n(\n    so.Plot(data=df, x='total_bill', y='tip')\n    .add(so.Dot(), color='day')\n    .add(so.Line(), so.PolyFit())\n)\n```\n\n## Plotting Functions by Category\n\n### Relational Plots (Relationships Between Variables)\n\n**Use for:** Exploring how two or more variables relate to each other\n\n- `scatterplot()` - Display individual observations as points\n- `lineplot()` - Show trends and changes (automatically aggregates and computes CI)\n- `relplot()` - Figure-level interface with automatic faceting\n\n**Key parameters:**\n- `x`, `y` - Primary variables\n- `hue` - Color encoding for additional categorical/continuous variable\n- `size` - Point/line size encoding\n- `style` - Marker/line style encoding\n- `col`, `row` - Facet into multiple subplots (figure-level only)\n\n```python\n# Scatter with multiple semantic mappings\nsns.scatterplot(data=df, x='total_bill', y='tip',\n                hue='time', size='size', style='sex')\n\n# Line plot with confidence intervals\nsns.lineplot(data=timeseries, x='date', y='value', hue='category')\n\n# Faceted relational plot\nsns.relplot(data=df, x='total_bill', y='tip',\n            col='time', row='sex', hue='smoker', kind='scatter')\n```\n\n### Distribution Plots (Single and Bivariate Distributions)\n\n**Use for:** Understanding data spread, shape, and probability density\n\n- `histplot()` - Bar-based frequency distributions with flexible binning\n- `kdeplot()` - Smooth density estimates using Gaussian kernels\n- `ecdfplot()` - Empirical cumulative distribution (no parameters to tune)\n- `rugplot()` - Individual observation tick marks\n- `displot()` - Figure-level interface for univariate and bivariate distributions\n- `jointplot()` - Bivariate plot with marginal distributions\n- `pairplot()` - Matrix of pairwise relationships across dataset\n\n**Key parameters:**\n- `x`, `y` - Variables (y optional for univariate)\n- `hue` - Separate distributions by category\n- `stat` - Normalization: \"count\", \"frequency\", \"probability\", \"density\"\n- `bins` / `binwidth` - Histogram binning control\n- `bw_adjust` - KDE bandwidth multiplier (higher = smoother)\n- `fill` - Fill area under curve\n- `multiple` - How to handle hue: \"layer\", \"stack\", \"dodge\", \"fill\"\n\n```python\n# Histogram with density normalization\nsns.histplot(data=df, x='total_bill', hue='time',\n             stat='density', multiple='stack')\n\n# Bivariate KDE with contours\nsns.kdeplot(data=df, x='total_bill', y='tip',\n            fill=True, levels=5, thresh=0.1)\n\n# Joint plot with marginals\nsns.jointplot(data=df, x='total_bill', y='tip',\n              kind='scatter', hue='time')\n\n# Pairwise relationships\nsns.pairplot(data=df, hue='species', corner=True)\n```\n\n### Categorical Plots (Comparisons Across Categories)\n\n**Use for:** Comparing distributions or statistics across discrete categories\n\n**Categorical scatterplots:**\n- `stripplot()` - Points with jitter to show all observations\n- `swarmplot()` - Non-overlapping points (beeswarm algorithm)\n\n**Distribution comparisons:**\n- `boxplot()` - Quartiles and outliers\n- `violinplot()` - KDE + quartile information\n- `boxenplot()` - Enhanced boxplot for larger datasets\n\n**Statistical estimates:**\n- `barplot()` - Mean/aggregate with confidence intervals\n- `pointplot()` - Point estimates with connecting lines\n- `countplot()` - Count of observations per category\n\n**Figure-level:**\n- `catplot()` - Faceted categorical plots (set `kind` parameter)\n\n**Key parameters:**\n- `x`, `y` - Variables (one typically categorical)\n- `hue` - Additional categorical grouping\n- `order`, `hue_order` - Control category ordering\n- `dodge` - Separate hue levels side-by-side\n- `orient` - \"v\" (vertical) or \"h\" (horizontal)\n- `kind` - Plot type for catplot: \"strip\", \"swarm\", \"box\", \"violin\", \"bar\", \"point\"\n\n```python\n# Swarm plot showing all points\nsns.swarmplot(data=df, x='day', y='total_bill', hue='sex')\n\n# Violin plot with split for comparison\nsns.violinplot(data=df, x='day', y='total_bill',\n               hue='sex', split=True)\n\n# Bar plot with error bars\nsns.barplot(data=df, x='day', y='total_bill',\n            hue='sex', estimator='mean', errorbar='ci')\n\n# Faceted categorical plot\nsns.catplot(data=df, x='day', y='total_bill',\n            col='time', kind='box')\n```\n\n### Regression Plots (Linear Relationships)\n\n**Use for:** Visualizing linear regressions and residuals\n\n- `regplot()` - Axes-level regression plot with scatter + fit line\n- `lmplot()` - Figure-level with faceting support\n- `residplot()` - Residual plot for assessing model fit\n\n**Key parameters:**\n- `x`, `y` - Variables to regress\n- `order` - Polynomial regression order\n- `logistic` - Fit logistic regression\n- `robust` - Use robust regression (less sensitive to outliers)\n- `ci` - Confidence interval width (default 95)\n- `scatter_kws`, `line_kws` - Customize scatter and line properties\n\n```python\n# Simple linear regression\nsns.regplot(data=df, x='total_bill', y='tip')\n\n# Polynomial regression with faceting\nsns.lmplot(data=df, x='total_bill', y='tip',\n           col='time', order=2, ci=95)\n\n# Check residuals\nsns.residplot(data=df, x='total_bill', y='tip')\n```\n\n### Matrix Plots (Rectangular Data)\n\n**Use for:** Visualizing matrices, correlations, and grid-structured data\n\n- `heatmap()` - Color-encoded matrix with annotations\n- `clustermap()` - Hierarchically-clustered heatmap\n\n**Key parameters:**\n- `data` - 2D rectangular dataset (DataFrame or array)\n- `annot` - Display values in cells\n- `fmt` - Format string for annotations (e.g., \".2f\")\n- `cmap` - Colormap name\n- `center` - Value at colormap center (for diverging colormaps)\n- `vmin`, `vmax` - Color scale limits\n- `square` - Force square cells\n- `linewidths` - Gap between cells\n\n```python\n# Correlation heatmap\ncorr = df.corr()\nsns.heatmap(corr, annot=True, fmt='.2f',\n            cmap='coolwarm', center=0, square=True)\n\n# Clustered heatmap\nsns.clustermap(data, cmap='viridis',\n               standard_scale=1, figsize=(10, 10))\n```\n\n## Multi-Plot Grids\n\nSeaborn provides grid objects for creating complex multi-panel figures:\n\n### FacetGrid\n\nCreate subplots based on categorical variables. Most useful when called through figure-level functions (`relplot`, `displot`, `catplot`), but can be used directly for custom plots.\n\n```python\ng = sns.FacetGrid(df, col='time', row='sex', hue='smoker')\ng.map(sns.scatterplot, 'total_bill', 'tip')\ng.add_legend()\n```\n\n### PairGrid\n\nShow pairwise relationships between all variables in a dataset.\n\n```python\ng = sns.PairGrid(df, hue='species')\ng.map_upper(sns.scatterplot)\ng.map_lower(sns.kdeplot)\ng.map_diag(sns.histplot)\ng.add_legend()\n```\n\n### JointGrid\n\nCombine bivariate plot with marginal distributions.\n\n```python\ng = sns.JointGrid(data=df, x='total_bill', y='tip')\ng.plot_joint(sns.scatterplot)\ng.plot_marginals(sns.histplot)\n```\n\n## Figure-Level vs Axes-Level Functions\n\nUnderstanding this distinction is crucial for effective seaborn usage:\n\n### Axes-Level Functions\n- Plot to a single matplotlib `Axes` object\n- Integrate easily into complex matplotlib figures\n- Accept `ax=` parameter for precise placement\n- Return `Axes` object\n- Examples: `scatterplot`, `histplot`, `boxplot`, `regplot`, `heatmap`\n\n**When to use:**\n- Building custom multi-plot layouts\n- Combining different plot types\n- Need matplotlib-level control\n- Integrating with existing matplotlib code\n\n```python\nfig, axes = plt.subplots(2, 2, figsize=(10, 10))\nsns.scatterplot(data=df, x='x', y='y', ax=axes[0, 0])\nsns.histplot(data=df, x='x', ax=axes[0, 1])\nsns.boxplot(data=df, x='cat', y='y', ax=axes[1, 0])\nsns.kdeplot(data=df, x='x', y='y', ax=axes[1, 1])\n```\n\n### Figure-Level Functions\n- Manage entire figure including all subplots\n- Built-in faceting via `col` and `row` parameters\n- Return `FacetGrid`, `JointGrid`, or `PairGrid` objects\n- Use `height` and `aspect` for sizing (per subplot)\n- Cannot be placed in existing figure\n- Examples: `relplot`, `displot`, `catplot`, `lmplot`, `jointplot`, `pairplot`\n\n**When to use:**\n- Faceted visualizations (small multiples)\n- Quick exploratory analysis\n- Consistent multi-panel layouts\n- Don't need to combine with other plot types\n\n```python\n# Automatic faceting\nsns.relplot(data=df, x='x', y='y', col='category', row='group',\n            hue='type', height=3, aspect=1.2)\n```\n\n## Data Structure Requirements\n\n### Long-Form Data (Preferred)\n\nEach variable is a column, each observation is a row. This \"tidy\" format provides maximum flexibility:\n\n```python\n# Long-form structure\n   subject  condition  measurement\n0        1    control         10.5\n1        1  treatment         12.3\n2        2    control          9.8\n3        2  treatment         13.1\n```\n\n**Advantages:**\n- Works with all seaborn functions\n- Easy to remap variables to visual properties\n- Supports arbitrary complexity\n- Natural for DataFrame operations\n\n### Wide-Form Data\n\nVariables are spread across columns. Useful for simple rectangular data:\n\n```python\n# Wide-form structure\n   control  treatment\n0     10.5       12.3\n1      9.8       13.1\n```\n\n**Use cases:**\n- Simple time series\n- Correlation matrices\n- Heatmaps\n- Quick plots of array data\n\n**Converting wide to long:**\n```python\ndf_long = df.melt(var_name='condition', value_name='measurement')\n```\n\n## Color Palettes\n\nSeaborn provides carefully designed color palettes for different data types:\n\n### Qualitative Palettes (Categorical Data)\n\nDistinguish categories through hue variation:\n- `\"deep\"` - Default, vivid colors\n- `\"muted\"` - Softer, less saturated\n- `\"pastel\"` - Light, desaturated\n- `\"bright\"` - Highly saturated\n- `\"dark\"` - Dark values\n- `\"colorblind\"` - Safe for color vision deficiency\n\n```python\nsns.set_palette(\"colorblind\")\nsns.color_palette(\"Set2\")\n```\n\n### Sequential Palettes (Ordered Data)\n\nShow progression from low to high values:\n- `\"rocket\"`, `\"mako\"` - Wide luminance range (good for heatmaps)\n- `\"flare\"`, `\"crest\"` - Restricted luminance (good for points/lines)\n- `\"viridis\"`, `\"magma\"`, `\"plasma\"` - Matplotlib perceptually uniform\n\n```python\nsns.heatmap(data, cmap='rocket')\nsns.kdeplot(data=df, x='x', y='y', cmap='mako', fill=True)\n```\n\n### Diverging Palettes (Centered Data)\n\nEmphasize deviations from a midpoint:\n- `\"vlag\"` - Blue to red\n- `\"icefire\"` - Blue to orange\n- `\"coolwarm\"` - Cool to warm\n- `\"Spectral\"` - Rainbow diverging\n\n```python\nsns.heatmap(correlation_matrix, cmap='vlag', center=0)\n```\n\n### Custom Palettes\n\n```python\n# Create custom palette\ncustom = sns.color_palette(\"husl\", 8)\n\n# Light to dark gradient\npalette = sns.light_palette(\"seagreen\", as_cmap=True)\n\n# Diverging palette from hues\npalette = sns.diverging_palette(250, 10, as_cmap=True)\n```\n\n## Theming and Aesthetics\n\n### Set Theme\n\n`set_theme()` controls overall appearance:\n\n```python\n# Set complete theme\nsns.set_theme(style='whitegrid', palette='pastel', font='sans-serif')\n\n# Reset to defaults\nsns.set_theme()\n```\n\n### Styles\n\nControl background and grid appearance:\n- `\"darkgrid\"` - Gray background with white grid (default)\n- `\"whitegrid\"` - White background with gray grid\n- `\"dark\"` - Gray background, no grid\n- `\"white\"` - White background, no grid\n- `\"ticks\"` - White background with axis ticks\n\n```python\nsns.set_style(\"whitegrid\")\n\n# Remove spines\nsns.despine(left=False, bottom=False, offset=10, trim=True)\n\n# Temporary style\nwith sns.axes_style(\"white\"):\n    sns.scatterplot(data=df, x='x', y='y')\n```\n\n### Contexts\n\nScale elements for different use cases:\n- `\"paper\"` - Smallest (default)\n- `\"notebook\"` - Slightly larger\n- `\"talk\"` - Presentation slides\n- `\"poster\"` - Large format\n\n```python\nsns.set_context(\"talk\", font_scale=1.2)\n\n# Temporary context\nwith sns.plotting_context(\"poster\"):\n    sns.barplot(data=df, x='category', y='value')\n```\n\n## Best Practices\n\n### 1. Data Preparation\n\nAlways use well-structured DataFrames with meaningful column names:\n\n```python\n# Good: Named columns in DataFrame\ndf = pd.DataFrame({'bill': bills, 'tip': tips, 'day': days})\nsns.scatterplot(data=df, x='bill', y='tip', hue='day')\n\n# Avoid: Unnamed arrays\nsns.scatterplot(x=x_array, y=y_array)  # Loses axis labels\n```\n\n### 2. Choose the Right Plot Type\n\n**Continuous x, continuous y:** `scatterplot`, `lineplot`, `kdeplot`, `regplot`\n**Continuous x, categorical y:** `violinplot`, `boxplot`, `stripplot`, `swarmplot`\n**One continuous variable:** `histplot`, `kdeplot`, `ecdfplot`\n**Correlations/matrices:** `heatmap`, `clustermap`\n**Pairwise relationships:** `pairplot`, `jointplot`\n\n### 3. Use Figure-Level Functions for Faceting\n\n```python\n# Instead of manual subplot creation\nsns.relplot(data=df, x='x', y='y', col='category', col_wrap=3)\n\n# Not: Creating subplots manually for simple faceting\n```\n\n### 4. Leverage Semantic Mappings\n\nUse `hue`, `size`, and `style` to encode additional dimensions:\n\n```python\nsns.scatterplot(data=df, x='x', y='y',\n                hue='category',      # Color by category\n                size='importance',    # Size by continuous variable\n                style='type')         # Marker style by type\n```\n\n### 5. Control Statistical Estimation\n\nMany functions compute statistics automatically. Understand and customize:\n\n```python\n# Lineplot computes mean and 95% CI by default\nsns.lineplot(data=df, x='time', y='value',\n             errorbar='sd')  # Use standard deviation instead\n\n# Barplot computes mean by default\nsns.barplot(data=df, x='category', y='value',\n            estimator='median',  # Use median instead\n            errorbar=('ci', 95))  # Bootstrapped CI\n```\n\n### 6. Combine with Matplotlib\n\nSeaborn integrates seamlessly with matplotlib for fine-tuning:\n\n```python\nax = sns.scatterplot(data=df, x='x', y='y')\nax.set(xlabel='Custom X Label', ylabel='Custom Y Label',\n       title='Custom Title')\nax.axhline(y=0, color='r', linestyle='--')\nplt.tight_layout()\n```\n\n### 7. Save High-Quality Figures\n\n```python\nfig = sns.relplot(data=df, x='x', y='y', col='group')\nfig.savefig('figure.png', dpi=300, bbox_inches='tight')\nfig.savefig('figure.pdf')  # Vector format for publications\n```\n\n## Common Patterns\n\n### Exploratory Data Analysis\n\n```python\n# Quick overview of all relationships\nsns.pairplot(data=df, hue='target', corner=True)\n\n# Distribution exploration\nsns.displot(data=df, x='variable', hue='group',\n            kind='kde', fill=True, col='category')\n\n# Correlation analysis\ncorr = df.corr()\nsns.heatmap(corr, annot=True, cmap='coolwarm', center=0)\n```\n\n### Publication-Quality Figures\n\n```python\nsns.set_theme(style='ticks', context='paper', font_scale=1.1)\n\ng = sns.catplot(data=df, x='treatment', y='response',\n                col='cell_line', kind='box', height=3, aspect=1.2)\ng.set_axis_labels('Treatment Condition', 'Response (M)')\ng.set_titles('{col_name}')\nsns.despine(trim=True)\n\ng.savefig('figure.pdf', dpi=300, bbox_inches='tight')\n```\n\n### Complex Multi-Panel Figures\n\n```python\n# Using matplotlib subplots with seaborn\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\nsns.scatterplot(data=df, x='x1', y='y', hue='group', ax=axes[0, 0])\nsns.histplot(data=df, x='x1', hue='group', ax=axes[0, 1])\nsns.violinplot(data=df, x='group', y='y', ax=axes[1, 0])\nsns.heatmap(df.pivot_table(values='y', index='x1', columns='x2'),\n            ax=axes[1, 1], cmap='viridis')\n\nplt.tight_layout()\n```\n\n### Time Series with Confidence Bands\n\n```python\n# Lineplot automatically aggregates and shows CI\nsns.lineplot(data=timeseries, x='date', y='measurement',\n             hue='sensor', style='location', errorbar='sd')\n\n# For more control\ng = sns.relplot(data=timeseries, x='date', y='measurement',\n                col='location', hue='sensor', kind='line',\n                height=4, aspect=1.5, errorbar=('ci', 95))\ng.set_axis_labels('Date', 'Measurement (units)')\n```\n\n## Troubleshooting\n\n### Issue: Legend Outside Plot Area\n\nFigure-level functions place legends outside by default. To move inside:\n\n```python\ng = sns.relplot(data=df, x='x', y='y', hue='category')\ng._legend.set_bbox_to_anchor((0.9, 0.5))  # Adjust position\n```\n\n### Issue: Overlapping Labels\n\n```python\nplt.xticks(rotation=45, ha='right')\nplt.tight_layout()\n```\n\n### Issue: Figure Too Small\n\nFor figure-level functions:\n```python\nsns.relplot(data=df, x='x', y='y', height=6, aspect=1.5)\n```\n\nFor axes-level functions:\n```python\nfig, ax = plt.subplots(figsize=(10, 6))\nsns.scatterplot(data=df, x='x', y='y', ax=ax)\n```\n\n### Issue: Colors Not Distinct Enough\n\n```python\n# Use a different palette\nsns.set_palette(\"bright\")\n\n# Or specify number of colors\npalette = sns.color_palette(\"husl\", n_colors=len(df['category'].unique()))\nsns.scatterplot(data=df, x='x', y='y', hue='category', palette=palette)\n```\n\n### Issue: KDE Too Smooth or Jagged\n\n```python\n# Adjust bandwidth\nsns.kdeplot(data=df, x='x', bw_adjust=0.5)  # Less smooth\nsns.kdeplot(data=df, x='x', bw_adjust=2)    # More smooth\n```\n\n## Resources\n\nThis skill includes reference materials for deeper exploration:\n\n### references/\n\n- `function_reference.md` - Comprehensive listing of all seaborn functions with parameters and examples\n- `objects_interface.md` - Detailed guide to the modern seaborn.objects API\n- `examples.md` - Common use cases and code patterns for different analysis scenarios\n\nLoad reference files as needed for detailed function signatures, advanced parameters, or specific examples.\n",
        "data/k-dense-ai/shap/SKILL.md": "---\nname: shap\ndescription: Model interpretability and explainability using SHAP (SHapley Additive exPlanations). Use this skill when explaining machine learning model predictions, computing feature importance, generating SHAP plots (waterfall, beeswarm, bar, scatter, force, heatmap), debugging models, analyzing model bias or fairness, comparing models, or implementing explainable AI. Works with tree-based models (XGBoost, LightGBM, Random Forest), deep learning (TensorFlow, PyTorch), linear models, and any black-box model.\n---\n\n# SHAP (SHapley Additive exPlanations)\n\n## Overview\n\nSHAP is a unified approach to explain machine learning model outputs using Shapley values from cooperative game theory. This skill provides comprehensive guidance for:\n\n- Computing SHAP values for any model type\n- Creating visualizations to understand feature importance\n- Debugging and validating model behavior\n- Analyzing fairness and bias\n- Implementing explainable AI in production\n\nSHAP works with all model types: tree-based models (XGBoost, LightGBM, CatBoost, Random Forest), deep learning models (TensorFlow, PyTorch, Keras), linear models, and black-box models.\n\n## When to Use This Skill\n\n**Trigger this skill when users ask about**:\n- \"Explain which features are most important in my model\"\n- \"Generate SHAP plots\" (waterfall, beeswarm, bar, scatter, force, heatmap, etc.)\n- \"Why did my model make this prediction?\"\n- \"Calculate SHAP values for my model\"\n- \"Visualize feature importance using SHAP\"\n- \"Debug my model's behavior\" or \"validate my model\"\n- \"Check my model for bias\" or \"analyze fairness\"\n- \"Compare feature importance across models\"\n- \"Implement explainable AI\" or \"add explanations to my model\"\n- \"Understand feature interactions\"\n- \"Create model interpretation dashboard\"\n\n## Quick Start Guide\n\n### Step 1: Select the Right Explainer\n\n**Decision Tree**:\n\n1. **Tree-based model?** (XGBoost, LightGBM, CatBoost, Random Forest, Gradient Boosting)\n   - Use `shap.TreeExplainer` (fast, exact)\n\n2. **Deep neural network?** (TensorFlow, PyTorch, Keras, CNNs, RNNs, Transformers)\n   - Use `shap.DeepExplainer` or `shap.GradientExplainer`\n\n3. **Linear model?** (Linear/Logistic Regression, GLMs)\n   - Use `shap.LinearExplainer` (extremely fast)\n\n4. **Any other model?** (SVMs, custom functions, black-box models)\n   - Use `shap.KernelExplainer` (model-agnostic but slower)\n\n5. **Unsure?**\n   - Use `shap.Explainer` (automatically selects best algorithm)\n\n**See `references/explainers.md` for detailed information on all explainer types.**\n\n### Step 2: Compute SHAP Values\n\n```python\nimport shap\n\n# Example with tree-based model (XGBoost)\nimport xgboost as xgb\n\n# Train model\nmodel = xgb.XGBClassifier().fit(X_train, y_train)\n\n# Create explainer\nexplainer = shap.TreeExplainer(model)\n\n# Compute SHAP values\nshap_values = explainer(X_test)\n\n# The shap_values object contains:\n# - values: SHAP values (feature attributions)\n# - base_values: Expected model output (baseline)\n# - data: Original feature values\n```\n\n### Step 3: Visualize Results\n\n**For Global Understanding** (entire dataset):\n```python\n# Beeswarm plot - shows feature importance with value distributions\nshap.plots.beeswarm(shap_values, max_display=15)\n\n# Bar plot - clean summary of feature importance\nshap.plots.bar(shap_values)\n```\n\n**For Individual Predictions**:\n```python\n# Waterfall plot - detailed breakdown of single prediction\nshap.plots.waterfall(shap_values[0])\n\n# Force plot - additive force visualization\nshap.plots.force(shap_values[0])\n```\n\n**For Feature Relationships**:\n```python\n# Scatter plot - feature-prediction relationship\nshap.plots.scatter(shap_values[:, \"Feature_Name\"])\n\n# Colored by another feature to show interactions\nshap.plots.scatter(shap_values[:, \"Age\"], color=shap_values[:, \"Education\"])\n```\n\n**See `references/plots.md` for comprehensive guide on all plot types.**\n\n## Core Workflows\n\nThis skill supports several common workflows. Choose the workflow that matches the current task.\n\n### Workflow 1: Basic Model Explanation\n\n**Goal**: Understand what drives model predictions\n\n**Steps**:\n1. Train model and create appropriate explainer\n2. Compute SHAP values for test set\n3. Generate global importance plots (beeswarm or bar)\n4. Examine top feature relationships (scatter plots)\n5. Explain specific predictions (waterfall plots)\n\n**Example**:\n```python\n# Step 1-2: Setup\nexplainer = shap.TreeExplainer(model)\nshap_values = explainer(X_test)\n\n# Step 3: Global importance\nshap.plots.beeswarm(shap_values)\n\n# Step 4: Feature relationships\nshap.plots.scatter(shap_values[:, \"Most_Important_Feature\"])\n\n# Step 5: Individual explanation\nshap.plots.waterfall(shap_values[0])\n```\n\n### Workflow 2: Model Debugging\n\n**Goal**: Identify and fix model issues\n\n**Steps**:\n1. Compute SHAP values\n2. Identify prediction errors\n3. Explain misclassified samples\n4. Check for unexpected feature importance (data leakage)\n5. Validate feature relationships make sense\n6. Check feature interactions\n\n**See `references/workflows.md` for detailed debugging workflow.**\n\n### Workflow 3: Feature Engineering\n\n**Goal**: Use SHAP insights to improve features\n\n**Steps**:\n1. Compute SHAP values for baseline model\n2. Identify nonlinear relationships (candidates for transformation)\n3. Identify feature interactions (candidates for interaction terms)\n4. Engineer new features\n5. Retrain and compare SHAP values\n6. Validate improvements\n\n**See `references/workflows.md` for detailed feature engineering workflow.**\n\n### Workflow 4: Model Comparison\n\n**Goal**: Compare multiple models to select best interpretable option\n\n**Steps**:\n1. Train multiple models\n2. Compute SHAP values for each\n3. Compare global feature importance\n4. Check consistency of feature rankings\n5. Analyze specific predictions across models\n6. Select based on accuracy, interpretability, and consistency\n\n**See `references/workflows.md` for detailed model comparison workflow.**\n\n### Workflow 5: Fairness and Bias Analysis\n\n**Goal**: Detect and analyze model bias across demographic groups\n\n**Steps**:\n1. Identify protected attributes (gender, race, age, etc.)\n2. Compute SHAP values\n3. Compare feature importance across groups\n4. Check protected attribute SHAP importance\n5. Identify proxy features\n6. Implement mitigation strategies if bias found\n\n**See `references/workflows.md` for detailed fairness analysis workflow.**\n\n### Workflow 6: Production Deployment\n\n**Goal**: Integrate SHAP explanations into production systems\n\n**Steps**:\n1. Train and save model\n2. Create and save explainer\n3. Build explanation service\n4. Create API endpoints for predictions with explanations\n5. Implement caching and optimization\n6. Monitor explanation quality\n\n**See `references/workflows.md` for detailed production deployment workflow.**\n\n## Key Concepts\n\n### SHAP Values\n\n**Definition**: SHAP values quantify each feature's contribution to a prediction, measured as the deviation from the expected model output (baseline).\n\n**Properties**:\n- **Additivity**: SHAP values sum to difference between prediction and baseline\n- **Fairness**: Based on Shapley values from game theory\n- **Consistency**: If a feature becomes more important, its SHAP value increases\n\n**Interpretation**:\n- Positive SHAP value  Feature pushes prediction higher\n- Negative SHAP value  Feature pushes prediction lower\n- Magnitude  Strength of feature's impact\n- Sum of SHAP values  Total prediction change from baseline\n\n**Example**:\n```\nBaseline (expected value): 0.30\nFeature contributions (SHAP values):\n  Age: +0.15\n  Income: +0.10\n  Education: -0.05\nFinal prediction: 0.30 + 0.15 + 0.10 - 0.05 = 0.50\n```\n\n### Background Data / Baseline\n\n**Purpose**: Represents \"typical\" input to establish baseline expectations\n\n**Selection**:\n- Random sample from training data (50-1000 samples)\n- Or use kmeans to select representative samples\n- For DeepExplainer/KernelExplainer: 100-1000 samples balances accuracy and speed\n\n**Impact**: Baseline affects SHAP value magnitudes but not relative importance\n\n### Model Output Types\n\n**Critical Consideration**: Understand what your model outputs\n\n- **Raw output**: For regression or tree margins\n- **Probability**: For classification probability\n- **Log-odds**: For logistic regression (before sigmoid)\n\n**Example**: XGBoost classifiers explain margin output (log-odds) by default. To explain probabilities, use `model_output=\"probability\"` in TreeExplainer.\n\n## Common Patterns\n\n### Pattern 1: Complete Model Analysis\n\n```python\n# 1. Setup\nexplainer = shap.TreeExplainer(model)\nshap_values = explainer(X_test)\n\n# 2. Global importance\nshap.plots.beeswarm(shap_values)\nshap.plots.bar(shap_values)\n\n# 3. Top feature relationships\ntop_features = X_test.columns[np.abs(shap_values.values).mean(0).argsort()[-5:]]\nfor feature in top_features:\n    shap.plots.scatter(shap_values[:, feature])\n\n# 4. Example predictions\nfor i in range(5):\n    shap.plots.waterfall(shap_values[i])\n```\n\n### Pattern 2: Cohort Comparison\n\n```python\n# Define cohorts\ncohort1_mask = X_test['Group'] == 'A'\ncohort2_mask = X_test['Group'] == 'B'\n\n# Compare feature importance\nshap.plots.bar({\n    \"Group A\": shap_values[cohort1_mask],\n    \"Group B\": shap_values[cohort2_mask]\n})\n```\n\n### Pattern 3: Debugging Errors\n\n```python\n# Find errors\nerrors = model.predict(X_test) != y_test\nerror_indices = np.where(errors)[0]\n\n# Explain errors\nfor idx in error_indices[:5]:\n    print(f\"Sample {idx}:\")\n    shap.plots.waterfall(shap_values[idx])\n\n    # Investigate key features\n    shap.plots.scatter(shap_values[:, \"Suspicious_Feature\"])\n```\n\n## Performance Optimization\n\n### Speed Considerations\n\n**Explainer Speed** (fastest to slowest):\n1. `LinearExplainer` - Nearly instantaneous\n2. `TreeExplainer` - Very fast\n3. `DeepExplainer` - Fast for neural networks\n4. `GradientExplainer` - Fast for neural networks\n5. `KernelExplainer` - Slow (use only when necessary)\n6. `PermutationExplainer` - Very slow but accurate\n\n### Optimization Strategies\n\n**For Large Datasets**:\n```python\n# Compute SHAP for subset\nshap_values = explainer(X_test[:1000])\n\n# Or use batching\nbatch_size = 100\nall_shap_values = []\nfor i in range(0, len(X_test), batch_size):\n    batch_shap = explainer(X_test[i:i+batch_size])\n    all_shap_values.append(batch_shap)\n```\n\n**For Visualizations**:\n```python\n# Sample subset for plots\nshap.plots.beeswarm(shap_values[:1000])\n\n# Adjust transparency for dense plots\nshap.plots.scatter(shap_values[:, \"Feature\"], alpha=0.3)\n```\n\n**For Production**:\n```python\n# Cache explainer\nimport joblib\njoblib.dump(explainer, 'explainer.pkl')\nexplainer = joblib.load('explainer.pkl')\n\n# Pre-compute for batch predictions\n# Only compute top N features for API responses\n```\n\n## Troubleshooting\n\n### Issue: Wrong explainer choice\n**Problem**: Using KernelExplainer for tree models (slow and unnecessary)\n**Solution**: Always use TreeExplainer for tree-based models\n\n### Issue: Insufficient background data\n**Problem**: DeepExplainer/KernelExplainer with too few background samples\n**Solution**: Use 100-1000 representative samples\n\n### Issue: Confusing units\n**Problem**: Interpreting log-odds as probabilities\n**Solution**: Check model output type; understand whether values are probabilities, log-odds, or raw outputs\n\n### Issue: Plots don't display\n**Problem**: Matplotlib backend issues\n**Solution**: Ensure backend is set correctly; use `plt.show()` if needed\n\n### Issue: Too many features cluttering plots\n**Problem**: Default max_display=10 may be too many or too few\n**Solution**: Adjust `max_display` parameter or use feature clustering\n\n### Issue: Slow computation\n**Problem**: Computing SHAP for very large datasets\n**Solution**: Sample subset, use batching, or ensure using specialized explainer (not KernelExplainer)\n\n## Integration with Other Tools\n\n### Jupyter Notebooks\n- Interactive force plots work seamlessly\n- Inline plot display with `show=True` (default)\n- Combine with markdown for narrative explanations\n\n### MLflow / Experiment Tracking\n```python\nimport mlflow\n\nwith mlflow.start_run():\n    # Train model\n    model = train_model(X_train, y_train)\n\n    # Compute SHAP\n    explainer = shap.TreeExplainer(model)\n    shap_values = explainer(X_test)\n\n    # Log plots\n    shap.plots.beeswarm(shap_values, show=False)\n    mlflow.log_figure(plt.gcf(), \"shap_beeswarm.png\")\n    plt.close()\n\n    # Log feature importance metrics\n    mean_abs_shap = np.abs(shap_values.values).mean(axis=0)\n    for feature, importance in zip(X_test.columns, mean_abs_shap):\n        mlflow.log_metric(f\"shap_{feature}\", importance)\n```\n\n### Production APIs\n```python\nclass ExplanationService:\n    def __init__(self, model_path, explainer_path):\n        self.model = joblib.load(model_path)\n        self.explainer = joblib.load(explainer_path)\n\n    def predict_with_explanation(self, X):\n        prediction = self.model.predict(X)\n        shap_values = self.explainer(X)\n\n        return {\n            'prediction': prediction[0],\n            'base_value': shap_values.base_values[0],\n            'feature_contributions': dict(zip(X.columns, shap_values.values[0]))\n        }\n```\n\n## Reference Documentation\n\nThis skill includes comprehensive reference documentation organized by topic:\n\n### references/explainers.md\nComplete guide to all explainer classes:\n- `TreeExplainer` - Fast, exact explanations for tree-based models\n- `DeepExplainer` - Deep learning models (TensorFlow, PyTorch)\n- `KernelExplainer` - Model-agnostic (works with any model)\n- `LinearExplainer` - Fast explanations for linear models\n- `GradientExplainer` - Gradient-based for neural networks\n- `PermutationExplainer` - Exact but slow for any model\n\nIncludes: Constructor parameters, methods, supported models, when to use, examples, performance considerations.\n\n### references/plots.md\nComprehensive visualization guide:\n- **Waterfall plots** - Individual prediction breakdowns\n- **Beeswarm plots** - Global importance with value distributions\n- **Bar plots** - Clean feature importance summaries\n- **Scatter plots** - Feature-prediction relationships and interactions\n- **Force plots** - Interactive additive force visualizations\n- **Heatmap plots** - Multi-sample comparison grids\n- **Violin plots** - Distribution-focused alternatives\n- **Decision plots** - Multiclass prediction paths\n\nIncludes: Parameters, use cases, examples, best practices, plot selection guide.\n\n### references/workflows.md\nDetailed workflows and best practices:\n- Basic model explanation workflow\n- Model debugging and validation\n- Feature engineering guidance\n- Model comparison and selection\n- Fairness and bias analysis\n- Deep learning model explanation\n- Production deployment\n- Time series model explanation\n- Common pitfalls and solutions\n- Advanced techniques\n- MLOps integration\n\nIncludes: Step-by-step instructions, code examples, decision criteria, troubleshooting.\n\n### references/theory.md\nTheoretical foundations:\n- Shapley values from game theory\n- Mathematical formulas and properties\n- Connection to other explanation methods (LIME, DeepLIFT, etc.)\n- SHAP computation algorithms (Tree SHAP, Kernel SHAP, etc.)\n- Conditional expectations and baseline selection\n- Interpreting SHAP values\n- Interaction values\n- Theoretical limitations and considerations\n\nIncludes: Mathematical foundations, proofs, comparisons, advanced topics.\n\n## Usage Guidelines\n\n**When to load reference files**:\n- Load `explainers.md` when user needs detailed information about specific explainer types or parameters\n- Load `plots.md` when user needs detailed visualization guidance or exploring plot options\n- Load `workflows.md` when user has complex multi-step tasks (debugging, fairness analysis, production deployment)\n- Load `theory.md` when user asks about theoretical foundations, Shapley values, or mathematical details\n\n**Default approach** (without loading references):\n- Use this SKILL.md for basic explanations and quick start\n- Provide standard workflows and common patterns\n- Reference files are available if more detail is needed\n\n**Loading references**:\n```python\n# To load reference files, use the Read tool with appropriate file path:\n# /path/to/shap/references/explainers.md\n# /path/to/shap/references/plots.md\n# /path/to/shap/references/workflows.md\n# /path/to/shap/references/theory.md\n```\n\n## Best Practices Summary\n\n1. **Choose the right explainer**: Use specialized explainers (TreeExplainer, DeepExplainer, LinearExplainer) when possible; avoid KernelExplainer unless necessary\n\n2. **Start global, then go local**: Begin with beeswarm/bar plots for overall understanding, then dive into waterfall/scatter plots for details\n\n3. **Use multiple visualizations**: Different plots reveal different insights; combine global (beeswarm) + local (waterfall) + relationship (scatter) views\n\n4. **Select appropriate background data**: Use 50-1000 representative samples from training data\n\n5. **Understand model output units**: Know whether explaining probabilities, log-odds, or raw outputs\n\n6. **Validate with domain knowledge**: SHAP shows model behavior; use domain expertise to interpret and validate\n\n7. **Optimize for performance**: Sample subsets for visualization, batch for large datasets, cache explainers in production\n\n8. **Check for data leakage**: Unexpectedly high feature importance may indicate data quality issues\n\n9. **Consider feature correlations**: Use TreeExplainer's correlation-aware options or feature clustering for redundant features\n\n10. **Remember SHAP shows association, not causation**: Use domain knowledge for causal interpretation\n\n## Installation\n\n```bash\n# Basic installation\nuv pip install shap\n\n# With visualization dependencies\nuv pip install shap matplotlib\n\n# Latest version\nuv pip install -U shap\n```\n\n**Dependencies**: numpy, pandas, scikit-learn, matplotlib, scipy\n\n**Optional**: xgboost, lightgbm, tensorflow, torch (depending on model types)\n\n## Additional Resources\n\n- **Official Documentation**: https://shap.readthedocs.io/\n- **GitHub Repository**: https://github.com/slundberg/shap\n- **Original Paper**: Lundberg & Lee (2017) - \"A Unified Approach to Interpreting Model Predictions\"\n- **Nature MI Paper**: Lundberg et al. (2020) - \"From local explanations to global understanding with explainable AI for trees\"\n\nThis skill provides comprehensive coverage of SHAP for model interpretability across all use cases and model types.\n",
        "data/k-dense-ai/simpy/SKILL.md": "---\nname: simpy\ndescription: Process-based discrete-event simulation framework in Python. Use this skill when building simulations of systems with processes, queues, resources, and time-based events such as manufacturing systems, service operations, network traffic, logistics, or any system where entities interact with shared resources over time.\n---\n\n# SimPy - Discrete-Event Simulation\n\n## Overview\n\nSimPy is a process-based discrete-event simulation framework based on standard Python. Use SimPy to model systems where entities (customers, vehicles, packets, etc.) interact with each other and compete for shared resources (servers, machines, bandwidth, etc.) over time.\n\n**Core capabilities:**\n- Process modeling using Python generator functions\n- Shared resource management (servers, containers, stores)\n- Event-driven scheduling and synchronization\n- Real-time simulations synchronized with wall-clock time\n- Comprehensive monitoring and data collection\n\n## When to Use This Skill\n\nUse the SimPy skill when:\n\n1. **Modeling discrete-event systems** - Systems where events occur at irregular intervals\n2. **Resource contention** - Entities compete for limited resources (servers, machines, staff)\n3. **Queue analysis** - Studying waiting lines, service times, and throughput\n4. **Process optimization** - Analyzing manufacturing, logistics, or service processes\n5. **Network simulation** - Packet routing, bandwidth allocation, latency analysis\n6. **Capacity planning** - Determining optimal resource levels for desired performance\n7. **System validation** - Testing system behavior before implementation\n\n**Not suitable for:**\n- Continuous simulations with fixed time steps (consider SciPy ODE solvers)\n- Independent processes without resource sharing\n- Pure mathematical optimization (consider SciPy optimize)\n\n## Quick Start\n\n### Basic Simulation Structure\n\n```python\nimport simpy\n\ndef process(env, name):\n    \"\"\"A simple process that waits and prints.\"\"\"\n    print(f'{name} starting at {env.now}')\n    yield env.timeout(5)\n    print(f'{name} finishing at {env.now}')\n\n# Create environment\nenv = simpy.Environment()\n\n# Start processes\nenv.process(process(env, 'Process 1'))\nenv.process(process(env, 'Process 2'))\n\n# Run simulation\nenv.run(until=10)\n```\n\n### Resource Usage Pattern\n\n```python\nimport simpy\n\ndef customer(env, name, resource):\n    \"\"\"Customer requests resource, uses it, then releases.\"\"\"\n    with resource.request() as req:\n        yield req  # Wait for resource\n        print(f'{name} got resource at {env.now}')\n        yield env.timeout(3)  # Use resource\n        print(f'{name} released resource at {env.now}')\n\nenv = simpy.Environment()\nserver = simpy.Resource(env, capacity=1)\n\nenv.process(customer(env, 'Customer 1', server))\nenv.process(customer(env, 'Customer 2', server))\nenv.run()\n```\n\n## Core Concepts\n\n### 1. Environment\n\nThe simulation environment manages time and schedules events.\n\n```python\nimport simpy\n\n# Standard environment (runs as fast as possible)\nenv = simpy.Environment(initial_time=0)\n\n# Real-time environment (synchronized with wall-clock)\nimport simpy.rt\nenv_rt = simpy.rt.RealtimeEnvironment(factor=1.0)\n\n# Run simulation\nenv.run(until=100)  # Run until time 100\nenv.run()  # Run until no events remain\n```\n\n### 2. Processes\n\nProcesses are defined using Python generator functions (functions with `yield` statements).\n\n```python\ndef my_process(env, param1, param2):\n    \"\"\"Process that yields events to pause execution.\"\"\"\n    print(f'Starting at {env.now}')\n\n    # Wait for time to pass\n    yield env.timeout(5)\n\n    print(f'Resumed at {env.now}')\n\n    # Wait for another event\n    yield env.timeout(3)\n\n    print(f'Done at {env.now}')\n    return 'result'\n\n# Start the process\nenv.process(my_process(env, 'value1', 'value2'))\n```\n\n### 3. Events\n\nEvents are the fundamental mechanism for process synchronization. Processes yield events and resume when those events are triggered.\n\n**Common event types:**\n- `env.timeout(delay)` - Wait for time to pass\n- `resource.request()` - Request a resource\n- `env.event()` - Create a custom event\n- `env.process(func())` - Process as an event\n- `event1 & event2` - Wait for all events (AllOf)\n- `event1 | event2` - Wait for any event (AnyOf)\n\n## Resources\n\nSimPy provides several resource types for different scenarios. For comprehensive details, see `references/resources.md`.\n\n### Resource Types Summary\n\n| Resource Type | Use Case |\n|---------------|----------|\n| Resource | Limited capacity (servers, machines) |\n| PriorityResource | Priority-based queuing |\n| PreemptiveResource | High-priority can interrupt low-priority |\n| Container | Bulk materials (fuel, water) |\n| Store | Python object storage (FIFO) |\n| FilterStore | Selective item retrieval |\n| PriorityStore | Priority-ordered items |\n\n### Quick Reference\n\n```python\nimport simpy\n\nenv = simpy.Environment()\n\n# Basic resource (e.g., servers)\nresource = simpy.Resource(env, capacity=2)\n\n# Priority resource\npriority_resource = simpy.PriorityResource(env, capacity=1)\n\n# Container (e.g., fuel tank)\nfuel_tank = simpy.Container(env, capacity=100, init=50)\n\n# Store (e.g., warehouse)\nwarehouse = simpy.Store(env, capacity=10)\n```\n\n## Common Simulation Patterns\n\n### Pattern 1: Customer-Server Queue\n\n```python\nimport simpy\nimport random\n\ndef customer(env, name, server):\n    arrival = env.now\n    with server.request() as req:\n        yield req\n        wait = env.now - arrival\n        print(f'{name} waited {wait:.2f}, served at {env.now}')\n        yield env.timeout(random.uniform(2, 4))\n\ndef customer_generator(env, server):\n    i = 0\n    while True:\n        yield env.timeout(random.uniform(1, 3))\n        i += 1\n        env.process(customer(env, f'Customer {i}', server))\n\nenv = simpy.Environment()\nserver = simpy.Resource(env, capacity=2)\nenv.process(customer_generator(env, server))\nenv.run(until=20)\n```\n\n### Pattern 2: Producer-Consumer\n\n```python\nimport simpy\n\ndef producer(env, store):\n    item_id = 0\n    while True:\n        yield env.timeout(2)\n        item = f'Item {item_id}'\n        yield store.put(item)\n        print(f'Produced {item} at {env.now}')\n        item_id += 1\n\ndef consumer(env, store):\n    while True:\n        item = yield store.get()\n        print(f'Consumed {item} at {env.now}')\n        yield env.timeout(3)\n\nenv = simpy.Environment()\nstore = simpy.Store(env, capacity=10)\nenv.process(producer(env, store))\nenv.process(consumer(env, store))\nenv.run(until=20)\n```\n\n### Pattern 3: Parallel Task Execution\n\n```python\nimport simpy\n\ndef task(env, name, duration):\n    print(f'{name} starting at {env.now}')\n    yield env.timeout(duration)\n    print(f'{name} done at {env.now}')\n    return f'{name} result'\n\ndef coordinator(env):\n    # Start tasks in parallel\n    task1 = env.process(task(env, 'Task 1', 5))\n    task2 = env.process(task(env, 'Task 2', 3))\n    task3 = env.process(task(env, 'Task 3', 4))\n\n    # Wait for all to complete\n    results = yield task1 & task2 & task3\n    print(f'All done at {env.now}')\n\nenv = simpy.Environment()\nenv.process(coordinator(env))\nenv.run()\n```\n\n## Workflow Guide\n\n### Step 1: Define the System\n\nIdentify:\n- **Entities**: What moves through the system? (customers, parts, packets)\n- **Resources**: What are the constraints? (servers, machines, bandwidth)\n- **Processes**: What are the activities? (arrival, service, departure)\n- **Metrics**: What to measure? (wait times, utilization, throughput)\n\n### Step 2: Implement Process Functions\n\nCreate generator functions for each process type:\n\n```python\ndef entity_process(env, name, resources, parameters):\n    # Arrival logic\n    arrival_time = env.now\n\n    # Request resources\n    with resource.request() as req:\n        yield req\n\n        # Service logic\n        service_time = calculate_service_time(parameters)\n        yield env.timeout(service_time)\n\n    # Departure logic\n    collect_statistics(env.now - arrival_time)\n```\n\n### Step 3: Set Up Monitoring\n\nUse monitoring utilities to collect data. See `references/monitoring.md` for comprehensive techniques.\n\n```python\nfrom scripts.resource_monitor import ResourceMonitor\n\n# Create and monitor resource\nresource = simpy.Resource(env, capacity=2)\nmonitor = ResourceMonitor(env, resource, \"Server\")\n\n# After simulation\nmonitor.report()\n```\n\n### Step 4: Run and Analyze\n\n```python\n# Run simulation\nenv.run(until=simulation_time)\n\n# Generate reports\nmonitor.report()\nstats.report()\n\n# Export data for further analysis\nmonitor.export_csv('results.csv')\n```\n\n## Advanced Features\n\n### Process Interaction\n\nProcesses can interact through events, process yields, and interrupts. See `references/process-interaction.md` for detailed patterns.\n\n**Key mechanisms:**\n- **Event signaling**: Shared events for coordination\n- **Process yields**: Wait for other processes to complete\n- **Interrupts**: Forcefully resume processes for preemption\n\n### Real-Time Simulations\n\nSynchronize simulation with wall-clock time for hardware-in-the-loop or interactive applications. See `references/real-time.md`.\n\n```python\nimport simpy.rt\n\nenv = simpy.rt.RealtimeEnvironment(factor=1.0)  # 1:1 time mapping\n# factor=0.5 means 1 sim unit = 0.5 seconds (2x faster)\n```\n\n### Comprehensive Monitoring\n\nMonitor processes, resources, and events. See `references/monitoring.md` for techniques including:\n- State variable tracking\n- Resource monkey-patching\n- Event tracing\n- Statistical collection\n\n## Scripts and Templates\n\n### basic_simulation_template.py\n\nComplete template for building queue simulations with:\n- Configurable parameters\n- Statistics collection\n- Customer generation\n- Resource usage\n- Report generation\n\n**Usage:**\n```python\nfrom scripts.basic_simulation_template import SimulationConfig, run_simulation\n\nconfig = SimulationConfig()\nconfig.num_resources = 2\nconfig.sim_time = 100\nstats = run_simulation(config)\nstats.report()\n```\n\n### resource_monitor.py\n\nReusable monitoring utilities:\n- `ResourceMonitor` - Track single resource\n- `MultiResourceMonitor` - Monitor multiple resources\n- `ContainerMonitor` - Track container levels\n- Automatic statistics calculation\n- CSV export functionality\n\n**Usage:**\n```python\nfrom scripts.resource_monitor import ResourceMonitor\n\nmonitor = ResourceMonitor(env, resource, \"My Resource\")\n# ... run simulation ...\nmonitor.report()\nmonitor.export_csv('data.csv')\n```\n\n## Reference Documentation\n\nDetailed guides for specific topics:\n\n- **`references/resources.md`** - All resource types with examples\n- **`references/events.md`** - Event system and patterns\n- **`references/process-interaction.md`** - Process synchronization\n- **`references/monitoring.md`** - Data collection techniques\n- **`references/real-time.md`** - Real-time simulation setup\n\n## Best Practices\n\n1. **Generator functions**: Always use `yield` in process functions\n2. **Resource context managers**: Use `with resource.request() as req:` for automatic cleanup\n3. **Reproducibility**: Set `random.seed()` for consistent results\n4. **Monitoring**: Collect data throughout simulation, not just at the end\n5. **Validation**: Compare simple cases with analytical solutions\n6. **Documentation**: Comment process logic and parameter choices\n7. **Modular design**: Separate process logic, statistics, and configuration\n\n## Common Pitfalls\n\n1. **Forgetting yield**: Processes must yield events to pause\n2. **Event reuse**: Events can only be triggered once\n3. **Resource leaks**: Use context managers or ensure release\n4. **Blocking operations**: Avoid Python blocking calls in processes\n5. **Time units**: Stay consistent with time unit interpretation\n6. **Deadlocks**: Ensure at least one process can make progress\n\n## Example Use Cases\n\n- **Manufacturing**: Machine scheduling, production lines, inventory management\n- **Healthcare**: Emergency room simulation, patient flow, staff allocation\n- **Telecommunications**: Network traffic, packet routing, bandwidth allocation\n- **Transportation**: Traffic flow, logistics, vehicle routing\n- **Service operations**: Call centers, retail checkout, appointment scheduling\n- **Computer systems**: CPU scheduling, memory management, I/O operations\n",
        "data/k-dense-ai/stable-baselines3/SKILL.md": "---\nname: stable-baselines3\ndescription: Use this skill for reinforcement learning tasks including training RL agents (PPO, SAC, DQN, TD3, DDPG, A2C, etc.), creating custom Gym environments, implementing callbacks for monitoring and control, using vectorized environments for parallel training, and integrating with deep RL workflows. This skill should be used when users request RL algorithm implementation, agent training, environment design, or RL experimentation.\n---\n\n# Stable Baselines3\n\n## Overview\n\nStable Baselines3 (SB3) is a PyTorch-based library providing reliable implementations of reinforcement learning algorithms. This skill provides comprehensive guidance for training RL agents, creating custom environments, implementing callbacks, and optimizing training workflows using SB3's unified API.\n\n## Core Capabilities\n\n### 1. Training RL Agents\n\n**Basic Training Pattern:**\n\n```python\nimport gymnasium as gym\nfrom stable_baselines3 import PPO\n\n# Create environment\nenv = gym.make(\"CartPole-v1\")\n\n# Initialize agent\nmodel = PPO(\"MlpPolicy\", env, verbose=1)\n\n# Train the agent\nmodel.learn(total_timesteps=10000)\n\n# Save the model\nmodel.save(\"ppo_cartpole\")\n\n# Load the model (without prior instantiation)\nmodel = PPO.load(\"ppo_cartpole\", env=env)\n```\n\n**Important Notes:**\n- `total_timesteps` is a lower bound; actual training may exceed this due to batch collection\n- Use `model.load()` as a static method, not on an existing instance\n- The replay buffer is NOT saved with the model to save space\n\n**Algorithm Selection:**\nUse `references/algorithms.md` for detailed algorithm characteristics and selection guidance. Quick reference:\n- **PPO/A2C**: General-purpose, supports all action space types, good for multiprocessing\n- **SAC/TD3**: Continuous control, off-policy, sample-efficient\n- **DQN**: Discrete actions, off-policy\n- **HER**: Goal-conditioned tasks\n\nSee `scripts/train_rl_agent.py` for a complete training template with best practices.\n\n### 2. Custom Environments\n\n**Requirements:**\nCustom environments must inherit from `gymnasium.Env` and implement:\n- `__init__()`: Define action_space and observation_space\n- `reset(seed, options)`: Return initial observation and info dict\n- `step(action)`: Return observation, reward, terminated, truncated, info\n- `render()`: Visualization (optional)\n- `close()`: Cleanup resources\n\n**Key Constraints:**\n- Image observations must be `np.uint8` in range [0, 255]\n- Use channel-first format when possible (channels, height, width)\n- SB3 normalizes images automatically by dividing by 255\n- Set `normalize_images=False` in policy_kwargs if pre-normalized\n- SB3 does NOT support `Discrete` or `MultiDiscrete` spaces with `start!=0`\n\n**Validation:**\n```python\nfrom stable_baselines3.common.env_checker import check_env\n\ncheck_env(env, warn=True)\n```\n\nSee `scripts/custom_env_template.py` for a complete custom environment template and `references/custom_environments.md` for comprehensive guidance.\n\n### 3. Vectorized Environments\n\n**Purpose:**\nVectorized environments run multiple environment instances in parallel, accelerating training and enabling certain wrappers (frame-stacking, normalization).\n\n**Types:**\n- **DummyVecEnv**: Sequential execution on current process (for lightweight environments)\n- **SubprocVecEnv**: Parallel execution across processes (for compute-heavy environments)\n\n**Quick Setup:**\n```python\nfrom stable_baselines3.common.env_util import make_vec_env\n\n# Create 4 parallel environments\nenv = make_vec_env(\"CartPole-v1\", n_envs=4, vec_env_cls=SubprocVecEnv)\n\nmodel = PPO(\"MlpPolicy\", env, verbose=1)\nmodel.learn(total_timesteps=25000)\n```\n\n**Off-Policy Optimization:**\nWhen using multiple environments with off-policy algorithms (SAC, TD3, DQN), set `gradient_steps=-1` to perform one gradient update per environment step, balancing wall-clock time and sample efficiency.\n\n**API Differences:**\n- `reset()` returns only observations (info available in `vec_env.reset_infos`)\n- `step()` returns 4-tuple: `(obs, rewards, dones, infos)` not 5-tuple\n- Environments auto-reset after episodes\n- Terminal observations available via `infos[env_idx][\"terminal_observation\"]`\n\nSee `references/vectorized_envs.md` for detailed information on wrappers and advanced usage.\n\n### 4. Callbacks for Monitoring and Control\n\n**Purpose:**\nCallbacks enable monitoring metrics, saving checkpoints, implementing early stopping, and custom training logic without modifying core algorithms.\n\n**Common Callbacks:**\n- **EvalCallback**: Evaluate periodically and save best model\n- **CheckpointCallback**: Save model checkpoints at intervals\n- **StopTrainingOnRewardThreshold**: Stop when target reward reached\n- **ProgressBarCallback**: Display training progress with timing\n\n**Custom Callback Structure:**\n```python\nfrom stable_baselines3.common.callbacks import BaseCallback\n\nclass CustomCallback(BaseCallback):\n    def _on_training_start(self):\n        # Called before first rollout\n        pass\n\n    def _on_step(self):\n        # Called after each environment step\n        # Return False to stop training\n        return True\n\n    def _on_rollout_end(self):\n        # Called at end of rollout\n        pass\n```\n\n**Available Attributes:**\n- `self.model`: The RL algorithm instance\n- `self.num_timesteps`: Total environment steps\n- `self.training_env`: The training environment\n\n**Chaining Callbacks:**\n```python\nfrom stable_baselines3.common.callbacks import CallbackList\n\ncallback = CallbackList([eval_callback, checkpoint_callback, custom_callback])\nmodel.learn(total_timesteps=10000, callback=callback)\n```\n\nSee `references/callbacks.md` for comprehensive callback documentation.\n\n### 5. Model Persistence and Inspection\n\n**Saving and Loading:**\n```python\n# Save model\nmodel.save(\"model_name\")\n\n# Save normalization statistics (if using VecNormalize)\nvec_env.save(\"vec_normalize.pkl\")\n\n# Load model\nmodel = PPO.load(\"model_name\", env=env)\n\n# Load normalization statistics\nvec_env = VecNormalize.load(\"vec_normalize.pkl\", vec_env)\n```\n\n**Parameter Access:**\n```python\n# Get parameters\nparams = model.get_parameters()\n\n# Set parameters\nmodel.set_parameters(params)\n\n# Access PyTorch state dict\nstate_dict = model.policy.state_dict()\n```\n\n### 6. Evaluation and Recording\n\n**Evaluation:**\n```python\nfrom stable_baselines3.common.evaluation import evaluate_policy\n\nmean_reward, std_reward = evaluate_policy(\n    model,\n    env,\n    n_eval_episodes=10,\n    deterministic=True\n)\n```\n\n**Video Recording:**\n```python\nfrom stable_baselines3.common.vec_env import VecVideoRecorder\n\n# Wrap environment with video recorder\nenv = VecVideoRecorder(\n    env,\n    \"videos/\",\n    record_video_trigger=lambda x: x % 2000 == 0,\n    video_length=200\n)\n```\n\nSee `scripts/evaluate_agent.py` for a complete evaluation and recording template.\n\n### 7. Advanced Features\n\n**Learning Rate Schedules:**\n```python\ndef linear_schedule(initial_value):\n    def func(progress_remaining):\n        # progress_remaining goes from 1 to 0\n        return progress_remaining * initial_value\n    return func\n\nmodel = PPO(\"MlpPolicy\", env, learning_rate=linear_schedule(0.001))\n```\n\n**Multi-Input Policies (Dict Observations):**\n```python\nmodel = PPO(\"MultiInputPolicy\", env, verbose=1)\n```\nUse when observations are dictionaries (e.g., combining images with sensor data).\n\n**Hindsight Experience Replay:**\n```python\nfrom stable_baselines3 import SAC, HerReplayBuffer\n\nmodel = SAC(\n    \"MultiInputPolicy\",\n    env,\n    replay_buffer_class=HerReplayBuffer,\n    replay_buffer_kwargs=dict(\n        n_sampled_goal=4,\n        goal_selection_strategy=\"future\",\n    ),\n)\n```\n\n**TensorBoard Integration:**\n```python\nmodel = PPO(\"MlpPolicy\", env, tensorboard_log=\"./tensorboard/\")\nmodel.learn(total_timesteps=10000)\n```\n\n## Workflow Guidance\n\n**Starting a New RL Project:**\n\n1. **Define the problem**: Identify observation space, action space, and reward structure\n2. **Choose algorithm**: Use `references/algorithms.md` for selection guidance\n3. **Create/adapt environment**: Use `scripts/custom_env_template.py` if needed\n4. **Validate environment**: Always run `check_env()` before training\n5. **Set up training**: Use `scripts/train_rl_agent.py` as starting template\n6. **Add monitoring**: Implement callbacks for evaluation and checkpointing\n7. **Optimize performance**: Consider vectorized environments for speed\n8. **Evaluate and iterate**: Use `scripts/evaluate_agent.py` for assessment\n\n**Common Issues:**\n\n- **Memory errors**: Reduce `buffer_size` for off-policy algorithms or use fewer parallel environments\n- **Slow training**: Consider SubprocVecEnv for parallel environments\n- **Unstable training**: Try different algorithms, tune hyperparameters, or check reward scaling\n- **Import errors**: Ensure `stable_baselines3` is installed: `uv pip install stable-baselines3[extra]`\n\n## Resources\n\n### scripts/\n- `train_rl_agent.py`: Complete training script template with best practices\n- `evaluate_agent.py`: Agent evaluation and video recording template\n- `custom_env_template.py`: Custom Gym environment template\n\n### references/\n- `algorithms.md`: Detailed algorithm comparison and selection guide\n- `custom_environments.md`: Comprehensive custom environment creation guide\n- `callbacks.md`: Complete callback system reference\n- `vectorized_envs.md`: Vectorized environment usage and wrappers\n\n## Installation\n\n```bash\n# Basic installation\nuv pip install stable-baselines3\n\n# With extra dependencies (Tensorboard, etc.)\nuv pip install stable-baselines3[extra]\n```\n",
        "data/k-dense-ai/statistical-analysis/SKILL.md": "---\nname: statistical-analysis\ndescription: \"Statistical analysis toolkit. Hypothesis tests (t-test, ANOVA, chi-square), regression, correlation, Bayesian stats, power analysis, assumption checks, APA reporting, for academic research.\"\n---\n\n# Statistical Analysis\n\n## Overview\n\nStatistical analysis is a systematic process for testing hypotheses and quantifying relationships. Conduct hypothesis tests (t-test, ANOVA, chi-square), regression, correlation, and Bayesian analyses with assumption checks and APA reporting. Apply this skill for academic research.\n\n## When to Use This Skill\n\nThis skill should be used when:\n- Conducting statistical hypothesis tests (t-tests, ANOVA, chi-square)\n- Performing regression or correlation analyses\n- Running Bayesian statistical analyses\n- Checking statistical assumptions and diagnostics\n- Calculating effect sizes and conducting power analyses\n- Reporting statistical results in APA format\n- Analyzing experimental or observational data for research\n\n---\n\n## Core Capabilities\n\n### 1. Test Selection and Planning\n- Choose appropriate statistical tests based on research questions and data characteristics\n- Conduct a priori power analyses to determine required sample sizes\n- Plan analysis strategies including multiple comparison corrections\n\n### 2. Assumption Checking\n- Automatically verify all relevant assumptions before running tests\n- Provide diagnostic visualizations (Q-Q plots, residual plots, box plots)\n- Recommend remedial actions when assumptions are violated\n\n### 3. Statistical Testing\n- Hypothesis testing: t-tests, ANOVA, chi-square, non-parametric alternatives\n- Regression: linear, multiple, logistic, with diagnostics\n- Correlations: Pearson, Spearman, with confidence intervals\n- Bayesian alternatives: Bayesian t-tests, ANOVA, regression with Bayes Factors\n\n### 4. Effect Sizes and Interpretation\n- Calculate and interpret appropriate effect sizes for all analyses\n- Provide confidence intervals for effect estimates\n- Distinguish statistical from practical significance\n\n### 5. Professional Reporting\n- Generate APA-style statistical reports\n- Create publication-ready figures and tables\n- Provide complete interpretation with all required statistics\n\n---\n\n## Workflow Decision Tree\n\nUse this decision tree to determine your analysis path:\n\n```\nSTART\n\n Need to SELECT a statistical test?\n   YES  See \"Test Selection Guide\"\n   NO  Continue\n\n Ready to check ASSUMPTIONS?\n   YES  See \"Assumption Checking\"\n   NO  Continue\n\n Ready to run ANALYSIS?\n   YES  See \"Running Statistical Tests\"\n   NO  Continue\n\n Need to REPORT results?\n    YES  See \"Reporting Results\"\n```\n\n---\n\n## Test Selection Guide\n\n### Quick Reference: Choosing the Right Test\n\nUse `references/test_selection_guide.md` for comprehensive guidance. Quick reference:\n\n**Comparing Two Groups:**\n- Independent, continuous, normal  Independent t-test\n- Independent, continuous, non-normal  Mann-Whitney U test\n- Paired, continuous, normal  Paired t-test\n- Paired, continuous, non-normal  Wilcoxon signed-rank test\n- Binary outcome  Chi-square or Fisher's exact test\n\n**Comparing 3+ Groups:**\n- Independent, continuous, normal  One-way ANOVA\n- Independent, continuous, non-normal  Kruskal-Wallis test\n- Paired, continuous, normal  Repeated measures ANOVA\n- Paired, continuous, non-normal  Friedman test\n\n**Relationships:**\n- Two continuous variables  Pearson (normal) or Spearman correlation (non-normal)\n- Continuous outcome with predictor(s)  Linear regression\n- Binary outcome with predictor(s)  Logistic regression\n\n**Bayesian Alternatives:**\nAll tests have Bayesian versions that provide:\n- Direct probability statements about hypotheses\n- Bayes Factors quantifying evidence\n- Ability to support null hypothesis\n- See `references/bayesian_statistics.md`\n\n---\n\n## Assumption Checking\n\n### Systematic Assumption Verification\n\n**ALWAYS check assumptions before interpreting test results.**\n\nUse the provided `scripts/assumption_checks.py` module for automated checking:\n\n```python\nfrom scripts.assumption_checks import comprehensive_assumption_check\n\n# Comprehensive check with visualizations\nresults = comprehensive_assumption_check(\n    data=df,\n    value_col='score',\n    group_col='group',  # Optional: for group comparisons\n    alpha=0.05\n)\n```\n\nThis performs:\n1. **Outlier detection** (IQR and z-score methods)\n2. **Normality testing** (Shapiro-Wilk test + Q-Q plots)\n3. **Homogeneity of variance** (Levene's test + box plots)\n4. **Interpretation and recommendations**\n\n### Individual Assumption Checks\n\nFor targeted checks, use individual functions:\n\n```python\nfrom scripts.assumption_checks import (\n    check_normality,\n    check_normality_per_group,\n    check_homogeneity_of_variance,\n    check_linearity,\n    detect_outliers\n)\n\n# Example: Check normality with visualization\nresult = check_normality(\n    data=df['score'],\n    name='Test Score',\n    alpha=0.05,\n    plot=True\n)\nprint(result['interpretation'])\nprint(result['recommendation'])\n```\n\n### What to Do When Assumptions Are Violated\n\n**Normality violated:**\n- Mild violation + n > 30 per group  Proceed with parametric test (robust)\n- Moderate violation  Use non-parametric alternative\n- Severe violation  Transform data or use non-parametric test\n\n**Homogeneity of variance violated:**\n- For t-test  Use Welch's t-test\n- For ANOVA  Use Welch's ANOVA or Brown-Forsythe ANOVA\n- For regression  Use robust standard errors or weighted least squares\n\n**Linearity violated (regression):**\n- Add polynomial terms\n- Transform variables\n- Use non-linear models or GAM\n\nSee `references/assumptions_and_diagnostics.md` for comprehensive guidance.\n\n---\n\n## Running Statistical Tests\n\n### Python Libraries\n\nPrimary libraries for statistical analysis:\n- **scipy.stats**: Core statistical tests\n- **statsmodels**: Advanced regression and diagnostics\n- **pingouin**: User-friendly statistical testing with effect sizes\n- **pymc**: Bayesian statistical modeling\n- **arviz**: Bayesian visualization and diagnostics\n\n### Example Analyses\n\n#### T-Test with Complete Reporting\n\n```python\nimport pingouin as pg\nimport numpy as np\n\n# Run independent t-test\nresult = pg.ttest(group_a, group_b, correction='auto')\n\n# Extract results\nt_stat = result['T'].values[0]\ndf = result['dof'].values[0]\np_value = result['p-val'].values[0]\ncohens_d = result['cohen-d'].values[0]\nci_lower = result['CI95%'].values[0][0]\nci_upper = result['CI95%'].values[0][1]\n\n# Report\nprint(f\"t({df:.0f}) = {t_stat:.2f}, p = {p_value:.3f}\")\nprint(f\"Cohen's d = {cohens_d:.2f}, 95% CI [{ci_lower:.2f}, {ci_upper:.2f}]\")\n```\n\n#### ANOVA with Post-Hoc Tests\n\n```python\nimport pingouin as pg\n\n# One-way ANOVA\naov = pg.anova(dv='score', between='group', data=df, detailed=True)\nprint(aov)\n\n# If significant, conduct post-hoc tests\nif aov['p-unc'].values[0] < 0.05:\n    posthoc = pg.pairwise_tukey(dv='score', between='group', data=df)\n    print(posthoc)\n\n# Effect size\neta_squared = aov['np2'].values[0]  # Partial eta-squared\nprint(f\"Partial  = {eta_squared:.3f}\")\n```\n\n#### Linear Regression with Diagnostics\n\n```python\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Fit model\nX = sm.add_constant(X_predictors)  # Add intercept\nmodel = sm.OLS(y, X).fit()\n\n# Summary\nprint(model.summary())\n\n# Check multicollinearity (VIF)\nvif_data = pd.DataFrame()\nvif_data[\"Variable\"] = X.columns\nvif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nprint(vif_data)\n\n# Check assumptions\nresiduals = model.resid\nfitted = model.fittedvalues\n\n# Residual plots\nimport matplotlib.pyplot as plt\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\n# Residuals vs fitted\naxes[0, 0].scatter(fitted, residuals, alpha=0.6)\naxes[0, 0].axhline(y=0, color='r', linestyle='--')\naxes[0, 0].set_xlabel('Fitted values')\naxes[0, 0].set_ylabel('Residuals')\naxes[0, 0].set_title('Residuals vs Fitted')\n\n# Q-Q plot\nfrom scipy import stats\nstats.probplot(residuals, dist=\"norm\", plot=axes[0, 1])\naxes[0, 1].set_title('Normal Q-Q')\n\n# Scale-Location\naxes[1, 0].scatter(fitted, np.sqrt(np.abs(residuals / residuals.std())), alpha=0.6)\naxes[1, 0].set_xlabel('Fitted values')\naxes[1, 0].set_ylabel('|Standardized residuals|')\naxes[1, 0].set_title('Scale-Location')\n\n# Residuals histogram\naxes[1, 1].hist(residuals, bins=20, edgecolor='black', alpha=0.7)\naxes[1, 1].set_xlabel('Residuals')\naxes[1, 1].set_ylabel('Frequency')\naxes[1, 1].set_title('Histogram of Residuals')\n\nplt.tight_layout()\nplt.show()\n```\n\n#### Bayesian T-Test\n\n```python\nimport pymc as pm\nimport arviz as az\nimport numpy as np\n\nwith pm.Model() as model:\n    # Priors\n    mu1 = pm.Normal('mu_group1', mu=0, sigma=10)\n    mu2 = pm.Normal('mu_group2', mu=0, sigma=10)\n    sigma = pm.HalfNormal('sigma', sigma=10)\n\n    # Likelihood\n    y1 = pm.Normal('y1', mu=mu1, sigma=sigma, observed=group_a)\n    y2 = pm.Normal('y2', mu=mu2, sigma=sigma, observed=group_b)\n\n    # Derived quantity\n    diff = pm.Deterministic('difference', mu1 - mu2)\n\n    # Sample\n    trace = pm.sample(2000, tune=1000, return_inferencedata=True)\n\n# Summarize\nprint(az.summary(trace, var_names=['difference']))\n\n# Probability that group1 > group2\nprob_greater = np.mean(trace.posterior['difference'].values > 0)\nprint(f\"P( >  | data) = {prob_greater:.3f}\")\n\n# Plot posterior\naz.plot_posterior(trace, var_names=['difference'], ref_val=0)\n```\n\n---\n\n## Effect Sizes\n\n### Always Calculate Effect Sizes\n\n**Effect sizes quantify magnitude, while p-values only indicate existence of an effect.**\n\nSee `references/effect_sizes_and_power.md` for comprehensive guidance.\n\n### Quick Reference: Common Effect Sizes\n\n| Test | Effect Size | Small | Medium | Large |\n|------|-------------|-------|--------|-------|\n| T-test | Cohen's d | 0.20 | 0.50 | 0.80 |\n| ANOVA | _p | 0.01 | 0.06 | 0.14 |\n| Correlation | r | 0.10 | 0.30 | 0.50 |\n| Regression | R | 0.02 | 0.13 | 0.26 |\n| Chi-square | Cramr's V | 0.07 | 0.21 | 0.35 |\n\n**Important**: Benchmarks are guidelines. Context matters!\n\n### Calculating Effect Sizes\n\nMost effect sizes are automatically calculated by pingouin:\n\n```python\n# T-test returns Cohen's d\nresult = pg.ttest(x, y)\nd = result['cohen-d'].values[0]\n\n# ANOVA returns partial eta-squared\naov = pg.anova(dv='score', between='group', data=df)\neta_p2 = aov['np2'].values[0]\n\n# Correlation: r is already an effect size\ncorr = pg.corr(x, y)\nr = corr['r'].values[0]\n```\n\n### Confidence Intervals for Effect Sizes\n\nAlways report CIs to show precision:\n\n```python\nfrom pingouin import compute_effsize_from_t\n\n# For t-test\nd, ci = compute_effsize_from_t(\n    t_statistic,\n    nx=len(group1),\n    ny=len(group2),\n    eftype='cohen'\n)\nprint(f\"d = {d:.2f}, 95% CI [{ci[0]:.2f}, {ci[1]:.2f}]\")\n```\n\n---\n\n## Power Analysis\n\n### A Priori Power Analysis (Study Planning)\n\nDetermine required sample size before data collection:\n\n```python\nfrom statsmodels.stats.power import (\n    tt_ind_solve_power,\n    FTestAnovaPower\n)\n\n# T-test: What n is needed to detect d = 0.5?\nn_required = tt_ind_solve_power(\n    effect_size=0.5,\n    alpha=0.05,\n    power=0.80,\n    ratio=1.0,\n    alternative='two-sided'\n)\nprint(f\"Required n per group: {n_required:.0f}\")\n\n# ANOVA: What n is needed to detect f = 0.25?\nanova_power = FTestAnovaPower()\nn_per_group = anova_power.solve_power(\n    effect_size=0.25,\n    ngroups=3,\n    alpha=0.05,\n    power=0.80\n)\nprint(f\"Required n per group: {n_per_group:.0f}\")\n```\n\n### Sensitivity Analysis (Post-Study)\n\nDetermine what effect size you could detect:\n\n```python\n# With n=50 per group, what effect could we detect?\ndetectable_d = tt_ind_solve_power(\n    effect_size=None,  # Solve for this\n    nobs1=50,\n    alpha=0.05,\n    power=0.80,\n    ratio=1.0,\n    alternative='two-sided'\n)\nprint(f\"Study could detect d  {detectable_d:.2f}\")\n```\n\n**Note**: Post-hoc power analysis (calculating power after study) is generally not recommended. Use sensitivity analysis instead.\n\nSee `references/effect_sizes_and_power.md` for detailed guidance.\n\n---\n\n## Reporting Results\n\n### APA Style Statistical Reporting\n\nFollow guidelines in `references/reporting_standards.md`.\n\n### Essential Reporting Elements\n\n1. **Descriptive statistics**: M, SD, n for all groups/variables\n2. **Test statistics**: Test name, statistic, df, exact p-value\n3. **Effect sizes**: With confidence intervals\n4. **Assumption checks**: Which tests were done, results, actions taken\n5. **All planned analyses**: Including non-significant findings\n\n### Example Report Templates\n\n#### Independent T-Test\n\n```\nGroup A (n = 48, M = 75.2, SD = 8.5) scored significantly higher than\nGroup B (n = 52, M = 68.3, SD = 9.2), t(98) = 3.82, p < .001, d = 0.77,\n95% CI [0.36, 1.18], two-tailed. Assumptions of normality (Shapiro-Wilk:\nGroup A W = 0.97, p = .18; Group B W = 0.96, p = .12) and homogeneity\nof variance (Levene's F(1, 98) = 1.23, p = .27) were satisfied.\n```\n\n#### One-Way ANOVA\n\n```\nA one-way ANOVA revealed a significant main effect of treatment condition\non test scores, F(2, 147) = 8.45, p < .001, _p = .10. Post hoc\ncomparisons using Tukey's HSD indicated that Condition A (M = 78.2,\nSD = 7.3) scored significantly higher than Condition B (M = 71.5,\nSD = 8.1, p = .002, d = 0.87) and Condition C (M = 70.1, SD = 7.9,\np < .001, d = 1.07). Conditions B and C did not differ significantly\n(p = .52, d = 0.18).\n```\n\n#### Multiple Regression\n\n```\nMultiple linear regression was conducted to predict exam scores from\nstudy hours, prior GPA, and attendance. The overall model was significant,\nF(3, 146) = 45.2, p < .001, R = .48, adjusted R = .47. Study hours\n(B = 1.80, SE = 0.31,  = .35, t = 5.78, p < .001, 95% CI [1.18, 2.42])\nand prior GPA (B = 8.52, SE = 1.95,  = .28, t = 4.37, p < .001,\n95% CI [4.66, 12.38]) were significant predictors, while attendance was\nnot (B = 0.15, SE = 0.12,  = .08, t = 1.25, p = .21, 95% CI [-0.09, 0.39]).\nMulticollinearity was not a concern (all VIF < 1.5).\n```\n\n#### Bayesian Analysis\n\n```\nA Bayesian independent samples t-test was conducted using weakly\ninformative priors (Normal(0, 1) for mean difference). The posterior\ndistribution indicated that Group A scored higher than Group B\n(M_diff = 6.8, 95% credible interval [3.2, 10.4]). The Bayes Factor\nBF = 45.3 provided very strong evidence for a difference between\ngroups, with a 99.8% posterior probability that Group A's mean exceeded\nGroup B's mean. Convergence diagnostics were satisfactory (all R < 1.01,\nESS > 1000).\n```\n\n---\n\n## Bayesian Statistics\n\n### When to Use Bayesian Methods\n\nConsider Bayesian approaches when:\n- You have prior information to incorporate\n- You want direct probability statements about hypotheses\n- Sample size is small or planning sequential data collection\n- You need to quantify evidence for the null hypothesis\n- The model is complex (hierarchical, missing data)\n\nSee `references/bayesian_statistics.md` for comprehensive guidance on:\n- Bayes' theorem and interpretation\n- Prior specification (informative, weakly informative, non-informative)\n- Bayesian hypothesis testing with Bayes Factors\n- Credible intervals vs. confidence intervals\n- Bayesian t-tests, ANOVA, regression, and hierarchical models\n- Model convergence checking and posterior predictive checks\n\n### Key Advantages\n\n1. **Intuitive interpretation**: \"Given the data, there is a 95% probability the parameter is in this interval\"\n2. **Evidence for null**: Can quantify support for no effect\n3. **Flexible**: No p-hacking concerns; can analyze data as it arrives\n4. **Uncertainty quantification**: Full posterior distribution\n\n---\n\n## Resources\n\nThis skill includes comprehensive reference materials:\n\n### References Directory\n\n- **test_selection_guide.md**: Decision tree for choosing appropriate statistical tests\n- **assumptions_and_diagnostics.md**: Detailed guidance on checking and handling assumption violations\n- **effect_sizes_and_power.md**: Calculating, interpreting, and reporting effect sizes; conducting power analyses\n- **bayesian_statistics.md**: Complete guide to Bayesian analysis methods\n- **reporting_standards.md**: APA-style reporting guidelines with examples\n\n### Scripts Directory\n\n- **assumption_checks.py**: Automated assumption checking with visualizations\n  - `comprehensive_assumption_check()`: Complete workflow\n  - `check_normality()`: Normality testing with Q-Q plots\n  - `check_homogeneity_of_variance()`: Levene's test with box plots\n  - `check_linearity()`: Regression linearity checks\n  - `detect_outliers()`: IQR and z-score outlier detection\n\n---\n\n## Best Practices\n\n1. **Pre-register analyses** when possible to distinguish confirmatory from exploratory\n2. **Always check assumptions** before interpreting results\n3. **Report effect sizes** with confidence intervals\n4. **Report all planned analyses** including non-significant results\n5. **Distinguish statistical from practical significance**\n6. **Visualize data** before and after analysis\n7. **Check diagnostics** for regression/ANOVA (residual plots, VIF, etc.)\n8. **Conduct sensitivity analyses** to assess robustness\n9. **Share data and code** for reproducibility\n10. **Be transparent** about violations, transformations, and decisions\n\n---\n\n## Common Pitfalls to Avoid\n\n1. **P-hacking**: Don't test multiple ways until something is significant\n2. **HARKing**: Don't present exploratory findings as confirmatory\n3. **Ignoring assumptions**: Check them and report violations\n4. **Confusing significance with importance**: p < .05  meaningful effect\n5. **Not reporting effect sizes**: Essential for interpretation\n6. **Cherry-picking results**: Report all planned analyses\n7. **Misinterpreting p-values**: They're NOT probability that hypothesis is true\n8. **Multiple comparisons**: Correct for family-wise error when appropriate\n9. **Ignoring missing data**: Understand mechanism (MCAR, MAR, MNAR)\n10. **Overinterpreting non-significant results**: Absence of evidence  evidence of absence\n\n---\n\n## Getting Started Checklist\n\nWhen beginning a statistical analysis:\n\n- [ ] Define research question and hypotheses\n- [ ] Determine appropriate statistical test (use test_selection_guide.md)\n- [ ] Conduct power analysis to determine sample size\n- [ ] Load and inspect data\n- [ ] Check for missing data and outliers\n- [ ] Verify assumptions using assumption_checks.py\n- [ ] Run primary analysis\n- [ ] Calculate effect sizes with confidence intervals\n- [ ] Conduct post-hoc tests if needed (with corrections)\n- [ ] Create visualizations\n- [ ] Write results following reporting_standards.md\n- [ ] Conduct sensitivity analyses\n- [ ] Share data and code\n\n---\n\n## Support and Further Reading\n\nFor questions about:\n- **Test selection**: See references/test_selection_guide.md\n- **Assumptions**: See references/assumptions_and_diagnostics.md\n- **Effect sizes**: See references/effect_sizes_and_power.md\n- **Bayesian methods**: See references/bayesian_statistics.md\n- **Reporting**: See references/reporting_standards.md\n\n**Key textbooks**:\n- Cohen, J. (1988). *Statistical Power Analysis for the Behavioral Sciences*\n- Field, A. (2013). *Discovering Statistics Using IBM SPSS Statistics*\n- Gelman, A., & Hill, J. (2006). *Data Analysis Using Regression and Multilevel/Hierarchical Models*\n- Kruschke, J. K. (2014). *Doing Bayesian Data Analysis*\n\n**Online resources**:\n- APA Style Guide: https://apastyle.apa.org/\n- Statistical Consulting: Cross Validated (stats.stackexchange.com)\n",
        "data/k-dense-ai/statsmodels/SKILL.md": "---\nname: statsmodels\ndescription: \"Statistical modeling toolkit. OLS, GLM, logistic, ARIMA, time series, hypothesis tests, diagnostics, AIC/BIC, for rigorous statistical inference and econometric analysis.\"\n---\n\n# Statsmodels: Statistical Modeling and Econometrics\n\n## Overview\n\nStatsmodels is Python's premier library for statistical modeling, providing tools for estimation, inference, and diagnostics across a wide range of statistical methods. Apply this skill for rigorous statistical analysis, from simple linear regression to complex time series models and econometric analyses.\n\n## When to Use This Skill\n\nThis skill should be used when:\n- Fitting regression models (OLS, WLS, GLS, quantile regression)\n- Performing generalized linear modeling (logistic, Poisson, Gamma, etc.)\n- Analyzing discrete outcomes (binary, multinomial, count, ordinal)\n- Conducting time series analysis (ARIMA, SARIMAX, VAR, forecasting)\n- Running statistical tests and diagnostics\n- Testing model assumptions (heteroskedasticity, autocorrelation, normality)\n- Detecting outliers and influential observations\n- Comparing models (AIC/BIC, likelihood ratio tests)\n- Estimating causal effects\n- Producing publication-ready statistical tables and inference\n\n## Quick Start Guide\n\n### Linear Regression (OLS)\n\n```python\nimport statsmodels.api as sm\nimport numpy as np\nimport pandas as pd\n\n# Prepare data - ALWAYS add constant for intercept\nX = sm.add_constant(X_data)\n\n# Fit OLS model\nmodel = sm.OLS(y, X)\nresults = model.fit()\n\n# View comprehensive results\nprint(results.summary())\n\n# Key results\nprint(f\"R-squared: {results.rsquared:.4f}\")\nprint(f\"Coefficients:\\\\n{results.params}\")\nprint(f\"P-values:\\\\n{results.pvalues}\")\n\n# Predictions with confidence intervals\npredictions = results.get_prediction(X_new)\npred_summary = predictions.summary_frame()\nprint(pred_summary)  # includes mean, CI, prediction intervals\n\n# Diagnostics\nfrom statsmodels.stats.diagnostic import het_breuschpagan\nbp_test = het_breuschpagan(results.resid, X)\nprint(f\"Breusch-Pagan p-value: {bp_test[1]:.4f}\")\n\n# Visualize residuals\nimport matplotlib.pyplot as plt\nplt.scatter(results.fittedvalues, results.resid)\nplt.axhline(y=0, color='r', linestyle='--')\nplt.xlabel('Fitted values')\nplt.ylabel('Residuals')\nplt.show()\n```\n\n### Logistic Regression (Binary Outcomes)\n\n```python\nfrom statsmodels.discrete.discrete_model import Logit\n\n# Add constant\nX = sm.add_constant(X_data)\n\n# Fit logit model\nmodel = Logit(y_binary, X)\nresults = model.fit()\n\nprint(results.summary())\n\n# Odds ratios\nodds_ratios = np.exp(results.params)\nprint(\"Odds ratios:\\\\n\", odds_ratios)\n\n# Predicted probabilities\nprobs = results.predict(X)\n\n# Binary predictions (0.5 threshold)\npredictions = (probs > 0.5).astype(int)\n\n# Model evaluation\nfrom sklearn.metrics import classification_report, roc_auc_score\n\nprint(classification_report(y_binary, predictions))\nprint(f\"AUC: {roc_auc_score(y_binary, probs):.4f}\")\n\n# Marginal effects\nmarginal = results.get_margeff()\nprint(marginal.summary())\n```\n\n### Time Series (ARIMA)\n\n```python\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\n# Check stationarity\nfrom statsmodels.tsa.stattools import adfuller\n\nadf_result = adfuller(y_series)\nprint(f\"ADF p-value: {adf_result[1]:.4f}\")\n\nif adf_result[1] > 0.05:\n    # Series is non-stationary, difference it\n    y_diff = y_series.diff().dropna()\n\n# Plot ACF/PACF to identify p, q\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\nplot_acf(y_diff, lags=40, ax=ax1)\nplot_pacf(y_diff, lags=40, ax=ax2)\nplt.show()\n\n# Fit ARIMA(p,d,q)\nmodel = ARIMA(y_series, order=(1, 1, 1))\nresults = model.fit()\n\nprint(results.summary())\n\n# Forecast\nforecast = results.forecast(steps=10)\nforecast_obj = results.get_forecast(steps=10)\nforecast_df = forecast_obj.summary_frame()\n\nprint(forecast_df)  # includes mean and confidence intervals\n\n# Residual diagnostics\nresults.plot_diagnostics(figsize=(12, 8))\nplt.show()\n```\n\n### Generalized Linear Models (GLM)\n\n```python\nimport statsmodels.api as sm\n\n# Poisson regression for count data\nX = sm.add_constant(X_data)\nmodel = sm.GLM(y_counts, X, family=sm.families.Poisson())\nresults = model.fit()\n\nprint(results.summary())\n\n# Rate ratios (for Poisson with log link)\nrate_ratios = np.exp(results.params)\nprint(\"Rate ratios:\\\\n\", rate_ratios)\n\n# Check overdispersion\noverdispersion = results.pearson_chi2 / results.df_resid\nprint(f\"Overdispersion: {overdispersion:.2f}\")\n\nif overdispersion > 1.5:\n    # Use Negative Binomial instead\n    from statsmodels.discrete.count_model import NegativeBinomial\n    nb_model = NegativeBinomial(y_counts, X)\n    nb_results = nb_model.fit()\n    print(nb_results.summary())\n```\n\n## Core Statistical Modeling Capabilities\n\n### 1. Linear Regression Models\n\nComprehensive suite of linear models for continuous outcomes with various error structures.\n\n**Available models:**\n- **OLS**: Standard linear regression with i.i.d. errors\n- **WLS**: Weighted least squares for heteroskedastic errors\n- **GLS**: Generalized least squares for arbitrary covariance structure\n- **GLSAR**: GLS with autoregressive errors for time series\n- **Quantile Regression**: Conditional quantiles (robust to outliers)\n- **Mixed Effects**: Hierarchical/multilevel models with random effects\n- **Recursive/Rolling**: Time-varying parameter estimation\n\n**Key features:**\n- Comprehensive diagnostic tests\n- Robust standard errors (HC, HAC, cluster-robust)\n- Influence statistics (Cook's distance, leverage, DFFITS)\n- Hypothesis testing (F-tests, Wald tests)\n- Model comparison (AIC, BIC, likelihood ratio tests)\n- Prediction with confidence and prediction intervals\n\n**When to use:** Continuous outcome variable, want inference on coefficients, need diagnostics\n\n**Reference:** See `references/linear_models.md` for detailed guidance on model selection, diagnostics, and best practices.\n\n### 2. Generalized Linear Models (GLM)\n\nFlexible framework extending linear models to non-normal distributions.\n\n**Distribution families:**\n- **Binomial**: Binary outcomes or proportions (logistic regression)\n- **Poisson**: Count data\n- **Negative Binomial**: Overdispersed counts\n- **Gamma**: Positive continuous, right-skewed data\n- **Inverse Gaussian**: Positive continuous with specific variance structure\n- **Gaussian**: Equivalent to OLS\n- **Tweedie**: Flexible family for semi-continuous data\n\n**Link functions:**\n- Logit, Probit, Log, Identity, Inverse, Sqrt, CLogLog, Power\n- Choose based on interpretation needs and model fit\n\n**Key features:**\n- Maximum likelihood estimation via IRLS\n- Deviance and Pearson residuals\n- Goodness-of-fit statistics\n- Pseudo R-squared measures\n- Robust standard errors\n\n**When to use:** Non-normal outcomes, need flexible variance and link specifications\n\n**Reference:** See `references/glm.md` for family selection, link functions, interpretation, and diagnostics.\n\n### 3. Discrete Choice Models\n\nModels for categorical and count outcomes.\n\n**Binary models:**\n- **Logit**: Logistic regression (odds ratios)\n- **Probit**: Probit regression (normal distribution)\n\n**Multinomial models:**\n- **MNLogit**: Unordered categories (3+ levels)\n- **Conditional Logit**: Choice models with alternative-specific variables\n- **Ordered Model**: Ordinal outcomes (ordered categories)\n\n**Count models:**\n- **Poisson**: Standard count model\n- **Negative Binomial**: Overdispersed counts\n- **Zero-Inflated**: Excess zeros (ZIP, ZINB)\n- **Hurdle Models**: Two-stage models for zero-heavy data\n\n**Key features:**\n- Maximum likelihood estimation\n- Marginal effects at means or average marginal effects\n- Model comparison via AIC/BIC\n- Predicted probabilities and classification\n- Goodness-of-fit tests\n\n**When to use:** Binary, categorical, or count outcomes\n\n**Reference:** See `references/discrete_choice.md` for model selection, interpretation, and evaluation.\n\n### 4. Time Series Analysis\n\nComprehensive time series modeling and forecasting capabilities.\n\n**Univariate models:**\n- **AutoReg (AR)**: Autoregressive models\n- **ARIMA**: Autoregressive integrated moving average\n- **SARIMAX**: Seasonal ARIMA with exogenous variables\n- **Exponential Smoothing**: Simple, Holt, Holt-Winters\n- **ETS**: Innovations state space models\n\n**Multivariate models:**\n- **VAR**: Vector autoregression\n- **VARMAX**: VAR with MA and exogenous variables\n- **Dynamic Factor Models**: Extract common factors\n- **VECM**: Vector error correction models (cointegration)\n\n**Advanced models:**\n- **State Space**: Kalman filtering, custom specifications\n- **Regime Switching**: Markov switching models\n- **ARDL**: Autoregressive distributed lag\n\n**Key features:**\n- ACF/PACF analysis for model identification\n- Stationarity tests (ADF, KPSS)\n- Forecasting with prediction intervals\n- Residual diagnostics (Ljung-Box, heteroskedasticity)\n- Granger causality testing\n- Impulse response functions (IRF)\n- Forecast error variance decomposition (FEVD)\n\n**When to use:** Time-ordered data, forecasting, understanding temporal dynamics\n\n**Reference:** See `references/time_series.md` for model selection, diagnostics, and forecasting methods.\n\n### 5. Statistical Tests and Diagnostics\n\nExtensive testing and diagnostic capabilities for model validation.\n\n**Residual diagnostics:**\n- Autocorrelation tests (Ljung-Box, Durbin-Watson, Breusch-Godfrey)\n- Heteroskedasticity tests (Breusch-Pagan, White, ARCH)\n- Normality tests (Jarque-Bera, Omnibus, Anderson-Darling, Lilliefors)\n- Specification tests (RESET, Harvey-Collier)\n\n**Influence and outliers:**\n- Leverage (hat values)\n- Cook's distance\n- DFFITS and DFBETAs\n- Studentized residuals\n- Influence plots\n\n**Hypothesis testing:**\n- t-tests (one-sample, two-sample, paired)\n- Proportion tests\n- Chi-square tests\n- Non-parametric tests (Mann-Whitney, Wilcoxon, Kruskal-Wallis)\n- ANOVA (one-way, two-way, repeated measures)\n\n**Multiple comparisons:**\n- Tukey's HSD\n- Bonferroni correction\n- False Discovery Rate (FDR)\n\n**Effect sizes and power:**\n- Cohen's d, eta-squared\n- Power analysis for t-tests, proportions\n- Sample size calculations\n\n**Robust inference:**\n- Heteroskedasticity-consistent SEs (HC0-HC3)\n- HAC standard errors (Newey-West)\n- Cluster-robust standard errors\n\n**When to use:** Validating assumptions, detecting problems, ensuring robust inference\n\n**Reference:** See `references/stats_diagnostics.md` for comprehensive testing and diagnostic procedures.\n\n## Formula API (R-style)\n\nStatsmodels supports R-style formulas for intuitive model specification:\n\n```python\nimport statsmodels.formula.api as smf\n\n# OLS with formula\nresults = smf.ols('y ~ x1 + x2 + x1:x2', data=df).fit()\n\n# Categorical variables (automatic dummy coding)\nresults = smf.ols('y ~ x1 + C(category)', data=df).fit()\n\n# Interactions\nresults = smf.ols('y ~ x1 * x2', data=df).fit()  # x1 + x2 + x1:x2\n\n# Polynomial terms\nresults = smf.ols('y ~ x + I(x**2)', data=df).fit()\n\n# Logit\nresults = smf.logit('y ~ x1 + x2 + C(group)', data=df).fit()\n\n# Poisson\nresults = smf.poisson('count ~ x1 + x2', data=df).fit()\n\n# ARIMA (not available via formula, use regular API)\n```\n\n## Model Selection and Comparison\n\n### Information Criteria\n\n```python\n# Compare models using AIC/BIC\nmodels = {\n    'Model 1': model1_results,\n    'Model 2': model2_results,\n    'Model 3': model3_results\n}\n\ncomparison = pd.DataFrame({\n    'AIC': {name: res.aic for name, res in models.items()},\n    'BIC': {name: res.bic for name, res in models.items()},\n    'Log-Likelihood': {name: res.llf for name, res in models.items()}\n})\n\nprint(comparison.sort_values('AIC'))\n# Lower AIC/BIC indicates better model\n```\n\n### Likelihood Ratio Test (Nested Models)\n\n```python\n# For nested models (one is subset of the other)\nfrom scipy import stats\n\nlr_stat = 2 * (full_model.llf - reduced_model.llf)\ndf = full_model.df_model - reduced_model.df_model\np_value = 1 - stats.chi2.cdf(lr_stat, df)\n\nprint(f\"LR statistic: {lr_stat:.4f}\")\nprint(f\"p-value: {p_value:.4f}\")\n\nif p_value < 0.05:\n    print(\"Full model significantly better\")\nelse:\n    print(\"Reduced model preferred (parsimony)\")\n```\n\n### Cross-Validation\n\n```python\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\n\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\ncv_scores = []\n\nfor train_idx, val_idx in kf.split(X):\n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n\n    # Fit model\n    model = sm.OLS(y_train, X_train).fit()\n\n    # Predict\n    y_pred = model.predict(X_val)\n\n    # Score\n    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n    cv_scores.append(rmse)\n\nprint(f\"CV RMSE: {np.mean(cv_scores):.4f}  {np.std(cv_scores):.4f}\")\n```\n\n## Best Practices\n\n### Data Preparation\n\n1. **Always add constant**: Use `sm.add_constant()` unless excluding intercept\n2. **Check for missing values**: Handle or impute before fitting\n3. **Scale if needed**: Improves convergence, interpretation (but not required for tree models)\n4. **Encode categoricals**: Use formula API or manual dummy coding\n\n### Model Building\n\n1. **Start simple**: Begin with basic model, add complexity as needed\n2. **Check assumptions**: Test residuals, heteroskedasticity, autocorrelation\n3. **Use appropriate model**: Match model to outcome type (binaryLogit, countPoisson)\n4. **Consider alternatives**: If assumptions violated, use robust methods or different model\n\n### Inference\n\n1. **Report effect sizes**: Not just p-values\n2. **Use robust SEs**: When heteroskedasticity or clustering present\n3. **Multiple comparisons**: Correct when testing many hypotheses\n4. **Confidence intervals**: Always report alongside point estimates\n\n### Model Evaluation\n\n1. **Check residuals**: Plot residuals vs fitted, Q-Q plot\n2. **Influence diagnostics**: Identify and investigate influential observations\n3. **Out-of-sample validation**: Test on holdout set or cross-validate\n4. **Compare models**: Use AIC/BIC for non-nested, LR test for nested\n\n### Reporting\n\n1. **Comprehensive summary**: Use `.summary()` for detailed output\n2. **Document decisions**: Note transformations, excluded observations\n3. **Interpret carefully**: Account for link functions (e.g., exp() for log link)\n4. **Visualize**: Plot predictions, confidence intervals, diagnostics\n\n## Common Workflows\n\n### Workflow 1: Linear Regression Analysis\n\n1. Explore data (plots, descriptives)\n2. Fit initial OLS model\n3. Check residual diagnostics\n4. Test for heteroskedasticity, autocorrelation\n5. Check for multicollinearity (VIF)\n6. Identify influential observations\n7. Refit with robust SEs if needed\n8. Interpret coefficients and inference\n9. Validate on holdout or via CV\n\n### Workflow 2: Binary Classification\n\n1. Fit logistic regression (Logit)\n2. Check for convergence issues\n3. Interpret odds ratios\n4. Calculate marginal effects\n5. Evaluate classification performance (AUC, confusion matrix)\n6. Check for influential observations\n7. Compare with alternative models (Probit)\n8. Validate predictions on test set\n\n### Workflow 3: Count Data Analysis\n\n1. Fit Poisson regression\n2. Check for overdispersion\n3. If overdispersed, fit Negative Binomial\n4. Check for excess zeros (consider ZIP/ZINB)\n5. Interpret rate ratios\n6. Assess goodness of fit\n7. Compare models via AIC\n8. Validate predictions\n\n### Workflow 4: Time Series Forecasting\n\n1. Plot series, check for trend/seasonality\n2. Test for stationarity (ADF, KPSS)\n3. Difference if non-stationary\n4. Identify p, q from ACF/PACF\n5. Fit ARIMA or SARIMAX\n6. Check residual diagnostics (Ljung-Box)\n7. Generate forecasts with confidence intervals\n8. Evaluate forecast accuracy on test set\n\n## Reference Documentation\n\nThis skill includes comprehensive reference files for detailed guidance:\n\n### references/linear_models.md\nDetailed coverage of linear regression models including:\n- OLS, WLS, GLS, GLSAR, Quantile Regression\n- Mixed effects models\n- Recursive and rolling regression\n- Comprehensive diagnostics (heteroskedasticity, autocorrelation, multicollinearity)\n- Influence statistics and outlier detection\n- Robust standard errors (HC, HAC, cluster)\n- Hypothesis testing and model comparison\n\n### references/glm.md\nComplete guide to generalized linear models:\n- All distribution families (Binomial, Poisson, Gamma, etc.)\n- Link functions and when to use each\n- Model fitting and interpretation\n- Pseudo R-squared and goodness of fit\n- Diagnostics and residual analysis\n- Applications (logistic, Poisson, Gamma regression)\n\n### references/discrete_choice.md\nComprehensive guide to discrete outcome models:\n- Binary models (Logit, Probit)\n- Multinomial models (MNLogit, Conditional Logit)\n- Count models (Poisson, Negative Binomial, Zero-Inflated, Hurdle)\n- Ordinal models\n- Marginal effects and interpretation\n- Model diagnostics and comparison\n\n### references/time_series.md\nIn-depth time series analysis guidance:\n- Univariate models (AR, ARIMA, SARIMAX, Exponential Smoothing)\n- Multivariate models (VAR, VARMAX, Dynamic Factor)\n- State space models\n- Stationarity testing and diagnostics\n- Forecasting methods and evaluation\n- Granger causality, IRF, FEVD\n\n### references/stats_diagnostics.md\nComprehensive statistical testing and diagnostics:\n- Residual diagnostics (autocorrelation, heteroskedasticity, normality)\n- Influence and outlier detection\n- Hypothesis tests (parametric and non-parametric)\n- ANOVA and post-hoc tests\n- Multiple comparisons correction\n- Robust covariance matrices\n- Power analysis and effect sizes\n\n**When to reference:**\n- Need detailed parameter explanations\n- Choosing between similar models\n- Troubleshooting convergence or diagnostic issues\n- Understanding specific test statistics\n- Looking for code examples for advanced features\n\n**Search patterns:**\n```bash\n# Find information about specific models\ngrep -r \"Quantile Regression\" references/\n\n# Find diagnostic tests\ngrep -r \"Breusch-Pagan\" references/stats_diagnostics.md\n\n# Find time series guidance\ngrep -r \"SARIMAX\" references/time_series.md\n```\n\n## Common Pitfalls to Avoid\n\n1. **Forgetting constant term**: Always use `sm.add_constant()` unless no intercept desired\n2. **Ignoring assumptions**: Check residuals, heteroskedasticity, autocorrelation\n3. **Wrong model for outcome type**: BinaryLogit/Probit, CountPoisson/NB, not OLS\n4. **Not checking convergence**: Look for optimization warnings\n5. **Misinterpreting coefficients**: Remember link functions (log, logit, etc.)\n6. **Using Poisson with overdispersion**: Check dispersion, use Negative Binomial if needed\n7. **Not using robust SEs**: When heteroskedasticity or clustering present\n8. **Overfitting**: Too many parameters relative to sample size\n9. **Data leakage**: Fitting on test data or using future information\n10. **Not validating predictions**: Always check out-of-sample performance\n11. **Comparing non-nested models**: Use AIC/BIC, not LR test\n12. **Ignoring influential observations**: Check Cook's distance and leverage\n13. **Multiple testing**: Correct p-values when testing many hypotheses\n14. **Not differencing time series**: Fit ARIMA on non-stationary data\n15. **Confusing prediction vs confidence intervals**: Prediction intervals are wider\n\n## Getting Help\n\nFor detailed documentation and examples:\n- Official docs: https://www.statsmodels.org/stable/\n- User guide: https://www.statsmodels.org/stable/user-guide.html\n- Examples: https://www.statsmodels.org/stable/examples/index.html\n- API reference: https://www.statsmodels.org/stable/api.html\n",
        "data/k-dense-ai/string-database/SKILL.md": "---\nname: string-database\ndescription: \"Query STRING API for protein-protein interactions (59M proteins, 20B interactions). Network analysis, GO/KEGG enrichment, interaction discovery, 5000+ species, for systems biology.\"\n---\n\n# STRING Database\n\n## Overview\n\nSTRING is a comprehensive database of known and predicted protein-protein interactions covering 59M proteins and 20B+ interactions across 5000+ organisms. Query interaction networks, perform functional enrichment, discover partners via REST API for systems biology and pathway analysis.\n\n## When to Use This Skill\n\nThis skill should be used when:\n- Retrieving protein-protein interaction networks for single or multiple proteins\n- Performing functional enrichment analysis (GO, KEGG, Pfam) on protein lists\n- Discovering interaction partners and expanding protein networks\n- Testing if proteins form significantly enriched functional modules\n- Generating network visualizations with evidence-based coloring\n- Analyzing homology and protein family relationships\n- Conducting cross-species protein interaction comparisons\n- Identifying hub proteins and network connectivity patterns\n\n## Quick Start\n\nThe skill provides:\n1. Python helper functions (`scripts/string_api.py`) for all STRING REST API operations\n2. Comprehensive reference documentation (`references/string_reference.md`) with detailed API specifications\n\nWhen users request STRING data, determine which operation is needed and use the appropriate function from `scripts/string_api.py`.\n\n## Core Operations\n\n### 1. Identifier Mapping (`string_map_ids`)\n\nConvert gene names, protein names, and external IDs to STRING identifiers.\n\n**When to use**: Starting any STRING analysis, validating protein names, finding canonical identifiers.\n\n**Usage**:\n```python\nfrom scripts.string_api import string_map_ids\n\n# Map single protein\nresult = string_map_ids('TP53', species=9606)\n\n# Map multiple proteins\nresult = string_map_ids(['TP53', 'BRCA1', 'EGFR', 'MDM2'], species=9606)\n\n# Map with multiple matches per query\nresult = string_map_ids('p53', species=9606, limit=5)\n```\n\n**Parameters**:\n- `species`: NCBI taxon ID (9606 = human, 10090 = mouse, 7227 = fly)\n- `limit`: Number of matches per identifier (default: 1)\n- `echo_query`: Include query term in output (default: 1)\n\n**Best practice**: Always map identifiers first for faster subsequent queries.\n\n### 2. Network Retrieval (`string_network`)\n\nGet protein-protein interaction network data in tabular format.\n\n**When to use**: Building interaction networks, analyzing connectivity, retrieving interaction evidence.\n\n**Usage**:\n```python\nfrom scripts.string_api import string_network\n\n# Get network for single protein\nnetwork = string_network('9606.ENSP00000269305', species=9606)\n\n# Get network with multiple proteins\nproteins = ['9606.ENSP00000269305', '9606.ENSP00000275493']\nnetwork = string_network(proteins, required_score=700)\n\n# Expand network with additional interactors\nnetwork = string_network('TP53', species=9606, add_nodes=10, required_score=400)\n\n# Physical interactions only\nnetwork = string_network('TP53', species=9606, network_type='physical')\n```\n\n**Parameters**:\n- `required_score`: Confidence threshold (0-1000)\n  - 150: low confidence (exploratory)\n  - 400: medium confidence (default, standard analysis)\n  - 700: high confidence (conservative)\n  - 900: highest confidence (very stringent)\n- `network_type`: `'functional'` (all evidence, default) or `'physical'` (direct binding only)\n- `add_nodes`: Add N most connected proteins (0-10)\n\n**Output columns**: Interaction pairs, confidence scores, and individual evidence scores (neighborhood, fusion, coexpression, experimental, database, text-mining).\n\n### 3. Network Visualization (`string_network_image`)\n\nGenerate network visualization as PNG image.\n\n**When to use**: Creating figures, visual exploration, presentations.\n\n**Usage**:\n```python\nfrom scripts.string_api import string_network_image\n\n# Get network image\nproteins = ['TP53', 'MDM2', 'ATM', 'CHEK2', 'BRCA1']\nimg_data = string_network_image(proteins, species=9606, required_score=700)\n\n# Save image\nwith open('network.png', 'wb') as f:\n    f.write(img_data)\n\n# Evidence-colored network\nimg = string_network_image(proteins, species=9606, network_flavor='evidence')\n\n# Confidence-based visualization\nimg = string_network_image(proteins, species=9606, network_flavor='confidence')\n\n# Actions network (activation/inhibition)\nimg = string_network_image(proteins, species=9606, network_flavor='actions')\n```\n\n**Network flavors**:\n- `'evidence'`: Colored lines show evidence types (default)\n- `'confidence'`: Line thickness represents confidence\n- `'actions'`: Shows activating/inhibiting relationships\n\n### 4. Interaction Partners (`string_interaction_partners`)\n\nFind all proteins that interact with given protein(s).\n\n**When to use**: Discovering novel interactions, finding hub proteins, expanding networks.\n\n**Usage**:\n```python\nfrom scripts.string_api import string_interaction_partners\n\n# Get top 10 interactors of TP53\npartners = string_interaction_partners('TP53', species=9606, limit=10)\n\n# Get high-confidence interactors\npartners = string_interaction_partners('TP53', species=9606,\n                                      limit=20, required_score=700)\n\n# Find interactors for multiple proteins\npartners = string_interaction_partners(['TP53', 'MDM2'],\n                                      species=9606, limit=15)\n```\n\n**Parameters**:\n- `limit`: Maximum number of partners to return (default: 10)\n- `required_score`: Confidence threshold (0-1000)\n\n**Use cases**:\n- Hub protein identification\n- Network expansion from seed proteins\n- Discovering indirect connections\n\n### 5. Functional Enrichment (`string_enrichment`)\n\nPerform enrichment analysis across Gene Ontology, KEGG pathways, Pfam domains, and more.\n\n**When to use**: Interpreting protein lists, pathway analysis, functional characterization, understanding biological processes.\n\n**Usage**:\n```python\nfrom scripts.string_enrichment import string_enrichment\n\n# Enrichment for a protein list\nproteins = ['TP53', 'MDM2', 'ATM', 'CHEK2', 'BRCA1', 'ATR', 'TP73']\nenrichment = string_enrichment(proteins, species=9606)\n\n# Parse results to find significant terms\nimport pandas as pd\ndf = pd.read_csv(io.StringIO(enrichment), sep='\\t')\nsignificant = df[df['fdr'] < 0.05]\n```\n\n**Enrichment categories**:\n- **Gene Ontology**: Biological Process, Molecular Function, Cellular Component\n- **KEGG Pathways**: Metabolic and signaling pathways\n- **Pfam**: Protein domains\n- **InterPro**: Protein families and domains\n- **SMART**: Domain architecture\n- **UniProt Keywords**: Curated functional keywords\n\n**Output columns**:\n- `category`: Annotation database (e.g., \"KEGG Pathways\", \"GO Biological Process\")\n- `term`: Term identifier\n- `description`: Human-readable term description\n- `number_of_genes`: Input proteins with this annotation\n- `p_value`: Uncorrected enrichment p-value\n- `fdr`: False discovery rate (corrected p-value)\n\n**Statistical method**: Fisher's exact test with Benjamini-Hochberg FDR correction.\n\n**Interpretation**: FDR < 0.05 indicates statistically significant enrichment.\n\n### 6. PPI Enrichment (`string_ppi_enrichment`)\n\nTest if a protein network has significantly more interactions than expected by chance.\n\n**When to use**: Validating if proteins form functional module, testing network connectivity.\n\n**Usage**:\n```python\nfrom scripts.string_api import string_ppi_enrichment\nimport json\n\n# Test network connectivity\nproteins = ['TP53', 'MDM2', 'ATM', 'CHEK2', 'BRCA1']\nresult = string_ppi_enrichment(proteins, species=9606, required_score=400)\n\n# Parse JSON result\ndata = json.loads(result)\nprint(f\"Observed edges: {data['number_of_edges']}\")\nprint(f\"Expected edges: {data['expected_number_of_edges']}\")\nprint(f\"P-value: {data['p_value']}\")\n```\n\n**Output fields**:\n- `number_of_nodes`: Proteins in network\n- `number_of_edges`: Observed interactions\n- `expected_number_of_edges`: Expected in random network\n- `p_value`: Statistical significance\n\n**Interpretation**:\n- p-value < 0.05: Network is significantly enriched (proteins likely form functional module)\n- p-value  0.05: No significant enrichment (proteins may be unrelated)\n\n### 7. Homology Scores (`string_homology`)\n\nRetrieve protein similarity and homology information.\n\n**When to use**: Identifying protein families, paralog analysis, cross-species comparisons.\n\n**Usage**:\n```python\nfrom scripts.string_api import string_homology\n\n# Get homology between proteins\nproteins = ['TP53', 'TP63', 'TP73']  # p53 family\nhomology = string_homology(proteins, species=9606)\n```\n\n**Use cases**:\n- Protein family identification\n- Paralog discovery\n- Evolutionary analysis\n\n### 8. Version Information (`string_version`)\n\nGet current STRING database version.\n\n**When to use**: Ensuring reproducibility, documenting methods.\n\n**Usage**:\n```python\nfrom scripts.string_api import string_version\n\nversion = string_version()\nprint(f\"STRING version: {version}\")\n```\n\n## Common Analysis Workflows\n\n### Workflow 1: Protein List Analysis (Standard Workflow)\n\n**Use case**: Analyze a list of proteins from experiment (e.g., differential expression, proteomics).\n\n```python\nfrom scripts.string_api import (string_map_ids, string_network,\n                                string_enrichment, string_ppi_enrichment,\n                                string_network_image)\n\n# Step 1: Map gene names to STRING IDs\ngene_list = ['TP53', 'BRCA1', 'ATM', 'CHEK2', 'MDM2', 'ATR', 'BRCA2']\nmapping = string_map_ids(gene_list, species=9606)\n\n# Step 2: Get interaction network\nnetwork = string_network(gene_list, species=9606, required_score=400)\n\n# Step 3: Test if network is enriched\nppi_result = string_ppi_enrichment(gene_list, species=9606)\n\n# Step 4: Perform functional enrichment\nenrichment = string_enrichment(gene_list, species=9606)\n\n# Step 5: Generate network visualization\nimg = string_network_image(gene_list, species=9606,\n                          network_flavor='evidence', required_score=400)\nwith open('protein_network.png', 'wb') as f:\n    f.write(img)\n\n# Step 6: Parse and interpret results\n```\n\n### Workflow 2: Single Protein Investigation\n\n**Use case**: Deep dive into one protein's interactions and partners.\n\n```python\nfrom scripts.string_api import (string_map_ids, string_interaction_partners,\n                                string_network_image)\n\n# Step 1: Map protein name\nprotein = 'TP53'\nmapping = string_map_ids(protein, species=9606)\n\n# Step 2: Get all interaction partners\npartners = string_interaction_partners(protein, species=9606,\n                                      limit=20, required_score=700)\n\n# Step 3: Visualize expanded network\nimg = string_network_image(protein, species=9606, add_nodes=15,\n                          network_flavor='confidence', required_score=700)\nwith open('tp53_network.png', 'wb') as f:\n    f.write(img)\n```\n\n### Workflow 3: Pathway-Centric Analysis\n\n**Use case**: Identify and visualize proteins in a specific biological pathway.\n\n```python\nfrom scripts.string_api import string_enrichment, string_network\n\n# Step 1: Start with known pathway proteins\ndna_repair_proteins = ['TP53', 'ATM', 'ATR', 'CHEK1', 'CHEK2',\n                       'BRCA1', 'BRCA2', 'RAD51', 'XRCC1']\n\n# Step 2: Get network\nnetwork = string_network(dna_repair_proteins, species=9606,\n                        required_score=700, add_nodes=5)\n\n# Step 3: Enrichment to confirm pathway annotation\nenrichment = string_enrichment(dna_repair_proteins, species=9606)\n\n# Step 4: Parse enrichment for DNA repair pathways\nimport pandas as pd\nimport io\ndf = pd.read_csv(io.StringIO(enrichment), sep='\\t')\ndna_repair = df[df['description'].str.contains('DNA repair', case=False)]\n```\n\n### Workflow 4: Cross-Species Analysis\n\n**Use case**: Compare protein interactions across different organisms.\n\n```python\nfrom scripts.string_api import string_network\n\n# Human network\nhuman_network = string_network('TP53', species=9606, required_score=700)\n\n# Mouse network\nmouse_network = string_network('Trp53', species=10090, required_score=700)\n\n# Yeast network (if ortholog exists)\nyeast_network = string_network('gene_name', species=4932, required_score=700)\n```\n\n### Workflow 5: Network Expansion and Discovery\n\n**Use case**: Start with seed proteins and discover connected functional modules.\n\n```python\nfrom scripts.string_api import (string_interaction_partners, string_network,\n                                string_enrichment)\n\n# Step 1: Start with seed protein(s)\nseed_proteins = ['TP53']\n\n# Step 2: Get first-degree interactors\npartners = string_interaction_partners(seed_proteins, species=9606,\n                                      limit=30, required_score=700)\n\n# Step 3: Parse partners to get protein list\nimport pandas as pd\nimport io\ndf = pd.read_csv(io.StringIO(partners), sep='\\t')\nall_proteins = list(set(df['preferredName_A'].tolist() +\n                       df['preferredName_B'].tolist()))\n\n# Step 4: Perform enrichment on expanded network\nenrichment = string_enrichment(all_proteins[:50], species=9606)\n\n# Step 5: Filter for interesting functional modules\nenrichment_df = pd.read_csv(io.StringIO(enrichment), sep='\\t')\nmodules = enrichment_df[enrichment_df['fdr'] < 0.001]\n```\n\n## Common Species\n\nWhen specifying species, use NCBI taxon IDs:\n\n| Organism | Common Name | Taxon ID |\n|----------|-------------|----------|\n| Homo sapiens | Human | 9606 |\n| Mus musculus | Mouse | 10090 |\n| Rattus norvegicus | Rat | 10116 |\n| Drosophila melanogaster | Fruit fly | 7227 |\n| Caenorhabditis elegans | C. elegans | 6239 |\n| Saccharomyces cerevisiae | Yeast | 4932 |\n| Arabidopsis thaliana | Thale cress | 3702 |\n| Escherichia coli | E. coli | 511145 |\n| Danio rerio | Zebrafish | 7955 |\n\nFull list available at: https://string-db.org/cgi/input?input_page_active_form=organisms\n\n## Understanding Confidence Scores\n\nSTRING provides combined confidence scores (0-1000) integrating multiple evidence types:\n\n### Evidence Channels\n\n1. **Neighborhood (nscore)**: Conserved genomic neighborhood across species\n2. **Fusion (fscore)**: Gene fusion events\n3. **Phylogenetic Profile (pscore)**: Co-occurrence patterns across species\n4. **Coexpression (ascore)**: Correlated RNA expression\n5. **Experimental (escore)**: Biochemical and genetic experiments\n6. **Database (dscore)**: Curated pathway and complex databases\n7. **Text-mining (tscore)**: Literature co-occurrence and NLP extraction\n\n### Recommended Thresholds\n\nChoose threshold based on analysis goals:\n\n- **150 (low confidence)**: Exploratory analysis, hypothesis generation\n- **400 (medium confidence)**: Standard analysis, balanced sensitivity/specificity\n- **700 (high confidence)**: Conservative analysis, high-confidence interactions\n- **900 (highest confidence)**: Very stringent, experimental evidence preferred\n\n**Trade-offs**:\n- Lower thresholds: More interactions (higher recall, more false positives)\n- Higher thresholds: Fewer interactions (higher precision, more false negatives)\n\n## Network Types\n\n### Functional Networks (Default)\n\nIncludes all evidence types (experimental, computational, text-mining). Represents proteins that are functionally associated, even without direct physical binding.\n\n**When to use**:\n- Pathway analysis\n- Functional enrichment studies\n- Systems biology\n- Most general analyses\n\n### Physical Networks\n\nOnly includes evidence for direct physical binding (experimental data and database annotations for physical interactions).\n\n**When to use**:\n- Structural biology studies\n- Protein complex analysis\n- Direct binding validation\n- When physical contact is required\n\n## API Best Practices\n\n1. **Always map identifiers first**: Use `string_map_ids()` before other operations for faster queries\n2. **Use STRING IDs when possible**: Use format `9606.ENSP00000269305` instead of gene names\n3. **Specify species for networks >10 proteins**: Required for accurate results\n4. **Respect rate limits**: Wait 1 second between API calls\n5. **Use versioned URLs for reproducibility**: Available in reference documentation\n6. **Handle errors gracefully**: Check for \"Error:\" prefix in returned strings\n7. **Choose appropriate confidence thresholds**: Match threshold to analysis goals\n\n## Detailed Reference\n\nFor comprehensive API documentation, complete parameter lists, output formats, and advanced usage, refer to `references/string_reference.md`. This includes:\n\n- Complete API endpoint specifications\n- All supported output formats (TSV, JSON, XML, PSI-MI)\n- Advanced features (bulk upload, values/ranks enrichment)\n- Error handling and troubleshooting\n- Integration with other tools (Cytoscape, R, Python libraries)\n- Data license and citation information\n\n## Troubleshooting\n\n**No proteins found**:\n- Verify species parameter matches identifiers\n- Try mapping identifiers first with `string_map_ids()`\n- Check for typos in protein names\n\n**Empty network results**:\n- Lower confidence threshold (`required_score`)\n- Check if proteins actually interact\n- Verify species is correct\n\n**Timeout or slow queries**:\n- Reduce number of input proteins\n- Use STRING IDs instead of gene names\n- Split large queries into batches\n\n**\"Species required\" error**:\n- Add `species` parameter for networks with >10 proteins\n- Always include species for consistency\n\n**Results look unexpected**:\n- Check STRING version with `string_version()`\n- Verify network_type is appropriate (functional vs physical)\n- Review confidence threshold selection\n\n## Additional Resources\n\nFor proteome-scale analysis or complete species network upload:\n- Visit https://string-db.org\n- Use \"Upload proteome\" feature\n- STRING will generate complete interaction network and predict functions\n\nFor bulk downloads of complete datasets:\n- Download page: https://string-db.org/cgi/download\n- Includes complete interaction files, protein annotations, and pathway mappings\n\n## Data License\n\nSTRING data is freely available under **Creative Commons BY 4.0** license:\n- Free for academic and commercial use\n- Attribution required when publishing\n- Cite latest STRING publication\n\n## Citation\n\nWhen using STRING in publications, cite the most recent publication from: https://string-db.org/cgi/about\n",
        "data/k-dense-ai/sympy/SKILL.md": "---\nname: sympy\ndescription: Use this skill when working with symbolic mathematics in Python. This skill should be used for symbolic computation tasks including solving equations algebraically, performing calculus operations (derivatives, integrals, limits), manipulating algebraic expressions, working with matrices symbolically, physics calculations, number theory problems, geometry computations, and generating executable code from mathematical expressions. Apply this skill when the user needs exact symbolic results rather than numerical approximations, or when working with mathematical formulas that contain variables and parameters.\n---\n\n# SymPy - Symbolic Mathematics in Python\n\n## Overview\n\nSymPy is a Python library for symbolic mathematics that enables exact computation using mathematical symbols rather than numerical approximations. This skill provides comprehensive guidance for performing symbolic algebra, calculus, linear algebra, equation solving, physics calculations, and code generation using SymPy.\n\n## When to Use This Skill\n\nUse this skill when:\n- Solving equations symbolically (algebraic, differential, systems of equations)\n- Performing calculus operations (derivatives, integrals, limits, series)\n- Manipulating and simplifying algebraic expressions\n- Working with matrices and linear algebra symbolically\n- Doing physics calculations (mechanics, quantum mechanics, vector analysis)\n- Number theory computations (primes, factorization, modular arithmetic)\n- Geometric calculations (2D/3D geometry, analytic geometry)\n- Converting mathematical expressions to executable code (Python, C, Fortran)\n- Generating LaTeX or other formatted mathematical output\n- Needing exact mathematical results (e.g., `sqrt(2)` not `1.414...`)\n\n## Core Capabilities\n\n### 1. Symbolic Computation Basics\n\n**Creating symbols and expressions:**\n```python\nfrom sympy import symbols, Symbol\nx, y, z = symbols('x y z')\nexpr = x**2 + 2*x + 1\n\n# With assumptions\nx = symbols('x', real=True, positive=True)\nn = symbols('n', integer=True)\n```\n\n**Simplification and manipulation:**\n```python\nfrom sympy import simplify, expand, factor, cancel\nsimplify(sin(x)**2 + cos(x)**2)  # Returns 1\nexpand((x + 1)**3)  # x**3 + 3*x**2 + 3*x + 1\nfactor(x**2 - 1)    # (x - 1)*(x + 1)\n```\n\n**For detailed basics:** See `references/core-capabilities.md`\n\n### 2. Calculus\n\n**Derivatives:**\n```python\nfrom sympy import diff\ndiff(x**2, x)        # 2*x\ndiff(x**4, x, 3)     # 24*x (third derivative)\ndiff(x**2*y**3, x, y)  # 6*x*y**2 (partial derivatives)\n```\n\n**Integrals:**\n```python\nfrom sympy import integrate, oo\nintegrate(x**2, x)              # x**3/3 (indefinite)\nintegrate(x**2, (x, 0, 1))      # 1/3 (definite)\nintegrate(exp(-x), (x, 0, oo))  # 1 (improper)\n```\n\n**Limits and Series:**\n```python\nfrom sympy import limit, series\nlimit(sin(x)/x, x, 0)  # 1\nseries(exp(x), x, 0, 6)  # 1 + x + x**2/2 + x**3/6 + x**4/24 + x**5/120 + O(x**6)\n```\n\n**For detailed calculus operations:** See `references/core-capabilities.md`\n\n### 3. Equation Solving\n\n**Algebraic equations:**\n```python\nfrom sympy import solveset, solve, Eq\nsolveset(x**2 - 4, x)  # {-2, 2}\nsolve(Eq(x**2, 4), x)  # [-2, 2]\n```\n\n**Systems of equations:**\n```python\nfrom sympy import linsolve, nonlinsolve\nlinsolve([x + y - 2, x - y], x, y)  # {(1, 1)} (linear)\nnonlinsolve([x**2 + y - 2, x + y**2 - 3], x, y)  # (nonlinear)\n```\n\n**Differential equations:**\n```python\nfrom sympy import Function, dsolve, Derivative\nf = symbols('f', cls=Function)\ndsolve(Derivative(f(x), x) - f(x), f(x))  # Eq(f(x), C1*exp(x))\n```\n\n**For detailed solving methods:** See `references/core-capabilities.md`\n\n### 4. Matrices and Linear Algebra\n\n**Matrix creation and operations:**\n```python\nfrom sympy import Matrix, eye, zeros\nM = Matrix([[1, 2], [3, 4]])\nM_inv = M**-1  # Inverse\nM.det()        # Determinant\nM.T            # Transpose\n```\n\n**Eigenvalues and eigenvectors:**\n```python\neigenvals = M.eigenvals()  # {eigenvalue: multiplicity}\neigenvects = M.eigenvects()  # [(eigenval, mult, [eigenvectors])]\nP, D = M.diagonalize()  # M = P*D*P^-1\n```\n\n**Solving linear systems:**\n```python\nA = Matrix([[1, 2], [3, 4]])\nb = Matrix([5, 6])\nx = A.solve(b)  # Solve Ax = b\n```\n\n**For comprehensive linear algebra:** See `references/matrices-linear-algebra.md`\n\n### 5. Physics and Mechanics\n\n**Classical mechanics:**\n```python\nfrom sympy.physics.mechanics import dynamicsymbols, LagrangesMethod\nfrom sympy import symbols\n\n# Define system\nq = dynamicsymbols('q')\nm, g, l = symbols('m g l')\n\n# Lagrangian (T - V)\nL = m*(l*q.diff())**2/2 - m*g*l*(1 - cos(q))\n\n# Apply Lagrange's method\nLM = LagrangesMethod(L, [q])\n```\n\n**Vector analysis:**\n```python\nfrom sympy.physics.vector import ReferenceFrame, dot, cross\nN = ReferenceFrame('N')\nv1 = 3*N.x + 4*N.y\nv2 = 1*N.x + 2*N.z\ndot(v1, v2)  # Dot product\ncross(v1, v2)  # Cross product\n```\n\n**Quantum mechanics:**\n```python\nfrom sympy.physics.quantum import Ket, Bra, Commutator\npsi = Ket('psi')\nA = Operator('A')\ncomm = Commutator(A, B).doit()\n```\n\n**For detailed physics capabilities:** See `references/physics-mechanics.md`\n\n### 6. Advanced Mathematics\n\nThe skill includes comprehensive support for:\n\n- **Geometry:** 2D/3D analytic geometry, points, lines, circles, polygons, transformations\n- **Number Theory:** Primes, factorization, GCD/LCM, modular arithmetic, Diophantine equations\n- **Combinatorics:** Permutations, combinations, partitions, group theory\n- **Logic and Sets:** Boolean logic, set theory, finite and infinite sets\n- **Statistics:** Probability distributions, random variables, expectation, variance\n- **Special Functions:** Gamma, Bessel, orthogonal polynomials, hypergeometric functions\n- **Polynomials:** Polynomial algebra, roots, factorization, Groebner bases\n\n**For detailed advanced topics:** See `references/advanced-topics.md`\n\n### 7. Code Generation and Output\n\n**Convert to executable functions:**\n```python\nfrom sympy import lambdify\nimport numpy as np\n\nexpr = x**2 + 2*x + 1\nf = lambdify(x, expr, 'numpy')  # Create NumPy function\nx_vals = np.linspace(0, 10, 100)\ny_vals = f(x_vals)  # Fast numerical evaluation\n```\n\n**Generate C/Fortran code:**\n```python\nfrom sympy.utilities.codegen import codegen\n[(c_name, c_code), (h_name, h_header)] = codegen(\n    ('my_func', expr), 'C'\n)\n```\n\n**LaTeX output:**\n```python\nfrom sympy import latex\nlatex_str = latex(expr)  # Convert to LaTeX for documents\n```\n\n**For comprehensive code generation:** See `references/code-generation-printing.md`\n\n## Working with SymPy: Best Practices\n\n### 1. Always Define Symbols First\n\n```python\nfrom sympy import symbols\nx, y, z = symbols('x y z')\n# Now x, y, z can be used in expressions\n```\n\n### 2. Use Assumptions for Better Simplification\n\n```python\nx = symbols('x', positive=True, real=True)\nsqrt(x**2)  # Returns x (not Abs(x)) due to positive assumption\n```\n\nCommon assumptions: `real`, `positive`, `negative`, `integer`, `rational`, `complex`, `even`, `odd`\n\n### 3. Use Exact Arithmetic\n\n```python\nfrom sympy import Rational, S\n# Correct (exact):\nexpr = Rational(1, 2) * x\nexpr = S(1)/2 * x\n\n# Incorrect (floating-point):\nexpr = 0.5 * x  # Creates approximate value\n```\n\n### 4. Numerical Evaluation When Needed\n\n```python\nfrom sympy import pi, sqrt\nresult = sqrt(8) + pi\nresult.evalf()    # 5.96371554103586\nresult.evalf(50)  # 50 digits of precision\n```\n\n### 5. Convert to NumPy for Performance\n\n```python\n# Slow for many evaluations:\nfor x_val in range(1000):\n    result = expr.subs(x, x_val).evalf()\n\n# Fast:\nf = lambdify(x, expr, 'numpy')\nresults = f(np.arange(1000))\n```\n\n### 6. Use Appropriate Solvers\n\n- `solveset`: Algebraic equations (primary)\n- `linsolve`: Linear systems\n- `nonlinsolve`: Nonlinear systems\n- `dsolve`: Differential equations\n- `solve`: General purpose (legacy, but flexible)\n\n## Reference Files Structure\n\nThis skill uses modular reference files for different capabilities:\n\n1. **`core-capabilities.md`**: Symbols, algebra, calculus, simplification, equation solving\n   - Load when: Basic symbolic computation, calculus, or solving equations\n\n2. **`matrices-linear-algebra.md`**: Matrix operations, eigenvalues, linear systems\n   - Load when: Working with matrices or linear algebra problems\n\n3. **`physics-mechanics.md`**: Classical mechanics, quantum mechanics, vectors, units\n   - Load when: Physics calculations or mechanics problems\n\n4. **`advanced-topics.md`**: Geometry, number theory, combinatorics, logic, statistics\n   - Load when: Advanced mathematical topics beyond basic algebra and calculus\n\n5. **`code-generation-printing.md`**: Lambdify, codegen, LaTeX output, printing\n   - Load when: Converting expressions to code or generating formatted output\n\n## Common Use Case Patterns\n\n### Pattern 1: Solve and Verify\n\n```python\nfrom sympy import symbols, solve, simplify\nx = symbols('x')\n\n# Solve equation\nequation = x**2 - 5*x + 6\nsolutions = solve(equation, x)  # [2, 3]\n\n# Verify solutions\nfor sol in solutions:\n    result = simplify(equation.subs(x, sol))\n    assert result == 0\n```\n\n### Pattern 2: Symbolic to Numeric Pipeline\n\n```python\n# 1. Define symbolic problem\nx, y = symbols('x y')\nexpr = sin(x) + cos(y)\n\n# 2. Manipulate symbolically\nsimplified = simplify(expr)\nderivative = diff(simplified, x)\n\n# 3. Convert to numerical function\nf = lambdify((x, y), derivative, 'numpy')\n\n# 4. Evaluate numerically\nresults = f(x_data, y_data)\n```\n\n### Pattern 3: Document Mathematical Results\n\n```python\n# Compute result symbolically\nintegral_expr = Integral(x**2, (x, 0, 1))\nresult = integral_expr.doit()\n\n# Generate documentation\nprint(f\"LaTeX: {latex(integral_expr)} = {latex(result)}\")\nprint(f\"Pretty: {pretty(integral_expr)} = {pretty(result)}\")\nprint(f\"Numerical: {result.evalf()}\")\n```\n\n## Integration with Scientific Workflows\n\n### With NumPy\n\n```python\nimport numpy as np\nfrom sympy import symbols, lambdify\n\nx = symbols('x')\nexpr = x**2 + 2*x + 1\n\nf = lambdify(x, expr, 'numpy')\nx_array = np.linspace(-5, 5, 100)\ny_array = f(x_array)\n```\n\n### With Matplotlib\n\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sympy import symbols, lambdify, sin\n\nx = symbols('x')\nexpr = sin(x) / x\n\nf = lambdify(x, expr, 'numpy')\nx_vals = np.linspace(-10, 10, 1000)\ny_vals = f(x_vals)\n\nplt.plot(x_vals, y_vals)\nplt.show()\n```\n\n### With SciPy\n\n```python\nfrom scipy.optimize import fsolve\nfrom sympy import symbols, lambdify\n\n# Define equation symbolically\nx = symbols('x')\nequation = x**3 - 2*x - 5\n\n# Convert to numerical function\nf = lambdify(x, equation, 'numpy')\n\n# Solve numerically with initial guess\nsolution = fsolve(f, 2)\n```\n\n## Quick Reference: Most Common Functions\n\n```python\n# Symbols\nfrom sympy import symbols, Symbol\nx, y = symbols('x y')\n\n# Basic operations\nfrom sympy import simplify, expand, factor, collect, cancel\nfrom sympy import sqrt, exp, log, sin, cos, tan, pi, E, I, oo\n\n# Calculus\nfrom sympy import diff, integrate, limit, series, Derivative, Integral\n\n# Solving\nfrom sympy import solve, solveset, linsolve, nonlinsolve, dsolve\n\n# Matrices\nfrom sympy import Matrix, eye, zeros, ones, diag\n\n# Logic and sets\nfrom sympy import And, Or, Not, Implies, FiniteSet, Interval, Union\n\n# Output\nfrom sympy import latex, pprint, lambdify, init_printing\n\n# Utilities\nfrom sympy import evalf, N, nsimplify\n```\n\n## Getting Started Examples\n\n### Example 1: Solve Quadratic Equation\n```python\nfrom sympy import symbols, solve, sqrt\nx = symbols('x')\nsolution = solve(x**2 - 5*x + 6, x)\n# [2, 3]\n```\n\n### Example 2: Calculate Derivative\n```python\nfrom sympy import symbols, diff, sin\nx = symbols('x')\nf = sin(x**2)\ndf_dx = diff(f, x)\n# 2*x*cos(x**2)\n```\n\n### Example 3: Evaluate Integral\n```python\nfrom sympy import symbols, integrate, exp\nx = symbols('x')\nintegral = integrate(x * exp(-x**2), (x, 0, oo))\n# 1/2\n```\n\n### Example 4: Matrix Eigenvalues\n```python\nfrom sympy import Matrix\nM = Matrix([[1, 2], [2, 1]])\neigenvals = M.eigenvals()\n# {3: 1, -1: 1}\n```\n\n### Example 5: Generate Python Function\n```python\nfrom sympy import symbols, lambdify\nimport numpy as np\nx = symbols('x')\nexpr = x**2 + 2*x + 1\nf = lambdify(x, expr, 'numpy')\nf(np.array([1, 2, 3]))\n# array([ 4,  9, 16])\n```\n\n## Troubleshooting Common Issues\n\n1. **\"NameError: name 'x' is not defined\"**\n   - Solution: Always define symbols using `symbols()` before use\n\n2. **Unexpected numerical results**\n   - Issue: Using floating-point numbers like `0.5` instead of `Rational(1, 2)`\n   - Solution: Use `Rational()` or `S()` for exact arithmetic\n\n3. **Slow performance in loops**\n   - Issue: Using `subs()` and `evalf()` repeatedly\n   - Solution: Use `lambdify()` to create a fast numerical function\n\n4. **\"Can't solve this equation\"**\n   - Try different solvers: `solve`, `solveset`, `nsolve` (numerical)\n   - Check if the equation is solvable algebraically\n   - Use numerical methods if no closed-form solution exists\n\n5. **Simplification not working as expected**\n   - Try different simplification functions: `simplify`, `factor`, `expand`, `trigsimp`\n   - Add assumptions to symbols (e.g., `positive=True`)\n   - Use `simplify(expr, force=True)` for aggressive simplification\n\n## Additional Resources\n\n- Official Documentation: https://docs.sympy.org/\n- Tutorial: https://docs.sympy.org/latest/tutorials/intro-tutorial/index.html\n- API Reference: https://docs.sympy.org/latest/reference/index.html\n- Examples: https://github.com/sympy/sympy/tree/master/examples\n",
        "data/k-dense-ai/tooluniverse/SKILL.md": "---\nname: tooluniverse\ndescription: Use this skill when working with scientific research tools and workflows across bioinformatics, cheminformatics, genomics, structural biology, proteomics, and drug discovery. This skill provides access to 600+ scientific tools including machine learning models, datasets, APIs, and analysis packages. Use when searching for scientific tools, executing computational biology workflows, composing multi-step research pipelines, accessing databases like OpenTargets/PubChem/UniProt/PDB/ChEMBL, performing tool discovery for research tasks, or integrating scientific computational resources into LLM workflows.\n---\n\n# ToolUniverse\n\n## Overview\n\nToolUniverse is a unified ecosystem that enables AI agents to function as research scientists by providing standardized access to 600+ scientific resources. Use this skill to discover, execute, and compose scientific tools across multiple research domains including bioinformatics, cheminformatics, genomics, structural biology, proteomics, and drug discovery.\n\n**Key Capabilities:**\n- Access 600+ scientific tools, models, datasets, and APIs\n- Discover tools using natural language, semantic search, or keywords\n- Execute tools through standardized AI-Tool Interaction Protocol\n- Compose multi-step workflows for complex research problems\n- Integration with Claude Desktop/Code via Model Context Protocol (MCP)\n\n## When to Use This Skill\n\nUse this skill when:\n- Searching for scientific tools by function or domain (e.g., \"find protein structure prediction tools\")\n- Executing computational biology workflows (e.g., disease target identification, drug discovery, genomics analysis)\n- Accessing scientific databases (OpenTargets, PubChem, UniProt, PDB, ChEMBL, KEGG, etc.)\n- Composing multi-step research pipelines (e.g., target discovery  structure prediction  virtual screening)\n- Working with bioinformatics, cheminformatics, or structural biology tasks\n- Analyzing gene expression, protein sequences, molecular structures, or clinical data\n- Performing literature searches, pathway enrichment, or variant annotation\n- Building automated scientific research workflows\n\n## Quick Start\n\n### Basic Setup\n```python\nfrom tooluniverse import ToolUniverse\n\n# Initialize and load tools\ntu = ToolUniverse()\ntu.load_tools()  # Loads 600+ scientific tools\n\n# Discover tools\ntools = tu.run({\n    \"name\": \"Tool_Finder_Keyword\",\n    \"arguments\": {\n        \"description\": \"disease target associations\",\n        \"limit\": 10\n    }\n})\n\n# Execute a tool\nresult = tu.run({\n    \"name\": \"OpenTargets_get_associated_targets_by_disease_efoId\",\n    \"arguments\": {\"efoId\": \"EFO_0000537\"}  # Hypertension\n})\n```\n\n### Model Context Protocol (MCP)\nFor Claude Desktop/Code integration:\n```bash\ntooluniverse-smcp\n```\n\n## Core Workflows\n\n### 1. Tool Discovery\n\nFind relevant tools for your research task:\n\n**Three discovery methods:**\n- `Tool_Finder` - Embedding-based semantic search (requires GPU)\n- `Tool_Finder_LLM` - LLM-based semantic search (no GPU required)\n- `Tool_Finder_Keyword` - Fast keyword search\n\n**Example:**\n```python\n# Search by natural language description\ntools = tu.run({\n    \"name\": \"Tool_Finder_LLM\",\n    \"arguments\": {\n        \"description\": \"Find tools for RNA sequencing differential expression analysis\",\n        \"limit\": 10\n    }\n})\n\n# Review available tools\nfor tool in tools:\n    print(f\"{tool['name']}: {tool['description']}\")\n```\n\n**See `references/tool-discovery.md` for:**\n- Detailed discovery methods and search strategies\n- Domain-specific keyword suggestions\n- Best practices for finding tools\n\n### 2. Tool Execution\n\nExecute individual tools through the standardized interface:\n\n**Example:**\n```python\n# Execute disease-target lookup\ntargets = tu.run({\n    \"name\": \"OpenTargets_get_associated_targets_by_disease_efoId\",\n    \"arguments\": {\"efoId\": \"EFO_0000616\"}  # Breast cancer\n})\n\n# Get protein structure\nstructure = tu.run({\n    \"name\": \"AlphaFold_get_structure\",\n    \"arguments\": {\"uniprot_id\": \"P12345\"}\n})\n\n# Calculate molecular properties\nproperties = tu.run({\n    \"name\": \"RDKit_calculate_descriptors\",\n    \"arguments\": {\"smiles\": \"CCO\"}  # Ethanol\n})\n```\n\n**See `references/tool-execution.md` for:**\n- Real-world execution examples across domains\n- Tool parameter handling and validation\n- Result processing and error handling\n- Best practices for production use\n\n### 3. Tool Composition and Workflows\n\nCompose multiple tools for complex research workflows:\n\n**Drug Discovery Example:**\n```python\n# 1. Find disease targets\ntargets = tu.run({\n    \"name\": \"OpenTargets_get_associated_targets_by_disease_efoId\",\n    \"arguments\": {\"efoId\": \"EFO_0000616\"}\n})\n\n# 2. Get protein structures\nstructures = []\nfor target in targets[:5]:\n    structure = tu.run({\n        \"name\": \"AlphaFold_get_structure\",\n        \"arguments\": {\"uniprot_id\": target['uniprot_id']}\n    })\n    structures.append(structure)\n\n# 3. Screen compounds\nhits = []\nfor structure in structures:\n    compounds = tu.run({\n        \"name\": \"ZINC_virtual_screening\",\n        \"arguments\": {\n            \"structure\": structure,\n            \"library\": \"lead-like\",\n            \"top_n\": 100\n        }\n    })\n    hits.extend(compounds)\n\n# 4. Evaluate drug-likeness\ndrug_candidates = []\nfor compound in hits:\n    props = tu.run({\n        \"name\": \"RDKit_calculate_drug_properties\",\n        \"arguments\": {\"smiles\": compound['smiles']}\n    })\n    if props['lipinski_pass']:\n        drug_candidates.append(compound)\n```\n\n**See `references/tool-composition.md` for:**\n- Complete workflow examples (drug discovery, genomics, clinical)\n- Sequential and parallel tool composition patterns\n- Output processing hooks\n- Workflow best practices\n\n## Scientific Domains\n\nToolUniverse supports 600+ tools across major scientific domains:\n\n**Bioinformatics:**\n- Sequence analysis, alignment, BLAST\n- Gene expression (RNA-seq, DESeq2)\n- Pathway enrichment (KEGG, Reactome, GO)\n- Variant annotation (VEP, ClinVar)\n\n**Cheminformatics:**\n- Molecular descriptors and fingerprints\n- Drug discovery and virtual screening\n- ADMET prediction and drug-likeness\n- Chemical databases (PubChem, ChEMBL, ZINC)\n\n**Structural Biology:**\n- Protein structure prediction (AlphaFold)\n- Structure retrieval (PDB)\n- Binding site detection\n- Protein-protein interactions\n\n**Proteomics:**\n- Mass spectrometry analysis\n- Protein databases (UniProt, STRING)\n- Post-translational modifications\n\n**Genomics:**\n- Genome assembly and annotation\n- Copy number variation\n- Clinical genomics workflows\n\n**Medical/Clinical:**\n- Disease databases (OpenTargets, OMIM)\n- Clinical trials and FDA data\n- Variant classification\n\n**See `references/domains.md` for:**\n- Complete domain categorization\n- Tool examples by discipline\n- Cross-domain applications\n- Search strategies by domain\n\n## Reference Documentation\n\nThis skill includes comprehensive reference files that provide detailed information for specific aspects:\n\n- **`references/installation.md`** - Installation, setup, MCP configuration, platform integration\n- **`references/tool-discovery.md`** - Discovery methods, search strategies, listing tools\n- **`references/tool-execution.md`** - Execution patterns, real-world examples, error handling\n- **`references/tool-composition.md`** - Workflow composition, complex pipelines, parallel execution\n- **`references/domains.md`** - Tool categorization by domain, use case examples\n- **`references/api_reference.md`** - Python API documentation, hooks, protocols\n\n**Workflow:** When helping with specific tasks, reference the appropriate file for detailed instructions. For example, if searching for tools, consult `references/tool-discovery.md` for search strategies.\n\n## Example Scripts\n\nTwo executable example scripts demonstrate common use cases:\n\n**`scripts/example_tool_search.py`** - Demonstrates all three discovery methods:\n- Keyword-based search\n- LLM-based search\n- Domain-specific searches\n- Getting detailed tool information\n\n**`scripts/example_workflow.py`** - Complete workflow examples:\n- Drug discovery pipeline (disease  targets  structures  screening  candidates)\n- Genomics analysis (expression data  differential analysis  pathways)\n\nRun examples to understand typical usage patterns and workflow composition.\n\n## Best Practices\n\n1. **Tool Discovery:**\n   - Start with broad searches, then refine based on results\n   - Use `Tool_Finder_Keyword` for fast searches with known terms\n   - Use `Tool_Finder_LLM` for complex semantic queries\n   - Set appropriate `limit` parameter (default: 10)\n\n2. **Tool Execution:**\n   - Always verify tool parameters before execution\n   - Implement error handling for production workflows\n   - Validate input data formats (SMILES, UniProt IDs, gene symbols)\n   - Check result types and structures\n\n3. **Workflow Composition:**\n   - Test each step individually before composing full workflows\n   - Implement checkpointing for long workflows\n   - Consider rate limits for remote APIs\n   - Use parallel execution when tools are independent\n\n4. **Integration:**\n   - Initialize ToolUniverse once and reuse the instance\n   - Call `load_tools()` once at startup\n   - Cache frequently used tool information\n   - Enable logging for debugging\n\n## Key Terminology\n\n- **Tool**: A scientific resource (model, dataset, API, package) accessible through ToolUniverse\n- **Tool Discovery**: Finding relevant tools using search methods (Finder, LLM, Keyword)\n- **Tool Execution**: Running a tool with specific arguments via `tu.run()`\n- **Tool Composition**: Chaining multiple tools for multi-step workflows\n- **MCP**: Model Context Protocol for integration with Claude Desktop/Code\n- **AI-Tool Interaction Protocol**: Standardized interface for LLM-tool communication\n\n## Resources\n\n- **Official Website**: https://aiscientist.tools\n- **GitHub**: https://github.com/mims-harvard/ToolUniverse\n- **Documentation**: https://zitniklab.hms.harvard.edu/ToolUniverse/\n- **Installation**: `uv uv pip install tooluniverse`\n- **MCP Server**: `tooluniverse-smcp`\n",
        "data/k-dense-ai/torch_geometric/SKILL.md": "---\nname: torch-geometric\ndescription: \"Graph Neural Networks (PyG). Node/graph classification, link prediction, GCN, GAT, GraphSAGE, heterogeneous graphs, molecular property prediction, for geometric deep learning.\"\n---\n\n# PyTorch Geometric (PyG)\n\n## Overview\n\nPyTorch Geometric is a library built on PyTorch for developing and training Graph Neural Networks (GNNs). Apply this skill for deep learning on graphs and irregular structures, including mini-batch processing, multi-GPU training, and geometric deep learning applications.\n\n## When to Use This Skill\n\nThis skill should be used when working with:\n- **Graph-based machine learning**: Node classification, graph classification, link prediction\n- **Molecular property prediction**: Drug discovery, chemical property prediction\n- **Social network analysis**: Community detection, influence prediction\n- **Citation networks**: Paper classification, recommendation systems\n- **3D geometric data**: Point clouds, meshes, molecular structures\n- **Heterogeneous graphs**: Multi-type nodes and edges (e.g., knowledge graphs)\n- **Large-scale graph learning**: Neighbor sampling, distributed training\n\n## Quick Start\n\n### Installation\n\n```bash\nuv pip install torch_geometric\n```\n\nFor additional dependencies (sparse operations, clustering):\n```bash\nuv pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-${TORCH}+${CUDA}.html\n```\n\n### Basic Graph Creation\n\n```python\nimport torch\nfrom torch_geometric.data import Data\n\n# Create a simple graph with 3 nodes\nedge_index = torch.tensor([[0, 1, 1, 2],  # source nodes\n                           [1, 0, 2, 1]], dtype=torch.long)  # target nodes\nx = torch.tensor([[-1], [0], [1]], dtype=torch.float)  # node features\n\ndata = Data(x=x, edge_index=edge_index)\nprint(f\"Nodes: {data.num_nodes}, Edges: {data.num_edges}\")\n```\n\n### Loading a Benchmark Dataset\n\n```python\nfrom torch_geometric.datasets import Planetoid\n\n# Load Cora citation network\ndataset = Planetoid(root='/tmp/Cora', name='Cora')\ndata = dataset[0]  # Get the first (and only) graph\n\nprint(f\"Dataset: {dataset}\")\nprint(f\"Nodes: {data.num_nodes}, Edges: {data.num_edges}\")\nprint(f\"Features: {data.num_node_features}, Classes: {dataset.num_classes}\")\n```\n\n## Core Concepts\n\n### Data Structure\n\nPyG represents graphs using the `torch_geometric.data.Data` class with these key attributes:\n\n- **`data.x`**: Node feature matrix `[num_nodes, num_node_features]`\n- **`data.edge_index`**: Graph connectivity in COO format `[2, num_edges]`\n- **`data.edge_attr`**: Edge feature matrix `[num_edges, num_edge_features]` (optional)\n- **`data.y`**: Target labels for nodes or graphs\n- **`data.pos`**: Node spatial positions `[num_nodes, num_dimensions]` (optional)\n- **Custom attributes**: Can add any attribute (e.g., `data.train_mask`, `data.batch`)\n\n**Important**: These attributes are not mandatoryextend Data objects with custom attributes as needed.\n\n### Edge Index Format\n\nEdges are stored in COO (coordinate) format as a `[2, num_edges]` tensor:\n- First row: source node indices\n- Second row: target node indices\n\n```python\n# Edge list: (01), (10), (12), (21)\nedge_index = torch.tensor([[0, 1, 1, 2],\n                           [1, 0, 2, 1]], dtype=torch.long)\n```\n\n### Mini-Batch Processing\n\nPyG handles batching by creating block-diagonal adjacency matrices, concatenating multiple graphs into one large disconnected graph:\n\n- Adjacency matrices are stacked diagonally\n- Node features are concatenated along the node dimension\n- A `batch` vector maps each node to its source graph\n- No padding neededcomputationally efficient\n\n```python\nfrom torch_geometric.loader import DataLoader\n\nloader = DataLoader(dataset, batch_size=32, shuffle=True)\nfor batch in loader:\n    print(f\"Batch size: {batch.num_graphs}\")\n    print(f\"Total nodes: {batch.num_nodes}\")\n    # batch.batch maps nodes to graphs\n```\n\n## Building Graph Neural Networks\n\n### Message Passing Paradigm\n\nGNNs in PyG follow a neighborhood aggregation scheme:\n1. Transform node features\n2. Propagate messages along edges\n3. Aggregate messages from neighbors\n4. Update node representations\n\n### Using Pre-Built Layers\n\nPyG provides 40+ convolutional layers. Common ones include:\n\n**GCNConv** (Graph Convolutional Network):\n```python\nfrom torch_geometric.nn import GCNConv\nimport torch.nn.functional as F\n\nclass GCN(torch.nn.Module):\n    def __init__(self, num_features, num_classes):\n        super().__init__()\n        self.conv1 = GCNConv(num_features, 16)\n        self.conv2 = GCNConv(16, num_classes)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, training=self.training)\n        x = self.conv2(x, edge_index)\n        return F.log_softmax(x, dim=1)\n```\n\n**GATConv** (Graph Attention Network):\n```python\nfrom torch_geometric.nn import GATConv\n\nclass GAT(torch.nn.Module):\n    def __init__(self, num_features, num_classes):\n        super().__init__()\n        self.conv1 = GATConv(num_features, 8, heads=8, dropout=0.6)\n        self.conv2 = GATConv(8 * 8, num_classes, heads=1, concat=False, dropout=0.6)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n        x = F.dropout(x, p=0.6, training=self.training)\n        x = F.elu(self.conv1(x, edge_index))\n        x = F.dropout(x, p=0.6, training=self.training)\n        x = self.conv2(x, edge_index)\n        return F.log_softmax(x, dim=1)\n```\n\n**GraphSAGE**:\n```python\nfrom torch_geometric.nn import SAGEConv\n\nclass GraphSAGE(torch.nn.Module):\n    def __init__(self, num_features, num_classes):\n        super().__init__()\n        self.conv1 = SAGEConv(num_features, 64)\n        self.conv2 = SAGEConv(64, num_classes)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, training=self.training)\n        x = self.conv2(x, edge_index)\n        return F.log_softmax(x, dim=1)\n```\n\n### Custom Message Passing Layers\n\nFor custom layers, inherit from `MessagePassing`:\n\n```python\nfrom torch_geometric.nn import MessagePassing\nfrom torch_geometric.utils import add_self_loops, degree\n\nclass CustomConv(MessagePassing):\n    def __init__(self, in_channels, out_channels):\n        super().__init__(aggr='add')  # \"add\", \"mean\", or \"max\"\n        self.lin = torch.nn.Linear(in_channels, out_channels)\n\n    def forward(self, x, edge_index):\n        # Add self-loops to adjacency matrix\n        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n\n        # Transform node features\n        x = self.lin(x)\n\n        # Compute normalization\n        row, col = edge_index\n        deg = degree(col, x.size(0), dtype=x.dtype)\n        deg_inv_sqrt = deg.pow(-0.5)\n        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n\n        # Propagate messages\n        return self.propagate(edge_index, x=x, norm=norm)\n\n    def message(self, x_j, norm):\n        # x_j: features of source nodes\n        return norm.view(-1, 1) * x_j\n```\n\nKey methods:\n- **`forward()`**: Main entry point\n- **`message()`**: Constructs messages from source to target nodes\n- **`aggregate()`**: Aggregates messages (usually don't overrideset `aggr` parameter)\n- **`update()`**: Updates node embeddings after aggregation\n\n**Variable naming convention**: Appending `_i` or `_j` to tensor names automatically maps them to target or source nodes.\n\n## Working with Datasets\n\n### Loading Built-in Datasets\n\nPyG provides extensive benchmark datasets:\n\n```python\n# Citation networks (node classification)\nfrom torch_geometric.datasets import Planetoid\ndataset = Planetoid(root='/tmp/Cora', name='Cora')  # or 'CiteSeer', 'PubMed'\n\n# Graph classification\nfrom torch_geometric.datasets import TUDataset\ndataset = TUDataset(root='/tmp/ENZYMES', name='ENZYMES')\n\n# Molecular datasets\nfrom torch_geometric.datasets import QM9\ndataset = QM9(root='/tmp/QM9')\n\n# Large-scale datasets\nfrom torch_geometric.datasets import Reddit\ndataset = Reddit(root='/tmp/Reddit')\n```\n\nCheck `references/datasets_reference.md` for a comprehensive list.\n\n### Creating Custom Datasets\n\nFor datasets that fit in memory, inherit from `InMemoryDataset`:\n\n```python\nfrom torch_geometric.data import InMemoryDataset, Data\nimport torch\n\nclass MyOwnDataset(InMemoryDataset):\n    def __init__(self, root, transform=None, pre_transform=None):\n        super().__init__(root, transform, pre_transform)\n        self.load(self.processed_paths[0])\n\n    @property\n    def raw_file_names(self):\n        return ['my_data.csv']  # Files needed in raw_dir\n\n    @property\n    def processed_file_names(self):\n        return ['data.pt']  # Files in processed_dir\n\n    def download(self):\n        # Download raw data to self.raw_dir\n        pass\n\n    def process(self):\n        # Read data, create Data objects\n        data_list = []\n\n        # Example: Create a simple graph\n        edge_index = torch.tensor([[0, 1], [1, 0]], dtype=torch.long)\n        x = torch.randn(2, 16)\n        y = torch.tensor([0], dtype=torch.long)\n\n        data = Data(x=x, edge_index=edge_index, y=y)\n        data_list.append(data)\n\n        # Apply pre_filter and pre_transform\n        if self.pre_filter is not None:\n            data_list = [d for d in data_list if self.pre_filter(d)]\n\n        if self.pre_transform is not None:\n            data_list = [self.pre_transform(d) for d in data_list]\n\n        # Save processed data\n        self.save(data_list, self.processed_paths[0])\n```\n\nFor large datasets that don't fit in memory, inherit from `Dataset` and implement `len()` and `get(idx)`.\n\n### Loading Graphs from CSV\n\n```python\nimport pandas as pd\nimport torch\nfrom torch_geometric.data import HeteroData\n\n# Load nodes\nnodes_df = pd.read_csv('nodes.csv')\nx = torch.tensor(nodes_df[['feat1', 'feat2']].values, dtype=torch.float)\n\n# Load edges\nedges_df = pd.read_csv('edges.csv')\nedge_index = torch.tensor([edges_df['source'].values,\n                           edges_df['target'].values], dtype=torch.long)\n\ndata = Data(x=x, edge_index=edge_index)\n```\n\n## Training Workflows\n\n### Node Classification (Single Graph)\n\n```python\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric.datasets import Planetoid\n\n# Load dataset\ndataset = Planetoid(root='/tmp/Cora', name='Cora')\ndata = dataset[0]\n\n# Create model\nmodel = GCN(dataset.num_features, dataset.num_classes)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n\n# Training\nmodel.train()\nfor epoch in range(200):\n    optimizer.zero_grad()\n    out = model(data)\n    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n    loss.backward()\n    optimizer.step()\n\n    if epoch % 10 == 0:\n        print(f'Epoch {epoch}, Loss: {loss.item():.4f}')\n\n# Evaluation\nmodel.eval()\npred = model(data).argmax(dim=1)\ncorrect = (pred[data.test_mask] == data.y[data.test_mask]).sum()\nacc = int(correct) / int(data.test_mask.sum())\nprint(f'Test Accuracy: {acc:.4f}')\n```\n\n### Graph Classification (Multiple Graphs)\n\n```python\nfrom torch_geometric.datasets import TUDataset\nfrom torch_geometric.loader import DataLoader\nfrom torch_geometric.nn import global_mean_pool\n\nclass GraphClassifier(torch.nn.Module):\n    def __init__(self, num_features, num_classes):\n        super().__init__()\n        self.conv1 = GCNConv(num_features, 64)\n        self.conv2 = GCNConv(64, 64)\n        self.lin = torch.nn.Linear(64, num_classes)\n\n    def forward(self, data):\n        x, edge_index, batch = data.x, data.edge_index, data.batch\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = self.conv2(x, edge_index)\n        x = F.relu(x)\n\n        # Global pooling (aggregate node features to graph-level)\n        x = global_mean_pool(x, batch)\n\n        x = self.lin(x)\n        return F.log_softmax(x, dim=1)\n\n# Load dataset\ndataset = TUDataset(root='/tmp/ENZYMES', name='ENZYMES')\nloader = DataLoader(dataset, batch_size=32, shuffle=True)\n\nmodel = GraphClassifier(dataset.num_features, dataset.num_classes)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\n# Training\nmodel.train()\nfor epoch in range(100):\n    total_loss = 0\n    for batch in loader:\n        optimizer.zero_grad()\n        out = model(batch)\n        loss = F.nll_loss(out, batch.y)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n    if epoch % 10 == 0:\n        print(f'Epoch {epoch}, Loss: {total_loss / len(loader):.4f}')\n```\n\n### Large-Scale Graphs with Neighbor Sampling\n\nFor large graphs, use `NeighborLoader` to sample subgraphs:\n\n```python\nfrom torch_geometric.loader import NeighborLoader\n\n# Create a neighbor sampler\ntrain_loader = NeighborLoader(\n    data,\n    num_neighbors=[25, 10],  # Sample 25 neighbors for 1st hop, 10 for 2nd hop\n    batch_size=128,\n    input_nodes=data.train_mask,\n)\n\n# Training\nmodel.train()\nfor batch in train_loader:\n    optimizer.zero_grad()\n    out = model(batch)\n    # Only compute loss on seed nodes (first batch_size nodes)\n    loss = F.nll_loss(out[:batch.batch_size], batch.y[:batch.batch_size])\n    loss.backward()\n    optimizer.step()\n```\n\n**Important**:\n- Output subgraphs are directed\n- Node indices are relabeled (0 to batch.num_nodes - 1)\n- Only use seed node predictions for loss computation\n- Sampling beyond 2-3 hops is generally not feasible\n\n## Advanced Features\n\n### Heterogeneous Graphs\n\nFor graphs with multiple node and edge types, use `HeteroData`:\n\n```python\nfrom torch_geometric.data import HeteroData\n\ndata = HeteroData()\n\n# Add node features for different types\ndata['paper'].x = torch.randn(100, 128)  # 100 papers with 128 features\ndata['author'].x = torch.randn(200, 64)  # 200 authors with 64 features\n\n# Add edges for different types (source_type, edge_type, target_type)\ndata['author', 'writes', 'paper'].edge_index = torch.randint(0, 200, (2, 500))\ndata['paper', 'cites', 'paper'].edge_index = torch.randint(0, 100, (2, 300))\n\nprint(data)\n```\n\nConvert homogeneous models to heterogeneous:\n\n```python\nfrom torch_geometric.nn import to_hetero\n\n# Define homogeneous model\nmodel = GNN(...)\n\n# Convert to heterogeneous\nmodel = to_hetero(model, data.metadata(), aggr='sum')\n\n# Use as normal\nout = model(data.x_dict, data.edge_index_dict)\n```\n\nOr use `HeteroConv` for custom edge-type-specific operations:\n\n```python\nfrom torch_geometric.nn import HeteroConv, GCNConv, SAGEConv\n\nclass HeteroGNN(torch.nn.Module):\n    def __init__(self, metadata):\n        super().__init__()\n        self.conv1 = HeteroConv({\n            ('paper', 'cites', 'paper'): GCNConv(-1, 64),\n            ('author', 'writes', 'paper'): SAGEConv((-1, -1), 64),\n        }, aggr='sum')\n\n        self.conv2 = HeteroConv({\n            ('paper', 'cites', 'paper'): GCNConv(64, 32),\n            ('author', 'writes', 'paper'): SAGEConv((64, 64), 32),\n        }, aggr='sum')\n\n    def forward(self, x_dict, edge_index_dict):\n        x_dict = self.conv1(x_dict, edge_index_dict)\n        x_dict = {key: F.relu(x) for key, x in x_dict.items()}\n        x_dict = self.conv2(x_dict, edge_index_dict)\n        return x_dict\n```\n\n### Transforms\n\nApply transforms to modify graph structure or features:\n\n```python\nfrom torch_geometric.transforms import NormalizeFeatures, AddSelfLoops, Compose\n\n# Single transform\ntransform = NormalizeFeatures()\ndataset = Planetoid(root='/tmp/Cora', name='Cora', transform=transform)\n\n# Compose multiple transforms\ntransform = Compose([\n    AddSelfLoops(),\n    NormalizeFeatures(),\n])\ndataset = Planetoid(root='/tmp/Cora', name='Cora', transform=transform)\n```\n\nCommon transforms:\n- **Structure**: `ToUndirected`, `AddSelfLoops`, `RemoveSelfLoops`, `KNNGraph`, `RadiusGraph`\n- **Features**: `NormalizeFeatures`, `NormalizeScale`, `Center`\n- **Sampling**: `RandomNodeSplit`, `RandomLinkSplit`\n- **Positional Encoding**: `AddLaplacianEigenvectorPE`, `AddRandomWalkPE`\n\nSee `references/transforms_reference.md` for the full list.\n\n### Model Explainability\n\nPyG provides explainability tools to understand model predictions:\n\n```python\nfrom torch_geometric.explain import Explainer, GNNExplainer\n\n# Create explainer\nexplainer = Explainer(\n    model=model,\n    algorithm=GNNExplainer(epochs=200),\n    explanation_type='model',  # or 'phenomenon'\n    node_mask_type='attributes',\n    edge_mask_type='object',\n    model_config=dict(\n        mode='multiclass_classification',\n        task_level='node',\n        return_type='log_probs',\n    ),\n)\n\n# Generate explanation for a specific node\nnode_idx = 10\nexplanation = explainer(data.x, data.edge_index, index=node_idx)\n\n# Visualize\nprint(f'Node {node_idx} explanation:')\nprint(f'Important edges: {explanation.edge_mask.topk(5).indices}')\nprint(f'Important features: {explanation.node_mask[node_idx].topk(5).indices}')\n```\n\n### Pooling Operations\n\nFor hierarchical graph representations:\n\n```python\nfrom torch_geometric.nn import TopKPooling, global_mean_pool\n\nclass HierarchicalGNN(torch.nn.Module):\n    def __init__(self, num_features, num_classes):\n        super().__init__()\n        self.conv1 = GCNConv(num_features, 64)\n        self.pool1 = TopKPooling(64, ratio=0.8)\n        self.conv2 = GCNConv(64, 64)\n        self.pool2 = TopKPooling(64, ratio=0.8)\n        self.lin = torch.nn.Linear(64, num_classes)\n\n    def forward(self, data):\n        x, edge_index, batch = data.x, data.edge_index, data.batch\n\n        x = F.relu(self.conv1(x, edge_index))\n        x, edge_index, _, batch, _, _ = self.pool1(x, edge_index, None, batch)\n\n        x = F.relu(self.conv2(x, edge_index))\n        x, edge_index, _, batch, _, _ = self.pool2(x, edge_index, None, batch)\n\n        x = global_mean_pool(x, batch)\n        x = self.lin(x)\n        return F.log_softmax(x, dim=1)\n```\n\n## Common Patterns and Best Practices\n\n### Check Graph Properties\n\n```python\n# Undirected check\nfrom torch_geometric.utils import is_undirected\nprint(f\"Is undirected: {is_undirected(data.edge_index)}\")\n\n# Connected components\nfrom torch_geometric.utils import connected_components\nprint(f\"Connected components: {connected_components(data.edge_index)}\")\n\n# Contains self-loops\nfrom torch_geometric.utils import contains_self_loops\nprint(f\"Has self-loops: {contains_self_loops(data.edge_index)}\")\n```\n\n### GPU Training\n\n```python\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)\ndata = data.to(device)\n\n# For DataLoader\nfor batch in loader:\n    batch = batch.to(device)\n    # Train...\n```\n\n### Save and Load Models\n\n```python\n# Save\ntorch.save(model.state_dict(), 'model.pth')\n\n# Load\nmodel = GCN(num_features, num_classes)\nmodel.load_state_dict(torch.load('model.pth'))\nmodel.eval()\n```\n\n### Layer Capabilities\n\nWhen choosing layers, consider these capabilities:\n- **SparseTensor**: Supports efficient sparse matrix operations\n- **edge_weight**: Handles one-dimensional edge weights\n- **edge_attr**: Processes multi-dimensional edge features\n- **Bipartite**: Works with bipartite graphs (different source/target dimensions)\n- **Lazy**: Enables initialization without specifying input dimensions\n\nSee the GNN cheatsheet at `references/layer_capabilities.md`.\n\n## Resources\n\n### Bundled References\n\nThis skill includes detailed reference documentation:\n\n- **`references/layers_reference.md`**: Complete listing of all 40+ GNN layers with descriptions and capabilities\n- **`references/datasets_reference.md`**: Comprehensive dataset catalog organized by category\n- **`references/transforms_reference.md`**: All available transforms and their use cases\n- **`references/api_patterns.md`**: Common API patterns and coding examples\n\n### Scripts\n\nUtility scripts are provided in `scripts/`:\n\n- **`scripts/visualize_graph.py`**: Visualize graph structure using networkx and matplotlib\n- **`scripts/create_gnn_template.py`**: Generate boilerplate code for common GNN architectures\n- **`scripts/benchmark_model.py`**: Benchmark model performance on standard datasets\n\nExecute scripts directly or read them for implementation patterns.\n\n### Official Resources\n\n- **Documentation**: https://pytorch-geometric.readthedocs.io/\n- **GitHub**: https://github.com/pyg-team/pytorch_geometric\n- **Tutorials**: https://pytorch-geometric.readthedocs.io/en/latest/get_started/introduction.html\n- **Examples**: https://github.com/pyg-team/pytorch_geometric/tree/master/examples\n",
        "data/k-dense-ai/torchdrug/SKILL.md": "---\nname: torchdrug\ndescription: \"Graph-based drug discovery toolkit. Molecular property prediction (ADMET), protein modeling, knowledge graph reasoning, molecular generation, retrosynthesis, GNNs (GIN, GAT, SchNet), 40+ datasets, for PyTorch-based ML on molecules, proteins, and biomedical graphs.\"\n---\n\n# TorchDrug\n\n## Overview\n\nTorchDrug is a comprehensive PyTorch-based machine learning toolbox for drug discovery and molecular science. Apply graph neural networks, pre-trained models, and task definitions to molecules, proteins, and biological knowledge graphs, including molecular property prediction, protein modeling, knowledge graph reasoning, molecular generation, retrosynthesis planning, with 40+ curated datasets and 20+ model architectures.\n\n## When to Use This Skill\n\nThis skill should be used when working with:\n\n**Data Types:**\n- SMILES strings or molecular structures\n- Protein sequences or 3D structures (PDB files)\n- Chemical reactions and retrosynthesis\n- Biomedical knowledge graphs\n- Drug discovery datasets\n\n**Tasks:**\n- Predicting molecular properties (solubility, toxicity, activity)\n- Protein function or structure prediction\n- Drug-target binding prediction\n- Generating new molecular structures\n- Planning chemical synthesis routes\n- Link prediction in biomedical knowledge bases\n- Training graph neural networks on scientific data\n\n**Libraries and Integration:**\n- TorchDrug is the primary library\n- Often used with RDKit for cheminformatics\n- Compatible with PyTorch and PyTorch Lightning\n- Integrates with AlphaFold and ESM for proteins\n\n## Getting Started\n\n### Installation\n\n```bash\nuv pip install torchdrug\n# Or with optional dependencies\nuv pip install torchdrug[full]\n```\n\n### Quick Example\n\n```python\nfrom torchdrug import datasets, models, tasks\nfrom torch.utils.data import DataLoader\n\n# Load molecular dataset\ndataset = datasets.BBBP(\"~/molecule-datasets/\")\ntrain_set, valid_set, test_set = dataset.split()\n\n# Define GNN model\nmodel = models.GIN(\n    input_dim=dataset.node_feature_dim,\n    hidden_dims=[256, 256, 256],\n    edge_input_dim=dataset.edge_feature_dim,\n    batch_norm=True,\n    readout=\"mean\"\n)\n\n# Create property prediction task\ntask = tasks.PropertyPrediction(\n    model,\n    task=dataset.tasks,\n    criterion=\"bce\",\n    metric=[\"auroc\", \"auprc\"]\n)\n\n# Train with PyTorch\noptimizer = torch.optim.Adam(task.parameters(), lr=1e-3)\ntrain_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n\nfor epoch in range(100):\n    for batch in train_loader:\n        loss = task(batch)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n```\n\n## Core Capabilities\n\n### 1. Molecular Property Prediction\n\nPredict chemical, physical, and biological properties of molecules from structure.\n\n**Use Cases:**\n- Drug-likeness and ADMET properties\n- Toxicity screening\n- Quantum chemistry properties\n- Binding affinity prediction\n\n**Key Components:**\n- 20+ molecular datasets (BBBP, HIV, Tox21, QM9, etc.)\n- GNN models (GIN, GAT, SchNet)\n- PropertyPrediction and MultipleBinaryClassification tasks\n\n**Reference:** See `references/molecular_property_prediction.md` for:\n- Complete dataset catalog\n- Model selection guide\n- Training workflows and best practices\n- Feature engineering details\n\n### 2. Protein Modeling\n\nWork with protein sequences, structures, and properties.\n\n**Use Cases:**\n- Enzyme function prediction\n- Protein stability and solubility\n- Subcellular localization\n- Protein-protein interactions\n- Structure prediction\n\n**Key Components:**\n- 15+ protein datasets (EnzymeCommission, GeneOntology, PDBBind, etc.)\n- Sequence models (ESM, ProteinBERT, ProteinLSTM)\n- Structure models (GearNet, SchNet)\n- Multiple task types for different prediction levels\n\n**Reference:** See `references/protein_modeling.md` for:\n- Protein-specific datasets\n- Sequence vs structure models\n- Pre-training strategies\n- Integration with AlphaFold and ESM\n\n### 3. Knowledge Graph Reasoning\n\nPredict missing links and relationships in biological knowledge graphs.\n\n**Use Cases:**\n- Drug repurposing\n- Disease mechanism discovery\n- Gene-disease associations\n- Multi-hop biomedical reasoning\n\n**Key Components:**\n- General KGs (FB15k, WN18) and biomedical (Hetionet)\n- Embedding models (TransE, RotatE, ComplEx)\n- KnowledgeGraphCompletion task\n\n**Reference:** See `references/knowledge_graphs.md` for:\n- Knowledge graph datasets (including Hetionet with 45k biomedical entities)\n- Embedding model comparison\n- Evaluation metrics and protocols\n- Biomedical applications\n\n### 4. Molecular Generation\n\nGenerate novel molecular structures with desired properties.\n\n**Use Cases:**\n- De novo drug design\n- Lead optimization\n- Chemical space exploration\n- Property-guided generation\n\n**Key Components:**\n- Autoregressive generation\n- GCPN (policy-based generation)\n- GraphAutoregressiveFlow\n- Property optimization workflows\n\n**Reference:** See `references/molecular_generation.md` for:\n- Generation strategies (unconditional, conditional, scaffold-based)\n- Multi-objective optimization\n- Validation and filtering\n- Integration with property prediction\n\n### 5. Retrosynthesis\n\nPredict synthetic routes from target molecules to starting materials.\n\n**Use Cases:**\n- Synthesis planning\n- Route optimization\n- Synthetic accessibility assessment\n- Multi-step planning\n\n**Key Components:**\n- USPTO-50k reaction dataset\n- CenterIdentification (reaction center prediction)\n- SynthonCompletion (reactant prediction)\n- End-to-end Retrosynthesis pipeline\n\n**Reference:** See `references/retrosynthesis.md` for:\n- Task decomposition (center ID  synthon completion)\n- Multi-step synthesis planning\n- Commercial availability checking\n- Integration with other retrosynthesis tools\n\n### 6. Graph Neural Network Models\n\nComprehensive catalog of GNN architectures for different data types and tasks.\n\n**Available Models:**\n- General GNNs: GCN, GAT, GIN, RGCN, MPNN\n- 3D-aware: SchNet, GearNet\n- Protein-specific: ESM, ProteinBERT, GearNet\n- Knowledge graph: TransE, RotatE, ComplEx, SimplE\n- Generative: GraphAutoregressiveFlow\n\n**Reference:** See `references/models_architectures.md` for:\n- Detailed model descriptions\n- Model selection guide by task and dataset\n- Architecture comparisons\n- Implementation tips\n\n### 7. Datasets\n\n40+ curated datasets spanning chemistry, biology, and knowledge graphs.\n\n**Categories:**\n- Molecular properties (drug discovery, quantum chemistry)\n- Protein properties (function, structure, interactions)\n- Knowledge graphs (general and biomedical)\n- Retrosynthesis reactions\n\n**Reference:** See `references/datasets.md` for:\n- Complete dataset catalog with sizes and tasks\n- Dataset selection guide\n- Loading and preprocessing\n- Splitting strategies (random, scaffold)\n\n## Common Workflows\n\n### Workflow 1: Molecular Property Prediction\n\n**Scenario:** Predict blood-brain barrier penetration for drug candidates.\n\n**Steps:**\n1. Load dataset: `datasets.BBBP()`\n2. Choose model: GIN for molecular graphs\n3. Define task: `PropertyPrediction` with binary classification\n4. Train with scaffold split for realistic evaluation\n5. Evaluate using AUROC and AUPRC\n\n**Navigation:** `references/molecular_property_prediction.md`  Dataset selection  Model selection  Training\n\n### Workflow 2: Protein Function Prediction\n\n**Scenario:** Predict enzyme function from sequence.\n\n**Steps:**\n1. Load dataset: `datasets.EnzymeCommission()`\n2. Choose model: ESM (pre-trained) or GearNet (with structure)\n3. Define task: `PropertyPrediction` with multi-class classification\n4. Fine-tune pre-trained model or train from scratch\n5. Evaluate using accuracy and per-class metrics\n\n**Navigation:** `references/protein_modeling.md`  Model selection (sequence vs structure)  Pre-training strategies\n\n### Workflow 3: Drug Repurposing via Knowledge Graphs\n\n**Scenario:** Find new disease treatments in Hetionet.\n\n**Steps:**\n1. Load dataset: `datasets.Hetionet()`\n2. Choose model: RotatE or ComplEx\n3. Define task: `KnowledgeGraphCompletion`\n4. Train with negative sampling\n5. Query for \"Compound-treats-Disease\" predictions\n6. Filter by plausibility and mechanism\n\n**Navigation:** `references/knowledge_graphs.md`  Hetionet dataset  Model selection  Biomedical applications\n\n### Workflow 4: De Novo Molecule Generation\n\n**Scenario:** Generate drug-like molecules optimized for target binding.\n\n**Steps:**\n1. Train property predictor on activity data\n2. Choose generation approach: GCPN for RL-based optimization\n3. Define reward function combining affinity, drug-likeness, synthesizability\n4. Generate candidates with property constraints\n5. Validate chemistry and filter by drug-likeness\n6. Rank by multi-objective scoring\n\n**Navigation:** `references/molecular_generation.md`  Conditional generation  Multi-objective optimization\n\n### Workflow 5: Retrosynthesis Planning\n\n**Scenario:** Plan synthesis route for target molecule.\n\n**Steps:**\n1. Load dataset: `datasets.USPTO50k()`\n2. Train center identification model (RGCN)\n3. Train synthon completion model (GIN)\n4. Combine into end-to-end retrosynthesis pipeline\n5. Apply recursively for multi-step planning\n6. Check commercial availability of building blocks\n\n**Navigation:** `references/retrosynthesis.md`  Task types  Multi-step planning\n\n## Integration Patterns\n\n### With RDKit\n\nConvert between TorchDrug molecules and RDKit:\n```python\nfrom torchdrug import data\nfrom rdkit import Chem\n\n# SMILES  TorchDrug molecule\nsmiles = \"CCO\"\nmol = data.Molecule.from_smiles(smiles)\n\n# TorchDrug  RDKit\nrdkit_mol = mol.to_molecule()\n\n# RDKit  TorchDrug\nrdkit_mol = Chem.MolFromSmiles(smiles)\nmol = data.Molecule.from_molecule(rdkit_mol)\n```\n\n### With AlphaFold/ESM\n\nUse predicted structures:\n```python\nfrom torchdrug import data\n\n# Load AlphaFold predicted structure\nprotein = data.Protein.from_pdb(\"AF-P12345-F1-model_v4.pdb\")\n\n# Build graph with spatial edges\ngraph = protein.residue_graph(\n    node_position=\"ca\",\n    edge_types=[\"sequential\", \"radius\"],\n    radius_cutoff=10.0\n)\n```\n\n### With PyTorch Lightning\n\nWrap tasks for Lightning training:\n```python\nimport pytorch_lightning as pl\n\nclass LightningTask(pl.LightningModule):\n    def __init__(self, torchdrug_task):\n        super().__init__()\n        self.task = torchdrug_task\n\n    def training_step(self, batch, batch_idx):\n        return self.task(batch)\n\n    def validation_step(self, batch, batch_idx):\n        pred = self.task.predict(batch)\n        target = self.task.target(batch)\n        return {\"pred\": pred, \"target\": target}\n\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=1e-3)\n```\n\n## Technical Details\n\nFor deep dives into TorchDrug's architecture:\n\n**Core Concepts:** See `references/core_concepts.md` for:\n- Architecture philosophy (modular, configurable)\n- Data structures (Graph, Molecule, Protein, PackedGraph)\n- Model interface and forward function signature\n- Task interface (predict, target, forward, evaluate)\n- Training workflows and best practices\n- Loss functions and metrics\n- Common pitfalls and debugging\n\n## Quick Reference Cheat Sheet\n\n**Choose Dataset:**\n- Molecular property  `references/datasets.md`  Molecular section\n- Protein task  `references/datasets.md`  Protein section\n- Knowledge graph  `references/datasets.md`  Knowledge graph section\n\n**Choose Model:**\n- Molecules  `references/models_architectures.md`  GNN section  GIN/GAT/SchNet\n- Proteins (sequence)  `references/models_architectures.md`  Protein section  ESM\n- Proteins (structure)  `references/models_architectures.md`  Protein section  GearNet\n- Knowledge graph  `references/models_architectures.md`  KG section  RotatE/ComplEx\n\n**Common Tasks:**\n- Property prediction  `references/molecular_property_prediction.md` or `references/protein_modeling.md`\n- Generation  `references/molecular_generation.md`\n- Retrosynthesis  `references/retrosynthesis.md`\n- KG reasoning  `references/knowledge_graphs.md`\n\n**Understand Architecture:**\n- Data structures  `references/core_concepts.md`  Data Structures\n- Model design  `references/core_concepts.md`  Model Interface\n- Task design  `references/core_concepts.md`  Task Interface\n\n## Troubleshooting Common Issues\n\n**Issue: Dimension mismatch errors**\n Check `model.input_dim` matches `dataset.node_feature_dim`\n See `references/core_concepts.md`  Essential Attributes\n\n**Issue: Poor performance on molecular tasks**\n Use scaffold splitting, not random\n Try GIN instead of GCN\n See `references/molecular_property_prediction.md`  Best Practices\n\n**Issue: Protein model not learning**\n Use pre-trained ESM for sequence tasks\n Check edge construction for structure models\n See `references/protein_modeling.md`  Training Workflows\n\n**Issue: Memory errors with large graphs**\n Reduce batch size\n Use gradient accumulation\n See `references/core_concepts.md`  Memory Efficiency\n\n**Issue: Generated molecules are invalid**\n Add validity constraints\n Post-process with RDKit validation\n See `references/molecular_generation.md`  Validation and Filtering\n\n## Resources\n\n**Official Documentation:** https://torchdrug.ai/docs/\n**GitHub:** https://github.com/DeepGraphLearning/torchdrug\n**Paper:** TorchDrug: A Powerful and Flexible Machine Learning Platform for Drug Discovery\n\n## Summary\n\nNavigate to the appropriate reference file based on your task:\n\n1. **Molecular property prediction**  `molecular_property_prediction.md`\n2. **Protein modeling**  `protein_modeling.md`\n3. **Knowledge graphs**  `knowledge_graphs.md`\n4. **Molecular generation**  `molecular_generation.md`\n5. **Retrosynthesis**  `retrosynthesis.md`\n6. **Model selection**  `models_architectures.md`\n7. **Dataset selection**  `datasets.md`\n8. **Technical details**  `core_concepts.md`\n\nEach reference provides comprehensive coverage of its domain with examples, best practices, and common use cases.\n",
        "data/k-dense-ai/transformers/SKILL.md": "---\nname: transformers\ndescription: This skill should be used when working with pre-trained transformer models for natural language processing, computer vision, audio, or multimodal tasks. Use for text generation, classification, question answering, translation, summarization, image classification, object detection, speech recognition, and fine-tuning models on custom datasets.\n---\n\n# Transformers\n\n## Overview\n\nThe Hugging Face Transformers library provides access to thousands of pre-trained models for tasks across NLP, computer vision, audio, and multimodal domains. Use this skill to load models, perform inference, and fine-tune on custom data.\n\n## Installation\n\nInstall transformers and core dependencies:\n\n```bash\nuv pip install torch transformers datasets evaluate accelerate\n```\n\nFor vision tasks, add:\n```bash\nuv pip install timm pillow\n```\n\nFor audio tasks, add:\n```bash\nuv pip install librosa soundfile\n```\n\n## Authentication\n\nMany models on the Hugging Face Hub require authentication. Set up access:\n\n```python\nfrom huggingface_hub import login\nlogin()  # Follow prompts to enter token\n```\n\nOr set environment variable:\n```bash\nexport HUGGINGFACE_TOKEN=\"your_token_here\"\n```\n\nGet tokens at: https://huggingface.co/settings/tokens\n\n## Quick Start\n\nUse the Pipeline API for fast inference without manual configuration:\n\n```python\nfrom transformers import pipeline\n\n# Text generation\ngenerator = pipeline(\"text-generation\", model=\"gpt2\")\nresult = generator(\"The future of AI is\", max_length=50)\n\n# Text classification\nclassifier = pipeline(\"text-classification\")\nresult = classifier(\"This movie was excellent!\")\n\n# Question answering\nqa = pipeline(\"question-answering\")\nresult = qa(question=\"What is AI?\", context=\"AI is artificial intelligence...\")\n```\n\n## Core Capabilities\n\n### 1. Pipelines for Quick Inference\n\nUse for simple, optimized inference across many tasks. Supports text generation, classification, NER, question answering, summarization, translation, image classification, object detection, audio classification, and more.\n\n**When to use**: Quick prototyping, simple inference tasks, no custom preprocessing needed.\n\nSee `references/pipelines.md` for comprehensive task coverage and optimization.\n\n### 2. Model Loading and Management\n\nLoad pre-trained models with fine-grained control over configuration, device placement, and precision.\n\n**When to use**: Custom model initialization, advanced device management, model inspection.\n\nSee `references/models.md` for loading patterns and best practices.\n\n### 3. Text Generation\n\nGenerate text with LLMs using various decoding strategies (greedy, beam search, sampling) and control parameters (temperature, top-k, top-p).\n\n**When to use**: Creative text generation, code generation, conversational AI, text completion.\n\nSee `references/generation.md` for generation strategies and parameters.\n\n### 4. Training and Fine-Tuning\n\nFine-tune pre-trained models on custom datasets using the Trainer API with automatic mixed precision, distributed training, and logging.\n\n**When to use**: Task-specific model adaptation, domain adaptation, improving model performance.\n\nSee `references/training.md` for training workflows and best practices.\n\n### 5. Tokenization\n\nConvert text to tokens and token IDs for model input, with padding, truncation, and special token handling.\n\n**When to use**: Custom preprocessing pipelines, understanding model inputs, batch processing.\n\nSee `references/tokenizers.md` for tokenization details.\n\n## Common Patterns\n\n### Pattern 1: Simple Inference\nFor straightforward tasks, use pipelines:\n```python\npipe = pipeline(\"task-name\", model=\"model-id\")\noutput = pipe(input_data)\n```\n\n### Pattern 2: Custom Model Usage\nFor advanced control, load model and tokenizer separately:\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"model-id\")\nmodel = AutoModelForCausalLM.from_pretrained(\"model-id\", device_map=\"auto\")\n\ninputs = tokenizer(\"text\", return_tensors=\"pt\")\noutputs = model.generate(**inputs, max_new_tokens=100)\nresult = tokenizer.decode(outputs[0])\n```\n\n### Pattern 3: Fine-Tuning\nFor task adaptation, use Trainer:\n```python\nfrom transformers import Trainer, TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=3,\n    per_device_train_batch_size=8,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n)\n\ntrainer.train()\n```\n\n## Reference Documentation\n\nFor detailed information on specific components:\n- **Pipelines**: `references/pipelines.md` - All supported tasks and optimization\n- **Models**: `references/models.md` - Loading, saving, and configuration\n- **Generation**: `references/generation.md` - Text generation strategies and parameters\n- **Training**: `references/training.md` - Fine-tuning with Trainer API\n- **Tokenizers**: `references/tokenizers.md` - Tokenization and preprocessing\n",
        "data/k-dense-ai/umap-learn/SKILL.md": "---\nname: umap-learn\ndescription: \"UMAP dimensionality reduction. Fast nonlinear manifold learning for 2D/3D visualization, clustering preprocessing (HDBSCAN), supervised/parametric UMAP, for high-dimensional data.\"\n---\n\n# UMAP-Learn\n\n## Overview\n\nUMAP (Uniform Manifold Approximation and Projection) is a dimensionality reduction technique for visualization and general non-linear dimensionality reduction. Apply this skill for fast, scalable embeddings that preserve local and global structure, supervised learning, and clustering preprocessing.\n\n## Quick Start\n\n### Installation\n\n```bash\nuv pip install umap-learn\n```\n\n### Basic Usage\n\nUMAP follows scikit-learn conventions and can be used as a drop-in replacement for t-SNE or PCA.\n\n```python\nimport umap\nfrom sklearn.preprocessing import StandardScaler\n\n# Prepare data (standardization is essential)\nscaled_data = StandardScaler().fit_transform(data)\n\n# Method 1: Single step (fit and transform)\nembedding = umap.UMAP().fit_transform(scaled_data)\n\n# Method 2: Separate steps (for reusing trained model)\nreducer = umap.UMAP(random_state=42)\nreducer.fit(scaled_data)\nembedding = reducer.embedding_  # Access the trained embedding\n```\n\n**Critical preprocessing requirement:** Always standardize features to comparable scales before applying UMAP to ensure equal weighting across dimensions.\n\n### Typical Workflow\n\n```python\nimport umap\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\n\n# 1. Preprocess data\nscaler = StandardScaler()\nscaled_data = scaler.fit_transform(raw_data)\n\n# 2. Create and fit UMAP\nreducer = umap.UMAP(\n    n_neighbors=15,\n    min_dist=0.1,\n    n_components=2,\n    metric='euclidean',\n    random_state=42\n)\nembedding = reducer.fit_transform(scaled_data)\n\n# 3. Visualize\nplt.scatter(embedding[:, 0], embedding[:, 1], c=labels, cmap='Spectral', s=5)\nplt.colorbar()\nplt.title('UMAP Embedding')\nplt.show()\n```\n\n## Parameter Tuning Guide\n\nUMAP has four primary parameters that control the embedding behavior. Understanding these is crucial for effective usage.\n\n### n_neighbors (default: 15)\n\n**Purpose:** Balances local versus global structure in the embedding.\n\n**How it works:** Controls the size of the local neighborhood UMAP examines when learning manifold structure.\n\n**Effects by value:**\n- **Low values (2-5):** Emphasizes fine local detail but may fragment data into disconnected components\n- **Medium values (15-20):** Balanced view of both local structure and global relationships (recommended starting point)\n- **High values (50-200):** Prioritizes broad topological structure at the expense of fine-grained details\n\n**Recommendation:** Start with 15 and adjust based on results. Increase for more global structure, decrease for more local detail.\n\n### min_dist (default: 0.1)\n\n**Purpose:** Controls how tightly points cluster in the low-dimensional space.\n\n**How it works:** Sets the minimum distance apart that points are allowed to be in the output representation.\n\n**Effects by value:**\n- **Low values (0.0-0.1):** Creates clumped embeddings useful for clustering; reveals fine topological details\n- **High values (0.5-0.99):** Prevents tight packing; emphasizes broad topological preservation over local structure\n\n**Recommendation:** Use 0.0 for clustering applications, 0.1-0.3 for visualization, 0.5+ for loose structure.\n\n### n_components (default: 2)\n\n**Purpose:** Determines the dimensionality of the embedded output space.\n\n**Key feature:** Unlike t-SNE, UMAP scales well in the embedding dimension, enabling use beyond visualization.\n\n**Common uses:**\n- **2-3 dimensions:** Visualization\n- **5-10 dimensions:** Clustering preprocessing (better preserves density than 2D)\n- **10-50 dimensions:** Feature engineering for downstream ML models\n\n**Recommendation:** Use 2 for visualization, 5-10 for clustering, higher for ML pipelines.\n\n### metric (default: 'euclidean')\n\n**Purpose:** Specifies how distance is calculated between input data points.\n\n**Supported metrics:**\n- **Minkowski variants:** euclidean, manhattan, chebyshev\n- **Spatial metrics:** canberra, braycurtis, haversine\n- **Correlation metrics:** cosine, correlation (good for text/document embeddings)\n- **Binary data metrics:** hamming, jaccard, dice, russellrao, kulsinski, rogerstanimoto, sokalmichener, sokalsneath, yule\n- **Custom metrics:** User-defined distance functions via Numba\n\n**Recommendation:** Use euclidean for numeric data, cosine for text/document vectors, hamming for binary data.\n\n### Parameter Tuning Example\n\n```python\n# For visualization with emphasis on local structure\numap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, metric='euclidean')\n\n# For clustering preprocessing\numap.UMAP(n_neighbors=30, min_dist=0.0, n_components=10, metric='euclidean')\n\n# For document embeddings\numap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, metric='cosine')\n\n# For preserving global structure\numap.UMAP(n_neighbors=100, min_dist=0.5, n_components=2, metric='euclidean')\n```\n\n## Supervised and Semi-Supervised Dimension Reduction\n\nUMAP supports incorporating label information to guide the embedding process, enabling class separation while preserving internal structure.\n\n### Supervised UMAP\n\nPass target labels via the `y` parameter when fitting:\n\n```python\n# Supervised dimension reduction\nembedding = umap.UMAP().fit_transform(data, y=labels)\n```\n\n**Key benefits:**\n- Achieves cleanly separated classes\n- Preserves internal structure within each class\n- Maintains global relationships between classes\n\n**When to use:** When you have labeled data and want to separate known classes while keeping meaningful point embeddings.\n\n### Semi-Supervised UMAP\n\nFor partial labels, mark unlabeled points with `-1` following scikit-learn convention:\n\n```python\n# Create semi-supervised labels\nsemi_labels = labels.copy()\nsemi_labels[unlabeled_indices] = -1\n\n# Fit with partial labels\nembedding = umap.UMAP().fit_transform(data, y=semi_labels)\n```\n\n**When to use:** When labeling is expensive or you have more data than labels available.\n\n### Metric Learning with UMAP\n\nTrain a supervised embedding on labeled data, then apply to new unlabeled data:\n\n```python\n# Train on labeled data\nmapper = umap.UMAP().fit(train_data, train_labels)\n\n# Transform unlabeled test data\ntest_embedding = mapper.transform(test_data)\n\n# Use as feature engineering for downstream classifier\nfrom sklearn.svm import SVC\nclf = SVC().fit(mapper.embedding_, train_labels)\npredictions = clf.predict(test_embedding)\n```\n\n**When to use:** For supervised feature engineering in machine learning pipelines.\n\n## UMAP for Clustering\n\nUMAP serves as effective preprocessing for density-based clustering algorithms like HDBSCAN, overcoming the curse of dimensionality.\n\n### Best Practices for Clustering\n\n**Key principle:** Configure UMAP differently for clustering than for visualization.\n\n**Recommended parameters:**\n- **n_neighbors:** Increase to ~30 (default 15 is too local and can create artificial fine-grained clusters)\n- **min_dist:** Set to 0.0 (pack points densely within clusters for clearer boundaries)\n- **n_components:** Use 5-10 dimensions (maintains performance while improving density preservation vs. 2D)\n\n### Clustering Workflow\n\n```python\nimport umap\nimport hdbscan\nfrom sklearn.preprocessing import StandardScaler\n\n# 1. Preprocess data\nscaled_data = StandardScaler().fit_transform(data)\n\n# 2. UMAP with clustering-optimized parameters\nreducer = umap.UMAP(\n    n_neighbors=30,\n    min_dist=0.0,\n    n_components=10,  # Higher than 2 for better density preservation\n    metric='euclidean',\n    random_state=42\n)\nembedding = reducer.fit_transform(scaled_data)\n\n# 3. Apply HDBSCAN clustering\nclusterer = hdbscan.HDBSCAN(\n    min_cluster_size=15,\n    min_samples=5,\n    metric='euclidean'\n)\nlabels = clusterer.fit_predict(embedding)\n\n# 4. Evaluate\nfrom sklearn.metrics import adjusted_rand_score\nscore = adjusted_rand_score(true_labels, labels)\nprint(f\"Adjusted Rand Score: {score:.3f}\")\nprint(f\"Number of clusters: {len(set(labels)) - (1 if -1 in labels else 0)}\")\nprint(f\"Noise points: {sum(labels == -1)}\")\n```\n\n### Visualization After Clustering\n\n```python\n# Create 2D embedding for visualization (separate from clustering)\nvis_reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\nvis_embedding = vis_reducer.fit_transform(scaled_data)\n\n# Plot with cluster labels\nimport matplotlib.pyplot as plt\nplt.scatter(vis_embedding[:, 0], vis_embedding[:, 1], c=labels, cmap='Spectral', s=5)\nplt.colorbar()\nplt.title('UMAP Visualization with HDBSCAN Clusters')\nplt.show()\n```\n\n**Important caveat:** UMAP does not completely preserve density and can create artificial cluster divisions. Always validate and explore resulting clusters.\n\n## Transforming New Data\n\nUMAP enables preprocessing of new data through its `transform()` method, allowing trained models to project unseen data into the learned embedding space.\n\n### Basic Transform Usage\n\n```python\n# Train on training data\ntrans = umap.UMAP(n_neighbors=15, random_state=42).fit(X_train)\n\n# Transform test data\ntest_embedding = trans.transform(X_test)\n```\n\n### Integration with Machine Learning Pipelines\n\n```python\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport umap\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2)\n\n# Preprocess\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Train UMAP\nreducer = umap.UMAP(n_components=10, random_state=42)\nX_train_embedded = reducer.fit_transform(X_train_scaled)\nX_test_embedded = reducer.transform(X_test_scaled)\n\n# Train classifier on embeddings\nclf = SVC()\nclf.fit(X_train_embedded, y_train)\naccuracy = clf.score(X_test_embedded, y_test)\nprint(f\"Test accuracy: {accuracy:.3f}\")\n```\n\n### Important Considerations\n\n**Data consistency:** The transform method assumes the overall distribution in the higher-dimensional space is consistent between training and test data. When this assumption fails, consider using Parametric UMAP instead.\n\n**Performance:** Transform operations are efficient (typically <1 second), though initial calls may be slower due to Numba JIT compilation.\n\n**Scikit-learn compatibility:** UMAP follows standard sklearn conventions and works seamlessly in pipelines:\n\n```python\nfrom sklearn.pipeline import Pipeline\n\npipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('umap', umap.UMAP(n_components=10)),\n    ('classifier', SVC())\n])\n\npipeline.fit(X_train, y_train)\npredictions = pipeline.predict(X_test)\n```\n\n## Advanced Features\n\n### Parametric UMAP\n\nParametric UMAP replaces direct embedding optimization with a learned neural network mapping function.\n\n**Key differences from standard UMAP:**\n- Uses TensorFlow/Keras to train encoder networks\n- Enables efficient transformation of new data\n- Supports reconstruction via decoder networks (inverse transform)\n- Allows custom architectures (CNNs for images, RNNs for sequences)\n\n**Installation:**\n```bash\nuv pip install umap-learn[parametric_umap]\n# Requires TensorFlow 2.x\n```\n\n**Basic usage:**\n```python\nfrom umap.parametric_umap import ParametricUMAP\n\n# Default architecture (3-layer 100-neuron fully-connected network)\nembedder = ParametricUMAP()\nembedding = embedder.fit_transform(data)\n\n# Transform new data efficiently\nnew_embedding = embedder.transform(new_data)\n```\n\n**Custom architecture:**\n```python\nimport tensorflow as tf\n\n# Define custom encoder\nencoder = tf.keras.Sequential([\n    tf.keras.layers.InputLayer(input_shape=(input_dim,)),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dense(2)  # Output dimension\n])\n\nembedder = ParametricUMAP(encoder=encoder, dims=(input_dim,))\nembedding = embedder.fit_transform(data)\n```\n\n**When to use Parametric UMAP:**\n- Need efficient transformation of new data after training\n- Require reconstruction capabilities (inverse transforms)\n- Want to combine UMAP with autoencoders\n- Working with complex data types (images, sequences) benefiting from specialized architectures\n\n**When to use standard UMAP:**\n- Need simplicity and quick prototyping\n- Dataset is small and computational efficiency isn't critical\n- Don't require learned transformations for future data\n\n### Inverse Transforms\n\nInverse transforms enable reconstruction of high-dimensional data from low-dimensional embeddings.\n\n**Basic usage:**\n```python\nreducer = umap.UMAP()\nembedding = reducer.fit_transform(data)\n\n# Reconstruct high-dimensional data from embedding coordinates\nreconstructed = reducer.inverse_transform(embedding)\n```\n\n**Important limitations:**\n- Computationally expensive operation\n- Works poorly outside the convex hull of the embedding\n- Accuracy decreases in regions with gaps between clusters\n\n**Use cases:**\n- Understanding structure of embedded data\n- Visualizing smooth transitions between clusters\n- Exploring interpolations between data points\n- Generating synthetic samples in embedding space\n\n**Example: Exploring embedding space:**\n```python\nimport numpy as np\n\n# Create grid of points in embedding space\nx = np.linspace(embedding[:, 0].min(), embedding[:, 0].max(), 10)\ny = np.linspace(embedding[:, 1].min(), embedding[:, 1].max(), 10)\nxx, yy = np.meshgrid(x, y)\ngrid_points = np.c_[xx.ravel(), yy.ravel()]\n\n# Reconstruct samples from grid\nreconstructed_samples = reducer.inverse_transform(grid_points)\n```\n\n### AlignedUMAP\n\nFor analyzing temporal or related datasets (e.g., time-series experiments, batch data):\n\n```python\nfrom umap import AlignedUMAP\n\n# List of related datasets\ndatasets = [day1_data, day2_data, day3_data]\n\n# Create aligned embeddings\nmapper = AlignedUMAP().fit(datasets)\naligned_embeddings = mapper.embeddings_  # List of embeddings\n```\n\n**When to use:** Comparing embeddings across related datasets while maintaining consistent coordinate systems.\n\n## Reproducibility\n\nTo ensure reproducible results, always set the `random_state` parameter:\n\n```python\nreducer = umap.UMAP(random_state=42)\n```\n\nUMAP uses stochastic optimization, so results will vary slightly between runs without a fixed random state.\n\n## Common Issues and Solutions\n\n**Issue:** Disconnected components or fragmented clusters\n- **Solution:** Increase `n_neighbors` to emphasize more global structure\n\n**Issue:** Clusters too spread out or not well separated\n- **Solution:** Decrease `min_dist` to allow tighter packing\n\n**Issue:** Poor clustering results\n- **Solution:** Use clustering-specific parameters (n_neighbors=30, min_dist=0.0, n_components=5-10)\n\n**Issue:** Transform results differ significantly from training\n- **Solution:** Ensure test data distribution matches training, or use Parametric UMAP\n\n**Issue:** Slow performance on large datasets\n- **Solution:** Set `low_memory=True` (default), or consider dimensionality reduction with PCA first\n\n**Issue:** All points collapsed to single cluster\n- **Solution:** Check data preprocessing (ensure proper scaling), increase `min_dist`\n\n## Resources\n\n### references/\n\nContains detailed API documentation:\n- `api_reference.md`: Complete UMAP class parameters and methods\n\nLoad these references when detailed parameter information or advanced method usage is needed.\n",
        "data/k-dense-ai/uniprot-database/SKILL.md": "---\nname: uniprot-database\ndescription: \"Direct REST API access to UniProt. Protein searches, FASTA retrieval, ID mapping, Swiss-Prot/TrEMBL. For Python workflows with multiple databases, prefer bioservices (unified interface to 40+ services). Use this for direct HTTP/REST work or UniProt-specific control.\"\n---\n\n# UniProt Database\n\n## Overview\n\nUniProt is the world's leading comprehensive protein sequence and functional information resource. Search proteins by name, gene, or accession, retrieve sequences in FASTA format, perform ID mapping across databases, access Swiss-Prot/TrEMBL annotations via REST API for protein analysis.\n\n## When to Use This Skill\n\nThis skill should be used when:\n- Searching for protein entries by name, gene symbol, accession, or organism\n- Retrieving protein sequences in FASTA or other formats\n- Mapping identifiers between UniProt and external databases (Ensembl, RefSeq, PDB, etc.)\n- Accessing protein annotations including GO terms, domains, and functional descriptions\n- Batch retrieving multiple protein entries efficiently\n- Querying reviewed (Swiss-Prot) vs. unreviewed (TrEMBL) protein data\n- Streaming large protein datasets\n- Building custom queries with field-specific search syntax\n\n## Core Capabilities\n\n### 1. Searching for Proteins\n\nSearch UniProt using natural language queries or structured search syntax.\n\n**Common search patterns:**\n```python\n# Search by protein name\nquery = \"insulin AND organism_name:\\\"Homo sapiens\\\"\"\n\n# Search by gene name\nquery = \"gene:BRCA1 AND reviewed:true\"\n\n# Search by accession\nquery = \"accession:P12345\"\n\n# Search by sequence length\nquery = \"length:[100 TO 500]\"\n\n# Search by taxonomy\nquery = \"taxonomy_id:9606\"  # Human proteins\n\n# Search by GO term\nquery = \"go:0005515\"  # Protein binding\n```\n\nUse the API search endpoint: `https://rest.uniprot.org/uniprotkb/search?query={query}&format={format}`\n\n**Supported formats:** JSON, TSV, Excel, XML, FASTA, RDF, TXT\n\n### 2. Retrieving Individual Protein Entries\n\nRetrieve specific protein entries by accession number.\n\n**Accession number formats:**\n- Classic: P12345, Q1AAA9, O15530 (6 characters: letter + 5 alphanumeric)\n- Extended: A0A022YWF9 (10 characters for newer entries)\n\n**Retrieve endpoint:** `https://rest.uniprot.org/uniprotkb/{accession}.{format}`\n\nExample: `https://rest.uniprot.org/uniprotkb/P12345.fasta`\n\n### 3. Batch Retrieval and ID Mapping\n\nMap protein identifiers between different database systems and retrieve multiple entries efficiently.\n\n**ID Mapping workflow:**\n1. Submit mapping job to: `https://rest.uniprot.org/idmapping/run`\n2. Check job status: `https://rest.uniprot.org/idmapping/status/{jobId}`\n3. Retrieve results: `https://rest.uniprot.org/idmapping/results/{jobId}`\n\n**Supported databases for mapping:**\n- UniProtKB AC/ID\n- Gene names\n- Ensembl, RefSeq, EMBL\n- PDB, AlphaFoldDB\n- KEGG, GO terms\n- And many more (see `/references/id_mapping_databases.md`)\n\n**Limitations:**\n- Maximum 100,000 IDs per job\n- Results stored for 7 days\n\n### 4. Streaming Large Result Sets\n\nFor large queries that exceed pagination limits, use the stream endpoint:\n\n`https://rest.uniprot.org/uniprotkb/stream?query={query}&format={format}`\n\nThe stream endpoint returns all results without pagination, suitable for downloading complete datasets.\n\n### 5. Customizing Retrieved Fields\n\nSpecify exactly which fields to retrieve for efficient data transfer.\n\n**Common fields:**\n- `accession` - UniProt accession number\n- `id` - Entry name\n- `gene_names` - Gene name(s)\n- `organism_name` - Organism\n- `protein_name` - Protein names\n- `sequence` - Amino acid sequence\n- `length` - Sequence length\n- `go_*` - Gene Ontology annotations\n- `cc_*` - Comment fields (function, interaction, etc.)\n- `ft_*` - Feature annotations (domains, sites, etc.)\n\n**Example:** `https://rest.uniprot.org/uniprotkb/search?query=insulin&fields=accession,gene_names,organism_name,length,sequence&format=tsv`\n\nSee `/references/api_fields.md` for complete field list.\n\n## Python Implementation\n\nFor programmatic access, use the provided helper script `scripts/uniprot_client.py` which implements:\n\n- `search_proteins(query, format)` - Search UniProt with any query\n- `get_protein(accession, format)` - Retrieve single protein entry\n- `map_ids(ids, from_db, to_db)` - Map between identifier types\n- `batch_retrieve(accessions, format)` - Retrieve multiple entries\n- `stream_results(query, format)` - Stream large result sets\n\n**Alternative Python packages:**\n- **Unipressed**: Modern, typed Python client for UniProt REST API\n- **bioservices**: Comprehensive bioinformatics web services client\n\n## Query Syntax Examples\n\n**Boolean operators:**\n```\nkinase AND organism_name:human\n(diabetes OR insulin) AND reviewed:true\ncancer NOT lung\n```\n\n**Field-specific searches:**\n```\ngene:BRCA1\naccession:P12345\norganism_id:9606\ntaxonomy_name:\"Homo sapiens\"\nannotation:(type:signal)\n```\n\n**Range queries:**\n```\nlength:[100 TO 500]\nmass:[50000 TO 100000]\n```\n\n**Wildcards:**\n```\ngene:BRCA*\nprotein_name:kinase*\n```\n\nSee `/references/query_syntax.md` for comprehensive syntax documentation.\n\n## Best Practices\n\n1. **Use reviewed entries when possible**: Filter with `reviewed:true` for Swiss-Prot (manually curated) entries\n2. **Specify format explicitly**: Choose the most appropriate format (FASTA for sequences, TSV for tabular data, JSON for programmatic parsing)\n3. **Use field selection**: Only request fields you need to reduce bandwidth and processing time\n4. **Handle pagination**: For large result sets, implement proper pagination or use the stream endpoint\n5. **Cache results**: Store frequently accessed data locally to minimize API calls\n6. **Rate limiting**: Be respectful of API resources; implement delays for large batch operations\n7. **Check data quality**: TrEMBL entries are computational predictions; Swiss-Prot entries are manually reviewed\n\n## Resources\n\n### scripts/\n`uniprot_client.py` - Python client with helper functions for common UniProt operations including search, retrieval, ID mapping, and streaming.\n\n### references/\n- `api_fields.md` - Complete list of available fields for customizing queries\n- `id_mapping_databases.md` - Supported databases for ID mapping operations\n- `query_syntax.md` - Comprehensive query syntax with advanced examples\n- `api_examples.md` - Code examples in multiple languages (Python, curl, R)\n\n## Additional Resources\n\n- **API Documentation**: https://www.uniprot.org/help/api\n- **Interactive API Explorer**: https://www.uniprot.org/api-documentation\n- **REST Tutorial**: https://www.uniprot.org/help/uniprot_rest_tutorial\n- **Query Syntax Help**: https://www.uniprot.org/help/query-fields\n- **SPARQL Endpoint**: https://sparql.uniprot.org/ (for advanced graph queries)\n",
        "data/k-dense-ai/uspto-database/SKILL.md": "---\nname: uspto-database\ndescription: \"Access USPTO APIs for patent/trademark searches, examination history (PEDS), assignments, citations, office actions, TSDR, for IP analysis and prior art searches.\"\n---\n\n# USPTO Database\n\n## Overview\n\nUSPTO provides specialized APIs for patent and trademark data. Search patents by keywords/inventors/assignees, retrieve examination history via PEDS, track assignments, analyze citations and office actions, access TSDR for trademarks, for IP analysis and prior art searches.\n\n## When to Use This Skill\n\nThis skill should be used when:\n\n- **Patent Search**: Finding patents by keywords, inventors, assignees, classifications, or dates\n- **Patent Details**: Retrieving full patent data including claims, abstracts, citations\n- **Trademark Search**: Looking up trademarks by serial or registration number\n- **Trademark Status**: Checking trademark status, ownership, and prosecution history\n- **Examination History**: Accessing patent prosecution data from PEDS (Patent Examination Data System)\n- **Office Actions**: Retrieving office action text, citations, and rejections\n- **Assignments**: Tracking patent/trademark ownership transfers\n- **Citations**: Analyzing patent citations (forward and backward)\n- **Litigation**: Accessing patent litigation records\n- **Portfolio Analysis**: Analyzing patent/trademark portfolios for companies or inventors\n\n## USPTO API Ecosystem\n\nThe USPTO provides multiple specialized APIs for different data needs:\n\n### Core APIs\n\n1. **PatentSearch API** - Modern ElasticSearch-based patent search (replaced legacy PatentsView in May 2025)\n   - Search patents by keywords, inventors, assignees, classifications, dates\n   - Access to patent data through June 30, 2025\n   - 45 requests/minute rate limit\n   - **Base URL**: `https://search.patentsview.org/api/v1/`\n\n2. **PEDS (Patent Examination Data System)** - Patent examination history\n   - Application status and transaction history from 1981-present\n   - Office action dates and examination events\n   - Use `uspto-opendata-python` Python library\n   - **Replaced**: PAIR Bulk Data (PBD) - decommissioned\n\n3. **TSDR (Trademark Status & Document Retrieval)** - Trademark data\n   - Trademark status, ownership, prosecution history\n   - Search by serial or registration number\n   - **Base URL**: `https://tsdrapi.uspto.gov/ts/cd/`\n\n### Additional APIs\n\n4. **Patent Assignment Search** - Ownership records and transfers\n5. **Trademark Assignment Search** - Trademark ownership changes\n6. **Enriched Citation API** - Patent citation analysis\n7. **Office Action Text Retrieval** - Full text of office actions\n8. **Office Action Citations** - Citations from office actions\n9. **Office Action Rejection** - Rejection reasons and types\n10. **PTAB API** - Patent Trial and Appeal Board proceedings\n11. **Patent Litigation Cases** - Federal district court litigation data\n12. **Cancer Moonshot Data Set** - Cancer-related patents\n\n## Quick Start\n\n### API Key Registration\n\nAll USPTO APIs require an API key. Register at:\n**https://account.uspto.gov/api-manager/**\n\nSet the API key as an environment variable:\n```bash\nexport USPTO_API_KEY=\"your_api_key_here\"\n```\n\n### Helper Scripts\n\nThis skill includes Python scripts for common operations:\n\n- **`scripts/patent_search.py`** - PatentSearch API client for searching patents\n- **`scripts/peds_client.py`** - PEDS client for examination history\n- **`scripts/trademark_client.py`** - TSDR client for trademark data\n\n## Task 1: Searching Patents\n\n### Using the PatentSearch API\n\nThe PatentSearch API uses a JSON query language with various operators for flexible searching.\n\n#### Basic Patent Search Examples\n\n**Search by keywords in abstract:**\n```python\nfrom scripts.patent_search import PatentSearchClient\n\nclient = PatentSearchClient()\n\n# Search for machine learning patents\nresults = client.search_patents({\n    \"patent_abstract\": {\"_text_all\": [\"machine\", \"learning\"]}\n})\n\nfor patent in results['patents']:\n    print(f\"{patent['patent_number']}: {patent['patent_title']}\")\n```\n\n**Search by inventor:**\n```python\nresults = client.search_by_inventor(\"John Smith\")\n```\n\n**Search by assignee/company:**\n```python\nresults = client.search_by_assignee(\"Google\")\n```\n\n**Search by date range:**\n```python\nresults = client.search_by_date_range(\"2024-01-01\", \"2024-12-31\")\n```\n\n**Search by CPC classification:**\n```python\nresults = client.search_by_classification(\"H04N\")  # Video/image tech\n```\n\n#### Advanced Patent Search\n\nCombine multiple criteria with logical operators:\n\n```python\nresults = client.advanced_search(\n    keywords=[\"artificial\", \"intelligence\"],\n    assignee=\"Microsoft\",\n    start_date=\"2023-01-01\",\n    end_date=\"2024-12-31\",\n    cpc_codes=[\"G06N\", \"G06F\"]  # AI and computing classifications\n)\n```\n\n#### Direct API Usage\n\nFor complex queries, use the API directly:\n\n```python\nimport requests\n\nurl = \"https://search.patentsview.org/api/v1/patent\"\nheaders = {\n    \"X-Api-Key\": \"YOUR_API_KEY\",\n    \"Content-Type\": \"application/json\"\n}\n\nquery = {\n    \"q\": {\n        \"_and\": [\n            {\"patent_date\": {\"_gte\": \"2024-01-01\"}},\n            {\"assignee_organization\": {\"_text_any\": [\"Google\", \"Alphabet\"]}},\n            {\"cpc_subclass_id\": [\"G06N\", \"H04N\"]}\n        ]\n    },\n    \"f\": [\"patent_number\", \"patent_title\", \"patent_date\", \"inventor_name\"],\n    \"s\": [{\"patent_date\": \"desc\"}],\n    \"o\": {\"per_page\": 100, \"page\": 1}\n}\n\nresponse = requests.post(url, headers=headers, json=query)\nresults = response.json()\n```\n\n### Query Operators\n\n- **Equality**: `{\"field\": \"value\"}` or `{\"field\": {\"_eq\": \"value\"}}`\n- **Comparison**: `_gt`, `_gte`, `_lt`, `_lte`, `_neq`\n- **Text search**: `_text_all`, `_text_any`, `_text_phrase`\n- **String matching**: `_begins`, `_contains`\n- **Logical**: `_and`, `_or`, `_not`\n\n**Best Practice**: Use `_text_*` operators for text fields (more performant than `_contains` or `_begins`)\n\n### Available Patent Endpoints\n\n- `/patent` - Granted patents\n- `/publication` - Pregrant publications\n- `/inventor` - Inventor information\n- `/assignee` - Assignee information\n- `/cpc_subclass`, `/cpc_at_issue` - CPC classifications\n- `/uspc` - US Patent Classification\n- `/ipc` - International Patent Classification\n- `/claims`, `/brief_summary_text`, `/detail_description_text` - Text data (beta)\n\n### Reference Documentation\n\nSee `references/patentsearch_api.md` for complete PatentSearch API documentation including:\n- All available endpoints\n- Complete field reference\n- Query syntax and examples\n- Response formats\n- Rate limits and best practices\n\n## Task 2: Retrieving Patent Examination Data\n\n### Using PEDS (Patent Examination Data System)\n\nPEDS provides comprehensive prosecution history including transaction events, status changes, and examination timeline.\n\n#### Installation\n\n```bash\nuv pip install uspto-opendata-python\n```\n\n#### Basic PEDS Usage\n\n**Get application data:**\n```python\nfrom scripts.peds_client import PEDSHelper\n\nhelper = PEDSHelper()\n\n# By application number\napp_data = helper.get_application(\"16123456\")\nprint(f\"Title: {app_data['title']}\")\nprint(f\"Status: {app_data['app_status']}\")\n\n# By patent number\npatent_data = helper.get_patent(\"11234567\")\n```\n\n**Get transaction history:**\n```python\ntransactions = helper.get_transaction_history(\"16123456\")\n\nfor trans in transactions:\n    print(f\"{trans['date']}: {trans['code']} - {trans['description']}\")\n```\n\n**Get office actions:**\n```python\noffice_actions = helper.get_office_actions(\"16123456\")\n\nfor oa in office_actions:\n    if oa['code'] == 'CTNF':\n        print(f\"Non-final rejection: {oa['date']}\")\n    elif oa['code'] == 'CTFR':\n        print(f\"Final rejection: {oa['date']}\")\n    elif oa['code'] == 'NOA':\n        print(f\"Notice of allowance: {oa['date']}\")\n```\n\n**Get status summary:**\n```python\nsummary = helper.get_status_summary(\"16123456\")\n\nprint(f\"Current status: {summary['current_status']}\")\nprint(f\"Filing date: {summary['filing_date']}\")\nprint(f\"Pendency: {summary['pendency_days']} days\")\n\nif summary['is_patented']:\n    print(f\"Patent number: {summary['patent_number']}\")\n    print(f\"Issue date: {summary['issue_date']}\")\n```\n\n#### Prosecution Analysis\n\nAnalyze prosecution patterns:\n\n```python\nanalysis = helper.analyze_prosecution(\"16123456\")\n\nprint(f\"Total office actions: {analysis['total_office_actions']}\")\nprint(f\"Non-final rejections: {analysis['non_final_rejections']}\")\nprint(f\"Final rejections: {analysis['final_rejections']}\")\nprint(f\"Allowed: {analysis['allowance']}\")\nprint(f\"Responses filed: {analysis['responses']}\")\n```\n\n### Common Transaction Codes\n\n- **CTNF** - Non-final rejection mailed\n- **CTFR** - Final rejection mailed\n- **NOA** - Notice of allowance mailed\n- **WRIT** - Response filed\n- **ISS.FEE** - Issue fee payment\n- **ABND** - Application abandoned\n- **AOPF** - Office action mailed\n\n### Reference Documentation\n\nSee `references/peds_api.md` for complete PEDS documentation including:\n- All available data fields\n- Transaction code reference\n- Python library usage\n- Portfolio analysis examples\n\n## Task 3: Searching and Monitoring Trademarks\n\n### Using TSDR (Trademark Status & Document Retrieval)\n\nAccess trademark status, ownership, and prosecution history.\n\n#### Basic Trademark Usage\n\n**Get trademark by serial number:**\n```python\nfrom scripts.trademark_client import TrademarkClient\n\nclient = TrademarkClient()\n\n# By serial number\ntm_data = client.get_trademark_by_serial(\"87654321\")\n\n# By registration number\ntm_data = client.get_trademark_by_registration(\"5678901\")\n```\n\n**Get trademark status:**\n```python\nstatus = client.get_trademark_status(\"87654321\")\n\nprint(f\"Mark: {status['mark_text']}\")\nprint(f\"Status: {status['status']}\")\nprint(f\"Filing date: {status['filing_date']}\")\n\nif status['is_registered']:\n    print(f\"Registration #: {status['registration_number']}\")\n    print(f\"Registration date: {status['registration_date']}\")\n```\n\n**Check trademark health:**\n```python\nhealth = client.check_trademark_health(\"87654321\")\n\nprint(f\"Mark: {health['mark']}\")\nprint(f\"Status: {health['status']}\")\n\nfor alert in health['alerts']:\n    print(alert)\n\nif health['needs_attention']:\n    print(\"  This mark needs attention!\")\n```\n\n#### Trademark Portfolio Monitoring\n\nMonitor multiple trademarks:\n\n```python\ndef monitor_portfolio(serial_numbers, api_key):\n    \"\"\"Monitor trademark portfolio health.\"\"\"\n    client = TrademarkClient(api_key)\n\n    results = {\n        'active': [],\n        'pending': [],\n        'problems': []\n    }\n\n    for sn in serial_numbers:\n        health = client.check_trademark_health(sn)\n\n        if 'REGISTERED' in health['status']:\n            results['active'].append(health)\n        elif 'PENDING' in health['status'] or 'PUBLISHED' in health['status']:\n            results['pending'].append(health)\n        elif health['needs_attention']:\n            results['problems'].append(health)\n\n    return results\n```\n\n### Common Trademark Statuses\n\n- **REGISTERED** - Active registered mark\n- **PENDING** - Under examination\n- **PUBLISHED FOR OPPOSITION** - In opposition period\n- **ABANDONED** - Application abandoned\n- **CANCELLED** - Registration cancelled\n- **SUSPENDED** - Examination suspended\n- **REGISTERED AND RENEWED** - Registration renewed\n\n### Reference Documentation\n\nSee `references/trademark_api.md` for complete trademark API documentation including:\n- TSDR API reference\n- Trademark Assignment Search API\n- All status codes\n- Prosecution history access\n- Ownership tracking\n\n## Task 4: Tracking Assignments and Ownership\n\n### Patent and Trademark Assignments\n\nBoth patents and trademarks have Assignment Search APIs for tracking ownership changes.\n\n#### Patent Assignment API\n\n**Base URL**: `https://assignment-api.uspto.gov/patent/v1.4/`\n\n**Search by patent number:**\n```python\nimport requests\nimport xml.etree.ElementTree as ET\n\ndef get_patent_assignments(patent_number, api_key):\n    url = f\"https://assignment-api.uspto.gov/patent/v1.4/assignment/patent/{patent_number}\"\n    headers = {\"X-Api-Key\": api_key}\n\n    response = requests.get(url, headers=headers)\n    if response.status_code == 200:\n        return response.text  # Returns XML\n\nassignments_xml = get_patent_assignments(\"11234567\", api_key)\nroot = ET.fromstring(assignments_xml)\n\nfor assignment in root.findall('.//assignment'):\n    recorded_date = assignment.find('recordedDate').text\n    assignor = assignment.find('.//assignor/name').text\n    assignee = assignment.find('.//assignee/name').text\n    conveyance = assignment.find('conveyanceText').text\n\n    print(f\"{recorded_date}: {assignor}  {assignee}\")\n    print(f\"  Type: {conveyance}\\n\")\n```\n\n**Search by company name:**\n```python\ndef find_company_patents(company_name, api_key):\n    url = \"https://assignment-api.uspto.gov/patent/v1.4/assignment/search\"\n    headers = {\"X-Api-Key\": api_key}\n    data = {\"criteria\": {\"assigneeName\": company_name}}\n\n    response = requests.post(url, headers=headers, json=data)\n    return response.text\n```\n\n### Common Assignment Types\n\n- **ASSIGNMENT OF ASSIGNORS INTEREST** - Ownership transfer\n- **SECURITY AGREEMENT** - Collateral/security interest\n- **MERGER** - Corporate merger\n- **CHANGE OF NAME** - Name change\n- **ASSIGNMENT OF PARTIAL INTEREST** - Partial ownership\n\n## Task 5: Accessing Additional USPTO Data\n\n### Office Actions, Citations, and Litigation\n\nMultiple specialized APIs provide additional patent data.\n\n#### Office Action Text Retrieval\n\nRetrieve full text of office actions using application number. Integrate with PEDS to identify which office actions exist, then retrieve full text.\n\n#### Enriched Citation API\n\nAnalyze patent citations:\n- Forward citations (patents citing this patent)\n- Backward citations (prior art cited)\n- Examiner vs. applicant citations\n- Citation context\n\n#### Patent Litigation Cases API\n\nAccess federal district court patent litigation records:\n- 74,623+ litigation records\n- Patents asserted\n- Parties and venues\n- Case outcomes\n\n#### PTAB API\n\nPatent Trial and Appeal Board proceedings:\n- Inter partes review (IPR)\n- Post-grant review (PGR)\n- Appeal decisions\n\n### Reference Documentation\n\nSee `references/additional_apis.md` for comprehensive documentation on:\n- Enriched Citation API\n- Office Action APIs (Text, Citations, Rejections)\n- Patent Litigation Cases API\n- PTAB API\n- Cancer Moonshot Data Set\n- OCE Status/Event Codes\n\n## Complete Analysis Example\n\n### Comprehensive Patent Analysis\n\nCombine multiple APIs for complete patent intelligence:\n\n```python\ndef comprehensive_patent_analysis(patent_number, api_key):\n    \"\"\"\n    Full patent analysis using multiple USPTO APIs.\n    \"\"\"\n    from scripts.patent_search import PatentSearchClient\n    from scripts.peds_client import PEDSHelper\n\n    results = {}\n\n    # 1. Get patent details\n    patent_client = PatentSearchClient(api_key)\n    patent_data = patent_client.get_patent(patent_number)\n    results['patent'] = patent_data\n\n    # 2. Get examination history\n    peds = PEDSHelper()\n    results['prosecution'] = peds.analyze_prosecution(patent_number)\n    results['status'] = peds.get_status_summary(patent_number)\n\n    # 3. Get assignment history\n    import requests\n    assign_url = f\"https://assignment-api.uspto.gov/patent/v1.4/assignment/patent/{patent_number}\"\n    assign_resp = requests.get(assign_url, headers={\"X-Api-Key\": api_key})\n    results['assignments'] = assign_resp.text if assign_resp.status_code == 200 else None\n\n    # 4. Analyze results\n    print(f\"\\n=== Patent {patent_number} Analysis ===\\n\")\n    print(f\"Title: {patent_data['patent_title']}\")\n    print(f\"Assignee: {', '.join(patent_data.get('assignee_organization', []))}\")\n    print(f\"Issue Date: {patent_data['patent_date']}\")\n\n    print(f\"\\nProsecution:\")\n    print(f\"  Office Actions: {results['prosecution']['total_office_actions']}\")\n    print(f\"  Rejections: {results['prosecution']['non_final_rejections']} non-final, {results['prosecution']['final_rejections']} final\")\n    print(f\"  Pendency: {results['prosecution']['pendency_days']} days\")\n\n    # Analyze citations\n    if 'cited_patent_number' in patent_data:\n        print(f\"\\nCitations:\")\n        print(f\"  Cites: {len(patent_data['cited_patent_number'])} patents\")\n    if 'citedby_patent_number' in patent_data:\n        print(f\"  Cited by: {len(patent_data['citedby_patent_number'])} patents\")\n\n    return results\n```\n\n## Best Practices\n\n1. **API Key Management**\n   - Store API key in environment variables\n   - Never commit keys to version control\n   - Use same key across all USPTO APIs\n\n2. **Rate Limiting**\n   - PatentSearch: 45 requests/minute\n   - Implement exponential backoff for rate limit errors\n   - Cache responses when possible\n\n3. **Query Optimization**\n   - Use `_text_*` operators for text fields (more performant)\n   - Request only needed fields to reduce response size\n   - Use date ranges to narrow searches\n\n4. **Data Handling**\n   - Not all fields populated for all patents/trademarks\n   - Handle missing data gracefully\n   - Parse dates consistently\n\n5. **Combining APIs**\n   - Use PatentSearch for discovery\n   - Use PEDS for prosecution details\n   - Use Assignment APIs for ownership tracking\n   - Combine data for comprehensive analysis\n\n## Important Notes\n\n- **Legacy API Sunset**: PatentsView legacy API discontinued May 1, 2025 - use PatentSearch API\n- **PAIR Bulk Data Decommissioned**: Use PEDS instead\n- **Data Coverage**: PatentSearch has data through June 30, 2025; PEDS from 1981-present\n- **Text Endpoints**: Claims and description endpoints are in beta with ongoing backfilling\n- **Rate Limits**: Respect rate limits to avoid service disruptions\n\n## Resources\n\n### API Documentation\n- **PatentSearch API**: https://search.patentsview.org/docs/\n- **USPTO Developer Portal**: https://developer.uspto.gov/\n- **USPTO Open Data Portal**: https://data.uspto.gov/\n- **API Key Registration**: https://account.uspto.gov/api-manager/\n\n### Python Libraries\n- **uspto-opendata-python**: https://pypi.org/project/uspto-opendata-python/\n- **USPTO Docs**: https://docs.ip-tools.org/uspto-opendata-python/\n\n### Reference Files\n- `references/patentsearch_api.md` - Complete PatentSearch API reference\n- `references/peds_api.md` - PEDS API and library documentation\n- `references/trademark_api.md` - Trademark APIs (TSDR and Assignment)\n- `references/additional_apis.md` - Citations, Office Actions, Litigation, PTAB\n\n### Scripts\n- `scripts/patent_search.py` - PatentSearch API client\n- `scripts/peds_client.py` - PEDS examination data client\n- `scripts/trademark_client.py` - Trademark search client\n",
        "data/k-dense-ai/vaex/SKILL.md": "---\nname: vaex\ndescription: Use this skill for processing and analyzing large tabular datasets (billions of rows) that exceed available RAM. Vaex excels at out-of-core DataFrame operations, lazy evaluation, fast aggregations, efficient visualization of big data, and machine learning on large datasets. Apply when users need to work with large CSV/HDF5/Arrow/Parquet files, perform fast statistics on massive datasets, create visualizations of big data, or build ML pipelines that don't fit in memory.\n---\n\n# Vaex\n\n## Overview\n\nVaex is a high-performance Python library designed for lazy, out-of-core DataFrames to process and visualize tabular datasets that are too large to fit into RAM. Vaex can process over a billion rows per second, enabling interactive data exploration and analysis on datasets with billions of rows.\n\n## When to Use This Skill\n\nUse Vaex when:\n- Processing tabular datasets larger than available RAM (gigabytes to terabytes)\n- Performing fast statistical aggregations on massive datasets\n- Creating visualizations and heatmaps of large datasets\n- Building machine learning pipelines on big data\n- Converting between data formats (CSV, HDF5, Arrow, Parquet)\n- Needing lazy evaluation and virtual columns to avoid memory overhead\n- Working with astronomical data, financial time series, or other large-scale scientific datasets\n\n## Core Capabilities\n\nVaex provides six primary capability areas, each documented in detail in the references directory:\n\n### 1. DataFrames and Data Loading\n\nLoad and create Vaex DataFrames from various sources including files (HDF5, CSV, Arrow, Parquet), pandas DataFrames, NumPy arrays, and dictionaries. Reference `references/core_dataframes.md` for:\n- Opening large files efficiently\n- Converting from pandas/NumPy/Arrow\n- Working with example datasets\n- Understanding DataFrame structure\n\n### 2. Data Processing and Manipulation\n\nPerform filtering, create virtual columns, use expressions, and aggregate data without loading everything into memory. Reference `references/data_processing.md` for:\n- Filtering and selections\n- Virtual columns and expressions\n- Groupby operations and aggregations\n- String operations and datetime handling\n- Working with missing data\n\n### 3. Performance and Optimization\n\nLeverage Vaex's lazy evaluation, caching strategies, and memory-efficient operations. Reference `references/performance.md` for:\n- Understanding lazy evaluation\n- Using `delay=True` for batching operations\n- Materializing columns when needed\n- Caching strategies\n- Asynchronous operations\n\n### 4. Data Visualization\n\nCreate interactive visualizations of large datasets including heatmaps, histograms, and scatter plots. Reference `references/visualization.md` for:\n- Creating 1D and 2D plots\n- Heatmap visualizations\n- Working with selections\n- Customizing plots and subplots\n\n### 5. Machine Learning Integration\n\nBuild ML pipelines with transformers, encoders, and integration with scikit-learn, XGBoost, and other frameworks. Reference `references/machine_learning.md` for:\n- Feature scaling and encoding\n- PCA and dimensionality reduction\n- K-means clustering\n- Integration with scikit-learn/XGBoost/CatBoost\n- Model serialization and deployment\n\n### 6. I/O Operations\n\nEfficiently read and write data in various formats with optimal performance. Reference `references/io_operations.md` for:\n- File format recommendations\n- Export strategies\n- Working with Apache Arrow\n- CSV handling for large files\n- Server and remote data access\n\n## Quick Start Pattern\n\nFor most Vaex tasks, follow this pattern:\n\n```python\nimport vaex\n\n# 1. Open or create DataFrame\ndf = vaex.open('large_file.hdf5')  # or .csv, .arrow, .parquet\n# OR\ndf = vaex.from_pandas(pandas_df)\n\n# 2. Explore the data\nprint(df)  # Shows first/last rows and column info\ndf.describe()  # Statistical summary\n\n# 3. Create virtual columns (no memory overhead)\ndf['new_column'] = df.x ** 2 + df.y\n\n# 4. Filter with selections\ndf_filtered = df[df.age > 25]\n\n# 5. Compute statistics (fast, lazy evaluation)\nmean_val = df.x.mean()\nstats = df.groupby('category').agg({'value': 'sum'})\n\n# 6. Visualize\ndf.plot1d(df.x, limits=[0, 100])\ndf.plot(df.x, df.y, limits='99.7%')\n\n# 7. Export if needed\ndf.export_hdf5('output.hdf5')\n```\n\n## Working with References\n\nThe reference files contain detailed information about each capability area. Load references into context based on the specific task:\n\n- **Basic operations**: Start with `references/core_dataframes.md` and `references/data_processing.md`\n- **Performance issues**: Check `references/performance.md`\n- **Visualization tasks**: Use `references/visualization.md`\n- **ML pipelines**: Reference `references/machine_learning.md`\n- **File I/O**: Consult `references/io_operations.md`\n\n## Best Practices\n\n1. **Use HDF5 or Apache Arrow formats** for optimal performance with large datasets\n2. **Leverage virtual columns** instead of materializing data to save memory\n3. **Batch operations** using `delay=True` when performing multiple calculations\n4. **Export to efficient formats** rather than keeping data in CSV\n5. **Use expressions** for complex calculations without intermediate storage\n6. **Profile with `df.stat()`** to understand memory usage and optimize operations\n\n## Common Patterns\n\n### Pattern: Converting Large CSV to HDF5\n```python\nimport vaex\n\n# Open large CSV (processes in chunks automatically)\ndf = vaex.from_csv('large_file.csv')\n\n# Export to HDF5 for faster future access\ndf.export_hdf5('large_file.hdf5')\n\n# Future loads are instant\ndf = vaex.open('large_file.hdf5')\n```\n\n### Pattern: Efficient Aggregations\n```python\n# Use delay=True to batch multiple operations\nmean_x = df.x.mean(delay=True)\nstd_y = df.y.std(delay=True)\nsum_z = df.z.sum(delay=True)\n\n# Execute all at once\nresults = vaex.execute([mean_x, std_y, sum_z])\n```\n\n### Pattern: Virtual Columns for Feature Engineering\n```python\n# No memory overhead - computed on the fly\ndf['age_squared'] = df.age ** 2\ndf['full_name'] = df.first_name + ' ' + df.last_name\ndf['is_adult'] = df.age >= 18\n```\n\n## Resources\n\nThis skill includes reference documentation in the `references/` directory:\n\n- `core_dataframes.md` - DataFrame creation, loading, and basic structure\n- `data_processing.md` - Filtering, expressions, aggregations, and transformations\n- `performance.md` - Optimization strategies and lazy evaluation\n- `visualization.md` - Plotting and interactive visualizations\n- `machine_learning.md` - ML pipelines and model integration\n- `io_operations.md` - File formats and data import/export\n",
        "data/k-dense-ai/zarr-python/SKILL.md": "---\nname: zarr-python\ndescription: \"Chunked N-D arrays for cloud storage. Compressed arrays, parallel I/O, S3/GCS integration, NumPy/Dask/Xarray compatible, for large-scale scientific computing pipelines.\"\n---\n\n# Zarr Python\n\n## Overview\n\nZarr is a Python library for storing large N-dimensional arrays with chunking and compression. Apply this skill for efficient parallel I/O, cloud-native workflows, and seamless integration with NumPy, Dask, and Xarray.\n\n## Quick Start\n\n### Installation\n\n```bash\nuv pip install zarr\n```\n\nRequires Python 3.11+. For cloud storage support, install additional packages:\n```python\nuv pip install s3fs  # For S3\nuv pip install gcsfs  # For Google Cloud Storage\n```\n\n### Basic Array Creation\n\n```python\nimport zarr\nimport numpy as np\n\n# Create a 2D array with chunking and compression\nz = zarr.create_array(\n    store=\"data/my_array.zarr\",\n    shape=(10000, 10000),\n    chunks=(1000, 1000),\n    dtype=\"f4\"\n)\n\n# Write data using NumPy-style indexing\nz[:, :] = np.random.random((10000, 10000))\n\n# Read data\ndata = z[0:100, 0:100]  # Returns NumPy array\n```\n\n## Core Operations\n\n### Creating Arrays\n\nZarr provides multiple convenience functions for array creation:\n\n```python\n# Create empty array\nz = zarr.zeros(shape=(10000, 10000), chunks=(1000, 1000), dtype='f4',\n               store='data.zarr')\n\n# Create filled arrays\nz = zarr.ones((5000, 5000), chunks=(500, 500))\nz = zarr.full((1000, 1000), fill_value=42, chunks=(100, 100))\n\n# Create from existing data\ndata = np.arange(10000).reshape(100, 100)\nz = zarr.array(data, chunks=(10, 10), store='data.zarr')\n\n# Create like another array\nz2 = zarr.zeros_like(z)  # Matches shape, chunks, dtype of z\n```\n\n### Opening Existing Arrays\n\n```python\n# Open array (read/write mode by default)\nz = zarr.open_array('data.zarr', mode='r+')\n\n# Read-only mode\nz = zarr.open_array('data.zarr', mode='r')\n\n# The open() function auto-detects arrays vs groups\nz = zarr.open('data.zarr')  # Returns Array or Group\n```\n\n### Reading and Writing Data\n\nZarr arrays support NumPy-like indexing:\n\n```python\n# Write entire array\nz[:] = 42\n\n# Write slices\nz[0, :] = np.arange(100)\nz[10:20, 50:60] = np.random.random((10, 10))\n\n# Read data (returns NumPy array)\ndata = z[0:100, 0:100]\nrow = z[5, :]\n\n# Advanced indexing\nz.vindex[[0, 5, 10], [2, 8, 15]]  # Coordinate indexing\nz.oindex[0:10, [5, 10, 15]]       # Orthogonal indexing\nz.blocks[0, 0]                     # Block/chunk indexing\n```\n\n### Resizing and Appending\n\n```python\n# Resize array\nz.resize(15000, 15000)  # Expands or shrinks dimensions\n\n# Append data along an axis\nz.append(np.random.random((1000, 10000)), axis=0)  # Adds rows\n```\n\n## Chunking Strategies\n\nChunking is critical for performance. Choose chunk sizes and shapes based on access patterns.\n\n### Chunk Size Guidelines\n\n- **Minimum chunk size**: 1 MB recommended for optimal performance\n- **Balance**: Larger chunks = fewer metadata operations; smaller chunks = better parallel access\n- **Memory consideration**: Entire chunks must fit in memory during compression\n\n```python\n# Configure chunk size (aim for ~1MB per chunk)\n# For float32 data: 1MB = 262,144 elements = 512512 array\nz = zarr.zeros(\n    shape=(10000, 10000),\n    chunks=(512, 512),  # ~1MB chunks\n    dtype='f4'\n)\n```\n\n### Aligning Chunks with Access Patterns\n\n**Critical**: Chunk shape dramatically affects performance based on how data is accessed.\n\n```python\n# If accessing rows frequently (first dimension)\nz = zarr.zeros((10000, 10000), chunks=(10, 10000))  # Chunk spans columns\n\n# If accessing columns frequently (second dimension)\nz = zarr.zeros((10000, 10000), chunks=(10000, 10))  # Chunk spans rows\n\n# For mixed access patterns (balanced approach)\nz = zarr.zeros((10000, 10000), chunks=(1000, 1000))  # Square chunks\n```\n\n**Performance example**: For a (200, 200, 200) array, reading along the first dimension:\n- Using chunks (1, 200, 200): ~107ms\n- Using chunks (200, 200, 1): ~1.65ms (65 faster!)\n\n### Sharding for Large-Scale Storage\n\nWhen arrays have millions of small chunks, use sharding to group chunks into larger storage objects:\n\n```python\nfrom zarr.codecs import ShardingCodec, BytesCodec\nfrom zarr.codecs.blosc import BloscCodec\n\n# Create array with sharding\nz = zarr.create_array(\n    store='data.zarr',\n    shape=(100000, 100000),\n    chunks=(100, 100),  # Small chunks for access\n    shards=(1000, 1000),  # Groups 100 chunks per shard\n    dtype='f4'\n)\n```\n\n**Benefits**:\n- Reduces file system overhead from millions of small files\n- Improves cloud storage performance (fewer object requests)\n- Prevents filesystem block size waste\n\n**Important**: Entire shards must fit in memory before writing.\n\n## Compression\n\nZarr applies compression per chunk to reduce storage while maintaining fast access.\n\n### Configuring Compression\n\n```python\nfrom zarr.codecs.blosc import BloscCodec\nfrom zarr.codecs import GzipCodec, ZstdCodec\n\n# Default: Blosc with Zstandard\nz = zarr.zeros((1000, 1000), chunks=(100, 100))  # Uses default compression\n\n# Configure Blosc codec\nz = zarr.create_array(\n    store='data.zarr',\n    shape=(1000, 1000),\n    chunks=(100, 100),\n    dtype='f4',\n    codecs=[BloscCodec(cname='zstd', clevel=5, shuffle='shuffle')]\n)\n\n# Available Blosc compressors: 'blosclz', 'lz4', 'lz4hc', 'snappy', 'zlib', 'zstd'\n\n# Use Gzip compression\nz = zarr.create_array(\n    store='data.zarr',\n    shape=(1000, 1000),\n    chunks=(100, 100),\n    dtype='f4',\n    codecs=[GzipCodec(level=6)]\n)\n\n# Disable compression\nz = zarr.create_array(\n    store='data.zarr',\n    shape=(1000, 1000),\n    chunks=(100, 100),\n    dtype='f4',\n    codecs=[BytesCodec()]  # No compression\n)\n```\n\n### Compression Performance Tips\n\n- **Blosc** (default): Fast compression/decompression, good for interactive workloads\n- **Zstandard**: Better compression ratios, slightly slower than LZ4\n- **Gzip**: Maximum compression, slower performance\n- **LZ4**: Fastest compression, lower ratios\n- **Shuffle**: Enable shuffle filter for better compression on numeric data\n\n```python\n# Optimal for numeric scientific data\ncodecs=[BloscCodec(cname='zstd', clevel=5, shuffle='shuffle')]\n\n# Optimal for speed\ncodecs=[BloscCodec(cname='lz4', clevel=1)]\n\n# Optimal for compression ratio\ncodecs=[GzipCodec(level=9)]\n```\n\n## Storage Backends\n\nZarr supports multiple storage backends through a flexible storage interface.\n\n### Local Filesystem (Default)\n\n```python\nfrom zarr.storage import LocalStore\n\n# Explicit store creation\nstore = LocalStore('data/my_array.zarr')\nz = zarr.open_array(store=store, mode='w', shape=(1000, 1000), chunks=(100, 100))\n\n# Or use string path (creates LocalStore automatically)\nz = zarr.open_array('data/my_array.zarr', mode='w', shape=(1000, 1000),\n                    chunks=(100, 100))\n```\n\n### In-Memory Storage\n\n```python\nfrom zarr.storage import MemoryStore\n\n# Create in-memory store\nstore = MemoryStore()\nz = zarr.open_array(store=store, mode='w', shape=(1000, 1000), chunks=(100, 100))\n\n# Data exists only in memory, not persisted\n```\n\n### ZIP File Storage\n\n```python\nfrom zarr.storage import ZipStore\n\n# Write to ZIP file\nstore = ZipStore('data.zip', mode='w')\nz = zarr.open_array(store=store, mode='w', shape=(1000, 1000), chunks=(100, 100))\nz[:] = np.random.random((1000, 1000))\nstore.close()  # IMPORTANT: Must close ZipStore\n\n# Read from ZIP file\nstore = ZipStore('data.zip', mode='r')\nz = zarr.open_array(store=store)\ndata = z[:]\nstore.close()\n```\n\n### Cloud Storage (S3, GCS)\n\n```python\nimport s3fs\nimport zarr\n\n# S3 storage\ns3 = s3fs.S3FileSystem(anon=False)  # Use credentials\nstore = s3fs.S3Map(root='my-bucket/path/to/array.zarr', s3=s3)\nz = zarr.open_array(store=store, mode='w', shape=(1000, 1000), chunks=(100, 100))\nz[:] = data\n\n# Google Cloud Storage\nimport gcsfs\ngcs = gcsfs.GCSFileSystem(project='my-project')\nstore = gcsfs.GCSMap(root='my-bucket/path/to/array.zarr', gcs=gcs)\nz = zarr.open_array(store=store, mode='w', shape=(1000, 1000), chunks=(100, 100))\n```\n\n**Cloud Storage Best Practices**:\n- Use consolidated metadata to reduce latency: `zarr.consolidate_metadata(store)`\n- Align chunk sizes with cloud object sizing (typically 5-100 MB optimal)\n- Enable parallel writes using Dask for large-scale data\n- Consider sharding to reduce number of objects\n\n## Groups and Hierarchies\n\nGroups organize multiple arrays hierarchically, similar to directories or HDF5 groups.\n\n### Creating and Using Groups\n\n```python\n# Create root group\nroot = zarr.group(store='data/hierarchy.zarr')\n\n# Create sub-groups\ntemperature = root.create_group('temperature')\nprecipitation = root.create_group('precipitation')\n\n# Create arrays within groups\ntemp_array = temperature.create_array(\n    name='t2m',\n    shape=(365, 720, 1440),\n    chunks=(1, 720, 1440),\n    dtype='f4'\n)\n\nprecip_array = precipitation.create_array(\n    name='prcp',\n    shape=(365, 720, 1440),\n    chunks=(1, 720, 1440),\n    dtype='f4'\n)\n\n# Access using paths\narray = root['temperature/t2m']\n\n# Visualize hierarchy\nprint(root.tree())\n# Output:\n# /\n#   temperature\n#      t2m (365, 720, 1440) f4\n#   precipitation\n#       prcp (365, 720, 1440) f4\n```\n\n### H5py-Compatible API\n\nZarr provides an h5py-compatible interface for familiar HDF5 users:\n\n```python\n# Create group with h5py-style methods\nroot = zarr.group('data.zarr')\ndataset = root.create_dataset('my_data', shape=(1000, 1000), chunks=(100, 100),\n                              dtype='f4')\n\n# Access like h5py\ngrp = root.require_group('subgroup')\narr = grp.require_dataset('array', shape=(500, 500), chunks=(50, 50), dtype='i4')\n```\n\n## Attributes and Metadata\n\nAttach custom metadata to arrays and groups using attributes:\n\n```python\n# Add attributes to array\nz = zarr.zeros((1000, 1000), chunks=(100, 100))\nz.attrs['description'] = 'Temperature data in Kelvin'\nz.attrs['units'] = 'K'\nz.attrs['created'] = '2024-01-15'\nz.attrs['processing_version'] = 2.1\n\n# Attributes are stored as JSON\nprint(z.attrs['units'])  # Output: K\n\n# Add attributes to groups\nroot = zarr.group('data.zarr')\nroot.attrs['project'] = 'Climate Analysis'\nroot.attrs['institution'] = 'Research Institute'\n\n# Attributes persist with the array/group\nz2 = zarr.open('data.zarr')\nprint(z2.attrs['description'])\n```\n\n**Important**: Attributes must be JSON-serializable (strings, numbers, lists, dicts, booleans, null).\n\n## Integration with NumPy, Dask, and Xarray\n\n### NumPy Integration\n\nZarr arrays implement the NumPy array interface:\n\n```python\nimport numpy as np\nimport zarr\n\nz = zarr.zeros((1000, 1000), chunks=(100, 100))\n\n# Use NumPy functions directly\nresult = np.sum(z, axis=0)  # NumPy operates on Zarr array\nmean = np.mean(z[:100, :100])\n\n# Convert to NumPy array\nnumpy_array = z[:]  # Loads entire array into memory\n```\n\n### Dask Integration\n\nDask provides lazy, parallel computation on Zarr arrays:\n\n```python\nimport dask.array as da\nimport zarr\n\n# Create large Zarr array\nz = zarr.open('data.zarr', mode='w', shape=(100000, 100000),\n              chunks=(1000, 1000), dtype='f4')\n\n# Load as Dask array (lazy, no data loaded)\ndask_array = da.from_zarr('data.zarr')\n\n# Perform computations (parallel, out-of-core)\nresult = dask_array.mean(axis=0).compute()  # Parallel computation\n\n# Write Dask array to Zarr\nlarge_array = da.random.random((100000, 100000), chunks=(1000, 1000))\nda.to_zarr(large_array, 'output.zarr')\n```\n\n**Benefits**:\n- Process datasets larger than memory\n- Automatic parallel computation across chunks\n- Efficient I/O with chunked storage\n\n### Xarray Integration\n\nXarray provides labeled, multidimensional arrays with Zarr backend:\n\n```python\nimport xarray as xr\nimport zarr\n\n# Open Zarr store as Xarray Dataset (lazy loading)\nds = xr.open_zarr('data.zarr')\n\n# Dataset includes coordinates and metadata\nprint(ds)\n\n# Access variables\ntemperature = ds['temperature']\n\n# Perform labeled operations\nsubset = ds.sel(time='2024-01', lat=slice(30, 60))\n\n# Write Xarray Dataset to Zarr\nds.to_zarr('output.zarr')\n\n# Create from scratch with coordinates\nds = xr.Dataset(\n    {\n        'temperature': (['time', 'lat', 'lon'], data),\n        'precipitation': (['time', 'lat', 'lon'], data2)\n    },\n    coords={\n        'time': pd.date_range('2024-01-01', periods=365),\n        'lat': np.arange(-90, 91, 1),\n        'lon': np.arange(-180, 180, 1)\n    }\n)\nds.to_zarr('climate_data.zarr')\n```\n\n**Benefits**:\n- Named dimensions and coordinates\n- Label-based indexing and selection\n- Integration with pandas for time series\n- NetCDF-like interface familiar to climate/geospatial scientists\n\n## Parallel Computing and Synchronization\n\n### Thread-Safe Operations\n\n```python\nfrom zarr import ThreadSynchronizer\nimport zarr\n\n# For multi-threaded writes\nsynchronizer = ThreadSynchronizer()\nz = zarr.open_array('data.zarr', mode='r+', shape=(10000, 10000),\n                    chunks=(1000, 1000), synchronizer=synchronizer)\n\n# Safe for concurrent writes from multiple threads\n# (when writes don't span chunk boundaries)\n```\n\n### Process-Safe Operations\n\n```python\nfrom zarr import ProcessSynchronizer\nimport zarr\n\n# For multi-process writes\nsynchronizer = ProcessSynchronizer('sync_data.sync')\nz = zarr.open_array('data.zarr', mode='r+', shape=(10000, 10000),\n                    chunks=(1000, 1000), synchronizer=synchronizer)\n\n# Safe for concurrent writes from multiple processes\n```\n\n**Note**:\n- Concurrent reads require no synchronization\n- Synchronization only needed for writes that may span chunk boundaries\n- Each process/thread writing to separate chunks needs no synchronization\n\n## Consolidated Metadata\n\nFor hierarchical stores with many arrays, consolidate metadata into a single file to reduce I/O operations:\n\n```python\nimport zarr\n\n# After creating arrays/groups\nroot = zarr.group('data.zarr')\n# ... create multiple arrays/groups ...\n\n# Consolidate metadata\nzarr.consolidate_metadata('data.zarr')\n\n# Open with consolidated metadata (faster, especially on cloud storage)\nroot = zarr.open_consolidated('data.zarr')\n```\n\n**Benefits**:\n- Reduces metadata read operations from N (one per array) to 1\n- Critical for cloud storage (reduces latency)\n- Speeds up `tree()` operations and group traversal\n\n**Cautions**:\n- Metadata can become stale if arrays update without re-consolidation\n- Not suitable for frequently-updated datasets\n- Multi-writer scenarios may have inconsistent reads\n\n## Performance Optimization\n\n### Checklist for Optimal Performance\n\n1. **Chunk Size**: Aim for 1-10 MB per chunk\n   ```python\n   # For float32: 1MB = 262,144 elements\n   chunks = (512, 512)  # 5125124 bytes = ~1MB\n   ```\n\n2. **Chunk Shape**: Align with access patterns\n   ```python\n   # Row-wise access  chunk spans columns: (small, large)\n   # Column-wise access  chunk spans rows: (large, small)\n   # Random access  balanced: (medium, medium)\n   ```\n\n3. **Compression**: Choose based on workload\n   ```python\n   # Interactive/fast: BloscCodec(cname='lz4')\n   # Balanced: BloscCodec(cname='zstd', clevel=5)\n   # Maximum compression: GzipCodec(level=9)\n   ```\n\n4. **Storage Backend**: Match to environment\n   ```python\n   # Local: LocalStore (default)\n   # Cloud: S3Map/GCSMap with consolidated metadata\n   # Temporary: MemoryStore\n   ```\n\n5. **Sharding**: Use for large-scale datasets\n   ```python\n   # When you have millions of small chunks\n   shards=(10*chunk_size, 10*chunk_size)\n   ```\n\n6. **Parallel I/O**: Use Dask for large operations\n   ```python\n   import dask.array as da\n   dask_array = da.from_zarr('data.zarr')\n   result = dask_array.compute(scheduler='threads', num_workers=8)\n   ```\n\n### Profiling and Debugging\n\n```python\n# Print detailed array information\nprint(z.info)\n\n# Output includes:\n# - Type, shape, chunks, dtype\n# - Compression codec and level\n# - Storage size (compressed vs uncompressed)\n# - Storage location\n\n# Check storage size\nprint(f\"Compressed size: {z.nbytes_stored / 1e6:.2f} MB\")\nprint(f\"Uncompressed size: {z.nbytes / 1e6:.2f} MB\")\nprint(f\"Compression ratio: {z.nbytes / z.nbytes_stored:.2f}x\")\n```\n\n## Common Patterns and Best Practices\n\n### Pattern: Time Series Data\n\n```python\n# Store time series with time as first dimension\n# This allows efficient appending of new time steps\nz = zarr.open('timeseries.zarr', mode='a',\n              shape=(0, 720, 1440),  # Start with 0 time steps\n              chunks=(1, 720, 1440),  # One time step per chunk\n              dtype='f4')\n\n# Append new time steps\nnew_data = np.random.random((1, 720, 1440))\nz.append(new_data, axis=0)\n```\n\n### Pattern: Large Matrix Operations\n\n```python\nimport dask.array as da\n\n# Create large matrix in Zarr\nz = zarr.open('matrix.zarr', mode='w',\n              shape=(100000, 100000),\n              chunks=(1000, 1000),\n              dtype='f8')\n\n# Use Dask for parallel computation\ndask_z = da.from_zarr('matrix.zarr')\nresult = (dask_z @ dask_z.T).compute()  # Parallel matrix multiply\n```\n\n### Pattern: Cloud-Native Workflow\n\n```python\nimport s3fs\nimport zarr\n\n# Write to S3\ns3 = s3fs.S3FileSystem()\nstore = s3fs.S3Map(root='s3://my-bucket/data.zarr', s3=s3)\n\n# Create array with appropriate chunking for cloud\nz = zarr.open_array(store=store, mode='w',\n                    shape=(10000, 10000),\n                    chunks=(500, 500),  # ~1MB chunks\n                    dtype='f4')\nz[:] = data\n\n# Consolidate metadata for faster reads\nzarr.consolidate_metadata(store)\n\n# Read from S3 (anywhere, anytime)\nstore_read = s3fs.S3Map(root='s3://my-bucket/data.zarr', s3=s3)\nz_read = zarr.open_consolidated(store_read)\nsubset = z_read[0:100, 0:100]\n```\n\n### Pattern: Format Conversion\n\n```python\n# HDF5 to Zarr\nimport h5py\nimport zarr\n\nwith h5py.File('data.h5', 'r') as h5:\n    dataset = h5['dataset_name']\n    z = zarr.array(dataset[:],\n                   chunks=(1000, 1000),\n                   store='data.zarr')\n\n# NumPy to Zarr\nimport numpy as np\ndata = np.load('data.npy')\nz = zarr.array(data, chunks='auto', store='data.zarr')\n\n# Zarr to NetCDF (via Xarray)\nimport xarray as xr\nds = xr.open_zarr('data.zarr')\nds.to_netcdf('data.nc')\n```\n\n## Common Issues and Solutions\n\n### Issue: Slow Performance\n\n**Diagnosis**: Check chunk size and alignment\n```python\nprint(z.chunks)  # Are chunks appropriate size?\nprint(z.info)    # Check compression ratio\n```\n\n**Solutions**:\n- Increase chunk size to 1-10 MB\n- Align chunks with access pattern\n- Try different compression codecs\n- Use Dask for parallel operations\n\n### Issue: High Memory Usage\n\n**Cause**: Loading entire array or large chunks into memory\n\n**Solutions**:\n```python\n# Don't load entire array\n# Bad: data = z[:]\n# Good: Process in chunks\nfor i in range(0, z.shape[0], 1000):\n    chunk = z[i:i+1000, :]\n    process(chunk)\n\n# Or use Dask for automatic chunking\nimport dask.array as da\ndask_z = da.from_zarr('data.zarr')\nresult = dask_z.mean().compute()  # Processes in chunks\n```\n\n### Issue: Cloud Storage Latency\n\n**Solutions**:\n```python\n# 1. Consolidate metadata\nzarr.consolidate_metadata(store)\nz = zarr.open_consolidated(store)\n\n# 2. Use appropriate chunk sizes (5-100 MB for cloud)\nchunks = (2000, 2000)  # Larger chunks for cloud\n\n# 3. Enable sharding\nshards = (10000, 10000)  # Groups many chunks\n```\n\n### Issue: Concurrent Write Conflicts\n\n**Solution**: Use synchronizers or ensure non-overlapping writes\n```python\nfrom zarr import ProcessSynchronizer\n\nsync = ProcessSynchronizer('sync.sync')\nz = zarr.open_array('data.zarr', mode='r+', synchronizer=sync)\n\n# Or design workflow so each process writes to separate chunks\n```\n\n## Additional Resources\n\nFor detailed API documentation, advanced usage, and the latest updates:\n\n- **Official Documentation**: https://zarr.readthedocs.io/\n- **Zarr Specifications**: https://zarr-specs.readthedocs.io/\n- **GitHub Repository**: https://github.com/zarr-developers/zarr-python\n- **Community Chat**: https://gitter.im/zarr-developers/community\n\n**Related Libraries**:\n- **Xarray**: https://docs.xarray.dev/ (labeled arrays)\n- **Dask**: https://docs.dask.org/ (parallel computing)\n- **NumCodecs**: https://numcodecs.readthedocs.io/ (compression codecs)\n",
        "data/k-dense-ai/zinc-database/SKILL.md": "---\nname: zinc-database\ndescription: \"Access ZINC (230M+ purchasable compounds). Search by ZINC ID/SMILES, similarity searches, 3D-ready structures for docking, analog discovery, for virtual screening and drug discovery.\"\n---\n\n# ZINC Database\n\n## Overview\n\nZINC is a freely accessible repository of 230M+ purchasable compounds maintained by UCSF. Search by ZINC ID or SMILES, perform similarity searches, download 3D-ready structures for docking, discover analogs for virtual screening and drug discovery.\n\n## When to Use This Skill\n\nThis skill should be used when:\n\n- **Virtual screening**: Finding compounds for molecular docking studies\n- **Lead discovery**: Identifying commercially-available compounds for drug development\n- **Structure searches**: Performing similarity or analog searches by SMILES\n- **Compound retrieval**: Looking up molecules by ZINC IDs or supplier codes\n- **Chemical space exploration**: Exploring purchasable chemical diversity\n- **Docking studies**: Accessing 3D-ready molecular structures\n- **Analog searches**: Finding similar compounds based on structural similarity\n- **Supplier queries**: Identifying compounds from specific chemical vendors\n- **Random sampling**: Obtaining random compound sets for screening\n\n## Database Versions\n\nZINC has evolved through multiple versions:\n\n- **ZINC22** (Current): Largest version with 230+ million purchasable compounds and multi-billion scale make-on-demand compounds\n- **ZINC20**: Still maintained, focused on lead-like and drug-like compounds\n- **ZINC15**: Predecessor version, legacy but still documented\n\nThis skill primarily focuses on ZINC22, the most current and comprehensive version.\n\n## Access Methods\n\n### Web Interface\n\nPrimary access point: https://zinc.docking.org/\nInteractive searching: https://cartblanche22.docking.org/\n\n### API Access\n\nAll ZINC22 searches can be performed programmatically via the CartBlanche22 API:\n\n**Base URL**: `https://cartblanche22.docking.org/`\n\nAll API endpoints return data in text or JSON format with customizable fields.\n\n## Core Capabilities\n\n### 1. Search by ZINC ID\n\nRetrieve specific compounds using their ZINC identifiers.\n\n**Web interface**: https://cartblanche22.docking.org/search/zincid\n\n**API endpoint**:\n```bash\ncurl \"https://cartblanche22.docking.org/[email protected]_fields=smiles,zinc_id\"\n```\n\n**Multiple IDs**:\n```bash\ncurl \"https://cartblanche22.docking.org/substances.txt:zinc_id=ZINC000000000001,ZINC000000000002&output_fields=smiles,zinc_id,tranche\"\n```\n\n**Response fields**: `zinc_id`, `smiles`, `sub_id`, `supplier_code`, `catalogs`, `tranche` (includes H-count, LogP, MW, phase)\n\n### 2. Search by SMILES\n\nFind compounds by chemical structure using SMILES notation, with optional distance parameters for analog searching.\n\n**Web interface**: https://cartblanche22.docking.org/search/smiles\n\n**API endpoint**:\n```bash\ncurl \"https://cartblanche22.docking.org/[email protected]=4-Fadist=4\"\n```\n\n**Parameters**:\n- `smiles`: Query SMILES string (URL-encoded if necessary)\n- `dist`: Tanimoto distance threshold (default: 0 for exact match)\n- `adist`: Alternative distance parameter for broader searches (default: 0)\n- `output_fields`: Comma-separated list of desired output fields\n\n**Example - Exact match**:\n```bash\ncurl \"https://cartblanche22.docking.org/smiles.txt:smiles=c1ccccc1\"\n```\n\n**Example - Similarity search**:\n```bash\ncurl \"https://cartblanche22.docking.org/smiles.txt:smiles=c1ccccc1&dist=3&output_fields=zinc_id,smiles,tranche\"\n```\n\n### 3. Search by Supplier Codes\n\nQuery compounds from specific chemical suppliers or retrieve all molecules from particular catalogs.\n\n**Web interface**: https://cartblanche22.docking.org/search/catitems\n\n**API endpoint**:\n```bash\ncurl \"https://cartblanche22.docking.org/catitems.txt:catitem_id=SUPPLIER-CODE-123\"\n```\n\n**Use cases**:\n- Verify compound availability from specific vendors\n- Retrieve all compounds from a catalog\n- Cross-reference supplier codes with ZINC IDs\n\n### 4. Random Compound Sampling\n\nGenerate random compound sets for screening or benchmarking purposes.\n\n**Web interface**: https://cartblanche22.docking.org/search/random\n\n**API endpoint**:\n```bash\ncurl \"https://cartblanche22.docking.org/substance/random.txt:count=100\"\n```\n\n**Parameters**:\n- `count`: Number of random compounds to retrieve (default: 100)\n- `subset`: Filter by subset (e.g., 'lead-like', 'drug-like', 'fragment')\n- `output_fields`: Customize returned data fields\n\n**Example - Random lead-like molecules**:\n```bash\ncurl \"https://cartblanche22.docking.org/substance/random.txt:count=1000&subset=lead-like&output_fields=zinc_id,smiles,tranche\"\n```\n\n## Common Workflows\n\n### Workflow 1: Preparing a Docking Library\n\n1. **Define search criteria** based on target properties or desired chemical space\n\n2. **Query ZINC22** using appropriate search method:\n   ```bash\n   # Example: Get drug-like compounds with specific LogP and MW\n   curl \"https://cartblanche22.docking.org/substance/random.txt:count=10000&subset=drug-like&output_fields=zinc_id,smiles,tranche\" > docking_library.txt\n   ```\n\n3. **Parse results** to extract ZINC IDs and SMILES:\n   ```python\n   import pandas as pd\n\n   # Load results\n   df = pd.read_csv('docking_library.txt', sep='\\t')\n\n   # Filter by properties in tranche data\n   # Tranche format: H##P###M###-phase\n   # H = H-bond donors, P = LogP*10, M = MW\n   ```\n\n4. **Download 3D structures** for docking using ZINC ID or download from file repositories\n\n### Workflow 2: Finding Analogs of a Hit Compound\n\n1. **Obtain SMILES** of the hit compound:\n   ```python\n   hit_smiles = \"CC(C)Cc1ccc(cc1)C(C)C(=O)O\"  # Example: Ibuprofen\n   ```\n\n2. **Perform similarity search** with distance threshold:\n   ```bash\n   curl \"https://cartblanche22.docking.org/smiles.txt:smiles=CC(C)Cc1ccc(cc1)C(C)C(=O)O&dist=5&output_fields=zinc_id,smiles,catalogs\" > analogs.txt\n   ```\n\n3. **Analyze results** to identify purchasable analogs:\n   ```python\n   import pandas as pd\n\n   analogs = pd.read_csv('analogs.txt', sep='\\t')\n   print(f\"Found {len(analogs)} analogs\")\n   print(analogs[['zinc_id', 'smiles', 'catalogs']].head(10))\n   ```\n\n4. **Retrieve 3D structures** for the most promising analogs\n\n### Workflow 3: Batch Compound Retrieval\n\n1. **Compile list of ZINC IDs** from literature, databases, or previous screens:\n   ```python\n   zinc_ids = [\n       \"ZINC000000000001\",\n       \"ZINC000000000002\",\n       \"ZINC000000000003\"\n   ]\n   zinc_ids_str = \",\".join(zinc_ids)\n   ```\n\n2. **Query ZINC22 API**:\n   ```bash\n   curl \"https://cartblanche22.docking.org/substances.txt:zinc_id=ZINC000000000001,ZINC000000000002&output_fields=zinc_id,smiles,supplier_code,catalogs\"\n   ```\n\n3. **Process results** for downstream analysis or purchasing\n\n### Workflow 4: Chemical Space Sampling\n\n1. **Select subset parameters** based on screening goals:\n   - Fragment: MW < 250, good for fragment-based drug discovery\n   - Lead-like: MW 250-350, LogP  3.5\n   - Drug-like: MW 350-500, follows Lipinski's Rule of Five\n\n2. **Generate random sample**:\n   ```bash\n   curl \"https://cartblanche22.docking.org/substance/random.txt:count=5000&subset=lead-like&output_fields=zinc_id,smiles,tranche\" > chemical_space_sample.txt\n   ```\n\n3. **Analyze chemical diversity** and prepare for virtual screening\n\n## Output Fields\n\nCustomize API responses with the `output_fields` parameter:\n\n**Available fields**:\n- `zinc_id`: ZINC identifier\n- `smiles`: SMILES string representation\n- `sub_id`: Internal substance ID\n- `supplier_code`: Vendor catalog number\n- `catalogs`: List of suppliers offering the compound\n- `tranche`: Encoded molecular properties (H-count, LogP, MW, reactivity phase)\n\n**Example**:\n```bash\ncurl \"https://cartblanche22.docking.org/substances.txt:zinc_id=ZINC000000000001&output_fields=zinc_id,smiles,catalogs,tranche\"\n```\n\n## Tranche System\n\nZINC organizes compounds into \"tranches\" based on molecular properties:\n\n**Format**: `H##P###M###-phase`\n\n- **H##**: Number of hydrogen bond donors (00-99)\n- **P###**: LogP  10 (e.g., P035 = LogP 3.5)\n- **M###**: Molecular weight in Daltons (e.g., M400 = 400 Da)\n- **phase**: Reactivity classification\n\n**Example tranche**: `H05P035M400-0`\n- 5 H-bond donors\n- LogP = 3.5\n- MW = 400 Da\n- Reactivity phase 0\n\nUse tranche data to filter compounds by drug-likeness criteria.\n\n## Downloading 3D Structures\n\nFor molecular docking, 3D structures are available via file repositories:\n\n**File repository**: https://files.docking.org/zinc22/\n\nStructures are organized by tranches and available in multiple formats:\n- MOL2: Multi-molecule format with 3D coordinates\n- SDF: Structure-data file format\n- DB2.GZ: Compressed database format for DOCK\n\nRefer to ZINC documentation at https://wiki.docking.org for downloading protocols and batch access methods.\n\n## Python Integration\n\n### Using curl with Python\n\n```python\nimport subprocess\nimport json\n\ndef query_zinc_by_id(zinc_id, output_fields=\"zinc_id,smiles,catalogs\"):\n    \"\"\"Query ZINC22 by ZINC ID.\"\"\"\n    url = f\"https://cartblanche22.docking.org/[email protected]_id={zinc_id}&output_fields={output_fields}\"\n    result = subprocess.run(['curl', url], capture_output=True, text=True)\n    return result.stdout\n\ndef search_by_smiles(smiles, dist=0, adist=0, output_fields=\"zinc_id,smiles\"):\n    \"\"\"Search ZINC22 by SMILES with optional distance parameters.\"\"\"\n    url = f\"https://cartblanche22.docking.org/smiles.txt:smiles={smiles}&dist={dist}&adist={adist}&output_fields={output_fields}\"\n    result = subprocess.run(['curl', url], capture_output=True, text=True)\n    return result.stdout\n\ndef get_random_compounds(count=100, subset=None, output_fields=\"zinc_id,smiles,tranche\"):\n    \"\"\"Get random compounds from ZINC22.\"\"\"\n    url = f\"https://cartblanche22.docking.org/substance/random.txt:count={count}&output_fields={output_fields}\"\n    if subset:\n        url += f\"&subset={subset}\"\n    result = subprocess.run(['curl', url], capture_output=True, text=True)\n    return result.stdout\n```\n\n### Parsing Results\n\n```python\nimport pandas as pd\nfrom io import StringIO\n\n# Query ZINC and parse as DataFrame\nresult = query_zinc_by_id(\"ZINC000000000001\")\ndf = pd.read_csv(StringIO(result), sep='\\t')\n\n# Extract tranche properties\ndef parse_tranche(tranche_str):\n    \"\"\"Parse ZINC tranche code to extract properties.\"\"\"\n    # Format: H##P###M###-phase\n    import re\n    match = re.match(r'H(\\d+)P(\\d+)M(\\d+)-(\\d+)', tranche_str)\n    if match:\n        return {\n            'h_donors': int(match.group(1)),\n            'logP': int(match.group(2)) / 10.0,\n            'mw': int(match.group(3)),\n            'phase': int(match.group(4))\n        }\n    return None\n\ndf['tranche_props'] = df['tranche'].apply(parse_tranche)\n```\n\n## Best Practices\n\n### Query Optimization\n\n- **Start specific**: Begin with exact searches before expanding to similarity searches\n- **Use appropriate distance parameters**: Small dist values (1-3) for close analogs, larger (5-10) for diverse analogs\n- **Limit output fields**: Request only necessary fields to reduce data transfer\n- **Batch queries**: Combine multiple ZINC IDs in a single API call when possible\n\n### Performance Considerations\n\n- **Rate limiting**: Respect server resources; avoid rapid consecutive requests\n- **Caching**: Store frequently accessed compounds locally\n- **Parallel downloads**: When downloading 3D structures, use parallel wget or aria2c for file repositories\n- **Subset filtering**: Use lead-like, drug-like, or fragment subsets to reduce search space\n\n### Data Quality\n\n- **Verify availability**: Supplier catalogs change; confirm compound availability before large orders\n- **Check stereochemistry**: SMILES may not fully specify stereochemistry; verify 3D structures\n- **Validate structures**: Use cheminformatics tools (RDKit, OpenBabel) to verify structure validity\n- **Cross-reference**: When possible, cross-check with other databases (PubChem, ChEMBL)\n\n## Resources\n\n### references/api_reference.md\n\nComprehensive documentation including:\n\n- Complete API endpoint reference\n- URL syntax and parameter specifications\n- Advanced query patterns and examples\n- File repository organization and access\n- Bulk download methods\n- Error handling and troubleshooting\n- Integration with molecular docking software\n\nConsult this document for detailed technical information and advanced usage patterns.\n\n## Important Disclaimers\n\n### Data Reliability\n\nZINC explicitly states: **\"We do not guarantee the quality of any molecule for any purpose and take no responsibility for errors arising from the use of this database.\"**\n\n- Compound availability may change without notice\n- Structure representations may contain errors\n- Supplier information should be verified independently\n- Use appropriate validation before experimental work\n\n### Appropriate Use\n\n- ZINC is intended for academic and research purposes in drug discovery\n- Verify licensing terms for commercial use\n- Respect intellectual property when working with patented compounds\n- Follow your institution's guidelines for compound procurement\n\n## Additional Resources\n\n- **ZINC Website**: https://zinc.docking.org/\n- **CartBlanche22 Interface**: https://cartblanche22.docking.org/\n- **ZINC Wiki**: https://wiki.docking.org/\n- **File Repository**: https://files.docking.org/zinc22/\n- **GitHub**: https://github.com/docking-org/\n- **Primary Publication**: Irwin et al., J. Chem. Inf. Model 2020 (ZINC15)\n- **ZINC22 Publication**: Irwin et al., J. Chem. Inf. Model 2023\n\n## Citations\n\nWhen using ZINC in publications, cite the appropriate version:\n\n**ZINC22**:\nIrwin, J. J., et al. \"ZINC22A Free Multi-Billion-Scale Database of Tangible Compounds for Ligand Discovery.\" *Journal of Chemical Information and Modeling* 2023.\n\n**ZINC15**:\nIrwin, J. J., et al. \"ZINC15  Ligand Discovery for Everyone.\" *Journal of Chemical Information and Modeling* 2020, 60, 60656073.\n",
        "data/openai/.curated/gh-address-comments/SKILL.md": "---\nname: gh-address-comments\ndescription: Help address review/issue comments on the open GitHub PR for the current branch using gh CLI; verify gh auth first and prompt the user to authenticate if not logged in.\nmetadata:\n  short-description: Address comments in a GitHub PR review\n---\n\n# PR Comment Handler\n\nGuide to find the open PR for the current branch and address its comments with gh CLI. Run all `gh` commands with elevated network access.\n\nPrereq: ensure `gh` is authenticated (for example, run `gh auth login` once), then run `gh auth status` with escalated permissions (include workflow/repo scopes) so `gh` commands succeed. If sandboxing blocks `gh auth status`, rerun it with `sandbox_permissions=require_escalated`.\n\n## 1) Inspect comments needing attention\n- Run scripts/fetch_comments.py which will print out all the comments and review threads on the PR\n\n## 2) Ask the user for clarification\n- Number all the review threads and comments and provide a short summary of what would be required to apply a fix for it\n- Ask the user which numbered comments should be addressed\n\n## 3) If user chooses comments\n- Apply fixes for the selected comments\n\nNotes:\n- If gh hits auth/rate issues mid-run, prompt the user to re-authenticate with `gh auth login`, then retry.\n",
        "data/openai/.curated/gh-fix-ci/SKILL.md": "---\nname: gh-fix-ci\ndescription: Inspect GitHub PR checks with gh, pull failing GitHub Actions logs, summarize failure context, then create a fix plan and implement after user approval. Use when a user asks to debug or fix failing PR CI/CD checks on GitHub Actions and wants a plan + code changes; for external checks (e.g., Buildkite), only report the details URL and mark them out of scope.\nmetadata:\n  short-description: Fix failing Github CI actions\n---\n\n# Gh Pr Checks Plan Fix\n\n## Overview\n\nUse gh to locate failing PR checks, fetch GitHub Actions logs for actionable failures, summarize the failure snippet, then propose a fix plan and implement after explicit approval.\n- Depends on the `plan` skill for drafting and approving the fix plan.\n\nPrereq: ensure `gh` is authenticated (for example, run `gh auth login` once), then run `gh auth status` with escalated permissions (include workflow/repo scopes) so `gh` commands succeed. If sandboxing blocks `gh auth status`, rerun it with `sandbox_permissions=require_escalated`.\n\n## Inputs\n\n- `repo`: path inside the repo (default `.`)\n- `pr`: PR number or URL (optional; defaults to current branch PR)\n- `gh` authentication for the repo host\n\n## Quick start\n\n- `python \"<path-to-skill>/scripts/inspect_pr_checks.py\" --repo \".\" --pr \"<number-or-url>\"`\n- Add `--json` if you want machine-friendly output for summarization.\n\n## Workflow\n\n1. Verify gh authentication.\n   - Run `gh auth status` in the repo with escalated scopes (workflow/repo) after running `gh auth login`.\n   - If sandboxed auth status fails, rerun the command with `sandbox_permissions=require_escalated` to allow network/keyring access.\n   - If unauthenticated, ask the user to log in before proceeding.\n2. Resolve the PR.\n   - Prefer the current branch PR: `gh pr view --json number,url`.\n   - If the user provides a PR number or URL, use that directly.\n3. Inspect failing checks (GitHub Actions only).\n   - Preferred: run the bundled script (handles gh field drift and job-log fallbacks):\n     - `python \"<path-to-skill>/scripts/inspect_pr_checks.py\" --repo \".\" --pr \"<number-or-url>\"`\n     - Add `--json` for machine-friendly output.\n   - Manual fallback:\n     - `gh pr checks <pr> --json name,state,bucket,link,startedAt,completedAt,workflow`\n       - If a field is rejected, rerun with the available fields reported by `gh`.\n     - For each failing check, extract the run id from `detailsUrl` and run:\n       - `gh run view <run_id> --json name,workflowName,conclusion,status,url,event,headBranch,headSha`\n       - `gh run view <run_id> --log`\n     - If the run log says it is still in progress, fetch job logs directly:\n       - `gh api \"/repos/<owner>/<repo>/actions/jobs/<job_id>/logs\" > \"<path>\"`\n4. Scope non-GitHub Actions checks.\n   - If `detailsUrl` is not a GitHub Actions run, label it as external and only report the URL.\n   - Do not attempt Buildkite or other providers; keep the workflow lean.\n5. Summarize failures for the user.\n   - Provide the failing check name, run URL (if any), and a concise log snippet.\n   - Call out missing logs explicitly.\n6. Create a plan.\n   - Use the `plan` skill to draft a concise plan and request approval.\n7. Implement after approval.\n   - Apply the approved plan, summarize diffs/tests, and ask about opening a PR.\n8. Recheck status.\n   - After changes, suggest re-running the relevant tests and `gh pr checks` to confirm.\n\n## Bundled Resources\n\n### scripts/inspect_pr_checks.py\n\nFetch failing PR checks, pull GitHub Actions logs, and extract a failure snippet. Exits non-zero when failures remain so it can be used in automation.\n\nUsage examples:\n- `python \"<path-to-skill>/scripts/inspect_pr_checks.py\" --repo \".\" --pr \"123\"`\n- `python \"<path-to-skill>/scripts/inspect_pr_checks.py\" --repo \".\" --pr \"https://github.com/org/repo/pull/123\" --json`\n- `python \"<path-to-skill>/scripts/inspect_pr_checks.py\" --repo \".\" --max-lines 200 --context 40`\n",
        "data/openai/.curated/notion-knowledge-capture/SKILL.md": "---\nname: notion-knowledge-capture\ndescription: Capture conversations and decisions into structured Notion pages; use when turning chats/notes into wiki entries, how-tos, decisions, or FAQs with proper linking.\nmetadata:\n  short-description: Capture conversations into structured Notion pages\n---\n\n# Knowledge Capture\n\nConvert conversations and notes into structured, linkable Notion pages for easy reuse.\n\n## Quick start\n1) Clarify what to capture (decision, how-to, FAQ, learning, documentation) and target audience.\n2) Identify the right database/template in `reference/` (team wiki, how-to, FAQ, decision log, learning, documentation).\n3) Pull any prior context from Notion with `Notion:notion-search`  `Notion:notion-fetch` (existing pages to update/link).\n4) Draft the page with `Notion:notion-create-pages` using the databases schema; include summary, context, source links, and tags/owners.\n5) Link from hub pages and related records; update status/owners with `Notion:notion-update-page` as the source evolves.\n\n## Workflow\n### 0) If any MCP call fails because Notion MCP is not connected, pause and set it up:\n1. Add the Notion MCP:\n   - `codex mcp add notion --url https://mcp.notion.com/mcp`\n2. Enable remote MCP client:\n   - Set `[features].rmcp_client = true` in `config.toml` **or** run `codex --enable rmcp_client`\n3. Log in with OAuth:\n   - `codex mcp login notion`\n\nAfter successful login, the user will have to restart codex. You should finish your answer and tell them so when they try again they can continue with Step 1.\n\n### 1) Define the capture\n- Ask purpose, audience, freshness, and whether this is new or an update.\n- Determine content type: decision, how-to, FAQ, concept/wiki entry, learning/note, documentation page.\n\n### 2) Locate destination\n- Pick the correct database using `reference/*-database.md` guides; confirm required properties (title, tags, owner, status, date, relations).\n- If multiple candidate databases, ask the user which to use; otherwise, create in the primary wiki/documentation DB.\n\n### 3) Extract and structure\n- Extract facts, decisions, actions, and rationale from the conversation.\n- For decisions, record alternatives, rationale, and outcomes.\n- For how-tos/docs, capture steps, pre-reqs, links to assets/code, and edge cases.\n- For FAQs, phrase as Q&A with concise answers and links to deeper docs.\n\n### 4) Create/update in Notion\n- Use `Notion:notion-create-pages` with the correct `data_source_id`; set properties (title, tags, owner, status, dates, relations).\n- Use templates in `reference/` to structure content (section headers, checklists).\n- If updating an existing page, fetch then edit via `Notion:notion-update-page`.\n\n### 5) Link and surface\n- Add relations/backlinks to hub pages, related specs/docs, and teams.\n- Add a short summary/changelog for future readers.\n- If follow-up tasks exist, create tasks in the relevant database and link them.\n\n## References and examples\n- `reference/`  database schemas and templates (e.g., `team-wiki-database.md`, `how-to-guide-database.md`, `faq-database.md`, `decision-log-database.md`, `documentation-database.md`, `learning-database.md`, `database-best-practices.md`).\n- `examples/`  capture patterns in practice (e.g., `decision-capture.md`, `how-to-guide.md`, `conversation-to-faq.md`).\n",
        "data/openai/.curated/notion-knowledge-capture/evaluations/README.md": "# Knowledge Capture Skill Evaluations\n\nEvaluation scenarios for testing the Knowledge Capture skill across different Codex models.\n\n## Purpose\n\nThese evaluations ensure the Knowledge Capture skill:\n- Correctly identifies content types (how-to guides, FAQs, decision records, wikis)\n- Extracts relevant information from conversations\n- Structures content appropriately for each type\n- Searches and places content in the right Notion location\n- Works consistently across Haiku, Sonnet, and Opus\n\n## Evaluation Files\n\n### conversation-to-wiki.json\nTests capturing conversation content as a how-to guide for the team wiki.\n\n**Scenario**: Save deployment discussion to wiki  \n**Key Behaviors**:\n- Extracts steps, gotchas, and best practices from conversation\n- Identifies content as How-To Guide\n- Structures with proper sections (Overview, Prerequisites, Steps, Troubleshooting)\n- Searches for team wiki location\n- Preserves technical details (commands, configs)\n\n### decision-record.json\nTests capturing architectural or technical decisions with full context.\n\n**Scenario**: Document database migration decision  \n**Key Behaviors**:\n- Extracts decision context, alternatives, and rationale\n- Follows decision record structure (Context, Decision, Alternatives, Consequences)\n- Captures both selected and rejected options with reasoning\n- Places in decision log or ADR database\n- Links to related technical documentation\n\n## Running Evaluations\n\n1. Enable the `knowledge-capture` skill\n2. Submit the query from the evaluation file\n3. Provide conversation context as specified\n4. Verify all expected behaviors are met\n5. Check success criteria for quality\n6. Test with Haiku, Sonnet, and Opus\n\n## Expected Skill Behaviors\n\nKnowledge Capture evaluations should verify:\n\n### Content Extraction\n- Accurately captures key points from conversation context\n- Preserves specific technical details, not generic placeholders\n- Maintains context and nuance from discussion\n\n### Content Type Selection\n- Correctly identifies appropriate content type (how-to, FAQ, decision record, wiki page)\n- Uses matching structure from reference documentation\n- Applies proper Notion markdown formatting\n\n### Notion Integration\n- Searches for appropriate target location (wiki, decision log, etc.)\n- Creates well-structured pages with clear titles\n- Uses proper parent placement\n- Includes discoverable titles and metadata\n\n### Quality Standards\n- Content is actionable and future-reference ready\n- Technical accuracy is preserved\n- Organization aids discoverability\n- Formatting enhances readability\n\n## Creating New Evaluations\n\nWhen adding Knowledge Capture evaluations:\n\n1. **Use realistic conversation content** - Include actual technical details, decisions, or processes\n2. **Test different content types** - How-to guides, FAQs, decision records, meeting notes, learnings\n3. **Vary complexity** - Simple captures vs. complex technical discussions\n4. **Test discovery** - Finding the right wiki section or database\n5. **Include edge cases** - Unclear content types, minimal context, overlapping categories\n\n## Example Success Criteria\n\n**Good** (specific, testable):\n- \"Structures content using How-To format with numbered steps\"\n- \"Preserves exact bash commands from conversation\"\n- \"Creates page with title format 'How to [Action]'\"\n- \"Places in Engineering Wiki  Deployment section\"\n\n**Bad** (vague, untestable):\n- \"Creates good documentation\"\n- \"Uses appropriate structure\"\n- \"Saves to the right place\"\n\n",
        "data/openai/.curated/notion-meeting-intelligence/SKILL.md": "---\nname: notion-meeting-intelligence\ndescription: Prepare meeting materials with Notion context and Codex research; use when gathering context, drafting agendas/pre-reads, and tailoring materials to attendees.\nmetadata:\n  short-description: Prep meetings with Notion context and tailored agendas\n---\n\n# Meeting Intelligence\n\nPrep meetings by pulling Notion context, tailoring agendas/pre-reads, and enriching with Codex research.\n\n## Quick start\n1) Confirm meeting goal, attendees, date/time, and decisions needed.\n2) Gather context: search with `Notion:notion-search`, then fetch with `Notion:notion-fetch` (prior notes, specs, OKRs, decisions).\n3) Pick the right template via `reference/template-selection-guide.md` (status, decision, planning, retro, 1:1, brainstorming).\n4) Draft agenda/pre-read in Notion with `Notion:notion-create-pages`, embedding source links and owner/timeboxes.\n5) Enrich with Codex research (industry insights, benchmarks, risks) and update the page with `Notion:notion-update-page` as plans change.\n\n## Workflow\n### 0) If any MCP call fails because Notion MCP is not connected, pause and set it up:\n1. Add the Notion MCP:\n   - `codex mcp add notion --url https://mcp.notion.com/mcp`\n2. Enable remote MCP client:\n   - Set `[features].rmcp_client = true` in `config.toml` **or** run `codex --enable rmcp_client`\n3. Log in with OAuth:\n   - `codex mcp login notion`\n\nAfter successful login, the user will have to restart codex. You should finish your answer and tell them so when they try again they can continue with Step 1.\n\n### 1) Gather inputs\n- Ask for objective, desired outcomes/decisions, attendees, duration, date/time, and prior materials.\n- Search Notion for relevant docs, past notes, specs, and action items (`Notion:notion-search`), then fetch key pages (`Notion:notion-fetch`).\n- Capture blockers/risks and open questions up front.\n\n### 2) Choose format\n- Status/update  status template.\n- Decision/approval  decision template.\n- Planning (sprint/project)  planning template.\n- Retro/feedback  retrospective template.\n- 1:1  one-on-one template.\n- Ideation  brainstorming template.\n- Use `reference/template-selection-guide.md` to confirm.\n\n### 3) Build the agenda/pre-read\n- Start from the chosen template in `reference/` and adapt sections (context, goals, agenda, owner/time per item, decisions, risks, prep asks).\n- Include links to pulled Notion pages and any required pre-reading.\n- Assign owners for each agenda item; call out timeboxes and expected outputs.\n\n### 4) Enrich with research\n- Add concise Codex research where helpful: market/industry facts, benchmarks, risks, best practices.\n- Keep claims cited with source links; separate fact from opinion.\n\n### 5) Finalize and share\n- Add next steps and owners for follow-ups.\n- If tasks arise, create/link tasks in the relevant Notion database.\n- Update the page via `Notion:notion-update-page` when details change; keep a brief changelog if multiple edits.\n\n## References and examples\n- `reference/`  template picker and meeting templates (e.g., `template-selection-guide.md`, `status-update-template.md`, `decision-meeting-template.md`, `sprint-planning-template.md`, `one-on-one-template.md`, `retrospective-template.md`, `brainstorming-template.md`).\n- `examples/`  end-to-end meeting preps (e.g., `executive-review.md`, `project-decision.md`, `sprint-planning.md`, `customer-meeting.md`).\n",
        "data/openai/.curated/notion-meeting-intelligence/evaluations/README.md": "# Meeting Intelligence Skill Evaluations\n\nEvaluation scenarios for testing the Meeting Intelligence skill across different Codex models.\n\n## Purpose\n\nThese evaluations ensure the Meeting Intelligence skill:\n- Gathers context from Notion workspace\n- Enriches with Codex research appropriately\n- Creates both internal pre-reads and external agendas\n- Distinguishes between Notion facts and Codex insights\n- Works consistently across Haiku, Sonnet, and Opus\n\n## Evaluation Files\n\n### decision-meeting-prep.json\nTests preparation for a decision-making meeting.\n\n**Scenario**: Prep for database migration decision meeting  \n**Key Behaviors**:\n- Searches Notion for migration context (specs, discussions, options)\n- Fetches 2-3 relevant pages\n- Enriches with Codex research (decision frameworks, migration best practices)\n- Creates comprehensive internal pre-read with recommendation\n- Creates clean, professional external agenda\n- Clearly distinguishes Notion facts from Codex insights\n- Cross-links both documents\n\n### status-meeting-prep.json\nTests preparation for a status update or review meeting.\n\n**Scenario**: Prep for project status review  \n**Key Behaviors**:\n- Gathers project metrics and progress from Notion\n- Fetches relevant pages (roadmap, tasks, milestones)\n- Adds Codex context (industry benchmarks, best practices)\n- Creates internal pre-read with honest assessment\n- Creates external agenda with structured flow\n- Includes source citations using mention-page tags\n- Time-boxes agenda items\n\n## Running Evaluations\n\n1. Enable the `meeting-intelligence` skill\n2. Submit the query from the evaluation file\n3. Verify the skill searches Notion first (not Codex research)\n4. Check that TWO documents are created (internal + external)\n5. Verify Codex enrichment adds value without replacing Notion content\n6. Test with Haiku, Sonnet, and Opus\n\n## Expected Skill Behaviors\n\nMeeting Intelligence evaluations should verify:\n\n### Notion Context Gathering\n- Searches workspace for relevant context first\n- Fetches specific pages (not generic)\n- Extracts key information from Notion content\n- Cites sources using mention-page tags\n\n### Codex Research Integration\n- Adds industry context, frameworks, or best practices\n- Enrichment is relevant and valuable (not filler)\n- Clearly distinguishes Notion facts from Codex insights\n- Research complements (doesn't replace) Notion content\n\n### Two-Document Creation\n- **Internal Pre-Read**: Comprehensive, includes strategy, recommendations, detailed pros/cons\n- **External Agenda**: Professional, focused on meeting flow, no internal strategy\n- Both documents are clearly labeled\n- Documents are cross-linked\n\n### Document Quality\n- Pre-read follows structure: Overview  Background  Current Status  Context & Insights  Discussion Points\n- Agenda follows structure: Details  Objective  Agenda Items (with times)  Decisions  Actions  Resources\n- Titles include date or meeting context\n- Content is actionable and meeting-ready\n\n## Creating New Evaluations\n\nWhen adding Meeting Intelligence evaluations:\n\n1. **Test different meeting types** - Decision, status, brainstorm, 1:1, sprint planning, retrospective\n2. **Vary complexity** - Simple updates vs. complex strategic decisions\n3. **Test with/without Notion content** - Rich workspace vs. minimal existing pages\n4. **Verify enrichment value** - Is Codex research genuinely helpful?\n5. **Check internal/external distinction** - Is sensitive info kept in pre-read only?\n\n## Example Success Criteria\n\n**Good** (specific, testable):\n- \"Creates TWO documents (internal pre-read + external agenda)\"\n- \"Internal pre-read marked 'INTERNAL ONLY' or 'For team only'\"\n- \"Cites at least 2-3 Notion pages using mention-page tags\"\n- \"Agenda includes time allocations for each section\"\n- \"Codex enrichment includes decision frameworks or best practices\"\n\n**Bad** (vague, untestable):\n- \"Creates meeting materials\"\n- \"Gathers context effectively\"\n- \"Prepares well\"\n",
        "data/openai/.curated/notion-research-documentation/SKILL.md": "---\nname: notion-research-documentation\ndescription: Research across Notion and synthesize into structured documentation; use when gathering info from multiple Notion sources to produce briefs, comparisons, or reports with citations.\nmetadata:\n  short-description: Research Notion content and produce briefs/reports\n---\n\n# Research & Documentation\n\nPull relevant Notion pages, synthesize findings, and publish clear briefs or reports (with citations and links to sources).\n\n## Quick start\n1) Find sources with `Notion:notion-search` using targeted queries; confirm scope with the user.\n2) Fetch pages via `Notion:notion-fetch`; note key sections and capture citations (`reference/citations.md`).\n3) Choose output format (brief, summary, comparison, comprehensive report) using `reference/format-selection-guide.md`.\n4) Draft in Notion with `Notion:notion-create-pages` using the matching template (quick, summary, comparison, comprehensive).\n5) Link sources and add a references/citations section; update as new info arrives with `Notion:notion-update-page`.\n\n## Workflow\n### 0) If any MCP call fails because Notion MCP is not connected, pause and set it up:\n1. Add the Notion MCP:\n   - `codex mcp add notion --url https://mcp.notion.com/mcp`\n2. Enable remote MCP client:\n   - Set `[features].rmcp_client = true` in `config.toml` **or** run `codex --enable rmcp_client`\n3. Log in with OAuth:\n   - `codex mcp login notion`\n\nAfter successful login, the user will have to restart codex. You should finish your answer and tell them so when they try again they can continue with Step 1.\n\n### 1) Gather sources\n- Search first (`Notion:notion-search`); refine queries, and ask the user to confirm if multiple results appear.\n- Fetch relevant pages (`Notion:notion-fetch`), skim for facts, metrics, claims, constraints, and dates.\n- Track each source URL/ID for later citation; prefer direct quotes for critical facts.\n\n### 2) Select the format\n- Quick readout  quick brief.\n- Single-topic dive  research summary.\n- Option tradeoffs  comparison.\n- Deep dive / exec-ready  comprehensive report.\n- See `reference/format-selection-guide.md` for when to pick each.\n\n### 3) Synthesize\n- Outline before writing; group findings by themes/questions.\n- Note evidence with source IDs; flag gaps or contradictions.\n- Keep user goal in view (decision, summary, plan, recommendation).\n\n### 4) Create the doc\n- Pick the matching template in `reference/` (brief, summary, comparison, comprehensive) and adapt it.\n- Create the page with `Notion:notion-create-pages`; include title, summary, key findings, supporting evidence, and recommendations/next steps when relevant.\n- Add citations inline and a references section; link back to source pages.\n\n### 5) Finalize & handoff\n- Add highlights, risks, and open questions.\n- If the user needs follow-ups, create tasks or a checklist in the page; link any task database entries if applicable.\n- Share a short changelog or status using `Notion:notion-update-page` when updating.\n\n## References and examples\n- `reference/`  search tactics, format selection, templates, and citation rules (e.g., `advanced-search.md`, `format-selection-guide.md`, `research-summary-template.md`, `comparison-template.md`, `citations.md`).\n- `examples/`  end-to-end walkthroughs (e.g., `competitor-analysis.md`, `technical-investigation.md`, `market-research.md`, `trip-planning.md`).\n",
        "data/openai/.curated/notion-research-documentation/evaluations/README.md": "# Research & Documentation Skill Evaluations\n\nEvaluation scenarios for testing the Research & Documentation skill across different Codex models.\n\n## Purpose\n\nThese evaluations ensure the Research & Documentation skill:\n- Searches across Notion workspace effectively\n- Synthesizes information from multiple sources\n- Selects appropriate research report format\n- Creates comprehensive documentation with proper citations\n- Works consistently across Haiku, Sonnet, and Opus\n\n## Evaluation Files\n\n### basic-research.json\nTests basic research workflow with synthesis across multiple Notion pages.\n\n**Scenario**: Research Q4 product roadmap and create summary  \n**Key Behaviors**:\n- Searches Notion for roadmap-related pages\n- Fetches multiple relevant pages (roadmap, product docs, meeting notes)\n- Synthesizes information from different sources\n- Selects appropriate format (Research Summary)\n- Includes citations linking back to source pages\n- Creates structured document with clear sections\n\n### research-to-database.json\nTests creating research documentation in a Notion database with properties.\n\n**Scenario**: Research competitor landscape and save to Research database  \n**Key Behaviors**:\n- Searches for existing competitive intelligence in Notion\n- Identifies Research database as target\n- Fetches database schema to understand properties\n- Creates page with correct property values (Research Type, Status, Date, etc.)\n- Structures content with comparison format\n- Includes source citations for both Notion pages and external research\n\n## Running Evaluations\n\n1. Enable the `research-documentation` skill\n2. Submit the query from the evaluation file\n3. Verify the skill searches Notion workspace comprehensively\n4. Check that multiple source pages are fetched and synthesized\n5. Verify appropriate format is selected (Research Summary, Comprehensive Report, Quick Brief, Comparison)\n6. Confirm citations link back to sources\n7. Test with Haiku, Sonnet, and Opus\n\n## Expected Skill Behaviors\n\nResearch & Documentation evaluations should verify:\n\n### Notion Search & Synthesis\n- Searches workspace with relevant queries\n- Fetches multiple source pages (3-5+)\n- Synthesizes information across sources\n- Identifies patterns and insights\n- Handles conflicting information appropriately\n\n### Format Selection\n- Chooses correct format based on scope and depth:\n  - **Research Summary**: Quick overview with key findings\n  - **Comprehensive Report**: Deep analysis with multiple sections\n  - **Quick Brief**: Fast facts and takeaways\n  - **Comparison**: Side-by-side analysis\n- Applies format structure consistently\n- Uses appropriate sections and headings\n\n### Citation & Attribution\n- Includes citations for all Notion sources\n- Uses mention-page tags: `<mention-page url=\"...\">`\n- Attributes findings to specific sources\n- Distinguishes between Notion content and Codex research\n- Links related documents\n\n### Document Quality\n- Title clearly indicates research topic and date\n- Executive summary or key findings upfront\n- Organized with clear hierarchy\n- Actionable insights and recommendations\n- Appropriate depth for the query\n\n## Creating New Evaluations\n\nWhen adding Research & Documentation evaluations:\n\n1. **Test different research types** - Product research, competitive analysis, technical investigation, market research\n2. **Vary source count** - Synthesis of 2-3 pages vs. 10+ pages\n3. **Test format selection** - Does it choose the right format for the scope?\n4. **Include database targets** - Not just standalone pages\n5. **Test citation accuracy** - Are all sources properly attributed?\n6. **Cross-workspace search** - Testing search across teamspaces if applicable\n\n## Example Success Criteria\n\n**Good** (specific, testable):\n- \"Searches Notion for 'roadmap' and 'Q4' and 'product'\"\n- \"Fetches at least 3 different source pages\"\n- \"Includes citation for each key finding using mention-page tags\"\n- \"Creates page with title format 'Research: [Topic] - [Date]'\"\n- \"Uses Research Summary format with sections: Executive Summary  Key Findings  Details  Recommendations  Sources\"\n\n**Bad** (vague, untestable):\n- \"Searches Notion effectively\"\n- \"Creates comprehensive research\"\n- \"Uses sources appropriately\"\n- \"Good documentation\"\n\n",
        "data/openai/.curated/notion-spec-to-implementation/SKILL.md": "---\nname: notion-spec-to-implementation\ndescription: Turn Notion specs into implementation plans, tasks, and progress tracking; use when implementing PRDs/feature specs and creating Notion plans + tasks from them.\nmetadata:\n  short-description: Turn Notion specs into implementation plans, tasks, and progress tracking\n---\n\n# Spec to Implementation\n\nConvert a Notion spec into linked implementation plans, tasks, and ongoing status updates.\n\n## Quick start\n1) Locate the spec with `Notion:notion-search`, then fetch it with `Notion:notion-fetch`.\n2) Parse requirements and ambiguities using `reference/spec-parsing.md`.\n3) Create a plan page with `Notion:notion-create-pages` (pick a template: quick vs. full).\n4) Find the task database, confirm schema, then create tasks with `Notion:notion-create-pages`.\n5) Link spec  plan  tasks; keep status current with `Notion:notion-update-page`.\n\n## Workflow\n\n### 0) If any MCP call fails because Notion MCP is not connected, pause and set it up:\n1. Add the Notion MCP:\n   - `codex mcp add notion --url https://mcp.notion.com/mcp`\n2. Enable remote MCP client:\n   - Set `[features].rmcp_client = true` in `config.toml` **or** run `codex --enable rmcp_client`\n3. Log in with OAuth:\n   - `codex mcp login notion`\n\nAfter successful login, the user will have to restart codex. You should finish your answer and tell them so when they try again they can continue with Step 1.\n\n### 1) Locate and read the spec\n- Search first (`Notion:notion-search`); if multiple hits, ask the user which to use.\n- Fetch the page (`Notion:notion-fetch`) and scan for requirements, acceptance criteria, constraints, and priorities. See `reference/spec-parsing.md` for extraction patterns.\n- Capture gaps/assumptions in a clarifications block before proceeding.\n\n### 2) Choose plan depth\n- Simple change  use `reference/quick-implementation-plan.md`.\n- Multi-phase feature/migration  use `reference/standard-implementation-plan.md`.\n- Create the plan via `Notion:notion-create-pages`, include: overview, linked spec, requirements summary, phases, dependencies/risks, and success criteria. Link back to the spec.\n\n### 3) Create tasks\n- Find the task database (`Notion:notion-search`  `Notion:notion-fetch` to confirm the data source and required properties). Patterns in `reference/task-creation.md`.\n- Size tasks to 12 days. Use `reference/task-creation-template.md` for content (context, objective, acceptance criteria, dependencies, resources).\n- Set properties: title/action verb, status, priority, relations to spec + plan, due date/story points/assignee if provided.\n- Create pages with `Notion:notion-create-pages` using the databases `data_source_id`.\n\n### 4) Link artifacts\n- Plan links to spec; tasks link to both plan and spec.\n- Optionally update the spec with a short Implementation section pointing to the plan and tasks using `Notion:notion-update-page`.\n\n### 5) Track progress\n- Use the cadence in `reference/progress-tracking.md`.\n- Post updates with `reference/progress-update-template.md`; close phases with `reference/milestone-summary-template.md`.\n- Keep checklists and status fields in plan/tasks in sync; note blockers and decisions.\n\n## References and examples\n- `reference/`  parsing patterns, plan/task templates, progress cadence (e.g., `spec-parsing.md`, `standard-implementation-plan.md`, `task-creation.md`, `progress-tracking.md`).\n- `examples/`  end-to-end walkthroughs (e.g., `ui-component.md`, `api-feature.md`, `database-migration.md`).\n",
        "data/openai/.curated/notion-spec-to-implementation/evaluations/README.md": "# Spec to Implementation Skill Evaluations\n\nEvaluation scenarios for testing the Spec to Implementation skill across different Codex models.\n\n## Purpose\n\nThese evaluations ensure the Spec to Implementation skill:\n- Finds and parses specification pages accurately\n- Breaks down specs into actionable implementation plans\n- Creates tasks that Codex can implement with clear acceptance criteria\n- Tracks progress and updates implementation status\n- Works consistently across Haiku, Sonnet, and Opus\n\n## Evaluation Files\n\n### basic-spec-implementation.json\nTests basic workflow of turning a spec into an implementation plan.\n\n**Scenario**: Implement user authentication feature from spec  \n**Key Behaviors**:\n- Searches for and finds the authentication spec page\n- Fetches spec and extracts requirements\n- Parses requirements into phases (setup, core features, polish)\n- Creates implementation plan page linked to original spec\n- Breaks down into clear phases with deliverables\n- Includes timeline and dependencies\n\n### spec-to-tasks.json\nTests creating concrete tasks from a specification in a task database.\n\n**Scenario**: Create tasks from API redesign spec  \n**Key Behaviors**:\n- Finds spec page in Notion\n- Extracts specific requirements and acceptance criteria\n- Searches for or creates task database\n- Fetches task database schema\n- Creates multiple tasks with proper properties (Status, Priority, Sprint, etc.)\n- Each task has clear title, description, and acceptance criteria\n- Tasks have dependencies where appropriate\n- Links all tasks back to original spec\n\n## Running Evaluations\n\n1. Enable the `spec-to-implementation` skill\n2. Submit the query from the evaluation file\n3. Verify the skill finds the spec page via search\n4. Check that requirements are accurately parsed\n5. Confirm implementation plan is created with phases\n6. Verify tasks have clear, implementable acceptance criteria\n7. Check that tasks link back to spec\n8. Test with Haiku, Sonnet, and Opus\n\n## Expected Skill Behaviors\n\nSpec to Implementation evaluations should verify:\n\n### Spec Discovery & Parsing\n- Searches Notion for specification pages\n- Fetches complete spec content\n- Extracts all requirements accurately\n- Identifies technical dependencies\n- Understands acceptance criteria\n- Notes any ambiguities or missing details\n\n### Implementation Planning\n- Creates implementation plan page\n- Breaks work into logical phases:\n  - Phase 1: Foundation/Setup\n  - Phase 2: Core Implementation\n  - Phase 3: Testing & Polish\n- Includes timeline estimates\n- Identifies dependencies between phases\n- Links back to original spec\n\n### Task Creation\n- Finds or identifies task database\n- Fetches database schema for property names\n- Creates tasks with correct properties\n- Each task has:\n  - Clear, specific title\n  - Context and description\n  - Acceptance criteria (checklist format)\n  - Appropriate priority and status\n  - Link to spec page\n- Tasks are right-sized (not too big, not too small)\n- Dependencies between tasks are noted\n\n### Progress Tracking\n- Implementation plan includes progress markers\n- Tasks can be updated as work progresses\n- Status updates link to completed work\n- Blockers or changes are noted\n\n## Creating New Evaluations\n\nWhen adding Spec to Implementation evaluations:\n\n1. **Test different spec types** - Features, migrations, refactors, API changes, UI components\n2. **Vary complexity** - Simple 1-phase vs. complex multi-phase implementations\n3. **Test task granularity** - Does it create appropriately-sized tasks?\n4. **Include edge cases** - Vague specs, conflicting requirements, missing details\n5. **Test database integration** - Creating tasks in existing task databases with various schemas\n6. **Progress tracking** - Updating implementation plans as tasks complete\n\n## Example Success Criteria\n\n**Good** (specific, testable):\n- \"Searches Notion for spec page using feature name\"\n- \"Creates implementation plan with 3 phases: Setup  Core  Polish\"\n- \"Creates 5-8 tasks in task database with properties: Task (title), Status, Priority, Sprint\"\n- \"Each task has acceptance criteria in checklist format (- [ ] ...)\"\n- \"Tasks link back to spec using mention-page tag\"\n- \"Task titles are specific and actionable (e.g., 'Create login API endpoint' not 'Authentication')\"\n\n**Bad** (vague, untestable):\n- \"Creates good implementation plan\"\n- \"Tasks are well-structured\"\n- \"Breaks down spec appropriately\"\n- \"Links to spec\"\n\n",
        "data/openai/.experimental/create-plan/SKILL.md": "---\nname: create-plan\ndescription: Create a concise plan. Use when a user explicitly asks for a plan related to a coding task.\nmetadata:\n  short-description: Create a plan\n---\n\n# Create Plan\n\n## Goal\n\nTurn a user prompt into a **single, actionable plan** delivered in the final assistant message.\n\n## Minimal workflow\n\nThroughout the entire workflow, operate in read-only mode. Do not write or update files.\n\n1. **Scan context quickly**\n   - Read `README.md` and any obvious docs (`docs/`, `CONTRIBUTING.md`, `ARCHITECTURE.md`).\n   - Skim relevant files (the ones most likely touched).\n   - Identify constraints (language, frameworks, CI/test commands, deployment shape).\n\n2. **Ask follow-ups only if blocking**\n   - Ask **at most 12 questions**.\n   - Only ask if you cannot responsibly plan without the answer; prefer multiple-choice.\n   - If unsure but not blocked, make a reasonable assumption and proceed.\n\n3. **Create a plan using the template below**\n   - Start with **1 short paragraph** describing the intent and approach.\n   - Clearly call out what is **in scope** and what is **not in scope** in short.\n   - Then provide a **small checklist** of action items (default 610 items).\n      - Each checklist item should be a concrete action and, when helpful, mention files/commands.\n      - **Make items atomic and ordered**: discovery  changes  tests  rollout.\n      - **Verb-first**: Add, Refactor, Verify, Ship.\n   - Include at least one item for **tests/validation** and one for **edge cases/risk** when applicable.\n   - If there are unknowns, include a tiny **Open questions** section (max 3).\n\n4. **Do not preface the plan with meta explanations; output only the plan as per template**\n\n## Plan template (follow exactly)\n\n```markdown\n# Plan\n\n<13 sentences: what were doing, why, and the high-level approach.>\n\n## Scope\n- In:\n- Out:\n\n## Action items\n[ ] <Step 1>\n[ ] <Step 2>\n[ ] <Step 3>\n[ ] <Step 4>\n[ ] <Step 5>\n[ ] <Step 6>\n\n## Open questions\n- <Question 1>\n- <Question 2>\n- <Question 3>\n```\n\n## Checklist item guidance\nGood checklist items:\n- Point to likely files/modules: src/..., app/..., services/...\n- Name concrete validation: Run npm test, Add unit tests for X\n- Include safe rollout when relevant: feature flag, migration plan, rollback note\n\nAvoid:\n- Vague steps (handle backend, do auth)\n- Too many micro-steps\n- Writing code snippets (keep the plan implementation-agnostic)\n",
        "data/openai/.experimental/linear/SKILL.md": "---\nname: linear\ndescription: Manage issues, projects & team workflows in Linear. Use when the user wants to read, create or updates tickets in Linear.\nmetadata:\n  short-description: Manage Linear issues in Codex\n---\n\n# Linear\n\n## Overview\n\nThis skill provides a structured workflow for managing issues, projects & team workflows in Linear. It ensures consistent integration with the Linear MCP server, which offers natural-language project management for issues, projects, documentation, and team collaboration.\n\n## Prerequisites\n- Linear MCP server must be connected and accessible via OAuth\n- Confirm access to the relevant Linear workspace, teams, and projects\n\n## Required Workflow\n\n**Follow these steps in order. Do not skip steps.**\n\n### Step 0: Set up Linear MCP (if not already configured)\n\nIf any MCP call fails because Linear MCP is not connected, pause and set it up:\n\n1. Add the Linear MCP:\n   - `codex mcp add linear --url https://mcp.linear.app/mcp`\n2. Enable remote MCP client:\n   - Set `[features] rmcp_client = true` in `config.toml` **or** run `codex --enable rmcp_client`\n3. Log in with OAuth:\n   - `codex mcp login linear`\n\nAfter successful login, the user will have to restart codex. You should finish your answer and tell them so when they try again they can continue with Step 1.\n\n**Windows/WSL note:** If you see connection errors on Windows, try configuring the Linear MCP to run via WSL:\n```json\n{\"mcpServers\": {\"linear\": {\"command\": \"wsl\", \"args\": [\"npx\", \"-y\", \"mcp-remote\", \"https://mcp.linear.app/sse\", \"--transport\", \"sse-only\"]}}}\n```\n\n### Step 1\nClarify the user's goal and scope (e.g., issue triage, sprint planning, documentation audit, workload balance). Confirm team/project, priority, labels, cycle, and due dates as needed.\n\n### Step 2\nSelect the appropriate workflow (see Practical Workflows below) and identify the Linear MCP tools you will need. Confirm required identifiers (issue ID, project ID, team key) before calling tools.\n\n### Step 3\nExecute Linear MCP tool calls in logical batches:\n- Read first (list/get/search) to build context.\n- Create or update next (issues, projects, labels, comments) with all required fields.\n- For bulk operations, explain the grouping logic before applying changes.\n\n### Step 4\nSummarize results, call out remaining gaps or blockers, and propose next actions (additional issues, label changes, assignments, or follow-up comments).\n\n## Available Tools\n\nIssue Management: `list_issues`, `get_issue`, `create_issue`, `update_issue`, `list_my_issues`, `list_issue_statuses`, `list_issue_labels`, `create_issue_label`\n\nProject & Team: `list_projects`, `get_project`, `create_project`, `update_project`, `list_teams`, `get_team`, `list_users`\n\nDocumentation & Collaboration: `list_documents`, `get_document`, `search_documentation`, `list_comments`, `create_comment`, `list_cycles`\n\n## Practical Workflows\n\n- Sprint Planning: Review open issues for a target team, pick top items by priority, and create a new cycle (e.g., \"Q1 Performance Sprint\") with assignments.\n- Bug Triage: List critical/high-priority bugs, rank by user impact, and move the top items to \"In Progress.\"\n- Documentation Audit: Search documentation (e.g., API auth), then open labeled \"documentation\" issues for gaps or outdated sections with detailed fixes.\n- Team Workload Balance: Group active issues by assignee, flag anyone with high load, and suggest or apply redistributions.\n- Release Planning: Create a project (e.g., \"v2.0 Release\") with milestones (feature freeze, beta, docs, launch) and generate issues with estimates.\n- Cross-Project Dependencies: Find all \"blocked\" issues, identify blockers, and create linked issues if missing.\n- Automated Status Updates: Find your issues with stale updates and add status comments based on current state/blockers.\n- Smart Labeling: Analyze unlabeled issues, suggest/apply labels, and create missing label categories.\n- Sprint Retrospectives: Generate a report for the last completed cycle, note completed vs. pushed work, and open discussion issues for patterns.\n\n## Tips for Maximum Productivity\n\n- Batch operations for related changes; consider smart templates for recurring issue structures.\n- Use natural queries when possible (\"Show me what John is working on this week\").\n- Leverage context: reference prior issues in new requests.\n- Break large updates into smaller batches to avoid rate limits; cache or reuse filters when listing frequently.\n\n## Troubleshooting\n\n- Authentication: Clear browser cookies, re-run OAuth, verify workspace permissions, ensure API access is enabled.\n- Tool Calling Errors: Confirm the model supports multiple tool calls, provide all required fields, and split complex requests.\n- Missing Data: Refresh token, verify workspace access, check for archived projects, and confirm correct team selection.\n- Performance: Remember Linear API rate limits; batch bulk operations, use specific filters, or cache frequent queries.\n",
        "data/openai/.system/skill-installer/SKILL.md": "---\nname: skill-installer\ndescription: Install Codex skills into $CODEX_HOME/skills from a curated list or a GitHub repo path. Use when a user asks to list installable skills, install a curated skill, or install a skill from another repo (including private repos).\nmetadata:\n  short-description: Install curated skills from openai/skills or other repos\n---\n\n# Skill Installer\n\nHelps install skills. By default these are from https://github.com/openai/skills/tree/main/skills/.curated, but users can also provide other locations.\n\nUse the helper scripts based on the task:\n- List curated skills when the user asks what is available, or if the user uses this skill without specifying what to do.\n- Install from the curated list when the user provides a skill name.\n- Install from another repo when the user provides a GitHub repo/path (including private repos).\n\nInstall skills with the helper scripts.\n\n## Communication\n\nWhen listing curated skills, output approximately as follows, depending on the context of the user's request:\n\"\"\"\nSkills from {repo}:\n1. skill-1\n2. skill-2 (already installed)\n3. ...\nWhich ones would you like installed?\n\"\"\"\n\nAfter installing a skill, tell the user: \"Restart Codex to pick up new skills.\"\n\n## Scripts\n\nAll of these scripts use network, so when running in the sandbox, request escalation when running them.\n\n- `scripts/list-curated-skills.py` (prints curated list with installed annotations)\n- `scripts/list-curated-skills.py --format json`\n- `scripts/install-skill-from-github.py --repo <owner>/<repo> --path <path/to/skill> [<path/to/skill> ...]`\n- `scripts/install-skill-from-github.py --url https://github.com/<owner>/<repo>/tree/<ref>/<path>`\n\n## Behavior and Options\n\n- Defaults to direct download for public GitHub repos.\n- If download fails with auth/permission errors, falls back to git sparse checkout.\n- Aborts if the destination skill directory already exists.\n- Installs into `$CODEX_HOME/skills/<skill-name>` (defaults to `~/.codex/skills`).\n- Multiple `--path` values install multiple skills in one run, each named from the path basename unless `--name` is supplied.\n- Options: `--ref <ref>` (default `main`), `--dest <path>`, `--method auto|download|git`.\n\n## Notes\n\n- Curated listing is fetched from `https://github.com/openai/skills/tree/main/skills/.curated` via the GitHub API. If it is unavailable, explain the error and exit.\n- Private GitHub repos can be accessed via existing git credentials or optional `GITHUB_TOKEN`/`GH_TOKEN` for download.\n- Git fallback tries HTTPS first, then SSH.\n- The skills at https://github.com/openai/skills/tree/main/skills/.system are preinstalled, so no need to help users install those. If they ask, just explain this. If they insist, you can download and overwrite.\n- Installed annotations come from `$CODEX_HOME/skills`.\n",
        "data/x-cmd/x-cmd-git/SKILL.md": "---\nname: x-cmd-git\ntag: git\ndescription: This skill provides comprehensive Git and code hosting platform management tools through x-cmd CLI, including GitHub, GitLab, Codeberg, Forgejo integration, and Git hooks management. This skill should be used when users need to manage Git repositories, work with code hosting platforms, automate Git workflows, or configure Git hooks from command line interfaces.\n---\n\n# x-cmd Git and Code Hosting Tools\n\n## Overview\n\nThis skill provides professional Git and code hosting platform management capabilities through the x-cmd ecosystem. The tools enable developers, DevOps engineers, and open source contributors to manage repositories, collaborate on code, automate workflows, and integrate with multiple Git hosting services directly from the terminal.\n\n## Available Tools\n\n### GitHub Management (gh)\nComprehensive GitHub platform integration and management.\n\n- **Usage**: `x gh [subcommand]`\n- **Key subcommands**:\n  - `repo` - Repository management\n  - `issue` - Issue tracking and management\n  - `pr` - Pull request management\n  - `action` - GitHub Actions workflow management\n  - `user` - User profile and account management\n  - `search` - Repository and topic search\n  - `browse` - Open GitHub resources in browser\n- **Examples**:\n  - `x gh user info` - Get current user information\n  - `x gh repo app` - Interactive repository viewer\n  - `x gh repo clone owner/repo` - Clone repository\n  - `x gh pr create` - Create pull request\n  - `x gh action workflow` - Manage workflows\n\n### GitLab Management (gl)\nComplete GitLab platform integration and administration.\n\n- **Usage**: `x gl [subcommand]`\n- **Key subcommands**:\n  - `repo` - Repository management\n  - `issue` - Issue management\n  - `mr` - Merge request management\n  - `user` - User administration\n  - `group` - Group and team management\n  - `deploy` - Deployment management\n  - `snippet` - Code snippet management\n- **Examples**:\n  - `x gl repo ls` - List repositories\n  - `x gl mr create` - Create merge request\n  - `x gl user info` - Get user information\n  - `x gl group ls` - List groups\n  - `x gl repo clone project` - Clone repository\n\n### Codeberg Management (cb)\nLightweight CLI for Codeberg open source hosting.\n\n- **Usage**: `x cb [subcommand]`\n- **Key subcommands**:\n  - `repo` - Repository management\n  - `user` - User profile management\n  - `org` - Organization administration\n  - `issue` - Issue tracking\n  - `pr` - Pull request management\n  - `notification` - Notification handling\n- **Examples**:\n  - `x cb repo ls` - List repositories\n  - `x cb user info` - Get user information\n  - `x cb issue create` - Create issue\n  - `x cb pr list` - List pull requests\n  - `x cb repo clone owner/repo` - Clone repository\n\n### Forgejo Management (fjo)\nSelf-hosted Git platform management for Forgejo instances.\n\n- **Usage**: `x fjo [subcommand]`\n- **Key subcommands**:\n  - `repo` - Repository management\n  - `user` - User administration\n  - `org` - Organization management\n  - `issue` - Issue tracking\n  - `notification` - Notification handling\n- **Examples**:\n  - `x fjo repo ls` - List repositories\n  - `x fjo user info` - Get user information\n  - `x fjo issue create` - Create issue\n  - `x fjo pr create` - Create pull request\n  - `x fjo repo clone project` - Clone repository\n\n### Git Hooks Management (githook)\nGit hooks configuration and automation.\n\n- **Usage**: `x githook [subcommand]`\n- **Key subcommands**:\n  - `apply` - Apply Git hooks configuration\n  - `clear` - Clear hooks and remove configuration\n- **Examples**:\n  - `x githook apply` - Apply hooks from configuration\n  - `x githook clear` - Remove all hooks configuration\n\n## Git and Code Hosting Use Cases\n\n### Repository Management\n- Use `x gh repo` for GitHub repository operations\n- Use `x gl repo` for GitLab repository management\n- Use `x cb repo` for Codeberg repository handling\n- Use `x fjo repo` for Forgejo repository administration\n\n### Collaboration and Code Review\n- Use `x gh pr` for GitHub pull request workflows\n- Use `x gl mr` for GitLab merge request processes\n- Use `x cb pr` for Codeberg pull request management\n- Use `x fjo pr` for Forgejo pull request handling\n\n### Issue Tracking and Project Management\n- Use `x gh issue` for GitHub issue management\n- Use `x gl issue` for GitLab issue tracking\n- Use `x cb issue` for Codeberg issue handling\n- Use `x fjo issue` for Forgejo issue management\n\n### CI/CD and Automation\n- Use `x gh action` for GitHub Actions workflows\n- Use `x gl` deployment features for GitLab CI/CD\n- Use `x githook` for local Git automation\n- Use platform-specific automation features\n\n### User and Team Administration\n- Use `x gh user` for GitHub user management\n- Use `x gl user` and `x gl group` for GitLab administration\n- Use `x cb user` and `x cb org` for Codeberg organization\n- Use `x fjo user` and `x fjo org` for Forgejo administration\n\n## Installation and Setup\n\n### Prerequisites\n- x-cmd CLI installed\n- Git installed and configured\n- Internet connectivity for platform operations\n\n### Platform Authentication\n\n#### GitHub Setup\n```bash\nx gh init  # Interactive configuration\nx gh --cfg token=<github-token>\n```\nGet GitHub token from: https://github.com/settings/tokens\n\n#### GitLab Setup\n```bash\nx gl init  # Interactive configuration\nx gl --cfg token=<gitlab-token>\n```\nGet GitLab token from: https://gitlab.com/-/profile/personal_access_tokens\n\n#### Codeberg Setup\n```bash\nx cb init  # Interactive configuration\nx cb --cfg token=<codeberg-token>\n```\nGet Codeberg token from: https://codeberg.org/user/settings/applications\n\n#### Forgejo Setup\n```bash\nx fjo init  # Interactive configuration\nx fjo --cfg token=<forgejo-token>\n```\nConfigure Forgejo instance and token\n\n### Git Hooks Configuration\n```bash\n# Apply hooks configuration\nx githook apply\n\n# Clear hooks configuration\nx githook clear\n```\n\n## Integration with Other Tools\n\n### AI and Code Assistance\n- Use `--co` flag for AI code copilot functionality\n- Use `ddgoai` for AI-powered search and summarization\n- Integrate with other AI tools for code generation\n\n### Data Processing and Analysis\n- Pipe output to `x jq` for JSON processing\n- Use with `@zh` for Chinese translation\n- Export to CSV/TSV for reporting and analysis\n\n### Development Workflows\n- Combine with `x curl` for API interactions\n- Use with shell scripts for automation\n- Integrate with CI/CD pipelines\n\n## Troubleshooting\n\n### Common Issues\n- **Authentication errors**: Verify API tokens and permissions\n- **Network connectivity**: Check internet connection for platform operations\n- **Permission issues**: Ensure appropriate repository access rights\n- **Configuration problems**: Verify platform-specific settings\n\n### Performance Optimization\n- Use specific queries rather than broad searches\n- Cache authentication tokens securely\n- Use interactive interfaces for complex operations\n- Limit API calls to respect rate limits\n\n### Security Best Practices\n- **Token security**: Store API tokens securely and rotate regularly\n- **Access control**: Follow principle of least privilege for repository access\n- **Audit logging**: Maintain records of platform operations\n- **Compliance**: Ensure usage complies with platform terms of service\n\n## Support and Resources\n\n- **x-cmd Git Documentation**: https://x-cmd.com/mod/git\n- **GitHub CLI**: https://x-cmd.com/mod/gh\n- **GitLab CLI**: https://x-cmd.com/mod/gl\n- **Codeberg CLI**: https://x-cmd.com/mod/cb\n- **Forgejo CLI**: https://x-cmd.com/mod/fjo\n- **Git Hooks**: https://x-cmd.com/mod/githook\n\nFor additional help:\n- Use `x [tool] --help` for specific tool documentation\n- Visit individual module pages for detailed usage\n- Check platform-specific API documentation\n- Consult Git and DevOps best practices",
        "data/x-cmd/x-cmd-knowledge/SKILL.md": "---\nname: x-cmd-knowledge\ndescription: This skill provides access to various knowledge search tools through x-cmd CLI, including Hacker News, Wikipedia, DuckDuckGo search, RFC documents, Project Gutenberg books, and Stack Exchange. This skill should be used when users need to search for technical information, browse online knowledge bases, or access documentation from command line interfaces.\n---\n\n# x-cmd Knowledge Search Tools\n\n## Overview\n\nThis skill provides comprehensive command-line access to major knowledge sources and search engines through the x-cmd ecosystem. The tools enable efficient information retrieval, technical documentation browsing, and knowledge discovery directly from the terminal.\n\n## Available Tools\n\n### Hacker News (hn)\nBrowse and search Hacker News content with interactive table interface.\n\n- **Usage**: `x hn [subcommand]`\n- **Key subcommands**:\n  - `top` - Display top posts\n  - `new` - Display new posts\n  - `best` - Display best posts\n  - `ask` - Display ask posts\n  - `::` - Search with DuckDuckGo and AI assistance\n- **Examples**:\n  - `x hn` - View top posts\n  - `x hn :: llama3` - Search for llama3 with AI assistance\n  - `x hn top --json 11,20` - Get posts 11-20 in JSON format\n\n### Wikipedia (wkp)\nSearch Wikipedia and extract article summaries.\n\n- **Usage**: `x wkp [subcommand] [query]`\n- **Key subcommands**:\n  - `search` - Search Wikipedia pages\n  - `extract` - Get article summaries\n  - `suggest` - Get search suggestions\n  - `:` - Search with DuckDuckGo\n- **Examples**:\n  - `x wkp search AI` - Search for AI articles\n  - `x wkp extract OpenAI` - Get OpenAI summary\n  - `x wkp suggest pythen` - Get spelling suggestions\n\n### DuckDuckGo Search (ddgo)\nWeb search engine with AI-powered results.\n\n- **Usage**: `x ddgo [query]`\n- **Key subcommands**:\n  - `--ai` - Use AI to select and summarize results\n  - `--top N` - Get top N results\n  - `dump --json` - Output results in JSON format\n  - `init` - Configure proxy settings\n- **Examples**:\n  - `x ddgo bash` - Search for bash information\n  - `x ddgo --ai bash` - AI-assisted bash search\n  - `x ddgo --top 10 bash` - Get top 10 bash results\n\n### RFC Documents (rfc)\nBrowse and search Internet RFC documents.\n\n- **Usage**: `x rfc [subcommand]`\n- **Key subcommands**:\n  - `ls` - List all RFC documents\n  - `txt` - Read RFC document content\n  - `:` - Search RFC content\n  - `::` - Search with AI summary\n- **Examples**:\n  - `x rfc ls` - List all RFCs\n  - `x rfc 1003` - Read RFC 1003\n  - `x rfc : csv` - Search for CSV-related RFCs\n\n### Project Gutenberg Books (gtb)\nSearch and browse free ebooks from Project Gutenberg.\n\n- **Usage**: `x gtb [subcommand]`\n- **Key subcommands**:\n  - `search` - Search books by keyword\n  - `show` - View book details interactively\n  - `txt` - Get book text content\n  - `:` - Search with DuckDuckGo\n- **Examples**:\n  - `x gtb` - List all books\n  - `x gtb search Dumas` - Search for Dumas books\n  - `x gtb show 100` - View book ID 100\n\n### Stack Exchange (se)\nSearch across Stack Exchange sites.\n\n- **Usage**: `x se [subcommand] [query]`\n- **Key subcommands**:\n  - `search` - Search questions\n  - `question` - Get question answers\n  - `:` - Search with DuckDuckGo\n  - `site` - View available sites\n- **Examples**:\n  - `x se search \"how to use jq\"` - Search for jq usage\n  - `x se :au \"how to use jq\"` - Search Ask Ubuntu\n  - `x se question 75261408` - Get question answers\n\n## Installation and Setup\n\n### Prerequisites\n- x-cmd CLI installed\n- Internet connection\n\n### Configuration\nEach tool supports configuration through:\n- `init` - Interactive configuration setup\n- `cfg` - Proxy and API endpoint configuration\n- `cur` - Session default management\n\n### Proxy Setup\nFor tools requiring proxy access:\n```bash\nx ddgo init  # Configure proxy for DuckDuckGo\nx hn init    # Configure proxy for Hacker News\n```\n\n## Usage Patterns\n\n### Quick Information Retrieval\n- Use `x ddgo --ai` for AI-assisted search\n- Use `x wkp extract` for quick summaries\n- Use `x hn top` for latest tech news\n\n### Technical Documentation\n- Use `x rfc` for protocol specifications\n- Use `x se` for programming questions\n- Use `x gtb` for reference books\n\n### Interactive Browsing\n- Most tools support interactive table interfaces\n- Use arrow keys for navigation\n- Press `o` to open links in browser\n- Press `u` to open user profiles\n\n## Troubleshooting\n\n### Common Issues\n- **Network errors**: Check proxy configuration with `init` subcommand\n- **No results**: Verify search query syntax\n- **Permission errors**: Ensure x-cmd has network access\n\n### Performance Tips\n- Use `--json` flag for programmatic output\n- Use `--top N` to limit results\n- Configure data retention settings with `cfg` subcommand\n\n## Integration\n\nThese tools can be combined with other x-cmd modules:\n- Pipe output to `@zh` for Chinese translation\n- Use with `x jq` for JSON processing\n- Combine with `x curl` for advanced HTTP requests\n\n## Support\n\nFor additional help:\n- Visit: https://x-cmd.com\n- Use `x [tool] --help` for specific tool documentation\n- Check individual module pages for detailed usage\n\n",
        "data/x-cmd/x-cmd-network/SKILL.md": "---\nname: x-cmd-network\ndescription: This skill provides comprehensive network administration and diagnostic tools through x-cmd CLI, including network scanning with Nmap, ARP table management, DNS configuration, routing table analysis, and enhanced ping utilities. This skill should be used when users need to perform network diagnostics, troubleshoot connectivity issues, analyze network topology, or monitor network performance from command line interfaces.\n---\n\n# x-cmd Network Administration Tools\n\n## Overview\n\nThis skill provides professional network administration and diagnostic capabilities through the x-cmd ecosystem. The tools enable network administrators, security professionals, and system administrators to perform comprehensive network analysis, troubleshoot connectivity issues, and monitor network infrastructure directly from the terminal.\n\n## Available Tools\n\n### Network Scanning (nmap)\nComprehensive network discovery and security scanning.\n\n- **Usage**: `x nmap [options] [targets]`\n- **Key capabilities**:\n  - Host discovery and port scanning\n  - Service and version detection\n  - OS fingerprinting\n  - Scriptable vulnerability scanning\n  - Network mapping and topology discovery\n- **Examples**:\n  - `x nmap -v -A scanme.nmap.org` - Comprehensive scan with OS detection\n  - `x nmap -v -sn 192.168.0.0/16` - Host discovery only\n  - `x nmap -p 22,80,443 192.168.1.0/24` - Targeted port scanning\n  - `x nmap -O --traceroute target.com` - OS detection with traceroute\n\n### ARP Table Management (arp)\nEnhanced ARP cache analysis with multiple output formats.\n\n- **Usage**: `x arp [flags]`\n- **Key features**:\n  - Interactive TUI application for ARP table viewing\n  - Multiple output formats (CSV, TSV, TUI)\n  - MAC address vendor lookup\n  - Suspicious entry detection\n  - Complete ARP table display\n- **Examples**:\n  - `x arp` - Auto-detect output format (TUI/TSV)\n  - `x arp --all` - Show all ARP entries including incomplete\n  - `x arp --csv` - CSV format output\n  - `x arp --app` - Interactive TUI application\n\n### DNS Configuration (dns)\nDomain Name System management and troubleshooting.\n\n- **Usage**: `x dns [subcommand]`\n- **Key subcommands**:\n  - `current` - View current DNS configuration\n  - `ls` - List available DNS servers\n  - `refresh` - Flush DNS cache\n  - `set` - Configure DNS settings (experimental)\n- **Examples**:\n  - `x dns` - View current DNS configuration\n  - `x dns current` - Detailed DNS configuration\n  - `x dns refresh` - Flush DNS cache\n  - `x dns ls` - List available DNS servers\n\n### Routing Table Analysis (route)\nEnhanced routing table management and analysis.\n\n- **Usage**: `x route [subcommand]`\n- **Key features**:\n  - Route table display and analysis\n  - Multiple output formats\n  - Experimental status with ongoing development\n- **Examples**:\n  - `x route` - Display routing table\n  - `x route --csv` - CSV format output\n  - `x route ls` - List routing information\n\n### Enhanced ICMP Ping (ping)\nAdvanced ping utility with visualization capabilities.\n\n- **Usage**: `x ping [flags] [target]`\n- **Key features**:\n  - Default ping to bing.com for quick testing\n  - Heatmap visualization of ping results\n  - Bar chart display of latency data\n  - Multiple output formats (CSV, TSV, raw)\n  - Visual processing of existing ping data\n- **Examples**:\n  - `x ping` - Default ping to bing.com\n  - `x ping 8.8.8.8` - Ping specific target\n  - `x ping --heatmap 8.8.8.8` - Heatmap visualization\n  - `x ping --bar google.com` - Bar chart display\n  - `ping google.com | x ping vis --heatmap` - Process existing ping data\n\n### TCP Port Ping (tping)\nTCP-based connectivity testing for service availability.\n\n- **Usage**: `x tping [flags] [target:port]`\n- **Key features**:\n  - TCP connectivity testing using curl\n  - Default port 80 testing\n  - Heatmap and bar chart visualizations\n  - Multiple output formats\n  - Integration with cosmo curl for better compatibility\n- **Examples**:\n  - `x tping bing.com` - TCP ping to port 80\n  - `x tping --heatmap bing.com` - Heatmap visualization\n  - `x tping --bar bing.com:80` - Bar chart display\n  - `x tping google.com:443` - Test HTTPS connectivity\n\n## Network Administration Use Cases\n\n### Network Discovery and Mapping\n- Use `x nmap` for comprehensive network scanning\n- Use `x arp` for local network device discovery\n- Use `x route` for routing topology analysis\n- Use `x ping` for host availability testing\n\n### Connectivity Troubleshooting\n- Use `x ping` for basic ICMP connectivity testing\n- Use `x tping` for TCP service availability testing\n- Use `x dns` for DNS configuration verification\n- Use `x dns refresh` for DNS cache troubleshooting\n\n### Network Security Assessment\n- Use `x nmap` for vulnerability scanning\n- Use `x arp` for ARP spoofing detection\n- Use `x tping` for service enumeration\n- Use `x nmap -sS` for stealth port scanning\n\n### Performance Monitoring\n- Use `x ping --heatmap` for latency trend analysis\n- Use `x tping --bar` for TCP connection performance\n- Use `x nmap` for service response time measurement\n- Use `x arp` for network device monitoring\n\n## Installation and Setup\n\n### Prerequisites\n- x-cmd CLI installed\n- Network connectivity for external testing\n- Appropriate permissions for network scanning\n\n### Platform-Specific Requirements\n\n#### Network Scanning (nmap)\n- Nmap installation required for full functionality\n- Administrator/root privileges for certain scan types\n- Network interface access for raw packet operations\n\n#### DNS Management\n- Works across all platforms\n- May require administrative privileges for configuration changes\n- Internet connectivity for external DNS testing\n\n#### Enhanced Ping Utilities\n- Works across all platforms\n- No special privileges required for basic functionality\n- Terminal support for visualizations\n\n### Configuration\n\n#### Nmap Integration\n```bash\n# Verify nmap installation\nx nmap --help\n```\n\n#### DNS Configuration\n```bash\n# Check current DNS settings\nx dns current\n```\n\n#### Network Interface Setup\n```bash\n# View ARP table for network analysis\nx arp --app\n```\n\n## Integration with Other Tools\n\n### Data Processing\n- Pipe output to `x jq` for JSON processing\n- Use with `@zh` for Chinese translation of network data\n- Export to CSV/TSV for spreadsheet analysis\n\n### Security Integration\n- Combine with `x shodan` for external network intelligence\n- Use with `x osv` for vulnerability correlation\n- Integrate with `x kev` for known vulnerability checking\n\n### Monitoring and Automation\n- Combine with shell scripts for automated network monitoring\n- Schedule regular network health checks\n- Create custom network diagnostic workflows\n\n## Troubleshooting\n\n### Common Issues\n- **Permission errors**: Ensure appropriate privileges for network operations\n- **Network connectivity**: Verify internet connection for external testing\n- **Scan limitations**: Some nmap features require elevated privileges\n- **DNS resolution**: Check network configuration for DNS issues\n\n### Performance Optimization\n- Use specific scan targets rather than broad ranges\n- Limit scan intensity for production networks\n- Cache results when appropriate for monitoring\n- Use visual interfaces for complex analysis\n\n### Security Considerations\n- **Responsible scanning**: Only scan networks you own or have permission to test\n- **Legal compliance**: Ensure network scanning complies with local laws\n- **Rate limiting**: Respect network resources and avoid aggressive scanning\n- **Documentation**: Maintain records of authorized network testing\n\n## Support and Resources\n\n- **x-cmd Network Documentation**: https://x-cmd.com/mod/network\n- **Nmap Official Documentation**: https://nmap.org/docs.html\n- **ARP Management**: https://x-cmd.com/mod/arp\n- **DNS Configuration**: https://x-cmd.com/mod/dns\n- **Enhanced Ping**: https://x-cmd.com/mod/ping\n- **TCP Ping**: https://x-cmd.com/mod/tping\n\nFor additional help:\n- Use `x [tool] --help` for specific tool documentation\n- Visit individual module pages for detailed usage\n- Check platform-specific requirements for network operations\n- Consult network administration best practices for responsible usage",
        "data/x-cmd/x-cmd-security/SKILL.md": "---\nname: x-cmd-security\ndescription: This skill provides comprehensive security assessment and vulnerability management tools through x-cmd CLI, including network reconnaissance with Shodan, vulnerability scanning with OSV, and known exploited vulnerability tracking with KEV. This skill should be used when users need to perform security assessments, vulnerability research, network reconnaissance, or security monitoring from command line interfaces.\n---\n\n# x-cmd Security Assessment Tools\n\n## Overview\n\nThis skill provides professional security assessment and vulnerability management capabilities through the x-cmd ecosystem. The tools enable security professionals, developers, and system administrators to perform network reconnaissance, vulnerability scanning, and security monitoring directly from the terminal.\n\n## Available Tools\n\n### Shodan Network Intelligence (shodan)\nComprehensive network reconnaissance and internet intelligence gathering.\n\n- **Usage**: `x shodan [subcommand]`\n- **Key subcommands**:\n  - `host` - Search and analyze host information\n  - `scan` - Network scanning and port discovery\n  - `dns` - DNS resolution and lookup\n  - `alert` - Network monitoring and alerts\n  - `cve` - CVE vulnerability checking\n  - `geo` - Geolocation-based network testing\n- **Examples**:\n  - `x shodan scan create 8.8.8.8 1.1.1.1=53/dns-udp,443/https` - Scan specific ports\n  - `x shodan dns res google.com facebook.com` - DNS resolution\n  - `x shodan cve` - Check specific product vulnerabilities\n  - `x shodan geo geoping 8.8.8.8,4.4.4.4` - Multi-location ping tests\n\n### OSV Vulnerability Scanner (osv)\nOpen Source Vulnerability scanning and dependency analysis.\n\n- **Usage**: `x osv [subcommand]`\n- **Key subcommands**:\n  - `query` - Query vulnerabilities for specific packages\n  - `scanner` - Use osv-scanner for vulnerability detection\n  - `sarif` - Generate SARIF vulnerability reports\n  - `vuln` - Get detailed vulnerability information\n  - `eco` - List supported ecosystems\n- **Examples**:\n  - `x osv q -p jq -v 1.7.1` - Query vulnerabilities for jq 1.7.1\n  - `x osv sarif` - Scan system packages and generate SARIF report\n  - `x osv vuln OSV-2020-111` - Get detailed vulnerability info\n  - `x osv : git` - Search for git-related vulnerabilities\n\n### KEV Vulnerability Catalog (kev)\nKnown Exploited Vulnerabilities tracking and management.\n\n- **Usage**: `x kev [subcommand]`\n- **Key subcommands**:\n  - `ls` - List all known exploited vulnerabilities\n  - `top` - List top N vulnerabilities\n- **Examples**:\n  - `x kev ls` - List all KEV entries\n  - `x kev top 100` - List top 100 exploited vulnerabilities\n\n## Security Use Cases\n\n### Network Security Assessment\n- Use `x shodan host` to discover exposed services\n- Use `x shodan scan` for targeted port scanning\n- Use `x shodan alert` for continuous monitoring\n\n### Vulnerability Management\n- Use `x osv scanner` for dependency vulnerability scanning\n- Use `x osv query` for specific package vulnerability checks\n- Use `x kev ls` to track actively exploited vulnerabilities\n\n### Security Research\n- Use `x shodan cve` for CVE-based vulnerability research\n- Use `x osv vuln` for detailed vulnerability analysis\n- Use `x shodan trend` for historical security trend analysis\n\n## Installation and Setup\n\n### Prerequisites\n- x-cmd CLI installed\n- Internet connection\n- Shodan API key (for full shodan functionality)\n\n### Configuration\n\n#### Shodan API Setup\n```bash\nx shodan init  # Interactive configuration\nx shodan --cfg key=<your-shodan-api-key>\n```\n\nGet Shodan API key from: https://account.shodan.io/\n\n#### OSV Configuration\nOSV typically works without additional configuration for basic queries. For advanced scanning:\n```bash\nx osv scanner --help  # View scanning options\n```\n\n## Security Best Practices\n\n### Responsible Usage\n- Only scan networks and systems you own or have explicit permission to test\n- Respect rate limits and terms of service for all tools\n- Use vulnerability information for defensive security purposes\n\n### Data Protection\n- API keys and sensitive configuration stored locally\n- Review data retention settings with `cfg` subcommands\n- Be mindful of information disclosure in shared environments\n\n### Compliance Considerations\n- Ensure usage complies with local laws and regulations\n- Obtain proper authorization before security testing\n- Document security assessments for audit purposes\n\n## Integration with Other Tools\n\n### Data Processing\n- Pipe output to `x jq` for JSON processing\n- Use with `@zh` for Chinese translation of security findings\n- Export to CSV/JSON for further analysis\n\n### Reporting\n- Generate SARIF reports with `x osv sarif`\n- Use `x shodan download` for data collection\n- Combine with documentation tools for security reports\n\n## Troubleshooting\n\n### Common Issues\n- **API key errors**: Verify Shodan API key configuration\n- **Rate limiting**: Respect API rate limits and use appropriate intervals\n- **Network connectivity**: Check internet connection and proxy settings\n- **Permission errors**: Ensure proper authorization for security testing\n\n### Performance Optimization\n- Use `--limit` flags to control data volume\n- Cache results when appropriate\n- Use specific queries rather than broad searches\n\n## Support and Resources\n\n- **Shodan Documentation**: https://help.shodan.io/\n- **OSV Project**: https://osv.dev/\n- **KEV Catalog**: CISA Known Exploited Vulnerabilities\n- **x-cmd Security**: https://x-cmd.com/mod/security\n\nFor additional help:\n- Use `x [tool] --help` for specific tool documentation\n- Visit individual module pages for detailed usage\n- Check tool-specific configuration options\n",
        "data/x-cmd/x-cmd-system/SKILL.md": "---\nname: x-cmd-system\ndescription: This skill provides comprehensive system administration and monitoring tools through x-cmd CLI, including process management, macOS system utilities, network configuration, disk health monitoring, and storage analysis. This skill should be used when users need to perform system administration tasks, monitor system performance, manage network configurations, or troubleshoot system issues from command line interfaces.\n---\n\n# x-cmd System Administration Tools\n\n## Overview\n\nThis skill provides professional system administration and monitoring capabilities through the x-cmd ecosystem. The tools enable system administrators, developers, and power users to manage processes, monitor system health, configure network settings, and analyze storage directly from the terminal.\n\n## Available Tools\n\n### Process Management (ps)\nEnhanced process monitoring and management with interactive interfaces.\n\n- **Usage**: `x ps [flags]`\n- **Key features**:\n  - Interactive CSV application for process viewing\n  - FZF-based interactive process selection\n  - Multiple output formats (CSV, TSV, JSON)\n  - Process data conversion utilities\n- **Examples**:\n  - `x ps` - Interactive process viewer\n  - `x ps fz` - FZF-based process selection\n  - `x ps --csv` - CSV format output\n  - `ps -ef | x ps --tojson` - Convert process data to JSON\n\n### macOS System Utilities (mac)\nComprehensive macOS management and automation tools.\n\n- **Usage**: `x mac [subcommand]`\n- **Key subcommands**:\n  - `info` - Display system information\n  - `battery` - Battery status and health\n  - `disk` - Disk management\n  - `vol` - Volume control\n  - `lock` - Desktop lock screen\n  - `sleep` - Enter sleep mode\n  - `net` - Network management\n  - `wifi` - Wi-Fi configuration\n- **Examples**:\n  - `x mac info` - Display system information\n  - `x mac b` - Get battery information\n  - `x mac vol set 24` - Set output volume to 24%\n  - `x mac lock` - Lock desktop\n  - `x mac net test` - Test network quality\n\n### Network Configuration (ip)\nAdvanced IP address management and network analysis.\n\n- **Usage**: `x ip [subcommand]`\n- **Key subcommands**:\n  - `ls` - List all local IP addresses\n  - `geolite` - IP geolocation lookup\n  - `config` - Network interface configuration\n  - `addr` - IP address information\n  - `cidr` - CIDR network calculations\n  - `map` - Active host discovery\n  - `tcp-portscan` - TCP port scanning\n- **Examples**:\n  - `x ip` - List all local IP addresses\n  - `x ip geolite 8.8.8.8` - Get IP geolocation\n  - `x ip map` - Discover active hosts in network\n  - `x ip tcp-portscan` - TCP port discovery\n\n### Disk Health Monitoring (smart)\nSMART disk health monitoring and analysis.\n\n- **Usage**: `x smart [flags] [device]`\n- **Key features**:\n  - Interactive disk selection interface\n  - Comprehensive SMART data display\n  - Root privilege handling automation\n  - Integration with AI analysis tools\n- **Examples**:\n  - `x smart` - Interactive disk health viewer\n  - `x smart -a /dev/disk0` - Display all SMART info for disk0\n  - `x smart -a /dev/disk0 | @gemini generate a report` - AI-generated disk health report\n  - `x smart : disk health` - Search disk health information\n\n### Storage Analysis (df)\nEnhanced disk space monitoring with multiple output formats.\n\n- **Usage**: `x df [flags]`\n- **Key features**:\n  - Automatic TUI/TSV output based on terminal type\n  - Multiple output formats (CSV, TSV, TUI)\n  - Cross-platform compatibility\n  - Interactive table interface\n- **Examples**:\n  - `x df` - Auto-detect output format (TUI/TSV)\n  - `x df --csv` - CSV format output\n  - `x df --tsv` - TSV format output\n  - `x df --app` - Interactive TUI application\n\n## System Administration Use Cases\n\n### System Monitoring\n- Use `x ps` for real-time process monitoring\n- Use `x df` for disk space analysis\n- Use `x mac info` for system health overview\n- Use `x smart` for disk health monitoring\n\n### Network Management\n- Use `x ip ls` for network interface analysis\n- Use `x ip geolite` for IP geolocation\n- Use `x ip map` for network discovery\n- Use `x mac net` for macOS network management\n\n### System Maintenance\n- Use `x mac disk` for disk management\n- Use `x mac trash` for system cleanup\n- Use `x mac battery` for power management\n- Use `x mac vol` for audio control\n\n### Security and Access Control\n- Use `x mac lock` for workstation security\n- Use `x mac tidsudo` for TouchID authentication\n- Use `x mac fw` for firewall management\n- Use `x mac sshd` for SSH server configuration\n\n## Installation and Setup\n\n### Prerequisites\n- x-cmd CLI installed\n- Appropriate system permissions\n- Internet connection (for geolocation and search features)\n\n### Platform-Specific Requirements\n\n#### macOS\n- Most `x mac` commands work natively\n- Some features require administrator privileges\n- TouchID integration available for supported systems\n\n#### Linux\n- `x smart` automatically handles sudo privileges\n- Network tools work across distributions\n- Process management compatible with standard Linux ps\n\n#### Windows\n- Limited macOS-specific functionality\n- Network and process tools available\n- Storage analysis works across platforms\n\n### Configuration\n\n#### Process Management\n```bash\nx ps --help  # View all available options\n```\n\n#### macOS System\n```bash\nx mac alias enable m  # Set alias for quick access\n```\n\n#### Network Tools\n```bash\nx ip --help  # View network analysis options\n```\n\n## Integration with Other Tools\n\n### Data Processing\n- Pipe output to `x jq` for JSON processing\n- Use with `@zh` for Chinese translation\n- Export to CSV/TSV for spreadsheet analysis\n\n### AI Integration\n- Use `@gemini` for AI-powered analysis\n- Generate reports from system data\n- Get recommendations for system optimization\n\n### Automation\n- Combine with shell scripts for automated monitoring\n- Schedule regular system health checks\n- Create custom system administration workflows\n\n## Troubleshooting\n\n### Common Issues\n- **Permission errors**: Ensure appropriate privileges for system commands\n- **Network connectivity**: Check internet connection for geolocation services\n- **Device detection**: Verify disk devices are accessible for SMART monitoring\n- **Platform compatibility**: Some tools are macOS-specific\n\n### Performance Optimization\n- Use specific queries rather than broad searches\n- Cache results when appropriate for monitoring\n- Use interactive interfaces for complex analysis\n\n### System-Specific Considerations\n- **macOS**: Some commands require SIP (System Integrity Protection) considerations\n- **Linux**: SMART monitoring requires appropriate device permissions\n- **Cross-platform**: Output formats standardized across platforms\n\n## Support and Resources\n\n- **x-cmd System Documentation**: https://x-cmd.com/mod/system\n- **Process Management**: https://x-cmd.com/mod/ps\n- **macOS Utilities**: https://x-cmd.com/mod/mac\n- **Network Tools**: https://x-cmd.com/mod/ip\n- **Disk Health**: https://x-cmd.com/mod/smart\n- **Storage Analysis**: https://x-cmd.com/mod/df\n\nFor additional help:\n- Use `x [tool] --help` for specific tool documentation\n- Visit individual module pages for detailed usage\n- Check platform-specific requirements for each tool"
      },
      "plugins": [
        {
          "name": "x-cmd-git",
          "description": "Comprehensive Git and code hosting platform management tools including GitHub, GitLab, Codeberg, Forgejo integration, and Git hooks management",
          "source": "./",
          "strict": false,
          "skills": [
            "./data/x-cmd/x-cmd-git"
          ],
          "categories": [],
          "install_commands": [
            "/plugin marketplace add x-cmd/skill",
            "/plugin install x-cmd-git@x-cmd-skill"
          ]
        },
        {
          "name": "x-cmd-knowledge",
          "description": "Access to various knowledge search tools including Hacker News, Wikipedia, DuckDuckGo search, RFC documents, Project Gutenberg books, and Stack Exchange",
          "source": "./",
          "strict": false,
          "skills": [
            "./data/x-cmd/x-cmd-knowledge"
          ],
          "categories": [],
          "install_commands": [
            "/plugin marketplace add x-cmd/skill",
            "/plugin install x-cmd-knowledge@x-cmd-skill"
          ]
        },
        {
          "name": "x-cmd-network",
          "description": "Network administration and diagnostic tools including network scanning with Nmap, ARP table management, DNS configuration, routing table analysis, and enhanced ping utilities",
          "source": "./",
          "strict": false,
          "skills": [
            "./data/x-cmd/x-cmd-network"
          ],
          "categories": [],
          "install_commands": [
            "/plugin marketplace add x-cmd/skill",
            "/plugin install x-cmd-network@x-cmd-skill"
          ]
        },
        {
          "name": "x-cmd-security",
          "description": "Security assessment and vulnerability management tools including network reconnaissance with Shodan, vulnerability scanning with OSV, and known exploited vulnerability tracking with KEV",
          "source": "./",
          "strict": false,
          "skills": [
            "./data/x-cmd/x-cmd-security"
          ],
          "categories": [],
          "install_commands": [
            "/plugin marketplace add x-cmd/skill",
            "/plugin install x-cmd-security@x-cmd-skill"
          ]
        },
        {
          "name": "x-cmd-system",
          "description": "System administration and monitoring tools including process management, macOS system utilities, network configuration, disk health monitoring, and storage analysis",
          "source": "./",
          "strict": false,
          "skills": [
            "./data/x-cmd/x-cmd-system"
          ],
          "categories": [],
          "install_commands": [
            "/plugin marketplace add x-cmd/skill",
            "/plugin install x-cmd-system@x-cmd-skill"
          ]
        }
      ]
    }
  ]
}