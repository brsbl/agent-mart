{
  "author": {
    "id": "vlm-run",
    "display_name": "VLM Run",
    "avatar_url": "https://avatars.githubusercontent.com/u/188105732?v=4"
  },
  "marketplaces": [
    {
      "name": "vlmrun-skills",
      "version": null,
      "description": "Agent Skills for visual AI tasks including image understanding, video processing, document extraction, and multi-modal generation using VLM Run's Orion agent",
      "repo_full_name": "vlm-run/skills",
      "repo_url": "https://github.com/vlm-run/skills",
      "repo_description": "Claude skill for VLM Run CLI",
      "signals": {
        "stars": 6,
        "forks": 0,
        "pushed_at": "2026-01-28T17:08:48Z"
      },
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"name\": \"vlmrun-skills\",\n  \"owner\": {\n    \"name\": \"VLM Run\"\n  },\n  \"metadata\": {\n    \"description\": \"Agent Skills for visual AI tasks including image understanding, video processing, document extraction, and multi-modal generation using VLM Run's Orion agent\",\n    \"version\": \"1.0.0\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"vlmrun-cli-skill\",\n      \"source\": \"./skills/vlmrun-cli-skill\",\n      \"skills\": \"./\",\n      \"description\": \"Use the VLM Run CLI to interact with Orion visual AI agent. Process images, videos, and documents with natural language. Supports image understanding/generation, object detection, OCR, video summarization, document extraction, and visual AI chat.\"\n    }\n  ]\n}\n",
        ".claude-plugin/plugin.json": "{\n  \"name\": \"vlmrun-skills\",\n  \"description\": \"Agent Skills for visual AI tasks including image understanding, video processing, document extraction, and multi-modal generation using VLM Run's Orion agent\",\n  \"version\": \"1.0.0\",\n  \"author\": {\n    \"name\": \"VLM Run\"\n  },\n  \"homepage\": \"https://vlm.run\",\n  \"repository\": \"https://github.com/vlm-run/skills\",\n  \"license\": \"Apache-2.0\",\n  \"keywords\": [\n    \"vlmrun\",\n    \"visual-ai\",\n    \"image-understanding\",\n    \"video-processing\",\n    \"document-extraction\",\n    \"ocr\",\n    \"image-generation\",\n    \"video-generation\",\n    \"multi-modal\"\n  ]\n}\n",
        "README.md": "<div align=\"center\">\n<p align=\"center\" style=\"width: 100%;\">\n    <img src=\"https://raw.githubusercontent.com/vlm-run/.github/refs/heads/main/profile/assets/vlm-black.svg\" alt=\"VLM Run Logo\" width=\"80\" style=\"margin-bottom: -5px; color: #2e3138; vertical-align: middle; padding-right: 5px;\"><br>\n</p>\n<h2>VLM Run Skills</h2>\n<p align=\"center\"><a href=\"https://docs.vlm.run\"><b>Website</b></a> | <a href=\"https://app.vlm.run/\"><b>Platform</b></a> | <a href=\"https://docs.vlm.run/\"><b>Docs</b></a> | <a href=\"https://docs.vlm.run/blog\"><b>Blog</b></a> | <a href=\"https://discord.gg/AMApC2UzVY\"><b>Discord</b></a>\n</p>\n<p align=\"center\">\n<a href=\"https://github.com/vlm-run/skills/blob/main/LICENSE\"><img alt=\"License\" src=\"https://img.shields.io/github/license/vlm-run/skills.svg\"></a>\n<a href=\"https://discord.gg/AMApC2UzVY\"><img alt=\"Discord\" src=\"https://img.shields.io/badge/discord-chat-purple?color=%235765F2&label=discord&logo=discord\"></a>\n<a href=\"https://twitter.com/vlmrun\"><img alt=\"Twitter Follow\" src=\"https://img.shields.io/twitter/follow/vlmrun.svg?style=social&logo=twitter\"></a>\n</p>\n</div>\n\nVLM Run Skills are definitions for visual AI tasks like image understanding, video processing, and document extraction. They are interoperable with Anthropic's Claude Code.\n\nThe Skills in this repository follow the standardized [Agent Skill](https://agentskills.io/home) format.\n\n## How do Skills work?\n\nIn practice, skills are self-contained folders that package instructions, scripts, and resources together for an AI agent to use on a specific use case. Each folder includes a `SKILL.md` file with YAML frontmatter (name and description) followed by the guidance your coding agent follows while the skill is active.\n\n<p align=\"center\">\n  <a href=\"https://vlm.run\" target=\"_blank\" rel=\"noopener noreferrer\">\n    <img\n      src=\"https://github.com/user-attachments/assets/68c63ddf-b0c0-4724-8caa-e7ae818c2edb\"\n      style=\"max-width: 400px; width: 70%;\"\n      alt=\"vlm.run\"\n    />\n  </a>\n</p>\n\n## Features\n\n### Image Intelligence\n- **Understanding & Captioning**: Describe, analyze, and interpret images with state-of-the-art visual intelligence\n- **Detection & Localization**: Detect and locate objects, people, faces, and custom entities with bounding boxes\n- **Segmentation**: Segment objects, scenes, and regions with pixel-level precision\n- **Generation & Editing**: Generate images from text, edit existing images, apply super-resolution, colorize B&W photos\n- **Tools**: Crop, rotate, enhance resolution (4x-8x upscaling), de-oldify (colorization)\n- **Visual Grounding**: Point to and extract specific elements using natural language queries\n- **UI Parsing**: Extract UI elements, layouts, and hierarchies from screenshots\n\n### Video Intelligence\n- **Understanding & Captioning**: Describe video content, generate summaries and detailed scene analysis\n- **Transcription**: Extract audio transcripts with timestamps\n- **Tools**: Trim videos, extract keyframes, sample frames at intervals, detect highlights\n- **Segmentation**: Identify and segment objects across video frames\n- **Generation & Editing**: Generate videos from text prompts, edit existing videos\n\n### Document Intelligence\n- **Layout Understanding**: Detect headers, paragraphs, tables, figures, lists, and structural elements\n- **Multi-Page Analysis**: Process and analyze PDFs with intelligent page-aware extraction\n- **Markdown Extraction**: Convert documents to clean, structured markdown with preserved formatting\n- **Visual Grounding**: Locate and extract specific fields, sections, or data points\n- **Data Extraction**: Extract key information from invoices, receipts, contracts, forms into structured JSON\n\n### Multi-modal Agents\n- **Multi-Modal Reasoning**: Execute complex multi-step workflows across images, documents, and videos\n- **Structured Outputs**: Get results in validated JSON schemas with automatic retry logic\n\nSee [docs](https://docs.vlm.run/agents/introduction) and [technical whitepaper](https://vlm.run/orion/whitepaper) for more information.\n\n## Installation\n\n### Prerequisites\n\n1. Get your VLM Run API key from [app.vlm.run](https://app.vlm.run)\n2. Have [uv](https://docs.astral.sh/uv/) installed for Python environment management\n\n### Claude Code\n\n1. Register the repository as a plugin marketplace:\n\n```\n/plugin marketplace add vlm-run/skills\n```\n\n2. To install a skill, run:\n\n```\n/plugin install <skill-name>@vlm-run/skills\n```\n\nFor example:\n\n```\n/plugin install vlmrun-cli-skill@vlm-run/skills\n```\n\n#### Update the .env file\n\nOnce the skill is installed, create an `.env` file (by copying from the `.env.template` file) with your API key that you can get from the dashboard at [https://app.vlm.run](https://app.vlm.run) and base URL.\n\n#### Verify Installation\n\nOnce installed, verify the skill is loaded by asking Claude Code (requires restart):\n```\nWhat skills are available in the /vlmrun-cli-skill?\n```\n\n### Installing in Claude for Desktop\n\nYou should be able to install the skill by simply asking Claude to create a new skill from the [`SKILL.md`](./skills/vlmrun-cli-skill/SKILL.md) file.\n\n## Skills\n\nThis repository contains skills for interacting with VLM Run's Orion visual AI agent. You can also contribute your own skills to the repository.\n\n### Available skills\n\n| Name | Description | Documentation |\n|------|-------------|---------------|\n| `vlmrun-cli-skill` | Use the VLM Run CLI to interact with Orion visual AI agent. Process images, videos, and documents with natural language. Supports image understanding/generation, object detection, OCR, video summarization, document extraction, and visual AI chat. | [SKILL.md](skills/vlmrun-cli-skill/SKILL.md) |\n\n### Using skills in your coding agent\n\nOnce a skill is installed, mention it directly while giving your coding agent instructions:\n\n- \"Use the VLM Run CLI skill to describe what's in this image\"\n- \"Use the VLM Run CLI skill to generate an image of a sunset over mountains\"\n- \"Use the VLM Run CLI skill to extract the text from this receipt\"\n- \"Use the VLM Run CLI skill to summarize this meeting recording\"\n\nYour coding agent automatically loads the corresponding `SKILL.md` instructions and helper scripts while it completes the task.\n\n### Contribute or customize a skill\n\n1. Copy one of the existing skill folders (for example, `skills/vlmrun-cli-skill/`) and rename it.\n2. Update the new folder's `SKILL.md` frontmatter:\n   ```markdown\n   ---\n   name: my-skill-name\n   description: Describe what the skill does and when to use it\n   ---\n\n   # Skill Title\n   Guidance + examples + guardrails\n   ```\n3. Add or edit supporting scripts, templates, and documents referenced by your instructions.\n4. Add an entry to `.claude-plugin/marketplace.json` with a concise, human-readable description.\n5. Reinstall or reload the skill bundle in your coding agent so the updated folder is available.\n\n## Environment Variables\n\n| Variable | Description |\n|----------|-------------|\n| `VLMRUN_API_KEY` | Your VLM Run API key (required) |\n| `VLMRUN_BASE_URL` | API base URL (default: `https://agent.vlm.run/v1`) |\n| `VLMRUN_CACHE_DIR` | Cache directory (default: `~/.vlmrun/cache/artifacts/`) |\n\n## License\n\nSee [LICENSE](./LICENSE) for details.\n"
      },
      "plugins": [
        {
          "name": "vlmrun-cli-skill",
          "source": "./skills/vlmrun-cli-skill",
          "skills": "./",
          "description": "Use the VLM Run CLI to interact with Orion visual AI agent. Process images, videos, and documents with natural language. Supports image understanding/generation, object detection, OCR, video summarization, document extraction, and visual AI chat.",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add vlm-run/skills",
            "/plugin install vlmrun-cli-skill@vlmrun-skills"
          ]
        }
      ]
    }
  ]
}