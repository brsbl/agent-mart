{
  "author": {
    "id": "sjawhar",
    "display_name": "Sami Jawhar",
    "avatar_url": "https://avatars.githubusercontent.com/u/5074378?u=633bc1fa3be7bc2d460dd0797a9149b0e47195be&v=4"
  },
  "marketplaces": [
    {
      "name": "pivot",
      "version": null,
      "description": "Skills for writing Pivot pipeline stages",
      "repo_full_name": "sjawhar/pivot",
      "repo_url": "https://github.com/sjawhar/pivot",
      "repo_description": null,
      "signals": {
        "stars": 1,
        "forks": 2,
        "pushed_at": "2026-02-11T16:34:01Z"
      },
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"name\": \"pivot\",\n  \"description\": \"Skills for writing Pivot pipeline stages\",\n  \"owner\": {\n    \"name\": \"Sami Jawhar\",\n    \"email\": \"sami@sjawhar.me\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"pivot\",\n      \"description\": \"Skills for writing Pivot pipeline stages - file I/O annotations, loaders, and output types\",\n      \"version\": \"0.1.0\",\n      \"source\": \"./\"\n    }\n  ]\n}\n",
        ".claude-plugin/plugin.json": "{\n  \"name\": \"pivot\",\n  \"description\": \"Skills for writing Pivot pipeline stages - file I/O annotations, loaders, and output types\",\n  \"version\": \"0.1.0\",\n  \"author\": {\n    \"name\": \"Sami Jawhar\"\n  },\n  \"homepage\": \"https://github.com/sjawhar/pivot\",\n  \"repository\": \"https://github.com/sjawhar/pivot\",\n  \"license\": \"MIT\",\n  \"keywords\": [\"pivot\", \"pipelines\", \"data-science\", \"stages\", \"annotations\"]\n}\n",
        "README.md": "# Pivot: High-Performance Python Pipeline Tool\n\n**Change your code. Pivot knows what to run.**\n\n**Python:** 3.13+ | **Coverage:** 90%+ | **License:** TBD\n\n---\n\n## What is Pivot?\n\nPivot is a Python pipeline tool with automatic code change detection. Define stages with typed Python functions and annotations, and Pivot figures out what needs to re-run—no manual dependency declarations, no stale caches. It provides:\n\n- **32x faster lock file operations** through per-stage lock files\n- **Automatic code change detection** using Python introspection (no manual declarations!)\n- **Warm worker pools** with preloaded imports (numpy/pandas loaded once)\n- **DVC compatibility** via YAML export for code reviews\n\n### Performance Comparison (monitoring-horizons pipeline, 176 stages)\n\n| Component         | DVC          | Pivot       | Improvement     |\n| ----------------- | ------------ | ----------- | --------------- |\n| Lock file writes  | 289s (23.8%) | ~9s (0.7%)  | **32x faster**  |\n| Total overhead    | 301s (24.8%) | ~20s (1.6%) | **15x faster**  |\n| **Total runtime** | **1214s**    | **~950s**   | **1.3x faster** |\n\n---\n\n## Quick Start\n\n```bash\npip install pivot\n```\n\n### Python-First Pipeline Definition\n\nDefine stages with typed Python functions using annotations:\n\n```python\n# pipeline.py\nimport pathlib\nfrom typing import Annotated, TypedDict\n\nimport pandas\nfrom pivot import loaders, outputs\nfrom pivot.pipeline import Pipeline\n\n\nclass PreprocessOutputs(TypedDict):\n    clean: Annotated[pathlib.Path, outputs.Out(\"processed.parquet\", loaders.PathOnly())]\n\n\ndef preprocess(\n    raw: Annotated[pandas.DataFrame, outputs.Dep(\"data.csv\", loaders.CSV())],\n) -> PreprocessOutputs:\n    df = raw.dropna()\n    out_path = pathlib.Path(\"processed.parquet\")\n    df.to_parquet(out_path)\n    return PreprocessOutputs(clean=out_path)\n\n\nclass TrainOutputs(TypedDict):\n    model: Annotated[pathlib.Path, outputs.Out(\"model.pkl\", loaders.PathOnly())]\n\n\ndef train(\n    data: Annotated[pathlib.Path, outputs.Dep(\"processed.parquet\", loaders.PathOnly())],\n) -> TrainOutputs:\n    df = pandas.read_parquet(data)\n    model_path = pathlib.Path(\"model.pkl\")\n    # ... train model ...\n    return TrainOutputs(model=model_path)\n\n\n# Create pipeline and register stages\npipeline = Pipeline(\"my_pipeline\")\npipeline.register(preprocess)\npipeline.register(train)\n```\n\n```bash\npivot repro  # Runs both stages\npivot repro  # Instant - nothing changed\n```\n\nModify `preprocess`, and Pivot automatically re-runs both stages. Modify `train`, and only `train` re-runs.\n\n**Why Python-first?**\n- Full type safety and IDE autocomplete\n- Dependencies and outputs declared in one place (the function)\n- Loader code changes automatically trigger re-runs\n- No YAML to keep in sync with your Python code\n\n---\n\n## Key Features\n\n### 1. Automatic Code Change Detection\n\n**No more manual dependency declarations!** Pivot automatically detects when your Python functions change:\n\n```python\ndef helper(x):\n    return x * 2  # Change this...\n\ndef process():\n    data = load(\"data.csv\")\n    return helper(data)  # ...and Pivot knows to re-run!\n```\n\n**How it works:**\n\n- Uses `inspect.getclosurevars()` to find referenced functions/constants\n- AST extraction for `module.attr` patterns (Google-style imports)\n- Recursive fingerprinting for transitive dependencies\n\n**Validated:** See [Architecture: Fingerprinting](docs/architecture/fingerprinting.md) and [tests/fingerprint/](tests/fingerprint/)\n\n### 2. Per-Stage Lock Files\n\n**DVC's bottleneck:** Every stage writes the entire `dvc.lock` file (O(n²) behavior)\n\n**Pivot's solution:** Each stage gets its own lock file\n\n- `.pivot/stages/train.lock` (~500 bytes)\n- Parallel writes without contention\n- **32x faster** on large pipelines\n\n### 3. Warm Worker Pool\n\n**Problem:** Importing numpy/pandas can take seconds per stage\n\n**Solution:** Reusable worker processes with preloaded imports. Pivot uses `loky.get_reusable_executor()` to keep workers warm between runs, avoiding repeated import overhead.\n\n### 4. DVC Compatibility\n\nExport Pivot pipelines to DVC YAML for code review:\n\n```bash\npivot export --validate  # Creates dvc.yaml and validates against DVC\n```\n\n### 5. Explain Mode\n\n**Killer feature:** See WHY a stage would run\n\n```bash\npivot repro --explain\n\nStage: train\n  Status: WILL RUN\n  Reason: Code dependency changed\n\n  Changes:\n    func:helper_a\n      Old: 5995c853\n      New: a1b2c3d4\n      File: src/utils.py:15\n```\n\n### 6. Stage Parameters\n\n**Type-safe parameters via Pydantic classes:**\n\n```python\nfrom pivot.stage_def import StageParams\n\nclass TrainParams(StageParams):\n    learning_rate: float = 0.01\n    epochs: int = 100\n    batch_size: int = 32\n\ndef train(\n    params: TrainParams,\n    data: Annotated[pandas.DataFrame, outputs.Dep(\"data.csv\", loaders.CSV())],\n) -> TrainOutputs:\n    # params.learning_rate, params.epochs, params.batch_size available\n    ...\n```\n\n**How it works:**\n\n- Define parameters as Pydantic classes inheriting from `StageParams`\n- Parameter changes trigger re-execution (tracked in lock files)\n\n**View and compare params:**\n\n```bash\npivot params show                    # Show current param values\npivot params show --format json      # JSON output\npivot params diff                    # Compare workspace vs last commit\npivot params diff --precision 4      # Control float precision\n```\n\n### 7. Incremental Outputs\n\nOutputs that preserve state between runs for append-only workloads:\n\n```python\nclass AppendOutputs(TypedDict):\n    database: Annotated[pathlib.Path, outputs.IncrementalOut(\"database.db\", loaders.PathOnly())]\n\ndef append_to_database(\n    new_data: Annotated[pandas.DataFrame, outputs.Dep(\"new_data.csv\", loaders.CSV())],\n) -> AppendOutputs:\n    # database.db is restored from cache before this runs\n    # Modify in place, then return the path\n    ...\n```\n\n**How it works:**\n- Before execution, incremental outputs restore the previous version from cache\n- Stage modifies the file in place (append, update, etc.)\n- New version is cached after execution\n- Uses COPY mode (not symlinks) so stages can safely modify files\n\n### 8. S3 Remote Cache Storage\n\n**Share cached outputs across machines and CI environments:**\n\n```bash\n# Add a remote\npivot config set remotes.origin s3://my-bucket/pivot-cache\npivot config set default_remote origin\n\n# Push cached outputs to remote\npivot push\n\n# Push specific stages only\npivot push train_model evaluate_model\n\n# Pull cached outputs from remote\npivot pull\n\n# Pull specific stages (downloads only what's needed)\npivot pull train_model\n```\n\n**How it works:**\n- Uses async I/O (aioboto3) for high-throughput parallel transfers\n- Local index in LMDB avoids repeated HEAD requests to S3\n- Stage-level filtering enables granular push/pull operations\n- AWS credentials via standard chain (env vars, ~/.aws/credentials, IAM roles)\n\n**Remote configuration stored in `.pivot/config.yaml`:**\n```yaml\nremotes:\n  origin: s3://my-bucket/pivot-cache\ndefault_remote: origin\n```\n\n**Local cache structure:**\n```\n.pivot/\n├── cache/\n│   └── files/           # Content-addressable cache (pushed/pulled)\n│       ├── ab/\n│       │   └── cdef0123...  # File content keyed by xxhash64\n│       └── ...\n├── stages/              # Per-stage lock files (local only)\n│   ├── preprocess.lock\n│   └── train.lock\n├── config.yaml          # Remote configuration (local only)\n└── state.lmdb/          # Hash cache, generation tracking (local only)\n```\n\n**What gets transferred:**\n\n| Data | Pushed | Pulled | Notes |\n|------|--------|--------|-------|\n| Cache files (`.pivot/cache/files/`) | ✅ | ✅ | Actual file contents, content-addressable by hash |\n| Lock files (`.pivot/stages/*.lock`) | ❌ | ❌ | Reference hashes; must exist locally to pull specific stages |\n| Config (`.pivot/config.yaml`) | ❌ | ❌ | Contains remote URLs; each machine has its own |\n| State DB (`.pivot/state.lmdb/`) | ❌ | ❌ | Local performance cache; rebuilt automatically |\n\n**Typical workflow:**\n1. Run pipeline locally → outputs cached in `.pivot/cache/files/`\n2. `pivot push` → upload cache files to S3\n3. On another machine: clone repo (includes lock files in git)\n4. `pivot pull train_model` → download only files needed for that stage\n5. `pivot repro` → stages with cached outputs skip execution\n\n### 9. Data Diff\n\nCompare data file changes between git HEAD and workspace:\n\n```bash\n# Interactive TUI mode (default)\npivot data diff output.csv\n\n# Non-interactive output\npivot data diff output.csv --no-tui\n\n# Use key columns for row matching (instead of positional)\npivot data diff output.csv --key id --key timestamp\n\n# JSON output for scripting\npivot data diff output.csv --no-tui --json\n```\n\n**Features:**\n- **Interactive TUI** - Navigate between files, view schema changes and row-level diffs\n- **Key-based matching** - Match rows by key columns to detect modifications vs adds/removes\n- **Positional matching** - Compare rows by position when no keys specified\n- **Schema detection** - Shows added/removed/type-changed columns\n- **Large file handling** - Summary-only mode for files exceeding memory threshold\n- **Multiple formats** - Plain text, markdown, or JSON output\n\n**Supported formats:** CSV, JSON, JSONL\n\n### 10. Typed Dependencies and Outputs (StageDef)\n\n**Declare typed deps/outs that auto-load and auto-save:**\n\n```python\nimport pandas as pd\nfrom pivot import loaders\nfrom pivot.stage_def import StageDef, Dep, Out\n\n\nclass TrainParams(StageDef):\n    # Deps - type-safe file loading\n    train_data: Dep[pd.DataFrame] = Dep(\"data/train.csv\", loaders.CSV())\n    config: Dep[dict] = Dep(\"config.json\", loaders.JSON())\n\n    # Outs - type-safe file saving\n    model: Out[dict] = Out(\"models/model.pkl\", loaders.Pickle())\n    metrics: Out[dict] = Out(\"metrics.json\", loaders.JSON())\n\n    # Regular params\n    learning_rate: float = 0.01\n    epochs: int = 100\n\n\ndef train(params: TrainParams) -> None:\n    # Deps are auto-loaded before function runs\n    df = params.train_data  # Returns pd.DataFrame\n    config = params.config  # Returns dict\n\n    # Train model...\n    model = {\"weights\": df.shape, \"lr\": params.learning_rate}\n    metrics = {\"accuracy\": 0.95}\n\n    # Assign outputs - auto-saved after function returns\n    params.model = model\n    params.metrics = metrics\n```\n\n**Benefits:**\n- **Type-safe** - IDE autocomplete, type checking for loaded data\n- **Auto-load/save** - No boilerplate file I/O code\n- **Automatic fingerprinting** - Loader code changes trigger re-runs\n- **Fully self-contained** - Deps/outs are extracted from StageDef, no external config needed\n\n**Built-in loaders:**\n\n| Loader | Load returns | Save accepts | Options |\n|--------|--------------|--------------|---------|\n| `loaders.CSV()` | DataFrame | DataFrame | `sep`, `index_col`, `dtype` |\n| `loaders.JSON()` | dict/list | dict/list | `indent` |\n| `loaders.YAML()` | dict/list | dict/list | - |\n| `loaders.Pickle()` | Any | Any | - |\n| `loaders.PathOnly()` | pathlib.Path | (validates exists) | - |\n\n**Custom loaders:**\n\n```python\nimport dataclasses\nfrom pivot.loaders import Loader\n\n\n@dataclasses.dataclass(frozen=True)\nclass Parquet(Loader[pd.DataFrame]):\n    compression: str = \"snappy\"\n\n    def load(self, path: pathlib.Path) -> pd.DataFrame:\n        return pd.read_parquet(path)\n\n    def save(self, data: pd.DataFrame, path: pathlib.Path) -> None:\n        data.to_parquet(path, compression=self.compression)\n```\n\n**Note:** Custom loaders must be defined at module level (not inside functions) for pickling across processes.\n\n---\n\n## Installation\n\n```bash\npip install pivot\n```\n\n**Requirements:**\n\n- Python 3.13+ (3.14+ for InterpreterPoolExecutor)\n- Unix only (Linux/macOS)\n\n---\n\n## Documentation\n\nFull documentation available at the [Pivot Documentation Site](https://anthropics.github.io/pivot/).\n\n- **[Quick Start](docs/getting-started/quickstart.md)** - Build your first pipeline in 5 minutes\n- **[CLI Reference](docs/cli/index.md)** - All available commands\n- **[Architecture](docs/architecture/overview.md)** - Design decisions and internals\n- **[Comparison](docs/comparison.md)** - How Pivot compares to DVC, Prefect, Dagster\n\n---\n\n## Development Roadmap\n\n### Completed ✅\n\n- **Core pipeline execution** - DAG construction, greedy parallel scheduling, per-stage lock files\n- **Automatic code change detection** - getclosurevars + AST fingerprinting, transitive dependencies\n- **Content-addressable cache** - xxhash64 hashing for files and code, hardlink/copy restoration\n- **Pydantic parameters** - Type-safe stage parameters with params.yaml overrides\n- **Watch mode** - File system monitoring with configurable globs and debounce\n- **Incremental outputs** - Restore-before-run for append-only workloads\n- **DVC export** - `pivot export` command for YAML generation\n- **Explain mode** - `pivot repro --explain` and `pivot status --explain` show detailed breakdown of WHY stages would run\n- **Observability** - `pivot metrics show/diff`, `pivot plots show/diff`, and `pivot params show/diff` commands\n- **Pipeline configuration** - Python-first with `Pipeline.register()`, optional `pivot.yaml` for matrix expansion\n- **S3 remote cache** - `pivot push/pull` with async I/O, LMDB index, per-stage filtering\n- **Data diff** - `pivot data diff` command with interactive TUI for comparing data file changes\n- **Version retrieval** - `pivot data get --rev` to materialize files from any git revision\n- **Shell completion** - Tab completion for bash, zsh, and fish\n- **Centralized configuration** - `pivot config` command for managing project settings\n- **Typed deps/outs (StageDef)** - Auto-loading/saving dependencies and outputs with type-safe loaders\n\n### Planned\n\n- **Web UI** - DAG visualization and execution monitoring\n- **Additional remotes** - GCS, Azure, SSH\n- **Cloud orchestration** - Integration with cloud schedulers\n\n---\n\n## Architecture\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│  User Pipeline Code (pipeline.py with Pipeline.register())  │\n│  @outputs.Dep(\"data.csv\", loaders.CSV())                    │\n│  @outputs.Out(\"model.pkl\", loaders.PathOnly())              │\n│  def train(data: ...) -> TrainOutputs: ...                  │\n└─────────────────────────────────────────────────────────────┘\n                         │\n                         ▼\n┌─────────────────────────────────────────────────────────────┐\n│  Stage Registry → DAG Builder → Scheduler                    │\n│  Automatic fingerprinting | Topological sort | Ready queue  │\n└─────────────────────────────────────────────────────────────┘\n                         │\n                         ▼\n┌─────────────────────────────────────────────────────────────┐\n│  Warm Workers / Interpreters                                 │\n│  Preloaded numpy/pandas | True parallelism                  │\n└─────────────────────────────────────────────────────────────┘\n                         │\n                         ▼\n┌─────────────────────────────────────────────────────────────┐\n│  Per-Stage Lock Files (.pivot/stages/<name>.lock)        │\n│  Code manifest | Params | Deps/Outs | Fast parallel writes │\n└─────────────────────────────────────────────────────────────┘\n```\n\n---\n\n## Contributing\n\nThis is currently an internal development project. Guidelines:\n\n### Code Quality Standards\n\n- **TDD:** Write tests before implementation\n- **Type hints:** All functions must be fully typed\n- **Formatting:** ruff format with line length 100\n- **Linting:** ruff check for fast linting\n- **Coverage:** >90% for all modules\n- **Documentation:** Update CLAUDE.md files when design changes\n\n### Before Committing\n\n```bash\n# Run tests\npytest tests/ -v\n\n# Check coverage\npytest --cov=src/pivot --cov-report=term --cov-fail-under=90\n\n# Format code\nruff format src/ tests/\n\n# Lint\nruff check src/ tests/\n\n# Type check\nbasedpyright src/\n```\n\n---\n\n## Comparison with DVC\n\n| Feature                   | DVC                         | Pivot                       |\n| ------------------------- | --------------------------- | --------------------------- |\n| **Lock file format**      | Monolithic dvc.lock         | Per-stage .lock files       |\n| **Lock file overhead**    | O(n²) - 289s for 176 stages | O(n) - ~9s for 176 stages   |\n| **Code change detection** | Manual (deps: [file.py])    | Automatic (getclosurevars)  |\n| **Executor**              | ThreadPoolExecutor          | Warm workers + Interpreters |\n| **Explain mode**          | ❌                          | ✅ Shows WHY stages run     |\n| **YAML export**           | N/A                         | ✅ For code review          |\n| **Python-first**          | Config-first (YAML)         | Python annotations + optional YAML |\n| **Remote storage**        | S3/GCS/Azure via dvc-data   | S3 with async I/O           |\n\n---\n\n## Technical Details\n\n### Fingerprinting\n\nPivot automatically detects code changes using Python introspection:\n\n- **[How Fingerprinting Works](docs/architecture/fingerprinting.md)** - AST + getclosurevars approach\n- **[Change Detection Matrix](tests/fingerprint/README.md)** - Comprehensive behavior catalog\n\n### Performance\n\nPivot was designed to address DVC's lock file bottleneck (O(n²) writes). Benchmarks on real pipelines showed:\n\n- 176-stage pipeline: Lock file writes reduced from 289s to ~9s (32x improvement)\n- Per-stage lock files enable parallel writes without contention\n\n---\n\n## FAQ\n\n### Q: Why not just contribute to DVC?\n\n**A:** The per-stage lock file approach requires fundamental architectural changes that would break backward compatibility. Pivot can coexist with DVC during migration.\n\n### Q: Can I use Pivot with existing DVC projects?\n\n**A:** Yes! Export your Pivot pipeline to dvc.yaml and run them side-by-side. Validate outputs match before fully migrating.\n\n### Q: What if automatic code detection doesn't work for my case?\n\n**A:** We provide escape hatches for edge cases (dynamic dispatch, method calls). You can manually declare dependencies when needed.\n\n### Q: Why Python 3.13+ requirement?\n\n**A:** We use modern typing features and performance improvements. Python 3.14+ unlocks experimental InterpreterPoolExecutor.\n\n### Q: Is this production-ready?\n\n**A:** Not yet! Core functionality is complete and well-tested (90%+ coverage), but we're polishing the final features before the 1.0 release.\n\n---\n\n## License\n\nTBD\n\n---\n\n## Contact\n\nInternal development team\nQuestions? Check CLAUDE.md files or ask the team!\n\n---\n\n**Last Updated:** 2026-01-10\n**Version:** 0.1.0-dev\n\n"
      },
      "plugins": [
        {
          "name": "pivot",
          "description": "Skills for writing Pivot pipeline stages - file I/O annotations, loaders, and output types",
          "version": "0.1.0",
          "source": "./",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add sjawhar/pivot",
            "/plugin install pivot@pivot"
          ]
        }
      ]
    }
  ]
}