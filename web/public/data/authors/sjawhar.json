{
  "author": {
    "id": "sjawhar",
    "display_name": "Sami Jawhar",
    "avatar_url": "https://avatars.githubusercontent.com/u/5074378?u=633bc1fa3be7bc2d460dd0797a9149b0e47195be&v=4"
  },
  "marketplaces": [
    {
      "name": "pivot",
      "version": null,
      "description": "Skills for writing Pivot pipeline stages",
      "repo_full_name": "sjawhar/pivot",
      "repo_url": "https://github.com/sjawhar/pivot",
      "repo_description": null,
      "signals": {
        "stars": 1,
        "forks": 2,
        "pushed_at": "2026-02-14T08:06:38Z"
      },
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"name\": \"pivot\",\n  \"description\": \"Skills for writing Pivot pipeline stages\",\n  \"owner\": {\n    \"name\": \"Sami Jawhar\",\n    \"email\": \"sami@sjawhar.me\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"pivot\",\n      \"description\": \"Skills for writing Pivot pipeline stages - file I/O annotations, loaders, and output types\",\n      \"version\": \"0.1.0\",\n      \"source\": \"./\"\n    }\n  ]\n}\n",
        ".claude-plugin/plugin.json": "{\n  \"name\": \"pivot\",\n  \"description\": \"Skills for writing Pivot pipeline stages - file I/O annotations, loaders, and output types\",\n  \"version\": \"0.1.0\",\n  \"author\": {\n    \"name\": \"Sami Jawhar\"\n  },\n  \"homepage\": \"https://github.com/sjawhar/pivot\",\n  \"repository\": \"https://github.com/sjawhar/pivot\",\n  \"license\": \"MIT\",\n  \"keywords\": [\"pivot\", \"pipelines\", \"data-science\", \"stages\", \"annotations\"]\n}\n",
        "README.md": "# Pivot: High-Performance Python Pipeline Tool\n\n**Change your code. Pivot knows what to run.**\n\n**Python:** 3.13+ | **Platform:** Unix (Linux/macOS) | **Coverage:** 90%+\n\n---\n\n## What is Pivot?\n\nPivot is a Python pipeline tool with automatic code change detection. Define stages with typed Python functions and annotations, and Pivot figures out what needs to re-run — no manual dependency declarations, no stale caches.\n\n- **Automatic code change detection** using Python introspection\n- **Per-stage lock files** for fast parallel writes (32x faster than monolithic locks)\n- **Warm worker pools** with preloaded imports\n- **Content-addressable caching** with S3 remote storage\n- **DVC compatibility** via YAML export\n\n---\n\n## Quick Start\n\n```bash\npip install pivot\npivot init\n```\n\n### Python-First Pipeline Definition\n\n```python\n# pipeline.py\nimport pathlib\nfrom typing import Annotated, TypedDict\n\nimport pandas\nimport pivot\n\n\nclass PreprocessOutputs(TypedDict):\n    clean: Annotated[pathlib.Path, pivot.Out(\"processed.parquet\", pivot.loaders.PathOnly())]\n\n\ndef preprocess(\n    raw: Annotated[pandas.DataFrame, pivot.Dep(\"data.csv\", pivot.loaders.CSV())],\n) -> PreprocessOutputs:\n    df = raw.dropna()\n    out_path = pathlib.Path(\"processed.parquet\")\n    df.to_parquet(out_path)\n    return PreprocessOutputs(clean=out_path)\n\n\nclass TrainOutputs(TypedDict):\n    model: Annotated[pathlib.Path, pivot.Out(\"model.pkl\", pivot.loaders.PathOnly())]\n\n\ndef train(\n    data: Annotated[pathlib.Path, pivot.Dep(\"processed.parquet\", pivot.loaders.PathOnly())],\n) -> TrainOutputs:\n    df = pandas.read_parquet(data)\n    model_path = pathlib.Path(\"model.pkl\")\n    # ... train model ...\n    return TrainOutputs(model=model_path)\n\n\n# Create pipeline and register stages\npipeline = pivot.Pipeline(\"my_pipeline\")\npipeline.register(preprocess)\npipeline.register(train)\n```\n\n```bash\npivot repro  # Runs both stages\npivot repro  # Instant - nothing changed\n```\n\nModify `preprocess`, and Pivot automatically re-runs both stages. Modify `train`, and only `train` re-runs.\n\n---\n\n## Installation\n\n```bash\npip install pivot\n```\n\n**Requirements:** Python 3.13+, Unix only (Linux/macOS)\n\n---\n\n## Key Features\n\n### Automatic Code Change Detection\n\nPivot detects when your Python functions change — no manual `deps:` declarations:\n\n```python\ndef helper(x):\n    return x * 2  # Change this...\n\ndef process():\n    data = load(\"data.csv\")\n    return helper(data)  # ...and Pivot knows to re-run!\n```\n\nUses `inspect.getclosurevars()` + AST extraction with recursive fingerprinting for transitive dependencies.\n\n### Explain Mode\n\nSee *why* a stage would run:\n\n```bash\npivot repro --explain\n\nStage: train\n  Status: WILL RUN\n  Reason: Code dependency changed\n  Changes:\n    func:helper_a  Old: 5995c853  New: a1b2c3d4\n```\n\n### Stage Parameters\n\nType-safe parameters via Pydantic:\n\n```python\nimport pivot\n\nclass TrainParams(pivot.StageParams):\n    learning_rate: float = 0.01\n    epochs: int = 100\n\ndef train(params: TrainParams, data: Annotated[...]) -> TrainOutputs:\n    ...\n```\n\nParameter changes are tracked in lock files and trigger re-execution.\n\n### S3 Remote Cache\n\nShare outputs across machines and CI:\n\n```bash\npivot config set remotes.origin s3://my-bucket/pivot-cache\npivot push                 # Upload to remote\npivot pull train_model     # Download specific stage outputs\n```\n\n### Import Artifacts\n\nImport artifacts from remote Pivot repositories:\n\n```bash\npivot import https://github.com/org/ml-models model.pkl --rev v1.0\npivot update               # Check and apply updates\n```\n\n### Incremental Outputs\n\nOutputs that preserve state between runs for append-only workloads. Before execution, previous versions are restored from cache; the stage modifies in place and the new version is cached.\n\n### Data Diff\n\nCompare data file changes interactively:\n\n```bash\npivot diff output.csv                    # Interactive TUI\npivot diff output.csv --key id --json    # Key-based matching, JSON output\n```\n\n---\n\n## Common Commands\n\n```bash\npivot repro                    # Run entire pipeline (DAG-aware)\npivot repro train evaluate     # Run specific stages + dependencies\npivot repro --watch            # Watch mode - re-run on file changes\npivot repro --show-output      # Stream stage stdout/stderr to terminal\npivot run my_stage             # Run ONLY my_stage (no dep resolution)\npivot repro -n                 # Dry run - see what would execute\n\npivot status                   # Show pipeline status\npivot status --explain train   # Understand why a stage is stale\npivot list --deps              # List stages with dependencies\npivot dag --mermaid            # Visualize DAG as Mermaid diagram\n\npivot diff output.csv          # Compare data files vs git HEAD\npivot metrics show             # Display metric values\npivot params diff              # Compare params against HEAD\n\npivot push                     # Push cached outputs to remote\npivot pull                     # Pull and restore from remote\npivot fetch                    # Fetch to local cache only\npivot verify --allow-missing   # CI gate: verify reproducibility\n\npivot import REPO PATH         # Import artifact from remote repo\npivot update --dry-run         # Check for import updates\npivot fingerprint reset        # Clear cached fingerprints\n```\n\nSee the full [CLI Reference](https://sjawhar.github.io/pivot/cli/) for all commands and options.\n\n---\n\n## Documentation\n\nFull documentation at [sjawhar.github.io/pivot/](https://sjawhar.github.io/pivot/).\n\n- [Quick Start](https://sjawhar.github.io/pivot/getting-started/quickstart/) — Build your first pipeline in 5 minutes\n- [Concepts](https://sjawhar.github.io/pivot/concepts/) — Linear learning path from first principles to advanced caching\n- [CLI Reference](https://sjawhar.github.io/pivot/cli/) — All available commands\n- [Architecture](https://sjawhar.github.io/pivot/architecture/overview/) — Design decisions and internals\n\n---\n\n## Development\n\n```bash\nuv sync --active                                                     # Install deps\nuv run pytest packages/pivot/tests packages/pivot-tui/tests -n auto  # Test\nuv run ruff format . && uv run ruff check . && uv run basedpyright   # Quality\n```\n\n---\n\n## License\n\nTBD\n\n"
      },
      "plugins": [
        {
          "name": "pivot",
          "description": "Skills for writing Pivot pipeline stages - file I/O annotations, loaders, and output types",
          "version": "0.1.0",
          "source": "./",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add sjawhar/pivot",
            "/plugin install pivot@pivot"
          ]
        }
      ]
    }
  ]
}